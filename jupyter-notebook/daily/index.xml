<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-05-29T01:45:04Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>kishan0725/AJAX-Movie-Recommendation-System-with-Sentiment-Analysis</title>
    <updated>2022-05-29T01:45:04Z</updated>
    <id>tag:github.com,2022-05-29:/kishan0725/AJAX-Movie-Recommendation-System-with-Sentiment-Analysis</id>
    <link href="https://github.com/kishan0725/AJAX-Movie-Recommendation-System-with-Sentiment-Analysis" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Content-Based Recommender System recommends movies similar to the movie user likes and analyses the sentiments on the reviews given by the user for that movie.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Content-Based-Movie-Recommender-System-with-sentiment-analysis-using-AJAX&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Python-3.8-blueviolet&#34; alt=&#34;Python&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Framework-Flask-red&#34; alt=&#34;Framework&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Frontend-HTML/CSS/JS-green&#34; alt=&#34;Frontend&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/API-TMDB-fcba03&#34; alt=&#34;API&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Updated version of this application can be found at:&lt;/strong&gt; &lt;a href=&#34;https://github.com/kishan0725/The-Movie-Cinema&#34;&gt;https://github.com/kishan0725/The-Movie-Cinema&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Content Based Recommender System recommends movies similar to the movie user likes and analyses the sentiments on the reviews given by the user for that movie.&lt;/p&gt; &#xA;&lt;p&gt;The details of the movies(title, genre, runtime, rating, poster, etc) are fetched using an API by TMDB, &lt;a href=&#34;https://www.themoviedb.org/documentation/api&#34;&gt;https://www.themoviedb.org/documentation/api&lt;/a&gt;, and using the IMDB id of the movie in the API, I did web scraping to get the reviews given by the user in the IMDB site using &lt;code&gt;beautifulsoup4&lt;/code&gt; and performed sentiment analysis on those reviews.&lt;/p&gt; &#xA;&lt;p&gt;Check out the live demo: &lt;a href=&#34;https://mrswsa.herokuapp.com/&#34;&gt;https://mrswsa.herokuapp.com/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Link to youtube demo: &lt;a href=&#34;https://www.youtube.com/watch?v=dhVePtyECFw&#34;&gt;https://www.youtube.com/watch?v=dhVePtyECFw&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Note&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;Use this URL - &lt;a href=&#34;https://the-movie-buff.herokuapp.com/&#34;&gt;https://the-movie-buff.herokuapp.com/&lt;/a&gt; - in case if you see application error in the above mentioned URL&lt;/h4&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;The Movie Cinema&lt;/h2&gt; &#xA;&lt;p&gt;I&#39;ve developed a similar application called &#34;The Movie Cinema&#34; which supports all language movies. But the only thing that differs from this application is that I&#39;ve used the TMDB&#39;s recommendation engine in &#34;The Movie Cinema&#34;. The recommendation part developed by me in this application doesn&#39;t support for multi-language movies as it consumes 200% of RAM (even after deploying it to Heroku) for generating Count Vectorizer matrix for all the 700,000+ movies in the TMDB.&lt;/p&gt; &#xA;&lt;p&gt;Link to &#34;The Movie Cinema&#34; application: &lt;a href=&#34;https://the-movie-cinema.herokuapp.com/&#34;&gt;https://the-movie-cinema.herokuapp.com/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Don&#39;t worry if the movie that you are looking for is not auto-suggested. Just type the movie name and click on &#34;enter&#34;. You will be good to go eventhough if you made some typo errors.&lt;/p&gt; &#xA;&lt;p&gt;Source Code: &lt;a href=&#34;https://github.com/kishan0725/The-Movie-Cinema&#34;&gt;https://github.com/kishan0725/The-Movie-Cinema&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Featured in Krish&#39;s Live Session on YouTube&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=A_78fGgQMjM&#34;&gt;&lt;img src=&#34;https://github.com/kishan0725/AJAX-Movie-Recommendation-System-with-Sentiment-Analysis/raw/master/static/krish-naik.PNG&#34; alt=&#34;krish youtube&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How to get the API key?&lt;/h2&gt; &#xA;&lt;p&gt;Create an account in &lt;a href=&#34;https://www.themoviedb.org/&#34;&gt;https://www.themoviedb.org/&lt;/a&gt;, click on the &lt;code&gt;API&lt;/code&gt; link from the left hand sidebar in your account settings and fill all the details to apply for API key. If you are asked for the website URL, just give &#34;NA&#34; if you don&#39;t have one. You will see the API key in your &lt;code&gt;API&lt;/code&gt; sidebar once your request is approved.&lt;/p&gt; &#xA;&lt;h2&gt;How to run the project?&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone or download this repository to your local machine.&lt;/li&gt; &#xA; &lt;li&gt;Install all the libraries mentioned in the &lt;a href=&#34;https://github.com/kishan0725/Movie-Recommendation-System-with-Sentiment-Analysis/raw/master/requirements.txt&#34;&gt;requirements.txt&lt;/a&gt; file with the command &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Get your API key from &lt;a href=&#34;https://www.themoviedb.org/&#34;&gt;https://www.themoviedb.org/&lt;/a&gt;. (Refer the above section on how to get the API key)&lt;/li&gt; &#xA; &lt;li&gt;Replace YOUR_API_KEY in &lt;strong&gt;both&lt;/strong&gt; the places (line no. 15 and 29) of &lt;code&gt;static/recommend.js&lt;/code&gt; file and hit save.&lt;/li&gt; &#xA; &lt;li&gt;Open your terminal/command prompt from your project directory and run the file &lt;code&gt;main.py&lt;/code&gt; by executing the command &lt;code&gt;python main.py&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Go to your browser and type &lt;code&gt;http://127.0.0.1:5000/&lt;/code&gt; in the address bar.&lt;/li&gt; &#xA; &lt;li&gt;Hurray! That&#39;s it.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Architecture&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/36665975/168742738-5435cf76-1a42-4d87-94b4-999e5bfc48d3.png&#34; alt=&#34;Recommendation App&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Similarity Score :&lt;/h2&gt; &#xA;&lt;p&gt;How does it decide which item is most similar to the item user likes? Here come the similarity scores.&lt;/p&gt; &#xA;&lt;p&gt;It is a numerical value ranges between zero to one which helps to determine how much two items are similar to each other on a scale of zero to one. This similarity score is obtained measuring the similarity between the text details of both of the items. So, similarity score is the measure of similarity between given text details of two items. This can be done by cosine-similarity.&lt;/p&gt; &#xA;&lt;h2&gt;How Cosine Similarity works?&lt;/h2&gt; &#xA;&lt;p&gt;Cosine similarity is a metric used to measure how similar the documents are irrespective of their size. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. The cosine similarity is advantageous because even if the two similar documents are far apart by the Euclidean distance (due to the size of the document), chances are they may still be oriented closer together. The smaller the angle, higher the cosine similarity.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/36665975/70401457-a7530680-1a55-11ea-9158-97d4e8515ca4.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;More about Cosine Similarity : &lt;a href=&#34;https://www.machinelearningplus.com/nlp/cosine-similarity/&#34;&gt;Understanding the Math behind Cosine Similarity&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Sources of the datasets&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.kaggle.com/carolzhangdc/imdb-5000-movie-dataset&#34;&gt;IMDB 5000 Movie Dataset&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.kaggle.com/rounakbanik/the-movies-dataset&#34;&gt;The Movies Dataset&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/List_of_American_films_of_2018&#34;&gt;List of movies in 2018&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/List_of_American_films_of_2019&#34;&gt;List of movies in 2019&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/List_of_American_films_of_2020&#34;&gt;List of movies in 2020&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/AI-For-Beginners</title>
    <updated>2022-05-29T01:45:04Z</updated>
    <id>tag:github.com,2022-05-29:/microsoft/AI-For-Beginners</id>
    <link href="https://github.com/microsoft/AI-For-Beginners" rel="alternate"></link>
    <summary type="html">&lt;p&gt;12 Weeks, 24 Lessons, AI for All!&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/AI-For-Beginners/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/microsoft/AI-For-Beginners.svg?sanitize=true&#34; alt=&#34;GitHub license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/AI-For-Beginners/graphs/contributors/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/microsoft/AI-For-Beginners.svg?sanitize=true&#34; alt=&#34;GitHub contributors&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/AI-For-Beginners/issues/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/microsoft/AI-For-Beginners.svg?sanitize=true&#34; alt=&#34;GitHub issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/AI-For-Beginners/pulls/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-pr/microsoft/AI-For-Beginners.svg?sanitize=true&#34; alt=&#34;GitHub pull-requests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://makeapullrequest.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square&#34; alt=&#34;PRs Welcome&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://GitHub.com/microsoft/AI-For-Beginners/watchers/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/watchers/microsoft/AI-For-Beginners.svg?style=social&amp;amp;label=Watch&#34; alt=&#34;GitHub watchers&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/AI-For-Beginners/network/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/forks/microsoft/AI-For-Beginners.svg?style=social&amp;amp;label=Fork&#34; alt=&#34;GitHub forks&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/AI-For-Beginners/stargazers/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/microsoft/AI-For-Beginners.svg?style=social&amp;amp;label=Star&#34; alt=&#34;GitHub stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://mybinder.org/v2/gh/microsoft/ai-for-beginners/HEAD&#34;&gt;&lt;img src=&#34;https://mybinder.org/badge_logo.svg?sanitize=true&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitter.im/Microsoft/ai-for-beginners?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/Microsoft/ai-for-beginners.svg?sanitize=true&#34; alt=&#34;Gitter&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Artificial Intelligence for Beginners - A Curriculum&lt;/h1&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/sketchnotes/ai-overview.png&#34; alt=&#34; Sketchnote by (@girlie_mac) &#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;AI For Beginners - &lt;em&gt;Sketchnote by &lt;a href=&#34;https://twitter.com/girlie_mac&#34;&gt;@girlie_mac&lt;/a&gt;&lt;/em&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Azure Cloud Advocates at Microsoft are pleased to offer a 12-week, 24-lesson curriculum all about &lt;strong&gt;Artificial Intelligence&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In this curriculum, you will learn:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Different approaches to Artificial Intelligence, including the &#34;good old&#34; symbolic approach with &lt;strong&gt;Knowledge Representation&lt;/strong&gt; and reasoning (&lt;a href=&#34;https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence&#34;&gt;GOFAI&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Neural Networks&lt;/strong&gt; and &lt;strong&gt;Deep Learning&lt;/strong&gt;, which are at the core of modern AI. We will illustrate the concepts behind these important topics using code in two of the most popular frameworks - &lt;a href=&#34;http://Tensorflow.org&#34;&gt;TensorFlow&lt;/a&gt; and &lt;a href=&#34;http://pytorch.org&#34;&gt;PyTorch&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Neural Architectures&lt;/strong&gt; for working with images and text. We will cover recent models but may lack a little bit on the state-of-the-art.&lt;/li&gt; &#xA; &lt;li&gt;Less popular AI approaches, such as &lt;strong&gt;Genetic Algorithms&lt;/strong&gt; and &lt;strong&gt;Multi-Agent Systems&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;What we will not cover in this curriculum:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Business cases of using &lt;strong&gt;AI in Business&lt;/strong&gt;. Consider taking &lt;a href=&#34;https://docs.microsoft.com/learn/paths/introduction-ai-for-business-users/?WT.mc_id=academic-57639-dmitryso&#34;&gt;Introduction to AI for business users&lt;/a&gt; learning path on Microsoft Learn, or &lt;a href=&#34;https://www.microsoft.com/ai/ai-business-school/?WT.mc_id=academic-57639-dmitryso&#34;&gt;AI Business School&lt;/a&gt;, developed in cooperation with &lt;a href=&#34;https://www.insead.edu/&#34;&gt;INSEAD&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Classic Machine Learning&lt;/strong&gt;, which is well described in our &lt;a href=&#34;http://github.com/Microsoft/ML-for-Beginners&#34;&gt;Machine Learning for Beginners Curriculum&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Practical AI applications built using &lt;strong&gt;&lt;a href=&#34;https://azure.microsoft.com/services/cognitive-services/?WT.mc_id=academic-57639-dmitryso&#34;&gt;Cognitive Services&lt;/a&gt;&lt;/strong&gt;. For this, we recommend that you start with modules Microsoft Learn for &lt;a href=&#34;https://docs.microsoft.com/learn/paths/create-computer-vision-solutions-azure-cognitive-services/?WT.mc_id=academic-57639-dmitryso&#34;&gt;vision&lt;/a&gt;, &lt;a href=&#34;https://docs.microsoft.com/learn/paths/explore-natural-language-processing/?WT.mc_id=academic-57639-dmitryso&#34;&gt;natural language processing&lt;/a&gt; and others.&lt;/li&gt; &#xA; &lt;li&gt;Specific ML &lt;strong&gt;Cloud Frameworks&lt;/strong&gt;, such as &lt;a href=&#34;https://azure.microsoft.com/services/machine-learning/?WT.mc_id=academic-57639-dmitryso&#34;&gt;Azure Machine Learning&lt;/a&gt; or &lt;a href=&#34;https://docs.microsoft.com/learn/paths/data-engineer-azure-databricks?WT.mc_id=academic-57639-dmitryso&#34;&gt;Azure Databricks&lt;/a&gt;. Consider using &lt;a href=&#34;https://docs.microsoft.com/learn/paths/build-ai-solutions-with-azure-ml-service/?WT.mc_id=academic-57639-dmitryso&#34;&gt;Build and operate machine learning solutions with Azure Machine Learning&lt;/a&gt; and &lt;a href=&#34;https://docs.microsoft.com/learn/paths/build-operate-machine-learning-solutions-azure-databricks/?WT.mc_id=academic-57639-dmitryso&#34;&gt;Build and Operate Machine Learning Solutions with Azure Databricks&lt;/a&gt; learning paths.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Conversational AI&lt;/strong&gt; and &lt;strong&gt;Chat Bots&lt;/strong&gt;. There is a separate &lt;a href=&#34;https://docs.microsoft.com/learn/paths/create-conversational-ai-solutions/?WT.mc_id=academic-57639-dmitryso&#34;&gt;Create conversational AI solutions&lt;/a&gt; learning path, and you can also refer to &lt;a href=&#34;https://soshnikov.com/azure/hello-bot-conversational-ai-on-microsoft-platform/&#34;&gt;this blog post&lt;/a&gt; for more detail.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Deep Mathematics&lt;/strong&gt; behind deep learning. For this, we would recommend &lt;a href=&#34;https://www.amazon.com/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618&#34;&gt;Deep Learning&lt;/a&gt; by Ian Goodfellow, Yoshua Bengio and Aaron Courville, which is also available online at &lt;a href=&#34;https://www.deeplearningbook.org/&#34;&gt;https://www.deeplearningbook.org/&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For a gentle introduction to &lt;em&gt;AI in the Cloud&lt;/em&gt; topics you may consider taking the &lt;a href=&#34;https://docs.microsoft.com/learn/paths/get-started-with-artificial-intelligence-on-azure/?WT.mc_id=academic-57639-dmitryso&#34;&gt;Get started with artificial intelligence on Azure&lt;/a&gt; Learning Path.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Content&lt;/h1&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;th&gt;No&lt;/th&gt;&#xA;   &lt;th&gt;Lesson&lt;/th&gt;&#xA;   &lt;th&gt;Intro&lt;/th&gt;&#xA;   &lt;th&gt;PyTorch&lt;/th&gt;&#xA;   &lt;th&gt;Keras/TensorFlow&lt;/th&gt;&#xA;   &lt;th&gt;Lab&lt;/th&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;I&lt;/td&gt;&#xA;   &lt;td colspan=&#34;4&#34;&gt;&lt;b&gt;Introduction to AI&lt;/b&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;1&lt;/td&gt;&#xA;   &lt;td&gt;Introduction and History of AI&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/1-Intro/README.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;II&lt;/td&gt;&#xA;   &lt;td colspan=&#34;4&#34;&gt;&lt;b&gt;Symbolic AI&lt;/b&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;2 &lt;/td&gt;&#xA;   &lt;td&gt;Knowledge Representation and Expert Systems&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/2-Symbolic/README.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td colspan=&#34;2&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/2-Symbolic/Animals.ipynb&#34;&gt;Expert System&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/2-Symbolic/FamilyOntology.ipynb&#34;&gt;Ontology&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/2-Symbolic/MSConceptGraph.ipynb&#34;&gt;Concept Graph&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;III&lt;/td&gt;&#xA;   &lt;td colspan=&#34;4&#34;&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/README.md&#34;&gt;Introduction to Neural Networks&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;3&lt;/td&gt;&#xA;   &lt;td&gt;Perceptron&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/03-Perceptron/README.md&#34;&gt;Text&lt;/a&gt; &lt;/td&gt;&#xA;   &lt;td colspan=&#34;2&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/03-Perceptron/Perceptron.ipynb&#34;&gt;Notebook&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/03-Perceptron/lab/README.md&#34;&gt;Lab&lt;/a&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;4 &lt;/td&gt;&#xA;   &lt;td&gt;Multi-Layered Perceptron and Creating our own Framework&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/04-OwnFramework/README.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td colspan=&#34;2&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb&#34;&gt;Notebook&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/04-OwnFramework/lab/README.md&#34;&gt;Lab&lt;/a&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;Intro to Frameworks (PyTorch/TensorFlow)&lt;br&gt;Overfitting&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/05-Frameworks/README.md&#34;&gt;Text&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/05-Frameworks/Overfitting.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/05-Frameworks/IntroPyTorch.ipynb&#34;&gt;PyTorch&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/05-Frameworks/IntroKerasTF.md&#34;&gt;Keras/TensorFlow&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/05-Frameworks/lab/README.md&#34;&gt;Lab&lt;/a&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;IV&lt;/td&gt;&#xA;   &lt;td&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/README.md&#34;&gt;Computer Vision&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;3&#34;&gt;&lt;a href=&#34;https://docs.microsoft.com/learn/paths/explore-computer-vision-microsoft-azure/?WT.mc_id=academic-57639-dmitryso&#34;&gt;&lt;i&gt;AI Fundamentals: Explore Computer Vision&lt;/i&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;   &lt;td colspan=&#34;2&#34;&gt;&lt;i&gt;Microsoft Learn Module on Computer Vision&lt;/i&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.microsoft.com/learn/modules/intro-computer-vision-pytorch/?WT.mc_id=academic-57639-dmitryso&#34;&gt;&lt;i&gt;PyTorch&lt;/i&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.microsoft.com/learn/modules/intro-computer-vision-TensorFlow/?WT.mc_id=academic-57639-dmitryso&#34;&gt;&lt;i&gt;TensorFlow&lt;/i&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;6&lt;/td&gt;&#xA;   &lt;td&gt;Intro to Computer Vision. OpenCV&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/06-IntroCV/README.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td colspan=&#34;2&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/06-IntroCV/OpenCV.ipynb&#34;&gt;Notebook&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/06-IntroCV/lab/README.md&#34;&gt;Lab&lt;/a&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;7&lt;/td&gt;&#xA;   &lt;td&gt;Convolutional Neural Networks&lt;br&gt;CNN Architectures&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/07-ConvNets/README.md&#34;&gt;Text&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/07-ConvNets/CNN_Architectures.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/07-ConvNets/ConvNetsPyTorch.ipynb&#34;&gt;PyTorch&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/07-ConvNets/ConvNetsTF.ipynb&#34;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/07-ConvNets/lab/README.md&#34;&gt;Lab&lt;/a&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;8&lt;/td&gt;&#xA;   &lt;td&gt;Pre-trained Networks and Transfer Learning&lt;br&gt;Training Tricks&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/08-TransferLearning/README.md&#34;&gt;Text&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/08-TransferLearning/TrainingTricks.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/08-TransferLearning/TransferLearningPyTorch.ipynb&#34;&gt;PyTorch&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/08-TransferLearning/TransferLearningTF.ipynb&#34;&gt;TensorFlow&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/08-TransferLearning/Dropout.ipynb&#34;&gt;Dropout sample&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/08-TransferLearning/lab/README.md&#34;&gt;Lab&lt;/a&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;9&lt;/td&gt;&#xA;   &lt;td&gt;Autoencoders and VAEs&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/09-Autoencoders/README.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/09-Autoencoders/AutoEncodersPytorch.ipynb&#34;&gt;PyTorch&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/09-Autoencoders/AutoencodersTF.ipynb&#34;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;10&lt;/td&gt;&#xA;   &lt;td&gt;Generative Adversarial Networks&lt;br&gt;Artistic Style Transfer&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/10-GANs/README.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/10-GANs/GANPyTorch.ipynb&#34;&gt;PyTorch&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/10-GANs/GANTF.ipynb&#34;&gt;TensorFlow GAN&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/10-GANs/StyleTransfer.ipynb&#34;&gt;Style Transfer&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;11&lt;/td&gt;&#xA;   &lt;td&gt;Object Detection&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/11-ObjectDetection/README.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;PyTorch&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/11-ObjectDetection/ObjectDetection-TF.ipynb&#34;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/11-ObjectDetection/lab/README.md&#34;&gt;Lab&lt;/a&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;12&lt;/td&gt;&#xA;   &lt;td&gt;Semantic Segmentation. U-Net&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/12-Segmentation/README.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/12-Segmentation/SemanticSegmentationPytorch.ipynb&#34;&gt;PyTorch&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/12-Segmentation/SemanticSegmentationTF.ipynb&#34;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;V&lt;/td&gt;&#xA;   &lt;td&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/README.md&#34;&gt;Natural Language Processing&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;3&#34;&gt;&lt;a href=&#34;https://docs.microsoft.com/learn/paths/explore-natural-language-processing/?WT.mc_id=academic-57639-dmitryso&#34;&gt;&lt;i&gt;AI Fundamentals: Explore Natural Language Processing&lt;/i&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;   &lt;td colspan=&#34;2&#34;&gt;&lt;i&gt;Microsoft Learn Module on Natural Language&lt;/i&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-57639-dmitryso&#34;&gt;&lt;i&gt;PyTorch&lt;/i&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.microsoft.com/learn/modules/intro-natural-language-processing-TensorFlow/?WT.mc_id=academic-57639-dmitryso&#34;&gt;&lt;i&gt;TensorFlow&lt;/i&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;13&lt;/td&gt;&#xA;   &lt;td&gt;Text Representation. Bow/TF-IDF&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/13-TextRep/README.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/13-TextRep/TextRepresentationPyTorch.ipynb&#34;&gt;PyTorch&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/13-TextRep/TextRepresentationTF.ipynb&#34;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;14&lt;/td&gt;&#xA;   &lt;td&gt;Semantic word embeddings. Word2Vec and GloVe&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/14-Embeddings/README.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/14-Embeddings/EmbeddingsPyTorch.ipynb&#34;&gt;PyTorch&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb&#34;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;15&lt;/td&gt;&#xA;   &lt;td&gt;Language Modeling. Training your own embeddings&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/15-LanguageModeling/README.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/15-LanguageModeling/CBoW-TF.ipynb&#34;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/15-LanguageModeling/lab/README.md&#34;&gt;Lab&lt;/a&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;16&lt;/td&gt;&#xA;   &lt;td&gt;Recurrent Neural Networks&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/16-RNN/README.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/16-RNN/RNNPyTorch.ipynb&#34;&gt;PyTorch&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/16-RNN/RNNTF.ipynb&#34;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;17&lt;/td&gt;&#xA;   &lt;td&gt;Generative Recurrent Networks&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/17-GenerativeNetworks/README.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/17-GenerativeNetworks/GenerativePyTorch.md&#34;&gt;PyTorch&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/17-GenerativeNetworks/GenerativeTF.md&#34;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/17-GenerativeNetworks/lab/README.md&#34;&gt;Lab&lt;/a&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;18&lt;/td&gt;&#xA;   &lt;td&gt;Transformers. BERT.&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/18-Transformers/README.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/18-Transformers/TransformersPyTorch.md&#34;&gt;PyTorch&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/18-Transformers/TransformersTF.md&#34;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;19&lt;/td&gt;&#xA;   &lt;td&gt;Named Entity Recognition&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/19-NER/README.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/19-NER/NER-TF.ipynb&#34;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/19-NER/lab/README.md&#34;&gt;Lab&lt;/a&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;20&lt;/td&gt;&#xA;   &lt;td&gt;Large Language Models, Prompt Programming and Few-Shot Tasks&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/20-LangModels/README.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/20-LangModels/GPT-PyTorch.ipynb&#34;&gt;PyTorch&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;VI&lt;/td&gt;&#xA;   &lt;td colspan=&#34;4&#34;&gt;&lt;b&gt;Other AI Techniques&lt;/b&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;21&lt;/td&gt;&#xA;   &lt;td&gt;Genetic Algorithms&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/6-Other/21-GeneticAlgorithms/README.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td colspan=&#34;2&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/6-Other/21-GeneticAlgorithms/Genetic.ipynb&#34;&gt;Notebook&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;22&lt;/td&gt;&#xA;   &lt;td&gt;Deep Reinforcement Learning&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/6-Other/22-DeepRL/README.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb&#34;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/6-Other/22-DeepRL/lab/README.md&#34;&gt;Lab&lt;/a&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;23&lt;/td&gt;&#xA;   &lt;td&gt;Multi-Agent Systems&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/6-Other/23-MultiagentSystems/README.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;VII&lt;/td&gt;&#xA;   &lt;td colspan=&#34;4&#34;&gt;&lt;b&gt;AI Ethics&lt;/b&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;24&lt;/td&gt;&#xA;   &lt;td&gt;AI Ethics and Responsible AI&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/7-Ethics/README.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td colspan=&#34;2&#34;&gt;&lt;a href=&#34;https://docs.microsoft.com/learn/paths/responsible-ai-business-principles/?WT.mc_id=academic-57639-dmitryso&#34;&gt;&lt;i&gt;MS Learn: Responsible AI Principles&lt;/i&gt;&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;   &lt;td colspan=&#34;4&#34;&gt;&lt;b&gt;Extras&lt;/b&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;X1&lt;/td&gt;&#xA;   &lt;td&gt;Multi-Modal Networks, CLIP and VQGAN&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/X-Extras/X1-MultiModal/README.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td colspan=&#34;2&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/X-Extras/X1-MultiModal/Clip.ipynb&#34;&gt;Notebook&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;http://soshnikov.com/courses/ai-for-beginners/mindmap.html&#34;&gt;Mindmap of the Course&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Each lesson contains some pre-reading material (linked as &lt;strong&gt;Text&lt;/strong&gt; above), and some executable Jupyter Notebooks, which are often specific to the framework (&lt;strong&gt;PyTorch&lt;/strong&gt; or &lt;strong&gt;TensorFlow&lt;/strong&gt;). The executable notebook also contains a lot of theoretical material, so to understand the topic you need to go through at least one version of the notebooks (either PyTorch or TensorFlow). There are also &lt;strong&gt;Labs&lt;/strong&gt; available for some topics, which give you an opportunity to try applying the material you have learned to a specific problem.&lt;/p&gt; &#xA;&lt;p&gt;Some sections also contain links to &lt;strong&gt;MS Learn&lt;/strong&gt; modules that cover related topics. Microsoft Learn provides a convenient GPU-enabled learning environment, although in terms of content you can expect this curriculum to go a bit deeper.&lt;/p&gt; &#xA;&lt;h1&gt;Getting Started&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Students&lt;/strong&gt;, there are a couple of ways to use the curriculum. First of all, you can just read the text and look through the code directly on GitHub. If you want to run the code in any of the notebooks - &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/etc/how-to-run.md&#34;&gt;read our instructions&lt;/a&gt;, and find more advice on how to do it &lt;a href=&#34;https://soshnikov.com/education/how-to-execute-notebooks-from-github/&#34;&gt;in this blog post&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/etc/how-to-run.md&#34;&gt;Instructions on how to run the code in this curriculum&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;However, if you would like to take the course as a self-study project, we suggest that you fork the entire repo to your own GitHub account and complete the exercises on your own or with a group:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Start with a pre-lecture quiz&lt;/li&gt; &#xA; &lt;li&gt;Read the intro text for the lecture&lt;/li&gt; &#xA; &lt;li&gt;If the lecture has additional notebooks, go through them, reading and executing the code. If both TensorFlow and PyTorch notebooks are provided, you can focus on one of them - chose your favorite framework&lt;/li&gt; &#xA; &lt;li&gt;Notebooks often contain some of the challenges that require you to tweak the code a little bit to experiment&lt;/li&gt; &#xA; &lt;li&gt;Take the post-lecture quiz&lt;/li&gt; &#xA; &lt;li&gt;If there is a lab attached to the module - complete the assignment&lt;/li&gt; &#xA; &lt;li&gt;Visit the &lt;a href=&#34;https://github.com/microsoft/AI-For-Beginners/discussions&#34;&gt;Discussion board&lt;/a&gt; to &#34;learn out loud&#34;.&lt;/li&gt; &#xA; &lt;li&gt;Chat with other learners &lt;a href=&#34;https://gitter.im/Microsoft/ai-for-beginners&#34;&gt;on Gitter&lt;/a&gt; or &lt;a href=&#34;http://t.me/ai_for_beginners&#34;&gt;in Telegram channel&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;For further study, we recommend following these &lt;a href=&#34;https://docs.microsoft.com/en-us/users/dmitrysoshnikov-9132/collections/31zgizg2p418yo/?WT.mc_id=academic-57639-dmitryso&#34;&gt;Microsoft Learn&lt;/a&gt; modules and learning paths.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;Teachers&lt;/strong&gt;, we have &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/etc/for-teachers.md&#34;&gt;included some suggestions&lt;/a&gt; on how to use this curriculum.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;‚úçÔ∏è Primary Author:&lt;/strong&gt; &lt;a href=&#34;http://soshnikov.com&#34;&gt;Dmitry Soshnikov&lt;/a&gt;, PhD &lt;br&gt; &lt;strong&gt;üî• Editor:&lt;/strong&gt; &lt;a href=&#34;https://twitter.com/jenlooper&#34;&gt;Jen Looper&lt;/a&gt;, PhD &lt;br&gt; &lt;strong&gt;üé® Sketchnote illustrator:&lt;/strong&gt; &lt;a href=&#34;https://twitter.com/girlie_mac&#34;&gt;Tomomi Imura&lt;/a&gt; &lt;br&gt; &lt;strong&gt;‚úÖ Quiz Creator:&lt;/strong&gt; &lt;a href=&#34;https://github.com/CinnamonXI&#34;&gt;Lateefah Bello&lt;/a&gt;, &lt;a href=&#34;https://studentambassadors.microsoft.com/&#34;&gt;MLSA&lt;/a&gt; &lt;br&gt; &lt;strong&gt;üôè Core Contributors:&lt;/strong&gt; &lt;a href=&#34;https://github.com/Pe4enIks&#34;&gt;Evgenii Pishchik&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Meet the Team&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/m2KrAk0cC1c&#34; title=&#34;Promo video&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/sketchnotes/ai-for-beginners.png&#34; alt=&#34;Promo video&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;üé• Click the image above for a video about the project and the folks who created it!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Pedagogy&lt;/h2&gt; &#xA;&lt;p&gt;We have chosen two pedagogical tenets while building this curriculum: ensuring that it is hands-on &lt;strong&gt;project-based&lt;/strong&gt; and that it includes &lt;strong&gt;frequent quizzes&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;By ensuring that the content aligns with projects, the process is made more engaging for students and retention of concepts will be augmented. In addition, a low-stakes quiz before a class sets the intention of the student towards learning a topic, while a second quiz after class ensures further retention. This curriculum was designed to be flexible and fun and can be taken in whole or in part. The projects start small and become increasingly complex by the end of the 12 week cycle.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Find our &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/etc/CODE_OF_CONDUCT.md&#34;&gt;Code of Conduct&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/etc/CONTRIBUTING.md&#34;&gt;Contributing&lt;/a&gt;, and &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/etc/TRANSLATIONS.md&#34;&gt;Translation&lt;/a&gt; guidelines. Find our &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/etc/SUPPORT.md&#34;&gt;Support Documentation here&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/etc/SECURITY.md&#34;&gt;security information here&lt;/a&gt;. We welcome your constructive feedback!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;A note about quizzes&lt;/strong&gt;: All quizzes are contained &lt;a href=&#34;https://black-ground-0cc93280f.1.azurestaticapps.net/&#34;&gt;in this app&lt;/a&gt;, for 50 total quizzes of three questions each. They are linked from within the lessons but the quiz app can be run locally; follow the instruction in the &lt;code&gt;etc/quiz-app&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Offline access&lt;/h2&gt; &#xA;&lt;p&gt;You can run this documentation offline by using &lt;a href=&#34;https://docsify.js.org/#/&#34;&gt;Docsify&lt;/a&gt;. Fork this repo, &lt;a href=&#34;https://docsify.js.org/#/quickstart&#34;&gt;install Docsify&lt;/a&gt; on your local machine, and then in the &lt;code&gt;etc/docsify&lt;/code&gt; folder of this repo, type &lt;code&gt;docsify serve&lt;/code&gt;. The website will be served on port 3000 on your localhost: &lt;code&gt;localhost:3000&lt;/code&gt;. A pdf of the curriculum is available &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/etc/pdf/readme.pdf&#34;&gt;at this link&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Help Wanted!&lt;/h2&gt; &#xA;&lt;p&gt;Would you like to contribute a translation? Please read our &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/etc/TRANSLATIONS.md&#34;&gt;translation guidelines&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Other Curricula&lt;/h2&gt; &#xA;&lt;p&gt;Our team produces other curricula! Check out:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/webdev-beginners&#34;&gt;Web Dev for Beginners&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/iot-beginners&#34;&gt;IoT for Beginners&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://aka.ms/ML-for-Beginners&#34;&gt;Machine Learning for Beginners&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://aka.ms/Data-Science-for-Beginners&#34;&gt;Data Science for Beginners&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>CoreyMSchafer/code_snippets</title>
    <updated>2022-05-29T01:45:04Z</updated>
    <id>tag:github.com,2022-05-29:/CoreyMSchafer/code_snippets</id>
    <link href="https://github.com/CoreyMSchafer/code_snippets" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;code_snippets&lt;/h1&gt;</summary>
  </entry>
  <entry>
    <title>eladrich/pixel2style2pixel</title>
    <updated>2022-05-29T01:45:04Z</updated>
    <id>tag:github.com,2022-05-29:/eladrich/pixel2style2pixel</id>
    <link href="https://github.com/eladrich/pixel2style2pixel" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official Implementation for &#34;Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation&#34; (CVPR 2021) presenting the pixel2style2pixel (pSp) framework&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2008.00951&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2008.00951-b31b1b.svg?sanitize=true&#34; height=&#34;22.5&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true&#34; height=&#34;22.5&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=bfvSwhqsTgM&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=CVPR 2021&amp;amp;message=5 Minute Video&amp;amp;color=red&#34; height=&#34;22.5&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://replicate.ai/eladrich/pixel2style2pixel&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Replicate&amp;amp;message=Demo and Docker Image&amp;amp;color=darkgreen&#34; height=&#34;22.5&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://colab.research.google.com/github/eladrich/pixel2style2pixel/blob/master/notebooks/inference_playground.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; height=&#34;22.5&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;We present a generic image-to-image translation framework, pixel2style2pixel (pSp). Our pSp framework is based on a novel encoder network that directly generates a series of style vectors which are fed into a pretrained StyleGAN generator, forming the extended W+ latent space. We first show that our encoder can directly embed real images into W+, with no additional optimization. Next, we propose utilizing our encoder to directly solve image-to-image translation tasks, defining them as encoding problems from some input domain into the latent domain. By deviating from the standard &#34;invert first, edit later&#34; methodology used with previous StyleGAN encoders, our approach can handle a variety of tasks even when the input image is not represented in the StyleGAN domain. We show that solving translation tasks through StyleGAN significantly simplifies the training process, as no adversary is required, has better support for solving tasks without pixel-to-pixel correspondence, and inherently supports multi-modal synthesis via the resampling of styles. Finally, we demonstrate the potential of our framework on a variety of facial image-to-image translation tasks, even when compared to state-of-the-art solutions designed specifically for a single task, and further show that it can be extended beyond the human facial domain.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/docs/teaser.png&#34; width=&#34;800px&#34;&gt; &lt;br&gt; The proposed pixel2style2pixel framework can be used to solve a wide variety of image-to-image translation tasks. Here we show results of pSp on StyleGAN inversion, multi-modal conditional image synthesis, facial frontalization, inpainting and super-resolution. &lt;/p&gt; &#xA;&lt;h2&gt;Description&lt;/h2&gt; &#xA;&lt;p&gt;Official Implementation of our pSp paper for both training and evaluation. The pSp method extends the StyleGAN model to allow solving different image-to-image translation problems using its encoder.&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/#description&#34;&gt;Description&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/#table-of-contents&#34;&gt;Table of Contents&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/#recent-updates&#34;&gt;Recent Updates&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/#applications&#34;&gt;Applications&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/#stylegan-encoding&#34;&gt;StyleGAN Encoding&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/#face-frontalization&#34;&gt;Face Frontalization&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/#conditional-image-synthesis&#34;&gt;Conditional Image Synthesis&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/#super-resolution&#34;&gt;Super Resolution&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/#getting-started&#34;&gt;Getting Started&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/#prerequisites&#34;&gt;Prerequisites&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/#inference-notebook&#34;&gt;Inference Notebook&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/#pretrained-models&#34;&gt;Pretrained Models&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/#training&#34;&gt;Training&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/#preparing-your-data&#34;&gt;Preparing your Data&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/#training-psp&#34;&gt;Training pSp&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/#training-the-psp-encoder&#34;&gt;Training the pSp Encoder&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/#frontalization&#34;&gt;Frontalization&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/#sketch-to-face&#34;&gt;Sketch to Face&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/#segmentation-map-to-face&#34;&gt;Segmentation Map to Face&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/#super-resolution-1&#34;&gt;Super Resolution&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/#additional-notes&#34;&gt;Additional Notes&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/#weights--biases-integration&#34;&gt;Weights &amp;amp; Biases Integration&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/#testing&#34;&gt;Testing&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/#inference&#34;&gt;Inference&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/#multi-modal-synthesis-with-style-mixing&#34;&gt;Multi-Modal Synthesis with Style-Mixing&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/#computing-metrics&#34;&gt;Computing Metrics&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/#additional-applications&#34;&gt;Additional Applications&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/#toonify&#34;&gt;Toonify&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/#repository-structure&#34;&gt;Repository structure&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/#todos&#34;&gt;TODOs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/#credits&#34;&gt;Credits&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/#inspired-by-psp&#34;&gt;Inspired by pSp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/#psp-in-the-media&#34;&gt;pSp in the Media&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Recent Updates&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;2020.10.04&lt;/code&gt;&lt;/strong&gt;: Initial code release&lt;br&gt; &lt;strong&gt;&lt;code&gt;2020.10.06&lt;/code&gt;&lt;/strong&gt;: Add pSp toonify model (Thanks to the great work from &lt;a href=&#34;https://linktr.ee/Norod78&#34;&gt;Doron Adler&lt;/a&gt; and &lt;a href=&#34;https://www.justinpinkney.com/&#34;&gt;Justin Pinkney&lt;/a&gt;)!&lt;br&gt; &lt;strong&gt;&lt;code&gt;2021.04.23&lt;/code&gt;&lt;/strong&gt;: Added several new features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Added supported for StyleGANs of different resolutions (e.g., 256, 512, 1024). This can be set using the flag &lt;code&gt;--output_size&lt;/code&gt;, which is set to 1024 by default.&lt;/li&gt; &#xA; &lt;li&gt;Added support for the MoCo-Based similarity loss introduced in &lt;a href=&#34;https://github.com/omertov/encoder4editing&#34;&gt;encoder4editing (Tov et al. 2021)&lt;/a&gt;. More details are provided &lt;a href=&#34;https://github.com/eladrich/pixel2style2pixel#training-psp&#34;&gt;below&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;2021.07.06&lt;/code&gt;&lt;/strong&gt;: Added support for training with Weights &amp;amp; Biases. &lt;a href=&#34;https://github.com/eladrich/pixel2style2pixel#weights--biases-integration&#34;&gt;See below for details&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Applications&lt;/h2&gt; &#xA;&lt;h3&gt;StyleGAN Encoding&lt;/h3&gt; &#xA;&lt;p&gt;Here, we use pSp to find the latent code of real images in the latent domain of a pretrained StyleGAN generator.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/docs/encoding_inputs.jpg&#34; width=&#34;800px&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/docs/encoding_outputs.jpg&#34; width=&#34;800px&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Face Frontalization&lt;/h3&gt; &#xA;&lt;p&gt;In this application we want to generate a front-facing face from a given input image.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/docs/frontalization_inputs.jpg&#34; width=&#34;800px&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/docs/frontalization_outputs.jpg&#34; width=&#34;800px&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Conditional Image Synthesis&lt;/h3&gt; &#xA;&lt;p&gt;Here we wish to generate photo-realistic face images from ambiguous sketch images or segmentation maps. Using style-mixing, we inherently support multi-modal synthesis for a single input.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/docs/seg2image.png&#34; width=&#34;800px&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/docs/sketch2image.png&#34; width=&#34;800px&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Super Resolution&lt;/h3&gt; &#xA;&lt;p&gt;Given a low-resolution input image, we generate a corresponding high-resolution image. As this too is an ambiguous task, we can use style-mixing to produce several plausible results.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/docs/super_res_32.jpg&#34; width=&#34;800px&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/docs/super_res_style_mixing.jpg&#34; width=&#34;800px&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Linux or macOS&lt;/li&gt; &#xA; &lt;li&gt;NVIDIA GPU + CUDA CuDNN (CPU may be possible with some modifications, but is not inherently supported)&lt;/li&gt; &#xA; &lt;li&gt;Python 2 or 3&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Clone this repo:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/eladrich/pixel2style2pixel.git&#xA;cd pixel2style2pixel&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Dependencies:&lt;br&gt; We recommend running this repository using &lt;a href=&#34;https://docs.anaconda.com/anaconda/install/&#34;&gt;Anaconda&lt;/a&gt;. All dependencies for defining the environment are provided in &lt;code&gt;environment/psp_env.yaml&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Inference Notebook&lt;/h3&gt; &#xA;&lt;p&gt;To help visualize the pSp framework on multiple tasks and to help you get started, we provide a Jupyter notebook found in &lt;code&gt;notebooks/inference_playground.ipynb&lt;/code&gt; that allows one to visualize the various applications of pSp.&lt;br&gt; The notebook will download the necessary pretrained models and run inference on the images found in &lt;code&gt;notebooks/images&lt;/code&gt;.&lt;br&gt; For the tasks of conditional image synthesis and super resolution, the notebook also demonstrates pSp&#39;s ability to perform multi-modal synthesis using style-mixing.&lt;/p&gt; &#xA;&lt;h3&gt;Pretrained Models&lt;/h3&gt; &#xA;&lt;p&gt;Please download the pre-trained models from the following links. Each pSp model contains the entire pSp architecture, including the encoder and decoder weights.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Path&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1bMTNWkh5LArlaWSc_wa8VKyq2V42T2z0/view?usp=sharing&#34;&gt;StyleGAN Inversion&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;pSp trained with the FFHQ dataset for StyleGAN inversion.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1_S4THAzXb-97DbpXmanjHtXRyKxqjARv/view?usp=sharing&#34;&gt;Face Frontalization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;pSp trained with the FFHQ dataset for face frontalization.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1lB7wk7MwtdxL-LL4Z_T76DuCfk00aSXA/view?usp=sharing&#34;&gt;Sketch to Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;pSp trained with the CelebA-HQ dataset for image synthesis from sketches.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1VpEKc6E6yG3xhYuZ0cq8D2_1CbT0Dstz/view?usp=sharing&#34;&gt;Segmentation to Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;pSp trained with the CelebAMask-HQ dataset for image synthesis from segmentation maps.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1ZpmSXBpJ9pFEov6-jjQstAlfYbkebECu/view?usp=sharing&#34;&gt;Super Resolution&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;pSp trained with the CelebA-HQ dataset for super resolution (up to x32 down-sampling).&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1YKoiVuFaqdvzDP5CZaqa3k5phL-VDmyz/view&#34;&gt;Toonify&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;pSp trained with the FFHQ dataset for toonification using StyleGAN generator from &lt;a href=&#34;https://linktr.ee/Norod78&#34;&gt;Doron Adler&lt;/a&gt; and &lt;a href=&#34;https://www.justinpinkney.com/&#34;&gt;Justin Pinkney&lt;/a&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;If you wish to use one of the pretrained models for training or inference, you may do so using the flag &lt;code&gt;--checkpoint_path&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In addition, we provide various auxiliary models needed for training your own pSp model from scratch as well as pretrained models needed for computing the ID metrics reported in the paper.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Path&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1EM87UquaoQmk17Q8d5kYIAHqu0dkYqdT/view?usp=sharing&#34;&gt;FFHQ StyleGAN&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;StyleGAN model pretrained on FFHQ taken from &lt;a href=&#34;https://github.com/rosinality/stylegan2-pytorch&#34;&gt;rosinality&lt;/a&gt; with 1024x1024 output resolution.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1KW7bjndL3QG3sxBbZxreGHigcCCpsDgn/view?usp=sharing&#34;&gt;IR-SE50 Model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Pretrained IR-SE50 model taken from &lt;a href=&#34;https://github.com/TreB1eN/InsightFace_Pytorch&#34;&gt;TreB1eN&lt;/a&gt; for use in our ID loss during pSp training.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/18rLcNGdteX5LwT7sv_F7HWr12HpVEzVe/view?usp=sharing&#34;&gt;MoCo ResNet-50&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Pretrained ResNet-50 model trained using MOCOv2 for computing MoCo-based similarity loss on non-facial domains. The model is taken from the &lt;a href=&#34;https://github.com/facebookresearch/moco&#34;&gt;official implementation&lt;/a&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1f4IwVa2-Bn9vWLwB-bUwm53U_MlvinAj/view?usp=sharing&#34;&gt;CurricularFace Backbone&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Pretrained CurricularFace model taken from &lt;a href=&#34;https://github.com/HuangYG123/CurricularFace&#34;&gt;HuangYG123&lt;/a&gt; for use in ID similarity metric computation.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1tJ7ih-wbCO6zc3JhI_1ZGjmwXKKaPlja/view?usp=sharing&#34;&gt;MTCNN&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Weights for MTCNN model taken from &lt;a href=&#34;https://github.com/TreB1eN/InsightFace_Pytorch&#34;&gt;TreB1eN&lt;/a&gt; for use in ID similarity metric computation. (Unpack the tar.gz to extract the 3 model weights.)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;By default, we assume that all auxiliary models are downloaded and saved to the directory &lt;code&gt;pretrained_models&lt;/code&gt;. However, you may use your own paths by changing the necessary values in &lt;code&gt;configs/path_configs.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;h3&gt;Preparing your Data&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Currently, we provide support for numerous datasets and experiments (encoding, frontalization, etc.). &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Refer to &lt;code&gt;configs/paths_config.py&lt;/code&gt; to define the necessary data paths and model paths for training and evaluation.&lt;/li&gt; &#xA;   &lt;li&gt;Refer to &lt;code&gt;configs/transforms_config.py&lt;/code&gt; for the transforms defined for each dataset/experiment.&lt;/li&gt; &#xA;   &lt;li&gt;Finally, refer to &lt;code&gt;configs/data_configs.py&lt;/code&gt; for the source/target data paths for the train and test sets as well as the transforms.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;If you wish to experiment with your own dataset, you can simply make the necessary adjustments in &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;code&gt;data_configs.py&lt;/code&gt; to define your data paths.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;transforms_configs.py&lt;/code&gt; to define your own data transforms.&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;As an example, assume we wish to run encoding using ffhq (&lt;code&gt;dataset_type=ffhq_encode&lt;/code&gt;). We first go to &lt;code&gt;configs/paths_config.py&lt;/code&gt; and define:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;dataset_paths = {&#xA;    &#39;ffhq&#39;: &#39;/path/to/ffhq/images256x256&#39;&#xA;    &#39;celeba_test&#39;: &#39;/path/to/CelebAMask-HQ/test_img&#39;,&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The transforms for the experiment are defined in the class &lt;code&gt;EncodeTransforms&lt;/code&gt; in &lt;code&gt;configs/transforms_config.py&lt;/code&gt;.&lt;br&gt; Finally, in &lt;code&gt;configs/data_configs.py&lt;/code&gt;, we define:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;DATASETS = {&#xA;   &#39;ffhq_encode&#39;: {&#xA;        &#39;transforms&#39;: transforms_config.EncodeTransforms,&#xA;        &#39;train_source_root&#39;: dataset_paths[&#39;ffhq&#39;],&#xA;        &#39;train_target_root&#39;: dataset_paths[&#39;ffhq&#39;],&#xA;        &#39;test_source_root&#39;: dataset_paths[&#39;celeba_test&#39;],&#xA;        &#39;test_target_root&#39;: dataset_paths[&#39;celeba_test&#39;],&#xA;    },&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When defining our datasets, we will take the values in the above dictionary.&lt;/p&gt; &#xA;&lt;h3&gt;Training pSp&lt;/h3&gt; &#xA;&lt;p&gt;The main training script can be found in &lt;code&gt;scripts/train.py&lt;/code&gt;.&lt;br&gt; Intermediate training results are saved to &lt;code&gt;opts.exp_dir&lt;/code&gt;. This includes checkpoints, train outputs, and test outputs.&lt;br&gt; Additionally, if you have tensorboard installed, you can visualize tensorboard logs in &lt;code&gt;opts.exp_dir/logs&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Training the pSp Encoder&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/train.py \&#xA;--dataset_type=ffhq_encode \&#xA;--exp_dir=/path/to/experiment \&#xA;--workers=8 \&#xA;--batch_size=8 \&#xA;--test_batch_size=8 \&#xA;--test_workers=8 \&#xA;--val_interval=2500 \&#xA;--save_interval=5000 \&#xA;--encoder_type=GradualStyleEncoder \&#xA;--start_from_latent_avg \&#xA;--lpips_lambda=0.8 \&#xA;--l2_lambda=1 \&#xA;--id_lambda=0.1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Frontalization&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/train.py \&#xA;--dataset_type=ffhq_frontalize \&#xA;--exp_dir=/path/to/experiment \&#xA;--workers=8 \&#xA;--batch_size=8 \&#xA;--test_batch_size=8 \&#xA;--test_workers=8 \&#xA;--val_interval=2500 \&#xA;--save_interval=5000 \&#xA;--encoder_type=GradualStyleEncoder \&#xA;--start_from_latent_avg \&#xA;--lpips_lambda=0.08 \&#xA;--l2_lambda=0.001 \&#xA;--lpips_lambda_crop=0.8 \&#xA;--l2_lambda_crop=0.01 \&#xA;--id_lambda=1 \&#xA;--w_norm_lambda=0.005&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Sketch to Face&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/train.py \&#xA;--dataset_type=celebs_sketch_to_face \&#xA;--exp_dir=/path/to/experiment \&#xA;--workers=8 \&#xA;--batch_size=8 \&#xA;--test_batch_size=8 \&#xA;--test_workers=8 \&#xA;--val_interval=2500 \&#xA;--save_interval=5000 \&#xA;--encoder_type=GradualStyleEncoder \&#xA;--start_from_latent_avg \&#xA;--lpips_lambda=0.8 \&#xA;--l2_lambda=1 \&#xA;--id_lambda=0 \&#xA;--w_norm_lambda=0.005 \&#xA;--label_nc=1 \&#xA;--input_nc=1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Segmentation Map to Face&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/train.py \&#xA;--dataset_type=celebs_seg_to_face \&#xA;--exp_dir=/path/to/experiment \&#xA;--workers=8 \&#xA;--batch_size=8 \&#xA;--test_batch_size=8 \&#xA;--test_workers=8 \&#xA;--val_interval=2500 \&#xA;--save_interval=5000 \&#xA;--encoder_type=GradualStyleEncoder \&#xA;--start_from_latent_avg \&#xA;--lpips_lambda=0.8 \&#xA;--l2_lambda=1 \&#xA;--id_lambda=0 \&#xA;--w_norm_lambda=0.005 \&#xA;--label_nc=19 \&#xA;--input_nc=19&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Notice with conditional image synthesis no identity loss is utilized (i.e. &lt;code&gt;--id_lambda=0&lt;/code&gt;)&lt;/p&gt; &#xA;&lt;h4&gt;Super Resolution&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/train.py \&#xA;--dataset_type=celebs_super_resolution \&#xA;--exp_dir=/path/to/experiment \&#xA;--workers=8 \&#xA;--batch_size=8 \&#xA;--test_batch_size=8 \&#xA;--test_workers=8 \&#xA;--val_interval=2500 \&#xA;--save_interval=5000 \&#xA;--encoder_type=GradualStyleEncoder \&#xA;--start_from_latent_avg \&#xA;--lpips_lambda=0.8 \&#xA;--l2_lambda=1 \&#xA;--id_lambda=0.1 \&#xA;--w_norm_lambda=0.005 \&#xA;--resize_factors=1,2,4,8,16,32&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Additional Notes&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;See &lt;code&gt;options/train_options.py&lt;/code&gt; for all training-specific flags.&lt;/li&gt; &#xA; &lt;li&gt;See &lt;code&gt;options/test_options.py&lt;/code&gt; for all test-specific flags.&lt;/li&gt; &#xA; &lt;li&gt;If you wish to resume from a specific checkpoint (e.g. a pretrained pSp model), you may do so using &lt;code&gt;--checkpoint_path&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;By default, we assume that the StyleGAN used outputs images at resolution &lt;code&gt;1024x1024&lt;/code&gt;. If you wish to use a StyleGAN at a smaller resolution, you can do so by using the flag &lt;code&gt;--output_size&lt;/code&gt; (e.g., &lt;code&gt;--output_size=256&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;If you wish to generate images from segmentation maps, please specify &lt;code&gt;--label_nc=N&lt;/code&gt; and &lt;code&gt;--input_nc=N&lt;/code&gt; where &lt;code&gt;N&lt;/code&gt; is the number of semantic categories.&lt;/li&gt; &#xA; &lt;li&gt;Similarly, for generating images from sketches, please specify &lt;code&gt;--label_nc=1&lt;/code&gt; and &lt;code&gt;--input_nc=1&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Specifying &lt;code&gt;--label_nc=0&lt;/code&gt; (the default value), will directly use the RGB colors as input.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;** Identity/Similarity Losses **&lt;br&gt; In pSp, we introduce a facial identity loss using a pre-trained ArcFace network for facial recognition. When operating on the human facial domain, we highly recommend employing this loss objective by using the flag &lt;code&gt;--id_lambda&lt;/code&gt;.&lt;br&gt; In a more recent paper, &lt;a href=&#34;https://github.com/omertov/encoder4editing&#34;&gt;encoder4editing&lt;/a&gt;, the authors generalize this identity loss to other domains by using a MoCo-based ResNet to extract features instead of an ArcFace network. Applying this MoCo-based similarity loss can be done by using the flag &lt;code&gt;--moco_lambda&lt;/code&gt;. We recommend setting &lt;code&gt;--moco_lambda=0.5&lt;/code&gt; in your experiments.&lt;br&gt; Please note, you &lt;ins&gt;cannot&lt;/ins&gt; set both &lt;code&gt;id_lambda&lt;/code&gt; and &lt;code&gt;moco_lambda&lt;/code&gt; to be active simultaneously (e.g., to use the MoCo-based loss, you should specify, &lt;code&gt;--moco_lambda=0.5 --id_lambda=0&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;h3&gt;Weights &amp;amp; Biases Integration&lt;/h3&gt; &#xA;&lt;p&gt;To help track your experiments, we&#39;ve integrated &lt;a href=&#34;https://wandb.ai/home&#34;&gt;Weights &amp;amp; Biases&lt;/a&gt; into our training process. To enable Weights &amp;amp; Biases (&lt;code&gt;wandb&lt;/code&gt;), first make an account on the platform&#39;s webpage and install &lt;code&gt;wandb&lt;/code&gt; using &lt;code&gt;pip install wandb&lt;/code&gt;. Then, to train pSp using &lt;code&gt;wandb&lt;/code&gt;, simply add the flag &lt;code&gt;--use_wandb&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Note that when running for the first time, you will be asked to provide your access key which can be accessed via the Weights &amp;amp; Biases platform.&lt;/p&gt; &#xA;&lt;p&gt;Using Weights &amp;amp; Biases will allow you to visualize the training and testing loss curves as well as intermediate training results.&lt;/p&gt; &#xA;&lt;h2&gt;Testing&lt;/h2&gt; &#xA;&lt;h3&gt;Inference&lt;/h3&gt; &#xA;&lt;p&gt;Having trained your model, you can use &lt;code&gt;scripts/inference.py&lt;/code&gt; to apply the model on a set of images.&lt;br&gt; For example,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/inference.py \&#xA;--exp_dir=/path/to/experiment \&#xA;--checkpoint_path=experiment/checkpoints/best_model.pt \&#xA;--data_path=/path/to/test_data \&#xA;--test_batch_size=4 \&#xA;--test_workers=4 \&#xA;--couple_outputs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Additional notes to consider:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;During inference, the options used during training are loaded from the saved checkpoint and are then updated using the test options passed to the inference script. For example, there is no need to pass &lt;code&gt;--dataset_type&lt;/code&gt; or &lt;code&gt;--label_nc&lt;/code&gt; to the inference script, as they are taken from the loaded &lt;code&gt;opts&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;When running inference for segmentation-to-image or sketch-to-image, it is highly recommend to do so with a style-mixing, as is done in the paper. This can simply be done by adding &lt;code&gt;--latent_mask=8,9,10,11,12,13,14,15,16,17&lt;/code&gt; when calling the script.&lt;/li&gt; &#xA; &lt;li&gt;When running inference for super-resolution, please provide a single down-sampling value using &lt;code&gt;--resize_factors&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Adding the flag &lt;code&gt;--couple_outputs&lt;/code&gt; will save an additional image containing the input and output images side-by-side in the sub-directory &lt;code&gt;inference_coupled&lt;/code&gt;. Otherwise, only the output image is saved to the sub-directory &lt;code&gt;inference_results&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;By default, the images will be saved at resolutiosn of 1024x1024, the original output size of StyleGAN. If you wish to save outputs resized to resolutions of 256x256, you can do so by adding the flag &lt;code&gt;--resize_outputs&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Multi-Modal Synthesis with Style-Mixing&lt;/h3&gt; &#xA;&lt;p&gt;Given a trained model for conditional image synthesis or super-resolution, we can easily generate multiple outputs for a given input image. This can be done using the script &lt;code&gt;scripts/style_mixing.py&lt;/code&gt;.&lt;br&gt; For example, running the following command will perform style-mixing for a segmentation-to-image experiment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/style_mixing.py \&#xA;--exp_dir=/path/to/experiment \&#xA;--checkpoint_path=/path/to/experiment/checkpoints/best_model.pt \&#xA;--data_path=/path/to/test_data/ \&#xA;--test_batch_size=4 \&#xA;--test_workers=4 \&#xA;--n_images=25 \&#xA;--n_outputs_to_generate=5 \&#xA;--latent_mask=8,9,10,11,12,13,14,15,16,17&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here, we inject &lt;code&gt;5&lt;/code&gt; randomly drawn vectors and perform style-mixing on the latents &lt;code&gt;[8,9,10,11,12,13,14,15,16,17]&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Additional notes to consider:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To perform style-mixing on a subset of images, you may use the flag &lt;code&gt;--n_images&lt;/code&gt;. The default value of &lt;code&gt;None&lt;/code&gt; will perform style mixing on every image in the given &lt;code&gt;data_path&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;You may also include the argument &lt;code&gt;--mix_alpha=m&lt;/code&gt; where &lt;code&gt;m&lt;/code&gt; is a float defining the mixing coefficient between the input latent and the randomly drawn latent.&lt;/li&gt; &#xA; &lt;li&gt;When performing style-mixing for super-resolution, please provide a single down-sampling value using &lt;code&gt;--resize_factors&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;By default, the images will be saved at resolutiosn of 1024x1024, the original output size of StyleGAN. If you wish to save outputs resized to resolutions of 256x256, you can do so by adding the flag &lt;code&gt;--resize_outputs&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Computing Metrics&lt;/h3&gt; &#xA;&lt;p&gt;Similarly, given a trained model and generated outputs, we can compute the loss metrics on a given dataset.&lt;br&gt; These scripts receive the inference output directory and ground truth directory.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Calculating the identity loss:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/calc_id_loss_parallel.py \&#xA;--data_path=/path/to/experiment/inference_outputs \&#xA;--gt_path=/path/to/test_images \&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Calculating LPIPS loss:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/calc_losses_on_images.py \&#xA;--mode lpips&#xA;--data_path=/path/to/experiment/inference_outputs \&#xA;--gt_path=/path/to/test_images \&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Calculating L2 loss:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/calc_losses_on_images.py \&#xA;--mode l2&#xA;--data_path=/path/to/experiment/inference_outputs \&#xA;--gt_path=/path/to/test_images \&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Additional Applications&lt;/h2&gt; &#xA;&lt;p&gt;To better show the flexibility of our pSp framework we present additional applications below.&lt;/p&gt; &#xA;&lt;p&gt;As with our main applications, you may download the pretrained models here:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Path&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1YKoiVuFaqdvzDP5CZaqa3k5phL-VDmyz/view&#34;&gt;Toonify&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;pSp trained with the FFHQ dataset for toonification using StyleGAN generator from &lt;a href=&#34;https://linktr.ee/Norod78&#34;&gt;Doron Adler&lt;/a&gt; and &lt;a href=&#34;https://www.justinpinkney.com/&#34;&gt;Justin Pinkney&lt;/a&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Toonify&lt;/h3&gt; &#xA;&lt;p&gt;Using the toonify StyleGAN built by &lt;a href=&#34;https://linktr.ee/Norod78&#34;&gt;Doron Adler&lt;/a&gt; and &lt;a href=&#34;https://www.justinpinkney.com/&#34;&gt;Justin Pinkney&lt;/a&gt;, we take a real face image and generate a toonified version of the given image. We train the pSp encoder to directly reconstruct real face images inside the toons latent space resulting in a projection of each image to the closest toon. We do so without requiring any labeled pairs or distillation!&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/docs/toonify_input.jpg&#34; width=&#34;800px&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/docs/toonify_output.jpg&#34; width=&#34;800px&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;This is trained exactly like the StyleGAN inversion task with several changes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Change from FFHQ StyleGAN to toonifed StyleGAN (can be set using &lt;code&gt;--stylegan_weights&lt;/code&gt;) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The toonify generator is taken from &lt;a href=&#34;https://linktr.ee/Norod78&#34;&gt;Doron Adler&lt;/a&gt; and &lt;a href=&#34;https://www.justinpinkney.com/&#34;&gt;Justin Pinkney&lt;/a&gt; and converted to Pytorch using &lt;a href=&#34;https://github.com/rosinality/stylegan2-pytorch&#34;&gt;rosinality&#39;s&lt;/a&gt; conversion script.&lt;/li&gt; &#xA;   &lt;li&gt;For convenience, the converted generator Pytorch model may be downloaded &lt;a href=&#34;https://drive.google.com/file/d/1r3XVCt_WYUKFZFxhNH-xO2dTtF6B5szu/view?usp=sharing&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Increase &lt;code&gt;id_lambda&lt;/code&gt; from &lt;code&gt;0.1&lt;/code&gt; to &lt;code&gt;1&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Increase &lt;code&gt;w_norm_lambda&lt;/code&gt; from &lt;code&gt;0.005&lt;/code&gt; to &lt;code&gt;0.025&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We obtain the best results after around &lt;code&gt;6000&lt;/code&gt; iterations of training (can be set using &lt;code&gt;--max_steps&lt;/code&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;Repository structure&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Path&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Description &lt;img width=&#34;200&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;pixel2style2pixel&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Repository root folder&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;‚îú&amp;nbsp; configs&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Folder containing configs defining model/data paths and data transforms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;‚îú&amp;nbsp; criteria&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Folder containing various loss criterias for training&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;‚îú&amp;nbsp; datasets&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Folder with various dataset objects and augmentations&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;‚îú&amp;nbsp; environment&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Folder containing Anaconda environment used in our experiments&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;‚îú models&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Folder containting all the models and training objects&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;‚îÇ&amp;nbsp; ‚îú&amp;nbsp; encoders&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Folder containing our pSp encoder architecture implementation and ArcFace encoder implementation from &lt;a href=&#34;https://github.com/TreB1eN/InsightFace_Pytorch&#34;&gt;TreB1eN&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;‚îÇ&amp;nbsp; ‚îú&amp;nbsp; mtcnn&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;MTCNN implementation from &lt;a href=&#34;https://github.com/TreB1eN/InsightFace_Pytorch&#34;&gt;TreB1eN&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;‚îÇ&amp;nbsp; ‚îú&amp;nbsp; stylegan2&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;StyleGAN2 model from &lt;a href=&#34;https://github.com/rosinality/stylegan2-pytorch&#34;&gt;rosinality&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;‚îÇ&amp;nbsp; ‚îî&amp;nbsp; psp.py&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Implementation of our pSp framework&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;‚îú&amp;nbsp; notebook&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Folder with jupyter notebook containing pSp inference playground&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;‚îú&amp;nbsp; options&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Folder with training and test command-line options&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;‚îú&amp;nbsp; scripts&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Folder with running scripts for training and inference&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;‚îú&amp;nbsp; training&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Folder with main training logic and Ranger implementation from &lt;a href=&#34;https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer&#34;&gt;lessw2020&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;‚îú&amp;nbsp; utils&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Folder with various utility functions&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img width=&#34;300&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;TODOs&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add multi-gpu support&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleGAN2 implementation:&lt;/strong&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/rosinality/stylegan2-pytorch&#34;&gt;https://github.com/rosinality/stylegan2-pytorch&lt;/a&gt;&lt;br&gt; Copyright (c) 2019 Kim Seonghyeon&lt;br&gt; License (MIT) &lt;a href=&#34;https://github.com/rosinality/stylegan2-pytorch/raw/master/LICENSE&#34;&gt;https://github.com/rosinality/stylegan2-pytorch/blob/master/LICENSE&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MTCNN, IR-SE50, and ArcFace models and implementations:&lt;/strong&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/TreB1eN/InsightFace_Pytorch&#34;&gt;https://github.com/TreB1eN/InsightFace_Pytorch&lt;/a&gt;&lt;br&gt; Copyright (c) 2018 TreB1eN&lt;br&gt; License (MIT) &lt;a href=&#34;https://github.com/TreB1eN/InsightFace_Pytorch/raw/master/LICENSE&#34;&gt;https://github.com/TreB1eN/InsightFace_Pytorch/blob/master/LICENSE&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CurricularFace model and implementation:&lt;/strong&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/HuangYG123/CurricularFace&#34;&gt;https://github.com/HuangYG123/CurricularFace&lt;/a&gt;&lt;br&gt; Copyright (c) 2020 HuangYG123&lt;br&gt; License (MIT) &lt;a href=&#34;https://github.com/HuangYG123/CurricularFace/raw/master/LICENSE&#34;&gt;https://github.com/HuangYG123/CurricularFace/blob/master/LICENSE&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Ranger optimizer implementation:&lt;/strong&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer&#34;&gt;https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer&lt;/a&gt;&lt;br&gt; License (Apache License 2.0) &lt;a href=&#34;https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer/raw/master/LICENSE&#34;&gt;https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer/blob/master/LICENSE&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LPIPS implementation:&lt;/strong&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/S-aiueo32/lpips-pytorch&#34;&gt;https://github.com/S-aiueo32/lpips-pytorch&lt;/a&gt;&lt;br&gt; Copyright (c) 2020, Sou Uchida&lt;br&gt; License (BSD 2-Clause) &lt;a href=&#34;https://github.com/S-aiueo32/lpips-pytorch/raw/master/LICENSE&#34;&gt;https://github.com/S-aiueo32/lpips-pytorch/blob/master/LICENSE&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Please Note&lt;/strong&gt;: The CUDA files under the &lt;a href=&#34;https://github.com/eladrich/pixel2style2pixel/tree/master/models/stylegan2/op&#34;&gt;StyleGAN2 ops directory&lt;/a&gt; are made available under the &lt;a href=&#34;https://nvlabs.github.io/stylegan2/license.html&#34;&gt;Nvidia Source Code License-NC&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Inspired by pSp&lt;/h2&gt; &#xA;&lt;p&gt;Below are several works inspired by pSp that we found particularly interesting:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Reverse Toonification&lt;/strong&gt;&lt;br&gt; Using our pSp encoder, artist &lt;a href=&#34;https://linktr.ee/nathan_shipley&#34;&gt;Nathan Shipley&lt;/a&gt; transformed animated figures and paintings into real life. Check out his amazing work on his &lt;a href=&#34;https://twitter.com/citizenplain?lang=en&#34;&gt;twitter page&lt;/a&gt; and &lt;a href=&#34;http://www.nathanshipley.com/gan&#34;&gt;website&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Deploying pSp with StyleSpace for Editing&lt;/strong&gt;&lt;br&gt; Awesome work from &lt;a href=&#34;https://www.justinpinkney.com/&#34;&gt;Justin Pinkney&lt;/a&gt; who deployed our pSp model on Runway and provided support for editing the resulting inversions using the &lt;a href=&#34;https://arxiv.org/abs/2011.12799&#34;&gt;StyleSpace Analysis paper&lt;/a&gt;. Check out his repository &lt;a href=&#34;https://github.com/justinpinkney/pixel2style2pixel&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Encoder4Editing (e4e)&lt;/strong&gt;&lt;br&gt; Building on the work of pSp, Tov et al. design an encoder to enable high quality edits on real images. Check out their &lt;a href=&#34;https://arxiv.org/abs/2102.02766&#34;&gt;paper&lt;/a&gt; and &lt;a href=&#34;https://github.com/omertov/encoder4editing&#34;&gt;code&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Style-based Age Manipulation (SAM)&lt;/strong&gt;&lt;br&gt; Leveraging pSp and the rich semantics of StyleGAN, SAM learns non-linear latent space paths for modeling the age transformation of real face images. Check out the project page &lt;a href=&#34;https://yuval-alaluf.github.io/SAM/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ReStyle&lt;/strong&gt;&lt;br&gt; ReStyle builds on recent encoders such as pSp and e4e by introducing an iterative refinment mechanism to gradually improve the inversion of real images. Check out the project page &lt;a href=&#34;https://yuval-alaluf.github.io/restyle-encoder/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;pSp in the Media&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;bycloud: &lt;a href=&#34;https://www.youtube.com/watch?v=g-N8lfceclI&amp;amp;ab_channel=bycloud&#34;&gt;AI Generates Cartoon Characters In Real Life Pixel2Style2Pixel&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Synced: &lt;a href=&#34;https://syncedreview.com/2020/08/07/pixel2style2pixel-novel-encoder-architecture-boosts-facial-image-to-image-translation/&#34;&gt;Pixel2Style2Pixel: Novel Encoder Architecture Boosts Facial Image-To-Image Translation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Cartoon Brew: &lt;a href=&#34;https://www.cartoonbrew.com/tech/an-artist-has-used-machine-learning-to-turn-animated-characters-into-creepy-photorealistic-figures-197975.html&#34;&gt;An Artist Has Used Machine Learning To Turn Animated Characters Into Creepy Photorealistic Figures&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use this code for your research, please cite our paper &lt;a href=&#34;https://arxiv.org/abs/2008.00951&#34;&gt;Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@InProceedings{richardson2021encoding,&#xA;      author = {Richardson, Elad and Alaluf, Yuval and Patashnik, Or and Nitzan, Yotam and Azar, Yaniv and Shapiro, Stav and Cohen-Or, Daniel},&#xA;      title = {Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation},&#xA;      booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},&#xA;      month = {June},&#xA;      year = {2021}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>Pierian-Data/Complete-Python-3-Bootcamp</title>
    <updated>2022-05-29T01:45:04Z</updated>
    <id>tag:github.com,2022-05-29:/Pierian-Data/Complete-Python-3-Bootcamp</id>
    <link href="https://github.com/Pierian-Data/Complete-Python-3-Bootcamp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Course Files for Complete Python 3 Bootcamp Course on Udemy&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Complete-Python-3-Bootcamp&lt;/h1&gt; &#xA;&lt;p&gt;Course Files for Complete Python 3 Bootcamp Course on Udemy&lt;/p&gt; &#xA;&lt;p&gt;Copyright(¬©) by Pierian Data Inc.&lt;/p&gt; &#xA;&lt;p&gt;Get it now for 95% off with the link: &lt;a href=&#34;https://www.udemy.com/complete-python-bootcamp/?couponCode=COMPLETE_GITHUB&#34;&gt;https://www.udemy.com/complete-python-bootcamp/?couponCode=COMPLETE_GITHUB&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Thanks!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>huseinzol05/Stock-Prediction-Models</title>
    <updated>2022-05-29T01:45:04Z</updated>
    <id>tag:github.com,2022-05-29:/huseinzol05/Stock-Prediction-Models</id>
    <link href="https://github.com/huseinzol05/Stock-Prediction-Models" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Gathers machine learning and deep learning models for Stock forecasting including trading bots and simulations&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/#readme&#34;&gt; &lt;img alt=&#34;logo&#34; width=&#34;50%&#34; src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output/evolution-strategy.png&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/huseinzol05/Stock-Prediction-Models/raw/master/LICENSE&#34;&gt;&lt;img alt=&#34;MIT License&#34; src=&#34;https://img.shields.io/badge/License-Apache--License--2.0-yellow.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/#&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/deeplearning-30--models-success.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/#&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/agent-23--models-success.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stock-Prediction-Models&lt;/strong&gt;, Gathers machine learning and deep learning models for Stock forecasting, included trading bots and simulations.&lt;/p&gt; &#xA;&lt;h2&gt;Table of contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/#models&#34;&gt;Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/#agents&#34;&gt;Agents&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/realtime-agent&#34;&gt;Realtime Agent&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/#data-explorations&#34;&gt;Data Explorations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/#simulations&#34;&gt;Simulations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/#tensorflow-js&#34;&gt;Tensorflow-js&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/#misc&#34;&gt;Misc&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/#results&#34;&gt;Results&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/#results-agent&#34;&gt;Results Agent&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/#results-signal-prediction&#34;&gt;Results signal prediction&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/#results-analysis&#34;&gt;Results analysis&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/#results-simulation&#34;&gt;Results simulation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;h3&gt;Models&lt;/h3&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/deep-learning&#34;&gt;Deep-learning models&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;LSTM&lt;/li&gt; &#xA; &lt;li&gt;LSTM Bidirectional&lt;/li&gt; &#xA; &lt;li&gt;LSTM 2-Path&lt;/li&gt; &#xA; &lt;li&gt;GRU&lt;/li&gt; &#xA; &lt;li&gt;GRU Bidirectional&lt;/li&gt; &#xA; &lt;li&gt;GRU 2-Path&lt;/li&gt; &#xA; &lt;li&gt;Vanilla&lt;/li&gt; &#xA; &lt;li&gt;Vanilla Bidirectional&lt;/li&gt; &#xA; &lt;li&gt;Vanilla 2-Path&lt;/li&gt; &#xA; &lt;li&gt;LSTM Seq2seq&lt;/li&gt; &#xA; &lt;li&gt;LSTM Bidirectional Seq2seq&lt;/li&gt; &#xA; &lt;li&gt;LSTM Seq2seq VAE&lt;/li&gt; &#xA; &lt;li&gt;GRU Seq2seq&lt;/li&gt; &#xA; &lt;li&gt;GRU Bidirectional Seq2seq&lt;/li&gt; &#xA; &lt;li&gt;GRU Seq2seq VAE&lt;/li&gt; &#xA; &lt;li&gt;Attention-is-all-you-Need&lt;/li&gt; &#xA; &lt;li&gt;CNN-Seq2seq&lt;/li&gt; &#xA; &lt;li&gt;Dilated-CNN-Seq2seq&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;Bonus&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;How to use one of the model to forecast &lt;code&gt;t + N&lt;/code&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/deep-learning/how-to-forecast.ipynb&#34;&gt;how-to-forecast.ipynb&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Consensus, how to use sentiment data to forecast &lt;code&gt;t + N&lt;/code&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/deep-learning/sentiment-consensus.ipynb&#34;&gt;sentiment-consensus.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/stacking&#34;&gt;Stacking models&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Deep Feed-forward Auto-Encoder Neural Network to reduce dimension + Deep Recurrent Neural Network + ARIMA + Extreme Boosting Gradient Regressor&lt;/li&gt; &#xA; &lt;li&gt;Adaboost + Bagging + Extra Trees + Gradient Boosting + Random Forest + XGB&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent&#34;&gt;Agents&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Turtle-trading agent&lt;/li&gt; &#xA; &lt;li&gt;Moving-average agent&lt;/li&gt; &#xA; &lt;li&gt;Signal rolling agent&lt;/li&gt; &#xA; &lt;li&gt;Policy-gradient agent&lt;/li&gt; &#xA; &lt;li&gt;Q-learning agent&lt;/li&gt; &#xA; &lt;li&gt;Evolution-strategy agent&lt;/li&gt; &#xA; &lt;li&gt;Double Q-learning agent&lt;/li&gt; &#xA; &lt;li&gt;Recurrent Q-learning agent&lt;/li&gt; &#xA; &lt;li&gt;Double Recurrent Q-learning agent&lt;/li&gt; &#xA; &lt;li&gt;Duel Q-learning agent&lt;/li&gt; &#xA; &lt;li&gt;Double Duel Q-learning agent&lt;/li&gt; &#xA; &lt;li&gt;Duel Recurrent Q-learning agent&lt;/li&gt; &#xA; &lt;li&gt;Double Duel Recurrent Q-learning agent&lt;/li&gt; &#xA; &lt;li&gt;Actor-critic agent&lt;/li&gt; &#xA; &lt;li&gt;Actor-critic Duel agent&lt;/li&gt; &#xA; &lt;li&gt;Actor-critic Recurrent agent&lt;/li&gt; &#xA; &lt;li&gt;Actor-critic Duel Recurrent agent&lt;/li&gt; &#xA; &lt;li&gt;Curiosity Q-learning agent&lt;/li&gt; &#xA; &lt;li&gt;Recurrent Curiosity Q-learning agent&lt;/li&gt; &#xA; &lt;li&gt;Duel Curiosity Q-learning agent&lt;/li&gt; &#xA; &lt;li&gt;Neuro-evolution agent&lt;/li&gt; &#xA; &lt;li&gt;Neuro-evolution with Novelty search agent&lt;/li&gt; &#xA; &lt;li&gt;ABCD strategy agent&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/misc&#34;&gt;Data Explorations&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;stock market study on TESLA stock, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/misc/tesla-study.ipynb&#34;&gt;tesla-study.ipynb&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Outliers study using K-means, SVM, and Gaussian on TESLA stock, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/misc/outliers.ipynb&#34;&gt;outliers.ipynb&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Overbought-Oversold study on TESLA stock, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/misc/overbought-oversold.ipynb&#34;&gt;overbought-oversold.ipynb&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Which stock you need to buy? &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/misc/which-stock.ipynb&#34;&gt;which-stock.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/simulation&#34;&gt;Simulations&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Simple Monte Carlo, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/simulation/monte-carlo-drift.ipynb&#34;&gt;monte-carlo-drift.ipynb&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Dynamic volatility Monte Carlo, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/simulation/monte-carlo-dynamic-volatility.ipynb&#34;&gt;monte-carlo-dynamic-volatility.ipynb&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Drift Monte Carlo, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/simulation/monte-carlo-drift.ipynb&#34;&gt;monte-carlo-drift.ipynb&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Multivariate Drift Monte Carlo BTC/USDT with Bitcurate sentiment, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/simulation/multivariate-drift-monte-carlo.ipynb&#34;&gt;multivariate-drift-monte-carlo.ipynb&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Portfolio optimization, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/simulation/portfolio-optimization.ipynb&#34;&gt;portfolio-optimization.ipynb&lt;/a&gt;, inspired from &lt;a href=&#34;https://pythonforfinance.net/2017/01/21/investment-portfolio-optimisation-with-python/&#34;&gt;https://pythonforfinance.net/2017/01/21/investment-portfolio-optimisation-with-python/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/stock-forecasting-js&#34;&gt;Tensorflow-js&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;I code &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/deep-learning/1.lstm.ipynb&#34;&gt;LSTM Recurrent Neural Network&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/simple-agent.ipynb&#34;&gt;Simple signal rolling agent&lt;/a&gt; inside Tensorflow JS, you can try it here, &lt;a href=&#34;https://huseinhouse.com/stock-forecasting-js/&#34;&gt;huseinhouse.com/stock-forecasting-js&lt;/a&gt;, you can download any historical CSV and upload dynamically.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/misc&#34;&gt;Misc&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;fashion trending prediction with cross-validation, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/misc/fashion-forecasting.ipynb&#34;&gt;fashion-forecasting.ipynb&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Bitcoin analysis with LSTM prediction, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/misc/bitcoin-analysis-lstm.ipynb&#34;&gt;bitcoin-analysis-lstm.ipynb&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Kijang Emas Bank Negara, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/misc/kijang-emas-bank-negara.ipynb&#34;&gt;kijang-emas-bank-negara.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Results&lt;/h2&gt; &#xA;&lt;h3&gt;Results Agent&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;This agent only able to buy or sell 1 unit per transaction.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Turtle-trading agent, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/1.turtle-agent.ipynb&#34;&gt;turtle-agent.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output-agent/turtle-agent.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Moving-average agent, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/2.moving-average-agent.ipynb&#34;&gt;moving-average-agent.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output-agent/moving-average-agent.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Signal rolling agent, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/3.signal-rolling-agent.ipynb&#34;&gt;signal-rolling-agent.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output-agent/signal-rolling-agent.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Policy-gradient agent, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/4.policy-gradient-agent.ipynb&#34;&gt;policy-gradient-agent.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output-agent/policy-gradient-agent.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;Q-learning agent, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/5.q-learning-agent.ipynb&#34;&gt;q-learning-agent.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output-agent/q-learning-agent.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;6&#34;&gt; &#xA; &lt;li&gt;Evolution-strategy agent, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/6.evolution-strategy-agent.ipynb&#34;&gt;evolution-strategy-agent.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output-agent/evolution-strategy-agent.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;7&#34;&gt; &#xA; &lt;li&gt;Double Q-learning agent, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/7.double-q-learning-agent.ipynb&#34;&gt;double-q-learning-agent.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output-agent/double-q-learning.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;8&#34;&gt; &#xA; &lt;li&gt;Recurrent Q-learning agent, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/8.recurrent-q-learning-agent.ipynb&#34;&gt;recurrent-q-learning-agent.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output-agent/recurrent-q-learning.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;9&#34;&gt; &#xA; &lt;li&gt;Double Recurrent Q-learning agent, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/9.double-recurrent-q-learning-agent.ipynb&#34;&gt;double-recurrent-q-learning-agent.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output-agent/double-recurrent-q-learning.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;10&#34;&gt; &#xA; &lt;li&gt;Duel Q-learning agent, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/10.duel-q-learning-agent.ipynb&#34;&gt;duel-q-learning-agent.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output-agent/double-q-learning.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;11&#34;&gt; &#xA; &lt;li&gt;Double Duel Q-learning agent, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/11.double-duel-q-learning-agent.ipynb&#34;&gt;double-duel-q-learning-agent.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output-agent/double-duel-q-learning.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;12&#34;&gt; &#xA; &lt;li&gt;Duel Recurrent Q-learning agent, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/12.duel-recurrent-q-learning-agent.ipynb&#34;&gt;duel-recurrent-q-learning-agent.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output-agent/duel-recurrent-q-learning.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;13&#34;&gt; &#xA; &lt;li&gt;Double Duel Recurrent Q-learning agent, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/13.double-duel-recurrent-q-learning-agent.ipynb&#34;&gt;double-duel-recurrent-q-learning-agent.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output-agent/double-duel-recurrent-q-learning.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;14&#34;&gt; &#xA; &lt;li&gt;Actor-critic agent, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/14.actor-critic-agent.ipynb&#34;&gt;actor-critic-agent.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output-agent/actor-critic.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;15&#34;&gt; &#xA; &lt;li&gt;Actor-critic Duel agent, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/14.actor-critic-duel-agent.ipynb&#34;&gt;actor-critic-duel-agent.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output-agent/actor-critic-duel.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;16&#34;&gt; &#xA; &lt;li&gt;Actor-critic Recurrent agent, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/16.actor-critic-recurrent-agent.ipynb&#34;&gt;actor-critic-recurrent-agent.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output-agent/actor-critic-recurrent.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;17&#34;&gt; &#xA; &lt;li&gt;Actor-critic Duel Recurrent agent, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/17.actor-critic-duel-recurrent-agent.ipynb&#34;&gt;actor-critic-duel-recurrent-agent.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output-agent/actor-critic-duel-recurrent.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;18&#34;&gt; &#xA; &lt;li&gt;Curiosity Q-learning agent, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/18.curiosity-q-learning-agent.ipynb&#34;&gt;curiosity-q-learning-agent.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output-agent/curiosity-q-learning.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;19&#34;&gt; &#xA; &lt;li&gt;Recurrent Curiosity Q-learning agent, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/19.recurrent-curiosity-q-learning-agent.ipynb&#34;&gt;recurrent-curiosity-q-learning.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output-agent/recurrent-curiosity-q-learning.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;20&#34;&gt; &#xA; &lt;li&gt;Duel Curiosity Q-learning agent, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/20.duel-curiosity-q-learning-agent.ipynb&#34;&gt;duel-curiosity-q-learning-agent.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output-agent/duel-curiosity-q-learning.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;21&#34;&gt; &#xA; &lt;li&gt;Neuro-evolution agent, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/21.neuro-evolution-agent.ipynb&#34;&gt;neuro-evolution.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output-agent/neuro-evolution.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;22&#34;&gt; &#xA; &lt;li&gt;Neuro-evolution with Novelty search agent, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/22.neuro-evolution-novelty-search-agent.ipynb&#34;&gt;neuro-evolution-novelty-search.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output-agent/neuro-evolution-novelty-search.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;23&#34;&gt; &#xA; &lt;li&gt;ABCD strategy agent, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/23.abcd-strategy-agent.ipynb&#34;&gt;abcd-strategy.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output-agent/abcd-strategy.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;h3&gt;Results signal prediction&lt;/h3&gt; &#xA;&lt;p&gt;I will cut the dataset to train and test datasets,&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Train dataset derived from starting timestamp until last 30 days&lt;/li&gt; &#xA; &lt;li&gt;Test dataset derived from last 30 days until end of the dataset&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;So we will let the model do forecasting based on last 30 days, and we will going to repeat the experiment for 10 times. You can increase it locally if you want, and tuning parameters will help you by a lot.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;LSTM, accuracy 95.693%, time taken for 1 epoch 01:09&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output/lstm.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;LSTM Bidirectional, accuracy 93.8%, time taken for 1 epoch 01:40&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output/bidirectional-lstm.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;LSTM 2-Path, accuracy 94.63%, time taken for 1 epoch 01:39&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output/lstm-2path.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;GRU, accuracy 94.63%, time taken for 1 epoch 02:10&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output/gru.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;GRU Bidirectional, accuracy 92.5673%, time taken for 1 epoch 01:40&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output/bidirectional-gru.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;6&#34;&gt; &#xA; &lt;li&gt;GRU 2-Path, accuracy 93.2117%, time taken for 1 epoch 01:39&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output/gru-2path.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;7&#34;&gt; &#xA; &lt;li&gt;Vanilla, accuracy 91.4686%, time taken for 1 epoch 00:52&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output/vanilla.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;8&#34;&gt; &#xA; &lt;li&gt;Vanilla Bidirectional, accuracy 88.9927%, time taken for 1 epoch 01:06&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output/bidirectional-vanilla.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;9&#34;&gt; &#xA; &lt;li&gt;Vanilla 2-Path, accuracy 91.5406%, time taken for 1 epoch 01:08&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output/vanilla-2path.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;10&#34;&gt; &#xA; &lt;li&gt;LSTM Seq2seq, accuracy 94.9817%, time taken for 1 epoch 01:36&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output/lstm-seq2seq.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;11&#34;&gt; &#xA; &lt;li&gt;LSTM Bidirectional Seq2seq, accuracy 94.517%, time taken for 1 epoch 02:30&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output/bidirectional-lstm-seq2seq.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;12&#34;&gt; &#xA; &lt;li&gt;LSTM Seq2seq VAE, accuracy 95.4190%, time taken for 1 epoch 01:48&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output/lstm-seq2seq-vae.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;13&#34;&gt; &#xA; &lt;li&gt;GRU Seq2seq, accuracy 90.8854%, time taken for 1 epoch 01:34&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output/gru-seq2seq.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;14&#34;&gt; &#xA; &lt;li&gt;GRU Bidirectional Seq2seq, accuracy 67.9915%, time taken for 1 epoch 02:30&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output/bidirectional-gru-seq2seq.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;15&#34;&gt; &#xA; &lt;li&gt;GRU Seq2seq VAE, accuracy 89.1321%, time taken for 1 epoch 01:48&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output/gru-seq2seq-vae.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;16&#34;&gt; &#xA; &lt;li&gt;Attention-is-all-you-Need, accuracy 94.2482%, time taken for 1 epoch 01:41&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output/attention-is-all-you-need.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;17&#34;&gt; &#xA; &lt;li&gt;CNN-Seq2seq, accuracy 90.74%, time taken for 1 epoch 00:43&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output/cnn-seq2seq.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;18&#34;&gt; &#xA; &lt;li&gt;Dilated-CNN-Seq2seq, accuracy 95.86%, time taken for 1 epoch 00:14&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output/dilated-cnn-seq2seq.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;p&gt;&lt;strong&gt;Bonus&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;How to forecast,&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output/how-to-forecast.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Sentiment consensus,&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output/sentiment-consensus.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;h3&gt;Results analysis&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Outliers study using K-means, SVM, and Gaussian on TESLA stock&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/misc/outliers.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Overbought-Oversold study on TESLA stock&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/misc/overbought-oversold.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Which stock you need to buy?&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/misc/which-stock.png&#34; width=&#34;40%&#34; align=&#34;&#34;&gt; &#xA;&lt;h3&gt;Results simulation&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Simple Monte Carlo&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/simulation/monte-carlo-simple.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Dynamic volatity Monte Carlo&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/simulation/monte-carlo-dynamic-volatility.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Drift Monte Carlo&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/simulation/monte-carlo-drift.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Multivariate Drift Monte Carlo BTC/USDT with Bitcurate sentiment&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/simulation/multivariate-drift-monte-carlo.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;Portfolio optimization&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/simulation/portfolio-optimization.png&#34; width=&#34;40%&#34; align=&#34;&#34;&gt;</summary>
  </entry>
  <entry>
    <title>https-deeplearning-ai/machine-learning-engineering-for-production-public</title>
    <updated>2022-05-29T01:45:04Z</updated>
    <id>tag:github.com,2022-05-29:/https-deeplearning-ai/machine-learning-engineering-for-production-public</id>
    <link href="https://github.com/https-deeplearning-ai/machine-learning-engineering-for-production-public" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Public repo for DeepLearning.AI MLEP Specialization&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Machine Learning Engineering for Production&lt;/h2&gt; &#xA;&lt;p&gt;Welcome to the public repo for &lt;a href=&#34;https://www.deeplearning.ai/&#34;&gt;deeplearning.ai&lt;/a&gt;&#39;s Machine Learning Engineering for Production Specialization.&lt;/p&gt; &#xA;&lt;p&gt;Here you will find public resources for the courses of this specialization.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>kwea123/nerf_pl</title>
    <updated>2022-05-29T01:45:04Z</updated>
    <id>tag:github.com,2022-05-29:/kwea123/nerf_pl</id>
    <link href="https://github.com/kwea123/nerf_pl" rel="alternate"></link>
    <summary type="html">&lt;p&gt;NeRF (Neural Radiance Fields) and NeRF in the Wild using pytorch-lightning&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;nerf_pl&lt;/h1&gt; &#xA;&lt;h3&gt;Update: NVIDIA just open-sourced a lightning-fast version of NeRF: &lt;a href=&#34;https://github.com/NVlabs/instant-ngp&#34;&gt;NGP&lt;/a&gt;. If you just want to see how strong NeRF is, try out this one! Sadly, it allows very little customization due to many dependencies, unless you are super familiar with those libraries and cuda programming (I&#39;m not &lt;span&gt;üòµüí´&lt;/span&gt;).&lt;/h3&gt; &#xA;&lt;h3&gt;Update: an improved &lt;a href=&#34;https://www.cs.cornell.edu/~zl548/NSFF/&#34;&gt;NSFF&lt;/a&gt; implementation to handle dynamic scene is &lt;a href=&#34;https://github.com/kwea123/nsff_pl&#34;&gt;open&lt;/a&gt;!&lt;/h3&gt; &#xA;&lt;h3&gt;Update: &lt;a href=&#34;https://nerf-w.github.io/&#34;&gt;NeRF-W&lt;/a&gt; (NeRF in the Wild) implementation is added to &lt;a href=&#34;https://github.com/kwea123/nerf_pl/tree/nerfw&#34;&gt;nerfw&lt;/a&gt; branch!&lt;/h3&gt; &#xA;&lt;h3&gt;Update: The lastest code (using the latest libraries) will be updated to &lt;a href=&#34;https://github.com/kwea123/nerf_pl/tree/dev&#34;&gt;dev&lt;/a&gt; branch. The master branch remains to support the colab files. If you don&#39;t use colab, it is recommended to switch to dev branch.&lt;/h3&gt; &#xA;&lt;h3&gt;Only issues of the dev and nerfw branch will be considered currently.&lt;/h3&gt; &#xA;&lt;h3&gt;&lt;span&gt;üíé&lt;/span&gt; &lt;a href=&#34;https://kwea123.github.io/nerf_pl/&#34;&gt;&lt;strong&gt;Project page&lt;/strong&gt;&lt;/a&gt; (live demo!)&lt;/h3&gt; &#xA;&lt;p&gt;Unofficial implementation of &lt;a href=&#34;https://arxiv.org/pdf/2003.08934.pdf&#34;&gt;NeRF&lt;/a&gt; (Neural Radiance Fields) using pytorch (&lt;a href=&#34;https://github.com/PyTorchLightning/pytorch-lightning&#34;&gt;pytorch-lightning&lt;/a&gt;). This repo doesn&#39;t aim at reproducibility, but aim at providing a simpler and faster training procedure (also simpler code with detailed comments to help to understand the work). Moreover, I try to extend much more opportunities by integrating this algorithm into game engine like Unity.&lt;/p&gt; &#xA;&lt;p&gt;Official implementation: &lt;a href=&#34;https://github.com/bmild/nerf&#34;&gt;nerf&lt;/a&gt; .. Reference pytorch implementation: &lt;a href=&#34;https://github.com/yenchenlin/nerf-pytorch&#34;&gt;nerf-pytorch&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Recommend to read: A detailed NeRF extension list: &lt;a href=&#34;https://github.com/yenchenlin/awesome-NeRF&#34;&gt;awesome-NeRF&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h2&gt;&lt;span&gt;üåå&lt;/span&gt; Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Multi-gpu training: Training on 8 GPUs finishes within 1 hour for the synthetic dataset!&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/#mortar_board-colab&#34;&gt;Colab&lt;/a&gt; notebooks to allow easy usage!&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/#ribbon-mesh&#34;&gt;Reconstruct&lt;/a&gt; &lt;strong&gt;colored&lt;/strong&gt; mesh!&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtu.be/S5phWFTs2iM&#34;&gt;Mixed Reality&lt;/a&gt; in Unity!&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtu.be/w9qTbVzCdWk&#34;&gt;REAL TIME volume rendering&lt;/a&gt; in Unity!&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/#portable-scenes&#34;&gt;Portable Scenes&lt;/a&gt; to let you play with other people&#39;s scenes!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;You can find the Unity project including mesh, mixed reality and volume rendering &lt;a href=&#34;https://github.com/kwea123/nerf_Unity&#34;&gt;here&lt;/a&gt;! See &lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/README_Unity.md&#34;&gt;README_Unity&lt;/a&gt; for generating your own data for Unity rendering!&lt;/h3&gt; &#xA;&lt;h2&gt;&lt;span&gt;üî∞&lt;/span&gt; Tutorial&lt;/h2&gt; &#xA;&lt;h3&gt;What can NeRF do?&lt;/h3&gt; &#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/11364490/82124460-1ccbbb80-97da-11ea-88ad-25e22868a5c1.png&#34; style=&#34;max-width:100%&#34;&gt; &#xA;&lt;h3&gt;Tutorial videos&lt;/h3&gt; &#xA;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLDV2CyUo4q-K02pNEyDr7DYpTQuka3mbV&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/11364490/80913471-d5781080-8d7f-11ea-9f72-9d68402b8271.png&#34;&gt; &lt;/a&gt; &#xA;&lt;h1&gt;&lt;span&gt;üíª&lt;/span&gt; Installation&lt;/h1&gt; &#xA;&lt;h2&gt;Hardware&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OS: Ubuntu 18.04&lt;/li&gt; &#xA; &lt;li&gt;NVIDIA GPU with &lt;strong&gt;CUDA&amp;gt;=10.1&lt;/strong&gt; (tested with 1 RTX2080Ti)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Software&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Clone this repo by &lt;code&gt;git clone --recursive https://github.com/kwea123/nerf_pl&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Python&amp;gt;=3.6 (installation via &lt;a href=&#34;https://www.anaconda.com/distribution/&#34;&gt;anaconda&lt;/a&gt; is recommended, use &lt;code&gt;conda create -n nerf_pl python=3.6&lt;/code&gt; to create a conda environment and activate it by &lt;code&gt;conda activate nerf_pl&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Python libraries &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Install core requirements by &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Install &lt;code&gt;torchsearchsorted&lt;/code&gt; by &lt;code&gt;cd torchsearchsorted&lt;/code&gt; then &lt;code&gt;pip install .&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;&lt;span&gt;üîë&lt;/span&gt; Training&lt;/h1&gt; &#xA;&lt;p&gt;Please see each subsection for training on different datasets. Available training datasets:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/#blender&#34;&gt;Blender&lt;/a&gt; (Realistic Synthetic 360)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/#llff&#34;&gt;LLFF&lt;/a&gt; (Real Forward-Facing)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/#your-own-data&#34;&gt;Your own data&lt;/a&gt; (Forward-Facing/360 inward-facing)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Blender&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Steps&lt;/summary&gt; &#xA; &lt;h3&gt;Data download&lt;/h3&gt; &#xA; &lt;p&gt;Download &lt;code&gt;nerf_synthetic.zip&lt;/code&gt; from &lt;a href=&#34;https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;h3&gt;Training model&lt;/h3&gt; &#xA; &lt;p&gt;Run (example)&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;python train.py \&#xA;   --dataset_name blender \&#xA;   --root_dir $BLENDER_DIR \&#xA;   --N_importance 64 --img_wh 400 400 --noise_std 0 \&#xA;   --num_epochs 16 --batch_size 1024 \&#xA;   --optimizer adam --lr 5e-4 \&#xA;   --lr_scheduler steplr --decay_step 2 4 8 --decay_gamma 0.5 \&#xA;   --exp_name exp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;These parameters are chosen to best mimic the training settings in the original repo. See &lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/opt.py&#34;&gt;opt.py&lt;/a&gt; for all configurations.&lt;/p&gt; &#xA; &lt;p&gt;NOTE: the above configuration doesn&#39;t work for some scenes like &lt;code&gt;drums&lt;/code&gt;, &lt;code&gt;ship&lt;/code&gt;. In that case, consider increasing the &lt;code&gt;batch_size&lt;/code&gt; or change the &lt;code&gt;optimizer&lt;/code&gt; to &lt;code&gt;radam&lt;/code&gt;. I managed to train on all scenes with these modifications.&lt;/p&gt; &#xA; &lt;p&gt;You can monitor the training process by &lt;code&gt;tensorboard --logdir logs/&lt;/code&gt; and go to &lt;code&gt;localhost:6006&lt;/code&gt; in your browser.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;LLFF&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Steps&lt;/summary&gt; &#xA; &lt;h3&gt;Data download&lt;/h3&gt; &#xA; &lt;p&gt;Download &lt;code&gt;nerf_llff_data.zip&lt;/code&gt; from &lt;a href=&#34;https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;h3&gt;Training model&lt;/h3&gt; &#xA; &lt;p&gt;Run (example)&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;python train.py \&#xA;   --dataset_name llff \&#xA;   --root_dir $LLFF_DIR \&#xA;   --N_importance 64 --img_wh 504 378 \&#xA;   --num_epochs 30 --batch_size 1024 \&#xA;   --optimizer adam --lr 5e-4 \&#xA;   --lr_scheduler steplr --decay_step 10 20 --decay_gamma 0.5 \&#xA;   --exp_name exp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;These parameters are chosen to best mimic the training settings in the original repo. See &lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/opt.py&#34;&gt;opt.py&lt;/a&gt; for all configurations.&lt;/p&gt; &#xA; &lt;p&gt;You can monitor the training process by &lt;code&gt;tensorboard --logdir logs/&lt;/code&gt; and go to &lt;code&gt;localhost:6006&lt;/code&gt; in your browser.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Your own data&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Steps&lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Install &lt;a href=&#34;https://github.com/colmap/colmap&#34;&gt;COLMAP&lt;/a&gt; following &lt;a href=&#34;https://colmap.github.io/install.html&#34;&gt;installation guide&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Prepare your images in a folder (around 20 to 30 for forward facing, and 40 to 50 for 360 inward-facing)&lt;/li&gt; &#xA;  &lt;li&gt;Clone &lt;a href=&#34;https://github.com/Fyusion/LLFF&#34;&gt;LLFF&lt;/a&gt; and run &lt;code&gt;python img2poses.py $your-images-folder&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Train the model using the same command as in &lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/#llff&#34;&gt;LLFF&lt;/a&gt;. If the scene is captured in a 360 inward-facing manner, add &lt;code&gt;--spheric&lt;/code&gt; argument.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;For more details of training a good model, please see the video &lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/#colab&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Pretrained models and logs&lt;/h2&gt; &#xA;&lt;p&gt;Download the pretrained models and training logs in &lt;a href=&#34;https://github.com/kwea123/nerf_pl/releases&#34;&gt;release&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Comparison with other repos&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;training GPU memory in GB&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Speed (1 step)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/bmild/nerf&#34;&gt;Original&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.177s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/yenchenlin/nerf-pytorch&#34;&gt;Ref pytorch&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.147s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;This repo&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.12s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The speed is measured on 1 RTX2080Ti. Detailed profile can be found in &lt;a href=&#34;https://github.com/kwea123/nerf_pl/releases&#34;&gt;release&lt;/a&gt;. Training memory is largely reduced, since the original repo loads the whole data to GPU at the beginning, while we only pass batches to GPU every step.&lt;/p&gt; &#xA;&lt;h1&gt;&lt;span&gt;üîé&lt;/span&gt; Testing&lt;/h1&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/test.ipynb&#34;&gt;test.ipynb&lt;/a&gt; for a simple view synthesis and depth prediction on 1 image.&lt;/p&gt; &#xA;&lt;p&gt;Use &lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/eval.py&#34;&gt;eval.py&lt;/a&gt; to create the whole sequence of moving views. E.g.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python eval.py \&#xA;   --root_dir $BLENDER \&#xA;   --dataset_name blender --scene_name lego \&#xA;   --img_wh 400 400 --N_importance 64 --ckpt_path $CKPT_PATH&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;IMPORTANT&lt;/strong&gt; : Don&#39;t forget to add &lt;code&gt;--spheric_poses&lt;/code&gt; if the model is trained under &lt;code&gt;--spheric&lt;/code&gt; setting!&lt;/p&gt; &#xA;&lt;p&gt;It will create folder &lt;code&gt;results/{dataset_name}/{scene_name}&lt;/code&gt; and run inference on all test data, finally create a gif out of them.&lt;/p&gt; &#xA;&lt;p&gt;Example of lego scene using pretrained model and the reconstructed &lt;strong&gt;colored&lt;/strong&gt; mesh: (PSNR=31.39, paper=32.54)&lt;/p&gt; &#xA;&lt;p&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/11364490/79932648-f8a1e680-8488-11ea-98fe-c11ec22fc8a1.gif&#34; width=&#34;200&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/11364490/80813179-822d8300-8c04-11ea-84e6-142f04714c58.png&#34; width=&#34;200&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Example of fern scene using pretrained model:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/11364490/79932650-f9d31380-8488-11ea-8dad-b70a6a3daa6e.gif&#34; alt=&#34;fern&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Example of own scene (&lt;a href=&#34;https://www.youtube.com/watch?v=hVQIvEq_Av0&#34;&gt;Silica GGO figure&lt;/a&gt;) and the reconstructed &lt;strong&gt;colored&lt;/strong&gt; mesh. Click to link to youtube video.&lt;/p&gt; &#xA;&lt;p&gt; &lt;a href=&#34;https://youtu.be/yH1ZBcdNsUY&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/11364490/80279695-324d4880-873a-11ea-961a-d6350e149ece.gif&#34; height=&#34;252&#34;&gt; &lt;/a&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/11364490/80813184-83f74680-8c04-11ea-8606-40580f753355.png&#34; height=&#34;252&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Portable scenes&lt;/h2&gt; &#xA;&lt;p&gt;The concept of NeRF is that the whole scene is compressed into a NeRF model, then we can render from any pose we want. To render from plausible poses, we can leverage the training poses; therefore, you can generate video with &lt;strong&gt;only&lt;/strong&gt; the trained model and the poses (hence the name of portable scenes). I provided my silica model in &lt;a href=&#34;https://github.com/kwea123/nerf_pl/releases&#34;&gt;release&lt;/a&gt;, feel free to play around with it!&lt;/p&gt; &#xA;&lt;p&gt;If you trained some interesting scenes, you are also welcomed to share the model (and the &lt;code&gt;poses_bounds.npy&lt;/code&gt;) by sending me an email, or post in issues! After all, a model is just around &lt;strong&gt;5MB&lt;/strong&gt;! Please run &lt;code&gt;python utils/save_weights_only.py --ckpt_path $YOUR_MODEL_PATH&lt;/code&gt; to extract the final model.&lt;/p&gt; &#xA;&lt;h1&gt;&lt;span&gt;üéÄ&lt;/span&gt; Mesh&lt;/h1&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/README_mesh.md&#34;&gt;README_mesh&lt;/a&gt; for reconstruction of &lt;strong&gt;colored&lt;/strong&gt; mesh. Only supported for blender dataset and 360 inward-facing data!&lt;/p&gt; &#xA;&lt;h1&gt;&lt;span&gt;‚ö†&lt;/span&gt; Notes on differences with the original repo&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The learning rate decay in the original repo is &lt;strong&gt;by step&lt;/strong&gt;, which means it decreases every step, here I use learning rate decay &lt;strong&gt;by epoch&lt;/strong&gt;, which means it changes only at the end of 1 epoch.&lt;/li&gt; &#xA; &lt;li&gt;The validation image for LLFF dataset is chosen as the most centered image here, whereas the original repo chooses every 8th image.&lt;/li&gt; &#xA; &lt;li&gt;The rendering spiral path is slightly different from the original repo (I use approximate values to simplify the code).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;&lt;span&gt;üéì&lt;/span&gt; COLAB&lt;/h1&gt; &#xA;&lt;p&gt;I also prepared colab notebooks that allow you to run the algorithm on any machine without GPU requirement.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gist.github.com/kwea123/f0e8f38ff2aa94495dbfe7ae9219f75c&#34;&gt;colmap&lt;/a&gt; to prepare camera poses for your own training data&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gist.github.com/kwea123/a3c541a325e895ef79ecbc0d2e6d7221&#34;&gt;nerf&lt;/a&gt; to train on your data&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gist.github.com/kwea123/77ed1640f9bc9550136dc13a6a419e88&#34;&gt;extract_mesh&lt;/a&gt; to extract colored mesh&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://www.youtube.com/playlist?list=PLDV2CyUo4q-K02pNEyDr7DYpTQuka3mbV&#34;&gt;this playlist&lt;/a&gt; for the detailed tutorials.&lt;/p&gt; &#xA;&lt;h1&gt;&lt;span&gt;üéÉ&lt;/span&gt; SHOWOFF&lt;/h1&gt; &#xA;&lt;p&gt;We can incorporate &lt;em&gt;ray tracing&lt;/em&gt; techniques into the volume rendering pipeline, and realize realistic scene editing (following is the &lt;code&gt;materials&lt;/code&gt; scene with an object removed, and a mesh is inserted and rendered with ray tracing). The code &lt;strong&gt;will not&lt;/strong&gt; be released.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/11364490/90312710-92face00-df41-11ea-9eea-10f24849b407.gif&#34; alt=&#34;add&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/11364490/90360796-92744b80-e097-11ea-859d-159aa2519375.gif&#34; alt=&#34;add2&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;With my integration in Unity, I can realize realistic mixed reality photos (note my character casts shadow on the scene, &lt;strong&gt;zero&lt;/strong&gt; post- image editing required): &lt;img src=&#34;https://user-images.githubusercontent.com/11364490/140264589-295acebe-8ace-4d61-b871-26eb8ae10ab0.png&#34; alt=&#34;defer&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/11364490/140264596-59daebe5-b88d-48e7-82bd-5ccaaff2283f.png&#34; alt=&#34;defer2&#34;&gt; BTW, I would like to visit the museum one day...&lt;/p&gt; &#xA;&lt;h1&gt;&lt;span&gt;üìñ&lt;/span&gt; Citation&lt;/h1&gt; &#xA;&lt;p&gt;If you use (part of) my code or find my work helpful, please consider citing&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{queianchen_nerf,&#xA;  author={Quei-An, Chen},&#xA;  title={Nerf_pl: a pytorch-lightning implementation of NeRF},&#xA;  url={https://github.com/kwea123/nerf_pl/},&#xA;  year={2020},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>jeffheaton/t81_558_deep_learning</title>
    <updated>2022-05-29T01:45:04Z</updated>
    <id>tag:github.com,2022-05-29:/jeffheaton/t81_558_deep_learning</id>
    <link href="https://github.com/jeffheaton/t81_558_deep_learning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Washington University (in St. Louis) Course T81-558: Applications of Deep Neural Networks&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;T81 558:Applications of Deep Neural Networks&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://www.wustl.edu&#34;&gt;Washington University in St. Louis&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Instructor: &lt;a href=&#34;https://sites.wustl.edu/jeffheaton/&#34;&gt;Jeff Heaton&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The content of this course changes as technology evolves&lt;/strong&gt;, to keep up to date with changes &lt;a href=&#34;https://github.com/jeffheaton&#34;&gt;follow me on GitHub&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Section 1. Fall 2022, Monday, 2:30 PM, Location: TBD&lt;/li&gt; &#xA; &lt;li&gt;Section 2. Fall 2022, Online&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Course Description&lt;/h1&gt; &#xA;&lt;p&gt;Deep learning is a group of exciting new technologies for neural networks. Through a combination of advanced training techniques and neural network architectural components, it is now possible to create neural networks that can handle tabular data, images, text, and audio as both input and output. Deep learning allows a neural network to learn hierarchies of information in a way that is like the function of the human brain. This course will introduce the student to classic neural network structures, Convolution Neural Networks (CNN), Long Short-Term Memory (LSTM), Gated Recurrent Neural Networks (GRU), General Adversarial Networks (GAN) and reinforcement learning. Application of these architectures to computer vision, time series, security, natural language processing (NLP), and data generation will be covered. High Performance Computing (HPC) aspects will demonstrate how deep learning can be leveraged both on graphical processing units (GPUs), as well as grids. Focus is primarily upon the application of deep learning to problems, with some introduction to mathematical foundations. Students will use the Python programming language to implement deep learning using Google TensorFlow and Keras. It is not necessary to know Python prior to this course; however, familiarity of at least one programming language is assumed. This course will be delivered in a hybrid format that includes both classroom and online instruction.&lt;/p&gt; &#xA;&lt;h1&gt;Textbook&lt;/h1&gt; &#xA;&lt;p&gt;The complete text for this course is here on GitHub. This same material is also available in &lt;a href=&#34;https://www.heatonresearch.com/book/applications-deep-neural-networks-keras.html&#34;&gt;book format&lt;/a&gt;. The course textbook is ‚ÄúApplications of Deep Neural networks with Keras‚Äú, ISBN 9798416344269.&lt;/p&gt; &#xA;&lt;p&gt;If you would like to cite the material from this course/book, please use the following BibTex citation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{heaton2020applications,&#xA;    title={Applications of Deep Neural Networks},&#xA;    author={Jeff Heaton},&#xA;    year={2020},&#xA;    eprint={2009.05673},&#xA;    archivePrefix={arXiv},&#xA;    primaryClass={cs.LG}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Objectives&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Explain how neural networks (deep and otherwise) compare to other machine learning models.&lt;/li&gt; &#xA; &lt;li&gt;Determine when a deep neural network would be a good choice for a particular problem.&lt;/li&gt; &#xA; &lt;li&gt;Demonstrate your understanding of the material through a final project uploaded to GitHub.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Syllabus&lt;/h1&gt; &#xA;&lt;p&gt;This syllabus presents the expected class schedule, due dates, and reading assignments. &lt;a href=&#34;https://data.heatonresearch.com/wustl/jheaton-t81-558-spring-2022-syllabus.pdf&#34;&gt;Download current syllabus.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Module&lt;/th&gt; &#xA;   &lt;th&gt;Content&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_01_1_overview.ipynb&#34;&gt;Module 1&lt;/a&gt;&lt;br&gt;&lt;strong&gt;Meet on 08/29/2022&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 1: Python Preliminaries&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 1.1: Course Overview&lt;/li&gt;&#xA;     &lt;li&gt;Part 1.2: Introduction to Python&lt;/li&gt;&#xA;     &lt;li&gt;Part 1.3: Python Lists, Dictionaries, Sets &amp;amp; JSON&lt;/li&gt;&#xA;     &lt;li&gt;Part 1.4: File Handling&lt;/li&gt;&#xA;     &lt;li&gt;Part 1.5: Functions, Lambdas, and Map/ReducePython Preliminaries&lt;/li&gt;&#xA;     &lt;li&gt;&lt;strong&gt;We will meet on campus this week! (first meeting)&lt;/strong&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_02_1_python_pandas.ipynb&#34;&gt;Module 2&lt;/a&gt;&lt;br&gt;Week of 09/12/2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 2: Python for Machine Learning&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt; Part 2.1: Introduction to Pandas for Deep Learning&lt;/li&gt;&#xA;     &lt;li&gt;Part 2.2: Encoding Categorical Values in Pandas&lt;/li&gt;&#xA;     &lt;li&gt;Part 2.3: Grouping, Sorting, and Shuffling&lt;/li&gt;&#xA;     &lt;li&gt;Part 2.4: Using Apply and Map in Pandas&lt;/li&gt;&#xA;     &lt;li&gt;Part 2.5: Feature Engineering in Padas&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class1.ipynb&#34;&gt;Module 1 Program&lt;/a&gt; due: 09/13/2022&lt;/li&gt;&#xA;     &lt;li&gt; Icebreaker due: 09/13/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_03_1_neural_net.ipynb&#34;&gt;Module 3&lt;/a&gt;&lt;br&gt;Week of 09/19/2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 3: TensorFlow and Keras for Neural Networks&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 3.1: Deep Learning and Neural Network Introduction&lt;/li&gt;&#xA;     &lt;li&gt;Part 3.2: Introduction to Tensorflow &amp;amp; Keras&lt;/li&gt;&#xA;     &lt;li&gt;Part 3.3: Saving and Loading a Keras Neural Network&lt;/li&gt;&#xA;     &lt;li&gt;Part 3.4: Early Stopping in Keras to Prevent Overfitting&lt;/li&gt;&#xA;     &lt;li&gt;Part 3.5: Extracting Keras Weights and Manual Neural Network Calculation&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class2.ipynb&#34;&gt;Module 2: Program&lt;/a&gt; due: 09/20/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_04_1_feature_encode.ipynb&#34;&gt;Module 4&lt;/a&gt;&lt;br&gt;Week of 09/26/2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 4: Training for Tabular Data&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 4.1: Encoding a Feature Vector for Keras Deep Learning&lt;/li&gt;&#xA;     &lt;li&gt;Part 4.2: Keras Multiclass Classification for Deep Neural Networks with ROC and AUC&lt;/li&gt;&#xA;     &lt;li&gt;Part 4.3: Keras Regression for Deep Neural Networks with RMSE&lt;/li&gt;&#xA;     &lt;li&gt;Part 4.4: Backpropagation, Nesterov Momentum, and ADAM Training&lt;/li&gt;&#xA;     &lt;li&gt;Part 4.5: Neural Network RMSE and Log Loss Error Calculation from Scratch&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class3.ipynb&#34;&gt;Module 3 Program&lt;/a&gt; due: 09/27/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_05_1_reg_ridge_lasso.ipynb&#34;&gt;Module 5&lt;/a&gt;&lt;br&gt;&lt;strong&gt;Meet on 10/03/2022&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 5: Regularization and Dropout&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 5.1: Introduction to Regularization: Ridge and Lasso&lt;/li&gt;&#xA;     &lt;li&gt;Part 5.2: Using K-Fold Cross Validation with Keras&lt;/li&gt;&#xA;     &lt;li&gt;Part 5.3: Using L1 and L2 Regularization with Keras to Decrease Overfitting&lt;/li&gt;&#xA;     &lt;li&gt;Part 5.4: Drop Out for Keras to Decrease Overfitting&lt;/li&gt;&#xA;     &lt;li&gt;Part 5.5: Bootstrapping and Benchmarking Hyperparameters&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class4.ipynb&#34;&gt;Module 4 Program&lt;/a&gt; due: 10/04/2022&lt;/li&gt;&#xA;     &lt;li&gt;&lt;strong&gt;We will meet on campus this week! (second meeting)&lt;/strong&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_06_1_python_images.ipynb&#34;&gt;Module 6&lt;/a&gt;&lt;br&gt;Week of 10/17/2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 6: CNN for Vision&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;      Part 6.1: Image Processing in Python&#xA;     &lt;li&gt;Part 6.2: Using Convolutional Networks with Keras&lt;/li&gt;&#xA;     &lt;li&gt;Part 6.3: Using Pretrained Neural Networks&lt;/li&gt;&#xA;     &lt;li&gt;Part 6.4: Looking at Keras Generators and Image Augmentation&lt;/li&gt;&#xA;     &lt;li&gt;Part 6.5: Recognizing Multiple Images with YOLOv5&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class5.ipynb&#34;&gt;Module 5 Program&lt;/a&gt; due: 10/18/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_07_1_gan_intro.ipynb&#34;&gt;Module 7&lt;/a&gt;&lt;br&gt;Week of 10/24/2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 7: Generative Adversarial Networks (GANs)&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 7.1: Introduction to GANS for Image and Data Generation&lt;/li&gt;&#xA;     &lt;li&gt;Part 7.2: Train StyleGAN3 with your Own Images&lt;/li&gt;&#xA;     &lt;li&gt;Part 7.3: Exploring the StyleGAN Latent Vector&lt;/li&gt;&#xA;     &lt;li&gt;Part 7.4: GANS to Enhance Old Photographs Deoldify&lt;/li&gt;&#xA;     &lt;li&gt;Part 7.5: GANs for Tabular Synthetic Data Generation&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class6.ipynb&#34;&gt;Module 6 Assignment&lt;/a&gt; due: 10/25/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_08_1_kaggle_intro.ipynb&#34;&gt;Module 8&lt;/a&gt;&lt;br&gt;&lt;strong&gt;Meet on 10/31/2022&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 8: Kaggle&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 8.1: Introduction to Kaggle&lt;/li&gt;&#xA;     &lt;li&gt;Part 8.2: Building Ensembles with Scikit-Learn and Keras&lt;/li&gt;&#xA;     &lt;li&gt;Part 8.3: How Should you Architect Your Keras Neural Network: Hyperparameters&lt;/li&gt;&#xA;     &lt;li&gt;Part 8.4: Bayesian Hyperparameter Optimization for Keras&lt;/li&gt;&#xA;     &lt;li&gt;Part 8.5: Current Semester&#39;s Kaggle&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class7.ipynb&#34;&gt;Module 7 Assignment&lt;/a&gt; due: 11/01/2022&lt;/li&gt;&#xA;     &lt;li&gt;&lt;strong&gt;We will meet on campus this week! (third meeting)&lt;/strong&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_09_1_keras_transfer.ipynb&#34;&gt;Module 9&lt;/a&gt;&lt;br&gt;Week of 11/07/2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 9: Transfer Learning&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 9.1: Introduction to Keras Transfer Learning&lt;/li&gt;&#xA;     &lt;li&gt;Part 9.2: Keras Transfer Learning for Computer Vision&lt;/li&gt;&#xA;     &lt;li&gt;Part 9.3: Transfer Learning for NLP with Keras&lt;/li&gt;&#xA;     &lt;li&gt;Part 9.4: Transfer Learning for Facial Feature Recognition&lt;/li&gt;&#xA;     &lt;li&gt;Part 9.5: Transfer Learning for Style Transfer&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class8.ipynb&#34;&gt;Module 8 Assignment&lt;/a&gt; due: 11/08/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_10_1_timeseries.ipynb&#34;&gt;Module 10&lt;/a&gt;&lt;br&gt;Week of 11/14/2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 10: Time Series in Keras&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 10.1: Time Series Data Encoding for Deep Learning, Keras&lt;/li&gt;&#xA;     &lt;li&gt;Part 10.2: Programming LSTM with Keras and&lt;/li&gt;&#xA;     &lt;li&gt;Part 10.3: Text Generation with Keras&lt;/li&gt;&#xA;     &lt;li&gt;Part 10.4: Introduction to Transformers&lt;/li&gt;&#xA;     &lt;li&gt;Part 10.5: Transformers for Timeseries&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class9.ipynb&#34;&gt;Module 9 Assignment&lt;/a&gt; due: 11/15/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_11_01_huggingface.ipynb&#34;&gt;Module 11&lt;/a&gt;&lt;br&gt;Week of 11/21/2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 11: Natural Language Processing&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 11.1: Hugging Face Introduction&lt;/li&gt;&#xA;     &lt;li&gt;Part 11.2: Hugging Face Tokenizers&lt;/li&gt;&#xA;     &lt;li&gt;Part 11.3: Hugging Face Data Sets&lt;/li&gt;&#xA;     &lt;li&gt;Part 11.4: Training a Model in Hugging Face&lt;/li&gt;&#xA;     &lt;li&gt;Part 11.5: What are Embedding Layers in Keras&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class10.ipynb&#34;&gt;Module 10 Assignment&lt;/a&gt; due: 11/22/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_12_01_ai_gym.ipynb&#34;&gt;Module 12&lt;/a&gt;&lt;br&gt;Week of 11/28/2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 12: Reinforcement Learning&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Kaggle Assignment due: 11/29/2022 (approx 4-6PM, due to Kaggle GMT timezone)&lt;/li&gt;&#xA;     &lt;li&gt;Part 12.1: Introduction to the OpenAI Gym&lt;/li&gt;&#xA;     &lt;li&gt;Part 12.2: Introduction to Q-Learning for Keras&lt;/li&gt;&#xA;     &lt;li&gt;Part 12.3: Keras Q-Learning in the OpenAI Gym&lt;/li&gt;&#xA;     &lt;li&gt;Part 12.4: Atari Games with Keras Neural Networks&lt;/li&gt;&#xA;     &lt;li&gt;Part 12.5: Application of Reinforcement Learning&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class11.ipynb&#34;&gt;Module 11 Assignment&lt;/a&gt; due: 11/29/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_13_01_flask.ipynb&#34;&gt;Module 13&lt;/a&gt;&lt;br&gt;&lt;strong&gt;Meet on 12/05/2022&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 13: Deployment and Monitoring&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 13.1: Flask and Deep Learning Web Services &lt;/li&gt;&#xA;     &lt;li&gt;Part 13.2: Interrupting and Continuing Training&lt;/li&gt;&#xA;     &lt;li&gt;Part 13.3: Using a Keras Deep Neural Network with a Web Application&lt;/li&gt;&#xA;     &lt;li&gt;Part 13.4: When to Retrain Your Neural Network&lt;/li&gt;&#xA;     &lt;li&gt;Part 13.5: Tensor Processing Units (TPUs)&lt;/li&gt;&#xA;     &lt;li&gt;&lt;strong&gt;We will meet on campus this week! (fourth meeting)&lt;/strong&gt;&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class12.ipynb&#34;&gt;Module 12 Assignment&lt;/a&gt; due: 12/06/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Datasets&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://data.heatonresearch.com/data/t81-558/index.html&#34;&gt;Datasets can be downloaded here&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>codebasics/py</title>
    <updated>2022-05-29T01:45:04Z</updated>
    <id>tag:github.com,2022-05-29:/codebasics/py</id>
    <link href="https://github.com/codebasics/py" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Repository to store sample python programs for python learning&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;py&lt;/h1&gt; &#xA;&lt;p&gt;Repository to store sample Python programs.&lt;/p&gt; &#xA;&lt;p&gt;This repository is meant for beginners to assist them in their learning of Python. The repository covers a wide range of algorithms and other programs, and would prove immensely helpful for everybody interested in Python programming.&lt;/p&gt; &#xA;&lt;p&gt;If this is your first time coding in Python, I would love to suggest you begin from the &lt;a href=&#34;https://github.com/codebasics/py/tree/master/Basics&#34;&gt;Basics&lt;/a&gt;. They are simple to understand and hopefully will prove fun to you.&lt;/p&gt; &#xA;&lt;p&gt;You can also pay a visit to my very own &lt;a href=&#34;https://www.youtube.com/channel/UCh9nVJoWXmFb7sLApWGcLPQ&#34;&gt;Youtube channel&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Contributions to the repository are welcome.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/channel/UCh9nVJoWXmFb7sLApWGcLPQ&#34;&gt;&lt;img src=&#34;https://yt3.ggpht.com/ytc/AAUvwnihwx4a5idwBTE5JFpXHb-ykyh-i1gXtFiGJYV1=s176-c-k-c0x00ffffff-no-rj&#34; alt=&#34;CodeBasics&#34;&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Happy coding!&lt;/h4&gt;</summary>
  </entry>
  <entry>
    <title>GoogleCloudPlatform/training-data-analyst</title>
    <updated>2022-05-29T01:45:04Z</updated>
    <id>tag:github.com,2022-05-29:/GoogleCloudPlatform/training-data-analyst</id>
    <link href="https://github.com/GoogleCloudPlatform/training-data-analyst" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Labs and demos for courses for GCP Training (http://cloud.google.com/training).&lt;/p&gt;&lt;hr&gt;</summary>
  </entry>
  <entry>
    <title>wolny/pytorch-3dunet</title>
    <updated>2022-05-29T01:45:04Z</updated>
    <id>tag:github.com,2022-05-29:/wolny/pytorch-3dunet</id>
    <link href="https://github.com/wolny/pytorch-3dunet" rel="alternate"></link>
    <summary type="html">&lt;p&gt;3D U-Net model for volumetric semantic segmentation written in pytorch&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://zenodo.org/badge/latestdoi/149826542&#34;&gt;&lt;img src=&#34;https://zenodo.org/badge/149826542.svg?sanitize=true&#34; alt=&#34;DOI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/wolny/pytorch-3dunet/actions/&#34;&gt;&lt;img src=&#34;https://github.com/wolny/pytorch-3dunet/actions/workflows/conda-build.yml/badge.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;pytorch-3dunet&lt;/h1&gt; &#xA;&lt;p&gt;PyTorch implementation 3D U-Net and its variants:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Standard 3D U-Net based on &lt;a href=&#34;https://arxiv.org/abs/1606.06650&#34;&gt;3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation&lt;/a&gt; √ñzg√ºn √ái√ßek et al.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Residual 3D U-Net based on &lt;a href=&#34;https://arxiv.org/pdf/1706.00120.pdf&#34;&gt;Superhuman Accuracy on the SNEMI3D Connectomics Challenge&lt;/a&gt; Kisuk Lee et al.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The code allows for training the U-Net for both: &lt;strong&gt;semantic segmentation&lt;/strong&gt; (binary and multi-class) and &lt;strong&gt;regression&lt;/strong&gt; problems (e.g. de-noising, learning deconvolutions).&lt;/p&gt; &#xA;&lt;h2&gt;2D U-Net&lt;/h2&gt; &#xA;&lt;p&gt;Training the standard 2D U-Net is also possible, see &lt;a href=&#34;https://raw.githubusercontent.com/wolny/pytorch-3dunet/master/resources/2DUnet_dsb2018/train_config.yml&#34;&gt;2DUnet_dsb2018&lt;/a&gt; for example configuration. Just make sure to keep the singleton z-dimension in your H5 dataset (i.e. &lt;code&gt;(1, Y, X)&lt;/code&gt; instead of &lt;code&gt;(Y, X)&lt;/code&gt;) , because data loading / data augmentation requires tensors of rank 3 always.&lt;/p&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Linux&lt;/li&gt; &#xA; &lt;li&gt;NVIDIA GPU&lt;/li&gt; &#xA; &lt;li&gt;CUDA CuDNN&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Running on Windows&lt;/h3&gt; &#xA;&lt;p&gt;The package has not been tested on Windows, however some users reported using it successfully on Windows.&lt;/p&gt; &#xA;&lt;h2&gt;Supported Loss Functions&lt;/h2&gt; &#xA;&lt;h3&gt;Semantic Segmentation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;BCEWithLogitsLoss&lt;/em&gt; (binary cross-entropy)&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;DiceLoss&lt;/em&gt; (standard &lt;code&gt;DiceLoss&lt;/code&gt; defined as &lt;code&gt;1 - DiceCoefficient&lt;/code&gt; used for binary semantic segmentation; when more than 2 classes are present in the ground truth, it computes the &lt;code&gt;DiceLoss&lt;/code&gt; per channel and averages the values)&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;BCEDiceLoss&lt;/em&gt; (Linear combination of BCE and Dice losses, i.e. &lt;code&gt;alpha * BCE + beta * Dice&lt;/code&gt;, &lt;code&gt;alpha, beta&lt;/code&gt; can be specified in the &lt;code&gt;loss&lt;/code&gt; section of the config)&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;CrossEntropyLoss&lt;/em&gt; (one can specify class weights via the &lt;code&gt;weight: [w_1, ..., w_k]&lt;/code&gt; in the &lt;code&gt;loss&lt;/code&gt; section of the config)&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;PixelWiseCrossEntropyLoss&lt;/em&gt; (one can specify per pixel weights in order to give more gradient to the important/under-represented regions in the ground truth)&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;WeightedCrossEntropyLoss&lt;/em&gt; (see &#39;Weighted cross-entropy (WCE)&#39; in the below paper for a detailed explanation)&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;GeneralizedDiceLoss&lt;/em&gt; (see &#39;Generalized Dice Loss (GDL)&#39; in the below paper for a detailed explanation) Note: use this loss function only if the labels in the training dataset are very imbalanced e.g. one class having at least 3 orders of magnitude more voxels than the others. Otherwise use standard &lt;em&gt;DiceLoss&lt;/em&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For a detailed explanation of some of the supported loss functions see: &lt;a href=&#34;https://arxiv.org/pdf/1707.03237.pdf&#34;&gt;Generalised Dice overlap as a deep learning loss function for highly unbalanced segmentations&lt;/a&gt; Carole H. Sudre et al.&lt;/p&gt; &#xA;&lt;h3&gt;Regression&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;MSELoss&lt;/em&gt; (mean squared error loss)&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;L1Loss&lt;/em&gt; (mean absolute errro loss)&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;SmoothL1Loss&lt;/em&gt; (less sensitive to outliers than MSELoss)&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;WeightedSmoothL1Loss&lt;/em&gt; (extension of the &lt;em&gt;SmoothL1Loss&lt;/em&gt; which allows to weight the voxel values above/below a given threshold differently)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Supported Evaluation Metrics&lt;/h2&gt; &#xA;&lt;h3&gt;Semantic Segmentation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;MeanIoU&lt;/em&gt; (mean intersection over union)&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;DiceCoefficient&lt;/em&gt; (computes per channel Dice Coefficient and returns the average) If a 3D U-Net was trained to predict cell boundaries, one can use the following semantic instance segmentation metrics (the metrics below are computed by running connected components on thresholded boundary map and comparing the resulted instances to the ground truth instance segmentation):&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;BoundaryAveragePrecision&lt;/em&gt; (Average Precision applied to the boundary probability maps: thresholds the output from the network, runs connected components to get the segmentation and computes AP between the resulting segmentation and the ground truth)&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;AdaptedRandError&lt;/em&gt; (see &lt;a href=&#34;http://brainiac2.mit.edu/SNEMI3D/evaluation&#34;&gt;http://brainiac2.mit.edu/SNEMI3D/evaluation&lt;/a&gt; for a detailed explanation)&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;AveragePrecision&lt;/em&gt; (see &lt;a href=&#34;https://www.kaggle.com/stkbailey/step-by-step-explanation-of-scoring-metric&#34;&gt;https://www.kaggle.com/stkbailey/step-by-step-explanation-of-scoring-metric&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If not specified &lt;code&gt;MeanIoU&lt;/code&gt; will be used by default.&lt;/p&gt; &#xA;&lt;h3&gt;Regression&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;PSNR&lt;/em&gt; (peak signal to noise ratio)&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;MSE&lt;/em&gt; (mean squared error)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The easiest way to install &lt;code&gt;pytorch-3dunet&lt;/code&gt; package is via conda:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n pytorch3dunet -c pytorch -c conda-forge -c awolny pytorch-3dunet&#xA;conda activate pytorch3dunet&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After installation the following commands are accessible within the conda environment: &lt;code&gt;train3dunet&lt;/code&gt; for training the network and &lt;code&gt;predict3dunet&lt;/code&gt; for prediction (see below).&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;One can also install directly from source:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Installation tips&lt;/h3&gt; &#xA;&lt;p&gt;Make sure that the installed &lt;code&gt;pytorch&lt;/code&gt; is compatible with your CUDA version, otherwise the training/prediction will fail to run on GPU. You can re-install &lt;code&gt;pytorch&lt;/code&gt; compatible with your CUDA in the &lt;code&gt;pytorch3dunet&lt;/code&gt; environment by:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda install -c pytorch cudatoolkit=&amp;lt;YOU_CUDA_VERSION&amp;gt; pytorch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Train&lt;/h2&gt; &#xA;&lt;p&gt;Given that &lt;code&gt;pytorch-3dunet&lt;/code&gt; package was installed via conda as described above, one can train the network by simply invoking:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;train3dunet --config &amp;lt;CONFIG&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where &lt;code&gt;CONFIG&lt;/code&gt; is the path to a YAML configuration file, which specifies all aspects of the training procedure.&lt;/p&gt; &#xA;&lt;p&gt;In order to train on your own data just provide the paths to your HDF5 training and validation datasets in the config.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;sample config for 3D semantic segmentation (cell boundary segmentation): &lt;a href=&#34;https://raw.githubusercontent.com/wolny/pytorch-3dunet/master/resources/3DUnet_confocal_boundary/train_config.yml&#34;&gt;train_config_segmentation.yaml&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;sample config for 3D regression task (denoising): &lt;a href=&#34;https://raw.githubusercontent.com/wolny/pytorch-3dunet/master/resources/3DUnet_denoising/train_config_regression.yaml&#34;&gt;train_config_regression.yaml&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The HDF5 files should contain the raw/label data sets in the following axis order: &lt;code&gt;DHW&lt;/code&gt; (in case of 3D) &lt;code&gt;CDHW&lt;/code&gt; (in case of 4D).&lt;/p&gt; &#xA;&lt;p&gt;One can monitor the training progress with Tensorboard &lt;code&gt;tensorboard --logdir &amp;lt;checkpoint_dir&amp;gt;/logs/&lt;/code&gt; (you need &lt;code&gt;tensorflow&lt;/code&gt; installed in your conda env), where &lt;code&gt;checkpoint_dir&lt;/code&gt; is the path to the checkpoint directory specified in the config.&lt;/p&gt; &#xA;&lt;h3&gt;Training tips&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;When training with binary-based losses, i.e.: &lt;code&gt;BCEWithLogitsLoss&lt;/code&gt;, &lt;code&gt;DiceLoss&lt;/code&gt;, &lt;code&gt;BCEDiceLoss&lt;/code&gt;, &lt;code&gt;GeneralizedDiceLoss&lt;/code&gt;: The target data has to be 4D (one target binary mask per channel). When training with &lt;code&gt;WeightedCrossEntropyLoss&lt;/code&gt;, &lt;code&gt;CrossEntropyLoss&lt;/code&gt;, &lt;code&gt;PixelWiseCrossEntropyLoss&lt;/code&gt; the target dataset has to be 3D, see also pytorch documentation for CE loss: &lt;a href=&#34;https://pytorch.org/docs/master/generated/torch.nn.CrossEntropyLoss.html&#34;&gt;https://pytorch.org/docs/master/generated/torch.nn.CrossEntropyLoss.html&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;final_sigmoid&lt;/code&gt; in the &lt;code&gt;model&lt;/code&gt; config section applies only to the inference time (validation, test): When training with cross entropy based losses (&lt;code&gt;WeightedCrossEntropyLoss&lt;/code&gt;, &lt;code&gt;CrossEntropyLoss&lt;/code&gt;, &lt;code&gt;PixelWiseCrossEntropyLoss&lt;/code&gt;) set &lt;code&gt;final_sigmoid=False&lt;/code&gt; so that &lt;code&gt;Softmax&lt;/code&gt; normalization is applied to the output. When training with &lt;code&gt;BCEWithLogitsLoss&lt;/code&gt;, &lt;code&gt;DiceLoss&lt;/code&gt;, &lt;code&gt;BCEDiceLoss&lt;/code&gt;, &lt;code&gt;GeneralizedDiceLoss&lt;/code&gt; set &lt;code&gt;final_sigmoid=True&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Prediction&lt;/h2&gt; &#xA;&lt;p&gt;Given that &lt;code&gt;pytorch-3dunet&lt;/code&gt; package was installed via conda as described above, one can run the prediction via:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;predict3dunet --config &amp;lt;CONFIG&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In order to predict on your own data, just provide the path to your model as well as paths to HDF5 test files (see example &lt;a href=&#34;https://raw.githubusercontent.com/wolny/pytorch-3dunet/master/resources/3DUnet_confocal_boundary/test_config.yml&#34;&gt;test_config_segmentation.yaml&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;h3&gt;Prediction tips&lt;/h3&gt; &#xA;&lt;p&gt;In order to avoid patch boundary artifacts in the output prediction masks the patch predictions are averaged, so make sure that &lt;code&gt;patch/stride&lt;/code&gt; params lead to overlapping blocks, e.g. &lt;code&gt;patch: [64, 128, 128] stride: [32, 96, 96]&lt;/code&gt; will give you a &#39;halo&#39; of 32 voxels in each direction.&lt;/p&gt; &#xA;&lt;h2&gt;Data Parallelism&lt;/h2&gt; &#xA;&lt;p&gt;By default, if multiple GPUs are available training/prediction will be run on all the GPUs using &lt;a href=&#34;https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html&#34;&gt;DataParallel&lt;/a&gt;. If training/prediction on all available GPUs is not desirable, restrict the number of GPUs using &lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt;, e.g.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_VISIBLE_DEVICES=0,1 train3dunet --config &amp;lt;CONFIG&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_VISIBLE_DEVICES=0,1 predict3dunet --config &amp;lt;CONFIG&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;h3&gt;Cell boundary predictions for lightsheet images of Arabidopsis thaliana lateral root&lt;/h3&gt; &#xA;&lt;p&gt;Training/predictions configs can be found in &lt;a href=&#34;https://raw.githubusercontent.com/wolny/pytorch-3dunet/master/resources/3DUnet_lightsheet_boundary&#34;&gt;3DUnet_lightsheet_boundary&lt;/a&gt;. Pre-trained model weights available &lt;a href=&#34;https://oc.embl.de/index.php/s/61s67Mg5VQy7dh9/download?path=%2FLateral-Root-Primordia%2Funet_bce_dice_ds1x&amp;amp;files=best_checkpoint.pytorch&#34;&gt;here&lt;/a&gt;. In order to use the pre-trained model on your own data:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;download the &lt;code&gt;best_checkpoint.pytorch&lt;/code&gt; from the above link&lt;/li&gt; &#xA; &lt;li&gt;add the path to the downloaded model and the path to your data in &lt;a href=&#34;https://raw.githubusercontent.com/wolny/pytorch-3dunet/master/resources/3DUnet_lightsheet_boundary/test_config.yml&#34;&gt;test_config.yml&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;run &lt;code&gt;predict3dunet --config test_config.yml&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;optionally fine-tune the pre-trained model with your own data, by setting the &lt;code&gt;pre_trained&lt;/code&gt; attribute in the YAML config to point to the &lt;code&gt;best_checkpoint.pytorch&lt;/code&gt; path&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The data used for training can be downloaded from the following OSF project:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;training set: &lt;a href=&#34;https://osf.io/9x3g2/&#34;&gt;https://osf.io/9x3g2/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;validation set: &lt;a href=&#34;https://osf.io/vs6gb/&#34;&gt;https://osf.io/vs6gb/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;test set: &lt;a href=&#34;https://osf.io/tn4xj/&#34;&gt;https://osf.io/tn4xj/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Sample z-slice predictions on the test set (top: raw input , bottom: boundary predictions):&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/wolny/pytorch-3dunet/raw/master/resources/3DUnet_lightsheet_boundary/root_movie1_t45_raw.png&#34; width=&#34;400&#34;&gt; &#xA;&lt;img src=&#34;https://github.com/wolny/pytorch-3dunet/raw/master/resources/3DUnet_lightsheet_boundary/root_movie1_t45_pred.png&#34; width=&#34;400&#34;&gt; &#xA;&lt;h3&gt;Cell boundary predictions for confocal images of Arabidopsis thaliana ovules&lt;/h3&gt; &#xA;&lt;p&gt;Training/predictions configs can be found in &lt;a href=&#34;https://raw.githubusercontent.com/wolny/pytorch-3dunet/master/resources/3DUnet_confocal_boundary&#34;&gt;3DUnet_confocal_boundary&lt;/a&gt;. Pre-trained model weights available &lt;a href=&#34;https://oc.embl.de/index.php/s/61s67Mg5VQy7dh9/download?path=%2FArabidopsis-Ovules%2Funet_bce_dice_ds2x&amp;amp;files=best_checkpoint.pytorch&#34;&gt;here&lt;/a&gt;. In order to use the pre-trained model on your own data:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;download the &lt;code&gt;best_checkpoint.pytorch&lt;/code&gt; from the above link&lt;/li&gt; &#xA; &lt;li&gt;add the path to the downloaded model and the path to your data in &lt;a href=&#34;https://raw.githubusercontent.com/wolny/pytorch-3dunet/master/resources/3DUnet_confocal_boundary/test_config.yml&#34;&gt;test_config.yml&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;run &lt;code&gt;predict3dunet --config test_config.yml&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;optionally fine-tune the pre-trained model with your own data, by setting the &lt;code&gt;pre_trained&lt;/code&gt; attribute in the YAML config to point to the &lt;code&gt;best_checkpoint.pytorch&lt;/code&gt; path&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The data used for training can be downloaded from the following OSF project:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;training set: &lt;a href=&#34;https://osf.io/x9yns/&#34;&gt;https://osf.io/x9yns/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;validation set: &lt;a href=&#34;https://osf.io/xp5uf/&#34;&gt;https://osf.io/xp5uf/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;test set: &lt;a href=&#34;https://osf.io/8jz7e/&#34;&gt;https://osf.io/8jz7e/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Sample z-slice predictions on the test set (top: raw input , bottom: boundary predictions):&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/wolny/pytorch-3dunet/raw/master/resources/3DUnet_confocal_boundary/ovules_raw.png&#34; width=&#34;400&#34;&gt; &#xA;&lt;img src=&#34;https://github.com/wolny/pytorch-3dunet/raw/master/resources/3DUnet_confocal_boundary/ovules_pred.png&#34; width=&#34;400&#34;&gt; &#xA;&lt;h3&gt;Nuclei predictions for lightsheet images of Arabidopsis thaliana lateral root&lt;/h3&gt; &#xA;&lt;p&gt;Training/predictions configs can be found in &lt;a href=&#34;https://raw.githubusercontent.com/wolny/pytorch-3dunet/master/resources/3DUnet_lightsheet_nuclei&#34;&gt;3DUnet_lightsheet_nuclei&lt;/a&gt;. Pre-trained model weights available &lt;a href=&#34;https://oc.embl.de/index.php/s/61s67Mg5VQy7dh9/download?path=%2FLateral-Root-Primordia%2Funet_bce_dice_nuclei_ds1x&amp;amp;files=best_checkpoint.pytorch&#34;&gt;here&lt;/a&gt;. In order to use the pre-trained model on your own data:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;download the &lt;code&gt;best_checkpoint.pytorch&lt;/code&gt; from the above link&lt;/li&gt; &#xA; &lt;li&gt;add the path to the downloaded model and the path to your data in &lt;a href=&#34;https://raw.githubusercontent.com/wolny/pytorch-3dunet/master/resources/3DUnet_lightsheet_nuclei/test_config.yaml&#34;&gt;test_config.yml&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;run &lt;code&gt;predict3dunet --config test_config.yml&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;optionally fine-tune the pre-trained model with your own data, by setting the &lt;code&gt;pre_trained&lt;/code&gt; attribute in the YAML config to point to the &lt;code&gt;best_checkpoint.pytorch&lt;/code&gt; path&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The training and validation sets can be downloaded from the following OSF project: &lt;a href=&#34;https://osf.io/thxzn/&#34;&gt;https://osf.io/thxzn/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Sample z-slice predictions on the test set (top: raw input, bottom: nuclei predictions):&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/wolny/pytorch-3dunet/raw/master/resources/3DUnet_lightsheet_nuclei/root_nuclei_t30_raw.png&#34; width=&#34;400&#34;&gt; &#xA;&lt;img src=&#34;https://github.com/wolny/pytorch-3dunet/raw/master/resources/3DUnet_lightsheet_nuclei/root_nuclei_t30_pred.png&#34; width=&#34;400&#34;&gt; &#xA;&lt;h3&gt;2D nuclei predictions for Kaggle DSB2018&lt;/h3&gt; &#xA;&lt;p&gt;The data can be downloaded from: &lt;a href=&#34;https://www.kaggle.com/c/data-science-bowl-2018/data&#34;&gt;https://www.kaggle.com/c/data-science-bowl-2018/data&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Training/predictions configs can be found in &lt;a href=&#34;https://raw.githubusercontent.com/wolny/pytorch-3dunet/master/resources/2DUnet_dsb2018&#34;&gt;2DUnet_dsb2018&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Sample predictions on the test image (top: raw input, bottom: nuclei predictions):&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/wolny/pytorch-3dunet/raw/master/resources/2DUnet_dsb2018/5f9d29d6388c700f35a3c29fa1b1ce0c1cba6667d05fdb70bd1e89004dcf71ed.png&#34; width=&#34;400&#34;&gt; &#xA;&lt;img src=&#34;https://github.com/wolny/pytorch-3dunet/raw/master/resources/2DUnet_dsb2018/5f9d29d6388c700f35a3c29fa1b1ce0c1cba6667d05fdb70bd1e89004dcf71ed_predictions.png&#34; width=&#34;400&#34;&gt; &#xA;&lt;h2&gt;Contribute&lt;/h2&gt; &#xA;&lt;p&gt;If you want to contribute back, please make a pull request.&lt;/p&gt; &#xA;&lt;h2&gt;Cite&lt;/h2&gt; &#xA;&lt;p&gt;If you use this code for your research, please cite as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article {10.7554/eLife.57613,&#xA;article_type = {journal},&#xA;title = {Accurate and versatile 3D segmentation of plant tissues at cellular resolution},&#xA;author = {Wolny, Adrian and Cerrone, Lorenzo and Vijayan, Athul and Tofanelli, Rachele and Barro, Amaya Vilches and Louveaux, Marion and Wenzl, Christian and Strauss, S√∂ren and Wilson-S√°nchez, David and Lymbouridou, Rena and Steigleder, Susanne S and Pape, Constantin and Bailoni, Alberto and Duran-Nebreda, Salva and Bassel, George W and Lohmann, Jan U and Tsiantis, Miltos and Hamprecht, Fred A and Schneitz, Kay and Maizel, Alexis and Kreshuk, Anna},&#xA;editor = {Hardtke, Christian S and Bergmann, Dominique C and Bergmann, Dominique C and Graeff, Moritz},&#xA;volume = 9,&#xA;year = 2020,&#xA;month = {jul},&#xA;pub_date = {2020-07-29},&#xA;pages = {e57613},&#xA;citation = {eLife 2020;9:e57613},&#xA;doi = {10.7554/eLife.57613},&#xA;url = {https://doi.org/10.7554/eLife.57613},&#xA;keywords = {instance segmentation, cell segmentation, deep learning, image analysis},&#xA;journal = {eLife},&#xA;issn = {2050-084X},&#xA;publisher = {eLife Sciences Publications, Ltd},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>Azure/MachineLearningNotebooks</title>
    <updated>2022-05-29T01:45:04Z</updated>
    <id>tag:github.com,2022-05-29:/Azure/MachineLearningNotebooks</id>
    <link href="https://github.com/Azure/MachineLearningNotebooks" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Python notebooks with ML and deep learning examples with Azure Machine Learning Python SDK | Microsoft&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Azure Machine Learning Python SDK notebooks&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;a community-driven repository of examples using mlflow for tracking can be found at &lt;a href=&#34;https://github.com/Azure/azureml-examples&#34;&gt;https://github.com/Azure/azureml-examples&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Welcome to the Azure Machine Learning Python SDK notebooks repository!&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;These notebooks are recommended for use in an Azure Machine Learning &lt;a href=&#34;https://docs.microsoft.com/azure/machine-learning/concept-compute-instance&#34;&gt;Compute Instance&lt;/a&gt;, where you can run them without any additional set up.&lt;/p&gt; &#xA;&lt;p&gt;However, the notebooks can be run in any development environment with the correct &lt;code&gt;azureml&lt;/code&gt; packages installed.&lt;/p&gt; &#xA;&lt;p&gt;Install the &lt;code&gt;azureml.core&lt;/code&gt; Python package:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install azureml-core&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install additional packages as needed:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install azureml-mlflow&#xA;pip install azureml-dataset-runtime&#xA;pip install azureml-automl-runtime&#xA;pip install azureml-pipeline&#xA;pip install azureml-pipeline-steps&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We recommend starting with one of the &lt;a href=&#34;https://raw.githubusercontent.com/Azure/MachineLearningNotebooks/master/tutorials/compute-instance-quickstarts&#34;&gt;quickstarts&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This repository is a push-only mirror. Pull requests are ignored.&lt;/p&gt; &#xA;&lt;h2&gt;Code of Conduct&lt;/h2&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. Please see the &lt;a href=&#34;https://raw.githubusercontent.com/Azure/MachineLearningNotebooks/master/CODE_OF_CONDUCT.md&#34;&gt;code of conduct&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;h2&gt;Reference&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/azure/machine-learning&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>probml/pyprobml</title>
    <updated>2022-05-29T01:45:04Z</updated>
    <id>tag:github.com,2022-05-29:/probml/pyprobml</id>
    <link href="https://github.com/probml/pyprobml" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Python code for &#34;Probabilistic Machine learning&#34; book by Kevin Murphy&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;pyprobml&lt;/h1&gt; &#xA;&lt;img src=&#34;https://img.shields.io/github/stars/probml/pyprobml?style=social&#34;&gt; &#xA;&lt;p&gt;Python 3 code to reproduce the figures in the book series &lt;a href=&#34;https://probml.github.io/pml-book/&#34;&gt;Probabilistic Machine Learning&lt;/a&gt; by Kevin Patrick Murphy. This is work in progress, so expect rough edges.&lt;/p&gt; &#xA;&lt;p&gt;See also &lt;a href=&#34;https://github.com/probml/probml-utils&#34;&gt;probml-utils&lt;/a&gt; for some utility code.&lt;/p&gt; &#xA;&lt;h2&gt;Running the notebooks&lt;/h2&gt; &#xA;&lt;p&gt;The notebooks needed to make all the figures are available at the following locations.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/probml/pyprobml/tree/master/notebooks/book1&#34;&gt;Volume 1 figure notebooks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/probml/pyprobml/tree/master/notebooks/book2&#34;&gt;Volume 2 figure notebooks&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Notebooks are saved in chapter-wise folders. For example, a notebook for figure 2.3 from book 1 is saved in the folder &lt;code&gt;notebooks/book1/02/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In addition to the figure notebooks, there are a series of notebooks which provide supplementary material for the book. These are stored in the &lt;a href=&#34;https://github.com/probml/pyprobml/tree/master/notebooks/misc&#34;&gt;&lt;code&gt;notebooks/misc&lt;/code&gt; folder&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Running notebooks in colab&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/notebooks/intro.ipynb&#34;&gt;Colab&lt;/a&gt; has most of the libraries you will need (e.g., scikit-learn, JAX) pre-installed, and gives you access to a free GPU and TPU. We have a created a &lt;a href=&#34;https://colab.research.google.com/github/probml/pyprobml/blob/master/notebooks/misc/colab_intro.ipynb&#34;&gt;colab intro&lt;/a&gt; notebook with more details. To run the notebooks on colab in any browser, you can go to a particular notebook on GitHub and change the domain from &lt;code&gt;github.com&lt;/code&gt; to &lt;code&gt;githubtocolab.com&lt;/code&gt; as suggested &lt;a href=&#34;https://stackoverflow.com/a/67344477/13330701&#34;&gt;here&lt;/a&gt;. If you are using Google Chrome browser, you can use &lt;a href=&#34;https://chrome.google.com/webstore/detail/open-in-colab/iogfkhleblhcpcekbiedikdehleodpjo&#34;&gt;&#34;Open in Colab&#34; Chrome extension&lt;/a&gt; to do the same with a single click.&lt;/p&gt; &#xA;&lt;h2&gt;Running the noteboks locally&lt;/h2&gt; &#xA;&lt;p&gt;We assume you have already installed &lt;a href=&#34;https://github.com/google/jax#installation&#34;&gt;JAX&lt;/a&gt; and &lt;a href=&#34;https://www.tensorflow.org/install&#34;&gt;Tensorflow&lt;/a&gt; and &lt;a href=&#34;https://pytorch.org/&#34;&gt;Torch&lt;/a&gt;, since the details on how to do this depend on whether you have a CPU, GPU, etc.&lt;/p&gt; &#xA;&lt;p&gt;You can use any of the following options to install the other requirements.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Option 1&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r https://raw.githubusercontent.com/probml/pyprobml/master/requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Option 2&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Download &lt;a href=&#34;https://github.com/probml/pyprobml/raw/master/requirements.txt&#34;&gt;requirements.txt&lt;/a&gt; locally to your path and run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;GCP, TPUs, and all that&lt;/h2&gt; &#xA;&lt;p&gt;When you want more power or control than colab gives you, you should get a Google Cloud Platform (GCP) account, and get access to a TPU VM. You can then use this as a virtual desktop which you can access via ssh from inside VScode. We have created &lt;a href=&#34;https://github.com/probml/pyprobml/raw/master/tutorials/colab_gcp_tpu_tutorial.md&#34;&gt;a short tutorial on Colab, GCP and TPUs&lt;/a&gt; with more information.&lt;/p&gt; &#xA;&lt;h2&gt;How to contribute&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/probml/pyprobml/raw/master/CONTRIBUTING.md&#34;&gt;this guide&lt;/a&gt; for how to contribute code. Please follow &lt;a href=&#34;https://github.com/probml/pyprobml/raw/master/notebooks/README.md&#34;&gt;these guidelines&lt;/a&gt; to contribute new notebooks to the notebooks directory.&lt;/p&gt; &#xA;&lt;h2&gt;Metrics&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://starchart.cc/probml/pyprobml&#34;&gt;&lt;img src=&#34;https://starchart.cc/probml/pyprobml.svg?sanitize=true&#34; alt=&#34;Stargazers over time&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;GSOC&lt;/h2&gt; &#xA;&lt;p&gt;For a summary of some of the contributions to this codebase during Google Summer of Code (GSOC) 2021, see &lt;a href=&#34;https://probml.github.io/pml-book/gsoc2021.html&#34;&gt;this link&lt;/a&gt;. Stay tuned for GSOC 2022.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a id=&#34;acknowledgements&#34;&gt;&lt;/a&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;I would like to thank the following people for contributing to the code (list autogenerated from &lt;a href=&#34;https://raw.githubusercontent.com/probml/pyprobml/master/internal/contributors/contributors.py&#34;&gt;this script&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/Abdelrahman350.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/alenm10.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/always-newbie161.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/AnandShegde.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/andrewnc.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/animesh-007.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/AnkitaKumariJain14.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/ashishpapanai.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/Drishttii.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/Duane321.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/firatoncel.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/Garvit9000c.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/gerdm.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/jdf22.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/karalleyna.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/karm-patel.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/khanshehjad.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/kzymgch.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/mjsML.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/murphyk.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/nalzok.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/nappaillav.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/Neoanarika.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/Nirzu97.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/nitish1295.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/nouranali.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/patel-zeel.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/patrickmineault.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/raymondyeh07.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/rohit-khoiwal-30.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/shivaditya-meduri.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/shobro.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/susnato.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/thvasilo.png&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Abdelrahman350&#34;&gt;Abdelrahman350&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/alenm10&#34;&gt;alenm10&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/always-newbie161&#34;&gt;always-newbie161&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/AnandShegde&#34;&gt;AnandShegde&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/andrewnc&#34;&gt;andrewnc&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/animesh-007&#34;&gt;animesh-007&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/AnkitaKumariJain14&#34;&gt;AnkitaKumariJain14&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ashishpapanai&#34;&gt;ashishpapanai&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Drishttii&#34;&gt;Drishttii&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Duane321&#34;&gt;Duane321&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/firatoncel&#34;&gt;firatoncel&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Garvit9000c&#34;&gt;Garvit9000c&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/gerdm&#34;&gt;gerdm&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/jdf22&#34;&gt;jdf22&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/karalleyna&#34;&gt;karalleyna&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/karm-patel&#34;&gt;karm-patel&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/khanshehjad&#34;&gt;khanshehjad&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/kzymgch&#34;&gt;kzymgch&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/mjsML&#34;&gt;mjsML&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/murphyk&#34;&gt;murphyk&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/nalzok&#34;&gt;nalzok&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/nappaillav&#34;&gt;nappaillav&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Neoanarika&#34;&gt;Neoanarika&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Nirzu97&#34;&gt;Nirzu97&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/nitish1295&#34;&gt;nitish1295&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/nouranali&#34;&gt;nouranali&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/patel-zeel&#34;&gt;patel-zeel&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/patrickmineault&#34;&gt;patrickmineault&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/raymondyeh07&#34;&gt;raymondyeh07&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/rohit-khoiwal-30&#34;&gt;rohit-khoiwal-30&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/shivaditya-meduri&#34;&gt;shivaditya-meduri&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/shobro&#34;&gt;shobro&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/susnato&#34;&gt;susnato&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/thvasilo&#34;&gt;thvasilo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt;</summary>
  </entry>
  <entry>
    <title>pycaret/pycaret</title>
    <updated>2022-05-29T01:45:04Z</updated>
    <id>tag:github.com,2022-05-29:/pycaret/pycaret</id>
    <link href="https://github.com/pycaret/pycaret" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An open-source, low-code machine learning library in Python&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/pycaret/pycaret/master/docs/images/logo.png&#34; alt=&#34;drawing&#34; width=&#34;200&#34;&gt; &#xA; &lt;p&gt;&lt;strong&gt;An open-source, low-code machine learning library in Python&lt;/strong&gt; &lt;br&gt; &lt;span&gt;üöÄ&lt;/span&gt; &lt;strong&gt;Version 2.3.10 out now!&lt;/strong&gt; &lt;a href=&#34;https://github.com/pycaret/pycaret/releases&#34;&gt;Check out the release notes here&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.pycaret.org&#34;&gt;Official&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://pycaret.gitbook.io/&#34;&gt;Docs&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://pycaret.gitbook.io/docs/get-started/installation&#34;&gt;Install&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://pycaret.gitbook.io/docs/get-started/tutorials&#34;&gt;Tutorials&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://pycaret.gitbook.io/docs/learn-pycaret/faqs&#34;&gt;FAQs&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://pycaret.gitbook.io/docs/learn-pycaret/cheat-sheet&#34;&gt;Cheat sheet&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://github.com/pycaret/pycaret/discussions&#34;&gt;Discussions&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://pycaret.readthedocs.io/en/latest/contribute.html&#34;&gt;Contribute&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://github.com/pycaret/pycaret/tree/master/resources&#34;&gt;Resources&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://pycaret.gitbook.io/docs/learn-pycaret/official-blog&#34;&gt;Blog&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://www.linkedin.com/company/pycaret/&#34;&gt;LinkedIn&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://www.youtube.com/channel/UCxA1YTYJ9BEeo50lxyI_B3g&#34;&gt;YouTube&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://join.slack.com/t/pycaret/shared_invite/zt-row9phbm-BoJdEVPYnGf7_NxNBP307w&#34;&gt;Slack&lt;/a&gt; &lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://badge.fury.io/py/pycaret&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Python-3.6%20%7C%203.7%20%7C%203.8-blue&#34; alt=&#34;Python&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://github.com/pycaret/pycaret/workflows/pytest%20on%20push/badge.svg?sanitize=true&#34; alt=&#34;pytest on push&#34;&gt; &lt;a href=&#34;http://pip.pypa.io/en/stable/?badge=stable&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/pip/badge/?version=stable&#34; alt=&#34;Documentation Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/py/pycaret&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/pycaret.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://img.shields.io/pypi/l/ansicolortags.svg&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/l/ansicolortags.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;!-- [![Git count](http://hits.dwyl.com/pycaret/pycaret/pycaret.svg)](http://hits.dwyl.com/pycaret/pycaret/pycaret) --&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://join.slack.com/t/pycaret/shared_invite/zt-row9phbm-BoJdEVPYnGf7_NxNBP307w&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/slack-chat-green.svg?logo=slack&#34; alt=&#34;Slack&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pycaret/pycaret/master/docs/images/quick_start.gif&#34; alt=&#34;alt text&#34;&gt;&lt;/p&gt; &#xA; &lt;div align=&#34;left&#34;&gt; &#xA;  &lt;h2&gt;Welcome to PyCaret&lt;/h2&gt; &#xA;  &lt;p&gt;PyCaret is an open-source, low-code machine learning library in Python that automates machine learning workflows. It is an end-to-end machine learning and model management tool that speeds up the experiment cycle exponentially and makes you more productive.&lt;/p&gt; &#xA;  &lt;p&gt;In comparison with the other open-source machine learning libraries, PyCaret is an alternate low-code library that can be used to replace hundreds of lines of code with few lines only. This makes experiments exponentially fast and efficient. PyCaret is essentially a Python wrapper around several machine learning libraries and frameworks such as scikit-learn, XGBoost, LightGBM, CatBoost, spaCy, Optuna, Hyperopt, Ray, and few more.&lt;/p&gt; &#xA;  &lt;p&gt;The design and simplicity of PyCaret are inspired by the emerging role of citizen data scientists, a term first used by Gartner. Citizen Data Scientists are power users who can perform both simple and moderately sophisticated analytical tasks that would previously have required more technical expertise.&lt;/p&gt; &#xA;  &lt;table&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th&gt;Important Links&lt;/th&gt; &#xA;     &lt;th&gt;&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;&lt;span&gt;‚≠ê&lt;/span&gt; &lt;strong&gt;&lt;a href=&#34;https://pycaret.gitbook.io/docs/get-started/tutorials&#34;&gt;Tutorials&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;New to PyCaret? Checkout our official notebooks!&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;&lt;span&gt;üìã&lt;/span&gt; &lt;strong&gt;&lt;a href=&#34;https://github.com/pycaret/pycaret/tree/master/examples&#34;&gt;Example Notebooks&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;Example notebooks created by community.&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;&lt;span&gt;üìô&lt;/span&gt; &lt;strong&gt;&lt;a href=&#34;https://pycaret.gitbook.io/docs/learn-pycaret/official-blog&#34;&gt;Official Blog&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;Tutorials and articles by contributors.&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;&lt;span&gt;üìö&lt;/span&gt; &lt;strong&gt;&lt;a href=&#34;https://pycaret.gitbook.io&#34;&gt;Documentation&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;The detailed API docs of PyCaret&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;&lt;span&gt;üì∫&lt;/span&gt; &lt;strong&gt;&lt;a href=&#34;https://pycaret.gitbook.io/docs/learn-pycaret/videos&#34;&gt;Video Tutorials&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;Our video tutorial from various events.&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;‚úàÔ∏è &lt;strong&gt;&lt;a href=&#34;https://pycaret.gitbook.io/docs/learn-pycaret/cheat-sheet&#34;&gt;Cheat sheet&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;Cheat sheet for all functions across modules.&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;&lt;span&gt;üì¢&lt;/span&gt; &lt;strong&gt;&lt;a href=&#34;https://github.com/pycaret/pycaret/discussions&#34;&gt;Discussions&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;Have questions? Engage with community and contributors.&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;&lt;span&gt;üõ†&lt;/span&gt; &lt;strong&gt;&lt;a href=&#34;https://pycaret.gitbook.io/docs/get-started/release-notes&#34;&gt;Changelog&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;Changes and version history.&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;&lt;span&gt;üå≥&lt;/span&gt; &lt;strong&gt;&lt;a href=&#34;https://github.com/pycaret/pycaret/issues/1756&#34;&gt;Roadmap&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;PyCaret&#39;s software and community development plan.&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &#xA;  &lt;h2&gt;Installation&lt;/h2&gt; &#xA;  &lt;p&gt;PyCaret&#39;s default installation only installs hard dependencies as listed in the &lt;a href=&#34;https://raw.githubusercontent.com/pycaret/pycaret/master/requirements.txt&#34;&gt;requirements.txt&lt;/a&gt; file.&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pip install pycaret&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;p&gt;To install the full version:&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pip install pycaret[full]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;div align=&#34;center&#34;&gt; &#xA;   &lt;h2&gt;Supervised Workflow&lt;/h2&gt; &#xA;   &lt;table&gt; &#xA;    &lt;thead&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;Classification&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;Regression&lt;/th&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/thead&gt; &#xA;    &lt;tbody&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pycaret/pycaret/master/docs/images/pycaret_classification.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pycaret/pycaret/master/docs/images/pycaret_regression.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/tbody&gt; &#xA;   &lt;/table&gt; &#xA;   &lt;h2&gt;Unsupervised Workflow&lt;/h2&gt; &#xA;   &lt;table&gt; &#xA;    &lt;thead&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;Clustering&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;Anomaly Detection&lt;/th&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/thead&gt; &#xA;    &lt;tbody&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pycaret/pycaret/master/docs/images/pycaret_clustering.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pycaret/pycaret/master/docs/images/pycaret_anomaly.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/tbody&gt; &#xA;   &lt;/table&gt; &#xA;   &lt;div align=&#34;left&#34;&gt; &#xA;    &lt;h2&gt;‚ö° PyCaret Time Series Module (beta)&lt;/h2&gt; &#xA;    &lt;p&gt;PyCaret new time series module is now available in beta. Staying true to simplicity of PyCaret, it is consistent with our existing API and fully loaded with functionalities. Statistical testing, model training and selection (30+ algorithms), model analysis, automated hyperparameter tuning, experiment logging, deployment on cloud, and more. All of this with only few lines of code (just like the other modules of pycaret). If you would like to give it a try, checkout our official &lt;a href=&#34;https://nbviewer.org/github/pycaret/pycaret/blob/time_series_beta/time_series_101.ipynb&#34;&gt;quick start&lt;/a&gt; notebook.&lt;/p&gt; &#xA;    &lt;p&gt;&lt;span&gt;üìö&lt;/span&gt; &lt;a href=&#34;https://pycaret.readthedocs.io/en/time_series/api/time_series.html&#34;&gt;Time Series Docs&lt;/a&gt;&lt;/p&gt; &#xA;    &lt;p&gt;&lt;span&gt;‚ùì&lt;/span&gt; &lt;a href=&#34;https://github.com/pycaret/pycaret/discussions/categories/faqs?discussions_q=category%3AFAQs+label%3Atime_series&#34;&gt;Time Series FAQs&lt;/a&gt;&lt;/p&gt; &#xA;    &lt;p&gt;&lt;span&gt;üöÄ&lt;/span&gt; &lt;a href=&#34;https://github.com/pycaret/pycaret/issues/1648&#34;&gt;Features and Roadmap&lt;/a&gt;&lt;/p&gt; &#xA;    &lt;p&gt;The module is still in beta. We are adding new functionalities every day and doing weekly pip releases. Please ensure to create a separate python environment to avoid dependency conflicts with main pycaret. The final release of this module will be merged with the main pycaret in next major release.&lt;/p&gt; &#xA;    &lt;pre&gt;&lt;code&gt;pip install pycaret-ts-alpha&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;    &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pycaret/pycaret/master/docs/images/pycaret_ts_quickdemo.gif&#34; alt=&#34;alt text&#34;&gt;&lt;/p&gt; &#xA;    &lt;h2&gt;Who should use PyCaret?&lt;/h2&gt; &#xA;    &lt;p&gt;PyCaret is an open source library that anybody can use. In our view the ideal target audience of PyCaret is: &lt;br&gt;&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Experienced Data Scientists who want to increase productivity.&lt;/li&gt; &#xA;     &lt;li&gt;Citizen Data Scientists who prefer a low code machine learning solution.&lt;/li&gt; &#xA;     &lt;li&gt;Data Science Professionals who want to build rapid prototypes.&lt;/li&gt; &#xA;     &lt;li&gt;Data Science and Machine Learning students and enthusiasts.&lt;/li&gt; &#xA;    &lt;/ul&gt; &#xA;    &lt;h2&gt;PyCaret GPU support&lt;/h2&gt; &#xA;    &lt;p&gt;With PyCaret &amp;gt;= 2.2, you can train models on GPU and speed up your workflow by 10x. To train models on GPU simply pass &lt;code&gt;use_gpu = True&lt;/code&gt; in the setup function. There is no change in the use of the API, however, in some cases, additional libraries have to be installed as they are not installed with the default version or the full version. As of the latest release, the following models can be trained on GPU:&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Extreme Gradient Boosting (requires no further installation)&lt;/li&gt; &#xA;     &lt;li&gt;CatBoost (requires no further installation)&lt;/li&gt; &#xA;     &lt;li&gt;Light Gradient Boosting Machine requires &lt;a href=&#34;https://lightgbm.readthedocs.io/en/latest/GPU-Tutorial.html&#34;&gt;GPU installation&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Logistic Regression, Ridge Classifier, Random Forest, K Neighbors Classifier, K Neighbors Regressor, Support Vector Machine, Linear Regression, Ridge Regression, Lasso Regression requires &lt;a href=&#34;https://github.com/rapidsai/cuml&#34;&gt;cuML &amp;gt;= 0.15&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &#xA;    &lt;h2&gt;License&lt;/h2&gt; &#xA;    &lt;p&gt;PyCaret is completely free and open-source and licensed under the &lt;a href=&#34;https://github.com/pycaret/pycaret/raw/master/LICENSE&#34;&gt;MIT&lt;/a&gt; license.&lt;/p&gt; &#xA;    &lt;h2&gt;Contributors&lt;/h2&gt; &#xA;    &lt;a href=&#34;https://github.com/pycaret/pycaret/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contributors-img.web.app/image?repo=pycaret/pycaret&#34; width=&#34;500/&#34;&gt; &lt;/a&gt; &#xA;   &lt;/div&gt;&#xA;  &lt;/div&gt;&#xA; &lt;/div&gt;&#xA;&lt;/div&gt;</summary>
  </entry>
  <entry>
    <title>matheusfacure/python-causality-handbook</title>
    <updated>2022-05-29T01:45:04Z</updated>
    <id>tag:github.com,2022-05-29:/matheusfacure/python-causality-handbook</id>
    <link href="https://github.com/matheusfacure/python-causality-handbook" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Causal Inference for the Brave and True. A light-hearted yet rigorous approach to learning about impact estimation and sensitivity analysis.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Causal Inference for The Brave and True&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/matheusfacure/python-causality-handbook/master/causal-inference-for-the-brave-and-true/data/img/brave-and-true.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://zenodo.org/badge/latestdoi/255903310&#34;&gt;&lt;img src=&#34;https://zenodo.org/badge/255903310.svg?sanitize=true&#34; alt=&#34;DOI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A light-hearted yet rigorous approach to learning impact estimation and sensitivity analysis. Everything in Python and with as many memes as I could find.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://matheusfacure.github.io/python-causality-handbook/landing-page.html&#34;&gt;Check out the book here!&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to read the book in Chinese, @xieliaing was very kind to make a translation:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/xieliaing/CausalInferenceIntro&#34;&gt;Âõ†ÊûúÊé®Êñ≠Ôºö‰ªéÊ¶ÇÂøµÂà∞ÂÆûË∑µ&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to read the book in Spanish, @donelianc was very kind to make a translation: &lt;a href=&#34;https://github.com/donelianc/introduccion-inferencia-causal&#34;&gt;Inferencia Causal para los Valientes y Verdaderos&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Also, some really kind folks (@vietecon, @dinhtrang24 and @anhpham52) also translated this content into Vietnamese:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/vietecon/NhanQuaPython&#34;&gt;Nh√¢n qu·∫£ Python&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;I like to think of this entire series as a tribute to Joshua Angrist, Alberto Abadie and Christopher Walters for their amazing Econometrics class. Most of the ideas here are taken from their classes at the American Economic Association. Watching them is what is keeping me sane during this tough year of 2020.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.aeaweb.org/conference/cont-ed/2017-webcasts&#34;&gt;Cross-Section Econometrics&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.aeaweb.org/conference/cont-ed/2020-webcasts&#34;&gt;Mastering Mostly Harmless Econometrics&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;I&#39;ll also like to reference the amazing books from Angrist. They have shown me that Econometrics, or &#39;Metrics as they call it, is not only extremely useful but also profoundly fun.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.mostlyharmlesseconometrics.com/&#34;&gt;Mostly Harmless Econometrics&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.masteringmetrics.com/&#34;&gt;Mastering &#39;Metrics&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;My final reference is Miguel Hernan and Jamie Robins&#39; book. It has been my trustworthy companion in the most thorny causal questions I had to answer.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/&#34;&gt;Causal Inference Book&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to Support This Work&lt;/h2&gt; &#xA;&lt;p&gt;Causal Inference for the Brave and True is an open-source material on mostly econometrics and the statistics of science. It uses only free software, based in Python. Its goal is to be accessible, not only financially, but intellectual. I&#39;ve tried my best to keep the writing entertaining while maintaining the necessary scientific rigor.&lt;br&gt; Recently, the book has been translated into Vietnamese by some very nice folks from the London School of Economics. Although I was thrilled by it, the translation process also revealed the insufiencies of my english. For this reason, I&#39;m looking for funds to hire professional proofreading services and sort that problem once and for all. To help me with that, go to &lt;a href=&#34;https://www.patreon.com/causal_inference_for_the_brave_and_true&#34;&gt;https://www.patreon.com/causal_inference_for_the_brave_and_true&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>slundberg/shap</title>
    <updated>2022-05-29T01:45:04Z</updated>
    <id>tag:github.com,2022-05-29:/slundberg/shap</id>
    <link href="https://github.com/slundberg/shap" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A game theoretic approach to explain the output of any machine learning model.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/shap_header.svg?sanitize=true&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/slundberg/shap/actions/workflows/run_tests.yml/badge.svg?sanitize=true&#34; alt=&#34;example workflow&#34;&gt; &lt;a href=&#34;https://mybinder.org/v2/gh/slundberg/shap/master&#34;&gt;&lt;img src=&#34;https://mybinder.org/badge_logo.svg?sanitize=true&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://shap.readthedocs.io/en/latest/?badge=latest&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/shap/badge/?version=latest&#34; alt=&#34;Documentation Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SHAP (SHapley Additive exPlanations)&lt;/strong&gt; is a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions (see &lt;a href=&#34;https://raw.githubusercontent.com/slundberg/shap/master/#citations&#34;&gt;papers&lt;/a&gt; for details and citations).&lt;/p&gt; &#xA;&lt;!--**SHAP (SHapley Additive exPlanations)** is a unified approach to explain the output of any machine learning model. SHAP connects game theory with local explanations, uniting several previous methods [1-7] and representing the only possible consistent and locally accurate additive feature attribution method based on expectations (see our [papers](#citations) for details and citations).--&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;SHAP can be installed from either &lt;a href=&#34;https://pypi.org/project/shap&#34;&gt;PyPI&lt;/a&gt; or &lt;a href=&#34;https://anaconda.org/conda-forge/shap&#34;&gt;conda-forge&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&#xA;pip install shap&#xA;&lt;i&gt;or&lt;/i&gt;&#xA;conda install -c conda-forge shap&#xA;&lt;/pre&gt; &#xA;&lt;h2&gt;Tree ensemble example (XGBoost/LightGBM/CatBoost/scikit-learn/pyspark models)&lt;/h2&gt; &#xA;&lt;p&gt;While SHAP can explain the output of any machine learning model, we have developed a high-speed exact algorithm for tree ensemble methods (see our &lt;a href=&#34;https://rdcu.be/b0z70&#34;&gt;Nature MI paper&lt;/a&gt;). Fast C++ implementations are supported for &lt;em&gt;XGBoost&lt;/em&gt;, &lt;em&gt;LightGBM&lt;/em&gt;, &lt;em&gt;CatBoost&lt;/em&gt;, &lt;em&gt;scikit-learn&lt;/em&gt; and &lt;em&gt;pyspark&lt;/em&gt; tree models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import xgboost&#xA;import shap&#xA;&#xA;# train an XGBoost model&#xA;X, y = shap.datasets.boston()&#xA;model = xgboost.XGBRegressor().fit(X, y)&#xA;&#xA;# explain the model&#39;s predictions using SHAP&#xA;# (same syntax works for LightGBM, CatBoost, scikit-learn, transformers, Spark, etc.)&#xA;explainer = shap.Explainer(model)&#xA;shap_values = explainer(X)&#xA;&#xA;# visualize the first prediction&#39;s explanation&#xA;shap.plots.waterfall(shap_values[0])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;616&#34; src=&#34;https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_waterfall.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;The above explanation shows features each contributing to push the model output from the base value (the average model output over the training dataset we passed) to the model output. Features pushing the prediction higher are shown in red, those pushing the prediction lower are in blue. Another way to visualize the same explanation is to use a force plot (these are introduced in our &lt;a href=&#34;https://rdcu.be/baVbR&#34;&gt;Nature BME paper&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# visualize the first prediction&#39;s explanation with a force plot&#xA;shap.plots.force(shap_values[0])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;811&#34; src=&#34;https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_instance.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;If we take many force plot explanations such as the one shown above, rotate them 90 degrees, and then stack them horizontally, we can see explanations for an entire dataset (in the notebook this plot is interactive):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# visualize all the training set predictions&#xA;shap.plots.force(shap_values)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;811&#34; src=&#34;https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_dataset.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;To understand how a single feature effects the output of the model we can plot the SHAP value of that feature vs. the value of the feature for all the examples in a dataset. Since SHAP values represent a feature&#39;s responsibility for a change in the model output, the plot below represents the change in predicted house price as RM (the average number of rooms per house in an area) changes. Vertical dispersion at a single value of RM represents interaction effects with other features. To help reveal these interactions we can color by another feature. If we pass the whole explanation tensor to the &lt;code&gt;color&lt;/code&gt; argument the scatter plot will pick the best feature to color by. In this case it picks RAD (index of accessibility to radial highways) since that highlights that the average number of rooms per house has less impact on home price for areas with a high RAD value.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# create a dependence scatter plot to show the effect of a single feature across the whole dataset&#xA;shap.plots.scatter(shap_values[:,&#34;RM&#34;], color=shap_values)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;544&#34; src=&#34;https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_scatter.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;To get an overview of which features are most important for a model we can plot the SHAP values of every feature for every sample. The plot below sorts features by the sum of SHAP value magnitudes over all samples, and uses SHAP values to show the distribution of the impacts each feature has on the model output. The color represents the feature value (red high, blue low). This reveals for example that a high LSTAT (% lower status of the population) lowers the predicted home price.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# summarize the effects of all the features&#xA;shap.plots.beeswarm(shap_values)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;583&#34; src=&#34;https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_beeswarm.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;We can also just take the mean absolute value of the SHAP values for each feature to get a standard bar plot (produces stacked bars for multi-class outputs):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;shap.plots.bar(shap_values)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;570&#34; src=&#34;https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_global_bar.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Natural language example (transformers)&lt;/h2&gt; &#xA;&lt;p&gt;SHAP has specific support for natural language models like those in the Hugging Face transformers library. By adding coalitional rules to traditional Shapley values we can form games that explain large modern NLP model using very few function evaluations. Using this functionality is as simple as passing a supported transformers pipeline to SHAP:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import transformers&#xA;import shap&#xA;&#xA;# load a transformers pipeline model&#xA;model = transformers.pipeline(&#39;sentiment-analysis&#39;, return_all_scores=True)&#xA;&#xA;# explain the model on two sample inputs&#xA;explainer = shap.Explainer(model) &#xA;shap_values = explainer([&#34;What a great movie! ...if you have no taste.&#34;])&#xA;&#xA;# visualize the first prediction&#39;s explanation for the POSITIVE output class&#xA;shap.plots.text(shap_values[0, :, &#34;POSITIVE&#34;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;811&#34; src=&#34;https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/sentiment_analysis_plot.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Deep learning example with DeepExplainer (TensorFlow/Keras models)&lt;/h2&gt; &#xA;&lt;p&gt;Deep SHAP is a high-speed approximation algorithm for SHAP values in deep learning models that builds on a connection with &lt;a href=&#34;https://arxiv.org/abs/1704.02685&#34;&gt;DeepLIFT&lt;/a&gt; described in the SHAP NIPS paper. The implementation here differs from the original DeepLIFT by using a distribution of background samples instead of a single reference value, and using Shapley equations to linearize components such as max, softmax, products, divisions, etc. Note that some of these enhancements have also been since integrated into DeepLIFT. TensorFlow models and Keras models using the TensorFlow backend are supported (there is also preliminary support for PyTorch):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# ...include code from https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py&#xA;&#xA;import shap&#xA;import numpy as np&#xA;&#xA;# select a set of background examples to take an expectation over&#xA;background = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]&#xA;&#xA;# explain predictions of the model on four images&#xA;e = shap.DeepExplainer(model, background)&#xA;# ...or pass tensors directly&#xA;# e = shap.DeepExplainer((model.layers[0].input, model.layers[-1].output), background)&#xA;shap_values = e.shap_values(x_test[1:5])&#xA;&#xA;# plot the feature attributions&#xA;shap.image_plot(shap_values, -x_test[1:5])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;820&#34; src=&#34;https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/mnist_image_plot.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;The plot above explains ten outputs (digits 0-9) for four different images. Red pixels increase the model&#39;s output while blue pixels decrease the output. The input images are shown on the left, and as nearly transparent grayscale backings behind each of the explanations. The sum of the SHAP values equals the difference between the expected model output (averaged over the background dataset) and the current model output. Note that for the &#39;zero&#39; image the blank middle is important, while for the &#39;four&#39; image the lack of a connection on top makes it a four instead of a nine.&lt;/p&gt; &#xA;&lt;h2&gt;Deep learning example with GradientExplainer (TensorFlow/Keras/PyTorch models)&lt;/h2&gt; &#xA;&lt;p&gt;Expected gradients combines ideas from &lt;a href=&#34;https://arxiv.org/abs/1703.01365&#34;&gt;Integrated Gradients&lt;/a&gt;, SHAP, and &lt;a href=&#34;https://arxiv.org/abs/1706.03825&#34;&gt;SmoothGrad&lt;/a&gt; into a single expected value equation. This allows an entire dataset to be used as the background distribution (as opposed to a single reference value) and allows local smoothing. If we approximate the model with a linear function between each background data sample and the current input to be explained, and we assume the input features are independent then expected gradients will compute approximate SHAP values. In the example below we have explained how the 7th intermediate layer of the VGG16 ImageNet model impacts the output probabilities.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from keras.applications.vgg16 import VGG16&#xA;from keras.applications.vgg16 import preprocess_input&#xA;import keras.backend as K&#xA;import numpy as np&#xA;import json&#xA;import shap&#xA;&#xA;# load pre-trained model and choose two images to explain&#xA;model = VGG16(weights=&#39;imagenet&#39;, include_top=True)&#xA;X,y = shap.datasets.imagenet50()&#xA;to_explain = X[[39,41]]&#xA;&#xA;# load the ImageNet class names&#xA;url = &#34;https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json&#34;&#xA;fname = shap.datasets.cache(url)&#xA;with open(fname) as f:&#xA;    class_names = json.load(f)&#xA;&#xA;# explain how the input to the 7th layer of the model explains the top two classes&#xA;def map2layer(x, layer):&#xA;    feed_dict = dict(zip([model.layers[0].input], [preprocess_input(x.copy())]))&#xA;    return K.get_session().run(model.layers[layer].input, feed_dict)&#xA;e = shap.GradientExplainer(&#xA;    (model.layers[7].input, model.layers[-1].output),&#xA;    map2layer(X, 7),&#xA;    local_smoothing=0 # std dev of smoothing noise&#xA;)&#xA;shap_values,indexes = e.shap_values(map2layer(to_explain, 7), ranked_outputs=2)&#xA;&#xA;# get the names for the classes&#xA;index_names = np.vectorize(lambda x: class_names[str(x)][1])(indexes)&#xA;&#xA;# plot the explanations&#xA;shap.image_plot(shap_values, to_explain, index_names)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;500&#34; src=&#34;https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/gradient_imagenet_plot.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Predictions for two input images are explained in the plot above. Red pixels represent positive SHAP values that increase the probability of the class, while blue pixels represent negative SHAP values the reduce the probability of the class. By using &lt;code&gt;ranked_outputs=2&lt;/code&gt; we explain only the two most likely classes for each input (this spares us from explaining all 1,000 classes).&lt;/p&gt; &#xA;&lt;h2&gt;Model agnostic example with KernelExplainer (explains any function)&lt;/h2&gt; &#xA;&lt;p&gt;Kernel SHAP uses a specially-weighted local linear regression to estimate SHAP values for any model. Below is a simple example for explaining a multi-class SVM on the classic iris dataset.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import sklearn&#xA;import shap&#xA;from sklearn.model_selection import train_test_split&#xA;&#xA;# print the JS visualization code to the notebook&#xA;shap.initjs()&#xA;&#xA;# train a SVM classifier&#xA;X_train,X_test,Y_train,Y_test = train_test_split(*shap.datasets.iris(), test_size=0.2, random_state=0)&#xA;svm = sklearn.svm.SVC(kernel=&#39;rbf&#39;, probability=True)&#xA;svm.fit(X_train, Y_train)&#xA;&#xA;# use Kernel SHAP to explain test set predictions&#xA;explainer = shap.KernelExplainer(svm.predict_proba, X_train, link=&#34;logit&#34;)&#xA;shap_values = explainer.shap_values(X_test, nsamples=100)&#xA;&#xA;# plot the SHAP values for the Setosa output of the first instance&#xA;shap.force_plot(explainer.expected_value[0], shap_values[0][0,:], X_test.iloc[0,:], link=&#34;logit&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;810&#34; src=&#34;https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/iris_instance.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;The above explanation shows four features each contributing to push the model output from the base value (the average model output over the training dataset we passed) towards zero. If there were any features pushing the class label higher they would be shown in red.&lt;/p&gt; &#xA;&lt;p&gt;If we take many explanations such as the one shown above, rotate them 90 degrees, and then stack them horizontally, we can see explanations for an entire dataset. This is exactly what we do below for all the examples in the iris test set:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# plot the SHAP values for the Setosa output of all instances&#xA;shap.force_plot(explainer.expected_value[0], shap_values[0], X_test, link=&#34;logit&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;813&#34; src=&#34;https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/iris_dataset.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;SHAP Interaction Values&lt;/h2&gt; &#xA;&lt;p&gt;SHAP interaction values are a generalization of SHAP values to higher order interactions. Fast exact computation of pairwise interactions are implemented for tree models with &lt;code&gt;shap.TreeExplainer(model).shap_interaction_values(X)&lt;/code&gt;. This returns a matrix for every prediction, where the main effects are on the diagonal and the interaction effects are off-diagonal. These values often reveal interesting hidden relationships, such as how the increased risk of death peaks for men at age 60 (see the NHANES notebook for details):&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;483&#34; src=&#34;https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/nhanes_age_sex_interaction.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Sample notebooks&lt;/h2&gt; &#xA;&lt;p&gt;The notebooks below demonstrate different use cases for SHAP. Look inside the notebooks directory of the repository if you want to try playing with the original notebooks yourself.&lt;/p&gt; &#xA;&lt;h3&gt;TreeExplainer&lt;/h3&gt; &#xA;&lt;p&gt;An implementation of Tree SHAP, a fast and exact algorithm to compute SHAP values for trees and ensembles of trees.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://slundberg.github.io/shap/notebooks/NHANES%20I%20Survival%20Model.html&#34;&gt;&lt;strong&gt;NHANES survival model with XGBoost and SHAP interaction values&lt;/strong&gt;&lt;/a&gt; - Using mortality data from 20 years of followup this notebook demonstrates how to use XGBoost and &lt;code&gt;shap&lt;/code&gt; to uncover complex risk factor relationships.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://slundberg.github.io/shap/notebooks/tree_explainer/Census%20income%20classification%20with%20LightGBM.html&#34;&gt;&lt;strong&gt;Census income classification with LightGBM&lt;/strong&gt;&lt;/a&gt; - Using the standard adult census income dataset, this notebook trains a gradient boosting tree model with LightGBM and then explains predictions using &lt;code&gt;shap&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://slundberg.github.io/shap/notebooks/League%20of%20Legends%20Win%20Prediction%20with%20XGBoost.html&#34;&gt;&lt;strong&gt;League of Legends Win Prediction with XGBoost&lt;/strong&gt;&lt;/a&gt; - Using a Kaggle dataset of 180,000 ranked matches from League of Legends we train and explain a gradient boosting tree model with XGBoost to predict if a player will win their match.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;DeepExplainer&lt;/h3&gt; &#xA;&lt;p&gt;An implementation of Deep SHAP, a faster (but only approximate) algorithm to compute SHAP values for deep learning models that is based on connections between SHAP and the DeepLIFT algorithm.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://slundberg.github.io/shap/notebooks/deep_explainer/Front%20Page%20DeepExplainer%20MNIST%20Example.html&#34;&gt;&lt;strong&gt;MNIST Digit classification with Keras&lt;/strong&gt;&lt;/a&gt; - Using the MNIST handwriting recognition dataset, this notebook trains a neural network with Keras and then explains predictions using &lt;code&gt;shap&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://slundberg.github.io/shap/notebooks/deep_explainer/Keras%20LSTM%20for%20IMDB%20Sentiment%20Classification.html&#34;&gt;&lt;strong&gt;Keras LSTM for IMDB Sentiment Classification&lt;/strong&gt;&lt;/a&gt; - This notebook trains an LSTM with Keras on the IMDB text sentiment analysis dataset and then explains predictions using &lt;code&gt;shap&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;GradientExplainer&lt;/h3&gt; &#xA;&lt;p&gt;An implementation of expected gradients to approximate SHAP values for deep learning models. It is based on connections between SHAP and the Integrated Gradients algorithm. GradientExplainer is slower than DeepExplainer and makes different approximation assumptions.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://slundberg.github.io/shap/notebooks/gradient_explainer/Explain%20an%20Intermediate%20Layer%20of%20VGG16%20on%20ImageNet.html&#34;&gt;&lt;strong&gt;Explain an Intermediate Layer of VGG16 on ImageNet&lt;/strong&gt;&lt;/a&gt; - This notebook demonstrates how to explain the output of a pre-trained VGG16 ImageNet model using an internal convolutional layer.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;LinearExplainer&lt;/h3&gt; &#xA;&lt;p&gt;For a linear model with independent features we can analytically compute the exact SHAP values. We can also account for feature correlation if we are willing to estimate the feature covariance matrix. LinearExplainer supports both of these options.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://slundberg.github.io/shap/notebooks/linear_explainer/Sentiment%20Analysis%20with%20Logistic%20Regression.html&#34;&gt;&lt;strong&gt;Sentiment Analysis with Logistic Regression&lt;/strong&gt;&lt;/a&gt; - This notebook demonstrates how to explain a linear logistic regression sentiment analysis model.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;KernelExplainer&lt;/h3&gt; &#xA;&lt;p&gt;An implementation of Kernel SHAP, a model agnostic method to estimate SHAP values for any model. Because it makes no assumptions about the model type, KernelExplainer is slower than the other model type specific algorithms.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://slundberg.github.io/shap/notebooks/Census%20income%20classification%20with%20scikit-learn.html&#34;&gt;&lt;strong&gt;Census income classification with scikit-learn&lt;/strong&gt;&lt;/a&gt; - Using the standard adult census income dataset, this notebook trains a k-nearest neighbors classifier using scikit-learn and then explains predictions using &lt;code&gt;shap&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://slundberg.github.io/shap/notebooks/ImageNet%20VGG16%20Model%20with%20Keras.html&#34;&gt;&lt;strong&gt;ImageNet VGG16 Model with Keras&lt;/strong&gt;&lt;/a&gt; - Explain the classic VGG16 convolutional nerual network&#39;s predictions for an image. This works by applying the model agnostic Kernel SHAP method to a super-pixel segmented image.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://slundberg.github.io/shap/notebooks/Iris%20classification%20with%20scikit-learn.html&#34;&gt;&lt;strong&gt;Iris classification&lt;/strong&gt;&lt;/a&gt; - A basic demonstration using the popular iris species dataset. It explains predictions from six different models in scikit-learn using &lt;code&gt;shap&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Documentation notebooks&lt;/h2&gt; &#xA;&lt;p&gt;These notebooks comprehensively demonstrate how to use specific functions and objects.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://slundberg.github.io/shap/notebooks/plots/decision_plot.html&#34;&gt;&lt;code&gt;shap.decision_plot&lt;/code&gt; and &lt;code&gt;shap.multioutput_decision_plot&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://slundberg.github.io/shap/notebooks/plots/dependence_plot.html&#34;&gt;&lt;code&gt;shap.dependence_plot&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Methods Unified by SHAP&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;em&gt;LIME:&lt;/em&gt; Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. &#34;Why should i trust you?: Explaining the predictions of any classifier.&#34; Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2016.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;em&gt;Shapley sampling values:&lt;/em&gt; Strumbelj, Erik, and Igor Kononenko. &#34;Explaining prediction models and individual predictions with feature contributions.&#34; Knowledge and information systems 41.3 (2014): 647-665.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;em&gt;DeepLIFT:&lt;/em&gt; Shrikumar, Avanti, Peyton Greenside, and Anshul Kundaje. &#34;Learning important features through propagating activation differences.&#34; arXiv preprint arXiv:1704.02685 (2017).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;em&gt;QII:&lt;/em&gt; Datta, Anupam, Shayak Sen, and Yair Zick. &#34;Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems.&#34; Security and Privacy (SP), 2016 IEEE Symposium on. IEEE, 2016.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;em&gt;Layer-wise relevance propagation:&lt;/em&gt; Bach, Sebastian, et al. &#34;On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation.&#34; PloS one 10.7 (2015): e0130140.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;em&gt;Shapley regression values:&lt;/em&gt; Lipovetsky, Stan, and Michael Conklin. &#34;Analysis of regression in game theory approach.&#34; Applied Stochastic Models in Business and Industry 17.4 (2001): 319-330.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;em&gt;Tree interpreter:&lt;/em&gt; Saabas, Ando. Interpreting random forests. &lt;a href=&#34;http://blog.datadive.net/interpreting-random-forests/&#34;&gt;http://blog.datadive.net/interpreting-random-forests/&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Citations&lt;/h2&gt; &#xA;&lt;p&gt;The algorithms and visualizations used in this package came primarily out of research in &lt;a href=&#34;https://suinlee.cs.washington.edu&#34;&gt;Su-In Lee&#39;s lab&lt;/a&gt; at the University of Washington, and Microsoft Research. If you use SHAP in your research we would appreciate a citation to the appropriate paper(s):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For general use of SHAP you can read/cite our &lt;a href=&#34;http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions&#34;&gt;NeurIPS paper&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/slundberg/shap/master/docs/references/shap_nips.bib&#34;&gt;bibtex&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;For TreeExplainer you can read/cite our &lt;a href=&#34;https://www.nature.com/articles/s42256-019-0138-9&#34;&gt;Nature Machine Intelligence paper&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/slundberg/shap/master/docs/references/tree_explainer.bib&#34;&gt;bibtex&lt;/a&gt;; &lt;a href=&#34;https://rdcu.be/b0z70&#34;&gt;free access&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;For GPUTreeExplainer you can read/cite &lt;a href=&#34;https://arxiv.org/abs/2010.13972&#34;&gt;this article&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For &lt;code&gt;force_plot&lt;/code&gt; visualizations and medical applications you can read/cite our &lt;a href=&#34;https://www.nature.com/articles/s41551-018-0304-0&#34;&gt;Nature Biomedical Engineering paper&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/slundberg/shap/master/docs/references/nature_bme.bib&#34;&gt;bibtex&lt;/a&gt;; &lt;a href=&#34;https://rdcu.be/baVbR&#34;&gt;free access&lt;/a&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img height=&#34;1&#34; width=&#34;1&#34; style=&#34;display:none&#34; src=&#34;https://www.facebook.com/tr?id=189147091855991&amp;amp;ev=PageView&amp;amp;noscript=1&#34;&gt;</summary>
  </entry>
  <entry>
    <title>openai/CLIP</title>
    <updated>2022-05-29T01:45:04Z</updated>
    <id>tag:github.com,2022-05-29:/openai/CLIP</id>
    <link href="https://github.com/openai/CLIP" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Contrastive Language-Image Pretraining&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CLIP&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://openai.com/blog/clip/&#34;&gt;[Blog]&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2103.00020&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/openai/CLIP/main/model-card.md&#34;&gt;[Model Card]&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/openai/clip/blob/master/notebooks/Interacting_with_CLIP.ipynb&#34;&gt;[Colab]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;CLIP (Contrastive Language-Image Pre-Training) is a neural network trained on a variety of (image, text) pairs. It can be instructed in natural language to predict the most relevant text snippet, given an image, without directly optimizing for the task, similarly to the zero-shot capabilities of GPT-2 and 3. We found CLIP matches the performance of the original ResNet50 on ImageNet ‚Äúzero-shot‚Äù without using any of the original 1.28M labeled examples, overcoming several major challenges in computer vision.&lt;/p&gt; &#xA;&lt;h2&gt;Approach&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/openai/CLIP/main/CLIP.png&#34; alt=&#34;CLIP&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;First, &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;install PyTorch 1.7.1&lt;/a&gt; (or later) and torchvision, as well as small additional dependencies, and then install this repo as a Python package. On a CUDA GPU machine, the following will do the trick:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ conda install --yes -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.0&#xA;$ pip install ftfy regex tqdm&#xA;$ pip install git+https://github.com/openai/CLIP.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Replace &lt;code&gt;cudatoolkit=11.0&lt;/code&gt; above with the appropriate CUDA version on your machine or &lt;code&gt;cpuonly&lt;/code&gt; when installing on a machine without a GPU.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;import clip&#xA;from PIL import Image&#xA;&#xA;device = &#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34;&#xA;model, preprocess = clip.load(&#34;ViT-B/32&#34;, device=device)&#xA;&#xA;image = preprocess(Image.open(&#34;CLIP.png&#34;)).unsqueeze(0).to(device)&#xA;text = clip.tokenize([&#34;a diagram&#34;, &#34;a dog&#34;, &#34;a cat&#34;]).to(device)&#xA;&#xA;with torch.no_grad():&#xA;    image_features = model.encode_image(image)&#xA;    text_features = model.encode_text(text)&#xA;    &#xA;    logits_per_image, logits_per_text = model(image, text)&#xA;    probs = logits_per_image.softmax(dim=-1).cpu().numpy()&#xA;&#xA;print(&#34;Label probs:&#34;, probs)  # prints: [[0.9927937  0.00421068 0.00299572]]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;API&lt;/h2&gt; &#xA;&lt;p&gt;The CLIP module &lt;code&gt;clip&lt;/code&gt; provides the following methods:&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;clip.available_models()&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Returns the names of the available CLIP models.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;clip.load(name, device=..., jit=False)&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Returns the model and the TorchVision transform needed by the model, specified by the model name returned by &lt;code&gt;clip.available_models()&lt;/code&gt;. It will download the model as necessary. The &lt;code&gt;name&lt;/code&gt; argument can also be a path to a local checkpoint.&lt;/p&gt; &#xA;&lt;p&gt;The device to run the model can be optionally specified, and the default is to use the first CUDA device if there is any, otherwise the CPU. When &lt;code&gt;jit&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;, a non-JIT version of the model will be loaded.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;clip.tokenize(text: Union[str, List[str]], context_length=77)&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Returns a LongTensor containing tokenized sequences of given text input(s). This can be used as the input to the model&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;The model returned by &lt;code&gt;clip.load()&lt;/code&gt; supports the following methods:&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;model.encode_image(image: Tensor)&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Given a batch of images, returns the image features encoded by the vision portion of the CLIP model.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;model.encode_text(text: Tensor)&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Given a batch of text tokens, returns the text features encoded by the language portion of the CLIP model.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;model(image: Tensor, text: Tensor)&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Given a batch of images and a batch of text tokens, returns two Tensors, containing the logit scores corresponding to each image and text input. The values are cosine similarities between the corresponding image and text features, times 100.&lt;/p&gt; &#xA;&lt;h2&gt;More Examples&lt;/h2&gt; &#xA;&lt;h3&gt;Zero-Shot Prediction&lt;/h3&gt; &#xA;&lt;p&gt;The code below performs zero-shot prediction using CLIP, as shown in Appendix B in the paper. This example takes an image from the &lt;a href=&#34;https://www.cs.toronto.edu/~kriz/cifar.html&#34;&gt;CIFAR-100 dataset&lt;/a&gt;, and predicts the most likely labels among the 100 textual labels from the dataset.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os&#xA;import clip&#xA;import torch&#xA;from torchvision.datasets import CIFAR100&#xA;&#xA;# Load the model&#xA;device = &#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34;&#xA;model, preprocess = clip.load(&#39;ViT-B/32&#39;, device)&#xA;&#xA;# Download the dataset&#xA;cifar100 = CIFAR100(root=os.path.expanduser(&#34;~/.cache&#34;), download=True, train=False)&#xA;&#xA;# Prepare the inputs&#xA;image, class_id = cifar100[3637]&#xA;image_input = preprocess(image).unsqueeze(0).to(device)&#xA;text_inputs = torch.cat([clip.tokenize(f&#34;a photo of a {c}&#34;) for c in cifar100.classes]).to(device)&#xA;&#xA;# Calculate features&#xA;with torch.no_grad():&#xA;    image_features = model.encode_image(image_input)&#xA;    text_features = model.encode_text(text_inputs)&#xA;&#xA;# Pick the top 5 most similar labels for the image&#xA;image_features /= image_features.norm(dim=-1, keepdim=True)&#xA;text_features /= text_features.norm(dim=-1, keepdim=True)&#xA;similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)&#xA;values, indices = similarity[0].topk(5)&#xA;&#xA;# Print the result&#xA;print(&#34;\nTop predictions:\n&#34;)&#xA;for value, index in zip(values, indices):&#xA;    print(f&#34;{cifar100.classes[index]:&amp;gt;16s}: {100 * value.item():.2f}%&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The output will look like the following (the exact numbers may be slightly different depending on the compute device):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Top predictions:&#xA;&#xA;           snake: 65.31%&#xA;          turtle: 12.29%&#xA;    sweet_pepper: 3.83%&#xA;          lizard: 1.88%&#xA;       crocodile: 1.75%&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that this example uses the &lt;code&gt;encode_image()&lt;/code&gt; and &lt;code&gt;encode_text()&lt;/code&gt; methods that return the encoded features of given inputs.&lt;/p&gt; &#xA;&lt;h3&gt;Linear-probe evaluation&lt;/h3&gt; &#xA;&lt;p&gt;The example below uses &lt;a href=&#34;https://scikit-learn.org/&#34;&gt;scikit-learn&lt;/a&gt; to perform logistic regression on image features.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os&#xA;import clip&#xA;import torch&#xA;&#xA;import numpy as np&#xA;from sklearn.linear_model import LogisticRegression&#xA;from torch.utils.data import DataLoader&#xA;from torchvision.datasets import CIFAR100&#xA;from tqdm import tqdm&#xA;&#xA;# Load the model&#xA;device = &#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34;&#xA;model, preprocess = clip.load(&#39;ViT-B/32&#39;, device)&#xA;&#xA;# Load the dataset&#xA;root = os.path.expanduser(&#34;~/.cache&#34;)&#xA;train = CIFAR100(root, download=True, train=True, transform=preprocess)&#xA;test = CIFAR100(root, download=True, train=False, transform=preprocess)&#xA;&#xA;&#xA;def get_features(dataset):&#xA;    all_features = []&#xA;    all_labels = []&#xA;    &#xA;    with torch.no_grad():&#xA;        for images, labels in tqdm(DataLoader(dataset, batch_size=100)):&#xA;            features = model.encode_image(images.to(device))&#xA;&#xA;            all_features.append(features)&#xA;            all_labels.append(labels)&#xA;&#xA;    return torch.cat(all_features).cpu().numpy(), torch.cat(all_labels).cpu().numpy()&#xA;&#xA;# Calculate the image features&#xA;train_features, train_labels = get_features(train)&#xA;test_features, test_labels = get_features(test)&#xA;&#xA;# Perform logistic regression&#xA;classifier = LogisticRegression(random_state=0, C=0.316, max_iter=1000, verbose=1)&#xA;classifier.fit(train_features, train_labels)&#xA;&#xA;# Evaluate using the logistic regression classifier&#xA;predictions = classifier.predict(test_features)&#xA;accuracy = np.mean((test_labels == predictions).astype(np.float)) * 100.&#xA;print(f&#34;Accuracy = {accuracy:.3f}&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that the &lt;code&gt;C&lt;/code&gt; value should be determined via a hyperparameter sweep using a validation split.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/EconML</title>
    <updated>2022-05-29T01:45:04Z</updated>
    <id>tag:github.com,2022-05-29:/microsoft/EconML</id>
    <link href="https://github.com/microsoft/EconML" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ALICE (Automated Learning and Intelligence for Causation and Economics) is a Microsoft Research project aimed at applying Artificial Intelligence concepts to economic decision making. One of its goals is to build a toolkit that combines state-of-the-art machine learning techniques with econometrics in order to bring automation to complex causal ‚Ä¶&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://dev.azure.com/ms/EconML/_build/latest?definitionId=49&amp;amp;branchName=main&#34;&gt;&lt;img src=&#34;https://dev.azure.com/ms/EconML/_apis/build/status/Microsoft.EconML?branchName=main&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/econml/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/econml.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/econml/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/wheel/econml.svg?sanitize=true&#34; alt=&#34;PyPI wheel&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/econml/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/econml.svg?sanitize=true&#34; alt=&#34;Supported Python versions&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/doc/econml-logo-icon.png&#34; width=&#34;80px&#34; align=&#34;left&#34; style=&#34;margin-right: 10px;&#34;&gt; EconML: A Python Package for ML-Based Heterogeneous Treatment Effects Estimation&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;EconML&lt;/strong&gt; is a Python package for estimating heterogeneous treatment effects from observational data via machine learning. This package was designed and built as part of the &lt;a href=&#34;https://www.microsoft.com/en-us/research/project/alice/&#34;&gt;ALICE project&lt;/a&gt; at Microsoft Research with the goal to combine state-of-the-art machine learning techniques with econometrics to bring automation to complex causal inference problems. The promise of EconML:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Implement recent techniques in the literature at the intersection of econometrics and machine learning&lt;/li&gt; &#xA; &lt;li&gt;Maintain flexibility in modeling the effect heterogeneity (via techniques such as random forests, boosting, lasso and neural nets), while preserving the causal interpretation of the learned model and often offering valid confidence intervals&lt;/li&gt; &#xA; &lt;li&gt;Use a unified API&lt;/li&gt; &#xA; &lt;li&gt;Build on standard Python packages for Machine Learning and Data Analysis&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;One of the biggest promises of machine learning is to automate decision making in a multitude of domains. At the core of many data-driven personalized decision scenarios is the estimation of heterogeneous treatment effects: what is the causal effect of an intervention on an outcome of interest for a sample with a particular set of features? In a nutshell, this toolkit is designed to measure the causal effect of some treatment variable(s) &lt;code&gt;T&lt;/code&gt; on an outcome variable &lt;code&gt;Y&lt;/code&gt;, controlling for a set of features &lt;code&gt;X, W&lt;/code&gt; and how does that effect vary as a function of &lt;code&gt;X&lt;/code&gt;. The methods implemented are applicable even with observational (non-experimental or historical) datasets. For the estimation results to have a causal interpretation, some methods assume no unobserved confounders (i.e. there is no unobserved variable not included in &lt;code&gt;X, W&lt;/code&gt; that simultaneously has an effect on both &lt;code&gt;T&lt;/code&gt; and &lt;code&gt;Y&lt;/code&gt;), while others assume access to an instrument &lt;code&gt;Z&lt;/code&gt; (i.e. an observed variable &lt;code&gt;Z&lt;/code&gt; that has an effect on the treatment &lt;code&gt;T&lt;/code&gt; but no direct effect on the outcome &lt;code&gt;Y&lt;/code&gt;). Most methods provide confidence intervals and inference results.&lt;/p&gt; &#xA;&lt;p&gt;For detailed information about the package, consult the documentation at &lt;a href=&#34;https://econml.azurewebsites.net/&#34;&gt;https://econml.azurewebsites.net/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For information on use cases and background material on causal inference and heterogeneous treatment effects see our webpage at &lt;a href=&#34;https://www.microsoft.com/en-us/research/project/econml/&#34;&gt;https://www.microsoft.com/en-us/research/project/econml/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;strong&gt;&lt;em&gt;Table of Contents&lt;/em&gt;&lt;/strong&gt;&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/#news&#34;&gt;News&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/#getting-started&#34;&gt;Getting Started&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/#usage-examples&#34;&gt;Usage Examples&lt;/a&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/#estimation-methods&#34;&gt;Estimation Methods&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/#interpretability&#34;&gt;Interpretability&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/#causal-model-selection-and-cross-validation&#34;&gt;Causal Model Selection and Cross-Validation&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/#inference&#34;&gt;Inference&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/#policy-learning&#34;&gt;Policy Learning&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/#for-developers&#34;&gt;For Developers&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/#running-the-tests&#34;&gt;Running the tests&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/#generating-the-documentation&#34;&gt;Generating the documentation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/#blogs-and-publications&#34;&gt;Blogs and Publications&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/#contributing-and-feedback&#34;&gt;Contributing and Feedback&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h1&gt;News&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;March 11, 2022:&lt;/strong&gt; Call for Content&lt;/p&gt; &#xA;&lt;p&gt;Hello everyone, Microsoft will be hosting a workshop to explore current and future applications for our EconML and DoWhy packages on Tuesday, May 3, 2022. So far, our goal has been to make cutting-edge methods for causal treatment effect estimation as accessible as machine learning models for prediction or classification. We‚Äôre charting the course for future development of EconML and need your help.&lt;/p&gt; &#xA;&lt;p&gt;What more would you like to see in the library? New kinds of tasks, better functionality for the core tasks? Let us know! We are also looking for stories of problems you have solved using DoWhy and/or EconML to highlight in the workshop. If you have one, please reach out to &lt;a href=&#34;mailto:econml@microsoft.com&#34;&gt;econml@microsoft.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;January 31, 2022:&lt;/strong&gt; Release v0.13.0, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.13.0&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Previous releases&lt;/summary&gt; &#xA; &lt;p&gt;&lt;strong&gt;August 13, 2021:&lt;/strong&gt; Release v0.12.0, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.12.0&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;August 5, 2021:&lt;/strong&gt; Release v0.12.0b6, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.12.0b6&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;August 3, 2021:&lt;/strong&gt; Release v0.12.0b5, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.12.0b5&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;July 9, 2021:&lt;/strong&gt; Release v0.12.0b4, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.12.0b4&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;June 25, 2021:&lt;/strong&gt; Release v0.12.0b3, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.12.0b3&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;June 18, 2021:&lt;/strong&gt; Release v0.12.0b2, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.12.0b2&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;June 7, 2021:&lt;/strong&gt; Release v0.12.0b1, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.12.0b1&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;May 18, 2021:&lt;/strong&gt; Release v0.11.1, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.11.1&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;May 8, 2021:&lt;/strong&gt; Release v0.11.0, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.11.0&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;March 22, 2021:&lt;/strong&gt; Release v0.10.0, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.10.0&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;March 11, 2021:&lt;/strong&gt; Release v0.9.2, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.9.2&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;March 3, 2021:&lt;/strong&gt; Release v0.9.1, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.9.1&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;February 20, 2021:&lt;/strong&gt; Release v0.9.0, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.9.0&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;January 20, 2021:&lt;/strong&gt; Release v0.9.0b1, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.9.0b1&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;November 20, 2020:&lt;/strong&gt; Release v0.8.1, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.8.1&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;November 18, 2020:&lt;/strong&gt; Release v0.8.0, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.8.0&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;September 4, 2020:&lt;/strong&gt; Release v0.8.0b1, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.8.0b1&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;March 6, 2020:&lt;/strong&gt; Release v0.7.0, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.7.0&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;February 18, 2020:&lt;/strong&gt; Release v0.7.0b1, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.7.0b1&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;January 10, 2020:&lt;/strong&gt; Release v0.6.1, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.6.1&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;December 6, 2019:&lt;/strong&gt; Release v0.6, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.6&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;November 21, 2019:&lt;/strong&gt; Release v0.5, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.5&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;June 3, 2019:&lt;/strong&gt; Release v0.4, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.4&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;May 3, 2019:&lt;/strong&gt; Release v0.3, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.3&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;April 10, 2019:&lt;/strong&gt; Release v0.2, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.2&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;March 6, 2019:&lt;/strong&gt; Release v0.1, welcome to have a try and provide feedback.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h1&gt;Getting Started&lt;/h1&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Install the latest release from &lt;a href=&#34;https://pypi.org/project/econml/&#34;&gt;PyPI&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install econml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To install from source, see &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/#for-developers&#34;&gt;For Developers&lt;/a&gt; section below.&lt;/p&gt; &#xA;&lt;h2&gt;Usage Examples&lt;/h2&gt; &#xA;&lt;h3&gt;Estimation Methods&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Double Machine Learning (aka RLearner) (click to expand)&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Linear final stage&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.dml import LinearDML&#xA;from sklearn.linear_model import LassoCV&#xA;from econml.inference import BootstrapInference&#xA;&#xA;est = LinearDML(model_y=LassoCV(), model_t=LassoCV())&#xA;### Estimate with OLS confidence intervals&#xA;est.fit(Y, T, X=X, W=W) # W -&amp;gt; high-dimensional confounders, X -&amp;gt; features&#xA;treatment_effects = est.effect(X_test)&#xA;lb, ub = est.effect_interval(X_test, alpha=0.05) # OLS confidence intervals&#xA;&#xA;### Estimate with bootstrap confidence intervals&#xA;est.fit(Y, T, X=X, W=W, inference=&#39;bootstrap&#39;)  # with default bootstrap parameters&#xA;est.fit(Y, T, X=X, W=W, inference=BootstrapInference(n_bootstrap_samples=100))  # or customized&#xA;lb, ub = est.effect_interval(X_test, alpha=0.05) # Bootstrap confidence intervals&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Sparse linear final stage&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.dml import SparseLinearDML&#xA;from sklearn.linear_model import LassoCV&#xA;&#xA;est = SparseLinearDML(model_y=LassoCV(), model_t=LassoCV())&#xA;est.fit(Y, T, X=X, W=W) # X -&amp;gt; high dimensional features&#xA;treatment_effects = est.effect(X_test)&#xA;lb, ub = est.effect_interval(X_test, alpha=0.05) # Confidence intervals via debiased lasso&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Generic Machine Learning last stage&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.dml import NonParamDML&#xA;from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier&#xA;&#xA;est = NonParamDML(model_y=RandomForestRegressor(),&#xA;                  model_t=RandomForestClassifier(),&#xA;                  model_final=RandomForestRegressor(),&#xA;                  discrete_treatment=True)&#xA;est.fit(Y, T, X=X, W=W) &#xA;treatment_effects = est.effect(X_test)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Dynamic Double Machine Learning (click to expand)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.dynamic.dml import DynamicDML&#xA;# Use defaults&#xA;est = DynamicDML()&#xA;# Or specify hyperparameters&#xA;est = DynamicDML(model_y=LassoCV(cv=3), &#xA;                 model_t=LassoCV(cv=3), &#xA;                 cv=3)&#xA;est.fit(Y, T, X=X, W=None, groups=groups, inference=&#34;auto&#34;)&#xA;# Effects&#xA;treatment_effects = est.effect(X_test)&#xA;# Confidence intervals&#xA;lb, ub = est.effect_interval(X_test, alpha=0.05)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Causal Forests (click to expand)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.dml import CausalForestDML&#xA;from sklearn.linear_model import LassoCV&#xA;# Use defaults&#xA;est = CausalForestDML()&#xA;# Or specify hyperparameters&#xA;est = CausalForestDML(criterion=&#39;het&#39;, n_estimators=500,       &#xA;                      min_samples_leaf=10, &#xA;                      max_depth=10, max_samples=0.5,&#xA;                      discrete_treatment=False,&#xA;                      model_t=LassoCV(), model_y=LassoCV())&#xA;est.fit(Y, T, X=X, W=W)&#xA;treatment_effects = est.effect(X_test)&#xA;# Confidence intervals via Bootstrap-of-Little-Bags for forests&#xA;lb, ub = est.effect_interval(X_test, alpha=0.05)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Orthogonal Random Forests (click to expand)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.orf import DMLOrthoForest, DROrthoForest&#xA;from econml.sklearn_extensions.linear_model import WeightedLasso, WeightedLassoCV&#xA;# Use defaults&#xA;est = DMLOrthoForest()&#xA;est = DROrthoForest()&#xA;# Or specify hyperparameters&#xA;est = DMLOrthoForest(n_trees=500, min_leaf_size=10,&#xA;                     max_depth=10, subsample_ratio=0.7,&#xA;                     lambda_reg=0.01,&#xA;                     discrete_treatment=False,&#xA;                     model_T=WeightedLasso(alpha=0.01), model_Y=WeightedLasso(alpha=0.01),&#xA;                     model_T_final=WeightedLassoCV(cv=3), model_Y_final=WeightedLassoCV(cv=3))&#xA;est.fit(Y, T, X=X, W=W)&#xA;treatment_effects = est.effect(X_test)&#xA;# Confidence intervals via Bootstrap-of-Little-Bags for forests&#xA;lb, ub = est.effect_interval(X_test, alpha=0.05)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Meta-Learners (click to expand)&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;XLearner&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.metalearners import XLearner&#xA;from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor&#xA;&#xA;est = XLearner(models=GradientBoostingRegressor(),&#xA;              propensity_model=GradientBoostingClassifier(),&#xA;              cate_models=GradientBoostingRegressor())&#xA;est.fit(Y, T, X=np.hstack([X, W]))&#xA;treatment_effects = est.effect(np.hstack([X_test, W_test]))&#xA;&#xA;# Fit with bootstrap confidence interval construction enabled&#xA;est.fit(Y, T, X=np.hstack([X, W]), inference=&#39;bootstrap&#39;)&#xA;treatment_effects = est.effect(np.hstack([X_test, W_test]))&#xA;lb, ub = est.effect_interval(np.hstack([X_test, W_test]), alpha=0.05) # Bootstrap CIs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;SLearner&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.metalearners import SLearner&#xA;from sklearn.ensemble import GradientBoostingRegressor&#xA;&#xA;est = SLearner(overall_model=GradientBoostingRegressor())&#xA;est.fit(Y, T, X=np.hstack([X, W]))&#xA;treatment_effects = est.effect(np.hstack([X_test, W_test]))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;TLearner&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.metalearners import TLearner&#xA;from sklearn.ensemble import GradientBoostingRegressor&#xA;&#xA;est = TLearner(models=GradientBoostingRegressor())&#xA;est.fit(Y, T, X=np.hstack([X, W]))&#xA;treatment_effects = est.effect(np.hstack([X_test, W_test]))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Doubly Robust Learners (click to expand) &lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Linear final stage&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.dr import LinearDRLearner&#xA;from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier&#xA;&#xA;est = LinearDRLearner(model_propensity=GradientBoostingClassifier(),&#xA;                      model_regression=GradientBoostingRegressor())&#xA;est.fit(Y, T, X=X, W=W)&#xA;treatment_effects = est.effect(X_test)&#xA;lb, ub = est.effect_interval(X_test, alpha=0.05)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Sparse linear final stage&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.dr import SparseLinearDRLearner&#xA;from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier&#xA;&#xA;est = SparseLinearDRLearner(model_propensity=GradientBoostingClassifier(),&#xA;                            model_regression=GradientBoostingRegressor())&#xA;est.fit(Y, T, X=X, W=W)&#xA;treatment_effects = est.effect(X_test)&#xA;lb, ub = est.effect_interval(X_test, alpha=0.05)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Nonparametric final stage&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.dr import ForestDRLearner&#xA;from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier&#xA;&#xA;est = ForestDRLearner(model_propensity=GradientBoostingClassifier(),&#xA;                      model_regression=GradientBoostingRegressor())&#xA;est.fit(Y, T, X=X, W=W) &#xA;treatment_effects = est.effect(X_test)&#xA;lb, ub = est.effect_interval(X_test, alpha=0.05)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Double Machine Learning with Instrumental Variables (click to expand)&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Orthogonal instrumental variable learner&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.iv.dml import OrthoIV&#xA;&#xA;est = OrthoIV(projection=False, &#xA;              discrete_treatment=True, &#xA;              discrete_instrument=True)&#xA;est.fit(Y, T, Z=Z, X=X, W=W)&#xA;treatment_effects = est.effect(X_test)&#xA;lb, ub = est.effect_interval(X_test, alpha=0.05) # OLS confidence intervals&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Nonparametric double machine learning with instrumental variable&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.iv.dml import NonParamDMLIV&#xA;&#xA;est = NonParamDMLIV(projection=False, &#xA;                    discrete_treatment=True, &#xA;                    discrete_instrument=True)&#xA;est.fit(Y, T, Z=Z, X=X, W=W) # no analytical confidence interval available&#xA;treatment_effects = est.effect(X_test)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Doubly Robust Machine Learning with Instrumental Variables (click to expand)&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Linear final stage&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.iv.dr import LinearDRIV&#xA;&#xA;est = LinearDRIV(discrete_instrument=True, discrete_treatment=True)&#xA;est.fit(Y, T, Z=Z, X=X, W=W)&#xA;treatment_effects = est.effect(X_test)&#xA;lb, ub = est.effect_interval(X_test, alpha=0.05) # OLS confidence intervals&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Sparse linear final stage&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.iv.dr import SparseLinearDRIV&#xA;&#xA;est = SparseLinearDRIV(discrete_instrument=True, discrete_treatment=True)&#xA;est.fit(Y, T, Z=Z, X=X, W=W)&#xA;treatment_effects = est.effect(X_test)&#xA;lb, ub = est.effect_interval(X_test, alpha=0.05) # Debiased lasso confidence intervals&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Nonparametric final stage&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.iv.dr import ForestDRIV&#xA;&#xA;est = ForestDRIV(discrete_instrument=True, discrete_treatment=True)&#xA;est.fit(Y, T, Z=Z, X=X, W=W)&#xA;treatment_effects = est.effect(X_test)&#xA;# Confidence intervals via Bootstrap-of-Little-Bags for forests&#xA;lb, ub = est.effect_interval(X_test, alpha=0.05) &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Linear intent-to-treat (discrete instrument, discrete treatment)&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.iv.dr import LinearIntentToTreatDRIV&#xA;from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier&#xA;&#xA;est = LinearIntentToTreatDRIV(model_y_xw=GradientBoostingRegressor(),&#xA;                              model_t_xwz=GradientBoostingClassifier(),&#xA;                              flexible_model_effect=GradientBoostingRegressor())&#xA;est.fit(Y, T, Z=Z, X=X, W=W)&#xA;treatment_effects = est.effect(X_test)&#xA;lb, ub = est.effect_interval(X_test, alpha=0.05) # OLS confidence intervals&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Deep Instrumental Variables (click to expand)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;import keras&#xA;from econml.iv.nnet import DeepIV&#xA;&#xA;treatment_model = keras.Sequential([keras.layers.Dense(128, activation=&#39;relu&#39;, input_shape=(2,)),&#xA;                                    keras.layers.Dropout(0.17),&#xA;                                    keras.layers.Dense(64, activation=&#39;relu&#39;),&#xA;                                    keras.layers.Dropout(0.17),&#xA;                                    keras.layers.Dense(32, activation=&#39;relu&#39;),&#xA;                                    keras.layers.Dropout(0.17)])&#xA;response_model = keras.Sequential([keras.layers.Dense(128, activation=&#39;relu&#39;, input_shape=(2,)),&#xA;                                  keras.layers.Dropout(0.17),&#xA;                                  keras.layers.Dense(64, activation=&#39;relu&#39;),&#xA;                                  keras.layers.Dropout(0.17),&#xA;                                  keras.layers.Dense(32, activation=&#39;relu&#39;),&#xA;                                  keras.layers.Dropout(0.17),&#xA;                                  keras.layers.Dense(1)])&#xA;est = DeepIV(n_components=10, # Number of gaussians in the mixture density networks)&#xA;             m=lambda z, x: treatment_model(keras.layers.concatenate([z, x])), # Treatment model&#xA;             h=lambda t, x: response_model(keras.layers.concatenate([t, x])), # Response model&#xA;             n_samples=1 # Number of samples used to estimate the response&#xA;             )&#xA;est.fit(Y, T, X=X, Z=Z) # Z -&amp;gt; instrumental variables&#xA;treatment_effects = est.effect(X_test)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/#references&#34;&gt;References&lt;/a&gt; section for more details.&lt;/p&gt; &#xA;&lt;h3&gt;Interpretability&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Tree Interpreter of the CATE model (click to expand)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.cate_interpreter import SingleTreeCateInterpreter&#xA;intrp = SingleTreeCateInterpreter(include_model_uncertainty=True, max_depth=2, min_samples_leaf=10)&#xA;# We interpret the CATE model&#39;s behavior based on the features used for heterogeneity&#xA;intrp.interpret(est, X)&#xA;# Plot the tree&#xA;plt.figure(figsize=(25, 5))&#xA;intrp.plot(feature_names=[&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;], fontsize=12)&#xA;plt.show()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/notebooks/images/dr_cate_tree.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Policy Interpreter of the CATE model (click to expand)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.cate_interpreter import SingleTreePolicyInterpreter&#xA;# We find a tree-based treatment policy based on the CATE model&#xA;intrp = SingleTreePolicyInterpreter(risk_level=0.05, max_depth=2, min_samples_leaf=1,min_impurity_decrease=.001)&#xA;intrp.interpret(est, X, sample_treatment_costs=0.2)&#xA;# Plot the tree&#xA;plt.figure(figsize=(25, 5))&#xA;intrp.plot(feature_names=[&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;], fontsize=12)&#xA;plt.show()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/notebooks/images/dr_policy_tree.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;SHAP values for the CATE model (click to expand)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;import shap&#xA;from econml.dml import CausalForestDML&#xA;est = CausalForestDML()&#xA;est.fit(Y, T, X=X, W=W)&#xA;shap_values = est.shap_values(X)&#xA;shap.summary_plot(shap_values[&#39;Y0&#39;][&#39;T0&#39;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Causal Model Selection and Cross-Validation&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Causal model selection with the `RScorer` (click to expand)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.score import Rscorer&#xA;&#xA;# split data in train-validation&#xA;X_train, X_val, T_train, T_val, Y_train, Y_val = train_test_split(X, T, y, test_size=.4)&#xA;&#xA;# define list of CATE estimators to select among&#xA;reg = lambda: RandomForestRegressor(min_samples_leaf=20)&#xA;clf = lambda: RandomForestClassifier(min_samples_leaf=20)&#xA;models = [(&#39;ldml&#39;, LinearDML(model_y=reg(), model_t=clf(), discrete_treatment=True,&#xA;                             linear_first_stages=False, cv=3)),&#xA;          (&#39;xlearner&#39;, XLearner(models=reg(), cate_models=reg(), propensity_model=clf())),&#xA;          (&#39;dalearner&#39;, DomainAdaptationLearner(models=reg(), final_models=reg(), propensity_model=clf())),&#xA;          (&#39;slearner&#39;, SLearner(overall_model=reg())),&#xA;          (&#39;drlearner&#39;, DRLearner(model_propensity=clf(), model_regression=reg(),&#xA;                                  model_final=reg(), cv=3)),&#xA;          (&#39;rlearner&#39;, NonParamDML(model_y=reg(), model_t=clf(), model_final=reg(),&#xA;                                   discrete_treatment=True, cv=3)),&#xA;          (&#39;dml3dlasso&#39;, DML(model_y=reg(), model_t=clf(),&#xA;                             model_final=LassoCV(cv=3, fit_intercept=False),&#xA;                             discrete_treatment=True,&#xA;                             featurizer=PolynomialFeatures(degree=3),&#xA;                             linear_first_stages=False, cv=3))&#xA;]&#xA;&#xA;# fit cate models on train data&#xA;models = [(name, mdl.fit(Y_train, T_train, X=X_train)) for name, mdl in models]&#xA;&#xA;# score cate models on validation data&#xA;scorer = RScorer(model_y=reg(), model_t=clf(),&#xA;                 discrete_treatment=True, cv=3, mc_iters=2, mc_agg=&#39;median&#39;)&#xA;scorer.fit(Y_val, T_val, X=X_val)&#xA;rscore = [scorer.score(mdl) for _, mdl in models]&#xA;# select the best model&#xA;mdl, _ = scorer.best_model([mdl for _, mdl in models])&#xA;# create weighted ensemble model based on score performance&#xA;mdl, _ = scorer.ensemble([mdl for _, mdl in models])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;First Stage Model Selection (click to expand)&lt;/summary&gt; &#xA; &lt;p&gt;First stage models can be selected either by passing in cross-validated models (e.g. &lt;code&gt;sklearn.linear_model.LassoCV&lt;/code&gt;) to EconML&#39;s estimators or perform the first stage model selection outside of EconML and pass in the selected model. Unless selecting among a large set of hyperparameters, choosing first stage models externally is the preferred method due to statistical and computational advantages.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.dml import LinearDML&#xA;from sklearn import clone&#xA;from sklearn.ensemble import RandomForestRegressor&#xA;from sklearn.model_selection import GridSearchCV&#xA;&#xA;cv_model = GridSearchCV(&#xA;              estimator=RandomForestRegressor(),&#xA;              param_grid={&#xA;                  &#34;max_depth&#34;: [3, None],&#xA;                  &#34;n_estimators&#34;: (10, 30, 50, 100, 200),&#xA;                  &#34;max_features&#34;: (2, 4, 6),&#xA;              },&#xA;              cv=5,&#xA;           )&#xA;# First stage model selection within EconML&#xA;# This is more direct, but computationally and statistically less efficient&#xA;est = LinearDML(model_y=cv_model, model_t=cv_model)&#xA;# First stage model selection ouside of EconML&#xA;# This is the most efficient, but requires boilerplate code&#xA;model_t = clone(cv_model).fit(W, T).best_estimator_&#xA;model_y = clone(cv_model).fit(W, Y).best_estimator_&#xA;est = LinearDML(model_y=model_t, model_t=model_y)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Inference&lt;/h3&gt; &#xA;&lt;p&gt;Whenever inference is enabled, then one can get a more structure &lt;code&gt;InferenceResults&lt;/code&gt; object with more elaborate inference information, such as p-values and z-statistics. When the CATE model is linear and parametric, then a &lt;code&gt;summary()&lt;/code&gt; method is also enabled. For instance:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.dml import LinearDML&#xA;# Use defaults&#xA;est = LinearDML()&#xA;est.fit(Y, T, X=X, W=W)&#xA;# Get the effect inference summary, which includes the standard error, z test score, p value, and confidence interval given each sample X[i]&#xA;est.effect_inference(X_test).summary_frame(alpha=0.05, value=0, decimals=3)&#xA;# Get the population summary for the entire sample X&#xA;est.effect_inference(X_test).population_summary(alpha=0.1, value=0, decimals=3, tol=0.001)&#xA;#  Get the parameter inference summary for the final model&#xA;est.summary()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Example Output (click to expand)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;# Get the effect inference summary, which includes the standard error, z test score, p value, and confidence interval given each sample X[i]&#xA;est.effect_inference(X_test).summary_frame(alpha=0.05, value=0, decimals=3)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/notebooks/images/summary_frame.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;# Get the population summary for the entire sample X&#xA;est.effect_inference(X_test).population_summary(alpha=0.1, value=0, decimals=3, tol=0.001)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/notebooks/images/population_summary.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;#  Get the parameter inference summary for the final model&#xA;est.summary()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/notebooks/images/summary.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Policy Learning&lt;/h3&gt; &#xA;&lt;p&gt;You can also perform direct policy learning from observational data, using the doubly robust method for offline policy learning. These methods directly predict a recommended treatment, without internally fitting an explicit model of the conditional average treatment effect.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Doubly Robust Policy Learning (click to expand)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.policy import DRPolicyTree, DRPolicyForest&#xA;from sklearn.ensemble import RandomForestRegressor&#xA;&#xA;# fit a single binary decision tree policy&#xA;policy = DRPolicyTree(max_depth=1, min_impurity_decrease=0.01, honest=True)&#xA;policy.fit(y, T, X=X, W=W)&#xA;# predict the recommended treatment&#xA;recommended_T = policy.predict(X)&#xA;# plot the binary decision tree&#xA;plt.figure(figsize=(10,5))&#xA;policy.plot()&#xA;# get feature importances&#xA;importances = policy.feature_importances_&#xA;&#xA;# fit a binary decision forest&#xA;policy = DRPolicyForest(max_depth=1, min_impurity_decrease=0.01, honest=True)&#xA;policy.fit(y, T, X=X, W=W)&#xA;# predict the recommended treatment&#xA;recommended_T = policy.predict(X)&#xA;# plot the first tree in the ensemble&#xA;plt.figure(figsize=(10,5))&#xA;policy.plot(0)&#xA;# get feature importances&#xA;importances = policy.feature_importances_&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/images/policy_tree.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;To see more complex examples, go to the &lt;a href=&#34;https://github.com/Microsoft/EconML/tree/main/notebooks&#34;&gt;notebooks&lt;/a&gt; section of the repository. For a more detailed description of the treatment effect estimation algorithms, see the EconML &lt;a href=&#34;https://econml.azurewebsites.net/&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;For Developers&lt;/h1&gt; &#xA;&lt;p&gt;You can get started by cloning this repository. We use &lt;a href=&#34;https://setuptools.readthedocs.io/en/latest/index.html&#34;&gt;setuptools&lt;/a&gt; for building and distributing our package. We rely on some recent features of setuptools, so make sure to upgrade to a recent version with &lt;code&gt;pip install setuptools --upgrade&lt;/code&gt;. Then from your local copy of the repository you can run &lt;code&gt;pip install -e .&lt;/code&gt; to get started (but depending on what you&#39;re doing you might want to install with extras instead, like &lt;code&gt;pip install -e .[plt]&lt;/code&gt; if you want to use matplotlib integration, or you can use &lt;code&gt;pip install -e .[all]&lt;/code&gt; to include all extras).&lt;/p&gt; &#xA;&lt;h2&gt;Running the tests&lt;/h2&gt; &#xA;&lt;p&gt;This project uses &lt;a href=&#34;https://docs.pytest.org/&#34;&gt;pytest&lt;/a&gt; for testing. To run tests locally after installing the package, you can use &lt;code&gt;pip install pytest-runner&lt;/code&gt; followed by &lt;code&gt;python setup.py pytest&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We have added pytest marks to some tests to make it easier to run a subset, and you can set the PYTEST_ADDOPTS environment variable to take advantage of this. For instance, you can set it to &lt;code&gt;-m &#34;not (notebook or automl)&#34;&lt;/code&gt; to skip notebook and automl tests that have some additional dependencies.&lt;/p&gt; &#xA;&lt;h2&gt;Generating the documentation&lt;/h2&gt; &#xA;&lt;p&gt;This project&#39;s documentation is generated via &lt;a href=&#34;https://www.sphinx-doc.org/en/main/index.html&#34;&gt;Sphinx&lt;/a&gt;. Note that we use &lt;a href=&#34;https://graphviz.org/&#34;&gt;graphviz&lt;/a&gt;&#39;s &lt;code&gt;dot&lt;/code&gt; application to produce some of the images in our documentation, so you should make sure that &lt;code&gt;dot&lt;/code&gt; is installed and in your path.&lt;/p&gt; &#xA;&lt;p&gt;To generate a local copy of the documentation from a clone of this repository, just run &lt;code&gt;python setup.py build_sphinx -W -E -a&lt;/code&gt;, which will build the documentation and place it under the &lt;code&gt;build/sphinx/html&lt;/code&gt; path.&lt;/p&gt; &#xA;&lt;p&gt;The reStructuredText files that make up the documentation are stored in the &lt;a href=&#34;https://github.com/Microsoft/EconML/tree/main/doc&#34;&gt;docs directory&lt;/a&gt;; module documentation is automatically generated by the Sphinx build process.&lt;/p&gt; &#xA;&lt;h1&gt;Blogs and Publications&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;June 2019: &lt;a href=&#34;https://arxiv.org/pdf/1905.10176.pdf&#34;&gt;Treatment Effects with Instruments paper&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;May 2019: &lt;a href=&#34;https://odsc.com/speakers/machine-learning-estimation-of-heterogeneous-treatment-effect-the-microsoft-econml-library/&#34;&gt;Open Data Science Conference Workshop&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2018: &lt;a href=&#34;http://proceedings.mlr.press/v97/oprescu19a.html&#34;&gt;Orthogonal Random Forests paper&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2017: &lt;a href=&#34;http://proceedings.mlr.press/v70/hartford17a/hartford17a.pdf&#34;&gt;DeepIV paper&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Citation&lt;/h1&gt; &#xA;&lt;p&gt;If you use EconML in your research, please cite us as follows:&lt;/p&gt; &#xA;&lt;p&gt;Keith Battocchi, Eleanor Dillon, Maggie Hei, Greg Lewis, Paul Oka, Miruna Oprescu, Vasilis Syrgkanis. &lt;strong&gt;EconML: A Python Package for ML-Based Heterogeneous Treatment Effects Estimation.&lt;/strong&gt; &lt;a href=&#34;https://github.com/microsoft/EconML&#34;&gt;https://github.com/microsoft/EconML&lt;/a&gt;, 2019. Version 0.x.&lt;/p&gt; &#xA;&lt;p&gt;BibTex:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{econml,&#xA;  author={Keith Battocchi, Eleanor Dillon, Maggie Hei, Greg Lewis, Paul Oka, Miruna Oprescu, Vasilis Syrgkanis},&#xA;  title={{EconML}: {A Python Package for ML-Based Heterogeneous Treatment Effects Estimation}},&#xA;  howpublished={https://github.com/microsoft/EconML},&#xA;  note={Version 0.x},&#xA;  year={2019}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Contributing and Feedback&lt;/h1&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href=&#34;https://cla.microsoft.com&#34;&gt;https://cla.microsoft.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When you submit a pull request, a CLA-bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; &#xA;&lt;h1&gt;References&lt;/h1&gt; &#xA;&lt;p&gt;Athey, Susan, and Stefan Wager. &lt;strong&gt;Policy learning with observational data.&lt;/strong&gt; Econometrica 89.1 (2021): 133-161.&lt;/p&gt; &#xA;&lt;p&gt;X Nie, S Wager. &lt;strong&gt;Quasi-Oracle Estimation of Heterogeneous Treatment Effects.&lt;/strong&gt; &lt;a href=&#34;https://doi.org/10.1093/biomet/asaa076&#34;&gt;&lt;em&gt;Biometrika&lt;/em&gt;&lt;/a&gt;, 2020&lt;/p&gt; &#xA;&lt;p&gt;V. Syrgkanis, V. Lei, M. Oprescu, M. Hei, K. Battocchi, G. Lewis. &lt;strong&gt;Machine Learning Estimation of Heterogeneous Treatment Effects with Instruments.&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/1905.10176&#34;&gt;&lt;em&gt;Proceedings of the 33rd Conference on Neural Information Processing Systems (NeurIPS)&lt;/em&gt;&lt;/a&gt;, 2019 &lt;strong&gt;(Spotlight Presentation)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;D. Foster, V. Syrgkanis. &lt;strong&gt;Orthogonal Statistical Learning.&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/pdf/1901.09036.pdf&#34;&gt;&lt;em&gt;Proceedings of the 32nd Annual Conference on Learning Theory (COLT)&lt;/em&gt;&lt;/a&gt;, 2019 &lt;strong&gt;(Best Paper Award)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;M. Oprescu, V. Syrgkanis and Z. S. Wu. &lt;strong&gt;Orthogonal Random Forest for Causal Inference.&lt;/strong&gt; &lt;a href=&#34;http://proceedings.mlr.press/v97/oprescu19a.html&#34;&gt;&lt;em&gt;Proceedings of the 36th International Conference on Machine Learning (ICML)&lt;/em&gt;&lt;/a&gt;, 2019.&lt;/p&gt; &#xA;&lt;p&gt;S. K√ºnzel, J. Sekhon, J. Bickel and B. Yu. &lt;strong&gt;Metalearners for estimating heterogeneous treatment effects using machine learning.&lt;/strong&gt; &lt;a href=&#34;https://www.pnas.org/content/116/10/4156&#34;&gt;&lt;em&gt;Proceedings of the national academy of sciences, 116(10), 4156-4165&lt;/em&gt;&lt;/a&gt;, 2019.&lt;/p&gt; &#xA;&lt;p&gt;S. Athey, J. Tibshirani, S. Wager. &lt;strong&gt;Generalized random forests.&lt;/strong&gt; &lt;a href=&#34;https://projecteuclid.org/euclid.aos/1547197251&#34;&gt;&lt;em&gt;Annals of Statistics, 47, no. 2, 1148--1178&lt;/em&gt;&lt;/a&gt;, 2019.&lt;/p&gt; &#xA;&lt;p&gt;V. Chernozhukov, D. Nekipelov, V. Semenova, V. Syrgkanis. &lt;strong&gt;Plug-in Regularized Estimation of High-Dimensional Parameters in Nonlinear Semiparametric Models.&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/1806.04823&#34;&gt;&lt;em&gt;Arxiv preprint arxiv:1806.04823&lt;/em&gt;&lt;/a&gt;, 2018.&lt;/p&gt; &#xA;&lt;p&gt;S. Wager, S. Athey. &lt;strong&gt;Estimation and Inference of Heterogeneous Treatment Effects using Random Forests.&lt;/strong&gt; &lt;a href=&#34;https://www.tandfonline.com/doi/citedby/10.1080/01621459.2017.1319839&#34;&gt;&lt;em&gt;Journal of the American Statistical Association, 113:523, 1228-1242&lt;/em&gt;&lt;/a&gt;, 2018.&lt;/p&gt; &#xA;&lt;p&gt;Jason Hartford, Greg Lewis, Kevin Leyton-Brown, and Matt Taddy. &lt;strong&gt;Deep IV: A flexible approach for counterfactual prediction.&lt;/strong&gt; &lt;a href=&#34;http://proceedings.mlr.press/v70/hartford17a/hartford17a.pdf&#34;&gt;&lt;em&gt;Proceedings of the 34th International Conference on Machine Learning, ICML&#39;17&lt;/em&gt;&lt;/a&gt;, 2017.&lt;/p&gt; &#xA;&lt;p&gt;V. Chernozhukov, D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, and a. W. Newey. &lt;strong&gt;Double Machine Learning for Treatment and Causal Parameters.&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/1608.00060&#34;&gt;&lt;em&gt;ArXiv preprint arXiv:1608.00060&lt;/em&gt;&lt;/a&gt;, 2016.&lt;/p&gt; &#xA;&lt;p&gt;Dudik, M., Erhan, D., Langford, J., &amp;amp; Li, L. &lt;strong&gt;Doubly robust policy evaluation and optimization.&lt;/strong&gt; Statistical Science, 29(4), 485-511, 2014.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>qiskit-community/ibm-quantum-spring-challenge-2022</title>
    <updated>2022-05-29T01:45:04Z</updated>
    <id>tag:github.com,2022-05-29:/qiskit-community/ibm-quantum-spring-challenge-2022</id>
    <link href="https://github.com/qiskit-community/ibm-quantum-spring-challenge-2022" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;IBM Quantum Spring Challenge 2022&lt;/h1&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;We are proud to welcome you back for another IBM Quantum Challenge!&lt;/p&gt; &#xA;&lt;p&gt;When Richard Feynman proposed quantum computing 41 years ago, he imagined devices which could be used to model the quantum world around us. This year&#39;s IBM Quantum Spring Challenge focuses exactly on Professor Feynman&#39;s vision, simulating chemistry and many-body systems with four inviting challenges to solve.&lt;/p&gt; &#xA;&lt;p&gt;The IBM Quantum Spring Challenge runs from &lt;strong&gt;Monday, May 23 at 9AM EDT until Friday, May 27 at 5PM EDT&lt;/strong&gt;, and will be divided into two segments. One part will invite participants to tackle problems revolving around many-body systems, while the other will focus on fermionic chemistry simulations. Participants will get a one-of-a-kind opportunity to investigate problems at the forefront of quantum computing research.&lt;/p&gt; &#xA;&lt;p&gt;To recognize your achievement, those who complete all the exercises will receive a digital achievement badge to showcase the skills you&#39;ve developed throughout the challenge.&lt;/p&gt; &#xA;&lt;p&gt;Think you&#39;re up for the task? &lt;strong&gt;&lt;a href=&#34;https://challenges.quantum-computing.ibm.com/spring-2022&#34;&gt;Register here&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Stay Connected&lt;/h2&gt; &#xA;&lt;p&gt;Make sure to join the dedicated Slack channel &lt;a href=&#34;https://qiskit.slack.com/archives/C03BRNA9UQY&#34;&gt;#spring-challenge-2022&lt;/a&gt; where you can connect with mentors and fellow attendees! Join the Qiskit Slack workspace &lt;a href=&#34;https://ibm.co/joinqiskitslack&#34;&gt;here&lt;/a&gt; if you haven&#39;t already. &lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h1&gt;&lt;a href=&#34;https://github.com/qiskit-community/ibm-quantum-spring-challenge-2022/raw/main/Code%20of%20Conduct.md#code-of-conduct-for-participation&#34;&gt;Event Code of Conduct&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;h1&gt;&lt;a href=&#34;https://github.com/qiskit-community/ibm-quantum-spring-challenge-2022/raw/main/Preliminary%20Content.md&#34;&gt;Preliminary Content&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;h1&gt;&lt;a href=&#34;https://github.com/qiskit-community/ibm-quantum-spring-challenge-2022/raw/main/faq.md&#34;&gt;FAQ&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;h1&gt;&lt;a href=&#34;https://github.com/qiskit-community/ibm-quantum-spring-challenge-2022/raw/main/Hints..md&#34;&gt;HINTS&lt;/a&gt;&lt;/h1&gt;</summary>
  </entry>
  <entry>
    <title>ShusenTang/Dive-into-DL-PyTorch</title>
    <updated>2022-05-29T01:45:04Z</updated>
    <id>tag:github.com,2022-05-29:/ShusenTang/Dive-into-DL-PyTorch</id>
    <link href="https://github.com/ShusenTang/Dive-into-DL-PyTorch" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Êú¨È°πÁõÆÂ∞Ü„ÄäÂä®ÊâãÂ≠¶Ê∑±Â∫¶Â≠¶‰π†„Äã(Dive into Deep Learning)Âéü‰π¶‰∏≠ÁöÑMXNetÂÆûÁé∞Êîπ‰∏∫PyTorchÂÆûÁé∞„ÄÇ&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img width=&#34;500&#34; src=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/img/cover.png&#34; alt=&#34;Â∞ÅÈù¢&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://tangshusen.me/Dive-into-DL-PyTorch&#34;&gt;Êú¨È°πÁõÆ&lt;/a&gt;Â∞Ü&lt;a href=&#34;http://zh.d2l.ai/&#34;&gt;„ÄäÂä®ÊâãÂ≠¶Ê∑±Â∫¶Â≠¶‰π†„Äã&lt;/a&gt;&amp;nbsp;Âéü‰π¶‰∏≠MXNet‰ª£Á†ÅÂÆûÁé∞Êîπ‰∏∫PyTorchÂÆûÁé∞„ÄÇÂéü‰π¶‰ΩúËÄÖÔºöÈòøÊñØÈ°ø¬∑Âº†„ÄÅÊùéÊ≤ê„ÄÅÊâéÂç°Èáå C. Á´ãÈ°ø„ÄÅ‰∫öÂéÜÂ±±Â§ß J. ÊñØËé´Êãâ‰ª•ÂèäÂÖ∂‰ªñÁ§æÂå∫Ë¥°ÁåÆËÄÖÔºåGitHubÂú∞ÂùÄÔºö&lt;a href=&#34;https://github.com/d2l-ai/d2l-zh&#34;&gt;https://github.com/d2l-ai/d2l-zh&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Ê≠§‰π¶ÁöÑ&lt;a href=&#34;https://zh.d2l.ai/&#34;&gt;‰∏≠&lt;/a&gt;&lt;a href=&#34;https://d2l.ai/&#34;&gt;Ëã±&lt;/a&gt;ÁâàÊú¨Â≠òÂú®‰∏Ä‰∫õ‰∏çÂêåÔºåÈíàÂØπÊ≠§‰π¶Ëã±ÊñáÁâàÁöÑPyTorchÈáçÊûÑÂèØÂèÇËÄÉ&lt;a href=&#34;https://github.com/dsgiitr/d2l-pytorch&#34;&gt;Ëøô‰∏™È°πÁõÆ&lt;/a&gt;„ÄÇ There are some differences between the &lt;a href=&#34;https://zh.d2l.ai/&#34;&gt;Chinese&lt;/a&gt; and &lt;a href=&#34;https://d2l.ai/&#34;&gt;English&lt;/a&gt; versions of this book. For the PyTorch modifying of the English version, you can refer to &lt;a href=&#34;https://github.com/dsgiitr/d2l-pytorch&#34;&gt;this repo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;ÁÆÄ‰ªã&lt;/h2&gt; &#xA;&lt;p&gt;Êú¨‰ªìÂ∫ì‰∏ªË¶ÅÂåÖÂê´codeÂíådocs‰∏§‰∏™Êñá‰ª∂Â§πÔºàÂ§ñÂä†‰∏Ä‰∫õÊï∞ÊçÆÂ≠òÊîæÂú®data‰∏≠Ôºâ„ÄÇÂÖ∂‰∏≠codeÊñá‰ª∂Â§πÂ∞±ÊòØÊØèÁ´†Áõ∏ÂÖ≥jupyter notebook‰ª£Á†ÅÔºàÂü∫‰∫éPyTorchÔºâÔºõdocsÊñá‰ª∂Â§πÂ∞±ÊòØmarkdownÊ†ºÂºèÁöÑ„ÄäÂä®ÊâãÂ≠¶Ê∑±Â∫¶Â≠¶‰π†„Äã‰π¶‰∏≠ÁöÑÁõ∏ÂÖ≥ÂÜÖÂÆπÔºåÁÑ∂ÂêéÂà©Áî®&lt;a href=&#34;https://docsify.js.org/#/zh-cn/&#34;&gt;docsify&lt;/a&gt;Â∞ÜÁΩëÈ°µÊñáÊ°£ÈÉ®ÁΩ≤Âà∞GitHub Pages‰∏äÔºåÁî±‰∫éÂéü‰π¶‰ΩøÁî®ÁöÑÊòØMXNetÊ°ÜÊû∂ÔºåÊâÄ‰ª•docsÂÜÖÂÆπÂèØËÉΩ‰∏éÂéü‰π¶Áï•Êúâ‰∏çÂêåÔºå‰ΩÜÊòØÊï¥‰ΩìÂÜÖÂÆπÊòØ‰∏ÄÊ†∑ÁöÑ„ÄÇÊ¨¢ËøéÂØπÊú¨È°πÁõÆÂÅöÂá∫Ë¥°ÁåÆÊàñÊèêÂá∫issue„ÄÇ&lt;/p&gt; &#xA;&lt;h2&gt;Èù¢Âêë‰∫∫Áæ§&lt;/h2&gt; &#xA;&lt;p&gt;Êú¨È°πÁõÆÈù¢ÂêëÂØπÊ∑±Â∫¶Â≠¶‰π†ÊÑüÂÖ¥Ë∂£ÔºåÂ∞§ÂÖ∂ÊòØÊÉ≥‰ΩøÁî®PyTorchËøõË°åÊ∑±Â∫¶Â≠¶‰π†ÁöÑÁ´•Èûã„ÄÇÊú¨È°πÁõÆÂπ∂‰∏çË¶ÅÊ±Ç‰Ω†Êúâ‰ªª‰ΩïÊ∑±Â∫¶Â≠¶‰π†ÊàñËÄÖÊú∫Âô®Â≠¶‰π†ÁöÑËÉåÊôØÁü•ËØÜÔºå‰Ω†Âè™ÈúÄ‰∫ÜËß£Âü∫Á°ÄÁöÑÊï∞Â≠¶ÂíåÁºñÁ®ãÔºåÂ¶ÇÂü∫Á°ÄÁöÑÁ∫øÊÄß‰ª£Êï∞„ÄÅÂæÆÂàÜÂíåÊ¶ÇÁéáÔºå‰ª•ÂèäÂü∫Á°ÄÁöÑPythonÁºñÁ®ã„ÄÇ&lt;/p&gt; &#xA;&lt;h2&gt;È£üÁî®ÊñπÊ≥ï&lt;/h2&gt; &#xA;&lt;h3&gt;ÊñπÊ≥ï‰∏Ä&lt;/h3&gt; &#xA;&lt;p&gt;Êú¨‰ªìÂ∫ìÂåÖÂê´‰∏Ä‰∫õlatexÂÖ¨ÂºèÔºå‰ΩÜgithubÁöÑmarkdownÂéüÁîüÊòØ‰∏çÊîØÊåÅÂÖ¨ÂºèÊòæÁ§∫ÁöÑÔºåËÄådocsÊñá‰ª∂Â§πÂ∑≤ÁªèÂà©Áî®&lt;a href=&#34;https://docsify.js.org/#/zh-cn/&#34;&gt;docsify&lt;/a&gt;Ë¢´ÈÉ®ÁΩ≤Âà∞‰∫ÜGitHub Pages‰∏äÔºåÊâÄ‰ª•Êü•ÁúãÊñáÊ°£ÊúÄÁÆÄ‰æøÁöÑÊñπÊ≥ïÂ∞±ÊòØÁõ¥Êé•ËÆøÈóÆ&lt;a href=&#34;https://tangshusen.me/Dive-into-DL-PyTorch&#34;&gt;Êú¨È°πÁõÆÁΩëÈ°µÁâà&lt;/a&gt;„ÄÇÂΩìÁÑ∂Â¶ÇÊûú‰Ω†ËøòÊÉ≥Ë∑ë‰∏Ä‰∏ãËøêË°åÁõ∏ÂÖ≥‰ª£Á†ÅÁöÑËØùËøòÊòØÂæóÊääÊú¨È°πÁõÆclone‰∏ãÊù•ÔºåÁÑ∂ÂêéËøêË°åcodeÊñá‰ª∂Â§π‰∏ãÁõ∏ÂÖ≥‰ª£Á†Å„ÄÇ&lt;/p&gt; &#xA;&lt;h3&gt;ÊñπÊ≥ï‰∫å&lt;/h3&gt; &#xA;&lt;p&gt;‰Ω†ËøòÂèØ‰ª•Âú®Êú¨Âú∞ËÆøÈóÆÊñáÊ°£ÔºåÂÖàÂÆâË£Ö&lt;code&gt;docsify-cli&lt;/code&gt;Â∑•ÂÖ∑:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;npm i docsify-cli -g&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;ÁÑ∂ÂêéÂ∞ÜÊú¨È°πÁõÆcloneÂà∞Êú¨Âú∞:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/ShusenTang/Dive-into-DL-PyTorch.git&#xA;cd Dive-into-DL-PyTorch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;ÁÑ∂ÂêéËøêË°å‰∏Ä‰∏™Êú¨Âú∞ÊúçÂä°Âô®ÔºåËøôÊ†∑Â∞±ÂèØ‰ª•ÂæàÊñπ‰æøÁöÑÂú®&lt;code&gt;http://localhost:3000&lt;/code&gt;ÂÆûÊó∂ËÆøÈóÆÊñáÊ°£ÁΩëÈ°µÊ∏≤ÊüìÊïàÊûú„ÄÇ&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docsify serve docs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;ÊñπÊ≥ï‰∏â&lt;/h3&gt; &#xA;&lt;p&gt;Â¶ÇÊûú‰Ω†‰∏çÊÉ≥ÂÆâË£Ö&lt;code&gt;docsify-cli&lt;/code&gt;Â∑•ÂÖ∑ÔºåÁîöËá≥‰Ω†ÁöÑÁîµËÑë‰∏äÈÉΩÊ≤°ÊúâÂÆâË£Ö&lt;code&gt;Node.js&lt;/code&gt;ÔºåËÄåÂá∫‰∫éÊüê‰∫õÂéüÂõ†‰Ω†ÂèàÊÉ≥Âú®Êú¨Âú∞ÊµèËßàÊñáÊ°£ÔºåÈÇ£‰πà‰Ω†ÂèØ‰ª•Âú®&lt;code&gt;docker&lt;/code&gt;ÂÆπÂô®‰∏≠ËøêË°åÁΩëÈ°µÊúçÂä°„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;È¶ñÂÖàÂ∞ÜÊú¨È°πÁõÆcloneÂà∞Êú¨Âú∞:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/ShusenTang/Dive-into-DL-PyTorch.git&#xA;cd Dive-into-DL-PyTorch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;‰πãÂêé‰ΩøÁî®Â¶Ç‰∏ãÂëΩ‰ª§ÂàõÂª∫‰∏Ä‰∏™ÂêçÁß∞‰∏∫„Äåd2dl„ÄçÁöÑ&lt;code&gt;docker&lt;/code&gt;ÈïúÂÉèÔºö&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker build -t d2dl .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;ÈïúÂÉèÂàõÂª∫Â•ΩÂêéÔºåËøêË°åÂ¶Ç‰∏ãÂëΩ‰ª§ÂàõÂª∫‰∏Ä‰∏™Êñ∞ÁöÑÂÆπÂô®Ôºö&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker run -dp 3000:3000 d2dl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;ÊúÄÂêéÂú®ÊµèËßàÂô®‰∏≠ÊâìÂºÄËøô‰∏™Âú∞ÂùÄ&lt;code&gt;http://localhost:3000/#/&lt;/code&gt;ÔºåÂ∞±ËÉΩÊÑâÂø´Âú∞ËÆøÈóÆÊñáÊ°£‰∫Ü„ÄÇÈÄÇÂêàÈÇ£‰∫õ‰∏çÊÉ≥Âú®ÁîµËÑë‰∏äË£ÖÂ§™Â§öÂ∑•ÂÖ∑ÁöÑÂ∞è‰ºô‰º¥„ÄÇ&lt;/p&gt; &#xA;&lt;h2&gt;ÁõÆÂΩï&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;&#34;&gt;ÁÆÄ‰ªã&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/read_guide.md&#34;&gt;ÈòÖËØªÊåáÂçó&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter01_DL-intro/deep-learning-intro.md&#34;&gt;1. Ê∑±Â∫¶Â≠¶‰π†ÁÆÄ‰ªã&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;2. È¢ÑÂ§áÁü•ËØÜ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter02_prerequisite/2.1_install.md&#34;&gt;2.1 ÁéØÂ¢ÉÈÖçÁΩÆ&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter02_prerequisite/2.2_tensor.md&#34;&gt;2.2 Êï∞ÊçÆÊìç‰Ωú&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter02_prerequisite/2.3_autograd.md&#34;&gt;2.3 Ëá™Âä®Ê±ÇÊ¢ØÂ∫¶&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;3. Ê∑±Â∫¶Â≠¶‰π†Âü∫Á°Ä &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter03_DL-basics/3.1_linear-regression.md&#34;&gt;3.1 Á∫øÊÄßÂõûÂΩí&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter03_DL-basics/3.2_linear-regression-scratch.md&#34;&gt;3.2 Á∫øÊÄßÂõûÂΩíÁöÑ‰ªéÈõ∂ÂºÄÂßãÂÆûÁé∞&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter03_DL-basics/3.3_linear-regression-pytorch.md&#34;&gt;3.3 Á∫øÊÄßÂõûÂΩíÁöÑÁÆÄÊ¥ÅÂÆûÁé∞&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter03_DL-basics/3.4_softmax-regression.md&#34;&gt;3.4 softmaxÂõûÂΩí&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter03_DL-basics/3.5_fashion-mnist.md&#34;&gt;3.5 ÂõæÂÉèÂàÜÁ±ªÊï∞ÊçÆÈõÜÔºàFashion-MNISTÔºâ&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter03_DL-basics/3.6_softmax-regression-scratch.md&#34;&gt;3.6 softmaxÂõûÂΩíÁöÑ‰ªéÈõ∂ÂºÄÂßãÂÆûÁé∞&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter03_DL-basics/3.7_softmax-regression-pytorch.md&#34;&gt;3.7 softmaxÂõûÂΩíÁöÑÁÆÄÊ¥ÅÂÆûÁé∞&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter03_DL-basics/3.8_mlp.md&#34;&gt;3.8 Â§öÂ±ÇÊÑüÁü•Êú∫&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter03_DL-basics/3.9_mlp-scratch.md&#34;&gt;3.9 Â§öÂ±ÇÊÑüÁü•Êú∫ÁöÑ‰ªéÈõ∂ÂºÄÂßãÂÆûÁé∞&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter03_DL-basics/3.10_mlp-pytorch.md&#34;&gt;3.10 Â§öÂ±ÇÊÑüÁü•Êú∫ÁöÑÁÆÄÊ¥ÅÂÆûÁé∞&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter03_DL-basics/3.11_underfit-overfit.md&#34;&gt;3.11 Ê®°ÂûãÈÄâÊã©„ÄÅÊ¨†ÊãüÂêàÂíåËøáÊãüÂêà&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter03_DL-basics/3.12_weight-decay.md&#34;&gt;3.12 ÊùÉÈáçË°∞Âáè&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter03_DL-basics/3.13_dropout.md&#34;&gt;3.13 ‰∏¢ÂºÉÊ≥ï&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter03_DL-basics/3.14_backprop.md&#34;&gt;3.14 Ê≠£Âêë‰º†Êí≠„ÄÅÂèçÂêë‰º†Êí≠ÂíåËÆ°ÁÆóÂõæ&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter03_DL-basics/3.15_numerical-stability-and-init.md&#34;&gt;3.15 Êï∞ÂÄºÁ®≥ÂÆöÊÄßÂíåÊ®°ÂûãÂàùÂßãÂåñ&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter03_DL-basics/3.16_kaggle-house-price.md&#34;&gt;3.16 ÂÆûÊàòKaggleÊØîËµõÔºöÊàø‰ª∑È¢ÑÊµã&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;4. Ê∑±Â∫¶Â≠¶‰π†ËÆ°ÁÆó &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter04_DL_computation/4.1_model-construction.md&#34;&gt;4.1 Ê®°ÂûãÊûÑÈÄ†&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter04_DL_computation/4.2_parameters.md&#34;&gt;4.2 Ê®°ÂûãÂèÇÊï∞ÁöÑËÆøÈóÆ„ÄÅÂàùÂßãÂåñÂíåÂÖ±‰∫´&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter04_DL_computation/4.3_deferred-init.md&#34;&gt;4.3 Ê®°ÂûãÂèÇÊï∞ÁöÑÂª∂ÂêéÂàùÂßãÂåñ&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter04_DL_computation/4.4_custom-layer.md&#34;&gt;4.4 Ëá™ÂÆö‰πâÂ±Ç&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter04_DL_computation/4.5_read-write.md&#34;&gt;4.5 ËØªÂèñÂíåÂ≠òÂÇ®&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter04_DL_computation/4.6_use-gpu.md&#34;&gt;4.6 GPUËÆ°ÁÆó&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;5. Âç∑ÁßØÁ•ûÁªèÁΩëÁªú &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter05_CNN/5.1_conv-layer.md&#34;&gt;5.1 ‰∫åÁª¥Âç∑ÁßØÂ±Ç&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter05_CNN/5.2_padding-and-strides.md&#34;&gt;5.2 Â°´ÂÖÖÂíåÊ≠•ÂπÖ&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter05_CNN/5.3_channels.md&#34;&gt;5.3 Â§öËæìÂÖ•ÈÄöÈÅìÂíåÂ§öËæìÂá∫ÈÄöÈÅì&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter05_CNN/5.4_pooling.md&#34;&gt;5.4 Ê±†ÂåñÂ±Ç&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter05_CNN/5.5_lenet.md&#34;&gt;5.5 Âç∑ÁßØÁ•ûÁªèÁΩëÁªúÔºàLeNetÔºâ&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter05_CNN/5.6_alexnet.md&#34;&gt;5.6 Ê∑±Â∫¶Âç∑ÁßØÁ•ûÁªèÁΩëÁªúÔºàAlexNetÔºâ&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter05_CNN/5.7_vgg.md&#34;&gt;5.7 ‰ΩøÁî®ÈáçÂ§çÂÖÉÁ¥†ÁöÑÁΩëÁªúÔºàVGGÔºâ&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter05_CNN/5.8_nin.md&#34;&gt;5.8 ÁΩëÁªú‰∏≠ÁöÑÁΩëÁªúÔºàNiNÔºâ&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter05_CNN/5.9_googlenet.md&#34;&gt;5.9 Âê´Âπ∂Ë°åËøûÁªìÁöÑÁΩëÁªúÔºàGoogLeNetÔºâ&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter05_CNN/5.10_batch-norm.md&#34;&gt;5.10 ÊâπÈáèÂΩí‰∏ÄÂåñ&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter05_CNN/5.11_resnet.md&#34;&gt;5.11 ÊÆãÂ∑ÆÁΩëÁªúÔºàResNetÔºâ&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter05_CNN/5.12_densenet.md&#34;&gt;5.12 Á®†ÂØÜËøûÊé•ÁΩëÁªúÔºàDenseNetÔºâ&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;6. Âæ™ÁéØÁ•ûÁªèÁΩëÁªú &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter06_RNN/6.1_lang-model.md&#34;&gt;6.1 ËØ≠Ë®ÄÊ®°Âûã&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter06_RNN/6.2_rnn.md&#34;&gt;6.2 Âæ™ÁéØÁ•ûÁªèÁΩëÁªú&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter06_RNN/6.3_lang-model-dataset.md&#34;&gt;6.3 ËØ≠Ë®ÄÊ®°ÂûãÊï∞ÊçÆÈõÜÔºàÂë®Êù∞‰º¶‰∏ìËæëÊ≠åËØçÔºâ&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter06_RNN/6.4_rnn-scratch.md&#34;&gt;6.4 Âæ™ÁéØÁ•ûÁªèÁΩëÁªúÁöÑ‰ªéÈõ∂ÂºÄÂßãÂÆûÁé∞&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter06_RNN/6.5_rnn-pytorch.md&#34;&gt;6.5 Âæ™ÁéØÁ•ûÁªèÁΩëÁªúÁöÑÁÆÄÊ¥ÅÂÆûÁé∞&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter06_RNN/6.6_bptt.md&#34;&gt;6.6 ÈÄöËøáÊó∂Èó¥ÂèçÂêë‰º†Êí≠&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter06_RNN/6.7_gru.md&#34;&gt;6.7 Èó®ÊéßÂæ™ÁéØÂçïÂÖÉÔºàGRUÔºâ&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter06_RNN/6.8_lstm.md&#34;&gt;6.8 ÈïøÁü≠ÊúüËÆ∞ÂøÜÔºàLSTMÔºâ&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter06_RNN/6.9_deep-rnn.md&#34;&gt;6.9 Ê∑±Â∫¶Âæ™ÁéØÁ•ûÁªèÁΩëÁªú&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter06_RNN/6.10_bi-rnn.md&#34;&gt;6.10 ÂèåÂêëÂæ™ÁéØÁ•ûÁªèÁΩëÁªú&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;7. ‰ºòÂåñÁÆóÊ≥ï &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter07_optimization/7.1_optimization-intro.md&#34;&gt;7.1 ‰ºòÂåñ‰∏éÊ∑±Â∫¶Â≠¶‰π†&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter07_optimization/7.2_gd-sgd.md&#34;&gt;7.2 Ê¢ØÂ∫¶‰∏ãÈôçÂíåÈöèÊú∫Ê¢ØÂ∫¶‰∏ãÈôç&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter07_optimization/7.3_minibatch-sgd.md&#34;&gt;7.3 Â∞èÊâπÈáèÈöèÊú∫Ê¢ØÂ∫¶‰∏ãÈôç&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter07_optimization/7.4_momentum.md&#34;&gt;7.4 Âä®ÈáèÊ≥ï&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter07_optimization/7.5_adagrad.md&#34;&gt;7.5 AdaGradÁÆóÊ≥ï&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter07_optimization/7.6_rmsprop.md&#34;&gt;7.6 RMSPropÁÆóÊ≥ï&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter07_optimization/7.7_adadelta.md&#34;&gt;7.7 AdaDeltaÁÆóÊ≥ï&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter07_optimization/7.8_adam.md&#34;&gt;7.8 AdamÁÆóÊ≥ï&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;8. ËÆ°ÁÆóÊÄßËÉΩ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter08_computational-performance/8.1_hybridize.md&#34;&gt;8.1 ÂëΩ‰ª§ÂºèÂíåÁ¨¶Âè∑ÂºèÊ∑∑ÂêàÁºñÁ®ã&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter08_computational-performance/8.2_async-computation.md&#34;&gt;8.2 ÂºÇÊ≠•ËÆ°ÁÆó&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter08_computational-performance/8.3_auto-parallelism.md&#34;&gt;8.3 Ëá™Âä®Âπ∂Ë°åËÆ°ÁÆó&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter08_computational-performance/8.4_multiple-gpus.md&#34;&gt;8.4 Â§öGPUËÆ°ÁÆó&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;9. ËÆ°ÁÆóÊú∫ËßÜËßâ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter09_computer-vision/9.1_image-augmentation.md&#34;&gt;9.1 ÂõæÂÉèÂ¢ûÂπø&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter09_computer-vision/9.2_fine-tuning.md&#34;&gt;9.2 ÂæÆË∞É&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter09_computer-vision/9.3_bounding-box.md&#34;&gt;9.3 ÁõÆÊ†áÊ£ÄÊµãÂíåËæπÁïåÊ°Ü&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter09_computer-vision/9.4_anchor.md&#34;&gt;9.4 ÈîöÊ°Ü&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter09_computer-vision/9.5_multiscale-object-detection.md&#34;&gt;9.5 Â§öÂ∞∫Â∫¶ÁõÆÊ†áÊ£ÄÊµã&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter09_computer-vision/9.6_object-detection-dataset.md&#34;&gt;9.6 ÁõÆÊ†áÊ£ÄÊµãÊï∞ÊçÆÈõÜÔºàÁöÆÂç°‰∏òÔºâ&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 9.7 ÂçïÂèëÂ§öÊ°ÜÊ£ÄÊµãÔºàSSDÔºâ&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter09_computer-vision/9.8_rcnn.md&#34;&gt;9.8 Âå∫ÂüüÂç∑ÁßØÁ•ûÁªèÁΩëÁªúÔºàR-CNNÔºâÁ≥ªÂàó&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter09_computer-vision/9.9_semantic-segmentation-and-dataset.md&#34;&gt;9.9 ËØ≠‰πâÂàÜÂâ≤ÂíåÊï∞ÊçÆÈõÜ&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 9.10 ÂÖ®Âç∑ÁßØÁΩëÁªúÔºàFCNÔºâ&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter09_computer-vision/9.11_neural-style.md&#34;&gt;9.11 Ê†∑ÂºèËøÅÁßª&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 9.12 ÂÆûÊàòKaggleÊØîËµõÔºöÂõæÂÉèÂàÜÁ±ªÔºàCIFAR-10Ôºâ&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 9.13 ÂÆûÊàòKaggleÊØîËµõÔºöÁãóÁöÑÂìÅÁßçËØÜÂà´ÔºàImageNet DogsÔºâ&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;10. Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter10_natural-language-processing/10.1_word2vec.md&#34;&gt;10.1 ËØçÂµåÂÖ•Ôºàword2vecÔºâ&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter10_natural-language-processing/10.2_approx-training.md&#34;&gt;10.2 Ëøë‰ººËÆ≠ÁªÉ&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter10_natural-language-processing/10.3_word2vec-pytorch.md&#34;&gt;10.3 word2vecÁöÑÂÆûÁé∞&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter10_natural-language-processing/10.4_fasttext.md&#34;&gt;10.4 Â≠êËØçÂµåÂÖ•ÔºàfastTextÔºâ&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter10_natural-language-processing/10.5_glove.md&#34;&gt;10.5 ÂÖ®Â±ÄÂêëÈáèÁöÑËØçÂµåÂÖ•ÔºàGloVeÔºâ&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter10_natural-language-processing/10.6_similarity-analogy.md&#34;&gt;10.6 Ê±ÇËøë‰πâËØçÂíåÁ±ªÊØîËØç&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter10_natural-language-processing/10.7_sentiment-analysis-rnn.md&#34;&gt;10.7 ÊñáÊú¨ÊÉÖÊÑüÂàÜÁ±ªÔºö‰ΩøÁî®Âæ™ÁéØÁ•ûÁªèÁΩëÁªú&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter10_natural-language-processing/10.8_sentiment-analysis-cnn.md&#34;&gt;10.8 ÊñáÊú¨ÊÉÖÊÑüÂàÜÁ±ªÔºö‰ΩøÁî®Âç∑ÁßØÁ•ûÁªèÁΩëÁªúÔºàtextCNNÔºâ&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter10_natural-language-processing/10.9_seq2seq.md&#34;&gt;10.9 ÁºñÁ†ÅÂô®‚ÄîËß£Á†ÅÂô®Ôºàseq2seqÔºâ&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter10_natural-language-processing/10.10_beam-search.md&#34;&gt;10.10 ÊùüÊêúÁ¥¢&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter10_natural-language-processing/10.11_attention.md&#34;&gt;10.11 Ê≥®ÊÑèÂäõÊú∫Âà∂&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShusenTang/Dive-into-DL-PyTorch/master/chapter10_natural-language-processing/10.12_machine-translation.md&#34;&gt;10.12 Êú∫Âô®ÁøªËØë&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;ÊåÅÁª≠Êõ¥Êñ∞‰∏≠......&lt;/p&gt; &#xA;&lt;h2&gt;Âéü‰π¶Âú∞ÂùÄ&lt;/h2&gt; &#xA;&lt;p&gt;‰∏≠ÊñáÁâàÔºö&lt;a href=&#34;https://zh.d2l.ai/&#34;&gt;Âä®ÊâãÂ≠¶Ê∑±Â∫¶Â≠¶‰π†&lt;/a&gt; | &lt;a href=&#34;https://github.com/d2l-ai/d2l-zh&#34;&gt;Github‰ªìÂ∫ì&lt;/a&gt;&lt;br&gt; English Version: &lt;a href=&#34;https://d2l.ai/&#34;&gt;Dive into Deep Learning&lt;/a&gt; | &lt;a href=&#34;https://github.com/d2l-ai/d2l-en&#34;&gt;Github Repo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ÂºïÁî®&lt;/h2&gt; &#xA;&lt;p&gt;Â¶ÇÊûúÊÇ®Âú®Á†îÁ©∂‰∏≠‰ΩøÁî®‰∫ÜËøô‰∏™È°πÁõÆËØ∑ÂºïÁî®Âéü‰π¶:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@book{zhang2019dive,&#xA;    title={Dive into Deep Learning},&#xA;    author={Aston Zhang and Zachary C. Lipton and Mu Li and Alexander J. Smola},&#xA;    note={\url{http://www.d2l.ai}},&#xA;    year={2020}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>google-research/google-research</title>
    <updated>2022-05-29T01:45:04Z</updated>
    <id>tag:github.com,2022-05-29:/google-research/google-research</id>
    <link href="https://github.com/google-research/google-research" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Google Research&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Google Research&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains code released by &lt;a href=&#34;https://research.google&#34;&gt;Google Research&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;All datasets in this repository are released under the CC BY 4.0 International license, which can be found here: &lt;a href=&#34;https://creativecommons.org/licenses/by/4.0/legalcode&#34;&gt;https://creativecommons.org/licenses/by/4.0/legalcode&lt;/a&gt;. All source files in this repository are released under the Apache 2.0 license, the text of which can be found in the LICENSE file.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Because the repo is large, we recommend you download only the subdirectory of interest:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;SUBDIR=foo&#xA;svn export https://github.com/google-research/google-research/trunk/$SUBDIR&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you&#39;d like to submit a pull request, you&#39;ll need to clone the repository; we recommend making a shallow clone (without history).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone git@github.com:google-research/google-research.git --depth=1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;em&gt;Disclaimer: This is not an official Google product.&lt;/em&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>plotly/dash-sample-apps</title>
    <updated>2022-05-29T01:45:04Z</updated>
    <id>tag:github.com,2022-05-29:/plotly/dash-sample-apps</id>
    <link href="https://github.com/plotly/dash-sample-apps" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Open-source demos hosted on Dash Gallery&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Dash Sample Apps&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://circleci.com/gh/plotly/dash-sample-apps&#34;&gt;&lt;img src=&#34;https://circleci.com/gh/plotly/dash-sample-apps.svg?style=svg&#34; alt=&#34;CircleCI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository hosts the code for over 100 open-source Dash apps written in Python or R. They can serve as a starting point for your own Dash app, as a learning tool to better understand how Dash works, as a reusable templates, and much more.&lt;/p&gt; &#xA;&lt;p&gt;Most apps in this repository are hosted on &lt;a href=&#34;https://dash-gallery.plotly.host/&#34;&gt;Dash Gallery&lt;/a&gt;, which is our internal server running on &lt;a href=&#34;https://plotly.com/dash/kubernetes/&#34;&gt;Dash Enterprise Kubernetes&lt;/a&gt;. Note that you can find both open-sourced apps and demos for our &lt;a href=&#34;https://plotly.com/dash/&#34;&gt;licensed products&lt;/a&gt;, including &lt;a href=&#34;https://plotly.com/dash/design-kit/&#34;&gt;Design Kit&lt;/a&gt; and &lt;a href=&#34;https://plotly.com/dash/snapshot-engine/&#34;&gt;Snapshot Engine&lt;/a&gt;. If you are interested in learning more, don&#39;t hesitate to reach out to &lt;a href=&#34;https://plotly.com/get-demo/&#34;&gt;get a demo&lt;/a&gt;. If you want to only see the open-sourced apps, select the &lt;a href=&#34;https://dash-gallery.plotly.host/Portal/?search=%5BOpen%20Source%5D&#34;&gt;&#34;Open Source&#34; tag&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Downloading and running a single app&lt;/h2&gt; &#xA;&lt;p&gt;Visit the &lt;a href=&#34;https://github.com/plotly/dash-sample-apps/releases&#34;&gt;releases page&lt;/a&gt; and download and &lt;code&gt;unzip&lt;/code&gt; the app you want. Then &lt;code&gt;cd&lt;/code&gt; into the app directory and install its dependencies in a virtual environment in the following way:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m venv venv&#xA;source venv/bin/activate  # Windows: \venv\scripts\activate&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;then you can run the app:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Cloning this whole repository&lt;/h2&gt; &#xA;&lt;p&gt;To clone this repository, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/plotly/dash-sample-apps&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note this might take a long time since it copies over 100 apps available in the repo. If you just want to try one app, refer to the section above.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;To contribute to this repository, please see the &lt;a href=&#34;https://raw.githubusercontent.com/plotly/dash-sample-apps/main/CONTRIBUTING.md&#34;&gt;contributing guide&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Azure/Azure-Sentinel</title>
    <updated>2022-05-29T01:45:04Z</updated>
    <id>tag:github.com,2022-05-29:/Azure/Azure-Sentinel</id>
    <link href="https://github.com/Azure/Azure-Sentinel" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Cloud-native SIEM for intelligent security analytics for your entire enterprise.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Microsoft Sentinel and Microsoft 365 Defender&lt;/h1&gt; &#xA;&lt;p&gt;Welcome to the unified Microsoft Sentinel and Microsoft 365 Defender repository! This repository contains out of the box detections, exploration queries, hunting queries, workbooks, playbooks and much more to help you get ramped up with Microsoft Sentinel and provide you security content to secure your environment and hunt for threats. The hunting queries also include Microsoft 365 Defender hunting queries for advanced hunting scenarios in both Microsoft 365 Defender and Microsoft Sentinel. You can also submit to &lt;a href=&#34;https://github.com/Azure/Azure-Sentinel/issues&#34;&gt;issues&lt;/a&gt; for any samples or resources you would like to see here as you onboard to Microsoft Sentinel. This repository welcomes contributions and refer to this repository&#39;s &lt;a href=&#34;https://aka.ms/threathunters&#34;&gt;wiki&lt;/a&gt; to get started. For questions and feedback, please contact &lt;a href=&#34;https://raw.githubusercontent.com/Azure/Azure-Sentinel/master/AzureSentinel@microsoft.com&#34;&gt;AzureSentinel@microsoft.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Resources&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://go.microsoft.com/fwlink/?linkid=2073774&amp;amp;clcid=0x409&#34;&gt;Microsoft Sentinel documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/microsoft-365/security/defender/microsoft-365-defender?view=o365-worldwide&#34;&gt;Microsoft 365 Defender documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/securitywebinars&#34;&gt;Security Community Webinars&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://help.github.com/en#dotcom&#34;&gt;Getting started with GitHub&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We value your feedback. Here are some channels to help surface your questions or feedback:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;General product specific Q&amp;amp;A for SIEM and SOAR - Join in the &lt;a href=&#34;https://techcommunity.microsoft.com/t5/microsoft-sentinel/bd-p/MicrosoftSentinel&#34;&gt;Microsoft Sentinel Tech Community conversations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;General product specific Q&amp;amp;A for XDR - Join in the &lt;a href=&#34;https://techcommunity.microsoft.com/t5/microsoft-365-defender/bd-p/MicrosoftThreatProtection&#34;&gt;Microsoft 365 Defender Tech Community conversations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Product specific feature requests - Upvote or post new on &lt;a href=&#34;https://feedback.azure.com/d365community/forum/37638d17-0625-ec11-b6e6-000d3a4f07b8&#34;&gt;Microsoft Sentinel feedback forums&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Report product or contribution bugs - File a GitHub Issue using &lt;a href=&#34;https://github.com/Azure/Azure-Sentinel/issues/new?assignees=&amp;amp;labels=&amp;amp;template=bug_report.md&amp;amp;title=&#34;&gt;Bug template&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;General feedback on community and contribution process - File a GitHub Issue using &lt;a href=&#34;https://github.com/Azure/Azure-Sentinel/issues/new?assignees=&amp;amp;labels=&amp;amp;template=feature_request.md&amp;amp;title=&#34;&gt;Feature Request template&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href=&#34;https://cla.microsoft.com&#34;&gt;https://cla.microsoft.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Add in your new or updated contributions to GitHub&lt;/h2&gt; &#xA;&lt;p&gt;Note: If you are a first time contributor to this repository, &lt;a href=&#34;https://docs.github.com/github/getting-started-with-github/fork-a-repo&#34;&gt;General GitHub Fork the repo guidance&lt;/a&gt; before cloning or &lt;a href=&#34;https://github.com/Azure/Azure-Sentinel/raw/master/GettingStarted.md&#34;&gt;Specific steps for the Sentinel repo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;General Steps&lt;/h2&gt; &#xA;&lt;p&gt;Brand new or update to a contribution via these methods:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Submit for review directly on GitHub website &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Browse to the folder you want to upload your file to&lt;/li&gt; &#xA;   &lt;li&gt;Choose Upload Files and browse to your file.&lt;/li&gt; &#xA;   &lt;li&gt;You will be required to create your own branch and then submit the Pull Request for review.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Use &lt;a href=&#34;https://help.github.com/en/desktop/getting-started-with-github-desktop&#34;&gt;GitHub Desktop&lt;/a&gt; or &lt;a href=&#34;https://visualstudio.microsoft.com/vs/&#34;&gt;Visual Studio&lt;/a&gt; or &lt;a href=&#34;https://code.visualstudio.com/?wt.mc_id=DX_841432&#34;&gt;VSCode&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://docs.github.com/github/getting-started-with-github/fork-a-repo&#34;&gt;Fork the repo&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://help.github.com/en/github/creating-cloning-and-archiving-repositories/cloning-a-repository&#34;&gt;Clone the repo&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://help.github.com/en/desktop/contributing-to-projects/creating-a-branch-for-your-work&#34;&gt;Create your own branch&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Do your additions/updates in GitHub Desktop&lt;/li&gt; &#xA;   &lt;li&gt;Be sure to merge master back to your branch before you push.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://help.github.com/en/github/using-git/pushing-commits-to-a-remote-repository&#34;&gt;Push your changes to GitHub&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Pull Request&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;After you push your changes, you will need to submit the &lt;a href=&#34;https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/about-pull-requests&#34;&gt;Pull Request (PR)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Details about the Proposed Changes are required, be sure to include a minimal level of detail so a review can clearly understand the reason for the change and what he change is related to in the code.&lt;/li&gt; &#xA; &lt;li&gt;After submission, check the &lt;a href=&#34;https://github.com/Azure/Azure-Sentinel/pulls&#34;&gt;Pull Request&lt;/a&gt; for comments&lt;/li&gt; &#xA; &lt;li&gt;Make changes as suggested and update your branch or explain why no change is needed. Resolve the comment when done.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Pull Request Detection Template Structure Validation Check&lt;/h3&gt; &#xA;&lt;p&gt;As part of the PR checks we run a structure validation to make sure all required parts of the YAML structure are included. For Detections, there is a new section that must be included. See the &lt;a href=&#34;https://github.com/Azure/Azure-Sentinel/wiki/Contribute-to-Sentinel-GitHub-Community-of-Queries#now-onto-the-how&#34;&gt;contribution guidelines&lt;/a&gt; for more information. If this section or any other required section is not included, then a validation error will occur similar to the below. The example is specifically if the YAML is missing the entityMappings section:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;A total of 1 test files matched the specified pattern.&#xA;[xUnit.net 00:00:00.95]     Kqlvalidations.Tests.DetectionTemplateStructureValidationTests.Validate_DetectionTemplates_HaveValidTemplateStructure(detectionsYamlFileName: &#34;ExcessiveBlockedTrafficGeneratedbyUser.yaml&#34;) [FAIL]&#xA;  X Kqlvalidations.Tests.DetectionTemplateStructureValidationTests.Validate_DetectionTemplates_HaveValidTemplateStructure(detectionsYamlFileName: &#34;ExcessiveBlockedTrafficGeneratedbyUser.yaml&#34;) [104ms]&#xA;  Error Message:&#xA;   Expected object to be &amp;lt;null&amp;gt;, but found System.ComponentModel.DataAnnotations.ValidationException with message &#34;An old mapping for entity &#39;AccountCustomEntity&#39; does not have a matching new mapping entry.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Pull Request Kql Validation Check&lt;/h3&gt; &#xA;&lt;p&gt;As part of the PR checks we run a syntax validation of the kql queries defined in the template. If this check fails go to Azure Pipeline (by pressing on the errors link on the checks tab in your PR) &lt;img src=&#34;https://raw.githubusercontent.com/Azure/Azure-Sentinel/master/.github/Media/Azurepipeline.png&#34; alt=&#34;Azurepipeline&#34;&gt; In the pipeline you can see which test failed and what is the cause: &lt;img src=&#34;https://raw.githubusercontent.com/Azure/Azure-Sentinel/master/.github/Media/PipelineTestsTab.png&#34; alt=&#34;Pipeline Tests Tab&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Example error message:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;A total of 1 test files matched the specified pattern.&#xA;[xUnit.net 00:00:01.81]     Kqlvalidations.Tests.KqlValidationTests.Validate_DetectionQueries_HaveValidKql(detectionsYamlFileName: &#34;ExcessiveBlockedTrafficGeneratedbyUser.yaml&#34;) [FAIL]&#xA;  X Kqlvalidations.Tests.KqlValidationTests.Validate_DetectionQueries_HaveValidKql(detectionsYamlFileName: &#34;ExcessiveBlockedTrafficGeneratedbyUser.yaml&#34;) [21ms]&#xA;  Error Message:&#xA;   Template Id:fa0ab69c-7124-4f62-acdd-61017cf6ce89 is not valid Errors:The name &#39;SymantecEndpointProtection&#39; does not refer to any known table, tabular variable or function., Code: &#39;KS204&#39;, Severity: &#39;Error&#39;, Location: &#39;67..93&#39;,The name &#39;SymantecEndpointProtection&#39; does not refer to any known table, tabular variable or function., Code: &#39;KS204&#39;, Severity: &#39;Error&#39;, Location: &#39;289..315&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are using custom logs table (a table which is not defined on all workspaces by default) you should verify your table schema is defined in json file in the folder &lt;em&gt;Azure-Sentinel\.script\tests\KqlvalidationsTests\CustomTables&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example for table tablexyz.json&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;Name&#34;: &#34;tablexyz&#34;,&#xA;  &#34;Properties&#34;: [&#xA;    {&#xA;      &#34;Name&#34;: &#34;SomeDateTimeColumn&#34;,&#xA;      &#34;Type&#34;: &#34;DateTime&#34;&#xA;    },&#xA;    {&#xA;      &#34;Name&#34;: &#34;SomeStringColumn&#34;,&#xA;      &#34;Type&#34;: &#34;String&#34;&#xA;    },&#xA;    {&#xA;      &#34;Name&#34;: &#34;SomeDynamicColumn&#34;,&#xA;      &#34;Type&#34;: &#34;Dynamic&#34;&#xA;    }&#xA;  ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Run Kql Validation Locally&lt;/h3&gt; &#xA;&lt;p&gt;In order to run the kql validation before submitting Pull Request in you local machine:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You need to have &lt;strong&gt;.Net Core 3.1 SDK&lt;/strong&gt; installed &lt;a href=&#34;https://dotnet.microsoft.com/download&#34;&gt;How to download .Net&lt;/a&gt; (Supports all platforms)&lt;/li&gt; &#xA; &lt;li&gt;Open Shell and navigate to &lt;code&gt;Azure-Sentinel\\.script\tests\KqlvalidationsTests\&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Execute &lt;code&gt;dotnet test&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Example of output (in Ubuntu):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Welcome to .NET Core 3.1!&#xA;---------------------&#xA;SDK Version: 3.1.403&#xA;&#xA;Telemetry&#xA;---------&#xA;The .NET Core tools collect usage data in order to help us improve your experience. The data is anonymous. It is collected by Microsoft and shared with the community. You can opt-out of telemetry by setting the DOTNET_CLI_TELEMETRY_OPTOUT environment variable to &#39;1&#39; or &#39;true&#39; using your favorite shell.&#xA;&#xA;Read more about .NET Core CLI Tools telemetry: https://aka.ms/dotnet-cli-telemetry&#xA;&#xA;----------------&#xA;Explore documentation: https://aka.ms/dotnet-docs&#xA;Report issues and find source on GitHub: https://github.com/dotnet/core&#xA;Find out what&#39;s new: https://aka.ms/dotnet-whats-new&#xA;Learn about the installed HTTPS developer cert: https://aka.ms/aspnet-core-https&#xA;Use &#39;dotnet --help&#39; to see available commands or visit: https://aka.ms/dotnet-cli-docs&#xA;Write your first app: https://aka.ms/first-net-core-app&#xA;--------------------------------------------------------------------------------------&#xA;Test run for /mnt/c/git/Azure-Sentinel/.script/tests/KqlvalidationsTests/bin/Debug/netcoreapp3.1/Kqlvalidations.Tests.dll(.NETCoreApp,Version=v3.1)&#xA;Microsoft (R) Test Execution Command Line Tool Version 16.7.0&#xA;Copyright (c) Microsoft Corporation.  All rights reserved.&#xA;&#xA;Starting test execution, please wait...&#xA;&#xA;A total of 1 test files matched the specified pattern.&#xA;&#xA;Test Run Successful.&#xA;Total tests: 171&#xA;     Passed: 171&#xA; Total time: 25.7973 Seconds&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Detection schema validation tests&lt;/h3&gt; &#xA;&lt;p&gt;Similarly to KQL Validation, there is an automatic validation of the schema of a detection. The schema validation includes the detection&#39;s frequency and period, the detection&#39;s trigger type and threshold, validity of connectors Ids (&lt;a href=&#34;https://github.com/Azure/Azure-Sentinel/raw/master/.script/tests/detectionTemplateSchemaValidation/ValidConnectorIds.json&#34;&gt;valid connectors Ids list&lt;/a&gt;), etc. A wrong format or missing attributes will result with an informative check failure, which should guide you through the resolution of the issue, but make sure to look into the format of already approved detection.&lt;/p&gt; &#xA;&lt;h3&gt;Run Detection Schema Validation Locally&lt;/h3&gt; &#xA;&lt;p&gt;In order to run the kql validation before submitting Pull Request in you local machine:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You need to have &lt;strong&gt;.Net Core 3.1 SDK&lt;/strong&gt; installed &lt;a href=&#34;https://dotnet.microsoft.com/download&#34;&gt;How to download .Net&lt;/a&gt; (Supports all platforms)&lt;/li&gt; &#xA; &lt;li&gt;Open Shell and navigate to &lt;code&gt;Azure-Sentinel\\.script\tests\DetectionTemplateSchemaValidation\&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Execute &lt;code&gt;dotnet test&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;When you submit a pull request, a CLA-bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; &#xA;&lt;p&gt;For information on what you can contribute and further details, refer to the &lt;a href=&#34;https://github.com/Azure/Azure-Sentinel/wiki#get-started&#34;&gt;&#34;get started&#34;&lt;/a&gt; section on the project&#39;s &lt;a href=&#34;https://aka.ms/threathunters&#34;&gt;wiki&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>NVIDIA/NeMo</title>
    <updated>2022-05-29T01:45:04Z</updated>
    <id>tag:github.com,2022-05-29:/NVIDIA/NeMo</id>
    <link href="https://github.com/NVIDIA/NeMo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;NeMo: a toolkit for conversational AI&lt;/p&gt;&lt;hr&gt;&lt;p&gt;|status| |documentation| |license| |lgtm_grade| |lgtm_alerts| |black|&lt;/p&gt; &#xA;&lt;p&gt;.. |status| image:: &lt;a href=&#34;http://www.repostatus.org/badges/latest/active.svg&#34;&gt;http://www.repostatus.org/badges/latest/active.svg&lt;/a&gt; :target: &lt;a href=&#34;http://www.repostatus.org/#active&#34;&gt;http://www.repostatus.org/#active&lt;/a&gt; :alt: Project Status: Active ‚Äì The project has reached a stable, usable state and is being actively developed.&lt;/p&gt; &#xA;&lt;p&gt;.. |documentation| image:: &lt;a href=&#34;https://readthedocs.com/projects/nvidia-nemo/badge/?version=main&#34;&gt;https://readthedocs.com/projects/nvidia-nemo/badge/?version=main&lt;/a&gt; :alt: Documentation :target: &lt;a href=&#34;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/&#34;&gt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;.. |license| image:: &lt;a href=&#34;https://img.shields.io/badge/License-Apache%202.0-brightgreen.svg&#34;&gt;https://img.shields.io/badge/License-Apache%202.0-brightgreen.svg&lt;/a&gt; :target: &lt;a href=&#34;https://github.com/NVIDIA/NeMo/raw/master/LICENSE&#34;&gt;https://github.com/NVIDIA/NeMo/blob/master/LICENSE&lt;/a&gt; :alt: NeMo core license and license for collections in this repo&lt;/p&gt; &#xA;&lt;p&gt;.. |lgtm_grade| image:: &lt;a href=&#34;https://img.shields.io/lgtm/grade/python/g/NVIDIA/NeMo.svg?logo=lgtm&amp;amp;logoWidth=18&#34;&gt;https://img.shields.io/lgtm/grade/python/g/NVIDIA/NeMo.svg?logo=lgtm&amp;amp;logoWidth=18&lt;/a&gt; :target: &lt;a href=&#34;https://lgtm.com/projects/g/NVIDIA/NeMo/context:python&#34;&gt;https://lgtm.com/projects/g/NVIDIA/NeMo/context:python&lt;/a&gt; :alt: Language grade: Python&lt;/p&gt; &#xA;&lt;p&gt;.. |lgtm_alerts| image:: &lt;a href=&#34;https://img.shields.io/lgtm/alerts/g/NVIDIA/NeMo.svg?logo=lgtm&amp;amp;logoWidth=18&#34;&gt;https://img.shields.io/lgtm/alerts/g/NVIDIA/NeMo.svg?logo=lgtm&amp;amp;logoWidth=18&lt;/a&gt; :target: &lt;a href=&#34;https://lgtm.com/projects/g/NVIDIA/NeMo/alerts/&#34;&gt;https://lgtm.com/projects/g/NVIDIA/NeMo/alerts/&lt;/a&gt; :alt: Total alerts&lt;/p&gt; &#xA;&lt;p&gt;.. |black| image:: &lt;a href=&#34;https://img.shields.io/badge/code%20style-black-000000.svg&#34;&gt;https://img.shields.io/badge/code%20style-black-000000.svg&lt;/a&gt; :target: &lt;a href=&#34;https://github.com/psf/black&#34;&gt;https://github.com/psf/black&lt;/a&gt; :alt: Code style: black&lt;/p&gt; &#xA;&lt;p&gt;.. _main-readme:&lt;/p&gt; &#xA;&lt;h1&gt;&lt;strong&gt;NVIDIA NeMo&lt;/strong&gt;&lt;/h1&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;NVIDIA NeMo is a conversational AI toolkit built for researchers working on automatic speech recognition (ASR), natural language processing (NLP), and text-to-speech synthesis (TTS). The primary objective of NeMo is to help researchers from industry and academia to reuse prior work (code and pretrained models) and make it easier to create new &lt;code&gt;conversational AI models &amp;lt;https://developer.nvidia.com/conversational-ai#started&amp;gt;&lt;/code&gt;_.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Pre-trained NeMo models. &amp;lt;https://catalog.ngc.nvidia.com/models?query=nemo&amp;amp;orderBy=weightPopularDESC&amp;gt;&lt;/code&gt;_&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Introductory video. &amp;lt;https://www.youtube.com/embed/wBgpMf_KQVw&amp;gt;&lt;/code&gt;_&lt;/p&gt; &#xA;&lt;h2&gt;Key Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speech processing &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;Automatic Speech Recognition (ASR) &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/intro.html&amp;gt;&lt;/code&gt;_ &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Supported models: Jasper, QuartzNet, CitriNet, Conformer-CTC, Conformer-Transducer, ContextNet, LSTM-Transducer (RNNT), LSTM-CTC, ...&lt;/li&gt; &#xA;     &lt;li&gt;Supports CTC and Transducer/RNNT losses/decoders&lt;/li&gt; &#xA;     &lt;li&gt;Beam Search decoding&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;Language Modelling for ASR &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/asr_language_modeling.html&amp;gt;&lt;/code&gt;_: N-gram LM in fusion with Beam Search decoding, Neural Rescoring with Transformer&lt;/li&gt; &#xA;     &lt;li&gt;Streaming and Buffered ASR (CTC/Transducer) - &lt;code&gt;Chunked Inference Examples &amp;lt;https://github.com/NVIDIA/NeMo/tree/main/examples/asr/asr_chunked_inference&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Speech Classification and Speech Command Recognition &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/speech_classification/intro.html&amp;gt;&lt;/code&gt;_: MatchboxNet (Command Recognition)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Voice activity Detection (VAD) &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/asr/speech_classification/models.html#marblenet-vad&amp;gt;&lt;/code&gt;_: MarbleNet&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Speaker Recognition &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/speaker_recognition/intro.html&amp;gt;&lt;/code&gt;_: TitaNet, ECAPA_TDNN, SpeakerNet&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Speaker Diarization &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/speaker_diarization/intro.html&amp;gt;&lt;/code&gt;_: TitaNet, ECAPA_TDNN, SpeakerNet&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Pretrained models on different languages. &amp;lt;https://ngc.nvidia.com/catalog/collections/nvidia:nemo_asr&amp;gt;&lt;/code&gt;_: English, Spanish, German, Russian, Chinese, French, Italian, Polish, ...&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;NGC collection of pre-trained speech processing models. &amp;lt;https://ngc.nvidia.com/catalog/collections/nvidia:nemo_asr&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Natural Language Processing &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;NeMo Megatron pre-training of Large Language Models &amp;lt;https://developer.nvidia.com/nemo-megatron-early-access&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Neural Machine Translation (NMT) &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/machine_translation.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Punctuation and Capitalization &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/punctuation_and_capitalization.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Token classification (named entity recognition) &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/token_classification.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Text classification &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/text_classification.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Joint Intent and Slot Classification &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/joint_intent_slot.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Question answering &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/question_answering.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;GLUE benchmark &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/glue_benchmark.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Information retrieval &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/information_retrieval.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Entity Linking &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/entity_linking.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Dialogue State Tracking &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/sgd_qa.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Prompt Tuning &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/prompt_learning.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;NGC collection of pre-trained NLP models. &amp;lt;https://ngc.nvidia.com/catalog/collections/nvidia:nemo_nlp&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Speech synthesis (TTS) &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/tts/intro.html#&amp;gt;&lt;/code&gt;_ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Spectrogram generation: Tacotron2, GlowTTS, TalkNet, FastPitch, FastSpeech2, Mixer-TTS, Mixer-TTS-X&lt;/li&gt; &#xA;   &lt;li&gt;Vocoders: WaveGlow, SqueezeWave, UniGlow, MelGAN, HiFiGAN, UnivNet&lt;/li&gt; &#xA;   &lt;li&gt;End-to-end speech generation: FastPitch_HifiGan_E2E, FastSpeech2_HifiGan_E2E&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;NGC collection of pre-trained TTS models. &amp;lt;https://ngc.nvidia.com/catalog/collections/nvidia:nemo_tts&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Tools &amp;lt;https://github.com/NVIDIA/NeMo/tree/main/tools&amp;gt;&lt;/code&gt;_ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;Text Processing (text normalization and inverse text normalization) &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/text_normalization/intro.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;CTC-Segmentation tool &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/tools/ctc_segmentation.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Speech Data Explorer &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/tools/speech_data_explorer.html&amp;gt;&lt;/code&gt;_: a dash-based tool for interactive exploration of ASR/TTS datasets&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Built for speed, NeMo can utilize NVIDIA&#39;s Tensor Cores and scale out training to multiple GPUs and multiple nodes.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Python 3.8 or above&lt;/li&gt; &#xA; &lt;li&gt;Pytorch 1.10.0 or above&lt;/li&gt; &#xA; &lt;li&gt;NVIDIA GPU for training&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;.. |main| image:: &lt;a href=&#34;https://readthedocs.com/projects/nvidia-nemo/badge/?version=main&#34;&gt;https://readthedocs.com/projects/nvidia-nemo/badge/?version=main&lt;/a&gt; :alt: Documentation Status :scale: 100% :target: &lt;a href=&#34;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/&#34;&gt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;.. |stable| image:: &lt;a href=&#34;https://readthedocs.com/projects/nvidia-nemo/badge/?version=stable&#34;&gt;https://readthedocs.com/projects/nvidia-nemo/badge/?version=stable&lt;/a&gt; :alt: Documentation Status :scale: 100% :target: &lt;a href=&#34;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/&#34;&gt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;+---------+-------------+------------------------------------------------------------------------------------------------------------------------------------------+ | Version | Status | Description | +=========+=============+==========================================================================================================================================+ | Latest | |main| | &lt;code&gt;Documentation of the latest (i.e. main) branch. &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/&amp;gt;&lt;/code&gt;_ | +---------+-------------+------------------------------------------------------------------------------------------------------------------------------------------+ | Stable | |stable| | &lt;code&gt;Documentation of the stable (i.e. most recent release) branch. &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/&amp;gt;&lt;/code&gt;_ | +---------+-------------+------------------------------------------------------------------------------------------------------------------------------------------+&lt;/p&gt; &#xA;&lt;h2&gt;Tutorials&lt;/h2&gt; &#xA;&lt;p&gt;A great way to start with NeMo is by checking &lt;code&gt;one of our tutorials &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/starthere/tutorials.html&amp;gt;&lt;/code&gt;_.&lt;/p&gt; &#xA;&lt;h2&gt;Getting help with NeMo&lt;/h2&gt; &#xA;&lt;p&gt;FAQ can be found on NeMo&#39;s &lt;code&gt;Discussions board &amp;lt;https://github.com/NVIDIA/NeMo/discussions&amp;gt;&lt;/code&gt;_. You are welcome to ask questions or start discussions there.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Conda&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#xA;We recommend installing NeMo in a fresh Conda environment.&#xA;&#xA;.. code-block:: bash&#xA;&#xA;    conda create --name nemo python==3.8&#xA;    conda activate nemo&#xA;&#xA;Install PyTorch using their `configurator &amp;lt;https://pytorch.org/get-started/locally/&amp;gt;`_. &#xA;&#xA;.. code-block:: bash&#xA;&#xA;    conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch&#xA;&#xA;.. note::&#xA;&#xA;  The command used to install PyTorch may depend on your system.&#xA;&#xA;Pip&#xA;~~~&#xA;Use this installation mode if you want the latest released version.&#xA;&#xA;.. code-block:: bash&#xA;&#xA;    apt-get update &amp;amp;&amp;amp; apt-get install -y libsndfile1 ffmpeg&#xA;    pip install Cython&#xA;    pip install nemo_toolkit[&#39;all&#39;]&#xA;&#xA;.. note::&#xA;&#xA;    Depending on the shell used, you may need to use ``&#34;nemo_toolkit[all]&#34;`` instead in the above command.&#xA;&#xA;Pip from source&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Use this installation mode if you want the a version from particular GitHub branch (e.g main).&lt;/p&gt; &#xA;&lt;p&gt;.. code-block:: bash&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;apt-get update &amp;amp;&amp;amp; apt-get install -y libsndfile1 ffmpeg&#xA;pip install Cython&#xA;python -m pip install git+https://github.com/NVIDIA/NeMo.git@{BRANCH}#egg=nemo_toolkit[all]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;From source&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Use this installation mode if you are contributing to NeMo.&#xA;&#xA;.. code-block:: bash&#xA;&#xA;    apt-get update &amp;amp;&amp;amp; apt-get install -y libsndfile1 ffmpeg&#xA;    git clone https://github.com/NVIDIA/NeMo&#xA;    cd NeMo&#xA;    ./reinstall.sh&#xA;&#xA;.. note::&#xA;&#xA;    If you only want the toolkit without additional conda-based dependencies, you may replace ``reinstall.sh``&#xA;    with ``pip install -e .`` when your PWD is the root of the NeMo repository.&#xA;&#xA;RNNT&#xA;~~~~&#xA;Note that RNNT requires numba to be installed from conda.&#xA;&#xA;.. code-block:: bash&#xA;&#xA;  conda remove numba&#xA;  pip uninstall numba&#xA;  conda install -c conda-forge numba&#xA;&#xA;Megatron GPT&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Megatron GPT training requires NVIDIA Apex to be installed.&lt;/p&gt; &#xA;&lt;p&gt;.. code-block:: bash&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/NVIDIA/apex&#xA;cd apex&#xA;git checkout 9263bc8c6c16555bd55dd759f1a1b8c0cd187d10&#xA;pip install -v --disable-pip-version-check --no-cache-dir --global-option=&#34;--cpp_ext&#34; --global-option=&#34;--cuda_ext&#34; --global-option=&#34;--fast_layer_norm&#34; ./&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Docker containers:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;To build a nemo container with Dockerfile from a branch, please run &#xA;&#xA;.. code-block:: bash&#xA;&#xA;    DOCKER_BUILDKIT=1 docker build -f Dockerfile -t nemo:latest .&#xA;&#xA;&#xA;If you chose to work with main branch, we recommend using NVIDIA&#39;s PyTorch container version 22.04-py3 and then installing from GitHub.&#xA;&#xA;.. code-block:: bash&#xA;&#xA;    docker run --gpus all -it --rm -v &amp;lt;nemo_github_folder&amp;gt;:/NeMo --shm-size=8g \&#xA;    -p 8888:8888 -p 6006:6006 --ulimit memlock=-1 --ulimit \&#xA;    stack=67108864 --device=/dev/snd nvcr.io/nvidia/pytorch:22.04-py3&#xA;&#xA;Examples&#xA;--------&#xA;&#xA;Many examples can be found under `&#34;Examples&#34; &amp;lt;https://github.com/NVIDIA/NeMo/tree/stable/examples&amp;gt;`_ folder.&#xA;&#xA;&#xA;Contributing&#xA;------------&#xA;&#xA;We welcome community contributions! Please refer to the  `CONTRIBUTING.md &amp;lt;https://github.com/NVIDIA/NeMo/blob/stable/CONTRIBUTING.md&amp;gt;`_ CONTRIBUTING.md for the process.&#xA;&#xA;Publications&#xA;------------&#xA;&#xA;We provide an ever growing list of publications that utilize the NeMo framework. Please refer to `PUBLICATIONS.md &amp;lt;https://github.com/NVIDIA/NeMo/blob/main/PUBLICATIONS.md&amp;gt;`_. We welcome the addition of your own articles to this list !&#xA;&#xA;Citation&#xA;--------&#xA;&#xA;.. code-block:: bash&#xA;&#xA;  @article{kuchaiev2019nemo,&#xA;    title={Nemo: a toolkit for building ai applications using neural modules},&#xA;    author={Kuchaiev, Oleksii and Li, Jason and Nguyen, Huyen and Hrinchuk, Oleksii and Leary, Ryan and Ginsburg, Boris and Kriman, Samuel and Beliaev, Stanislav and Lavrukhin, Vitaly and Cook, Jack and others},&#xA;    journal={arXiv preprint arXiv:1909.09577},&#xA;    year={2019}&#xA;  }&#xA;&#xA;License&#xA;-------&#xA;NeMo is under `Apache 2.0 license &amp;lt;https://github.com/NVIDIA/NeMo/blob/stable/LICENSE&amp;gt;`_.&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>