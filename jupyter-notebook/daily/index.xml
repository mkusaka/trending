<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-29T01:31:43Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>iyaja/llama-fs</title>
    <updated>2024-05-29T01:31:43Z</updated>
    <id>tag:github.com,2024-05-29:/iyaja/llama-fs</id>
    <link href="https://github.com/iyaja/llama-fs" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A self-organizing file system with llama 3&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LlamaFS&lt;/h1&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/iyaja/llama-fs/main/electron-react-app/assets/llama_fs.png&#34; width=&#34;30%&#34;&gt; &#xA;&lt;h2&gt;Inspiration&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://x.com/AlexReibman/status/1789895425828204553&#34;&gt;Watch the explainer video&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Open your &lt;code&gt;~/Downloads&lt;/code&gt; directory. Or your Desktop. It&#39;s probably a mess...&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;There are only two hard things in Computer Science: cache invalidation and &lt;strong&gt;naming things&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;What it does&lt;/h2&gt; &#xA;&lt;p&gt;LlamaFS is a self-organizing file manager. It automatically renames and organizes your files based on their contents and well-known conventions (e.g., time). It supports many kinds of file, and even images (through Moondream) and audio (through Whisper).&lt;/p&gt; &#xA;&lt;p&gt;LlamaFS runs in two &#34;modes&#34; - as a batch job (batch mode), and an interactive daemon (watch mode).&lt;/p&gt; &#xA;&lt;p&gt;In batch mode, you can send a directory to LlamaFS, and it will return a suggested file structure and organize your files.&lt;/p&gt; &#xA;&lt;p&gt;In watch mode, LlamaFS starts a daemon that watches your directory. It intercepts all filesystem operations, updates i and uses your most recent edits in context to proactively learn and how, so you don&#39;t learns predict how you rename file. e.g. if you create a folder for 2023 tax documents, and start moving 1-3 file in it, LlamaFS will automatically creates, and move the right!&lt;/p&gt; &#xA;&lt;p&gt;Uhh... Sending all my personal files to an API provider?! No thank you!&lt;/p&gt; &#xA;&lt;p&gt;It also has a toggle for &#34;incognito mode&#34;, allowing you route every request through Ollama instead of Groq. Since they use the same Llama 3 model, the perform identically.&lt;/p&gt; &#xA;&lt;h2&gt;How we built it&lt;/h2&gt; &#xA;&lt;p&gt;We built LlamaFS on a Python backend, leveraging the Llama3 model through Groq for file content summarization and tree structuring. For local processing, we integrated Ollama running the same model to ensure privacy in incognito mode. The frontend is crafted with Electron, providing a sleek, user-friendly interface that allows users to interact with the suggested file structures before finalizing changes.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;It&#39;s extremely fast!&lt;/strong&gt; (for LLM standards)! Most file ops are processed in &amp;lt;500ms in watch mode (benchmarked by &lt;a href=&#34;https://agentops.ai/?utm_source=llama-fs&#34;&gt;AgentOps&lt;/a&gt;). This is because of our smart caching, that selectively rewrites sections of the index based on the minimum nessecary filesystem diff. And of course, Groq&#39;s super fast inference API. ðŸ˜‰&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;It&#39;s immediately useful&lt;/strong&gt; - It&#39;s very low friction to use, and a problem almost everyone has. We started using it ourselves on this project (very Meta)&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What&#39;s next for LlamaFS&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Find and remove old/unused files&lt;/li&gt; &#xA; &lt;li&gt;We have some really cool ideas for - filesystem diffs are hard...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;p&gt;Before installing, ensure you have the following requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.10 or higher&lt;/li&gt; &#xA; &lt;li&gt;pip (Python package installer)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installing&lt;/h3&gt; &#xA;&lt;p&gt;To install the project, follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone the repository:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/iyaja/llama-fs.git&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Navigate to the project directory:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd llama-fs&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install requirements&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;To serve the application locally using FastAPI, run the command&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;fastapi dev server.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will run the server by default on port 8000. The API can be queried using a curl command, and passing in the file path as the argument. For example, on the Downloads folder&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -X POST http://127.0.0.1:8000 \&#xA; -H &#34;Content-Type: application/json&#34; \&#xA; -d &#39;{&#34;path&#34;: &#34;/Users/&amp;lt;username&amp;gt;/Downloads/&#34;, &#34;instruction&#34;: &#34;string&#34;, &#34;incognito&#34;: false}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>WASasquatch/was-node-suite-comfyui</title>
    <updated>2024-05-29T01:31:43Z</updated>
    <id>tag:github.com,2024-05-29:/WASasquatch/was-node-suite-comfyui</id>
    <link href="https://github.com/WASasquatch/was-node-suite-comfyui" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An extensive node suite for ComfyUI with over 210 new nodes&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;strong&gt;WAS&lt;/strong&gt; Node Suite &amp;nbsp; &lt;a href=&#34;https://colab.research.google.com/github/WASasquatch/was-node-suite-comfyui/blob/main/ComfyUI_%2B_WAS_Node_Suite_and_ComfyUI_Manager.ipynb&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hits.seeyoufarm.com&#34;&gt;&lt;img src=&#34;https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2FWASasquatch%2Fwas-node-suite-comfyui&amp;amp;count_bg=%233D9CC8&amp;amp;title_bg=%23555555&amp;amp;icon=&amp;amp;icon_color=%23E7E7E7&amp;amp;title=hits&amp;amp;edge_flat=false&#34; alt=&#34;Hits&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paypal.me/ThompsonJordan?country.x=US&amp;amp;locale.x=en_US&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Donate-PayPal-blue.svg?sanitize=true&#34; alt=&#34;Donate&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/1151589/228982359-4a6215cc-3ca9-4c24-8a7b-d229d7bce277.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;A node suite for &lt;a href=&#34;https://github.com/comfyanonymous/ComfyUI&#34;&gt;ComfyUI&lt;/a&gt; with many new nodes, such as image processing, text processing, and more.&lt;/h3&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://github.com/WASasquatch/was-node-suite-comfyui/wiki/Workflow-Examples&#34;&gt;Share Workflows&lt;/a&gt; to the workflows wiki. Preferably embedded PNGs with workflows, but JSON is OK too. &lt;a href=&#34;https://colab.research.google.com/drive/1hQMjNUdhMQ3rw1Wcm3_umvmOMeS_K4s8?usp=sharing&#34;&gt;You can use this tool to add a workflow to a PNG file easily&lt;/a&gt;.&lt;/h4&gt; &#xA;&lt;h4&gt;Consider &lt;a href=&#34;https://paypal.me/ThompsonJordan?country.x=US&amp;amp;locale.x=en_US&#34;&gt;donating to the project&lt;/a&gt; to help it&#39;s continued development.&lt;/h4&gt; &#xA;&lt;h1&gt;Important Updates&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;12/15/2023&lt;/strong&gt; WAS-NS is not under active development. I do not have the time and have other obligations. Feel free to fork and continue the project. I will approve appropriate and beneficial PRs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[Updated 10/8/2023]&lt;/strong&gt; BLIP is now a shipped module of WAS-NS and no longer requires the BLIP Repo&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[Updated 5/29/2023]&lt;/strong&gt; &lt;code&gt;ASCII&lt;/code&gt; &lt;strong&gt;is deprecated&lt;/strong&gt;. The new preferred method of text node output is &lt;code&gt;STRING&lt;/code&gt;. This is a change from &lt;code&gt;ASCII&lt;/code&gt; so that it is more clear what data is being passed. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The &lt;code&gt;was_suite_config.json&lt;/code&gt; will automatically set &lt;code&gt;use_legacy_ascii_text&lt;/code&gt; to &lt;code&gt;false&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/WASasquatch/was-node-suite-comfyui#video-nodes&#34;&gt;Video Nodes&lt;/a&gt; - There are two new video nodes, &lt;code&gt;Write to Video&lt;/code&gt; and &lt;code&gt;Create Video from Path&lt;/code&gt;. These are experimental nodes.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Current Nodes:&lt;/h1&gt; &#xA;&lt;h3&gt;There is documentation from &lt;a href=&#34;https://getsalt.ai/&#34;&gt;Salt AI&lt;/a&gt; available here: &lt;a href=&#34;https://docs.getsalt.ai/md/was-node-suite-comfyui/&#34;&gt;https://docs.getsalt.ai/md/was-node-suite-comfyui/&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;$\Large\color{orange}{Expand\ Node\ List}$&lt;/summary&gt; &#xA; &lt;br&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;BLIP Model Loader: Load a BLIP model to input into the BLIP Analyze node&lt;/li&gt; &#xA;  &lt;li&gt;BLIP Analyze Image: Get a text caption from a image, or interrogate the image with a question. &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Model will download automatically from default URL, but you can point the download to another location/caption model in &lt;code&gt;was_suite_config&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;li&gt;Models will be stored in &lt;code&gt;ComfyUI/models/blip/checkpoints/&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;SAM Model Loader: Load a SAM Segmentation model&lt;/li&gt; &#xA;  &lt;li&gt;SAM Parameters: Define your SAM parameters for segmentation of a image&lt;/li&gt; &#xA;  &lt;li&gt;SAM Parameters Combine: Combine SAM parameters&lt;/li&gt; &#xA;  &lt;li&gt;SAM Image Mask: SAM image masking&lt;/li&gt; &#xA;  &lt;li&gt;Image Bounds: Bounds a image&lt;/li&gt; &#xA;  &lt;li&gt;Inset Image Bounds: Inset a image bounds&lt;/li&gt; &#xA;  &lt;li&gt;Bounded Image Blend: Blend bounds image&lt;/li&gt; &#xA;  &lt;li&gt;Bounded Image Blend with Mask: Blend a bounds image by mask&lt;/li&gt; &#xA;  &lt;li&gt;Bounded Image Crop: Crop a bounds image&lt;/li&gt; &#xA;  &lt;li&gt;Bounded Image Crop with Mask: Crop a bounds image by mask&lt;/li&gt; &#xA;  &lt;li&gt;Bus Node: condense the 5 common connectors into one, keep your workspace tidy (Model, Clip, VAE, Positive Conditioning, Negative Conditioning)&lt;/li&gt; &#xA;  &lt;li&gt;Cache Node: Cache Latnet, Tensor Batches (Image), and Conditioning to disk to use later.&lt;/li&gt; &#xA;  &lt;li&gt;CLIPTextEncode (NSP): Parse noodle soups from the NSP pantry, or parse wildcards from a directory containing A1111 style wildacrds. &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Wildcards are in the style of &lt;code&gt;__filename__&lt;/code&gt;, which also includes subdirectories like &lt;code&gt;__appearance/haircolour__&lt;/code&gt; (if you noodle_key is set to &lt;code&gt;__&lt;/code&gt;)&lt;/li&gt; &#xA;    &lt;li&gt;You can set a custom wildcards path in &lt;code&gt;was_suite_config.json&lt;/code&gt; file with key: &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;&lt;code&gt; &#34;wildcards_path&#34;: &#34;E:\\python\\automatic\\webui3\\stable-diffusion-webui\\extensions\\sd-dynamic-prompts\\wildcards&#34;&lt;/code&gt;&lt;/li&gt; &#xA;      &lt;li&gt;If no path is set the wildcards dir is located at the root of WAS Node Suite as &lt;code&gt;/wildcards&lt;/code&gt;&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;CLIP Input Switch: Switch between two CLIP inputs based on a boolean switch.&lt;/li&gt; &#xA;  &lt;li&gt;CLIP Vision Input Switch: Switch between two CLIP Vision inputs based on a boolean switch.&lt;/li&gt; &#xA;  &lt;li&gt;Conditioning Input Switch: Switch between two conditioning inputs.&lt;/li&gt; &#xA;  &lt;li&gt;Constant Number&lt;/li&gt; &#xA;  &lt;li&gt;Control Net Model Input Switch: Switch between two Control Net Model inputs based on a boolean switch.&lt;/li&gt; &#xA;  &lt;li&gt;Create Grid Image: Create a image grid from images at a destination with customizable glob pattern. Optional border size and color.&lt;/li&gt; &#xA;  &lt;li&gt;Create Grid Image from Batch: Create a grid image from a batch tensor of images.&lt;/li&gt; &#xA;  &lt;li&gt;Create Morph Image: Create a GIF/APNG animation from two images, fading between them.&lt;/li&gt; &#xA;  &lt;li&gt;Create Morph Image by Path: Create a GIF/APNG animation from a path to a directory containing images, with optional pattern.&lt;/li&gt; &#xA;  &lt;li&gt;Create Video from Path: Create video from images from a specified path.&lt;/li&gt; &#xA;  &lt;li&gt;CLIPSeg Masking: Mask a image with CLIPSeg and return a raw mask&lt;/li&gt; &#xA;  &lt;li&gt;CLIPSeg Masking Batch: Create a batch image (from image inputs) and batch mask with CLIPSeg&lt;/li&gt; &#xA;  &lt;li&gt;Dictionary to Console: Print a dictionary input to the console&lt;/li&gt; &#xA;  &lt;li&gt;Image Analyze &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Black White Levels&lt;/li&gt; &#xA;    &lt;li&gt;RGB Levels &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;Depends on &lt;code&gt;matplotlib&lt;/code&gt;, will attempt to install on first run&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;Diffusers Hub Down-Loader: Download a diffusers model from the HuggingFace Hub and load it&lt;/li&gt; &#xA;  &lt;li&gt;Image SSAO (Ambient Occlusion): [Expiremental Beta Node] Create Screen Space Ambient Occlusion with a image and MiDaS depth approximation (or provided depth map).&lt;/li&gt; &#xA;  &lt;li&gt;Image SSDO (Direct Occlusion): [Expiremental Beta Node] Create a Screen Space Direct Occlusion with a image input. Direct Occlusion presents you with direct lighting highliths, similar to how Ambient Occlusion finds the crevices and shadowy areas around objets.&lt;/li&gt; &#xA;  &lt;li&gt;Image Aspect Ratio: Fetch image aspect ratio in float format, common format (eg 16:9), and in if the image is portrait, landscape, or square.&lt;/li&gt; &#xA;  &lt;li&gt;Image Batch: Create one batch out of multiple batched tensors.&lt;/li&gt; &#xA;  &lt;li&gt;Image Blank: Create a blank image in any color&lt;/li&gt; &#xA;  &lt;li&gt;Image Blend by Mask: Blend two images by a mask&lt;/li&gt; &#xA;  &lt;li&gt;Image Blend: Blend two images by opacity&lt;/li&gt; &#xA;  &lt;li&gt;Image Blending Mode: Blend two images by various blending modes&lt;/li&gt; &#xA;  &lt;li&gt;Image Bloom Filter: Apply a high-pass based bloom filter&lt;/li&gt; &#xA;  &lt;li&gt;Image Canny Filter: Apply a canny filter to a image&lt;/li&gt; &#xA;  &lt;li&gt;Image Chromatic Aberration: Apply chromatic aberration lens effect to a image like in sci-fi films, movie theaters, and video games&lt;/li&gt; &#xA;  &lt;li&gt;Image Color Palette &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Generate a color palette based on the input image. &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;Depends on &lt;code&gt;scikit-learn&lt;/code&gt;, will attempt to install on first run.&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;li&gt;Supports color range of 8-256&lt;/li&gt; &#xA;    &lt;li&gt;Utilizes font in &lt;code&gt;./res/&lt;/code&gt; unless unavailable, then it will utilize internal better then nothing font.&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;Image Crop Face: Crop a face out of a image &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;strong&gt;Limitations:&lt;/strong&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;Sometimes no faces are found in badly generated images, or faces at angles&lt;/li&gt; &#xA;      &lt;li&gt;Sometimes face crop is black, this is because the padding is too large and intersected with the image edge. Use a smaller padding size.&lt;/li&gt; &#xA;      &lt;li&gt;face_recognition mode sometimes finds random things as faces. It also requires a [CUDA] GPU.&lt;/li&gt; &#xA;      &lt;li&gt;Only detects one face. This is a design choice to make it&#39;s use easy.&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;li&gt;&lt;strong&gt;Notes:&lt;/strong&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;Detection runs in succession. If nothing is found with the selected detection cascades, it will try the next available cascades file.&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;Image Crop Location: Crop a image to specified location in top, left, right, and bottom locations relating to the pixel dimensions of the image in X and Y coordinats.&lt;/li&gt; &#xA;  &lt;li&gt;Image Crop Square Location: Crop a location by X/Y center, creating a square crop around that point.&lt;/li&gt; &#xA;  &lt;li&gt;Image Displacement Warp: Warp a image by a displacement map image by a given amplitude.&lt;/li&gt; &#xA;  &lt;li&gt;Image Dragan Photography Filter: Apply a Andrzej Dragan photography style to a image&lt;/li&gt; &#xA;  &lt;li&gt;Image Edge Detection Filter: Detect edges in a image&lt;/li&gt; &#xA;  &lt;li&gt;Image Film Grain: Apply film grain to a image&lt;/li&gt; &#xA;  &lt;li&gt;Image Filter Adjustments: Apply various image adjustments to a image&lt;/li&gt; &#xA;  &lt;li&gt;Image Flip: Flip a image horizontal, or vertical&lt;/li&gt; &#xA;  &lt;li&gt;Image Gradient Map: Apply a gradient map to a image&lt;/li&gt; &#xA;  &lt;li&gt;Image Generate Gradient: Generate a gradient map with desired stops and colors&lt;/li&gt; &#xA;  &lt;li&gt;Image High Pass Filter: Apply a high frequency pass to the image returning the details&lt;/li&gt; &#xA;  &lt;li&gt;Image History Loader: Load images from history based on the Load Image Batch node. Can define max history in config file. &lt;em&gt;(requires restart to show last sessions files at this time)&lt;/em&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Image Input Switch: Switch between two image inputs based on a boolean switch&lt;/li&gt; &#xA;  &lt;li&gt;Image Levels Adjustment: Adjust the levels of a image&lt;/li&gt; &#xA;  &lt;li&gt;Image Load: Load a &lt;em&gt;image&lt;/em&gt; from any path on the system, or a url starting with &lt;code&gt;http&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Image Median Filter: Apply a median filter to a image, such as to smooth out details in surfaces&lt;/li&gt; &#xA;  &lt;li&gt;Image Mix RGB Channels: Mix together RGB channels into a single iamge&lt;/li&gt; &#xA;  &lt;li&gt;Image Monitor Effects Filter: Apply various monitor effects to a image &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Digital Distortion &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;A digital breakup distortion effect&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;li&gt;Signal Distortion &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;A analog signal distortion effect on vertical bands like a CRT monitor&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;li&gt;TV Distortion &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;A TV scanline and bleed distortion effect&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;Image Nova Filter: A image that uses a sinus frequency to break apart a image into RGB frequencies&lt;/li&gt; &#xA;  &lt;li&gt;Image Perlin Noise: Generate perlin noise&lt;/li&gt; &#xA;  &lt;li&gt;Image Perlin Power Fractal: Generate a perlin power fractal&lt;/li&gt; &#xA;  &lt;li&gt;Image Paste Face Crop: Paste face crop back on a image at it&#39;s original location and size &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Features a better blending funciton than GFPGAN/CodeFormer so there shouldn&#39;t be visible seams, and coupled with Diffusion Result, looks better than GFPGAN/CodeFormer.&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;Image Paste Crop: Paste a crop (such as from Image Crop Location) at it&#39;s original location and size utilizing the &lt;code&gt;crop_data&lt;/code&gt; node input. This uses a different blending algorithm then Image Paste Face Crop, which may be desired in certain instances.&lt;/li&gt; &#xA;  &lt;li&gt;Image Power Noise: Generate power-law noise &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;frequency: The frequency parameter controls the distribution of the noise across different frequencies. In the context of Fourier analysis, higher frequencies represent fine details or high-frequency components, while lower frequencies represent coarse details or low-frequency components. Adjusting the frequency parameter can result in different textures and levels of detail in the generated noise. The specific range and meaning of the frequency parameter may vary depending on the noise type.&lt;/li&gt; &#xA;    &lt;li&gt;attenuation: The attenuation parameter determines the strength or intensity of the noise. It controls how much the noise values deviate from the mean or central value. Higher values of attenuation lead to more significant variations and a stronger presence of noise, while lower values result in a smoother and less noticeable noise. The specific range and interpretation of the attenuation parameter may vary depending on the noise type.&lt;/li&gt; &#xA;    &lt;li&gt;noise_type: The tyoe of Power-Law noise to generate (white, grey, pink, green, blue)&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;Image Paste Crop by Location: Paste a crop top a custom location. This uses the same blending algorithm as Image Paste Crop.&lt;/li&gt; &#xA;  &lt;li&gt;Image Pixelate: Turn a image into pixel art! Define the max number of colors, the pixelation mode, the random state, and max iterations, and max those sprites shine.&lt;/li&gt; &#xA;  &lt;li&gt;Image Remove Background (Alpha): Remove the background from a image by threshold and tolerance.&lt;/li&gt; &#xA;  &lt;li&gt;Image Remove Color: Remove a color from a image and replace it with another&lt;/li&gt; &#xA;  &lt;li&gt;Image Resize&lt;/li&gt; &#xA;  &lt;li&gt;Image Rotate: Rotate an image&lt;/li&gt; &#xA;  &lt;li&gt;Image Rotate Hue: Rotate the hue of a image. A hue_shift of &lt;code&gt;0.0&lt;/code&gt; would represent no change, and &lt;code&gt;1.0&lt;/code&gt; would represent a full circle of the hue, and also exhibit no change.&lt;/li&gt; &#xA;  &lt;li&gt;Image Save: A save image node with format support and path support. &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;code&gt;show_history&lt;/code&gt; will show previously saved images with the WAS Save Image node. ComfyUI unfortunately resizes displayed images to the same size however, so if images are in different sizes it will force them in a different size.&lt;/li&gt; &#xA;    &lt;li&gt;Doesn&#39;t display images saved outside &lt;code&gt;/ComfyUI/output/&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;li&gt;You can save as &lt;code&gt;webp&lt;/code&gt; if you have webp available to you system. On windows you can get that support with this &lt;a href=&#34;https://storage.googleapis.com/downloads.webmproject.org/releases/webp/libwebp-1.3.0-windows-x64.zip&#34;&gt;precompiled libarary&lt;/a&gt; from the &lt;a href=&#34;https://developers.google.com/speed/webp/download&#34;&gt;webp project&lt;/a&gt;. On linux you can run &lt;code&gt;apt-get install webp&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;Image Seamless Texture: Create a seamless texture out of a image with optional tiling&lt;/li&gt; &#xA;  &lt;li&gt;Image Select Channel: Select a single channel of an RGB image&lt;/li&gt; &#xA;  &lt;li&gt;Image Select Color: Return the select image only on a black canvas&lt;/li&gt; &#xA;  &lt;li&gt;Image Shadows and Highlights: Adjust the shadows and highlights of an image&lt;/li&gt; &#xA;  &lt;li&gt;Image Size to Number: Get the &lt;code&gt;width&lt;/code&gt; and &lt;code&gt;height&lt;/code&gt; of an input image to use with &lt;strong&gt;Number&lt;/strong&gt; nodes.&lt;/li&gt; &#xA;  &lt;li&gt;Image Stitch: Stitch images together on different sides with optional feathering blending between them.&lt;/li&gt; &#xA;  &lt;li&gt;Image Style Filter: Style a image with Pilgram instragram-like filters &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Depends on &lt;code&gt;pilgram&lt;/code&gt; module&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;Image Threshold: Return the desired threshold range of a image&lt;/li&gt; &#xA;  &lt;li&gt;Image Tile: Split a image up into a image batch of tiles. Can be used with Tensor Batch to Image to select a individual tile from the batch.&lt;/li&gt; &#xA;  &lt;li&gt;Image Transpose&lt;/li&gt; &#xA;  &lt;li&gt;Image fDOF Filter: Apply a fake depth of field effect to an image&lt;/li&gt; &#xA;  &lt;li&gt;Image to Latent Mask: Convert a image into a latent mask&lt;/li&gt; &#xA;  &lt;li&gt;Image to Noise: Convert a image into noise, useful for init blending or init input to theme a diffusion.&lt;/li&gt; &#xA;  &lt;li&gt;Images to RGB: Convert a tensor image batch to RGB if they are RGBA or some other mode.&lt;/li&gt; &#xA;  &lt;li&gt;Image to Seed: Convert a image to a reproducible seed&lt;/li&gt; &#xA;  &lt;li&gt;Image Voronoi Noise Filter &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;A custom implementation of the worley voronoi noise diagram&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;Input Switch (Disable until &lt;code&gt;*&lt;/code&gt; wildcard fix)&lt;/li&gt; &#xA;  &lt;li&gt;KSampler (WAS): A sampler that accepts a seed as a node inputs&lt;/li&gt; &#xA;  &lt;li&gt;KSampler Cycle: A KSampler able to do HR pass loops, you can specify an upscale factor, and how many steps to achieve that factor. Accepts a upscale_model, as well as a 1x processor model. A secondary diffusion model can also be used.&lt;/li&gt; &#xA;  &lt;li&gt;Load Cache: Load cached Latent, Tensor Batch (image), and Conditioning files.&lt;/li&gt; &#xA;  &lt;li&gt;Load Text File &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Now supports outputting a dictionary named after the file, or custom input.&lt;/li&gt; &#xA;    &lt;li&gt;The dictionary contains a list of all lines in the file.&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;Load Batch Images &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Increment images in a folder, or fetch a single image out of a batch.&lt;/li&gt; &#xA;    &lt;li&gt;Will reset it&#39;s place if the path, or pattern is changed.&lt;/li&gt; &#xA;    &lt;li&gt;pattern is a glob that allows you to do things like &lt;code&gt;**/*&lt;/code&gt; to get all files in the directory and subdirectory or things like &lt;code&gt;*.jpg&lt;/code&gt; to select only JPEG images in the directory specified.&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;Mask to Image: Convert &lt;code&gt;MASK&lt;/code&gt; to &lt;code&gt;IMAGE&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Mask Batch to Mask: Return a single mask from a batch of masks&lt;/li&gt; &#xA;  &lt;li&gt;Mask Invert: Invert a mask.&lt;/li&gt; &#xA;  &lt;li&gt;Mask Add: Add masks together.&lt;/li&gt; &#xA;  &lt;li&gt;Mask Subtract: Subtract from a mask by another.&lt;/li&gt; &#xA;  &lt;li&gt;Mask Dominant Region: Return the dominant region in a mask (the largest area)&lt;/li&gt; &#xA;  &lt;li&gt;Mask Minority Region: Return the smallest region in a mask (the smallest area)&lt;/li&gt; &#xA;  &lt;li&gt;Mask Crop Dominant Region: Crop mask to the dominant region with optional padding in pixels&lt;/li&gt; &#xA;  &lt;li&gt;Mask Crop Minority Region: Crop mask to the minority region with optional padding in pixels&lt;/li&gt; &#xA;  &lt;li&gt;Mask Crop Region: Crop to dominant or minority region and return &lt;code&gt;crop_data&lt;/code&gt; for pasting back. Additionally outputs region location and size for other nodes like Crop Image Location.&lt;/li&gt; &#xA;  &lt;li&gt;Mask Arbitrary Region: Return a region that most closely matches the size input (size is not a direct representation of pixels, but approximate)&lt;/li&gt; &#xA;  &lt;li&gt;Mask Smooth Region: Smooth the boundaries of a mask&lt;/li&gt; &#xA;  &lt;li&gt;Mask Erode Region: Erode the boundaries of a mask&lt;/li&gt; &#xA;  &lt;li&gt;Mask Dilate Region: Dilate the boundaries of a mask&lt;/li&gt; &#xA;  &lt;li&gt;Mask Fill Region: Fill holes within the masks regions&lt;/li&gt; &#xA;  &lt;li&gt;Mask Ceiling Region&#34;: Return only white pixels within a offset range.&lt;/li&gt; &#xA;  &lt;li&gt;Mask Floor Region: Return the lower most pixel values as white (255)&lt;/li&gt; &#xA;  &lt;li&gt;Mask Threshold Region: Apply a thresholded image between a black value and white value&lt;/li&gt; &#xA;  &lt;li&gt;Mask Gaussian Region: Apply a Gaussian blur to the mask&lt;/li&gt; &#xA;  &lt;li&gt;Masks Combine Masks: Combine 2 or more masks into one mask.&lt;/li&gt; &#xA;  &lt;li&gt;Masks Combine Batch: Combine batched masks into one mask.&lt;/li&gt; &#xA;  &lt;li&gt;Model Input Switch: Switch between two model inputs based on a boolean switch&lt;/li&gt; &#xA;  &lt;li&gt;ComfyUI Loaders: A set of ComfyUI loaders that also output a string that contains the name of the model being loaded.&lt;/li&gt; &#xA;  &lt;li&gt;Latent Noise Injection: Inject latent noise into a latent image&lt;/li&gt; &#xA;  &lt;li&gt;Latent Size to Number: Latent sizes in tensor width/height&lt;/li&gt; &#xA;  &lt;li&gt;Latent Upscale by Factor: Upscale a latent image by a factor&lt;/li&gt; &#xA;  &lt;li&gt;Latent Input Switch: Switch between two latent inputs based on a boolean switch&lt;/li&gt; &#xA;  &lt;li&gt;Logic Boolean: A simple &lt;code&gt;1&lt;/code&gt; or &lt;code&gt;0&lt;/code&gt; output to use with logic&lt;/li&gt; &#xA;  &lt;li&gt;Logic Boolean Primitive: True/False boolean input, to use with native boolean nodes&lt;/li&gt; &#xA;  &lt;li&gt;Logic AND: Given 2 booleans, performs &#34;AND&#34;&lt;/li&gt; &#xA;  &lt;li&gt;Logic OR: Given 2 booleans, performs &#34;OR&#34;&lt;/li&gt; &#xA;  &lt;li&gt;Logic XOR: Given 2 booleans, performs &#34;!=&#34;&lt;/li&gt; &#xA;  &lt;li&gt;Logic NOT: Given 1 boolean, returns the opposite&lt;/li&gt; &#xA;  &lt;li&gt;Lora Input Switch: Switch between two LORAs based on a boolean switch&lt;/li&gt; &#xA;  &lt;li&gt;MiDaS Model Loader: Load a MiDaS model as an optional input for MiDaS Depth Approximation&lt;/li&gt; &#xA;  &lt;li&gt;MiDaS Depth Approximation: Produce a depth approximation of a single image input&lt;/li&gt; &#xA;  &lt;li&gt;MiDaS Mask Image: Mask a input image using MiDaS with a desired color&lt;/li&gt; &#xA;  &lt;li&gt;Number Operation&lt;/li&gt; &#xA;  &lt;li&gt;Number to Seed&lt;/li&gt; &#xA;  &lt;li&gt;Number to Float&lt;/li&gt; &#xA;  &lt;li&gt;Number Input Switch: Switch between two number inputs based on a boolean switch&lt;/li&gt; &#xA;  &lt;li&gt;Number Input Condition: Compare between two inputs or against the A input&lt;/li&gt; &#xA;  &lt;li&gt;Number to Int&lt;/li&gt; &#xA;  &lt;li&gt;Number to String&lt;/li&gt; &#xA;  &lt;li&gt;Number to Text&lt;/li&gt; &#xA;  &lt;li&gt;Boolean to Text&lt;/li&gt; &#xA;  &lt;li&gt;Perlin Power Fractal Latent: Create a power fractal based latent image. Doesn&#39;t work with all samplers (unless you add noise).&lt;/li&gt; &#xA;  &lt;li&gt;Random Number &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Random integer between min and max (inclusive), uniformly distributed random number&lt;/li&gt; &#xA;    &lt;li&gt;Random float between min and max (inclusive), uniformly distributed random number&lt;/li&gt; &#xA;    &lt;li&gt;Random number from 0 to 1 inclusive, this will be a 0 or 1 boolean if you use the &#39;int&#39; output&lt;/li&gt; &#xA;    &lt;li&gt;Random shuffled list of integers between min and max inclusive. E.g. if min=0 and max=3, a possible outcome would be the string &#39;3,1,2,0&#39;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;Save Text File: Save a text string to a file&lt;/li&gt; &#xA;  &lt;li&gt;Samples Passthrough (Stat System): Logs RAM, VRAM, and Disk usage to the console.&lt;/li&gt; &#xA;  &lt;li&gt;Seed: Return a seed&lt;/li&gt; &#xA;  &lt;li&gt;Tensor Batch to Image: Select a single image out of a latent batch for post processing with filters&lt;/li&gt; &#xA;  &lt;li&gt;Text Add Tokens: Add custom tokens to parse in filenames or other text.&lt;/li&gt; &#xA;  &lt;li&gt;Text Add Token by Input: Add custom token by inputs representing single &lt;strong&gt;single line&lt;/strong&gt; name and value of the token&lt;/li&gt; &#xA;  &lt;li&gt;Text Compare: Compare two strings. Returns a boolean if they are the same, a score of similarity, and the similarity or difference text.&lt;/li&gt; &#xA;  &lt;li&gt;Text Concatenate: Merge two strings&lt;/li&gt; &#xA;  &lt;li&gt;Text Dictionary Update: Merge two dictionaries&lt;/li&gt; &#xA;  &lt;li&gt;Text Dictionary Get: Get a value from a dictionary (as a string)&lt;/li&gt; &#xA;  &lt;li&gt;Text Dictionary Convert: Convert text to dictionary object&lt;/li&gt; &#xA;  &lt;li&gt;Text Dictionary New: Create a new dictionary&lt;/li&gt; &#xA;  &lt;li&gt;Text Dictionary Keys: Returns the keys, as a list from a dictionary object&lt;/li&gt; &#xA;  &lt;li&gt;Text Dictionary To Text: Returns the dictionary as text&lt;/li&gt; &#xA;  &lt;li&gt;Text File History: Show previously opened text files &lt;em&gt;(requires restart to show last sessions files at this time)&lt;/em&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Text Find: Find a substring or pattern within another string. Returns boolean&lt;/li&gt; &#xA;  &lt;li&gt;Text Find and Replace: Find and replace a substring in a string&lt;/li&gt; &#xA;  &lt;li&gt;Text Find and Replace by Dictionary: Replace substrings in a ASCII text input with a dictionary. &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;The dictionary keys are used as the key to replace, and the list of lines it contains chosen at random based on the seed.&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;Text Input Switch: Switch between two text inputs&lt;/li&gt; &#xA;  &lt;li&gt;Text List: Create a list of text strings&lt;/li&gt; &#xA;  &lt;li&gt;Text Load Line From File: Load lines from a file sequentially each &lt;em&gt;batch prompt&lt;/em&gt; run, or select a line index.&lt;/li&gt; &#xA;  &lt;li&gt;Text Concatenate: Merge lists of strings&lt;/li&gt; &#xA;  &lt;li&gt;Text Contains: Checks if substring is in another string (case insensitive optional)&lt;/li&gt; &#xA;  &lt;li&gt;Text Multiline: Write a multiline text string&lt;/li&gt; &#xA;  &lt;li&gt;Text Parse A1111 Embeddings: Convert embeddings filenames in your prompts to &lt;code&gt;embedding:[filename]]&lt;/code&gt; format based on your &lt;code&gt;/ComfyUI/models/embeddings/&lt;/code&gt; files.&lt;/li&gt; &#xA;  &lt;li&gt;Text Parse Noodle Soup Prompts: Parse NSP in a text input&lt;/li&gt; &#xA;  &lt;li&gt;Text Parse Tokens: Parse custom tokens in text.&lt;/li&gt; &#xA;  &lt;li&gt;Text Random Line: Select a random line from a text input string&lt;/li&gt; &#xA;  &lt;li&gt;Text Random Prompt: Feeling lucky? Get a random prompt based on a search seed, such as &#34;superhero&#34;&lt;/li&gt; &#xA;  &lt;li&gt;Text String: Write a single line text string value&lt;/li&gt; &#xA;  &lt;li&gt;Text String Truncate: Truncate a string from the beginning or end by characters or words.&lt;/li&gt; &#xA;  &lt;li&gt;Text to Conditioning: Convert a text string to conditioning.&lt;/li&gt; &#xA;  &lt;li&gt;True Random.org Number Generator: Generate a truly random number online from atmospheric noise with &lt;a href=&#34;https://random.org/&#34;&gt;Random.org&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://accounts.random.org/&#34;&gt;Get your API key from your account page&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;Upscale Model Input Switch: Switch between two Upscale Models inputs based on a boolean switch.&lt;/li&gt; &#xA;  &lt;li&gt;Write to Morph GIF: Write a new frame to an existing GIF (or create new one) with interpolation between frames.&lt;/li&gt; &#xA;  &lt;li&gt;Write to Video: Write a frame as you generate to a video (Best used with FFV1 for lossless images)&lt;/li&gt; &#xA;  &lt;li&gt;VAE Input Switch: Switch between two VAE inputs based on boolean input&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;br&gt; &#xA;&lt;h3&gt;Extra Nodes&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CLIPTextEncode (BlenderNeko Advanced + NSP): Only available if you have BlenderNeko&#39;s &lt;a href=&#34;https://github.com/BlenderNeko/ComfyUI_ADV_CLIP_emb&#34;&gt;Advanced CLIP Text Encode&lt;/a&gt;. Allows for NSP and Wildcard use with their advanced CLIPTextEncode.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Notes:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;CLIPTextEncode (NSP)&lt;/strong&gt; and &lt;strong&gt;CLIPTextEncode (BlenderNeko Advanced + NSP)&lt;/strong&gt;: Accept dynamic prompts in &lt;code&gt;&amp;lt;option1|option2|option3&amp;gt;&lt;/code&gt; format. This will respect the nodes input seed to yield reproducible results like NSP and Wildcards.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;CLIPTextEncode (NSP)&lt;/strong&gt; and &lt;strong&gt;CLIPTextEncode (BlenderNeko Advanced + NSP)&lt;/strong&gt;: Assign variables with &lt;code&gt;$|prompt words|$&lt;/code&gt; format. You can then print this word again within the prompt with the number corresponding the order you created them. So the first prompt var would be printed with &lt;code&gt;$1&lt;/code&gt; and the second with &lt;code&gt;$2&lt;/code&gt; and so on.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Video Nodes&lt;/h2&gt; &#xA;&lt;h3&gt;Codecs&lt;/h3&gt; &#xA;&lt;p&gt;You can use codecs that are available to your ffmpeg binaries by adding their fourcc ID (in one string), and appropriate container extension to the &lt;code&gt;was_suite_config.json&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Example &lt;a href=&#34;https://github.com/cisco/openh264/releases/tag/v1.8.0&#34;&gt;H264 Codecs&lt;/a&gt; (Defaults)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    &#34;ffmpeg_extra_codecs&#34;: {&#xA;        &#34;avc1&#34;: &#34;.mp4&#34;,&#xA;        &#34;h264&#34;: &#34;.mkv&#34;&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Notes&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For now I am only supporting &lt;strong&gt;Windows&lt;/strong&gt; installations for video nodes. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;I do not have access to Mac or a stand-alone linux distro. If you get them working and want to PR a patch/directions, feel free.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Video nodes require &lt;a href=&#34;https://ffmpeg.org/download.html&#34;&gt;FFMPEG&lt;/a&gt;. You should download the proper FFMPEG binaries for you system and set the FFMPEG path in the config file.&lt;/li&gt; &#xA; &lt;li&gt;Additionally, if you want to use H264 codec need to &lt;a href=&#34;https://github.com/cisco/openh264/releases/tag/v1.8.0&#34;&gt;download OpenH264 1.8.0&lt;/a&gt; and place it in the root of ComfyUI (Example: &lt;code&gt;C:\ComfyUI_windows_portable&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;FFV1 will complain about invalid container. You can ignore this. The resulting MKV file is readable. I have not figured out what this issue is about. Documentaion tells me to use MKV, but it&#39;s telling me it&#39;s unsupported. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;If you know how to resolve this, I&#39;d love a PR&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Write to Video&lt;/code&gt; node should use a lossless video codec or when it copies frames, and reapplies compression, it will start expontentially ruining the starting frames run to run.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Text Tokens&lt;/h1&gt; &#xA;&lt;p&gt;Text tokens can be used in the Save Text File and Save Image nodes. You can also add your own custom tokens with the Text Add Tokens node.&lt;/p&gt; &#xA;&lt;p&gt;The token name can be anything excluding the &lt;code&gt;:&lt;/code&gt; character to define your token. It can also be simple Regular Expressions.&lt;/p&gt; &#xA;&lt;h2&gt;Built-in Tokens&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[time] &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The current system microtime&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;[time(&lt;code&gt;format_code&lt;/code&gt;)] &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The current system time in human readable format. Utilizing &lt;a href=&#34;https://docs.python.org/3/library/datetime.html&#34;&gt;datetime&lt;/a&gt; formatting&lt;/li&gt; &#xA;   &lt;li&gt;Example: &lt;code&gt;[hostname]_[time]__[time(%Y-%m-%d__%I-%M%p)]&lt;/code&gt; would output: &lt;strong&gt;SKYNET-MASTER_1680897261__2023-04-07__07-54PM&lt;/strong&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;[hostname] &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The hostname of the system executing ComfyUI&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;[cuda_device] &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The cuda device from &lt;code&gt;comfy.model_management.get_cuda_device()&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;[cuda_name] &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The cuda name from &lt;code&gt;comfy.model_management.get_cuda_device_name()&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;$\color{orange}{Expand\ Date\ Code\ List}$&lt;/summary&gt; &#xA; &lt;br&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Directive&lt;/th&gt; &#xA;    &lt;th&gt;Meaning&lt;/th&gt; &#xA;    &lt;th&gt;Example&lt;/th&gt; &#xA;    &lt;th&gt;Notes&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;%a&lt;/td&gt; &#xA;    &lt;td&gt;Weekday as localeâ€™s abbreviated name.&lt;/td&gt; &#xA;    &lt;td&gt;Sun, Mon, â€¦, Sat (en_US); So, Mo, â€¦, Sa (de_DE)&lt;/td&gt; &#xA;    &lt;td&gt;(1)&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;%A&lt;/td&gt; &#xA;    &lt;td&gt;Weekday as localeâ€™s full name.&lt;/td&gt; &#xA;    &lt;td&gt;Sunday, Monday, â€¦, Saturday (en_US); Sonntag, Montag, â€¦, Samstag (de_DE)&lt;/td&gt; &#xA;    &lt;td&gt;(1)&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;%w&lt;/td&gt; &#xA;    &lt;td&gt;Weekday as a decimal number, where 0 is Sunday and 6 is Saturday.&lt;/td&gt; &#xA;    &lt;td&gt;0, 1, â€¦, 6&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;%d&lt;/td&gt; &#xA;    &lt;td&gt;Day of the month as a zero-padded decimal number.&lt;/td&gt; &#xA;    &lt;td&gt;01, 02, â€¦, 31&lt;/td&gt; &#xA;    &lt;td&gt;(9)&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;%b&lt;/td&gt; &#xA;    &lt;td&gt;Month as localeâ€™s abbreviated name.&lt;/td&gt; &#xA;    &lt;td&gt;Jan, Feb, â€¦, Dec (en_US); Jan, Feb, â€¦, Dez (de_DE)&lt;/td&gt; &#xA;    &lt;td&gt;(1)&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;%B&lt;/td&gt; &#xA;    &lt;td&gt;Month as localeâ€™s full name.&lt;/td&gt; &#xA;    &lt;td&gt;January, February, â€¦, December (en_US); Januar, Februar, â€¦, Dezember (de_DE)&lt;/td&gt; &#xA;    &lt;td&gt;(1)&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;%m&lt;/td&gt; &#xA;    &lt;td&gt;Month as a zero-padded decimal number.&lt;/td&gt; &#xA;    &lt;td&gt;01, 02, â€¦, 12&lt;/td&gt; &#xA;    &lt;td&gt;(9)&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;%y&lt;/td&gt; &#xA;    &lt;td&gt;Year without century as a zero-padded decimal number.&lt;/td&gt; &#xA;    &lt;td&gt;00, 01, â€¦, 99&lt;/td&gt; &#xA;    &lt;td&gt;(9)&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;%Y&lt;/td&gt; &#xA;    &lt;td&gt;Year with century as a decimal number.&lt;/td&gt; &#xA;    &lt;td&gt;0001, 0002, â€¦, 2013, 2014, â€¦, 9998, 9999&lt;/td&gt; &#xA;    &lt;td&gt;(2)&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;%H&lt;/td&gt; &#xA;    &lt;td&gt;Hour (24-hour clock) as a zero-padded decimal number.&lt;/td&gt; &#xA;    &lt;td&gt;00, 01, â€¦, 23&lt;/td&gt; &#xA;    &lt;td&gt;(9)&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;%I&lt;/td&gt; &#xA;    &lt;td&gt;Hour (12-hour clock) as a zero-padded decimal number.&lt;/td&gt; &#xA;    &lt;td&gt;01, 02, â€¦, 12&lt;/td&gt; &#xA;    &lt;td&gt;(9)&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;%p&lt;/td&gt; &#xA;    &lt;td&gt;Localeâ€™s equivalent of either AM or PM.&lt;/td&gt; &#xA;    &lt;td&gt;AM, PM (en_US); am, pm (de_DE)&lt;/td&gt; &#xA;    &lt;td&gt;(1), (3)&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;%M&lt;/td&gt; &#xA;    &lt;td&gt;Minute as a zero-padded decimal number.&lt;/td&gt; &#xA;    &lt;td&gt;00, 01, â€¦, 59&lt;/td&gt; &#xA;    &lt;td&gt;(9)&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;%S&lt;/td&gt; &#xA;    &lt;td&gt;Second as a zero-padded decimal number.&lt;/td&gt; &#xA;    &lt;td&gt;00, 01, â€¦, 59&lt;/td&gt; &#xA;    &lt;td&gt;(4), (9)&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;%f&lt;/td&gt; &#xA;    &lt;td&gt;Microsecond as a decimal number, zero-padded to 6 digits.&lt;/td&gt; &#xA;    &lt;td&gt;000000, 000001, â€¦, 999999&lt;/td&gt; &#xA;    &lt;td&gt;(5)&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;%z&lt;/td&gt; &#xA;    &lt;td&gt;UTC offset in the form Â±HHMM[SS[.ffffff]] (empty string if the object is naive).&lt;/td&gt; &#xA;    &lt;td&gt;(empty), +0000, -0400, +1030, +063415, -030712.345216&lt;/td&gt; &#xA;    &lt;td&gt;(6)&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;%Z&lt;/td&gt; &#xA;    &lt;td&gt;Time zone name (empty string if the object is naive).&lt;/td&gt; &#xA;    &lt;td&gt;(empty), UTC, GMT&lt;/td&gt; &#xA;    &lt;td&gt;(6)&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;%j&lt;/td&gt; &#xA;    &lt;td&gt;Day of the year as a zero-padded decimal number.&lt;/td&gt; &#xA;    &lt;td&gt;001, 002, â€¦, 366&lt;/td&gt; &#xA;    &lt;td&gt;(9)&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;%U&lt;/td&gt; &#xA;    &lt;td&gt;Week number of the year (Sunday as the first day of the week) as a zero-padded decimal number. All days in a new year preceding the first Sunday are considered to be in week 0.&lt;/td&gt; &#xA;    &lt;td&gt;00, 01, â€¦, 53&lt;/td&gt; &#xA;    &lt;td&gt;(7), (9)&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;%W&lt;/td&gt; &#xA;    &lt;td&gt;Week number of the year (Monday as the first day of the week) as a zero-padded decimal number. All days in a new year preceding the first Monday are considered to be in week 0.&lt;/td&gt; &#xA;    &lt;td&gt;00, 01, â€¦, 53&lt;/td&gt; &#xA;    &lt;td&gt;(7), (9)&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;%c&lt;/td&gt; &#xA;    &lt;td&gt;Localeâ€™s appropriate date and time representation.&lt;/td&gt; &#xA;    &lt;td&gt;Tue Aug 16 21:30:00 1988 (en_US); Di 16 Aug 21:30:00 1988 (de_DE)&lt;/td&gt; &#xA;    &lt;td&gt;(1)&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;%x&lt;/td&gt; &#xA;    &lt;td&gt;Localeâ€™s appropriate date representation.&lt;/td&gt; &#xA;    &lt;td&gt;08/16/88 (None); 08/16/1988 (en_US); 16.08.1988 (de_DE)&lt;/td&gt; &#xA;    &lt;td&gt;(1)&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;%X&lt;/td&gt; &#xA;    &lt;td&gt;Localeâ€™s appropriate time representation.&lt;/td&gt; &#xA;    &lt;td&gt;21:30:00 (en_US); 21:30:00 (de_DE)&lt;/td&gt; &#xA;    &lt;td&gt;(1)&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;%%&lt;/td&gt; &#xA;    &lt;td&gt;A literal &#39;%&#39; character.&lt;/td&gt; &#xA;    &lt;td&gt;%&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;br&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Other Features&lt;/h1&gt; &#xA;&lt;h3&gt;Import AUTOMATIC1111 WebUI Styles&lt;/h3&gt; &#xA;&lt;p&gt;When using the latest builds of WAS Node Suite a &lt;code&gt;was_suite_config.json&lt;/code&gt; file will be generated (if it doesn&#39;t exist). In this file you can setup a A1111 styles import.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Run ComfyUI to generate the new &lt;code&gt;/custom-nodes/was-node-suite-comfyui/was_Suite_config.json&lt;/code&gt; file.&lt;/li&gt; &#xA; &lt;li&gt;Open the &lt;code&gt;was_suite_config.json&lt;/code&gt; file with a text editor.&lt;/li&gt; &#xA; &lt;li&gt;Replace the &lt;code&gt;webui_styles&lt;/code&gt; value from &lt;code&gt;None&lt;/code&gt; to the path of your A1111 styles file called &lt;strong&gt;styles.csv&lt;/strong&gt;. Be sure to use double backslashes for Windows paths. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Example &lt;code&gt;C:\\python\\stable-diffusion-webui\\styles.csv&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Restart ComfyUI&lt;/li&gt; &#xA; &lt;li&gt;Select a style with the &lt;code&gt;Prompt Styles Node&lt;/code&gt;. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The first ASCII output is your positive prompt, and the second ASCII output is your negative prompt.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can set &lt;code&gt;webui_styles_persistent_update&lt;/code&gt; to &lt;code&gt;true&lt;/code&gt; to update the WAS Node Suite styles from WebUI every start of ComfyUI&lt;/p&gt; &#xA;&lt;h1&gt;Recommended Installation:&lt;/h1&gt; &#xA;&lt;p&gt;If you&#39;re running on Linux, or non-admin account on windows you&#39;ll want to ensure &lt;code&gt;/ComfyUI/custom_nodes&lt;/code&gt;, &lt;code&gt;was-node-suite-comfyui&lt;/code&gt;, and &lt;code&gt;WAS_Node_Suite.py&lt;/code&gt; has write permissions.&lt;/p&gt; &#xA;&lt;p&gt;There is now a &lt;strong&gt;install.bat&lt;/strong&gt; you can run to install to portable if detected. Otherwise it will default to system and assume you followed ConfyUI&#39;s manual installation steps.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Navigate to your &lt;code&gt;/ComfyUI/custom_nodes/&lt;/code&gt; folder&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;git clone https://github.com/WASasquatch/was-node-suite-comfyui/&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Navigate to your &lt;code&gt;was-node-suite-comfyui&lt;/code&gt; folder &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Portable/venv: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Run &lt;code&gt;path/to/ComfUI/python_embeded/python.exe -s -m pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;With system python &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Run &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Start ComfyUI &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;WAS Suite should uninstall legacy nodes automatically for you.&lt;/li&gt; &#xA;   &lt;li&gt;Tools will be located in the WAS Suite menu.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Alternate [Legacy] Installation:&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;re running on Linux, or non-admin account on windows you&#39;ll want to ensure &lt;code&gt;/ComfyUI/custom_nodes&lt;/code&gt;, and &lt;code&gt;WAS_Node_Suite.py&lt;/code&gt; has write permissions.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download &lt;code&gt;WAS_Node_Suite.py&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Move the file to your &lt;code&gt;/ComfyUI/custom_nodes/&lt;/code&gt; folder&lt;/li&gt; &#xA; &lt;li&gt;WAS Node Suite will attempt install dependencies on it&#39;s own, but you may need to manually do so. The dependencies required are in the &lt;code&gt;requirements.txt&lt;/code&gt; on this repo. See installation steps above. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;If this process fails attempt the following: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Navigate to your &lt;code&gt;was-node-suite-comfyui&lt;/code&gt; folder&lt;/li&gt; &#xA;     &lt;li&gt;Portable/venv: &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;Run &lt;code&gt;path/to/ComfUI/python_embeded/python.exe -s -m pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;With system python&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Run &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Start, or Restart ComfyUI &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;WAS Suite should uninstall legacy nodes automatically for you.&lt;/li&gt; &#xA;   &lt;li&gt;Tools will be located in the WAS Suite menu.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This method will not install the resources required for Image Crop Face node, and you&#39;ll have to download the &lt;a href=&#34;https://github.com/WASasquatch/was-node-suite-comfyui/tree/main/res&#34;&gt;./res/&lt;/a&gt; folder yourself.&lt;/p&gt; &#xA;&lt;h2&gt;Installing on Colab&lt;/h2&gt; &#xA;&lt;p&gt;Create a new cell and add the following code, then run the cell. You may need to edit the path to your &lt;code&gt;custom_nodes&lt;/code&gt; folder. You can also use the &lt;a href=&#34;https://colab.research.google.com/github/WASasquatch/comfyui-colab-was-node-suite/blob/main/ComfyUI_%2B_WAS_Node_Suite.ipynb&#34;&gt;colab hosted here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;!git clone https://github.com/WASasquatch/was-node-suite-comfyui /content/ComfyUI/custom_nodes/was-node-suite-comfyui&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;!pip install -r /content/ComfyUI/custom_nodes/was-node-suite-comfyui/requirements.txt&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Restart Colab Runtime (don&#39;t disconnect) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Tools will be located in the WAS Suite menu.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>AI4Finance-Foundation/FinRobot</title>
    <updated>2024-05-29T01:31:43Z</updated>
    <id>tag:github.com,2024-05-29:/AI4Finance-Foundation/FinRobot</id>
    <link href="https://github.com/AI4Finance-Foundation/FinRobot" rel="alternate"></link>
    <summary type="html">&lt;p&gt;FinRobot: An Open-Source AI Agent Platform for Financial Applications using LLMs ðŸš€ ðŸš€ ðŸš€&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img align=&#34;center&#34; width=&#34;30%&#34; alt=&#34;image&#34; src=&#34;https://github.com/AI4Finance-Foundation/FinGPT/assets/31713746/e0371951-1ce1-488e-aa25-0992dafcc139&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;FinRobot: An Open-Source AI Agent Platform for Financial Applications using Large Language Models&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AI4Finance-Foundation/FinRobot/master/%5Bhttps://pepy.tech/project/finrobot%5D(https://pepy.tech/project/finrobot)&#34;&gt;&lt;img src=&#34;https://static.pepy.tech/badge/finrobot&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/finrobot&#34;&gt;&lt;img src=&#34;https://static.pepy.tech/badge/finrobot/week&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.python.org/downloads/release/python-360/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.6-blue.svg?sanitize=true&#34; alt=&#34;Python 3.8&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/finrobot/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/finrobot.svg?sanitize=true&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/license/AI4Finance-Foundation/finrobot.svg?color=brightgreen&#34; alt=&#34;License&#34;&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img align=&#34;center&#34; src=&#34;https://raw.githubusercontent.com/AI4Finance-Foundation/FinRobot/master/figs/logo_white_background.jpg&#34; width=&#34;40%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;FinRobot&lt;/strong&gt; is an AI Agent Platform that transcends the scope of FinGPT, representing a comprehensive solution meticulously designed for financial applications. It integrates &lt;strong&gt;a diverse array of AI technologies&lt;/strong&gt;, extending beyond mere language models. This expansive vision highlights the platform&#39;s versatility and adaptability, addressing the multifaceted needs of the financial industry.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Concept of AI Agent&lt;/strong&gt;: an AI Agent is an intelligent entity that uses large language models as its brain to perceive its environment, make decisions, and execute actions. Unlike traditional artificial intelligence, AI Agents possess the ability to independently think and utilize tools to progressively achieve given objectives.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2405.14767&#34;&gt;Whitepaper of FinRobot&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/trsr8SXpW5&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/trsr8SXpW5&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;FinRobot Ecosystem&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img align=&#34;center&#34; src=&#34;https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/6b30d9c1-35e5-4d36-a138-7e2769718f62&#34; width=&#34;90%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;The overall framework of FinRobot is organized into four distinct layers, each designed to address specific aspects of financial AI processing and application:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Financial AI Agents Layer&lt;/strong&gt;: The Financial AI Agents Layer now includes Financial Chain-of-Thought (CoT) prompting, enhancing complex analysis and decision-making capacity. Market Forecasting Agents, Document Analysis Agents, and Trading Strategies Agents utilize CoT to dissect financial challenges into logical steps, aligning their advanced algorithms and domain expertise with the evolving dynamics of financial markets for precise, actionable insights.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Financial LLMs Algorithms Layer&lt;/strong&gt;: The Financial LLMs Algorithms Layer configures and utilizes specially tuned models tailored to specific domains and global market analysis.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLMOps and DataOps Layers&lt;/strong&gt;: The LLMOps layer implements a multi-source integration strategy that selects the most suitable LLMs for specific financial tasks, utilizing a range of state-of-the-art models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-source LLM Foundation Models Layer&lt;/strong&gt;: This foundational layer supports the plug-and-play functionality of various general and specialized LLMs.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;FinRobot: Agent Workflow&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img align=&#34;center&#34; src=&#34;https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/ff8033be-2326-424a-ac11-17e2c9c4983d&#34; width=&#34;60%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Perception&lt;/strong&gt;: This module captures and interprets multimodal financial data from market feeds, news, and economic indicators, using sophisticated techniques to structure the data for thorough analysis.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Brain&lt;/strong&gt;: Acting as the core processing unit, this module perceives data from the Perception module with LLMs and utilizes Financial Chain-of-Thought (CoT) processes to generate structured instructions.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Action&lt;/strong&gt;: This module executes instructions from the Brain module, applying tools to translate analytical insights into actionable outcomes. Actions include trading, portfolio adjustments, generating reports, or sending alerts, thereby actively influencing the financial environment.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;FinRobot: Smart Scheduler&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img align=&#34;center&#34; src=&#34;https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/06fa0b78-ac53-48d3-8a6e-98d15386327e&#34; width=&#34;60%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;The Smart Scheduler is central to ensuring model diversity and optimizing the integration and selection of the most appropriate LLM for each task.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Director Agent&lt;/strong&gt;: This component orchestrates the task assignment process, ensuring that tasks are allocated to agents based on their performance metrics and suitability for specific tasks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Agent Registration&lt;/strong&gt;: Manages the registration and tracks the availability of agents within the system, facilitating an efficient task allocation process.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Agent Adaptor&lt;/strong&gt;: Tailor agent functionalities to specific tasks, enhancing their performance and integration within the overall system.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Task Manager&lt;/strong&gt;: Manages and stores different general and fine-tuned LLMs-based agents tailored for various financial tasks, updated periodically to ensure relevance and efficacy.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;File Structure&lt;/h2&gt; &#xA;&lt;p&gt;The main folder &lt;strong&gt;finrobot&lt;/strong&gt; has three subfolders &lt;strong&gt;agents, data_source, functional&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;FinRobot&#xA;â”œâ”€â”€ finrobot (main folder)&#xA;â”‚   â”œâ”€â”€ agents&#xA;â”‚   &#x9;â”œâ”€â”€ agent_library.py&#xA;â”‚   &#x9;â””â”€â”€ workflow.py&#xA;â”‚   â”œâ”€â”€ data_source&#xA;â”‚   &#x9;â”œâ”€â”€ finnhub_utils.py&#xA;â”‚   &#x9;â”œâ”€â”€ finnlp_utils.py&#xA;â”‚   &#x9;â”œâ”€â”€ fmp_utils.py&#xA;â”‚   &#x9;â”œâ”€â”€ sec_utils.py&#xA;â”‚   &#x9;â””â”€â”€ yfinance_utils.py&#xA;â”‚   â”œâ”€â”€ functional&#xA;â”‚   &#x9;â”œâ”€â”€ analyzer.py&#xA;â”‚   &#x9;â”œâ”€â”€ charting.py&#xA;â”‚   &#x9;â”œâ”€â”€ coding.py&#xA;â”‚   &#x9;â”œâ”€â”€ quantitative.py&#xA;â”‚   &#x9;â”œâ”€â”€ reportlab.py&#xA;â”‚   &#x9;â””â”€â”€ text.py&#xA;â”‚   â”œâ”€â”€ toolkits.py&#xA;â”‚   â””â”€â”€ utils.py&#xA;â”‚&#xA;â”œâ”€â”€ configs&#xA;â”œâ”€â”€ experiments&#xA;â”œâ”€â”€ tutorials_beginner (hands-on tutorial)&#xA;â”‚   â”œâ”€â”€ agent_fingpt_forecaster.ipynb&#xA;â”‚   â””â”€â”€ agent_annual_report.ipynb &#xA;â”œâ”€â”€ tutorials_advanced (advanced tutorials for potential finrobot developers)&#xA;â”‚   â”œâ”€â”€ agent_trade_strategist.ipynb&#xA;â”‚   â”œâ”€â”€ agent_fingpt_forecaster.ipynb&#xA;â”‚   â”œâ”€â”€ agent_annual_report.ipynb &#xA;â”‚   â”œâ”€â”€ lmm_agent_mplfinance.ipynb&#xA;â”‚   â””â”€â”€ lmm_agent_opt_smacross.ipynb&#xA;â”œâ”€â”€ setup.py&#xA;â”œâ”€â”€ OAI_CONFIG_LIST_sample&#xA;â”œâ”€â”€ config_api_keys_sample&#xA;â”œâ”€â”€ requirements.txt&#xA;â””â”€â”€ README.md&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Installation:&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;1. (Recommended) Create a new virtual environment&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda create --name finrobot python=3.10&#xA;conda activate finrobot&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;2. download the FinRobot repo use terminal or download it manually&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/AI4Finance-Foundation/FinRobot.git&#xA;cd FinRobot&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;3. install finrobot &amp;amp; dependencies from source or pypi&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;get our latest release from pypi&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -U finrobot&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or install from this repo directly&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;4. modify OAI_CONFIG_LIST_sample file&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;1) rename OAI_CONFIG_LIST_sample to OAI_CONFIG_LIST&#xA;2) remove the four lines of comment within the OAI_CONFIG_LIST file&#xA;3) add your own openai api-key &amp;lt;your OpenAI API key here&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;5. modify config_api_keys_sample file&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;1) rename config_api_keys_sample to config_api_keys&#xA;2) remove the comment within the config_api_keys file&#xA;3) add your own finnhub-api &#34;YOUR_FINNHUB_API_KEY&#34;&#xA;4) add your own financialmodelingprep and sec-api keys &#34;YOUR_FMP_API_KEY&#34; and &#34;YOUR_SEC_API_KEY&#34; (for financial report generation)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;6. start navigating the tutorials or the demos below:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# find these notebooks in tutorials&#xA;1) agent_annual_report.ipynb&#xA;2) agent_fingpt_forecaster.ipynb&#xA;3) agent_trade_strategist.ipynb&#xA;4) lmm_agent_mplfinance.ipynb&#xA;5) lmm_agent_opt_smacross.ipynb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Demos&lt;/h2&gt; &#xA;&lt;h3&gt;1. Market Forecaster Agent (Predict Stock Movements Direction)&lt;/h3&gt; &#xA;&lt;p&gt;Takes a company&#39;s ticker symbol, recent basic financials, and market news as input and predicts its stock movements.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Import&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import autogen&#xA;from finrobot.utils import get_current_date, register_keys_from_json&#xA;from finrobot.agents.workflow import SingleAssistant&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Config&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Read OpenAI API keys from a JSON file&#xA;llm_config = {&#xA;    &#34;config_list&#34;: autogen.config_list_from_json(&#xA;        &#34;../OAI_CONFIG_LIST&#34;,&#xA;        filter_dict={&#34;model&#34;: [&#34;gpt-4-0125-preview&#34;]},&#xA;    ),&#xA;    &#34;timeout&#34;: 120,&#xA;    &#34;temperature&#34;: 0,&#xA;}&#xA;&#xA;# Register FINNHUB API keys&#xA;register_keys_from_json(&#34;../config_api_keys&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Run&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;company = &#34;NVDA&#34;&#xA;&#xA;assitant = SingleAssistant(&#xA;    &#34;Market_Analyst&#34;,&#xA;    llm_config,&#xA;    # set to &#34;ALWAYS&#34; if you want to chat instead of simply receiving the prediciton&#xA;    human_input_mode=&#34;NEVER&#34;,&#xA;)&#xA;assitant.chat(&#xA;    f&#34;Use all the tools provided to retrieve information available for {company} upon {get_current_date()}. Analyze the positive developments and potential concerns of {company} &#34;&#xA;    &#34;with 2-4 most important factors respectively and keep them concise. Most factors should be inferred from company related news. &#34;&#xA;    f&#34;Then make a rough prediction (e.g. up/down by 2-3%) of the {company} stock price movement for next week. Provide a summary analysis to support your prediction.&#34;&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Result&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img align=&#34;center&#34; src=&#34;https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/812ec23a-9cb3-4fad-b716-78533ddcd9dc&#34; width=&#34;40%&#34;&gt; &#xA; &lt;img align=&#34;center&#34; src=&#34;https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/9a2f9f48-b0e1-489c-8679-9a4c530f313c&#34; width=&#34;41%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;2. Financial Analyst Agent for Report Writing (Equity Research Report)&lt;/h3&gt; &#xA;&lt;p&gt;Take a company&#39;s 10-k form, financial data, and market data as input and output an equity research report&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Import&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os&#xA;import autogen&#xA;from textwrap import dedent&#xA;from finrobot.utils import register_keys_from_json&#xA;from finrobot.agents.workflow import SingleAssistantShadow&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Config&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;llm_config = {&#xA;    &#34;config_list&#34;: autogen.config_list_from_json(&#xA;        &#34;../OAI_CONFIG_LIST&#34;,&#xA;        filter_dict={&#xA;            &#34;model&#34;: [&#34;gpt-4-0125-preview&#34;],&#xA;        },&#xA;    ),&#xA;    &#34;timeout&#34;: 120,&#xA;    &#34;temperature&#34;: 0.5,&#xA;}&#xA;register_keys_from_json(&#34;../config_api_keys&#34;)&#xA;&#xA;# Intermediate strategy modules will be saved in this directory&#xA;work_dir = &#34;../report&#34;&#xA;os.makedirs(work_dir, exist_ok=True)&#xA;&#xA;assistant = SingleAssistantShadow(&#xA;    &#34;Expert_Investor&#34;,&#xA;    llm_config,&#xA;    max_consecutive_auto_reply=None,&#xA;    human_input_mode=&#34;TERMINATE&#34;,&#xA;)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Run&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;company = &#34;Microsoft&#34;&#xA;fyear = &#34;2023&#34;&#xA;&#xA;message = dedent(&#xA;    f&#34;&#34;&#34;&#xA;    With the tools you&#39;ve been provided, write an annual report based on {company}&#39;s {fyear} 10-k report, format it into a pdf.&#xA;    Pay attention to the followings:&#xA;    - Explicitly explain your working plan before you kick off.&#xA;    - Use tools one by one for clarity, especially when asking for instructions. &#xA;    - All your file operations should be done in &#34;{work_dir}&#34;. &#xA;    - Display any image in the chat once generated.&#xA;    - All the paragraphs should combine between 400 and 450 words, don&#39;t generate the pdf until this is explicitly fulfilled.&#xA;&#34;&#34;&#34;&#xA;)&#xA;&#xA;assistant.chat(message, use_cache=True, max_turns=50,&#xA;               summary_method=&#34;last_msg&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Result&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img align=&#34;center&#34; src=&#34;https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/d2d999e0-dc0e-4196-aca1-218f5fadcc5b&#34; width=&#34;60%&#34;&gt; &#xA; &lt;img align=&#34;center&#34; src=&#34;https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/3a21873f-9498-4d73-896b-3740bf6d116d&#34; width=&#34;60%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;Financial CoT&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Gather Preliminary Data&lt;/strong&gt;: 10-K report, market data, financial ratios&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Analyze Financial Statements&lt;/strong&gt;: balance sheet, income statement, cash flow&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Company Overview and Performance&lt;/strong&gt;: company description, business highlights, segment analysis&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Risk Assessment&lt;/strong&gt;: assess risks&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Financial Performance Visualization&lt;/strong&gt;: plot PE ratio and EPS&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Synthesize Findings into Paragraphs&lt;/strong&gt;: combine all parts into a coherent summary&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Generate PDF Report&lt;/strong&gt;: use tools to generate PDF automatically&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Quality Assurance&lt;/strong&gt;: check word counts&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;3. Trade Strategist Agent with multimodal capabilities&lt;/h3&gt; &#xA;&lt;h2&gt;AI Agent Papers&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[Stanford University + Microsoft Research] &lt;a href=&#34;https://arxiv.org/abs/2401.03568&#34;&gt;Agent AI: Surveying the Horizons of Multimodal Interaction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Stanford University] &lt;a href=&#34;https://arxiv.org/abs/2304.03442&#34;&gt;Generative Agents: Interactive Simulacra of Human Behavior&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Fudan NLP Group] &lt;a href=&#34;https://arxiv.org/abs/2309.07864&#34;&gt;The Rise and Potential of Large Language Model Based Agents: A Survey&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Fudan NLP Group] &lt;a href=&#34;https://github.com/WooooDyy/LLM-Agent-Paper-List&#34;&gt;LLM-Agent-Paper-List&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Tsinghua University] &lt;a href=&#34;https://arxiv.org/abs/2312.11970&#34;&gt;Large Language Models Empowered Agent-based Modeling and Simulation: A Survey and Perspectives&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Renmin University] &lt;a href=&#34;https://arxiv.org/pdf/2308.11432.pdf&#34;&gt;A Survey on Large Language Model-based Autonomous Agents&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Nanyang Technological University] &lt;a href=&#34;https://arxiv.org/abs/2402.18485&#34;&gt;FinAgent: A Multimodal Foundation Agent for Financial Trading: Tool-Augmented, Diversified, and Generalist&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;AI Agent Blogs and Videos&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[Medium] &lt;a href=&#34;https://medium.com/humansdotai/an-introduction-to-ai-agents-e8c4afd2ee8f&#34;&gt;An Introduction to AI Agents&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Medium] &lt;a href=&#34;https://medium.com/@aitrendorbit/unmasking-the-best-character-ai-chatbots-2024-351de43792f4#the-best-character-ai-chatbots&#34;&gt;Unmasking the Best Character AI Chatbots | 2024&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[big-picture] &lt;a href=&#34;https://blog.big-picture.com/en/chatgpt-next-level-meet-10-autonomous-ai-agents-auto-gpt-babyagi-agentgpt-microsoft-jarvis-chaosgpt-friends/&#34;&gt;ChatGPT, Next Level: Meet 10 Autonomous AI Agents&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[TowardsDataScience] &lt;a href=&#34;https://towardsdatascience.com/navigating-the-world-of-llm-agents-a-beginners-guide-3b8d499db7a9&#34;&gt;Navigating the World of LLM Agents: A Beginnerâ€™s Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[YouTube] &lt;a href=&#34;https://www.youtube.com/watch?v=iVbN95ica_k&#34;&gt;Introducing Devin - The &#34;First&#34; AI Agent Software Engineer&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;AI Agent Open-Source Framework &amp;amp; Tool&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Significant-Gravitas/AutoGPT&#34;&gt;AutoGPT (161k stars)&lt;/a&gt; is a tool for everyone to use, aiming to democratize AI, making it accessible for everyone to use and build upon.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/langchain-ai/langchain&#34;&gt;LangChain (82.7k stars)&lt;/a&gt; is a framework for developing context-aware applications powered by language models, enabling them to connect to sources of context and rely on the model&#39;s reasoning capabilities for responses and actions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/geekan/MetaGPT&#34;&gt;MetaGPT (39.1k stars)&lt;/a&gt; is a multi-agent open-source framework that assigns different roles to GPTs, forming a collaborative software entity to execute complex tasks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/autogen&#34;&gt;AutoGen (24.8k stars)&lt;/a&gt; is a framework for developing LLM applications with conversational agents that collaborate to solve tasks. These agents are customizable, support human interaction, and operate in modes combining LLMs, human inputs, and tools.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/langgenius/dify&#34;&gt;dify (22.7k stars)&lt;/a&gt; is an LLM application development platform. It integrates the concepts of Backend as a Service and LLMOps, covering the core tech stack required for building generative AI-native applications, including a built-in RAG engine&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/OpenBMB/ChatDev&#34;&gt;ChatDev (22.7k stars)&lt;/a&gt; is a framework that focuses on developing conversational AI Agents capable of dialogue and question-answering. It provides a range of pre-trained models and interactive interfaces, facilitating the development of customized chat Agents for users.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yoheinakajima/babyagi&#34;&gt;BabyAGI (19.2k stars)&lt;/a&gt; is an AI-powered task management system, dedicated to building AI Agents with preliminary general intelligence.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/TransformerOptimus/SuperAGI&#34;&gt;SuperAGI (14.4k stars)&lt;/a&gt; is a dev-first open-source autonomous AI agent framework enabling developers to build, manage &amp;amp; run useful autonomous agents.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labring/FastGPT&#34;&gt;FastGPT (12.5k stars)&lt;/a&gt; is a knowledge-based platform built on the LLM, offers out-of-the-box data processing and model invocation capabilities, allows for workflow orchestration through Flow visualization.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/joaomdmoura/crewAI&#34;&gt;CrewAI (12.1k stars)&lt;/a&gt; is a framework for orchestrating role-playing, autonomous AI agents. By fostering collaborative intelligence, CrewAI empowers agents to work together seamlessly, tackling complex tasks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/OpenBMB/XAgent&#34;&gt;XAgent (7.5k stars)&lt;/a&gt; is an open-source experimental Large Language Model (LLM) driven autonomous agent that can automatically solve various tasks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dataelement/bisheng&#34;&gt;Bisheng (5.5k stars)&lt;/a&gt; is a leading open-source platform for developing LLM applications.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/OpenBMB/XAgent&#34;&gt;Voyager (5.1k stars)&lt;/a&gt; An Open-Ended Embodied Agent with Large Language Models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/camel-ai/camel&#34;&gt;CAMEL (4.4k stars)&lt;/a&gt; is a framework that offers a comprehensive set of tools and algorithms for building multimodal AI Agents, enabling them to handle various data forms such as text, images, and speech.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/langfuse/langfuse&#34;&gt;Langfuse (2.9k stars)&lt;/a&gt; is a language fusion framework that can integrate the language abilities of multiple AI Agents, enabling them to simultaneously possess multilingual understanding and generation capabilities.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Disclaimer&lt;/strong&gt;: The codes and documents provided herein are released under the Apache-2.0 license. They should not be construed as financial counsel or recommendations for live trading. It is imperative to exercise caution and consult with qualified financial professionals prior to any trading or investment actions.&lt;/p&gt;</summary>
  </entry>
</feed>