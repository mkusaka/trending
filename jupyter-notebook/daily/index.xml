<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-01-06T01:33:36Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>PlayingNumbers/ML_Process_Course</title>
    <updated>2023-01-06T01:33:36Z</updated>
    <id>tag:github.com,2023-01-06:/PlayingNumbers/ML_Process_Course</id>
    <link href="https://github.com/PlayingNumbers/ML_Process_Course" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Public repo for the 365 Data Science ML Process Course&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ML Process Course&lt;/h1&gt; &#xA;&lt;p&gt;This is the public repository for the ML Process Course (&lt;a href=&#34;https://365datascience.com/learn-machine-learning-process-a-z/&#34;&gt;https://365datascience.com/learn-machine-learning-process-a-z/&lt;/a&gt;). In this course, we take you through the end-to-end process of building a Machine Learning Model. We did not build this course ourselves. We stood on the shoulders of giants. We think its only fair to credit all the resources we used to build this course, as we could not have created this course without the help of the ML community.&lt;/p&gt; &#xA;&lt;h2&gt;Flashcards&lt;/h2&gt; &#xA;&lt;p&gt;Please go to &lt;a href=&#34;https://ankiweb.net&#34;&gt;Ankiweb.net&lt;/a&gt; to download Anki and to sign up for account. Please got &lt;a href=&#34;https://github.com/PlayingNumbers/ML_Process_Course/raw/main/365datascience_ml_process_flashcards.apkg&#34;&gt;here&lt;/a&gt; to download the flashcards for this course.&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ol start=&#34;0&#34;&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PlayingNumbers/ML_Process_Course/raw/main/README.md#coding-workbooks-for-each-course&#34;&gt;Coding Workbooks for Each Course&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PlayingNumbers/ML_Process_Course/raw/main/README.md#data-science-blogs&#34;&gt;Data Science Blogs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PlayingNumbers/ML_Process_Course/raw/main/README.md#2-applying-ml&#34;&gt;Applying ML&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PlayingNumbers/ML_Process_Course/raw/main/README.md#3-problem-framing&#34;&gt;Problem Framing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PlayingNumbers/ML_Process_Course/raw/main/README.md#4-data-collection&#34;&gt;Data Collection&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PlayingNumbers/ML_Process_Course/raw/main/README.md#5-data-preprocessing&#34;&gt;Data Preprocessing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PlayingNumbers/ML_Process_Course/raw/main/README.md#6-exploratory-data-analysis&#34;&gt;Exploratory Data Analysis&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PlayingNumbers/ML_Process_Course/raw/main/README.md#7-feature-engineering&#34;&gt;Feature Engineering&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PlayingNumbers/ML_Process_Course/raw/main/README.md#8-cross-validation&#34;&gt;Cross Validation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PlayingNumbers/ML_Process_Course/raw/main/README.md#9-feature-selection&#34;&gt;Feature Selection&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PlayingNumbers/ML_Process_Course/raw/main/README.md#10-imbalanced-data&#34;&gt;Imbalanced Data&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PlayingNumbers/ML_Process_Course/raw/main/README.md#11-modeling&#34;&gt;Modeling&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PlayingNumbers/ML_Process_Course/raw/main/README.md#12-model-evaluation&#34;&gt;Model Evaluation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Coding Workbooks for Each Course&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Kaggle Workbook&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Google Colab&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://www.kaggle.com/code/kenjee/dealing-with-missing-values-section-5-1&#34;&gt;5-Missing Values&lt;/a&gt;, &lt;a href=&#34;https://www.kaggle.com/code/kenjee/dealing-with-outliers-section-5-2&#34;&gt;5-Outliers&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1P-4i_T1UE8_PLZibNApGbGPDxhOJDnd8?usp=sharing&#34;&gt;5-Missing Values&lt;/a&gt;, &lt;a href=&#34;https://colab.research.google.com/drive/1e_9VUn48sOebsEmDMRZ2R7OEkJLM9Zxt?usp=sharing&#34;&gt;5-Outliers&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://www.kaggle.com/code/kenjee/basic-eda-example-section-6&#34;&gt;6-EDA&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/18sWPkf2o6yX2RJu0YRZRCbJqNzc5q8ZS?usp=sharing&#34;&gt;6-EDA&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://www.kaggle.com/code/kenjee/categorical-feature-engineering-section-7-1&#34;&gt;7-Categoricals&lt;/a&gt;, &lt;a href=&#34;https://www.kaggle.com/code/kenjee/numeric-feature-engineering-section-7-2&#34;&gt;7-Continuous&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1F94kWYM_GTb-Neh_jCte04BxfcUdBodn?usp=sharing&#34;&gt;7-Categoricals&lt;/a&gt;, &lt;a href=&#34;https://colab.research.google.com/drive/1SGwguOuloOG7nd3OoOGBALR9jOg26UNt?usp=sharing&#34;&gt;7-Continuous&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://www.kaggle.com/code/kenjee/cross-validation-foundations-section-8&#34;&gt;8-Cross Validation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1xsVT5MWAX1Yq8KXMPbqU6DBti-NN17YH?usp=sharing&#34;&gt;8-Cross Validation&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://www.kaggle.com/code/kenjee/feature-selection-section-9&#34;&gt;9-Feature Selection&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/19uXC7Cm_K1FDTjkcRVjxcAY4f7dv6LDV?usp=sharing&#34;&gt;9-Feature Selection&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://www.kaggle.com/code/kenjee/dealing-with-imbalanced-data-section-10&#34;&gt;10-Imbalanced Data&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1pulqugw0V1xyoMbQrB3aTdlwO0KDXDFf?usp=sharing&#34;&gt;10-Imbalanced Data&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://www.kaggle.com/code/kenjee/model-building-example-section-11&#34;&gt;11-Model Selection&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1oV675pKGmCLIYE44a_Quw1s66c4LVcOq?usp=sharing&#34;&gt;11-Model Selection&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://www.kaggle.com/code/kenjee/model-evaluation-classification-section-12&#34;&gt;12-Model Evaluation Classification&lt;/a&gt;, &lt;a href=&#34;https://www.kaggle.com/code/kenjee/model-evaluation-regression-12&#34;&gt;12-Model Evaluation Regression&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1FYHAL3lbv7Rdh3EV9TooMFkPNv6MEJly?usp=sharing&#34;&gt;12-Model Evaluation Classification&lt;/a&gt;, &lt;a href=&#34;https://colab.research.google.com/drive/1_of9a48P-rGkrS8US9YgwQm_58_1unAV?usp=sharing&#34;&gt;12-Model Evaluation Regression&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Data Science Blogs&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/airbnb-engineering&#34;&gt;AirBnB Engineering&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://research.atspotify.com/blog/&#34;&gt;Spotify Research&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://research.netflix.com/&#34;&gt;Netflix Research&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doordash.engineering/category/data-science-and-machine-learning/&#34;&gt;DoorDash ML Blog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.uber.com/blog/honolulu/engineering/&#34;&gt;Uber Engineering&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://eng.lyft.com/tagged/data-science&#34;&gt;Lyft Engineering&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://shopify.engineering/topics/data-science-engineering&#34;&gt;Shopify Engineering&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://engineering.fb.com/&#34;&gt;Meta Engineering&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://engineering.linkedin.com/blog&#34;&gt;LinkedIn Engineering&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;2. Applying ML&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/the-analytics-hierarchy-of-needs-6d57d0e205e2&#34;&gt;Analytics Hierarchy of Needs by Ryan Foley&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://eugeneyan.com/writing/first-rule-of-ml/&#34;&gt;The First Rule of ML by Eugene Yan&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=L8D8LEDmLyE&amp;amp;ab_channel=cnvrg.io&#34;&gt;ML for Product Analytics by Ron Tidhar&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;3. Problem Framing&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.stratechi.com/problem-statement/&#34;&gt;Problem Understanding by Stratechi&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://userpilot.com/blog/product-problems/&#34;&gt;How to Define a Product Problem&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/related-works-inc/intro-to-opportunity-sizing-ce9d7e5a29c4&#34;&gt;Intro to Opportunity Sizing&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;4. Data Collection&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/swlh/searching-through-a-database-52eaaf64f89c&#34;&gt;Getting Data From a Database&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=27axs9dO7AE&amp;amp;ab_channel=DanielleTh%C3%A9&#34;&gt;What is SQL - Video&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=GEBzsz8ZSXs&amp;amp;ab_channel=LukeBarousse&#34;&gt;How Data Analysts Use SQL - Video&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=s1gD35Z4eUc&amp;amp;ab_channel=KenJee&#34;&gt;Why Are APIs Important For Data Science - Video&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.freecodecamp.org/news/what-is-an-api-in-english-please-b880a3214a82/&#34;&gt;What is an API?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://kb.narrative.io/how-do-companies-collect-data&#34;&gt;How Companies Caputre Data&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.acxiom.com/about-us/privacy/how-cookies-work/#:~:text=What%20Are%20Cookies%2C%20and%20How,some%20useful%20information%20about%20you.&#34;&gt;How Cookies Work&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/web-scraping-basics-82f8b5acd45c&#34;&gt;Web Scraping Basics&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.questionpro.com/blog/survey-data-collection/&#34;&gt;Types of Survey Techniques&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://365datascience.com/courses/sql/&#34;&gt;365 Data Science SQL Course&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;5. Data Preprocessing&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.analyticsvidhya.com/blog/2021/10/handling-missing-value/#:~:text=Types%20Of%20Missing%20Values,Missing%20Not%20At%20Random%20(MNAR)&#34;&gt;All You Need To Know About Different Types Of Missing Data Values And How To Handle It&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/7-ways-to-handle-missing-values-in-machine-learning-1a6326adf79e&#34;&gt;7 Ways to Handle Missing Values in Machine Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.kaggle.com/general/248836&#34;&gt;Null Values Imputation by Utkarsh Gupta&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.quora.com/What-are-methods-to-make-a-predictive-model-more-robust-to-outliers&#34;&gt;What are methods to make a predictive model more robust to outliers?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://statisticsbyjim.com/basics/remove-outliers&#34;&gt;Guidelines for Removing and Handling Outliers in Data by Jim Frost&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;6. Exploratory Data Analysis&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.analyticssteps.com/blogs/10-types-statistical-data-distribution-models&#34;&gt;10 Types of Statistical Data Distribution Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/exploratory-data-analysis-8fc1cb20fd15&#34;&gt;What is EDA?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=k8YxyrcAXJs&amp;amp;ab_channel=KenJee&#34;&gt;Is Data Visualization Important for Data Science? - Video&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://byjus.com/maths/box-plot/&#34;&gt;Box Plot Details&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/histograms-why-how-431a5cfbfcd5#:~:text=A%20histogram%20provides%20a%20visual,or%20gaps%20in%20the%20data.&#34;&gt;Histogram Additional Details&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.analyticsvidhya.com/blog/2017/09/6-probability-distributions-data-science/&#34;&gt;Types of Data Distributions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Skewness&#34;&gt;Understanding Skew&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.khanacademy.org/math/statistics-probability/describing-relationships-quantitative-data/introduction-to-scatterplots/a/scatterplots-and-correlation-review&#34;&gt;Scatterplots and Correlation Additional Details&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ncl.ac.uk/webtemplate/ask-assets/external/maths-resources/statistics/regression-and-correlation/types-of-correlation.html&#34;&gt;Types of Correlations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.displayr.com/what-is-a-correlation-matrix/&#34;&gt;Correlation Matrix Details&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/@szabo.bibor/how-to-create-a-seaborn-correlation-heatmap-in-python-834c0686b88e&#34;&gt;Different correlation matrices in python&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.pivot_table.html&#34;&gt;Creating Pivot Tables Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://help.flourish.studio/article/25-line-bar-and-pie-charts#:~:text=Line%20charts%20are%20ideal%20for,or%20axis%20labels%20are%20long.&#34;&gt;When to use different types of charts&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;7. Feature Engineering&lt;/h2&gt; &#xA;&lt;h3&gt;Categorical Feature Engineering&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02&#34;&gt;All about Categorical Variable Encoding by Baijayanta Roy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://machinelearningmastery.com/one-hot-encoding-for-categorical-data/&#34;&gt;Ordinal and One-Hot Encodings for Categorical Data by Jason Brownlee&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/feature-engineering-ordinal-variables-bfea697f5eee&#34;&gt;Feature Engineering Ordinal Variables by Sheng Jun&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.kaggle.com/code/ryanholbrook/target-encoding&#34;&gt;Target Encoding by Ryan Holbrook&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.mwsug.org/proceedings/2016/AA/MWSUG-2016-AA15.pdf&#34;&gt;Weight of Evidence Coding by Bruce Lund&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Continuous Feature Engineering&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sebastianraschka.com/Articles/2014_about_feature_scaling.html&#34;&gt;About Feature Scaling and Normalization by Sebastian Raschka&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.analyticsvidhya.com/blog/2021/05/feature-scaling-techniques-in-python-a-complete-guide/&#34;&gt;Feature Scaling Techniques in Python – A Complete Guide by Eddie_4072&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/#:~:text=Normalization%20is%20a%20scaling%20technique,known%20as%20Min%2DMax%20scaling.&amp;amp;text=Here%2C%20Xmax%20and%20Xmin%20are,values%20of%20the%20feature%20respectively.&#34;&gt;Feature Scaling for Machine Learning: Understanding the Difference Between Normalization vs. Standardization by Aniruddha Bhandari&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html&#34;&gt;Robust Scaler - Sklearn Docs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/@kyawsawhtoon/log-transformation-purpose-and-interpretation-9444b4b049c9&#34;&gt;Log Transformation: Purpose and Interpretation by Kyaw Saw Htoon&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/best-exponential-transformation-to-linearize-your-data-with-scipy-cca6110313a6&#34;&gt;Best exponential transformation to linearize your data with Scipy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://rikunert.com/exponential_scaler&#34;&gt;Exponentially scaling your data in order to zoom in on small differences&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sixsigmastudyguide.com/box-cox-transformation/&#34;&gt;Box Cox Transformation by Ted Hessing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://builtin.com/data-science/box-cox-transformation-target-variable&#34;&gt;Box-Cox Transformation and Target Variable: Explained&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;8. Cross Validation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/understanding-8-types-of-cross-validation-80c935a4976d&#34;&gt;Understanding 8 types of Cross-Validation by Satyam Kumar&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.analyticssteps.com/blogs/7-types-cross-validation&#34;&gt;7 Types of Cross Validation by Soumyaa Rawat&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/k-fold-cross-validation-explained-in-plain-english-659e33c0bc0&#34;&gt;k-fold cross-validation explained in plain English by Rukshan Pramoditha&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=EuBBz3bI-aA&amp;amp;t=4s&amp;amp;ab_channel=StatQuestwithJoshStarmer&#34;&gt;Machine Learning Fundamentals: Bias and Variance by Josh Starmer/Statquest&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.statology.org/leave-one-out-cross-validation/&#34;&gt;A Quick Intro to Leave-One-Out Cross-Validation (LOOCV) by Statology&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://otexts.com/fpp3/tscv.html&#34;&gt;Time Series Cross Validation by Robert Hyndman&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/time-based-cross-validation-d259b13d42b8&#34;&gt;Time Based Cross Validation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/cross-validation-k-fold-vs-monte-carlo-e54df2fc179b&#34;&gt;KFold vs. Monte Carlo by Rebecca Patro&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;9. Feature Selection&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.simplilearn.com/tutorials/machine-learning-tutorial/feature-selection-in-machine-learning&#34;&gt;Everything You Need to Know About Feature Selection In Machine Learning by Kartik Menon&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.analyticsvidhya.com/blog/2020/10/a-comprehensive-guide-to-feature-selection-using-wrapper-methods-in-python/&#34;&gt;A comprehensive guide to Feature Selection using Wrapper methods in Python&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sebastianraschka.com/faq/docs/feature_sele_categories.html&#34;&gt;What is the difference between filter, wrapper, and embedded methods for feature selection? by Sebastian Raschka&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/&#34;&gt;Introduction to Feature Selection methods with an example (or how to select the right variables?)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/feature-selection-in-python-using-filter-method-7ae5cbc4ee05&#34;&gt;Feature selection in Python using the Filter method by Renu Khandelwal&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;10. Imbalanced Data&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.analyticsvidhya.com/blog/2020/07/10-techniques-to-deal-with-class-imbalance-in-machine-learning/&#34;&gt;10 Techniques to deal with Imbalanced Classes in Machine Learning by Analytics Vidhya&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/oversampling-and-undersampling-5e2bbaf56dcf&#34;&gt;Oversampling and Undersampling by Kurtis Pykes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.analyticsvidhya.com/blog/2020/10/overcoming-class-imbalance-using-smote-techniques/&#34;&gt;Overcoming Class Imbalance using SMOTE Techniques&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://rikunert.com/smote_explained&#34;&gt;SMOTE explained for noobs – Synthetic Minority Over-sampling TEchnique line by line by Rich Data&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/smote-fdce2f605729&#34;&gt;SMOTE by Joos Korstanje&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/5-smote-techniques-for-oversampling-your-imbalance-data-b8155bdbe2b5&#34;&gt;5 SMOTE Techniques for Oversampling your Imbalance Data by Cornellius Yudha Wijaya&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/@ruinian/an-introduction-to-adasyn-with-code-1383a5ece7aa&#34;&gt;Fixing Imbalanced Datasets: An Introduction to ADASYN by Rui Nian&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;11. Modeling&lt;/h2&gt; &#xA;&lt;h3&gt;Hyperparameter Tuning&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://datascience.stackexchange.com/questions/30912/what-does-baseline-mean-in-the-context-of-machine-learning&#34;&gt;What does &#34;baseline&#34; mean in the context of machine learning?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://scikit-learn.org/stable/modules/model_evaluation.html#dummy-estimators&#34;&gt;Sklearn&#39;s Dummy Estimators&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/7-hyperparameter-optimization-techniques-every-data-scientist-should-know-12cdebe713da&#34;&gt;7 Hyperparameter Optimization Techniques Every Data Scientist Should Know&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.analyticsvidhya.com/blog/2022/02/a-comprehensive-guide-on-hyperparameter-tuning-and-its-techniques/&#34;&gt;A Comprehensive Guide on Hyperparameter Tuning and its Techniques&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/hyperparameter-tuning-in-python-21a76794a1f7&#34;&gt;Hyperparameter tuning in Python by Tooba Jamal&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf&#34;&gt;Random Search for Hyper-Parameter Optimization by James Bergestra and Yoshua Bengio&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f&#34;&gt;A Conceptual Explanation of Bayesian Hyperparameter Optimization for Machine Learning by Will Koehrsen&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://static.sigopt.com/b/20a144d208ef255d3b981ce419667ec25d8412e2/static/pdf/SigOpt_Bayesian_Optimization_Primer.pdf&#34;&gt;Bayesian Optimization Primer by SigOpt&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/genetic-algorithm-to-optimize-machine-learning-hyperparameters-72bd6e2596fc&#34;&gt;Genetic Algorithms by Marcos Del Cueto&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://machinelearningmastery.com/simulated-annealing-from-scratch-in-python/#:~:text=Simulated%20Annealing-,Simulated%20Annealing%20is%20a%20stochastic%20global%20search%20optimization%20algorithm.,it%20easier%20to%20work%20with.&#34;&gt;Simulated Annealing From Scratch in Python by Jason Brownlee&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/optimization-techniques-simulated-annealing-d6a4785a1de7&#34;&gt;Optimization Techniques — Simulated Annealing by Frank Liang&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html#id13&#34;&gt;Hyperparameter optimization for Neural Networks&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Ensembling&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.toptal.com/machine-learning/ensemble-methods-machine-learning#:~:text=Ensemble%20methods%20are%20techniques%20that,than%20a%20single%20model%20would&#34;&gt;Ensemble Methods: Elegant Techniques to Produce Improved Machine Learning Results&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://machinelearningmastery.com/tour-of-ensemble-learning-algorithms/#:~:text=The%20three%20main%20classes%20of,on%20your%20predictive%20modeling%20project.&#34;&gt;A Gentle Introduction to Ensemble Learning Algorithms by Jason Brownlee&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/types-of-ensemble-methods-in-machine-learning-4ddaf73879db&#34;&gt;Types of Ensemble methods in Machine learning by Anju Tajbangshi&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.kaggle.com/code/arthurtok/introduction-to-ensembling-stacking-in-python&#34;&gt;Introduction to Ensembling/Stacking in Python by Anisotropic&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.kaggle.com/code/eshaan90/ensembles-and-model-stacking/notebook&#34;&gt;Ensembles and Model Stacking by Eshaan Kirpal&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;12. Model Evaluation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.kdnuggets.com/2020/05/model-evaluation-metrics-machine-learning.html&#34;&gt;Model Evaluation Metrics in Machine Learning by Nagesh Singh Chauhan&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.analyticsvidhya.com/blog/2019/08/11-important-model-evaluation-error-metrics/&#34;&gt;11 Important Model Evaluation Metrics for Machine Learning Everyone should know&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://statisticsbyjim.com/regression/interpret-r-squared-regression/&#34;&gt;How To Interpret R-squared in Regression Analysis by Jim Frost&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.analyticsvidhya.com/blog/2021/05/know-the-best-evaluation-metrics-for-your-regression-model/&#34;&gt;Know The Best Evaluation Metrics for Your Regression Model by Raghav Agrawal&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/swlh/recall-precision-f1-roc-auc-and-everything-542aedf322b9&#34;&gt;Recall, Precision, F1, ROC, AUC, and everything by Ofir Shalev&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc&#34;&gt;F1 Score vs ROC AUC vs Accuracy vs PR AUC: Which Evaluation Metric Should You Choose? by Jakub Czakon&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/intuition-behind-log-loss-score-4e0c9979680a#:~:text=is%20dependent%20on.-,What%20does%20log%2Dloss%20conceptually%20mean%3F,is%20the%20log%2Dloss%20value.&#34;&gt;Intuition behind Log Loss Score by Gaurav Dembla&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://stats.stackexchange.com/questions/190216/why-is-roc-auc-equivalent-to-the-probability-that-two-randomly-selected-samples&#34;&gt;Why is ROC AUC equivalent to the probability that two randomly-selected samples are correctly ranked?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test#Area-under-curve_(AUC)_statistic_for_ROC_curves&#34;&gt;Man U Whitney Test&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test#Area-under-curve_(AUC)_statistic_for_ROC_curves&#34;&gt;Essential Things You Need to Know About F1-Score&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://paulvanderlaken.com/2019/08/16/roc-auc-precision-and-recall-visually-explained/&#34;&gt;ROC, AUC, precision, and recall visually explained by Paul Vanderlaken&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://statisticsbyjim.com/regression/r-squared-invalid-nonlinear-regression/#:~:text=Nonlinear%20regression%20is%20an%20extremely,just%20don%27t%20go%20together.&#34;&gt;R-squared Is Not Valid for Nonlinear Regression by Jim Frost&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/what-are-the-best-metrics-to-evaluate-your-regression-model-418ca481755b&#34;&gt;3 Best metrics to evaluate Regression Model? by Songhao Wu&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;12. Model Productionization&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=_0rHU6qAQe0&amp;amp;ab_channel=KenJee&#34;&gt;Git For Data Scientists - Video&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://git-scm.com/doc&#34;&gt;Git Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.freecodecamp.org/news/learn-the-basics-of-git-in-under-10-minutes-da548267cc91/&#34;&gt;Git Basics in 10 minutes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://365datascience.com/courses/git-and-github/&#34;&gt;365 Data Science Git &amp;amp; Github Course&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://machinelearningmastery.com/save-load-machine-learning-models-python-scikit-learn/&#34;&gt;Save and load ml models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/5-different-ways-to-save-your-machine-learning-model-b7996489d433&#34;&gt;5 Different Ways to Save your ML Model&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/5-tips-to-improve-your-analytics-slide-decks-c5d0559259c0&#34;&gt;Improve analytics slide decks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://streamlit.io/gallery&#34;&gt;Streamlit Gallery&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=JwSS70SZdyM&amp;amp;ab_channel=freeCodeCamp.org&#34;&gt;Build 12 streamlit apps - Video&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Yk-unX4KnV4&amp;amp;ab_channel=KenJee&#34;&gt;Streamlit Project Example - Video&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=5ZMpbdK0uqU&amp;amp;ab_channel=Indently&#34;&gt;Build an api in python - Video&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://anderfernandez.com/en/blog/how-to-create-api-python/#:~:text=To%20create%20an%20API%20in%20Python%20with%20Flask%2C%20we%20have,app%20%3D%20Flask()%20%40app.&#34;&gt;How to create an API in python&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/analytics-vidhya/how-to-create-a-python-library-7d5aea80cc3f&#34;&gt;How to create a python library&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>salesforce/LAVIS</title>
    <updated>2023-01-06T01:33:36Z</updated>
    <id>tag:github.com,2023-01-06:/salesforce/LAVIS</id>
    <link href="https://github.com/salesforce/LAVIS" rel="alternate"></link>
    <summary type="html">&lt;p&gt;LAVIS - A One-stop Library for Language-Vision Intelligence&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/salesforce/LAVIS/main/docs/_static/logo_final.png&#34; width=&#34;400&#34;&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/salesforce/LAVIS/releases&#34;&gt;&lt;img alt=&#34;Latest Release&#34; src=&#34;https://img.shields.io/github/release/salesforce/LAVIS.svg?sanitize=true&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://opensource.salesforce.com/LAVIS/index.html&#34;&gt; &lt;img alt=&#34;docs&#34; src=&#34;https://github.com/salesforce/LAVIS/actions/workflows/docs.yaml/badge.svg?sanitize=true&#34;&gt; &lt;/a&gt;&#xA; &lt;a href=&#34;https://opensource.org/licenses/BSD-3-Clause&#34;&gt; &lt;img alt=&#34;license&#34; src=&#34;https://img.shields.io/badge/License-BSD_3--Clause-blue.svg?sanitize=true&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://opensource.salesforce.com/LAVIS//latest/benchmark.html&#34;&gt;Benchmark&lt;/a&gt;, &#xA; &lt;a href=&#34;https://arxiv.org/abs/2209.09019&#34;&gt;Technical Report&lt;/a&gt;, &#xA; &lt;a href=&#34;https://opensource.salesforce.com/LAVIS//latest/index.html&#34;&gt;Documentation&lt;/a&gt;, &#xA; &lt;a href=&#34;https://github.com/salesforce/LAVIS/tree/main/examples&#34;&gt;Jupyter Notebook Examples&lt;/a&gt;, &#xA; &lt;a href=&#34;https://blog.salesforceairesearch.com/lavis-language-vision-library/&#34;&gt;Blog&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;LAVIS - A Library for Language-Vision Intelligence&lt;/h1&gt; &#xA;&lt;h2&gt;What&#39;s New: 🎉&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[Model Release] Dec 2022, released implementation of &lt;strong&gt;Img2prompt-VQA&lt;/strong&gt;, a plug-and-play module that enables off-the-shelf use of Large Language Models (LLMs) for visual question answering (VQA). Our model Img2Prompt-VQA surpasses Flamingo on zero-shot VQA on VQAv2 (61.9 vs 56.3), while in contrast requiring no end-to-end training! (&lt;a href=&#34;https://arxiv.org/pdf/2212.10846.pdf&#34;&gt;Paper&lt;/a&gt;, &lt;a href=&#34;https://github.com/salesforce/LAVIS/tree/main/projects/img2prompt-vqa&#34;&gt;Project Page&lt;/a&gt;, &lt;a href=&#34;https://colab.research.google.com/github/salesforce/LAVIS/blob/main/projects/img2prompt-vqa/img2prompt_vqa.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;[Model Release] Oct 2022, released implementation of &lt;strong&gt;PNP-VQA&lt;/strong&gt; (&lt;strong&gt;EMNLP Findings 2022&lt;/strong&gt;, by Anthony T.M.H. et al), &lt;em&gt;&#34;Plug-and-Play VQA: Zero-shot VQA by Conjoining Large Pretrained Models with Zero Training&#34;&lt;/em&gt;, a modular zero-shot VQA framework that requires no PLMs training, achieving SoTA zero-shot VQA performance. (&lt;a href=&#34;https://arxiv.org/abs/2210.08773&#34;&gt;Paper&lt;/a&gt;, &lt;a href=&#34;https://github.com/salesforce/LAVIS/tree/main/projects/pnp-vqa&#34;&gt;Project Page&lt;/a&gt;, &lt;a href=&#34;https://colab.research.google.com/github/salesforce/LAVIS/blob/main/projects/pnp-vqa/pnp_vqa.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/salesforce/LAVIS/main/#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/salesforce/LAVIS/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/salesforce/LAVIS/main/#getting-started&#34;&gt;Getting Started&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/salesforce/LAVIS/main/#model-zoo&#34;&gt;Model Zoo&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/salesforce/LAVIS/main/#image-captioning&#34;&gt;Image Captioning&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/salesforce/LAVIS/main/#visual-question-answering-vqa&#34;&gt;Visual question answering (VQA)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/salesforce/LAVIS/main/#unified-feature-extraction-interface&#34;&gt;Unified Feature Extraction Interface&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/salesforce/LAVIS/main/#load-datasets&#34;&gt;Load Datasets&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/salesforce/LAVIS/main/#jupyter-notebook-examples&#34;&gt;Jupyter Notebook Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/salesforce/LAVIS/main/#resources-and-tools&#34;&gt;Resources and Tools&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/salesforce/LAVIS/main/#documentations&#34;&gt;Documentations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/salesforce/LAVIS/main/#ethical-and-responsible-use&#34;&gt;Ethical and Responsible Use&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/salesforce/LAVIS/main/#technical-report-and-citing-lavis&#34;&gt;Technical Report and Citing LAVIS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/salesforce/LAVIS/main/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;LAVIS is a Python deep learning library for LAnguage-and-VISion intelligence research and applications. This library aims to provide engineers and researchers with a one-stop solution to rapidly develop models for their specific multimodal scenarios, and benchmark them across standard and customized datasets. It features a unified interface design to access&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;10+&lt;/strong&gt; tasks (retrieval, captioning, visual question answering, multimodal classification etc.);&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;20+&lt;/strong&gt; datasets (COCO, Flickr, Nocaps, Conceptual Commons, SBU, etc.);&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;30+&lt;/strong&gt; pretrained weights of state-of-the-art foundation language-vision models and their task-specific adaptations, including &lt;a href=&#34;https://arxiv.org/pdf/2107.07651.pdf&#34;&gt;ALBEF&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/2201.12086.pdf&#34;&gt;BLIP&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/2112.09583.pdf&#34;&gt;ALPRO&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/2103.00020.pdf&#34;&gt;CLIP&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/salesforce/LAVIS/main/assets/demo-6.png&#34;&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p&gt;Key features of LAVIS include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Unified and Modular Interface&lt;/strong&gt;: facilitating to easily leverage and repurpose existing modules (datasets, models, preprocessors), also to add new modules.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Easy Off-the-shelf Inference and Feature Extraction&lt;/strong&gt;: readily available pre-trained models let you take advantage of state-of-the-art multimodal understanding and generation capabilities on your own data.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Reproducible Model Zoo and Training Recipes&lt;/strong&gt;: easily replicate and extend state-of-the-art models on existing and new tasks.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Dataset Zoo and Automatic Downloading Tools&lt;/strong&gt;: it can be a hassle to prepare the many language-vision datasets. LAVIS provides automatic downloading scripts to help prepare a large variety of datasets and their annotations.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The following table shows the supported tasks, datasets and models in our library. This is a continuing effort and we are working on further growing the list.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Tasks&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Supported Models&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Supported Datasets&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Image-text Pre-training&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ALBEF, BLIP&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO, VisualGenome, SBU ConceptualCaptions&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Image-text Retrieval&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ALBEF, BLIP, CLIP&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO, Flickr30k&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Text-image Retrieval&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ALBEF, BLIP, CLIP&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO, Flickr30k&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Visual Question Answering&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ALBEF, BLIP&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;VQAv2, OKVQA, A-OKVQA&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Image Captioning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;BLIP&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO, NoCaps&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Image Classification&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;CLIP&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Natural Language Visual Reasoning (NLVR)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ALBEF, BLIP&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;NLVR2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Visual Entailment (VE)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ALBEF&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SNLI-VE&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Visual Dialogue&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;BLIP&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;VisDial&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Video-text Retrieval&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;BLIP, ALPRO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;MSRVTT, DiDeMo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Text-video Retrieval&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;BLIP, ALPRO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;MSRVTT, DiDeMo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Video Question Answering (VideoQA)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;BLIP, ALPRO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;MSRVTT, MSVD&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Video Dialogue&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;VGD-GPT&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;AVSD&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Multimodal Feature Extraction&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ALBEF, CLIP, BLIP, ALPRO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;customized&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Text-to-image Generation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;[COMING SOON]&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;(Optional) Creating conda environment&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n lavis python=3.8&#xA;conda activate lavis&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Cloning and building from source&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/salesforce/LAVIS.git&#xA;cd LAVIS&#xA;pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you would like to develop on LAVIS, it is recommended to install in editable mode:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Model Zoo&lt;/h3&gt; &#xA;&lt;p&gt;Model zoo summarizes supported models in LAVIS, to view:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from lavis.models import model_zoo&#xA;print(model_zoo)&#xA;# ==================================================&#xA;# Architectures                  Types&#xA;# ==================================================&#xA;# albef_classification           ve&#xA;# albef_feature_extractor        base&#xA;# albef_nlvr                     nlvr&#xA;# albef_pretrain                 base&#xA;# albef_retrieval                coco, flickr&#xA;# albef_vqa                      vqav2&#xA;# alpro_qa                       msrvtt, msvd&#xA;# alpro_retrieval                msrvtt, didemo&#xA;# blip_caption                   base_coco, large_coco&#xA;# blip_classification            base&#xA;# blip_feature_extractor         base&#xA;# blip_nlvr                      nlvr&#xA;# blip_pretrain                  base&#xA;# blip_retrieval                 coco, flickr&#xA;# blip_vqa                       vqav2, okvqa, aokvqa&#xA;# clip_feature_extractor         ViT-B-32, ViT-B-16, ViT-L-14, ViT-L-14-336, RN50&#xA;# clip                           ViT-B-32, ViT-B-16, ViT-L-14, ViT-L-14-336, RN50&#xA;# gpt_dialogue                   base&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Let’s see how to use models in LAVIS to perform inference on example data. We first load a sample image from local.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from PIL import Image&#xA;# setup device to use&#xA;device = torch.device(&#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34;)&#xA;# load sample image&#xA;raw_image = Image.open(&#34;docs/_static/merlion.png&#34;).convert(&#34;RGB&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This example image shows &lt;a href=&#34;https://en.wikipedia.org/wiki/Merlion&#34;&gt;Merlion park&lt;/a&gt; (&lt;a href=&#34;https://theculturetrip.com/asia/singapore/articles/what-exactly-is-singapores-merlion-anyway/&#34;&gt;source&lt;/a&gt;), a landmark in Singapore.&lt;/p&gt; &#xA;&lt;h3&gt;Image Captioning&lt;/h3&gt; &#xA;&lt;p&gt;In this example, we use the BLIP model to generate a caption for the image. To make inference even easier, we also associate each pre-trained model with its preprocessors (transforms), accessed via &lt;code&gt;load_model_and_preprocess()&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from lavis.models import load_model_and_preprocess&#xA;device = torch.device(&#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34;)&#xA;# loads BLIP caption base model, with finetuned checkpoints on MSCOCO captioning dataset.&#xA;# this also loads the associated image processors&#xA;model, vis_processors, _ = load_model_and_preprocess(name=&#34;blip_caption&#34;, model_type=&#34;base_coco&#34;, is_eval=True, device=device)&#xA;# preprocess the image&#xA;# vis_processors stores image transforms for &#34;train&#34; and &#34;eval&#34; (validation / testing / inference)&#xA;image = vis_processors[&#34;eval&#34;](raw_image).unsqueeze(0).to(device)&#xA;# generate caption&#xA;model.generate({&#34;image&#34;: image})&#xA;# [&#39;a large fountain spewing water into the air&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Visual question answering (VQA)&lt;/h3&gt; &#xA;&lt;p&gt;BLIP model is able to answer free-form questions about images in natural language. To access the VQA model, simply replace the &lt;code&gt;name&lt;/code&gt; and &lt;code&gt;model_type&lt;/code&gt; arguments passed to &lt;code&gt;load_model_and_preprocess()&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from lavis.models import load_model_and_preprocess&#xA;model, vis_processors, txt_processors = load_model_and_preprocess(name=&#34;blip_vqa&#34;, model_type=&#34;vqav2&#34;, is_eval=True, device=device)&#xA;# ask a random question.&#xA;question = &#34;Which city is this photo taken?&#34;&#xA;image = vis_processors[&#34;eval&#34;](raw_image).unsqueeze(0).to(device)&#xA;question = txt_processors[&#34;eval&#34;](question)&#xA;model.predict_answers(samples={&#34;image&#34;: image, &#34;text_input&#34;: question}, inference_method=&#34;generate&#34;)&#xA;# [&#39;singapore&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Unified Feature Extraction Interface&lt;/h3&gt; &#xA;&lt;p&gt;LAVIS provides a unified interface to extract features from each architecture. To extract features, we load the feature extractor variants of each model. The multimodal feature can be used for multimodal classification. The low-dimensional unimodal features can be used to compute cross-modal similarity.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from lavis.models import load_model_and_preprocess&#xA;model, vis_processors, txt_processors = load_model_and_preprocess(name=&#34;blip_feature_extractor&#34;, model_type=&#34;base&#34;, is_eval=True, device=device)&#xA;caption = &#34;a large fountain spewing water into the air&#34;&#xA;image = vis_processors[&#34;eval&#34;](raw_image).unsqueeze(0).to(device)&#xA;text_input = txt_processors[&#34;eval&#34;](caption)&#xA;sample = {&#34;image&#34;: image, &#34;text_input&#34;: [text_input]}&#xA;&#xA;features_multimodal = model.extract_features(sample)&#xA;print(features_multimodal.multimodal_embeds.shape)&#xA;# torch.Size([1, 12, 768]), use features_multimodal[:,0,:] for multimodal classification tasks&#xA;&#xA;features_image = model.extract_features(sample, mode=&#34;image&#34;)&#xA;features_text = model.extract_features(sample, mode=&#34;text&#34;)&#xA;print(features_image.image_embeds.shape)&#xA;# torch.Size([1, 197, 768])&#xA;print(features_text.text_embeds.shape)&#xA;# torch.Size([1, 12, 768])&#xA;&#xA;# low-dimensional projected features&#xA;print(features_image.image_embeds_proj.shape)&#xA;# torch.Size([1, 197, 256])&#xA;print(features_text.text_embeds_proj.shape)&#xA;# torch.Size([1, 12, 256])&#xA;similarity = features_image.image_embeds_proj[:,0,:] @ features_text.text_embeds_proj[:,0,:].t()&#xA;print(similarity)&#xA;# tensor([[0.2622]])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Load Datasets&lt;/h3&gt; &#xA;&lt;p&gt;LAVIS inherently supports a wide variety of common language-vision datasets by providing &lt;a href=&#34;https://opensource.salesforce.com/LAVIS//latest/benchmark&#34;&gt;automatic download tools&lt;/a&gt; to help download and organize these datasets. After downloading, to load the datasets, use the following code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from lavis.datasets.builders import dataset_zoo&#xA;dataset_names = dataset_zoo.get_names()&#xA;print(dataset_names)&#xA;# [&#39;aok_vqa&#39;, &#39;coco_caption&#39;, &#39;coco_retrieval&#39;, &#39;coco_vqa&#39;, &#39;conceptual_caption_12m&#39;,&#xA;#  &#39;conceptual_caption_3m&#39;, &#39;didemo_retrieval&#39;, &#39;flickr30k&#39;, &#39;imagenet&#39;, &#39;laion2B_multi&#39;,&#xA;#  &#39;msrvtt_caption&#39;, &#39;msrvtt_qa&#39;, &#39;msrvtt_retrieval&#39;, &#39;msvd_caption&#39;, &#39;msvd_qa&#39;, &#39;nlvr&#39;,&#xA;#  &#39;nocaps&#39;, &#39;ok_vqa&#39;, &#39;sbu_caption&#39;, &#39;snli_ve&#39;, &#39;vatex_caption&#39;, &#39;vg_caption&#39;, &#39;vg_vqa&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After downloading the images, we can use &lt;code&gt;load_dataset()&lt;/code&gt; to obtain the dataset.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from lavis.datasets.builders import load_dataset&#xA;coco_dataset = load_dataset(&#34;coco_caption&#34;)&#xA;print(coco_dataset.keys())&#xA;# dict_keys([&#39;train&#39;, &#39;val&#39;, &#39;test&#39;])&#xA;print(len(coco_dataset[&#34;train&#34;]))&#xA;# 566747&#xA;print(coco_dataset[&#34;train&#34;][0])&#xA;# {&#39;image&#39;: &amp;lt;PIL.Image.Image image mode=RGB size=640x480&amp;gt;,&#xA;#  &#39;text_input&#39;: &#39;A woman wearing a net on her head cutting a cake. &#39;,&#xA;#  &#39;image_id&#39;: 0}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you already host a local copy of the dataset, you can pass in the &lt;code&gt;vis_path&lt;/code&gt; argument to change the default location to load images.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;coco_dataset = load_dataset(&#34;coco_caption&#34;, vis_path=YOUR_LOCAL_PATH)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Jupyter Notebook Examples&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/salesforce/LAVIS/tree/main/examples&#34;&gt;examples&lt;/a&gt; for more inference examples, e.g. captioning, feature extraction, VQA, GradCam, zeros-shot classification.&lt;/p&gt; &#xA;&lt;h2&gt;Resources and Tools&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Benchmarks&lt;/strong&gt;: see &lt;a href=&#34;https://opensource.salesforce.com/LAVIS//latest/benchmark&#34;&gt;Benchmark&lt;/a&gt; for instructions to evaluate and train supported models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Dataset Download and Browsing&lt;/strong&gt;: see &lt;a href=&#34;https://opensource.salesforce.com/LAVIS//latest/benchmark&#34;&gt;Dataset Download&lt;/a&gt; for instructions and automatic tools on download common language-vision datasets.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;GUI Demo&lt;/strong&gt;: to run the demo locally, run &lt;code&gt;bash run_scripts/run_demo.sh&lt;/code&gt; and then follow the instruction on the prompts to view in browser. A web demo is coming soon.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Documentations&lt;/h2&gt; &#xA;&lt;p&gt;For more details and advanced usages, please refer to &lt;a href=&#34;https://opensource.salesforce.com/LAVIS//latest/index.html#&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Ethical and Responsible Use&lt;/h2&gt; &#xA;&lt;p&gt;We note that models in LAVIS provide no guarantees on their multimodal abilities; incorrect or biased predictions may be observed. In particular, the datasets and pretrained models utilized in LAVIS may contain socioeconomic biases which could result in misclassification and other unwanted behaviors such as offensive or inappropriate speech. We strongly recommend that users review the pre-trained models and overall system in LAVIS before practical adoption. We plan to improve the library by investigating and mitigating these potential biases and inappropriate behaviors in the future.&lt;/p&gt; &#xA;&lt;h2&gt;Technical Report and Citing LAVIS&lt;/h2&gt; &#xA;&lt;p&gt;You can find more details in our &lt;a href=&#34;https://arxiv.org/abs/2209.09019&#34;&gt;technical report&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;re using LAVIS in your research or applications, please cite using this BibTeX:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{li2022lavis,&#xA;      title={LAVIS: A Library for Language-Vision Intelligence}, &#xA;      author={Dongxu Li and Junnan Li and Hung Le and Guangsen Wang and Silvio Savarese and Steven C. H. Hoi},&#xA;      year={2022},&#xA;      eprint={2209.09019},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contact us&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions, comments or suggestions, please do not hesitate to contact us at &lt;a href=&#34;mailto:lavis@salesforce.com&#34;&gt;lavis@salesforce.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/salesforce/LAVIS/main/LICENSE.txt&#34;&gt;BSD 3-Clause License&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>afizs/chatgpt-clone</title>
    <updated>2023-01-06T01:33:36Z</updated>
    <id>tag:github.com,2023-01-06:/afizs/chatgpt-clone</id>
    <link href="https://github.com/afizs/chatgpt-clone" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Build Your own ChatGPT with OpenAI API and Streamlit&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;chatgpt-clone&lt;/h1&gt; &#xA;&lt;p&gt;Build Your own ChatGPT with OpenAI API and Streamlit&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Get your OpenAI API key here - &lt;a href=&#34;https://beta.openai.com/account/api-keys&#34;&gt;https://beta.openai.com/account/api-keys&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Replace that key in the &lt;code&gt;config.py&lt;/code&gt; code&lt;/li&gt; &#xA; &lt;li&gt;Install the required libraries &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;streamlit run chatgpt.py&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick Demo:&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/5618143/208082095-7e5cadea-4478-4e7d-a96d-e47a52cdce9b.mp4&#34;&gt;https://user-images.githubusercontent.com/5618143/208082095-7e5cadea-4478-4e7d-a96d-e47a52cdce9b.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This project is not affiliated with OpenAI in any way. Use at your own risk. I am not responsible for any damage caused by this project. Please read the &lt;a href=&#34;https://beta.openai.com/terms&#34;&gt;OpenAI Terms of Service&lt;/a&gt; before using this project.&lt;/p&gt;</summary>
  </entry>
</feed>