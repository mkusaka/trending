<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-08T01:38:50Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Anil-matcha/Notion-to-Chatbot</title>
    <updated>2023-07-08T01:38:50Z</updated>
    <id>tag:github.com,2023-07-08:/Anil-matcha/Notion-to-Chatbot</id>
    <link href="https://github.com/Anil-matcha/Notion-to-Chatbot" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Chat with any Notion document. Easily input the document content you&#39;d like to chat with. Instant answers. Ask questions, extract information, and summarize documents with AI. Sources included.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Notion to Chatbot&lt;/h1&gt; &#xA;&lt;p&gt;Chat with any Notion document.&lt;/p&gt; &#xA;&lt;p&gt;Easily input the document content you&#39;d like to chat with. Instant answers. Ask questions, extract information, and summarize documents with AI. Sources included.&lt;/p&gt; &#xA;&lt;h3&gt;Getting Started&lt;/h3&gt; &#xA;&lt;p&gt;Code is up now, ‚≠ê (Star) the repo to receive updates&lt;/p&gt; &#xA;&lt;p&gt;Replit and streamlit version coming soon&lt;/p&gt; &#xA;&lt;p&gt;Follow &lt;a href=&#34;https://twitter.com/matchaman11&#34;&gt;Anil Chandra Naidu Matcha&lt;/a&gt; on twitter for updates&lt;/p&gt; &#xA;&lt;p&gt;Subscribe to &lt;a href=&#34;https://www.youtube.com/@AnilChandraNaiduMatcha&#34;&gt;https://www.youtube.com/@AnilChandraNaiduMatcha&lt;/a&gt; for more such video tutorials&lt;/p&gt; &#xA;&lt;h3&gt;Also check&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Anil-matcha/ChatPDF&#34;&gt;Chat with PDF code&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Anil-matcha/Website-to-Chatbot&#34;&gt;Chat with Website code&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Anil-matcha/Chat-With-Excel&#34;&gt;Chat with CSV code&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Anil-matcha/DiscordGPT&#34;&gt;ChatGPT in Discord code&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>bowang-lab/scGPT</title>
    <updated>2023-07-08T01:38:50Z</updated>
    <id>tag:github.com,2023-07-08:/bowang-lab/scGPT</id>
    <link href="https://github.com/bowang-lab/scGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;scGPT&lt;/h1&gt; &#xA;&lt;p&gt;This is the official codebase for &lt;strong&gt;scGPT: Towards Building a Foundation Model for Single-Cell Multi-omics Using Generative AI&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.biorxiv.org/content/10.1101/2023.04.30.538439&#34;&gt;Preprint&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://scgpt.readthedocs.io/en/latest/&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;!UPDATE: We have released several new pretrained scGPT checkpoints. Please see the &lt;a href=&#34;https://raw.githubusercontent.com/bowang-lab/scGPT/main/#pretrained-scGPT-checkpoints&#34;&gt;Pretrained scGPT checkpoints&lt;/a&gt; section for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;scGPT is available on PyPI. To install scGPT, run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pip install scgpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;[Optional] We recommend using &lt;a href=&#34;https://wandb.ai/&#34;&gt;wandb&lt;/a&gt; for logging and visualization.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pip install wandb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For developing, we are using the &lt;a href=&#34;https://python-poetry.org/&#34;&gt;Poetry&lt;/a&gt; package manager. To install Poetry, follow the instructions &lt;a href=&#34;https://python-poetry.org/docs/#installation&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git clone this-repo-url&#xA;$ cd scGPT&#xA;$ poetry install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The &lt;code&gt;flash-attn&lt;/code&gt; dependency usually requires specific GPU and CUDA version. If you encounter any issues, please refer to the &lt;a href=&#34;https://github.com/HazyResearch/flash-attention/tree/main&#34;&gt;flash-attn&lt;/a&gt; repository for installation instructions. For now, May 2023, we recommend using CUDA 11.7 and flash-attn&amp;lt;1.0.5 due to various issues reported about installing new versions of flash-attn.&lt;/p&gt; &#xA;&lt;h2&gt;Pretrained scGPT Model Zoo&lt;/h2&gt; &#xA;&lt;p&gt;Here is the list of pretrained models. Please find the links for downloading the checkpoint folders. We recommend using the &lt;code&gt;whole-human&lt;/code&gt; model for most applications by default. If your fine-tuning dataset shares similar cell type context with the training data of the organ-specific models, these models can usually demonstrate competitive performance as well.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model name&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;whole-human (recommended)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Pretrained on 33 million normal human cells.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/1oWh_-ZRdhtoGQ2Fw24HP41FgLoomVo-y?usp=sharing&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;brain&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Pretrained on 13.2 million brain cells.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/1vf1ijfQSk7rGdDGpBntR5bi5g6gNt-Gx?usp=sharing&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;blood&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Pretrained on 10.3 million blood and bone marrow cells.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/1kkug5C7NjvXIwQGGaGoqXTk_Lb_pDrBU?usp=sharing&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;heart&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Pretrained on 1.8 million heart cells&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/1GcgXrd7apn6y4Ze_iSCncskX3UsWPY2r?usp=sharing&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;lung&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Pretrained on 2.1 million lung cells&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/16A1DJ30PT6bodt4bWLa4hpS7gbWZQFBG?usp=sharing&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;kidney&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Pretrained on 814 thousand kidney cells&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/1S-1AR65DF120kNFpEbWCvRHPhpkGK3kK?usp=sharing&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;pan-cancer&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Pretrained on 5.7 million cells of various cancer types&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/13QzLHilYUd0v3HTwa_9n4G4yEF-hdkqa?usp=sharing&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Fine-tune scGPT for scRNA-seq integration&lt;/h2&gt; &#xA;&lt;p&gt;Please see our example code in &lt;a href=&#34;https://raw.githubusercontent.com/bowang-lab/scGPT/main/examples/finetune_integration.py&#34;&gt;examples/finetune_integration.py&lt;/a&gt;. By default, the script assumes the scGPT checkpoint folder stored in the &lt;code&gt;examples/save&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;h2&gt;To-do-list&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Upload the pretrained model checkpoint&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Publish to pypi&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Provide the pretraining code with generative attention masking&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Finetuning examples for multi-omics integration, cell type annotation, perturbation prediction, cell generation&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Example code for Gene Regulatory Network analysis&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Documentation website with readthedocs&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Bump up to pytorch 2.0&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; New pretraining on larger datasets&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Reference mapping example&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Publish to huggingface model hub&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We greatly welcome contributions to scGPT. Please submit a pull request if you have any ideas or bug fixes. We also welcome any issues you encounter while using scGPT.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;We sincerely thank the authors of following open-source projects:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/HazyResearch/flash-attention&#34;&gt;flash-attention&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/scverse/scanpy&#34;&gt;scanpy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/scverse/scvi-tools&#34;&gt;scvi-tools&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/theislab/scib&#34;&gt;scib&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/datasets&#34;&gt;datasets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;transformers&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citing scGPT&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{cui2023scGPT,&#xA;title={scGPT: Towards Building a Foundation Model for Single-Cell Multi-omics Using Generative AI},&#xA;author={Cui, Haotian and Wang, Chloe and Maan, Hassaan and Pang, Kuan and Luo, Fengning and Wang, Bo},&#xA;journal={bioRxiv},&#xA;year={2023},&#xA;publisher={Cold Spring Harbor Laboratory}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>Sentdex/Falcon-LLM</title>
    <updated>2023-07-08T01:38:50Z</updated>
    <id>tag:github.com,2023-07-08:/Sentdex/Falcon-LLM</id>
    <link href="https://github.com/Sentdex/Falcon-LLM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Helper scripts and examples for exploring the Falcon LLM models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Falcon-LLM&lt;/h1&gt; &#xA;&lt;p&gt;Helper scripts and examples for exploring the Falcon LLM models&lt;/p&gt; &#xA;&lt;p&gt;Overview of the model and use-cases: &lt;a href=&#34;https://www.youtube.com/watch?v=-IV1NTGy6Mg&#34;&gt;https://www.youtube.com/watch?v=-IV1NTGy6Mg&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Files:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;api_server.py&lt;/code&gt; - Run locally or in cloud. Should fully set up a proper web server if you intend to host on a public IP, this is using the basic flask demo web server.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;api_client.py&lt;/code&gt; - Make requests to the server. Makes R&amp;amp;D a lot easier if you can load and access the model separately, even if everything is on the same machine, so you&#39;re not re-loading the model every single time you make a change to your script. You can also use a notebook, but, depending on the complexity of your project, this might not be good enough.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Falcon-40B-demo.ipynb&lt;/code&gt; - a short notebook example of loading Falcon 40B with options for various datatypes (4, 8, and 16bit).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;setup.sh&lt;/code&gt; - a quick shell script for setup of requirements that I used for Lambda H100 machines. (&lt;code&gt;chmod +x setup.sh&lt;/code&gt; &amp;amp; &lt;code&gt;./setup.sh&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>