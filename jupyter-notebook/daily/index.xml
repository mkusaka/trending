<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-06-21T01:32:36Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>facebookresearch/EdgeTAM</title>
    <updated>2025-06-21T01:32:36Z</updated>
    <id>tag:github.com,2025-06-21:/facebookresearch/EdgeTAM</id>
    <link href="https://github.com/facebookresearch/EdgeTAM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[CVPR 2025] Official PyTorch implementation of &#34;EdgeTAM: On-Device Track Anything Model&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;EdgeTAM: On-Device Track Anything Model&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://chongzhou96.github.io/&#34;&gt;Chong Zhou&lt;sup&gt;1,2*&lt;/sup&gt;&lt;/a&gt;, &lt;a href=&#34;https://sites.google.com/andrew.cmu.edu/zcckernel/home&#34;&gt;Chenchen Zhu&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, &lt;a href=&#34;https://pages.cs.wisc.edu/~yunyang/&#34;&gt;Yunyang Xiong&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, &lt;a href=&#34;https://www.cs.umd.edu/~sakshams/&#34;&gt;Saksham Suri&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, &lt;a href=&#34;https://fanyix.cs.ucdavis.edu/&#34;&gt;Fanyi Xiao&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, &lt;a href=&#34;https://sites.google.com/view/lemeng-wu/home&#34;&gt;Lemeng Wu&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=F1mr9C0AAAAJ&amp;amp;hl=en&#34;&gt;Raghuraman Krishnamoorthi&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, &lt;a href=&#34;https://daibo.info/&#34;&gt;Bo Dai&lt;sup&gt;3,4&lt;/sup&gt;&lt;/a&gt;, &lt;a href=&#34;https://www.mmlab-ntu.com/person/ccloy/&#34;&gt;Chen Change Loy&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;, &lt;a href=&#34;https://v-chandra.github.io/&#34;&gt;Vikas Chandra&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=9nXD6pwAAAAJ&amp;amp;hl=en&#34;&gt;Bilge Soran&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;Meta Reality Labs, &lt;sup&gt;2&lt;/sup&gt;S-Lab, Nanyang Technological University, &lt;sup&gt;3&lt;/sup&gt;University of Hong Kong, &lt;sup&gt;4&lt;/sup&gt;Feeling AI&lt;/p&gt; &#xA;&lt;p&gt;(*) Work done during the internship at Meta Reality Labs.&lt;/p&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://arxiv.org/abs/2501.07256&#34;&gt;&lt;code&gt;Paper&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/spaces/facebook/EdgeTAM&#34;&gt;&lt;code&gt;Demo&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/EdgeTAM/main/#citing-edgetam&#34;&gt;&lt;code&gt;BibTeX&lt;/code&gt;&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;EdgeTAM&lt;/strong&gt; is an on-device executable variant of the SAM 2 for promptable segmentation and tracking in videos. It runs &lt;strong&gt;22× faster&lt;/strong&gt; than SAM 2 and achieves &lt;strong&gt;16 FPS&lt;/strong&gt; on iPhone 15 Pro Max without quantization.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/EdgeTAM/main/assets/speed-performance.png?raw=true&#34; width=&#34;400&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;In this figure, we show the speed-performance trade-offs of EdgeTAM and other models on iPhone 15 Pro Max (red) and NVIDIA A100 (blue). We report the J&amp;amp;F on the SA-V val dataset as the evaluation metric.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;EdgeTAM needs to be installed first before use. The code requires &lt;code&gt;python&amp;gt;=3.10&lt;/code&gt;, as well as &lt;code&gt;torch&amp;gt;=2.3.1&lt;/code&gt; and &lt;code&gt;torchvision&amp;gt;=0.18.1&lt;/code&gt;. Please follow the instructions &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;here&lt;/a&gt; to install both PyTorch and TorchVision dependencies. You can install EdgeTAM on a GPU machine using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/facebookresearch/EdgeTAM.git &amp;amp;&amp;amp; cd EdgeTAM&#xA;&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use the EdgeTAM predictor and run the example notebooks, &lt;code&gt;jupyter&lt;/code&gt; and &lt;code&gt;matplotlib&lt;/code&gt; are required and can be installed by:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e &#34;.[notebooks]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;It&#39;s recommended to create a new Python environment via &lt;a href=&#34;https://www.anaconda.com/&#34;&gt;Anaconda&lt;/a&gt; for this installation and install PyTorch 2.3.1 (or higher) via &lt;code&gt;pip&lt;/code&gt; following &lt;a href=&#34;https://pytorch.org/&#34;&gt;https://pytorch.org/&lt;/a&gt;. If you have a PyTorch version lower than 2.3.1 in your current environment, the installation command above will try to upgrade it to the latest PyTorch version using &lt;code&gt;pip&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The step above requires compiling a custom CUDA kernel with the &lt;code&gt;nvcc&lt;/code&gt; compiler. If it isn&#39;t already available on your machine, please install the &lt;a href=&#34;https://developer.nvidia.com/cuda-toolkit-archive&#34;&gt;CUDA toolkits&lt;/a&gt; with a version that matches your PyTorch CUDA version.&lt;/li&gt; &#xA; &lt;li&gt;If you see a message like &lt;code&gt;Failed to build the SAM 2 CUDA extension&lt;/code&gt; during installation, you can ignore it and still use EdgeTAM (some post-processing functionality may be limited, but it doesn&#39;t affect the results in most cases).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Downloading the model&lt;/h3&gt; &#xA;&lt;p&gt;Model is available &lt;a href=&#34;https://github.com/facebookresearch/EdgeTAM/tree/main/checkpoints/edgetam.pt&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;On-device Gradio demo for EdgeTAM&lt;/h3&gt; &#xA;&lt;p&gt;Follow the instructions below to run the on-device demo for EdgeTAM. If you want to quickly try out the demo, you can also go to &lt;a href=&#34;https://huggingface.co/spaces/facebook/EdgeTAM&#34;&gt;Hugging Face Spaces&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Install the dependencies for the Gradio demo:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e &#34;.[gradio]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run the demo:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python gradio_app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The demo will be available at &lt;a href=&#34;http://127.0.0.1:7860/&#34;&gt;http://127.0.0.1:7860/&lt;/a&gt; by default. You can change the port by setting the &lt;code&gt;--port&lt;/code&gt; argument.&lt;/p&gt; &#xA;&lt;h3&gt;Image prediction&lt;/h3&gt; &#xA;&lt;p&gt;EdgeTAM has all the capabilities of &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;SAM&lt;/a&gt; on static images, and we provide image prediction APIs that closely resemble SAM for image use cases. The &lt;code&gt;SAM2ImagePredictor&lt;/code&gt; class has an easy interface for image prompting.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from sam2.build_sam import build_sam2&#xA;from sam2.sam2_image_predictor import SAM2ImagePredictor&#xA;&#xA;checkpoint = &#34;./checkpoints/edgetam.pt&#34;&#xA;model_cfg = &#34;configs/edgetam.yaml&#34;&#xA;predictor = SAM2ImagePredictor(build_sam2(model_cfg, checkpoint))&#xA;&#xA;with torch.inference_mode(), torch.autocast(&#34;cuda&#34;, dtype=torch.bfloat16):&#xA;    predictor.set_image(&amp;lt;your_image&amp;gt;)&#xA;    masks, _, _ = predictor.predict(&amp;lt;input_prompts&amp;gt;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please refer to the examples in &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/EdgeTAM/main/notebooks/image_predictor_example.ipynb&#34;&gt;image_predictor_example.ipynb&lt;/a&gt; for static image use cases.&lt;/p&gt; &#xA;&lt;p&gt;EdgeTAM also supports automatic mask generation on images just like SAM. Please see &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/EdgeTAM/main/notebooks/automatic_mask_generator_example.ipynb&#34;&gt;automatic_mask_generator_example.ipynb&lt;/a&gt; for automatic mask generation in images.&lt;/p&gt; &#xA;&lt;h3&gt;Video prediction&lt;/h3&gt; &#xA;&lt;p&gt;For promptable segmentation and tracking in videos, we provide a video predictor with APIs for example to add prompts and propagate masklets throughout a video. EdgeTAM supports video inference on multiple objects and uses an inference state to keep track of the interactions in each video.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from sam2.build_sam import build_sam2_video_predictor&#xA;&#xA;checkpoint = &#34;./checkpoints/edgetam.pt&#34;&#xA;model_cfg = &#34;configs/edgetam.yaml&#34;&#xA;predictor = build_sam2_video_predictor(model_cfg, checkpoint)&#xA;&#xA;with torch.inference_mode(), torch.autocast(&#34;cuda&#34;, dtype=torch.bfloat16):&#xA;    state = predictor.init_state(&amp;lt;your_video&amp;gt;)&#xA;&#xA;    # add new prompts and instantly get the output on the same frame&#xA;    frame_idx, object_ids, masks = predictor.add_new_points_or_box(state, &amp;lt;your_prompts&amp;gt;):&#xA;&#xA;    # propagate the prompts to get masklets throughout the video&#xA;    for frame_idx, object_ids, masks in predictor.propagate_in_video(state):&#xA;        ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please refer to the examples in &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/EdgeTAM/main/notebooks/video_predictor_example.ipynb&#34;&gt;video_predictor_example.ipynb&lt;/a&gt; for details on how to add click or box prompts, make refinements, and track multiple objects in videos.&lt;/p&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;h3&gt;Promptable Video Segmentation (PVS)&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/EdgeTAM/main/assets/pvs.png?raw=true&#34; width=&#34;700&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Zero-shot PVS accuracy across 9 datasets in offline and online settings.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Video Object Segmentation (VOS)&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Method&lt;/th&gt; &#xA;   &lt;th&gt;MOSE val&lt;/th&gt; &#xA;   &lt;th&gt;DAVIS 2017 val&lt;/th&gt; &#xA;   &lt;th&gt;SA-V val&lt;/th&gt; &#xA;   &lt;th&gt;SA-V test&lt;/th&gt; &#xA;   &lt;th&gt;YTVOS 2019 val&lt;/th&gt; &#xA;   &lt;th&gt;A100&lt;/th&gt; &#xA;   &lt;th&gt;V100&lt;/th&gt; &#xA;   &lt;th&gt;iPhone&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;STCN&lt;/td&gt; &#xA;   &lt;td&gt;52.5&lt;/td&gt; &#xA;   &lt;td&gt;85.4&lt;/td&gt; &#xA;   &lt;td&gt;61.0&lt;/td&gt; &#xA;   &lt;td&gt;62.5&lt;/td&gt; &#xA;   &lt;td&gt;82.7&lt;/td&gt; &#xA;   &lt;td&gt;62.8&lt;/td&gt; &#xA;   &lt;td&gt;13.2&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SwinB-AOT&lt;/td&gt; &#xA;   &lt;td&gt;59.4&lt;/td&gt; &#xA;   &lt;td&gt;85.4&lt;/td&gt; &#xA;   &lt;td&gt;51.1&lt;/td&gt; &#xA;   &lt;td&gt;50.3&lt;/td&gt; &#xA;   &lt;td&gt;84.5&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SwinB-DeAOT&lt;/td&gt; &#xA;   &lt;td&gt;59.9&lt;/td&gt; &#xA;   &lt;td&gt;86.2&lt;/td&gt; &#xA;   &lt;td&gt;61.4&lt;/td&gt; &#xA;   &lt;td&gt;61.8&lt;/td&gt; &#xA;   &lt;td&gt;86.1&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RDE&lt;/td&gt; &#xA;   &lt;td&gt;46.8&lt;/td&gt; &#xA;   &lt;td&gt;84.2&lt;/td&gt; &#xA;   &lt;td&gt;51.8&lt;/td&gt; &#xA;   &lt;td&gt;53.9&lt;/td&gt; &#xA;   &lt;td&gt;81.9&lt;/td&gt; &#xA;   &lt;td&gt;88.8&lt;/td&gt; &#xA;   &lt;td&gt;24.4&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;XMem&lt;/td&gt; &#xA;   &lt;td&gt;59.6&lt;/td&gt; &#xA;   &lt;td&gt;86.0&lt;/td&gt; &#xA;   &lt;td&gt;60.1&lt;/td&gt; &#xA;   &lt;td&gt;62.3&lt;/td&gt; &#xA;   &lt;td&gt;85.6&lt;/td&gt; &#xA;   &lt;td&gt;61.2&lt;/td&gt; &#xA;   &lt;td&gt;22.6&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SimVOS-B&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;88.0&lt;/td&gt; &#xA;   &lt;td&gt;44.2&lt;/td&gt; &#xA;   &lt;td&gt;44.1&lt;/td&gt; &#xA;   &lt;td&gt;84.2&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;3.3&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;JointFormer&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;90.1&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;87.4&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;3.0&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ISVOS&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;88.2&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;86.3&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;5.8&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DEVA&lt;/td&gt; &#xA;   &lt;td&gt;66.0&lt;/td&gt; &#xA;   &lt;td&gt;87.0&lt;/td&gt; &#xA;   &lt;td&gt;55.4&lt;/td&gt; &#xA;   &lt;td&gt;56.2&lt;/td&gt; &#xA;   &lt;td&gt;85.4&lt;/td&gt; &#xA;   &lt;td&gt;65.2&lt;/td&gt; &#xA;   &lt;td&gt;25.3&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Cutie-base&lt;/td&gt; &#xA;   &lt;td&gt;69.9&lt;/td&gt; &#xA;   &lt;td&gt;87.9&lt;/td&gt; &#xA;   &lt;td&gt;60.7&lt;/td&gt; &#xA;   &lt;td&gt;62.7&lt;/td&gt; &#xA;   &lt;td&gt;87.0&lt;/td&gt; &#xA;   &lt;td&gt;65.0&lt;/td&gt; &#xA;   &lt;td&gt;36.4&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Cutie-base+&lt;/td&gt; &#xA;   &lt;td&gt;71.7&lt;/td&gt; &#xA;   &lt;td&gt;88.1&lt;/td&gt; &#xA;   &lt;td&gt;61.3&lt;/td&gt; &#xA;   &lt;td&gt;62.8&lt;/td&gt; &#xA;   &lt;td&gt;87.5&lt;/td&gt; &#xA;   &lt;td&gt;57.2&lt;/td&gt; &#xA;   &lt;td&gt;17.9&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SAM 2-B+&lt;/td&gt; &#xA;   &lt;td&gt;75.8&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;90.9&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;73.6&lt;/td&gt; &#xA;   &lt;td&gt;74.1&lt;/td&gt; &#xA;   &lt;td&gt;88.4&lt;/td&gt; &#xA;   &lt;td&gt;64.8&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;0.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SAM 2.1-B+&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;76.6&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;90.2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;76.8&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;77.0&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;88.6&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;64.1&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;0.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;EdgeTAM&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;70.0&lt;/td&gt; &#xA;   &lt;td&gt;87.7&lt;/td&gt; &#xA;   &lt;td&gt;72.3&lt;/td&gt; &#xA;   &lt;td&gt;71.7&lt;/td&gt; &#xA;   &lt;td&gt;86.2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;150.9&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;15.7&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;em&gt;We report the G for YTVOS and J&amp;amp;F for other datasets. The FPS on A100 is obtained with torch compile. Nota that, for SAM 2, SAM 2.1, and EdgeTAM, we evaluate all the datasets with the same model.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Segment Anything (SA)&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Data&lt;/th&gt; &#xA;   &lt;th&gt;SA-23 All&lt;/th&gt; &#xA;   &lt;th&gt;SA-23 Image&lt;/th&gt; &#xA;   &lt;th&gt;SA-23 Video&lt;/th&gt; &#xA;   &lt;th&gt;FPS&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SAM&lt;/td&gt; &#xA;   &lt;td&gt;SA-1B&lt;/td&gt; &#xA;   &lt;td&gt;58.1 (81.3)&lt;/td&gt; &#xA;   &lt;td&gt;60.8 (82.1)&lt;/td&gt; &#xA;   &lt;td&gt;54.5 (80.3)&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SAM 2&lt;/td&gt; &#xA;   &lt;td&gt;SA-1B&lt;/td&gt; &#xA;   &lt;td&gt;58.9 (81.7)&lt;/td&gt; &#xA;   &lt;td&gt;60.8 (82.1)&lt;/td&gt; &#xA;   &lt;td&gt;56.4 (81.2)&lt;/td&gt; &#xA;   &lt;td&gt;1.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SAM 2&lt;/td&gt; &#xA;   &lt;td&gt;SAM2’s mix&lt;/td&gt; &#xA;   &lt;td&gt;61.4 (83.7)&lt;/td&gt; &#xA;   &lt;td&gt;63.1 (83.9)&lt;/td&gt; &#xA;   &lt;td&gt;59.1 (83.3)&lt;/td&gt; &#xA;   &lt;td&gt;1.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SAM 2.1&lt;/td&gt; &#xA;   &lt;td&gt;SAM2’s mix&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;61.9 (83.5)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;63.3 (83.8)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;60.1 (83.2)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;EdgeTAM&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Our mix&lt;/td&gt; &#xA;   &lt;td&gt;55.5 (81.7)&lt;/td&gt; &#xA;   &lt;td&gt;56.0 (81.9)&lt;/td&gt; &#xA;   &lt;td&gt;54.8 (81.5)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;40.4&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;em&gt;We report 1 (5) click mIoU results. FPS is measured on iPhone 15 Pro Max. Our mix does not contain the internal datasets that SAM 2 uses.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The EdgeTAM model checkpoints and code are licensed under &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/EdgeTAM/main/LICENSE&#34;&gt;Apache 2.0&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citing EdgeTAM&lt;/h2&gt; &#xA;&lt;p&gt;If you use EdgeTAM in your research, please use the following BibTeX entry.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{zhou2025edgetam,&#xA;  title={EdgeTAM: On-Device Track Anything Model},&#xA;  author={Zhou, Chong and Zhu, Chenchen and Xiong, Yunyang and Suri, Saksham and Xiao, Fanyi and Wu, Lemeng and Krishnamoorthi, Raghuraman and Dai, Bo and Loy, Chen Change and Chandra, Vikas and Soran, Bilge},&#xA;  journal={arXiv preprint arXiv:2501.07256},&#xA;  year={2025}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>