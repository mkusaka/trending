<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-30T01:30:38Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>TommyZihao/vlm_arm</title>
    <updated>2024-05-30T01:30:38Z</updated>
    <id>tag:github.com,2024-05-30:/TommyZihao/vlm_arm</id>
    <link href="https://github.com/TommyZihao/vlm_arm" rel="alternate"></link>
    <summary type="html">&lt;p&gt;机械臂+大模型+多模态=人机协作具身智能体&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;机械臂+大模型+多模态=人机协作具身智能体&lt;/h1&gt; &#xA;&lt;p&gt;视频【机械臂接入GPT4o大模型，秒变多模态AI贾维斯】：&lt;a href=&#34;https://www.bilibili.com/video/BV18w4m1U7Fi&#34;&gt;https://www.bilibili.com/video/BV18w4m1U7Fi&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/TommyZihao/vlm_arm/assets/36354458/c1eacaaa-f895-4a27-b736-f314fe7cb1f2&#34; alt=&#34;架构图&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;目标：听人话、看图像、找坐标、排动作、定格式&lt;/p&gt; &#xA;&lt;p&gt;智能体Agent大语言模型：Yi-Large、Claude 3 Opus&lt;/p&gt; &#xA;&lt;p&gt;多模态视觉理解大模型：GPT4v、GPT4o、Yi-Vision、Claude 3 Opus、通义千问Qwen-VL-Max&lt;/p&gt; &#xA;&lt;p&gt;机械臂：大象机器人 Mycobot 280 Pi&lt;/p&gt; &#xA;&lt;p&gt;开发板：树莓派4B Ubuntu 20.04&lt;/p&gt; &#xA;&lt;p&gt;作者：同济子豪兄&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>2g-XzenG/Claim-PT</title>
    <updated>2024-05-30T01:30:38Z</updated>
    <id>tag:github.com,2024-05-30:/2g-XzenG/Claim-PT</id>
    <link href="https://github.com/2g-XzenG/Claim-PT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Claim-PT: Pretrained transformer framework on pediatric claims data for population specific tasks&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains the tensorflow implementation of the following paper:&lt;/p&gt; &#xA;&lt;p&gt;Paper Name: Pretrained transformer framework on pediatric claims data for population specific tasks&lt;/p&gt; &#xA;&lt;p&gt;Authors: Xianlong Zeng, Simon L. Linwood, Chang Liu&lt;/p&gt; &#xA;&lt;p&gt;Abstract: The adoption of electronic health records (EHR) has become universal during the past decade, which has afforded in-depth data-based research. By learning from the large amount of healthcare data, various data-driven models have been built to predict future events for different medical tasks, such as auto diagnosis and heart-attack prediction. Although EHR is abundant, the population that satisfies specific criteria for learning population-specific tasks is scarce, making it challenging to train data-hungry deep learning models. This study presents the Claim Pre-Training (Claim-PT) framework, a generic pre-training model that first trains on the entire pediatric claims dataset, followed by a discriminative fine-tuning on each population-specific task. The semantic meaning of medical events can be captured in the pre-training stage, and the effective knowledge transfer is completed through the task-aware fine-tuning stage. The fine-tuning process requires minimal parameter modification without changing the model architecture, which mitigates the data scarcity issue and helps train the deep learning model adequately on small patient cohorts. We conducted experiments on a real-world claims dataset with more than one million patient records. Experimental results on two downstream tasks demonstrated the effectiveness of our method: our general task-agnostic pre-training framework outperformed tailored task-specific models, achieving more than 10% higher in model performance as compared to baselines. In addition, our framework showed a great generalizability potential to transfer learned knowledge from one institution to another, paving the way for future healthcare model pre-training across institutions.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.nature.com/articles/s41598-022-07545-1&#34;&gt;Paper Link&lt;/a&gt;: Zeng, Xianlong, Simon L. Linwood, and Chang Liu. &#34;Pretrained transformer framework on pediatric claims data for population specific tasks.&#34; Scientific Reports 12, no. 1 (2022): 3651.&lt;/p&gt; &#xA;&lt;h1&gt;Environment&lt;/h1&gt; &#xA;&lt;p&gt;Ubuntu16.04, Python3.7, TensorFlow2.1&lt;/p&gt; &#xA;&lt;h1&gt;Model Pretraining&lt;/h1&gt; &#xA;&lt;p&gt;in folder pretraining&lt;/p&gt; &#xA;&lt;h3&gt;List of hyper-parameter we used in the pre-training stage&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;MAX_VISIT=30&lt;/li&gt; &#xA; &lt;li&gt;MAX_CODE=10&lt;/li&gt; &#xA; &lt;li&gt;MAX_DEMO=2&lt;/li&gt; &#xA; &lt;li&gt;PATIENT_DIM=100&lt;/li&gt; &#xA; &lt;li&gt;BATCH_SIZE = 100&lt;/li&gt; &#xA; &lt;li&gt;TRAIN_RATIO = 0.8&lt;/li&gt; &#xA; &lt;li&gt;DATA_SIZE = len(age_seq)&lt;/li&gt; &#xA; &lt;li&gt;EPOCHS = 1000&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Downstream finetune&lt;/h1&gt; &#xA;&lt;p&gt;in folder fineune&lt;/p&gt; &#xA;&lt;h1&gt;Attention playground&lt;/h1&gt; &#xA;&lt;p&gt;in folder attention&lt;/p&gt;</summary>
  </entry>
</feed>