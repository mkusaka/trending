<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-10-08T01:35:02Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ashishpatel26/LLM-Finetuning</title>
    <updated>2023-10-08T01:35:02Z</updated>
    <id>tag:github.com,2023-10-08:/ashishpatel26/LLM-Finetuning</id>
    <link href="https://github.com/ashishpatel26/LLM-Finetuning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;LLM Finetuning with peft&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LLM-Finetuning&lt;/h1&gt; &#xA;&lt;h1&gt;PEFT Fine-Tuning Project üöÄ&lt;/h1&gt; &#xA;&lt;p&gt;Welcome to the PEFT (Pretraining-Evaluation Fine-Tuning) project repository! This project focuses on efficiently fine-tuning large language models using LoRA and Hugging Face&#39;s transformers library.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/images/trl_overview.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Fine Tuning Notebook Table üìë&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Notebook Title&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Colab Badge&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;1. Efficiently Train Large Language Models with LoRA and Hugging Face&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Details and code for efficient training of large language models using LoRA and Hugging Face.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/ashishpatel26/LLM-Finetuning/blob/main/1.Efficiently_train_Large_Language_Models_with_LoRA_and_Hugging_Face.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open in Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;2. Fine-Tune Your Own Llama 2 Model in a Colab Notebook&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Guide to fine-tuning your Llama 2 model using Colab.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/ashishpatel26/LLM-Finetuning/blob/main/2.Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open in Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;3. Guanaco Chatbot Demo with LLaMA-7B Model&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Showcase of a chatbot demo powered by LLaMA-7B model.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/ashishpatel26/LLM-Finetuning/blob/main/3.Guanaco%20Chatbot%20Demo%20with%20LLaMA-7B%20Model.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open in Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;4. PEFT Finetune-Bloom-560m-tagger&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Project details for PEFT Finetune-Bloom-560m-tagger.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/ashishpatel26/LLM-Finetuning/blob/main/4.PEFT%20Finetune-Bloom-560m-tagger.ipynb#scrollTo=MDqJWba-tpnv&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open in Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;5. Finetune_Meta_OPT-6-1b_Model_bnb_peft&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Details and guide for finetuning the Meta OPT-6-1b Model using PEFT and Bloom-560m-tagger.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/ashishpatel26/LLM-Finetuning/blob/main/5.Finetune_Meta_OPT-6-1b_Model_bnb_peft.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open in Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;6.Finetune Falcon-7b with BNB Self Supervised Training&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Guide for finetuning Falcon-7b using BNB self-supervised training.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/ashishpatel26/LLM-Finetuning/blob/main/6.Finetune%20Falcon-7b%20with%20BNB%20Self%20Supervised%20Training.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open in Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;7.FineTune LLaMa2 with QLoRa&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Guide to fine-tune the Llama 2 7B pre-trained model using the PEFT library and QLoRa method&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/ashishpatel26/LLM-Finetuning/blob/main/7.FineTune_LLAMA2_with_QLORA.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open in Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;8.Stable_Vicuna13B_8bit_in_Colab&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Guide of Fine Tuning Vecuna 13B_8bit&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/ashishpatel26/LLM-Finetuning/blob/main/8.Stable_Vicuna13B_8bit_in_Colab.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open in Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;9. GPT-Neo-X-20B-bnb2bit_training&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Guide How to train the GPT-NeoX-20B model using bfloat16 precision&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/ashishpatel26/LLM-Finetuning/blob/main/9.GPT-neo-x-20B-bnb_4bit_training.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open in Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;10. MPT-Instruct-30B Model Training&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MPT-Instruct-30B is a large language model from MosaicML that is trained on a dataset of short-form instructions. It can be used to follow instructions, answer questions, and generate text.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/ashishpatel26/LLM-Finetuning/blob/main/10.MPT_Instruct_30B.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open in Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;11.RLHF_Training_for_CustomDataset_for_AnyModel&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;How train a Model with RLHF training on any LLM model with custom dataset&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/ashishpatel26/LLM-Finetuning/blob/main/11_RLHF_Training_for_CustomDataset_for_AnyModel.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open in Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;12.Fine_tuning_Microsoft_Phi_1_5b_on_custom_dataset(dialogstudio)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;How train a model with trl SFT Training on Microsoft Phi 1.5 with custom&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/ashishpatel26/LLM-Finetuning/blob/main/12_Fine_tuning_Microsoft_Phi_1_5b_on_custom_dataset(dialogstudio).ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open in Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;13. Finetuning OpenAI GPT3.5 Turbo&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;How to finetune GPT 3.5 on your own data&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/ashishpatel26/LLM-Finetuning/blob/main/13.Fine_tuning_OpenAI_GPT_3_5_turbo.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open in Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;14. Finetuning Mistral-7b FineTuning Model using Autotrain-advanced&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;How to finetune Mistral-7b using autotrained-advanced&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/ashishpatel26/LLM-Finetuning/blob/main/14.Finetuning_Mistral_7b_Using_AutoTrain.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open in Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Contributing ü§ù&lt;/h2&gt; &#xA;&lt;p&gt;Contributions are welcome! If you&#39;d like to contribute to this project, feel free to open an issue or submit a pull request.&lt;/p&gt; &#xA;&lt;h2&gt;License üìù&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/ashishpatel26/LLM-Finetuning/main/LICENSE&#34;&gt;MIT License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Created with ‚ù§Ô∏è by &lt;a href=&#34;https://github.com/ashishpatel26/&#34;&gt;Ashish&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>luca-medeiros/lang-segment-anything</title>
    <updated>2023-10-08T01:35:02Z</updated>
    <id>tag:github.com,2023-10-08:/luca-medeiros/lang-segment-anything</id>
    <link href="https://github.com/luca-medeiros/lang-segment-anything" rel="alternate"></link>
    <summary type="html">&lt;p&gt;SAM with text prompt&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Language Segment-Anything&lt;/h1&gt; &#xA;&lt;p&gt;Language Segment-Anything is an open-source project that combines the power of instance segmentation and text prompts to generate masks for specific objects in images. Built on the recently released Meta model, segment-anything, and the GroundingDINO detection model, it&#39;s an easy-to-use and effective tool for object detection and image segmentation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/luca-medeiros/lang-segment-anything/main/assets/outputs/person.png&#34; alt=&#34;person.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Zero-shot text-to-bbox approach for object detection.&lt;/li&gt; &#xA; &lt;li&gt;GroundingDINO detection model integration.&lt;/li&gt; &#xA; &lt;li&gt;Easy deployment using the Lightning AI app platform.&lt;/li&gt; &#xA; &lt;li&gt;Customizable text prompts for precise object segmentation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.7 or higher&lt;/li&gt; &#xA; &lt;li&gt;torch (tested 2.0)&lt;/li&gt; &#xA; &lt;li&gt;torchvision&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install torch torchvision&#xA;pip install -U git+https://github.com/luca-medeiros/lang-segment-anything.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or Clone the repository and install the required packages:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/luca-medeiros/lang-segment-anything &amp;amp;&amp;amp; cd lang-segment-anything&#xA;pip install torch torchvision&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or use Conda Create a Conda environment from the &lt;code&gt;environment.yml&lt;/code&gt; file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env create -f environment.yml&#xA;# Activate the new environment:&#xA;conda activate lsa&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Usage&lt;/h3&gt; &#xA;&lt;p&gt;To run the Lightning AI APP:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;lightning run app app.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Use as a library:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from PIL import Image&#xA;from lang_sam import LangSAM&#xA;&#xA;model = LangSAM()&#xA;image_pil = Image.open(&#34;./assets/car.jpeg&#34;).convert(&#34;RGB&#34;)&#xA;text_prompt = &#34;wheel&#34;&#xA;masks, boxes, phrases, logits = model.predict(image_pil, text_prompt)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Use with custom checkpoint:&lt;/p&gt; &#xA;&lt;p&gt;First download a model checkpoint.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from PIL import Image&#xA;from lang_sam import LangSAM&#xA;&#xA;model = LangSAM(&#34;&amp;lt;model_type&amp;gt;&#34;, &#34;&amp;lt;path/to/checkpoint&amp;gt;&#34;)&#xA;image_pil = Image.open(&#34;./assets/car.jpeg&#34;).convert(&#34;RGB&#34;)&#xA;text_prompt = &#34;wheel&#34;&#xA;masks, boxes, phrases, logits = model.predict(image_pil, text_prompt)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/luca-medeiros/lang-segment-anything/main/assets/outputs/car.png&#34; alt=&#34;car.png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/luca-medeiros/lang-segment-anything/main/assets/outputs/kiwi.png&#34; alt=&#34;kiwi.png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/luca-medeiros/lang-segment-anything/main/assets/outputs/person.png&#34; alt=&#34;person.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;p&gt;Future goals for this project include:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;FastAPI integration&lt;/strong&gt;: To streamline deployment even further, we plan to add FastAPI code to our project, making it easier for users to deploy and interact with the model.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Labeling pipeline&lt;/strong&gt;: We want to create a labeling pipeline that allows users to input both the text prompt and the image and receive labeled instance segmentation outputs. This would help users efficiently generate results for further analysis and training.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Implement CLIP version&lt;/strong&gt;: To (maybe) enhance the model&#39;s capabilities and performance, we will explore the integration of OpenAI&#39;s CLIP model. This could provide improved language understanding and potentially yield better instance segmentation results.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;This project is based on the following repositories:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO&#34;&gt;GroundingDINO&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;Segment-Anything&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Lightning-AI/lightning&#34;&gt;Lightning AI&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the Apache 2.0 License&lt;/p&gt;</summary>
  </entry>
</feed>