<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-09-09T01:37:24Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>leandromoreira/digital_video_introduction</title>
    <updated>2022-09-09T01:37:24Z</updated>
    <id>tag:github.com,2022-09-09:/leandromoreira/digital_video_introduction</id>
    <link href="https://github.com/leandromoreira/digital_video_introduction" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A hands-on introduction to video technology: image, video, codec (av1, vp9, h265) and more (ffmpeg encoding).&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/README-cn.md&#34; title=&#34;Simplified Chinese&#34;&gt;🇨🇳&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/README-ja.md&#34; title=&#34;Japanese&#34;&gt;🇯🇵&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/README-it.md&#34; title=&#34;Italian&#34;&gt;🇮🇹&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/README-ko.md&#34; title=&#34;Korean&#34;&gt;🇰🇷&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/README-ru.md&#34; title=&#34;Russian&#34;&gt;🇷🇺&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://img.shields.io/badge/license-BSD--3--Clause-blue.svg&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-BSD--3--Clause-blue.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Intro&lt;/h1&gt; &#xA;&lt;p&gt;A gentle introduction to video technology, although it&#39;s aimed at software developers / engineers, we want to make it easy &lt;strong&gt;for anyone to learn&lt;/strong&gt;. This idea was born during a &lt;a href=&#34;https://docs.google.com/presentation/d/17Z31kEkl_NGJ0M66reqr9_uTG6tI5EDDVXpdPKVuIrs/edit#slide=id.p&#34;&gt;mini workshop for newcomers to video technology&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The goal is to introduce some digital video concepts with a &lt;strong&gt;simple vocabulary, lots of visual elements and practical examples&lt;/strong&gt; when possible, and make this knowledge available everywhere. Please, feel free to send corrections, suggestions and improve it.&lt;/p&gt; &#xA;&lt;p&gt;There will be &lt;strong&gt;hands-on&lt;/strong&gt; sections which require you to have &lt;strong&gt;docker installed&lt;/strong&gt; and this repository cloned.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/leandromoreira/digital_video_introduction.git&#xA;cd digital_video_introduction&#xA;./setup.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;WARNING&lt;/strong&gt;: when you see a &lt;code&gt;./s/ffmpeg&lt;/code&gt; or &lt;code&gt;./s/mediainfo&lt;/code&gt; command, it means we&#39;re running a &lt;strong&gt;containerized version&lt;/strong&gt; of that program, which already includes all the needed requirements.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;All the &lt;strong&gt;hands-on should be performed from the folder you cloned&lt;/strong&gt; this repository. For the &lt;strong&gt;jupyter examples&lt;/strong&gt; you must start the server &lt;code&gt;./s/start_jupyter.sh&lt;/code&gt; and copy the URL and use it in your browser.&lt;/p&gt; &#xA;&lt;h1&gt;Changelog&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;added DRM system&lt;/li&gt; &#xA; &lt;li&gt;released version 1.0.0&lt;/li&gt; &#xA; &lt;li&gt;added simplified Chinese translation&lt;/li&gt; &#xA; &lt;li&gt;added FFmpeg oscilloscope filter example&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Index&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#intro&#34;&gt;Intro&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#index&#34;&gt;Index&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#basic-terminology&#34;&gt;Basic terminology&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#other-ways-to-encode-a-color-image&#34;&gt;Other ways to encode a color image&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#hands-on-play-around-with-image-and-color&#34;&gt;Hands-on: play around with image and color&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#dvd-is-dar-43&#34;&gt;DVD is DAR 4:3&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#hands-on-check-video-properties&#34;&gt;Hands-on: Check video properties&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#redundancy-removal&#34;&gt;Redundancy removal&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#colors-luminance-and-our-eyes&#34;&gt;Colors, Luminance and our eyes&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#color-model&#34;&gt;Color model&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#converting-between-ycbcr-and-rgb&#34;&gt;Converting between YCbCr and RGB&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#chroma-subsampling&#34;&gt;Chroma subsampling&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#hands-on-check-ycbcr-histogram&#34;&gt;Hands-on: Check YCbCr histogram&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#frame-types&#34;&gt;Frame types&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#i-frame-intra-keyframe&#34;&gt;I Frame (intra, keyframe)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#p-frame-predicted&#34;&gt;P Frame (predicted)&lt;/a&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#hands-on-a-video-with-a-single-i-frame&#34;&gt;Hands-on: A video with a single I-frame&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#b-frame-bi-predictive&#34;&gt;B Frame (bi-predictive)&lt;/a&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#hands-on-compare-videos-with-b-frame&#34;&gt;Hands-on: Compare videos with B-frame&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#summary&#34;&gt;Summary&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#temporal-redundancy-inter-prediction&#34;&gt;Temporal redundancy (inter prediction)&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#hands-on-see-the-motion-vectors&#34;&gt;Hands-on: See the motion vectors&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#spatial-redundancy-intra-prediction&#34;&gt;Spatial redundancy (intra prediction)&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#hands-on-check-intra-predictions&#34;&gt;Hands-on: Check intra predictions&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#how-does-a-video-codec-work&#34;&gt;How does a video codec work?&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#what-why-how&#34;&gt;What? Why? How?&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#history&#34;&gt;History&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#the-birth-of-av1&#34;&gt;The birth of AV1&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#a-generic-codec&#34;&gt;A generic codec&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#1st-step---picture-partitioning&#34;&gt;1st step - picture partitioning&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#hands-on-check-partitions&#34;&gt;Hands-on: Check partitions&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#2nd-step---predictions&#34;&gt;2nd step - predictions&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#3rd-step---transform&#34;&gt;3rd step - transform&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#hands-on-throwing-away-different-coefficients&#34;&gt;Hands-on: throwing away different coefficients&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#4th-step---quantization&#34;&gt;4th step - quantization&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#hands-on-quantization&#34;&gt;Hands-on: quantization&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#5th-step---entropy-coding&#34;&gt;5th step - entropy coding&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#vlc-coding&#34;&gt;VLC coding&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#arithmetic-coding&#34;&gt;Arithmetic coding&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#hands-on-cabac-vs-cavlc&#34;&gt;Hands-on: CABAC vs CAVLC&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#6th-step---bitstream-format&#34;&gt;6th step - bitstream format&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#h264-bitstream&#34;&gt;H.264 bitstream&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#hands-on-inspect-the-h264-bitstream&#34;&gt;Hands-on: Inspect the H.264 bitstream&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#review&#34;&gt;Review&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#how-does-h265-achieve-a-better-compression-ratio-than-h264&#34;&gt;How does H.265 achieve a better compression ratio than H.264?&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#online-streaming&#34;&gt;Online streaming&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#general-architecture&#34;&gt;General architecture&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#progressive-download-and-adaptive-streaming&#34;&gt;Progressive download and adaptive streaming&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#content-protection&#34;&gt;Content protection&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#how-to-use-jupyter&#34;&gt;How to use jupyter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#conferences&#34;&gt;Conferences&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Basic terminology&lt;/h1&gt; &#xA;&lt;p&gt;An &lt;strong&gt;image&lt;/strong&gt; can be thought of as a &lt;strong&gt;2D matrix&lt;/strong&gt;. If we think about &lt;strong&gt;colors&lt;/strong&gt;, we can extrapolate this idea seeing this image as a &lt;strong&gt;3D matrix&lt;/strong&gt; where the &lt;strong&gt;additional dimensions&lt;/strong&gt; are used to provide &lt;strong&gt;color data&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If we chose to represent these colors using the &lt;a href=&#34;https://en.wikipedia.org/wiki/Primary_color&#34;&gt;primary colors (red, green and blue)&lt;/a&gt;, we define three planes: the first one for &lt;strong&gt;red&lt;/strong&gt;, the second for &lt;strong&gt;green&lt;/strong&gt;, and the last one for the &lt;strong&gt;blue&lt;/strong&gt; color.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/image_3d_matrix_rgb.png&#34; alt=&#34;an image is a 3d matrix RGB&#34; title=&#34;An image is a 3D matrix&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;We&#39;ll call each point in this matrix &lt;strong&gt;a pixel&lt;/strong&gt; (picture element). One pixel represents the &lt;strong&gt;intensity&lt;/strong&gt; (usually a numeric value) of a given color. For example, a &lt;strong&gt;red pixel&lt;/strong&gt; means 0 of green, 0 of blue and maximum of red. The &lt;strong&gt;pink color pixel&lt;/strong&gt; can be formed with a combination of the three colors. Using a representative numeric range from 0 to 255, the pink pixel is defined by &lt;strong&gt;Red=255, Green=192 and Blue=203&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;Other ways to encode a color image&lt;/h4&gt; &#xA; &lt;p&gt;Many other possible models may be used to represent the colors that make up an image. We could, for instance, use an indexed palette where we&#39;d only need a single byte to represent each pixel instead of the 3 needed when using the RGB model. In such a model we could use a 2D matrix instead of a 3D matrix to represent our color, this would save on memory but yield fewer color options.&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/nes-color-palette.png&#34; alt=&#34;NES palette&#34; title=&#34;NES palette&#34;&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;For instance, look at the picture down below. The first face is fully colored. The others are the red, green, and blue planes (shown as gray tones).&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/rgb_channels_intensity.png&#34; alt=&#34;RGB channels intensity&#34; title=&#34;RGB channels intensity&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;We can see that the &lt;strong&gt;red color&lt;/strong&gt; will be the one that &lt;strong&gt;contributes more&lt;/strong&gt; (the brightest parts in the second face) to the final color while the &lt;strong&gt;blue color&lt;/strong&gt; contribution can be mostly &lt;strong&gt;only seen in Mario&#39;s eyes&lt;/strong&gt; (last face) and part of his clothes, see how &lt;strong&gt;all planes contribute less&lt;/strong&gt; (darkest parts) to the &lt;strong&gt;Mario&#39;s mustache&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;And each color intensity requires a certain amount of bits, this quantity is known as &lt;strong&gt;bit depth&lt;/strong&gt;. Let&#39;s say we spend &lt;strong&gt;8 bits&lt;/strong&gt; (accepting values from 0 to 255) per color (plane), therefore we have a &lt;strong&gt;color depth&lt;/strong&gt; of &lt;strong&gt;24 bits&lt;/strong&gt; (8 bits * 3 planes R/G/B), and we can also infer that we could use 2 to the power of 24 different colors.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;It&#39;s great&lt;/strong&gt; to learn &lt;a href=&#34;http://www.cambridgeincolour.com/tutorials/camera-sensors.htm&#34;&gt;how an image is captured from the world to the bits&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Another property of an image is the &lt;strong&gt;resolution&lt;/strong&gt;, which is the number of pixels in one dimension. It is often presented as width × height, for example, the &lt;strong&gt;4×4&lt;/strong&gt; image below.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/resolution.png&#34; alt=&#34;image resolution&#34; title=&#34;image resolution&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;Hands-on: play around with image and color&lt;/h4&gt; &#xA; &lt;p&gt;You can &lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/image_as_3d_array.ipynb&#34;&gt;play around with image and colors&lt;/a&gt; using &lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#how-to-use-jupyter&#34;&gt;jupyter&lt;/a&gt; (python, numpy, matplotlib and etc).&lt;/p&gt; &#xA; &lt;p&gt;You can also learn &lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/filters_are_easy.ipynb&#34;&gt;how image filters (edge detection, sharpen, blur...) work&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Another property we can see while working with images or video is the &lt;strong&gt;aspect ratio&lt;/strong&gt; which simply describes the proportional relationship between width and height of an image or pixel.&lt;/p&gt; &#xA;&lt;p&gt;When people says this movie or picture is &lt;strong&gt;16x9&lt;/strong&gt; they usually are referring to the &lt;strong&gt;Display Aspect Ratio (DAR)&lt;/strong&gt;, however we also can have different shapes of individual pixels, we call this &lt;strong&gt;Pixel Aspect Ratio (PAR)&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/DAR.png&#34; alt=&#34;display aspect ratio&#34; title=&#34;display aspect ratio&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/PAR.png&#34; alt=&#34;pixel aspect ratio&#34; title=&#34;pixel aspect ratio&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;DVD is DAR 4:3&lt;/h4&gt; &#xA; &lt;p&gt;Although the real resolution of a DVD is 704x480 it still keeps a 4:3 aspect ratio because it has a PAR of 10:11 (704x10/480x11)&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Finally, we can define a &lt;strong&gt;video&lt;/strong&gt; as a &lt;strong&gt;succession of &lt;em&gt;n&lt;/em&gt; frames&lt;/strong&gt; in &lt;strong&gt;time&lt;/strong&gt; which can be seen as another dimension, &lt;em&gt;n&lt;/em&gt; is the frame rate or frames per second (FPS).&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/video.png&#34; alt=&#34;video&#34; title=&#34;video&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The number of bits per second needed to show a video is its &lt;strong&gt;bit rate&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;bit rate = width * height * bit depth * frames per second&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;For example, a video with 30 frames per second, 24 bits per pixel, resolution of 480x240 will need &lt;strong&gt;82,944,000 bits per second&lt;/strong&gt; or 82.944 Mbps (30x480x240x24) if we don&#39;t employ any kind of compression.&lt;/p&gt; &#xA;&lt;p&gt;When the &lt;strong&gt;bit rate&lt;/strong&gt; is nearly constant it&#39;s called constant bit rate (&lt;strong&gt;CBR&lt;/strong&gt;) but it also can vary then called variable bit rate (&lt;strong&gt;VBR&lt;/strong&gt;).&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;This graph shows a constrained VBR which doesn&#39;t spend too many bits while the frame is black.&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/vbr.png&#34; alt=&#34;constrained vbr&#34; title=&#34;constrained vbr&#34;&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;In the early days, engineers came up with a technique for doubling the perceived frame rate of a video display &lt;strong&gt;without consuming extra bandwidth&lt;/strong&gt;. This technique is known as &lt;strong&gt;interlaced video&lt;/strong&gt;; it basically sends half of the screen in 1 &#34;frame&#34; and the other half in the next &#34;frame&#34;.&lt;/p&gt; &#xA;&lt;p&gt;Today screens render mostly using &lt;strong&gt;progressive scan technique&lt;/strong&gt;. Progressive is a way of displaying, storing, or transmitting moving images in which all the lines of each frame are drawn in sequence.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/interlaced_vs_progressive.png&#34; alt=&#34;interlaced vs progressive&#34; title=&#34;interlaced vs progressive&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Now we have an idea about how an &lt;strong&gt;image&lt;/strong&gt; is represented digitally, how its &lt;strong&gt;colors&lt;/strong&gt; are arranged, how many &lt;strong&gt;bits per second&lt;/strong&gt; do we spend to show a video, if it&#39;s constant (CBR) or variable (VBR), with a given &lt;strong&gt;resolution&lt;/strong&gt; using a given &lt;strong&gt;frame rate&lt;/strong&gt; and many other terms such as interlaced, PAR and others.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;Hands-on: Check video properties&lt;/h4&gt; &#xA; &lt;p&gt;You can &lt;a href=&#34;https://github.com/leandromoreira/introduction_video_technology/raw/master/encoding_pratical_examples.md#inspect-stream&#34;&gt;check most of the explained properties with ffmpeg or mediainfo.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h1&gt;Redundancy removal&lt;/h1&gt; &#xA;&lt;p&gt;We learned that it&#39;s not feasible to use video without any compression; &lt;strong&gt;a single one hour video&lt;/strong&gt; at 720p resolution with 30fps would &lt;strong&gt;require 278GB&lt;sup&gt;*&lt;/sup&gt;&lt;/strong&gt;. Since &lt;strong&gt;using solely lossless data compression algorithms&lt;/strong&gt; like DEFLATE (used in PKZIP, Gzip, and PNG), &lt;strong&gt;won&#39;t&lt;/strong&gt; decrease the required bandwidth sufficiently we need to find other ways to compress the video.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;sup&gt;*&lt;/sup&gt; We found this number by multiplying 1280 x 720 x 24 x 30 x 3600 (width, height, bits per pixel, fps and time in seconds)&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;In order to do this, we can &lt;strong&gt;exploit how our vision works&lt;/strong&gt;. We&#39;re better at distinguishing brightness than colors, the &lt;strong&gt;repetitions in time&lt;/strong&gt;, a video contains a lot of images with few changes, and the &lt;strong&gt;repetitions within the image&lt;/strong&gt;, each frame also contains many areas using the same or similar color.&lt;/p&gt; &#xA;&lt;h2&gt;Colors, Luminance and our eyes&lt;/h2&gt; &#xA;&lt;p&gt;Our eyes are &lt;a href=&#34;http://vanseodesign.com/web-design/color-luminance/&#34;&gt;more sensitive to brightness than colors&lt;/a&gt;, you can test it for yourself, look at this picture.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/luminance_vs_color.png&#34; alt=&#34;luminance vs color&#34; title=&#34;luminance vs color&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you are unable to see that the colors of the &lt;strong&gt;squares A and B are identical&lt;/strong&gt; on the left side, that&#39;s fine, it&#39;s our brain playing tricks on us to &lt;strong&gt;pay more attention to light and dark than color&lt;/strong&gt;. There is a connector, with the same color, on the right side so we (our brain) can easily spot that in fact, they&#39;re the same color.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Simplistic explanation of how our eyes work&lt;/strong&gt; The &lt;a href=&#34;http://www.biologymad.com/nervoussystem/eyenotes.htm&#34;&gt;eye is a complex organ&lt;/a&gt;, it is composed of many parts but we are mostly interested in the cones and rods cells. The eye &lt;a href=&#34;https://en.wikipedia.org/wiki/Photoreceptor_cell&#34;&gt;contains about 120 million rod cells and 6 million cone cells&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;To &lt;strong&gt;oversimplify&lt;/strong&gt;, let&#39;s try to put colors and brightness in the eye&#39;s parts function. The &lt;strong&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Rod_cell&#34;&gt;rod cells&lt;/a&gt; are mostly responsible for brightness&lt;/strong&gt; while the &lt;strong&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Cone_cell&#34;&gt;cone cells&lt;/a&gt; are responsible for color&lt;/strong&gt;, there are three types of cones, each with different pigment, namely: &lt;a href=&#34;https://upload.wikimedia.org/wikipedia/commons/1/1e/Cones_SMJ2_E.svg&#34;&gt;S-cones (Blue), M-cones (Green) and L-cones (Red)&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;Since we have many more rod cells (brightness) than cone cells (color), one can infer that we are more capable of distinguishing dark and light than colors.&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/eyes.jpg&#34; alt=&#34;eyes composition&#34; title=&#34;eyes composition&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Contrast sensitivity functions&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;Researchers of experimental psychology and many other fields have developed many theories on human vision. And one of them is called Contrast sensitivity functions. They are related to spatio and temporal of the light and their value presents at given init light, how much change is required before an observer reported there was a change. Notice the plural of the word &#34;function&#34;, this is for the reason that we can measure Contrast sensitivity functions with not only black-white but also colors. The result of these experiments shows that in most cases our eyes are more sensitive to brightness than color.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Once we know that we&#39;re more sensitive to &lt;strong&gt;luma&lt;/strong&gt; (the brightness in an image) we can try to exploit it.&lt;/p&gt; &#xA;&lt;h3&gt;Color model&lt;/h3&gt; &#xA;&lt;p&gt;We first learned &lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#basic-terminology&#34;&gt;how to color images&lt;/a&gt; work using the &lt;strong&gt;RGB model&lt;/strong&gt;, but there are other models too. In fact, there is a model that separates luma (brightness) from chrominance (colors) and it is known as &lt;strong&gt;YCbCr&lt;/strong&gt;&lt;sup&gt;*&lt;/sup&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;sup&gt;*&lt;/sup&gt; there are more models which do the same separation.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;This color model uses &lt;strong&gt;Y&lt;/strong&gt; to represent the brightness and two color channels &lt;strong&gt;Cb&lt;/strong&gt; (chroma blue) and &lt;strong&gt;Cr&lt;/strong&gt; (chroma red). The &lt;a href=&#34;https://en.wikipedia.org/wiki/YCbCr&#34;&gt;YCbCr&lt;/a&gt; can be derived from RGB and it also can be converted back to RGB. Using this model we can create full colored images as we can see down below.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/ycbcr.png&#34; alt=&#34;ycbcr example&#34; title=&#34;ycbcr example&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Converting between YCbCr and RGB&lt;/h3&gt; &#xA;&lt;p&gt;Some may argue, how can we produce all the &lt;strong&gt;colors without using the green&lt;/strong&gt;?&lt;/p&gt; &#xA;&lt;p&gt;To answer this question, we&#39;ll walk through a conversion from RGB to YCbCr. We&#39;ll use the coefficients from the &lt;strong&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Rec._601&#34;&gt;standard BT.601&lt;/a&gt;&lt;/strong&gt; that was recommended by the &lt;strong&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/ITU-R&#34;&gt;group ITU-R&lt;sup&gt;*&lt;/sup&gt;&lt;/a&gt;&lt;/strong&gt; . The first step is to &lt;strong&gt;calculate the luma&lt;/strong&gt;, we&#39;ll use the constants suggested by ITU and replace the RGB values.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Y = 0.299R + 0.587G + 0.114B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once we had the luma, we can &lt;strong&gt;split the colors&lt;/strong&gt; (chroma blue and red):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Cb = 0.564(B - Y)&#xA;Cr = 0.713(R - Y)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And we can also &lt;strong&gt;convert it back&lt;/strong&gt; and even get the &lt;strong&gt;green by using YCbCr&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;R = Y + 1.402Cr&#xA;B = Y + 1.772Cb&#xA;G = Y - 0.344Cb - 0.714Cr&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;sup&gt;*&lt;/sup&gt; groups and standards are common in digital video, they usually define what are the standards, for instance, &lt;a href=&#34;https://en.wikipedia.org/wiki/Rec._2020&#34;&gt;what is 4K? what frame rate should we use? resolution? color model?&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Generally, &lt;strong&gt;displays&lt;/strong&gt; (monitors, TVs, screens and etc) utilize &lt;strong&gt;only the RGB model&lt;/strong&gt;, organized in different manners, see some of them magnified below:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/new_pixel_geometry.jpg&#34; alt=&#34;pixel geometry&#34; title=&#34;pixel geometry&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Chroma subsampling&lt;/h3&gt; &#xA;&lt;p&gt;With the image represented as luma and chroma components, we can take advantage of the human visual system&#39;s greater sensitivity for luma resolution rather than chroma to selectively remove information. &lt;strong&gt;Chroma subsampling&lt;/strong&gt; is the technique of encoding images using &lt;strong&gt;less resolution for chroma than for luma&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/ycbcr_subsampling_resolution.png&#34; alt=&#34;ycbcr subsampling resolutions&#34; title=&#34;ycbcr subsampling resolutions&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;How much should we reduce the chroma resolution?! It turns out that there are already some schemas that describe how to handle resolution and the merge (&lt;code&gt;final color = Y + Cb + Cr&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;p&gt;These schemas are known as subsampling systems and are expressed as a 3 part ratio - &lt;code&gt;a&lt;span&gt;❌&lt;/span&gt;y&lt;/code&gt; which defines the chroma resolution in relation to a &lt;code&gt;a x 2&lt;/code&gt; block of luma pixels.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;a&lt;/code&gt; is the horizontal sampling reference (usually 4)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;x&lt;/code&gt; is the number of chroma samples in the first row of &lt;code&gt;a&lt;/code&gt; pixels (horizontal resolution in relation to &lt;code&gt;a&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;y&lt;/code&gt; is the number of changes of chroma samples between the first and seconds rows of &lt;code&gt;a&lt;/code&gt; pixels.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;An exception to this exists with 4:1:0, which provides a single chroma sample within each &lt;code&gt;4 x 4&lt;/code&gt; block of luma resolution.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Common schemes used in modern codecs are: &lt;strong&gt;4:4:4&lt;/strong&gt; &lt;em&gt;(no subsampling)&lt;/em&gt;, &lt;strong&gt;4:2:2, 4:1:1, 4:2:0, 4:1:0 and 3:1:1&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;You can follow some discussions &lt;a href=&#34;https://github.com/leandromoreira/digital_video_introduction/issues?q=YCbCr&#34;&gt;to learn more about Chroma Subsampling&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;YCbCr 4:2:0 merge&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;Here&#39;s a merged piece of an image using YCbCr 4:2:0, notice that we only spend 12 bits per pixel.&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/ycbcr_420_merge.png&#34; alt=&#34;YCbCr 4:2:0 merge&#34; title=&#34;YCbCr 4:2:0 merge&#34;&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;You can see the same image encoded by the main chroma subsampling types, images in the first row are the final YCbCr while the last row of images shows the chroma resolution. It&#39;s indeed a great win for such small loss.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/chroma_subsampling_examples.jpg&#34; alt=&#34;chroma subsampling examples&#34; title=&#34;chroma subsampling examples&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Previously we had calculated that we needed &lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#redundancy-removal&#34;&gt;278GB of storage to keep a video file with one hour at 720p resolution and 30fps&lt;/a&gt;. If we use &lt;strong&gt;YCbCr 4:2:0&lt;/strong&gt; we can cut &lt;strong&gt;this size in half (139 GB)&lt;/strong&gt;&lt;sup&gt;*&lt;/sup&gt; but it is still far from ideal.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;sup&gt;*&lt;/sup&gt; we found this value by multiplying width, height, bits per pixel and fps. Previously we needed 24 bits, now we only need 12.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;br&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h3&gt;Hands-on: Check YCbCr histogram&lt;/h3&gt; &#xA; &lt;p&gt;You can &lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/encoding_pratical_examples.md#generates-yuv-histogram&#34;&gt;check the YCbCr histogram with ffmpeg.&lt;/a&gt; This scene has a higher blue contribution, which is showed by the &lt;a href=&#34;https://en.wikipedia.org/wiki/Histogram&#34;&gt;histogram&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/yuv_histogram.png&#34; alt=&#34;ycbcr color histogram&#34; title=&#34;ycbcr color histogram&#34;&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Color, luma, luminance, gamma video review&lt;/h3&gt; &#xA;&lt;p&gt;Watch this incredible video explaining what is luma and learn about luminance, gamma, and color. &lt;a href=&#34;http://www.youtube.com/watch?v=Ymt47wXUDEU&#34;&gt;&lt;img src=&#34;http://img.youtube.com/vi/Ymt47wXUDEU/0.jpg&#34; alt=&#34;Analog Luma - A history and explanation of video&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h3&gt;Hands-on: Check YCbCr intensity&lt;/h3&gt; &#xA; &lt;p&gt;You can visualize the Y intensity for a given line of a video using &lt;a href=&#34;https://ffmpeg.org/ffmpeg-filters.html#oscilloscope&#34;&gt;FFmpeg&#39;s oscilloscope filter&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ffplay -f lavfi -i &#39;testsrc2=size=1280x720:rate=30000/1001,format=yuv420p&#39; -vf oscilloscope=x=0.5:y=200/720:s=1:c=1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/ffmpeg_oscilloscope.png&#34; alt=&#34;y color oscilloscope&#34; title=&#34;y color oscilloscope&#34;&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Frame types&lt;/h2&gt; &#xA;&lt;p&gt;Now we can move on and try to eliminate the &lt;strong&gt;redundancy in time&lt;/strong&gt; but before that let&#39;s establish some basic terminology. Suppose we have a movie with 30fps, here are its first 4 frames.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/smw_background_ball_1.png&#34; alt=&#34;ball 1&#34; title=&#34;ball 1&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/smw_background_ball_2.png&#34; alt=&#34;ball 2&#34; title=&#34;ball 2&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/smw_background_ball_3.png&#34; alt=&#34;ball 3&#34; title=&#34;ball 3&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/smw_background_ball_4.png&#34; alt=&#34;ball 4&#34; title=&#34;ball 4&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;We can see &lt;strong&gt;lots of repetitions&lt;/strong&gt; within frames like &lt;strong&gt;the blue background&lt;/strong&gt;, it doesn&#39;t change from frame 0 to frame 3. To tackle this problem, we can &lt;strong&gt;abstractly categorize&lt;/strong&gt; them as three types of frames.&lt;/p&gt; &#xA;&lt;h3&gt;I Frame (intra, keyframe)&lt;/h3&gt; &#xA;&lt;p&gt;An I-frame (reference, keyframe, intra) is a &lt;strong&gt;self-contained frame&lt;/strong&gt;. It doesn&#39;t rely on anything to be rendered, an I-frame looks similar to a static photo. The first frame is usually an I-frame but we&#39;ll see I-frames inserted regularly among other types of frames.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/smw_background_ball_1.png&#34; alt=&#34;ball 1&#34; title=&#34;ball 1&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;P Frame (predicted)&lt;/h3&gt; &#xA;&lt;p&gt;A P-frame takes advantage of the fact that almost always the current picture can be &lt;strong&gt;rendered using the previous frame.&lt;/strong&gt; For instance, in the second frame, the only change was the ball that moved forward. We can &lt;strong&gt;rebuild frame 1, only using the difference and referencing to the previous frame&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/smw_background_ball_1.png&#34; alt=&#34;ball 1&#34; title=&#34;ball 1&#34;&gt; &amp;lt;- &lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/smw_background_ball_2_diff.png&#34; alt=&#34;ball 2&#34; title=&#34;ball 2&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;Hands-on: A video with a single I-frame&lt;/h4&gt; &#xA; &lt;p&gt;Since a P-frame uses less data why can&#39;t we encode an entire &lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/encoding_pratical_examples.md#1-i-frame-and-the-rest-p-frames&#34;&gt;video with a single I-frame and all the rest being P-frames?&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;After you encoded this video, start to watch it and do a &lt;strong&gt;seek for an advanced&lt;/strong&gt; part of the video, you&#39;ll notice &lt;strong&gt;it takes some time&lt;/strong&gt; to really move to that part. That&#39;s because a &lt;strong&gt;P-frame needs a reference frame&lt;/strong&gt; (I-frame for instance) to be rendered.&lt;/p&gt; &#xA; &lt;p&gt;Another quick test you can do is to encode a video using a single I-Frame and then &lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/encoding_pratical_examples.md#1-i-frames-per-second-vs-05-i-frames-per-second&#34;&gt;encode it inserting an I-frame each 2s&lt;/a&gt; and &lt;strong&gt;check the size of each rendition&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;B Frame (bi-predictive)&lt;/h3&gt; &#xA;&lt;p&gt;What about referencing the past and future frames to provide even a better compression?! That&#39;s basically what a B-frame is.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/smw_background_ball_1.png&#34; alt=&#34;ball 1&#34; title=&#34;ball 1&#34;&gt; &amp;lt;- &lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/smw_background_ball_2_diff.png&#34; alt=&#34;ball 2&#34; title=&#34;ball 2&#34;&gt; -&amp;gt; &lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/smw_background_ball_3.png&#34; alt=&#34;ball 3&#34; title=&#34;ball 3&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;Hands-on: Compare videos with B-frame&lt;/h4&gt; &#xA; &lt;p&gt;You can generate two renditions, first with B-frames and other with &lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/encoding_pratical_examples.md#no-b-frames-at-all&#34;&gt;no B-frames at all&lt;/a&gt; and check the size of the file as well as the quality.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Summary&lt;/h3&gt; &#xA;&lt;p&gt;These frames types are used to &lt;strong&gt;provide better compression&lt;/strong&gt;. We&#39;ll look how this happens in the next section, but for now we can think of &lt;strong&gt;I-frame as expensive while P-frame is cheaper but the cheapest is the B-frame.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/frame_types.png&#34; alt=&#34;frame types example&#34; title=&#34;frame types example&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Temporal redundancy (inter prediction)&lt;/h2&gt; &#xA;&lt;p&gt;Let&#39;s explore the options we have to reduce the &lt;strong&gt;repetitions in time&lt;/strong&gt;, this type of redundancy can be solved with techniques of &lt;strong&gt;inter prediction&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We will try to &lt;strong&gt;spend fewer bits&lt;/strong&gt; to encode the sequence of frames 0 and 1.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/original_frames.png&#34; alt=&#34;original frames&#34; title=&#34;original frames&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;One thing we can do it&#39;s a subtraction, we simply &lt;strong&gt;subtract frame 1 from frame 0&lt;/strong&gt; and we get just what we need to &lt;strong&gt;encode the residual&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/difference_frames.png&#34; alt=&#34;delta frames&#34; title=&#34;delta frames&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;But what if I tell you that there is a &lt;strong&gt;better method&lt;/strong&gt; which uses even fewer bits?! First, let&#39;s treat the &lt;code&gt;frame 0&lt;/code&gt; as a collection of well-defined partitions and then we&#39;ll try to match the blocks from &lt;code&gt;frame 0&lt;/code&gt; on &lt;code&gt;frame 1&lt;/code&gt;. We can think of it as &lt;strong&gt;motion estimation&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h3&gt;Wikipedia - block motion compensation&lt;/h3&gt; &#xA; &lt;p&gt;&#34;&lt;strong&gt;Block motion compensation&lt;/strong&gt; divides up the current frame into non-overlapping blocks, and the motion compensation vector &lt;strong&gt;tells where those blocks come from&lt;/strong&gt; (a common misconception is that the previous frame is divided up into non-overlapping blocks, and the motion compensation vectors tell where those blocks move to). The source blocks typically overlap in the source frame. Some video compression algorithms assemble the current frame out of pieces of several different previously-transmitted frames.&#34;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/original_frames_motion_estimation.png&#34; alt=&#34;delta frames&#34; title=&#34;delta frames&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;We could estimate that the ball moved from &lt;code&gt;x=0, y=25&lt;/code&gt; to &lt;code&gt;x=6, y=26&lt;/code&gt;, the &lt;strong&gt;x&lt;/strong&gt; and &lt;strong&gt;y&lt;/strong&gt; values are the &lt;strong&gt;motion vectors&lt;/strong&gt;. One &lt;strong&gt;further step&lt;/strong&gt; we can do to save bits is to &lt;strong&gt;encode only the motion vector difference&lt;/strong&gt; between the last block position and the predicted, so the final motion vector would be &lt;code&gt;x=6 (6-0), y=1 (26-25)&lt;/code&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;In a real-world situation, this &lt;strong&gt;ball would be sliced into n partitions&lt;/strong&gt; but the process is the same.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;The objects on the frame &lt;strong&gt;move in a 3D way&lt;/strong&gt;, the ball can become smaller when it moves to the background. It&#39;s normal that &lt;strong&gt;we won&#39;t find the perfect match&lt;/strong&gt; to the block we tried to find a match. Here&#39;s a superposed view of our estimation vs the real picture.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/motion_estimation.png&#34; alt=&#34;motion estimation&#34; title=&#34;motion estimation&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;But we can see that when we apply &lt;strong&gt;motion estimation&lt;/strong&gt; the &lt;strong&gt;data to encode is smaller&lt;/strong&gt; than using simply delta frame techniques.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/comparison_delta_vs_motion_estimation.png&#34; alt=&#34;motion estimation vs delta &#34; title=&#34;motion estimation delta&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h3&gt;How real motion compensation would look&lt;/h3&gt; &#xA; &lt;p&gt;This technique is applied to all blocks, very often a ball would be partitioned in more than one block. &lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/real_world_motion_compensation.png&#34; alt=&#34;real world motion compensation&#34; title=&#34;real world motion compensation&#34;&gt; Source: &lt;a href=&#34;https://web.stanford.edu/class/ee398a/handouts/lectures/EE398a_MotionEstimation_2012.pdf&#34;&gt;https://web.stanford.edu/class/ee398a/handouts/lectures/EE398a_MotionEstimation_2012.pdf&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;You can &lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/frame_difference_vs_motion_estimation_plus_residual.ipynb&#34;&gt;play around with these concepts using jupyter&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;Hands-on: See the motion vectors&lt;/h4&gt; &#xA; &lt;p&gt;We can &lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/encoding_pratical_examples.md#generate-debug-video&#34;&gt;generate a video with the inter prediction (motion vectors) with ffmpeg.&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/motion_vectors_ffmpeg.png&#34; alt=&#34;inter prediction (motion vectors) with ffmpeg&#34; title=&#34;inter prediction (motion vectors) with ffmpeg&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;Or we can use the &lt;a href=&#34;https://software.intel.com/en-us/intel-video-pro-analyzer&#34;&gt;Intel Video Pro Analyzer&lt;/a&gt; (which is paid but there is a free trial version which limits you to only work with the first 10 frames).&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/inter_prediction_intel_video_pro_analyzer.png&#34; alt=&#34;inter prediction intel video pro analyzer&#34; title=&#34;inter prediction intel video pro analyzer&#34;&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Spatial redundancy (intra prediction)&lt;/h2&gt; &#xA;&lt;p&gt;If we analyze &lt;strong&gt;each frame&lt;/strong&gt; in a video we&#39;ll see that there are also &lt;strong&gt;many areas that are correlated&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/repetitions_in_space.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Let&#39;s walk through an example. This scene is mostly composed of blue and white colors.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/smw_bg.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is an &lt;code&gt;I-frame&lt;/code&gt; and we &lt;strong&gt;can&#39;t use previous frames&lt;/strong&gt; to predict from but we still can compress it. We will encode the red block selection. If we &lt;strong&gt;look at its neighbors&lt;/strong&gt;, we can &lt;strong&gt;estimate&lt;/strong&gt; that there is a &lt;strong&gt;trend of colors around it&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/smw_bg_block.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;We will &lt;strong&gt;predict&lt;/strong&gt; that the frame will continue to &lt;strong&gt;spread the colors vertically&lt;/strong&gt;, it means that the colors of the &lt;strong&gt;unknown pixels will hold the values of its neighbors&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/smw_bg_prediction.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Our &lt;strong&gt;prediction can be wrong&lt;/strong&gt;, for that reason we need to apply this technique (&lt;strong&gt;intra prediction&lt;/strong&gt;) and then &lt;strong&gt;subtract the real values&lt;/strong&gt; which gives us the residual block, resulting in a much more compressible matrix compared to the original.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/smw_residual.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;There are many different types of this sort of prediction. The one you see pictured here is a form of straight planar prediction, where the pixels from the row above the block are copied row to row within the block. Planar prediction also can involve an angular component, where pixels from both the left and the top are used to help predict the current block. And there is also DC prediction, which involves taking the average of the samples right above and to the left of the block.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;Hands-on: Check intra predictions&lt;/h4&gt; &#xA; &lt;p&gt;You can &lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/encoding_pratical_examples.md#generate-debug-video&#34;&gt;generate a video with macro blocks and their predictions with ffmpeg.&lt;/a&gt; Please check the ffmpeg documentation to understand the &lt;a href=&#34;https://trac.ffmpeg.org/wiki/Debug/MacroblocksAndMotionVectors#AnalyzingMacroblockTypes&#34;&gt;meaning of each block color&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/macro_blocks_ffmpeg.png&#34; alt=&#34;intra prediction (macro blocks) with ffmpeg&#34; title=&#34;inter prediction (motion vectors) with ffmpeg&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;Or we can use the &lt;a href=&#34;https://software.intel.com/en-us/intel-video-pro-analyzer&#34;&gt;Intel Video Pro Analyzer&lt;/a&gt; (which is paid but there is a free trial version which limits you to only work with the first 10 frames).&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/intra_prediction_intel_video_pro_analyzer.png&#34; alt=&#34;intra prediction intel video pro analyzer&#34; title=&#34;intra prediction intel video pro analyzer&#34;&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h1&gt;How does a video codec work?&lt;/h1&gt; &#xA;&lt;h2&gt;What? Why? How?&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;What?&lt;/strong&gt; It&#39;s a piece of software / hardware that compresses or decompresses digital video. &lt;strong&gt;Why?&lt;/strong&gt; Market and society demands higher quality videos with limited bandwidth or storage. Remember when we &lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#basic-terminology&#34;&gt;calculated the needed bandwidth&lt;/a&gt; for 30 frames per second, 24 bits per pixel, resolution of a 480x240 video? It was &lt;strong&gt;82.944 Mbps&lt;/strong&gt; with no compression applied. It&#39;s the only way to deliver HD/FullHD/4K in TVs and the Internet. &lt;strong&gt;How?&lt;/strong&gt; We&#39;ll take a brief look at the major techniques here.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;CODEC vs Container&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;One common mistake that beginners often do is to confuse digital video CODEC and &lt;a href=&#34;https://en.wikipedia.org/wiki/Digital_container_format&#34;&gt;digital video container&lt;/a&gt;. We can think of &lt;strong&gt;containers&lt;/strong&gt; as a wrapper format which contains metadata of the video (and possible audio too), and the &lt;strong&gt;compressed video&lt;/strong&gt; can be seen as its payload.&lt;/p&gt; &#xA; &lt;p&gt;Usually, the extension of a video file defines its video container. For instance, the file &lt;code&gt;video.mp4&lt;/code&gt; is probably a &lt;strong&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/MPEG-4_Part_14&#34;&gt;MPEG-4 Part 14&lt;/a&gt;&lt;/strong&gt; container and a file named &lt;code&gt;video.mkv&lt;/code&gt; it&#39;s probably a &lt;strong&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Matroska&#34;&gt;matroska&lt;/a&gt;&lt;/strong&gt;. To be completely sure about the codec and container format we can use &lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/encoding_pratical_examples.md#inspect-stream&#34;&gt;ffmpeg or mediainfo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;History&lt;/h2&gt; &#xA;&lt;p&gt;Before we jump into the inner workings of a generic codec, let&#39;s look back to understand a little better about some old video codecs.&lt;/p&gt; &#xA;&lt;p&gt;The video codec &lt;a href=&#34;https://en.wikipedia.org/wiki/H.261&#34;&gt;H.261&lt;/a&gt; was born in 1990 (technically 1988), and it was designed to work with &lt;strong&gt;data rates of 64 kbit/s&lt;/strong&gt;. It already uses ideas such as chroma subsampling, macro block, etc. In the year of 1995, the &lt;strong&gt;H.263&lt;/strong&gt; video codec standard was published and continued to be extended until 2001.&lt;/p&gt; &#xA;&lt;p&gt;In 2003 the first version of &lt;strong&gt;H.264/AVC&lt;/strong&gt; was completed. In the same year, &lt;strong&gt;On2 Technologies&lt;/strong&gt; (formerly known as the Duck Corporation) released their video codec as a &lt;strong&gt;royalty-free&lt;/strong&gt; lossy video compression called &lt;strong&gt;VP3&lt;/strong&gt;. In 2008, &lt;strong&gt;Google bought&lt;/strong&gt; this company, releasing &lt;strong&gt;VP8&lt;/strong&gt; in the same year. In December of 2012, Google released the &lt;strong&gt;VP9&lt;/strong&gt; and it&#39;s &lt;strong&gt;supported by roughly ¾ of the browser market&lt;/strong&gt; (mobile included).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/AOMedia_Video_1&#34;&gt;AV1&lt;/a&gt;&lt;/strong&gt; is a new &lt;strong&gt;royalty-free&lt;/strong&gt; and open source video codec that&#39;s being designed by the &lt;a href=&#34;http://aomedia.org/&#34;&gt;Alliance for Open Media (AOMedia)&lt;/a&gt;, which is composed of the &lt;strong&gt;companies: Google, Mozilla, Microsoft, Amazon, Netflix, AMD, ARM, NVidia, Intel and Cisco&lt;/strong&gt; among others. The &lt;strong&gt;first version&lt;/strong&gt; 0.1.0 of the reference codec was &lt;strong&gt;published on April 7, 2016&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/codec_history_timeline.png&#34; alt=&#34;codec history timeline&#34; title=&#34;codec history timeline&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;The birth of AV1&lt;/h4&gt; &#xA; &lt;p&gt;Early 2015, Google was working on &lt;a href=&#34;https://en.wikipedia.org/wiki/VP9#Successor:_from_VP10_to_AV1&#34;&gt;VP10&lt;/a&gt;, Xiph (Mozilla) was working on &lt;a href=&#34;https://xiph.org/daala/&#34;&gt;Daala&lt;/a&gt; and Cisco open-sourced its royalty-free video codec called &lt;a href=&#34;https://tools.ietf.org/html/draft-fuldseth-netvc-thor-03&#34;&gt;Thor&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;Then MPEG LA first announced annual caps for HEVC (H.265) and fees 8 times higher than H.264 but soon they changed the rules again:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;no annual cap&lt;/strong&gt;,&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;content fee&lt;/strong&gt; (0.5% of revenue) and&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;per-unit fees about 10 times higher than h264&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;The &lt;a href=&#34;http://aomedia.org/about/&#34;&gt;alliance for open media&lt;/a&gt; was created by companies from hardware manufacturer (Intel, AMD, ARM , Nvidia, Cisco), content delivery (Google, Netflix, Amazon), browser maintainers (Google, Mozilla), and others.&lt;/p&gt; &#xA; &lt;p&gt;The companies had a common goal, a royalty-free video codec and then AV1 was born with a much &lt;a href=&#34;http://aomedia.org/license/patent/&#34;&gt;simpler patent license&lt;/a&gt;. &lt;strong&gt;Timothy B. Terriberry&lt;/strong&gt; did an awesome presentation, which is the source of this section, about the &lt;a href=&#34;https://www.youtube.com/watch?v=lzPaldsmJbk&#34;&gt;AV1 conception, license model and its current state&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;You&#39;ll be surprised to know that you can &lt;strong&gt;analyze the AV1 codec through your browser&lt;/strong&gt;, go to &lt;a href=&#34;https://arewecompressedyet.com/analyzer/&#34;&gt;https://arewecompressedyet.com/analyzer/&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/av1_browser_analyzer.png&#34; alt=&#34;av1 browser analyzer&#34; title=&#34;av1 browser analyzer&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;PS: If you want to learn more about the history of the codecs you must learn the basics behind &lt;a href=&#34;https://www.vcodex.com/video-compression-patents/&#34;&gt;video compression patents&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;A generic codec&lt;/h2&gt; &#xA;&lt;p&gt;We&#39;re going to introduce the &lt;strong&gt;main mechanics behind a generic video codec&lt;/strong&gt; but most of these concepts are useful and used in modern codecs such as VP9, AV1 and HEVC. Be sure to understand that we&#39;re going to simplify things a LOT. Sometimes we&#39;ll use a real example (mostly H.264) to demonstrate a technique.&lt;/p&gt; &#xA;&lt;h2&gt;1st step - picture partitioning&lt;/h2&gt; &#xA;&lt;p&gt;The first step is to &lt;strong&gt;divide the frame&lt;/strong&gt; into several &lt;strong&gt;partitions, sub-partitions&lt;/strong&gt; and beyond.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/picture_partitioning.png&#34; alt=&#34;picture partitioning&#34; title=&#34;picture partitioning&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;But why?&lt;/strong&gt; There are many reasons, for instance, when we split the picture we can work the predictions more precisely, using small partitions for the small moving parts while using bigger partitions to a static background.&lt;/p&gt; &#xA;&lt;p&gt;Usually, the CODECs &lt;strong&gt;organize these partitions&lt;/strong&gt; into slices (or tiles), macro (or coding tree units) and many sub-partitions. The max size of these partitions varies, HEVC sets 64x64 while AVC uses 16x16 but the sub-partitions can reach sizes of 4x4.&lt;/p&gt; &#xA;&lt;p&gt;Remember that we learned how &lt;strong&gt;frames are typed&lt;/strong&gt;?! Well, you can &lt;strong&gt;apply those ideas to blocks&lt;/strong&gt; too, therefore we can have I-Slice, B-Slice, I-Macroblock and etc.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h3&gt;Hands-on: Check partitions&lt;/h3&gt; &#xA; &lt;p&gt;We can also use the &lt;a href=&#34;https://software.intel.com/en-us/intel-video-pro-analyzer&#34;&gt;Intel Video Pro Analyzer&lt;/a&gt; (which is paid but there is a free trial version which limits you to only work with the first 10 frames). Here are &lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/encoding_pratical_examples.md#transcoding&#34;&gt;VP9 partitions&lt;/a&gt; analyzed.&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/paritions_view_intel_video_pro_analyzer.png&#34; alt=&#34;VP9 partitions view intel video pro analyzer &#34; title=&#34;VP9 partitions view intel video pro analyzer&#34;&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;2nd step - predictions&lt;/h2&gt; &#xA;&lt;p&gt;Once we have the partitions, we can make predictions over them. For the &lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#temporal-redundancy-inter-prediction&#34;&gt;inter prediction&lt;/a&gt; we need &lt;strong&gt;to send the motion vectors and the residual&lt;/strong&gt; and the &lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#spatial-redundancy-intra-prediction&#34;&gt;intra prediction&lt;/a&gt; we&#39;ll &lt;strong&gt;send the prediction direction and the residual&lt;/strong&gt; as well.&lt;/p&gt; &#xA;&lt;h2&gt;3rd step - transform&lt;/h2&gt; &#xA;&lt;p&gt;After we get the residual block (&lt;code&gt;predicted partition - real partition&lt;/code&gt;), we can &lt;strong&gt;transform&lt;/strong&gt; it in a way that lets us know which &lt;strong&gt;pixels we can discard&lt;/strong&gt; while keeping the &lt;strong&gt;overall quality&lt;/strong&gt;. There are some transformations for this exact behavior.&lt;/p&gt; &#xA;&lt;p&gt;Although there are &lt;a href=&#34;https://en.wikipedia.org/wiki/List_of_Fourier-related_transforms#Discrete_transforms&#34;&gt;other transformations&lt;/a&gt;, we&#39;ll look more closely at the discrete cosine transform (DCT). The &lt;a href=&#34;https://en.wikipedia.org/wiki/Discrete_cosine_transform&#34;&gt;&lt;strong&gt;DCT&lt;/strong&gt;&lt;/a&gt; main features are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;converts&lt;/strong&gt; blocks of &lt;strong&gt;pixels&lt;/strong&gt; into same-sized blocks of &lt;strong&gt;frequency coefficients&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;compacts&lt;/strong&gt; energy, making it easy to eliminate spatial redundancy.&lt;/li&gt; &#xA; &lt;li&gt;is &lt;strong&gt;reversible&lt;/strong&gt;, a.k.a. you can reverse to pixels.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;On 2 Feb 2017, Cintra, R. J. and Bayer, F. M have published their paper &lt;a href=&#34;https://arxiv.org/abs/1702.00817&#34;&gt;DCT-like Transform for Image Compression Requires 14 Additions Only&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Don&#39;t worry if you didn&#39;t understand the benefits from every bullet point, we&#39;ll try to make some experiments in order to see the real value from it.&lt;/p&gt; &#xA;&lt;p&gt;Let&#39;s take the following &lt;strong&gt;block of pixels&lt;/strong&gt; (8x8):&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/pixel_matrice.png&#34; alt=&#34;pixel values matrix&#34; title=&#34;pixel values matrix&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Which renders to the following block image (8x8):&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/gray_image.png&#34; alt=&#34;pixel values matrix&#34; title=&#34;pixel values matrix&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;When we &lt;strong&gt;apply the DCT&lt;/strong&gt; over this block of pixels and we get the &lt;strong&gt;block of coefficients&lt;/strong&gt; (8x8):&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/dct_coefficient_values.png&#34; alt=&#34;coefficients values&#34; title=&#34;coefficients values&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;And if we render this block of coefficients, we&#39;ll get this image:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/dct_coefficient_image.png&#34; alt=&#34;dct coefficients image&#34; title=&#34;dct coefficients image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;As you can see it looks nothing like the original image, we might notice that the &lt;strong&gt;first coefficient&lt;/strong&gt; is very different from all the others. This first coefficient is known as the DC coefficient which represents of &lt;strong&gt;all the samples&lt;/strong&gt; in the input array, something &lt;strong&gt;similar to an average&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This block of coefficients has an interesting property which is that it separates the high-frequency components from the low frequency.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/dctfrequ.jpg&#34; alt=&#34;dct frequency coefficients property&#34; title=&#34;dct frequency coefficients property&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;In an image, &lt;strong&gt;most of the energy&lt;/strong&gt; will be concentrated in the &lt;a href=&#34;https://web.archive.org/web/20150129171151/https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm&#34;&gt;&lt;strong&gt;lower frequencies&lt;/strong&gt;&lt;/a&gt;, so if we transform an image into its frequency components and &lt;strong&gt;throw away the higher frequency coefficients&lt;/strong&gt;, we can &lt;strong&gt;reduce the amount of data&lt;/strong&gt; needed to describe the image without sacrificing too much image quality.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;frequency means how fast a signal is changing&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Let&#39;s try to apply the knowledge we acquired in the test by converting the original image to its frequency (block of coefficients) using DCT and then throwing away part of the least important coefficients.&lt;/p&gt; &#xA;&lt;p&gt;First, we convert it to its &lt;strong&gt;frequency domain&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/dct_coefficient_values.png&#34; alt=&#34;coefficients values&#34; title=&#34;coefficients values&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Next, we discard part (67%) of the coefficients, mostly the bottom right part of it.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/dct_coefficient_zeroed.png&#34; alt=&#34;zeroed coefficients&#34; title=&#34;zeroed coefficients&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Finally, we reconstruct the image from this discarded block of coefficients (remember, it needs to be reversible) and compare it to the original.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/original_vs_quantized.png&#34; alt=&#34;original vs quantized&#34; title=&#34;original vs quantized&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;As we can see it resembles the original image but it introduced lots of differences from the original, we &lt;strong&gt;throw away 67.1875%&lt;/strong&gt; and we still were able to get at least something similar to the original. We could more intelligently discard the coefficients to have a better image quality but that&#39;s the next topic.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Each coefficient is formed using all the pixels&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;It&#39;s important to note that each coefficient doesn&#39;t directly map to a single pixel but it&#39;s a weighted sum of all pixels. This amazing graph shows how the first and second coefficient is calculated, using weights which are unique for each index.&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/applicat.jpg&#34; alt=&#34;dct calculation&#34; title=&#34;dct calculation&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;Source: &lt;a href=&#34;https://web.archive.org/web/20150129171151/https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm&#34;&gt;https://web.archive.org/web/20150129171151/https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;You can also try to &lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/dct_better_explained.ipynb&#34;&gt;visualize the DCT by looking at a simple image&lt;/a&gt; formation over the DCT basis. For instance, here&#39;s the &lt;a href=&#34;https://en.wikipedia.org/wiki/Discrete_cosine_transform#Example_of_IDCT&#34;&gt;A character being formed&lt;/a&gt; using each coefficient weight.&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/5/5e/Idct-animation.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;br&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h3&gt;Hands-on: throwing away different coefficients&lt;/h3&gt; &#xA; &lt;p&gt;You can play around with the &lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/uniform_quantization_experience.ipynb&#34;&gt;DCT transform&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;4th step - quantization&lt;/h2&gt; &#xA;&lt;p&gt;When we throw away some of the coefficients, in the last step (transform), we kinda did some form of quantization. This step is where we chose to lose information (the &lt;strong&gt;lossy part&lt;/strong&gt;) or in simple terms, we&#39;ll &lt;strong&gt;quantize coefficients to achieve compression&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;How can we quantize a block of coefficients? One simple method would be a uniform quantization, where we take a block, &lt;strong&gt;divide it by a single value&lt;/strong&gt; (10) and round this value.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/quantize.png&#34; alt=&#34;quantize&#34; title=&#34;quantize&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;How can we &lt;strong&gt;reverse&lt;/strong&gt; (re-quantize) this block of coefficients? We can do that by &lt;strong&gt;multiplying the same value&lt;/strong&gt; (10) we divide it first.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/re-quantize.png&#34; alt=&#34;re-quantize&#34; title=&#34;re-quantize&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This &lt;strong&gt;approach isn&#39;t the best&lt;/strong&gt; because it doesn&#39;t take into account the importance of each coefficient, we could use a &lt;strong&gt;matrix of quantizers&lt;/strong&gt; instead of a single value, this matrix can exploit the property of the DCT, quantizing most the bottom right and less the upper left, the &lt;a href=&#34;https://www.hdm-stuttgart.de/~maucher/Python/MMCodecs/html/jpegUpToQuant.html&#34;&gt;JPEG uses a similar approach&lt;/a&gt;, you can check &lt;a href=&#34;https://github.com/google/guetzli/raw/master/guetzli/jpeg_data.h#L40&#34;&gt;source code to see this matrix&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h3&gt;Hands-on: quantization&lt;/h3&gt; &#xA; &lt;p&gt;You can play around with the &lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/dct_experiences.ipynb&#34;&gt;quantization&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;5th step - entropy coding&lt;/h2&gt; &#xA;&lt;p&gt;After we quantized the data (image blocks/slices/frames) we still can compress it in a lossless way. There are many ways (algorithms) to compress data. We&#39;re going to briefly experience some of them, for a deeper understanding you can read the amazing book &lt;a href=&#34;https://www.amazon.com/Understanding-Compression-Data-Modern-Developers/dp/1491961538/&#34;&gt;Understanding Compression: Data Compression for Modern Developers&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;VLC coding:&lt;/h3&gt; &#xA;&lt;p&gt;Let&#39;s suppose we have a stream of the symbols: &lt;strong&gt;a&lt;/strong&gt;, &lt;strong&gt;e&lt;/strong&gt;, &lt;strong&gt;r&lt;/strong&gt; and &lt;strong&gt;t&lt;/strong&gt; and their probability (from 0 to 1) is represented by this table.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;a&lt;/th&gt; &#xA;   &lt;th&gt;e&lt;/th&gt; &#xA;   &lt;th&gt;r&lt;/th&gt; &#xA;   &lt;th&gt;t&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;probability&lt;/td&gt; &#xA;   &lt;td&gt;0.3&lt;/td&gt; &#xA;   &lt;td&gt;0.3&lt;/td&gt; &#xA;   &lt;td&gt;0.2&lt;/td&gt; &#xA;   &lt;td&gt;0.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;We can assign unique binary codes (preferable small) to the most probable and bigger codes to the least probable ones.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;a&lt;/th&gt; &#xA;   &lt;th&gt;e&lt;/th&gt; &#xA;   &lt;th&gt;r&lt;/th&gt; &#xA;   &lt;th&gt;t&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;probability&lt;/td&gt; &#xA;   &lt;td&gt;0.3&lt;/td&gt; &#xA;   &lt;td&gt;0.3&lt;/td&gt; &#xA;   &lt;td&gt;0.2&lt;/td&gt; &#xA;   &lt;td&gt;0.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;binary code&lt;/td&gt; &#xA;   &lt;td&gt;0&lt;/td&gt; &#xA;   &lt;td&gt;10&lt;/td&gt; &#xA;   &lt;td&gt;110&lt;/td&gt; &#xA;   &lt;td&gt;1110&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Let&#39;s compress the stream &lt;strong&gt;eat&lt;/strong&gt;, assuming we would spend 8 bits for each symbol, we would spend &lt;strong&gt;24 bits&lt;/strong&gt; without any compression. But in case we replace each symbol for its code we can save space.&lt;/p&gt; &#xA;&lt;p&gt;The first step is to encode the symbol &lt;strong&gt;e&lt;/strong&gt; which is &lt;code&gt;10&lt;/code&gt; and the second symbol is &lt;strong&gt;a&lt;/strong&gt; which is added (not in a mathematical way) &lt;code&gt;[10][0]&lt;/code&gt; and finally the third symbol &lt;strong&gt;t&lt;/strong&gt; which makes our final compressed bitstream to be &lt;code&gt;[10][0][1110]&lt;/code&gt; or &lt;code&gt;1001110&lt;/code&gt; which only requires &lt;strong&gt;7 bits&lt;/strong&gt; (3.4 times less space than the original).&lt;/p&gt; &#xA;&lt;p&gt;Notice that each code must be a unique prefixed code &lt;a href=&#34;https://en.wikipedia.org/wiki/Huffman_coding&#34;&gt;Huffman can help you to find these numbers&lt;/a&gt;. Though it has some issues there are &lt;a href=&#34;https://en.wikipedia.org/wiki/Context-adaptive_variable-length_coding&#34;&gt;video codecs that still offers&lt;/a&gt; this method and it&#39;s the algorithm for many applications which requires compression.&lt;/p&gt; &#xA;&lt;p&gt;Both encoder and decoder &lt;strong&gt;must know&lt;/strong&gt; the symbol table with its code, therefore, you need to send the table too.&lt;/p&gt; &#xA;&lt;h3&gt;Arithmetic coding:&lt;/h3&gt; &#xA;&lt;p&gt;Let&#39;s suppose we have a stream of the symbols: &lt;strong&gt;a&lt;/strong&gt;, &lt;strong&gt;e&lt;/strong&gt;, &lt;strong&gt;r&lt;/strong&gt;, &lt;strong&gt;s&lt;/strong&gt; and &lt;strong&gt;t&lt;/strong&gt; and their probability is represented by this table.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;a&lt;/th&gt; &#xA;   &lt;th&gt;e&lt;/th&gt; &#xA;   &lt;th&gt;r&lt;/th&gt; &#xA;   &lt;th&gt;s&lt;/th&gt; &#xA;   &lt;th&gt;t&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;probability&lt;/td&gt; &#xA;   &lt;td&gt;0.3&lt;/td&gt; &#xA;   &lt;td&gt;0.3&lt;/td&gt; &#xA;   &lt;td&gt;0.15&lt;/td&gt; &#xA;   &lt;td&gt;0.05&lt;/td&gt; &#xA;   &lt;td&gt;0.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;With this table in mind, we can build ranges containing all the possible symbols sorted by the most frequents.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/range.png&#34; alt=&#34;initial arithmetic range&#34; title=&#34;initial arithmetic range&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Now let&#39;s encode the stream &lt;strong&gt;eat&lt;/strong&gt;, we pick the first symbol &lt;strong&gt;e&lt;/strong&gt; which is located within the subrange &lt;strong&gt;0.3 to 0.6&lt;/strong&gt; (but not included) and we take this subrange and split it again using the same proportions used before but within this new range.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/second_subrange.png&#34; alt=&#34;second sub range&#34; title=&#34;second sub range&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Let&#39;s continue to encode our stream &lt;strong&gt;eat&lt;/strong&gt;, now we take the second symbol &lt;strong&gt;a&lt;/strong&gt; which is within the new subrange &lt;strong&gt;0.3 to 0.39&lt;/strong&gt; and then we take our last symbol &lt;strong&gt;t&lt;/strong&gt; and we do the same process again and we get the last subrange &lt;strong&gt;0.354 to 0.372&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/arithimetic_range.png&#34; alt=&#34;final arithmetic range&#34; title=&#34;final arithmetic range&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;We just need to pick a number within the last subrange &lt;strong&gt;0.354 to 0.372&lt;/strong&gt;, let&#39;s choose &lt;strong&gt;0.36&lt;/strong&gt; but we could choose any number within this subrange. With &lt;strong&gt;only&lt;/strong&gt; this number we&#39;ll be able to recover our original stream &lt;strong&gt;eat&lt;/strong&gt;. If you think about it, it&#39;s like if we were drawing a line within ranges of ranges to encode our stream.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/range_show.png&#34; alt=&#34;final range traverse&#34; title=&#34;final range traverse&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The &lt;strong&gt;reverse process&lt;/strong&gt; (A.K.A. decoding) is equally easy, with our number &lt;strong&gt;0.36&lt;/strong&gt; and our original range we can run the same process but now using this number to reveal the stream encoded behind this number.&lt;/p&gt; &#xA;&lt;p&gt;With the first range, we notice that our number fits at the slice, therefore, it&#39;s our first symbol, now we split this subrange again, doing the same process as before, and we&#39;ll notice that &lt;strong&gt;0.36&lt;/strong&gt; fits the symbol &lt;strong&gt;a&lt;/strong&gt; and after we repeat the process we came to the last symbol &lt;strong&gt;t&lt;/strong&gt; (forming our original encoded stream &lt;em&gt;eat&lt;/em&gt;).&lt;/p&gt; &#xA;&lt;p&gt;Both encoder and decoder &lt;strong&gt;must know&lt;/strong&gt; the symbol probability table, therefore you need to send the table.&lt;/p&gt; &#xA;&lt;p&gt;Pretty neat, isn&#39;t it? People are damn smart to come up with a such solution, some &lt;a href=&#34;https://en.wikipedia.org/wiki/Context-adaptive_binary_arithmetic_coding&#34;&gt;video codecs use&lt;/a&gt; this technique (or at least offer it as an option).&lt;/p&gt; &#xA;&lt;p&gt;The idea is to lossless compress the quantized bitstream, for sure this article is missing tons of details, reasons, trade-offs and etc. But &lt;a href=&#34;https://www.amazon.com/Understanding-Compression-Data-Modern-Developers/dp/1491961538/&#34;&gt;you should learn more&lt;/a&gt; as a developer. Newer codecs are trying to use different &lt;a href=&#34;https://en.wikipedia.org/wiki/Asymmetric_Numeral_Systems&#34;&gt;entropy coding algorithms like ANS.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h3&gt;Hands-on: CABAC vs CAVLC&lt;/h3&gt; &#xA; &lt;p&gt;You can &lt;a href=&#34;https://github.com/leandromoreira/introduction_video_technology/raw/master/encoding_pratical_examples.md#cabac-vs-cavlc&#34;&gt;generate two streams, one with CABAC and other with CAVLC&lt;/a&gt; and &lt;strong&gt;compare the time&lt;/strong&gt; it took to generate each of them as well as &lt;strong&gt;the final size&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;6th step - bitstream format&lt;/h2&gt; &#xA;&lt;p&gt;After we did all these steps we need to &lt;strong&gt;pack the compressed frames and context to these steps&lt;/strong&gt;. We need to explicitly inform to the decoder about &lt;strong&gt;the decisions taken by the encoder&lt;/strong&gt;, such as bit depth, color space, resolution, predictions info (motion vectors, intra prediction direction), profile, level, frame rate, frame type, frame number and much more.&lt;/p&gt; &#xA;&lt;p&gt;We&#39;re going to study, superficially, the H.264 bitstream. Our first step is to &lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/encoding_pratical_examples.md#generate-a-single-frame-h264-bitstream&#34;&gt;generate a minimal H.264 &lt;sup&gt;*&lt;/sup&gt; bitstream&lt;/a&gt;, we can do that using our own repository and &lt;a href=&#34;http://ffmpeg.org/&#34;&gt;ffmpeg&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./s/ffmpeg -i /files/i/minimal.png -pix_fmt yuv420p /files/v/minimal_yuv420.h264&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;sup&gt;*&lt;/sup&gt; ffmpeg adds, by default, all the encoding parameter as a &lt;strong&gt;SEI NAL&lt;/strong&gt;, soon we&#39;ll define what is a NAL.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;This command will generate a raw h264 bitstream with a &lt;strong&gt;single frame&lt;/strong&gt;, 64x64, with color space yuv420 and using the following image as the frame.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/minimal.png&#34; alt=&#34;used frame to generate minimal h264 bitstream&#34; title=&#34;used frame to generate minimal h264 bitstream&#34;&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;H.264 bitstream&lt;/h3&gt; &#xA;&lt;p&gt;The AVC (H.264) standard defines that the information will be sent in &lt;strong&gt;macro frames&lt;/strong&gt; (in the network sense), called &lt;strong&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Network_Abstraction_Layer&#34;&gt;NAL&lt;/a&gt;&lt;/strong&gt; (Network Abstraction Layer). The main goal of the NAL is the provision of a &#34;network-friendly&#34; video representation, this standard must work on TVs (stream based), the Internet (packet based) among others.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/nal_units.png&#34; alt=&#34;NAL units H.264&#34; title=&#34;NAL units H.264&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;There is a &lt;strong&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Frame_synchronization&#34;&gt;synchronization marker&lt;/a&gt;&lt;/strong&gt; to define the boundaries of the NAL&#39;s units. Each synchronization marker holds a value of &lt;code&gt;0x00 0x00 0x01&lt;/code&gt; except to the very first one which is &lt;code&gt;0x00 0x00 0x00 0x01&lt;/code&gt;. If we run the &lt;strong&gt;hexdump&lt;/strong&gt; on the generated h264 bitstream, we can identify at least three NALs in the beginning of the file.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/minimal_yuv420_hex.png&#34; alt=&#34;synchronization marker on NAL units&#34; title=&#34;synchronization marker on NAL units&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;As we said before, the decoder needs to know not only the picture data but also the details of the video, frame, colors, used parameters, and others. The &lt;strong&gt;first byte&lt;/strong&gt; of each NAL defines its category and &lt;strong&gt;type&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;NAL type id&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0&lt;/td&gt; &#xA;   &lt;td&gt;Undefined&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;Coded slice of a non-IDR picture&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;Coded slice data partition A&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;Coded slice data partition B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;Coded slice data partition C&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;IDR&lt;/strong&gt; Coded slice of an IDR picture&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;6&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;SEI&lt;/strong&gt; Supplemental enhancement information&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;SPS&lt;/strong&gt; Sequence parameter set&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;PPS&lt;/strong&gt; Picture parameter set&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;9&lt;/td&gt; &#xA;   &lt;td&gt;Access unit delimiter&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;10&lt;/td&gt; &#xA;   &lt;td&gt;End of sequence&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;11&lt;/td&gt; &#xA;   &lt;td&gt;End of stream&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;...&lt;/td&gt; &#xA;   &lt;td&gt;...&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Usually, the first NAL of a bitstream is a &lt;strong&gt;SPS&lt;/strong&gt;, this type of NAL is responsible for informing the general encoding variables like &lt;strong&gt;profile&lt;/strong&gt;, &lt;strong&gt;level&lt;/strong&gt;, &lt;strong&gt;resolution&lt;/strong&gt; and others.&lt;/p&gt; &#xA;&lt;p&gt;If we skip the first synchronization marker we can decode the &lt;strong&gt;first byte&lt;/strong&gt; to know what &lt;strong&gt;type of NAL&lt;/strong&gt; is the first one.&lt;/p&gt; &#xA;&lt;p&gt;For instance the first byte after the synchronization marker is &lt;code&gt;01100111&lt;/code&gt;, where the first bit (&lt;code&gt;0&lt;/code&gt;) is to the field &lt;strong&gt;forbidden_zero_bit&lt;/strong&gt;, the next 2 bits (&lt;code&gt;11&lt;/code&gt;) tell us the field &lt;strong&gt;nal_ref_idc&lt;/strong&gt; which indicates whether this NAL is a reference field or not and the rest 5 bits (&lt;code&gt;00111&lt;/code&gt;) inform us the field &lt;strong&gt;nal_unit_type&lt;/strong&gt;, in this case, it&#39;s a &lt;strong&gt;SPS&lt;/strong&gt; (7) NAL unit.&lt;/p&gt; &#xA;&lt;p&gt;The second byte (&lt;code&gt;binary=01100100, hex=0x64, dec=100&lt;/code&gt;) of an SPS NAL is the field &lt;strong&gt;profile_idc&lt;/strong&gt; which shows the profile that the encoder has used, in this case, we used the &lt;strong&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/H.264/MPEG-4_AVC#Profiles&#34;&gt;high profile&lt;/a&gt;&lt;/strong&gt;. Also the third byte contains several flags which determine the exact profile (like constrained or progressive). But in our case the third byte is 0x00 and therefore the encoder has used just high profile.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/minimal_yuv420_bin.png&#34; alt=&#34;SPS binary view&#34; title=&#34;SPS binary view&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;When we read the H.264 bitstream spec for an SPS NAL we&#39;ll find many values for the &lt;strong&gt;parameter name&lt;/strong&gt;, &lt;strong&gt;category&lt;/strong&gt; and a &lt;strong&gt;description&lt;/strong&gt;, for instance, let&#39;s look at &lt;code&gt;pic_width_in_mbs_minus_1&lt;/code&gt; and &lt;code&gt;pic_height_in_map_units_minus_1&lt;/code&gt; fields.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Parameter name&lt;/th&gt; &#xA;   &lt;th&gt;Category&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;pic_width_in_mbs_minus_1&lt;/td&gt; &#xA;   &lt;td&gt;0&lt;/td&gt; &#xA;   &lt;td&gt;ue(v)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;pic_height_in_map_units_minus_1&lt;/td&gt; &#xA;   &lt;td&gt;0&lt;/td&gt; &#xA;   &lt;td&gt;ue(v)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;ue(v)&lt;/strong&gt;: unsigned integer &lt;a href=&#34;https://ghostarchive.org/archive/JBwdI&#34;&gt;Exp-Golomb-coded&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;If we do some math with the value of these fields we will end up with the &lt;strong&gt;resolution&lt;/strong&gt;. We can represent a &lt;code&gt;1920 x 1080&lt;/code&gt; using a &lt;code&gt;pic_width_in_mbs_minus_1&lt;/code&gt; with the value of &lt;code&gt;119 ( (119 + 1) * macroblock_size = 120 * 16 = 1920) &lt;/code&gt;, again saving space, instead of encode &lt;code&gt;1920&lt;/code&gt; we did it with &lt;code&gt;119&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If we continue to examine our created video with a binary view (ex: &lt;code&gt;xxd -b -c 11 v/minimal_yuv420.h264&lt;/code&gt;), we can skip to the last NAL which is the frame itself.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/slice_nal_idr_bin.png&#34; alt=&#34;h264 idr slice header&#34; title=&#34;h264 idr slice header&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;We can see its first 6 bytes values: &lt;code&gt;01100101 10001000 10000100 00000000 00100001 11111111&lt;/code&gt;. As we already know the first byte tell us about what type of NAL it is, in this case, (&lt;code&gt;00101&lt;/code&gt;) it&#39;s an &lt;strong&gt;IDR Slice (5)&lt;/strong&gt; and we can further inspect it:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/slice_header.png&#34; alt=&#34;h264 slice header spec&#34; title=&#34;h264 slice header spec&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Using the spec info we can decode what type of slice (&lt;strong&gt;slice_type&lt;/strong&gt;), the frame number (&lt;strong&gt;frame_num&lt;/strong&gt;) among others important fields.&lt;/p&gt; &#xA;&lt;p&gt;In order to get the values of some fields (&lt;code&gt;ue(v), me(v), se(v) or te(v)&lt;/code&gt;) we need to decode it using a special decoder called &lt;a href=&#34;https://ghostarchive.org/archive/JBwdI&#34;&gt;Exponential-Golomb&lt;/a&gt;, this method is &lt;strong&gt;very efficient to encode variable values&lt;/strong&gt;, mostly when there are many default values.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;The values of &lt;strong&gt;slice_type&lt;/strong&gt; and &lt;strong&gt;frame_num&lt;/strong&gt; of this video are 7 (I slice) and 0 (the first frame).&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;We can see the &lt;strong&gt;bitstream as a protocol&lt;/strong&gt; and if you want or need to learn more about this bitstream please refer to the &lt;a href=&#34;http://www.itu.int/rec/T-REC-H.264-201610-I&#34;&gt;ITU H.264 spec.&lt;/a&gt; Here&#39;s a macro diagram which shows where the picture data (compressed YUV) resides.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/h264_bitstream_macro_diagram.png&#34; alt=&#34;h264 bitstream macro diagram&#34; title=&#34;h264 bitstream macro diagram&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;We can explore others bitstreams like the &lt;a href=&#34;https://storage.googleapis.com/downloads.webmproject.org/docs/vp9/vp9-bitstream-specification-v0.6-20160331-draft.pdf&#34;&gt;VP9 bitstream&lt;/a&gt;, &lt;a href=&#34;http://handle.itu.int/11.1002/1000/11885-en?locatt=format:pdf&#34;&gt;H.265 (HEVC)&lt;/a&gt; or even our &lt;strong&gt;new best friend&lt;/strong&gt; &lt;a href=&#34;https://medium.com/@mbebenita/av1-bitstream-analyzer-d25f1c27072b#.d5a89oxz8&#34;&gt;&lt;strong&gt;AV1&lt;/strong&gt; bitstream&lt;/a&gt;, &lt;a href=&#34;http://www.gpac-licensing.com/2016/07/12/vp9-av1-bitstream-format/&#34;&gt;do they all look similar? No&lt;/a&gt;, but once you learned one you can easily get the others.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h3&gt;Hands-on: Inspect the H.264 bitstream&lt;/h3&gt; &#xA; &lt;p&gt;We can &lt;a href=&#34;https://github.com/leandromoreira/introduction_video_technology/raw/master/encoding_pratical_examples.md#generate-a-single-frame-video&#34;&gt;generate a single frame video&lt;/a&gt; and use &lt;a href=&#34;https://en.wikipedia.org/wiki/MediaInfo&#34;&gt;mediainfo&lt;/a&gt; to inspect its H.264 bitstream. In fact, you can even see the &lt;a href=&#34;https://github.com/MediaArea/MediaInfoLib/raw/master/Source/MediaInfo/Video/File_Avc.cpp&#34;&gt;source code that parses h264 (AVC)&lt;/a&gt; bitstream.&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/mediainfo_details_1.png&#34; alt=&#34;mediainfo details h264 bitstream&#34; title=&#34;mediainfo details h264 bitstream&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;We can also use the &lt;a href=&#34;https://software.intel.com/en-us/intel-video-pro-analyzer&#34;&gt;Intel Video Pro Analyzer&lt;/a&gt; which is paid but there is a free trial version which limits you to only work with the first 10 frames but that&#39;s okay for learning purposes.&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/intel-video-pro-analyzer.png&#34; alt=&#34;intel video pro analyzer details h264 bitstream&#34; title=&#34;intel video pro analyzer details h264 bitstream&#34;&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Review&lt;/h2&gt; &#xA;&lt;p&gt;We&#39;ll notice that many of the &lt;strong&gt;modern codecs uses this same model we learned&lt;/strong&gt;. In fact, let&#39;s look at the Thor video codec block diagram, it contains all the steps we studied. The idea is that you now should be able to at least understand better the innovations and papers for the area.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/thor_codec_block_diagram.png&#34; alt=&#34;thor_codec_block_diagram&#34; title=&#34;thor_codec_block_diagram&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Previously we had calculated that we needed &lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/#chroma-subsampling&#34;&gt;139GB of storage to keep a video file with one hour at 720p resolution and 30fps&lt;/a&gt; if we use the techniques we learned here, like &lt;strong&gt;inter and intra prediction, transform, quantization, entropy coding and other&lt;/strong&gt; we can achieve, assuming we are spending &lt;strong&gt;0.031 bit per pixel&lt;/strong&gt;, the same perceivable quality video &lt;strong&gt;requiring only 367.82MB vs 139GB&lt;/strong&gt; of store.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;We choose to use &lt;strong&gt;0.031 bit per pixel&lt;/strong&gt; based on the example video provided here.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;How does H.265 achieve a better compression ratio than H.264?&lt;/h2&gt; &#xA;&lt;p&gt;Now that we know more about how codecs work, then it is easy to understand how new codecs are able to deliver higher resolutions with fewer bits.&lt;/p&gt; &#xA;&lt;p&gt;We will compare AVC and HEVC, let&#39;s keep in mind that it is almost always a trade-off between more CPU cycles (complexity) and compression rate.&lt;/p&gt; &#xA;&lt;p&gt;HEVC has bigger and more &lt;strong&gt;partitions&lt;/strong&gt; (and &lt;strong&gt;sub-partitions&lt;/strong&gt;) options than AVC, more &lt;strong&gt;intra predictions directions/angles&lt;/strong&gt;, &lt;strong&gt;improved entropy coding&lt;/strong&gt; and more, all these improvements made H.265 capable to compress 50% more than H.264.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/avc_vs_hevc.png&#34; alt=&#34;h264 vs h265&#34; title=&#34;H.264 vs H.265&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Online streaming&lt;/h1&gt; &#xA;&lt;h2&gt;General architecture&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/general_architecture.png&#34; alt=&#34;general architecture&#34; title=&#34;general architecture&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;[TODO]&lt;/p&gt; &#xA;&lt;h2&gt;Progressive download and adaptive streaming&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/progressive_download.png&#34; alt=&#34;progressive download&#34; title=&#34;progressive download&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/adaptive_streaming.png&#34; alt=&#34;adaptive streaming&#34; title=&#34;adaptive streaming&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;[TODO]&lt;/p&gt; &#xA;&lt;h2&gt;Content protection&lt;/h2&gt; &#xA;&lt;p&gt;We can use &lt;strong&gt;a simple token system&lt;/strong&gt; to protect the content. The user without a token tries to request a video and the CDN forbids her or him while a user with a valid token can play the content, it works pretty similarly to most of the web authentication systems.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/token_protection.png&#34; alt=&#34;token protection&#34; title=&#34;token_protection&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The sole use of this token system still allows a user to download a video and distribute it. Then the &lt;strong&gt;DRM (digital rights management)&lt;/strong&gt; systems can be used to try to avoid this.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/drm.png&#34; alt=&#34;drm&#34; title=&#34;drm&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;In real life production systems, people often use both techniques to provide authorization and authentication.&lt;/p&gt; &#xA;&lt;h3&gt;DRM&lt;/h3&gt; &#xA;&lt;h4&gt;Main systems&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FPS - &lt;a href=&#34;https://developer.apple.com/streaming/fps/&#34;&gt;&lt;strong&gt;FairPlay Streaming&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;PR - &lt;a href=&#34;https://www.microsoft.com/playready/&#34;&gt;&lt;strong&gt;PlayReady&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;WV - &lt;a href=&#34;http://www.widevine.com/&#34;&gt;&lt;strong&gt;Widevine&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;What?&lt;/h4&gt; &#xA;&lt;p&gt;DRM means &lt;a href=&#34;https://sander.saares.eu/categories/drm-is-not-a-black-box/&#34;&gt;Digital rights management&lt;/a&gt;, it&#39;s a way &lt;strong&gt;to provide copyright protection for digital media&lt;/strong&gt;, for instance, digital video and audio. Although it&#39;s used in many places &lt;a href=&#34;https://en.wikipedia.org/wiki/Digital_rights_management#DRM-free_works&#34;&gt;it&#39;s not universally accepted&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Why?&lt;/h4&gt; &#xA;&lt;p&gt;Content creator (mostly studios) want to protect its intelectual property against copy to prevent unauthorized redistribution of digital media.&lt;/p&gt; &#xA;&lt;h4&gt;How?&lt;/h4&gt; &#xA;&lt;p&gt;We&#39;re going to describe an abstract and generic form of DRM in a very simplified way.&lt;/p&gt; &#xA;&lt;p&gt;Given a &lt;strong&gt;content C1&lt;/strong&gt; (i.e. an hls or dash video streaming), with a &lt;strong&gt;player P1&lt;/strong&gt; (i.e. shaka-clappr, exo-player or ios) in a &lt;strong&gt;device D1&lt;/strong&gt; (i.e. a smartphone, TV, tablet or desktop/notebook) using a &lt;strong&gt;DRM system DRM1&lt;/strong&gt; (widevine, playready or FairPlay).&lt;/p&gt; &#xA;&lt;p&gt;The content C1 is encrypted with a &lt;strong&gt;symmetric-key K1&lt;/strong&gt; from the system DRM1, generating the &lt;strong&gt;encrypted content C&#39;1&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/drm_general_flow.jpeg&#34; alt=&#34;drm general flow&#34; title=&#34;drm general flow&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The player P1, of a device D1, has two keys (asymmetric), a &lt;strong&gt;private key PRK1&lt;/strong&gt; (this key is protected&lt;sup&gt;1&lt;/sup&gt; and only known by &lt;strong&gt;D1&lt;/strong&gt;) and a &lt;strong&gt;public key PUK1&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;&lt;sup&gt;1&lt;/sup&gt;protected&lt;/strong&gt;: this protection can be &lt;strong&gt;via hardware&lt;/strong&gt;, for instance, this key can be stored inside a special (read-only) chip that works like &lt;a href=&#34;https://en.wikipedia.org/wiki/Black_box&#34;&gt;a black-box&lt;/a&gt; to provide decryption, or &lt;strong&gt;by software&lt;/strong&gt; (less safe), the DRM system provides means to know which type of protection a given device has.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;When the &lt;strong&gt;player P1 wants to play&lt;/strong&gt; the &lt;strong&gt;content C&#39;1&lt;/strong&gt;, it needs to deal with the &lt;strong&gt;DRM system DRM1&lt;/strong&gt;, giving its public key &lt;strong&gt;PUK1&lt;/strong&gt;. The DRM system DRM1 returns the &lt;strong&gt;key K1 encrypted&lt;/strong&gt; with the client&#39;&#39;s public key &lt;strong&gt;PUK1&lt;/strong&gt;. In theory, this response is something that &lt;strong&gt;only D1 is capable of decrypting&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;K1P1D1 = enc(K1, PUK1)&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;P1&lt;/strong&gt; uses its DRM local system (it could be a &lt;a href=&#34;https://en.wikipedia.org/wiki/System_on_a_chip&#34;&gt;SOC&lt;/a&gt;, a specialized hardware or software), this system is &lt;strong&gt;able to decrypt&lt;/strong&gt; the content using its private key PRK1, it can decrypt &lt;strong&gt;the symmetric-key K1 from the K1P1D1&lt;/strong&gt; and &lt;strong&gt;play C&#39;1&lt;/strong&gt;. At best case, the keys are not exposed through RAM.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;K1 = dec(K1P1D1, PRK1)&#xA;&#xA;P1.play(dec(C&#39;1, K1))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/digital_video_introduction/master/i/drm_decoder_flow.jpeg&#34; alt=&#34;drm decoder flow&#34; title=&#34;drm decoder flow&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;How to use jupyter&lt;/h1&gt; &#xA;&lt;p&gt;Make sure you have &lt;strong&gt;docker installed&lt;/strong&gt; and just run &lt;code&gt;./s/start_jupyter.sh&lt;/code&gt; and follow the instructions on the terminal.&lt;/p&gt; &#xA;&lt;h1&gt;Conferences&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://demuxed.com/&#34;&gt;DEMUXED&lt;/a&gt; - you can &lt;a href=&#34;https://www.youtube.com/channel/UCIc_DkRxo9UgUSTvWVNCmpA&#34;&gt;check the last 2 events presentations.&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;References&lt;/h1&gt; &#xA;&lt;p&gt;The richest content is here, it&#39;s where all the info we saw in this text was extracted, based or inspired by. You can deepen your knowledge with these amazing links, books, videos and etc.&lt;/p&gt; &#xA;&lt;p&gt;Online Courses and Tutorials:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.coursera.org/learn/digital/&#34;&gt;https://www.coursera.org/learn/digital/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://people.xiph.org/~tterribe/pubs/lca2012/auckland/intro_to_video1.pdf&#34;&gt;https://people.xiph.org/~tterribe/pubs/lca2012/auckland/intro_to_video1.pdf&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://xiph.org/video/vid1.shtml&#34;&gt;https://xiph.org/video/vid1.shtml&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://xiph.org/video/vid2.shtml&#34;&gt;https://xiph.org/video/vid2.shtml&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://wiki.multimedia.cx&#34;&gt;https://wiki.multimedia.cx&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mahanstreamer.net&#34;&gt;https://mahanstreamer.net&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://slhck.info/ffmpeg-encoding-course&#34;&gt;http://slhck.info/ffmpeg-encoding-course&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cambridgeincolour.com/tutorials/camera-sensors.htm&#34;&gt;http://www.cambridgeincolour.com/tutorials/camera-sensors.htm&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.slideshare.net/vcodex/a-short-history-of-video-coding&#34;&gt;http://www.slideshare.net/vcodex/a-short-history-of-video-coding&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.slideshare.net/vcodex/introduction-to-video-compression-13394338&#34;&gt;http://www.slideshare.net/vcodex/introduction-to-video-compression-13394338&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://developer.android.com/guide/topics/media/media-formats.html&#34;&gt;https://developer.android.com/guide/topics/media/media-formats.html&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.slideshare.net/MadhawaKasun/audio-compression-23398426&#34;&gt;http://www.slideshare.net/MadhawaKasun/audio-compression-23398426&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://inst.eecs.berkeley.edu/~ee290t/sp04/lectures/02-Motion_Compensation_girod.pdf&#34;&gt;http://inst.eecs.berkeley.edu/~ee290t/sp04/lectures/02-Motion_Compensation_girod.pdf&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Books:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.amazon.com/Understanding-Compression-Data-Modern-Developers/dp/1491961538/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1486395327&amp;amp;sr=1-1&#34;&gt;https://www.amazon.com/Understanding-Compression-Data-Modern-Developers/dp/1491961538/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1486395327&amp;amp;sr=1-1&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.amazon.com/H-264-Advanced-Video-Compression-Standard/dp/0470516925&#34;&gt;https://www.amazon.com/H-264-Advanced-Video-Compression-Standard/dp/0470516925&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.amazon.com/High-Efficiency-Video-Coding-HEVC/dp/3319068946&#34;&gt;https://www.amazon.com/High-Efficiency-Video-Coding-HEVC/dp/3319068946&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.amazon.com/Practical-Guide-Video-Audio-Compression/dp/0240806301/ref=sr_1_3?s=books&amp;amp;ie=UTF8&amp;amp;qid=1486396914&amp;amp;sr=1-3&amp;amp;keywords=A+PRACTICAL+GUIDE+TO+VIDEO+AUDIO&#34;&gt;https://www.amazon.com/Practical-Guide-Video-Audio-Compression/dp/0240806301/ref=sr_1_3?s=books&amp;amp;ie=UTF8&amp;amp;qid=1486396914&amp;amp;sr=1-3&amp;amp;keywords=A+PRACTICAL+GUIDE+TO+VIDEO+AUDIO&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.amazon.com/Video-Encoding-Numbers-Eliminate-Guesswork/dp/0998453005/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1486396940&amp;amp;sr=1-1&amp;amp;keywords=jan+ozer&#34;&gt;https://www.amazon.com/Video-Encoding-Numbers-Eliminate-Guesswork/dp/0998453005/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1486396940&amp;amp;sr=1-1&amp;amp;keywords=jan+ozer&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Onboarding material:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Eyevinn/streaming-onboarding&#34;&gt;https://github.com/Eyevinn/streaming-onboarding&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://howvideo.works/&#34;&gt;https://howvideo.works/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.aws.training/Details/eLearning?id=17775&#34;&gt;https://www.aws.training/Details/eLearning?id=17775&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.aws.training/Details/eLearning?id=17887&#34;&gt;https://www.aws.training/Details/eLearning?id=17887&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.aws.training/Details/Video?id=24750&#34;&gt;https://www.aws.training/Details/Video?id=24750&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Bitstream Specifications:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.itu.int/rec/T-REC-H.264-201610-I&#34;&gt;http://www.itu.int/rec/T-REC-H.264-201610-I&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.itu.int/ITU-T/recommendations/rec.aspx?rec=12904&amp;amp;lang=en&#34;&gt;http://www.itu.int/ITU-T/recommendations/rec.aspx?rec=12904&amp;amp;lang=en&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://storage.googleapis.com/downloads.webmproject.org/docs/vp9/vp9-bitstream-specification-v0.6-20160331-draft.pdf&#34;&gt;https://storage.googleapis.com/downloads.webmproject.org/docs/vp9/vp9-bitstream-specification-v0.6-20160331-draft.pdf&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://iphome.hhi.de/wiegand/assets/pdfs/2012_12_IEEE-HEVC-Overview.pdf&#34;&gt;http://iphome.hhi.de/wiegand/assets/pdfs/2012_12_IEEE-HEVC-Overview.pdf&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://phenix.int-evry.fr/jct/doc_end_user/current_document.php?id=7243&#34;&gt;http://phenix.int-evry.fr/jct/doc_end_user/current_document.php?id=7243&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://gentlelogic.blogspot.com.br/2011/11/exploring-h264-part-2-h264-bitstream.html&#34;&gt;http://gentlelogic.blogspot.com.br/2011/11/exploring-h264-part-2-h264-bitstream.html&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://forum.doom9.org/showthread.php?t=167081&#34;&gt;https://forum.doom9.org/showthread.php?t=167081&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://forum.doom9.org/showthread.php?t=168947&#34;&gt;https://forum.doom9.org/showthread.php?t=168947&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Software:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ffmpeg.org/&#34;&gt;https://ffmpeg.org/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ffmpeg.org/ffmpeg-all.html&#34;&gt;https://ffmpeg.org/ffmpeg-all.html&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ffmpeg.org/ffprobe.html&#34;&gt;https://ffmpeg.org/ffprobe.html&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mediaarea.net/en/MediaInfo&#34;&gt;https://mediaarea.net/en/MediaInfo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.jongbel.com/&#34;&gt;https://www.jongbel.com/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://trac.ffmpeg.org/wiki/&#34;&gt;https://trac.ffmpeg.org/wiki/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://software.intel.com/en-us/intel-video-pro-analyzer&#34;&gt;https://software.intel.com/en-us/intel-video-pro-analyzer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/@mbebenita/av1-bitstream-analyzer-d25f1c27072b#.d5a89oxz8&#34;&gt;https://medium.com/@mbebenita/av1-bitstream-analyzer-d25f1c27072b#.d5a89oxz8&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Non-ITU Codecs:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aomedia.googlesource.com/&#34;&gt;https://aomedia.googlesource.com/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/webmproject/libvpx/tree/master/vp9&#34;&gt;https://github.com/webmproject/libvpx/tree/master/vp9&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ghostarchive.org/archive/0W0d8&#34;&gt;https://ghostarchive.org/archive/0W0d8&lt;/a&gt; (was: &lt;a href=&#34;https://people.xiph.org/~xiphmont/demo/daala/demo1.shtml&#34;&gt;https://people.xiph.org/~xiphmont/demo/daala/demo1.shtml&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://people.xiph.org/~jm/daala/revisiting/&#34;&gt;https://people.xiph.org/~jm/daala/revisiting/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=lzPaldsmJbk&#34;&gt;https://www.youtube.com/watch?v=lzPaldsmJbk&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://fosdem.org/2017/schedule/event/om_av1/&#34;&gt;https://fosdem.org/2017/schedule/event/om_av1/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jmvalin.ca/papers/AV1_tools.pdf&#34;&gt;https://jmvalin.ca/papers/AV1_tools.pdf&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Encoding Concepts:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://x265.org/hevc-h265/&#34;&gt;http://x265.org/hevc-h265/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://slhck.info/video/2017/03/01/rate-control.html&#34;&gt;http://slhck.info/video/2017/03/01/rate-control.html&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://slhck.info/video/2017/02/24/vbr-settings.html&#34;&gt;http://slhck.info/video/2017/02/24/vbr-settings.html&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://slhck.info/video/2017/02/24/crf-guide.html&#34;&gt;http://slhck.info/video/2017/02/24/crf-guide.html&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1702.00817v1.pdf&#34;&gt;https://arxiv.org/pdf/1702.00817v1.pdf&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://trac.ffmpeg.org/wiki/Debug/MacroblocksAndMotionVectors&#34;&gt;https://trac.ffmpeg.org/wiki/Debug/MacroblocksAndMotionVectors&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://web.ece.ucdavis.edu/cerl/ReliableJPEG/Cung/jpeg.html&#34;&gt;http://web.ece.ucdavis.edu/cerl/ReliableJPEG/Cung/jpeg.html&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.adobe.com/devnet/adobe-media-server/articles/h264_encoding.html&#34;&gt;http://www.adobe.com/devnet/adobe-media-server/articles/h264_encoding.html&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://prezi.com/8m7thtvl4ywr/mp3-and-aac-explained/&#34;&gt;https://prezi.com/8m7thtvl4ywr/mp3-and-aac-explained/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blogs.gnome.org/rbultje/2016/12/13/overview-of-the-vp9-video-codec/&#34;&gt;https://blogs.gnome.org/rbultje/2016/12/13/overview-of-the-vp9-video-codec/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://videoblerg.wordpress.com/2017/11/10/ffmpeg-and-how-to-use-it-wrong/&#34;&gt;https://videoblerg.wordpress.com/2017/11/10/ffmpeg-and-how-to-use-it-wrong/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Video Sequences for Testing:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://bbb3d.renderfarming.net/download.html&#34;&gt;http://bbb3d.renderfarming.net/download.html&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.its.bldrdoc.gov/vqeg/video-datasets-and-organizations.aspx&#34;&gt;https://www.its.bldrdoc.gov/vqeg/video-datasets-and-organizations.aspx&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Miscellaneous:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Eyevinn/streaming-onboarding&#34;&gt;https://github.com/Eyevinn/streaming-onboarding&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://stackoverflow.com/a/24890903&#34;&gt;http://stackoverflow.com/a/24890903&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://stackoverflow.com/questions/38094302/how-to-understand-header-of-h264&#34;&gt;http://stackoverflow.com/questions/38094302/how-to-understand-header-of-h264&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://techblog.netflix.com/2016/08/a-large-scale-comparison-of-x264-x265.html&#34;&gt;http://techblog.netflix.com/2016/08/a-large-scale-comparison-of-x264-x265.html&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://vanseodesign.com/web-design/color-luminance/&#34;&gt;http://vanseodesign.com/web-design/color-luminance/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.biologymad.com/nervoussystem/eyenotes.htm&#34;&gt;http://www.biologymad.com/nervoussystem/eyenotes.htm&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.compression.ru/video/codec_comparison/h264_2012/mpeg4_avc_h264_video_codecs_comparison.pdf&#34;&gt;http://www.compression.ru/video/codec_comparison/h264_2012/mpeg4_avc_h264_video_codecs_comparison.pdf&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://web.archive.org/web/20100728070421/http://www.csc.villanova.edu/~rschumey/csc4800/dct.html&#34;&gt;https://web.archive.org/web/20100728070421/http://www.csc.villanova.edu/~rschumey/csc4800/dct.html&lt;/a&gt; (was: &lt;a href=&#34;http://www.csc.villanova.edu/~rschumey/csc4800/dct.html&#34;&gt;http://www.csc.villanova.edu/~rschumey/csc4800/dct.html&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.explainthatstuff.com/digitalcameras.html&#34;&gt;http://www.explainthatstuff.com/digitalcameras.html&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.hkvstar.com&#34;&gt;http://www.hkvstar.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.hometheatersound.com/&#34;&gt;http://www.hometheatersound.com/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.lighterra.com/papers/videoencodingh264/&#34;&gt;http://www.lighterra.com/papers/videoencodingh264/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.red.com/learn/red-101/video-chroma-subsampling&#34;&gt;http://www.red.com/learn/red-101/video-chroma-subsampling&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.slideshare.net/ManoharKuse/hevc-intra-coding&#34;&gt;http://www.slideshare.net/ManoharKuse/hevc-intra-coding&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.slideshare.net/mwalendo/h264vs-hevc&#34;&gt;http://www.slideshare.net/mwalendo/h264vs-hevc&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.slideshare.net/rvarun7777/final-seminar-46117193&#34;&gt;http://www.slideshare.net/rvarun7777/final-seminar-46117193&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.springer.com/cda/content/document/cda_downloaddocument/9783642147029-c1.pdf&#34;&gt;http://www.springer.com/cda/content/document/cda_downloaddocument/9783642147029-c1.pdf&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.streamingmedia.com/Articles/Editorial/Featured-Articles/A-Progress-Report-The-Alliance-for-Open-Media-and-the-AV1-Codec-110383.aspx&#34;&gt;http://www.streamingmedia.com/Articles/Editorial/Featured-Articles/A-Progress-Report-The-Alliance-for-Open-Media-and-the-AV1-Codec-110383.aspx&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.streamingmediaglobal.com/Articles/ReadArticle.aspx?ArticleID=116505&amp;amp;PageNum=1&#34;&gt;http://www.streamingmediaglobal.com/Articles/ReadArticle.aspx?ArticleID=116505&amp;amp;PageNum=1&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://yumichan.net/video-processing/video-compression/introduction-to-h264-nal-unit/&#34;&gt;http://yumichan.net/video-processing/video-compression/introduction-to-h264-nal-unit/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cardinalpeak.com/blog/the-h-264-sequence-parameter-set/&#34;&gt;https://cardinalpeak.com/blog/the-h-264-sequence-parameter-set/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cardinalpeak.com/blog/worlds-smallest-h-264-encoder/&#34;&gt;https://cardinalpeak.com/blog/worlds-smallest-h-264-encoder/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://codesequoia.wordpress.com/category/video/&#34;&gt;https://codesequoia.wordpress.com/category/video/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://developer.apple.com/library/content/technotes/tn2224/_index.html&#34;&gt;https://developer.apple.com/library/content/technotes/tn2224/_index.html&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://en.wikibooks.org/wiki/MeGUI/x264_Settings&#34;&gt;https://en.wikibooks.org/wiki/MeGUI/x264_Settings&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Adaptive_bitrate_streaming&#34;&gt;https://en.wikipedia.org/wiki/Adaptive_bitrate_streaming&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/AOMedia_Video_1&#34;&gt;https://en.wikipedia.org/wiki/AOMedia_Video_1&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Chroma_subsampling#/media/File:Colorcomp.jpg&#34;&gt;https://en.wikipedia.org/wiki/Chroma_subsampling#/media/File:Colorcomp.jpg&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Cone_cell&#34;&gt;https://en.wikipedia.org/wiki/Cone_cell&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/File:H.264_block_diagram_with_quality_score.jpg&#34;&gt;https://en.wikipedia.org/wiki/File:H.264_block_diagram_with_quality_score.jpg&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Inter_frame&#34;&gt;https://en.wikipedia.org/wiki/Inter_frame&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Intra-frame_coding&#34;&gt;https://en.wikipedia.org/wiki/Intra-frame_coding&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Photoreceptor_cell&#34;&gt;https://en.wikipedia.org/wiki/Photoreceptor_cell&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Pixel_aspect_ratio&#34;&gt;https://en.wikipedia.org/wiki/Pixel_aspect_ratio&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Presentation_timestamp&#34;&gt;https://en.wikipedia.org/wiki/Presentation_timestamp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Rod_cell&#34;&gt;https://en.wikipedia.org/wiki/Rod_cell&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://it.wikipedia.org/wiki/File:Pixel_geometry_01_Pengo.jpg&#34;&gt;https://it.wikipedia.org/wiki/File:Pixel_geometry_01_Pengo.jpg&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://leandromoreira.com.br/2016/10/09/how-to-measure-video-quality-perception/&#34;&gt;https://leandromoreira.com.br/2016/10/09/how-to-measure-video-quality-perception/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sites.google.com/site/linuxencoding/x264-ffmpeg-mapping&#34;&gt;https://sites.google.com/site/linuxencoding/x264-ffmpeg-mapping&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://softwaredevelopmentperestroika.wordpress.com/2014/02/11/image-processing-with-python-numpy-scipy-image-convolution/&#34;&gt;https://softwaredevelopmentperestroika.wordpress.com/2014/02/11/image-processing-with-python-numpy-scipy-image-convolution/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tools.ietf.org/html/draft-fuldseth-netvc-thor-03&#34;&gt;https://tools.ietf.org/html/draft-fuldseth-netvc-thor-03&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.encoding.com/android/&#34;&gt;https://www.encoding.com/android/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.encoding.com/http-live-streaming-hls/&#34;&gt;https://www.encoding.com/http-live-streaming-hls/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://web.archive.org/web/20150129171151/https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm&#34;&gt;https://web.archive.org/web/20150129171151/https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.lifewire.com/cmos-image-sensor-493271&#34;&gt;https://www.lifewire.com/cmos-image-sensor-493271&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/pulse/brief-history-video-codecs-yoav-nativ&#34;&gt;https://www.linkedin.com/pulse/brief-history-video-codecs-yoav-nativ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/pulse/video-streaming-methodology-reema-majumdar&#34;&gt;https://www.linkedin.com/pulse/video-streaming-methodology-reema-majumdar&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.vcodex.com/h264avc-intra-precition/&#34;&gt;https://www.vcodex.com/h264avc-intra-precition/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=9vgtJJ2wwMA&#34;&gt;https://www.youtube.com/watch?v=9vgtJJ2wwMA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=LFXN9PiOGtY&#34;&gt;https://www.youtube.com/watch?v=LFXN9PiOGtY&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Lto-ajuqW3w&amp;amp;list=PLzH6n4zXuckpKAj1_88VS-8Z6yn9zX_P6&#34;&gt;https://www.youtube.com/watch?v=Lto-ajuqW3w&amp;amp;list=PLzH6n4zXuckpKAj1_88VS-8Z6yn9zX_P6&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=LWxu4rkZBLw&#34;&gt;https://www.youtube.com/watch?v=LWxu4rkZBLw&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://web.stanford.edu/class/ee398a/handouts/lectures/EE398a_MotionEstimation_2012.pdf&#34;&gt;https://web.stanford.edu/class/ee398a/handouts/lectures/EE398a_MotionEstimation_2012.pdf&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sander.saares.eu/categories/drm-is-not-a-black-box/&#34;&gt;https://sander.saares.eu/categories/drm-is-not-a-black-box/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>victordibia/peacasso</title>
    <updated>2022-09-09T01:37:24Z</updated>
    <id>tag:github.com,2022-09-09:/victordibia/peacasso</id>
    <link href="https://github.com/victordibia/peacasso" rel="alternate"></link>
    <summary type="html">&lt;p&gt;UI interface for experimenting with multimodal (text, image) models (stable diffusion).&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Peacasso&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/victordibia/peacasso/blob/master/notebooks/tutorial.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Peacasso [Beta] is a UI tool to help you generate art (and experiment) with multimodal (text, image) AI models (stable diffusion). This project is still in development (see roadmap below).&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/victordibia/peacasso/master/docs/images/screenpc.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Why Use Peacasso?&lt;/h2&gt; &#xA;&lt;img width=&#34;100%&#34; src=&#34;https://raw.githubusercontent.com/victordibia/peacasso/master/docs/images/peacasso.gif&#34;&gt; &#xA;&lt;p&gt;Because you deserve a nice UI and great workflow that makes exploring stable diffusion models fun! But seriously, here are a few things that make Peacasson interesting:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Easy installation&lt;/strong&gt;. Instead of cobbling together command line scripts, Peacasso provides a &lt;code&gt;pip install&lt;/code&gt; flow and a UI that supports a set of curated default operations.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;UI with good defaults&lt;/strong&gt;. The current implementation of Peacasso provides a UI for basic operations - text and image based prompting, remixing generated images as prompts, model parameter selection. Also covers the little things .. like light and dark mode.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Python API&lt;/strong&gt;. While the UI is the focus here, there is an underlying python api which will bake in experimentation features (e.g. saving intermediate images in the sampling loop, exploring model explanations etc. . see roadmap below).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Clearly, Peacasso (UI) might not be for those interested in low level code.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements and Installation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Step 1: &lt;strong&gt;Access to Weights via HuggingFace&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Access to the diffusion model weights requires a HuggingFace model account and access token. Please create an account at &lt;a href=&#34;https://huggingface.co/&#34;&gt;huggingface.co&lt;/a&gt;, get an &lt;a href=&#34;https://huggingface.co/settings/tokens&#34;&gt;access token&lt;/a&gt; and agree to the model terms &lt;a href=&#34;https://huggingface.co/CompVis/stable-diffusion-v1-4&#34;&gt;here&lt;/a&gt;. Next, create a &lt;code&gt;HF_API_TOKEN&lt;/code&gt; environment variable containing your token. &lt;code&gt;export HF_API_TOKEN=your_token&lt;/code&gt;. Note that the first time you run peacasso, the weights for the SD model are &lt;a href=&#34;https://huggingface.co/transformers/v4.0.1/installation.html#caching-models&#34;&gt;cached locally&lt;/a&gt; on your machine. In theory, you can download the weights, and run peacasso by pointing to the folder with the weights.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Step 2: &lt;strong&gt;Verify Environment - Pythong 3.7+ and CUDA&lt;/strong&gt; Setup and verify that your python environment is &lt;code&gt;python 3.7&lt;/code&gt; or higher (preferably, use Conda). Also verify that you have CUDA installed correctly (&lt;code&gt;torch.cuda.is_available()&lt;/code&gt; is true) and your GPU has about &lt;a href=&#34;https://stability.ai/blog/stable-diffusion-public-release&#34;&gt;7GB of VRAM memory&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Once requirements are met, run the following command to install the library:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install peacasso&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Don&#39;t have a GPU, you can still use the python api and UI in a colab notebook. See this &lt;a href=&#34;https://colab.research.google.com/github/victordibia/peacasso/blob/master/notebooks/tutorial.ipynb&#34;&gt;colab notebook&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Usage - UI and Python API&lt;/h2&gt; &#xA;&lt;p&gt;You can use the library from the ui by running the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;peacasso ui  --port=8080&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then navigate to &lt;a href=&#34;http://localhost:8080/&#34;&gt;http://localhost:8080/&lt;/a&gt; in your browser.&lt;/p&gt; &#xA;&lt;p&gt;You can also use the python api by running the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;import os&#xA;from dotenv import load_dotenv&#xA;from peacasso.generator import ImageGenerator&#xA;from peacasso.datamodel import GeneratorConfig&#xA;&#xA;token = os.environ.get(&#34;HF_API_TOKEN&#34;)&#xA;gen = ImageGenerator(token=token)&#xA;prompt = &#34;A sea lion wandering the streets of post apocalyptic London&#34;&#xA;&#xA;prompt_config = GeneratorConfig(&#xA;    prompt=prompt,&#xA;    num_images=3,&#xA;    width=512,&#xA;    height=512,&#xA;    guidance_scale=7.5,&#xA;    num_inference_steps=50,&#xA;    mode=&#34;prompt&#34;,  # prompt, image&#xA;    return_intermediates=True, # return intermediate images in the generate dict response&#xA;)&#xA;&#xA;result = gen.generate(prompt_config)&#xA;for i, image in enumerate(result[&#34;images&#34;]):&#xA;    image.save(f&#34;image_{i}.png&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Features and Road Map&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Command line interface&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; UI Features. Query models with multiple parametrs &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Text prompting (text2img)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Image based prompting (img2img)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Editor (for inpainting and outpainting)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Latent space exploration&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Experimentation tools &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Save intermediate images in the sampling loop&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Prompt recommendation tools&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Model explanations&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Curation/sharing experiment results&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;This work builds on the stable diffusion model and code is adapted from the HuggingFace &lt;a href=&#34;https://huggingface.co/blog/stable_diffusion&#34;&gt;implementation&lt;/a&gt;. Please note the - &lt;a href=&#34;https://huggingface.co/spaces/CompVis/stable-diffusion-license&#34;&gt;CreativeML Open RAIL-M&lt;/a&gt; license associated with the stable diffusion model.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>datawhalechina/competition-baseline</title>
    <updated>2022-09-09T01:37:24Z</updated>
    <id>tag:github.com,2022-09-09:/datawhalechina/competition-baseline</id>
    <link href="https://github.com/datawhalechina/competition-baseline" rel="alternate"></link>
    <summary type="html">&lt;p&gt;数据科学竞赛知识、代码、思路&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;数据竞赛Baseline &amp;amp; Topline分享&lt;/h1&gt; &#xA;&lt;p&gt;假如你是数据竞赛的初学者、爱好者，比赛的baseline不仅是比赛思路分享，同时也是一类数据问题的方法总结。本Repo想做的就是将收集并整理并分享各种比赛的baseline方案。&lt;/p&gt; &#xA;&lt;p&gt;你可能会问为什么是baseline，而不是获胜者的代码分享？相比于获胜者的代码baseline代码都比较简单，容易整理和学习；其次baseline代码更加实用和简洁，适合入门学习。&lt;/p&gt; &#xA;&lt;h2&gt;数据竞赛&lt;/h2&gt; &#xA;&lt;p&gt;竞赛日历：&lt;a href=&#34;http://coggle.club/&#34;&gt;http://coggle.club/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;最新的竞赛信息和baseline推送，请关注：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;竞赛公众号：&lt;a href=&#34;https://t.zsxq.com/Eyn6EQr&#34;&gt;&lt;strong&gt;Coggle数据科学&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;知乎专栏：&lt;a href=&#34;https://zhuanlan.zhihu.com/DataAI&#34;&gt;机器学习理论与数据竞赛实战&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;竞赛分享&lt;/h2&gt; &#xA;&lt;p&gt;每个比赛的详细分享请见&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition&#34;&gt;competition文件夹&lt;/a&gt;；&lt;/p&gt; &#xA;&lt;p&gt;如果本仓库访问速度慢，可以访问国内备份：&lt;a href=&#34;https://gitee.com/coggle/competition-baseline&#34;&gt;https://gitee.com/coggle/competition-baseline&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;2022年度 iFLYTEK A.I. 开发者大赛&lt;/h3&gt; &#xA;&lt;p&gt;2022年度 iFLYTEK A.I. 开发者大赛来了。本届大赛的总奖金池已升级到了超420万元，除此外还将进一步开放海量数据与核心技术，汇聚更多人工智能开发者，提供创孵平台，培育优质团队，给予扶持政策等。&lt;/p&gt; &#xA;&lt;p&gt;本届大赛按照算法、应用、编程赛、虚拟形象选拔、辩论赛、创意集市创意赛等等方向设置众多赛道；覆盖了智能语音、视觉、自然语言、图文识别等AI热门技术；涵盖了元宇宙、遗址文化、生物与环保、医疗健康、智能家居、电商销售等众多领域。大赛地址：&lt;a href=&#34;https://challenge.xfyun.cn/?ch=ds22-dw-sq04&#34;&gt;https://challenge.xfyun.cn/?ch=ds22-dw-sq04&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;AIWIN 秋季竞赛&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;赛题1- 手写体 OCR 识别竞赛&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;手写体 OCR 识别竞赛由交通银行命题，设立两个任务，其中任务一由第四范式提供开放数据集，特别针对金额和日期做识别，任务二要求在指定训练环境完成不可下载训练集的训练，增加了银行机构的文本内容。任务一适合新手，并配套学习营和特别的学习奖励。&lt;/p&gt; &#xA;&lt;p&gt;比赛地址：&lt;a href=&#34;http://ailab.aiwin.org.cn/competitions/65&#34;&gt;http://ailab.aiwin.org.cn/competitions/65&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;baseline地址：&lt;a href=&#34;https://aistudio.baidu.com/aistudio/projectdetail/2612313&#34;&gt;https://aistudio.baidu.com/aistudio/projectdetail/2612313&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;赛题2- 心电图智能诊断竞赛&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;心电图智能诊断竞赛由数创医疗和复旦大学附属中山医院共同命题，设立两个任务，其中任务一诊断心电图的正常异常与否，任务二对10+种不同症状予以判断综合分类。任务一同步设有学习营和配套的学习奖励，欢迎新手参与。&lt;/p&gt; &#xA;&lt;p&gt;比赛地址：&lt;a href=&#34;http://ailab.aiwin.org.cn/competitions/64&#34;&gt;http://ailab.aiwin.org.cn/competitions/64&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;baseline地址：&lt;a href=&#34;https://aistudio.baidu.com/aistudio/projectdetail/2653802&#34;&gt;https://aistudio.baidu.com/aistudio/projectdetail/2653802&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;2021阿里云供应链大赛——需求预测及单级库存优化&lt;/h3&gt; &#xA;&lt;p&gt;报名链接：&lt;a href=&#34;https://tianchi.aliyun.com/competition/entrance/531934/introduction&#34;&gt;https://tianchi.aliyun.com/competition/entrance/531934/introduction&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;比赛baseline:&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/2021%E9%98%BF%E9%87%8C%E4%BA%91%E4%BE%9B%E5%BA%94%E9%93%BE%E5%A4%A7%E8%B5%9B%E2%80%94%E2%80%94%E9%9C%80%E6%B1%82%E9%A2%84%E6%B5%8B%E5%8F%8A%E5%8D%95%E7%BA%A7%E5%BA%93%E5%AD%98%E4%BC%98%E5%8C%96&#34;&gt;https://github.com/datawhalechina/competition-baseline/tree/master/competition/2021阿里云供应链大赛——需求预测及单级库存优化&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;CCF BDCI 2021&lt;/h3&gt; &#xA;&lt;p&gt;baseline汇总：&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/DataFountain-CCFBDI-2021&#34;&gt;https://github.com/datawhalechina/competition-baseline/tree/master/competition/DataFountain-CCFBDI-2021&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;基于飞桨实现花样滑冰选手骨骼点动作识别，计算机视觉、姿态估计&lt;/li&gt; &#xA; &lt;li&gt;千言-问题匹配鲁棒性评测，自然语言处理、文本匹配&lt;/li&gt; &#xA; &lt;li&gt;基于MindSpore AI框架实现零售商品识别，计算机视觉、图像分类&lt;/li&gt; &#xA; &lt;li&gt;产品评论观点提取，自然语言处理、实体抽取&lt;/li&gt; &#xA; &lt;li&gt;个贷违约预测，结构化数据挖掘、金融风控&lt;/li&gt; &#xA; &lt;li&gt;剧本角色情感识别，自然语言处理、实体抽取&lt;/li&gt; &#xA; &lt;li&gt;基于UEBA的用户上网异常行为分析，结构化数据挖掘、异常检测&lt;/li&gt; &#xA; &lt;li&gt;POI名称生成，计算机视觉、OCR&lt;/li&gt; &#xA; &lt;li&gt;客服通话文本摘要提取，自然语言处理、文本摘要&lt;/li&gt; &#xA; &lt;li&gt;系统认证风险预测，结构化数据挖掘、风险检测&lt;/li&gt; &#xA; &lt;li&gt;泛在感知数据关联融合计算，结构化数据挖掘、相似度计算&lt;/li&gt; &#xA; &lt;li&gt;openLooKeng跨域数据分析性能提升，数据仓储SQL优化&lt;/li&gt; &#xA; &lt;li&gt;大规模金融仿真图数据中金融交易环路查询的设计与性能优化，金融交易图谱挖掘&lt;/li&gt; &#xA; &lt;li&gt;基于BERT的大模型容量挑战赛，深度学习模型优化&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;华为DIGIX2021：全球校园AI算法精英大赛&lt;/h3&gt; &#xA;&lt;p&gt;报名链接：&lt;a href=&#34;https://developer.huawei.com/consumer/cn/activity/digixActivity/digixdetail/201621215957378831?ha_source=gb_sf&amp;amp;ha_sourceId=89000073&#34;&gt;https://developer.huawei.com/consumer/cn/activity/digixActivity/digixdetail/201621215957378831?ha_source=gb_sf&amp;amp;ha_sourceId=89000073&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;赛题1：基于多目标多视图的用户留存周期预测&lt;/li&gt; &#xA; &lt;li&gt;赛题2：基于多模型迁移预训练文章质量判别&lt;/li&gt; &#xA; &lt;li&gt;赛题3：基于多目标优化的视频推荐&lt;/li&gt; &#xA; &lt;li&gt;赛题4：基于多模态多语言的搜索排序&lt;/li&gt; &#xA; &lt;li&gt;赛题5：小样本菜单识别&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;比赛baseline和学习资料：&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/DIGIX2021&#34;&gt;https://github.com/datawhalechina/competition-baseline/tree/master/competition/DIGIX2021&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;科大讯飞AI开发者大赛2021&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://challenge.xfyun.cn/topic/info?type=chinese-question-similarity&amp;amp;ch=dw-sq-1&#34;&gt;中文问题相似度挑战赛&lt;/a&gt;, &lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/raw/master/competition/%E7%A7%91%E5%A4%A7%E8%AE%AF%E9%A3%9EAI%E5%BC%80%E5%8F%91%E8%80%85%E5%A4%A7%E8%B5%9B2021/%E4%B8%AD%E6%96%87%E9%97%AE%E9%A2%98%E7%9B%B8%E4%BC%BC%E5%BA%A6%E6%8C%91%E6%88%98%E8%B5%9B/bert-nsp-xunfei.ipynb&#34;&gt;学习资料&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://challenge.xfyun.cn/topic/info?type=offline-store-sales-forecast&amp;amp;ch=dw-sq-1&#34;&gt;线下商店销量预测挑战赛&lt;/a&gt;, &lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/%E7%A7%91%E5%A4%A7%E8%AE%AF%E9%A3%9EAI%E5%BC%80%E5%8F%91%E8%80%85%E5%A4%A7%E8%B5%9B2021/%E7%A7%91%E5%A4%A7%E8%AE%AF%E9%A3%9E%E5%95%86%E5%BA%97%E9%94%80%E9%87%8F%E9%A2%84%E6%B5%8B&#34;&gt;学习资料&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://challenge.xfyun.cn/topic/info?type=e-commerce-image-retrieval&amp;amp;ch=dw-sq-1&#34;&gt;电商图像检索挑战赛&lt;/a&gt;, &lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/%E7%A7%91%E5%A4%A7%E8%AE%AF%E9%A3%9EAI%E5%BC%80%E5%8F%91%E8%80%85%E5%A4%A7%E8%B5%9B2021/%E7%94%B5%E5%95%86%E5%9B%BE%E5%83%8F%E6%A3%80%E7%B4%A2%E6%8C%91%E6%88%98%E8%B5%9B&#34;&gt;学习资料&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://challenge.xfyun.cn/topic/info?type=facial-emotion-recognition&amp;amp;ch=dw-sq-1&#34;&gt;人脸情绪识别挑战赛&lt;/a&gt;, &lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/%E7%A7%91%E5%A4%A7%E8%AE%AF%E9%A3%9EAI%E5%BC%80%E5%8F%91%E8%80%85%E5%A4%A7%E8%B5%9B2021/%E4%BA%BA%E8%84%B8%E6%83%85%E7%BB%AA%E8%AF%86%E5%88%AB%E6%8C%91%E6%88%98%E8%B5%9B&#34;&gt;学习资料&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://challenge.xfyun.cn/topic/info?type=academic-paper-classification&amp;amp;ch=dw-sq-1&#34;&gt;学术论文分类挑战赛&lt;/a&gt;, &lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/%E7%A7%91%E5%A4%A7%E8%AE%AF%E9%A3%9EAI%E5%BC%80%E5%8F%91%E8%80%85%E5%A4%A7%E8%B5%9B2021/%E5%AD%A6%E6%9C%AF%E8%AE%BA%E6%96%87%E5%88%86%E7%B1%BB%E6%8C%91%E6%88%98%E8%B5%9B&#34;&gt;学习资料&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://challenge.xfyun.cn/topic/info?type=car-loan&amp;amp;ch=dw-sq-1&#34;&gt;车辆贷款违约预测挑战赛&lt;/a&gt;, &lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/%E7%A7%91%E5%A4%A7%E8%AE%AF%E9%A3%9EAI%E5%BC%80%E5%8F%91%E8%80%85%E5%A4%A7%E8%B5%9B2021/%E8%BD%A6%E8%BE%86%E8%B4%B7%E6%AC%BE%E8%BF%9D%E7%BA%A6%E9%A2%84%E6%B5%8B%E6%8C%91%E6%88%98%E8%B5%9B&#34;&gt;学习资料&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://challenge.xfyun.cn/topic/info?type=ad-2021&amp;amp;ch=dw-sq-1&#34;&gt;广告图片素材分类算法挑战赛&lt;/a&gt;，&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/%E7%A7%91%E5%A4%A7%E8%AE%AF%E9%A3%9EAI%E5%BC%80%E5%8F%91%E8%80%85%E5%A4%A7%E8%B5%9B2021&#34;&gt;基础的分类思路&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://challenge.xfyun.cn/topic/info?type=crop&amp;amp;ch=dw-sq-1&#34;&gt;农作物生长情况识别挑战赛&lt;/a&gt;, &lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/raw/master/competition/%E7%A7%91%E5%A4%A7%E8%AE%AF%E9%A3%9EAI%E5%BC%80%E5%8F%91%E8%80%85%E5%A4%A7%E8%B5%9B2021/%E4%B8%AD%E5%9B%BD%E5%86%9C%E4%B8%9A%E5%A4%A7%E5%AD%A6_%E5%86%9C%E4%BD%9C%E7%89%A9%E7%94%9F%E9%95%BF%E6%83%85%E5%86%B5%E8%AF%86%E5%88%AB%E6%8C%91%E6%88%98%E8%B5%9B.ipynb&#34;&gt;keras&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://challenge.xfyun.cn/topic/info?type=guide-photo&amp;amp;ch=dw-sq-1&#34;&gt;引导拍照挑战赛&lt;/a&gt;, &lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/raw/master/competition/%E7%A7%91%E5%A4%A7%E8%AE%AF%E9%A3%9EAI%E5%BC%80%E5%8F%91%E8%80%85%E5%A4%A7%E8%B5%9B2021/%E4%B8%AD%E5%9B%BD%E5%86%9C%E4%B8%9A%E5%A4%A7%E5%AD%A6_%E5%BC%95%E5%AF%BC%E6%8B%8D%E7%85%A7%E6%8C%91%E6%88%98%E8%B5%9B.ipynb&#34;&gt;keras&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://challenge.xfyun.cn/topic/info?type=PET&amp;amp;ch=dw-sq-1&#34;&gt;脑部PETMR图像疾病预测挑战赛&lt;/a&gt;, &lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/raw/master/competition/%E7%A7%91%E5%A4%A7%E8%AE%AF%E9%A3%9EAI%E5%BC%80%E5%8F%91%E8%80%85%E5%A4%A7%E8%B5%9B2021/%E5%AE%89%E5%BE%BD%E5%A4%A7%E5%AD%A6-%E8%84%91%E9%83%A8PETMR%E5%9B%BE%E5%83%8F%E7%96%BE%E7%97%85%E9%A2%84%E6%B5%8B%E6%8C%91%E6%88%98%E8%B5%9B.ipynb&#34;&gt;keras&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://challenge.xfyun.cn/topic/info?type=time-frequency&amp;amp;ch=dw-sq-1&#34;&gt;智能硬件语音控制的时频图分类挑战赛&lt;/a&gt;, &lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/raw/master/competition/%E7%A7%91%E5%A4%A7%E8%AE%AF%E9%A3%9EAI%E5%BC%80%E5%8F%91%E8%80%85%E5%A4%A7%E8%B5%9B2021/%E6%99%BA%E8%83%BD%E7%A1%AC%E4%BB%B6%E8%AF%AD%E9%9F%B3%E6%8E%A7%E5%88%B6%E7%9A%84%E6%97%B6%E9%A2%91%E5%9B%BE%E5%88%86%E7%B1%BB%E6%8C%91%E6%88%98%E8%B5%9B.ipynb&#34;&gt;pytorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://challenge.xfyun.cn/topic/info?type=user-portrait&amp;amp;ch=dw-sq-1&#34;&gt;基于用户画像的商品推荐挑战赛&lt;/a&gt;, &lt;a href=&#34;https://mp.weixin.qq.com/s/KDH_klH_74726S8gX4FEyQ&#34;&gt;LSTM-0.6786&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://challenge.xfyun.cn/topic/info?type=protein&amp;amp;ch=dw-sq-1&#34;&gt;蛋白质结构预测挑战赛算法&lt;/a&gt;, &lt;a href=&#34;https://mp.weixin.qq.com/s/0oGWD0O5ARokxrAiW2T9uQ&#34;&gt;CNN-0.21&lt;/a&gt;, &lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/raw/master/competition/%E7%A7%91%E5%A4%A7%E8%AE%AF%E9%A3%9EAI%E5%BC%80%E5%8F%91%E8%80%85%E5%A4%A7%E8%B5%9B2021/%E4%B8%8A%E6%B5%B7%E6%B5%B7%E4%BA%8B%E5%A4%A7%E5%AD%A6_%E8%9B%8B%E7%99%BD%E8%B4%A8%E7%BB%93%E6%9E%84%E9%A2%84%E6%B5%8B%E8%B5%9B.ipynb&#34;&gt;XGB基础代码&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://challenge.xfyun.cn/topic/info?type=air-quality&amp;amp;ch=dw-sq-1&#34;&gt;环境空气质量评价挑战赛&lt;/a&gt;, &lt;a href=&#34;https://mp.weixin.qq.com/s/9gZJ6ScwW1urRMc-6p6n5A&#34;&gt;LR-0.04385&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://challenge.xfyun.cn/topic/info?type=pig-check&amp;amp;ch=dw-sq-1&#34;&gt;猪只盘点挑战赛&lt;/a&gt;, &lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/raw/master/competition/%E7%A7%91%E5%A4%A7%E8%AE%AF%E9%A3%9EAI%E5%BC%80%E5%8F%91%E8%80%85%E5%A4%A7%E8%B5%9B2021/%E7%A7%91%E5%A4%A7%E8%AE%AF%E9%A3%9E%E8%82%A1%E4%BB%BD%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8_%E7%8C%AA%E5%8F%AA%E7%9B%98%E7%82%B9%E6%8C%91%E6%88%98%E8%B5%9B.ipynb&#34;&gt;预训练模型&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://challenge.xfyun.cn/topic/info?type=covid-19&amp;amp;ch=dw-sq-1&#34;&gt;新冠肺炎声音诊断挑战赛&lt;/a&gt;，&lt;a href=&#34;https://github.com/zfs1998/data-science/raw/main/IFLYTEK/%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E5%A4%A7%E5%AD%A6_%E6%96%B0%E5%86%A0%E8%82%BA%E7%82%8E%E5%A3%B0%E9%9F%B3%E8%AF%8A%E6%96%AD%E6%8C%91%E6%88%98%E8%B5%9B.ipynb&#34;&gt;baseline 0.53532 Top20&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://algo.qq.com/&#34;&gt;腾讯广告算法大赛&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h4&gt;2021年度腾讯赛&lt;/h4&gt; &#xA;&lt;p&gt;本届从广告应用场景痛点出发，开设“视频广告秒级语义解析”和“多模态视频广告标签”两大赛道，兼具算法挑战性和商业应用价值。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cloud.tencent.com/developer/article/1807916&#34;&gt;TI-ONE 产品使用教程&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cloud.tencent.com/developer/article/1807942?from=10680&#34;&gt;如何使用 Notebook 功能完成赛事训练&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://algo-1256087447.cos.ap-nanjing.myqcloud.com/admin/20210430/cf48d04caf878b9d2773fda0e60ba8a8.pdf&#34;&gt;腾讯广告算法大赛参赛手册&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.qq.com/doc/DV1hFUGpMV1l3eVdV&#34;&gt;腾讯广告算法大赛FAQ&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;2020年度腾讯赛&lt;/h4&gt; &#xA;&lt;p&gt;本届以用户在广告系统中的交互行为作为输入来预测用户的人口统计学属性。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/-lizDyP2y357plcG1M64TA&#34;&gt;冠军分享&lt;/a&gt;，&lt;a href=&#34;https://github.com/guoday/Tencent2020_Rank1st&#34;&gt;开源代码&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/UWt4hZitX9bW1Y_RNyCJCg&#34;&gt;亚军分享&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/rkhwLsCKTIDzUkjVIEj3LQ&#34;&gt;季军分享&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/170603281&#34;&gt;第5名分享&lt;/a&gt;，&lt;a href=&#34;https://github.com/zhangqibot/Tencent2020_Top5&#34;&gt;开源代码&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;第11名：&lt;a href=&#34;https://github.com/wujiekd/2020-Tencent-advertising-algorithm-contest-rank11&#34;&gt;开源代码1&lt;/a&gt;, &lt;a href=&#34;https://github.com/llllllyu/Tencent2020_Rank11&#34;&gt;开源代码2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;第12名：&lt;a href=&#34;https://github.com/LogicJake/Tencent_Ads_Algo_2020_TOP12&#34;&gt;开源代码&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;第19名：&lt;a href=&#34;https://github.com/PerpetualSmile/2020-Tencent-Advertisement-Algorithm-Competition-Rank19&#34;&gt;开源代码&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;http://ailab.aiwin.org.cn/&#34;&gt;2021世界人工智能创新大赛&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/AIWIN2021&#34;&gt;互联网舆情企业风险事件的识别和预警&lt;/a&gt;, NLP类型比赛&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/AIWIN2021&#34;&gt;保险文本视觉认知问答竞赛&lt;/a&gt;, CV/NLP，多模态类型比赛&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://www.datafountain.cn/special/BDCI2020&#34;&gt;CCF BDCI2020大数据与计算智能大赛&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;通用音频分类, &lt;a href=&#34;https://github.com/zjuzpw/baseline/raw/CCF2020BDCI/baseline_lgb.ipynb&#34;&gt;LGB&lt;/a&gt;, &lt;a href=&#34;https://blog.csdn.net/wherewegogo/article/details/110369729&#34;&gt;CNN, 0.9+&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;遥感影像地块分割, &lt;a href=&#34;https://aistudio.baidu.com/aistudio/projectdetail/1090790&#34;&gt;U-Net&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;房产行业聊天问答匹配, &lt;a href=&#34;https://github.com/syzong/2020_ccf_qa_match&#34;&gt;Bert&lt;/a&gt;, &lt;a href=&#34;https://github.com/LogicJake/competition_baselines/tree/master/competitions/property_chat_pair&#34;&gt;RoBERTa&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;小学数学应用题自动解题, &lt;a href=&#34;https://discussion.datafountain.cn/questions/3169?new=0&#34;&gt;规则思路&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;路况状态时空预测, &lt;a href=&#34;https://github.com/juzstu/ccf2020_didi&#34;&gt;OTTO Lab&lt;/a&gt;, &lt;a href=&#34;https://mp.weixin.qq.com/s/1vJDOInUOdBgXtLdVcpsEA&#34;&gt;异度侵入&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;企业非法集资风险预测, &lt;a href=&#34;https://github.com/LogicJake/competition_baselines/tree/master/competitions/fund_raising_risk_prediction&#34;&gt;第一次打比赛&lt;/a&gt;, &lt;a href=&#34;https://github.com/DLLXW/data-science-competition/tree/main/datafountain&#34;&gt;DLLXW&lt;/a&gt;, &lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/DataFountain-%E4%BC%81%E4%B8%9A%E9%9D%9E%E6%B3%95%E9%9B%86%E8%B5%84%E9%A3%8E%E9%99%A9%E9%A2%84%E6%B5%8B&#34;&gt;阿水&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;大数据时代的Serverless工作负载预测, &lt;a href=&#34;https://github.com/LogicJake/competition_baselines/tree/master/competitions/serverless_load_prediction&#34;&gt;第一次打比赛（A榜0.208）&lt;/a&gt;, &lt;a href=&#34;https://blog.csdn.net/qq_48081601/article/details/109338443&#34;&gt;siguo（A榜0.211）&lt;/a&gt;, &lt;a href=&#34;https://zhuanlan.zhihu.com/p/301092469&#34;&gt;CNN-LSTM&lt;/a&gt;,&lt;a href=&#34;https://mp.weixin.qq.com/s/Ovb1pic2nleQhTObIaj2Ww&#34;&gt;鱼佬(0.285)&lt;/a&gt;, &lt;a href=&#34;https://github.com/siliconx/serverless&#34;&gt;siliconx(0.311)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;重点区域人群密度预测, &lt;a href=&#34;https://github.com/agave233/2020-CCF-Crowd-Flow-Prediction&#34;&gt;第1名方案&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;http://www.industrial-bigdata.com/Challenge/title?competitionId=GKLEW707XP2O58KZNLO4UPYKCOIEQONH&#34;&gt;第四届工业大数据创新竞赛——算法赛道&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;学习手册：&lt;a href=&#34;https://coggle.club/learn/industrial-bigdata-4th/&#34;&gt;https://coggle.club/learn/industrial-bigdata-4th/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/raw/master/competition/%E7%AC%AC%E5%9B%9B%E5%B1%8A%E5%B7%A5%E4%B8%9A%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%9B%E6%96%B0%E7%AB%9E%E8%B5%9B%EF%BC%9A%E7%AE%97%E6%B3%95%E8%B5%9B%E9%81%93/%E6%B3%A8%E5%A1%91%E6%88%90%E5%9E%8B%E8%B5%9B%E9%81%93baseline.ipynb&#34;&gt;注塑成型工艺的虚拟量测和调机优化&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://data.xm.gov.cn/opendata-competition/index.html#/&#34;&gt;2020数字中国创新大赛大数据赛道&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;入门注册手册：&lt;a href=&#34;https://mp.weixin.qq.com/s/NurvUDyGwVC4sSwzEzDrwg&#34;&gt;https://mp.weixin.qq.com/s/NurvUDyGwVC4sSwzEzDrwg&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;高德地图城市交通健康榜：&lt;a href=&#34;https://report.amap.com/diagnosis/index.do&#34;&gt;https://report.amap.com/diagnosis/index.do&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;赛题1数据分析：&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/2020DCIC-%E5%88%9B%E6%96%B0%E5%A4%A7%E8%B5%9B%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%B5%9B%E9%81%93&#34;&gt;链接&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;赛题2学习内容：&lt;a href=&#34;https://coggle.club/learn/DCIC2020/&#34;&gt;链接&lt;/a&gt;, &lt;a href=&#34;https://www.bilibili.com/video/BV1tz4y1f7Wg/&#34;&gt;录屏&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;http://challenge.xfyun.cn/&#34;&gt;科大讯飞AI开发者大赛（2020年度）&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/%E7%A7%91%E5%A4%A7%E8%AE%AF%E9%A3%9EAI%E5%BC%80%E5%8F%91%E8%80%85%E5%A4%A7%E8%B5%9B-%E8%84%91PET%E5%9B%BE%E5%83%8F%E5%88%86%E6%9E%90%E5%92%8C%E7%96%BE%E7%97%85%E9%A2%84%E6%B5%8B%E6%8C%91%E6%88%98%E8%B5%9B%E7%AE%97%E6%B3%95%E6%8C%91%E6%88%98%E5%A4%A7%E8%B5%9B&#34;&gt;脑PET图像分析和疾病预测挑战赛算法挑战大赛&lt;/a&gt;, CV类型比赛&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/%E7%A7%91%E5%A4%A7%E8%AE%AF%E9%A3%9EAI%E5%BC%80%E5%8F%91%E8%80%85%E5%A4%A7%E8%B5%9B-%E6%B8%A9%E5%AE%A4%E6%B8%A9%E5%BA%A6%E9%A2%84%E6%B5%8B%E6%8C%91%E6%88%98%E8%B5%9B&#34;&gt;温室温度预测挑战赛&lt;/a&gt;, 结构化数据比赛&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/%E7%A7%91%E5%A4%A7%E8%AE%AF%E9%A3%9EAI%E5%BC%80%E5%8F%91%E8%80%85%E5%A4%A7%E8%B5%9B-%E5%A9%B4%E5%84%BF%E5%95%BC%E5%93%AD%E5%A3%B0%E8%AF%86%E5%88%AB%E6%8C%91%E6%88%98%E8%B5%9B&#34;&gt;婴儿啼哭声识别挑战赛&lt;/a&gt;, 语音赛题比赛&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/%E7%A7%91%E5%A4%A7%E8%AE%AF%E9%A3%9EAI%E5%BC%80%E5%8F%91%E8%80%85%E5%A4%A7%E8%B5%9B-%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96%E6%8C%91%E6%88%98&#34;&gt;事件抽取挑战赛&lt;/a&gt;, NLP类型比赛&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;结构化比赛&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tianchi.aliyun.com/s/ea4fbcbaadab849b7389354501f38e2e&#34;&gt;第三届 Apache Flink 极客挑战赛暨AAIG CUP&lt;/a&gt;, &lt;a href=&#34;https://gitee.com/coggle/tianchi-3rd-AAIG-CUP&#34;&gt;TF2 baseline&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://data.sd.gov.cn/cmpt/cmptDetail.html?id=24&#34;&gt;山东省第二届数据应用创新创业大赛-临沂分赛场-供水管网压力预测&lt;/a&gt;, &lt;a href=&#34;https://github.com/China-ChallengeHub/ChallengeHub-Baselines/tree/main/shandong_shuiguan/code&#34;&gt;ChallengeHub&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://data.sd.gov.cn/cmpt/cmptDetail.html?id=22&#34;&gt;山东省第二届数据应用创新创业大赛-济南分赛场-健康医疗&lt;/a&gt;, &lt;a href=&#34;https://github.com/China-ChallengeHub/ChallengeHub-Baselines/tree/main/%E5%B1%B1%E4%B8%9C%E5%81%A5%E5%BA%B7%E5%8C%BB%E7%96%97&#34;&gt;ChallengeHub&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://data.sd.gov.cn/cmpt/cmptDetail.html?id=26&#34;&gt;山东省第二届数据应用创新创业大赛-日照分赛场-公积金贷款逾期预测&lt;/a&gt;, &lt;a href=&#34;https://github.com/China-ChallengeHub/ChallengeHub-Baselines/tree/main/shandong_gongjijin/code&#34;&gt;ChallengeHub&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.dcjingsai.com/v2/cmptDetail.html?id=439&amp;amp;=76f6724e6fa9455a9b5ef44402c08653&#34;&gt;2020厦门国际银行数创金融杯建模大赛&lt;/a&gt;, &lt;a href=&#34;https://github.com/LogicJake/competition_baselines/tree/master/competitions/xiamen_international_bank_2020&#34;&gt;第一次打比赛&lt;/a&gt;, &lt;a href=&#34;https://github.com/cXPromise/Datacasle_2020XM_Baseline&#34;&gt;OTTO Lab&lt;/a&gt;, &lt;a href=&#34;https://github.com/BirderEric/XianmenBank&#34;&gt;0.46&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://m.dcjingsai.com/cmptDetail.html?id=319&#34;&gt;2019厦门国际银行“数创金融杯”数据建模大赛&lt;/a&gt;, &lt;a href=&#34;https://github.com/yanqiangmiffy/Data-Finance-Cup&#34;&gt;yanqiangmiffy&lt;/a&gt;, &lt;a href=&#34;https://github.com/shenxiangzhuang/Bank-Competition&#34;&gt;shenxiangzhuang&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tianchi.aliyun.com/competition/entrance/231784/introduction&#34;&gt;天池-零基础入门数据挖掘 - 二手车交易价格预测&lt;/a&gt;, &lt;a href=&#34;https://github.com/yangjiada/used_cars&#34;&gt;baseline链接&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/Tianchi-2020%E6%95%B0%E5%AD%97%E4%B8%AD%E5%9B%BD%E5%88%9B%E6%96%B0%E5%A4%A7%E8%B5%9B%E2%80%94%E7%AE%97%E6%B3%95%E8%B5%9B%EF%BC%9A%E6%99%BA%E6%85%A7%E6%B5%B7%E6%B4%8B%E5%BB%BA%E8%AE%BE&#34;&gt;天池-2020数字中国创新大赛—算法赛：智慧海洋建设&lt;/a&gt;, 结构化数据比赛&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/DataFountain-%E4%B9%98%E7%94%A8%E8%BD%A6%E7%BB%86%E5%88%86%E5%B8%82%E5%9C%BA%E9%94%80%E9%87%8F%E9%A2%84%E6%B5%8B&#34;&gt;DataFountain-乘用车细分市场销量预测&lt;/a&gt;, 结构化 数据比赛&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/DataFountain-%E7%A6%BB%E6%95%A3%E5%88%B6%E9%80%A0%E8%BF%87%E7%A8%8B%E4%B8%AD%E5%85%B8%E5%9E%8B%E5%B7%A5%E4%BB%B6%E7%9A%84%E8%B4%A8%E9%87%8F%E7%AC%A6%E5%90%88%E7%8E%87%E9%A2%84%E6%B5%8B&#34;&gt;DataFountain-离散制造过程中典型工件的质量符合率预测&lt;/a&gt;, 结构化数据比赛&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/%E8%85%BE%E8%AE%AF-2018%E8%85%BE%E8%AE%AF%E5%B9%BF%E5%91%8A%E7%AE%97%E6%B3%95%E5%A4%A7%E8%B5%9B&#34;&gt;腾讯-2018腾讯广告算法大赛 Rank11&lt;/a&gt;，结构化数据比赛&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/%E8%85%BE%E8%AE%AF-2019%E8%85%BE%E8%AE%AF%E5%B9%BF%E5%91%8A%E7%AE%97%E6%B3%95%E5%A4%A7%E8%B5%9B&#34;&gt;腾讯-2018腾讯广告算法大赛 冠军&lt;/a&gt;，结构化数据比赛&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/Tianchi-%E5%AE%89%E6%B3%B0%E6%9D%AF%E8%B7%A8%E5%A2%83%E7%94%B5%E5%95%86%E6%99%BA%E8%83%BD%E7%AE%97%E6%B3%95%E5%A4%A7%E8%B5%9B&#34;&gt;天池-安泰杯跨境电商智能算法大赛&lt;/a&gt;，结构化数据比赛，&lt;strong&gt;冠军法国南部&lt;/strong&gt;分享&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/%E7%82%B9%E7%9F%B3-Retention%20Rate%20of%20Baidu%20Hao%20Kan%20APP%20Users&#34;&gt;点石-Retention Rate of Baidu Hao Kan APP Users&lt;/a&gt;，结构化数据比赛&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/kaggle-two-sigma-connect-rental-listing-inquiries&#34;&gt;kaggle-two-sigma-connect-rental-listing-inquiries&lt;/a&gt;，结构化数据比赛&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/kaggle-allstate-claims-severity&#34;&gt;kaggle-allstate-claims-severity&lt;/a&gt;，结构化数据比赛&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/yanxishe-%E7%99%BD%E8%91%A1%E8%90%84%E9%85%92%E5%93%81%E8%B4%A8%E9%A2%84%E6%B5%8B&#34;&gt;AI研习社-白葡萄酒品质预测&lt;/a&gt;，结构化数据比赛&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/yanxishe-%E8%82%8C%E8%82%89%E6%B4%BB%E5%8A%A8%E7%94%B5%E4%BF%A1%E5%8F%B7%E6%8E%A8%E6%B5%8B%E6%89%8B%E5%8A%BF&#34;&gt;AI研习社-肌肉活动电信号推测手势&lt;/a&gt;，结构化数据比赛&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;CV类型比赛&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tianchi.aliyun.com/competition/entrance/531902/introduction&#34;&gt;“英特尔创新大师杯”深度学习挑战赛 赛道1：通用场景OCR文本识别任务&lt;/a&gt;, OCR比赛, &lt;a href=&#34;https://gitee.com/coggle/tianchi-intel-PaddleOCR&#34;&gt;baseline&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tianchi.aliyun.com/competition/entrance/531860/introduction&#34;&gt;2021全国数字生态创新大赛-智能算法赛&lt;/a&gt;, 语义分割比赛, &lt;a href=&#34;https://tianchi.aliyun.com/notebook-ai/detail?postId=169396&#34;&gt;34.5-Unet&lt;/a&gt;, &lt;a href=&#34;https://github.com/DLLXW/data-science-competition/tree/main/%E5%A4%A9%E6%B1%A0/2021%E5%85%A8%E5%9B%BD%E6%95%B0%E5%AD%97%E7%94%9F%E6%80%81%E5%88%9B%E6%96%B0%E5%A4%A7%E8%B5%9B-%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E9%81%A5%E6%84%9F%E5%BD%B1%E5%83%8F%E5%88%86%E5%89%B2&#34;&gt;38.5-Unet++&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/DC%E7%AB%9E%E8%B5%9B-AI%E5%8A%A9%E7%96%AB%C2%B7%E5%8F%A3%E7%BD%A9%E4%BD%A9%E6%88%B4%E6%A3%80%E6%B5%8B%E5%A4%A7%E8%B5%9B&#34;&gt;DC竞赛-AI助疫·口罩佩戴检测大赛&lt;/a&gt;, CV类型比赛&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/Kesci-%E4%B8%AD%E5%9B%BD%E5%8D%8E%E5%BD%95%E6%9D%AF%E4%BA%BA%E7%BE%A4%E5%AF%86%E5%BA%A6%E6%A3%80%E6%B5%8B&#34;&gt;Kesci-中国华录杯人群密度检测&lt;/a&gt;, CV类型比赛&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/Tianchi-%E5%BF%83%E7%94%B5%E4%BA%BA%E6%9C%BA%E6%99%BA%E8%83%BD%E5%A4%A7%E8%B5%9B%E5%BF%83%E7%94%B5%E5%BC%82%E5%B8%B8%E4%BA%8B%E4%BB%B6%E9%A2%84%E6%B5%8B&#34;&gt;天池-心电人机智能大赛心电异常事件预测&lt;/a&gt;, CV类型比赛&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/DataFountain-%E5%A4%9A%E4%BA%BA%E7%A7%8D%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB&#34;&gt;DataFountain-多人种人脸识别&lt;/a&gt;, CV类型比赛&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/DataFountain-%E5%9F%BA%E4%BA%8EOCR%E7%9A%84%E8%BA%AB%E4%BB%BD%E8%AF%81%E8%A6%81%E7%B4%A0%E6%8F%90%E5%8F%96&#34;&gt;DataFountain-基于OCR的身份证要素提取&lt;/a&gt;, CV类型比赛&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/DataFountain-%E8%A7%86%E9%A2%91%E7%89%88%E6%9D%83%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95&#34;&gt;DataFountain-视频版权检测算法&lt;/a&gt;，CV类型比赛&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/kaggle-quickdraw-doodle-recognition&#34;&gt;kaggle-quickdraw-doodle-recognition&lt;/a&gt;，CV类型比赛&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/TinyMind%E4%BA%BA%E6%B0%91%E5%B8%81%E9%9D%A2%E5%80%BC%26%E5%86%A0%E5%AD%97%E5%8F%B7%E7%BC%96%E7%A0%81%E8%AF%86%E5%88%AB%E6%8C%91%E6%88%98%E8%B5%9B&#34;&gt;TinyMind人民币面值&amp;amp;冠字号编码识别挑战赛&lt;/a&gt;，CV类型比赛&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/yanxishe-%E8%83%B8%E8%85%94X%E5%85%89%E8%82%BA%E7%82%8E%E6%A3%80%E6%B5%8B&#34;&gt;AI研习社-胸腔X光肺炎检测&lt;/a&gt;，CV类型比赛&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/yanxishe-%E8%82%BA%E7%82%8EX%E5%85%89%E7%97%85%E7%81%B6%E8%AF%86%E5%88%AB&#34;&gt;AI研习社-肺炎X光病灶识别&lt;/a&gt;，CV类型比赛&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/yanxishe-%E4%BA%BA%E8%84%B8%E5%B9%B4%E9%BE%84%E8%AF%86%E5%88%AB&#34;&gt;AI研习社-人脸年龄识别&lt;/a&gt;，CV类型比赛&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/yanxishe-%E7%BE%8E%E9%A3%9F%E8%AF%86%E5%88%AB%E6%8C%91%E6%88%98%EF%BC%881%EF%BC%89%EF%BC%9A%E8%B1%86%E8%85%90VS%E5%9C%9F%E8%B1%86&#34;&gt;AI研习社-美食识别挑战（1）：豆腐VS土豆&lt;/a&gt;，CV类型比赛&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/yanxishe-%E5%96%B5%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B&#34;&gt;AI研习社-猫脸关键点检测&lt;/a&gt;，CV类型比赛&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;NLP类型比赛&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/yanxishe-IMDB%E8%AF%84%E8%AE%BA%E5%89%A7%E9%80%8F%E6%A3%80%E6%B5%8B&#34;&gt;AI研习社-IMDB评论剧透检测&lt;/a&gt;，NLP类型比赛&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/DataFountain-%E9%87%91%E8%9E%8D%E4%BF%A1%E6%81%AF%E8%B4%9F%E9%9D%A2%E5%8F%8A%E4%B8%BB%E4%BD%93%E5%88%A4%E5%AE%9A&#34;&gt;DataFountain-金融信息负面及主体判定&lt;/a&gt;, NLP类型比赛&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/DataFountain-%E4%BA%92%E8%81%94%E7%BD%91%E9%87%91%E8%9E%8D%E6%96%B0%E5%AE%9E%E4%BD%93%E5%8F%91%E7%8E%B0&#34;&gt;DataFountain-互联网金融新实体发现&lt;/a&gt;，NLP类型比赛&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/DataFountain-%E6%8A%80%E6%9C%AF%E9%9C%80%E6%B1%82%E4%B8%8E%E6%8A%80%E6%9C%AF%E6%88%90%E6%9E%9C%E9%A1%B9%E7%9B%AE%E4%B9%8B%E9%97%B4%E5%85%B3%E8%81%94%E5%BA%A6%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%9E%8B&#34;&gt;DataFountain-技术需求与技术成果项目之间关联度计算模型&lt;/a&gt;，NLP类型比赛&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/DataFountain-%E4%BA%92%E8%81%94%E7%BD%91%E6%96%B0%E9%97%BB%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90&#34;&gt;DataFountain-互联网新闻情感分析&lt;/a&gt;，NLP类型比赛&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/biendata-%E6%99%BA%E6%BA%90%26%E8%AE%A1%E7%AE%97%E6%89%80-%E4%BA%92%E8%81%94%E7%BD%91%E8%99%9A%E5%81%87%E6%96%B0%E9%97%BB%E6%A3%80%E6%B5%8B%E6%8C%91%E6%88%98%E8%B5%9B&#34;&gt;biendata-智源&amp;amp;计算所-互联网虚假新闻检测挑战赛&lt;/a&gt;，NLP类型比赛&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/Tianchi-%E7%AC%AC%E4%B8%89%E5%B1%8A%E9%98%BF%E9%87%8C%E4%BA%91%E5%AE%89%E5%85%A8%E7%AE%97%E6%B3%95%E6%8C%91%E6%88%98%E8%B5%9B&#34;&gt;Tianchi-第三届阿里云安全算法挑战赛&lt;/a&gt;，NLP类型比赛&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;其他类型&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/DataFountain-%E4%BC%81%E4%B8%9A%E7%BD%91%E7%BB%9C%E8%B5%84%E4%BA%A7%E5%8F%8A%E5%AE%89%E5%85%A8%E4%BA%8B%E4%BB%B6%E5%88%86%E6%9E%90%E4%B8%8E%E5%8F%AF%E8%A7%86%E5%8C%96&#34;&gt;DataFountain-企业网络资产及安全事件分析与可视化&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/DataFountain-%E4%B8%89%E8%A7%92%E5%BD%A2%E5%9B%BE%E8%AE%A1%E7%AE%97%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E5%8F%8A%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96&#34;&gt;DataFountain-三角形图计算算法设计及性能优化&lt;/a&gt;, 计算优化&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/tree/master/competition/DataFountain-%E4%BA%91%E8%AE%A1%E7%AE%97%E6%97%B6%E4%BB%A3%E7%9A%84%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%9F%A5%E8%AF%A2%E5%88%86%E6%9E%90%E4%BC%98%E5%8C%96&#34;&gt;DataFountain-云计算时代的大数据查询分析优化&lt;/a&gt;, 查询优化&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;其他链接：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Smilexuhc/Data-Competition-TopSolution&#34;&gt;Smile整理的竞赛优胜者代码分享&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/chizhu/BDC2019&#34;&gt;chizhu开源的高校赛2019 文本点击预测&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;贡献者(按照贡献ID排序)&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.zhihu.com/people/finlayliu/&#34;&gt;阿水&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.zhihu.com/people/yuconan/&#34;&gt;DOTA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.zhihu.com/people/kingdoms/activities&#34;&gt;Rain&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.zhihu.com/people/wang-he-13-93/&#34;&gt;鱼遇雨欲语与余&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yphacker&#34;&gt;yphacker&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;协作规范&lt;/h2&gt; &#xA;&lt;p&gt;欢迎大家fork并贡献代码，但请大家遵守以下规范和建议：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;代码请按照比赛的形式进行整理，写明比赛的网址、数据类型和解题赛题；&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;代码请注明运行的环境，以及机器最低配置，如：&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;操作系统：Linux，内存16G，硬盘无要求；&lt;/li&gt; &#xA;   &lt;li&gt;Python环境：Python2/3&lt;/li&gt; &#xA;   &lt;li&gt;Pytorch版本：0.4.0&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;baseline代码只能提供可运行的代码和思路，&lt;strong&gt;请不要提供直接可以提交的结果文件；&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;代码提供者应对代码版权和共享权负责；&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;如果发现Repo存在版权等相关问题，请邮件联系&lt;a href=&#34;mailto:finlayliu@qq.com&#34;&gt;finlayliu@qq.com&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;关注我们&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;img src=&#34;https://cdn.coggle.club/dw_qrcode.jpeg&#34; width=&#34;250&#34; height=&#34;270&#34; alt=&#34;Datawhale是一个专注AI领域的开源组织，以“for the learner，和学习者一起成长”为愿景，构建对学习者最有价值的开源学习社区。关注我们，一起学习成长。&#34;&gt;&#xA;&lt;/div&gt; &#xA;&lt;h2&gt;LICENSE&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/datawhalechina/competition-baseline/raw/master/LICENSE&#34;&gt;GNU General Public License v3.0&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>