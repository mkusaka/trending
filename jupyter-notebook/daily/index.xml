<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-08-10T05:58:46Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>imanoop7/Ollama-OCR</title>
    <updated>2025-08-10T05:58:46Z</updated>
    <id>tag:github.com,2025-08-10:/imanoop7/Ollama-OCR</id>
    <link href="https://github.com/imanoop7/Ollama-OCR" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/imanoop7/Ollama-OCR&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/imanoop7/Ollama-OCR.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Stargazers&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/imanoop7/Ollama-OCR/graphs/commit-activity&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/commit-activity/m/imanoop7/Ollama-OCR.svg?sanitize=true&#34; alt=&#34;Commit Activity&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/imanoop7/Ollama-OCR&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/last-commit/imanoop7/Ollama-OCR.svg?sanitize=true&#34; alt=&#34;Last Commit&#34; /&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/imanoop7/Ollama-OCR/main/logo_file.jpg&#34; alt=&#34;Ollama OCR Logo&#34; /&gt;&lt;/p&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt;Ollama OCR&lt;/h1&gt; &#xA;&lt;p&gt;A powerful OCR (Optical Character Recognition) package that uses state-of-the-art vision language models through Ollama to extract text from images and PDF. Available both as a Python package and a Streamlit web application.&lt;/p&gt; &#xA;&lt;h2&gt;üåü Features&lt;/h2&gt; &#xA;&lt;h3&gt;Supports PDF and Images (New! üÜï)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multiple Vision Models Support&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://ollama.com/library/llava&#34;&gt;LLaVA&lt;/a&gt;: Efficient vision-language model for real-time processing (LLaVa model can generate wrong output sometimes)&lt;/li&gt; &#xA;   &lt;li&gt;Llama 3.2 Vision: Advanced model with high accuracy for complex documents&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://ollama.com/library/granite3.2-vision&#34;&gt;Granite3.2-vision&lt;/a&gt;: A compact and efficient vision-language model, specifically designed for visual document understanding, enabling automated content extraction from tables, charts, infographics, plots, diagrams, and more.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://ollama.com/library/moondream&#34;&gt;Moondream&lt;/a&gt;: Small vision language model designed to run efficiently on edge devices.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://ollama.com/library/minicpm-v&#34;&gt;Minicpm-v&lt;/a&gt;: MiniCPM-V 2.6 can process images with any aspect ratio and up to 1.8 million pixels (e.g., 1344x1344).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multiple Output Formats&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Markdown: Preserves text formatting with headers and lists&lt;/li&gt; &#xA;   &lt;li&gt;Plain Text: Clean, simple text extraction&lt;/li&gt; &#xA;   &lt;li&gt;JSON: Structured data format&lt;/li&gt; &#xA;   &lt;li&gt;Structured: Tables and organized data&lt;/li&gt; &#xA;   &lt;li&gt;Key-Value Pairs: Extracts labeled information&lt;/li&gt; &#xA;   &lt;li&gt;Table: Extract all tabular data.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Batch Processing&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Process multiple images in parallel&lt;/li&gt; &#xA;   &lt;li&gt;Progress tracking for each image&lt;/li&gt; &#xA;   &lt;li&gt;Image preprocessing (resize, normalize, etc.)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Custom Prompts&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Override default prompts with custom instructions for text extraction.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üì¶ Package Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install ollama-ocr&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install Ollama&lt;/li&gt; &#xA; &lt;li&gt;Pull the required model:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ollama pull llama3.2-vision:11b&#xA;ollama pull granite3.2-vision&#xA;ollama pull moondream&#xA;ollama pull minicpm-v&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Using the Package&lt;/h2&gt; &#xA;&lt;h3&gt;Single File Processing&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from ollama_ocr import OCRProcessor&#xA;&#xA;# Initialize OCR processor&#xA;ocr = OCRProcessor(model_name=&#39;llama3.2-vision:11b&#39;, base_url=&#34;http://host.docker.internal:11434/api/generate&#34;)  # You can use any vision model available on Ollama&#xA;# you can pass your custom ollama api&#xA;&#xA;# Process an image&#xA;result = ocr.process_image(&#xA;    image_path=&#34;path/to/your/image.png&#34;, # path to your pdf files &#34;path/to/your/file.pdf&#34;&#xA;    format_type=&#34;markdown&#34;,  # Options: markdown, text, json, structured, key_value&#xA;    custom_prompt=&#34;Extract all text, focusing on dates and names.&#34;, # Optional custom prompt&#xA;    language=&#34;English&#34; # Specify the language of the text (New! üÜï)&#xA;)&#xA;print(result)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Batch File&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from ollama_ocr import OCRProcessor&#xA;&#xA;# Initialize OCR processor&#xA;ocr = OCRProcessor(model_name=&#39;llama3.2-vision:11b&#39;, max_workers=4)  # max workers for parallel processing&#xA;&#xA;# Process multiple images&#xA;# Process multiple images with progress tracking&#xA;batch_results = ocr.process_batch(&#xA;    input_path=&#34;path/to/images/folder&#34;,  # Directory or list of image paths&#xA;    format_type=&#34;markdown&#34;,&#xA;    recursive=True,  # Search subdirectories&#xA;    preprocess=True,  # Enable image preprocessing&#xA;    custom_prompt=&#34;Extract all text, focusing on dates and names.&#34;, # Optional custom prompt&#xA;    language=&#34;English&#34; # Specify the language of the text (New! üÜï)&#xA;)&#xA;# Access results&#xA;for file_path, text in batch_results[&#39;results&#39;].items():&#xA;    print(f&#34;\nFile: {file_path}&#34;)&#xA;    print(f&#34;Extracted Text: {text}&#34;)&#xA;&#xA;# View statistics&#xA;print(&#34;\nProcessing Statistics:&#34;)&#xA;print(f&#34;Total images: {batch_results[&#39;statistics&#39;][&#39;total&#39;]}&#34;)&#xA;print(f&#34;Successfully processed: {batch_results[&#39;statistics&#39;][&#39;successful&#39;]}&#34;)&#xA;print(f&#34;Failed: {batch_results[&#39;statistics&#39;][&#39;failed&#39;]}&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üìã Output Format Details&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Markdown Format&lt;/strong&gt;: The output is a markdown string containing the extracted text from the image.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Text Format&lt;/strong&gt;: The output is a plain text string containing the extracted text from the image.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;JSON Format&lt;/strong&gt;: The output is a JSON object containing the extracted text from the image.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Structured Format&lt;/strong&gt;: The output is a structured object containing the extracted text from the image.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Key-Value Format&lt;/strong&gt;: The output is a dictionary containing the extracted text from the image.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Table Format&lt;/strong&gt;: Extract all tabular data.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr /&gt; &#xA;&lt;h2&gt;üåê Streamlit Web Application(supports batch processing)&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;User-Friendly Interface&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Drag-and-drop file upload&lt;/li&gt; &#xA;   &lt;li&gt;Real-time processing&lt;/li&gt; &#xA;   &lt;li&gt;Download extracted text&lt;/li&gt; &#xA;   &lt;li&gt;Image preview with details&lt;/li&gt; &#xA;   &lt;li&gt;Responsive design&lt;/li&gt; &#xA;   &lt;li&gt;Language Selection: Specify the language for better OCR accuracy. (New! üÜï)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the repository:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/imanoop7/Ollama-OCR.git&#xA;cd Ollama-OCR&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Install dependencies:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Go to the directory where app.py is located:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd src/ollama_ocr      &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Run the Streamlit app:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;streamlit run app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üìí Example Notebooks&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/imanoop7/Ollama-OCR/main/example_notebooks%5Collama_ocr_on_colab.ipynb&#34;&gt;Ollama OCR on Colab&lt;/a&gt;: How to use Ollama-OCR on Google Colab.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/imanoop7/Ollama-OCR/main/example_notebooks%5Cexample.ipynb&#34;&gt;Example Notebook&lt;/a&gt;: Example usage of Ollama OCR.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/imanoop7/Ollama-OCR/main/example_notebooks%5Collama-ocr-with-autogen.ipynb&#34;&gt;Ollama OCR with Autogen&lt;/a&gt;: Use Ollama-OCR with autogen.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/imanoop7/Ollama-OCR/main/example_notebooks%5Collama-ocr-with-langgraph.ipynb&#34;&gt;Ollama OCR with LangGraph&lt;/a&gt;: Use Ollama-OCR with LangGraph.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Examples Output&lt;/h2&gt; &#xA;&lt;h3&gt;Input Image&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/imanoop7/Ollama-OCR/main/input/img.png&#34; alt=&#34;Input Image&#34; /&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Sample Output&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/imanoop7/Ollama-OCR/main/output/image.png&#34; alt=&#34;Sample Output&#34; /&gt; &lt;img src=&#34;https://raw.githubusercontent.com/imanoop7/Ollama-OCR/main/output/markdown.png&#34; alt=&#34;Sample Output&#34; /&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üìÑ License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the MIT License - see the LICENSE file for details.&lt;/p&gt; &#xA;&lt;h2&gt;üôè Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;Built with Ollama Powered by Vision Models&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;a href=&#34;https://www.star-history.com/#imanoop7/Ollama-OCR&amp;amp;Date&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://api.star-history.com/svg?repos=imanoop7/Ollama-OCR&amp;amp;type=Date&amp;amp;theme=dark&#34; /&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;https://api.star-history.com/svg?repos=imanoop7/Ollama-OCR&amp;amp;type=Date&#34; /&gt; &#xA;  &lt;img alt=&#34;Star History Chart&#34; src=&#34;https://api.star-history.com/svg?repos=imanoop7/Ollama-OCR&amp;amp;type=Date&#34; /&gt; &#xA; &lt;/picture&gt; &lt;/a&gt;</summary>
  </entry>
</feed>