<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-04T01:34:12Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>svpino/llm</title>
    <updated>2024-04-04T01:34:12Z</updated>
    <id>tag:github.com,2024-04-04:/svpino/llm</id>
    <link href="https://github.com/svpino/llm" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A bunch of experiments using Large Language Models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;A bunch of experiments with LLMs&lt;/h1&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create a virtual environment and install the required packages:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ python3 -m venv .venv&#xA;$ source .venv/bin/activate&#xA;$ pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;Create a free Pinecone account and get your API key from &lt;a href=&#34;https://www.pinecone.io/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create a &lt;code&gt;.env&lt;/code&gt; file with the following variables:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;OPENAI_API_KEY = [ENTER YOUR OPENAI API KEY HERE]&#xA;PINECONE_API_KEY = [ENTER YOUR PINECONE API KEY HERE]&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>mahmoodlab/UNI</title>
    <updated>2024-04-04T01:34:12Z</updated>
    <id>tag:github.com,2024-04-04:/mahmoodlab/UNI</id>
    <link href="https://github.com/mahmoodlab/UNI" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Towards a general-purpose foundation model for computational pathology - Nature Medicine&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;UNI&lt;/h1&gt; &#xA;&lt;h2&gt;Towards a General-Purpose Foundation Model for Computational Pathology&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;Nature Medicine&lt;/em&gt; &lt;img src=&#34;https://raw.githubusercontent.com/mahmoodlab/UNI/main/.github/uni.jpg&#34; width=&#34;300px&#34; align=&#34;right&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.nature.com/articles/s41591-024-02857-3&#34;&gt;Journal Link&lt;/a&gt; | &lt;a href=&#34;https://rdcu.be/dBMgh&#34;&gt;Open Access Read Link&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/MahmoodLab/uni&#34;&gt;Download Model&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/mahmoodlab/UNI/main/#reference&#34;&gt;Cite&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt; Quantitative evaluation of tissue images is crucial for computational pathology (CPath) tasks, requiring the objective characterization of histopathological entities from whole-slide images (WSIs). The high resolution of WSIs and the variability of morphological features present significant challenges, complicating the large-scale annotation of data for high-performance applications. To address this challenge, current efforts have proposed the use of pretrained image encoders through transfer learning from natural image datasets or self-supervised learning on publicly available histopathology datasets, but have not been extensively developed and evaluated across diverse tissue types at scale. We introduce UNI, a general-purpose self-supervised model for pathology, pretrained using more than 100 million images from over 100,000 diagnostic H&amp;amp;E-stained WSIs (&amp;gt;77 TB of data) across 20 major tissue types. The model was evaluated on 34 representative CPath tasks of varying diagnostic difficulty. In addition to outperforming previous state-of-the-art models, we demonstrate new modeling capabilities in CPath such as resolution-agnostic tissue classification, slide classification using few-shot class prototypes, and disease subtyping generalization in classifying up to 108 cancer types in the OncoTree classification system. UNI advances unsupervised representation learning at scale in CPath in terms of both pretraining data and downstream evaluation, enabling data-efficient artificial intelligence models that can generalize and transfer to a wide range of diagnostically challenging tasks and clinical workflows in anatomic pathology.&lt;/p&gt; &#xA;&lt;h2&gt;What is UNI?&lt;/h2&gt; &#xA;&lt;p&gt;UNI is the largest pretrained vision encoder for histopathology (100M images, 100K WSIs) &lt;em&gt;&lt;strong&gt;developed on internal neoplastic, infectious, inflamatory and normal tissue and also made publicly available&lt;/strong&gt;&lt;/em&gt;. We show state-of-the-art performance across 34 clinical tasks, with strong performance gains on rare and underrepresented cancer types.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;strong&gt;Why use UNI?&lt;/strong&gt;&lt;/em&gt;: UNI does not use open datasets and large public histology slide collections (TCGA, CPTAC, PAIP, CAMELYON, PANDA, and others in TCIA) for pretraining, which are routinely used in benchmark development in computational pathology. We make UNI available for the research community in building and evaluating pathology AI models without risk of data contamination on public benchmarks or private histopathology slide collections.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;First clone the repo and cd into the directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/mahmoodlab/UNI.git&#xA;cd UNI&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then create a conda env and install the dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda create -n UNI python=3.10 -y&#xA;conda activate UNI&#xA;pip install --upgrade pip  # enable PEP 660 support&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;1. Getting access&lt;/h3&gt; &#xA;&lt;p&gt;Request access to the model weights from the Huggingface model page at: &lt;a href=&#34;https://huggingface.co/mahmoodlab/uni&#34;&gt;https://huggingface.co/mahmoodlab/UNI&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;2. Downloading weights + Creating model&lt;/h3&gt; &#xA;&lt;p&gt;Following authentication (using &lt;code&gt;huggingface_hub&lt;/code&gt;), the ViT-L/16 model architecture with pretrained weights and image transforms for UNI can be directly loaded using the &lt;a href=&#34;https://huggingface.co//github/hub/en/timm&#34;&gt;timm&lt;/a&gt; library. This method automatically downloads the model weights to the &lt;a href=&#34;https://huggingface.co//github/huggingface_hub/en/guides/manage-cache&#34;&gt;huggingface_hub cache&lt;/a&gt; in your home directory (&lt;code&gt;~/.cache/huggingface/hub/models--MahmoodLab--UNI&lt;/code&gt;), which &lt;code&gt;timm&lt;/code&gt; will automatically find when using the commands below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import timm&#xA;from timm.data import resolve_data_config&#xA;from timm.data.transforms_factory import create_transform&#xA;from huggingface_hub import login&#xA;&#xA;login()  # login with your User Access Token, found at https://huggingface.co/settings/tokens&#xA;&#xA;# pretrained=True needed to load UNI weights (and download weights for the first time)&#xA;# init_values need to be passed in to successfully load LayerScale parameters (e.g. - block.0.ls1.gamma)&#xA;model = timm.create_model(&#34;hf-hub:MahmoodLab/uni&#34;, pretrained=True, init_values=1e-5, dynamic_img_size=True)&#xA;transform = create_transform(**resolve_data_config(model.pretrained_cfg, model=model))&#xA;model.eval()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also download the model weights to a specified checkpoint location in your local directory. The &lt;code&gt;timm&lt;/code&gt; library is still used for defining the ViT-L/16 model architecture. Pretrained weights and image transforms for UNI need to be manually loaded and defined.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os&#xA;import torch&#xA;from torchvision import transforms&#xA;import timm&#xA;from huggingface_hub import login, hf_hub_download&#xA;&#xA;login()  # login with your User Access Token, found at https://huggingface.co/settings/tokens&#xA;&#xA;local_dir = &#34;../assets/ckpts/vit_large_patch16_224.dinov2.uni_mass100k/&#34;&#xA;os.makedirs(local_dir, exist_ok=True)  # create directory if it does not exist&#xA;hf_hub_download(&#34;MahmoodLab/UNI&#34;, filename=&#34;pytorch_model.bin&#34;, local_dir=local_dir, force_download=True)&#xA;model = timm.create_model(&#xA;    &#34;vit_large_patch16_224&#34;, img_size=224, patch_size=16, init_values=1e-5, num_classes=0, dynamic_img_size=True&#xA;)&#xA;model.load_state_dict(torch.load(os.path.join(local_dir, &#34;pytorch_model.bin&#34;), map_location=&#34;cpu&#34;), strict=True)&#xA;transform = transforms.Compose(&#xA;    [&#xA;        transforms.Resize(224),&#xA;        transforms.ToTensor(),&#xA;        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),&#xA;    ]&#xA;)&#xA;model.eval()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The function &lt;code&gt;get_encoder&lt;/code&gt; performs the commands above, downloading in the checkpoint in the &lt;code&gt;./assets/ckpts/&lt;/code&gt; relative path of this GitHub repository.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from uni import get_encoder&#xA;model, transform = get_encoder(enc_name=&#39;uni&#39;, device=device)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;3. Running Inference&lt;/h3&gt; &#xA;&lt;p&gt;You can use the UNI pretrained encoder to extract features from histopathology ROIs, as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from PIL import Image&#xA;image = Image.open(&#34;uni.jpg&#34;)&#xA;image = transform(image).unsqueeze(dim=0) # Image (torch.Tensor) with shape [1, 3, 224, 224] following image resizing and normalization (ImageNet parameters)&#xA;with torch.inference_mode():&#xA;    feature_emb = model(image) # Extracted features (torch.Tensor) with shape [1,1024]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;These pre-extracted features can then be used ROI classification (via linear probing), slide classification (via multiple instance learning), and other machine learning settings.&lt;/p&gt; &#xA;&lt;h2&gt;Overview of specific usages&lt;/h2&gt; &#xA;&lt;p&gt;We provide high-level functions for loading the model and using it for inference. For model loading, the function &lt;code&gt;get_encoder&lt;/code&gt; performs the commands above in Step 2, downloading in the checkpoint in the &lt;code&gt;./assets/ckpts/&lt;/code&gt; relative path of this GitHub repository.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from uni import get_encoder&#xA;model, transform = get_encoder(enc_name=&#39;uni&#39;, device=device)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For inference:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from uni.downstream.extract_patch_features import extract_patch_features_from_dataloader&#xA;from uni.downstream.eval_patch_features.linear_probe import eval_linear_probe&#xA;from uni.downstream.eval_patch_features.fewshot import eval_knn, eval_fewshot&#xA;from uni.downstream.eval_patch_features.protonet import ProtoNet, prototype_topk_vote&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Refer to the notebooks below for detailed examples.&lt;/p&gt; &#xA;&lt;h3&gt;More detailed starter code for loading / using the model:&lt;/h3&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/mahmoodlab/UNI/main/notebooks/uni_walkthrough.ipynb&#34;&gt;&lt;strong&gt;./notebooks/uni_walkthrough.ipynb&lt;/strong&gt;&lt;/a&gt; to get started with loading and using the model to create embeddings, and example code for extracting ROI features and performing ROI classification / retrieval.&lt;/p&gt; &#xA;&lt;h2&gt;Comparisons &amp;amp; Additional Benchmarks&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img width=&#34;100%&#34; alt=&#34;ROI and slide classification results&#34; src=&#34;https://raw.githubusercontent.com/mahmoodlab/UNI/main/.github/benchmarks.jpg&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;A detailed set of benchmarks are in the paper &lt;a href=&#34;https://www.nature.com/articles/s41591-024-02857-3&#34;&gt;[1]&lt;/a&gt; (also shown above). Some models were released after our study was in review. For a more comprehensive comparison, we have provided additional results on EBRAINS, PANDA, OncoTree, IHC ER / PR assessment, CRC-100K-Raw, and TCGA Uniform Tumor datasets as a representative set of benchmarks which cover a wide range of tissue types, diseases, difficulty levels (up to 108-classes) and staining (H&amp;amp;E and IHC). Results are reported using ABMIL and KNN (K=20) slide and ROI tasks respectively.&lt;/p&gt; &#xA;&lt;p&gt;Please refer to the UNI &lt;a href=&#34;https://www.nature.com/articles/s41591-024-02857-3&#34;&gt;[1]&lt;/a&gt; and CONCH &lt;a href=&#34;https://www.nature.com/articles/s41591-024-02856-4&#34;&gt;[2]&lt;/a&gt; papers for more detailed benchmarking.&lt;/p&gt; &#xA;&lt;h3&gt;Slide Benchmarks&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model name&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Pretraining&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;EBRAINS-C (12 classes, Public)&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;EBRAINS-F (30 classes, Public)&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;PANDA (5 classes, Public)&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;OncoTree-108 (108 classes, Internal)&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;IHC ER / PR Assess. (6 classes, Internal)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;Balanced acc.&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;Balanced acc.&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;Quadratic-weight $\kappa$&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;Balanced acc.&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;Quadratic-weight $\kappa$&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;UNI&lt;/strong&gt; &lt;a href=&#34;https://www.nature.com/articles/s41591-024-02857-3&#34;&gt;[1]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Vision&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;strong&gt;0.883&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;ins&gt;0.675&lt;/ins&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;ins&gt;0.946&lt;/ins&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;strong&gt;0.538&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.785&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;CONCH&lt;/strong&gt; &lt;a href=&#34;https://www.nature.com/articles/s41591-024-02856-4&#34;&gt;[2]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Vision-language&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;ins&gt;0.868&lt;/ins&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;strong&gt;0.689&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.934&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;ins&gt;0.515&lt;/ins&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;strong&gt;0.819&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Phikon &lt;a href=&#34;https://doi.org/10.1101/2023.07.21.23292757&#34;&gt;[3]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Vision&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.810&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.659&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;strong&gt;0.950&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.486&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.744&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;REMEDIS &lt;a href=&#34;https://doi.org/10.1038/s41551-023-01049-7&#34;&gt;[4]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Vision&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.687&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.382&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.932&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.412&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.762&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;CTransPath &lt;a href=&#34;https://doi.org/10.1016/j.media.2022.102559&#34;&gt;[5]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Vision&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.666&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.514&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.927&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.399&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;ins&gt;0.786&lt;ins&gt;&lt;/ins&gt;&lt;/ins&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Quilt-Net &lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2023/file/775ec578876fa6812c062644964b9870-Paper-Datasets_and_Benchmarks.pdf&#34;&gt;[6]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Vision-language&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.728&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.608&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.909&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.389&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.784&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;PLIP &lt;a href=&#34;https://doi.org/10.1038/s41591-023-02504-3&#34;&gt;[7]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Vision-language&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.683&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.562&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.901&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.369&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.759&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ResNet-50 (Tr) &lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf&#34;&gt;[8]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ImageNet Transfer&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.302&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.219&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.831&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.148&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.709&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;ROI Benchmarks&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model name&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Pretraining&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;CRC-100K-Raw (9 classes, Public)&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;TCGA Uniform Tumor (32 classes, Public)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;Balanced acc.&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;Balanced acc.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;UNI&lt;/strong&gt; &lt;a href=&#34;https://www.nature.com/articles/s41591-024-02857-3&#34;&gt;[1]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Vision&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;ins&gt;0.925&lt;/ins&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;strong&gt;0.595&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;CONCH&lt;/strong&gt; &lt;a href=&#34;https://www.nature.com/articles/s41591-024-02856-4&#34;&gt;[2]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Vision-language&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;strong&gt;0.941&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;ins&gt;0.556&lt;/ins&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Phikon &lt;a href=&#34;https://doi.org/10.1101/2023.07.21.23292757&#34;&gt;[3]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Vision&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.845&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.533&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;REMEDIS &lt;a href=&#34;https://doi.org/10.1038/s41551-023-01049-7&#34;&gt;[4]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Vision&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.908&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.541&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;CTransPath &lt;a href=&#34;https://doi.org/10.1016/j.media.2022.102559&#34;&gt;[5]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Vision&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.836&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.463&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Quilt-Net &lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2023/file/775ec578876fa6812c062644964b9870-Paper-Datasets_and_Benchmarks.pdf&#34;&gt;[6]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Vision-language&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.878&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.359&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;PLIP &lt;a href=&#34;https://doi.org/10.1038/s41591-023-02504-3&#34;&gt;[7]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Vision-language&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.840&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.370&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ResNet-50 &lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf&#34;&gt;[8]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ImageNet Transfer&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.797&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.318&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;License and Terms of Tuse&lt;/h2&gt; &#xA;&lt;p&gt;ⓒ Mahmood Lab. This model and associated code are released under the &lt;a href=&#34;https://raw.githubusercontent.com/mahmoodlab/UNI/main/(https://creativecommons.org/licenses/by-nc-nd/4.0/deed.en)&#34;&gt;CC-BY-NC-ND 4.0&lt;/a&gt; license and may only be used for non-commercial, academic research purposes with proper attribution. Any commercial use, sale, or other monetization of the UNI model and its derivatives, which include models trained on outputs from the UNI model or datasets created from the UNI model, is prohibited and requires prior approval. Downloading the model requires prior registration on Hugging Face and agreeing to the terms of use. By downloading this model, you agree not to distribute, publish or reproduce a copy of the model. If another user within your organization wishes to use the UNI model, they must register as an individual user and agree to comply with the terms of use. Users may not attempt to re-identify the deidentified data used to develop the underlying model. If you are a commercial entity, please contact the corresponding author or Mass General Brigham Innovation Office.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;The project was built on top of amazing repositories such as &lt;a href=&#34;https://github.com/google-research/big_vision&#34;&gt;ViT&lt;/a&gt;, &lt;a href=&#34;https://github.com/facebookresearch/dinov2&#34;&gt;DINOv2&lt;/a&gt;, &lt;a href=&#34;https://github.com/mbanani/lgssl&#34;&gt;LGSSL&lt;/a&gt;, and &lt;a href=&#34;https://github.com/huggingface/pytorch-image-models/&#34;&gt;Timm&lt;/a&gt; (ViT model implementation). We thank the authors and developers for their contribution.&lt;/p&gt; &#xA;&lt;h2&gt;Reference&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work useful in your research or if you use parts of this code please consider citing our &lt;a href=&#34;https://www.nature.com/articles/s41591-024-02857-3&#34;&gt;paper&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;p&gt;Chen, R.J., Ding, T., Lu, M.Y., Williamson, D.F.K., et al. Towards a general-purpose foundation model for computational pathology. Nat Med (2024). &lt;a href=&#34;https://doi.org/10.1038/s41591-024-02857-3&#34;&gt;https://doi.org/10.1038/s41591-024-02857-3&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{chen2024uni,&#xA;  title={Towards a General-Purpose Foundation Model for Computational Pathology},&#xA;  author={Chen, Richard J and Ding, Tong and Lu, Ming Y and Williamson, Drew FK and Jaume, Guillaume and Chen, Bowen and Zhang, Andrew and Shao, Daniel and Song, Andrew H and Shaban, Muhammad and others},&#xA;  journal={Nature Medicine},&#xA;  publisher={Nature Publishing Group},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/mahmoodlab/UNI/main/.github/joint_logo.jpg&#34;&gt;</summary>
  </entry>
</feed>