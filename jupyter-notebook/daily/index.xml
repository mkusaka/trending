<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-03T01:41:05Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>srush/LLM-Training-Puzzles</title>
    <updated>2023-07-03T01:41:05Z</updated>
    <id>tag:github.com,2023-07-03:/srush/LLM-Training-Puzzles</id>
    <link href="https://github.com/srush/LLM-Training-Puzzles" rel="alternate"></link>
    <summary type="html">&lt;p&gt;What would you do with 1000 H100s...&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LLM Training Puzzles&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;by &lt;a href=&#34;http://rush-nlp.com&#34;&gt;Sasha Rush&lt;/a&gt; - &lt;a href=&#34;https://twitter.com/srush_nlp&#34;&gt;srush_nlp&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/srush/LLM-Training-Puzzles/assets/35882/0c46911f-ad1c-4e7a-a42b-2bc2537cccc3&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is a collection of 8 challenging puzzles about training large language models (or really any NN) on many, many GPUs. Very few people actually get a chance to train on thousands of of computers, but it is an interesting challenge and one that is critically important for modern AI. The goal of these puzzles is to get hands-on experience with the key primitives and to understand the goals of memory efficiency and compute pipelining.&lt;/p&gt; &#xA;&lt;p&gt;I recommend running in Colab. Click here and copy the notebook to get start.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/srush/LLM-Training-Puzzles/blob/main/puzzles.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/srush/LLM-Training-Puzzles/assets/35882/6d16fc9e-3d14-4bd0-b7c7-d056e49854ac&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you are into this kind of thing, this is 6th in a series of these puzzles.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/srush/gpu-puzzles&#34;&gt;https://github.com/srush/gpu-puzzles&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/srush/tensor-puzzles&#34;&gt;https://github.com/srush/tensor-puzzles&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/srush/autodiff-puzzles&#34;&gt;https://github.com/srush/autodiff-puzzles&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/srush/transformer-puzzles&#34;&gt;https://github.com/srush/transformer-puzzles&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/srush/GPTworld&#34;&gt;https://github.com/srush/GPTworld&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>ContextualAI/lens</title>
    <updated>2023-07-03T01:41:05Z</updated>
    <id>tag:github.com,2023-07-03:/ContextualAI/lens</id>
    <link href="https://github.com/ContextualAI/lens" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This is the official repository for the LENS (Large Language Models Enhanced to See) system.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LENSüîç&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://contextual.ai/introducing-lens&#34;&gt;[Blog]&lt;/a&gt; &lt;a href=&#34;https://lens.contextual.ai/&#34;&gt;[Demo]&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2306.16410&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/ContextualAI/lens/blob/main/notebooks/example_usage.ipynb&#34;&gt;[Colab]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is the official repository for the LENS (Large Language Models Enhanced to See) &lt;a href=&#34;https://arxiv.org/abs/2306.16410&#34;&gt;paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;We recommend that you get a machine with GPUs and CUDA. A machine with a single GPU or even a CPU works, although for large datasets you should get several GPUs.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create a python 3.9 conda or virtual environment and then install this repo as a pip package with:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install llm-lens&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;First, the system runs a series of highly descriptive vision modules on your own dataset of images. These modules provide a large set of natural language captions, tags, objects, and attributes for each image. These natural language descriptions are then provided to a large language model (LLM) so that it can solve a variety of tasks relating to the image. Despite the simplicity of our system, and the fact that it requires no finetuning, we demonstrate in our paper that it often performs better than other SOTA image-language models such as Flamingo, CLIP, and Kosmos.&lt;/p&gt; &#xA;&lt;h3&gt;Visual Descriptions&lt;/h3&gt; &#xA;&lt;p&gt;To simply get visual descriptions from LENS to pass on to an LLM, use the following code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import requests&#xA;from lens import Lens, LensProcessor&#xA;from PIL import Image&#xA;import torch&#xA;img_url = &#39;https://images.unsplash.com/photo-1465056836041-7f43ac27dcb5?w=720&#39;&#xA;raw_image = Image.open(requests.get(img_url, stream=True).raw).convert(&#39;RGB&#39;)&#xA;question = &#34;What is the image about?&#34;&#xA;&#xA;lens = Lens()&#xA;processor = LensProcessor()&#xA;with torch.no_grad():&#xA;    samples = processor([raw_image],[question])&#xA;    output = lens(samples)&#xA;    prompts = output[&#34;prompts&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The generated prompts can be passed to an LLM for solving a vision task. The &lt;code&gt;output&lt;/code&gt; object also contains other useful information (tags, attributes, objects) that can be used to generate your custom prompts.&lt;/p&gt; &#xA;&lt;h3&gt;Advanced Use Cases&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Pass the image descriptions to a language model, and ask it a question&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- #region --&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoTokenizer, AutoModelForSeq2SeqLM&#xA;&#xA;tokenizer = AutoTokenizer.from_pretrained(&#34;google/flan-t5-small&#34;,truncation_side = &#39;left&#39;,padding = True)&#xA;LLM_model = AutoModelForSeq2SeqLM.from_pretrained(&#34;google/flan-t5-small&#34;)&#xA;&#xA;input_ids = tokenizer(samples[&#34;prompts&#34;], return_tensors=&#34;pt&#34;).input_ids&#xA;outputs = LLM_model.generate(input_ids)&#xA;print(tokenizer.decode(outputs[0]))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;!-- #endregion --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can also augment your huggingface dataset&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- #region --&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from datasets import load_dataset&#xA;from lens import Lens, LensProcessor&#xA;&#xA;lens = Lens()&#xA;processor = LensProcessor()&#xA;ds = load_dataset(&#34;llm-lens/lens_sample_test&#34;, split=&#34;test&#34;)   &#xA;output_ds = lens.hf_dataset_transform(ds, processor, return_global_caption = False)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Architecture&lt;/h2&gt; &#xA;&lt;h3&gt;LENS for open ended visual question answering&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/ContextualAI/lens/assets/20826878/e2e9d993-3ae8-43d8-9152-0e73340afa41&#34; alt=&#34;lens_open_ended&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;LENS for image classification&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/ContextualAI/lens/assets/20826878/45f3ae43-e3e4-48fc-b424-1899445d5c6f&#34; alt=&#34;lens_close_ended&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Coming Soon&lt;/h2&gt; &#xA;&lt;p&gt;We are working on bringing the complete LENS experience to you and following scripts will be added soon:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Evaluation on VQA and other datasets present in the paper.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Generating vocabularies used in the paper.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Other scripts needed to reproduce the paper.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{berrios2023language,&#xA;      title={Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language}, &#xA;      author={William Berrios and Gautam Mittal and Tristan Thrush and Douwe Kiela and Amanpreet Singh},&#xA;      year={2023},&#xA;      eprint={2306.16410},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;!-- #endregion --&gt;</summary>
  </entry>
  <entry>
    <title>unlearning-challenge/starting-kit</title>
    <updated>2023-07-03T01:41:05Z</updated>
    <id>tag:github.com,2023-07-03:/unlearning-challenge/starting-kit</id>
    <link href="https://github.com/unlearning-challenge/starting-kit" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Starting kit for the NeurIPS 2023 unlearning challenge&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://github.com/unlearning-challenge/starting-kit/assets/277639/d1fa7889-5d91-4e6d-8082-7d59ef728f9c&#34; style=&#34;width: 100px&#34;&gt; &#xA;&lt;h1&gt;Starting kit for the NeurIPS 2023 Machine Unlearning Challenge&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains the starting kit for the NeurIPS 2023 Machine Unlearning Challenge. The starting kit currently contains the following examples:&lt;/p&gt; &#xA;&lt;h3&gt;Unlearning on CIFAR10&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/unlearning-challenge/starting-kit/assets/277639/acee217a-9ecd-484b-be81-8dcf5992eece&#34; alt=&#34;sample images from the CIFAR10 dataset&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The notebook &lt;a href=&#34;https://nbviewer.org/github/unlearning-challenge/starting-kit/tree/main/unlearning-CIFAR10.ipynb&#34;&gt;&lt;code&gt;unlearning-CIFAR10.ipynb&lt;/code&gt;&lt;/a&gt; provides a foundation for participants to build their unlearning models on the CIFAR-10 dataset. This jupyter notebook can be run locally, &lt;a href=&#34;https://colab.research.google.com/github/unlearning-challenge/starting-kit/blob/main/unlearning-CIFAR10.ipynb&#34;&gt;on Colab&lt;/a&gt;, or &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/unlearning-challenge/starting-kit/main/unlearning-CIFAR10.ipynb&#34;&gt;on Kaggle&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>