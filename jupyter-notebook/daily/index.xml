<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-08-11T01:34:16Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>nv-tlabs/GEN3C</title>
    <updated>2025-08-11T01:34:16Z</updated>
    <id>tag:github.com,2025-08-11:/nv-tlabs/GEN3C</id>
    <link href="https://github.com/nv-tlabs/GEN3C" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[CVPR 2025 Highlight] GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control&lt;/h1&gt; &#xA;&lt;!-- Note: this video is hosted by GitHub and gets embedded automatically when viewing in the GitHub UI --&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/247e1719-9f8f-4504-bfa3-f9706bd8682d&#34;&gt;https://github.com/user-attachments/assets/247e1719-9f8f-4504-bfa3-f9706bd8682d&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control&lt;/strong&gt;&lt;br /&gt; &lt;a href=&#34;https://xuanchiren.com/&#34;&gt;Xuanchi Ren*&lt;/a&gt;, &lt;a href=&#34;https://www.cs.toronto.edu/~shenti11/&#34;&gt;Tianchang Shen*&lt;/a&gt;, &lt;a href=&#34;https://huangjh-pub.github.io/&#34;&gt;Jiahui Huang&lt;/a&gt;, &lt;a href=&#34;https://www.cs.toronto.edu/~linghuan/&#34;&gt;Huan Ling&lt;/a&gt;, &lt;a href=&#34;https://yifanlu0227.github.io/&#34;&gt;Yifan Lu&lt;/a&gt;, &lt;a href=&#34;https://merlin.nimierdavid.fr/&#34;&gt;Merlin Nimier-David&lt;/a&gt;, &lt;a href=&#34;https://research.nvidia.com/person/thomas-muller&#34;&gt;Thomas Müller&lt;/a&gt;, &lt;a href=&#34;https://research.nvidia.com/person/alex-keller&#34;&gt;Alexander Keller&lt;/a&gt;, &lt;a href=&#34;https://www.cs.toronto.edu/~fidler/&#34;&gt;Sanja Fidler&lt;/a&gt;, &lt;a href=&#34;https://www.cs.toronto.edu/~jungao/&#34;&gt;Jun Gao&lt;/a&gt; &lt;br /&gt; * indicates equal contribution &lt;br /&gt; &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/pdf/2503.03751&#34;&gt;Paper&lt;/a&gt;, &lt;a href=&#34;https://research.nvidia.com/labs/toronto-ai/GEN3C/&#34;&gt;Project Page&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/collections/nvidia/gen3c-683f3f9540a8f9c98cf46a8d&#34;&gt;HuggingFace&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Abstract: We present GEN3C, a generative video model with precise Camera Control and temporal 3D Consistency. Prior video models already generate realistic videos, but they tend to leverage little 3D information, leading to inconsistencies, such as objects popping in and out of existence. Camera control, if implemented at all, is imprecise, because camera parameters are mere inputs to the neural network which must then infer how the video depends on the camera. In contrast, GEN3C is guided by a 3D cache: point clouds obtained by predicting the pixel-wise depth of seed images or previously generated frames. When generating the next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with the new camera trajectory provided by the user. Crucially, this means that GEN3C neither has to remember what it previously generated nor does it have to infer the image structure from the camera pose. The model, instead, can focus all its generative power on previously unobserved regions, as well as advancing the scene state to the next frame. Our results demonstrate more precise camera control than prior work, as well as state-of-the-art results in sparse-view novel view synthesis, even in challenging settings such as driving scenes and monocular dynamic video. Results are best viewed in videos.&lt;/p&gt; &#xA;&lt;p&gt;For business inquiries, please visit our website and submit the form: &lt;a href=&#34;https://www.nvidia.com/en-us/research/inquiries/&#34;&gt;NVIDIA Research Licensing&lt;/a&gt;. For any other questions related to the model, please contact Xuanchi, Tianchang or Jun.&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2025-06-06 Code and model released! In a future update, we plan to include the pipeline for jointly predicting depth and camera pose from video, as well as a driving-finetuned model. Stay tuned!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Please follow the &#34;Inference&#34; section in &lt;a href=&#34;https://raw.githubusercontent.com/nv-tlabs/GEN3C/main/INSTALL.md&#34;&gt;INSTALL.md&lt;/a&gt; to set up your environment.&lt;/p&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;h3&gt;Download checkpoints&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Generate a &lt;a href=&#34;https://huggingface.co/settings/tokens&#34;&gt;Hugging Face&lt;/a&gt; access token (if you haven&#39;t done so already). Set the access token to &lt;code&gt;Read&lt;/code&gt; permission (default is &lt;code&gt;Fine-grained&lt;/code&gt;).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Log in to Hugging Face with the access token:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;huggingface-cli login&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Download the GEN3C model weights from &lt;a href=&#34;https://huggingface.co/nvidia/GEN3C-Cosmos-7B&#34;&gt;Hugging Face&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_HOME=$CONDA_PREFIX PYTHONPATH=$(pwd) python scripts/download_gen3c_checkpoints.py --checkpoint_dir checkpoints&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Interactive GUI usage&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/nv-tlabs/GEN3C/main/gui/assets/gui_preview.webp&#34; alt=&#34;GEN3C interactive GUI&#34; width=&#34;1080px&#34; /&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;GEN3C can be used through an interactive GUI, allowing to visualize the inputs in 3D, author arbitrary camera trajectories, and start inference from a single window. Please see the &lt;a href=&#34;https://raw.githubusercontent.com/nv-tlabs/GEN3C/main/gui/README.md&#34;&gt;dedicated instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Command-line usage&lt;/h3&gt; &#xA;&lt;p&gt;GEN3C supports both images and videos as input. Below are examples of running GEN3C on single images and videos with predefined camera trajectory patterns.&lt;/p&gt; &#xA;&lt;h3&gt;Example 1: Single Image to Video Generation&lt;/h3&gt; &#xA;&lt;h4&gt;Single GPU&lt;/h4&gt; &#xA;&lt;p&gt;Generate a 121-frame video from a single image:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_HOME=$CONDA_PREFIX PYTHONPATH=$(pwd) python cosmos_predict1/diffusion/inference/gen3c_single_image.py \&#xA;    --checkpoint_dir checkpoints \&#xA;    --input_image_path assets/diffusion/000000.png \&#xA;    --video_save_name test_single_image \&#xA;    --guidance 1 \&#xA;    --foreground_masking&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Multi-GPU (8 GPUs)&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;NUM_GPUS=8&#xA;CUDA_HOME=$CONDA_PREFIX PYTHONPATH=$(pwd) torchrun --nproc_per_node=${NUM_GPUS} cosmos_predict1/diffusion/inference/gen3c_single_image.py \&#xA;    --checkpoint_dir checkpoints \&#xA;    --input_image_path assets/diffusion/000000.png \&#xA;    --video_save_name test_single_image_multigpu \&#xA;    --num_gpus ${NUM_GPUS} \&#xA;    --guidance 1 \&#xA;    --foreground_masking&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Additional Options&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To generate longer videos autoregressively, specify the number of frames using &lt;code&gt;--num_video_frames&lt;/code&gt;. The number of frames must follow the pattern: 121 * N - 1 (e.g., 241, 361, etc.)&lt;/li&gt; &#xA; &lt;li&gt;To save buffer images alongside the output video, add the &lt;code&gt;--save_buffer&lt;/code&gt; flag&lt;/li&gt; &#xA; &lt;li&gt;You can control camera trajectories using &lt;code&gt;--trajectory&lt;/code&gt;, &lt;code&gt;--camera_rotation&lt;/code&gt;, and &lt;code&gt;--movement_distance&lt;/code&gt; arguments. See the &#34;Camera Movement Options&#34; section below for details.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Camera Movement Options&lt;/h4&gt; &#xA;&lt;h5&gt;Trajectory Types&lt;/h5&gt; &#xA;&lt;p&gt;The &lt;code&gt;--trajectory&lt;/code&gt; argument controls the path the camera takes during video generation. Available options:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Option&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;left&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Camera moves to the left (default)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;right&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Camera moves to the right&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;up&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Camera moves upward&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;down&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Camera moves downward&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;zoom_in&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Camera moves closer to the scene&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;zoom_out&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Camera moves away from the scene&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;clockwise&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Camera moves in a clockwise circular path&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;counterclockwise&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Camera moves in a counterclockwise circular path&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h5&gt;Camera Rotation Modes&lt;/h5&gt; &#xA;&lt;p&gt;The &lt;code&gt;--camera_rotation&lt;/code&gt; argument controls how the camera rotates during movement. Available options:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Option&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;center_facing&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Camera always rotates to look at the (estimated) center of the scene (default)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;no_rotation&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Camera maintains its original orientation while moving&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;trajectory_aligned&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Camera rotates to align with the direction of movement&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h5&gt;Movement Distance&lt;/h5&gt; &#xA;&lt;p&gt;The &lt;code&gt;--movement_distance&lt;/code&gt; argument controls how far the camera moves from its initial position. The default value is 0.3. A larger value will result in more dramatic camera movement, while a smaller value will create more subtle movement.&lt;/p&gt; &#xA;&lt;h5&gt;GPU Memory Requirements&lt;/h5&gt; &#xA;&lt;p&gt;We have tested GEN3C only on H100 and A100 GPUs. For GPUs with limited memory, you can fully offload all models by appending the following flags to your command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;--offload_diffusion_transformer \&#xA;--offload_tokenizer \&#xA;--offload_text_encoder_model \&#xA;--offload_prompt_upsampler \&#xA;--offload_guardrail_models \&#xA;--disable_guardrail \&#xA;--disable_prompt_encoder&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Maximum observed memory during inference with full offloading: ~43GB. Note: Memory usage may vary depending on system specifications and is provided for reference only.&lt;/p&gt; &#xA;&lt;h3&gt;Example 2: Video to Video Generation&lt;/h3&gt; &#xA;&lt;p&gt;For video input, GEN3C requires additional depth information, camera intrinsics, and extrinsics. These can be obtained using your choice of SLAM packages. For testing purposes, we provide example data.&lt;/p&gt; &#xA;&lt;p&gt;First, you need to download the test samples:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Download test samples from Hugging Face&#xA;huggingface-cli download nvidia/GEN3C-Testing-Example --repo-type dataset --local-dir assets/diffusion/dynamic_video_samples&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Single GPU&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_HOME=$CONDA_PREFIX PYTHONPATH=$(pwd) python cosmos_predict1/diffusion/inference/gen3c_dynamic.py \&#xA;    --checkpoint_dir checkpoints \&#xA;    --input_image_path assets/diffusion/dynamic_video_samples/batch_0000 \&#xA;    --video_save_name test_dynamic_video \&#xA;    --guidance 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Multi-GPU (8 GPUs)&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;NUM_GPUS=8&#xA;CUDA_HOME=$CONDA_PREFIX PYTHONPATH=$(pwd) torchrun --nproc_per_node=${NUM_GPUS} cosmos_predict1/diffusion/inference/gen3c_dynamic.py \&#xA;    --checkpoint_dir checkpoints \&#xA;    --input_image_path assets/diffusion/dynamic_video_samples/batch_0000 \&#xA;    --video_save_name test_dynamic_video_multigpu \&#xA;    --num_gpus ${NUM_GPUS} \&#xA;    --guidance 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Gallery&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;GEN3C&lt;/strong&gt; can be easily applied to video/scene creation from a single image&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/nv-tlabs/GEN3C/main/assets/demo_3.gif&#34; alt=&#34;&#34; width=&#34;1100&#34; /&gt; &#xA;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;... or sparse-view images (we use 5 images here)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/nv-tlabs/GEN3C/main/assets/demo_2.gif&#34; alt=&#34;&#34; width=&#34;1100&#34; /&gt; &#xA;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;.. and dynamic videos&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/nv-tlabs/GEN3C/main/assets/demo_dynamic.gif&#34; alt=&#34;&#34; width=&#34;1100&#34; /&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;Our model is based on &lt;a href=&#34;https://github.com/NVIDIA/Cosmos&#34;&gt;NVIDIA Cosmos&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/stabilityai/stable-video-diffusion-img2vid&#34;&gt;Stable Video Diffusion&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We are also grateful to several other open-source repositories that we drew inspiration from or built upon during the development of our pipeline:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/MoGe&#34;&gt;MoGe&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/TrajectoryCrafter/TrajectoryCrafter&#34;&gt;TrajectoryCrafter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/wenqsun/DimensionX&#34;&gt;DimensionX&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/DepthAnything/Depth-Anything-V2&#34;&gt;Depth Anything V2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/DepthAnything/Video-Depth-Anything&#34;&gt;Video Depth Anything&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt; @inproceedings{ren2025gen3c,&#xA;    title={GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control},&#xA;    author={Ren, Xuanchi and Shen, Tianchang and Huang, Jiahui and Ling, Huan and&#xA;        Lu, Yifan and Nimier-David, Merlin and Müller, Thomas and Keller, Alexander and&#xA;        Fidler, Sanja and Gao, Jun},&#xA;    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},&#xA;    year={2025}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License and Contact&lt;/h2&gt; &#xA;&lt;p&gt;This project will download and install additional third-party open source software projects. Review the license terms of these open source projects before use.&lt;/p&gt; &#xA;&lt;p&gt;GEN3C source code is released under the &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;Apache 2 License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;GEN3C models are released under the &lt;a href=&#34;https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-open-model-license&#34;&gt;NVIDIA Open Model License&lt;/a&gt;. For a custom license, please visit our website and submit the form: &lt;a href=&#34;https://www.nvidia.com/en-us/research/inquiries/&#34;&gt;NVIDIA Research Licensing&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>