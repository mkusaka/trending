<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-30T01:27:58Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>baofff/U-ViT</title>
    <updated>2024-04-30T01:27:58Z</updated>
    <id>tag:github.com,2024-04-30:/baofff/U-ViT</id>
    <link href="https://github.com/baofff/U-ViT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A PyTorch implementation of the paper &#34;All are Worth Words: A ViT Backbone for Diffusion Models&#34;.&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;U-ViT&lt;br&gt; &lt;sub&gt;&lt;small&gt;Official PyTorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2209.12152&#34;&gt;All are Worth Words: A ViT Backbone for Diffusion Models&lt;/a&gt; (CVPR 2023)&lt;/small&gt;&lt;/sub&gt;&lt;/h2&gt; &#xA;&lt;p&gt;üí°Projects with U-ViT:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/thu-ml/unidiffuser&#34;&gt;UniDiffuser&lt;/a&gt;, a multi-modal large-scale diffusion model based on a 1B U-ViT, is open-sourced&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.10586&#34;&gt;DPT&lt;/a&gt;, &lt;a href=&#34;https://github.com/ML-GSAI/DPT&#34;&gt;code&lt;/a&gt;, &lt;a href=&#34;https://ml-gsai.github.io/DPT-demo&#34;&gt;demo&lt;/a&gt; a conditional diffusion model trained with 1 label/class with SOTA SSL generation and classification results on ImageNet&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/baofff/U-ViT/main/uvit.png&#34; alt=&#34;drawing&#34; width=&#34;400&#34;&gt; &#xA;&lt;p&gt;Vision transformers (ViT) have shown promise in various vision tasks while the U-Net based on a convolutional neural network (CNN) remains dominant in diffusion models. We design a simple and general ViT-based architecture (named U-ViT) for image generation with diffusion models. U-ViT is characterized by treating all inputs including the time, condition and noisy image patches as tokens and employing long skip connections between shallow and deep layers. We evaluate U-ViT in unconditional and class-conditional image generation, as well as text-to-image generation tasks, where U-ViT is comparable if not superior to a CNN-based U-Net of a similar size. In particular, latent diffusion models with U-ViT achieve record-breaking FID scores of 2.29 in class-conditional image generation on ImageNet 256x256, and 5.48 in text-to-image generation on MS-COCO, among methods without accessing large external datasets during the training of generative models.&lt;/p&gt; &#xA;&lt;p&gt;Our results suggest that, for diffusion-based image modeling, the long skip connection is crucial while the down-sampling and up-sampling operators in CNN-based U-Net are not always necessary. We believe that U-ViT can provide insights for future research on backbones in diffusion models and benefit generative modeling on large scale cross-modality datasets.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;This codebase implements the transformer-based backbone üìå&lt;em&gt;U-ViT&lt;/em&gt;üìå for diffusion models, as introduced in the &lt;a href=&#34;https://arxiv.org/abs/2209.12152&#34;&gt;paper&lt;/a&gt;. U-ViT treats all inputs as tokens and employs long skip connections. &lt;em&gt;The long skip connections grealy promote the performance and the convergence speed&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/baofff/U-ViT/main/skip_im.png&#34; alt=&#34;drawing&#34; width=&#34;400&#34;&gt; &#xA;&lt;p&gt;üí°This codebase contains:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;An implementation of &lt;a href=&#34;https://raw.githubusercontent.com/baofff/U-ViT/main/libs/uvit.py&#34;&gt;U-ViT&lt;/a&gt; with optimized attention computation&lt;/li&gt; &#xA; &lt;li&gt;Pretrained U-ViT models on common image generation benchmarks (CIFAR10, CelebA 64x64, ImageNet 64x64, ImageNet 256x256, ImageNet 512x512)&lt;/li&gt; &#xA; &lt;li&gt;Efficient training scripts for &lt;a href=&#34;https://raw.githubusercontent.com/baofff/U-ViT/main/train.py&#34;&gt;pixel-space diffusion models&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/baofff/U-ViT/main/train_ldm_discrete.py&#34;&gt;latent space diffusion models&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/baofff/U-ViT/main/train_t2i_discrete.py&#34;&gt;text-to-image diffusion models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Efficient evaluation scripts for &lt;a href=&#34;https://raw.githubusercontent.com/baofff/U-ViT/main/eval.py&#34;&gt;pixel-space diffusion models&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/baofff/U-ViT/main/eval_ldm_discrete.py&#34;&gt;latent space diffusion models&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/baofff/U-ViT/main/eval_t2i_discrete.py&#34;&gt;text-to-image diffusion models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;A Colab notebook demo for sampling from U-ViT on ImageNet (FID=2.29) &lt;a href=&#34;https://colab.research.google.com/github/baofff/U-ViT/blob/main/UViT_ImageNet_demo.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/baofff/U-ViT/main/sample.png&#34; alt=&#34;drawing&#34; width=&#34;800&#34;&gt; &#xA;&lt;p&gt;üí°This codebase supports useful techniques for efficient training and sampling of diffusion models:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Mixed precision training with the &lt;a href=&#34;https://github.com/huggingface/accelerate&#34;&gt;huggingface accelerate&lt;/a&gt; library (ü•∞automatically turned on)&lt;/li&gt; &#xA; &lt;li&gt;Efficient attention computation with the &lt;a href=&#34;https://github.com/facebookresearch/xformers&#34;&gt;facebook xformers&lt;/a&gt; library (needs additional installation)&lt;/li&gt; &#xA; &lt;li&gt;Gradient checkpointing trick, which reduces ~65% memory (ü•∞automatically turned on)&lt;/li&gt; &#xA; &lt;li&gt;With these techniques, we are able to train our largest U-ViT-H on ImageNet at high resolutions such as 256x256 and 512x512 using a large batch size of 1024 with &lt;em&gt;only 2 A100&lt;/em&gt;‚ùó&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Training speed and memory of U-ViT-H/2 on ImageNet 256x256 using a batch size of 128 with a A100:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;mixed precision training&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;xformers&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;gradient checkpointing&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;training speed&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;memory&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;out of memory&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úî&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.97 steps/second&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;78852 MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úî&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úî&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.14 steps/second&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54324 MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úî&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úî&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úî&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.87 steps/second&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18858 MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Dependency&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu116  # install torch-1.13.1&#xA;pip install accelerate==0.12.0 absl-py ml_collections einops wandb ftfy==6.1.1 transformers==4.23.1&#xA;&#xA;# xformers is optional, but it would greatly speed up the attention computation.&#xA;pip install -U xformers&#xA;pip install -U --pre triton&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This repo is based on &lt;a href=&#34;https://github.com/rwightman/pytorch-image-models&#34;&gt;&lt;code&gt;timm==0.3.2&lt;/code&gt;&lt;/a&gt;, for which a &lt;a href=&#34;https://github.com/rwightman/pytorch-image-models/issues/420#issuecomment-776459842&#34;&gt;fix&lt;/a&gt; is needed to work with PyTorch 1.8.1+. (Perhaps other versions also work, but I haven&#39;t tested it.)&lt;/li&gt; &#xA; &lt;li&gt;We highly suggest install &lt;a href=&#34;https://github.com/facebookresearch/xformers&#34;&gt;xformers&lt;/a&gt;, which would greatly speed up the attention computation for &lt;em&gt;both training and inference&lt;/em&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Pretrained Models&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;FID&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;training iterations&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;batch size&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1yoYyuzR_hQYWU0mkTj659tMTnoCWCMv-/view?usp=share_link&#34;&gt;CIFAR10 (U-ViT-S/2)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.11&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;500K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;128&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/13YpbRtlqF1HDBNLNRlKxLTbKbKeLE06C/view?usp=share_link&#34;&gt;CelebA 64x64 (U-ViT-S/4)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.87&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;500K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;128&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1igVgRY7-A0ZV3XqdNcMGOnIGOxKr9azv/view?usp=share_link&#34;&gt;ImageNet 64x64 (U-ViT-M/4)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5.85&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;300K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1024&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/19rmun-T7RwkNC1feEPWinIo-1JynpW7J/view?usp=share_link&#34;&gt;ImageNet 64x64 (U-ViT-L/4)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4.26&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;300K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1024&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1w7T1hiwKODgkYyMH9Nc9JNUThbxFZgs3/view?usp=share_link&#34;&gt;ImageNet 256x256 (U-ViT-L/2)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.40&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;300K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1024&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/13StUdrjaaSXjfqqF7M47BzPyhMAArQ4u/view?usp=share_link&#34;&gt;ImageNet 256x256 (U-ViT-H/2)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.29&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;500K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1024&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1mkj4aN2utHMBTWQX9l1nYue9vleL7ZSB/view?usp=share_link&#34;&gt;ImageNet 512x512 (U-ViT-L/4)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4.67&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;500K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1024&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1uegr2o7cuKXtf2akWGAN2Vnlrtw5YKQq/view?usp=share_link&#34;&gt;ImageNet 512x512 (U-ViT-H/4)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4.05&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;500K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1024&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/15JsZWRz2byYNU6K093et5e5Xqd4uwA8S/view?usp=share_link&#34;&gt;MS-COCO (U-ViT-S/2)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5.95&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;256&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1gHRy8sn039Wy-iFL21wH8TiheHK8Ky71/view?usp=share_link&#34;&gt;MS-COCO (U-ViT-S/2, Deep)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5.48&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;256&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Preparation Before Training and Evaluation&lt;/h2&gt; &#xA;&lt;h4&gt;Autoencoder&lt;/h4&gt; &#xA;&lt;p&gt;Download &lt;code&gt;stable-diffusion&lt;/code&gt; directory from this &lt;a href=&#34;https://drive.google.com/drive/folders/1yo-XhqbPue3rp5P57j6QbA5QZx6KybvP?usp=sharing&#34;&gt;link&lt;/a&gt; (which contains image autoencoders converted from &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;Stable Diffusion&lt;/a&gt;). Put the downloaded directory as &lt;code&gt;assets/stable-diffusion&lt;/code&gt; in this codebase. The autoencoders are used in latent diffusion models.&lt;/p&gt; &#xA;&lt;h4&gt;Data&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ImageNet 64x64: Put the standard ImageNet dataset (which contains the &lt;code&gt;train&lt;/code&gt; and &lt;code&gt;val&lt;/code&gt; directory) to &lt;code&gt;assets/datasets/ImageNet&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;ImageNet 256x256 and ImageNet 512x512: Extract ImageNet features according to &lt;code&gt;scripts/extract_imagenet_feature.py&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;MS-COCO: Download COCO 2014 &lt;a href=&#34;http://images.cocodataset.org/zips/train2014.zip&#34;&gt;training&lt;/a&gt;, &lt;a href=&#34;http://images.cocodataset.org/zips/val2014.zip&#34;&gt;validation&lt;/a&gt; data and &lt;a href=&#34;http://images.cocodataset.org/annotations/annotations_trainval2014.zip&#34;&gt;annotations&lt;/a&gt;. Then extract their features according to &lt;code&gt;scripts/extract_mscoco_feature.py&lt;/code&gt; &lt;code&gt;scripts/extract_test_prompt_feature.py&lt;/code&gt; &lt;code&gt;scripts/extract_empty_feature.py&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Reference statistics for FID&lt;/h4&gt; &#xA;&lt;p&gt;Download &lt;code&gt;fid_stats&lt;/code&gt; directory from this &lt;a href=&#34;https://drive.google.com/drive/folders/1yo-XhqbPue3rp5P57j6QbA5QZx6KybvP?usp=sharing&#34;&gt;link&lt;/a&gt; (which contains reference statistics for FID). Put the downloaded directory as &lt;code&gt;assets/fid_stats&lt;/code&gt; in this codebase. In addition to evaluation, these reference statistics are used to monitor FID during the training process.&lt;/p&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;We use the &lt;a href=&#34;https://github.com/huggingface/accelerate&#34;&gt;huggingface accelerate&lt;/a&gt; library to help train with distributed data parallel and mixed precision. The following is the training command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# the training setting&#xA;num_processes=2  # the number of gpus you have, e.g., 2&#xA;train_script=train.py  # the train script, one of &amp;lt;train.py|train_ldm.py|train_ldm_discrete.py|train_t2i_discrete.py&amp;gt;&#xA;                       # train.py: training on pixel space&#xA;                       # train_ldm.py: training on latent space with continuous timesteps&#xA;                       # train_ldm_discrete.py: training on latent space with discrete timesteps&#xA;                       # train_t2i_discrete.py: text-to-image training on latent space&#xA;config=configs/cifar10_uvit_small.py  # the training configuration&#xA;                                      # you can change other hyperparameters by modifying the configuration file&#xA;&#xA;# launch training&#xA;accelerate launch --multi_gpu --num_processes $num_processes --mixed_precision fp16 $train_script --config=$config&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We provide all commands to reproduce U-ViT training in the paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# CIFAR10 (U-ViT-S/2)&#xA;accelerate launch --multi_gpu --num_processes 4 --mixed_precision fp16 train.py --config=configs/cifar10_uvit_small.py&#xA;&#xA;# CelebA 64x64 (U-ViT-S/4)&#xA;accelerate launch --multi_gpu --num_processes 4 --mixed_precision fp16 train.py --config=configs/celeba64_uvit_small.py &#xA;&#xA;# ImageNet 64x64 (U-ViT-M/4)&#xA;accelerate launch --multi_gpu --num_processes 8 --mixed_precision fp16 train.py --config=configs/imagenet64_uvit_mid.py&#xA;&#xA;# ImageNet 64x64 (U-ViT-L/4)&#xA;accelerate launch --multi_gpu --num_processes 8 --mixed_precision fp16 train.py --config=configs/imagenet64_uvit_large.py&#xA;&#xA;# ImageNet 256x256 (U-ViT-L/2)&#xA;accelerate launch --multi_gpu --num_processes 8 --mixed_precision fp16 train_ldm.py --config=configs/imagenet256_uvit_large.py&#xA;&#xA;# ImageNet 256x256 (U-ViT-H/2)&#xA;accelerate launch --multi_gpu --num_processes 8 --mixed_precision fp16 train_ldm_discrete.py --config=configs/imagenet256_uvit_huge.py&#xA;&#xA;# ImageNet 512x512 (U-ViT-L/4)&#xA;accelerate launch --multi_gpu --num_processes 8 --mixed_precision fp16 train_ldm.py --config=configs/imagenet512_uvit_large.py&#xA;&#xA;# ImageNet 512x512 (U-ViT-H/4)&#xA;accelerate launch --multi_gpu --num_processes 8 --mixed_precision fp16 train_ldm_discrete.py --config=configs/imagenet512_uvit_huge.py&#xA;&#xA;# MS-COCO (U-ViT-S/2)&#xA;accelerate launch --multi_gpu --num_processes 4 --mixed_precision fp16 train_t2i_discrete.py --config=configs/mscoco_uvit_small.py&#xA;&#xA;# MS-COCO (U-ViT-S/2, Deep)&#xA;accelerate launch --multi_gpu --num_processes 4 --mixed_precision fp16 train_t2i_discrete.py --config=configs/mscoco_uvit_small.py --config.nnet.depth=16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Evaluation (Compute FID)&lt;/h2&gt; &#xA;&lt;p&gt;We use the &lt;a href=&#34;https://github.com/huggingface/accelerate&#34;&gt;huggingface accelerate&lt;/a&gt; library for efficient inference with mixed precision and multiple gpus. The following is the evaluation command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# the evaluation setting&#xA;num_processes=2  # the number of gpus you have, e.g., 2&#xA;eval_script=eval.py  # the evaluation script, one of &amp;lt;eval.py|eval_ldm.py|eval_ldm_discrete.py|eval_t2i_discrete.py&amp;gt;&#xA;                     # eval.py: for models trained with train.py (i.e., pixel space models)&#xA;                     # eval_ldm.py: for models trained with train_ldm.py (i.e., latent space models with continuous timesteps)&#xA;                     # eval_ldm_discrete.py: for models trained with train_ldm_discrete.py (i.e., latent space models with discrete timesteps)&#xA;                     # eval_t2i_discrete.py: for models trained with train_t2i_discrete.py (i.e., text-to-image models on latent space)&#xA;config=configs/cifar10_uvit_small.py  # the training configuration&#xA;&#xA;# launch evaluation&#xA;accelerate launch --multi_gpu --num_processes $num_processes --mixed_precision fp16 eval_script --config=$config&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The generated images are stored in a temperary directory, and will be deleted after evaluation. If you want to keep these images, set &lt;code&gt;--config.sample.path=/save/dir&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We provide all commands to reproduce FID results in the paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# CIFAR10 (U-ViT-S/2)&#xA;accelerate launch --multi_gpu --num_processes 4 --mixed_precision fp16 eval.py --config=configs/cifar10_uvit_small.py --nnet_path=cifar10_uvit_small.pth&#xA;&#xA;# CelebA 64x64 (U-ViT-S/4)&#xA;accelerate launch --multi_gpu --num_processes 4 --mixed_precision fp16 eval.py --config=configs/celeba64_uvit_small.py --nnet_path=celeba64_uvit_small.pth&#xA;&#xA;# ImageNet 64x64 (U-ViT-M/4)&#xA;accelerate launch --multi_gpu --num_processes 8 --mixed_precision fp16 eval.py --config=configs/imagenet64_uvit_mid.py --nnet_path=imagenet64_uvit_mid.pth&#xA;&#xA;# ImageNet 64x64 (U-ViT-L/4)&#xA;accelerate launch --multi_gpu --num_processes 8 --mixed_precision fp16 eval.py --config=configs/imagenet64_uvit_large.py --nnet_path=imagenet64_uvit_large.pth&#xA;&#xA;# ImageNet 256x256 (U-ViT-L/2)&#xA;accelerate launch --multi_gpu --num_processes 8 --mixed_precision fp16 eval_ldm.py --config=configs/imagenet256_uvit_large.py --nnet_path=imagenet256_uvit_large.pth&#xA;&#xA;# ImageNet 256x256 (U-ViT-H/2)&#xA;accelerate launch --multi_gpu --num_processes 8 --mixed_precision fp16 eval_ldm_discrete.py --config=configs/imagenet256_uvit_huge.py --nnet_path=imagenet256_uvit_huge.pth&#xA;&#xA;# ImageNet 512x512 (U-ViT-L/4)&#xA;accelerate launch --multi_gpu --num_processes 8 --mixed_precision fp16 eval_ldm.py --config=configs/imagenet512_uvit_large.py --nnet_path=imagenet512_uvit_large.pth&#xA;&#xA;# ImageNet 512x512 (U-ViT-H/4)&#xA;accelerate launch --multi_gpu --num_processes 8 --mixed_precision fp16 eval_ldm_discrete.py --config=configs/imagenet512_uvit_huge.py --nnet_path=imagenet512_uvit_huge.pth&#xA;&#xA;# MS-COCO (U-ViT-S/2)&#xA;accelerate launch --multi_gpu --num_processes 4 --mixed_precision fp16 eval_t2i_discrete.py --config=configs/mscoco_uvit_small.py --nnet_path=mscoco_uvit_small.pth&#xA;&#xA;# MS-COCO (U-ViT-S/2, Deep)&#xA;accelerate launch --multi_gpu --num_processes 4 --mixed_precision fp16 eval_t2i_discrete.py --config=configs/mscoco_uvit_small.py --config.nnet.depth=16 --nnet_path=mscoco_uvit_small_deep.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;p&gt;If you find the code useful for your research, please consider citing&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bib&#34;&gt;@inproceedings{bao2022all,&#xA;  title={All are Worth Words: A ViT Backbone for Diffusion Models},&#xA;  author={Bao, Fan and Nie, Shen and Xue, Kaiwen and Cao, Yue and Li, Chongxuan and Su, Hang and Zhu, Jun},&#xA;  booktitle = {CVPR},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This implementation is based on&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/baofff/Extended-Analytic-DPM&#34;&gt;Extended Analytic-DPM&lt;/a&gt; (provide the FID reference statistics on CIFAR10 and CelebA 64x64)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/guided-diffusion&#34;&gt;guided-diffusion&lt;/a&gt; (provide the FID reference statistics on ImageNet)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mseitzer/pytorch-fid&#34;&gt;pytorch-fid&lt;/a&gt; (provide the official implementation of FID to PyTorch)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/LuChengTHU/dpm-solver&#34;&gt;dpm-solver&lt;/a&gt; (provide the sampler)&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>