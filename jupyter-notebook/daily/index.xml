<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-09-21T01:39:07Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>nviennot/core-to-core-latency</title>
    <updated>2022-09-21T01:39:07Z</updated>
    <id>tag:github.com,2022-09-21:/nviennot/core-to-core-latency</id>
    <link href="https://github.com/nviennot/core-to-core-latency" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Measures the latency between CPU cores&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Measuring CPU core-to-core latency&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/license-MIT-green.svg?sanitize=true&#34; alt=&#34;License&#34;&gt; &lt;a href=&#34;https://crates.io/crates/core-to-core-latency&#34;&gt;&lt;img src=&#34;https://img.shields.io/crates/v/core-to-core-latency.svg?sanitize=true&#34; alt=&#34;Cargo&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.rust-lang.org/tools/install&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/rust-1.57+-lightgray.svg?sanitize=true&#34; alt=&#34;Rust 1.57+&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;We measure the latency it takes for a CPU to send a message to another CPU via its cache coherence protocol.&lt;/p&gt; &#xA;&lt;p&gt;By pinning two threads on two different CPU cores, we can get them to do a bunch of compare-exchange operation, and measure the latency.&lt;/p&gt; &#xA;&lt;p&gt;How to run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ cargo install core-to-core-latency&#xA;$ core-to-core-latency&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Single socket results&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;CPU&lt;/th&gt; &#xA;   &lt;th&gt;Median Latency&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Intel Core i9-12900K, 8P+8E Cores, Alder Lake, 12th gen, 2021-Q4&lt;/td&gt; &#xA;   &lt;td&gt;35ns, 44ns, 50ns&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Intel Core i9-9900K, 3.60GHz, 8 Cores, Coffee Lake, 9th gen, 2018-Q4&lt;/td&gt; &#xA;   &lt;td&gt;21ns&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Intel Core i7-1165G7, 2.80GHz, 4 Cores, Tiger Lake, 11th gen, 2020-Q3&lt;/td&gt; &#xA;   &lt;td&gt;27ns&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Intel Core i7-6700K, 4.00GHz, 4 Cores, Skylake, 6th gen, 2015-Q3&lt;/td&gt; &#xA;   &lt;td&gt;27ns&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Intel Core i5-10310U, 4 Cores, Comet Lake, 10th gen, 2020-Q2&lt;/td&gt; &#xA;   &lt;td&gt;21ns&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Intel Core i5-4590, 3.30GHz 4 Cores, Haswell, 4th gen, 2014-Q2&lt;/td&gt; &#xA;   &lt;td&gt;21ns&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Apple M1 Pro, 6P+2E Cores, 2021-Q4&lt;/td&gt; &#xA;   &lt;td&gt;40ns, 53ns, 145ns&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Intel Xeon Platinum 8375C, 2.90GHz, 32 Cores, Ice Lake, 3rd gen, 2021-Q2&lt;/td&gt; &#xA;   &lt;td&gt;51ns&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Intel Xeon Platinum 8275CL, 3.00GHz, 24 Cores, Cascade Lake, 2nd gen, 2019-Q2&lt;/td&gt; &#xA;   &lt;td&gt;47ns&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Intel Xeon E5-2695 v4, 2.10GHz, 18 Cores, Broadwell, 5th gen, 2016-Q1&lt;/td&gt; &#xA;   &lt;td&gt;44ns&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AMD EPYC 7R13, 48 Cores, Milan, 3rd gen, 2021-Q1&lt;/td&gt; &#xA;   &lt;td&gt;23ns, 107ns&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AMD Ryzen Threadripper 3960X, 3.80GHz, 24 Cores, Zen 2, 3rd Gen, 2019-Q4&lt;/td&gt; &#xA;   &lt;td&gt;24ns, 94ns&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AMD Ryzen Threadripper 1950X, 3.40GHz, 16 Cores, Zen, 1st Gen, 2017-Q3&lt;/td&gt; &#xA;   &lt;td&gt;25ns, 154ns&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AMD Ryzen 9 5950X, 3.40GHz, 16 Cores, Zen3, 4th gen, 2020-Q4&lt;/td&gt; &#xA;   &lt;td&gt;17ns, 85ns&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AMD Ryzen 9 5900X, 3.40GHz, 12 Cores, Zen3, 4th gen, 2020-Q4&lt;/td&gt; &#xA;   &lt;td&gt;16ns, 84ns&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AMD Ryzen 7 5700X, 3.40GHz, 8 Cores, Zen3, 4th gen, 2022-Q2&lt;/td&gt; &#xA;   &lt;td&gt;18ns&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AMD Ryzen 7 2700X, 3.70GHz, 8 Cores, Zen+, 2nd gen, 2018-Q3&lt;/td&gt; &#xA;   &lt;td&gt;24ns, 92ns&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AWS Graviton3, 64 Cores, Arm Neoverse, 3rd gen, 2021-Q4&lt;/td&gt; &#xA;   &lt;td&gt;46ns&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AWS Graviton2, 64 Cores, Arm Neoverse, 2rd gen, 2020-Q1&lt;/td&gt; &#xA;   &lt;td&gt;47ns&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Sun/Oracle SPARC T4, 2.85GHz, 8 cores, 2011-Q3&lt;/td&gt; &#xA;   &lt;td&gt;98ns&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;IBM Power7, 3.3GHz, 8 Cores, 2010-Q1&lt;/td&gt; &#xA;   &lt;td&gt;173ns&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;IBM PowerPC 970, 1.8GHz, 2 Cores, 2003-Q2&lt;/td&gt; &#xA;   &lt;td&gt;576ns&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Intel Core i9-12900K, 8P+8E Cores, Alder Lake, 12th gen, 2021-Q4&lt;/h2&gt; &#xA;&lt;p&gt;Data provided by &lt;a href=&#34;https://github.com/bizude&#34;&gt;bizude&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This CPU has 8 performance cores, and 2 groups of 4 efficient cores. We see CPU=8 with fast access to all other cores.&lt;/p&gt; &#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/297060/190930511-337ef53e-52c0-4350-9022-485689b7f242.png&#34; width=&#34;530&#34;&gt; &#xA;&lt;h2&gt;Intel Core i9-9900K, 3.60GHz, 8 Cores, Coffee Lake, 8th gen, 2018-Q4&lt;/h2&gt; &#xA;&lt;p&gt;My gaming machine, it&#39;s twice as fast as the other server-oriented CPUs.&lt;/p&gt; &#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/297060/190918912-8b551b33-14e6-4cd3-a82d-8ac241d1abb6.png&#34; width=&#34;400&#34;&gt; &#xA;&lt;h2&gt;Intel Core i7-1165G7, 2.80GHz, 4 Cores, Tiger Lake, 11th gen, 2020-Q3&lt;/h2&gt; &#xA;&lt;p&gt;Data provided by &lt;a href=&#34;https://github.com/jonas-w&#34;&gt;Jonas Wunderlich&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/297060/190963117-ee579206-b352-41c7-8cc2-f21c10ce2506.png&#34; width=&#34;450&#34;&gt; &#xA;&lt;h2&gt;Intel Core i7-6700K, 4.00GHz, 4 Cores, Skylake, 6th gen, 2015-Q3&lt;/h2&gt; &#xA;&lt;p&gt;Data provided by &lt;a href=&#34;https://github.com/CanIGetaPR&#34;&gt;CanIGetaPR&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/297060/190945571-a48078bc-0399-489a-81ea-271413aeec13.png&#34; width=&#34;450&#34;&gt; &#xA;&lt;h2&gt;Intel Core i5-10310U, 4 Cores, Comet Lake, 10th gen, 2020-Q2&lt;/h2&gt; &#xA;&lt;p&gt;Data provided by &lt;a href=&#34;https://github.com/ashleysommer&#34;&gt;Ashley Sommer&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/297060/190940870-526100e5-18bd-4a53-8d96-982627db581d.png&#34; width=&#34;450&#34;&gt; &#xA;&lt;h2&gt;Intel Core i5-4590, 3.30GHz, 4 Cores, Haswell, 4th gen, 2014-Q2&lt;/h2&gt; &#xA;&lt;p&gt;Data provided by &lt;a href=&#34;https://github.com/felubra&#34;&gt;Felipe Lube de BraganÃ§a&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/297060/190928985-42e13598-f5dc-4b49-b67e-dc300207d3c7.png&#34; width=&#34;450&#34;&gt; &#xA;&lt;h2&gt;Apple M1 Pro, 6P+2E Cores, 2021-Q4&lt;/h2&gt; &#xA;&lt;p&gt;Data provided by &lt;a href=&#34;https://github.com/epk&#34;&gt;Aditya Sharma&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We see the two efficent cores clustered together with a latency of 53ns, then two groups of 3 performance cores, with a latency of 40ns. Cross-group communication is slow at ~145ns, which is a latency typically seen in multi-socket configurations.&lt;/p&gt; &#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/297060/190963421-ce5b59f6-c6ec-4066-b275-9cb5af0fc4be.png&#34; width=&#34;400&#34;&gt; &#xA;&lt;h2&gt;Intel Xeon Platinum 8375C, 2.90GHz 32 Cores, Ice Lake, 3rd gen, 2021-Q2&lt;/h2&gt; &#xA;&lt;p&gt;From an AWS &lt;code&gt;c6i.metal&lt;/code&gt; machine.&lt;/p&gt; &#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/297060/190918865-7eaae192-6da6-41db-8faf-9496f6a7754b.png&#34; width=&#34;1000&#34;&gt; &#xA;&lt;h2&gt;Intel Xeon Platinum 8275CL, 3.00GHz 24 Cores, Cascade Lake, 2nd gen, 2019-Q2&lt;/h2&gt; &#xA;&lt;p&gt;From an AWS &lt;code&gt;c5.metal&lt;/code&gt; machine.&lt;/p&gt; &#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/297060/190918895-8b90cc12-2e72-41d1-808b-6f03a8771898.png&#34; width=&#34;700&#34;&gt; &#xA;&lt;h2&gt;Intel Xeon E5-2695 v4, 2.10GHz 18 Cores, Broadwell, 5th gen, 2016-Q1&lt;/h2&gt; &#xA;&lt;p&gt;From a machine provided by GTHost&lt;/p&gt; &#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/297060/190918934-a2b11676-e6e1-4b88-a8e4-6bf69663d477.png&#34; width=&#34;550&#34;&gt; &#xA;&lt;h2&gt;AMD EPYC 7R13, 48 Cores, Milan, 3rd gen, 2021-Q1&lt;/h2&gt; &#xA;&lt;p&gt;From an AWS &lt;code&gt;c6a.metal&lt;/code&gt; machine.&lt;/p&gt; &#xA;&lt;p&gt;We can see cores arranged in 6 groups of 8 in which latency is excellent within (23ns). When data crosses groups, the latency jumps to around 110ns. Note, that the last 3 groups have a better cross-group latency than the first 3 (~90ns).&lt;/p&gt; &#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/297060/190893255-56ea9890-9e06-4f2d-bcef-249a70c4597b.png&#34; width=&#34;1000&#34;&gt; &#xA;&lt;h2&gt;AMD Ryzen Threadripper 3960X, 3.80GHz, 24 Cores, Zen 2, 3rd Gen, 2019-Q4&lt;/h2&gt; &#xA;&lt;p&gt;Data provided by &lt;a href=&#34;https://github.com/ToolsDevler&#34;&gt;Mathias Siegel&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We see the CPUs in 8 groups of 3, and better performance for CPUS in the group [13,24].&lt;/p&gt; &#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/297060/190967989-efbe3341-9930-45e4-8cfe-c21d03abdb08.png&#34; width=&#34;1000&#34;&gt; &#xA;&lt;h2&gt;AMD Ryzen Threadripper 1950X, 3.40GHz, 16 Cores, Zen, 1st Gen, 2017-Q3&lt;/h2&gt; &#xA;&lt;p&gt;Data provided by &lt;a href=&#34;https://github.com/farnoy&#34;&gt;Jakub OkoÅ„ski&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;We see the CPUs in 4 groups of 4, and better performance for CPUS in the group [9,16].&lt;/p&gt; &#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/297060/190970174-2dfc378c-c2fe-4084-8a51-c15226aed52f.png&#34; width=&#34;530&#34;&gt; &#xA;&lt;h2&gt;AMD Ryzen 9 5950X, 3.40GHz 16 Cores, Zen3, 4th gen, 2020-Q1&lt;/h2&gt; &#xA;&lt;p&gt;Data provided by &lt;a href=&#34;https://github.com/Nephyrin&#34;&gt;John Schoenick&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We can see two groups of 8 cores with latencies of 17ns intra-group, and 85ns inter-group.&lt;/p&gt; &#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/297060/190926938-400092a0-45ff-4a6c-816a-1b694767c993.png&#34; width=&#34;530&#34;&gt; &#xA;&lt;h2&gt;AMD Ryzen 9 5900X, 3.40GHz, 12 Cores, Zen3, 4th gen, 2020-Q4&lt;/h2&gt; &#xA;&lt;p&gt;Data provided by &lt;a href=&#34;https://github.com/smarkwell&#34;&gt;Scott Markwell&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We see two groups of 6 cores with latencies of 16ns intra-group and 84ns inter-group.&lt;/p&gt; &#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/297060/190958644-c2dc7ff8-8ba9-430a-9441-de0b720e57e1.png&#34; width=&#34;500&#34;&gt; &#xA;&lt;h2&gt;AMD Ryzen 7 5700X, 3.40GHz, 8 Cores, Zen3, 4th gen, 2022-Q2&lt;/h2&gt; &#xA;&lt;p&gt;Data provided by &lt;a href=&#34;https://github.com/ashleysommer&#34;&gt;Ashley Sommer&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/297060/190940634-7c2b8beb-4630-4bfd-833b-df932808c8fb.png&#34; width=&#34;400&#34;&gt; &#xA;&lt;h2&gt;AMD Ryzen 7 2700X, 3.70GHz, 8 Cores, Zen+, 2nd gen, 2018-Q3&lt;/h2&gt; &#xA;&lt;p&gt;Data provided by &lt;a href=&#34;https://github.com/Demindiro&#34;&gt;David Hoppenbrouwers&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We can see 2 groups of 4 cores with latencies of 24ns intra-group, and 92ns inter-group.&lt;/p&gt; &#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/297060/190931275-a4f8e842-a033-4438-9ceb-7f8a78951ec4.png&#34; width=&#34;400&#34;&gt; &#xA;&lt;h2&gt;AWS Graviton3, 64 Cores, Arm Neoverse, 3rd gen, 2021-Q4&lt;/h2&gt; &#xA;&lt;p&gt;From an AWS &lt;code&gt;c7g.16xlarge&lt;/code&gt; machine.&lt;/p&gt; &#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/297060/190919040-7d6d2283-cbef-4544-8b07-f93f71754343.png&#34; width=&#34;1000&#34;&gt; &#xA;&lt;h2&gt;AWS Graviton2, 64 Cores, Arm Neoverse, 2nd gen, 2020-Q1&lt;/h2&gt; &#xA;&lt;p&gt;From an AWS &lt;code&gt;c6gd.metal&lt;/code&gt; machine.&lt;/p&gt; &#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/297060/190919053-11480075-6731-49ce-af03-f50bb27e8b33.png&#34; width=&#34;1000&#34;&gt; &#xA;&lt;h2&gt;Sun/Oracle SPARC T4, 2.85GHz, 8 cores, 2011-Q3&lt;/h2&gt; &#xA;&lt;p&gt;Data provided by &lt;a href=&#34;https://github.com/koachan&#34;&gt;Kokoa van Houten&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/297060/190962084-238b6491-4364-4c13-b9db-200cff80d33c.png&#34; width=&#34;400&#34;&gt; &#xA;&lt;h2&gt;IBM Power7, 3.3GHz, 8 Cores, 2010-Q1&lt;/h2&gt; &#xA;&lt;p&gt;Data provided by &lt;a href=&#34;https://github.com/koachan&#34;&gt;Kokoa van Houten&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/297060/190960589-4ffe3233-757a-402f-8194-9290e12a942b.png&#34; width=&#34;400&#34;&gt; &#xA;&lt;h2&gt;Dual sockets results&lt;/h2&gt; &#xA;&lt;p&gt;The following shows dual-socket configuration latency where one CPU on the first socket sends a message to another CPU on the second socket. The number in parenthesis next to the latency denotes the slowdown compared to single socket.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;CPU&lt;/th&gt; &#xA;   &lt;th&gt;Median Latency&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Intel Xeon Platinum 8375C, 2.90GHz, 32 Cores, Ice Lake, 3rd gen, 2021-Q2&lt;/td&gt; &#xA;   &lt;td&gt;108ns (2.1x)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Intel Xeon Platinum 8275CL, 3.00GHz, 24 Cores, Cascade Lake, 2nd gen, 2019-Q2&lt;/td&gt; &#xA;   &lt;td&gt;134ns (2.8x)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Intel Xeon E5-2695 v4, 2.10GHz, 18 Cores, Broadwell, 5th gen, 2016-Q1&lt;/td&gt; &#xA;   &lt;td&gt;118ns (2.7x)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AMD EPYC 7R13, 48 Cores, Milan, 3rd gen, 2021-Q1&lt;/td&gt; &#xA;   &lt;td&gt;197ns&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Sun/Oracle SPARC T4, 2.85GHz, 8 cores, 2011-Q3&lt;/td&gt; &#xA;   &lt;td&gt;356ns (3.6x)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;IBM Power7, 3.3GHz, 8 Cores, 2010-Q1&lt;/td&gt; &#xA;   &lt;td&gt;443ns (2.5x)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Dual Intel Xeon Platinum 8375C, 2.90GHz 32 Cores, Ice Lake, 3rd gen, 2021-Q2&lt;/h2&gt; &#xA;&lt;p&gt;From an AWS &lt;code&gt;c6i.metal&lt;/code&gt; machine.&lt;/p&gt; &#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/297060/190943800-7c3a10b0-1ebb-4b49-b4a8-9f24cff5bd88.png&#34; width=&#34;1000&#34;&gt; &#xA;&lt;h2&gt;Dual Intel Xeon Platinum 8275CL, 3.00GHz 24 Cores, Cascade Lake, 2nd gen, 2019-Q2&lt;/h2&gt; &#xA;&lt;p&gt;From an AWS &lt;code&gt;c5.metal&lt;/code&gt; machine.&lt;/p&gt; &#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/297060/190943799-80b844c1-3ddb-443f-82c4-20ddd2870130.png&#34; width=&#34;1000&#34;&gt; &#xA;&lt;h2&gt;Dual Intel Xeon E5-2695 v4, 2.10GHz 18 Cores, Broadwell, 5th gen, 2016-Q1&lt;/h2&gt; &#xA;&lt;p&gt;From a machine provided by GTHost&lt;/p&gt; &#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/297060/190943798-69b63b1f-42d5-496c-b527-6e301cb38086.png&#34; width=&#34;1000&#34;&gt; &#xA;&lt;h2&gt;Dual AMD EPYC 7R13, 48 Cores, Milan, 3rd gen, 2021-Q1&lt;/h2&gt; &#xA;&lt;p&gt;From an AWS &lt;code&gt;c6a.metal&lt;/code&gt; machine.&lt;/p&gt; &#xA;&lt;p&gt;This one is a bit odd. The single socket test for Socket 1 shows median latencies of 107ns cross-groups, but Socket 2 shows 200ns. It&#39;s 2x slower, very odd. The other platforms don&#39;t behave this way. In fact, the socket-to-socket latencies are than the core-to-core within Socket 2.&lt;/p&gt; &#xA;&lt;p&gt;Anandtech have measured &lt;a href=&#34;https://www.anandtech.com/show/16529/amd-epyc-milan-review/4&#34;&gt;similar results on a Dual-Socket AMD EPYC 7763 and 7742&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Socket 2 does not behave similarly than Socket 1, it&#39;s twice as slow&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/297060/190943333-3297c0aa-5d99-478a-8518-9eb6f96e4bc5.png&#34; width=&#34;1000&#34;&gt; &#xA;&lt;h2&gt;Sun/Oracle SPARC T4, 2.85GHz, 8 cores, 2011-Q3&lt;/h2&gt; &#xA;&lt;p&gt;Data provided by &lt;a href=&#34;https://github.com/koachan&#34;&gt;Kokoa van Houten&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/297060/190962083-2a781b0a-6923-45c9-a8cf-1ce87eb3dd58.png&#34; width=&#34;530&#34;&gt; &#xA;&lt;h2&gt;Dual IBM Power7, 3.3GHz, 8 Cores, 2010-Q1&lt;/h2&gt; &#xA;&lt;p&gt;Data provided by &lt;a href=&#34;https://github.com/koachan&#34;&gt;Kokoa van Houten&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/297060/190960588-e079d97d-a028-4da7-ac55-ae982e1cd430.png&#34; width=&#34;530&#34;&gt; &#xA;&lt;h2&gt;Hyper-threads&lt;/h2&gt; &#xA;&lt;p&gt;We measure the latency between two hyper-threads of the same core&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;CPU&lt;/th&gt; &#xA;   &lt;th&gt;Median Latency&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Intel Core i9-12900K, 8+8 Cores, Alder Lake, 12th gen, 2021-Q4&lt;/td&gt; &#xA;   &lt;td&gt;4.3ns&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Intel Core i9-9900K, 3.60GHz, 8 Cores, Coffee Lake, 9th gen, 2018-Q4&lt;/td&gt; &#xA;   &lt;td&gt;6.2ns&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Intel Core i7-1165G7, 2.80GHz, 4 Cores, Tiger Lake, 11th gen, 2020-Q3&lt;/td&gt; &#xA;   &lt;td&gt;5.9ns&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Intel Core i7-6700K, 4.00GHz, 4 Cores, Skylake, 6th gen, 2015-Q3&lt;/td&gt; &#xA;   &lt;td&gt;6.9ns&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Intel Core i5-10310U, 4 Cores, Comet Lake, 10th gen, 2020-Q2&lt;/td&gt; &#xA;   &lt;td&gt;7.3ns&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Intel Xeon Platinum 8375C, 2.90GHz, 32 Cores, Ice Lake, 3rd gen, 2021-Q2&lt;/td&gt; &#xA;   &lt;td&gt;8.1ns&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Intel Xeon Platinum 8275CL, 3.00GHz, 24 Cores, Cascade Lake, 2nd gen, 2019-Q2&lt;/td&gt; &#xA;   &lt;td&gt;7.6ns&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Intel Xeon E5-2695 v4, 2.10GHz, 18 Cores, Broadwell, 5th gen, 2016-Q1&lt;/td&gt; &#xA;   &lt;td&gt;7.6ns&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AMD EPYC 7R13, 48 Cores, Milan, 3rd gen, 2021-Q1&lt;/td&gt; &#xA;   &lt;td&gt;9.8ns&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AMD Ryzen Threadripper 3960X, 3.80GHz, 24 Cores, Zen 2, 3rd Gen, 2019-Q4&lt;/td&gt; &#xA;   &lt;td&gt;6.5ns&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AMD Ryzen Threadripper 1950X, 3.40GHz, 16 Cores, Zen, 1st Gen, 2017-Q3&lt;/td&gt; &#xA;   &lt;td&gt;10ns&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AMD Ryzen 9 5950X, 3.40GHz, 16 Cores, Zen3, 4th gen, 2020-Q4&lt;/td&gt; &#xA;   &lt;td&gt;7.8ns&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AMD Ryzen 9 5900X, 3.40GHz, 12 Cores, Zen3, 4th gen, 2020-Q4&lt;/td&gt; &#xA;   &lt;td&gt;7.6ns&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AMD Ryzen 7 5700X, 3.40GHz, 8 Cores, Zen3, 4th gen, 2022-Q2&lt;/td&gt; &#xA;   &lt;td&gt;7.8ns&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AMD Ryzen 7 2700X, 3.70GHz, 8 Cores, Zen+, 2nd gen, 2018-Q3&lt;/td&gt; &#xA;   &lt;td&gt;9.7ns&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Sun/Oracle SPARC T4, 2.85GHz, 8 cores, 2011-Q3&lt;/td&gt; &#xA;   &lt;td&gt;24ns&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;IBM Power7, 3.3GHz, 8 Cores, 2010-Q1&lt;/td&gt; &#xA;   &lt;td&gt;70ns&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;The notebook &lt;a href=&#34;https://raw.githubusercontent.com/nviennot/core-to-core-latency/main/results/results.ipynb&#34;&gt;results/results.ipynb&lt;/a&gt; contains the code to generate these graphs&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How to use&lt;/h2&gt; &#xA;&lt;p&gt;First &lt;a href=&#34;https://www.rust-lang.org/tools/install&#34;&gt;install Rust&lt;/a&gt; and &lt;code&gt;gcc&lt;/code&gt; on linux, then:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ cargo install core-to-core-latency&#xA;$ core-to-core-latency&#xA;Num cores: 10&#xA;Using RDTSC to measure time: false&#xA;Num round trips per samples: 1000&#xA;Num samples: 300&#xA;Showing latency=round-trip-time/2 in nanoseconds:&#xA;&#xA;       0       1       2       3       4       5       6       7       8       9&#xA;  0&#xA;  1   52Â±6&#xA;  2   38Â±6    39Â±4&#xA;  3   39Â±5    39Â±6    38Â±6&#xA;  4   34Â±6    38Â±4    37Â±6    36Â±5&#xA;  5   38Â±5    38Â±6    38Â±6    38Â±6    37Â±6&#xA;  6   38Â±5    37Â±6    39Â±6    36Â±4    49Â±6    38Â±6&#xA;  7   36Â±6    39Â±5    39Â±6    37Â±6    35Â±6    36Â±6    38Â±6&#xA;  8   37Â±5    38Â±6    35Â±5    39Â±5    38Â±6    38Â±5    37Â±6    37Â±6&#xA;  9   48Â±6    39Â±6    36Â±6    39Â±6    38Â±6    36Â±6    41Â±6    38Â±6    39Â±6&#xA;&#xA;Min  latency: 34.5ns Â±6.1 cores: (4,0)&#xA;Max  latency: 52.1ns Â±9.4 cores: (1,0)&#xA;Mean latency: 38.4ns&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contribute&lt;/h2&gt; &#xA;&lt;p&gt;Use &lt;code&gt;core-to-core-latency 5000 --csv &amp;gt; output.csv&lt;/code&gt; to instruct the program to use 5000 iterations per sample to reduce the noise, and save the results.&lt;/p&gt; &#xA;&lt;p&gt;It can be used in the jupter notebook &lt;a href=&#34;https://raw.githubusercontent.com/nviennot/core-to-core-latency/main/results/results.ipynb&#34;&gt;results/results.ipynb&lt;/a&gt; for rendering graphs.&lt;/p&gt; &#xA;&lt;p&gt;Create a GitHub issue with the generated &lt;code&gt;output.csv&lt;/code&gt; file and I&#39;ll add your results.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This software is licensed under the MIT license&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>zju3dv/LoFTR</title>
    <updated>2022-09-21T01:39:07Z</updated>
    <id>tag:github.com,2022-09-21:/zju3dv/LoFTR</id>
    <link href="https://github.com/zju3dv/LoFTR" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code for &#34;LoFTR: Detector-Free Local Feature Matching with Transformers&#34;, CVPR 2021&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LoFTR: Detector-Free Local Feature Matching with Transformers&lt;/h1&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://zju3dv.github.io/loftr&#34;&gt;Project Page&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/pdf/2104.00680.pdf&#34;&gt;Paper&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;br&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;LoFTR: Detector-Free Local Feature Matching with Transformers&lt;br&gt; &lt;a href=&#34;https://jiamingsun.ml&#34;&gt;Jiaming Sun&lt;/a&gt;&lt;sup&gt;*&lt;/sup&gt;, &lt;a href=&#34;https://zehongs.github.io/&#34;&gt;Zehong Shen&lt;/a&gt;&lt;sup&gt;*&lt;/sup&gt;, &lt;a href=&#34;https://github.com/angshine&#34;&gt;Yu&#39;ang Wang&lt;/a&gt;&lt;sup&gt;*&lt;/sup&gt;, &lt;a href=&#34;http://www.cad.zju.edu.cn/home/bao/&#34;&gt;Hujun Bao&lt;/a&gt;, &lt;a href=&#34;http://www.cad.zju.edu.cn/home/xzhou/&#34;&gt;Xiaowei Zhou&lt;/a&gt;&lt;br&gt; CVPR 2021&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zju3dv/LoFTR/master/assets/loftr-github-demo.gif&#34; alt=&#34;demo_vid&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;TODO List and ETA&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Inference code and pretrained models (DS and OT) (2021-4-7)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Code for reproducing the test-set results (2021-4-7)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Webcam demo to reproduce the result shown in the GIF above (2021-4-13)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Training code and training data preparation (expected 2021-6-10)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Discussions about the paper are welcomed in the &lt;a href=&#34;https://github.com/zju3dv/LoFTR/discussions&#34;&gt;discussion panel&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;ðŸš©&lt;/span&gt; &lt;strong&gt;Updates&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Check out &lt;a href=&#34;https://github.com/Tangshitao/QuadTreeAttention&#34;&gt;QuadTreeAttention&lt;/a&gt;, a new attention machanism that improves the efficiency and performance of LoFTR with less demanding GPU requirements for training.&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;âœ…&lt;/span&gt; Integrated to &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces&lt;/a&gt; with &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;. See &lt;a href=&#34;https://huggingface.co/spaces/akhaliq/Kornia-LoFTR&#34;&gt;Gradio Web Demo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Colab demo&lt;/h2&gt; &#xA;&lt;p&gt;Want to run LoFTR with custom image pairs without configuring your own GPU environment? Try the Colab demo: &lt;a href=&#34;https://colab.research.google.com/drive/1BgNIOjFHauFoNB95LGesHBIjioX74USW?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Using from kornia&lt;/h2&gt; &#xA;&lt;p&gt;LoFTR is integrated into &lt;a href=&#34;https://github.com/kornia/kornia&#34;&gt;kornia&lt;/a&gt; library since version 0.5.11.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install kornia&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you can import it as&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python3&#34;&gt;from kornia.feature import LoFTR&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See tutorial on using LoFTR from kornia &lt;a href=&#34;https://kornia-tutorials.readthedocs.io/en/latest/image_matching.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# For full pytorch-lightning trainer features (recommended)&#xA;conda env create -f environment.yaml&#xA;conda activate loftr&#xA;&#xA;# For the LoFTR matcher only&#xA;pip install torch einops yacs kornia&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We provide the &lt;a href=&#34;https://drive.google.com/drive/folders/1DOcOPZb3-5cWxLqn256AhwUVjBPifhuf?usp=sharing&#34;&gt;download link&lt;/a&gt; to&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;the scannet-1500-testset (~1GB).&lt;/li&gt; &#xA; &lt;li&gt;the megadepth-1500-testset (~600MB).&lt;/li&gt; &#xA; &lt;li&gt;4 pretrained models of indoor-ds, indoor-ot, outdoor-ds and outdoor-ot (each ~45MB).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;By now, the environment is all set and the LoFTR-DS model is ready to go! If you want to run LoFTR-OT, some extra steps are needed:&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;[Requirements for LoFTR-OT]&lt;/summary&gt; &#xA; &lt;p&gt;We use the code from &lt;a href=&#34;https://github.com/magicleap/SuperGluePretrainedNetwork&#34;&gt;SuperGluePretrainedNetwork&lt;/a&gt; for optimal transport. However, we can&#39;t provide the code directly due its strict LICENSE requirements. We recommend downloading it with the following command instead.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd src/loftr/utils  &#xA;wget https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/master/models/superglue.py &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Run LoFTR demos&lt;/h2&gt; &#xA;&lt;h3&gt;Match image pairs with LoFTR&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;[code snippets]&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.loftr import LoFTR, default_cfg&#xA;&#xA;# Initialize LoFTR&#xA;matcher = LoFTR(config=default_cfg)&#xA;matcher.load_state_dict(torch.load(&#34;weights/indoor_ds.ckpt&#34;)[&#39;state_dict&#39;])&#xA;matcher = matcher.eval().cuda()&#xA;&#xA;# Inference&#xA;with torch.no_grad():&#xA;    matcher(batch)    # batch = {&#39;image0&#39;: img0, &#39;image1&#39;: img1}&#xA;    mkpts0 = batch[&#39;mkpts0_f&#39;].cpu().numpy()&#xA;    mkpts1 = batch[&#39;mkpts1_f&#39;].cpu().numpy()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;An example is given in &lt;code&gt;notebooks/demo_single_pair.ipynb&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Online demo&lt;/h3&gt; &#xA;&lt;p&gt;Run the online demo with a webcam or video to reproduce the result shown in the GIF above.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd demo&#xA;./run_demo.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;[run_demo.sh]&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash&#xA;set -e&#xA;# set -x&#xA;&#xA;if [ ! -f utils.py ]; then&#xA;    echo &#34;Downloading utils.py from the SuperGlue repo.&#34;&#xA;    echo &#34;We cannot provide this file directly due to its strict licence.&#34;&#xA;    wget https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/master/models/utils.py&#xA;fi&#xA;&#xA;# Use webcam 0 as input source. &#xA;input=0&#xA;# or use a pre-recorded video given the path.&#xA;# input=/home/sunjiaming/Downloads/scannet_test/$scene_name.mp4&#xA;&#xA;# Toggle indoor/outdoor model here.&#xA;model_ckpt=../weights/indoor_ds.ckpt&#xA;# model_ckpt=../weights/outdoor_ds.ckpt&#xA;&#xA;# Optionally assign the GPU ID.&#xA;# export CUDA_VISIBLE_DEVICES=0&#xA;&#xA;echo &#34;Running LoFTR demo..&#34;&#xA;eval &#34;$(conda shell.bash hook)&#34;&#xA;conda activate loftr&#xA;python demo_loftr.py --weight $model_ckpt --input $input&#xA;# To save the input video and output match visualizations.&#xA;# python demo_loftr.py --weight $model_ckpt --input $input --save_video --save_input&#xA;&#xA;# Running on remote GPU servers with no GUI.&#xA;# Save images first.&#xA;# python demo_loftr.py --weight $model_ckpt --input $input --no_display --output_dir=&#34;./demo_images/&#34;&#xA;# Then convert them to a video.&#xA;# ffmpeg -framerate 15 -pattern_type glob -i &#39;*.png&#39; -c:v libx264 -r 30 -pix_fmt yuv420p out.mp4&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Reproduce the testing results with pytorch-lightning&lt;/h3&gt; &#xA;&lt;p&gt;You need to setup the testing subsets of ScanNet and MegaDepth first. We create symlinks from the previously downloaded datasets to &lt;code&gt;data/{{dataset}}/test&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# set up symlinks&#xA;ln -s /path/to/scannet-1500-testset/* /path/to/LoFTR/data/scannet/test&#xA;ln -s /path/to/megadepth-1500-testset/* /path/to/LoFTR/data/megadepth/test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda activate loftr&#xA;# with shell script&#xA;bash ./scripts/reproduce_test/indoor_ds.sh&#xA;&#xA;# or&#xA;python test.py configs/data/scannet_test_1500.py configs/loftr/loftr_ds.py --ckpt_path weights/indoor_ds.ckpt --profiler_name inference --gpus=1 --accelerator=&#34;ddp&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For visualizing the results, please refer to &lt;code&gt;notebooks/visualize_dump_results.ipynb&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;!-- ### Image pair info for training on ScanNet&#xA;You can download the data at [here](https://drive.google.com/file/d/1fC2BezUSsSQy7_H65A0ZfrYK0RB3TXXj/view?usp=sharing).&#xA;&#xA;&lt;details&gt;&#xA;  &lt;summary&gt;[data format]&lt;/summary&gt;&#xA;&#xA;```python&#xA;In [14]: npz_path = &#39;./cfg_1513_-1_0.2_0.8_0.15/scene_data/train/scene0000_01.npz&#39;&#xA;&#xA;In [15]: data = np.load(npz_path)&#xA;&#xA;In [16]: data[&#39;name&#39;]&#xA;Out[16]:&#xA;array([[   0,    1,  276,  567],&#xA;       [   0,    1, 1147, 1170],&#xA;       [   0,    1,  541, 5757],&#xA;       ...,&#xA;       [   0,    1, 5366, 5393],&#xA;       [   0,    1, 2607, 5278],&#xA;       [   0,    1,  736, 5844]], dtype=uint16)&#xA;&#xA;In [17]: data[&#39;score&#39;]&#xA;Out[17]: array([0.2903, 0.7715, 0.5986, ..., 0.7227, 0.5527, 0.4148], dtype=float16)&#xA;&#xA;In [18]: len(data[&#39;name&#39;])&#xA;Out[18]: 1684276&#xA;&#xA;In [19]: len(data[&#39;score&#39;])&#xA;Out[19]: 1684276&#xA;```&#xA;`data[&#39;name&#39;]` is the image pair info, organized as [`scene_id`, `seq_id`, `image0_id`, `image1_id`].&#xA;&#xA;`data[&#39;score&#39;]` is the overlapping score defined in [SuperGlue](https://arxiv.org/pdf/1911.11763) (Page 12).&#xA;&lt;/details&gt; --&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/zju3dv/LoFTR/master/docs/TRAINING.md&#34;&gt;Training LoFTR&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this code useful for your research, please use the following BibTeX entry.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{sun2021loftr,&#xA;  title={{LoFTR}: Detector-Free Local Feature Matching with Transformers},&#xA;  author={Sun, Jiaming and Shen, Zehong and Wang, Yuang and Bao, Hujun and Zhou, Xiaowei},&#xA;  journal={{CVPR}},&#xA;  year={2021}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Copyright&lt;/h2&gt; &#xA;&lt;p&gt;This work is affiliated with ZJU-SenseTime Joint Lab of 3D Vision, and its intellectual property belongs to SenseTime Group Ltd.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Copyright SenseTime. All Rights Reserved.&#xA;&#xA;Licensed under the Apache License, Version 2.0 (the &#34;License&#34;);&#xA;you may not use this file except in compliance with the License.&#xA;You may obtain a copy of the License at&#xA;&#xA;    http://www.apache.org/licenses/LICENSE-2.0&#xA;&#xA;Unless required by applicable law or agreed to in writing, software&#xA;distributed under the License is distributed on an &#34;AS IS&#34; BASIS,&#xA;WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&#xA;See the License for the specific language governing permissions and&#xA;limitations under the License.&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>asharifiz/Introduction_to_Machine_Learning</title>
    <updated>2022-09-21T01:39:07Z</updated>
    <id>tag:github.com,2022-09-21:/asharifiz/Introduction_to_Machine_Learning</id>
    <link href="https://github.com/asharifiz/Introduction_to_Machine_Learning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Introduction to Machine Learning, for B.Sc. Students, Computer Engineering Department, Sharif University of Technology&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Introduction to Machine Learning&lt;/h1&gt; &#xA;&lt;p&gt;A course for B.Sc. students, Computer Engineering Department, Sharif University of Technology.&lt;/p&gt; &#xA;&lt;p&gt;The course is instructed in Persian (Farsi) language.&lt;/p&gt; &#xA;&lt;p&gt;All course materials (Slides, Jupyter Notebooks, Theory and Practical Assignments, Projects, etc.) are freely available under Creative Commons BY license.&lt;/p&gt; &#xA;&lt;p&gt;Co-instructors: Behrooz Azarkhalili, Ali Sharifi-Zarchi&lt;/p&gt; &#xA;&lt;h2&gt;Copyright Notice&lt;/h2&gt; &#xA;&lt;p&gt;All materials are provided under Creative-Commons BY license. This course is crowd-sourced by a group of volunteers university students. If you noticed any copyright violation in texts, figures, etc., please report it by sending an email to sharifi [at] sharif [dot] edu&lt;/p&gt;</summary>
  </entry>
</feed>