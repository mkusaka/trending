<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-01-05T01:35:30Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>googlecolab/colabtools</title>
    <updated>2024-01-05T01:35:30Z</updated>
    <id>tag:github.com,2024-01-05:/googlecolab/colabtools</id>
    <link href="https://github.com/googlecolab/colabtools" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Python libraries for Google Colaboratory&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Google Colaboratory&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com&#34;&gt;Colaboratory&lt;/a&gt; is a research project created to help disseminate machine learning education and research. It’s a Jupyter notebook environment that requires no setup to use. For more information, see our &lt;a href=&#34;https://research.google.com/colaboratory/faq.html&#34;&gt;FAQ&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This repository contains the code for the Python libraries available in the Colab.&lt;/p&gt; &#xA;&lt;h2&gt;Intended Use&lt;/h2&gt; &#xA;&lt;p&gt;This repo is intended to share code and other resources with the Colab community and to solicit feedback on the Colab product via &lt;a href=&#34;https://github.com/googlecolab/colabtools/issues&#34;&gt;github issues&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The code published here is not intended for private reuse.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contacting Us&lt;/h2&gt; &#xA;&lt;p&gt;For support or help using Colab, please submit questions tagged with &lt;code&gt;google-colaboratory&lt;/code&gt; on &lt;a href=&#34;https://stackoverflow.com/questions/tagged/google-colaboratory&#34;&gt;StackOverflow&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For any product issues, you can either &lt;a href=&#34;https://github.com/googlecolab/colabtools/issues&#34;&gt;submit an issue&lt;/a&gt; or &#34;Help&#34; -&amp;gt; &#34;Send Feedback&#34; in Colab.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;If you have a problem, or see something that could be improved, please file an issue. However, we don&#39;t have the bandwidth to support review of external contributions, and we don&#39;t want user PRs to languish, so we aren&#39;t accepting any external contributions right now.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>pytorch/tutorials</title>
    <updated>2024-01-05T01:35:30Z</updated>
    <id>tag:github.com,2024-01-05:/pytorch/tutorials</id>
    <link href="https://github.com/pytorch/tutorials" rel="alternate"></link>
    <summary type="html">&lt;p&gt;PyTorch tutorials.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;PyTorch Tutorials&lt;/h1&gt; &#xA;&lt;p&gt;All the tutorials are now presented as sphinx style documentation at:&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://pytorch.org/tutorials&#34;&gt;https://pytorch.org/tutorials&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h1&gt;Asking a question&lt;/h1&gt; &#xA;&lt;p&gt;If you have a question about a tutorial, post in &lt;a href=&#34;https://dev-discuss.pytorch.org/&#34;&gt;https://dev-discuss.pytorch.org/&lt;/a&gt; rather than creating an issue in this repo. Your question will be answered much faster on the dev-discuss forum.&lt;/p&gt; &#xA;&lt;h1&gt;Submitting an issue&lt;/h1&gt; &#xA;&lt;p&gt;You can submit the following types of issues:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Feature request - request a new tutorial to be added. Please explain why this tutorial is needed and how it demonstrates PyTorch value.&lt;/li&gt; &#xA; &lt;li&gt;Bug report - report a failure or outdated information in an existing tutorial. When submitting a bug report, please run: &lt;code&gt;python3 -m torch.utils.collect_env&lt;/code&gt; to get information about your environment and add the output to the bug report.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;We use sphinx-gallery&#39;s &lt;a href=&#34;https://sphinx-gallery.github.io/stable/tutorials/index.html&#34;&gt;notebook styled examples&lt;/a&gt; to create the tutorials. Syntax is very simple. In essence, you write a slightly well formatted Python file and it shows up as an HTML page. In addition, a Jupyter notebook is autogenerated and available to run in Google Colab.&lt;/p&gt; &#xA;&lt;p&gt;Here is how you can create a new tutorial (for a detailed description, see &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/tutorials/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create a Python file. If you want it executed while inserted into documentation, save the file with the suffix &lt;code&gt;tutorial&lt;/code&gt; so that the file name is &lt;code&gt;your_tutorial.py&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Put it in one of the &lt;code&gt;beginner_source&lt;/code&gt;, &lt;code&gt;intermediate_source&lt;/code&gt;, &lt;code&gt;advanced_source&lt;/code&gt; directory based on the level of difficulty. If it is a recipe, add it to &lt;code&gt;recipes_source&lt;/code&gt;. For tutorials demonstrating unstable prototype features, add to the &lt;code&gt;prototype_source&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For Tutorials (except if it is a prototype feature), include it in the &lt;code&gt;toctree&lt;/code&gt; directive and create a &lt;code&gt;customcarditem&lt;/code&gt; in &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/tutorials/main/index.rst&#34;&gt;index.rst&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For Tutorials (except if it is a prototype feature), create a thumbnail in the &lt;a href=&#34;https://github.com/pytorch/tutorials/raw/main/index.rst&#34;&gt;index.rst file&lt;/a&gt; using a command like &lt;code&gt;.. customcarditem:: beginner/your_tutorial.html&lt;/code&gt;. For Recipes, create a thumbnail in the &lt;a href=&#34;https://github.com/pytorch/tutorials/raw/main/recipes_source/recipes_index.rst&#34;&gt;recipes_index.rst&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;If you are starting off with a Jupyter notebook, you can use &lt;a href=&#34;https://gist.github.com/chsasank/7218ca16f8d022e02a9c0deb94a310fe&#34;&gt;this script&lt;/a&gt; to convert the notebook to Python file. After conversion and addition to the project, please make sure that section headings and other things are in logical order.&lt;/p&gt; &#xA;&lt;h2&gt;Building locally&lt;/h2&gt; &#xA;&lt;p&gt;The tutorial build is very large and requires a GPU. If your machine does not have a GPU device, you can preview your HTML build without actually downloading the data and running the tutorial code:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install required dependencies by running: &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;If you want to use &lt;code&gt;virtualenv&lt;/code&gt;, in the root of the repo, run: &lt;code&gt;virtualenv venv&lt;/code&gt;, then &lt;code&gt;source venv/bin/activate&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you have a GPU-powered laptop, you can build using &lt;code&gt;make docs&lt;/code&gt;. This will download the data, execute the tutorials and build the documentation to &lt;code&gt;docs/&lt;/code&gt; directory. This might take about 60-120 min for systems with GPUs. If you do not have a GPU installed on your system, then see next step.&lt;/li&gt; &#xA; &lt;li&gt;You can skip the computationally intensive graph generation by running &lt;code&gt;make html-noplot&lt;/code&gt; to build basic html documentation to &lt;code&gt;_build/html&lt;/code&gt;. This way, you can quickly preview your tutorial.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;If you get &lt;strong&gt;ModuleNotFoundError: No module named &#39;pytorch_sphinx_theme&#39; make: *** [html-noplot] Error 2&lt;/strong&gt; from /tutorials/src/pytorch-sphinx-theme or /venv/src/pytorch-sphinx-theme (while using virtualenv), run &lt;code&gt;python setup.py install&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Building a single tutorial&lt;/h2&gt; &#xA;&lt;p&gt;You can build a single tutorial by using the &lt;code&gt;GALLERY_PATTERN&lt;/code&gt; environment variable. For example to run only &lt;code&gt;neural_style_transfer_tutorial.py&lt;/code&gt;, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;GALLERY_PATTERN=&#34;neural_style_transfer_tutorial.py&#34; make html&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;GALLERY_PATTERN=&#34;neural_style_transfer_tutorial.py&#34; sphinx-build . _build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;GALLERY_PATTERN&lt;/code&gt; variable respects regular expressions.&lt;/p&gt; &#xA;&lt;h2&gt;About contributing to PyTorch Documentation and Tutorials&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can find information about contributing to PyTorch documentation in the PyTorch Repo &lt;a href=&#34;https://github.com/pytorch/pytorch/raw/master/README.md&#34;&gt;README.md&lt;/a&gt; file.&lt;/li&gt; &#xA; &lt;li&gt;Additional information can be found in &lt;a href=&#34;https://github.com/pytorch/pytorch/raw/master/CONTRIBUTING.md&#34;&gt;PyTorch CONTRIBUTING.md&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>shansongliu/M2UGen</title>
    <updated>2024-01-05T01:35:30Z</updated>
    <id>tag:github.com,2024-01-05:/shansongliu/M2UGen</id>
    <link href="https://github.com/shansongliu/M2UGen" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This is the official repository for M2UGen&lt;/p&gt;&lt;hr&gt;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h1&gt; &lt;img src=&#34;https://raw.githubusercontent.com/shansongliu/M2UGen/main/assets/logo.png&#34; height=&#34;120px&#34; align=&#34;right&#34;&gt; M&lt;sup&gt;2&lt;/sup&gt;UGen: Multi-modal Music Understanding and Generation with the Power of Large Language Models &lt;/h1&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2311.11255&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%93%8E%20arXiv-Paper-red&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://crypto-code.github.io/M2UGen-Demo/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%8C%8E%20Website-Official%20Page-blue&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/M2UGen/M2UGen-Demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/HuggingFace-Demo-Green&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is the official repository for &lt;em&gt;&lt;a href=&#34;https://arxiv.org/abs/2308.11276&#34;&gt;M&lt;sup&gt;2&lt;/sup&gt;UGen: Multi-modal Music Understanding and Generation with the Power of Large Language Models&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;🚀 Introduction&lt;/h2&gt; &#xA;&lt;p&gt;The M&lt;sup&gt;2&lt;/sup&gt;UGen model is a Music Understanding and Generation model that is capable of Music Question Answering and also Music Generation from texts, images, videos and audios, as well as Music Editing. The model utilizes encoders such as MERT for music understanding, ViT for image understanding and ViViT for video understanding and the MusicGen/AudioLDM2 model as the music generation model (music decoder), coupled with adapters and the LLaMA 2 model to make the model possible for multiple abilities. The model architecture is given in &lt;a href=&#34;https://raw.githubusercontent.com/shansongliu/M2UGen/main/M2UGen/llama/m2ugen.py&#34;&gt;&lt;strong&gt;&lt;em&gt;m2ugen.py&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/shansongliu/M2UGen/main/assets/M2UGen.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;To train our model, we generate datasets using a music captioning and question answering model, i.e. the &lt;a href=&#34;https://github.com/crypto-code/MU-LLaMA&#34;&gt;MU-LLaMA&lt;/a&gt; model. The dataset generation methods are given in the &lt;a href=&#34;https://raw.githubusercontent.com/shansongliu/M2UGen/main/Datasets&#34;&gt;Datasets&lt;/a&gt; folder.&lt;/p&gt; &#xA;&lt;h2&gt;🤗 HuggingFace Demo&lt;/h2&gt; &#xA;&lt;p&gt;We have provided a HuggingFace Space to see our model in action: &lt;a href=&#34;https://huggingface.co/spaces/M2UGen/M2UGen-Demo&#34;&gt;M2UGen/M2UGen-Demo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;🤖 Model Setup&lt;/h2&gt; &#xA;&lt;p&gt;We use Python 3.9.17 for this project and the library requirements are given in requirements.txt. Create a conda environment using&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create --name &amp;lt;env&amp;gt; --file requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Ensure that the NVIDIA Driver is version 12 or above to be compatible with PyTorch 2.1.0.&lt;/p&gt; &#xA;&lt;p&gt;For the working of our model, Facebook&#39;s LLaMA-2 model weights are required, details on obtaining these weights are given on &lt;a href=&#34;https://huggingface.co/docs/transformers/main/model_doc/llama&#34;&gt;HuggingFace&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The trained checkpoints for our model is available here:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/M2UGen/M2UGen-MusicGen-small&#34;&gt;M&lt;sup&gt;2&lt;/sup&gt;UGen with MusicGen Small&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/M2UGen/M2UGen-MusicGen-medium&#34;&gt;M&lt;sup&gt;2&lt;/sup&gt;UGen with MusicGen Medium&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/M2UGen/M2UGen-AudioLDM2&#34;&gt;M&lt;sup&gt;2&lt;/sup&gt;UGen with AudioLDM2&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The needed pretrained multi-modal encoder and music decoder models can be found here:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/m-a-p/MERT-v1-330M&#34;&gt;MERT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/google/vit-base-patch16-224-in21k&#34;&gt;ViT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/google/vivit-b-16x2-kinetics400&#34;&gt;ViViT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/facebook/musicgen-medium&#34;&gt;MusicGen&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/cvssp/audioldm2-music&#34;&gt;AudioLDM 2&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The directory of the checkpoints folder can be organized as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;.&#xA;├── ...&#xA;├── M2UGen                &#xA;│   ├── ckpts&#xA;│   │   │── LLaMA&#xA;│   │   │   │── 7B&#xA;│   │   │   │   │── checklist.chk&#xA;│   │   │   │   │── consolidated.00.pth&#xA;│   │   │   │   │── params.json&#xA;│   │   │   │── llama.sh&#xA;│   │   │   │── tokenizer.model&#xA;│   │   │   │── tokenizer_checklist.chk&#xA;│   │   │── M2UGen-MusicGen&#xA;│   │   │   │── checkpoint.pth&#xA;│   │   │── M2UGen-AudioLDM2&#xA;│   │   │   │── checkpoint.pth&#xA;│   │   │── knn.index&#xA;└── ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once downloaded, the Gradio demo can be run using these checkpoints.&lt;/p&gt; &#xA;&lt;p&gt;For model with MusicGen&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python gradio_app.py --model ./ckpts/M2UGen-MusicGen --llama_dir ./ckpts/LLaMA-2 --music_decoder musicgen&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For model with AudioLDM2&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python gradio_app.py --model ./ckpts/M2UGen-AudioLDM2 --llama_dir ./ckpts/LLaMA-2 --music_decoder audioldm2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🗄️ Dataset Generation&lt;/h2&gt; &#xA;&lt;p&gt;We use the &lt;a href=&#34;https://github.com/crypto-code/MU-LLaMA&#34;&gt;MU-LLaMA&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/mosaicml/mpt-7b-chat&#34;&gt;MPT-7B&lt;/a&gt; models to generate the MUCaps, MUEdit, MUImge and MUVideo datasets. For each of the datasets, run the scripts in the folder &lt;a href=&#34;https://raw.githubusercontent.com/shansongliu/M2UGen/main/Datasets&#34;&gt;Datasets&lt;/a&gt; in its numbered order to generate the datasets.&lt;/p&gt; &#xA;&lt;p&gt;The datasets are also available for download here:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/M2UGen/MUCaps&#34;&gt;MUCaps&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/M2UGen/MUEdit&#34;&gt;MUEdit&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/M2UGen/MUImage&#34;&gt;MUImage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/M2UGen/MUVideo&#34;&gt;MUVideo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🔧 Model Training&lt;/h2&gt; &#xA;&lt;p&gt;To train the M&lt;sup&gt;2&lt;/sup&gt;UGen model, run the &lt;a href=&#34;https://raw.githubusercontent.com/shansongliu/M2UGen/main/M2UGen/train_musicgen.sh&#34;&gt;&lt;strong&gt;&lt;em&gt;train_musicgen.sh&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt; or &lt;a href=&#34;https://raw.githubusercontent.com/shansongliu/M2UGen/main/M2UGen/train_audioldm2.sh&#34;&gt;&lt;strong&gt;&lt;em&gt;train_audioldm2.sh&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt; script. The scripts are designed to train the model for all three stages with &lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/musicgen&#34;&gt;MusicGen&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/docs/diffusers/main/en/api/pipelines/audioldm2&#34;&gt;AudioLDM2&lt;/a&gt; music decoders respectively.&lt;/p&gt; &#xA;&lt;p&gt;The main model architecture is given in &lt;a href=&#34;https://raw.githubusercontent.com/shansongliu/M2UGen/main/M2UGen/llama/m2ugen.py&#34;&gt;&lt;strong&gt;&lt;em&gt;m2ugen.py&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt; and the modified MusicGen and AudioLDM2 architectures are present within the &lt;a href=&#34;https://raw.githubusercontent.com/shansongliu/M2UGen/main/M2UGen/llama/musicgen/&#34;&gt;&lt;strong&gt;&lt;em&gt;musicgen&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/shansongliu/M2UGen/main/M2UGen/llama/audioldm2/&#34;&gt;&lt;strong&gt;&lt;em&gt;audioldm2&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt; folders respectively. The &lt;a href=&#34;https://raw.githubusercontent.com/shansongliu/M2UGen/main/M2UGen/data/&#34;&gt;&lt;strong&gt;&lt;em&gt;data&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt; folder contains the python files to handle loading the dataset. The &lt;a href=&#34;https://raw.githubusercontent.com/shansongliu/M2UGen/main/M2UGen/data/dataset.py&#34;&gt;&lt;strong&gt;&lt;em&gt;dataset.py&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt; file will show the use of different datasets based on the training stage. The code for the training epochs are present in &lt;a href=&#34;https://raw.githubusercontent.com/shansongliu/M2UGen/main/M2UGen/engine_train.py&#34;&gt;&lt;strong&gt;&lt;em&gt;engine_train.py&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;🔨 Model Testing and Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;To test the M&lt;sup&gt;2&lt;/sup&gt;UGen model, run &lt;a href=&#34;https://raw.githubusercontent.com/shansongliu/M2UGen/main/M2UGen/gradio_app.py&#34;&gt;&lt;strong&gt;&lt;em&gt;gradio_app.py&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;usage: gradio_app.py [-h] [--model MODEL] [--llama_type LLAMA_TYPE] [--llama_dir LLAMA_DIR]&#xA;                      [--mert_path MERT_PATH] [--vit_path VIT_PATH] [--vivit_path VIVIT_PATH]&#xA;                      [--knn_dir KNN_DIR] [--music_decoder MUSIC_DECODER]&#xA;&#xA;optional arguments:&#xA;  -h, --help            show this help message and exit&#xA;  --model MODEL         Name of or path to M2UGen pretrained checkpoint&#xA;  --llama_type LLAMA_TYPE&#xA;                        Type of llama original weight&#xA;  --llama_dir LLAMA_DIR&#xA;                        Path to LLaMA pretrained checkpoint&#xA;  --mert_path MERT_PATH&#xA;                        Path to MERT pretrained checkpoint&#xA;  --vit_path VIT_PATH   Path to ViT pretrained checkpoint&#xA;  --vivit_path VIVIT_PATH&#xA;                        Path to ViViT pretrained checkpoint&#xA;  --knn_dir KNN_DIR     Path to directory with KNN Index&#xA;  --music_decoder MUSIC_DECODER&#xA;                        Decoder to use musicgen/audioldm2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To evaluate the M&lt;sup&gt;2&lt;/sup&gt;UGen model and other compared models in our paper, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/shansongliu/M2UGen/main/Evaluation&#34;&gt;&lt;strong&gt;&lt;em&gt;Evaluation&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt; folder.&lt;/p&gt; &#xA;&lt;h2&gt;🧰 System Hardware requirements&lt;/h2&gt; &#xA;&lt;p&gt;For training, stage 1 and 2 use a single 32GB V100 GPU while stage 3 uses 2 32GB V100 GPUs. For inference, a single 32GB V100 GPU is used. For loading model checkpoint, approximately 49GB of CPU memory is required.&lt;/p&gt; &#xA;&lt;h2&gt;🫡 Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;This code contains elements from the following repo:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/crypto-code/MU-LLaMA&#34;&gt;crypto-code/MU-LLaMA&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;✨ Cite our work&lt;/h2&gt; &#xA;&lt;p&gt;If you find this repo useful, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{hussain2023m,&#xA;  title={{M$^{2}$UGen: Multi-modal Music Understanding and Generation with the Power of Large Language Models}},&#xA;  author={Hussain, Atin Sakkeer and Liu, Shansong and Sun, Chenshuo and Shan, Ying},&#xA;  journal={arXiv preprint arXiv:2311.11255},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#shansongliu/M2UGen&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=shansongliu/M2UGen&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>