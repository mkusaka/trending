<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-05T01:31:56Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Muennighoff/sgpt</title>
    <updated>2023-04-05T01:31:56Z</updated>
    <id>tag:github.com,2023-04-05:/Muennighoff/sgpt</id>
    <link href="https://github.com/Muennighoff/sgpt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;SGPT: GPT Sentence Embeddings for Semantic Search&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SGPT: GPT Sentence Embeddings for Semantic Search&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains code, results &amp;amp; pre-trained models for the paper &lt;a href=&#34;https://arxiv.org/abs/2202.08904&#34;&gt;SGPT: GPT Sentence Embeddings for Semantic Search&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;**************************** Updates ****************************&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2022-09: SGPT Bi-Encoders are now easy to use with &lt;a href=&#34;https://github.com/UKPLab/sentence-transformers&#34;&gt;Sentence Transformers&lt;/a&gt;, see &lt;a href=&#34;https://raw.githubusercontent.com/Muennighoff/sgpt/main/#use-sgpt-with-sentence-transformers&#34;&gt;new scripts&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;2022-08: Multilingual BLOOM SGPT models were released: &lt;a href=&#34;https://huggingface.co/bigscience/sgpt-bloom-7b1-msmarco&#34;&gt;Asymmetric, 7.1B parameters&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://huggingface.co/bigscience-data/sgpt-bloom-1b7-nli&#34;&gt;Symmetric, 1.7B parameters&lt;/a&gt;. Feel free to open an issue if you need a different model.&lt;/li&gt; &#xA; &lt;li&gt;2022-06: OpenAI released the mechanism of their Search Endpoint that we compared to SGPT Cross-Encoders in the &lt;a href=&#34;https://arxiv.org/abs/2202.08904&#34;&gt;paper&lt;/a&gt;. Our methods are very similar. Feel free to test their prompt as seen in &lt;code&gt;crossencoder/beir/openai_search_endpoint_functionality.py&lt;/code&gt;!&lt;/li&gt; &#xA; &lt;li&gt;2022-03: 5.8B Bi-Encoder models are now 4% &amp;amp; 1% better on USEB &amp;amp; BEIR, respectively. &lt;a href=&#34;https://arxiv.org/abs/2202.08904&#34;&gt;Paper&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://huggingface.co/models?search=sgpt-5.8b&#34;&gt;models&lt;/a&gt; on HF have been updated. This has been done by using larger batch sizes with GradCache, see the paper for more info. If you have previously downloaded them, we recommend replacing it with the new version.&lt;/li&gt; &#xA; &lt;li&gt;2022-02: We released &lt;a href=&#34;https://arxiv.org/abs/2202.08904&#34;&gt;our paper&lt;/a&gt;. Check it out! :)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick Links&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Muennighoff/sgpt/main/#overview&#34;&gt;Overview&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Muennighoff/sgpt/main/#structure&#34;&gt;Structure&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Muennighoff/sgpt/main/#use-sgpt-with-huggingface&#34;&gt;Use SGPT with Huggingface&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Muennighoff/sgpt/main/#bi-encoder&#34;&gt;Bi-Encoder&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Muennighoff/sgpt/main/#symmetric-semantic-search-be&#34;&gt;Symmetric Semantic Search BE&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Muennighoff/sgpt/main/#asymmetric-semantic-search-be&#34;&gt;Asymmetric Semantic Search BE&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Muennighoff/sgpt/main/#cross-encoder&#34;&gt;Cross-Encoder&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Muennighoff/sgpt/main/#asymmetric-semantic-search-ce&#34;&gt;Asymmetric Semantic Search CE&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Muennighoff/sgpt/main/#symmetric-semantic-search-ce&#34;&gt;Symmetric Semantic Search CE&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Muennighoff/sgpt/main/#use-sgpt-with-sentence-transformers&#34;&gt;Use SGPT with Sentence Transformers&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Muennighoff/sgpt/main/#bi-encoder-st&#34;&gt;Bi-Encoder ST&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Muennighoff/sgpt/main/#symmetric-semantic-search-be-st&#34;&gt;Symmetric Semantic Search BE ST&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Muennighoff/sgpt/main/#asymmetric-semantic-search-be-st&#34;&gt;Asymmetric Semantic Search BE ST&lt;/a&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Muennighoff/sgpt/main/#sgpt-sentence-transformers&#34;&gt;SGPT Sentence Transformers&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Muennighoff/sgpt/main/#original-sentence-transformers&#34;&gt;Original Sentence Transformers&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Muennighoff/sgpt/main/#acknowledgements&#34;&gt;Acknowledgements&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Muennighoff/sgpt/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;We present SGPT-BE and SGPT-CE for applying GPT models as Bi-Encoders or Cross-Encoders to symmetric or asymmetric search. SGPT-BE produces semantically meaningful sentence embeddings by contrastive fine-tuning of only bias tensors and position-weighted mean pooling. SGPT-CE uses log probabilities from GPT models without any fine-tuning. An illustration of the methods follows.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Muennighoff/sgpt/main/other/sgpt_graphic.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Feel free to open an issue should you have any questions~&lt;/p&gt; &#xA;&lt;h2&gt;Structure&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;.&#xA;├── biencoder  # Training &amp;amp; Inference of Bi-Encoders&#xA;│&amp;nbsp;&amp;nbsp; ├── beir&#xA;│&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; ├── custommodels # Directory providing BEIR compatibility for asymmetric mdoels &amp;amp; models with special tokens&#xA;│&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; └── ...&#xA;│&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; ├── io_utils # Exclusively used for beir_openai_embeddings_batched_parallel.py&#xA;│&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; └── ...&#xA;│   │   ├── parallelizer # Exclusively used for beir_openai_embeddings_batched_parallel.py&#xA;│&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; └── ...&#xA;│   │   ├── beir_dense_retriever.py&#xA;│   │   ├── beir_openai_embeddings_batched_parallel.py&#xA;│   │   ├── requirements.txt&#xA;│   │   ├── *.bash # Bash scripts to run multiple experiments&#xA;│&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; └── README.md&#xA;│&amp;nbsp;&amp;nbsp; ├── nli_msmarco&#xA;│&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; ├── sentence-transformers # An adapted version of sentence-transformers - Install this version for all biencoder experiments&#xA;│&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; └── ...&#xA;│&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; └── README.md&#xA;│&amp;nbsp;&amp;nbsp; └── useb&#xA;│&amp;nbsp;&amp;nbsp;  &amp;nbsp;&amp;nbsp; ├── useb&#xA;│&amp;nbsp;&amp;nbsp;  &amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; └── ...&#xA;│&amp;nbsp;&amp;nbsp;  &amp;nbsp;&amp;nbsp; ├── *.bash # Bash scripts to run multiple experiments&#xA;│&amp;nbsp;&amp;nbsp;  &amp;nbsp;&amp;nbsp; ├── useb_dense_retriever.py&#xA;│&amp;nbsp;&amp;nbsp;  &amp;nbsp;&amp;nbsp; └── README.md&#xA;├── crossencoder  # Inference of Cross-Encoders&#xA;│&amp;nbsp;&amp;nbsp; └── beir&#xA;│&amp;nbsp;&amp;nbsp;  &amp;nbsp;&amp;nbsp; ├── *.ipynb # Notebooks explained in the README&#xA;│&amp;nbsp;&amp;nbsp;  &amp;nbsp;&amp;nbsp; └──&amp;nbsp;README.md&#xA;├── other&#xA;│&amp;nbsp;&amp;nbsp; ├── sgpt_graphic.png&#xA;│&amp;nbsp;&amp;nbsp; └── sgpt_utils.ipynb # Code for creating the graphs in the paper &amp;amp; other&#xA;├── requirements.txt&#xA;└──&amp;nbsp;README.md&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Each data sub-directory provides its own README with an overview of its &lt;strong&gt;Structure&lt;/strong&gt;, &lt;strong&gt;Downloads&lt;/strong&gt; (Datasets, Models) &amp;amp; &lt;strong&gt;Commands&lt;/strong&gt; used to produce the datasets, models &amp;amp; other things. Generally, you can find all models at &lt;a href=&#34;https://huggingface.co/Muennighoff&#34;&gt;https://huggingface.co/Muennighoff&lt;/a&gt; and json results in various datasets at &lt;a href=&#34;https://www.kaggle.com/muennighoff/datasets&#34;&gt;https://www.kaggle.com/muennighoff/datasets&lt;/a&gt;. Model names are explained in their Huggingface READMEs. Dataset names are explained in the sub-folders of this repository.&lt;/p&gt; &#xA;&lt;h2&gt;Use SGPT with Huggingface&lt;/h2&gt; &#xA;&lt;p&gt;Below we provide python examples to use the pre-trained models for your own semantic search use case. We highly recommend replacing the model names with larger models, e.g. &lt;code&gt;Muennighoff/SGPT-5.8B-weightedmean-nli-bitfit&lt;/code&gt; for biencoder/symmetric.&lt;/p&gt; &#xA;&lt;h3&gt;Bi-Encoder&lt;/h3&gt; &#xA;&lt;h4&gt;Symmetric Semantic Search BE&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from transformers import AutoModel, AutoTokenizer&#xA;from scipy.spatial.distance import cosine&#xA;&#xA;# Get our models - The package will take care of downloading the models automatically&#xA;# For best performance: Muennighoff/SGPT-5.8B-weightedmean-nli-bitfit&#xA;tokenizer = AutoTokenizer.from_pretrained(&#34;Muennighoff/SGPT-125M-weightedmean-nli-bitfit&#34;)&#xA;model = AutoModel.from_pretrained(&#34;Muennighoff/SGPT-125M-weightedmean-nli-bitfit&#34;)&#xA;# Deactivate Dropout (There is no dropout in the above models so it makes no difference here but other SGPT models may have dropout)&#xA;model.eval()&#xA;&#xA;# Tokenize input texts&#xA;texts = [&#xA;    &#34;deep learning&#34;,&#xA;    &#34;artificial intelligence&#34;,&#xA;    &#34;deep diving&#34;,&#xA;    &#34;artificial snow&#34;,&#xA;]&#xA;batch_tokens = tokenizer(texts, padding=True, truncation=True, return_tensors=&#34;pt&#34;)&#xA;&#xA;# Get the embeddings&#xA;with torch.no_grad():&#xA;    # Get hidden state of shape [bs, seq_len, hid_dim]&#xA;    last_hidden_state = model(**batch_tokens, output_hidden_states=True, return_dict=True).last_hidden_state&#xA;&#xA;# Get weights of shape [bs, seq_len, hid_dim]&#xA;weights = (&#xA;    torch.arange(start=1, end=last_hidden_state.shape[1] + 1)&#xA;    .unsqueeze(0)&#xA;    .unsqueeze(-1)&#xA;    .expand(last_hidden_state.size())&#xA;    .float().to(last_hidden_state.device)&#xA;)&#xA;&#xA;# Get attn mask of shape [bs, seq_len, hid_dim]&#xA;input_mask_expanded = (&#xA;    batch_tokens[&#34;attention_mask&#34;]&#xA;    .unsqueeze(-1)&#xA;    .expand(last_hidden_state.size())&#xA;    .float()&#xA;)&#xA;&#xA;# Perform weighted mean pooling across seq_len: bs, seq_len, hidden_dim -&amp;gt; bs, hidden_dim&#xA;sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded * weights, dim=1)&#xA;sum_mask = torch.sum(input_mask_expanded * weights, dim=1)&#xA;&#xA;embeddings = sum_embeddings / sum_mask&#xA;&#xA;# Calculate cosine similarities&#xA;# Cosine similarities are in [-1, 1]. Higher means more similar&#xA;cosine_sim_0_1 = 1 - cosine(embeddings[0], embeddings[1])&#xA;cosine_sim_0_2 = 1 - cosine(embeddings[0], embeddings[2])&#xA;cosine_sim_0_3 = 1 - cosine(embeddings[0], embeddings[3])&#xA;&#xA;print(&#34;Cosine similarity between \&#34;%s\&#34; and \&#34;%s\&#34; is: %.3f&#34; % (texts[0], texts[1], cosine_sim_0_1))&#xA;print(&#34;Cosine similarity between \&#34;%s\&#34; and \&#34;%s\&#34; is: %.3f&#34; % (texts[0], texts[2], cosine_sim_0_2))&#xA;print(&#34;Cosine similarity between \&#34;%s\&#34; and \&#34;%s\&#34; is: %.3f&#34; % (texts[0], texts[3], cosine_sim_0_3))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Asymmetric Semantic Search BE&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from transformers import AutoModel, AutoTokenizer&#xA;from scipy.spatial.distance import cosine&#xA;&#xA;# Get our models - The package will take care of downloading the models automatically&#xA;# For best performance: Muennighoff/SGPT-5.8B-weightedmean-msmarco-specb-bitfit&#xA;tokenizer = AutoTokenizer.from_pretrained(&#34;Muennighoff/SGPT-125M-weightedmean-msmarco-specb-bitfit&#34;)&#xA;model = AutoModel.from_pretrained(&#34;Muennighoff/SGPT-125M-weightedmean-msmarco-specb-bitfit&#34;)&#xA;# Deactivate Dropout (There is no dropout in the above models so it makes no difference here but other SGPT models may have dropout)&#xA;model.eval()&#xA;&#xA;queries = [&#xA;    &#34;I&#39;m searching for a planet not too far from Earth.&#34;,&#xA;]&#xA;&#xA;docs = [&#xA;    &#34;Neptune is the eighth and farthest-known Solar planet from the Sun. In the Solar System, it is the fourth-largest planet by diameter, the third-most-massive planet, and the densest giant planet. It is 17 times the mass of Earth, slightly more massive than its near-twin Uranus.&#34;,&#xA;    &#34;TRAPPIST-1d, also designated as 2MASS J23062928-0502285 d, is a small exoplanet (about 30% the mass of the earth), which orbits on the inner edge of the habitable zone of the ultracool dwarf star TRAPPIST-1 approximately 40 light-years (12.1 parsecs, or nearly 3.7336×1014 km) away from Earth in the constellation of Aquarius.&#34;,&#xA;    &#34;A harsh desert world orbiting twin suns in the galaxy’s Outer Rim, Tatooine is a lawless place ruled by Hutt gangsters. Many settlers scratch out a living on moisture farms, while spaceport cities such as Mos Eisley and Mos Espa serve as home base for smugglers, criminals, and other rogues.&#34;,&#xA;]&#xA;&#xA;SPECB_QUE_BOS = tokenizer.encode(&#34;[&#34;, add_special_tokens=False)[0]&#xA;SPECB_QUE_EOS = tokenizer.encode(&#34;]&#34;, add_special_tokens=False)[0]&#xA;&#xA;SPECB_DOC_BOS = tokenizer.encode(&#34;{&#34;, add_special_tokens=False)[0]&#xA;SPECB_DOC_EOS = tokenizer.encode(&#34;}&#34;, add_special_tokens=False)[0]&#xA;&#xA;&#xA;def tokenize_with_specb(texts, is_query):&#xA;    # Tokenize without padding&#xA;    batch_tokens = tokenizer(texts, padding=False, truncation=True)   &#xA;    # Add special brackets &amp;amp; pay attention to them&#xA;    for seq, att in zip(batch_tokens[&#34;input_ids&#34;], batch_tokens[&#34;attention_mask&#34;]):&#xA;        if is_query:&#xA;            seq.insert(0, SPECB_QUE_BOS)&#xA;            seq.append(SPECB_QUE_EOS)&#xA;        else:&#xA;            seq.insert(0, SPECB_DOC_BOS)&#xA;            seq.append(SPECB_DOC_EOS)&#xA;        att.insert(0, 1)&#xA;        att.append(1)&#xA;    # Add padding&#xA;    batch_tokens = tokenizer.pad(batch_tokens, padding=True, return_tensors=&#34;pt&#34;)&#xA;    return batch_tokens&#xA;&#xA;def get_weightedmean_embedding(batch_tokens, model):&#xA;    # Get the embeddings&#xA;    with torch.no_grad():&#xA;        # Get hidden state of shape [bs, seq_len, hid_dim]&#xA;        last_hidden_state = model(**batch_tokens, output_hidden_states=True, return_dict=True).last_hidden_state&#xA;&#xA;    # Get weights of shape [bs, seq_len, hid_dim]&#xA;    weights = (&#xA;        torch.arange(start=1, end=last_hidden_state.shape[1] + 1)&#xA;        .unsqueeze(0)&#xA;        .unsqueeze(-1)&#xA;        .expand(last_hidden_state.size())&#xA;        .float().to(last_hidden_state.device)&#xA;    )&#xA;&#xA;    # Get attn mask of shape [bs, seq_len, hid_dim]&#xA;    input_mask_expanded = (&#xA;        batch_tokens[&#34;attention_mask&#34;]&#xA;        .unsqueeze(-1)&#xA;        .expand(last_hidden_state.size())&#xA;        .float()&#xA;    )&#xA;&#xA;    # Perform weighted mean pooling across seq_len: bs, seq_len, hidden_dim -&amp;gt; bs, hidden_dim&#xA;    sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded * weights, dim=1)&#xA;    sum_mask = torch.sum(input_mask_expanded * weights, dim=1)&#xA;&#xA;    embeddings = sum_embeddings / sum_mask&#xA;&#xA;    return embeddings&#xA;&#xA;&#xA;query_embeddings = get_weightedmean_embedding(tokenize_with_specb(queries, is_query=True), model)&#xA;doc_embeddings = get_weightedmean_embedding(tokenize_with_specb(docs, is_query=False), model)&#xA;&#xA;# Calculate cosine similarities&#xA;# Cosine similarities are in [-1, 1]. Higher means more similar&#xA;cosine_sim_0_1 = 1 - cosine(query_embeddings[0], doc_embeddings[0])&#xA;cosine_sim_0_2 = 1 - cosine(query_embeddings[0], doc_embeddings[1])&#xA;cosine_sim_0_3 = 1 - cosine(query_embeddings[0], doc_embeddings[2])&#xA;&#xA;print(&#34;Cosine similarity between \&#34;%s\&#34; and \&#34;%s\&#34; is: %.3f&#34; % (queries[0], docs[0][:20] + &#34;...&#34;, cosine_sim_0_1))&#xA;print(&#34;Cosine similarity between \&#34;%s\&#34; and \&#34;%s\&#34; is: %.3f&#34; % (queries[0], docs[1][:20] + &#34;...&#34;, cosine_sim_0_2))&#xA;print(&#34;Cosine similarity between \&#34;%s\&#34; and \&#34;%s\&#34; is: %.3f&#34; % (queries[0], docs[2][:20] + &#34;...&#34;, cosine_sim_0_3))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Cross-Encoder&lt;/h3&gt; &#xA;&lt;h4&gt;Asymmetric Semantic Search CE&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from transformers import AutoModelForCausalLM, AutoTokenizer&#xA;from scipy.spatial.distance import cosine&#xA;&#xA;# Get models - The package will take care of downloading the models automatically&#xA;# For best performance: EleutherAI/gpt-j-6B&#xA;tokenizer = AutoTokenizer.from_pretrained(&#34;EleutherAI/gpt-neo-125M&#34;)&#xA;model = AutoModelForCausalLM.from_pretrained(&#34;EleutherAI/gpt-neo-125M&#34;)&#xA;# Deactivate Dropout (There is no dropout in the above models so it makes no difference here but other SGPT models may have dropout)&#xA;model.eval()&#xA;&#xA;prompt = &#39;Documents are searched to find matches with the same content.\nThe document &#34;{}&#34; is a good search result for &#34;&#39;&#xA;&#xA;queries = [&#xA;    &#34;I&#39;m searching for a planet not too far from Earth.&#34;,&#xA;]&#xA;&#xA;docs = [&#xA;    &#34;Neptune is the eighth and farthest-known Solar planet from the Sun. In the Solar System, it is the fourth-largest planet by diameter, the third-most-massive planet, and the densest giant planet. It is 17 times the mass of Earth, slightly more massive than its near-twin Uranus.&#34;,&#xA;    &#34;TRAPPIST-1d, also designated as 2MASS J23062928-0502285 d, is a small exoplanet (about 30% the mass of the earth), which orbits on the inner edge of the habitable zone of the ultracool dwarf star TRAPPIST-1 approximately 40 light-years (12.1 parsecs, or nearly 3.7336×1014 km) away from Earth in the constellation of Aquarius.&#34;,&#xA;    &#34;A harsh desert world orbiting twin suns in the galaxy’s Outer Rim, Tatooine is a lawless place ruled by Hutt gangsters. Many settlers scratch out a living on moisture farms, while spaceport cities such as Mos Eisley and Mos Espa serve as home base for smugglers, criminals, and other rogues.&#34;,&#xA;]&#xA;&#xA;for query in queries:&#xA;    print(f&#34;Query: {query}&#34;)&#xA;    for doc in docs:&#xA;        context = prompt.format(doc)&#xA;&#xA;        context_enc = tokenizer.encode(context, add_special_tokens=False)&#xA;        continuation_enc = tokenizer.encode(query, add_special_tokens=False)&#xA;        # Slice off the last token, as we take its probability from the one before&#xA;        model_input = torch.tensor(context_enc+continuation_enc[:-1])&#xA;        continuation_len = len(continuation_enc)&#xA;        input_len, = model_input.shape&#xA;&#xA;        # [seq_len] -&amp;gt; [seq_len, vocab]&#xA;        logprobs = torch.nn.functional.log_softmax(model(model_input)[0], dim=-1).cpu()&#xA;        # [seq_len, vocab] -&amp;gt; [continuation_len, vocab]&#xA;        logprobs = logprobs[input_len-continuation_len:]&#xA;        # Gather the log probabilities of the continuation tokens -&amp;gt; [continuation_len]&#xA;        logprobs = torch.gather(logprobs, 1, torch.tensor(continuation_enc).unsqueeze(-1)).squeeze(-1)&#xA;        score = torch.sum(logprobs)&#xA;        # The higher (closer to 0), the more similar&#xA;        print(f&#34;Document: {doc[:20] + &#39;...&#39;} Score: {score}&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Symmetric Semantic Search CE&lt;/h4&gt; &#xA;&lt;p&gt;You can use the same code as in the above &lt;a href=&#34;https://raw.githubusercontent.com/Muennighoff/sgpt/main/#asymmetric-semantic-search-1&#34;&gt;CE-Asym section&lt;/a&gt; but change the prompt. Feel free to share prompts that work well :)&lt;/p&gt; &#xA;&lt;h2&gt;Use SGPT with Sentence Transformers&lt;/h2&gt; &#xA;&lt;h3&gt;Bi-Encoder ST&lt;/h3&gt; &#xA;&lt;h4&gt;Symmetric Semantic Search BE ST&lt;/h4&gt; &#xA;&lt;p&gt;Symmetric models are now 100% compatible with the latest &lt;a href=&#34;https://github.com/UKPLab/sentence-transformers&#34;&gt;sentence-transformers&lt;/a&gt; via &lt;code&gt;pip install git+https://github.com/UKPLab/sentence-transformers.git&lt;/code&gt;. You should get the same results as in &lt;a href=&#34;https://raw.githubusercontent.com/Muennighoff/sgpt/main/#symmetric-semantic-search-be&#34;&gt;the HuggingFace script above.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.spatial.distance import cosine&#xA;from sentence_transformers import SentenceTransformer&#xA;&#xA;texts = [&#xA;    &#34;deep learning&#34;,&#xA;    &#34;artificial intelligence&#34;,&#xA;    &#34;deep diving&#34;,&#xA;    &#34;artificial snow&#34;,&#xA;]&#xA;&#xA;model = SentenceTransformer(&#34;Muennighoff/SGPT-125M-weightedmean-nli-bitfit&#34;)&#xA;embeddings = model.encode(texts)&#xA;&#xA;cosine_sim_0_1 = 1 - cosine(embeddings[0], embeddings[1])&#xA;cosine_sim_0_2 = 1 - cosine(embeddings[0], embeddings[2])&#xA;cosine_sim_0_3 = 1 - cosine(embeddings[0], embeddings[3])&#xA;&#xA;print(&#34;Cosine similarity between \&#34;%s\&#34; and \&#34;%s\&#34; is: %.3f&#34; % (texts[0], texts[1], cosine_sim_0_1))&#xA;print(&#34;Cosine similarity between \&#34;%s\&#34; and \&#34;%s\&#34; is: %.3f&#34; % (texts[0], texts[2], cosine_sim_0_2))&#xA;print(&#34;Cosine similarity between \&#34;%s\&#34; and \&#34;%s\&#34; is: %.3f&#34; % (texts[0], texts[3], cosine_sim_0_3))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Asymmetric Semantic Search BE ST&lt;/h4&gt; &#xA;&lt;h5&gt;SGPT Sentence Transformers&lt;/h5&gt; &#xA;&lt;p&gt;Install: &lt;code&gt;pip install --upgrade git+https://github.com/Muennighoff/sentence-transformers.git@sgpt_poolings_specb&lt;/code&gt; Use the below, which produces the exact same scores as the &lt;a href=&#34;https://raw.githubusercontent.com/Muennighoff/sgpt/main/#asymmetric-semantic-search-be&#34;&gt;HuggingFace solution above.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.spatial.distance import cosine&#xA;from sentence_transformers import SentenceTransformer&#xA;&#xA;queries = [&#xA;    &#34;I&#39;m searching for a planet not too far from Earth.&#34;,&#xA;]&#xA;&#xA;docs = [&#xA;    &#34;Neptune is the eighth and farthest-known Solar planet from the Sun. In the Solar System, it is the fourth-largest planet by diameter, the third-most-massive planet, and the densest giant planet. It is 17 times the mass of Earth, slightly more massive than its near-twin Uranus.&#34;,&#xA;    &#34;TRAPPIST-1d, also designated as 2MASS J23062928-0502285 d, is a small exoplanet (about 30% the mass of the earth), which orbits on the inner edge of the habitable zone of the ultracool dwarf star TRAPPIST-1 approximately 40 light-years (12.1 parsecs, or nearly 3.7336×1014 km) away from Earth in the constellation of Aquarius.&#34;,&#xA;    &#34;A harsh desert world orbiting twin suns in the galaxy’s Outer Rim, Tatooine is a lawless place ruled by Hutt gangsters. Many settlers scratch out a living on moisture farms, while spaceport cities such as Mos Eisley and Mos Espa serve as home base for smugglers, criminals, and other rogues.&#34;,&#xA;]&#xA;&#xA;class SentenceTransformerSpecb(SentenceTransformer):&#xA;    # Requires:&#xA;    # pip install git+https://github.com/Muennighoff/sentence-transformers.git@sgpt_poolings_specb&#xA;    def __init__(self, *args, **kwargs):&#xA;        super().__init__(*args, **kwargs)&#xA;        tokens = [&#34;[SOS]&#34;, &#34;{SOS}&#34;]&#xA;        self._first_module().tokenizer.add_tokens(tokens, special_tokens=True)&#xA;        self._first_module().auto_model.resize_token_embeddings(len(self._first_module().tokenizer))&#xA;        # Will be replaced with the rep tokens in the model ones&#xA;        # The problem is we don&#39;t know if a text is query or document when tokenizing in the Transformer.py module, &#xA;        # so we use the SOS tokens as an identifier if we have a query or document at hand &amp;amp; then replace them&#xA;        # If we would directly use the brackets here, they may become part of another token&#xA;        self._first_module().bos_spec_token_q = self._first_module().tokenizer.encode(&#34;[SOS]&#34;, add_special_tokens=False)[0]&#xA;        self._first_module().bos_spec_token_d = self._first_module().tokenizer.encode(&#34;{SOS}&#34;, add_special_tokens=False)[0]&#xA;        self._first_module().bos_spec_token_q_rep = self._first_module().tokenizer.encode(&#34;[&#34;, add_special_tokens=False)[0]&#xA;        self._first_module().eos_spec_token_q = self._first_module().tokenizer.encode(&#34;]&#34;, add_special_tokens=False)[0]&#xA;        self._first_module().bos_spec_token_d_rep = self._first_module().tokenizer.encode(&#34;{&#34;, add_special_tokens=False)[0]&#xA;        self._first_module().eos_spec_token_d = self._first_module().tokenizer.encode(&#34;}&#34;, add_special_tokens=False)[0]&#xA;        self._first_module().replace_bos = True&#xA;&#xA;    def encode(self, sentences, **kwargs):&#xA;        is_query = kwargs.pop(&#34;is_query&#34;, True)&#xA;        if is_query:&#xA;            sentences = &#34;[SOS]&#34; + sentences if isinstance(sentences, str) else [&#34;[SOS]&#34; + sent for sent in sentences]&#xA;        else:&#xA;            sentences = &#34;{SOS}&#34; + sentences if isinstance(sentences, str) else [&#34;{SOS}&#34; + sent for sent in sentences]    &#xA;        return super().encode(sentences, **kwargs)&#xA;        &#xA;model = SentenceTransformerSpecb(&#34;Muennighoff/SGPT-125M-weightedmean-msmarco-specb-bitfit&#34;)&#xA;&#xA;query_embeddings = model.encode(queries, is_query=True)&#xA;doc_embeddings = model.encode(docs, is_query=False)&#xA;&#xA;# Calculate cosine similarities&#xA;# Cosine similarities are in [-1, 1]. Higher means more similar&#xA;cosine_sim_0_1 = 1 - cosine(query_embeddings[0], doc_embeddings[0])&#xA;cosine_sim_0_2 = 1 - cosine(query_embeddings[0], doc_embeddings[1])&#xA;cosine_sim_0_3 = 1 - cosine(query_embeddings[0], doc_embeddings[2])&#xA;&#xA;print(&#34;Cosine similarity between \&#34;%s\&#34; and \&#34;%s\&#34; is: %.3f&#34; % (queries[0], docs[0][:20] + &#34;...&#34;, cosine_sim_0_1))&#xA;print(&#34;Cosine similarity between \&#34;%s\&#34; and \&#34;%s\&#34; is: %.3f&#34; % (queries[0], docs[1][:20] + &#34;...&#34;, cosine_sim_0_2))&#xA;print(&#34;Cosine similarity between \&#34;%s\&#34; and \&#34;%s\&#34; is: %.3f&#34; % (queries[0], docs[2][:20] + &#34;...&#34;, cosine_sim_0_3))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Original Sentence Transformers&lt;/h5&gt; &#xA;&lt;p&gt;If you want to use the Sentence Transformers at &lt;code&gt;https://github.com/UKPLab/sentence-transformers&lt;/code&gt;, you can use the below. Make sure to use the latest version (&lt;code&gt;pip install --upgrade git+https://github.com/UKPLab/sentence-transformers.git&lt;/code&gt;). Note that this will produce slightly worse scores than &lt;a href=&#34;https://raw.githubusercontent.com/Muennighoff/sgpt/main/#sgpt-sentence-transformers&#34;&gt;SGPT Sentence Transformers&lt;/a&gt;, as the special brackets may get intermingled with other tokens upon tokenization. On SciFact (BEIR) NDCG@10 of the below decreases to 0.566 from 0.569 for &lt;code&gt;SGPT-125M-weightedmean-msmarco-specb-bitfit&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.spatial.distance import cosine&#xA;from sentence_transformers import SentenceTransformer&#xA;&#xA;queries = [&#xA;    &#34;I&#39;m searching for a planet not too far from Earth.&#34;,&#xA;]&#xA;&#xA;docs = [&#xA;    &#34;Neptune is the eighth and farthest-known Solar planet from the Sun. In the Solar System, it is the fourth-largest planet by diameter, the third-most-massive planet, and the densest giant planet. It is 17 times the mass of Earth, slightly more massive than its near-twin Uranus.&#34;,&#xA;    &#34;TRAPPIST-1d, also designated as 2MASS J23062928-0502285 d, is a small exoplanet (about 30% the mass of the earth), which orbits on the inner edge of the habitable zone of the ultracool dwarf star TRAPPIST-1 approximately 40 light-years (12.1 parsecs, or nearly 3.7336×1014 km) away from Earth in the constellation of Aquarius.&#34;,&#xA;    &#34;A harsh desert world orbiting twin suns in the galaxy’s Outer Rim, Tatooine is a lawless place ruled by Hutt gangsters. Many settlers scratch out a living on moisture farms, while spaceport cities such as Mos Eisley and Mos Espa serve as home base for smugglers, criminals, and other rogues.&#34;,&#xA;]&#xA;&#xA;class SentenceTransformerSpecb(SentenceTransformer):&#xA;    def encode(self, sentences, **kwargs):&#xA;        is_query = kwargs.pop(&#34;is_query&#34;, True)&#xA;        if is_query:&#xA;            sentences = &#34;[&#34; + sentences + &#34;]&#34; if isinstance(sentences, str) else [&#34;[&#34; + sent + &#34;]&#34; for sent in sentences]&#xA;        else:&#xA;            sentences = &#34;{&#34; + sentences + &#34;}&#34; if isinstance(sentences, str) else [&#34;{&#34; + sent + &#34;}&#34; for sent in sentences]    &#xA;        return super().encode(sentences, **kwargs)&#xA;        &#xA;model = SentenceTransformerSpecb(&#34;Muennighoff/SGPT-125M-weightedmean-msmarco-specb-bitfit&#34;)&#xA;&#xA;query_embeddings = model.encode(queries, is_query=True)&#xA;doc_embeddings = model.encode(docs, is_query=False)&#xA;&#xA;# Calculate cosine similarities&#xA;# Cosine similarities are in [-1, 1]. Higher means more similar&#xA;cosine_sim_0_1 = 1 - cosine(query_embeddings[0], doc_embeddings[0])&#xA;cosine_sim_0_2 = 1 - cosine(query_embeddings[0], doc_embeddings[1])&#xA;cosine_sim_0_3 = 1 - cosine(query_embeddings[0], doc_embeddings[2])&#xA;&#xA;print(&#34;Cosine similarity between \&#34;%s\&#34; and \&#34;%s\&#34; is: %.3f&#34; % (queries[0], docs[0][:20] + &#34;...&#34;, cosine_sim_0_1))&#xA;print(&#34;Cosine similarity between \&#34;%s\&#34; and \&#34;%s\&#34; is: %.3f&#34; % (queries[0], docs[1][:20] + &#34;...&#34;, cosine_sim_0_2))&#xA;print(&#34;Cosine similarity between \&#34;%s\&#34; and \&#34;%s\&#34; is: %.3f&#34; % (queries[0], docs[2][:20] + &#34;...&#34;, cosine_sim_0_3))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;We thank Constantin Eichenberg and Samuel Weinbach for insightful discussions and valuable feedback throughout the project. We thank Robert Baldock, Marco Bellagente and Koen Oostermeijer for reading drafts of the paper. This work has been supported by OpenAI under the academic access program. This work would not have been possible without:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;UKPLab: &lt;a href=&#34;https://github.com/UKPLab/sentence-transformers&#34;&gt;SBERT&lt;/a&gt;, &lt;a href=&#34;https://github.com/UKPLab/beir&#34;&gt;BEIR&lt;/a&gt;, &lt;a href=&#34;https://github.com/UKPLab/useb&#34;&gt;USEB&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/EleutherAI/gpt-neox&#34;&gt;Eleuther AI Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;Huggingface Transformers&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Feel free to cite our paper if SGPT is helpful to you :)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{muennighoff2022sgpt,&#xA;  title={SGPT: GPT Sentence Embeddings for Semantic Search},&#xA;  author={Muennighoff, Niklas},&#xA;  journal={arXiv preprint arXiv:2202.08904},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>