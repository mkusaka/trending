<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-08-24T01:40:37Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>fengdu78/WZU-machine-learning-course</title>
    <updated>2022-08-24T01:40:37Z</updated>
    <id>tag:github.com,2022-08-24:/fengdu78/WZU-machine-learning-course</id>
    <link href="https://github.com/fengdu78/WZU-machine-learning-course" rel="alternate"></link>
    <summary type="html">&lt;p&gt;温州大学《机器学习》课程资料（代码、课件等）&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;机器学习课程-温州大学&lt;/h1&gt; &#xA;&lt;p&gt;本学期我给研究生上机器学习课程，我把课件分享下，后续陆续更新。&lt;/p&gt; &#xA;&lt;p&gt;课程登陆了中国大学慕课平台，已经在2021年9月6日开课。&lt;/p&gt; &#xA;&lt;p&gt;课程地址：&lt;a href=&#34;https://www.icourse163.org/course/WZU-1464096179&#34;&gt;https://www.icourse163.org/course/WZU-1464096179&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;如果有老师需要ppt原版文件，请联系我：&lt;a href=&#34;mailto:haiguang2000@wzu.edu.cn&#34;&gt;haiguang2000@wzu.edu.cn&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;（邮件较多，请告知姓名、学校，我都会回复。）&lt;/p&gt; &#xA;&lt;p&gt;黄海广&lt;/p&gt; &#xA;&lt;h2&gt;目录说明&lt;/h2&gt; &#xA;&lt;p&gt;ppt：课程的课件&lt;/p&gt; &#xA;&lt;p&gt;code：课程的代码（Jupyter notebook格式）&lt;/p&gt; &#xA;&lt;p&gt;video：课程的视频&lt;/p&gt; &#xA;&lt;p&gt;DeepLearning：《深度学习》课程的ppt&lt;/p&gt; &#xA;&lt;p&gt;PPT的百度云地址（链接: &lt;a href=&#34;https://pan.baidu.com/s/1xjHdEpg6uqe27I_XltAn_w&#34;&gt;https://pan.baidu.com/s/1xjHdEpg6uqe27I_XltAn_w&lt;/a&gt; 提取码: ikhf ）&lt;/p&gt; &#xA;&lt;p&gt;内容首发于微信公众号：机器学习初学者 &lt;img src=&#34;https://raw.githubusercontent.com/fengdu78/WZU-machine-learning-course/main/images/gongzhong.jpg&#34; alt=&#34;gongzhong&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>meituan/YOLOv6</title>
    <updated>2022-08-24T01:40:37Z</updated>
    <id>tag:github.com,2022-08-24:/meituan/YOLOv6</id>
    <link href="https://github.com/meituan/YOLOv6" rel="alternate"></link>
    <summary type="html">&lt;p&gt;YOLOv6: a single-stage object detection framework dedicated to industrial applications.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;YOLOv6&lt;/h1&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;YOLOv6 is a single-stage object detection framework dedicated to industrial applications, with hardware-friendly efficient design and high performance.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/meituan/YOLOv6/main/assets/picture.png&#34; width=&#34;800&#34;&gt; &#xA;&lt;p&gt;YOLOv6-nano achieves 35.0 mAP on COCO val2017 dataset with 1242 FPS on T4 using TensorRT FP16 for bs32 inference, and YOLOv6-s achieves 43.1 mAP on COCO val2017 dataset with 520 FPS on T4 using TensorRT FP16 for bs32 inference.&lt;/p&gt; &#xA;&lt;p&gt;YOLOv6 is composed of the following methods:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Hardware-friendly Design for Backbone and Neck&lt;/li&gt; &#xA; &lt;li&gt;Efficient Decoupled Head with SIoU Loss&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Coming soon&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; YOLOv6 m/l/x model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Deployment for MNN/TNN/NCNN/CoreML...&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Quantization tools&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;h3&gt;Install&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/meituan/YOLOv6&#xA;cd YOLOv6&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Inference&lt;/h3&gt; &#xA;&lt;p&gt;First, download a pretrained model from the YOLOv6 &lt;a href=&#34;https://github.com/meituan/YOLOv6/releases/tag/0.1.0&#34;&gt;release&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Second, run inference with &lt;code&gt;tools/infer.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tools/infer.py --weights yolov6s.pt --source img.jpg / imgdir&#xA;                                yolov6n.pt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;p&gt;Single GPU&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tools/train.py --batch 32 --conf configs/yolov6s.py --data data/coco.yaml --device 0&#xA;                                        configs/yolov6n.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Multi GPUs (DDP mode recommended)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m torch.distributed.launch --nproc_per_node 8 tools/train.py --batch 256 --conf configs/yolov6s.py --data data/coco.yaml --device 0,1,2,3,4,5,6,7&#xA;                                                                                        configs/yolov6n.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;conf: select config file to specify network/optimizer/hyperparameters&lt;/li&gt; &#xA; &lt;li&gt;data: prepare &lt;a href=&#34;http://cocodataset.org&#34;&gt;COCO&lt;/a&gt; dataset, &lt;a href=&#34;https://github.com/meituan/YOLOv6/releases/download/0.1.0/coco2017labels.zip&#34;&gt;YOLO format coco labels&lt;/a&gt; and specify dataset paths in data.yaml&lt;/li&gt; &#xA; &lt;li&gt;make sure your dataset structure as follows:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;├── coco&#xA;│   ├── annotations&#xA;│   │   ├── instances_train2017.json&#xA;│   │   └── instances_val2017.json&#xA;│   ├── images&#xA;│   │   ├── train2017&#xA;│   │   └── val2017&#xA;│   ├── labels&#xA;│   │   ├── train2017&#xA;│   │   ├── val2017&#xA;│   ├── LICENSE&#xA;│   ├── README.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;Reproduce mAP on COCO val2017 dataset&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tools/eval.py --data data/coco.yaml --batch 32 --weights yolov6s.pt --task val&#xA;                                                                yolov6n.pt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Resume&lt;/h3&gt; &#xA;&lt;p&gt;If your training process is corrupted, you can resume training by&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# single GPU traning.&#xA;python tools/train.py --resume&#xA;# multi GPU training.&#xA;python -m torch.distributed.launch --nproc_per_node 8 tools/train.py --resume&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Your can also specify a checkpoint path to &lt;code&gt;--resume&lt;/code&gt; parameter by&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# remember replace /path/to/your/checkpoint/path to the checkpoint path which you want to resume training.&#xA;--resume /path/to/your/checkpoint/path&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Deployment&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meituan/YOLOv6/main/deploy/ONNX&#34;&gt;ONNX&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meituan/YOLOv6/main/deploy/ONNX/OpenCV&#34;&gt;OpenCV Python/C++&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meituan/YOLOv6/main/deploy/OpenVINO&#34;&gt;OpenVINO&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meituan/YOLOv6/main/tools/partial_quantization&#34;&gt;Partial Quantization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meituan/YOLOv6/main/deploy/TensorRT&#34;&gt;TensorRT&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Tutorials&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meituan/YOLOv6/main/docs/Train_custom_data.md&#34;&gt;Train custom data&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meituan/YOLOv6/main/docs/Test_speed.md&#34;&gt;Test speed&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meituan/YOLOv6/main/docs/tutorial_repopt.md&#34;&gt;Tutorial of RepOpt for YOLOv6&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Benchmark&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Size&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;mAP&lt;sup&gt;val&lt;br&gt;0.5:0.95&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Speed&lt;sup&gt;V100&lt;br&gt;fp16 b32 &lt;br&gt;(ms)&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Speed&lt;sup&gt;V100&lt;br&gt;fp32 b32 &lt;br&gt;(ms)&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Speed&lt;sup&gt;T4&lt;br&gt;trt fp16 b1 &lt;br&gt;(fps)&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Speed&lt;sup&gt;T4&lt;br&gt;trt fp16 b32 &lt;br&gt;(fps)&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Params&lt;br&gt;&lt;sup&gt; (M)&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th&gt;FLOPs&lt;br&gt;&lt;sup&gt; (G)&lt;/sup&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/meituan/YOLOv6/releases/download/0.1.0/yolov6n.pt&#34;&gt;&lt;strong&gt;YOLOv6-n&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;416&lt;br&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;30.8&lt;br&gt;35.0&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;0.3&lt;br&gt;0.5&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;0.4&lt;br&gt;0.7&lt;/td&gt; &#xA;   &lt;td&gt;1100&lt;br&gt;788&lt;/td&gt; &#xA;   &lt;td&gt;2716&lt;br&gt;1242&lt;/td&gt; &#xA;   &lt;td&gt;4.3&lt;br&gt;4.3&lt;/td&gt; &#xA;   &lt;td&gt;4.7&lt;br&gt;11.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/meituan/YOLOv6/releases/download/0.1.0/yolov6t.pt&#34;&gt;&lt;strong&gt;YOLOv6-tiny&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;41.3&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;0.9&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;1.5&lt;/td&gt; &#xA;   &lt;td&gt;425&lt;/td&gt; &#xA;   &lt;td&gt;602&lt;/td&gt; &#xA;   &lt;td&gt;15.0&lt;/td&gt; &#xA;   &lt;td&gt;36.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/meituan/YOLOv6/releases/download/0.1.0/yolov6s.pt&#34;&gt;&lt;strong&gt;YOLOv6-s&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;43.1&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;1.0&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;1.7&lt;/td&gt; &#xA;   &lt;td&gt;373&lt;/td&gt; &#xA;   &lt;td&gt;520&lt;/td&gt; &#xA;   &lt;td&gt;17.2&lt;/td&gt; &#xA;   &lt;td&gt;44.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Comparisons of the mAP and speed of different object detectors are tested on &lt;a href=&#34;https://cocodataset.org/#download&#34;&gt;COCO val2017&lt;/a&gt; dataset.&lt;/li&gt; &#xA; &lt;li&gt;Refer to &lt;a href=&#34;https://raw.githubusercontent.com/meituan/YOLOv6/main/docs/Test_speed.md&#34;&gt;Test speed&lt;/a&gt; tutorial to reproduce the speed results of YOLOv6.&lt;/li&gt; &#xA; &lt;li&gt;Params and FLOPs of YOLOv6 are estimated on deployed model.&lt;/li&gt; &#xA; &lt;li&gt;Speed results of other methods are tested in our environment using official codebase and model if not found from the corresponding official release.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Third-party resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;YOLOv6 NCNN Android app demo: &lt;a href=&#34;https://github.com/FeiGeChuanShu/ncnn-android-yolov6&#34;&gt;ncnn-android-yolov6&lt;/a&gt; from &lt;a href=&#34;https://github.com/FeiGeChuanShu&#34;&gt;FeiGeChuanShu&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;YOLOv6 ONNXRuntime/MNN/TNN C++: &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/lite/ort/cv/yolov6.cpp&#34;&gt;YOLOv6-ORT&lt;/a&gt;, &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/lite/mnn/cv/mnn_yolov6.cpp&#34;&gt;YOLOv6-MNN&lt;/a&gt; and &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/lite/tnn/cv/tnn_yolov6.cpp&#34;&gt;YOLOv6-TNN&lt;/a&gt; from &lt;a href=&#34;https://github.com/DefTruth&#34;&gt;DefTruth&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;YOLOv6 TensorRT Python: &lt;a href=&#34;https://github.com/Linaom1214/tensorrt-python/raw/main/yolov6/trt.py&#34;&gt;yolov6-tensorrt-python&lt;/a&gt; from &lt;a href=&#34;https://github.com/Linaom1214&#34;&gt;Linaom1214&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;YOLOv6 TensorRT Windows C++: &lt;a href=&#34;https://github.com/zhiqwang/yolov5-rt-stack/tree/main/deployment/tensorrt-yolov6&#34;&gt;yolort&lt;/a&gt; from &lt;a href=&#34;https://github.com/Wulingtian&#34;&gt;Wei Zeng&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;YOLOv6 Quantization and Auto Compression Example &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSlim/tree/develop/example/auto_compression/pytorch_yolov6&#34;&gt;YOLOv6-ACT&lt;/a&gt; from &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSlim&#34;&gt;PaddleSlim&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/nateraw/yolov6&#34;&gt;YOLOv6 web demo&lt;/a&gt; on &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces&lt;/a&gt; with &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;. &lt;a href=&#34;https://huggingface.co/spaces/nateraw/yolov6&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Tutorial: &lt;a href=&#34;https://blog.roboflow.com/how-to-train-yolov6-on-a-custom-dataset/&#34;&gt;How to train YOLOv6 on a custom dataset&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1YnbqOinBZV-c9I7fk_UL6acgnnmkXDMM&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Demo of YOLOv6 inference on Google Colab &lt;a href=&#34;https://colab.research.google.com/github/mahdilamb/YOLOv6/blob/main/inference.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>rinongal/textual_inversion</title>
    <updated>2022-08-24T01:40:37Z</updated>
    <id>tag:github.com,2022-08-24:/rinongal/textual_inversion</id>
    <link href="https://github.com/rinongal/textual_inversion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2208.01618&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2208.01618-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://textual-inversion.github.io/&#34;&gt;Project Website&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion&lt;/strong&gt;&lt;br&gt; Rinon Gal&lt;sup&gt;1,2&lt;/sup&gt;, Yuval Alaluf&lt;sup&gt;1&lt;/sup&gt;, Yuval Atzmon&lt;sup&gt;2&lt;/sup&gt;, Or Patashnik&lt;sup&gt;1&lt;/sup&gt;, Amit H. Bermano&lt;sup&gt;1&lt;/sup&gt;, Gal Chechik&lt;sup&gt;2&lt;/sup&gt;, Daniel Cohen-Or&lt;sup&gt;1&lt;/sup&gt; &lt;br&gt; &lt;sup&gt;1&lt;/sup&gt;Tel Aviv University, &lt;sup&gt;2&lt;/sup&gt;NVIDIA&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;: &lt;br&gt; Text-to-image models offer unprecedented freedom to guide creation through natural language. Yet, it is unclear how such freedom can be exercised to generate images of specific unique concepts, modify their appearance, or compose them in new roles and novel scenes. In other words, we ask: how can we use language-guided models to turn &lt;i&gt;our&lt;/i&gt; cat into a painting, or imagine a new product based on &lt;i&gt;our&lt;/i&gt; favorite toy? Here we present a simple approach that allows such creative freedom. Using only 3-5 images of a user-provided concept, like an object or a style, we learn to represent it through new &#34;words&#34; in the embedding space of a frozen text-to-image model. These &#34;words&#34; can be composed into natural language sentences, guiding &lt;i&gt;personalized&lt;/i&gt; creation in an intuitive way. Notably, we find evidence that a &lt;i&gt;single&lt;/i&gt; word embedding is sufficient for capturing unique and varied concepts. We compare our approach to a wide range of baselines, and demonstrate that it can more faithfully portray the concepts across a range of applications and tasks.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Description&lt;/h2&gt; &#xA;&lt;p&gt;This repo contains the official code, data and sample inversions for our Textual Inversion paper.&lt;/p&gt; &#xA;&lt;h2&gt;Updates&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;21/08/2022 (C)&lt;/strong&gt; Code released!&lt;/p&gt; &#xA;&lt;h2&gt;TODO:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release code!&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Optimize gradient storing / checkpointing. Memory requirements, training times reduced by ~55%&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release data sets&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release pre-trained embeddings&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add Stable Diffusion support&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;Our code builds on, and shares requirements with &lt;a href=&#34;https://github.com/CompVis/latent-diffusion&#34;&gt;Latent Diffusion Models (LDM)&lt;/a&gt;. To set up their environment, please run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env create -f environment.yaml&#xA;conda activate ldm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You will also need the official LDM text-to-image checkpoint, available through the &lt;a href=&#34;https://github.com/CompVis/latent-diffusion&#34;&gt;LDM project page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Currently, the model can be downloaded by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mkdir -p models/ldm/text2img-large/&#xA;wget -O models/ldm/text2img-large/model.ckpt https://ommer-lab.com/files/latent-diffusion/nitro/txt2img-f8-large/model.ckpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Inversion&lt;/h3&gt; &#xA;&lt;p&gt;To invert an image set, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python main.py --base configs/latent-diffusion/txt2img-1p4B-finetune.yaml &#xA;               -t &#xA;               --actual_resume /path/to/pretrained/model.ckpt &#xA;               -n &amp;lt;run_name&amp;gt; &#xA;               --gpus 0, &#xA;               --data_root /path/to/directory/with/images&#xA;               --init_word &amp;lt;initialization_word&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where the initialization word should be a single-token rough description of the object (e.g., &#39;toy&#39;, &#39;painting&#39;, &#39;sculpture&#39;). If the input is comprised of more than a single token, you will be prompted to replace it.&lt;/p&gt; &#xA;&lt;p&gt;In the paper, we use 5k training iterations. However, some concepts (particularly styles) can converge much faster.&lt;/p&gt; &#xA;&lt;p&gt;To run on multiple GPUs, provide a comma-delimited list of GPU indices to the --gpus argument (e.g., &lt;code&gt;--gpus 0,3,7,8&lt;/code&gt;)&lt;/p&gt; &#xA;&lt;p&gt;Embeddings and output images will be saved in the log directory.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;code&gt;configs/latent-diffusion/txt2img-1p4B-finetune.yaml&lt;/code&gt; for more options, such as changing the placeholder string which denotes the concept (defaults to &#34;*&#34;)&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt; All training set images should be upright. If you are using phone captured images, check the inputs_gs*.jpg files in the output image directory and make sure they are oriented correctly. Many phones capture images with a 90 degree rotation and denote this in the image metadata. Windows parses these correctly, but PIL does not. Hence you will need to correct them manually (e.g. by pasting them into paint and re-saving) or wait until we add metadata parsing.&lt;/p&gt; &#xA;&lt;h3&gt;Generation&lt;/h3&gt; &#xA;&lt;p&gt;To generate new images of the learned concept, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/txt2img.py --ddim_eta 0.0 &#xA;                          --n_samples 8 &#xA;                          --n_iter 2 &#xA;                          --scale 10.0 &#xA;                          --ddim_steps 50 &#xA;                          --embedding_path /path/to/logs/trained_model/checkpoints/embeddings_gs-5049.pt &#xA;                          --ckpt_path /path/to/pretrained/model.ckpt &#xA;                          --prompt &#34;a photo of *&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where * is the placeholder string used during inversion.&lt;/p&gt; &#xA;&lt;h3&gt;Merging Checkpoints&lt;/h3&gt; &#xA;&lt;p&gt;LDM embedding checkpoints can be merged into a single file by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python merge_embeddings.py &#xA;--manager_ckpts /path/to/first/embedding.pt /path/to/second/embedding.pt [...]&#xA;--output_path /path/to/output/embedding.pt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If the checkpoints contain conflicting placeholder strings, you will be prompted to select new placeholders. The merged checkpoint can later be used to prompt multiple concepts at once (&#34;A photo of * in the style of @&#34;).&lt;/p&gt; &#xA;&lt;h3&gt;Pretrained Models / Data&lt;/h3&gt; &#xA;&lt;p&gt;Coming soon&lt;/p&gt; &#xA;&lt;h2&gt;Stable Diffusion&lt;/h2&gt; &#xA;&lt;p&gt;Stable Diffusion support is a work in progress and will be completed soon™.&lt;/p&gt; &#xA;&lt;h2&gt;Tips and Tricks&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Adding &#34;a photo of&#34; to the prompt usually results in better target consistency.&lt;/li&gt; &#xA; &lt;li&gt;Results can be seed sensititve. If you&#39;re unsatisfied with the model, try re-inverting with a new seed (by adding &lt;code&gt;--seed &amp;lt;#&amp;gt;&lt;/code&gt; to the prompt).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you make use of our work, please cite our paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{gal2022textual,&#xA;      doi = {10.48550/ARXIV.2208.01618},&#xA;      url = {https://arxiv.org/abs/2208.01618},&#xA;      author = {Gal, Rinon and Alaluf, Yuval and Atzmon, Yuval and Patashnik, Or and Bermano, Amit H. and Chechik, Gal and Cohen-Or, Daniel},&#xA;      title = {An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion},&#xA;      publisher = {arXiv},&#xA;      year = {2022},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Results&lt;/h2&gt; &#xA;&lt;p&gt;Here are some sample results. Please visit our &lt;a href=&#34;https://textual-inversion.github.io/&#34;&gt;project page&lt;/a&gt; or read our paper for more!&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rinongal/textual_inversion/main/img/teaser.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rinongal/textual_inversion/main/img/samples.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rinongal/textual_inversion/main/img/style.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;</summary>
  </entry>
</feed>