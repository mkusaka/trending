<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-07-07T01:32:36Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>NVIDIA/GenerativeAIExamples</title>
    <updated>2024-07-07T01:32:36Z</updated>
    <id>tag:github.com,2024-07-07:/NVIDIA/GenerativeAIExamples</id>
    <link href="https://github.com/NVIDIA/GenerativeAIExamples" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Generative AI reference workflows optimized for accelerated infrastructure and microservice architecture.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;NVIDIA Generative AI Examples&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://nvidia.github.io/GenerativeAIExamples/latest&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/documentation-blue.svg?sanitize=true&#34; alt=&#34;documentation&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;State-of-the-art Generative AI examples that are easy to deploy, test, and extend. All examples run on the high performance NVIDIA CUDA-X software stack and NVIDIA GPUs.&lt;/p&gt; &#xA;&lt;h2&gt;NVIDIA NGC&lt;/h2&gt; &#xA;&lt;p&gt;Generative AI Examples can use models and GPUs from the &lt;a href=&#34;https://catalog.ngc.nvidia.com&#34;&gt;NVIDIA API Catalog&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Sign up for a &lt;a href=&#34;https://ngc.nvidia.com/signin&#34;&gt;free NGC developer account&lt;/a&gt; to access:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GPU-optimized containers used in these examples&lt;/li&gt; &#xA; &lt;li&gt;Release notes and developer documentation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Retrieval Augmented Generation (RAG)&lt;/h2&gt; &#xA;&lt;p&gt;A RAG pipeline embeds multimodal data -- such as documents, images, and video -- into a database connected to a LLM. RAG lets users chat with their data!&lt;/p&gt; &#xA;&lt;h3&gt;Developer RAG Examples&lt;/h3&gt; &#xA;&lt;p&gt;The developer RAG examples run on a single VM. The examples demonstrate how to combine NVIDIA GPU acceleration with popular LLM programming frameworks using NVIDIA&#39;s &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/#open-source-integrations&#34;&gt;open source connectors&lt;/a&gt;. The examples are easy to deploy with &lt;a href=&#34;https://docs.docker.com/compose/&#34;&gt;Docker Compose&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Examples support local and remote inference endpoints. If you have a GPU, you can inference locally with an &lt;a href=&#34;https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nim/containers/nim_llm&#34;&gt;NVIDIA NIM for LLMs&lt;/a&gt;. If you don&#39;t have a GPU, you can inference and embed remotely with &lt;a href=&#34;https://build.nvidia.com/explore/discover&#34;&gt;NVIDIA API Catalog endpoints&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Embedding&lt;/th&gt; &#xA;   &lt;th&gt;Framework&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Multi-GPU&lt;/th&gt; &#xA;   &lt;th&gt;TRT-LLM&lt;/th&gt; &#xA;   &lt;th&gt;NVIDIA Endpoints&lt;/th&gt; &#xA;   &lt;th&gt;Triton&lt;/th&gt; &#xA;   &lt;th&gt;Vector Database&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;llama3-70b&lt;/td&gt; &#xA;   &lt;td&gt;snowflake-arctic-embed-l&lt;/td&gt; &#xA;   &lt;td&gt;LangChain&lt;/td&gt; &#xA;   &lt;td&gt;NVIDIA API Catalog endpoints chat bot [&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/RetrievalAugmentedGeneration/examples/nvidia_api_catalog/&#34;&gt;code&lt;/a&gt;, &lt;a href=&#34;https://nvidia.github.io/GenerativeAIExamples/latest/api-catalog.html&#34;&gt;docs&lt;/a&gt;]&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Milvus or pgvector&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;llama3-8b&lt;/td&gt; &#xA;   &lt;td&gt;snowflake-arctic-embed-l&lt;/td&gt; &#xA;   &lt;td&gt;LlamaIndex&lt;/td&gt; &#xA;   &lt;td&gt;Canonical QA Chatbot [&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/RetrievalAugmentedGeneration/examples/developer_rag/&#34;&gt;code&lt;/a&gt;, &lt;a href=&#34;https://nvidia.github.io/GenerativeAIExamples/latest/api-catalog.html#using-the-llamaindex-data-framework&#34;&gt;docs&lt;/a&gt;]&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://nvidia.github.io/GenerativeAIExamples/latest/multi-gpu.html&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Milvus or pgvector&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;llama3-70b&lt;/td&gt; &#xA;   &lt;td&gt;snowflake-arctic-embed-l&lt;/td&gt; &#xA;   &lt;td&gt;LangChain&lt;/td&gt; &#xA;   &lt;td&gt;Chat bot with query decomposition agent [&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/RetrievalAugmentedGeneration/examples/query_decomposition_rag/&#34;&gt;code&lt;/a&gt;, &lt;a href=&#34;https://nvidia.github.io/GenerativeAIExamples/latest/query-decomposition.html&#34;&gt;docs&lt;/a&gt;]&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Milvus or pgvector&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;llama3-70b&lt;/td&gt; &#xA;   &lt;td&gt;ai-embed-qa-4&lt;/td&gt; &#xA;   &lt;td&gt;LangChain&lt;/td&gt; &#xA;   &lt;td&gt;Minimilastic example: RAG with NVIDIA AI Foundation Models [&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/examples/5_mins_rag_no_gpu/&#34;&gt;code&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/examples/README.md#rag-in-5-minutes-example&#34;&gt;README&lt;/a&gt;]&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;FAISS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;llama3-8b&lt;br&gt;Deplot&lt;br&gt;Neva-22b&lt;/td&gt; &#xA;   &lt;td&gt;snowflake-arctic-embed-l&lt;/td&gt; &#xA;   &lt;td&gt;Custom&lt;/td&gt; &#xA;   &lt;td&gt;Chat bot with multimodal data [&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/RetrievalAugmentedGeneration/examples/multimodal_rag/&#34;&gt;code&lt;/a&gt;, &lt;a href=&#34;https://nvidia.github.io/GenerativeAIExamples/latest/multimodal-data.html&#34;&gt;docs&lt;/a&gt;]&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Milvus or pvgector&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;llama3-70b&lt;/td&gt; &#xA;   &lt;td&gt;none&lt;/td&gt; &#xA;   &lt;td&gt;PandasAI&lt;/td&gt; &#xA;   &lt;td&gt;Chat bot with structured data [&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/RetrievalAugmentedGeneration/examples/structured_data_rag/&#34;&gt;code&lt;/a&gt;, &lt;a href=&#34;https://nvidia.github.io/GenerativeAIExamples/latest/structured-data.html&#34;&gt;docs&lt;/a&gt;]&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;none&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;llama3-8b&lt;/td&gt; &#xA;   &lt;td&gt;snowflake-arctic-embed-l&lt;/td&gt; &#xA;   &lt;td&gt;LangChain&lt;/td&gt; &#xA;   &lt;td&gt;Chat bot with multi-turn conversation [&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/RetrievalAugmentedGeneration/examples/multi_turn_rag/&#34;&gt;code&lt;/a&gt;, &lt;a href=&#34;https://nvidia.github.io/GenerativeAIExamples/latest/multi-turn.html&#34;&gt;docs&lt;/a&gt;]&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Milvus or pgvector&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Enterprise RAG Examples&lt;/h3&gt; &#xA;&lt;p&gt;The enterprise RAG examples run as microservices distributed across multiple VMs and GPUs. These examples show how to orchestrate RAG pipelines with &lt;a href=&#34;https://kubernetes.io/&#34;&gt;Kubernetes&lt;/a&gt; and deployed with &lt;a href=&#34;https://helm.sh/&#34;&gt;Helm&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Enterprise RAG examples include a &lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/operator/&#34;&gt;Kubernetes operator&lt;/a&gt; for LLM lifecycle management. It is compatible with the &lt;a href=&#34;https://catalog.ngc.nvidia.com/orgs/nvidia/containers/gpu-operator&#34;&gt;NVIDIA GPU Operator&lt;/a&gt; that automates GPU discovery and lifecycle management in a Kubernetes cluster.&lt;/p&gt; &#xA;&lt;p&gt;Enterprise RAG examples also support local and remote inference with &lt;a href=&#34;https://github.com/NVIDIA/TensorRT-LLM&#34;&gt;TensorRT-LLM&lt;/a&gt; and &lt;a href=&#34;https://build.nvidia.com/explore/discover&#34;&gt;NVIDIA API Catalog endpoints&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Embedding&lt;/th&gt; &#xA;   &lt;th&gt;Framework&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Multi-GPU&lt;/th&gt; &#xA;   &lt;th&gt;Multi-node&lt;/th&gt; &#xA;   &lt;th&gt;TRT-LLM&lt;/th&gt; &#xA;   &lt;th&gt;NVIDIA Endpoints&lt;/th&gt; &#xA;   &lt;th&gt;Triton&lt;/th&gt; &#xA;   &lt;th&gt;Vector Database&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;llama-3&lt;/td&gt; &#xA;   &lt;td&gt;nv-embed-qa-4&lt;/td&gt; &#xA;   &lt;td&gt;LlamaIndex&lt;/td&gt; &#xA;   &lt;td&gt;Chat bot, Kubernetes deployment [&lt;a href=&#34;https://registry.ngc.nvidia.com/orgs/ohlfw0olaadg/teams/ea-participants/helm-charts/rag-app-text-chatbot&#34;&gt;chart&lt;/a&gt;]&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Milvus&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Generative AI Model Examples&lt;/h3&gt; &#xA;&lt;p&gt;The generative AI model examples include end-to-end steps for pre-training, customizing, aligning and running inference on state-of-the-art generative AI models leveraging the &lt;a href=&#34;https://github.com/NVIDIA/NeMo&#34;&gt;NVIDIA NeMo Framework&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Resources(s)&lt;/th&gt; &#xA;   &lt;th&gt;Framework&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gemma&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/models/Gemma/&#34;&gt;Docs&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/models/Gemma/lora.ipynb&#34;&gt;LoRA&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/models/Gemma/sft.ipynb&#34;&gt;SFT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;NeMo&lt;/td&gt; &#xA;   &lt;td&gt;Aligning and customizing Gemma, and exporting to TensorRT-LLM format for inference&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;codegemma&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/models/Codegemma/&#34;&gt;Docs&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/models/Codegemma/lora.ipynb&#34;&gt;LoRA&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;NeMo&lt;/td&gt; &#xA;   &lt;td&gt;Customizing Codegemma, and exporting to TensorRT-LLM format for inference&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;starcoder-2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/models/StarCoder2/lora.ipynb&#34;&gt;LoRA&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/models/StarCoder2/inference.ipynb&#34;&gt;Inference&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;NeMo&lt;/td&gt; &#xA;   &lt;td&gt;Customizing Starcoder-2 with NeMo Framework, optimizing with NVIDIA TensorRT-LLM, and deploying with NVIDIA Triton Inference Server&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;small language models (SLMs)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/models/NeMo/slm/&#34;&gt;Docs&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/models/NeMo/slm/slm_pretraining_sft.ipynb&#34;&gt;Pre-training and SFT&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/models/NeMo/slm/megatron_gpt_eval_server.ipynb&#34;&gt;Eval&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;NeMo&lt;/td&gt; &#xA;   &lt;td&gt;Training, alignment, and running evaluation on SLMs using various techniques&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Tools&lt;/h2&gt; &#xA;&lt;p&gt;Example tools and tutorials to enhance LLM development and productivity when using NVIDIA RAG pipelines.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;NVIDIA Endpoints&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Evaluation&lt;/td&gt; &#xA;   &lt;td&gt;RAG evaluation using synthetic data generation and LLM-as-a-judge [&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/tools/evaluation/&#34;&gt;code&lt;/a&gt;, &lt;a href=&#34;https://nvidia.github.io/GenerativeAIExamples/latest/evaluation.html&#34;&gt;docs&lt;/a&gt;]&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Observability&lt;/td&gt; &#xA;   &lt;td&gt;Monitoring and debugging RAG pipelines [&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/tools/observability/&#34;&gt;code&lt;/a&gt;, &lt;a href=&#34;https://nvidia.github.io/GenerativeAIExamples/latest/observability.html&#34;&gt;docs&lt;/a&gt;]&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Open Source Integrations&lt;/h2&gt; &#xA;&lt;p&gt;These are open source connectors for NVIDIA-hosted and self-hosted API endpoints. These open source connectors are maintained and tested by NVIDIA engineers.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Framework&lt;/th&gt; &#xA;   &lt;th&gt;Chat&lt;/th&gt; &#xA;   &lt;th&gt;Text Embedding&lt;/th&gt; &#xA;   &lt;th&gt;Python&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://python.langchain.com/docs/integrations/providers/nvidia&#34;&gt;NVIDIA AI Foundation Endpoints&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.langchain.com/&#34;&gt;Langchain&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://python.langchain.com/docs/integrations/chat/nvidia_ai_endpoints&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://python.langchain.com/docs/integrations/text_embedding/nvidia_ai_endpoints&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pypi.org/project/langchain-nvidia-ai-endpoints/&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Easy access to NVIDIA hosted models. Supports chat, embedding, code generation, steerLM, multimodal, and RAG.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/langchain-ai/langchain/tree/master/libs/partners/nvidia-trt&#34;&gt;NVIDIA Triton + TensorRT-LLM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.langchain.com/&#34;&gt;Langchain&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/langchain-ai/langchain-nvidia/raw/main/libs/trt/docs/llms.ipynb&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/langchain-ai/langchain-nvidia/raw/main/libs/trt/docs/llms.ipynb&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pypi.org/project/langchain-nvidia-trt/&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;This connector allows Langchain to remotely interact with a Triton inference server over GRPC or HTTP for optimized LLM inference.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.llamaindex.ai/en/stable/examples/llm/nvidia_triton.html&#34;&gt;NVIDIA Triton Inference Server&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.llamaindex.ai/&#34;&gt;LlamaIndex&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Triton inference server provides API access to hosted LLM models over gRPC.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.llamaindex.ai/en/stable/examples/llm/nvidia_tensorrt.html&#34;&gt;NVIDIA TensorRT-LLM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.llamaindex.ai/&#34;&gt;LlamaIndex&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;TensorRT-LLM provides a Python API to build TensorRT engines with state-of-the-art optimizations for LLM inference on NVIDIA GPUs.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Related NVIDIA RAG Projects&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://docs.nvidia.com/ace/latest/workflows/tokkio/text/Tokkio_LLM_RAG_Bot.html&#34;&gt;NVIDIA Tokkio LLM-RAG&lt;/a&gt;: Use Tokkio to add avatar animation for RAG responses.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/NVIDIA/ChatRTX&#34;&gt;RAG on Windows using TensorRT-LLM and LlamaIndex&lt;/a&gt;: Create RAG chatbots on Windows using TensorRT-LLM.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/NVIDIA/workbench-example-hybrid-rag&#34;&gt;Hybrid RAG Project on AI Workbench&lt;/a&gt;: Run an NVIDIA AI Workbench example project for RAG.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What&#39;s new?&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Tips for Building a RAG Pipeline with NVIDIA AI LangChain AI Endpoints by Amit Bleiweiss. [&lt;a href=&#34;https://developer.nvidia.com/blog/tips-for-building-a-rag-pipeline-with-nvidia-ai-langchain-ai-endpoints/&#34;&gt;Blog&lt;/a&gt;, &lt;a href=&#34;https://github.com/NVIDIA/GenerativeAIExamples/raw/v0.7.0/notebooks/08_RAG_Langchain_with_Local_NIM.ipynb&#34;&gt;notebook&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Experimental examples:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/NVIDIA/GenerativeAIExamples/raw/v0.7.0/experimental/rag-developer-chatbot&#34;&gt;How to create a developer-focused RAG chatbot using RAPIDS cuDF&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/NVIDIA/GenerativeAIExamples/raw/v0.7.0/experimental/event-driven-rag-cve-analysis&#34;&gt;NVIDIA Morpheus, NIMs, and RAG pipelines integrated to create LLM-based agent pipelines&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Refer to the &lt;a href=&#34;https://github.com/NVIDIA/GenerativeAIExamples/releases&#34;&gt;releases page&lt;/a&gt; for information about previous releases.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Support, Feedback, and Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We&#39;re posting these examples on GitHub to support the NVIDIA LLM community and facilitate feedback. We invite contributions via GitHub Issues or pull requests!&lt;/p&gt; &#xA;&lt;h2&gt;Known Issues&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Some known issues are identified as TODOs in the Python code.&lt;/li&gt; &#xA; &lt;li&gt;The datasets provided as part of this project are under a different license for research and evaluation purposes.&lt;/li&gt; &#xA; &lt;li&gt;This project downloads and installs third-party open source software projects. Review the license terms of these open source projects before use.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>