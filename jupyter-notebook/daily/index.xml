<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-11-23T01:36:06Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>adapter-hub/adapters</title>
    <updated>2023-11-23T01:36:06Z</updated>
    <id>tag:github.com,2023-11-23:/adapter-hub/adapters</id>
    <link href="https://github.com/adapter-hub/adapters" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Unified Library for Parameter-Efficient and Modular Transfer Learning&lt;/p&gt;&lt;hr&gt;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: This repository holds the codebase of the &lt;em&gt;Adapters&lt;/em&gt; library, which has replaced &lt;code&gt;adapter-transformers&lt;/code&gt;. For the legacy codebase, go to: &lt;a href=&#34;https://github.com/adapter-hub/adapter-transformers-legacy&#34;&gt;https://github.com/adapter-hub/adapter-transformers-legacy&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img style=&#34;vertical-align:middle&#34; src=&#34;https://raw.githubusercontent.com/Adapter-Hub/adapters/main/docs/logo.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt; &lt;span&gt;&lt;i&gt;Adapters&lt;/i&gt;&lt;/span&gt; &lt;/h1&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt; A Unified Library for Parameter-Efficient and Modular Transfer Learning &lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/Adapter-Hub/adapters/workflows/Tests/badge.svg?branch=adapters&#34; alt=&#34;Tests&#34;&gt; &lt;a href=&#34;https://github.com/adapter-hub/adapters/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/adapter-hub/adapters.svg?color=blue&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/adapters/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/adapters&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;adapters&lt;/code&gt; is an add-on to &lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;HuggingFace&#39;s Transformers&lt;/a&gt; library, integrating adapters into state-of-the-art language models by incorporating &lt;strong&gt;&lt;a href=&#34;https://adapterhub.ml&#34;&gt;AdapterHub&lt;/a&gt;&lt;/strong&gt;, a central repository for pre-trained adapter modules.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;adapters&lt;/code&gt; currently supports &lt;strong&gt;Python 3.8+&lt;/strong&gt; and &lt;strong&gt;PyTorch 1.10+&lt;/strong&gt;. After &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;installing PyTorch&lt;/a&gt;, you can install &lt;code&gt;adapters&lt;/code&gt; from PyPI ...&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -U adapters&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;... or from source by cloning the repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/adapter-hub/adapters.git&#xA;git checkout adapters&#xA;cd adapters&#xA;pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Quick Tour&lt;/h2&gt; &#xA;&lt;h4&gt;Load pre-trained adapters:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from adapters import AutoAdapterModel&#xA;from transformers import AutoTokenizer&#xA;&#xA;model = AutoAdapterModel.from_pretrained(&#34;roberta-base&#34;)&#xA;tokenizer = AutoTokenizer.from_pretrained(&#34;roberta-base&#34;)&#xA;&#xA;model.load_adapter(&#34;AdapterHub/roberta-base-pf-imdb&#34;, source=&#34;hf&#34;, set_active=True)&#xA;&#xA;print(model(**tokenizer(&#34;This works great!&#34;, return_tensors=&#34;pt&#34;)).logits)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://docs.adapterhub.ml/loading.html&#34;&gt;Learn More&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Adapt existing model setups:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import adapters&#xA;from transformers import AutoModelForSequenceClassification&#xA;&#xA;model = AutoModelForSequenceClassification.from_pretrained(&#34;t5-base&#34;)&#xA;&#xA;adapters.init(model)&#xA;&#xA;model.add_adapter(&#34;my_lora_adapter&#34;, config=&#34;lora&#34;)&#xA;model.train_adapter(&#34;my_lora_adapter&#34;)&#xA;&#xA;# Your regular training loop...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://docs.adapterhub.ml/quickstart.html&#34;&gt;Learn More&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Flexibly configure adapters:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from adapters import ConfigUnion, PrefixTuningConfig, ParBnConfig, AutoAdapterModel&#xA;&#xA;model = AutoAdapterModel.from_pretrained(&#34;microsoft/deberta-v3-base&#34;)&#xA;&#xA;adapter_config = ConfigUnion(&#xA;    PrefixTuningConfig(prefix_length=20),&#xA;    ParBnConfig(reduction_factor=4),&#xA;)&#xA;model.add_adapter(&#34;my_adapter&#34;, config=adapter_config, set_active=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://docs.adapterhub.ml/overview.html&#34;&gt;Learn More&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Easily compose adapters in a single model:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from adapters import AdapterSetup, AutoAdapterModel&#xA;import adapters.composition as ac&#xA;&#xA;model = AutoAdapterModel.from_pretrained(&#34;roberta-base&#34;)&#xA;&#xA;qc = model.load_adapter(&#34;AdapterHub/roberta-base-pf-trec&#34;)&#xA;sent = model.load_adapter(&#34;AdapterHub/roberta-base-pf-imdb&#34;)&#xA;&#xA;with AdapterSetup(ac.Parallel(qc, sent)):&#xA;    print(model(**tokenizer(&#34;What is AdapterHub?&#34;, return_tensors=&#34;pt&#34;)))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://docs.adapterhub.ml/adapter_composition.html&#34;&gt;Learn More&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Useful Resources&lt;/h2&gt; &#xA;&lt;p&gt;HuggingFace&#39;s great documentation on getting started with &lt;em&gt;Transformers&lt;/em&gt; can be found &lt;a href=&#34;https://huggingface.co/transformers/index.html&#34;&gt;here&lt;/a&gt;. &lt;code&gt;adapters&lt;/code&gt; is fully compatible with &lt;em&gt;Transformers&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To get started with adapters, refer to these locations:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/Adapter-Hub/adapters/tree/main/notebooks&#34;&gt;Colab notebook tutorials&lt;/a&gt;&lt;/strong&gt;, a series notebooks providing an introduction to all the main concepts of (adapter-)transformers and AdapterHub&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://docs.adapterhub.ml&#34;&gt;https://docs.adapterhub.ml&lt;/a&gt;&lt;/strong&gt;, our documentation on training and using adapters with &lt;em&gt;adapters&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://adapterhub.ml&#34;&gt;https://adapterhub.ml&lt;/a&gt;&lt;/strong&gt; to explore available pre-trained adapter modules and share your own adapters&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/Adapter-Hub/adapters/tree/main/examples/pytorch&#34;&gt;Examples folder&lt;/a&gt;&lt;/strong&gt; of this repository containing HuggingFace&#39;s example training scripts, many adapted for training adapters&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Implemented Methods&lt;/h2&gt; &#xA;&lt;p&gt;Currently, adapters integrates all architectures and methods listed below:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Method&lt;/th&gt; &#xA;   &lt;th&gt;Paper(s)&lt;/th&gt; &#xA;   &lt;th&gt;Quick Links&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Bottleneck adapters&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1902.00751.pdf&#34;&gt;Houlsby et al. (2019)&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/1909.08478.pdf&#34;&gt;Bapna and Firat (2019)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.adapterhub.ml/quickstart.html&#34;&gt;Quickstart&lt;/a&gt;, &lt;a href=&#34;https://colab.research.google.com/github/Adapter-Hub/adapters/blob/main/notebooks/01_Adapter_Training.ipynb&#34;&gt;Notebook&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AdapterFusion&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://aclanthology.org/2021.eacl-main.39.pdf&#34;&gt;Pfeiffer et al. (2021)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.adapterhub.ml/training.html#train-adapterfusion&#34;&gt;Docs: Training&lt;/a&gt;, &lt;a href=&#34;https://colab.research.google.com/github/Adapter-Hub/adapters/blob/main/notebooks/03_Adapter_Fusion.ipynb&#34;&gt;Notebook&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MAD-X,&lt;br&gt; Invertible adapters&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://aclanthology.org/2020.emnlp-main.617/&#34;&gt;Pfeiffer et al. (2020)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/Adapter-Hub/adapters/blob/main/notebooks/04_Cross_Lingual_Transfer.ipynb&#34;&gt;Notebook&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AdapterDrop&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2010.11918.pdf&#34;&gt;Rücklé et al. (2021)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/Adapter-Hub/adapters/blob/main/notebooks/05_Adapter_Drop_Training.ipynb&#34;&gt;Notebook&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MAD-X 2.0,&lt;br&gt; Embedding training&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2012.15562.pdf&#34;&gt;Pfeiffer et al. (2021)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.adapterhub.ml/embeddings.html&#34;&gt;Docs: Embeddings&lt;/a&gt;, &lt;a href=&#34;https://colab.research.google.com/github/Adapter-Hub/adapters/blob/main/notebooks/08_NER_Wikiann.ipynb&#34;&gt;Notebook&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Prefix Tuning&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2101.00190.pdf&#34;&gt;Li and Liang (2021)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.adapterhub.ml/overview.html#prefix-tuning&#34;&gt;Docs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Parallel adapters,&lt;br&gt; Mix-and-Match adapters&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2110.04366.pdf&#34;&gt;He et al. (2021)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.adapterhub.ml/overview.html#mix-and-match-adapters&#34;&gt;Docs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Compacter&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2106.04647.pdf&#34;&gt;Mahabadi et al. (2021)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.adapterhub.ml/overview.html#compacter&#34;&gt;Docs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LoRA&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2106.09685.pdf&#34;&gt;Hu et al. (2021)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.adapterhub.ml/overview.html#lora&#34;&gt;Docs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;(IA)^3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2205.05638.pdf&#34;&gt;Liu et al. (2022)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.adapterhub.ml/overview.html#ia-3&#34;&gt;Docs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;UniPELT&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2110.07577.pdf&#34;&gt;Mao et al. (2022)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.adapterhub.ml/overview.html#unipelt&#34;&gt;Docs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Supported Models&lt;/h2&gt; &#xA;&lt;p&gt;We currently support the PyTorch versions of all models listed on the &lt;strong&gt;&lt;a href=&#34;https://docs.adapterhub.ml/model_overview.html&#34;&gt;Model Overview&lt;/a&gt; page&lt;/strong&gt; in our documentation.&lt;/p&gt; &#xA;&lt;h2&gt;Developing &amp;amp; Contributing&lt;/h2&gt; &#xA;&lt;p&gt;To get started with developing on &lt;em&gt;Adapters&lt;/em&gt; yourself and learn more about ways to contribute, please see &lt;a href=&#34;https://docs.adapterhub.ml/contributing.html&#34;&gt;https://docs.adapterhub.ml/contributing.html&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use this library for your work, please consider citing our paper &lt;a href=&#34;https://arxiv.org/abs/2007.07779&#34;&gt;AdapterHub: A Framework for Adapting Transformers&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{pfeiffer2020AdapterHub,&#xA;    title={AdapterHub: A Framework for Adapting Transformers},&#xA;    author={Pfeiffer, Jonas and&#xA;            R{\&#34;u}ckl{\&#39;e}, Andreas and&#xA;            Poth, Clifton and&#xA;            Kamath, Aishwarya and&#xA;            Vuli{\&#39;c}, Ivan and&#xA;            Ruder, Sebastian and&#xA;            Cho, Kyunghyun and&#xA;            Gurevych, Iryna},&#xA;    booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},&#xA;    pages={46--54},&#xA;    year={2020}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>