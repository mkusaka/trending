<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-01T01:39:25Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>beyondguo/LLM-Tuning</title>
    <updated>2023-07-01T01:39:25Z</updated>
    <id>tag:github.com,2023-07-01:/beyondguo/LLM-Tuning</id>
    <link href="https://github.com/beyondguo/LLM-Tuning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Tuning LLMs with no tearsğŸ’¦, sharing LLM-tools with loveâ¤ï¸.&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Tuning LLMs with no tears.&lt;/h2&gt; &#xA;&lt;p&gt;ç›®å‰æ”¯æŒï¼š&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;æ¸…å &lt;a href=&#34;https://huggingface.co/THUDM/chatglm2-6b&#34;&gt;ChatGLM2-6B&lt;/a&gt; çš„ LoRA å¾®è°ƒ (New!ğŸ”¥)&lt;/li&gt; &#xA; &lt;li&gt;ç™¾å·æ™ºèƒ½ &lt;a href=&#34;https://huggingface.co/baichuan-inc/baichuan-7B&#34;&gt;baichuan-7B&lt;/a&gt; çš„ LoRA å¾®è°ƒ&lt;/li&gt; &#xA; &lt;li&gt;æ¸…å &lt;a href=&#34;https://huggingface.co/THUDM/chatglm-6b&#34;&gt;ChatGLM-6B&lt;/a&gt; çš„ LoRA å¾®è°ƒ&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;ä¸¤è¡Œä»£ç å¼€å¯è®­ç»ƒï¼š&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;æ•°æ®é›†åˆ†è¯é¢„å¤„ç†ï¼š&lt;code&gt;sh tokenize.sh&lt;/code&gt;ï¼Œå¯¹æ¯”ä¸åŒçš„ LLMï¼Œéœ€åœ¨ tokenize.sh æ–‡ä»¶é‡Œåˆ‡æ¢ model_checkpoint å‚æ•°&lt;/li&gt; &#xA; &lt;li&gt;å¼€å¯ LoRA å¾®è°ƒï¼š&lt;code&gt;sh train.sh&lt;/code&gt;ï¼Œå¯¹äºä¸åŒçš„ LLMï¼Œéœ€åˆ‡æ¢ä¸åŒçš„ python æ–‡ä»¶æ¥æ‰§è¡Œï¼š &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ChatGLM-6B åº”ä½¿ç”¨ &lt;code&gt;chatglm_lora_tuning.py&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;ChatGLM2-6B åº”ä½¿ç”¨ &lt;code&gt;chatglm2_lora_tuning.py&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;baichuan-7B åº”ä½¿ç”¨ &lt;code&gt;baichuan_lora_tuning.py&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;å¯å¤ç°çš„å°é¡¹ç›®ï¼š&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/beyondguo/LLM-Tuning/master/projects/ChatBaichuan-HC3/&#34;&gt;&lt;strong&gt;ChatBaichuan&lt;/strong&gt;ï¼šåŸºäº HC3 æ•°æ®é›†è®© ç™¾å·å¤§æ¨¡å‹ï¼ˆbaichuan-7Bï¼‰æœ‰å¯¹è¯èƒ½åŠ›ï¼&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/beyondguo/LLM-Tuning/master/projects/RulaiGPT/&#34;&gt;ã€å¨±ä¹å‘ã€‘&lt;strong&gt;RulaiGPT&lt;/strong&gt;ï¼šå¦‚æ¥~è¯¶ï¼Œå®ƒçœŸæ¥äº†å—ï¼Ÿå¦‚~æ¥~ï¼ˆæ‹æ¡Œï¼ï¼‰&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;ç¯å¢ƒå‡†å¤‡&lt;/strong&gt;ï¼š&lt;br&gt; &lt;code&gt;pip install transformers datasets accelerate sentencepiece tensorboard peft&lt;/code&gt;&lt;br&gt; ç›®å‰æµ‹è¯•çš„ç¯å¢ƒä¸ºï¼š&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;- Python 3.9.16&#xA;- torch, Version: 2.0.1&#xA;- transformers, Version: 4.29.1&#xA;- datasets, Version: 2.12.0&#xA;- accelerate, Version: 0.19.0&#xA;- peft, Version: 0.3.0&#xA;- sentencepiece, Version: 0.1.99&#xA;- tensorboard, Version: 2.13.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;æ•™ç¨‹ï¼š&lt;/h2&gt; &#xA;&lt;p&gt;ä¸‹é¢çš„æ•™ç¨‹ä»¥åŠä»£ç ä½¿ç”¨ &lt;code&gt;ChatGLM-6B&lt;/code&gt; ä½œä¸ºä¾‹å­ï¼Œå¦‚æœæ›´æ¢å…¶ä»–æ¨¡å‹ï¼Œå¯èƒ½éœ€è¦ç•¥å¾®ä¿®æ”¹å…·ä½“æ–‡ä»¶ä»£ç ã€‚&lt;/p&gt; &#xA;&lt;h3&gt;1. æŒ‡ä»¤å¾®è°ƒæ•°æ®å‡†å¤‡ Instruction Data Preparation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;åŸå§‹æ–‡ä»¶çš„å‡†å¤‡&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;æŒ‡ä»¤å¾®è°ƒæ•°æ®ä¸€èˆ¬æœ‰è¾“å…¥å’Œè¾“å‡ºä¸¤éƒ¨åˆ†ï¼Œè¾“å…¥æ˜¯ç‰¹å®šçš„contentåŠ ä¸Šinstructionï¼Œè¿™é‡Œæˆ‘ä»¬å°†äºŒè€…ç›´æ¥æ‹¼åœ¨ä¸€èµ·ï¼Œä¸å•ç‹¬åŒºåˆ†ï¼›è¾“å‡ºåˆ™æ˜¯å¸Œæœ›æ¨¡å‹çš„å›ç­”ã€‚ æˆ‘ä»¬ç»Ÿä¸€ä½¿ç”¨&lt;code&gt;json&lt;/code&gt;çš„æ ¼å¼åœ¨æ•´ç†æ•°æ®ï¼Œå¯ä»¥è‡ªå®šä¹‰è¾“å‡ºè¾“å‡ºçš„å­—æ®µåï¼Œä¾‹å¦‚ä¸‹é¢çš„ä¾‹å­ä¸­æˆ‘ä½¿ç”¨çš„æ˜¯&lt;code&gt;q&lt;/code&gt;å’Œ&lt;code&gt;a&lt;/code&gt;ä»£è¡¨æ¨¡å‹çš„è¾“å…¥å’Œè¾“å‡ºï¼š&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#34;q&#34;: &#34;è¯·è®¡ç®—ï¼š39 * 0 = ä»€ä¹ˆï¼Ÿ&#34;, &#34;a&#34;: &#34;è¿™æ˜¯ç®€å•çš„ä¹˜æ³•è¿ç®—ï¼Œ39ä¹˜ä»¥0å¾—åˆ°çš„æ˜¯0&#34;}&#xA;{&#34;q&#34;: &#34;é¢˜ç›®ï¼š51/186çš„ç­”æ¡ˆæ˜¯ä»€ä¹ˆ?&#34;, &#34;a&#34;: &#34;è¿™æ˜¯ç®€å•çš„é™¤æ³•è¿ç®—ï¼Œ51é™¤ä»¥186å¤§æ¦‚ä¸º0.274&#34;}&#xA;{&#34;q&#34;: &#34;é¹¿å¦ˆå¦ˆä¹°äº†24ä¸ªè‹¹æœï¼Œå¥¹æƒ³å¹³å‡åˆ†ç»™å¥¹çš„3åªå°é¹¿åƒï¼Œæ¯åªå°é¹¿å¯ä»¥åˆ†åˆ°å‡ ä¸ªè‹¹æœï¼Ÿ&#34;, &#34;a&#34;: &#34;é¹¿å¦ˆå¦ˆä¹°äº†24ä¸ªè‹¹æœï¼Œå¹³å‡åˆ†ç»™3åªå°é¹¿åƒï¼Œé‚£ä¹ˆæ¯åªå°é¹¿å¯ä»¥åˆ†åˆ°çš„è‹¹æœæ•°å°±æ˜¯æ€»è‹¹æœæ•°é™¤ä»¥å°é¹¿çš„åªæ•°ã€‚\n24Ã·3=8\næ¯åªå°é¹¿å¯ä»¥åˆ†åˆ°8ä¸ªè‹¹æœã€‚æ‰€ä»¥ï¼Œç­”æ¡ˆæ˜¯æ¯åªå°é¹¿å¯ä»¥åˆ†åˆ°8ä¸ªè‹¹æœã€‚&#34;}&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;æ•´ç†å¥½æ•°æ®åï¼Œä¿å­˜ä¸º&lt;code&gt;.json&lt;/code&gt;æˆ–è€…&lt;code&gt;.jsonl&lt;/code&gt;æ–‡ä»¶ï¼Œç„¶åæ”¾å…¥ç›®å½•ä¸­çš„&lt;code&gt;data/&lt;/code&gt;æ–‡ä»¶å¤¹ä¸­ã€‚&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;ä¸ºäº†é¿å…æ¯æ¬¡è®­ç»ƒçš„æ—¶å€™éƒ½è¦é‡æ–°å¯¹æ•°æ®é›†åˆ†è¯ï¼Œæˆ‘ä»¬å…ˆåˆ†å¥½è¯å½¢æˆç‰¹å¾åä¿å­˜æˆå¯ç›´æ¥ç”¨äºè®­ç»ƒçš„æ•°æ®é›†ã€‚&lt;/p&gt; &#xA;&lt;p&gt;ä¾‹å¦‚ï¼Œ&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;æˆ‘ä»¬çš„åŸå§‹æŒ‡ä»¤å¾®è°ƒæ–‡ä»¶ä¸ºï¼š&lt;code&gt;data/&lt;/code&gt; æ–‡ä»¶å¤¹ä¸‹çš„ &lt;code&gt;simple_math_4op.json&lt;/code&gt; æ–‡ä»¶&lt;/li&gt; &#xA; &lt;li&gt;è¾“å…¥å­—æ®µä¸º&lt;code&gt;q&lt;/code&gt;ï¼Œè¾“å‡ºå­—æ®µä¸º&lt;code&gt;a&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;å¸Œæœ›ç»è¿‡ tokenize ä¹‹åä¿å­˜åˆ° &lt;code&gt;data/tokenized_data/&lt;/code&gt; ä¸‹åä¸º &lt;code&gt;simple_math_4op&lt;/code&gt; çš„æ–‡ä»¶å¤¹ä¸­&lt;/li&gt; &#xA; &lt;li&gt;è®¾å®šæ–‡æœ¬æœ€å¤§ç¨‹åº¦ä¸º 2000&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;åˆ™æˆ‘ä»¬å¯ä»¥ç›´æ¥ä½¿ç”¨ä¸‹é¢è¿™æ®µå‘½ä»¤(å³&lt;code&gt;tokenize.sh&lt;/code&gt;æ–‡ä»¶)è¿›è¡Œå¤„ç†ï¼š&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=0,1 python tokenize_dataset_rows.py \&#xA;    --model_checkpoint THUDM/chatglm-6b \&#xA;    --input_file simple_math_4op.json \&#xA;    --prompt_key q \&#xA;    --target_key a \&#xA;    --save_name simple_math_4op \&#xA;    --max_seq_length 2000 \&#xA;    --skip_overlength False&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;å¤„ç†å®Œæ¯•ä¹‹åï¼Œæˆ‘ä»¬ä¼šåœ¨ &lt;code&gt;data/tokenized_data/&lt;/code&gt; ä¸‹å‘ç°åä¸º &lt;code&gt;simple_math_4op&lt;/code&gt; çš„æ–‡ä»¶å¤¹ï¼Œè¿™å°±æ˜¯ä¸‹ä¸€æ­¥ä¸­æˆ‘ä»¬å¯ä»¥ç›´æ¥ç”¨äºè®­ç»ƒçš„æ•°æ®ã€‚&lt;/p&gt; &#xA;&lt;h3&gt;2. ä½¿ç”¨ &lt;code&gt;LoRA&lt;/code&gt; å¾®è°ƒ&lt;/h3&gt; &#xA;&lt;p&gt;å¾—åˆ° tokenize ä¹‹åçš„æ•°æ®é›†ï¼Œå°±å¯ä»¥ç›´æ¥è¿è¡Œ &lt;code&gt;chatglm_lora_tuning.py&lt;/code&gt; æ¥è®­ç»ƒ LoRA æ¨¡å‹äº†ï¼Œå…·ä½“å¯è®¾ç½®çš„ä¸»è¦å‚æ•°åŒ…æ‹¬ï¼š&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;tokenized_dataset&lt;/code&gt;, åˆ†è¯åçš„æ•°æ®é›†ï¼Œå³åœ¨ data/tokenized_data/ åœ°å€ä¸‹çš„æ–‡ä»¶å¤¹åç§°&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;lora_rank&lt;/code&gt;, è®¾ç½® LoRA çš„ç§©ï¼Œæ¨èä¸º4æˆ–8ï¼Œæ˜¾å­˜å¤Ÿçš„è¯ä½¿ç”¨8&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;per_device_train_batch_size&lt;/code&gt;, æ¯å— GPU ä¸Šçš„ batch size&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;gradient_accumulation_steps&lt;/code&gt;, æ¢¯åº¦ç´¯åŠ ï¼Œå¯ä»¥åœ¨ä¸æå‡æ˜¾å­˜å ç”¨çš„æƒ…å†µä¸‹å¢å¤§ batch size&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;max_steps&lt;/code&gt;, è®­ç»ƒæ­¥æ•°&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;save_steps&lt;/code&gt;, å¤šå°‘æ­¥ä¿å­˜ä¸€æ¬¡&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;save_total_limit&lt;/code&gt;, ä¿å­˜å¤šå°‘ä¸ªcheckpoint&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;logging_steps&lt;/code&gt;, å¤šå°‘æ­¥æ‰“å°ä¸€æ¬¡è®­ç»ƒæƒ…å†µ(loss, lr, etc.)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;output_dir&lt;/code&gt;, æ¨¡å‹æ–‡ä»¶ä¿å­˜åœ°å€&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;ä¾‹å¦‚æˆ‘ä»¬çš„æ•°æ®é›†ä¸º simple_math_4opï¼Œå¸Œæœ›ä¿å­˜åˆ° weights/simple_math_4op ï¼Œåˆ™æ‰§è¡Œä¸‹é¢å‘½ä»¤(å³&lt;code&gt;train.sh&lt;/code&gt;æ–‡ä»¶)ï¼š&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=2,3 python chatglm_lora_tuning.py \&#xA;    --tokenized_dataset simple_math_4op \&#xA;    --lora_rank 8 \&#xA;    --per_device_train_batch_size 10 \&#xA;    --gradient_accumulation_steps 1 \&#xA;    --max_steps 100000 \&#xA;    --save_steps 200 \&#xA;    --save_total_limit 2 \&#xA;    --learning_rate 1e-4 \&#xA;    --fp16 \&#xA;    --remove_unused_columns false \&#xA;    --logging_steps 50 \&#xA;    --output_dir weights/simple_math_4op&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;è®­ç»ƒå®Œä¹‹åï¼Œå¯ä»¥åœ¨ output_dir ä¸­æ‰¾åˆ° LoRA çš„ç›¸å…³æ¨¡å‹æƒé‡ï¼Œä¸»è¦æ˜¯&lt;code&gt;adapter_model.bin&lt;/code&gt;å’Œ&lt;code&gt;adapter_config.json&lt;/code&gt;ä¸¤ä¸ªæ–‡ä»¶ã€‚&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;å¦‚ä½•æŸ¥çœ‹ tensorboardï¼š&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;åœ¨ output_dir ä¸­æ‰¾åˆ° runs æ–‡ä»¶å¤¹ï¼Œå¤åˆ¶å…¶ä¸­æ—¥æœŸæœ€å¤§çš„æ–‡ä»¶å¤¹çš„åœ°å€ï¼Œå‡è®¾ä¸º &lt;code&gt;your_log_path&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;æ‰§è¡Œ &lt;code&gt;tensorboard --logdir your_log_path&lt;/code&gt; å‘½ä»¤ï¼Œå°±ä¼šåœ¨ &lt;a href=&#34;http://localhost:6006/&#34;&gt;http://localhost:6006/&lt;/a&gt; ä¸Šå¼€å¯tensorboard&lt;/li&gt; &#xA; &lt;li&gt;å¦‚æœæ˜¯åœ¨æœåŠ¡å™¨ä¸Šå¼€å¯ï¼Œåˆ™è¿˜éœ€è¦åšç«¯å£æ˜ å°„åˆ°æœ¬åœ°ã€‚æ¨èä½¿ç”¨ VSCode åœ¨æœåŠ¡å™¨ä¸Šå†™ä»£ç ï¼Œå¯ä»¥è‡ªåŠ¨å¸®ä½ è¿›è¡Œç«¯å£æ˜ å°„ã€‚&lt;/li&gt; &#xA; &lt;li&gt;å¦‚æœè¦è‡ªå·±æ‰‹åŠ¨è¿›è¡Œç«¯å£æ˜ å°„ï¼Œå…·ä½“æ–¹å¼æ˜¯åœ¨ä½¿ç”¨ ssh ç™»å½•æ—¶ï¼Œåé¢åŠ ä¸Š &lt;code&gt;-L 6006:127.0.0.1:6006&lt;/code&gt; å‚æ•°ï¼Œå°†æœåŠ¡å™¨ç«¯çš„6006ç«¯å£æ˜ å°„åˆ°æœ¬åœ°çš„6006ç«¯å£ã€‚&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;3. æ‹¿èµ° LoRA å°å°çš„æ–‡ä»¶ï¼Œåˆ°ä½ æœ¬åœ°çš„å¤§æ¨¡å‹ä¸ŠåŠ è½½å¹¶æ¨ç†&lt;/h3&gt; &#xA;&lt;p&gt;æˆ‘ä»¬å¯ä»¥æŠŠä¸Šé¢çš„ output_dir æ‰“åŒ…å¸¦èµ°ï¼Œå‡è®¾æ–‡ä»¶å¤¹ä¸º &lt;code&gt;weights/simple_math_4op&lt;/code&gt;ï¼Œ å…¶ä¸­ï¼ˆè‡³å°‘ï¼‰åŒ…å« &lt;code&gt;adapter_model.bin&lt;/code&gt; å’Œ &lt;code&gt;adapter_config.json&lt;/code&gt; ä¸¤ä¸ªæ–‡ä»¶ï¼Œåˆ™æˆ‘ä»¬å¯ä»¥ç”¨ä¸‹é¢çš„æ–¹å¼ç›´æ¥åŠ è½½ï¼Œå¹¶æ¨ç†&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from peft import PeftModel&#xA;from transformers import AutoTokenizer, AutoModel&#xA;import torch&#xA;&#xA;device = torch.device(1)&#xA;# åŠ è½½åŸå§‹ LLM&#xA;model_path = &#34;THUDM/chatglm-6b&#34;&#xA;model = AutoModel.from_pretrained(model_path, trust_remote_code=True).half().to(device)&#xA;tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)&#xA;model.chat(tokenizer, &#34;ä½ å¥½&#34;, history=[])&#xA;&#xA;&#xA;# ç»™åŸå§‹ LLM å®‰è£…ä¸Šä½ çš„ LoRA tool&#xA;model = PeftModel.from_pretrained(model, &#34;weights/simple_math_4op&#34;).half()&#xA;model.chat(tokenizer, &#34;ä½ å¥½&#34;, history=[])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;ç†è®ºä¸Šï¼Œå¯ä»¥é€šè¿‡å¤šæ¬¡æ‰§è¡Œ &lt;code&gt;model = PeftModel.from_pretrained(model, &#34;weights/simple_math_4op&#34;).half()&lt;/code&gt; çš„æ–¹å¼ï¼ŒåŠ è½½å¤šä¸ª LoRA æ¨¡å‹ï¼Œä»è€Œæ··åˆä¸åŒToolçš„èƒ½åŠ›ï¼Œä½†å®é™…æµ‹è¯•çš„æ—¶å€™ï¼Œç”±äºæš‚æ—¶è¿˜ä¸æ”¯æŒè®¾ç½®ä¸åŒ LoRA weightsçš„æƒé‡ï¼Œå¾€å¾€æ•ˆæœä¸å¤ªå¥½ï¼Œå­˜åœ¨è¦†ç›–æˆ–è€…é—å¿˜çš„æƒ…å†µã€‚&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Acknowledgement&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;é¦–å…ˆæœ€æ„Ÿè°¢çš„æ˜¯ ğŸ¤—Huggingface å›¢é˜Ÿå¼€æºçš„ &lt;a href=&#34;https://github.com/huggingface/peft&#34;&gt;peft&lt;/a&gt; å·¥å…·åŒ…ï¼Œæ‡‚çš„éƒ½æ‡‚ï¼&lt;/li&gt; &#xA; &lt;li&gt;ChatGLM çš„ LoRA å¾®è°ƒä»£ç ä¸»è¦åŸºäº &lt;a href=&#34;https://github.com/mymusise/ChatGLM-Tuning&#34;&gt;ChatGLM-Tuning&lt;/a&gt; é¡¹ç›®ä¸­çš„ LoRA å¾®è°ƒéƒ¨åˆ†ä¿®æ”¹è€Œæ¥ï¼›&lt;/li&gt; &#xA; &lt;li&gt;baichuan-7B å¾®è°ƒéƒ¨åˆ†ï¼Œå‚è€ƒäº† &lt;a href=&#34;https://github.com/hiyouga/LLaMA-Efficient-Tuning/issues/43&#34;&gt;LLaMA-Efficient-Tuning&lt;/a&gt; é¡¹ç›®ä¸­çš„è§£å†³æ–¹æ¡ˆï¼›&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;å¯¹è¿™äº›ä¼˜ç§€å¼€æºé¡¹ç›®è¡¨ç¤ºæ„Ÿè°¢ï¼&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>commaai/commavq</title>
    <updated>2023-07-01T01:39:25Z</updated>
    <id>tag:github.com,2023-07-01:/commaai/commavq</id>
    <link href="https://github.com/commaai/commavq" rel="alternate"></link>
    <summary type="html">&lt;p&gt;commaVQ is a dataset of compressed driving video&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;commavq&lt;/h1&gt; &#xA;&lt;p&gt;commaVQ is a dataset of 100,000 heavily compressed driving videos for Machine Learning research. A heavily compressed driving video like this is useful to experiment with GPT-like video prediction models. This repo includes an encoder/decoder and an example of a video prediction model.&lt;/p&gt; &#xA;&lt;h2&gt;2x$500 Challenges!&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Get 1.92 cross entropy loss or less in the val set and in our private val set (using &lt;code&gt;./notebooks/eval.ipynb&lt;/code&gt;). gpt2m trained on a larger dataset gets 2.02 cross entropy loss.&lt;/li&gt; &#xA; &lt;li&gt;Make gpt2m.onnx run at 0.9 sec/frame or less on a consumer GPU (e.g. NVIDIA 3090) without degredation in cross entropy loss. The current implementation runs at 1.5 sec/frame with kvcaching and float16.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;A VQ-VAE [1,2] was used to heavily compress each frame into 128 &#34;tokens&#34; of 10 bits each. Each entry of the dataset is a &#34;segment&#34; of compressed driving video, i.e. 1min of frames at 20 FPS. Each file is of shape 1200x8x16 and saved as int16.&lt;/p&gt; &#xA;&lt;p&gt;Note that the compressor is extremely lossy on purpose. It makes the dataset smaller and easy to play with (train GPT with large context size, fast autoregressive generation, etc.). We might extend the dataset to a less lossy version when we see fit.&lt;/p&gt; &#xA;&lt;h2&gt;Download&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Using huggingface datasets&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np&#xA;from datasets import load_dataset&#xA;num_proc = 40 # CPUs go brrrr&#xA;ds = load_dataset(&#39;commaai/commavq&#39;, num_proc=num_proc)&#xA;tokens = np.load(ds[&#39;0&#39;][0][&#39;path&#39;]) # first segment from the first data shard&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Manually download from huggingface datasets repository: &lt;a href=&#34;https://huggingface.co/datasets/commaai/commavq&#34;&gt;https://huggingface.co/datasets/commaai/commavq&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;From Academic Torrents (soon)&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Models&lt;/h2&gt; &#xA;&lt;p&gt;In ./models/ you will find 3 Neural Networks saved in the onnx format&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;./models/encoder.onnx&lt;/code&gt;: is the encoder used to compress the frames&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;./models/decoder.onnx&lt;/code&gt;: is the decoder used to decompress the frames&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;./models/gtp2m.onnx&lt;/code&gt;: a 300M parameter GPT trained on a larger version of this dataset&lt;/li&gt; &#xA; &lt;li&gt;(experimental) &lt;code&gt;./models/temporal_decoder.onnx&lt;/code&gt;: a temporal decoder which is a stateful version of the vanilla decoder&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;Checkout &lt;code&gt;./notebooks/encode.ipynb&lt;/code&gt; and &lt;code&gt;./notebooks/decode.ipynb&lt;/code&gt; for an example of how to visualize the dataset using a segment of driving video from &lt;a href=&#34;https://blog.comma.ai/taco-bell/&#34;&gt;comma&#39;s drive to Taco Bell&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Checkout &lt;code&gt;./notebooks/gpt.ipynb&lt;/code&gt; for an example of how to use a pretrained GPT model to imagine future frames.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/commaai/commavq/assets/29985433/91894bf7-592b-4204-b3f2-3e805984045c&#34;&gt;https://github.com/commaai/commavq/assets/29985433/91894bf7-592b-4204-b3f2-3e805984045c&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/commaai/commavq/assets/29985433/3a799ac8-781e-461c-bf14-c15cea42b985&#34;&gt;https://github.com/commaai/commavq/assets/29985433/3a799ac8-781e-461c-bf14-c15cea42b985&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/commaai/commavq/assets/29985433/f6f7699b-b6cb-4f9c-80c9-8e00d75fbfae&#34;&gt;https://github.com/commaai/commavq/assets/29985433/f6f7699b-b6cb-4f9c-80c9-8e00d75fbfae&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;p&gt;[1] Van Den Oord, Aaron, and Oriol Vinyals. &#34;Neural discrete representation learning.&#34; Advances in neural information processing systems 30 (2017).&lt;/p&gt; &#xA;&lt;p&gt;[2] Esser, Patrick, Robin Rombach, and Bjorn Ommer. &#34;Taming transformers for high-resolution image synthesis.&#34; Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021.&lt;/p&gt;</summary>
  </entry>
</feed>