<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-03T01:30:07Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>qdrant/fastembed</title>
    <updated>2024-04-03T01:30:07Z</updated>
    <id>tag:github.com,2024-04-03:/qdrant/fastembed</id>
    <link href="https://github.com/qdrant/fastembed" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Fast, Accurate, Lightweight Python library to make State of the Art Embedding&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;‚ö°Ô∏è What is FastEmbed?&lt;/h1&gt; &#xA;&lt;p&gt;FastEmbed is a lightweight, fast, Python library built for embedding generation. We &lt;a href=&#34;https://qdrant.github.io/fastembed/examples/Supported_Models/&#34;&gt;support popular text models&lt;/a&gt;. Please &lt;a href=&#34;https://github.com/qdrant/fastembed/issues/new&#34;&gt;open a GitHub issue&lt;/a&gt; if you want us to add a new model.&lt;/p&gt; &#xA;&lt;p&gt;The default text embedding (&lt;code&gt;TextEmbedding&lt;/code&gt;) model is Flag Embedding, presented in the &lt;a href=&#34;https://huggingface.co/spaces/mteb/leaderboard&#34;&gt;MTEB&lt;/a&gt; leaderboard. It supports &#34;query&#34; and &#34;passage&#34; prefixes for the input text. Here is an example for &lt;a href=&#34;https://qdrant.github.io/fastembed/examples/Retrieval_with_FastEmbed/&#34;&gt;Retrieval Embedding Generation&lt;/a&gt; and how to use &lt;a href=&#34;https://qdrant.github.io/fastembed/examples/Usage_With_Qdrant/&#34;&gt;FastEmbed with Qdrant&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üìà Why FastEmbed?&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Light: FastEmbed is a lightweight library with few external dependencies. We don&#39;t require a GPU and don&#39;t download GBs of PyTorch dependencies, and instead use the ONNX Runtime. This makes it a great candidate for serverless runtimes like AWS Lambda.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Fast: FastEmbed is designed for speed. We use the ONNX Runtime, which is faster than PyTorch. We also use data-parallelism for encoding large datasets.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Accurate: FastEmbed is better than OpenAI Ada-002. We also &lt;a href=&#34;https://qdrant.github.io/fastembed/examples/Supported_Models/&#34;&gt;supported&lt;/a&gt; an ever expanding set of models, including a few multilingual models.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;üöÄ Installation&lt;/h2&gt; &#xA;&lt;p&gt;To install the FastEmbed library, pip works:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install fastembed&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üìñ Quickstart&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from fastembed import TextEmbedding&#xA;from typing import List&#xA;&#xA;# Example list of documents&#xA;documents: List[str] = [&#xA;    &#34;This is built to be faster and lighter than other embedding libraries e.g. Transformers, Sentence-Transformers, etc.&#34;,&#xA;    &#34;fastembed is supported by and maintained by Qdrant.&#34;,&#xA;]&#xA;&#xA;# This will trigger the model download and initialization&#xA;embedding_model = TextEmbedding()&#xA;print(&#34;The model BAAI/bge-small-en-v1.5 is ready to use.&#34;)&#xA;&#xA;embeddings_generator = embedding_model.embed(documents)  # reminder this is a generator&#xA;embeddings_list = list(embedding_model.embed(documents))&#xA;  # you can also convert the generator to a list, and that to a numpy array&#xA;len(embeddings_list[0]) # Vector of 384 dimensions&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage with Qdrant&lt;/h2&gt; &#xA;&lt;p&gt;Installation with Qdrant Client in Python:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install qdrant-client[fastembed]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You might have to use &lt;code&gt;pip install &#39;qdrant-client[fastembed]&#39;&lt;/code&gt; on zsh.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from qdrant_client import QdrantClient&#xA;&#xA;# Initialize the client&#xA;client = QdrantClient(&#34;localhost&#34;, port=6333) # For production&#xA;# client = QdrantClient(&#34;:memory:&#34;) # For small experiments&#xA;&#xA;# Prepare your documents, metadata, and IDs&#xA;docs = [&#34;Qdrant has Langchain integrations&#34;, &#34;Qdrant also has Llama Index integrations&#34;]&#xA;metadata = [&#xA;    {&#34;source&#34;: &#34;Langchain-docs&#34;},&#xA;    {&#34;source&#34;: &#34;Llama-index-docs&#34;},&#xA;]&#xA;ids = [42, 2]&#xA;&#xA;# If you want to change the model:&#xA;# client.set_model(&#34;sentence-transformers/all-MiniLM-L6-v2&#34;)&#xA;# List of supported models: https://qdrant.github.io/fastembed/examples/Supported_Models&#xA;&#xA;# Use the new add() instead of upsert()&#xA;# This internally calls embed() of the configured embedding model&#xA;client.add(&#xA;    collection_name=&#34;demo_collection&#34;,&#xA;    documents=docs,&#xA;    metadata=metadata,&#xA;    ids=ids&#xA;)&#xA;&#xA;search_result = client.query(&#xA;    collection_name=&#34;demo_collection&#34;,&#xA;    query_text=&#34;This is a query document&#34;&#xA;)&#xA;print(search_result)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Similar Work&lt;/h4&gt; &#xA;&lt;p&gt;Ilyas M. wrote about using &lt;a href=&#34;https://twitter.com/IlysMoutawwakil/status/1705215192425288017&#34;&gt;FlagEmbeddings with Optimum&lt;/a&gt; over CUDA.&lt;/p&gt;</summary>
  </entry>
</feed>