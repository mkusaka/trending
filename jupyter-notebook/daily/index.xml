<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-09T01:41:00Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>veekaybee/what_are_embeddings</title>
    <updated>2023-06-09T01:41:00Z</updated>
    <id>tag:github.com,2023-06-09:/veekaybee/what_are_embeddings</id>
    <link href="https://github.com/veekaybee/what_are_embeddings" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A deep dive into embeddings starting from fundamentals&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/veekaybee/what_are_embeddings/raw/main/kandinsky.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;What are embeddings?&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains the generated LaTex document, website, and complementary notebook code for &lt;a href=&#34;https://vickiboykis.com/what_are_embeddings/&#34;&gt;&#34;What are Embeddings&#34;.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://zenodo.org/badge/latestdoi/644343479&#34;&gt;&lt;img src=&#34;https://zenodo.org/badge/644343479.svg?sanitize=true&#34; alt=&#34;DOI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Abstract&lt;/h2&gt; &#xA;&lt;p&gt;Over the past decade, embeddings --- numerical representations of non-tabular machine learning features used as input to deep learning models --- have become a foundational data structure in industrial machine learning systems. TF-IDF, PCA, and one-hot encoding have always been key tools in machine learning systems as ways to compress and make sense of large amounts of textual data. However, traditional approaches were limited in the amount of context they could reason about with increasing amounts of data. As the volume, velocity, and variety of data captured by modern applications has exploded, creating approaches specifically tailored to scale has become increasingly important.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1301.3781&#34;&gt;Google&#39;s Word2Vec paper&lt;/a&gt; made an important step in moving from simple statistical representations to semantic meaning of words. The subsequent rise of the &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34;&gt;Transformer architecture&lt;/a&gt; and transfer learning, as well as the latest surge in generative methods has enabled the growth of embeddings as a foundational machine learning data structure. This survey paper aims to provide a deep dive into what embeddings are, their history, and usage patterns in industry.&lt;/p&gt; &#xA;&lt;h2&gt;Running&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://github.com/veekaybee/what_are_embeddings/raw/main/.github/workflows/main.yaml&#34;&gt;LaTex document&lt;/a&gt; is written in Overleaf and deployed to GitHub, where it&#39;s compiled via Actions. The site is likewise generated via Actions from the &lt;code&gt;site&lt;/code&gt; branch. The notebooks are flying fast and free and not under any kind of CI whatsoever.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;If you have any changes that you&#39;d like to make to the document including clarification or typo fixes, you&#39;ll need to build the LaTeX artifact. I use GitHub to track issues and feature requests, as well as accept pull requests. Pull requests are the best way to propose changes to the codebase:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Fork the repo and create your branch from &lt;code&gt;main&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Make your changes in your fork.&lt;/li&gt; &#xA; &lt;li&gt;Make sure that your LaTeX document compiles. The GH action that triggers the PDF is set to run on PR into main.&lt;/li&gt; &#xA; &lt;li&gt;Ensure that the document compiles to a PDF correctly and inspect the output.&lt;/li&gt; &#xA; &lt;li&gt;Make sure your code lints.&lt;/li&gt; &#xA; &lt;li&gt;Issue that pull request!&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Citing&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@software{Boykis_What_are_embeddings_2023,&#xA;author = {Boykis, Vicki},&#xA;doi = {10.5281/zenodo.8015029},&#xA;month = jun,&#xA;title = {{What are embeddings?}},&#xA;url = {https://github.com/veekaybee/what_are_embeddings},&#xA;version = {1.0.1},&#xA;year = {2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>