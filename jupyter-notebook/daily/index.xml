<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-02-03T01:39:09Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>PsorTheDoctor/artificial-intelligence</title>
    <updated>2023-02-03T01:39:09Z</updated>
    <id>tag:github.com,2023-02-03:/PsorTheDoctor/artificial-intelligence</id>
    <link href="https://github.com/PsorTheDoctor/artificial-intelligence" rel="alternate"></link>
    <summary type="html">&lt;p&gt;AI projects in python, mostly Jupyter notebooks.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;artificial-intelligence&lt;/h1&gt; &#xA;&lt;p&gt;AI projects in python, mostly Jupyter notebooks.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>tanelp/tiny-diffusion</title>
    <updated>2023-02-03T01:39:09Z</updated>
    <id>tag:github.com,2023-02-03:/tanelp/tiny-diffusion</id>
    <link href="https://github.com/tanelp/tiny-diffusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A minimal PyTorch implementation of probabilistic diffusion models for 2D datasets.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;tiny-diffusion&lt;/h1&gt; &#xA;&lt;p&gt;A minimal PyTorch implementation of probabilistic diffusion models for 2D datasets. Get started by running &lt;code&gt;python ddpm.py -h&lt;/code&gt; to explore the available options for training.&lt;/p&gt; &#xA;&lt;h2&gt;Forward process&lt;/h2&gt; &#xA;&lt;p&gt;A visualization of the forward diffusion process being applied to a dataset of one thousand 2D points. Note that the dinosaur is not a single training example, it represents each 2D point in the dataset.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tanelp/tiny-diffusion/master/static/forward.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Reverse process&lt;/h2&gt; &#xA;&lt;p&gt;This illustration shows how the reverse process recovers the distribution of the training data.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tanelp/tiny-diffusion/master/static/reverse.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Ablations&lt;/h2&gt; &#xA;&lt;p&gt;I have run a series of ablations experiments on hyperparameters, such as learning rate and model size, and visualized the learning process. The columns in the graphs represent the checkpoint epoch, and the rows indicate the hyperparameter values. Each cell displays one thousand generated 2D points.&lt;/p&gt; &#xA;&lt;h3&gt;learning rate&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tanelp/tiny-diffusion/master/static/learning_rate.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The learning process is sensitive to the learning rate. At first, the model&#39;s output was poor, causing me to suspect a bug. However, simply changing the learning rate value resolved the problem.&lt;/p&gt; &#xA;&lt;h3&gt;dataset&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tanelp/tiny-diffusion/master/static/datasets.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The current model configuration doesn&#39;t work well on the &lt;code&gt;line&lt;/code&gt; dataset, which I consider the most basic among them. The corners should be clear and sharp, but they are fuzzy.&lt;/p&gt; &#xA;&lt;h3&gt;num timesteps&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tanelp/tiny-diffusion/master/static/num_timesteps.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;A longer diffusion process results in a better output. With fewer timesteps, the dinosaur is incomplete, missing points from the top and bottom.&lt;/p&gt; &#xA;&lt;h3&gt;variance schedule&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tanelp/tiny-diffusion/master/static/beta_schedule.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The quadratic schedule does not yield better results. Other schedules like cosine or sigmoid should also be considered.&lt;/p&gt; &#xA;&lt;h3&gt;hidden size&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tanelp/tiny-diffusion/master/static/hidden_size.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The capacity of the model doesn&#39;t seem to be a bottleneck, as similar results are obtained across various hidden layer sizes.&lt;/p&gt; &#xA;&lt;h3&gt;number of hidden layers&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tanelp/tiny-diffusion/master/static/num_hidden_layers.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;As in the hidden size ablation run, the capacity of the model does not seem to be a limiting factor.&lt;/p&gt; &#xA;&lt;h3&gt;positional embedding (timestep)&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tanelp/tiny-diffusion/master/static/time_embedding.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The model benefits from the timestep information, but the specific method of encoding the timestep is not important.&lt;/p&gt; &#xA;&lt;h3&gt;positional embedding (inputs)&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tanelp/tiny-diffusion/master/static/input_embedding.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The use of sinusoidal embeddings for the inputs helps with learning high-frequency functions in low-dimensional problem domains, such as mapping each (x, y) pixel coordinate to (r, g, b) color, as demonstrated in &lt;a href=&#34;https://bmild.github.io/fourfeat/&#34;&gt;this study&lt;/a&gt;. The same holds true in the current scenario.&lt;/p&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The dino dataset comes from the &lt;a href=&#34;https://www.autodesk.com/research/publications/same-stats-different-graphs&#34;&gt;Datasaurus Dozen&lt;/a&gt; data.&lt;/li&gt; &#xA; &lt;li&gt;HuggingFace&#39;s &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;diffusers&lt;/a&gt; library.&lt;/li&gt; &#xA; &lt;li&gt;lucidrains&#39; &lt;a href=&#34;https://github.com/lucidrains/denoising-diffusion-pytorch&#34;&gt;DDPM implementation in PyTorch&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Jonathan Ho&#39;s &lt;a href=&#34;https://github.com/hojonathanho/diffusion&#34;&gt;implementation of DDPM&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;InFoCusp&#39;s &lt;a href=&#34;https://github.com/InFoCusp/diffusion_models&#34;&gt;DDPM implementation in tf&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>