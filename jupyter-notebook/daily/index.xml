<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-14T01:39:07Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>kyegomez/LongNet</title>
    <updated>2023-07-14T01:39:07Z</updated>
    <id>tag:github.com,2023-07-14:/kyegomez/LongNet</id>
    <link href="https://github.com/kyegomez/LongNet" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Implementation of plug in and play Attention from &#34;LongNet: Scaling Transformers to 1,000,000,000 Tokens&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Agora&lt;/h1&gt; &#xA;&lt;p&gt;This implementation of LongNet is brought to you by Agora, we&#39;re an all-new open source AI research organization with 1,500+ AI researchers all striving to advance Humanity!&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/kyegomez/LongNet/master/agora-banner-water.png&#34; alt=&#34;Agora banner&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/qUtxnK2NMf&#34;&gt;Join us and help contribute to LongNet and or recieve FAST support in the Agora discord!&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;LongNet: Scaling Transformers to 1,000,000,000 Tokens&lt;/h1&gt; &#xA;&lt;p&gt;This is an open source implementation for the paper &lt;a href=&#34;https://arxiv.org/abs/2307.02486&#34;&gt;LongNet: Scaling Transformers to 1,000,000,000 Tokens&lt;/a&gt; by Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Furu Wei. The LongNet is a Transformer variant designed to scale sequence length up to more than 1 billion tokens without sacrificing performance on shorter sequences.&lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;Scaling sequence length has become a critical bottleneck in the era of large language models. However, existing methods struggle with either computational complexity or model expressivity, rendering the maximum sequence length restricted. In this paper, they introduce LongNet, a Transformer variant that can scale sequence length to more than 1 billion tokens, without sacrificing the performance on shorter sequences. Specifically, they propose dilated attention, which expands the attentive field exponentially as the distance grows.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;LongNet has significant advantages:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;It has a linear computation complexity and a logarithm dependency between tokens.&lt;/li&gt; &#xA; &lt;li&gt;It can be served as a distributed trainer for extremely long sequences.&lt;/li&gt; &#xA; &lt;li&gt;Its dilated attention is a drop-in replacement for standard attention, which can be seamlessly integrated with the existing Transformer-based optimization.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Experiment results demonstrate that LongNet yields strong performance on both long-sequence modeling and general language tasks. Their work opens up new possibilities for modeling very long sequences, e.g., treating a whole corpus or even the entire Internet as a sequence.&lt;/p&gt; &#xA;&lt;p&gt;Here&#39;s the updated usage and installation section with two methods: git clone or pip install LongNet:&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;c You can install LongNet using one of the following methods:&lt;/p&gt; &#xA;&lt;h3&gt;Method 1: Git Clone&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the LongNet repository from GitHub:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/kyegomez/LongNet.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Navigate to the cloned directory:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd LongNet&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Prepare &lt;code&gt;flash_attn&lt;/code&gt; library&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&#xA;cd flash_attn&#xA;&#xA;python setup.py install&#xA;&#xA;cd ..&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Install the required dependencies:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Method 2: Pip Install&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Note that pip install does not work as the &lt;code&gt;flash-attn&lt;/code&gt; library cannot be compiled since it has custom CUDA Kernels and they need to be built manually.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install LongNet directly from PyPI using pip:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install LongNet&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please note that LongNet requires a compatible Python version (tested with Python 3.7).&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Once you have installed LongNet, you can use the &lt;code&gt;DilatedAttention&lt;/code&gt; class as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;import torch.nn as nn&#xA;from LongNet import DilatedAttention&#xA;&#xA;# Replace this with your correct GPU device&#xA;device = &#34;cuda:0&#34;&#xA;dtype = torch.float16&#xA;&#xA;# Create an instance of DilatedAttention&#xA;d_model = 512&#xA;num_heads = 8&#xA;dilation_rate = 2&#xA;segment_size = 64&#xA;dropout = 0.2  # Specify the dropout rate&#xA;attention = DilatedAttention(&#xA;    d_model=d_model,&#xA;    num_heads=num_heads,&#xA;    dilation_rate=dilation_rate,&#xA;    segment_size=segment_size,&#xA;    dropout=dropout,&#xA;).to(device, dtype=dtype)&#xA;&#xA;# Create some dummy input data&#xA;batch_size = 16&#xA;seq_len = 128&#xA;input_dim = d_model&#xA;inputs = torch.randn(batch_size, seq_len, input_dim, device=device, dtype=dtype)&#xA;&#xA;# Forward pass&#xA;outputs = attention(inputs)&#xA;&#xA;# Print the output shape&#xA;print(outputs.shape)  # Expected: [batch_size, seq_len, d_model]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Training the Model&lt;/h1&gt; &#xA;&lt;p&gt;There are 2 methods, one is &lt;code&gt;accelerate&lt;/code&gt; and the other &lt;code&gt;from LongNet import Train&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Method 1&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Git clone installation&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Init your parameters &lt;code&gt;accelerate config&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Then &lt;code&gt;accelerate launch LongNet/training.py&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Method 2&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Pip install method&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;from LongNet import Train&#xA;&#xA;Train()&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In the example above, we create an instance of the &lt;code&gt;DilatedAttention&lt;/code&gt; class with the specified hyperparameters. We then generate some dummy input data and pass it through the attention mechanism to obtain the outputs. Finally, we print the shape of the output tensor.&lt;/p&gt; &#xA;&lt;h1&gt;DilatedAttention Documentation&lt;/h1&gt; &#xA;&lt;h2&gt;Algorithmic Psueodocode:&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;1. Initialize the input (Q, K, V) and split them into segments {(Qei, Kei, Vei)} with equal segment length w.&#xA;2. Sparsify each segment along the sequence dimension by selecting the rows with an interval r.&#xA;3. Feed the sparsified segments into the attention in parallel.&#xA;4. Scatter and concatenate the output O from the attention.&#xA;5. Implement a mixture of dilated attentions with different segment sizes and dilation rates {ri, wi}.&#xA;6. For multi-head dilated attention, differ the computation among different heads by sparsifying different parts of the query-key-value pairs.&#xA;7. Concatenate the outputs of different heads into a final output.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Class Definition&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class DilatedAttention(nn.Module):&#xA;    def __init__(self, d_model, num_heads, dilation_rate, segment_size, dropout=0.0, causal=False, use_xpos=False, use_rel_pos_bias=False ):&#xA;        ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Parameters&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;d_model&lt;/code&gt; (int): The dimensionality of the model. This should match the dimension of the input to the layer.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;num_heads&lt;/code&gt; (int): The number of attention heads to use in the &lt;code&gt;FlashMHA&lt;/code&gt; attention mechanism.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;dilation_rate&lt;/code&gt; (int): The dilation rate to use when processing the input sequence. Larger values will result in fewer, but wider, attention computations.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;segment_size&lt;/code&gt; (int): The size of the segments into which the input sequence is divided before dilating and computing attention.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;dropout&lt;/code&gt; (float, optional): The dropout rate to apply to the attention outputs. Default is 0.0.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;causal&lt;/code&gt; (bool, optional): If True, a causal mask will be applied to the attention outputs, preventing any given position from attending to future positions. Default is False.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;use_xpos&lt;/code&gt; (optional): If set to True, xpos is used for positional encoding. Default: False&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;use_rel_pos_bias&lt;/code&gt; (optional): If set to True, relative position bias is used in the attention mechanism. Default: False&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Creating an Instance&lt;/h3&gt; &#xA;&lt;p&gt;First, you need to create an instance of the &lt;code&gt;DilatedAttention&lt;/code&gt; class. Here is how you do it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dilated_attn = DilatedAttention(d_model=512, num_heads=8, dilation_rate=2, segment_size=64, dropout=0.1, causal=True, use_xpos=False, use_rel_pos_bias=False)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In this example, we&#39;re creating a &lt;code&gt;DilatedAttention&lt;/code&gt; layer with a model dimensionality of 512, 8 attention heads, a dilation rate of 2, a segment size of 64, a dropout rate of 0.1, and causal masking enabled.&lt;/p&gt; &#xA;&lt;h3&gt;Forward Pass&lt;/h3&gt; &#xA;&lt;p&gt;To perform a forward pass through the layer, simply call the instance as if it were a function, passing in your input tensor:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;&#xA;# Assume x is your input tensor with shape (batch_size, sequence_length, d_model)&#xA;x = torch.rand(16, 1000, 512).to(device)&#xA;&#xA;output = dilated_attn(x)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In this example, the input tensor &lt;code&gt;x&lt;/code&gt; has a batch size of 16, a sequence length of 1000, and a model dimensionality of 512. The output tensor will have the same shape as the input tensor.&lt;/p&gt; &#xA;&lt;h3&gt;Integration with Other Layers&lt;/h3&gt; &#xA;&lt;p&gt;You can integrate the &lt;code&gt;DilatedAttention&lt;/code&gt; layer into a larger model just like any other PyTorch layer. For example, here&#39;s how you might use it as part of a simple transformer-like model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class SimpleTransformer(nn.Module):&#xA;    def __init__(self, d_model, num_heads, dilation_rate, segment_size, dropout):&#xA;        super().__init__()&#xA;&#xA;        self.dilated_attn = DilatedAttention(d_model, num_heads, dilation_rate, segment_size, dropout, causal=True, use_xpos=False, use_rel_pos_bias=False)&#xA;        self.fc = nn.Linear(d_model, 10)  # Assume we&#39;re doing a 10-class classification task&#xA;&#xA;    def forward(self, x):&#xA;        x = self.dilated_attn(x)&#xA;        x = self.fc(x[:, 0])  # Use the first position output as the &#34;CLS&#34; token&#xA;        return x&#xA;&#xA;model = SimpleTransformer(d_model=512, num_heads=8, dilation_rate=2, segment_size=64, dropout=0.1)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In this example, we first pass the input tensor through the &lt;code&gt;DilatedAttention&lt;/code&gt; layer, then we pass the output of the first position through a fully-connected layer to perform a classification task.&lt;/p&gt; &#xA;&lt;h2&gt;DilationAttention Overview&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;DilatedAttention&lt;/code&gt; is a neural network architecture that incorporates attention mechanisms, specifically the multi-head attention, in a dilated manner. The main idea behind this architecture is to leverage the efficient attention calculation capabilities of the &lt;code&gt;FlashMHA&lt;/code&gt; method, which is part of the &lt;code&gt;flash_attn&lt;/code&gt; module, while also providing the ability to handle longer sequences with reduced computation through dilation.&lt;/p&gt; &#xA;&lt;h2&gt;Components&lt;/h2&gt; &#xA;&lt;p&gt;The class &lt;code&gt;DilatedAttention&lt;/code&gt; has the following primary components:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;FlashMHA attention&lt;/strong&gt;: A fast and efficient multi-head attention mechanism implemented using the &lt;code&gt;FlashMHA&lt;/code&gt; method. This is the main attention computation method used in the architecture.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Dilation&lt;/strong&gt;: Dilating the input sequences allows the model to handle longer sequences with fewer computations, making the architecture more scalable and efficient.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Causal masking (optional)&lt;/strong&gt;: If the &lt;code&gt;causal&lt;/code&gt; argument is set to &lt;code&gt;True&lt;/code&gt;, a causal mask is applied to the attention outputs, ensuring that each output position only depends on earlier positions in the sequence. This feature is particularly useful when dealing with sequential data where future dependencies should not be considered.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Dropout&lt;/strong&gt;: A dropout layer that can be configured to add regularization to the model and prevent overfitting.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How It Works&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;DilatedAttention&lt;/code&gt; model works in the following steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Input Reshape&lt;/strong&gt;: Reshapes the input into smaller segments based on the provided &lt;code&gt;segment_size&lt;/code&gt; and then dilates it by selecting every &lt;code&gt;dilation_rate&lt;/code&gt; segment.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Attention Computation&lt;/strong&gt;: Uses &lt;code&gt;FlashMHA&lt;/code&gt; to compute the attention over the dilated segments.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Causal Masking&lt;/strong&gt;: If &lt;code&gt;causal&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;, a causal mask is applied to the attention output. This ensures that the output at each position in the sequence does not depend on any future positions.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Dropout&lt;/strong&gt;: Applies dropout to the attention outputs as a means of regularization.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Output Reshape&lt;/strong&gt;: Reshapes the output to match the original sequence length, concatenating the dilated segments.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Why It Works&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;DilatedAttention&lt;/code&gt; model achieves efficiency and scalability in several ways:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Efficient attention calculation&lt;/strong&gt;: The use of &lt;code&gt;FlashMHA&lt;/code&gt; enables efficient and fast attention computation.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Dilation&lt;/strong&gt;: Dilation allows the model to handle longer sequences with reduced computation, effectively making the model more scalable.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Causal masking&lt;/strong&gt;: By ensuring that each output position only depends on earlier positions in the sequence, the model becomes suitable for tasks involving sequential data.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Potential Optimizations&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Parallelization&lt;/strong&gt;: Take advantage of the parallel processing capabilities of modern GPUs for the dilation and reshaping steps.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Memory optimization&lt;/strong&gt;: Efficient memory usage could be achieved through gradient checkpointing or activation pruning.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Pre-computation&lt;/strong&gt;: If some portions of the input data remain constant across multiple operations, pre-compute those portions and store the results for reuse.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Batch normalization&lt;/strong&gt;: Incorporating batch normalization layers could help to speed up the learning process and improve generalization.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Pruning and Quantization&lt;/strong&gt;: Pruning unnecessary connections and quantizing the model parameters can help in reducing the model&#39;s memory footprint and speed up computation without sacrificing much accuracy.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Share with Friends&lt;/h2&gt; &#xA;&lt;p&gt;Share LongNet with your friends and colleagues who might find it useful. Simply click on the links below to share on various platforms:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fgithub.com%2Fkyegomez%2FLongNet&#34;&gt;Facebook&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/intent/tweet?url=https%3A%2F%2Fgithub.com%2Fkyegomez%2FLongNet&amp;amp;text=Check%20out%20the%20LongNet%20repository%2C%20an%20implementation%20for%20scaling%20Transformers%20to%201%2C000%2C000%2C000%20tokens.%20%23LongNet%20%23Transformers&#34;&gt;Twitter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/shareArticle?url=https%3A%2F%2Fgithub.com%2Fkyegomez%2FLongNet&amp;amp;title=LongNet%3A%20Scaling%20Transformers%20to%201%2C000%2C000%2C000%20Tokens&#34;&gt;LinkedIn&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://reddit.com/submit?url=https%3A%2F%2Fgithub.com%2Fkyegomez%2FLongNet&amp;amp;title=LongNet%3A%20Scaling%20Transformers%20to%201%2C000%2C000%2C000%20Tokens&#34;&gt;Reddit&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://wa.me/?text=Check%20out%20the%20LongNet%20repository%2C%20an%20implementation%20for%20scaling%20Transformers%20to%201%2C000%2C000%2C000%20tokens%3A%20https%3A%2F%2Fgithub.com%2Fkyegomez%2FLongNet&#34;&gt;WhatsApp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;mailto:?subject=Check%20out%20the%20LongNet%20repository&amp;amp;body=Hey%2C%0A%0ACheck%20out%20the%20LongNet%20repository%2C%20an%20implementation%20for%20scaling%20Transformers%20to%201%2C000%2C000%2C000%20tokens%3A%0A%0Ahttps%3A%2F%2Fgithub.com%2Fkyegomez%2FLongNet%0A%0AEnjoy%21&#34;&gt;Email&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://news.ycombinator.com/submitlink?u=https%3A%2F%2Fgithub.com%2Fkyegomez%2FLongNet&amp;amp;t=LongNet%3A%20Scaling%20Transformers%20to%201%2C000%2C000%2C000%20Tokens&#34;&gt;Hacker News&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Thank you for sharing!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/kyegomez/LongNet&#34;&gt;Share LongNet Repository&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Roadmap&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Test and evaluate and patch.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;And, create an interation of &lt;code&gt;DilatedAttention&lt;/code&gt; with &lt;code&gt;FlashBlocksparseMHA&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create a multi-modal &lt;code&gt;DilationAttention&lt;/code&gt; with multiway, sub layernorm, and xpos, sub layernorm, QK Layernorm, One write query head maybe&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Integrate Alibi and xpos for even further ridicoulus length extrapolation&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Recreate in Triton or Jax for ultra mega speed boost&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Integrate &lt;a href=&#34;https://github.com/epfml/dynamic-sparse-flash-attention/raw/main/runtime-experiments/timeperf-hash-and-qk-sparse.ipynb&#34;&gt;Dynamic sparse flash attention&lt;/a&gt; with DilatedAttention&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{ding2023longnet,&#xA;  title={LongNet: Scaling Transformers to 1,000,000,000 Tokens},&#xA;  author={Ding, Jiayu and Ma, Shuming and Dong, Li and Zhang, Xingxing and Huang, Shaohan and Wang, Wenhui and Wei, Furu},&#xA;  booktitle={Proceedings of the 10th International Conference on Learning Representations},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>open-mmlab/mmagic</title>
    <updated>2023-07-14T01:39:07Z</updated>
    <id>tag:github.com,2023-07-14:/open-mmlab/mmagic</id>
    <link href="https://github.com/open-mmlab/mmagic" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OpenMMLab Multimodal Advanced, Generative, and Intelligent Creation Toolbox. Unlock the magic 🪄: Generative-AI (AIGC), easy-to-use APIs, awsome model zoo, diffusion models, for text-to-image generation, image/video restoration/enhancement, etc.&lt;/p&gt;&lt;hr&gt;&lt;div id=&#34;top&#34; align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/docs/en/_static/image/mmagic-logo.png&#34; width=&#34;500px&#34;&gt; &#xA; &lt;div&gt;&#xA;  &amp;nbsp;&#xA; &lt;/div&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;font size=&#34;10&#34;&gt;&lt;b&gt;M&lt;/b&gt;ultimodal &lt;b&gt;A&lt;/b&gt;dvanced, &lt;b&gt;G&lt;/b&gt;enerative, and &lt;b&gt;I&lt;/b&gt;ntelligent &lt;b&gt;C&lt;/b&gt;reation (MMagic [em&#39;mædʒɪk])&lt;/font&gt; &#xA; &lt;/div&gt; &#xA; &lt;div&gt;&#xA;  &amp;nbsp;&#xA; &lt;/div&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;b&gt;&lt;font size=&#34;5&#34;&gt;OpenMMLab website&lt;/font&gt;&lt;/b&gt; &#xA;  &lt;sup&gt; &lt;a href=&#34;https://openmmlab.com&#34;&gt; &lt;i&gt;&lt;font size=&#34;4&#34;&gt;HOT&lt;/font&gt;&lt;/i&gt; &lt;/a&gt; &lt;/sup&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &#xA;  &lt;b&gt;&lt;font size=&#34;5&#34;&gt;OpenMMLab platform&lt;/font&gt;&lt;/b&gt; &#xA;  &lt;sup&gt; &lt;a href=&#34;https://platform.openmmlab.com&#34;&gt; &lt;i&gt;&lt;font size=&#34;4&#34;&gt;TRY IT OUT&lt;/font&gt;&lt;/i&gt; &lt;/a&gt; &lt;/sup&gt; &#xA; &lt;/div&gt; &#xA; &lt;div&gt;&#xA;  &amp;nbsp;&#xA; &lt;/div&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://pypi.org/project/mmagic/&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/mmagic.svg?sanitize=true&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://mmagic.readthedocs.io/en/latest/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/docs-latest-blue&#34; alt=&#34;docs&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmagic/actions&#34;&gt;&lt;img src=&#34;https://github.com/open-mmlab/mmagic/workflows/build/badge.svg?sanitize=true&#34; alt=&#34;badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/open-mmlab/mmagic&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/open-mmlab/mmagic/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmagic/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/open-mmlab/mmagic.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmagic/issues&#34;&gt;&lt;img src=&#34;https://isitmaintained.com/badge/open/open-mmlab/mmagic.svg?sanitize=true&#34; alt=&#34;open issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmagic/issues&#34;&gt;&lt;img src=&#34;https://isitmaintained.com/badge/resolution/open-mmlab/mmagic.svg?sanitize=true&#34; alt=&#34;issue resolution&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://mmagic.readthedocs.io/en/latest/&#34;&gt;📘Documentation&lt;/a&gt; | &lt;a href=&#34;https://mmagic.readthedocs.io/en/latest/get_started/install.html&#34;&gt;🛠️Installation&lt;/a&gt; | &lt;a href=&#34;https://mmagic.readthedocs.io/en/latest/model_zoo/overview.html&#34;&gt;📊Model Zoo&lt;/a&gt; | &lt;a href=&#34;https://mmagic.readthedocs.io/en/latest/changelog.html&#34;&gt;🆕Update News&lt;/a&gt; | &lt;a href=&#34;https://github.com/open-mmlab/mmagic/projects&#34;&gt;🚀Ongoing Projects&lt;/a&gt; | &lt;a href=&#34;https://github.com/open-mmlab/mmagic/issues&#34;&gt;🤔Reporting Issues&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;English | &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/README_zh-CN.md&#34;&gt;简体中文&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://openmmlab.medium.com/&#34; style=&#34;text-decoration:none;&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/25839884/218352562-cdded397-b0f3-4ca1-b8dd-a60df8dca75b.png&#34; width=&#34;3%&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png&#34; width=&#34;3%&#34; alt=&#34;&#34;&gt; &#xA; &lt;a href=&#34;https://discord.gg/raweFPmdzG&#34; style=&#34;text-decoration:none;&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/25839884/218347213-c080267f-cbb6-443e-8532-8e1ed9a58ea9.png&#34; width=&#34;3%&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png&#34; width=&#34;3%&#34; alt=&#34;&#34;&gt; &#xA; &lt;a href=&#34;https://twitter.com/OpenMMLab&#34; style=&#34;text-decoration:none;&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/25839884/218346637-d30c8a0f-3eba-4699-8131-512fb06d46db.png&#34; width=&#34;3%&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png&#34; width=&#34;3%&#34; alt=&#34;&#34;&gt; &#xA; &lt;a href=&#34;https://www.youtube.com/openmmlab&#34; style=&#34;text-decoration:none;&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/25839884/218346691-ceb2116a-465a-40af-8424-9f30d2348ca9.png&#34; width=&#34;3%&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;🚀 What&#39;s New &lt;a&gt;&lt;img width=&#34;35&#34; height=&#34;20&#34; src=&#34;https://user-images.githubusercontent.com/12782558/212848161-5e783dd6-11e8-4fe0-bbba-39ffb77730be.png&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h3&gt;New release &lt;a href=&#34;https://github.com/open-mmlab/mmagic/releases/tag/v1.0.1&#34;&gt;&lt;strong&gt;MMagic v1.0.1&lt;/strong&gt;&lt;/a&gt; [26/05/2023]:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support tomesd for StableDiffusion speed-up.&lt;/li&gt; &#xA; &lt;li&gt;Support all inpainting/matting/image restoration models inferencer.&lt;/li&gt; &#xA; &lt;li&gt;Support animated drawings.&lt;/li&gt; &#xA; &lt;li&gt;Support Style-Based Global Appearance Flow for Virtual Try-On.&lt;/li&gt; &#xA; &lt;li&gt;Fix inferencer in pip-install.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We are excited to announce the release of MMagic v1.0.0 that inherits from &lt;a href=&#34;https://github.com/open-mmlab/mmediting&#34;&gt;MMEditing&lt;/a&gt; and &lt;a href=&#34;https://github.com/open-mmlab/mmgeneration&#34;&gt;MMGeneration&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;After iterative updates with OpenMMLab 2.0 framework and merged with MMGeneration, MMEditing has become a powerful tool that supports low-level algorithms based on both GAN and CNN. Today, MMEditing embraces Generative AI and transforms into a more advanced and comprehensive AIGC toolkit: &lt;strong&gt;MMagic&lt;/strong&gt; (&lt;strong&gt;M&lt;/strong&gt;ultimodal &lt;strong&gt;A&lt;/strong&gt;dvanced, &lt;strong&gt;G&lt;/strong&gt;enerative, and &lt;strong&gt;I&lt;/strong&gt;ntelligent &lt;strong&gt;C&lt;/strong&gt;reation). MMagic will provide more agile and flexible experimental support for researchers and AIGC enthusiasts, and help you on your AIGC exploration journey.&lt;/p&gt; &#xA;&lt;p&gt;We highlight the following new features.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;1. New Models&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We support 11 new models in 4 new tasks.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Text2Image / Diffusion &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ControlNet&lt;/li&gt; &#xA;   &lt;li&gt;DreamBooth&lt;/li&gt; &#xA;   &lt;li&gt;Stable Diffusion&lt;/li&gt; &#xA;   &lt;li&gt;Disco Diffusion&lt;/li&gt; &#xA;   &lt;li&gt;GLIDE&lt;/li&gt; &#xA;   &lt;li&gt;Guided Diffusion&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;3D-aware Generation &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;EG3D&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Image Restoration &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;NAFNet&lt;/li&gt; &#xA;   &lt;li&gt;Restormer&lt;/li&gt; &#xA;   &lt;li&gt;SwinIR&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Image Colorization &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;InstColorization&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;2. Magic Diffusion Model&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;For the Diffusion Model, we provide the following &#34;magic&#34; :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support image generation based on Stable Diffusion and Disco Diffusion.&lt;/li&gt; &#xA; &lt;li&gt;Support Finetune methods such as Dreambooth and DreamBooth LoRA.&lt;/li&gt; &#xA; &lt;li&gt;Support controllability in text-to-image generation using ControlNet.&lt;/li&gt; &#xA; &lt;li&gt;Support acceleration and optimization strategies based on xFormers to improve training and inference efficiency.&lt;/li&gt; &#xA; &lt;li&gt;Support video generation based on MultiFrame Render.&lt;/li&gt; &#xA; &lt;li&gt;Support calling basic models and sampling strategies through DiffuserWrapper.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;3. Upgraded Framework&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;By using MMEngine and MMCV of OpenMMLab 2.0 framework, MMagic has upgraded in the following new features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Refactor DataSample to support the combination and splitting of batch dimensions.&lt;/li&gt; &#xA; &lt;li&gt;Refactor DataPreprocessor and unify the data format for various tasks during training and inference.&lt;/li&gt; &#xA; &lt;li&gt;Refactor MultiValLoop and MultiTestLoop, supporting the evaluation of both generation-type metrics (e.g. FID) and reconstruction-type metrics (e.g. SSIM), and supporting the evaluation of multiple datasets at once.&lt;/li&gt; &#xA; &lt;li&gt;Support visualization on local files or using tensorboard and wandb.&lt;/li&gt; &#xA; &lt;li&gt;Support for 33+ algorithms accelerated by Pytorch 2.0.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;MMagic&lt;/strong&gt; has supported all the tasks, models, metrics, and losses in &lt;a href=&#34;https://github.com/open-mmlab/mmediting&#34;&gt;MMEditing&lt;/a&gt; and &lt;a href=&#34;https://github.com/open-mmlab/mmgeneration&#34;&gt;MMGeneration&lt;/a&gt; and unifies interfaces of all components based on &lt;a href=&#34;https://github.com/open-mmlab/mmengine&#34;&gt;MMEngine&lt;/a&gt; 😍.&lt;/p&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/docs/en/changelog.md&#34;&gt;changelog.md&lt;/a&gt; for details and release history.&lt;/p&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/docs/en/migration/overview.md&#34;&gt;migration documents&lt;/a&gt; to migrate from &lt;a href=&#34;https://github.com/open-mmlab/mmagic/tree/0.x&#34;&gt;old version&lt;/a&gt; MMEditing 0.x to new version MMagic 1.x .&lt;/p&gt; &#xA;&lt;h2&gt;📄 Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#-introduction&#34;&gt;📖 Introduction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#-contributing&#34;&gt;🙌 Contributing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#%EF%B8%8F-installation&#34;&gt;🛠️ Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#-model-zoo&#34;&gt;📊 Model Zoo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#-acknowledgement&#34;&gt;🤝 Acknowledgement&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#%EF%B8%8F-citation&#34;&gt;🖊️ Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#-license&#34;&gt;🎫 License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#%EF%B8%8F-%EF%B8%8Fopenmmlab-family&#34;&gt;🏗️ ️OpenMMLab Family&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#top&#34;&gt;🔝Back to top&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;📖 Introduction&lt;/h2&gt; &#xA;&lt;p&gt;MMagic (&lt;strong&gt;M&lt;/strong&gt;ultimodal &lt;strong&gt;A&lt;/strong&gt;dvanced, &lt;strong&gt;G&lt;/strong&gt;enerative, and &lt;strong&gt;I&lt;/strong&gt;ntelligent &lt;strong&gt;C&lt;/strong&gt;reation) is an advanced and comprehensive AIGC toolkit that inherits from &lt;a href=&#34;https://github.com/open-mmlab/mmediting&#34;&gt;MMEditing&lt;/a&gt; and &lt;a href=&#34;https://github.com/open-mmlab/mmgeneration&#34;&gt;MMGeneration&lt;/a&gt;. It is an open-source image and video editing&amp;amp;generating toolbox based on PyTorch. It is a part of the &lt;a href=&#34;https://openmmlab.com/&#34;&gt;OpenMMLab&lt;/a&gt; project.&lt;/p&gt; &#xA;&lt;p&gt;Currently, MMagic support multiple image and video generation/editing tasks.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/49083766/233564593-7d3d48ed-e843-4432-b610-35e3d257765c.mp4&#34;&gt;https://user-images.githubusercontent.com/49083766/233564593-7d3d48ed-e843-4432-b610-35e3d257765c.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The best practice on our main branch works with &lt;strong&gt;Python 3.8+&lt;/strong&gt; and &lt;strong&gt;PyTorch 1.9+&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;✨ Major features&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;State of the Art Models&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;MMagic provides state-of-the-art generative models to process, edit and synthesize images and videos.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Powerful and Popular Applications&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;MMagic supports popular and contemporary image restoration, text-to-image, 3D-aware generation, inpainting, matting, super-resolution and generation applications. Specifically, MMagic supports fine-tuning for stable diffusion and many exciting diffusion&#39;s application such as ControlNet Animation with SAM. MMagic also supports GAN interpolation, GAN projection, GAN manipulations and many other popular GAN’s applications. It’s time to begin your AIGC exploration journey!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Efficient Framework&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;By using MMEngine and MMCV of OpenMMLab 2.0 framework, MMagic decompose the editing framework into different modules and one can easily construct a customized editor framework by combining different modules. We can define the training process just like playing with Legos and provide rich components and strategies. In MMagic, you can complete controls on the training process with different levels of APIs. With the support of &lt;a href=&#34;https://github.com/open-mmlab/mmengine/raw/main/mmengine/model/wrappers/seperate_distributed.py&#34;&gt;MMSeparateDistributedDataParallel&lt;/a&gt;, distributed training for dynamic architectures can be easily implemented.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#top&#34;&gt;🔝Back to top&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🙌 Contributing&lt;/h2&gt; &#xA;&lt;p&gt;More and more community contributors are joining us to make our repo better. Some recent projects are contributed by the community including:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/projects/glide/configs/README.md&#34;&gt;GLIDE&lt;/a&gt; is contributed by @Taited.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/restormer/README.md&#34;&gt;Restormer&lt;/a&gt; is contributed by @AlexZou14.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/swinir/README.md&#34;&gt;SwinIR&lt;/a&gt; is contributed by @Zdafeng.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/projects/README.md&#34;&gt;Projects&lt;/a&gt; is opened to make it easier for everyone to add projects to MMagic.&lt;/p&gt; &#xA;&lt;p&gt;We appreciate all contributions to improve MMagic. Please refer to &lt;a href=&#34;https://github.com/open-mmlab/mmcv/raw/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; in MMCV and &lt;a href=&#34;https://github.com/open-mmlab/mmengine/raw/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; in MMEngine for more details about the contributing guideline.&lt;/p&gt; &#xA;&lt;p align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#top&#34;&gt;🔝Back to top&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🛠️ Installation&lt;/h2&gt; &#xA;&lt;p&gt;MMagic depends on &lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch&lt;/a&gt;, &lt;a href=&#34;https://github.com/open-mmlab/mmengine&#34;&gt;MMEngine&lt;/a&gt; and &lt;a href=&#34;https://github.com/open-mmlab/mmcv&#34;&gt;MMCV&lt;/a&gt;. Below are quick steps for installation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step 1.&lt;/strong&gt; Install PyTorch following &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;official instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step 2.&lt;/strong&gt; Install MMCV, MMEngine and MMagic with &lt;a href=&#34;https://github.com/open-mmlab/mim&#34;&gt;MIM&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip3 install openmim&#xA;mim install &#39;mmcv&amp;gt;=2.0.0&#39;&#xA;mim install &#39;mmengine&#39;&#xA;mim install &#39;mmagic&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step 3.&lt;/strong&gt; Verify MMagic has been successfully installed.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd ~&#xA;python -c &#34;import mmagic; print(mmagic.__version__)&#34;&#xA;# Example output: 1.0.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Getting Started&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;After installing MMagic successfully, now you are able to play with MMagic! To generate an image from text, you only need several lines of codes by MMagic!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from mmagic.apis import MMagicInferencer&#xA;sd_inferencer = MMagicInferencer(model_name=&#39;stable_diffusion&#39;)&#xA;text_prompts = &#39;A panda is having dinner at KFC&#39;&#xA;result_out_dir = &#39;output/sd_res.png&#39;&#xA;sd_inferencer.infer(text=text_prompts, result_out_dir=result_out_dir)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/docs/en/get_started/quick_run.md&#34;&gt;quick run&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/docs/en/user_guides/inference.md&#34;&gt;inference&lt;/a&gt; for the basic usage of MMagic.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Install MMagic from source&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can also experiment on the latest developed version rather than the stable release by installing MMagic from source with the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/open-mmlab/mmagic.git&#xA;cd mmagic&#xA;pip3 install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/docs/en/get_started/install.md&#34;&gt;installation&lt;/a&gt; for more detailed instruction.&lt;/p&gt; &#xA;&lt;p align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#top&#34;&gt;🔝Back to top&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;📊 Model Zoo&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;b&gt;Supported algorithms&lt;/b&gt; &#xA;&lt;/div&gt; &#xA;&lt;table align=&#34;center&#34;&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr align=&#34;center&#34; valign=&#34;bottom&#34;&gt; &#xA;   &lt;td&gt; &lt;b&gt;Conditional GANs&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;Unconditional GANs&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;Image Restoration&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;Image Super-Resolution&lt;/b&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr valign=&#34;top&#34;&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/sngan_proj/README.md&#34;&gt;SNGAN/Projection GAN (ICLR&#39;2018)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/sagan/README.md&#34;&gt;SAGAN (ICML&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/biggan/README.md&#34;&gt;BIGGAN/BIGGAN-DEEP (ICLR&#39;2018)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/dcgan/README.md&#34;&gt;DCGAN (ICLR&#39;2016)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/wgan-gp/README.md&#34;&gt;WGAN-GP (NeurIPS&#39;2017)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/lsgan/README.md&#34;&gt;LSGAN (ICCV&#39;2017)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/ggan/README.md&#34;&gt;GGAN (ArXiv&#39;2017)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/pggan/README.md&#34;&gt;PGGAN (ICLR&#39;2018)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/singan/README.md&#34;&gt;SinGAN (ICCV&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/styleganv1/README.md&#34;&gt;StyleGANV1 (CVPR&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/styleganv2/README.md&#34;&gt;StyleGANV2 (CVPR&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/styleganv3/README.md&#34;&gt;StyleGANV3 (NeurIPS&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/swinir/README.md&#34;&gt;SwinIR (ICCVW&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/nafnet/README.md&#34;&gt;NAFNet (ECCV&#39;2022)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/restormer/README.md&#34;&gt;Restormer (CVPR&#39;2022)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/srcnn/README.md&#34;&gt;SRCNN (TPAMI&#39;2015)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/srgan_resnet/README.md&#34;&gt;SRResNet&amp;amp;SRGAN (CVPR&#39;2016)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/edsr/README.md&#34;&gt;EDSR (CVPR&#39;2017)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/esrgan/README.md&#34;&gt;ESRGAN (ECCV&#39;2018)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/rdn/README.md&#34;&gt;RDN (CVPR&#39;2018)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/dic/README.md&#34;&gt;DIC (CVPR&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/ttsr/README.md&#34;&gt;TTSR (CVPR&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/glean/README.md&#34;&gt;GLEAN (CVPR&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/liif/README.md&#34;&gt;LIIF (CVPR&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/real_esrgan/README.md&#34;&gt;Real-ESRGAN (ICCVW&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt;   &#xA; &lt;/tbody&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr align=&#34;center&#34; valign=&#34;bottom&#34;&gt; &#xA;   &lt;td&gt; &lt;b&gt;Video Super-Resolution&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;Video Interpolation&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;Image Colorization&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;Image Translation&lt;/b&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr valign=&#34;top&#34;&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/edvr/README.md&#34;&gt;EDVR (CVPR&#39;2018)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/tof/README.md&#34;&gt;TOF (IJCV&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/tdan/README.md&#34;&gt;TDAN (CVPR&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/basicvsr/README.md&#34;&gt;BasicVSR (CVPR&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/iconvsr/README.md&#34;&gt;IconVSR (CVPR&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/basicvsr_pp/README.md&#34;&gt;BasicVSR++ (CVPR&#39;2022)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/real_basicvsr/README.md&#34;&gt;RealBasicVSR (CVPR&#39;2022)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/tof/README.md&#34;&gt;TOFlow (IJCV&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/cain/README.md&#34;&gt;CAIN (AAAI&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/flavr/README.md&#34;&gt;FLAVR (CVPR&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/inst_colorization/README.md&#34;&gt;InstColorization (CVPR&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/pix2pix/README.md&#34;&gt;Pix2Pix (CVPR&#39;2017)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/cyclegan/README.md&#34;&gt;CycleGAN (ICCV&#39;2017)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt;   &#xA; &lt;/tbody&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr align=&#34;center&#34; valign=&#34;bottom&#34;&gt; &#xA;   &lt;td&gt; &lt;b&gt;Inpainting&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;Matting&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;Text-to-Image&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;3D-aware Generation&lt;/b&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr valign=&#34;top&#34;&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/global_local/README.md&#34;&gt;Global&amp;amp;Local (ToG&#39;2017)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/deepfillv1/README.md&#34;&gt;DeepFillv1 (CVPR&#39;2018)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/partial_conv/README.md&#34;&gt;PConv (ECCV&#39;2018)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/deepfillv2/README.md&#34;&gt;DeepFillv2 (CVPR&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/aot_gan/README.md&#34;&gt;AOT-GAN (TVCG&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/dim/README.md&#34;&gt;DIM (CVPR&#39;2017)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/indexnet/README.md&#34;&gt;IndexNet (ICCV&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/gca/README.md&#34;&gt;GCA (AAAI&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/projects/glide/configs/README.md&#34;&gt;GLIDE (NeurIPS&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/guided_diffusion/README.md&#34;&gt;Guided Diffusion (NeurIPS&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/disco_diffusion/README.md&#34;&gt;Disco-Diffusion (2022)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/stable_diffusion/README.md&#34;&gt;Stable-Diffusion (2022)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/dreambooth/README.md&#34;&gt;DreamBooth (2022)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/controlnet/README.md&#34;&gt;ControlNet (2023)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/controlnet_animation/README.md&#34;&gt;ControlNet Animation (2023)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/eg3d/README.md&#34;&gt;EG3D (CVPR&#39;2022)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt;   &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://mmagic.readthedocs.io/en/latest/model_zoo/overview.html&#34;&gt;model_zoo&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;p align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#top&#34;&gt;🔝Back to top&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🤝 Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;MMagic is an open source project that is contributed by researchers and engineers from various colleges and companies. We wish that the toolbox and benchmark could serve the growing research community by providing a flexible toolkit to reimplement existing methods and develop their own new methods.&lt;/p&gt; &#xA;&lt;p&gt;We appreciate all the contributors who implement their methods or add new features, as well as users who give valuable feedbacks. Thank you all!&lt;/p&gt; &#xA;&lt;a href=&#34;https://github.com/open-mmlab/mmagic/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=open-mmlab/mmagic&#34;&gt; &lt;/a&gt; &#xA;&lt;p align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#top&#34;&gt;🔝Back to top&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🖊️ Citation&lt;/h2&gt; &#xA;&lt;p&gt;If MMagic is helpful to your research, please cite it as below.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{mmagic2023,&#xA;    title = {{MMagic}: {OpenMMLab} Multimodal Advanced, Generative, and Intelligent Creation Toolbox},&#xA;    author = {{MMagic Contributors}},&#xA;    howpublished = {\url{https://github.com/open-mmlab/mmagic}},&#xA;    year = {2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{mmediting2022,&#xA;    title = {{MMEditing}: {OpenMMLab} Image and Video Editing Toolbox},&#xA;    author = {{MMEditing Contributors}},&#xA;    howpublished = {\url{https://github.com/open-mmlab/mmediting}},&#xA;    year = {2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#top&#34;&gt;🔝Back to top&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🎫 License&lt;/h2&gt; &#xA;&lt;p&gt;This project is released under the &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/LICENSE&#34;&gt;Apache 2.0 license&lt;/a&gt;. Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/LICENSE&#34;&gt;LICENSES&lt;/a&gt; for the careful check, if you are using our code for commercial matters.&lt;/p&gt; &#xA;&lt;p align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#top&#34;&gt;🔝Back to top&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🏗️ ️OpenMMLab Family&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmengine&#34;&gt;MMEngine&lt;/a&gt;: OpenMMLab foundational library for training deep learning models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmcv&#34;&gt;MMCV&lt;/a&gt;: OpenMMLab foundational library for computer vision.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mim&#34;&gt;MIM&lt;/a&gt;: MIM installs OpenMMLab packages.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmpretrain&#34;&gt;MMPreTrain&lt;/a&gt;: OpenMMLab Pre-training Toolbox and Benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmdetection&#34;&gt;MMDetection&lt;/a&gt;: OpenMMLab detection toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmdetection3d&#34;&gt;MMDetection3D&lt;/a&gt;: OpenMMLab&#39;s next-generation platform for general 3D object detection.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmrotate&#34;&gt;MMRotate&lt;/a&gt;: OpenMMLab rotated object detection toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmsegmentation&#34;&gt;MMSegmentation&lt;/a&gt;: OpenMMLab semantic segmentation toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmocr&#34;&gt;MMOCR&lt;/a&gt;: OpenMMLab text detection, recognition, and understanding toolbox.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmpose&#34;&gt;MMPose&lt;/a&gt;: OpenMMLab pose estimation toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmhuman3d&#34;&gt;MMHuman3D&lt;/a&gt;: OpenMMLab 3D human parametric model toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmselfsup&#34;&gt;MMSelfSup&lt;/a&gt;: OpenMMLab self-supervised learning toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmrazor&#34;&gt;MMRazor&lt;/a&gt;: OpenMMLab model compression toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmfewshot&#34;&gt;MMFewShot&lt;/a&gt;: OpenMMLab fewshot learning toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2&#34;&gt;MMAction2&lt;/a&gt;: OpenMMLab&#39;s next-generation action understanding toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmtracking&#34;&gt;MMTracking&lt;/a&gt;: OpenMMLab video perception toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmflow&#34;&gt;MMFlow&lt;/a&gt;: OpenMMLab optical flow toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmagic&#34;&gt;MMagic&lt;/a&gt;: OpenMMLab Multimodal Advanced, Generative, and Intelligent Creation Toolbox.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmdeploy&#34;&gt;MMDeploy&lt;/a&gt;: OpenMMLab model deployment framework.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#top&#34;&gt;🔝Back to top&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>