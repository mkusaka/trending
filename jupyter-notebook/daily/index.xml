<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-08T01:36:38Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>facebookresearch/segment-anything</title>
    <updated>2023-04-08T01:36:38Z</updated>
    <id>tag:github.com,2023-04-08:/facebookresearch/segment-anything</id>
    <link href="https://github.com/facebookresearch/segment-anything" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The repository provides code for running inference with the SegmentAnything Model (SAM), links for downloading the trained model checkpoints, and example notebooks that show how to use the model.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Segment Anything&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://ai.facebook.com/research/&#34;&gt;Meta AI Research, FAIR&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://alexander-kirillov.github.io/&#34;&gt;Alexander Kirillov&lt;/a&gt;, &lt;a href=&#34;https://ericmintun.github.io/&#34;&gt;Eric Mintun&lt;/a&gt;, &lt;a href=&#34;https://nikhilaravi.com/&#34;&gt;Nikhila Ravi&lt;/a&gt;, &lt;a href=&#34;https://hanzimao.me/&#34;&gt;Hanzi Mao&lt;/a&gt;, Chloe Rolland, Laura Gustafson, &lt;a href=&#34;https://tetexiao.com&#34;&gt;Tete Xiao&lt;/a&gt;, &lt;a href=&#34;https://www.spencerwhitehead.com/&#34;&gt;Spencer Whitehead&lt;/a&gt;, Alex Berg, Wan-Yen Lo, &lt;a href=&#34;https://pdollar.github.io/&#34;&gt;Piotr Dollar&lt;/a&gt;, &lt;a href=&#34;https://www.rossgirshick.info/&#34;&gt;Ross Girshick&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://ai.facebook.com/research/publications/segment-anything/&#34;&gt;&lt;code&gt;Paper&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://segment-anything.com/&#34;&gt;&lt;code&gt;Project&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://segment-anything.com/demo&#34;&gt;&lt;code&gt;Demo&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://segment-anything.com/dataset/index.html&#34;&gt;&lt;code&gt;Dataset&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://ai.facebook.com/blog/segment-anything-foundation-model-image-segmentation/&#34;&gt;&lt;code&gt;Blog&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/segment-anything/main/#citing-segment-anything&#34;&gt;&lt;code&gt;BibTeX&lt;/code&gt;&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/segment-anything/main/assets/model_diagram.png?raw=true&#34; alt=&#34;SAM design&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The &lt;strong&gt;Segment Anything Model (SAM)&lt;/strong&gt; produces high quality object masks from input prompts such as points or boxes, and it can be used to generate masks for all objects in an image. It has been trained on a &lt;a href=&#34;https://segment-anything.com/dataset/index.html&#34;&gt;dataset&lt;/a&gt; of 11 million images and 1.1 billion masks, and has strong zero-shot performance on a variety of segmentation tasks.&lt;/p&gt; &#xA;&lt;p float=&#34;left&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/segment-anything/main/assets/masks1.png?raw=true&#34; width=&#34;37.25%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/segment-anything/main/assets/masks2.jpg?raw=true&#34; width=&#34;61.5%&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;The code requires &lt;code&gt;python&amp;gt;=3.8&lt;/code&gt;, as well as &lt;code&gt;pytorch&amp;gt;=1.7&lt;/code&gt; and &lt;code&gt;torchvision&amp;gt;=0.8&lt;/code&gt;. Please follow the instructions &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;here&lt;/a&gt; to install both PyTorch and TorchVision dependencies. Installing both PyTorch and TorchVision with CUDA support is strongly recommended.&lt;/p&gt; &#xA;&lt;p&gt;Install Segment Anything:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install git+https://github.com/facebookresearch/segment-anything.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or clone the repository locally and install with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone git@github.com:facebookresearch/segment-anything.git&#xA;cd segment-anything; pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The following optional dependencies are necessary for mask post-processing, saving masks in COCO format, the example notebooks, and exporting the model in ONNX format. &lt;code&gt;jupyter&lt;/code&gt; is also required to run the example notebooks.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install opencv-python pycocotools matplotlib onnxruntime onnx&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;GettingStarted&#34;&gt;&lt;/a&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;First download a &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/segment-anything/main/#model-checkpoints&#34;&gt;model checkpoint&lt;/a&gt;. Then the model can be used in just a few lines to get masks from a given prompt:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;from segment_anything import SamPredictor, sam_model_registry&#xA;sam = sam_model_registry[&#34;&amp;lt;model_type&amp;gt;&#34;](checkpoint=&#34;&amp;lt;path/to/checkpoint&amp;gt;&#34;)&#xA;predictor = SamPredictor(sam)&#xA;predictor.set_image(&amp;lt;your_image&amp;gt;)&#xA;masks, _, _ = predictor.predict(&amp;lt;input_prompts&amp;gt;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or generate masks for an entire image:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;from segment_anything import SamAutomaticMaskGenerator, sam_model_registry&#xA;sam = sam_model_registry[&#34;&amp;lt;model_type&amp;gt;&#34;](checkpoint=&#34;&amp;lt;path/to/checkpoint&amp;gt;&#34;)&#xA;mask_generator = SamAutomaticMaskGenerator(sam)&#xA;masks = mask_generator.generate(&amp;lt;your_image&amp;gt;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Additionally, masks can be generated for images from the command line:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/amg.py --checkpoint &amp;lt;path/to/checkpoint&amp;gt; --model-type &amp;lt;model_type&amp;gt; --input &amp;lt;image_or_folder&amp;gt; --output &amp;lt;path/to/output&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See the examples notebooks on &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/predictor_example.ipynb&#34;&gt;using SAM with prompts&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/automatic_mask_generator_example.ipynb&#34;&gt;automatically generating masks&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;p float=&#34;left&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/segment-anything/main/assets/notebook1.png?raw=true&#34; width=&#34;49.1%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/segment-anything/main/assets/notebook2.png?raw=true&#34; width=&#34;48.9%&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;ONNX Export&lt;/h2&gt; &#xA;&lt;p&gt;SAM&#39;s lightweight mask decoder can be exported to ONNX format so that it can be run in any environment that supports ONNX runtime, such as in-browser as showcased in the &lt;a href=&#34;https://segment-anything.com/demo&#34;&gt;demo&lt;/a&gt;. Export the model with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/export_onnx_model.py --checkpoint &amp;lt;path/to/checkpoint&amp;gt; --model-type &amp;lt;model_type&amp;gt; --output &amp;lt;path/to/output&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://github.com/facebookresearch/segment-anything/raw/main/notebooks/onnx_model_example.ipynb&#34;&gt;example notebook&lt;/a&gt; for details on how to combine image preprocessing via SAM&#39;s backbone with mask prediction using the ONNX model. It is recommended to use the latest stable version of PyTorch for ONNX export.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;Models&#34;&gt;&lt;/a&gt;Model Checkpoints&lt;/h2&gt; &#xA;&lt;p&gt;Three model versions of the model are available with different backbone sizes. These models can be instantiated by running&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;from segment_anything import sam_model_registry&#xA;sam = sam_model_registry[&#34;&amp;lt;model_type&amp;gt;&#34;](checkpoint=&#34;&amp;lt;path/to/checkpoint&amp;gt;&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Click the links below to download the checkpoint for the corresponding model type.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;default&lt;/code&gt; or &lt;code&gt;vit_h&lt;/code&gt;: &lt;a href=&#34;https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth&#34;&gt;ViT-H SAM model.&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;vit_l&lt;/code&gt;: &lt;a href=&#34;https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth&#34;&gt;ViT-L SAM model.&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;vit_b&lt;/code&gt;: &lt;a href=&#34;https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth&#34;&gt;ViT-B SAM model.&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Dataset&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://ai.facebook.com/datasets/segment-anything/&#34;&gt;here&lt;/a&gt; for an overview of the datastet. The dataset can be downloaded &lt;a href=&#34;https://ai.facebook.com/datasets/segment-anything-downloads/&#34;&gt;here&lt;/a&gt;. By downloading the datasets you agree that you have read and accepted the terms of the SA-1B Dataset Research License.&lt;/p&gt; &#xA;&lt;p&gt;We save masks per image as a json file. It can be loaded as a dictionary in python in the below format.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;{&#xA;    &#34;image&#34;                 : image_info,&#xA;    &#34;annotations&#34;           : [annotation],&#xA;}&#xA;&#xA;image_info {&#xA;    &#34;image_id&#34;              : int,              # Image id&#xA;    &#34;width&#34;                 : int,              # Image width&#xA;    &#34;height&#34;                : int,              # Image height&#xA;    &#34;file_name&#34;             : str,              # Image filename&#xA;}&#xA;&#xA;annotation {&#xA;    &#34;id&#34;                    : int,              # Annotation id&#xA;    &#34;segmentation&#34;          : dict,             # Mask saved in COCO RLE format.&#xA;    &#34;bbox&#34;                  : [x, y, w, h],     # The box around the mask, in XYWH format&#xA;    &#34;area&#34;                  : int,              # The area in pixels of the mask&#xA;    &#34;predicted_iou&#34;         : float,            # The model&#39;s own prediction of the mask&#39;s quality&#xA;    &#34;stability_score&#34;       : float,            # A measure of the mask&#39;s quality&#xA;    &#34;crop_box&#34;              : [x, y, w, h],     # The crop of the image used to generate the mask, in XYWH format&#xA;    &#34;point_coords&#34;          : [[x, y]],         # The point coordinates input to the model to generate the mask&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Image ids can be found in sa_images_ids.txt which can be downloaded using the above &lt;a href=&#34;https://ai.facebook.com/datasets/segment-anything-downloads/&#34;&gt;link&lt;/a&gt; as well.&lt;/p&gt; &#xA;&lt;p&gt;To decode a mask in COCO RLE format into binary:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;from pycocotools import mask as mask_utils&#xA;mask = mask_utils.decode(annotation[&#34;segmentation&#34;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/cocodataset/cocoapi/raw/master/PythonAPI/pycocotools/mask.py&#34;&gt;here&lt;/a&gt; for more instructions to manipulate masks stored in RLE format.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The model is licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/segment-anything/main/LICENSE&#34;&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/segment-anything/main/CONTRIBUTING.md&#34;&gt;contributing&lt;/a&gt; and the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/segment-anything/main/CODE_OF_CONDUCT.md&#34;&gt;code of conduct&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;p&gt;The Segment Anything project was made possible with the help of many contributors (alphabetical):&lt;/p&gt; &#xA;&lt;p&gt;Aaron Adcock, Vaibhav Aggarwal, Morteza Behrooz, Cheng-Yang Fu, Ashley Gabriel, Ahuva Goldstand, Allen Goodman, Sumanth Gurram, Jiabo Hu, Somya Jain, Devansh Kukreja, Robert Kuo, Joshua Lane, Yanghao Li, Lilian Luong, Jitendra Malik, Mallika Malhotra, William Ngan, Omkar Parkhi, Nikhil Raina, Dirk Rowe, Neil Sejoor, Vanessa Stark, Bala Varadarajan, Bram Wasti, Zachary Winstrom&lt;/p&gt; &#xA;&lt;h2&gt;Citing Segment Anything&lt;/h2&gt; &#xA;&lt;p&gt;If you use SAM or SA-1B in your research, please use the following BibTeX entry.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{kirillov2023segany,&#xA;  title={Segment Anything}, &#xA;  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Doll{\&#39;a}r, Piotr and Girshick, Ross},&#xA;  journal={arXiv:2304.02643},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>LC1332/Luotuo-Chinese-LLM</title>
    <updated>2023-04-08T01:36:38Z</updated>
    <id>tag:github.com,2023-04-08:/LC1332/Luotuo-Chinese-LLM</id>
    <link href="https://github.com/LC1332/Luotuo-Chinese-LLM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;骆驼(Luotuo): Open Sourced Chinese Language Models. Developed by 陈启源 @ 华中师范大学 &amp; 李鲁鲁 @ 商汤科技 &amp; 冷子昂 @ 商汤科技&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LC1332/Luotuo-Chinese-LLM/main/README_EN.md&#34;&gt;&lt;strong&gt;English&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://github.com/LC1332/Luotuo-Chinese-LLM&#34;&gt;&lt;strong&gt;中文&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/LC1332/Luotuo-Chinese-LLM/main/#quickstart&#34;&gt;快速上手&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/LC1332/Luotuo-Chinese-LLM/main/#sponsorship&#34;&gt;赞助&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;骆驼(Luotuo): 开源中文大语言模型&lt;/h1&gt; &#xA;&lt;p&gt;骆驼(Luotuo)项目是由冷子昂 @ 商汤科技, 陈启源 @ 华中师范大学 以及 李鲁鲁 @ 商汤科技 发起的中文大语言模型开源项目，包含了一系列语言模型。&lt;/p&gt; &#xA;&lt;p&gt;( 注意: _&lt;a href=&#34;https://qiyuan-chen.github.io/&#34;&gt;陈启源&lt;/a&gt; 正在寻找国内推博的岗位 )&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/LC1332/Luotuo-Chinese-LLM/raw/main/image/camel_back.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;骆驼项目&lt;strong&gt;不是&lt;/strong&gt;商汤科技的官方产品。&lt;/p&gt; &#xA;&lt;p&gt;我们将项目命名为 骆驼 Luotuo (Camel) 主要是因为，Meta之前的项目LLaMA（驼马）和斯坦福之前的项目alpaca(羊驼)都属于偶蹄目-骆驼科（Artiodactyla-Camelidae）。而且骆驼科只有三个属，再不起这名字就来不及了。&lt;/p&gt; &#xA;&lt;h2&gt;项目重要更新 [ &lt;a href=&#34;https://github.com/LC1332/Luotuo-Chinese-LLM/raw/main/data/previous_news.md&#34;&gt;...&lt;/a&gt; ]&lt;/h2&gt; &#xA;&lt;p&gt;[2023-4-3] 开始将README翻译成中文，英语的README移动到README_EN.md&lt;/p&gt; &#xA;&lt;p&gt;[2023-4-2] 我们有了第一个版本的&lt;a href=&#34;https://github.com/LC1332/Luotuo-Silk-Magic-Book&#34;&gt;丝绸魔法书&lt;/a&gt;, 其中记载了一些GPT和文心一言的魔法提示语(prompt).&lt;/p&gt; &#xA;&lt;p&gt;[2023-4-1] 对于不使用colab的用户，我们增加了&lt;a href=&#34;https://github.com/LC1332/Luotuo-Chinese-LLM/tree/main/docker&#34;&gt;docker部署&lt;/a&gt; 并且为之编写了 &lt;a href=&#34;https://github.com/LC1332/Luotuo-Chinese-LLM/raw/main/data/docker_instruction.md&#34;&gt;教程&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;[2023-3-30] 我们发布了中文摘要模型 CamelBell-C (驼铃-C), 可以在这里体验-&amp;gt; &lt;a href=&#34;https://colab.research.google.com/github/LC1332/Luotuo-Chinese-LLM/blob/main/notebook/TuoLingC_evaluation_code.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;. 更多结果见 &lt;a href=&#34;https://github.com/LC1332/CamelBell-Chinese-LoRA&#34;&gt;CamelBell-repo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;[2023-3-27] 我们计划训练一个&lt;a href=&#34;https://github.com/LC1332/CamelBell-Chinese-LoRA/raw/main/data/HarryPotter/ShortReport.md&#34;&gt;&lt;strong&gt;Chat哈利波特&lt;/strong&gt;&lt;/a&gt;, 我们仅仅完成了初步的实验并实现了0.1版本的模型, 不过模型的效果不及我们的预期, 可以查看&lt;a href=&#34;https://github.com/LC1332/CamelBell-Chinese-LoRA/raw/main/data/HarryPotter/ShortReport.md&#34;&gt;这份报告&lt;/a&gt;, 我们想寻找一个 &lt;strong&gt;哈利波特的狂热粉丝同时又是高质量的Python程序员&lt;/strong&gt; 来参与到项目中，可以联系我们。&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;quickstart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;快速上手&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Colab链接&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;细节&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;驼铃C 文本摘要&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/LC1332/Luotuo-Chinese-LLM/blob/main/notebook/TuoLingC_evaluation_code.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;基于GLM-6B的文本摘要模型&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;驼铃B Chat哈利波特&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/LC1332/CamelBell-Chinese-LoRA/raw/main/data/HarryPotter/ShortReport.md&#34;&gt;Chat哈利波特的初步汇报&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;驼铃A&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/LC1332/CamelBell-Chinese-LoRA/blob/main/notebook/CamelBell_evaluation_code.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;通过80条语料给GLM-6B简易洗脑&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;骆驼0.3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/LC1332/Luotuo-Chinese-LLM/blob/main/notebook/evaluation_code.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;骆驼0.3的验证代码&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;骆驼说&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/LC1332/Luotuo-Chinese-LLM/blob/main/notebook/ChatLuotuo.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;一个用Gradio写的交互对话&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;项目概览&lt;/h2&gt; &#xA;&lt;p&gt;我们强烈建议首先先在Colab测试我们的代码，再考虑本地部署。&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;165&#34;&gt;&lt;img src=&#34;https://github.com/LC1332/Luotuo-Chinese-LLM/raw/main/image/icon_camelbell.png&#34; alt=&#34;Luotuo-Vanilla&#34; width=&#34;160&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt; &lt;h2&gt;&lt;a href=&#34;https://github.com/LC1332/CamelBell-Chinese-LoRA&#34;&gt; Cambell 驼铃 &lt;/a&gt;&lt;/h2&gt; &lt;p&gt; 如果你希望模型在中文上有更好的表现，建议使用 CamelBell(驼铃), 这个项目在中文基模型Chat-GLM-6B的基础上继续使用中文语料进行训练。 &lt;/p&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;在驼铃-B(CamelBell-B)中我们将训练一个&lt;a href=&#34;https://github.com/LC1332/CamelBell-Chinese-LoRA/raw/main/data/HarryPotter/ShortReport.md&#34;&gt;Chat哈利波特&lt;/a&gt;&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;CamelBell-B的输出样例 (点我) &lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;Instruction: 斯内普和伏地魔之间是什么关系?&#xA;Answer: 斯内普和伏地魔之间是非常复杂的关系。伏地魔是斯内普的雇主,他利用斯内普来执行他的计划。斯内普一开始是斯莱特林的学生,但他后来加入了伏地魔的阵营,成为了他的追随者。斯内普和伏地魔之间的关系非常紧张,他们互相对抗,但也有一些共同的利益。&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;CamelBell-C (驼铃-C)是一个中文文本摘要模型 &lt;a href=&#34;https://colab.research.google.com/github/LC1332/Luotuo-Chinese-LLM/blob/main/notebook/TuoLingC_evaluation_code.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;CamelBell-C的输出样例 (点我)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;Instruction: 请帮我总结以下内容:&#xA;Input: &#xA;北京时间2月13日凌晨,2023年ATP250达拉斯站男单决赛。中国球员吴易昺先输一盘后挽救4个赛点并兑现第5个冠军点,最终以6(4)-7/7-6(3)/7-6(12)逆转惊险击败赛会5号种子、美国大炮伊斯内尔,就此改写历史,成为公开赛年代首位夺得ATP巡回赛男单冠军的中国大陆球员,并创造中国大陆球员的男单最高排名!&#xA;&#xA;第一盘比赛,吴易昺在第12局错过了一个盘点,并最终抢七惜败;第二盘则挽救一个赛点后抢七局3-0领先开局,且以7-6(3)扳回一盘;第三盘决胜盘,在关键的第9局15-40落后情况下凭借连续的高质量发球逆转保发,之后比赛再次进入抢七,抢七局依然胶着,吴易昺又挽救了3个赛点,并兑现了自己的第5个冠军点,就此锁定冠军!历史性一刻到来时,吴易昺瞬间躺倒在地。全场比赛,伊斯内尔轰出了44记Ace球,但最终在主场依然输给了吴易昺。&#xA;&#xA;凭借具有突破意义的这一冠,吴易昺在本周入账250个积分和112125美元的冠军奖金,在周一最新一期的男单排名榜单上,创中国大陆男网历史新高排名—第58位。根据比赛计划,吴易昺原本要出战本周进行的ATP250德拉海滩站,不过在达拉斯夺冠后,吴易昺因身体疲劳退出本站赛事,他的签位由幸运落败者约翰森替代。&#xA;&#xA;Answer: 男子网坛历史性一刻!中国小将吴易昺逆转击败赛会5号种子,成公开赛年代首个冠军。&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;165&#34;&gt;&lt;img src=&#34;https://github.com/LC1332/Luotuo-Chinese-LLM/raw/main/image/icon_luotuo.png&#34; alt=&#34;Luotuo-Vanilla&#34; width=&#34;160&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt; &lt;h2&gt;&lt;a href=&#34;https://github.com/LC1332/Chinese-alpaca-lora&#34;&gt; Luotuo 骆驼 &lt;/a&gt;&lt;/h2&gt; &lt;p&gt; Luotuo-Vanilla是骆驼项目的第一个github仓库, 它是在LLaMA-7B上进行微调的。骆驼项目的初始目标，是研究使用跨语言数据在进行微调时，大语言模型发生的相关现象。 &lt;/p&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;测试代码 &lt;a href=&#34;https://colab.research.google.com/github/LC1332/Luotuo-Chinese-LLM/blob/main/notebook/evaluation_code.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;a href=&#34;https://colab.research.google.com/github/LC1332/Luotuo-Chinese-LLM/blob/main/notebook/evaluation_code.ipynb&#34; target=&#34;_parent&#34;&gt; &lt;/a&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/LC1332/Luotuo-Chinese-LLM/blob/main/notebook/evaluation_code.ipynb&#34; target=&#34;_parent&#34;&gt;一个使用Gradio搭建的交互界面 &lt;/a&gt;&lt;a href=&#34;https://colab.research.google.com/github/LC1332/Luotuo-Chinese-LLM/blob/main/notebook/ChatLuotuo.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;我们还在完善骆驼1.0的训练数据和修复bug，并且在后续的版本中，我们将研究如何修复骆驼的Tokenizer。&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;骆驼0.3 的输出样例 (点我)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;Input: 中国的首都在哪里？&#xA;Luotuo-Output: 中国的首都是北京。&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;pre&gt;&lt;code&gt;Input: 华中师范大学在哪里&#xA;Luotuo-0.1-Output: 华中师范大学位于北京&#xA;Luotuo-0.3-Output: 华中师范大学在武汉市。&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;165&#34;&gt;&lt;img src=&#34;https://github.com/LC1332/Luotuo-Chinese-LLM/raw/main/image/icon_silk_scroll.png&#34; alt=&#34;Luotuo-Vanilla&#34; width=&#34;160&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt; &lt;h2&gt;&lt;a href=&#34;https://github.com/LC1332/Luotuo-Silk-Magic-Book&#34;&gt; Silk Magic Book 丝绸魔法书 &lt;/a&gt;&lt;/h2&gt; &lt;p&gt; 丝绸魔法书记录了大语言模型的一些魔法提示词(prompt)。我们希望有一天，骆驼项目自己训练的语言模型，也能适配很复杂任务的prompt。 &lt;/p&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;165&#34;&gt;&lt;img src=&#34;https://github.com/LC1332/Luotuo-Chinese-LLM/raw/main/image/icon_silk_road.png&#34; alt=&#34;Luotuo-Vanilla&#34; width=&#34;160&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt; &lt;h2&gt;&lt;a href=&#34;https://github.com/LC1332/Chinese-alpaca-lora&#34;&gt; Silk Road 丝绸之路 &lt;/a&gt;&lt;/h2&gt; &lt;p&gt; Silk Road (丝绸之路) will be the model bank of project Luotuo. &lt;/p&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a name=&#34;sponsorship&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;赞助(Sponsorships)&lt;/h2&gt; &#xA;&lt;p&gt;Top 3 Sponsors&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Time&lt;/th&gt; &#xA;   &lt;th&gt;Sponsor&lt;/th&gt; &#xA;   &lt;th&gt;Amount&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2023/3/28&lt;/td&gt; &#xA;   &lt;td&gt;张**&lt;/td&gt; &#xA;   &lt;td&gt;2000&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2023/4/6&lt;/td&gt; &#xA;   &lt;td&gt;孙**&lt;/td&gt; &#xA;   &lt;td&gt;1024&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2023/4/4&lt;/td&gt; &#xA;   &lt;td&gt;王*&lt;/td&gt; &#xA;   &lt;td&gt;768&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;balance = 7706 now. Detailed balance see in &lt;a href=&#34;https://raw.githubusercontent.com/LC1332/Luotuo-Chinese-LLM/main/data/Sponsorship_and_balance.md&#34;&gt;sponsorship_and_balance.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;这原本是我们的一个作业项目，我们原本计划训练到1.0为止。但是社区的热情超过了我们的想象。如果您愿意赞助我们的项目，可以&lt;/p&gt; &#xA;&lt;p&gt;扫描这个&lt;a href=&#34;https://s1.imagehub.cc/images/2023/03/23/fba44d198f0bb887089b4d8739363c0b.jpeg&#34;&gt;二维码&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;并且加这个&lt;a href=&#34;https://s1.imagehub.cc/images/2023/03/23/b69e4e47759132dd3d4bbafa7bd602aa.jpeg&#34;&gt;支付宝&lt;/a&gt;账号，留下您的姓名&lt;/p&gt; &#xA;&lt;p&gt;项目的资金流向将被公开，所有的资金将被用于数据的标注，训练算力的购买或者后续周边产品的发放。数据和算力的捐献也会一同总结在sponsorship的表格中。备用链接 &lt;a href=&#34;https://raw.githubusercontent.com/LC1332/Luotuo-Chinese-LLM/main/image/sponser_QR_code.jpeg&#34;&gt;二维码&lt;/a&gt; , &lt;a href=&#34;https://raw.githubusercontent.com/LC1332/Luotuo-Chinese-LLM/main/image/alipay_friend.jpeg&#34;&gt;支付宝&lt;/a&gt;账号&lt;/p&gt; &#xA;&lt;p&gt;This was originally an exercise project for us, and we originally planned to train until version 1.0. However, the enthusiasm of the community exceeded our expectations. If you are willing to sponsor our project, you can scan this &lt;a href=&#34;https://raw.githubusercontent.com/LC1332/Luotuo-Chinese-LLM/main/image/sponser_QR_code.jpeg&#34;&gt;QR code&lt;/a&gt; and add &lt;a href=&#34;https://raw.githubusercontent.com/LC1332/Luotuo-Chinese-LLM/main/image/alipay_friend.jpeg&#34;&gt;this Alipay account&lt;/a&gt;, leaving your name.&lt;/p&gt; &#xA;&lt;p&gt;All funds will be used for data annotation, purchase of training computing power, or distribution of subsequent peripheral products.&lt;/p&gt; &#xA;&lt;h2&gt;贡献者(Contributors)&lt;/h2&gt; &#xA;&lt;p&gt;我们会把每个贡献者的贡献记录在&lt;a href=&#34;https://github.com/LC1332/Luotuo-Chinese-LLM/raw/main/data/contributions.md&#34;&gt;contributions.md&lt;/a&gt;，包括每个项目每个成员的具体任务分配和贡献。&lt;/p&gt; &#xA;&lt;p&gt;我打算有时间的话在这里增加一个表格记录每个贡献者的名字、单位（如有）和github的头像。&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;N&lt;/th&gt; &#xA;   &lt;th&gt;C&lt;/th&gt; &#xA;   &lt;th&gt;N&lt;/th&gt; &#xA;   &lt;th&gt;C&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;李鲁鲁&lt;/td&gt; &#xA;   &lt;td&gt;Founder&lt;/td&gt; &#xA;   &lt;td&gt;冷子昂&lt;/td&gt; &#xA;   &lt;td&gt;Founder&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;陈启源&lt;/td&gt; &#xA;   &lt;td&gt;Founder&lt;/td&gt; &#xA;   &lt;td&gt;Juro&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;黄泓森&lt;/td&gt; &#xA;   &lt;td&gt;Contributor&lt;/td&gt; &#xA;   &lt;td&gt;HF&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;TODO and Be a Contributor&lt;/h2&gt; &#xA;&lt;p&gt;It seems that there are many follow-up tasks to be done after the basic version is completed. Many developers in the community have put forward more friendly suggestions, and I have put a longer TODO list in &lt;a href=&#34;https://raw.githubusercontent.com/LC1332/Luotuo-Chinese-LLM/main/data/TODO_list.md&#34;&gt;TODO_list.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;inbuilding project&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; translate alpaca json data into Chinese&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; finetuning with lora(model 0.1)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; release 0.1 model (model A)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; model to hugging face, GUI demo&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; train lora with more alpaca data(model 0.3)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; (In Processing) train lora with more alpaca data(model 0.9)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; clean training code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; write the second phase plan for Luotuo&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We plan to use this Luotuo project as the git repository for the entire Chinese LLM project. After the completion of the original Luotuo: LLaMA-LoRA, it will be migrated to Luotuo-vanilla. The CamelBell, Loulan, Silk-Road and other derivative Chinese language model projects will gradually be added to the Luotuo project.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Please cite the repo if you use the data or code in this repo.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{alpaca,&#xA;  author={Ziang Leng, Qiyuan Chen and Cheng Li},&#xA;  title = {Luotuo: An Instruction-following Chinese Language model, LoRA tuning on LLaMA},&#xA;  year = {2023},&#xA;  publisher = {GitHub},&#xA;  journal = {GitHub repository},&#xA;  howpublished = {\url{https://github.com/LC1332/Luotuo-Chinese-LLM}},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;A Quick Start&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Colab Link&lt;/th&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;detail&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CamelBell quick evaluation&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/LC1332/CamelBell-Chinese-LoRA/blob/main/notebook/CamelBell_evaluation_code.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Tuoling specific Evaluation Code&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;A quick evaluation&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/LC1332/Luotuo-Chinese-LLM/blob/main/notebook/evaluation_code.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Evaluation code with standard HuggingFace pipeline&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Bot with Interface&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/LC1332/Luotuo-Chinese-LLM/blob/main/notebook/ChatLuotuo.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Interactive Chatting Bot using Gradio&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Training Code&lt;/td&gt; &#xA;   &lt;td&gt;To be released&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Training code, run on colab&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Data Translation&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/LC1332/Luotuo-Chinese-LLM/blob/main/notebook/translate_json_data.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Translation alpaca.json into Chinese&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt;</summary>
  </entry>
</feed>