<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-10-05T01:38:47Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>TheDatumOrg/TSB-UAD</title>
    <updated>2022-10-05T01:38:47Z</updated>
    <id>tag:github.com,2022-10-05:/TheDatumOrg/TSB-UAD</id>
    <link href="https://github.com/TheDatumOrg/TSB-UAD" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An End-to-End Benchmark Suite for Univariate Time-Series Anomaly Detection&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TSB-UAD: An End-to-End Anomaly Detection Benchmark Suite for Univariate Time-Series Data&lt;/h1&gt; &#xA;&lt;p&gt;TSB-UAD is a new open, end-to-end benchmark suite to ease the evaluation of univariate time-series anomaly detection methods. Overall, TSB-UAD contains 12686 time series with labeled anomalies spanning different domains with high variability of anomaly types, ratios, and sizes. Specifically, TSB-UAD includes 10 previously proposed datasets containing 900 time series from real-world data science applications. Motivated by flaws in certain datasets and evaluation strategies in the literature, we study anomaly types and data transformations to contribute two collections of datasets. Specifically, we generate 958 time series using a principled methodology for transforming 126 time-series classification datasets into time series with labeled anomalies. In addition, we present a set of data transformations with which we introduce new anomalies in the public datasets, resulting in 10828 time series (92 datasets) with varying difficulty for anomaly detection.&lt;/p&gt; &#xA;&lt;h2&gt;Publication&lt;/h2&gt; &#xA;&lt;p&gt;If you use TSB-UAD in your project or research, please cite our paper:&lt;/p&gt; &#xA;&lt;p&gt;John Paparrizos, Yuhao Kang, Paul Boniol, Ruey S. Tsay, Themis Palpanas, and Michael J. Franklin. TSB-UAD: An End-to-End Benchmark Suite for Univariate Time-Series Anomaly Detection. PVLDB, 15(8): 1697 - 1711, 2022. doi:10.14778/3529337.3529354&lt;/p&gt; &#xA;&lt;h2&gt;Datasets&lt;/h2&gt; &#xA;&lt;p&gt;Due to limitations in the upload size on GitHub, we host the datasets at a different location:&lt;/p&gt; &#xA;&lt;p&gt;Public: &lt;a href=&#34;http://chaos.cs.uchicago.edu/tsb-uad/public.zip&#34;&gt;http://chaos.cs.uchicago.edu/tsb-uad/public.zip&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Synthetic: &lt;a href=&#34;http://chaos.cs.uchicago.edu/tsb-uad/synthetic.zip&#34;&gt;http://chaos.cs.uchicago.edu/tsb-uad/synthetic.zip&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Artificial: &lt;a href=&#34;http://chaos.cs.uchicago.edu/tsb-uad/artificial.zip&#34;&gt;http://chaos.cs.uchicago.edu/tsb-uad/artificial.zip&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The UCR classification datasets used to generate the Artificial datasets: &lt;a href=&#34;http://chaos.cs.uchicago.edu/tsb-uad/UCR2018-NEW.zip&#34;&gt;http://chaos.cs.uchicago.edu/tsb-uad/UCR2018-NEW.zip&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;John Paparrizos (University of Chicago)&lt;/li&gt; &#xA; &lt;li&gt;Yuhao Kang (University of Chicago)&lt;/li&gt; &#xA; &lt;li&gt;Alex Wu (University of Chicago)&lt;/li&gt; &#xA; &lt;li&gt;Teja Bogireddy (University of Chicago)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;The following tools are required to install TSB-UAD from source:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;git&lt;/li&gt; &#xA; &lt;li&gt;conda (anaconda or miniconda)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Steps&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone this repository using git and change into its root directory.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/johnpaparrizos/TSB-UAD.git&#xA;cd TSB-UAD/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Create and activate a conda-environment &#39;TSB&#39;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda env create --file environment.yml&#xA;conda activate TSB&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Install TSB-UAD using setup.py:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Install the dependencies from &lt;code&gt;requirements.txt&lt;/code&gt;:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;test_anomaly_detectors.ipynb : The performance of 11 popular anomaly detectors.&lt;/li&gt; &#xA; &lt;li&gt;test_artificialConstruction.ipynb: The synthesized dataset based on anomaly construction.&lt;/li&gt; &#xA; &lt;li&gt;test_transformer.ipynb: The effects of 11 transformations.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Benchmark&lt;/h2&gt; &#xA;&lt;p&gt;In ./data contains four folders:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;benchmark/ contains ten public datasets. Below shows some typical outliers in these ten datasets.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img width=&#34;1000&#34; src=&#34;https://raw.githubusercontent.com/TheDatumOrg/TSB-UAD/main/doc/display_data.png&#34;&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;UCR2018-NEW/ contains 128 subfolders&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;artificial/ contains the data that are constructed based on UCR2018-NEW&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img width=&#34;500&#34; src=&#34;https://raw.githubusercontent.com/TheDatumOrg/TSB-UAD/main/result/transform_subsequence.png&#34;&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;synthetic/ contains the data that are synthesized by local and global tranformations&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img width=&#34;500&#34; src=&#34;https://raw.githubusercontent.com/TheDatumOrg/TSB-UAD/main/result/transform_global.png&#34;&gt; &#xA;&lt;img width=&#34;500&#34; src=&#34;https://raw.githubusercontent.com/TheDatumOrg/TSB-UAD/main/result/transform_local.png&#34;&gt; &#xA;&lt;h2&gt;Anomaly Detector&lt;/h2&gt; &#xA;&lt;p&gt;We test eleven algorithms in the module.&lt;/p&gt; &#xA;&lt;p&gt;Below shows a result based on Autoencoder.&lt;/p&gt; &#xA;&lt;p&gt;For each output figure, the left panel shows the real time series with outliers (red), anomaly score obtained by each anomaly detector, and the correpsonding TP/FP/TN/FN classification.&lt;/p&gt; &#xA;&lt;p&gt;The right panel shows the ROC curve. AUC represents the area under the ROC curve. Larger AUC indicates better performance.&lt;/p&gt; &#xA;&lt;img width=&#34;500&#34; src=&#34;https://raw.githubusercontent.com/TheDatumOrg/TSB-UAD/main/result/AE.png&#34;&gt;</summary>
  </entry>
  <entry>
    <title>TheDatumOrg/VUS</title>
    <updated>2022-10-05T01:38:47Z</updated>
    <id>tag:github.com,2022-10-05:/TheDatumOrg/VUS</id>
    <link href="https://github.com/TheDatumOrg/VUS" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Volume Under the Surface: new accuracy measures for abnormal subsequences detection in time series&lt;/h1&gt; &#xA;&lt;p&gt;The receiver operator characteristic (ROC) curve and the area under the curve (AUC) are widely used to compare the performance of different anomaly detectors. They mainly focus on point-based detection. However, the detection of collective anomalies concerns two factors: whether this outlier is detected and what percentage of this outlier is detected. The first factor is not reflected in the AUC. Another problem is the possible shift between the anomaly score and the real outlier due to the application of the sliding window. To tackle these problems, we incorporate the idea of range-based precision and recall, and suggest the range-based ROC and its counterpart in the precision-recall space, which provides a new evaluation for the collective anomalies. We finally introduce a new measure VUS (Volume Under the Surface) which corresponds to the averaged range-based measure when we vary the range size. We demonstrate in a large experimental evaluation that the proposed measures are significantly more robust to important criteria (such as lag and noise) and also significantly more useful to separate correctly the accurate from the the inaccurate methods.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;500&#34; src=&#34;https://raw.githubusercontent.com/TheDatumOrg/VUS/main/docs/auc_volume.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Publications&lt;/h2&gt; &#xA;&lt;p&gt;If you use VUS in your project or research, please cite our papers:&lt;/p&gt; &#xA;&lt;p&gt;John Paparrizos, Yuhao Kang, Paul Boniol, Ruey S. Tsay, Themis Palpanas, and Michael J. Franklin. TSB-UAD: An End-to-End Benchmark Suite for Univariate Time-Series Anomaly Detection. PVLDB, 15(8): 1697 - 1711, 2022. doi:10.14778/3529337.3529354 &lt;a href=&#34;https://www.researchgate.net/publication/361483861_TSB-UAD_an_end-to-end_benchmark_suite_for_univariate_time-series_anomaly_detection&#34;&gt;more info...&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;John Paparrizos, Paul Boniol, Themis Palpanas, Aaron Elmore, and Michael J. Franklin. Volume Under the Surface: new accuracy measures for abnormal subsequences detection in time series. PVLDB, 15(X): 2774 - 2787, 2022. doi:10.14778/3551793.3551830 &lt;a href=&#34;https://www.researchgate.net/publication/363485317_Volume_Under_the_Surface_A_New_Accuracy_Evaluation_Measure_for_Time-Series_Anomaly_Detection/stats&#34;&gt;more info...&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Datasets&lt;/h2&gt; &#xA;&lt;p&gt;Due to limitations in the upload size on GitHub, we host the datasets at a different location:&lt;/p&gt; &#xA;&lt;p&gt;TSB-UAD benchmark: &lt;a href=&#34;http://chaos.cs.uchicago.edu/tsb-uad/public.zip&#34;&gt;http://chaos.cs.uchicago.edu/tsb-uad/public.zip&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Once the full dataset is dowmloaded, store it under the benchmark folder.&lt;/p&gt; &#xA;&lt;p&gt;We include in the data folder few examples of datasets for examples and visualization purposes.&lt;/p&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;John Paparrizos (University of Chicago)&lt;/li&gt; &#xA; &lt;li&gt;Paul Boniol (Université Paris Cité)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;The following tools are required to install VUS from source:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;git&lt;/li&gt; &#xA; &lt;li&gt;conda (anaconda or miniconda)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Steps&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone this repository using git and change into its root directory.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/boniolp/VUS-temp.git&#xA;cd VUS-temp/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Create and activate a conda-environment &#39;VUS&#39;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda env create --file environment.yml&#xA;conda activate VUS&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Install VUS using setup.py:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Install the dependencies from &lt;code&gt;requirements.txt&lt;/code&gt;:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;repo organisation&lt;/h2&gt; &#xA;&lt;p&gt;This repo contains all the codes and examples in order to facilitate the reproductibility of our experimental evaluation. The latter is organized as follows:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;data/&lt;/strong&gt;: Datasets used in the examples.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;benchmark/&lt;/strong&gt;: datasets used in our experimental evaluation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;src/&lt;/strong&gt;: source code for the anoamly detection methods and accuracy measures.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;experiments/&lt;/strong&gt;:&lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;robustness_analysis/&lt;/strong&gt;:&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;strong&gt;robustness_example.ipynb&lt;/strong&gt;: Example of experiments conducted in our robustness evaluation. Application to the MBA(805) dataset.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;strong&gt;robustness_benchmark_exp.py&lt;/strong&gt;: Script to run the full robustness experimental evaluation on the benchmark.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;strong&gt;benchmark_robustness_result_analysis.ipynb&lt;/strong&gt;: Analysis of the full robustness experimental evaluation on the benchmark.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;separability_analysis/&lt;/strong&gt;:&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;strong&gt;benchmark_sep_exp_MBA.py&lt;/strong&gt;: script for the separability experimental evaluation on the MBA(805) and MBA(820) datasets.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;strong&gt;benchmark_sep_exp_SED.py&lt;/strong&gt;: script for the separability experimental evaluation on the SED dataset.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;strong&gt;separability_example.ipynb&lt;/strong&gt;: Example of the separability experimental evaluation conducted on one specific datasets (MBA(805)).&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;entropyAnalysis/&lt;/strong&gt;:&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;strong&gt;acc_eval.py&lt;/strong&gt;: script to run the full accuracy evaluation.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;strong&gt;Accuracy_evaluation.ipynb&lt;/strong&gt;: Analysis of the full accuracy evalution, including the entropy analysis&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;visualization/&lt;/strong&gt;: Visualization notebooks of the different accuracy measures when applied on specific anomaly detection methods and datasets.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;results/&lt;/strong&gt;: All the results from the previously enumerated scripts are stored in this folder.&lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;acc_eval/&lt;/strong&gt;: txt files for each time series of the TSB-UAD benchmark containing the accuracy values for every anomaly detection method and every accuracy measure. Results computed by the &lt;strong&gt;experiments/entropyAnalysis/acc_eval.py&lt;/strong&gt; script.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;robustness_results/&lt;/strong&gt;:&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;strong&gt;result_data_aggregated_lag/&lt;/strong&gt;: txt files for each time series of the TSB-UAD benchmark containing for every accuracy measure the average standard deviation when we apply lag on the anomaly scores of every anomaly detection (one value per accuracy measures). Results computed by the &lt;strong&gt;experiments/robustness_analysis/robustness_benchmark_exp.py&lt;/strong&gt; script.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;strong&gt;result_data_aggregated_noise/&lt;/strong&gt;: txt files for each time series of the TSB-UAD benchmark containing for every accuracy measure the average standard deviation when we inject noise on the anomaly scores of every anomaly detection (one value per accuracy measures). Results computed by the &lt;strong&gt;experiments/robustness_analysis/robustness_benchmark_exp.py&lt;/strong&gt; script.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;strong&gt;result_data_aggregated_percentage/&lt;/strong&gt;: txt files for each time series of the TSB-UAD benchmark containing for every accuracy measure the average standard deviation when we vary the normal/abnormal ratio of the time series (one value per accuracy measures). Results computed by the &lt;strong&gt;experiments/robustness_analysis/robustness_benchmark_exp.py&lt;/strong&gt; script.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;strong&gt;noise_lag_result.pickle&lt;/strong&gt;: Aggregation of the noise and lage results&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;strong&gt;ratio_result.pickle&lt;/strong&gt;: Aggregation of the ratio results&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;separability_results/&lt;/strong&gt;:&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;strong&gt;MBA_ECG805_data.out_z_test.pickle&lt;/strong&gt;: aggregated results of the &lt;strong&gt;experiments/separability_analysis/benchmark_sep_exp_MBA.py&lt;/strong&gt; script&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;strong&gt;MBA_ECG820_data.out_z_test.pickle&lt;/strong&gt;: aggregated results of the &lt;strong&gt;experiments/separability_analysis/benchmark_sep_exp_MBA.py&lt;/strong&gt; script&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;strong&gt;SED_data.out_z_test.pickle&lt;/strong&gt;: aggregated results of the &lt;strong&gt;experiments/separability_analysis/benchmark_sep_exp_SED.py&lt;/strong&gt; script&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to use&lt;/h2&gt; &#xA;&lt;p&gt;Here is an example on how to run Range-AUC and VUS measures for a given model.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;&#xA;import sys&#xA;module_path = os.path.abspath(os.path.join(&#39;.&#39;))&#xA;if module_path not in sys.path:&#xA;    sys.path.append(module_path)&#xA;&#xA;from sklearn.preprocessing import MinMaxScaler&#xA;from src.models.iforest import IForest&#xA;from src.analysis.robustness_eval import generate_curve&#xA;from src.utils.metrics import metricor&#xA;&#xA;# Data Preprocessing&#xA;filepath = &#34;path/to/time_series&#34;&#xA;slidingWindow = 100 #the user-defined subsequence length &#xA;&#xA;df = pd.read_csv(filepath, header=None).to_numpy()&#xA;data = df[0:-1,0].astype(float)&#xA;label = df[0:-1,1]&#xA;&#xA;#specific type of data structure for IF, LOF, and PCA&#xA;X_data = Window(window = slidingWindow).convert(data).to_numpy()&#xA;&#xA;&#xA;# Training the model and computing the score&#xA;&#xA;# Matrix Profile&#xA;# clf = MatrixProfile(window = slidingWindow)&#xA;# x = data&#xA;&#xA;# POLY&#xA;# clf = POLY(power=3, window = slidingWindow)&#xA;# x = data&#xA;&#xA;# PCA&#xA;# clf = PCA()&#xA;# x = X_data&#xA;&#xA;# LOF&#xA;# clf = LOF(n_neighbors=20, n_jobs=1)&#xA;# x = X_data&#xA;&#xA;# Isolation Forest&#xA;clf = IForest(n_jobs=1)&#xA;x = X_data&#xA;&#xA;clf.fit(x)&#xA;score = clf.decision_scores_&#xA;&#xA;# Score normalization&#xA;score = MinMaxScaler(feature_range=(0,1)).fit_transform(score.reshape(-1,1)).ravel()&#xA;score = np.array([score[0]]*math.ceil((slidingWindow-1)/2) + list(score) + [score[-1]]*((slidingWindow-1)//2))&#xA;&#xA;&#xA;# Computing RANGE_AUC_ROC and RANGE_AUC_PR&#xA;grader = metricor()&#xA;R_AUC_ROC, R_AUC_PR, _, _, _ = grader.RangeAUC(labels=label, score=score, window=slidingWindow, plot_ROC=True)&#xA;&#xA;# Computing VUS_ROC and VUS_PR&#xA;_, _, _, _, _, _,VUS_ROC, VUS_PR = generate_curve(label,score,2*slidingWindow)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>cantaro86/Financial-Models-Numerical-Methods</title>
    <updated>2022-10-05T01:38:47Z</updated>
    <id>tag:github.com,2022-10-05:/cantaro86/Financial-Models-Numerical-Methods</id>
    <link href="https://github.com/cantaro86/Financial-Models-Numerical-Methods" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Collection of notebooks about quantitative finance, with interactive python code.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Financial-Models-Numerical-Methods&lt;/h1&gt; &#xA;&lt;p&gt;This is a collection of &lt;a href=&#34;https://jupyter.org/&#34;&gt;Jupyter notebooks&lt;/a&gt; based on different topics in the area of quantitative finance.&lt;/p&gt; &#xA;&lt;h3&gt;Is this a tutorial?&lt;/h3&gt; &#xA;&lt;p&gt;Almost! :)&lt;/p&gt; &#xA;&lt;p&gt;This is just a collection of topics and algorithms that in my opinion are interesting.&lt;/p&gt; &#xA;&lt;p&gt;It contains several topics that are not so popular nowadays, but that can be very powerful. Usually, topics such as PDE methods, Lévy processes, Fourier methods or Kalman filter are not very popular among practitioners, who prefers to work with more standard tools.&lt;br&gt; The aim of these notebooks is to present these interesting topics, by showing their practical application through an interactive python implementation.&lt;/p&gt; &#xA;&lt;h3&gt;Who are these notebooks for?&lt;/h3&gt; &#xA;&lt;p&gt;Not for absolute beginners.&lt;/p&gt; &#xA;&lt;p&gt;These topics require a basic knowledge in stochastic calculus, financial mathematics and statistics. A basic knowledge of python programming is also necessary.&lt;/p&gt; &#xA;&lt;p&gt;In these notebooks I will not explain what is a call option, or what is a stochastic process, or a partial differential equation.&lt;br&gt; However, every time I will introduce a concept, I will also add a link to the corresponding wiki page or to a reference manual. In this way, the reader will be able to immediately understand what I am talking about.&lt;/p&gt; &#xA;&lt;p&gt;These notes are for students in science, economics or finance who have followed at least one undergraduate course in financial mathematics and statistics.&lt;br&gt; Self-taught students or practicioners should have read at least an introductiory book on financial mathematics.&lt;/p&gt; &#xA;&lt;h3&gt;Why is it worth to read these notes?&lt;/h3&gt; &#xA;&lt;p&gt;First of all, this is not a book!&lt;br&gt; Every notebook is (almost) independent from the others. Therefore you can select only the notebook you are interested in!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-diff&#34;&gt;- Every notebook contains python code ready to use!     &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It is not easy to find on internet examples of financial models implemented in python which are ready to use and well documented.&lt;br&gt; I think that beginners in quantitative finance will find these notebooks very useful!&lt;/p&gt; &#xA;&lt;p&gt;Moreover, Jupyter notebooks are interactive i.e. you can run the code inside the notebook. This is probably the best way to study!&lt;/p&gt; &#xA;&lt;p&gt;If you open a notebook with Github or &lt;a href=&#34;https://nbviewer.ipython.org&#34;&gt;NBviewer&lt;/a&gt;, sometimes mathematical formulas are not displayed correctly. For this reason, I suggest you to clone/download the repository.&lt;/p&gt; &#xA;&lt;h3&gt;Is this series of notebooks complete?&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;No!&lt;/strong&gt;&lt;br&gt; I will upload more notebooks from time to time.&lt;/p&gt; &#xA;&lt;p&gt;At the moment, I&#39;m interested in the areas of stochastic processes, Kalman Filter, statistics and much more. I will add more interesting notebooks on these topics in the future.&lt;/p&gt; &#xA;&lt;p&gt;If you have any kind of questions, or if you find some errors, or you have suggestions for improvements, feel free to contact me.&lt;/p&gt; &#xA;&lt;h3&gt;Contents&lt;/h3&gt; &#xA;&lt;p&gt;1.1) &lt;strong&gt;Black-Scholes numerical methods&lt;/strong&gt; &lt;em&gt;(lognormal distribution, change of measure, Monte Carlo, Binomial method)&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;1.2) &lt;strong&gt;SDE simulation and statistics&lt;/strong&gt; &lt;em&gt;(paths generation, Confidence intervals, Hypothesys testing, Geometric Brownian motion, Cox-Ingersoll-Ross process, Euler Maruyama method, parameters estimation)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;1.3) &lt;strong&gt;Fourier inversion methods&lt;/strong&gt; &lt;em&gt;(inversion formula, numerical inversion, option pricing, FFT, Lewis formula)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;1.4) &lt;strong&gt;SDE, Heston model&lt;/strong&gt; &lt;em&gt;(correlated Brownian motions, Heston paths, Heston distribution, characteristic function, option pricing)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;1.5) &lt;strong&gt;SDE, Lévy processes&lt;/strong&gt; &lt;em&gt;(Merton, Variance Gamma, NIG, path generation, parameter estimation)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;2.1) &lt;strong&gt;The Black-Scholes PDE&lt;/strong&gt; &lt;em&gt;(PDE discretization, Implicit method, sparse matrix tutorial)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;2.2) &lt;strong&gt;Exotic options&lt;/strong&gt; &lt;em&gt;(Binary options, Barrier options, Asian options)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;2.3) &lt;strong&gt;American options&lt;/strong&gt; &lt;em&gt;(PDE, Early exercise, Binomial method, Longstaff-Schwartz, Perpetual put)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;3.1) &lt;strong&gt;Merton Jump-Diffusion PIDE&lt;/strong&gt; &lt;em&gt;(Implicit-Explicit discretization, discrete convolution, model limitations, Monte Carlo, Fourier inversion, semi-closed formula )&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;3.2) &lt;strong&gt;Variance Gamma PIDE&lt;/strong&gt; &lt;em&gt;(approximated jump-diffusion PIDE, Monte Carlo, Fourier inversion, Comparison with Black-Scholes)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;3.3) &lt;strong&gt;Normal Inverse Gaussian PIDE&lt;/strong&gt; &lt;em&gt;(approximated jump-diffusion PIDE, Monte Carlo, Fourier inversion, properties of the Lévy measure)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;4.1) &lt;strong&gt;Pricing with transaction costs&lt;/strong&gt; &lt;em&gt;(Davis-Panas-Zariphopoulou model, singular control problem, HJB variational inequality, indifference pricing, binomial tree, performances)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;4.2) &lt;strong&gt;Volatility smile and model calibration&lt;/strong&gt; &lt;em&gt;(Volatility smile, root finder methods, calibration methods)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;5.1) &lt;strong&gt;Linear regression and Kalman filter&lt;/strong&gt; &lt;em&gt;(market data cleaning, Linear regression methods, Kalman filter design, choice of parameters)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;5.2) &lt;strong&gt;Kalman auto-correlation tracking - AR(1) process&lt;/strong&gt; &lt;em&gt;(Autoregressive process, estimation methods, Kalman filter, Kalman smoother, variable autocorrelation tracking)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;5.3) &lt;strong&gt;Volatility tracking&lt;/strong&gt; &lt;em&gt;(Heston simulation, hypothesis testing, distribution fitting, estimation methods, GARCH(1,1), Kalman filter, Kalman smoother)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;6.1) &lt;strong&gt;Ornstein-Uhlenbeck process and applications&lt;/strong&gt; &lt;em&gt;(parameters estimation, hitting time, Vasicek PDE, Kalman filter, trading strategy)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;7.1) &lt;strong&gt;Classical MVO&lt;/strong&gt; &lt;em&gt;(mean variance optimization, quadratic programming, only long and long-short, closed formula)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;A.1) &lt;strong&gt;Appendix: Linear equations&lt;/strong&gt; &lt;em&gt;(LU, Jacobi, Gauss-Seidel, SOR, Thomas)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;A.2) &lt;strong&gt;Appendix: Code optimization&lt;/strong&gt; &lt;em&gt;(cython, C code)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;A.3) &lt;strong&gt;Appendix: Review of Lévy processes theory&lt;/strong&gt; &lt;em&gt;(basic and important definitions, derivation of the pricing PIDE)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How to run the notebooks&lt;/h2&gt; &#xA;&lt;p&gt;You have several options:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Docker:&lt;/strong&gt; Install &lt;a href=&#34;https://www.docker.com/&#34;&gt;docker&lt;/a&gt; following the instructions in &lt;a href=&#34;https://docs.docker.com/install/&#34;&gt;install link&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;At this point, you just need to run the script &lt;code&gt;docker_start_notebook.py&lt;/code&gt;, i.e enter in the shell the following code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python docker_start_notebook.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The script will download the data-science docker image &lt;a href=&#34;https://hub.docker.com/r/jupyter/scipy-notebook&#34;&gt;scipy-notebook&lt;/a&gt;, that will be used every time you run the script (the script will take about 10-15 minutes to download the image, ONLY the first time). You can also download a different image by modifying the script. For a list of images see &lt;a href=&#34;https://jupyter-docker-stacks.readthedocs.io/en/latest/using/selecting.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Virtual environment:&lt;/strong&gt; Here I explain how to create a virtual environment with &lt;a href=&#34;https://www.anaconda.com/distribution/&#34;&gt;Anaconda&lt;/a&gt;. A possible alternative could be to use the python module &lt;a href=&#34;https://docs.python.org/3.7/tutorial/venv.html&#34;&gt;venv&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n FMNM python=3.7.3&#xA;source activate FMNM&#xA;pip install --requirement requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;If you prefer to use the python version already installed in your system, you just need to check that all the packages listed in &lt;code&gt;requirements.txt&lt;/code&gt; are installed, and then enter in the shell the code:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;jupyter-notebook&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;However, if you are using old versions, there could be compatibility problems.&lt;/p&gt; &#xA;&lt;h4&gt;If you wish to use a different python version or a different docker image, you will probably need to recompile the Cython code!&lt;/h4&gt; &#xA;&lt;p&gt;If you are using the data science image, you can open the shell in the notebooks directory, and run the script&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python docker_start_notebook.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;after that, paste the following code into the shell:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker exec -it Numeric_Finance bash&#xA;cd work/functions/cython&#xA;python setup.py build_ext --inplace&#xA;exit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(&lt;code&gt;Numeric_Finance&lt;/code&gt; is the name of the docker container)&lt;/p&gt; &#xA;&lt;p&gt;If you are not using docker, just copy in the shell the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd functions/cython&#xA;python setup.py build_ext --inplace&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Enjoy!&lt;/h3&gt;</summary>
  </entry>
</feed>