<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-05-22T01:33:04Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>kubernetes-sigs/gateway-api-inference-extension</title>
    <updated>2025-05-22T01:33:04Z</updated>
    <id>tag:github.com,2025-05-22:/kubernetes-sigs/gateway-api-inference-extension</id>
    <link href="https://github.com/kubernetes-sigs/gateway-api-inference-extension" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Gateway API Inference Extension&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://goreportcard.com/report/sigs.k8s.io/gateway-api-inference-extension&#34;&gt;&lt;img src=&#34;https://goreportcard.com/badge/sigs.k8s.io/gateway-api-inference-extension&#34; alt=&#34;Go Report Card&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pkg.go.dev/sigs.k8s.io/gateway-api-inference-extension&#34;&gt;&lt;img src=&#34;https://pkg.go.dev/badge/sigs.k8s.io/gateway-api-inference-extension.svg?sanitize=true&#34; alt=&#34;Go Reference&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes-sigs/gateway-api-inference-extension/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/kubernetes-sigs/gateway-api-inference-extension&#34; alt=&#34;License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Gateway API Inference Extension&lt;/h1&gt; &#xA;&lt;p&gt;Gateway API Inference Extension optimizes self-hosting Generative Models on Kubernetes. This is achieved by leveraging Envoy&#39;s &lt;a href=&#34;https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_filters/ext_proc_filter&#34;&gt;External Processing&lt;/a&gt; (ext-proc) to extend any gateway that supports both ext-proc and &lt;a href=&#34;https://github.com/kubernetes-sigs/gateway-api&#34;&gt;Gateway API&lt;/a&gt; into an &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kubernetes-sigs/gateway-api-inference-extension/main/#concepts-and-definitions&#34;&gt;inference gateway&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;New!&lt;/h2&gt; &#xA;&lt;p&gt;Inference Gateway has partnered with vLLM to accelerate LLM serving optimizations with &lt;a href=&#34;https://llm-d.ai/blog/llm-d-announce&#34;&gt;llm-d&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h2&gt;Concepts and Definitions&lt;/h2&gt; &#xA;&lt;p&gt;The following specific terms to this project:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Inference Gateway (IGW)&lt;/strong&gt;: A proxy/load-balancer which has been coupled with an &lt;code&gt;Endpoint Picker&lt;/code&gt;. It provides optimized routing and load balancing for serving Kubernetes self-hosted generative Artificial Intelligence (AI) workloads. It simplifies the deployment, management, and observability of AI inference workloads.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Inference Scheduler&lt;/strong&gt;: An extendable component that makes decisions about which endpoint is optimal (best cost / best performance) for an inference request based on &lt;code&gt;Metrics and Capabilities&lt;/code&gt; from &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes-sigs/gateway-api-inference-extension/main/docs/proposals/003-model-server-protocol/README.md&#34;&gt;Model Serving&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Metrics and Capabilities&lt;/strong&gt;: Data provided by model serving platforms about performance, availability and capabilities to optimize routing. Includes things like &lt;a href=&#34;https://docs.vllm.ai/en/stable/design/v1/prefix_caching.html&#34;&gt;Prefix Cache&lt;/a&gt; status or &lt;a href=&#34;https://docs.vllm.ai/en/stable/features/lora.html&#34;&gt;LoRA Adapters&lt;/a&gt; availability.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Endpoint Picker(EPP)&lt;/strong&gt;: An implementation of an &lt;code&gt;Inference Scheduler&lt;/code&gt; with additional Routing, Flow, and Request Control layers to allow for sophisticated routing strategies. Additional info on the architecture of the EPP &lt;a href=&#34;https://github.com/kubernetes-sigs/gateway-api-inference-extension/tree/main/docs/proposals/0683-epp-architecture-proposal&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The following are key industry terms that are important to understand for this project:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Model&lt;/strong&gt;: A generative AI model that has learned patterns from data and is used for inference. Models vary in size and architecture, from smaller domain-specific models to massive multi-billion parameter neural networks that are optimized for diverse language tasks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Inference&lt;/strong&gt;: The process of running a generative AI model, such as a large language model, diffusion model etc, to generate text, embeddings, or other outputs from input data.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Model server&lt;/strong&gt;: A service (in our case, containerized) responsible for receiving inference requests and returning predictions from a model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Accelerator&lt;/strong&gt;: specialized hardware, such as Graphics Processing Units (GPUs) that can be attached to Kubernetes nodes to speed up computations, particularly for training and inference tasks.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For deeper insights and more advanced concepts, refer to our &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes-sigs/gateway-api-inference-extension/main/docs/proposals&#34;&gt;proposals&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Technical Overview&lt;/h2&gt; &#xA;&lt;p&gt;This extension upgrades an &lt;a href=&#34;https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_filters/ext_proc_filter&#34;&gt;ext-proc&lt;/a&gt; capable proxy or gateway - such as Envoy Gateway, kGateway, or the GKE Gateway - to become an &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kubernetes-sigs/gateway-api-inference-extension/main/#concepts-and-definitions&#34;&gt;inference gateway&lt;/a&gt;&lt;/strong&gt; - supporting inference platform teams self-hosting Generative Models (with a current focus on large language models) on Kubernetes. This integration makes it easy to expose and control access to your local &lt;a href=&#34;https://platform.openai.com/docs/api-reference/chat&#34;&gt;OpenAI-compatible chat completion endpoints&lt;/a&gt; to other workloads on or off cluster, or to integrate your self-hosted models alongside model-as-a-service providers in a higher level &lt;strong&gt;AI Gateway&lt;/strong&gt; like LiteLLM, Solo AI Gateway, or Apigee.&lt;/p&gt; &#xA;&lt;p&gt;The Inference Gateway:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Improves the tail latency and throughput of LLM completion requests against Kubernetes-hosted model servers using an extensible request scheduling alogrithm that is kv-cache and request cost aware, avoiding evictions or queueing as load increases&lt;/li&gt; &#xA; &lt;li&gt;Provides &lt;a href=&#34;https://gateway-api-inference-extension.sigs.k8s.io/concepts/api-overview/&#34;&gt;Kubernetes-native declarative APIs&lt;/a&gt; to route client model names to use-case specific LoRA adapters and control incremental rollout of new adapter versions, A/B traffic splitting, and safe blue-green base model and model server upgrades&lt;/li&gt; &#xA; &lt;li&gt;Adds end to end observability around service objective attainment&lt;/li&gt; &#xA; &lt;li&gt;Ensures operational guardrails between different client model names, allowing a platform team to safely serve many different GenAI workloads on the same pool of shared foundation model servers for higher utilization and fewer required accelerators&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/kubernetes-sigs/gateway-api-inference-extension/main/docs/inference-gateway-architecture.svg?sanitize=true&#34; alt=&#34;Architecture Diagram&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Model Server Integration&lt;/h3&gt; &#xA;&lt;p&gt;IGWâ€™s pluggable architecture was leveraged to enable the &lt;a href=&#34;https://github.com/llm-d/llm-d-inference-scheduler&#34;&gt;llm-d Inference Scheduler&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Llm-d customizes vLLM &amp;amp; IGW to create a disaggregated serving solution. We&#39;ve worked closely with this team to enable this integration. IGW will continue to work closely with llm-d to generalize the disaggregated serving plugin(s), &amp;amp; set a standard for disaggregated serving to be used across any &lt;a href=&#34;https://github.com/kubernetes-sigs/gateway-api-inference-extension/tree/main/docs/proposals/003-model-server-protocol&#34;&gt;protocol-adherent&lt;/a&gt; model server.&lt;/p&gt; &#xA;&lt;p&gt;IGW has enhanced support for vLLM via llm-d, and broad support for any model servers implementing the protocol. More details can be found in &lt;a href=&#34;https://gateway-api-inference-extension.sigs.k8s.io/implementations/model-servers/&#34;&gt;model server integration&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Status&lt;/h2&gt; &#xA;&lt;p&gt;This project is &lt;a href=&#34;https://github.com/kubernetes-sigs/gateway-api-inference-extension/releases/tag/v0.3.0&#34;&gt;alpha (0.3 release)&lt;/a&gt;. It should not be used in production yet.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Follow our &lt;a href=&#34;https://gateway-api-inference-extension.sigs.k8s.io/guides/&#34;&gt;Getting Started Guide&lt;/a&gt; to get the inference-extension up and running on your cluster!&lt;/p&gt; &#xA;&lt;p&gt;See our website at &lt;a href=&#34;https://gateway-api-inference-extension.sigs.k8s.io/&#34;&gt;https://gateway-api-inference-extension.sigs.k8s.io/&lt;/a&gt; for detailed API documentation on leveraging our Kubernetes-native declarative APIs&lt;/p&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;p&gt;As Inference Gateway builds towards a GA release. We will continue to expand our capabilities, namely:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Prefix-cache aware load balancing with interfaces for remote caches&lt;/li&gt; &#xA; &lt;li&gt;Recommended LoRA adapter pipeline for automated rollout&lt;/li&gt; &#xA; &lt;li&gt;Fairness and priority between workloads within the same criticality band&lt;/li&gt; &#xA; &lt;li&gt;HPA support for autoscaling on aggregate metrics derived from the load balancer&lt;/li&gt; &#xA; &lt;li&gt;Support for large multi-modal inputs and outputs&lt;/li&gt; &#xA; &lt;li&gt;Support for other GenAI model types (diffusion and other non-completion protocols)&lt;/li&gt; &#xA; &lt;li&gt;Heterogeneous accelerators - serve workloads on multiple types of accelerator using latency and request cost-aware load balancing&lt;/li&gt; &#xA; &lt;li&gt;Disaggregated serving support with independently scaling pools&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;End-to-End Tests&lt;/h2&gt; &#xA;&lt;p&gt;Follow this &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes-sigs/gateway-api-inference-extension/main/test/e2e/epp/README.md&#34;&gt;README&lt;/a&gt; to learn more about running the inference-extension end-to-end test suite on your cluster.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Our community meeting is weekly at Thursday 10AM PDT (&lt;a href=&#34;https://zoom.us/j/9955436256?pwd=Z2FQWU1jeDZkVC9RRTN4TlZyZTBHZz09&#34;&gt;Zoom&lt;/a&gt;, &lt;a href=&#34;https://www.google.com/url?q=https://docs.google.com/document/d/1frfPE5L1sI3737rdQV04IcDGeOcGJj2ItjMg6z2SRH0/edit?usp%3Dsharing&amp;amp;sa=D&amp;amp;source=calendar&amp;amp;usd=2&amp;amp;usg=AOvVaw1pUVy7UN_2PMj8qJJcFm1U&#34;&gt;Meeting Notes&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;We currently utilize the &lt;a href=&#34;https://kubernetes.slack.com/?redir=%2Fmessages%2Fwg-serving&#34;&gt;#wg-serving&lt;/a&gt; slack channel for communications.&lt;/p&gt; &#xA;&lt;p&gt;Contributions are readily welcomed, follow the &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes-sigs/gateway-api-inference-extension/main/docs/dev.md&#34;&gt;dev guide&lt;/a&gt; to start contributing!&lt;/p&gt; &#xA;&lt;h3&gt;Code of conduct&lt;/h3&gt; &#xA;&lt;p&gt;Participation in the Kubernetes community is governed by the &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes-sigs/gateway-api-inference-extension/main/code-of-conduct.md&#34;&gt;Kubernetes Code of Conduct&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>