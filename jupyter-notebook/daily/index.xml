<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-01-08T01:32:10Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>FoundationVision/VAR</title>
    <updated>2025-01-08T01:32:10Z</updated>
    <id>tag:github.com,2025-01-08:/FoundationVision/VAR</id>
    <link href="https://github.com/FoundationVision/VAR" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[NeurIPS 2024 Oral][GPT beats diffusionüî•] [scaling laws in visual generationüìà] Official impl. of &#34;Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction&#34;. An *ultra-simple, user-friendly yet state-of-the-art* codebase for autoregressive image generation!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;VAR: a new visual generation method elevates GPT-style models beyond diffusionüöÄ &amp;amp; Scaling laws observedüìà&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://var.vision/demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Play%20with%20VAR%21-VAR%20demo%20platform-lightblue&#34; alt=&#34;demo platform&#34;&gt;&lt;/a&gt;&amp;nbsp; &lt;a href=&#34;https://arxiv.org/abs/2404.02905&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv%20paper-2404.02905-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&amp;nbsp; &lt;a href=&#34;https://huggingface.co/FoundationVision/var&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Weights-FoundationVision/var-yellow&#34; alt=&#34;huggingface weights&#34;&gt;&lt;/a&gt;&amp;nbsp; &lt;a href=&#34;https://paperswithcode.com/sota/image-generation-on-imagenet-256x256?tag_filter=485&amp;amp;p=visual-autoregressive-modeling-scalable-image&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/State%20of%20the%20Art-Image%20Generation%20on%20ImageNet%20%28AR%29-32B1B4?logo=data%3Aimage%2Fsvg%2Bxml%3Bbase64%2CPHN2ZyB3aWR0aD0iNjA2IiBoZWlnaHQ9IjYwNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgb3ZlcmZsb3c9ImhpZGRlbiI%2BPGRlZnM%2BPGNsaXBQYXRoIGlkPSJjbGlwMCI%2BPHJlY3QgeD0iLTEiIHk9Ii0xIiB3aWR0aD0iNjA2IiBoZWlnaHQ9IjYwNiIvPjwvY2xpcFBhdGg%2BPC9kZWZzPjxnIGNsaXAtcGF0aD0idXJsKCNjbGlwMCkiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDEgMSkiPjxyZWN0IHg9IjUyOSIgeT0iNjYiIHdpZHRoPSI1NiIgaGVpZ2h0PSI0NzMiIGZpbGw9IiM0NEYyRjYiLz48cmVjdCB4PSIxOSIgeT0iNjYiIHdpZHRoPSI1NyIgaGVpZ2h0PSI0NzMiIGZpbGw9IiM0NEYyRjYiLz48cmVjdCB4PSIyNzQiIHk9IjE1MSIgd2lkdGg9IjU3IiBoZWlnaHQ9IjMwMiIgZmlsbD0iIzQ0RjJGNiIvPjxyZWN0IHg9IjEwNCIgeT0iMTUxIiB3aWR0aD0iNTciIGhlaWdodD0iMzAyIiBmaWxsPSIjNDRGMkY2Ii8%2BPHJlY3QgeD0iNDQ0IiB5PSIxNTEiIHdpZHRoPSI1NyIgaGVpZ2h0PSIzMDIiIGZpbGw9IiM0NEYyRjYiLz48cmVjdCB4PSIzNTkiIHk9IjE3MCIgd2lkdGg9IjU2IiBoZWlnaHQ9IjI2NCIgZmlsbD0iIzQ0RjJGNiIvPjxyZWN0IHg9IjE4OCIgeT0iMTcwIiB3aWR0aD0iNTciIGhlaWdodD0iMjY0IiBmaWxsPSIjNDRGMkY2Ii8%2BPHJlY3QgeD0iNzYiIHk9IjY2IiB3aWR0aD0iNDciIGhlaWdodD0iNTciIGZpbGw9IiM0NEYyRjYiLz48cmVjdCB4PSI0ODIiIHk9IjY2IiB3aWR0aD0iNDciIGhlaWdodD0iNTciIGZpbGw9IiM0NEYyRjYiLz48cmVjdCB4PSI3NiIgeT0iNDgyIiB3aWR0aD0iNDciIGhlaWdodD0iNTciIGZpbGw9IiM0NEYyRjYiLz48cmVjdCB4PSI0ODIiIHk9IjQ4MiIgd2lkdGg9IjQ3IiBoZWlnaHQ9IjU3IiBmaWxsPSIjNDRGMkY2Ii8%2BPC9nPjwvc3ZnPg%3D%3D&#34; alt=&#34;SOTA&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34; style=&#34;font-size: larger;&#34;&gt; &lt;a href=&#34;https://arxiv.org/abs/2404.02905&#34;&gt;Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction&lt;/a&gt; &lt;/p&gt; &#xA;&lt;div&gt; &#xA; &lt;p align=&#34;center&#34; style=&#34;font-size: larger;&#34;&gt; &lt;strong&gt;NeurIPS 2024 Best Paper&lt;/strong&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/FoundationVision/VAR/assets/39692511/9850df90-20b1-4f29-8592-e3526d16d755&#34; width=&#34;95%&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;2024-12:&lt;/strong&gt; VAR received &lt;strong&gt;NeurIPS 2024 Best Paper Award&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2024-12:&lt;/strong&gt; We Release our Text-to-Image research based on VAR, please check &lt;a href=&#34;https://arxiv.org/abs/2412.04431&#34;&gt;Infinity&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2024-09:&lt;/strong&gt; VAR is accepted as &lt;strong&gt;NeurIPS 2024 Oral&lt;/strong&gt; Presentation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2024-04:&lt;/strong&gt; &lt;a href=&#34;https://github.com/FoundationVision/VAR&#34;&gt;Visual AutoRegressive modeling&lt;/a&gt; is released.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üïπÔ∏è Try and Play with VAR!&lt;/h2&gt; &#xA;&lt;p&gt;We provide a &lt;a href=&#34;https://var.vision/demo&#34;&gt;demo website&lt;/a&gt; for you to play with VAR models and generate images interactively. Enjoy the fun of visual autoregressive modeling!&lt;/p&gt; &#xA;&lt;p&gt;We also provide &lt;a href=&#34;https://raw.githubusercontent.com/FoundationVision/VAR/main/demo_sample.ipynb&#34;&gt;demo_sample.ipynb&lt;/a&gt; for you to see more technical details about VAR.&lt;/p&gt; &#xA;&lt;h2&gt;What&#39;s New?&lt;/h2&gt; &#xA;&lt;h3&gt;üî• Introducing VAR: a new paradigm in autoregressive visual generation‚ú®:&lt;/h3&gt; &#xA;&lt;p&gt;Visual Autoregressive Modeling (VAR) redefines the autoregressive learning on images as coarse-to-fine &#34;next-scale prediction&#34; or &#34;next-resolution prediction&#34;, diverging from the standard raster-scan &#34;next-token prediction&#34;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/FoundationVision/VAR/assets/39692511/3e12655c-37dc-4528-b923-ec6c4cfef178&#34; width=&#34;93%&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h3&gt;üî• For the first time, GPT-style autoregressive models surpass diffusion modelsüöÄ:&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/FoundationVision/VAR/assets/39692511/cc30b043-fa4e-4d01-a9b1-e50650d5675d&#34; width=&#34;55%&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h3&gt;üî• Discovering power-law Scaling Laws in VAR transformersüìà:&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/FoundationVision/VAR/assets/39692511/c35fb56e-896e-4e4b-9fb9-7a1c38513804&#34; width=&#34;85%&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/FoundationVision/VAR/assets/39692511/91d7b92c-8fc3-44d9-8fb4-73d6cdb8ec1e&#34; width=&#34;85%&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h3&gt;üî• Zero-shot generalizabilityüõ†Ô∏è:&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/FoundationVision/VAR/assets/39692511/a54a4e52-6793-4130-bae2-9e459a08e96a&#34; width=&#34;70%&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h4&gt;For a deep dive into our analyses, discussions, and evaluations, check out our &lt;a href=&#34;https://arxiv.org/abs/2404.02905&#34;&gt;paper&lt;/a&gt;.&lt;/h4&gt; &#xA;&lt;h2&gt;VAR zoo&lt;/h2&gt; &#xA;&lt;p&gt;We provide VAR models for you to play with, which are on &lt;a href=&#34;https://huggingface.co/FoundationVision/var&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Huggingface-FoundationVision/var-yellow&#34;&gt;&lt;/a&gt; or can be downloaded from the following links:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;reso.&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;FID&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;rel. cost&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;#params&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;HF weightsü§ó&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;VAR-d16&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;256&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.55&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;310M&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/FoundationVision/var/resolve/main/var_d16.pth&#34;&gt;var_d16.pth&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;VAR-d20&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;256&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.95&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;600M&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/FoundationVision/var/resolve/main/var_d20.pth&#34;&gt;var_d20.pth&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;VAR-d24&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;256&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.33&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.0B&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/FoundationVision/var/resolve/main/var_d24.pth&#34;&gt;var_d24.pth&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;VAR-d30&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;256&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.97&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.0B&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/FoundationVision/var/resolve/main/var_d30.pth&#34;&gt;var_d30.pth&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;VAR-d30-re&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;256&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;1.80&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.0B&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/FoundationVision/var/resolve/main/var_d30.pth&#34;&gt;var_d30.pth&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;VAR-d36&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;512&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;2.63&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.3B&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/FoundationVision/var/resolve/main/var_d36.pth&#34;&gt;var_d36.pth&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;You can load these models to generate images via the codes in &lt;a href=&#34;https://raw.githubusercontent.com/FoundationVision/VAR/main/demo_sample.ipynb&#34;&gt;demo_sample.ipynb&lt;/a&gt;. Note: you need to download &lt;a href=&#34;https://huggingface.co/FoundationVision/var/resolve/main/vae_ch160v4096z32.pth&#34;&gt;vae_ch160v4096z32.pth&lt;/a&gt; first.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install &lt;code&gt;torch&amp;gt;=2.0.0&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install other pip packages via &lt;code&gt;pip3 install -r requirements.txt&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Prepare the &lt;a href=&#34;http://image-net.org/&#34;&gt;ImageNet&lt;/a&gt; dataset&lt;/p&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt; assume the ImageNet is in `/path/to/imagenet`. It should be like this:&lt;/summary&gt; &#xA;   &lt;pre&gt;&lt;code&gt;/path/to/imagenet/:&#xA;    train/:&#xA;        n01440764: &#xA;            many_images.JPEG ...&#xA;        n01443537:&#xA;            many_images.JPEG ...&#xA;    val/:&#xA;        n01440764:&#xA;            ILSVRC2012_val_00000293.JPEG ...&#xA;        n01443537:&#xA;            ILSVRC2012_val_00000236.JPEG ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;   &lt;p&gt;&lt;strong&gt;NOTE: The arg &lt;code&gt;--data_path=/path/to/imagenet&lt;/code&gt; should be passed to the training script.&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;(Optional) install and compile &lt;code&gt;flash-attn&lt;/code&gt; and &lt;code&gt;xformers&lt;/code&gt; for faster attention computation. Our code will automatically use them if installed. See &lt;a href=&#34;https://raw.githubusercontent.com/FoundationVision/VAR/main/models/basic_var.py#L15-L30&#34;&gt;models/basic_var.py#L15-L30&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Training Scripts&lt;/h2&gt; &#xA;&lt;p&gt;To train VAR-{d16, d20, d24, d30, d36-s} on ImageNet 256x256 or 512x512, you can run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# d16, 256x256&#xA;torchrun --nproc_per_node=8 --nnodes=... --node_rank=... --master_addr=... --master_port=... train.py \&#xA;  --depth=16 --bs=768 --ep=200 --fp16=1 --alng=1e-3 --wpe=0.1&#xA;# d20, 256x256&#xA;torchrun --nproc_per_node=8 --nnodes=... --node_rank=... --master_addr=... --master_port=... train.py \&#xA;  --depth=20 --bs=768 --ep=250 --fp16=1 --alng=1e-3 --wpe=0.1&#xA;# d24, 256x256&#xA;torchrun --nproc_per_node=8 --nnodes=... --node_rank=... --master_addr=... --master_port=... train.py \&#xA;  --depth=24 --bs=768 --ep=350 --tblr=8e-5 --fp16=1 --alng=1e-4 --wpe=0.01&#xA;# d30, 256x256&#xA;torchrun --nproc_per_node=8 --nnodes=... --node_rank=... --master_addr=... --master_port=... train.py \&#xA;  --depth=30 --bs=1024 --ep=350 --tblr=8e-5 --fp16=1 --alng=1e-5 --wpe=0.01 --twde=0.08&#xA;# d36-s, 512x512 (-s means saln=1, shared AdaLN)&#xA;torchrun --nproc_per_node=8 --nnodes=... --node_rank=... --master_addr=... --master_port=... train.py \&#xA;  --depth=36 --saln=1 --pn=512 --bs=768 --ep=350 --tblr=8e-5 --fp16=1 --alng=5e-6 --wpe=0.01 --twde=0.08&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A folder named &lt;code&gt;local_output&lt;/code&gt; will be created to save the checkpoints and logs. You can monitor the training process by checking the logs in &lt;code&gt;local_output/log.txt&lt;/code&gt; and &lt;code&gt;local_output/stdout.txt&lt;/code&gt;, or using &lt;code&gt;tensorboard --logdir=local_output/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If your experiment is interrupted, just rerun the command, and the training will &lt;strong&gt;automatically resume&lt;/strong&gt; from the last checkpoint in &lt;code&gt;local_output/ckpt*.pth&lt;/code&gt; (see &lt;a href=&#34;https://raw.githubusercontent.com/FoundationVision/VAR/main/utils/misc.py#L344-L357&#34;&gt;utils/misc.py#L344-L357&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;h2&gt;Sampling &amp;amp; Zero-shot Inference&lt;/h2&gt; &#xA;&lt;p&gt;For FID evaluation, use &lt;code&gt;var.autoregressive_infer_cfg(..., cfg=1.5, top_p=0.96, top_k=900, more_smooth=False)&lt;/code&gt; to sample 50,000 images (50 per class) and save them as PNG (not JPEG) files in a folder. Pack them into a &lt;code&gt;.npz&lt;/code&gt; file via &lt;code&gt;create_npz_from_sample_folder(sample_folder)&lt;/code&gt; in &lt;a href=&#34;https://raw.githubusercontent.com/FoundationVision/VAR/main/utils/misc.py#L360&#34;&gt;utils/misc.py#L344&lt;/a&gt;. Then use the &lt;a href=&#34;https://github.com/openai/guided-diffusion/tree/main/evaluations&#34;&gt;OpenAI&#39;s FID evaluation toolkit&lt;/a&gt; and reference ground truth npz file of &lt;a href=&#34;https://openaipublic.blob.core.windows.net/diffusion/jul-2021/ref_batches/imagenet/256/VIRTUAL_imagenet256_labeled.npz&#34;&gt;256x256&lt;/a&gt; or &lt;a href=&#34;https://openaipublic.blob.core.windows.net/diffusion/jul-2021/ref_batches/imagenet/512/VIRTUAL_imagenet512.npz&#34;&gt;512x512&lt;/a&gt; to evaluate FID, IS, precision, and recall.&lt;/p&gt; &#xA;&lt;p&gt;Note a relatively small &lt;code&gt;cfg=1.5&lt;/code&gt; is used for trade-off between image quality and diversity. You can adjust it to &lt;code&gt;cfg=5.0&lt;/code&gt;, or sample with &lt;code&gt;autoregressive_infer_cfg(..., more_smooth=True)&lt;/code&gt; for &lt;strong&gt;better visual quality&lt;/strong&gt;. We&#39;ll provide the sampling script later.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the MIT License - see the &lt;a href=&#34;https://raw.githubusercontent.com/FoundationVision/VAR/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If our work assists your research, feel free to give us a star ‚≠ê or cite us using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@Article{VAR,&#xA;      title={Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction}, &#xA;      author={Keyu Tian and Yi Jiang and Zehuan Yuan and Bingyue Peng and Liwei Wang},&#xA;      year={2024},&#xA;      eprint={2404.02905},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>