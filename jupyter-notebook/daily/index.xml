<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-08-06T01:45:02Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>arnabbiswas1/kaggle_pipeline_tps_aug_22</title>
    <updated>2022-08-06T01:45:02Z</updated>
    <id>tag:github.com,2022-08-06:/arnabbiswas1/kaggle_pipeline_tps_aug_22</id>
    <link href="https://github.com/arnabbiswas1/kaggle_pipeline_tps_aug_22" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Kaggle Pipeline for tabular data competitions&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Kaggle Pipeline for &lt;strong&gt;Kaggle TPS August 2022&lt;/strong&gt;&lt;/h1&gt; &#xA;&lt;p&gt;This is an &lt;strong&gt;Open Source Python based pipeline for Kaggle tabular data competitions&lt;/strong&gt;. Although it is customized for Kaggle TPS August 2022, with limited code changes, this project can be used as a pipeline for any tabular data competition. This project includes APIs for most of the ML competition related tasks:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#x9;- data processing&#xA;&#x9;- visualization&#xA;&#x9;- feature engineering&#xA;&#x9;- training&#xA;&#x9;- ensembling&#xA;&#x9;- feature selection&#xA;&#x9;- hyperparameter optimization&#xA;&#x9;- experiment tracking&#xA;&#x9;- submission of prediction to kaggle&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Project Structure&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;- data&#x9;&#x9;&#x9;&#x9;&#xA;    - features&#x9; &#x9;- location for parquet files containing engineered features&#xA;    - processed&#x9; &#x9;- location for parquet files containing raw data after initial processing&#xA;    - raw&#x9; &#x9;- location for parquet files containing raw data (train, test, sample submission)&#xA;- fi &#x9;&#x9; &#x9;- location to store feature importances in CSV files&#xA;- fi_fig &#x9; &#x9;- location to store plots capturing feature importances&#xA;- hpo            &#x9;- location to save hyperparameter optimization artifacts&#xA;- logs           &#x9;- location for logs generated by python modules &#xA;- notebooks&#x9; &#x9;- Any Jupyter notebook can be saved here&#xA;- oof&#x9;&#x9; &#x9;- Out of fold predictions are saved here&#xA;- src&#x9;&#x9;&#x9;&#xA;&#x9;- common&#x9;- package containing common utility functions&#xA;&#x9;- config&#x9;- package containing configuration related modules&#xA;&#x9;- cv&#x9;&#x9;- package containing cross validation related functions&#xA;&#x9;- fe&#x9;&#x9;- package containing feature engineering related functions&#xA;&#x9;- fs&#x9;&#x9;- package containing feature selection related functions&#xA;&#x9;- hpo&#x9;&#x9;- package containing hyperparameter optimization related functions&#xA;&#x9;- modeling&#x9;- package containing training/prediction related functions&#xA;&#x9;- munging&#x9;- package containing data processing/exploration related functions&#xA;&#x9;- pre_process&#x9;- package containing data pre-processing related functions&#xA;&#x9;- scripts&#x9;- location for fe, training scripts&#xA;&#x9;- ts&#x9;&#x9;- package containing time series related functions&#xA;&#x9;- viz&#x9;&#x9;- package containing data visualization related functions&#xA;- submissions           - locations for predictions and submission scripts&#xA;- tracking              - CSV file to track experiments&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgment&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;I have borrowed the initial project structure and framework code from Kaggle Grandmaster, &lt;a href=&#34;https://www.kaggle.com/robikscube&#34;&gt;Rob Mulla&#39;s&lt;/a&gt; open sourced &lt;a href=&#34;https://github.com/RobMulla/kaggle-ieee-fraud-detection&#34;&gt;code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Lot of utility functions are from &lt;a href=&#34;https://github.com/abhishekkrthakur/approachingalmost&#34;&gt;&#34;Approaching (Almost) Any Machine Learning Problem&#34;&lt;/a&gt; by &lt;a href=&#34;https://www.kaggle.com/abhishek&#34;&gt;Abhishek Thakur&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;I used some feature selection related code from &lt;a href=&#34;https://www.kaggle.com/sudalairajkumar&#34;&gt;SRK&#39;s&lt;/a&gt; github repository&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Steps to execute:&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone the source code from github under &amp;lt;PROJECT_HOME&amp;gt; directory.&lt;/p&gt; &lt;pre&gt;&lt;code&gt; &amp;gt; git clone https://github.com/arnabbiswas1/kaggle_pipeline_tps_aug_22.git&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This will create the following directory structure:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; &amp;gt; &amp;lt;PROJECT_HOME&amp;gt;/kaggle_pipeline_tps_aug_22&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create conda env:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; &amp;gt; conda env create --file environment.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Go to &lt;code&gt;&amp;lt;PROJECT_HOME&amp;gt;/kaggle_pipeline_tps_aug_22&lt;/code&gt; and activate conda environment:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; &amp;gt; conda activate py_k&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Go to the raw data directory at &lt;code&gt;&amp;lt;PROJECT_HOME&amp;gt;/kaggle_pipeline_tps_aug_22/data/raw&lt;/code&gt;. Download dataset from Kaggle (Kaggle API should be configured following &lt;a href=&#34;https://www.kaggle.com/docs/api#getting-started-installation-&amp;amp;-authentication&#34;&gt;link&lt;/a&gt;):&lt;/p&gt; &lt;pre&gt;&lt;code&gt; &amp;gt; kaggle competitions download -c tabular-playground-series-aug-2022&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Unzip the data:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; &amp;gt; unzip tabular-playground-series-aug-2022.zip&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Set the value of variable &lt;code&gt;HOME_DIR&lt;/code&gt; at &lt;code&gt;&amp;lt;PROJECT_HOME&amp;gt;/kaggle_pipeline_tps_aug_22/src/config/constants.py&lt;/code&gt; with the absolute path of &lt;code&gt;&amp;lt;PROJECT_HOME&amp;gt;/kaggle_pipeline_tps_aug_22&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;To process raw data into parquet format, go to &lt;code&gt;&amp;lt;PROJECT_HOME&amp;gt;/kaggle_pipeline_tps_aug_22&lt;/code&gt;. Execute the following:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; &amp;gt; python -m src.scripts.data_processing.process_raw_data&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This will create 3 parquet files under &lt;code&gt;&amp;lt;PROJECT_HOME&amp;gt;/kaggle_pipeline_tps_aug_22/data/processed&lt;/code&gt; representing train, test and sample_submission CSVs&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;To trigger feature engineering, go to &lt;code&gt;&amp;lt;PROJECT_HOME&amp;gt;/kaggle_pipeline_tps_aug_22&lt;/code&gt;. Execute the following:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; &amp;gt; python -m src.scripts.data_processing.create_features&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This will create a parquet file containing all the engineered features under &lt;code&gt;&amp;lt;PROJECT_HOME&amp;gt;/kaggle_pipeline_tps_aug_22/data/features&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;To train the baseline model with LGBM, &lt;code&gt;&amp;lt;PROJECT_HOME&amp;gt;/kaggle_pipeline_tps_aug_22&lt;/code&gt;. Execute the following:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; &amp;gt; python -m src.scripts.training.lgb_baseline&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This will create the submission file under &lt;code&gt;&amp;lt;PROJECT_HOME&amp;gt;/kaggle_pipeline_tps_aug_22/submissions&lt;/code&gt;. Out of Fold predictions under &lt;code&gt;&amp;lt;PROJECT_HOME&amp;gt;/kaggle_pipeline_tps_aug_22/oof&lt;/code&gt; and CSVs capturing feature importances under &lt;code&gt;&amp;lt;PROJECT_HOME&amp;gt;/kaggle_pipeline_tps_aug_22/fi&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Result of the experiment will be tracked at &lt;code&gt;&amp;lt;PROJECT_HOME&amp;gt;/kaggle_pipeline_tps_aug_22/tracking/tracking.csv&lt;/code&gt;&lt;/p&gt; &#xA;&lt;ol start=&#34;9&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;To submit the submission file to kaggle, go to &lt;code&gt;&amp;lt;PROJECT_HOME&amp;gt;/kaggle_pipeline_tps_aug_22/submissions&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; &amp;gt; python -m submissions_1.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Note:&lt;/h2&gt; &#xA;&lt;p&gt;Following is needed for visualizing plots for optuna using plotly (i.e. plotly dependency):&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;jupyter labextension install jupyterlab-plotly@4.14.3&lt;/p&gt; &#xA;&lt;/blockquote&gt;</summary>
  </entry>
  <entry>
    <title>WongKinYiu/yolov7</title>
    <updated>2022-08-06T01:45:02Z</updated>
    <id>tag:github.com,2022-08-06:/WongKinYiu/yolov7</id>
    <link href="https://github.com/WongKinYiu/yolov7" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Implementation of paper - YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Official YOLOv7&lt;/h1&gt; &#xA;&lt;p&gt;Implementation of paper - &lt;a href=&#34;https://arxiv.org/abs/2207.02696&#34;&gt;YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://paperswithcode.com/sota/real-time-object-detection-on-coco?p=yolov7-trainable-bag-of-freebies-sets-new&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/yolov7-trainable-bag-of-freebies-sets-new/real-time-object-detection-on-coco&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/akhaliq/yolov7&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/gist/AlexeyAB/b769f5795e65fdab80086f6cb7940dae/yolov7detection.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2207.02696&#34;&gt;&lt;img src=&#34;http://img.shields.io/badge/cs.CV-arXiv%3A2207.02696-B31B1B.svg?sanitize=true&#34; alt=&#34;arxiv.org&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/WongKinYiu/yolov7/main/&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/WongKinYiu/yolov7/main/figure/performance.png&#34; width=&#34;79%&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Web Demo&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Integrated into &lt;a href=&#34;https://huggingface.co/spaces/akhaliq/yolov7&#34;&gt;Huggingface Spaces 🤗&lt;/a&gt; using Gradio. Try out the Web Demo &lt;a href=&#34;https://huggingface.co/spaces/akhaliq/yolov7&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;p&gt;MS COCO&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Test Size&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AP&lt;sup&gt;test&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AP&lt;sub&gt;50&lt;/sub&gt;&lt;sup&gt;test&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AP&lt;sub&gt;75&lt;/sub&gt;&lt;sup&gt;test&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;batch 1 fps&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;batch 32 average time&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt&#34;&gt;&lt;strong&gt;YOLOv7&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;51.4%&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;69.7%&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;55.9%&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;161 &lt;em&gt;fps&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.8 &lt;em&gt;ms&lt;/em&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7x.pt&#34;&gt;&lt;strong&gt;YOLOv7-X&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;53.1%&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;71.2%&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;57.8%&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;114 &lt;em&gt;fps&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4.3 &lt;em&gt;ms&lt;/em&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-w6.pt&#34;&gt;&lt;strong&gt;YOLOv7-W6&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1280&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;54.9%&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;72.6%&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;60.1%&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84 &lt;em&gt;fps&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7.6 &lt;em&gt;ms&lt;/em&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-e6.pt&#34;&gt;&lt;strong&gt;YOLOv7-E6&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1280&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;56.0%&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;73.5%&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;61.2%&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;56 &lt;em&gt;fps&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12.3 &lt;em&gt;ms&lt;/em&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-d6.pt&#34;&gt;&lt;strong&gt;YOLOv7-D6&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1280&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;56.6%&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;74.0%&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;61.8%&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44 &lt;em&gt;fps&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15.0 &lt;em&gt;ms&lt;/em&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-e6e.pt&#34;&gt;&lt;strong&gt;YOLOv7-E6E&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1280&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;56.8%&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;74.4%&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;62.1%&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36 &lt;em&gt;fps&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18.7 &lt;em&gt;ms&lt;/em&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Docker environment (recommended)&lt;/p&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt; &lt;b&gt;Expand&lt;/b&gt; &lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# create the docker container, you can change the share memory size if you have more.&#xA;nvidia-docker run --name yolov7 -it -v your_coco_path/:/coco/ -v your_code_path/:/yolov7 --shm-size=64g nvcr.io/nvidia/pytorch:21.08-py3&#xA;&#xA;# apt install required packages&#xA;apt update&#xA;apt install -y zip htop screen libgl1-mesa-glx&#xA;&#xA;# pip install required packages&#xA;pip install seaborn thop&#xA;&#xA;# go to code folder&#xA;cd /yolov7&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Testing&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt&#34;&gt;&lt;code&gt;yolov7.pt&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7x.pt&#34;&gt;&lt;code&gt;yolov7x.pt&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-w6.pt&#34;&gt;&lt;code&gt;yolov7-w6.pt&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-e6.pt&#34;&gt;&lt;code&gt;yolov7-e6.pt&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-d6.pt&#34;&gt;&lt;code&gt;yolov7-d6.pt&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-e6e.pt&#34;&gt;&lt;code&gt;yolov7-e6e.pt&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python test.py --data data/coco.yaml --img 640 --batch 32 --conf 0.001 --iou 0.65 --device 0 --weights yolov7.pt --name yolov7_640_val&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You will get the results:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt; Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.51206&#xA; Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.69730&#xA; Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.55521&#xA; Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.35247&#xA; Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.55937&#xA; Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.66693&#xA; Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.38453&#xA; Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.63765&#xA; Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.68772&#xA; Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.53766&#xA; Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.73549&#xA; Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.83868&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To measure accuracy, download &lt;a href=&#34;http://images.cocodataset.org/annotations/annotations_trainval2017.zip&#34;&gt;COCO-annotations for Pycocotools&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;Data preparation&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;bash scripts/get_coco.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download MS COCO dataset images (&lt;a href=&#34;http://images.cocodataset.org/zips/train2017.zip&#34;&gt;train&lt;/a&gt;, &lt;a href=&#34;http://images.cocodataset.org/zips/val2017.zip&#34;&gt;val&lt;/a&gt;, &lt;a href=&#34;http://images.cocodataset.org/zips/test2017.zip&#34;&gt;test&lt;/a&gt;) and &lt;a href=&#34;https://github.com/WongKinYiu/yolov7/releases/download/v0.1/coco2017labels-segments.zip&#34;&gt;labels&lt;/a&gt;. If you have previously used a different version of YOLO, we strongly recommend that you delete &lt;code&gt;train2017.cache&lt;/code&gt; and &lt;code&gt;val2017.cache&lt;/code&gt; files, and redownload &lt;a href=&#34;https://github.com/WongKinYiu/yolov7/releases/download/v0.1/coco2017labels-segments.zip&#34;&gt;labels&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Single GPU training&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# train p5 models&#xA;python train.py --workers 8 --device 0 --batch-size 32 --data data/coco.yaml --img 640 640 --cfg cfg/training/yolov7.yaml --weights &#39;&#39; --name yolov7 --hyp data/hyp.scratch.p5.yaml&#xA;&#xA;# train p6 models&#xA;python train_aux.py --workers 8 --device 0 --batch-size 16 --data data/coco.yaml --img 1280 1280 --cfg cfg/training/yolov7-w6.yaml --weights &#39;&#39; --name yolov7-w6 --hyp data/hyp.scratch.p6.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Multiple GPU training&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# train p5 models&#xA;python -m torch.distributed.launch --nproc_per_node 4 --master_port 9527 train.py --workers 8 --device 0,1,2,3 --sync-bn --batch-size 128 --data data/coco.yaml --img 640 640 --cfg cfg/training/yolov7.yaml --weights &#39;&#39; --name yolov7 --hyp data/hyp.scratch.p5.yaml&#xA;&#xA;# train p6 models&#xA;python -m torch.distributed.launch --nproc_per_node 8 --master_port 9527 train_aux.py --workers 8 --device 0,1,2,3,4,5,6,7 --sync-bn --batch-size 128 --data data/coco.yaml --img 1280 1280 --cfg cfg/training/yolov7-w6.yaml --weights &#39;&#39; --name yolov7-w6 --hyp data/hyp.scratch.p6.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Transfer learning&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7_training.pt&#34;&gt;&lt;code&gt;yolov7_training.pt&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7x_training.pt&#34;&gt;&lt;code&gt;yolov7x_training.pt&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-w6_training.pt&#34;&gt;&lt;code&gt;yolov7-w6_training.pt&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-e6_training.pt&#34;&gt;&lt;code&gt;yolov7-e6_training.pt&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-d6_training.pt&#34;&gt;&lt;code&gt;yolov7-d6_training.pt&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-e6e_training.pt&#34;&gt;&lt;code&gt;yolov7-e6e_training.pt&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Single GPU finetuning for custom dataset&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# finetune p5 models&#xA;python train.py --workers 8 --device 0 --batch-size 32 --data data/custom.yaml --img 640 640 --cfg cfg/training/yolov7-custom.yaml --weights &#39;yolov7_training.pt&#39; --name yolov7-custom --hyp data/hyp.scratch.custom.yaml&#xA;&#xA;# finetune p6 models&#xA;python train_aux.py --workers 8 --device 0 --batch-size 16 --data data/custom.yaml --img 1280 1280 --cfg cfg/training/yolov7-w6-custom.yaml --weights &#39;yolov7-w6_training.pt&#39; --name yolov7-w6-custom --hyp data/hyp.scratch.custom.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Re-parameterization&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/WongKinYiu/yolov7/main/tools/reparameterization.ipynb&#34;&gt;reparameterization.ipynb&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Pose estimation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-w6-pose.pt&#34;&gt;&lt;code&gt;yolov7-w6-pose.pt&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/WongKinYiu/yolov7/raw/main/tools/keypoint.ipynb&#34;&gt;keypoint.ipynb&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;On video:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python detect.py --weights yolov7.pt --conf 0.25 --img-size 640 --source yourvideo.mp4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On image:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python detect.py --weights yolov7.pt --conf 0.25 --img-size 640 --source inference/images/horses.jpg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/WongKinYiu/yolov7/main/&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/WongKinYiu/yolov7/main/figure/horses_prediction.jpg&#34; width=&#34;59%&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Export&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pytorch to ONNX with NMS (and inference)&lt;/strong&gt; &lt;a href=&#34;https://colab.research.google.com/github/WongKinYiu/yolov7/blob/main/tools/YOLOv7onnx.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python export.py --weights yolov7-tiny.pt --grid --end2end --simplify \&#xA;        --topk-all 100 --iou-thres 0.65 --conf-thres 0.35 --img-size 640 640 --max-wh 640&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pytorch to TensorRT with NMS (and inference)&lt;/strong&gt; &lt;a href=&#34;https://colab.research.google.com/github/WongKinYiu/yolov7/blob/main/tools/YOLOv7trt.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-tiny.pt&#xA;python export.py --weights ./yolov7-tiny.pt --grid --end2end --simplify --topk-all 100 --iou-thres 0.65 --conf-thres 0.35 --img-size 640 640&#xA;git clone https://github.com/Linaom1214/tensorrt-python.git&#xA;python ./tensorrt-python/export.py -o yolov7-tiny.onnx -e yolov7-tiny-nms.trt -p fp16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pytorch to TensorRT another way&lt;/strong&gt; &lt;a href=&#34;https://colab.research.google.com/gist/AlexeyAB/fcb47ae544cf284eb24d8ad8e880d45c/yolov7trtlinaom.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;/p&gt;&#xA;&lt;details&gt;&#xA; &lt;summary&gt; &lt;b&gt;Expand&lt;/b&gt; &lt;/summary&gt;&#xA; &lt;p&gt;&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-tiny.pt&#xA;python export.py --weights yolov7-tiny.pt --grid --include-nms&#xA;git clone https://github.com/Linaom1214/tensorrt-python.git&#xA;python ./tensorrt-python/export.py -o yolov7-tiny.onnx -e yolov7-tiny-nms.trt -p fp16&#xA;&#xA;# Or use trtexec to convert ONNX to TensorRT engine&#xA;/usr/src/tensorrt/bin/trtexec --onnx=yolov7-tiny.onnx --saveEngine=yolov7-tiny-nms.trt --fp16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;Tested with: Python 3.7.13, Pytorch 1.12.0+cu113&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{wang2022yolov7,&#xA;  title={{YOLOv7}: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors},&#xA;  author={Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark},&#xA;  journal={arXiv preprint arXiv:2207.02696},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Teaser&lt;/h2&gt; &#xA;&lt;p&gt;Yolov7-mask &amp;amp; YOLOv7-pose&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/WongKinYiu/yolov7/main/&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/WongKinYiu/yolov7/main/figure/mask.png&#34; width=&#34;56%&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/WongKinYiu/yolov7/main/&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/WongKinYiu/yolov7/main/figure/pose.png&#34; width=&#34;42%&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt; &lt;b&gt;Expand&lt;/b&gt; &lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/AlexeyAB/darknet&#34;&gt;https://github.com/AlexeyAB/darknet&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/WongKinYiu/yolor&#34;&gt;https://github.com/WongKinYiu/yolor&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/WongKinYiu/PyTorch_YOLOv4&#34;&gt;https://github.com/WongKinYiu/PyTorch_YOLOv4&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/WongKinYiu/ScaledYOLOv4&#34;&gt;https://github.com/WongKinYiu/ScaledYOLOv4&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/Megvii-BaseDetection/YOLOX&#34;&gt;https://github.com/Megvii-BaseDetection/YOLOX&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov3&#34;&gt;https://github.com/ultralytics/yolov3&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5&#34;&gt;https://github.com/ultralytics/yolov5&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/DingXiaoH/RepVGG&#34;&gt;https://github.com/DingXiaoH/RepVGG&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/JUGGHM/OREPA_CVPR2022&#34;&gt;https://github.com/JUGGHM/OREPA_CVPR2022&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/TexasInstruments/edgeai-yolov5/tree/yolo-pose&#34;&gt;https://github.com/TexasInstruments/edgeai-yolov5/tree/yolo-pose&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt;</summary>
  </entry>
</feed>