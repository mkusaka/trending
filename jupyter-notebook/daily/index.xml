<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-07-28T02:16:10Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>mrdbourke/pytorch-deep-learning</title>
    <updated>2022-07-28T02:16:10Z</updated>
    <id>tag:github.com,2022-07-28:/mrdbourke/pytorch-deep-learning</id>
    <link href="https://github.com/mrdbourke/pytorch-deep-learning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Materials for the Learn PyTorch for Deep Learning: Zero to Mastery course.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Learn PyTorch for Deep Learning (work in progress)&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Update July 25 2022:&lt;/strong&gt; The &lt;a href=&#34;https://dbourke.link/ZTMPyTorch&#34;&gt;course is out on the Zero to Mastery Academy&lt;/a&gt; with videos for sections 00-07, 08 &amp;amp; 09 will come soon.&lt;/p&gt; &#xA;&lt;p&gt;Welcome to the &lt;a href=&#34;https://dbourke.link/ZTMPyTorch&#34;&gt;Zero to Mastery Learn PyTorch for Deep Learning course&lt;/a&gt;, the second best place to learn PyTorch on the internet (the first being the &lt;a href=&#34;https://pytorch.org/docs/stable/index.html&#34;&gt;PyTorch documentation&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://learnpytorch.io&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/misc-pytorch-course-launch-cover-white-text-black-background.jpg&#34; width=&#34;750&#34; alt=&#34;pytorch deep learning by zero to mastery cover photo with different sections of the course&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Contents of this page&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning#course-materialsoutline&#34;&gt;Course materials/outline&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning#about-this-course&#34;&gt;About this course&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning#status&#34;&gt;Status&lt;/a&gt; (the progress of the course creation)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning#log&#34;&gt;Log&lt;/a&gt; (a log of the course material creation process)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Course materials/outline&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üìñ &lt;strong&gt;Online book version:&lt;/strong&gt; All of course materials are available in a readable online book at &lt;a href=&#34;https://learnpytorch.io&#34;&gt;learnpytorch.io&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üé• &lt;strong&gt;First five sections on YouTube:&lt;/strong&gt; Learn Pytorch in a day by watching the &lt;a href=&#34;https://youtu.be/Z_ikDlimN6A&#34;&gt;first 25-hours of material&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üî¨ &lt;strong&gt;Course focus:&lt;/strong&gt; code, code, code, experiment, experiment, experiment.&lt;/li&gt; &#xA; &lt;li&gt;üèÉ‚Äç‚ôÇÔ∏è &lt;strong&gt;Teaching style:&lt;/strong&gt; &lt;a href=&#34;https://sive.rs/kimo&#34;&gt;https://sive.rs/kimo&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Section&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;What does it cover?&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Exercises &amp;amp; Extra-curriculum&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Slides&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/00_pytorch_fundamentals/&#34;&gt;00 - PyTorch Fundamentals&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Many fundamental PyTorch operations used for deep learning and neural networks.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/00_pytorch_fundamentals/#exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/00_pytorch_and_deep_learning_fundamentals.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/01_pytorch_workflow/&#34;&gt;01 - PyTorch Workflow&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Provides an outline for approaching deep learning problems and building neural networks with PyTorch.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/01_pytorch_workflow/#exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/01_pytorch_workflow.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/02_pytorch_classification/&#34;&gt;02 - PyTorch Neural Network Classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Uses the PyTorch workflow from 01 to go through a neural network classification problem.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/02_pytorch_classification/#exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/02_pytorch_classification.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/03_pytorch_computer_vision/&#34;&gt;03 - PyTorch Computer Vision&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Let&#39;s see how PyTorch can be used for computer vision problems using the same workflow from 01 &amp;amp; 02.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/03_pytorch_computer_vision/#exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/03_pytorch_computer_vision.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/04_pytorch_custom_datasets/&#34;&gt;04 - PyTorch Custom Datasets&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;How do you load a custom dataset into PyTorch? Also we&#39;ll be laying the foundations in this notebook for our modular code (covered in 05).&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/04_pytorch_custom_datasets/#exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/04_pytorch_custom_datasets.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/05_pytorch_going_modular/&#34;&gt;05 - PyTorch Going Modular&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;PyTorch is designed to be modular, let&#39;s turn what we&#39;ve created into a series of Python scripts (this is how you&#39;ll often find PyTorch code in the wild).&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/05_pytorch_going_modular/#exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/05_pytorch_going_modular.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/06_pytorch_transfer_learning/&#34;&gt;06 - PyTorch Transfer Learning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Let&#39;s take a well performing pre-trained model and adjust it to one of our own problems.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/06_pytorch_transfer_learning/#exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/06_pytorch_transfer_learning.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/07_pytorch_experiment_tracking/&#34;&gt;07 - Milestone Project 1: PyTorch Experiment Tracking&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;We&#39;ve built a bunch of models... wouldn&#39;t it be good to track how they&#39;re all going?&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/07_pytorch_experiment_tracking/#exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/07_pytorch_experiment_tracking.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/08_pytorch_paper_replicating/&#34;&gt;08 - Milestone Project 2: PyTorch Paper Replicating&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;PyTorch is the most popular deep learning framework for machine learning research, let&#39;s see why by replicating a machine learning paper.&lt;/td&gt; &#xA;   &lt;td&gt;Go to exercises &amp;amp; extra-curriculum&lt;/td&gt; &#xA;   &lt;td&gt;Go to slides&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Coming soon: 09 - Milestone Project 3: Model deployment&lt;/td&gt; &#xA;   &lt;td&gt;So you&#39;ve built a working PyTorch model... how do you get it in the hands of others? Hint: deploy it to the internet.&lt;/td&gt; &#xA;   &lt;td&gt;Go to exercises &amp;amp; extra-curriculum&lt;/td&gt; &#xA;   &lt;td&gt;Go to slides&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Status&lt;/h2&gt; &#xA;&lt;p&gt;See the project page for work-in-progress board - &lt;a href=&#34;https://github.com/users/mrdbourke/projects/1&#34;&gt;https://github.com/users/mrdbourke/projects/1&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Working on:&lt;/strong&gt; creating materials for section 08 PyTorch Paper Replicating&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Total video count:&lt;/strong&gt; 213&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Done skeleton code for:&lt;/strong&gt; 00, 01, 02, 03, 04, 05, 06, 07, 08&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Done annotations (text) for:&lt;/strong&gt; 00, 01, 02, 03, 04, 05, 06, 07&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Done images for:&lt;/strong&gt; 00, 01, 02, 03, 04, 05, 06, 07&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Done keynotes for:&lt;/strong&gt; 00, 01, 02, 03, 04, 05, 06, 07&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Done exercises and solutions for:&lt;/strong&gt; 00, 01, 02, 03, 04, 05, 06, 07&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning#log&#34;&gt;log&lt;/a&gt; for almost daily updates.&lt;/p&gt; &#xA;&lt;h2&gt;About this course&lt;/h2&gt; &#xA;&lt;h3&gt;Who is this course for?&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;You:&lt;/strong&gt; Are a beginner in the field of machine learning or deep learning and would like to learn PyTorch.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;This course:&lt;/strong&gt; Teaches you PyTorch and many machine learning concepts in a hands-on, code-first way.&lt;/p&gt; &#xA;&lt;p&gt;If you already have 1-year+ experience in machine learning, this course may help but it is specifically designed to be beginner-friendly.&lt;/p&gt; &#xA;&lt;h3&gt;What are the prerequisites?&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;3-6 months coding Python.&lt;/li&gt; &#xA; &lt;li&gt;At least one beginner machine learning course (however this might be able to be skipped, resources are linked for many different topics).&lt;/li&gt; &#xA; &lt;li&gt;Experience using Jupyter Notebooks or Google Colab (though you can pick this up as we go along).&lt;/li&gt; &#xA; &lt;li&gt;A willingness to learn (most important).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For 1 &amp;amp; 2, I&#39;d recommend the &lt;a href=&#34;https://dbourke.link/ZTMMLcourse&#34;&gt;Zero to Mastery Data Science and Machine Learning Bootcamp&lt;/a&gt;, it&#39;ll teach you the fundamentals of machine learning and Python (I&#39;m biased though, I also teach that course).&lt;/p&gt; &#xA;&lt;h3&gt;How is the course taught?&lt;/h3&gt; &#xA;&lt;p&gt;All of the course materials are available for free in an online book at &lt;a href=&#34;https://learnpytorch.io&#34;&gt;learnpytorch.io&lt;/a&gt;. If you like to read, I&#39;d recommend going through the resources there.&lt;/p&gt; &#xA;&lt;p&gt;If you prefer to learn via video, the course is also taught in apprenticeship-style format, meaning I write PyTorch code, you write PyTorch code.&lt;/p&gt; &#xA;&lt;p&gt;There&#39;s a reason the course motto&#39;s include &lt;em&gt;if in doubt, run the code&lt;/em&gt; and &lt;em&gt;experiment, experiment, experiment!&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;My whole goal is to help you to do one thing: learn machine learning by writing PyTorch code.&lt;/p&gt; &#xA;&lt;p&gt;The code is all written via &lt;a href=&#34;https://colab.research.google.com&#34;&gt;Google Colab Notebooks&lt;/a&gt; (you could also use Jupyter Notebooks), an incredible free resource to experiment with machine learning.&lt;/p&gt; &#xA;&lt;h3&gt;What will I get if I finish the course?&lt;/h3&gt; &#xA;&lt;p&gt;There&#39;s certificates and all that jazz if you go through the videos.&lt;/p&gt; &#xA;&lt;p&gt;But certificates are meh.&lt;/p&gt; &#xA;&lt;p&gt;You can consider this course a machine learning momentum builder.&lt;/p&gt; &#xA;&lt;p&gt;By the end, you&#39;ll have written hundreds of lines of PyTorch code.&lt;/p&gt; &#xA;&lt;p&gt;And will have been exposed to many of the most important concepts in machine learning.&lt;/p&gt; &#xA;&lt;p&gt;So when you go to build your own machine learning projects or inspect a public machine learning project made with PyTorch, it&#39;ll feel familiar and if it doesn&#39;t, at least you&#39;ll know where to look.&lt;/p&gt; &#xA;&lt;h3&gt;What will I build in the course?&lt;/h3&gt; &#xA;&lt;p&gt;We start with the barebone fundamentals of PyTorch and machine learning, so even if you&#39;re new to machine learning you&#39;ll be caught up to speed.&lt;/p&gt; &#xA;&lt;p&gt;Then we‚Äôll explore more advanced areas including PyTorch neural network classification, PyTorch workflows, computer vision, custom datasets, experiment tracking, model deployment, and my personal favourite: transfer learning, a powerful technique for taking what one machine learning model has learned on another problem and applying it to your own!&lt;/p&gt; &#xA;&lt;p&gt;Along the way, you‚Äôll build three milestone projects surrounding an overarching project called FoodVision, a neural network computer vision model to classify images of food.&lt;/p&gt; &#xA;&lt;p&gt;These milestone projects will help you practice using PyTorch to cover important machine learning concepts and create a portfolio you can show employers and say &#34;here&#39;s what I&#39;ve done&#34;.&lt;/p&gt; &#xA;&lt;h3&gt;How do I get started?&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Click on one of the notebook or section links above like &#34;&lt;a href=&#34;https://www.learnpytorch.io/00_pytorch_fundamentals/&#34;&gt;00. PyTorch Fundamentals&lt;/a&gt;&#34;.&lt;/li&gt; &#xA; &lt;li&gt;Click the &#34;Open in Colab&#34; button up the top.&lt;/li&gt; &#xA; &lt;li&gt;Press SHIFT+Enter a few times and see what happens.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;My question isn&#39;t answered&lt;/h3&gt; &#xA;&lt;p&gt;Please leave a &lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning/discussions&#34;&gt;discussion&lt;/a&gt; or send me an email directly: daniel (at) mrdbourke (dot) com.&lt;/p&gt; &#xA;&lt;h2&gt;Log&lt;/h2&gt; &#xA;&lt;p&gt;Almost daily updates of what&#39;s happening.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;27 July 2022 - cleanup much of 08, start on slides for 08, exercises and extra-curriculum next&lt;/li&gt; &#xA; &lt;li&gt;26 July 2022 - add annotations and images for 08&lt;/li&gt; &#xA; &lt;li&gt;25 July 2022 - add annotations for 08&lt;/li&gt; &#xA; &lt;li&gt;24 July 2022 - launched first half of course (notebooks 00-04) in a single video (25+ hours!!!) on YouTube: &lt;a href=&#34;https://youtu.be/Z_ikDlimN6A&#34;&gt;https://youtu.be/Z_ikDlimN6A&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;21 July 2022 - add annotations and images for 08&lt;/li&gt; &#xA; &lt;li&gt;20 July 2022 - add annotations and images for 08, getting so close! this is an epic section&lt;/li&gt; &#xA; &lt;li&gt;19 July 2022 - add annotations and images for 08&lt;/li&gt; &#xA; &lt;li&gt;15 July 2022 - add annotations and images for 08&lt;/li&gt; &#xA; &lt;li&gt;14 July 2022 - add annotations for 08&lt;/li&gt; &#xA; &lt;li&gt;12 July 2022 - add annotations for 08, woo woo this is bigggg section!&lt;/li&gt; &#xA; &lt;li&gt;11 July 2022 - add annotations for 08&lt;/li&gt; &#xA; &lt;li&gt;9 July 2022 - add annotations for 08&lt;/li&gt; &#xA; &lt;li&gt;8 July 2022 - add a bunch of annotations to 08&lt;/li&gt; &#xA; &lt;li&gt;6 July 2022 - course launched on ZTM Academy with videos for sections 00-07! üöÄ - &lt;a href=&#34;https://dbourke.link/ZTMPyTorch&#34;&gt;https://dbourke.link/ZTMPyTorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;1 July 2022 - add annotations and images for 08&lt;/li&gt; &#xA; &lt;li&gt;30 June 2022 - add annotations for 08&lt;/li&gt; &#xA; &lt;li&gt;28 June 2022 - recorded 11 videos for section 07, total video count 213, all videos for section 07 complete!&lt;/li&gt; &#xA; &lt;li&gt;27 June 2022 - recorded 11 videos for section 07, total video count 202&lt;/li&gt; &#xA; &lt;li&gt;25 June 2022 - recreated 7 videos for section 06 to include updated APIs, total video count 191&lt;/li&gt; &#xA; &lt;li&gt;24 June 2022 - recreated 12 videos for section 06 to include updated APIs&lt;/li&gt; &#xA; &lt;li&gt;23 June 2022 - finish annotations for 07, add exercise template and solutions for 07 + video walkthrough on YouTube: &lt;a href=&#34;https://youtu.be/cO_r2FYcAjU&#34;&gt;https://youtu.be/cO_r2FYcAjU&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;21 June 2022 - make 08 runnable end-to-end, add images and annotations for 07&lt;/li&gt; &#xA; &lt;li&gt;17 June 2022 - fix up 06, 07 v2 for upcoming torchvision version upgrade, add plenty of annotations to 08&lt;/li&gt; &#xA; &lt;li&gt;13 June 2022 - add notebook 08 first version, starting to replicate the Vision Transformer paper&lt;/li&gt; &#xA; &lt;li&gt;10 June 2022 - add annotations for 07 v2&lt;/li&gt; &#xA; &lt;li&gt;09 June 2022 - create 07 v2 for &lt;code&gt;torchvision&lt;/code&gt; v0.13 (this will replace 07 v1 when &lt;code&gt;torchvision=0.13&lt;/code&gt; is released)&lt;/li&gt; &#xA; &lt;li&gt;08 June 2022 - adapt 06 v2 for &lt;code&gt;torchvision&lt;/code&gt; v0.13 (this will replace 06 v1 when &lt;code&gt;torchvision=0.13&lt;/code&gt; is released)&lt;/li&gt; &#xA; &lt;li&gt;07 June 2022 - create notebook 06 v2 for upcoming &lt;code&gt;torchvision&lt;/code&gt; v0.13 update (new transfer learning methods)&lt;/li&gt; &#xA; &lt;li&gt;04 June 2022 - add annotations for 07&lt;/li&gt; &#xA; &lt;li&gt;03 June 2022 - huuuuuuge amount of annotations added to 07&lt;/li&gt; &#xA; &lt;li&gt;31 May 2022 - add a bunch of annotations for 07, make code runnable end-to-end&lt;/li&gt; &#xA; &lt;li&gt;30 May 2022 - record 4 videos for 06, finished section 06, onto section 07, total videos 186&lt;/li&gt; &#xA; &lt;li&gt;28 May 2022 - record 10 videos for 06, total videos 182&lt;/li&gt; &#xA; &lt;li&gt;24 May 2022 - add solutions and exercises for 06&lt;/li&gt; &#xA; &lt;li&gt;23 May 2022 - finished annotations and images for 06, time to do exercises and solutions&lt;/li&gt; &#xA; &lt;li&gt;22 May 2202 - add plenty of images to 06&lt;/li&gt; &#xA; &lt;li&gt;18 May 2022 - add plenty of annotations to 06&lt;/li&gt; &#xA; &lt;li&gt;17 May 2022 - added a bunch of annotations for section 06&lt;/li&gt; &#xA; &lt;li&gt;16 May 2022 - recorded 10 videos for section 05, finish videos for section 05 ‚úÖ&lt;/li&gt; &#xA; &lt;li&gt;12 May 2022 - added exercises and solutions for 05&lt;/li&gt; &#xA; &lt;li&gt;11 May 2022 - clean up part 1 and part 2 notebooks for 05, make slides for 05, start on exercises and solutions for 05&lt;/li&gt; &#xA; &lt;li&gt;10 May 2022 - huuuuge updates to the 05 section, see the website, it looks pretty: &lt;a href=&#34;https://www.learnpytorch.io/05_pytorch_going_modular/&#34;&gt;https://www.learnpytorch.io/05_pytorch_going_modular/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;09 May 2022 - add a bunch of materials for 05, cleanup docs&lt;/li&gt; &#xA; &lt;li&gt;08 May 2022 - add a bunch of materials for 05&lt;/li&gt; &#xA; &lt;li&gt;06 May 2022 - continue making materials for 05&lt;/li&gt; &#xA; &lt;li&gt;05 May 2022 - update section 05 with headings/outline&lt;/li&gt; &#xA; &lt;li&gt;28 Apr 2022 - recorded 13 videos for 04, finished videos for 04, now to make materials for 05&lt;/li&gt; &#xA; &lt;li&gt;27 Apr 2022 - recorded 3 videos for 04&lt;/li&gt; &#xA; &lt;li&gt;26 Apr 2022 - recorded 10 videos for 04&lt;/li&gt; &#xA; &lt;li&gt;25 Apr 2022 - recorded 11 videos for 04&lt;/li&gt; &#xA; &lt;li&gt;24 Apr 2022 - prepared slides for 04&lt;/li&gt; &#xA; &lt;li&gt;23 Apr 2022 - recorded 6 videos for 03, finished videos for 03, now to 04&lt;/li&gt; &#xA; &lt;li&gt;22 Apr 2022 - recorded 5 videos for 03&lt;/li&gt; &#xA; &lt;li&gt;21 Apr 2022 - recorded 9 videos for 03&lt;/li&gt; &#xA; &lt;li&gt;20 Apr 2022 - recorded 3 videos for 03&lt;/li&gt; &#xA; &lt;li&gt;19 Apr 2022 - recorded 11 videos for 03&lt;/li&gt; &#xA; &lt;li&gt;18 Apr 2022 - finish exercises/solutions for 04, added live-coding walkthrough of 04 exercises/solutions on YouTube: &lt;a href=&#34;https://youtu.be/vsFMF9wqWx0&#34;&gt;https://youtu.be/vsFMF9wqWx0&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;16 Apr 2022 - finish exercises/solutions for 03, added live-coding walkthrough of 03 exercises/solutions on YouTube: &lt;a href=&#34;https://youtu.be/_PibmqpEyhA&#34;&gt;https://youtu.be/_PibmqpEyhA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;14 Apr 2022 - add final images/annotations for 04, begin on exercises/solutions for 03 &amp;amp; 04&lt;/li&gt; &#xA; &lt;li&gt;13 Apr 2022 - add more images/annotations for 04&lt;/li&gt; &#xA; &lt;li&gt;3 Apr 2022 - add more annotations for 04&lt;/li&gt; &#xA; &lt;li&gt;2 Apr 2022 - add more annotations for 04&lt;/li&gt; &#xA; &lt;li&gt;1 Apr 2022 - add more annotations for 04&lt;/li&gt; &#xA; &lt;li&gt;31 Mar 2022 - add more annotations for 04&lt;/li&gt; &#xA; &lt;li&gt;29 Mar 2022 - add more annotations for 04&lt;/li&gt; &#xA; &lt;li&gt;27 Mar 2022 - starting to add annotations for 04&lt;/li&gt; &#xA; &lt;li&gt;26 Mar 2022 - making dataset for 04&lt;/li&gt; &#xA; &lt;li&gt;25 Mar 2022 - make slides for 03&lt;/li&gt; &#xA; &lt;li&gt;24 Mar 2022 - fix error for 03 not working in docs (finally)&lt;/li&gt; &#xA; &lt;li&gt;23 Mar 2022 - add more images for 03&lt;/li&gt; &#xA; &lt;li&gt;22 Mar 2022 - add images for 03&lt;/li&gt; &#xA; &lt;li&gt;20 Mar 2022 - add more annotations for 03&lt;/li&gt; &#xA; &lt;li&gt;18 Mar 2022 - add more annotations for 03&lt;/li&gt; &#xA; &lt;li&gt;17 Mar 2022 - add more annotations for 03&lt;/li&gt; &#xA; &lt;li&gt;16 Mar 2022 - add more annotations for 03&lt;/li&gt; &#xA; &lt;li&gt;15 Mar 2022 - add more annotations for 03&lt;/li&gt; &#xA; &lt;li&gt;14 Mar 2022 - start adding annotations for notebook 03, see the work in progress here: &lt;a href=&#34;https://www.learnpytorch.io/03_pytorch_computer_vision/&#34;&gt;https://www.learnpytorch.io/03_pytorch_computer_vision/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;12 Mar 2022 - recorded 12 videos for 02, finished section 02, now onto making materials for 03, 04, 05&lt;/li&gt; &#xA; &lt;li&gt;11 Mar 2022 - recorded 9 videos for 02&lt;/li&gt; &#xA; &lt;li&gt;10 Mar 2022 - recorded 10 videos for 02&lt;/li&gt; &#xA; &lt;li&gt;9 Mar 2022 - cleaning up slides/code for 02, getting ready for recording&lt;/li&gt; &#xA; &lt;li&gt;8 Mar 2022 - recorded 9 videos for section 01, finished section 01, now onto 02&lt;/li&gt; &#xA; &lt;li&gt;7 Mar 2022 - recorded 4 videos for section 01&lt;/li&gt; &#xA; &lt;li&gt;6 Mar 2022 - recorded 4 videos for section 01&lt;/li&gt; &#xA; &lt;li&gt;4 Mar 2022 - recorded 10 videos for section 01&lt;/li&gt; &#xA; &lt;li&gt;20 Feb 2022 - recorded 8 videos for section 00, finished section, now onto 01&lt;/li&gt; &#xA; &lt;li&gt;18 Feb 2022 - recorded 13 videos for section 00&lt;/li&gt; &#xA; &lt;li&gt;17 Feb 2022 - recorded 11 videos for section 00&lt;/li&gt; &#xA; &lt;li&gt;16 Feb 2022 - added setup guide&lt;/li&gt; &#xA; &lt;li&gt;12 Feb 2022 - tidy up README with table of course materials, finish images and slides for 01&lt;/li&gt; &#xA; &lt;li&gt;10 Feb 2022 - finished slides and images for 00, notebook is ready for publishing: &lt;a href=&#34;https://www.learnpytorch.io/00_pytorch_fundamentals/&#34;&gt;https://www.learnpytorch.io/00_pytorch_fundamentals/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;01-07 Feb 2022 - add annotations for 02, finished, still need images, going to work on exercises/solutions today&lt;/li&gt; &#xA; &lt;li&gt;31 Jan 2022 - start adding annotations for 02&lt;/li&gt; &#xA; &lt;li&gt;28 Jan 2022 - add exercies and solutions for 01&lt;/li&gt; &#xA; &lt;li&gt;26 Jan 2022 - lots more annotations to 01, should be finished tomorrow, will do exercises + solutions then too&lt;/li&gt; &#xA; &lt;li&gt;24 Jan 2022 - add a bunch of annotations to 01&lt;/li&gt; &#xA; &lt;li&gt;21 Jan 2022 - start adding annotations for 01&lt;/li&gt; &#xA; &lt;li&gt;20 Jan 2022 - finish annotations for 00 (still need to add images), add exercises and solutions for 00&lt;/li&gt; &#xA; &lt;li&gt;19 Jan 2022 - add more annotations for 00&lt;/li&gt; &#xA; &lt;li&gt;18 Jan 2022 - add more annotations for 00&lt;/li&gt; &#xA; &lt;li&gt;17 Jan 2022 - back from holidays, adding more annotations to 00&lt;/li&gt; &#xA; &lt;li&gt;10 Dec 2021 - start adding annoations for 00&lt;/li&gt; &#xA; &lt;li&gt;9 Dec 2021 - Created a website for the course (&lt;a href=&#34;https://learnpytorch.io&#34;&gt;learnpytorch.io&lt;/a&gt;) you&#39;ll see updates posted there as development continues&lt;/li&gt; &#xA; &lt;li&gt;8 Dec 2021 - Clean up notebook 07, starting to go back through code and add annotations&lt;/li&gt; &#xA; &lt;li&gt;26 Nov 2021 - Finish skeleton code for 07, added four different experiments, need to clean up and make more straightforward&lt;/li&gt; &#xA; &lt;li&gt;25 Nov 2021 - clean code for 06, add skeleton code for 07 (experiment tracking)&lt;/li&gt; &#xA; &lt;li&gt;24 Nov 2021 - Update 04, 05, 06 notebooks for easier digestion and learning, each section should cover a max of 3 big ideas, 05 is now dedicated to turning notebook code into modular code&lt;/li&gt; &#xA; &lt;li&gt;22 Nov 2021 - Update 04 train and test functions to make more straightforward&lt;/li&gt; &#xA; &lt;li&gt;19 Nov 2021 - Added 05 (transfer learning) notebook, update custom data loading code in 04&lt;/li&gt; &#xA; &lt;li&gt;18 Nov 2021 - Updated vision code for 03 and added custom dataset loading code in 04&lt;/li&gt; &#xA; &lt;li&gt;12 Nov 2021 - Added a bunch of skeleton code to notebook 04 for custom dataset loading, next is modelling with custom data&lt;/li&gt; &#xA; &lt;li&gt;10 Nov 2021 - researching best practice for custom datasets for 04&lt;/li&gt; &#xA; &lt;li&gt;9 Nov 2021 - Update 03 skeleton code to finish off building CNN model, onto 04 for loading custom datasets&lt;/li&gt; &#xA; &lt;li&gt;4 Nov 2021 - Add GPU code to 03 + train/test loops + &lt;code&gt;helper_functions.py&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;3 Nov 2021 - Add basic start for 03, going to finish by end of week&lt;/li&gt; &#xA; &lt;li&gt;29 Oct 2021 - Tidied up skeleton code for 02, still a few more things to clean/tidy, created 03&lt;/li&gt; &#xA; &lt;li&gt;28 Oct 2021 - Finished skeleton code for 02, going to clean/tidy tomorrow, 03 next week&lt;/li&gt; &#xA; &lt;li&gt;27 Oct 2021 - add a bunch of code for 02, going to finish tomorrow/by end of week&lt;/li&gt; &#xA; &lt;li&gt;26 Oct 2021 - update 00, 01, 02 with outline/code, skeleton code for 00 &amp;amp; 01 done, 02 next&lt;/li&gt; &#xA; &lt;li&gt;23, 24 Oct 2021 - update 00 and 01 notebooks with more outline/code&lt;/li&gt; &#xA; &lt;li&gt;20 Oct 2021 - add v0 outlines for 01 and 02, add rough outline of course to README, this course will focus on less but better&lt;/li&gt; &#xA; &lt;li&gt;19 Oct 2021 - Start repo üî•, add fundamentals notebook draft v0&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>jonkrohn/ML-foundations</title>
    <updated>2022-07-28T02:16:10Z</updated>
    <id>tag:github.com,2022-07-28:/jonkrohn/ML-foundations</id>
    <link href="https://github.com/jonkrohn/ML-foundations" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Machine Learning Foundations: Linear Algebra, Calculus, Statistics &amp; Computer Science&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Machine Learning Foundations&lt;/h1&gt; &#xA;&lt;p&gt;This repo is home to the code that accompanies Jon Krohn&#39;s &lt;em&gt;Machine Learning Foundations&lt;/em&gt; curriculum, which provides a comprehensive overview of all of the subjects ‚Äî across mathematics, statistics, and computer science ‚Äî that underlie contemporary machine learning approaches, including deep learning and other artificial intelligence techniques.&lt;/p&gt; &#xA;&lt;p&gt;There are eight subjects in the curriculum, organized into four subject areas. See the &#34;Machine Learning House&#34; section below for detail on why these are the essential foundational subject areas:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Linear Algebra&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;1: &lt;a href=&#34;https://github.com/jonkrohn/ML-foundations/raw/master/notebooks/1-intro-to-linear-algebra.ipynb&#34;&gt;Intro to Linear Algebra&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;2: &lt;a href=&#34;https://github.com/jonkrohn/ML-foundations/raw/master/notebooks/2-linear-algebra-ii.ipynb&#34;&gt;Linear Algebra II: Matrix Operations&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Calculus&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;3: &lt;a href=&#34;https://github.com/jonkrohn/ML-foundations/raw/master/notebooks/3-calculus-i.ipynb&#34;&gt;Calculus I: Limits &amp;amp; Derivatives&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;4: &lt;a href=&#34;https://github.com/jonkrohn/ML-foundations/raw/master/notebooks/4-calculus-ii.ipynb&#34;&gt;Calculus II: Partial Derivatives &amp;amp; Integrals&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Probability and Statistics&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;5: &lt;a href=&#34;https://github.com/jonkrohn/ML-foundations/raw/master/notebooks/5-probability.ipynb&#34;&gt;Probability &amp;amp; Information Theory&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;6: &lt;a href=&#34;https://github.com/jonkrohn/ML-foundations/raw/master/notebooks/6-statistics.ipynb&#34;&gt;Intro to Statistics&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Computer Science&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;7: &lt;a href=&#34;https://github.com/jonkrohn/ML-foundations/raw/master/notebooks/7-algos-and-data-structures.ipynb&#34;&gt;Algorithms &amp;amp; Data Structures&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;8: &lt;a href=&#34;https://github.com/jonkrohn/ML-foundations/raw/master/notebooks/8-optimization.ipynb&#34;&gt;Optimization&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Later subjects build upon content from earlier subjects, so the recommended approach is to progress through the eight subjects in the order provided. That said, you&#39;re welcome to pick and choose individual subjects based on your interest or existing familiarity with the material. In particular, each of the four subject areas are fairly independent so could be approached separately.&lt;/p&gt; &#xA;&lt;h3&gt;Where and When&lt;/h3&gt; &#xA;&lt;p&gt;The eight &lt;em&gt;ML Foundations&lt;/em&gt; subjects were initially offered by &lt;a href=&#34;https://raw.githubusercontent.com/jonkrohn/ML-foundations/master/jonkrohn.com&#34;&gt;Jon Krohn&lt;/a&gt; as live online trainings in the &lt;a href=&#34;https://learning.oreilly.com/home/&#34;&gt;O&#39;Reilly learning platform&lt;/a&gt; from May-Sep 2020 (and were offered a second time from Jul-Dec 2021; see &lt;a href=&#34;https://www.jonkrohn.com/talks&#34;&gt;here&lt;/a&gt; for individual lecture dates).&lt;/p&gt; &#xA;&lt;p&gt;To suit your preferred mode of learning, the content is now available via several channels:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;YouTube&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Linear Algebra &lt;a href=&#34;https://www.youtube.com/playlist?list=PLRDl2inPrWQW1QSWhBU0ki-jq_uElkh2a&#34;&gt;complete playlist here&lt;/a&gt; and &lt;a href=&#34;https://www.jonkrohn.com/posts/2021/5/9/linear-algebra-for-machine-learning-complete-math-course-on-youtube&#34;&gt;detailed blog post here&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Calculus &lt;a href=&#34;https://www.youtube.com/playlist?list=PLRDl2inPrWQVu2OvnTvtkRpJ-wz-URMJx&#34;&gt;complete playlist here&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLRDl2inPrWQWwJ1mh4tCUxlLfZ76C1zge&#34;&gt;Probability playlist&lt;/a&gt; is in active development&lt;/li&gt; &#xA;   &lt;li&gt;By continuing to release a new video every Wednesday, all of the subjects of my ML Foundations curriculum will eventually be freely available on YouTube.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;O&#39;Reilly&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://learning.oreilly.com/videos/linear-algebra-for/9780137398119/&#34;&gt;Linear Algebra videos&lt;/a&gt; published in Dec 2020 (&lt;a href=&#34;https://www.youtube.com/watch?v=uG_wjmuigGg&#34;&gt;free hour-long lesson&lt;/a&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://learning.oreilly.com/videos/calculus-for-machine/9780137398171/&#34;&gt;Calculus videos&lt;/a&gt; published in Jan 2021 (&lt;a href=&#34;https://youtu.be/ZDAX17OGMAM&#34;&gt;free hour-long lesson&lt;/a&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://learning.oreilly.com/videos/probability-and-statistics/9780137566273/&#34;&gt;Probability and Stats videos&lt;/a&gt; published in May 2021 (&lt;a href=&#34;https://youtu.be/uJcGj-k50iE&#34;&gt;free hour-long lesson&lt;/a&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://learning.oreilly.com/videos/data-structures-algorithms/9780137644889/&#34;&gt;Computer Science videos&lt;/a&gt; published in Jun 2021 (&lt;a href=&#34;https://youtu.be/yfKkMdndY-E&#34;&gt;free hour-long lesson&lt;/a&gt;)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Udemy&lt;/strong&gt;: All the Linear Algebra and Calculus content has been &lt;a href=&#34;https://www.udemy.com/course/machine-learning-data-science-foundations-masterclass/&#34;&gt;live in a &lt;em&gt;Mathematical Foundations of ML&lt;/em&gt; course&lt;/a&gt; since Sep 2021 (free overview video &lt;a href=&#34;https://youtu.be/qhLo19EIA4g&#34;&gt;here&lt;/a&gt;). While this course stands alone as a complete introduction to the math subjects, Subjects 5-8 will eventually be added as free bonus material.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Open Data Science Conference&lt;/strong&gt;: The entire series was taught live online from Dec 2020 to Jun 2021. On-demand recordings of all these trainings are now available in the &lt;a href=&#34;https://aiplus.odsc.com/pages/mlbootcamp&#34;&gt;Ai+ Platform&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Book&lt;/strong&gt;: Chapter drafts to begin appearing in 2022&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;(Note that while YouTube contains 100% of the taught content, the paid options ‚Äî e.g., Udemy, O&#39;Reilly, and ODSC ‚Äî contain comprehensive solution walk-throughs for exercises that are not available on YouTube. Some of the paid options also include exclusive, platform-specific features such as interactive testing, &#34;cheat sheets&#34; and the awarding of a certificate for successful course completion.)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Push Notifications&lt;/h3&gt; &#xA;&lt;p&gt;To stay informed of future live training sessions, new video releases, and book chapter releases, consider signing up for Jon Krohn&#39;s &lt;a href=&#34;https://www.jonkrohn.com/&#34;&gt;email newsletter via his homepage&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Notebooks&lt;/h3&gt; &#xA;&lt;p&gt;All code is provided within Jupyter notebooks &lt;a href=&#34;https://github.com/jonkrohn/DLTFpT/raw/master/notebooks/&#34;&gt;in this directory&lt;/a&gt;. These notebooks are intended for use within the (free) &lt;a href=&#34;https://colab.research.google.com&#34;&gt;Colab cloud environment&lt;/a&gt; and that is the only environment currently actively supported.&lt;/p&gt; &#xA;&lt;p&gt;That said, if you are familiar with running Jupyter notebooks locally, you&#39;re welcome to do so (note that the library versions in this repo&#39;s &lt;a href=&#34;https://github.com/jonkrohn/ML-foundations/raw/master/Dockerfile&#34;&gt;Dockerfile&lt;/a&gt; are not necessarily current, but may provide a reasonable starting point for running Jupyter within a Docker container).&lt;/p&gt; &#xA;&lt;h3&gt;The Machine Learning House&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/jonkrohn/ML-foundations/raw/master/img/ML-house.png&#34; width=&#34;500&#34; align=&#34;center&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;To be an outstanding data scientist or ML engineer, it doesn&#39;t suffice to only know how to use ML algorithms via the abstract interfaces that the most popular libraries (e.g., scikit-learn, Keras) provide. To train innovative models or deploy them to run performantly in production, an in-depth appreciation of machine learning theory (pictured as the central, purple floor of the &#34;Machine Learning House&#34;) may be helpful or essential. And, to cultivate such in-depth appreciation of ML, one must possess a working understanding of the foundational subjects.&lt;/p&gt; &#xA;&lt;p&gt;When the foundations of the &#34;Machine Learning House&#34; are firm, it also makes it much easier to make the jump from general ML principles (purple floor) to specialized ML domains (the top floor, shown in gray) such as deep learning, natural language processing, machine vision, and reinforcement learning. This is because, the more specialized the application, the more likely its details for implementation are available only in academic papers or graduate-level textbooks, either of which typically assume an understanding of the foundational subjects.&lt;/p&gt; &#xA;&lt;p&gt;The content in this series may be particularly relevant for you if:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;You use high-level software libraries&lt;/strong&gt; to train or deploy machine learning algorithms, and would now like to understand the fundamentals underlying the abstractions, enabling you to expand your capabilities&lt;/li&gt; &#xA; &lt;li&gt;You‚Äôre a &lt;strong&gt;data scientist&lt;/strong&gt; who would like to reinforce your understanding of the subjects at the core of your professional discipline&lt;/li&gt; &#xA; &lt;li&gt;You‚Äôre a &lt;strong&gt;software developer&lt;/strong&gt; who would like to develop a firm foundation for the deployment of machine learning algorithms into production systems&lt;/li&gt; &#xA; &lt;li&gt;You‚Äôre a &lt;strong&gt;data analyst&lt;/strong&gt; or &lt;strong&gt;A.I. enthusiast&lt;/strong&gt; who would like to become a data scientist or data/ML engineer, and so you‚Äôre keen to deeply understand the field you‚Äôre entering from the ground up (very wise of you!)&lt;/li&gt; &#xA; &lt;li&gt;You&#39;re simply keen to understand the essentials of linear algebra, calculus, probability, stats, algorithms and/or data structures&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The foundational subjects have largely been unchanged in recent decades and are likely to remain so for the coming decades, yet they&#39;re critical across all machine learning and data science approaches. Thus, the foundations provide a solid, career-long bedrock.&lt;/p&gt; &#xA;&lt;h3&gt;Pedagogical Approach&lt;/h3&gt; &#xA;&lt;p&gt;The purpose of this series it to provide you with a practical, functional understanding of the content covered. Context will be given for each topic, highlighting its relevance to machine learning.&lt;/p&gt; &#xA;&lt;p&gt;As with other materials created by Jon Krohn (such as the book &lt;em&gt;&lt;a href=&#34;https://www.deeplearningillustrated.com/&#34;&gt;Deep Learning Illustrated&lt;/a&gt;&lt;/em&gt; and his 18-hour video series &lt;em&gt;&lt;a href=&#34;https://github.com/jonkrohn/DLTFpT/&#34;&gt;Deep Learning with TensorFlow, Keras, and PyTorch&lt;/a&gt;)&lt;/em&gt;, the content in the series is brought to life through the combination of:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Vivid full-color illustrations&lt;/li&gt; &#xA; &lt;li&gt;Paper-and-pencil comprehension exercises with fully-worked solutions&lt;/li&gt; &#xA; &lt;li&gt;Hundreds of straightforward examples of Python code within hands-on Jupyter notebooks (with a particular focus on the PyTorch and TensorFlow libraries)&lt;/li&gt; &#xA; &lt;li&gt;Resources for digging even deeper into topics that pique your curiosity&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Programming&lt;/strong&gt;: All code demos will be in Python so experience with it or another object-oriented programming language would be helpful for following along with the code examples. A good (and free!) resource for getting started with Python is Al Sweigart&#39;s &lt;a href=&#34;https://automatetheboringstuff.com/&#34;&gt;Automate the Boring Stuff&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Mathematics&lt;/strong&gt;: Familiarity with secondary school-level mathematics will make the class easier to follow along with. If you are comfortable dealing with quantitative information ‚Äì such as understanding charts and rearranging simple equations ‚Äî then you should be well-prepared to follow along with all of the mathematics. If you discover you have some math gaps as you work through this &lt;em&gt;ML Foundations&lt;/em&gt; curriculum, I recommend the free, comprehensive &lt;a href=&#34;https://www.khanacademy.org&#34;&gt;Khan Academy&lt;/a&gt; to fill those gaps in.&lt;/p&gt; &#xA;&lt;h3&gt;Oboe&lt;/h3&gt; &#xA;&lt;p&gt;Finally, here&#39;s an illustration of Oboe, the &lt;em&gt;Machine Learning Foundations&lt;/em&gt; mascot, created by the wonderful artist &lt;a href=&#34;https://www.aglaebassens.com&#34;&gt;Agla√© Bassens&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/jonkrohn/ML-foundations/raw/master/img/Oboe.jpg&#34; width=&#34;400&#34; align=&#34;center&#34;&gt; &lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>NirAharon/BoT-SORT</title>
    <updated>2022-07-28T02:16:10Z</updated>
    <id>tag:github.com,2022-07-28:/NirAharon/BoT-SORT</id>
    <link href="https://github.com/NirAharon/BoT-SORT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;BoT-SORT: Robust Associations Multi-Pedestrian Tracking&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;BoT-SORT&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2206.14651&#34;&gt;&lt;strong&gt;BoT-SORT: Robust Associations Multi-Pedestrian Tracking&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;Nir Aharon, Roy Orfaig, Ben-Zion Bobrovsky&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://paperswithcode.com/sota/multi-object-tracking-on-mot17?p=bot-sort-robust-associations-multi-pedestrian&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bot-sort-robust-associations-multi-pedestrian/multi-object-tracking-on-mot17&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://paperswithcode.com/sota/multi-object-tracking-on-mot20-1?p=bot-sort-robust-associations-multi-pedestrian&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bot-sort-robust-associations-multi-pedestrian/multi-object-tracking-on-mot20-1&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://arxiv.org/abs/2206.14651&#34;&gt;https://arxiv.org/abs/2206.14651&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/NirAharon/BoT-SORT/main/assets/Results_Bubbles.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Highlights üöÄ&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;YOLOX &amp;amp; YOLOv7 support&lt;/li&gt; &#xA; &lt;li&gt;Multi-class support&lt;/li&gt; &#xA; &lt;li&gt;Camera motion compensation&lt;/li&gt; &#xA; &lt;li&gt;Re-identification&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Coming Soon&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Trained YOLOv7 models for MOTChallenge.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; YOLOv7 detector.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Multi-class support.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Create OpenCV VideoStab GMC python binding or write Python version.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Deployment code.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Abstract&lt;/h2&gt; &#xA;&lt;p&gt;The goal of multi-object tracking (MOT) is detecting and tracking all the objects in a scene, while keeping a unique identifier for each object. In this paper, we present a new robust state-of-the-art tracker, which can combine the advantages of motion and appearance information, along with camera-motion compensation, and a more accurate Kalman filter state vector. Our new trackers BoT-SORT, and BoT-SORT-ReID rank first in the datasets of MOTChallenge [29, 11] on both MOT17 and MOT20 test sets, in terms of all the main MOT metrics: MOTA, IDF1, and HOTA. For MOT17: 80.5 MOTA, 80.2 IDF1, and 65.0 HOTA are achieved.&lt;/p&gt; &#xA;&lt;h3&gt;Visualization results on MOT challenge test set&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/57259165/177045531-947d3146-4d07-4549-a095-3d2daa4692be.mp4&#34;&gt;https://user-images.githubusercontent.com/57259165/177045531-947d3146-4d07-4549-a095-3d2daa4692be.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/57259165/177048139-05dcb382-010e-41a6-b607-bb2b76afc7db.mp4&#34;&gt;https://user-images.githubusercontent.com/57259165/177048139-05dcb382-010e-41a6-b607-bb2b76afc7db.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/57259165/180818066-f67d1f78-515e-4ee2-810f-abfed5a0afcb.mp4&#34;&gt;https://user-images.githubusercontent.com/57259165/180818066-f67d1f78-515e-4ee2-810f-abfed5a0afcb.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Tracking performance&lt;/h2&gt; &#xA;&lt;h3&gt;Results on MOT17 challenge test set&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Tracker&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;MOTA&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;IDF1&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;HOTA&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;BoT-SORT&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;80.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;79.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;BoT-SORT-ReID&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;80.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;80.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;65.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Results on MOT20 challenge test set&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Tracker&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;MOTA&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;IDF1&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;HOTA&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;BoT-SORT&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;77.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;76.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;BoT-SORT-ReID&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;77.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;77.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;The code was tested on Ubuntu 20.04&lt;/p&gt; &#xA;&lt;p&gt;BoT-SORT code is based on ByteTrack and FastReID. &lt;br&gt; Visit their installation guides for more setup options.&lt;/p&gt; &#xA;&lt;h3&gt;Setup with Anaconda&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step 1.&lt;/strong&gt; Create Conda environment and install pytorch.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda create -n botsort_env python=3.7&#xA;conda activate botsort_env&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step 2.&lt;/strong&gt; Install torch and matched torchvision from &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;pytorch.org&lt;/a&gt;.&lt;br&gt; The code was tested using torch 1.11.0+cu113 and torchvision==0.12.0&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step 3.&lt;/strong&gt; Install BoT-SORT.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/NirAharon/BoT-SORT.git&#xA;cd BoT-SORT&#xA;pip3 install -r requirements.txt&#xA;python3 setup.py develop&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step 4.&lt;/strong&gt; Install &lt;a href=&#34;https://github.com/cocodataset/cocoapi&#34;&gt;pycocotools&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip3 install cython; pip3 install &#39;git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Step 5. Others&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Cython-bbox&#xA;pip3 install cython_bbox&#xA;&#xA;# faiss cpu / gpu&#xA;pip3 install faiss-cpu&#xA;pip3 install faiss-gpu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Data Preparation&lt;/h2&gt; &#xA;&lt;p&gt;Download &lt;a href=&#34;https://motchallenge.net/data/MOT17/&#34;&gt;MOT17&lt;/a&gt; and &lt;a href=&#34;https://motchallenge.net/data/MOT20/&#34;&gt;MOT20&lt;/a&gt; from the &lt;a href=&#34;https://motchallenge.net/&#34;&gt;official website&lt;/a&gt;. And put them in the following structure:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;dataets_dir&amp;gt;&#xA;      ‚îÇ&#xA;      ‚îú‚îÄ‚îÄ MOT17&#xA;      ‚îÇ      ‚îú‚îÄ‚îÄ train&#xA;      ‚îÇ      ‚îî‚îÄ‚îÄ test    &#xA;      ‚îÇ&#xA;      ‚îî‚îÄ‚îÄ MOT20&#xA;             ‚îú‚îÄ‚îÄ train&#xA;             ‚îî‚îÄ‚îÄ test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For training the ReID, detection patches must be generated as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd &amp;lt;BoT-SORT_dir&amp;gt;&#xA;&#xA;# For MOT17 &#xA;python3 fast_reid/datasets/generate_mot_patches.py --data_path &amp;lt;dataets_dir&amp;gt; --mot 17&#xA;&#xA;# For MOT20&#xA; python3 fast_reid/datasets/generate_mot_patches.py --data_path &amp;lt;dataets_dir&amp;gt; --mot 20&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Link dataset to FastReID &lt;code&gt;export FASTREID_DATASETS=&amp;lt;BoT-SORT_dir&amp;gt;/fast_reid/datasets&lt;/code&gt;. If left unset, the default is &lt;code&gt;fast_reid/datasets&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Model Zoo&lt;/h2&gt; &#xA;&lt;p&gt;Download and store the trained models in &#39;pretrained&#39; folder as follow:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;BoT-SORT_dir&amp;gt;/pretrained&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;We used the publicly available &lt;a href=&#34;https://github.com/ifzhang/ByteTrack&#34;&gt;ByteTrack&lt;/a&gt; model zoo trained on MOT17, MOT20 and ablation study for YOLOX object detection.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Ours trained ReID models can be downloaded from &lt;a href=&#34;https://drive.google.com/file/d/1QZFWpoa80rqo7O-HXmlss8J8CnS7IUsN/view?usp=sharing&#34;&gt;MOT17-SBS-S50&lt;/a&gt;, &lt;a href=&#34;https://drive.google.com/file/d/1KqPQyj6MFyftliBHEIER7m_OrGpcrJwi/view?usp=sharing&#34;&gt;MOT20-SBS-S50&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;For multi-class MOT use &lt;a href=&#34;https://github.com/Megvii-BaseDetection/YOLOX&#34;&gt;YOLOX&lt;/a&gt; or &lt;a href=&#34;https://github.com/WongKinYiu/yolov7&#34;&gt;YOLOv7&lt;/a&gt; trained on COCO (or any custom weights).&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;h3&gt;Train the ReID Module&lt;/h3&gt; &#xA;&lt;p&gt;After generating MOT ReID dataset as described in the &#39;Data Preparation&#39; section.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd &amp;lt;BoT-SORT_dir&amp;gt;&#xA;&#xA;# For training MOT17 &#xA;python3 fast_reid/tools/train_net.py --config-file ./fast_reid/configs/MOT17/sbs_S50.yml MODEL.DEVICE &#34;cuda:0&#34;&#xA;&#xA;# For training MOT20&#xA;python3 fast_reid/tools/train_net.py --config-file ./fast_reid/configs/MOT20/sbs_S50.yml MODEL.DEVICE &#34;cuda:0&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Refer to &lt;a href=&#34;https://github.com/JDAI-CV/fast-reid&#34;&gt;FastReID&lt;/a&gt; repository for addition explanations and options.&lt;/p&gt; &#xA;&lt;h2&gt;Tracking&lt;/h2&gt; &#xA;&lt;p&gt;By submitting the txt files produced in this part to &lt;a href=&#34;https://motchallenge.net/&#34;&gt;MOTChallenge&lt;/a&gt; website and you can get the same results as in the paper.&lt;br&gt; Tuning the tracking parameters carefully could lead to higher performance. In the paper we apply ByteTrack&#39;s calibration.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Test on MOT17&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd &amp;lt;BoT-SORT_dir&amp;gt;&#xA;python3 tools/track.py &amp;lt;dataets_dir/MOT17&amp;gt; --default-parameters --with-reid --benchmark &#34;MOT17&#34; --eval &#34;test&#34; --fp16 --fuse&#xA;python3 tools/interpolation.py --txt_path &amp;lt;path_to_track_result&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Test on MOT20&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd &amp;lt;BoT-SORT_dir&amp;gt;&#xA;python3 tools/track.py &amp;lt;dataets_dir/MOT20&amp;gt; --default-parameters --with-reid --benchmark &#34;MOT20&#34; --eval &#34;test&#34; --fp16 --fuse&#xA;python3 tools/interpolation.py --txt_path &amp;lt;path_to_track_result&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Evaluation on MOT17 validation set (the second half of the train set)&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd &amp;lt;BoT-SORT_dir&amp;gt;&#xA;&#xA;# BoT-SORT&#xA;python3 tools/track.py &amp;lt;dataets_dir/MOT17&amp;gt; --default-parameters --benchmark &#34;MOT17&#34; --eval &#34;val&#34; --fp16 --fuse&#xA;&#xA;# BoT-SORT-ReID&#xA;python3 tools/track.py &amp;lt;dataets_dir/MOT17&amp;gt; --default-parameters --with-reid --benchmark &#34;MOT17&#34; --eval &#34;val&#34; --fp16 --fuse&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Other experiments&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Other parameters can be used &lt;strong&gt;without&lt;/strong&gt; passing --default-parameters flag. &lt;br&gt; For evaluating the train and validation sets we recommend using the official MOTChallenge evaluation code from &lt;a href=&#34;https://github.com/JonathonLuiten/TrackEval&#34;&gt;TrackEval&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# For all the available tracking parameters, see:&#xA;python3 tools/track.py -h &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Experiments with YOLOv7&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Other parameters can be used &lt;strong&gt;without&lt;/strong&gt; passing --default-parameters flag. &lt;br&gt; For evaluating the train and validation sets we recommend using the official MOTChallenge evaluation code from &lt;a href=&#34;https://github.com/JonathonLuiten/TrackEval&#34;&gt;TrackEval&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# For all the available tracking parameters, see:&#xA;python3 tools/track_yolov7.py -h &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;Demo with BoT-SORT(-ReID) based YOLOX and multi-class.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd &amp;lt;BoT-SORT_dir&amp;gt;&#xA;&#xA;# Original example&#xA;python3 tools/demo.py video --path &amp;lt;path_to_video&amp;gt; -f yolox/exps/example/mot/yolox_x_mix_det.py -c pretrained/bytetrack_x_mot17.pth.tar --with-reid --fuse-score --fp16 --fuse --save_result&#xA;&#xA;# Multi-class example&#xA;python3 tools/mc_demo.py video --path &amp;lt;path_to_video&amp;gt; -f yolox/exps/example/mot/yolox_x_mix_det.py -c pretrained/bytetrack_x_mot17.pth.tar --with-reid --fuse-score --fp16 --fuse --save_result&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Demo with BoT-SORT(-ReID) based YOLOv7 and multi-class.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd &amp;lt;BoT-SORT_dir&amp;gt;&#xA;python3 tools/mc_demo_yolov7.py --weights pretrained/yolov7-d6.pt --source &amp;lt;path_to_video/images&amp;gt; --fuse-score --agnostic-nms (--with-reid)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Note&lt;/h2&gt; &#xA;&lt;p&gt;Our camera motion compensation module is based on the OpenCV contrib C++ version of VideoStab Global Motion Estimation, which currently does not have a Python version. &lt;br&gt; Motion files can be generated using the C++ project called &#39;VideoCameraCorrection&#39; in the GMC folder. &lt;br&gt; The generated files can be used from the tracker. &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;In addition, python-based motion estimation techniques are available and can be chosen by passing &lt;br&gt; &#39;--cmc-method&#39; &amp;lt;files | orb | ecc&amp;gt; to demo.py or track.py.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{aharon2022bot,&#xA;  title={BoT-SORT: Robust Associations Multi-Pedestrian Tracking},&#xA;  author={Aharon, Nir and Orfaig, Roy and Bobrovsky, Ben-Zion},&#xA;  journal={arXiv preprint arXiv:2206.14651},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;A large part of the codes, ideas and results are borrowed from &lt;a href=&#34;https://github.com/ifzhang/ByteTrack&#34;&gt;ByteTrack&lt;/a&gt;, &lt;a href=&#34;https://github.com/dyhBUPT/StrongSORT&#34;&gt;StorngSORT&lt;/a&gt;, &lt;a href=&#34;https://github.com/JDAI-CV/fast-reid&#34;&gt;FastReID&lt;/a&gt;, &lt;a href=&#34;https://github.com/Megvii-BaseDetection/YOLOX&#34;&gt;YOLOX&lt;/a&gt; and &lt;a href=&#34;https://github.com/wongkinyiu/yolov7&#34;&gt;YOLOv7&lt;/a&gt;. Thanks for their excellent work!&lt;/p&gt;</summary>
  </entry>
</feed>