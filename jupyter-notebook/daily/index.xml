<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-30T01:38:39Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>PhoebusSi/Alpaca-CoT</title>
    <updated>2023-03-30T01:38:39Z</updated>
    <id>tag:github.com,2023-03-30:/PhoebusSi/Alpaca-CoT</id>
    <link href="https://github.com/PhoebusSi/Alpaca-CoT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;We extend CoT data to Alpaca to boost its reasoning ability. We are constantly expanding our collection of instruction-tuning data, and integrating more LLMs together for easy use. （我们将CoT数据扩展到Alpaca以提高其推理能力，同时我们将不断收集更多的instruction-tuning数据集,并在我们框架下集成进更多的LLM。）&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://github.com/PhoebusSi/alpaca-CoT/raw/main/figures/Alpaca-CoT-2.jpg&#34; alt=&#34;Alpaca-CoT&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Evolving Alpaca: An Empirical Study on Instruction Tuning for Large Language Models (&lt;strong&gt;Alpaca-CoT&lt;/strong&gt;)&lt;/h1&gt; &#xA;&lt;p&gt;中文README，请看&lt;a href=&#34;https://github.com/PhoebusSi/Alpaca-CoT/raw/main/CN_README.md&#34;&gt;这里&lt;/a&gt;。(Chinese READEME can be found &lt;a href=&#34;https://github.com/PhoebusSi/Alpaca-CoT/raw/main/CN_README.md&#34;&gt;here&lt;/a&gt;.)&lt;/p&gt; &#xA;&lt;p&gt;This is the repository for the &lt;code&gt;Evolving Alpaca&lt;/code&gt; project, which aims to extensively collect instruction-tuning datasets (especially the CoT datasets) and conduct an in-depth empirical study based on &lt;a href=&#34;https://arxiv.org/abs/2302.13971v1&#34;&gt;LLaMA&lt;/a&gt; model [1]. &lt;code&gt;Evolving&lt;/code&gt; is used to describe the continuous expansion of our &lt;a href=&#34;https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/&#34;&gt;instruction-tuning data collection&lt;/a&gt;, which will continuously enhance &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Alpaca&lt;/a&gt;&#39;s [2] instruction-following capabilities.&lt;/p&gt; &#xA;&lt;p&gt;You are in a warm welcome to provide us with any non-collected instruction-tuning datasets (or their sources). We will uniformly format them, train Alpaca model (and other LLMs in the early future) with these datasets, open source the &lt;a href=&#34;https://huggingface.co/QingyiSi/Alpaca-CoT/tree/main&#34;&gt;model checkpoints&lt;/a&gt;, and conduct extensive empirical studies. We hope that our project can make a modest contribution to the open-source process of large language models, and reduce its threshold for NLP researchers to get started.&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;3.30: LLM ChatGLM (THUDM/chatglm-6b) is merged in &lt;code&gt;uniform_finetune.py&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;3.29: LLM BLOOM (bloomz-7b1-mt) is merged in &lt;code&gt;uniform_finetune.py&lt;/code&gt;. (The corresponding model and command will be released later.)&lt;/li&gt; &#xA; &lt;li&gt;3.28: To facilitate downloading, all model(LoRA) weights have been uploaded &lt;a href=&#34;https://huggingface.co/QingyiSi/Alpaca-CoT/tree/main&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;3.28: Chinese instruction dataset(1M, Does not contain the original 0.5M ones) published by BELLE has been formatted and collected &lt;a href=&#34;https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main&#34;&gt;here&lt;/a&gt;. (The model will be released later.)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;LLaMA [1] is a great work that demonstrates the amazing zero-shot and few-shot ability. It significantly reduces the cost of training, finetuning, and using competitive large language models, i.e., LLaMA-13B outperforms GPT-3(175B) and LLaMA-65B is competitive to PaLM-540M. Recently, to boost the instruction-following ability of LLaMA, Stanford Alpaca [2] finetuned LLaMA-7B on 52K instruction-following data generated by the &lt;a href=&#34;https://arxiv.org/abs/2212.10560&#34;&gt;Self-Instruct&lt;/a&gt; [3] techniques. However, at present, the LLM research community still faces two challenges: 1. Even LLaMA still has high requirements for computing resources, and 2. There are not many open source datasets for instruction finetuning.&lt;/p&gt; &#xA;&lt;p&gt;To this end, we propose this project, which leverages various improvements that were subsequently proposed, with the following advantages:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This repo contains code, modified from &lt;a href=&#34;https://github.com/tloen/alpaca-lora&#34;&gt;here&lt;/a&gt;, which can &lt;strong&gt;&lt;em&gt;finetune LLaMA cheaply and efficiently&lt;/em&gt;&lt;/strong&gt; (without performance degradation compared to Stanford Alpaca) by using &lt;a href=&#34;https://arxiv.org/pdf/2106.09685.pdf&#34;&gt;low-rank adaptation (LoRA)&lt;/a&gt; [4], &lt;a href=&#34;https://github.com/huggingface/peft&#34;&gt;PEFT&lt;/a&gt; and &lt;a href=&#34;https://github.com/TimDettmers/bitsandbytes&#34;&gt;bitsandbytes&lt;/a&gt;. The &lt;code&gt;7b&lt;/code&gt;, &lt;code&gt;13b&lt;/code&gt; and &lt;code&gt;30b&lt;/code&gt; versions of LLaMA models can be easily trained on a single 80G A100.&lt;/li&gt; &#xA; &lt;li&gt;The models published in this repo significantly &lt;strong&gt;&lt;em&gt;improve the CoT (reasoning) capability&lt;/em&gt;&lt;/strong&gt;, using CoT datasets published by FLAN [5].&lt;/li&gt; &#xA; &lt;li&gt;The models published in this repo significantly &lt;strong&gt;&lt;em&gt;improve the ability to follow Chinese instructions&lt;/em&gt;&lt;/strong&gt;, with the help of Chinese instruction datasets published by BELLE [6].&lt;/li&gt; &#xA; &lt;li&gt;This repo contains &lt;strong&gt;&lt;em&gt;a &lt;a href=&#34;https://huggingface.co/datasets/QingyiSi/Alpaca-CoT&#34;&gt;collection of instruction-finetuning datasets&lt;/a&gt; that are continuously collected&lt;/em&gt;&lt;/strong&gt;, which so far includes English, Chinese and CoT instructions. In addition, a collection of checkpoints trained with various instruction datasets is also provided.&lt;/li&gt; &#xA; &lt;li&gt;This repo contains &lt;strong&gt;&lt;em&gt;extensive empirical studies and qualitative analysis&lt;/em&gt;&lt;/strong&gt;, which may provide valuable findings and promote the exploration of LLM in the future.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;To the best of our knowledge, this work is the first to study &lt;em&gt;CoT reasoning&lt;/em&gt; based on LLaMA and Alpaca.&lt;/strong&gt; Therefore, we abbreviate our work to &lt;code&gt;Alpaca-CoT&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;[1]: &lt;a href=&#34;https://arxiv.org/abs/2302.13971v1&#34;&gt;LLaMA: Open and Efficient Foundation Language Models&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[2]: &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca: An Instruction-following LLaMA model&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[3]: &lt;a href=&#34;https://arxiv.org/abs/2212.10560&#34;&gt;Self-Instruct: Aligning Language Model with Self Generated Instructions&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[4]: &lt;a href=&#34;https://arxiv.org/pdf/2106.09685.pdf&#34;&gt;LoRA: Low-Rank Adaptation of Large Language Models&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[5]: &lt;a href=&#34;https://arxiv.org/abs/2210.11416&#34;&gt;FLAN: Scaling Instruction-Finetuned Language Models&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[6]: &lt;a href=&#34;https://github.com/LianjiaTech/BELLE&#34;&gt;BELLE: Bloom-Enhanced Large Language model Engine&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Data Collection&lt;/h2&gt; &#xA;&lt;h3&gt;Statistics&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/PhoebusSi/alpaca-CoT/raw/main/figures/piechart.png&#34; alt=&#34;data collection statistics&#34;&gt; The current collection of instruction-finetuning datasets consists mainly of three parts:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;alpaca_data_cleaned.json&lt;/code&gt;: about 52K English instruction-following training samples.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;belle_data_cn.json&lt;/code&gt;: about 0.5M Chinese |instruction-following training samples.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;CoT_data.json&lt;/code&gt;: 9 CoT datasets involving about 75k samples.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;More details on the usage and sources of different datasets can be found &lt;a href=&#34;https://github.com/PhoebusSi/alpaca-CoT/tree/main/data&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Download&lt;/h3&gt; &#xA;&lt;p&gt;You can download all the formatted data &lt;a href=&#34;https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main&#34;&gt;here&lt;/a&gt;. Then you should put them in the &lt;a href=&#34;https://github.com/PhoebusSi/alpaca-CoT/tree/main/data&#34;&gt;data&lt;/a&gt; folder.&lt;/p&gt; &#xA;&lt;p&gt;You can download all checkpoints trained on various types of instruction data from &lt;a href=&#34;https://huggingface.co/QingyiSi/Alpaca-CoT/tree/main&#34;&gt;here&lt;/a&gt;. Then, after setting &lt;code&gt;LoRA_Weights&lt;/code&gt; (in &lt;code&gt;generate.py&lt;/code&gt;) to the local path, you can directly execute the model inference.&lt;/p&gt; &#xA;&lt;h3&gt;Data Fomatting&lt;/h3&gt; &#xA;&lt;p&gt;All data in our collection is formatted into the same templates, where each sample is as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;[&#xA;{&#34;instruction&#34;: instruction string,&#xA;&#34;input&#34;: input string, # (may be empty)&#xA;&#34;output&#34;: output string}&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that, for CoT datasets, we first use the &lt;a href=&#34;https://github.com/google-research/FLAN/raw/main/flan/v2/templates.py&#34;&gt;template&lt;/a&gt; provided by FLAN to change the original dataset into various Chain-of-Thoughts forms, and then convert it to the above format. The formatting script can be found &lt;a href=&#34;https://github.com/PhoebusSi/alpaca-CoT/raw/main/data/origin_cot_data/formating.py&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Dict of Data_type and Path&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;             &#34;alpaca&#34;: &#34;./data/alpaca_data_cleaned.json&#34;,&#xA;             &#34;belle&#34;: &#34;./data/belle_data_cn.json&#34;,&#xA;             &#34;alpaca-belle&#34;: &#34;./data/alpaca_plus_belle_data.json&#34;,&#xA;             &#34;cot&#34;: &#34;./data/CoT_data.json&#34;,&#xA;             &#34;alpaca-cot&#34;: &#34;./data/alcapa_plus_cot.json&#34;,&#xA;             &#34;alpaca-belle-cot&#34;: &#34;./data/alcapa_plus_belle_plus_cot.json&#34;,&#xA;             &#34;belle1.5m&#34;: &#34;./data/belle_data1.5M_cn.json.json&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Instruction Finetuning&lt;/h2&gt; &#xA;&lt;h3&gt;Setup&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that, make sure python&amp;gt;=3.9 when finetuning ChatGLM.&lt;/p&gt; &#xA;&lt;h3&gt;Instruction Tuning&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Single GPU&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;for LLaMA&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 uniform_finetune.py --model_type llama --model_name_or_path decapoda-research/llama-7b-hf \&#xA;    --data alpaca-belle-cot --lora_target_modules q_proj v_proj \&#xA;    --per_gpu_train_batch_size 128 --learning_rate 3e-4 --epochs 1 &#xA;    &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;for ChatGLM&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 uniform_finetune.py   --model_type chatglm --model_name_or_path THUDM/chatglm-6b \&#xA;    --data alpaca-belle-cot --lora_target_modules query_key_value \&#xA;    --lora_r 32 --lora_alpha 32 --lora_dropout 0.1 --per_gpu_train_batch_size 2 \&#xA;    --learning_rate 2e-5 --epochs 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that &lt;code&gt;load_in_8bit&lt;/code&gt; is not yet suitable for ChatGLM, so batch_size must be much smaller than others.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;for BLOOM&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 uniform_finetune.py   --model_type bloom --model_name_or_path bigscience/bloomz-7b1-mt \&#xA;    --data alpaca-belle-cot --lora_target_modules query_key_value \&#xA;    --per_gpu_train_batch_size 128 --learning_rate 3e-4 --epochs 1 &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that you can also pass the local path (where the LLM weights saved) to &lt;code&gt;--model_name_or_path&lt;/code&gt;. And the data type &lt;code&gt;--data&lt;/code&gt; can be freely set according to your interests.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multiple GPUs&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;for LLaMA&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 -m torch.distributed.launch --nproc_per_node 4  \&#xA;    --nnodes=1 --node_rank=0 --master_addr=xxx --master_port=yyy uniform_finetune.py \&#xA;    --model_type llama --model_name_or_path decapoda-research/llama-7b-hf \&#xA;    --data alpaca-belle-cot --lora_target_modules q_proj v_proj \&#xA;    --per_gpu_train_batch_size 128 --learning_rate 3e-4 --epochs 1 &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;for ChatGLM&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 -m torch.distributed.launch --nproc_per_node 4  \&#xA;    --nnodes=1 --node_rank=0 --master_addr=xxx --master_port=yyy \&#xA;    uniform_finetune.py   --model_type chatglm --model_name_or_path THUDM/chatglm-6b \&#xA;    --data alpaca-belle-cot --lora_target_modules query_key_value \&#xA;    --lora_r 32 --lora_alpha 32 --lora_dropout 0.1 --per_gpu_train_batch_size 2 \&#xA;    --learning_rate 2e-5 --epochs 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that &lt;code&gt;load_in_8bit&lt;/code&gt; is not yet suitable for ChatGLM, so batch_size must be much smaller than others.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;for BLOOM&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 -m torch.distributed.launch --nproc_per_node 4  \&#xA;    --nnodes=1 --node_rank=0 --master_addr=xxx --master_port=yyy \&#xA;    uniform_finetune.py   --model_type bloom --model_name_or_path bigscience/bloomz-7b1-mt \&#xA;    --data alpaca-belle-cot --lora_target_modules query_key_value \&#xA;    --per_gpu_train_batch_size 128 --learning_rate 3e-4 --epochs 1  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Inference #To be modified&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 generate.py --size 7 --data alpaca-belle-cot&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;More details of instruction finetuing and inference can be found &lt;a href=&#34;https://github.com/tloen/alpaca-lora&#34;&gt;here&lt;/a&gt; where we modified from. Note that the folders &lt;code&gt;saved-xxx7b&lt;/code&gt; are the save path for LoRA weights, and LLaMA weights are automatically downloaded from Hugging Face.&lt;/p&gt; &#xA;&lt;h2&gt;Quantitative Analysis&lt;/h2&gt; &#xA;&lt;h3&gt;Ablation of CoT and Chinese Instructions&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/PhoebusSi/alpaca-CoT/raw/main/figures/ablation-cot.png&#34; alt=&#34;ablation-cot&#34;&gt; &#34;w/o CoT&#34; and &#34;w/o CN&#34; denote models that exclude CoT data and Chinese instructions from their instruction finetuning data, respectively.&lt;/p&gt; &#xA;&lt;p&gt;The above table shows two examples (invoving with numerical calculations) that require a certain amount of reasoning ability to respond correctly. As shown in the middle column, &lt;code&gt;Ours w/o CoT&lt;/code&gt; fails to generate the correct response, which shows that once the finetuning data does not contain CoT data, the model&#39;s reasoning ability significantly decreases. This further demonstrates that CoT data is essential for LLM models.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/PhoebusSi/alpaca-CoT/raw/main/figures/ablation-cn.png&#34; alt=&#34;ablation-cot&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The above table shows two examples that require the ability to respond to Chinese instructions. As shown in the right column, either the generated content of &lt;code&gt;Ours w/o CN&lt;/code&gt; is unreasonable, or the Chinese instructions are answered in English by &lt;code&gt;Ours w/o CN&lt;/code&gt;. This shows that removing Chinese data during finetuning will cause the model to be unable to handle Chinese instructions, and further demonstrates the need to collect Chinese instruction finetuning data.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/PhoebusSi/alpaca-CoT/raw/main/figures/ablation-both.png&#34; alt=&#34;ablation-cot&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The above table shows a relatively difficult example, which requires both a certain accumulation of knowledge of Chinese history and a logical and complete ability to state historical events. As shown in this table, &lt;code&gt;Ours w/o CN&lt;/code&gt; can only generate a short and erroneous response, because due to the lack of Chinese finetuning data, the corresponding knowledge of Chinese history is naturally lacking. Although &lt;code&gt;Ours w/o CoT&lt;/code&gt; lists some relevant Chinese historical events, its logic of expression is self-contradictory, which is caused by the lack of CoT data. `&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;In summary, the models finetuned from our complete dataset (English, Chinese, and CoT instruction data) can significantly improve model reasoning and Chinese instruction following abilities.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;The Effect of CoT Data&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/PhoebusSi/alpaca-CoT/raw/main/figures/CoT-comparison.png&#34; alt=&#34;CoT-comparison&#34;&gt; Samples of each odd number of rows do not apply the CoT prompt, such as &#34;step-by-step reasoning.&#34; Both &lt;code&gt;Ours(w/CoT)&lt;/code&gt; and Alpaca are based on LLaMA-7B, and the only difference between them two is that the instruction-finetuning data of &lt;code&gt;Ours(w/CoT)&lt;/code&gt; has a extra CoT data than that of Alpaca.&lt;/p&gt; &#xA;&lt;p&gt;From the above table, we find that:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;Ours(w/CoT)&lt;/code&gt; always generates the correct rationale before the answer, while Alpaca fails to generate any reasonable rationale, as shown in the first 4 examples (commonsense questions). This shows that using CoT data for finetuning can significantly improve reasoning ability.&lt;/li&gt; &#xA; &lt;li&gt;For &lt;code&gt;Ours(w/CoT)&lt;/code&gt;, the CoT prompt (e.g., concatenate &#39;step-by-step&#39; with the input question) has little effect on easy examples (e.g., commonsense questions) and has an important effect on challenging questions (e.g., questions requiring reasoning, like the last four examples).&lt;/li&gt; &#xA; &lt;li&gt;For Alpaca, CoT prompt always has little effect or even negative impact. For the last two examples, after adding CoT prompt, Aplpaca changes the correct generated answer to the wrong one. This may be due to the inconsistency between the input forms of finetuning and inference.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;The Effect of Chinese Instruction Data&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;Quantitative comparison of responses to Chinese instructions.&lt;/em&gt; &lt;img src=&#34;https://github.com/PhoebusSi/alpaca-CoT/raw/main/figures/CN-compareCN.png&#34; alt=&#34;CN_compare_CN&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Our model is finetuned from a 7B LLaMA on 52K English instructions and 0.5M Chinese instructions. Stanford Alpaca (our reimplementation) is finetuned from a 7B LLaMA on 52K English instructions. BELLE is finetuned from a 7B BLOOM on 2B Chinese instructions.&lt;/p&gt; &#xA;&lt;p&gt;From the above table, several observations can be found:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Compared to Alpaca, &lt;code&gt;ours (w/ CN)&lt;/code&gt; has a stronger ability to understand Chinese instructions. For the first example, Alpaca fails to distinguish between the &lt;code&gt;instruction&lt;/code&gt; part and &lt;code&gt;input&lt;/code&gt; part, while we do.&lt;/li&gt; &#xA; &lt;li&gt;Chinese instruction finetuning data can significant enhance the ability to interact in Chinese. For the second example, &lt;code&gt;ours (w/ CN)&lt;/code&gt; not only provides the correct code, but also provides the corresponding Chinese annotation, while Alpaca does not. In addition, as shown in the 3-5 examples, Alpaca can only respond to Chinese instruction with an English response.&lt;/li&gt; &#xA; &lt;li&gt;Compared to BELLE, &lt;code&gt;ours (w/ CN)&lt;/code&gt;&#39;s performance on instructions requiring an open response (as shown in last two examples) still needs to be improved. BELLE&#39;s outstanding performance against such instructions is due to: 1. Its BLOOM backbone model encounters much more multilingual data during pre-training; 2. Its Chinese instruction finetuning data is more than ours, that is, 2M vs 0.5M.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;Quantitative comparison of responses to English instructions. The purpose of this subsection is to explore whether finetuning on Chinese instructions has a negative impact on Alpaca.&lt;/em&gt; &lt;img src=&#34;https://github.com/PhoebusSi/alpaca-CoT/raw/main/figures/CN_compareEN.png&#34; alt=&#34;CN_compare_EN&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;From the above table, we find that:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Finetuning with Chinese instruction data does not weaken the original English instruction–following ability, on the contrary, there is also a certain enhancement in genearting a better response to English intructions. The response of &lt;code&gt;ours (w/ CN)&lt;/code&gt; shows more detail than that of Alpaca, e.g. for the third example, &lt;code&gt;ours (w/ CN)&lt;/code&gt; list three more provinces than Alpaca.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Future Work&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Exploration of few-shot ability.&lt;/li&gt; &#xA; &lt;li&gt;Ablation study of various sizes of models.&lt;/li&gt; &#xA; &lt;li&gt;Evaluate on instruction-following evaluation suite.&lt;/li&gt; &#xA; &lt;li&gt;Collect more instruction finetuning datasets.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Please cite the repo if you use the data collection, code, and experimental findings in this repo.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{alpaca-cot,&#xA;  author = {Qingyi Si, Rui Liu, Zheng Lin },&#xA;  school = {Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China},&#xA;  title = {Evolving Alpaca: An Empirical Study on Instruction Tuning for Large Language Models},&#xA;  year = {2023},&#xA;  publisher = {GitHub},&#xA;  journal = {GitHub repository},&#xA;  howpublished = {\url{https://github.com/PhoebusSi/alpaca-CoT}},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For data, please cite the original Stanford Alpaca, BELLE and FLAN papers as well.&lt;/p&gt; &#xA;&lt;p&gt;For models, please cite the original LLaMA, Stanford Alpaca, Self-Instruct and LoRA papers as well.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>LC1332/CamelBell-Chinese-LoRA</title>
    <updated>2023-03-30T01:38:39Z</updated>
    <id>tag:github.com,2023-03-30:/LC1332/CamelBell-Chinese-LoRA</id>
    <link href="https://github.com/LC1332/CamelBell-Chinese-LoRA" rel="alternate"></link>
    <summary type="html">&lt;p&gt;CamelBell（驼铃) is be a Chinese Language Tuning project based on LoRA. CamelBell is belongs to Project Luotuo(骆驼), an open sourced Chinese-LLM project created by 冷子昂 @ 商汤科技 &amp; 陈启源 @ 华中师范大学 &amp; 李鲁鲁 @ 商汤科技&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;驼铃 CamelBell-Chinese-LoRA&lt;/h1&gt; &#xA;&lt;p&gt;CamelBell (驼铃) is be a Chinese Language Tuning project based on LoRA. CamelBell is belongs to Project Luotuo(&lt;a href=&#34;https://github.com/LC1332/Chinese-alpaca-lora&#34;&gt;骆驼&lt;/a&gt;), an open sourced Chinese-LLM project created by 冷子昂 @ 商汤科技 &amp;amp; 陈启源 @ 华中师范大学 &amp;amp; 李鲁鲁 @ 商汤科技&lt;/p&gt; &#xA;&lt;p&gt;CamelBell is NOT an official product of SenseTime&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;p&gt;[2023-3-30] We released Chinese Summarization Model, CamelBell-C (驼铃-C), try in this &lt;a href=&#34;https://colab.research.google.com/github/LC1332/Chinese-alpaca-lora/blob/main/notebook/TuoLingC_evaluation_code.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Instruction: 请帮我总结以下内容:&#xA;Input: &#xA;北京时间2月13日凌晨,2023年ATP250达拉斯站男单决赛。中国球员吴易昺先输一盘后挽救4个赛点并兑现第5个冠军点,最终以6(4)-7/7-6(3)/7-6(12)逆转惊险击败赛会5号种子、美国大炮伊斯内尔,就此改写历史,成为公开赛年代首位夺得ATP巡回赛男单冠军的中国大陆球员,并创造中国大陆球员的男单最高排名!&#xA;&#xA;第一盘比赛,吴易昺在第12局错过了一个盘点,并最终抢七惜败;第二盘则挽救一个赛点后抢七局3-0领先开局,且以7-6(3)扳回一盘;第三盘决胜盘,在关键的第9局15-40落后情况下凭借连续的高质量发球逆转保发,之后比赛再次进入抢七,抢七局依然胶着,吴易昺又挽救了3个赛点,并兑现了自己的第5个冠军点,就此锁定冠军!历史性一刻到来时,吴易昺瞬间躺倒在地。全场比赛,伊斯内尔轰出了44记Ace球,但最终在主场依然输给了吴易昺。&#xA;&#xA;凭借具有突破意义的这一冠,吴易昺在本周入账250个积分和112125美元的冠军奖金,在周一最新一期的男单排名榜单上,创中国大陆男网历史新高排名—第58位。根据比赛计划,吴易昺原本要出战本周进行的ATP250德拉海滩站,不过在达拉斯夺冠后,吴易昺因身体疲劳退出本站赛事,他的签位由幸运落败者约翰森替代。&#xA;&#xA;Answer: 男子网坛历史性一刻!中国小将吴易昺逆转击败赛会5号种子,成公开赛年代首个冠军。&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;[2023-3-27] We plan to train a &lt;a href=&#34;https://github.com/LC1332/CamelBell-Chinese-LoRA/raw/main/data/HarryPotter/ShortReport.md&#34;&gt;ChatHarryPotter&lt;/a&gt;, we&#39;ve just finished the prelimiary experiment and have ver. 0.1 model, but it did not meet our expectation, see this &lt;a href=&#34;https://github.com/LC1332/CamelBell-Chinese-LoRA/raw/main/data/HarryPotter/ShortReport.md&#34;&gt;report&lt;/a&gt;, and we are pursuing &lt;em&gt;a Harry Potter enthusiast Pythoner to join&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;[2023-3-24] We&#39;ve just released Evaluation code, tuning Chinese LLM with very few data on GLM-6B via LoRA, try &lt;a href=&#34;https://colab.research.google.com/github/LC1332/CamelBell-Chinese-LoRA/blob/main/notebook/CamelBell_evaluation_code.ipynb&#34;&gt;here&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/LC1332/CamelBell-Chinese-LoRA/blob/main/notebook/CamelBell_evaluation_code.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;CamelBell-A&lt;/h2&gt; &#xA;&lt;p&gt;驼铃-A是一个小型的实验，仅仅使用80条语料，1小时的训练时间（A100），将开发者的信息Encode到LoRA之中。初步验证了在中文基模型上使用超小语料finetune的可行性。 &lt;a href=&#34;https://colab.research.google.com/github/LC1332/CamelBell-Chinese-LoRA/blob/main/notebook/CamelBell_evaluation_code.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; .&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/5266090/227672185-3f4905ca-417b-4fe7-9dcf-30c5fef2e542.mov&#34;&gt;https://user-images.githubusercontent.com/5266090/227672185-3f4905ca-417b-4fe7-9dcf-30c5fef2e542.mov&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;CamelBell-B&lt;/h2&gt; &#xA;&lt;p&gt;We are building CamelBell-B, ChatHarryPotter, we now have a 0.1 version, check the &lt;a href=&#34;https://github.com/LC1332/CamelBell-Chinese-LoRA/raw/main/data/HarryPotter/ShortReport.md&#34;&gt;report&lt;/a&gt; here.&lt;/p&gt; &#xA;&lt;h2&gt;CamelBell-C&lt;/h2&gt; &#xA;&lt;p&gt;驼铃-C是一个中文摘要模型，我们在字节发布的CNewSum数据集上，抽样了210k条语句进行训练。 &lt;a href=&#34;https://colab.research.google.com/github/LC1332/Chinese-alpaca-lora/blob/main/notebook/TuoLingC_evaluation_code.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Instruction: 请帮我总结以下内容:&#xA;Input: 从招聘网站的搜索结果来看,目前游戏厂商对于AIGC人才的招聘,主要分为两个方向,一是更加针对性的AI美术方向,二则是相对面向AIGC生产流程构建底层的算法工程师。&#xA;&#xA;在AI画图技术证明了自己的产出能力之后,就早已有游戏公司迅速开始了布局。似乎从2022年底开始,就时常能够听到某家游戏大厂内部已经将AI美术产出流程跑通的消息,光是这个月就有恺英网络、完美世界等几家大厂宣布了入局AIGC技术应用的消息。&#xA;&#xA;在快速进行内部摸索的同时,有厂商也寻求外部人才协助,想要实现AI绘图创作通路的快速落地。倘若在招聘网站上搜索AI绘画,将范围限定在“游戏”方面,就已经能看到不少相关职业。其中包括巨人网络、成都IGG、厦门梦加游戏等游戏厂商都放出了相应的岗位需求,要求应聘者熟悉Midjoumey、Stable Diffusion这类AI美术工具,开出的价格也在10K到35K之间不等。&#xA;&#xA;广州的四三九九的AI绘图岗位除了对应聘者的AI艺术工具使用熟练度有要求之外,在岗位职责里,还加入了“帮助美术部门了解和使用AI及相关工具,协助研队提高对AI的使用度和效率”的需求,想要通过招聘专业的AIGC人才来培养内部的美术团队AIGC创作能力。&#xA;&#xA;另外寻求外部人才培养美术团队AIGC能力的还有腾讯,在某个远未来科幻欧卡SLG项目的AI原画招聘需求中,除了研究和生产AI美术素材之外,还需要应聘者充实公司AI词库,并定期对美术团队进行培训,提升团队AI美术工具的熟练&#xA;Answer: 游戏公司招聘AIGC人才,要求熟悉AI美术工具,开出价格从10K到35K不等。&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;Instruction: 请帮我总结以下内容:&#xA;&#xA;Input: 美国硅谷银行 (Silicon Valley Bank, SVB ;矽谷银行)和“标志银行”(Signature Bank,又译“签名银行”)在几天内相继倒闭后,美国当局采取了紧急措施来支撑银行系统。这是美国自2008年金融海啸以来最大的银行倒闭事件,恐引起金融市场骨牌效应。硅谷银行因资不抵债在48小时内倒闭后,3月10日由美国联邦存款保险公司(FDIC)接管。美国财政部、联储局及联邦存款保险公司12日晚间发表联合声明指,正采取果断行动,以增强公众对银行体系的信心,并向硅谷银行存户保证,可以在13日周一取回所有存款。&#xA;&#xA;3月12日,美国财政部以存在系统性风险为由关闭总部设在纽约州的“标志银行”。该银行是加密货币公司的主要融资来源之一。美国总统拜登发表讲话,承诺将“竭尽所能”保护银行系统。但投资者担心其他银行可能仍会受到影响,引发全球股价大幅下跌。事件震荡全球金融市场,加拿大、英国等多个市场都受牵连。法兰克福、巴黎和米兰的股市大幅下跌。周一(3月13日)早些时候,西班牙桑坦德银行(Santander)和德国商业银行(Commerzbank)的股价一度下跌超过10%。极地资本(Polar Capital)基金经理乔治·戈德伯(George Godber)表示,市场下跌是因为“担心那里可能还有其他事情发生”。他说:“迫在眉睫的危机可能已经避免,但它提醒人们注意这样一个事实,即有一群公司的商业模式将在高利率环境中苦苦挣扎。”但他表示,事件对英国经济和英国市场的直接影响有限。BBC中文梳理这次危机的来龙去脉,以及对全球的影响。&#xA;&#xA;Answer: 硅谷银行和“标志银行”倒闭,美国当局采取紧急措施支撑银行系统。&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;We have tuned a Chinese model based on &lt;a href=&#34;https://github.com/THUDM/ChatGLM-6B&#34;&gt;ChatGLM-6B&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The training code was baed on &lt;a href=&#34;https://github.com/mymusise/ChatGLM-Tuning&#34;&gt;ChatGLM-Tuning&lt;/a&gt;. However, the original code of ChatGLM-Tuning is still in building and not support Chinese Tuning. We modified part of training code.&lt;/p&gt; &#xA;&lt;p&gt;Our training code is in cleaning, if you are in hurry, check the ChatGLM-Tuning project and try to debug the part of Tokenizer and INT4/INT8 switcher stuff.&lt;/p&gt; &#xA;&lt;h2&gt;Data&lt;/h2&gt; &#xA;&lt;p&gt;This is an inbuilding project, we plan to finish at least 3 demo LoRA models in this project&lt;/p&gt; &#xA;&lt;p&gt;A. The model A will tuned on very few instruction (only around 80 questions, see in &lt;a href=&#34;https://raw.githubusercontent.com/LC1332/CamelBell-Chinese-LoRA/main/data/developer_instruction.json&#34;&gt;developer_instruction.json&lt;/a&gt;), the model has been released now&lt;/p&gt; &#xA;&lt;p&gt;B. The model B, we plan to do something interesting. 李鲁鲁 plan to write a script, selecting a character in a movie/ a book/ or history, query thousands of QA data from OpenAI api. and tuning GLM into a character chat bot.&lt;/p&gt; &#xA;&lt;p&gt;C. The model C, find some specific domain QA data.&lt;/p&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;p&gt;inbuilding project&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; release evaluation code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; release model A&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; write data scipt for model B&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; collecting data for model B&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; release model B&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; collecting data for model C&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; release model C&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; clean and release training code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; refactor GLM code into standard HuggingFace pipeline&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Sponsorships(赞助)&lt;/h2&gt; &#xA;&lt;p&gt;Detailed Sponsorship and Balance see in &lt;a href=&#34;https://github.com/LC1332/Chinese-alpaca-lora/raw/main/data/Sponsorship_and_balance.md&#34;&gt;Sponsorship_and_balance.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Top 3 Sponsors(爸爸) until 3/24, this table in sub-repo may delay than the &lt;a href=&#34;https://github.com/LC1332/Chinese-alpaca-lora&#34;&gt;major Luotuo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Time&lt;/th&gt; &#xA;   &lt;th&gt;Sponsor&lt;/th&gt; &#xA;   &lt;th&gt;Amount&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2023/3/25&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mobe1978&#34;&gt;肖**&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;520&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2023/3/24&lt;/td&gt; &#xA;   &lt;td&gt;*潇&lt;/td&gt; &#xA;   &lt;td&gt;518&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2023/3/24&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/pandodao/botastic&#34;&gt;yiplee&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;512&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/LC1332/Chinese-alpaca-lora&#34;&gt;骆驼&lt;/a&gt;原本是我们的一个作业项目，我们原本计划训练到1.0为止。但是社区的热情超过了我们的想象。如果您愿意赞助我们的项目，可以&lt;/p&gt; &#xA;&lt;p&gt;扫描这个&lt;a href=&#34;https://s1.imagehub.cc/images/2023/03/23/fba44d198f0bb887089b4d8739363c0b.jpeg&#34;&gt;二维码&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;并且加这个&lt;a href=&#34;https://s1.imagehub.cc/images/2023/03/23/b69e4e47759132dd3d4bbafa7bd602aa.jpeg&#34;&gt;支付宝&lt;/a&gt;账号，留下您的姓名&lt;/p&gt; &#xA;&lt;p&gt;项目的资金流向将被公开，所有的资金将被用于数据的标注，训练算力的购买或者后续周边产品的发放。数据和算力的捐献也会一同总结在sponsorship的表格中。备用链接 &lt;a href=&#34;https://github.com/LC1332/Chinese-alpaca-lora/raw/main/image/sponser_QR_code.jpeg&#34;&gt;二维码&lt;/a&gt; , &lt;a href=&#34;https://github.com/LC1332/Chinese-alpaca-lora/raw/main/image/alipay_friend.jpeg&#34;&gt;支付宝&lt;/a&gt;账号&lt;/p&gt; &#xA;&lt;p&gt;This was originally an exercise project for us, and we originally planned to train until version 1.0. However, the enthusiasm of the community exceeded our expectations. If you are willing to sponsor our project, you can scan this &lt;a href=&#34;https://github.com/LC1332/Chinese-alpaca-lora/raw/main/image/sponser_QR_code.jpeg&#34;&gt;QR code&lt;/a&gt; and add &lt;a href=&#34;https://github.com/LC1332/Chinese-alpaca-lora/raw/main/image/alipay_friend.jpeg&#34;&gt;this Alipay account&lt;/a&gt;, leaving your name.&lt;/p&gt; &#xA;&lt;p&gt;All funds will be used for data annotation, purchase of training computing power, or distribution of subsequent peripheral products.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Please cite the repo if you use the data or code in this repo.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{alpaca,&#xA;  author={Ziang Leng, Qiyuan Chen and Cheng Li},&#xA;  title = {Luotuo: An Instruction-following Chinese Language model, LoRA tuning on LLaMA},&#xA;  year = {2023},&#xA;  publisher = {GitHub},&#xA;  journal = {GitHub repository},&#xA;  howpublished = {\url{https://github.com/LC1332/Chinese-alpaca-lora}},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>alura-es-cursos/inmersion-datos</title>
    <updated>2023-03-30T01:38:39Z</updated>
    <id>tag:github.com,2023-03-30:/alura-es-cursos/inmersion-datos</id>
    <link href="https://github.com/alura-es-cursos/inmersion-datos" rel="alternate"></link>
    <summary type="html">&lt;p&gt;En este repositorio encontrarás el notebook que usaremos durante la Inmersión Datos con Python&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;inmersion-datos&lt;/h1&gt; &#xA;&lt;p&gt;En este repositorio encontrarás el notebook que usaremos durante la &lt;strong&gt;Inmersión Datos con Python&lt;/strong&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>