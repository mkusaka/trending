<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-27T01:37:03Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>drboog/ProFusion</title>
    <updated>2023-05-27T01:37:03Z</updated>
    <id>tag:github.com,2023-05-27:/drboog/ProFusion</id>
    <link href="https://github.com/drboog/ProFusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code for Enhancing Detail Preservation for Customized Text-to-Image Generation: A Regularization-Free Approach&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ProFusion&lt;/h1&gt; &#xA;&lt;br&gt; &#xA;&lt;div style=&#34;text-align: center;&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/drboog/ProFusion/main/imgs/examples.png&#34; alt=&#34;examples&#34; width=&#34;90%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;Code for &lt;a href=&#34;https://arxiv.org/abs/2305.13579&#34;&gt;Enhancing Detail Preservation for Customized Text-to-Image Generation: A Regularization-Free Approach&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;ProFusion is a framework for customizing pre-trained large-scale text-to-image generation models, which is &lt;a href=&#34;https://github.com/Stability-AI/stablediffusion&#34;&gt;Stable Diffusion 2&lt;/a&gt; in our examples. &lt;br&gt;&lt;/p&gt; &#xA;&lt;div style=&#34;text-align: center;&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/drboog/ProFusion/main/imgs/framework.jpg&#34; alt=&#34;framework&#34; width=&#34;90%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;With ProFusion, you can generate infinite number of creative images for a novel/unique concept, with single testing image, on single GPU (~20GB are needed when fine-tune with batch size 1).&lt;/p&gt; &#xA;&lt;div style=&#34;text-align: center;&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/drboog/ProFusion/main/imgs/daniel.jpg&#34; alt=&#34;framework&#34; width=&#34;80%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Example&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Install dependencies (we revised original &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;diffusers&lt;/a&gt;);&lt;/p&gt; &lt;pre&gt;&lt;code&gt;  cd ./diffusers&#xA;  pip install -e .&#xA;  cd ..&#xA;  pip install accelerate==0.16.0 torchvision transformers&amp;gt;=4.25.1 datasets ftfy tensorboard Jinja2 regex tqdm joblib &#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Initialize &lt;a href=&#34;https://github.com/huggingface/accelerate/&#34;&gt;Accelerate&lt;/a&gt;;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;  accelerate config&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Download a model &lt;a href=&#34;https://drive.google.com/file/d/1n6jZXpb2nE_ptftKjSr7JZ22TsCbZHCh/view?usp=share_link&#34;&gt;pre-trained on FFHQ&lt;/a&gt;;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Customize model with a testing image, example is shown in the notebook &lt;a href=&#34;https://raw.githubusercontent.com/drboog/ProFusion/main/test.ipynb&#34;&gt;test.ipynb&lt;/a&gt;;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Train Your Own Encoder&lt;/h2&gt; &#xA;&lt;p&gt;If you want to train a PromptNet encoder for other domains, or on your own dataset.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;First, prepare an image-only dataset;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;In our example, we use &lt;a href=&#34;https://github.com/NVlabs/ffhq-dataset&#34;&gt;FFHQ&lt;/a&gt;. Our pre-processed FFHQ can be found at &lt;a href=&#34;https://drive.google.com/file/d/1cObckM1omlMgG5x1z9sMGqbrgCKNkPhu/view?usp=share_link&#34;&gt;google drive link&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Then, run&lt;/p&gt; &lt;pre&gt;&lt;code&gt;  accelerate launch --mixed_precision=&#34;fp16&#34; train.py\&#xA;        --pretrained_model_name_or_path=&#34;stabilityai/stable-diffusion-2-base&#34; \&#xA;        --train_data_dir=./images_512 \&#xA;        --max_train_steps=80000 \&#xA;        --learning_rate=2e-05 \&#xA;        --output_dir=&#34;./promptnet&#34; \&#xA;        --train_batch_size=8 \&#xA;        --promptnet_l2_reg=0.000 \&#xA;        --gradient_checkpointing&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{zhou2023enhancing,&#xA;  title={Enhancing Detail Preservation for Customized Text-to-Image Generation: A Regularization-Free Approach}, &#xA;  author={Yufan Zhou and Ruiyi Zhang and Tong Sun and Jinhui Xu},&#xA;  year={2023},&#xA;  eprint={2305.13579},&#xA;  archivePrefix={arXiv},&#xA;  primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>caoyunkang/Segment-Any-Anomaly</title>
    <updated>2023-05-27T01:37:03Z</updated>
    <id>tag:github.com,2023-05-27:/caoyunkang/Segment-Any-Anomaly</id>
    <link href="https://github.com/caoyunkang/Segment-Any-Anomaly" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This project addresses zero-shot anomaly detection by combining SAM and Grouding DINO.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Segment Any Anomaly&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1Rwio_KfziuLp79Qh_ugum64Hjnq4ZwsE?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This project aims to segment any anomaly without any training. We develop this interesting demo by combining &lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO&#34;&gt;Grounding DINO&lt;/a&gt; and &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;Segment Anything&lt;/a&gt;! Most of the codes are borrowed from &lt;a href=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything&#34;&gt;Grounded Segment Anything&lt;/a&gt;. Thanks to their excellent work!&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Why this project?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;Segment Anything&lt;/a&gt; is a strong segmentation model. But it need prompts (like boxes/points) to generate masks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO&#34;&gt;Grounding DINO&lt;/a&gt; is a strong zero-shot detector which enable to generate high quality boxes and labels with free-form text.&lt;/li&gt; &#xA; &lt;li&gt;The combination of the two models enable to &lt;strong&gt;detect and segment everything&lt;/strong&gt; with text inputs!&lt;/li&gt; &#xA; &lt;li&gt;In real world industrial inspection applications, models trained with zero or few normal images, is essential in many cases as defects are rare with a wide range of variations.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;How we do?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We feed the origin image and anomaly specific description to Grouding DINO, and then filter the bouding boxes using several strategies. Then the filtered bounding boxes are denoted as the prompts in SAM for final anomaly segmentation. &lt;img src=&#34;https://raw.githubusercontent.com/caoyunkang/Segment-Any-Anomaly/master/assets/framework.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Imagine Space&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Some possible avenues for future work ...&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Stronger foundation models with segmentation pre-training.&lt;/li&gt; &#xA; &lt;li&gt;Collaboration with (Chat-)GPT.&lt;/li&gt; &#xA; &lt;li&gt;More advanced normality- and abnormality-specific prompts for better zero-shot anomaly detection performance.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Examples on the MVTec AD dataset&lt;/strong&gt; &lt;img src=&#34;https://raw.githubusercontent.com/caoyunkang/Segment-Any-Anomaly/master/assets/demo_results.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🔥 What&#39;s New&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;🆕 We have added a &lt;a href=&#34;https://colab.research.google.com/drive/1Rwio_KfziuLp79Qh_ugum64Hjnq4ZwsE?usp=sharing&#34;&gt;colab demo&lt;/a&gt;, enjoy it~&lt;/li&gt; &#xA; &lt;li&gt;🆕 We have added a &lt;a href=&#34;https://raw.githubusercontent.com/caoyunkang/Segment-Any-Anomaly/master/gradio_zero_shot_ad_app.py&#34;&gt;gradio app&lt;/a&gt;, enjoy it~&lt;/li&gt; &#xA; &lt;li&gt;🆕 Show the way of using anomaly specific prompts to detect anomalies more precise. For example, using text_prompt like &#34;the black hole on the cable&#34;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🛠 Installation&lt;/h2&gt; &#xA;&lt;p&gt;The code requires &lt;code&gt;python&amp;gt;=3.8&lt;/code&gt;, as well as &lt;code&gt;pytorch&amp;gt;=1.7&lt;/code&gt; and &lt;code&gt;torchvision&amp;gt;=0.8&lt;/code&gt;. Please follow the instructions &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;here&lt;/a&gt; to install both PyTorch and TorchVision dependencies. Installing both PyTorch and TorchVision with CUDA(&amp;gt;=11.1) support is strongly recommended.&lt;/p&gt; &#xA;&lt;p&gt;Install Segment Anything:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m pip install -e segment_anything&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install GroundingDINO:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m pip install -e GroundingDINO&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The following optional dependencies are necessary for mask post-processing, saving masks in COCO format, the example notebooks, and exporting the model in ONNX format. &lt;code&gt;jupyter&lt;/code&gt; is also required to run the example notebooks.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install opencv-python pycocotools matplotlib onnxruntime onnx ipykernel&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;More details can be found in &lt;a href=&#34;https://github.com/facebookresearch/segment-anything#installation&#34;&gt;install segment anything&lt;/a&gt; and &lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO#install&#34;&gt;install GroundingDINO&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Or you can simply use our script one-click setup environment and download the Model!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash install.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🏃 Run Grounded-Segment-Anything Demo&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download the checkpoint for segment-anything and grounding-dino:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd $ProjectRoot&#xA;mkdir weights&#xA;cd ./weights&#xA;wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth&#xA;wget https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Run demo&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CUDA_VISIBLE_DEVICES=0&#xA;python zero_shot_ad_demo.py \&#xA;  --config GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py \&#xA;  --grounded_checkpoint weights/groundingdino_swint_ogc.pth \&#xA;  --category &#34;cable&#34; \&#xA;  --input_image assets/cable_demo.png \&#xA;  --output_dir &#34;outputs&#34; \&#xA;  --box_threshold 0.20 \&#xA;  --text_threshold 0.20 \&#xA;  --area_threshold 0.90 \&#xA;  --text_prompt &#34;the black hole on the cable&#34; \&#xA;  --device &#34;cuda&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The model prediction visualization will be saved in &lt;code&gt;output_dir&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🏃 Run Grounded-Segment-Anything + Gradio APP&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python gradio_zero_shot_ad_app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The gradio app visulization as follow: &lt;img src=&#34;https://raw.githubusercontent.com/caoyunkang/Segment-Any-Anomaly/master/assets/gradio.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;span&gt;🔨&lt;/span&gt;Todolist&lt;/h2&gt; &#xA;&lt;p&gt;We will add following features in the near future...&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Detail the zero-shot anomaly detection framework.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Evaluate on other image anomaly detection datasets.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add video demo.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add UI for easy evaluation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Colab demo.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; HuggingFace demo.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;💘 Acknowledgements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;segment-anything&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO&#34;&gt;GroundingDINO&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything&#34;&gt;Grounded Segment Anything&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🎆Related Work&lt;/h2&gt; &#xA;&lt;p&gt;If you feel good about our work, there are some work you might be interested in：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/JerryX1110/awesome-segment-anything-extensions&#34;&gt;Awesome-segment-anything-extensions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/caoyunkang/IKD&#34;&gt;IKD, image anomaly detection&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/caoyunkang/CDO&#34;&gt;CDO, image anomaly detection&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/smiler96/PFM-and-PEFM-for-Image-Anomaly-Detection-and-Segmentation&#34;&gt;PFM, image anomaly detection&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/smiler96/GCPF&#34;&gt;GCPF, image anomaly detection&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/caoyunkang/CPMF&#34;&gt;CPMF, point cloud anomaly detection&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/M-3LAB/awesome-industrial-anomaly-detection&#34;&gt;awesome-industrial-anomaly-detection, survey of anomaly detection&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this project helpful for your research, please consider citing the following BibTeX entry.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-BibTex&#34;&gt;@article{kirillov2023segany,&#xA;  title={Segment Anything}, &#xA;  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Doll{\&#39;a}r, Piotr and Girshick, Ross},&#xA;  journal={arXiv:2304.02643},&#xA;  year={2023}&#xA;}&#xA;&#xA;@inproceedings{ShilongLiu2023GroundingDM,&#xA;  title={Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection},&#xA;  author={Shilong Liu and Zhaoyang Zeng and Tianhe Ren and Feng Li and Hao Zhang and Jie Yang and Chunyuan Li and Jianwei Yang and Hang Su and Jun Zhu and Lei Zhang},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>artidoro/qlora</title>
    <updated>2023-05-27T01:37:03Z</updated>
    <id>tag:github.com,2023-05-27:/artidoro/qlora</id>
    <link href="https://github.com/artidoro/qlora" rel="alternate"></link>
    <summary type="html">&lt;p&gt;QLoRA: Efficient Finetuning of Quantized LLMs&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;QLoRA: Efficient Finetuning of Quantized LLMs&lt;/h1&gt; &#xA;&lt;p&gt;| &lt;a href=&#34;https://arxiv.org/abs/2305.14314&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/timdettmers&#34;&gt;Adapter Weights&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/spaces/uwnlp/guanaco-playground-tgi&#34;&gt;Demo&lt;/a&gt; |&lt;/p&gt; &#xA;&lt;p&gt;This repo supports the paper &#34;QLoRA: Efficient Finetuning of Quantized LLMs&#34;, an effort to democratize access to LLM research.&lt;/p&gt; &#xA;&lt;p&gt;QLoRA uses &lt;a href=&#34;https://github.com/TimDettmers/bitsandbytes&#34;&gt;bitsandbytes&lt;/a&gt; for quantization and is integrated with Huggingface&#39;s &lt;a href=&#34;https://github.com/huggingface/peft&#34;&gt;PEFT&lt;/a&gt; and &lt;a href=&#34;https://github.com/huggingface/transformers/&#34;&gt;transformers&lt;/a&gt; libraries. QLoRA was developed by members of the &lt;a href=&#34;https://twitter.com/uwnlp?s=20&#34;&gt;University of Washington&#39;s UW NLP group&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) Double Quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) Paged Optimizers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. We release all of our models and code, including CUDA kernels for 4-bit training.&lt;/p&gt; &#xA;&lt;h2&gt;License and Intended Use&lt;/h2&gt; &#xA;&lt;p&gt;We release the resources associated with QLoRA finetuning in this repository under MIT license. In addition, we release the Guanaco model family for base LLaMA model sizes of 7B, 13B, 33B, and 65B. These models are intended for purposes in line with the LLaMA license and require access to the LLaMA models.&lt;/p&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;Guanaco is a system purely intended for research purposes and could produce problematic outputs.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Access the &lt;a href=&#34;https://huggingface.co/spaces/uwnlp/guanaco-playground-tgi&#34;&gt;live demo here&lt;/a&gt;. Note this is the 33B model, the 65B model demo will come later.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Or host your own Guanaco gradio demo directly in Colab with &lt;a href=&#34;https://colab.research.google.com/drive/17XEqL1JcmVWjHkT-WczdYkJlNINacwG7?usp=sharing&#34;&gt;this notebook&lt;/a&gt;. Works with free GPUs for 7B and 13B models.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Alternatively, can you distinguish ChatGPT from Guanaco? Give it a try! You can access &lt;a href=&#34;https://colab.research.google.com/drive/1kK6xasHiav9nhiRUJjPMZb4fAED4qRHb?usp=sharing&#34;&gt;the model response Colab here&lt;/a&gt; comparing ChatGPT and Guanaco 65B on Vicuna prompts.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;To load models in 4bits with transformers and bitsandbytes, you have to install accelerate and transformers from source and make sure you have the latest version of the bitsandbytes library (0.39.0). You can achieve the above with the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -q -U bitsandbytes&#xA;pip install -q -U git+https://github.com/huggingface/transformers.git&#xA;pip install -q -U git+https://github.com/huggingface/peft.git&#xA;pip install -q -U git+https://github.com/huggingface/accelerate.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;qlora.py&lt;/code&gt; code is a starting point for finetuning and inference on various datasets. Basic command for finetuning a baseline model on the Alpaca dataset:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python qlora.py --model_name_or_path &amp;lt;path_or_name&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For models larger than 13B, we recommend adjusting the learning rate:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python qlora.py –learning_rate 0.0001 --model_name_or_path &amp;lt;path_or_name&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Quantization&lt;/h2&gt; &#xA;&lt;p&gt;Quantization parameters are controlled from the &lt;code&gt;BitsandbytesConfig&lt;/code&gt; (&lt;a href=&#34;https://huggingface.co/docs/transformers/main_classes/quantization#transformers.BitsAndBytesConfig&#34;&gt;see HF documenation&lt;/a&gt;) as follows:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Loading in 4 bits is activated through &lt;code&gt;load_in_4bit&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;The datatype used for the linear layer computations with &lt;code&gt;bnb_4bit_compute_dtype&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Nested quantization is activated through &lt;code&gt;bnb_4bit_use_double_quant&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;The datatype used for qunatization is specified with &lt;code&gt;bnb_4bit_quant_type&lt;/code&gt;. Note that there are two supported quantization datatypes &lt;code&gt;fp4&lt;/code&gt; (four bit float) and &lt;code&gt;nf4&lt;/code&gt; (normal four bit float). The latter is theoretically optimal for normally distributed weights and we recommend using &lt;code&gt;nf4&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;    model = AutoModelForCausalLM.from_pretrained(&#xA;        model_name_or_path=&#39;/name/or/path/to/your/model&#39;,&#xA;        load_in_4bit=True,&#xA;        device_map=&#39;auto&#39;,&#xA;        max_memory=max_memory,&#xA;        torch_dtype=torch.bfloat16,&#xA;        quantization_config=BitsAndBytesConfig(&#xA;            load_in_4bit=True,&#xA;            bnb_4bit_compute_dtype=torch.bfloat16,&#xA;            bnb_4bit_use_double_quant=True,&#xA;            bnb_4bit_quant_type=&#39;nf4&#39;&#xA;        ),&#xA;    )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Paged Optimizer&lt;/h2&gt; &#xA;&lt;p&gt;You can access the paged optimizer with the argument &lt;code&gt;--optim paged_adamw_32bit&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Tutorials and Demonstrations&lt;/h2&gt; &#xA;&lt;p&gt;Here is &lt;a href=&#34;https://huggingface.co/blog/4bit-transformers-bitsandbytes&#34;&gt;a blog&lt;/a&gt; discussing 4-bit quantization, QLoRA, and how they are integrated in transformers.&lt;/p&gt; &#xA;&lt;p&gt;You can host your own gradio Guanaco demo directly in Colab following &lt;a href=&#34;https://colab.research.google.com/drive/17XEqL1JcmVWjHkT-WczdYkJlNINacwG7?usp=sharing&#34;&gt;this notebook&lt;/a&gt;. In addition, here are Colab notebooks with examples for inference and finetuning using QLoRA:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1VoYNfYDKcKRQRor98Zbf2-9VQTtGJ24k?usp=sharing&#34;&gt;Inference notebook&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1VoYNfYDKcKRQRor98Zbf2-9VQTtGJ24k?usp=sharing&#34;&gt;Finetuning notebook&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Other examples are found under the &lt;code&gt;examples/&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;h2&gt;Sample Outputs&lt;/h2&gt; &#xA;&lt;p&gt;We provide generations for the models described in the paper for both OA and Vicuna queries in the &lt;code&gt;eval/generations&lt;/code&gt; folder. These are intended to foster further research on model evaluation and analysis.&lt;/p&gt; &#xA;&lt;p&gt;Can you distinguish ChatGPT from Guanaco? Give it a try! You can access &lt;a href=&#34;https://colab.research.google.com/drive/1kK6xasHiav9nhiRUJjPMZb4fAED4qRHb?usp=sharing&#34;&gt;the model response Colab here&lt;/a&gt; comparing ChatGPT and Guanaco 65B on Vicuna prompts.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;We include scripts adapted from the FastChat repo to automatically evaluate model generations using GPT-4. We include script for comparisons relative to ChatGPT with scores out of 10 as well as &#34;pairwise comparisons&#34; with three class labeling (win, loose, or tie). These are found in the &lt;code&gt;eval&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;p&gt;To facilitate the replication of our evaluation and future work in this area, we release GPT-4 and human ratings of our systems. These are found under &lt;code&gt;eval/ratings-human&lt;/code&gt; and &lt;code&gt;eval/ratings-gpt4&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;More details can be found at &lt;code&gt;eval/EVAL_README.md&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Known Issues and Limitations&lt;/h2&gt; &#xA;&lt;p&gt;Here a list of known issues and bugs. If your issue is not reported here, please open a new issue and describe the problem.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;4-bit inference is slow. Currently, our 4-bit inference implementation is not yet integrated with the 4-bit matrix multiplication&lt;/li&gt; &#xA; &lt;li&gt;Resuming a LoRA training run with the Trainer currently runs on an error&lt;/li&gt; &#xA; &lt;li&gt;Currently, using &lt;code&gt;bnb_4bit_compute_type=&#39;fp16&#39;&lt;/code&gt; can lead to instabilities. For 7B LLaMA, only 80% of finetuning runs complete without error. We have solutions, but they are not integrated yet into bitsandbytes.&lt;/li&gt; &#xA; &lt;li&gt;Make sure that &lt;code&gt;tokenizer.bos_token_id = 1&lt;/code&gt; to avoid generation issues.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{dettmers2023qlora,&#xA;  title={QLoRA: Efficient Finetuning of Quantized LLMs},&#xA;  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},&#xA;  journal={arXiv preprint arXiv:2305.14314},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;We thank the Huggingface team, in particular Younes Belkada, for their support integrating QLoRA with PEFT and transformers libraries. We also thank Meta for releasing the LLaMA models without which this work would not have been possible.&lt;/p&gt; &#xA;&lt;p&gt;This repo builds on the &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca&lt;/a&gt; and &lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;LMSYS FastChat&lt;/a&gt; repos.&lt;/p&gt;</summary>
  </entry>
</feed>