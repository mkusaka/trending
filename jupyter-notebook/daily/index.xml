<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-02-26T01:32:04Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>langchain-ai/open_deep_research</title>
    <updated>2025-02-26T01:32:04Z</updated>
    <id>tag:github.com,2025-02-26:/langchain-ai/open_deep_research</id>
    <link href="https://github.com/langchain-ai/open_deep_research" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Open Deep Research&lt;/h1&gt; &#xA;&lt;p&gt;Open Deep Research is a web research assistant that generates comprehensive reports on any topic following a workflow similar to &lt;a href=&#34;https://openai.com/index/introducing-deep-research/&#34;&gt;OpenAI&lt;/a&gt; and &lt;a href=&#34;https://blog.google/products/gemini/google-gemini-deep-research/&#34;&gt;Gemini&lt;/a&gt; Deep Research. However, it allows you to customize the models, prompts, report structure, search API, and research depth. Specifically, you can customize:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;provide an outline with a desired report structure&lt;/li&gt; &#xA; &lt;li&gt;set the planner model (e.g., DeepSeek, OpenAI reasoning model, etc)&lt;/li&gt; &#xA; &lt;li&gt;give feedback on the plan of report sections and iterate until user approval&lt;/li&gt; &#xA; &lt;li&gt;set the search API (e.g., Tavily, Perplexity) and # of searches to run for each research iteration&lt;/li&gt; &#xA; &lt;li&gt;set the depth of search for each section (# of iterations of writing, reflection, search, re-write)&lt;/li&gt; &#xA; &lt;li&gt;customize the writer model (e.g., Anthropic)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/6595d5cd-c981-43ec-8e8b-209e4fefc596&#34; alt=&#34;report-generation&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ðŸš€ Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;Ensure you have API keys set for your desired tools.&lt;/p&gt; &#xA;&lt;p&gt;Select a web search tool (by default Open Deep Research uses Tavily):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tavily.com/&#34;&gt;Tavily API&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.perplexity.ai/hub/blog/introducing-the-sonar-pro-api&#34;&gt;Perplexity API&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Select a writer model (by default Open Deep Research uses Anthropic Claude 3.5 Sonnet):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.anthropic.com/&#34;&gt;Anthropic&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openai.com/&#34;&gt;OpenAI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://groq.com/&#34;&gt;Groq&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Select a planner model (by default Open Deep Research uses OpenAI o3-mini):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.anthropic.com/&#34;&gt;Anthropic&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openai.com/&#34;&gt;OpenAI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://groq.com/&#34;&gt;Groq&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Using the package&lt;/h3&gt; &#xA;&lt;p&gt;(Recommended: Create a virtual environment):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m venv open_deep_research&#xA;source open_deep_research/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install open-deep-research&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/langchain-ai/open_deep_research/main/src/open_deep_research/graph.ipynb&#34;&gt;src/open_deep_research/graph.ipynb&lt;/a&gt; for an example of usage in a Jupyter notebook.&lt;/p&gt; &#xA;&lt;p&gt;Import and compile the graph:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from langgraph.checkpoint.memory import MemorySaver&#xA;from open_deep_research.graph import builder&#xA;memory = MemorySaver()&#xA;graph = builder.compile(checkpointer=memory)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;View the graph:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from IPython.display import Image, display&#xA;display(Image(graph.get_graph(xray=1).draw_mermaid_png()))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run the graph with a desired topic and configuration:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import uuid &#xA;thread = {&#34;configurable&#34;: {&#34;thread_id&#34;: str(uuid.uuid4()),&#xA;                           &#34;search_api&#34;: &#34;tavily&#34;,&#xA;                           &#34;planner_provider&#34;: &#34;openai&#34;,&#xA;                           &#34;planner_model&#34;: &#34;o3-mini&#34;,&#xA;                           &#34;writer_provider&#34;: &#34;anthropic&#34;,&#xA;                           &#34;writer_model&#34;: &#34;claude-3-5-sonnet-latest&#34;,&#xA;                           &#34;max_search_depth&#34;: 1,&#xA;                           }}&#xA;&#xA;topic = &#34;Overview of the AI inference market with focus on Fireworks, Together.ai, Groq&#34;&#xA;async for event in graph.astream({&#34;topic&#34;:topic,}, thread, stream_mode=&#34;updates&#34;):&#xA;    print(event)&#xA;    print(&#34;\n&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The graph will stop when the report plan is generated, and you can pass feedback to update the report plan:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from langgraph.types import Command&#xA;async for event in graph.astream(Command(resume=&#34;Include a revenue estimate (ARR) in the sections&#34;), thread, stream_mode=&#34;updates&#34;):&#xA;    print(event)&#xA;    print(&#34;\n&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When you are satisfied with the report plan, you can pass &lt;code&gt;True&lt;/code&gt; to proceed to report generation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Pass True to approve the report plan and proceed to report generation&#xA;async for event in graph.astream(Command(resume=True), thread, stream_mode=&#34;updates&#34;):&#xA;    print(event)&#xA;    print(&#34;\n&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running LangGraph Studio UI locally&lt;/h3&gt; &#xA;&lt;p&gt;Clone the repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/langchain-ai/open_deep_research.git&#xA;cd open_deep_research&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Edit the &lt;code&gt;.env&lt;/code&gt; file with your API keys (e.g., the API keys for default selections are shown below):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cp .env.example .env&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Set:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export TAVILY_API_KEY=&amp;lt;your_tavily_api_key&amp;gt;&#xA;export ANTHROPIC_API_KEY=&amp;lt;your_anthropic_api_key&amp;gt;&#xA;export OPENAI_API_KEY=&amp;lt;your_openai_api_key&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Launch the assistant with the LangGraph server locally, which will open in your browser:&lt;/p&gt; &#xA;&lt;h4&gt;Mac&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Install uv package manager&#xA;curl -LsSf https://astral.sh/uv/install.sh | sh&#xA;&#xA;# Install dependencies and start the LangGraph server&#xA;uvx --refresh --from &#34;langgraph-cli[inmem]&#34; --with-editable . --python 3.11 langgraph dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Windows&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;# Install dependencies &#xA;pip install -e .&#xA;pip install langgraph-cli[inmem]&#xA;&#xA;# Start the LangGraph server&#xA;langgraph dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Use this to open the Studio UI:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;- ðŸš€ API: http://127.0.0.1:2024&#xA;- ðŸŽ¨ Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024&#xA;- ðŸ“š API Docs: http://127.0.0.1:2024/docs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(1) Provide a &lt;code&gt;Topic&lt;/code&gt; and hit &lt;code&gt;Submit&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;img width=&#34;1326&#34; alt=&#34;input&#34; src=&#34;https://github.com/user-attachments/assets/de264b1b-8ea5-4090-8e72-e1ef1230262f&#34;&gt; &#xA;&lt;p&gt;(2) This will generate a report plan and present it to the user for review.&lt;/p&gt; &#xA;&lt;p&gt;(3) We can pass a string (&lt;code&gt;&#34;...&#34;&lt;/code&gt;) with feedback to regenerate the plan based on the feedback.&lt;/p&gt; &#xA;&lt;img width=&#34;1326&#34; alt=&#34;feedback&#34; src=&#34;https://github.com/user-attachments/assets/c308e888-4642-4c74-bc78-76576a2da919&#34;&gt; &#xA;&lt;p&gt;(4) Or, we can just pass &lt;code&gt;true&lt;/code&gt; to accept the plan.&lt;/p&gt; &#xA;&lt;img width=&#34;1480&#34; alt=&#34;accept&#34; src=&#34;https://github.com/user-attachments/assets/ddeeb33b-fdce-494f-af8b-bd2acc1cef06&#34;&gt; &#xA;&lt;p&gt;(5) Once accepted, the report sections will be generated.&lt;/p&gt; &#xA;&lt;img width=&#34;1326&#34; alt=&#34;report_gen&#34; src=&#34;https://github.com/user-attachments/assets/74ff01cc-e7ed-47b8-bd0c-4ef615253c46&#34;&gt; &#xA;&lt;p&gt;The report is produced as markdown.&lt;/p&gt; &#xA;&lt;img width=&#34;1326&#34; alt=&#34;report&#34; src=&#34;https://github.com/user-attachments/assets/92d9f7b7-3aea-4025-be99-7fb0d4b47289&#34;&gt; &#xA;&lt;h2&gt;ðŸ“– Customizing the report&lt;/h2&gt; &#xA;&lt;p&gt;You can customize the research assistant&#39;s behavior through several parameters:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;report_structure&lt;/code&gt;: Define a custom structure for your report (defaults to a standard research report format)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;number_of_queries&lt;/code&gt;: Number of search queries to generate per section (default: 2)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;max_search_depth&lt;/code&gt;: Maximum number of reflection and search iterations (default: 2)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;planner_provider&lt;/code&gt;: Model provider for planning phase (default: &#34;openai&#34;, but can be &#34;groq&#34;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;planner_model&lt;/code&gt;: Specific model for planning (default: &#34;o3-mini&#34;, but can be any Groq hosted model such as &#34;deepseek-r1-distill-llama-70b&#34;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;writer_model&lt;/code&gt;: Model for writing the report (default: &#34;claude-3-5-sonnet-latest&#34;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;search_api&lt;/code&gt;: API to use for web searches (default: Tavily)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;These configurations allow you to fine-tune the research process based on your needs, from adjusting the depth of research to selecting specific AI models for different phases of report generation.&lt;/p&gt; &#xA;&lt;h3&gt;Model Considerations&lt;/h3&gt; &#xA;&lt;p&gt;(1) With Groq, there are token per minute (TPM) limits if you are on the &lt;code&gt;on_demand&lt;/code&gt; service tier:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;code&gt;on_demand&lt;/code&gt; service tier has a limit of &lt;code&gt;6000 TPM&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;You will want a &lt;a href=&#34;https://github.com/cline/cline/issues/47#issuecomment-2640992272&#34;&gt;paid plan&lt;/a&gt; for section writing with Groq models&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;(2) &lt;code&gt;deepseek&lt;/code&gt; &lt;a href=&#34;https://api-docs.deepseek.com/guides/reasoning_model&#34;&gt;isn&#39;t great at function calling&lt;/a&gt;. Our assistant uses function calling to generate structured outputs for report sections and search queries within each section.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Because, section writing performs a larger number of function calls, OpenAI, Anthropic, and certain OSS models that are stromng at function calling like Groq&#39;s &lt;code&gt;llama-3.3-70b-versatile&lt;/code&gt; are advised.&lt;/li&gt; &#xA; &lt;li&gt;If you see the following error, it is likely due to the model not being able to produce structured outputs (see &lt;a href=&#34;https://smith.langchain.com/public/8a6da065-3b8b-4a92-8df7-5468da336cbe/r&#34;&gt;trace&lt;/a&gt;):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;groq.APIError: Failed to call a function. Please adjust your prompt. See &#39;failed_generation&#39; for more details.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;How it works&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Plan and Execute&lt;/code&gt; - Open Deep Research follows a &lt;a href=&#34;https://github.com/assafelovic/gpt-researcher&#34;&gt;plan-and-execute workflow&lt;/a&gt; that separates planning from research, allowing for human-in-the-loop approval of a report plan before the more time-consuming research phase. It uses, by default, a &lt;a href=&#34;https://www.youtube.com/watch?v=f0RbwrBcFmc&#34;&gt;reasoning model&lt;/a&gt; to plan the report sections. During this phase, it uses web search to gather general information about the report topic to help in planning the report sections. But, it also accepts a report structure from the user to help guide the report sections as well as human feedback on the report plan.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Research and Write&lt;/code&gt; - Each section of the report is written in parallel. The research assistant uses web search via &lt;a href=&#34;https://tavily.com/&#34;&gt;Tavily API&lt;/a&gt; or &lt;a href=&#34;https://www.perplexity.ai/hub/blog/introducing-the-sonar-pro-api&#34;&gt;Perplexity&lt;/a&gt; to gather information about each section topic. It will reflect on each report section and suggest follow-up questions for web search. This &#34;depth&#34; of research will proceed for any many iterations as the user wants. Any final sections, such as introductions and conclusions, are written after the main body of the report is written, which helps ensure that the report is cohesive and coherent. The planner determines main body versus final sections during the planning phase.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Managing different types&lt;/code&gt; - Open Deep Research is built on LangGraph, which has native support for configuration management &lt;a href=&#34;https://langchain-ai.github.io/langgraph/concepts/assistants/&#34;&gt;using assistants&lt;/a&gt;. The report &lt;code&gt;structure&lt;/code&gt; is a field in the graph configuration, which allows users to create different assistants for different types of reports.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;UX&lt;/h2&gt; &#xA;&lt;h3&gt;Local deployment&lt;/h3&gt; &#xA;&lt;p&gt;Follow the &lt;a href=&#34;https://raw.githubusercontent.com/langchain-ai/open_deep_research/main/#quickstart&#34;&gt;quickstart&lt;/a&gt; to start LangGraph server locally.&lt;/p&gt; &#xA;&lt;h3&gt;Hosted deployment&lt;/h3&gt; &#xA;&lt;p&gt;You can easily deploy to &lt;a href=&#34;https://langchain-ai.github.io/langgraph/concepts/#deployment-options&#34;&gt;LangGraph Platform &lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>