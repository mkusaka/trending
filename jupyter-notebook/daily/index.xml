<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-22T01:32:45Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>cgpotts/cs224u</title>
    <updated>2023-08-22T01:32:45Z</updated>
    <id>tag:github.com,2023-08-22:/cgpotts/cs224u</id>
    <link href="https://github.com/cgpotts/cs224u" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code for Stanford CS224u&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CS224u: Natural Language Understanding&lt;/h1&gt; &#xA;&lt;p&gt;Code for &lt;a href=&#34;http://web.stanford.edu/class/cs224u/&#34;&gt;the Stanford course&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Spring 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://web.stanford.edu/~cgpotts/&#34;&gt;Christopher Potts&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Core components&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;code&gt;setup.ipynb&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Details on how to get set up to work with this code.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;hw_*.ipynb&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;The set of homeworks for the current run of the course.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;tutorial_*&lt;/code&gt; notebooks&lt;/h3&gt; &#xA;&lt;p&gt;Introductions to Juypter notebooks, scientific computing with NumPy and friends, and PyTorch.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;torch_*.py&lt;/code&gt; modules&lt;/h3&gt; &#xA;&lt;p&gt;A generic optimization class (&lt;code&gt;torch_model_base.py&lt;/code&gt;) and subclasses for GloVe, Autoencoders, shallow neural classifiers, RNN classifiers, tree-structured networks, and grounded natural language generation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;tutorial_pytorch_models.ipynb&lt;/code&gt; shows how to use these modules as a general framework for creating original systems.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;evaluation_*.ipynb&lt;/code&gt; and &lt;code&gt;projects.md&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Notebooks covering key experimental methods and practical considerations, and tips on writing up and presenting work in the field.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;iit*&lt;/code&gt; and &lt;code&gt;feature_attribution.ipynb&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Part of our unit on explainability and model analysis.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;np_*.py&lt;/code&gt; modules&lt;/h3&gt; &#xA;&lt;p&gt;This is now considered background material for the course.&lt;/p&gt; &#xA;&lt;p&gt;Reference implementations for the &lt;code&gt;torch_*.py&lt;/code&gt; models, designed to reveal more about how the optimization process works.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;vsm_*&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;This is now considered background material for the course.&lt;/p&gt; &#xA;&lt;p&gt;A unit on vector space models of meaning, covering traditional methods like PMI and LSA as well as newer methods like Autoencoders and GloVe. &lt;code&gt;vsm.py&lt;/code&gt; provides a lot of the core functionality, and &lt;code&gt;torch_glove.py&lt;/code&gt; and &lt;code&gt;torch_autoencoder.py&lt;/code&gt; are the learned models that we cover. &lt;code&gt;vsm_03_contextualreps.ipynb&lt;/code&gt; explores methods for deriving static representations from contextual models.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;sst_*&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;This is now considered background material for the course.&lt;/p&gt; &#xA;&lt;p&gt;A unit on sentiment analysis with the &lt;a href=&#34;https://nlp.stanford.edu/sentiment/treebank.html&#34;&gt;English Stanford Sentiment Treebank&lt;/a&gt;. The core code is &lt;code&gt;sst.py&lt;/code&gt;, which includes a flexible experimental framework. All the PyTorch classifiers are put to use as well: &lt;code&gt;torch_shallow_neural_network.py&lt;/code&gt;, &lt;code&gt;torch_rnn_classifier.py&lt;/code&gt;, and &lt;code&gt;torch_tree_nn.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;finetuning.ipynb&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;This is now considered background material for the course.&lt;/p&gt; &#xA;&lt;p&gt;Using pretrained parameters from &lt;a href=&#34;https://huggingface.co&#34;&gt;Hugging Face&lt;/a&gt; for featurization and fine-tuning.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;utils.py&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Miscellaneous core functions used throughout the code.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;test/&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;To run these tests, use&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;py.test -vv test/*&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;or, for just the tests in &lt;code&gt;test_shallow_neural_classifiers.py&lt;/code&gt;,&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;py.test -vv test/test_shallow_neural_classifiers.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;If the above commands don&#39;t work, try&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;python3 -m pytest -vv test/test_shallow_neural_classifiers.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The materials in this repo are licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/cgpotts/cs224u/main/LICENSE&#34;&gt;Apache 2.0 license&lt;/a&gt; and a &lt;a href=&#34;http://creativecommons.org/licenses/by-sa/4.0/&#34;&gt;Creative Commons Attribution-ShareAlike 4.0 International license&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Algomancer/Bayesian-Flow-Networks</title>
    <updated>2023-08-22T01:32:45Z</updated>
    <id>tag:github.com,2023-08-22:/Algomancer/Bayesian-Flow-Networks</id>
    <link href="https://github.com/Algomancer/Bayesian-Flow-Networks" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A simple implimentation of Bayesian Flow Networks&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Bayesian Flow Networks&lt;/h1&gt; &#xA;&lt;p&gt;A PyTorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2308.07037&#34;&gt;Bayesian Flow Networks&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/Algomancer/Bayesian-Flow-Networks/raw/main/bfn.jpeg&#34; alt=&#34;Paper Figure - BFN&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/Algomancer/Bayesian-Flow-Networks/raw/main/correctness.png&#34; alt=&#34;Correctness&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Discrete model with continuous-time loss, training and sampling (completed)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; SOTA performance on XOR dataset&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Wiki Text8 Dataset&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Bayesian Flow GPT-2 Scale&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Fancy Visuals&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>