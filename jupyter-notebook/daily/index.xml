<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-06-05T01:47:37Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>chenyuntc/simple-faster-rcnn-pytorch</title>
    <updated>2022-06-05T01:47:37Z</updated>
    <id>tag:github.com,2022-06-05:/chenyuntc/simple-faster-rcnn-pytorch</id>
    <link href="https://github.com/chenyuntc/simple-faster-rcnn-pytorch" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A simplified implemention of Faster R-CNN that replicate performance from origin paper&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;A Simple and Fast Implementation of Faster R-CNN&lt;/h1&gt; &#xA;&lt;h2&gt;1. Introduction&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;[Update:]&lt;/strong&gt; I&#39;ve further simplified the code to pytorch 1.5, torchvision 0.6, and replace the customized ops roipool and nms with the one from torchvision. if you want the old version code, please checkout branch &lt;a href=&#34;https://github.com/chenyuntc/simple-faster-rcnn-pytorch/tree/v1.0&#34;&gt;v1.0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This project is a &lt;strong&gt;Simplified&lt;/strong&gt; Faster R-CNN implementation based on &lt;a href=&#34;https://github.com/chainer/chainercv&#34;&gt;chainercv&lt;/a&gt; and other &lt;a href=&#34;https://raw.githubusercontent.com/chenyuntc/simple-faster-rcnn-pytorch/master/#acknowledgement&#34;&gt;projects&lt;/a&gt; . I hope it can serve as an start code for those who want to know the detail of Faster R-CNN. It aims to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Simplify the code (&lt;em&gt;Simple is better than complex&lt;/em&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Make the code more straightforward (&lt;em&gt;Flat is better than nested&lt;/em&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Match the performance reported in &lt;a href=&#34;https://arxiv.org/abs/1506.01497&#34;&gt;origin paper&lt;/a&gt; (&lt;em&gt;Speed Counts and mAP Matters&lt;/em&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;And it has the following features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;It can be run as pure Python code, no more build affair.&lt;/li&gt; &#xA; &lt;li&gt;It&#39;s a minimal implemention in around 2000 lines valid code with a lot of comment and instruction.(thanks to chainercv&#39;s excellent documentation)&lt;/li&gt; &#xA; &lt;li&gt;It achieves higher mAP than the origin implementation (0.712 VS 0.699)&lt;/li&gt; &#xA; &lt;li&gt;It achieve speed compariable with other implementation (6fps and 14fps for train and test in TITAN XP)&lt;/li&gt; &#xA; &lt;li&gt;It&#39;s memory-efficient (about 3GB for vgg16)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/chenyuntc/simple-faster-rcnn-pytorch/master/imgs/faster-speed.jpg&#34; alt=&#34;img&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;2. Performance&lt;/h2&gt; &#xA;&lt;h3&gt;2.1 mAP&lt;/h3&gt; &#xA;&lt;p&gt;VGG16 train on &lt;code&gt;trainval&lt;/code&gt; and test on &lt;code&gt;test&lt;/code&gt; split.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: the training shows great randomness, you may need a bit of luck and more epoches of training to reach the highest mAP. However, it should be easy to surpass the lower bound.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Implementation&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;mAP&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/1506.01497&#34;&gt;origin paper&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.699&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;train with caffe pretrained model&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.700-0.712&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;train with torchvision pretrained model&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.685-0.701&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;model converted from &lt;a href=&#34;https://github.com/chainer/chainercv/tree/master/examples/faster_rcnn&#34;&gt;chainercv&lt;/a&gt; (reported 0.706)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.7053&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;2.2 Speed&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Implementation&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;GPU&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Inference&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Trainining&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/1506.01497&#34;&gt;origin paper&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;K40&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5 fps&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;NA&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;This[1]&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;TITAN Xp&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14-15 fps&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6 fps&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ruotianluo/pytorch-faster-rcnn&#34;&gt;pytorch-faster-rcnn&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;TITAN Xp&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15-17fps&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6fps&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;[1]: make sure you install cupy correctly and only one program run on the GPU. The training speed is sensitive to your gpu status. see &lt;a href=&#34;https://raw.githubusercontent.com/chenyuntc/simple-faster-rcnn-pytorch/master/troubleshooting&#34;&gt;troubleshooting&lt;/a&gt; for more info. Morever it&#39;s slow in the start of the program -- it need time to warm up.&lt;/p&gt; &#xA;&lt;p&gt;It could be faster by removing visualization, logging, averaging loss etc.&lt;/p&gt; &#xA;&lt;h2&gt;3. Install dependencies&lt;/h2&gt; &#xA;&lt;p&gt;Here is an example of create environ &lt;strong&gt;from scratch&lt;/strong&gt; with &lt;code&gt;anaconda&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# create conda env&#xA;conda create --name simp python=3.7&#xA;conda activate simp&#xA;# install pytorch&#xA;conda install pytorch torchvision cudatoolkit=10.2 -c pytorch&#xA;&#xA;# install other dependancy&#xA;pip install visdom scikit-image tqdm fire ipdb pprint matplotlib torchnet&#xA;&#xA;# start visdom&#xA;nohup python -m visdom.server &amp;amp;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you don&#39;t use anaconda, then:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;install PyTorch with GPU (code are GPU-only), refer to &lt;a href=&#34;http://pytorch.org&#34;&gt;official website&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;install other dependencies: &lt;code&gt;pip install visdom scikit-image tqdm fire ipdb pprint matplotlib torchnet&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;start visdom for visualization&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Bash&#34;&gt;nohup python -m visdom.server &amp;amp;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;4. Demo&lt;/h2&gt; &#xA;&lt;p&gt;Download pretrained model from &lt;a href=&#34;https://drive.google.com/open?id=1cQ27LIn-Rig4-Uayzy_gH5-cW-NRGVzY&#34;&gt;Google Drive&lt;/a&gt; or &lt;a href=&#34;https://pan.baidu.com/s/1o87RuXW&#34;&gt;Baidu Netdisk( passwd: scxn)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/chenyuntc/simple-faster-rcnn-pytorch/raw/master/demo.ipynb&#34;&gt;demo.ipynb&lt;/a&gt; for more detail.&lt;/p&gt; &#xA;&lt;h2&gt;5. Train&lt;/h2&gt; &#xA;&lt;h3&gt;5.1 Prepare data&lt;/h3&gt; &#xA;&lt;h4&gt;Pascal VOC2007&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Download the training, validation, test data and VOCdevkit&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-Bash&#34;&gt;wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar&#xA;wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar&#xA;wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Extract all of these tars into one directory named &lt;code&gt;VOCdevkit&lt;/code&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-Bash&#34;&gt;tar xvf VOCtrainval_06-Nov-2007.tar&#xA;tar xvf VOCtest_06-Nov-2007.tar&#xA;tar xvf VOCdevkit_08-Jun-2007.tar&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;It should have this basic structure&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-Bash&#34;&gt;$VOCdevkit/                           # development kit&#xA;$VOCdevkit/VOCcode/                   # VOC utility code&#xA;$VOCdevkit/VOC2007                    # image sets, annotations, etc.&#xA;# ... and several other directories ...&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;modify &lt;code&gt;voc_data_dir&lt;/code&gt; cfg item in &lt;code&gt;utils/config.py&lt;/code&gt;, or pass it to program using argument like &lt;code&gt;--voc-data-dir=/path/to/VOCdevkit/VOC2007/&lt;/code&gt; .&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;5.2 [Optional]Prepare caffe-pretrained vgg16&lt;/h3&gt; &#xA;&lt;p&gt;If you want to use caffe-pretrain model as initial weight, you can run below to get vgg16 weights converted from caffe, which is the same as the origin paper use.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Bash&#34;&gt;python misc/convert_caffe_pretrain.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This scripts would download pretrained model and converted it to the format compatible with torchvision. If you are in China and can not download the pretrain model, you may refer to &lt;a href=&#34;https://github.com/chenyuntc/simple-faster-rcnn-pytorch/issues/63&#34;&gt;this issue&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Then you could specify where caffe-pretraind model &lt;code&gt;vgg16_caffe.pth&lt;/code&gt; stored in &lt;code&gt;utils/config.py&lt;/code&gt; by setting &lt;code&gt;caffe_pretrain_path&lt;/code&gt;. The default path is ok.&lt;/p&gt; &#xA;&lt;p&gt;If you want to use pretrained model from torchvision, you may skip this step.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;, caffe pretrained model has shown slight better performance.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: caffe model require images in BGR 0-255, while torchvision model requires images in RGB and 0-1. See &lt;code&gt;data/dataset.py&lt;/code&gt;for more detail.&lt;/p&gt; &#xA;&lt;h3&gt;5.3 begin training&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python train.py train --env=&#39;fasterrcnn&#39; --plot-every=100&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;you may refer to &lt;code&gt;utils/config.py&lt;/code&gt; for more argument.&lt;/p&gt; &#xA;&lt;p&gt;Some Key arguments:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--caffe-pretrain=False&lt;/code&gt;: use pretrain model from caffe or torchvision (Default: torchvison)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--plot-every=n&lt;/code&gt;: visualize prediction, loss etc every &lt;code&gt;n&lt;/code&gt; batches.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--env&lt;/code&gt;: visdom env for visualization&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--voc_data_dir&lt;/code&gt;: where the VOC data stored&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--use-drop&lt;/code&gt;: use dropout in RoI head, default False&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--use-Adam&lt;/code&gt;: use Adam instead of SGD, default SGD. (You need set a very low &lt;code&gt;lr&lt;/code&gt; for Adam)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--load-path&lt;/code&gt;: pretrained model path, default &lt;code&gt;None&lt;/code&gt;, if it&#39;s specified, it would be loaded.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;you may open browser, visit &lt;code&gt;http://&amp;lt;ip&amp;gt;:8097&lt;/code&gt; and see the visualization of training procedure as below:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/chenyuntc/simple-faster-rcnn-pytorch/master/imgs/visdom-fasterrcnn.png&#34; alt=&#34;visdom&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Troubleshooting&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;dataloader: &lt;code&gt;received 0 items of ancdata&lt;/code&gt;&lt;/p&gt; &lt;p&gt;see &lt;a href=&#34;https://github.com/pytorch/pytorch/issues/973#issuecomment-346405667&#34;&gt;discussion&lt;/a&gt;, It&#39;s alreadly fixed in &lt;a href=&#34;https://github.com/chenyuntc/simple-faster-rcnn-pytorch/raw/master/train.py#L17-L22&#34;&gt;train.py&lt;/a&gt;. So I think you are free from this problem.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Windows support&lt;/p&gt; &lt;p&gt;I don&#39;t have windows machine with GPU to debug and test it. It&#39;s welcome if anyone could make a pull request and test it.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;This work builds on many excellent works, which include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/chainer/chainercv&#34;&gt;Yusuke Niitani&#39;s ChainerCV&lt;/a&gt; (mainly)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ruotianluo/pytorch-faster-rcnn&#34;&gt;Ruotian Luo&#39;s pytorch-faster-rcnn&lt;/a&gt; which based on &lt;a href=&#34;https://github.com/endernewton/tf-faster-rcnn&#34;&gt;Xinlei Chen&#39;s tf-faster-rcnn&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jwyang/faster-rcnn.pytorch&#34;&gt;faster-rcnn.pytorch by Jianwei Yang and Jiasen Lu&lt;/a&gt;.It mainly refer to &lt;a href=&#34;https://github.com/longcw/faster_rcnn_pytorch&#34;&gt;longcw&#39;s faster_rcnn_pytorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;All the above Repositories have referred to &lt;a href=&#34;https://github.com/rbgirshick/py-faster-rcnn&#34;&gt;py-faster-rcnn by Ross Girshick and Sean Bell&lt;/a&gt; either directly or indirectly.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;^_^&lt;/h2&gt; &#xA;&lt;p&gt;Licensed under MIT, see the LICENSE for more detail.&lt;/p&gt; &#xA;&lt;p&gt;Contribution Welcome.&lt;/p&gt; &#xA;&lt;p&gt;If you encounter any problem, feel free to open an issue, but too busy lately.&lt;/p&gt; &#xA;&lt;p&gt;Correct me if anything is wrong or unclear.&lt;/p&gt; &#xA;&lt;p&gt;model structure &lt;img src=&#34;https://raw.githubusercontent.com/chenyuntc/simple-faster-rcnn-pytorch/master/imgs/model_all.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ctgk/PRML</title>
    <updated>2022-06-05T01:47:37Z</updated>
    <id>tag:github.com,2022-06-05:/ctgk/PRML</id>
    <link href="https://github.com/ctgk/PRML" rel="alternate"></link>
    <summary type="html">&lt;p&gt;PRML algorithms implemented in Python&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;PRML&lt;/h1&gt; &#xA;&lt;p&gt;Python codes implementing algorithms described in Bishop&#39;s book &#34;Pattern Recognition and Machine Learning&#34;&lt;/p&gt; &#xA;&lt;h2&gt;Required Packages&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;python 3&lt;/li&gt; &#xA; &lt;li&gt;numpy&lt;/li&gt; &#xA; &lt;li&gt;scipy&lt;/li&gt; &#xA; &lt;li&gt;jupyter (optional: to run jupyter notebooks)&lt;/li&gt; &#xA; &lt;li&gt;matplotlib (optional: to plot results in the notebooks)&lt;/li&gt; &#xA; &lt;li&gt;sklearn (optional: to fetch data)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Notebooks&lt;/h2&gt; &#xA;&lt;p&gt;The notebooks in this repository can be viewed with nbviewer or other tools, or you can use &lt;a href=&#34;https://studiolab.sagemaker.aws/&#34;&gt;Amazon SageMaker Studio Lab&lt;/a&gt;, a free computing environment on AWS (prior &lt;a href=&#34;https://studiolab.sagemaker.aws/requestAccount&#34;&gt;registration with an email address&lt;/a&gt; is required. Please refer to &lt;a href=&#34;https://docs.aws.amazon.com/sagemaker/latest/dg/studio-lab-onboard.html&#34;&gt;this document&lt;/a&gt; for usage).&lt;/p&gt; &#xA;&lt;p&gt;From the table below, you can open the notebooks for each chapter in each of these environments.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;nbviewer&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Amazon SageMaker Studio Lab&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/ctgk/PRML/blob/main/notebooks/ch01_Introduction.ipynb&#34;&gt;ch1. Introduction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/ctgk/PRML/blob/main/notebooks/ch01_Introduction.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open in SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/ctgk/PRML/blob/main/notebooks/ch02_Probability_Distributions.ipynb&#34;&gt;ch2. Probability Distributions&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/ctgk/PRML/blob/main/notebooks/ch02_Probability_Distributions.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open in SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/ctgk/PRML/blob/main/notebooks/ch03_Linear_Models_for_Regression.ipynb&#34;&gt;ch3. Linear Models for Regression&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/ctgk/PRML/blob/main/notebooks/ch03_Linear_Models_for_Regression.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open in SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/ctgk/PRML/blob/main/notebooks/ch04_Linear_Models_for_Classfication.ipynb&#34;&gt;ch4. Linear Models for Classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/ctgk/PRML/blob/main/notebooks/ch04_Linear_Models_for_Classfication.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open in SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/ctgk/PRML/blob/main/notebooks/ch05_Neural_Networks.ipynb&#34;&gt;ch5. Neural Networks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/ctgk/PRML/blob/main/notebooks/ch05_Neural_Networks.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open in SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/ctgk/PRML/blob/main/notebooks/ch06_Kernel_Methods.ipynb&#34;&gt;ch6. Kernel Methods&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/ctgk/PRML/blob/main/notebooks/ch06_Kernel_Methods.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open in SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/ctgk/PRML/blob/main/notebooks/ch07_Sparse_Kernel_Machines.ipynb&#34;&gt;ch7. Sparse Kernel Machines&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/ctgk/PRML/blob/main/notebooks/ch07_Sparse_Kernel_Machines.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open in SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/ctgk/PRML/blob/main/notebooks/ch08_Graphical_Models.ipynb&#34;&gt;ch8. Graphical Models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/ctgk/PRML/blob/main/notebooks/ch08_Graphical_Models.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open in SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/ctgk/PRML/blob/main/notebooks/ch09_Mixture_Models_and_EM.ipynb&#34;&gt;ch9. Mixture Models and EM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/ctgk/PRML/blob/main/notebooks/ch09_Mixture_Models_and_EM.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open in SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/ctgk/PRML/blob/main/notebooks/ch10_Approximate_Inference.ipynb&#34;&gt;ch10. Approximate Inference&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/ctgk/PRML/blob/main/notebooks/ch10_Approximate_Inference.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open in SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/ctgk/PRML/blob/main/notebooks/ch11_Sampling_Methods.ipynb&#34;&gt;ch11. Sampling Methods&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/ctgk/PRML/blob/main/notebooks/ch11_Sampling_Methods.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open in SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/ctgk/PRML/blob/main/notebooks/ch12_Continuous_Latent_Variables.ipynb&#34;&gt;ch12. Continuous Latent Variables&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/ctgk/PRML/blob/main/notebooks/ch12_Continuous_Latent_Variables.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open in SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/ctgk/PRML/blob/main/notebooks/ch13_Sequential_Data.ipynb&#34;&gt;ch13. Sequential Data&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/ctgk/PRML/blob/main/notebooks/ch13_Sequential_Data.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open in SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;If you use the SageMaker Studio Lab, open a terminal and execute the following commands to install the required libraries.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda env create -f environment.yaml  # might be optional&#xA;conda activate prml&#xA;python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>KaliYuga-ai/Pixel-Art-Diffusion</title>
    <updated>2022-06-05T01:47:37Z</updated>
    <id>tag:github.com,2022-06-05:/KaliYuga-ai/Pixel-Art-Diffusion</id>
    <link href="https://github.com/KaliYuga-ai/Pixel-Art-Diffusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;</summary>
  </entry>
</feed>