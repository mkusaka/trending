<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-23T01:37:55Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>airobotlab/KoChatGPT</title>
    <updated>2023-03-23T01:37:55Z</updated>
    <id>tag:github.com,2023-03-23:/airobotlab/KoChatGPT</id>
    <link href="https://github.com/airobotlab/KoChatGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ChatGPT의 RLHF를 학습을 위한 3가지 step별 한국어 데이터셋&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;KoChatGPT-replica(RLHF) 프로젝트&lt;/h1&gt; &#xA;&lt;p&gt;ChatGPT. 활용을 넘어서 ChatGPT-replica 모델을 직접 만들어 봅시다. ChatGPT는 공개 코드가 없습니다. 본 세미나에서는 ChatGPT를 만든 원리인 GPT fine-tuning, 강화학습(PPO), RLHF, ChatGPT 데이터셋 구축에 대해 다루고 코드 실습을 합니다. 만들어진 모델을 활용만 하는 건 재미없잖아요?? 우리 분야만의 ChatGPT(한국어/전문분야)를 직접 만드는 방법을 소개합니다. ※ 구현 모델은 ChatGPT-replica입니다. 실제 ChatGPT와 다를 수 있습니다. ※ GPT3가 아닌 GPT2+RLHF로 구현합니다. 거대언어모델로 개발시 어려움이 있을 수 있습니다. ※ 실습환경: Jupyter or Colab, 선수 지식: 파이썬&lt;/p&gt; &#xA;&lt;h4&gt;ChatGPT-replica 실습 Requirement&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;데이터(RLHF): &lt;a href=&#34;https://raw.githubusercontent.com/airobotlab/KoChatGPT/main/data_kochatgpt&#34;&gt;data_kochatgpt&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;LLM 모델: GPT2(&lt;a href=&#34;https://github.com/SKT-AI/KoGPT2&#34;&gt;skt/kogpt2-base-v2&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;GPU: &lt;a href=&#34;https://colab.research.google.com/?hl=ko&#34;&gt;Colab&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;1) kochatgpt &lt;a href=&#34;https://github.com/airobotlab/KoChatGPT/raw/main/kochatgpt_data_230320.ipynb&#34;&gt;데이터 구축코드&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;chatgpt의 RLHF를 학습하기 위한 3가지 step의 한국어 데이터셋&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; 1.1) 한국어 질문 데이터셋에서 질문 수집&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;data_kochatgpt/kochatgpt_seed_data.txt&lt;/strong&gt; : 한국어 질문 수집 데이터셋 &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;예시&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code&gt;불고기용 고기 한우에요?&#xA;쓰던 앱이 유료로 전환됐어&#xA;여친이랑 다툼&#xA;술 먹고 싶어&#xA;잊고싶다.&#xA;입냄새 안나나?&#xA;새로운 사랑은 찾아와&#xA;이명박 대통형은 어느 경축사를 통해 집권 후반기 국정운영에 대한 언급을 하였나?&#xA;금액은 얼마에요&#xA;리처드 닉슨이 43대 부통령직을 수행한 년도는?&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;출처 &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://github.com/songys/Chatbot_data/raw/master/ChatbotData.csv&#34;&gt;data1, ChatbotData/11824문장&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&amp;amp;topMenu=100&amp;amp;aihubDataSe=realm&amp;amp;dataSetSn=116&#34;&gt;data2, AI허브_한국어 대화/49711문장&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&amp;amp;topMenu=100&amp;amp;aihubDataSe=realm&amp;amp;dataSetSn=106&#34;&gt;data3, AI허브_일반상식/100268&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://korquad.github.io/KorQuad%201.0/&#34;&gt;data4, KorQuad/5774&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;4가지 데이터셋에서 총 167,577 질문셋 수집&lt;/li&gt; &#xA;    &lt;li&gt;12,000개 질문 랜덤 추출&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;RLHF 데이터셋 생성&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; 1.2) step1, SFT(Supervised Fine-tuning) 데이터셋 구축&lt;/summary&gt; &#xA; &lt;h6&gt;1.2.1) 한국어 QA 데이터셋의 질문으로 답변 자동 생성(ChatGPT): 12,000개&lt;/h6&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;SFT 데이터: &lt;a href=&#34;https://github.com/airobotlab/KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl&#34;&gt;&lt;strong&gt;data_kochatgpt/kochatgpt_1_SFT.jsonl&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;다음 단어만 잘 생성하는 모델에서 질문에 &#39;잘&#39; 대답하는 모델 만들기&lt;/li&gt; &#xA;  &lt;li&gt;Human labling이 필요하지만 ChatGPT 이용&lt;/li&gt; &#xA;  &lt;li&gt;질문을 입력으로 주고 ChatGPT로 답변 자동생성&lt;/li&gt; &#xA;  &lt;li&gt;데이터셋 예시&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/airobotlab/KoChatGPT/main/img/data_img_1.JPG&#34; width=&#34;500&#34;&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;    &#34;prompt&#34;: &#34;술 먹고 싶어&#34;,&#xA;    &#34;completion&#34;: &#34;&#39;술은 알코올이 함유된 음료수이며, 건강에 나쁜 영향을 미칠 수 있습니다. 따라서 술을 마시기 전에 본인의 건강 상태와 함께 적당한 양을 마시는 것이 중요합니다. 술을 마시기 전에 반드시 20세 이상인지 확인하시고, 규정된 양 이내에서 적절하게 즐기시길 바랍니다. 또한 술을 마시고 운전하는 것은 절대 금지하시기 바랍니다.&#34;,&#xA;    &#34;tokens&#34;: 189&#xA;},&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h6&gt;1.2.2) langchain을 이용한 채팅데이터 자동생성(ChatGPT): 439개&lt;/h6&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;SFT 데이터: &lt;a href=&#34;https://github.com/airobotlab/KoChatGPT/data_kochatgpt/kochatgpt_1_SFT_conversation.jsonl&#34;&gt;&lt;strong&gt;data_kochatgpt/kochatgpt_1_SFT_conversation.jsonl&lt;/strong&gt;&lt;/a&gt; : 한국어 질문 수집 데이터셋&lt;/li&gt; &#xA;  &lt;li&gt;다음 단어만 잘 생성하는 모델에서 질문에 &#39;잘&#39; 대답하는 모델 만들기&lt;/li&gt; &#xA;  &lt;li&gt;ChatGPT로 사람과 AI가 대화를 주고받는 채팅 데이터 생성&lt;/li&gt; &#xA;  &lt;li&gt;데이터셋 예시&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code&gt;&amp;lt;&amp;lt;start conversation&amp;gt;&amp;gt;&#xA;&amp;lt;사람&amp;gt;: 그렇군요. 무엇에 대해 이야기하고 싶으세요?&#xA;&amp;lt;챗봇&amp;gt;: 궁금하신 주제에 대해 이야기하고 싶으신가요? 예를 들어 여행, 식품, 스포츠, 등등 다양한 토론에 참여할 수 있습니다.&#xA;&amp;lt;사람&amp;gt;: 그래서 스포츠에 대해 이야기해볼까요?&#xA;&amp;lt;챗봇&amp;gt;: 그렇습니다. 스포츠에 대해 말씀하시겠습니까? 스포츠는 다양한 종목과 관련된 주제로 비슷한 경기를 비교하거나 새로운 경기를 분석할 수 있습니다. 또한 스포츠 관련 소식과 업데이트 등을 제공할 수 있습니다.&#xA;&amp;lt;사람&amp;gt;: 괜찮습니다. 소식이나 업데이트는 어떻게 받을 수 있나요?&#xA;&amp;lt;챗봇&amp;gt;: 귀하는 스포츠 관련 소식과 업데이트를 접할 수 있는 다양한 방법을 사용할 수 있습니다. 예를 들면 스포츠 사이트, 스포츠 앱, 스포츠 유튜브 채널 등을 통해 스포츠 소식과 업데이트를 받을 수 있습니다. 또한 소셜 미디어에서도 스포츠 소식과 업데이트를 받을 수 있습니다.&#xA;&amp;lt;사람&amp;gt;: 감사합니다. 이제 스포츠 소식과 업데이트를 받아볼 준비가 되었습니다.&#xA;&amp;lt;챗봇&amp;gt;: 그렇게 되었군요. 스포츠 소식과 업데이트를 받기 위해 귀하가 해야 할 준비는 무엇인가요? 스포츠 사이트, 스포츠 앱, 스포츠 유튜브 채널, 소셜 미디어 등을 통해 스포츠 소식과 업데이트를 받기 위해 준비해야 합니다.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; 1.3) step2, RM(Reward Model, 좋은글 채점모델) 데이터셋 구축&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;RM 데이터: &lt;a href=&#34;https://github.com/airobotlab/KoChatGPT/data_kochatgpt/kochatgpt_2_RM.jsonl&#34;&gt;&lt;strong&gt;data_kochatgpt/kochatgpt_2_RM.jsonl&lt;/strong&gt;&lt;/a&gt;: 10,220개&lt;/li&gt; &#xA;  &lt;li&gt;Ranking 데이터가 필요하므로 동일한 prompt에 대해 각기 다른 3가지 답변 자동 생성 &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt; &#xA;     &lt;ol&gt; &#xA;      &lt;li&gt;ChatGPT로 생성&lt;/li&gt; &#xA;     &lt;/ol&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &#xA;     &lt;ol start=&#34;2&#34;&gt; &#xA;      &lt;li&gt;GPT3로 생성(&#39;text-davinci-003&#39;)&lt;/li&gt; &#xA;     &lt;/ol&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &#xA;     &lt;ol start=&#34;3&#34;&gt; &#xA;      &lt;li&gt;GPT3로 생성(&#39;text-ada-001&#39;)&lt;/li&gt; &#xA;     &lt;/ol&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;ChatGPT &amp;gt; GPT3-davinci &amp;gt; GPT3-ada 순으로 랜덤하게 섞은 후 ranking 자동 생성&lt;/li&gt; &#xA;  &lt;li&gt;데이터셋 예시&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code&gt;[&#xA;    {&#xA;        &#34;prompt&#34;: &#34;번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독했다고 누구에게 말했나?&#34;,&#xA;        &#34;completion_0&#34;: &#34;번디는 다큐멘터리 방송 감독티비 에게 말했다.&#34;,&#xA;        &#34;completion_1&#34;: &#34;라이언에게 말했다.&#34;,&#xA;        &#34;completion_2&#34;: &#34;It is said that the doctor said to her: \&#34;You are in a condition that makes you harmful to yourself and others, but do not worry, I will take care of you.\&#34; \n\n\&#34;I am not worried about myself, why should I be?\&#34; \n\&#34;Because you are aLily and should as much as possible avoid touching yourself in order to protect yourself.\&#34;&#34;,&#xA;        &#34;ranking&#34;: [&#xA;            1,&#xA;            0,&#xA;            2&#xA;        ]&#xA;    }, ...&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;사람이 labeling 시 문장을 읽고 ranking을 0~2로 순위를 매긴다&lt;/li&gt; &#xA;  &lt;li&gt;향후 Step2) RM 모델을 학습할 때는 아래 형식으로 ranking을 2개씩 묶어 chosen과 rejected 데이터셋으로 변환하여 사용함&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code&gt;data = {}&#xA;data[&#39;prompt&#39;] = &#39;prompt&#39;&#xA;data[&#39;chosen&#39;] = &#39;good_sentence&#39;&#xA;data[&#39;rejected&#39;] = &#39;bad_sentence&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; 1.4) step3, PPO(질문에 ``더`` 잘 답하는 모델) 데이터셋 구축&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;PPO 데이터: &lt;a href=&#34;https://github.com/airobotlab/KoChatGPT/data_kochatgpt/kochatgpt_3_PPO.jsonl&#34;&gt;&lt;strong&gt;data_kochatgpt/kochatgpt_3_PPO.jsonl&lt;/strong&gt;&lt;/a&gt;: 12,000개&lt;/li&gt; &#xA;  &lt;li&gt;AI가 자동으로 글을 생성하기 위한 prompt 데이터셋&lt;/li&gt; &#xA;  &lt;li&gt;SFT 데이터셋에서 prompt만 가져와서 jsonl 형태로 변형후 저장&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code&gt;[&#xA;    {&#xA;        &#34;prompt&#34;: &#34;&#34;&#xA;    },&#xA;    {&#xA;        &#34;prompt&#34;: &#34;&#34;&#xA;    }, ...    &#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;hr&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;2) kochatgpt &lt;a href=&#34;https://github.com/airobotlab/KoChatGPT/raw/main/kochatgpt_code_230320.ipynb&#34;&gt;RLHF hands on 코드&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;a href=&#34;https://bit.ly/401rCrd&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt; &lt;/a&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ChatGPT의 학습방법인 RLHF(Reinforcement Learning from Human Feedback) 실습코드: &lt;strong&gt;kochatgpt_code_230320.ipynb&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;한국어 ChatGPT 데이터셋으로 ChatGPT-replica를 만드는 실습코드&lt;/li&gt; &#xA; &lt;li&gt;RLHF(Reinforcement Learning from Human Feedback)의 3단계 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Step1) SFT(지도학습)&lt;/li&gt; &#xA;   &lt;li&gt;Step2) RM(보상모델)&lt;/li&gt; &#xA;   &lt;li&gt;Step3) PPO(강화학습)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; 2.0) Colab 환경설정 &lt;/summary&gt; - 1min 소요 - python&amp;gt;=3.8 &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# for ColossalAI&#xA;!pip install colossalai==0.2.7&#xA;&#xA;# setup data&#xA;!git clone https://github.com/airobotlab/KoChatGPT&#xA;!mv KoChatGPT/data_kochatgpt .&#xA;!mv KoChatGPT/img .&#xA;&#xA;# install chatgpt(colossalai) library&#xA;%cd KoChatGPT/colossalai_ChatGPT_230319/&#xA;!pip install .&#xA;%cd ../../&#xA;&#xA;# setup etc library&#xA;!pip install openai&#xA;!pip install langchain==0.0.113&#xA;!pip install pandas&amp;gt;=1.4.1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; 2.1) Step 1) SFT: 질문에 대답을 잘하는 모델 만들기 &lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;SFT: Supervised Fine Tuning&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Fine-tune a pretrained LLM on a specific domain or corpus of instructions and human demonstrations&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;기존 GPT3는 다음 단어를 잘 맞추는 모델. But 질문에 대해 답을 맞추는 모델이 X&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;질문에 응답을 잘하도록 SFT 수행&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;먼저 사람이 지시에 대한 대답을 직접 작성(데이터 13,000개)하고, 이 데이터셋으로 SFT&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;데이터: 질문-응답 쌍 데이터셋(12,000개)&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;예시)&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;질문(prompt): 인공지능을 설명해보세요&lt;/li&gt; &#xA;    &lt;li&gt;응답(completion): 인공지능은 인간의 학습능력, 추론능력, 지각능력을 인공적으로 구현하려는 컴퓨터 과학의 세부분야 중 하나이다. ...&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;code reference&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://github.com/philschmid/fine-tune-GPT-2/raw/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb&#34;&gt;fine tuning code_1&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://github.com/Beomi/KoAlpaca/raw/main/train.py&#34;&gt;fine tuning code_2&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;SFT 예시&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/airobotlab/KoChatGPT/main/img/1_SFT_1.png&#34; width=&#34;500&#34;&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;모델 입출력 예시&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/airobotlab/KoChatGPT/main/img/image_step1.JPG&#34; width=&#34;500&#34;&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;전체 구조&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;img src=&#34;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/rlhf/pretraining.png&#34; width=&#34;500&#34;&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;데이터셋 형태&lt;/strong&gt; step1) SFT(actor_training_data): SFT 지도 미세 조정에 사용되는 JSON 데이터&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;[&#xA;    {&#xA;        &#34;prompt&#34;: &#34;&#34;,&#xA;        &#34;completion&#34;: &#34;&#34;        &#xA;    }, ...&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;결과물&lt;/strong&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Before: 다음 단어만 잘 생성 했었음&lt;/li&gt; &#xA;    &lt;li&gt;After: 질문에 ‘잘’ 대답하는 모델&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; 2.2) Step 2) RM: 좋은 글 채점기 만들기 &lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;Collect a human annotated dataset and train a reward model&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;배경&lt;/strong&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;기존 AI는 주관적인 글을 채점(점수화) 할 수 없었음&lt;/li&gt; &#xA;    &lt;li&gt;사람이 직접 피드백을 줘서 글 채점의 척도로 사용하자&lt;/li&gt; &#xA;    &lt;li&gt;매번 사람이 채점할 수 없으니, 사람의 채점을 모방하는 &lt;strong&gt;좋은글 채점 AI모델&lt;/strong&gt; 을 만들자&lt;/li&gt; &#xA;    &lt;li&gt;채점 AI모델을 만드려면, 사람이 글을 채점한 데이터셋(33,000개)이 필요하다&lt;/li&gt; &#xA;    &lt;li&gt;동일 질문에 대해 AI모델이 생성한 여러 글(한 번에 4~6개 세트)을 사람이 직접 ranking을 매긴다.&lt;/li&gt; &#xA;    &lt;li&gt;왜?? 사람이 생성한 글에 바로 점수를 매기게 되면 사람마다 기준이 다를 수 있기 때문에 순위로&lt;/li&gt; &#xA;    &lt;li&gt;&lt;strong&gt;C &amp;gt; B &amp;gt; A&lt;/strong&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Human labeling 예시&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/airobotlab/KoChatGPT/main/img/2_RM_1.png&#34; width=&#34;700&#34;&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;좋은글 채점 모델 학습(RM, Reward Model)&lt;/strong&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;1등 글은 높은 점수를&lt;/li&gt; &#xA;    &lt;li&gt;꼴등 데이터는 낮은 점수를&lt;/li&gt; &#xA;    &lt;li&gt;입력: AI가 생성한 글&lt;/li&gt; &#xA;    &lt;li&gt;출력: 0~1점&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;보상모델 입출력&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/airobotlab/KoChatGPT/main/img/2_RM_2.png&#34; width=&#34;700&#34;&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;결과물&lt;/strong&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Before: 좋은 글, 나쁜 글 판단 불가능&lt;/li&gt; &#xA;    &lt;li&gt;After: 사람이 읽기에 좋은글/나쁜글 판단 모델&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;전체 구조&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;img src=&#34;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/rlhf/reward-model.png&#34; width=&#34;500&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; 2.3) Step3) PPO 학습: 사람의 피드백을 반영하여 학습 &lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Further fine-tune the LLM from step 1 with the reward model and this dataset using RL (e.g. PPO)&lt;/li&gt; &#xA;  &lt;li&gt;배경 &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;strong&gt;사람의 순위를 모사한 보상모델(RM)&lt;/strong&gt; 의 점수가 높아지도록 학습 (31,000개)&lt;/li&gt; &#xA;    &lt;li&gt;초기 모델에 비해 너무 많이 바뀌지 않도록&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/airobotlab/KoChatGPT/main/img/3_PPO_1.png&#34; width=&#34;650&#34;&gt; &#xA; &lt;img src=&#34;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/rlhf/rlhf.png&#34; width=&#34;500&#34;&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Fine-tuning 태스크를 강화학습 문제로 다음과 같이 정형화 &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Policy: 언어모델-프롬프트를 입력으로 받아 텍스트의 시퀀스(혹은 그 확률)를 리턴&lt;/li&gt; &#xA;    &lt;li&gt;Action space : 언어모델의 모든 단어 (일반적으로 5만개 분량)&lt;/li&gt; &#xA;    &lt;li&gt;Observation space : 가능한 인풋 토큰 시퀀스 (단어개수^시퀀스길이 이므로 엄청 큼!)&lt;/li&gt; &#xA;    &lt;li&gt;Reward function : 보상모델과 policy shift에 대한 제약조건의 조합으로 정의됨&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/airobotlab/KoChatGPT/main/img/3_PPO_2.png&#34; width=&#34;500&#34;&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Frozen Model과 Non-frozen(trainable) Model의 텍스트 출력 확률간 KL divergence를 계산&lt;/li&gt; &#xA;  &lt;li&gt;trainable Model의 weight가 완전히 바뀌는 것을 방지하고 Reward Model에 말도 되지 않는 텍스트로 출력을 시작하는 것을 방지&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/airobotlab/KoChatGPT/main/img/3_PPO_3.png&#34; width=&#34;500&#34;&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;PPO process [1] 초기화를 위해 intial probs(initial output text probabilities)를 new probs(new output text probabilities)와 동일하게 만듬&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;while:&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt; &lt;p&gt;[2] New probs와 initial probs간 ratio을 계산함&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;[3] 아래 공식에 따라 loss를 계산함.&lt;/p&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;loss = -min(ratio * R, clip(ratio, 0.8, 1.2) * R) &#xA;       &lt;ul&gt; &#xA;        &lt;li&gt;R = reward + KL (or 0.8&lt;em&gt;reward + 0.2&lt;/em&gt;KL와 같은 weighted average)&lt;/li&gt; &#xA;        &lt;li&gt;clip(ratio, 0.8, 1.2) → 0.8 ≤ ratio ≤ 1.2&lt;/li&gt; &#xA;       &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;[4] Loss를 backpropagating하여 SFT Model의 weight를 업데이트함&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;[5] 새롭게 업데이트된 SFT 모델로 new probs를 계산함&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;[6] 2번부터 6번을 N 번 반복함&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/hpcaitech/ColossalAI/raw/1216d1e7bdf223d831895e34c01fb40df36ea9c7/applications/ChatGPT/chatgpt/experience_maker/naive.py#L7&#34;&gt;loss1&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/hpcaitech/ColossalAI/raw/1216d1e7bdf223d831895e34c01fb40df36ea9c7/applications/ChatGPT/chatgpt/models/utils.py#L31&#34;&gt;loss2&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; 실습에서 사용하는 ColossalAI 소개 &lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/hpcaitech/ColossalAI/tree/main/applications/ChatGPT&#34;&gt;ColossalAI&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;step2 RM 학습과 step3 PPO 코드 깔끔하게 제공&lt;/li&gt; &#xA;    &lt;li&gt;Multi-GPU로 DDP, ColossalAIStrategy, LoRA 학습코드 제공!!&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ColossalAI 장점&lt;/strong&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;ColossalAI는 pytorch에 비해 추론시 1.4배 빠르고, 학습시 7.7배 빠르다!!&lt;/li&gt; &#xA;    &lt;li&gt;ColossalAI는 pytorch와 비교해 10.3배 큰 모델을 처리할수 있다!!&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chatgpt/ChatGPT%20scaling.png&#34; width=&#34;800&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chatgpt/ChatGPT-1GPU.jpg&#34; width=&#34;500&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;hr&gt; &#xA;&lt;hr&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;※ 구현 모델은 ChatGPT-replica입니다. 실제 ChatGPT와 다를 수 있습니다.&lt;/li&gt; &#xA; &lt;li&gt;※ 실습을 위해 GPT3가 아닌 KoGPT2+RLHF로 구현합니다. 거대 언어모델로 개발 시 어려움이 있을 수 있습니다.&lt;/li&gt; &#xA; &lt;li&gt;※ 실습환경: Colab, 선수 지식: 파이썬&lt;/li&gt; &#xA; &lt;li&gt;※ Colab에서 돌아가기 위해 각 Step을 학습한 후 저장된 모델을 local로 다운받고 &#39;런타임 연결 해제 및 삭제&#39;를 눌러야 다음 Step이 돌아갑니다.(colab 메모리 부족) Step1/Step2에서 학습된 모델을 Step3에 입력해줘야 합니다.&lt;/li&gt; &#xA; &lt;li&gt;※ 데이터는 OpenAI API로 자동생성 했습니다. 사람의 검수가 필요합니다.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>PineappleExpress808/lex-gpt</title>
    <updated>2023-03-23T01:37:55Z</updated>
    <id>tag:github.com,2023-03-23:/PineappleExpress808/lex-gpt</id>
    <link href="https://github.com/PineappleExpress808/lex-gpt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Lex GPT&lt;/h1&gt; &#xA;&lt;p&gt;This app enables AI-powered search for Lex Fridman podcast.&lt;/p&gt; &#xA;&lt;p&gt;This is also a testbed for exploring Langchain functionality.&lt;/p&gt; &#xA;&lt;h2&gt;Dataset&lt;/h2&gt; &#xA;&lt;p&gt;Scrape ep 1-325 Whisper transcriptions via @karpathy for first 325 episodes:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://karpathy.ai/lexicap/index.html&#34;&gt;https://karpathy.ai/lexicap/index.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Trascribe remaining episodes (through episode 365) with Whisper.&lt;/p&gt; &#xA;&lt;p&gt;Transcribed data is split / embedded (Pinecone) with Langchain.&lt;/p&gt; &#xA;&lt;p&gt;All steps outlined in: &lt;code&gt;scripts/get_data.ipynb&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Search&lt;/h2&gt; &#xA;&lt;p&gt;Use Langchain &lt;code&gt;VectorDBQAChain&lt;/code&gt; to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Embed the user query&lt;/li&gt; &#xA; &lt;li&gt;Perform similarity search on Pinecone embeddings&lt;/li&gt; &#xA; &lt;li&gt;Synthesize the answer from relevant chunks with &lt;code&gt;GPT 3.5&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Search&lt;/h2&gt; &#xA;&lt;p&gt;Relevant chunks with metadata (links) are displayed as source documents.&lt;/p&gt; &#xA;&lt;p&gt;This builds on the excellent UI from &lt;a href=&#34;https://github.com/mckaywrigley/wait-but-why-gpt&#34;&gt;https://github.com/mckaywrigley/wait-but-why-gpt&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Deploy&lt;/h2&gt; &#xA;&lt;p&gt;Note: the app that supports streaming is deployed to fly.io: &lt;a href=&#34;https://lex-gpt.fly.dev/&#34;&gt;https://lex-gpt.fly.dev/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is because Vercel requires edge functions for streaming.&lt;/p&gt; &#xA;&lt;p&gt;We are working on getting edge functions working with Langchain.&lt;/p&gt; &#xA;&lt;p&gt;In the meantime, use &lt;a href=&#34;https://lex-gpt.fly.dev/&#34;&gt;https://lex-gpt.fly.dev/&lt;/a&gt; for the more performant app.&lt;/p&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;p&gt;Thanks to &lt;a href=&#34;https://twitter.com/mckaywrigley&#34;&gt;Mckay Wrigley&lt;/a&gt; for open-sourcing his UI.&lt;/p&gt; &#xA;&lt;p&gt;Thanks to Lex Fridman for the excellent podcast.&lt;/p&gt; &#xA;&lt;p&gt;Thanks to Karapthy for the Whisper transcriptions.&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions, feel free to reach out to me on &lt;a href=&#34;https://twitter.com/RLanceMartin&#34;&gt;Twitter&lt;/a&gt;!&lt;/p&gt;</summary>
  </entry>
</feed>