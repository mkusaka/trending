<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-23T01:36:24Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>abdullahtarek/football_analysis</title>
    <updated>2024-04-23T01:36:24Z</updated>
    <id>tag:github.com,2024-04-23:/abdullahtarek/football_analysis</id>
    <link href="https://github.com/abdullahtarek/football_analysis" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This repository contains a comprehensive computer vision/machine learning football project that uses YOLO for object detection, Kmeans for pixel segmentation, optical flow for motion tracking, and perspective transformation to analyze player movements in football videos&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Football Analysis Project&lt;/h1&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;The goal of this project is to detect and track players, referees, and footballs in a video using YOLO, one of the best AI object detection models available. We will also train the model to improve its performance. Additionally, we will assign players to teams based on the colors of their t-shirts using Kmeans for pixel segmentation and clustering. With this information, we can measure a team&#39;s ball acquisition percentage in a match. We will also use optical flow to measure camera movement between frames, enabling us to accurately measure a player&#39;s movement. Furthermore, we will implement perspective transformation to represent the scene&#39;s depth and perspective, allowing us to measure a player&#39;s movement in meters rather than pixels. Finally, we will calculate a player&#39;s speed and the distance covered. This project covers various concepts and addresses real-world problems, making it suitable for both beginners and experienced machine learning engineers.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/abdullahtarek/football_analysis/main/output_videos/screenshot.png&#34; alt=&#34;Screenshot&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Modules Used&lt;/h2&gt; &#xA;&lt;p&gt;The following modules are used in this project:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;YOLO: AI object detection model&lt;/li&gt; &#xA; &lt;li&gt;Kmeans: Pixel segmentation and clustering to detect t-shirt color&lt;/li&gt; &#xA; &lt;li&gt;Optical Flow: Measure camera movement&lt;/li&gt; &#xA; &lt;li&gt;Perspective Transformation: Represent scene depth and perspective&lt;/li&gt; &#xA; &lt;li&gt;Speed and distance calculation per player&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Trained Models&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://drive.google.com/file/d/1DC2kCygbBWUKheQ_9cFziCsYVSRw6axK/view?usp=sharing&#34;&gt;Trained Yolo v5&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Sample video&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://drive.google.com/file/d/1t6agoqggZKx6thamUuPAIdN_1zR9v9S_/view?usp=sharing&#34;&gt;Sample input video&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;To run this project, you need to have the following requirements installed:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.x&lt;/li&gt; &#xA; &lt;li&gt;ultralytics&lt;/li&gt; &#xA; &lt;li&gt;supervision&lt;/li&gt; &#xA; &lt;li&gt;OpenCV&lt;/li&gt; &#xA; &lt;li&gt;NumPy&lt;/li&gt; &#xA; &lt;li&gt;Matplotlib&lt;/li&gt; &#xA; &lt;li&gt;Pandas&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>deep-diver/llamaduo</title>
    <updated>2024-04-23T01:36:24Z</updated>
    <id>tag:github.com,2024-04-23:/deep-diver/llamaduo</id>
    <link href="https://github.com/deep-diver/llamaduo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LLaMADuo&lt;/h1&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/deep-diver/llamaduo/main/assets/logo.png&#34; style=&#34;display: block; margin-left: auto; margin-right: auto;&#34;&gt; &#xA;&lt;p&gt;This project showcases an LLMOps pipeline that fine-tunes a small-size LLM model to prepare for the outage of the service LLM. For this project, we choose &lt;a href=&#34;https://deepmind.google/technologies/gemini/&#34;&gt;Gemini 1.0 Pro&lt;/a&gt; for service type LLM and &lt;a href=&#34;https://blog.google/technology/developers/gemma-open-models/&#34;&gt;Gemma&lt;/a&gt; 2B/7B for small sized LLM model.&lt;/p&gt; &#xA;&lt;p&gt;For this project, the following tech stacks are chosen:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Hugging Face open source ecosystem (&lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;&lt;code&gt;transformers&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/huggingface/peft&#34;&gt;&lt;code&gt;peft&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/huggingface/alignment-handbook&#34;&gt;&lt;code&gt;alignment-handbook&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/docs/hub/en/index&#34;&gt;&lt;code&gt;huggingface_hub&lt;/code&gt;&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ai.google.dev/docs&#34;&gt;Gemini API&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Additionally, this project implements desirable features when calling the Gemini API: concurrency and rate-limiting.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Table of contents&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deep-diver/llamaduo/main/#motivation&#34;&gt;Motivation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deep-diver/llamaduo/main/#overview&#34;&gt;Overview&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deep-diver/llamaduo/main/#building-on-top-of-this-project&#34;&gt;Building on top of this project&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deep-diver/llamaduo/main/#acknowledgments&#34;&gt;Acknowledgements&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Motivation&lt;/h2&gt; &#xA;&lt;p&gt;We assume that a small LLM could show comparable performance to that of a service-type LLM on a specific task, and this project tries to showcase such a possibility in a practically grounded manner. Furthermore, this project shows how to migrate from service LLM to small LLM smoothly.&lt;/p&gt; &#xA;&lt;p&gt;Assume that service LLM is integrated into your service or system. However, from time to time, the service LLM should be replaced for the following reaons:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;failure of service LLM, which may be operationally impacting a business.&lt;/li&gt; &#xA; &lt;li&gt;data privacy issue. You don&#39;t want to expose your private data.&lt;/li&gt; &#xA; &lt;li&gt;some system runs without internet connection. Service LLM did a great job on PoC, but now you need the same intelligence in an on-premise environment.&lt;/li&gt; &#xA; &lt;li&gt;version control issue. Service LLMs change their versions from time to time, and legacy versions will become obsolete. However, we just want to keep the behavior as is.&lt;/li&gt; &#xA; &lt;li&gt;...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To better prepare for such impacting situations, this project suggests migrating from a service LLM to a local small LLM. Since we are satisfied with the results from service LLM, we know our inputs (prompts) and the desired outputs. Then, we can fine-tune small size LLM on the collected prompts to match the desired outputs. Furthermore, if the fine-tuned LLM&#39;s performance is still poor, we can grow the size of the dataset by generating more similar data via service LLM.&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/deep-diver/llamaduo/main/assets/figure.png&#34; style=&#34;display: block; margin-left: auto; margin-right: auto;&#34;&gt; &#xA;&lt;p&gt;This project comes with the toolset of batch inference, evaluation, and synthetic data generation. Each tool can be run independently, but they could be hooked up to form a pipeline. It&#39;s on the end user to figure out the best way to collate these together.&lt;/p&gt; &#xA;&lt;p&gt;The prerequisite to run this toolset is to have a dataset consisting of desired &lt;code&gt;(prompt, response)&lt;/code&gt; pairs. The exact format of the dataset can be found &lt;a href=&#34;https://huggingface.co/datasets/sayakpaul/no_robots_only_coding&#34;&gt;here&lt;/a&gt;. The &lt;code&gt;prompt&lt;/code&gt; is the input to the small-size LLM to generate output. Then, &lt;code&gt;prompt&lt;/code&gt;, &lt;code&gt;response&lt;/code&gt;, and the &lt;code&gt;generated output&lt;/code&gt; are going to be used to evaluate the fine-tuned small-size LLM. The main idea is to make a small LLM output similar to the given response.&lt;/p&gt; &#xA;&lt;h3&gt;Hugging Face Hub authentication&lt;/h3&gt; &#xA;&lt;p&gt;All steps in this project leverages Hugging Face Hub for accessing and managing of models and datasets. To avoid any unexpected erros, we recommend authenticating Hugging Face Hub before procceding steps. Hugging Face Hub authentication could be done with the follwing CLI. Simply paste the Hugging Face access token into the prompt that appears.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;# Alternative way is setting HUGGING_FACE_HUB_TOKEN environment variable&#xA;# Hugging Face libraries will look up the HUGGING_FACE_HUB_TOKEN value&#xA;$ huggingface-cli login&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Fine-tuning&lt;/h3&gt; &#xA;&lt;p&gt;We leverage Hugging Face&#39;s &lt;a href=&#34;https://github.com/huggingface/alignment-handbook&#34;&gt;alignment-handbook&lt;/a&gt; to streamline the LLM fine-tuning. Specifically, all the detailed fine-tuning parameters for this project can be found in &lt;a href=&#34;https://raw.githubusercontent.com/deep-diver/llamaduo/main/config/sample_config.yaml&#34;&gt;this config&lt;/a&gt;. Also note that the same config can be reused for the batch inference in the next section to make sure there are no mismatched configurations.&lt;/p&gt; &#xA;&lt;p&gt;Also, we plan to add scripts to run the fine-tuning on the cloud. The list of supported cloud platforms will be updated below:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sky.dstack.ai/&#34;&gt;&lt;code&gt;dstack Sky&lt;/code&gt;&lt;/a&gt;: detailed instruction can be found in &lt;a href=&#34;https://raw.githubusercontent.com/deep-diver/llamaduo/main/dstack/&#34;&gt;dstack directory&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Batch inference&lt;/h3&gt; &#xA;&lt;p&gt;Batch inference lets fine-tuned LLM to generate text and push the results on the Hugging Face Dataset repository.&lt;/p&gt; &#xA;&lt;p&gt;To perform this, you need to run the following commands in the terminal:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;# All parameters defined in the config/batch_inference.yaml file&#xA;# could be manually inputted as CLI arguments (arg names are the same)&#xA;$ python batch_inference.py --from-config config/batch_inference.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, the resulting outputs will be pushed to Hugging Face Dataset repository in the following structure (&lt;a href=&#34;https://huggingface.co/datasets/chansung/lm_response_test&#34;&gt;example&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;column names&lt;/th&gt; &#xA;   &lt;th&gt;instructions&lt;/th&gt; &#xA;   &lt;th&gt;target_responses&lt;/th&gt; &#xA;   &lt;th&gt;candidate_responses&lt;/th&gt; &#xA;   &lt;th&gt;model_id&lt;/th&gt; &#xA;   &lt;th&gt;model_sha&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;descriptions&lt;/td&gt; &#xA;   &lt;td&gt;the input prompts&lt;/td&gt; &#xA;   &lt;td&gt;desired outputs&lt;/td&gt; &#xA;   &lt;td&gt;model generated outputs&lt;/td&gt; &#xA;   &lt;td&gt;model id that generated outputs&lt;/td&gt; &#xA;   &lt;td&gt;the version of the model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;Evaluation evaluates the generated text from fine-tuned LLM with the help of service LLM. The evaluation criteria is the similarity and quality by comparing to the given desired outputs.&lt;/p&gt; &#xA;&lt;p&gt;To perform this you need to run the following commands in terminal:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;# GEMINI_API_KEY is required to call Gemini API&#xA;$ export GEMINI_API_KEY=&amp;lt;YOUR-GEMINI-API-KEY&amp;gt;&#xA;&#xA;# All parameters defined in the config/evaluation.yaml file&#xA;# could be manually inputted as CLI arguments (arg names are the same)&#xA;$ python batch_inference.py --from-config config/evaluation.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, the resulting outputs will be pushed to Hugging Face Dataset repository in the following structure (&lt;a href=&#34;https://huggingface.co/datasets/chansung/eval_dataset_test&#34;&gt;example&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;column names&lt;/th&gt; &#xA;   &lt;th&gt;ommited..&lt;/th&gt; &#xA;   &lt;th&gt;eval_prompts&lt;/th&gt; &#xA;   &lt;th&gt;similarity_scores&lt;/th&gt; &#xA;   &lt;th&gt;precision_scores&lt;/th&gt; &#xA;   &lt;th&gt;evaluators&lt;/th&gt; &#xA;   &lt;th&gt;dates&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;descriptions&lt;/td&gt; &#xA;   &lt;td&gt;all columns are copied from batch inference&lt;/td&gt; &#xA;   &lt;td&gt;prompts input to the evaluator&lt;/td&gt; &#xA;   &lt;td&gt;similarity score in 0~100 scale&lt;/td&gt; &#xA;   &lt;td&gt;precision score in 0~100 scale&lt;/td&gt; &#xA;   &lt;td&gt;model name used as evaluator&lt;/td&gt; &#xA;   &lt;td&gt;dates&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Synthetic data generation&lt;/h3&gt; &#xA;&lt;p&gt;Synthetic data generation generates similar data to the ones used to fine-tune the LLM. This could be performed based on the evaluation results. For instance, if you are not satisfied with the evaluation results, and if you think the training dataset is not large enough, you can create more of the similar data to boost the performance of the LLM.&lt;/p&gt; &#xA;&lt;p&gt;To perform this you need to run the following commands in terminal:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;# GEMINI_API_KEY is required to call Gemini API&#xA;$ export GEMINI_API_KEY=&amp;lt;YOUR-GEMINI-API-KEY&amp;gt;&#xA;&#xA;# All parameters defined in the config/synth_data_gen.yaml file&#xA;# could be manually inputted as CLI arguments (arg names are the same)&#xA;$ python data_gen.py --from-config config/synth_data_gen.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, the resulting outputs will be pushed to Hugging Face Dataset repository in the following structure (&lt;a href=&#34;https://huggingface.co/datasets/chansung/synth_ds_test2&#34;&gt;example&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;column names&lt;/th&gt; &#xA;   &lt;th&gt;generators&lt;/th&gt; &#xA;   &lt;th&gt;prompt_ids&lt;/th&gt; &#xA;   &lt;th&gt;seed_prompts&lt;/th&gt; &#xA;   &lt;th&gt;messages&lt;/th&gt; &#xA;   &lt;th&gt;category&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;descriptions&lt;/td&gt; &#xA;   &lt;td&gt;model used to generate data&lt;/td&gt; &#xA;   &lt;td&gt;--&lt;/td&gt; &#xA;   &lt;td&gt;the base prompts used to generate data&lt;/td&gt; &#xA;   &lt;td&gt;generated synthetic data&lt;/td&gt; &#xA;   &lt;td&gt;category this data belongs to&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Merging generated dataset&lt;/h3&gt; &#xA;&lt;p&gt;Synthetically generated datasets are a means of supplementing the original dataset. In order to original and synthetica datasets into account when fine-tuning a language model, both datasets better to be merged into a single dataset. This project provies a script for such purpose.&lt;/p&gt; &#xA;&lt;p&gt;To perform this you need to run the following commands in terminal. If you have more than one synthetic dataset, consider to run the same script iteratively:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;# All parameters defined in the config/dataset_merge.yaml file&#xA;# could be manually inputted as CLI arguments (arg names are the same)&#xA;$ python dataset_merge.py --from-config config/dataset_merge.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Building on top of this project&lt;/h2&gt; &#xA;&lt;p&gt;This project is NOT a library. It&#39;s rather a template for people to build their LLMOps pipelines on top of. Below, we give a few concrete examples to explain how this could be done:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;The automated evaluation metrics used in this project may have to change depending on your business use case. So, you could customize the &lt;code&gt;eval&lt;/code&gt; prompt (refer to &lt;a href=&#34;https://raw.githubusercontent.com/deep-diver/llamaduo/main/config/prompts.toml&#34;&gt;&lt;code&gt;config/prompts.toml&lt;/code&gt;&lt;/a&gt;) so that the underlying LLM returns the desired metrics.&lt;/li&gt; &#xA; &lt;li&gt;The small LLM is fine-tuned using the &lt;a href=&#34;https://github.com/huggingface/alignment-handbook&#34;&gt;&lt;code&gt;alignment-handbook&lt;/code&gt;&lt;/a&gt; repository from Hugging Face. You may consider bringing your models that were fine-tuned using other tools. This project relies on the model-specific output structures for collating the results. So, if you&#39;re bringing your own models, those heuristics might have to change a bit. Feel free to open an issue on this repository if you face any difficulties. We will try our best to help :)&lt;/li&gt; &#xA; &lt;li&gt;Finally, Gemini is used as our teacher for the small LLM. You may have a different teacher LLM in mind (GPT-4, Claude, etc. or maybe even a much bigger open-weights LLM).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;This is a project built during the Gemma/Gemini sprints held by Google&#39;s ML Developer Programs team. We are thankful to be granted good amount of GCP credits to finish up this project. Thanks to Hugging Face for providing Sayak with resources to run some fine-tuning experiments.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>PacktPublishing/Polars-Cookbook</title>
    <updated>2024-04-23T01:36:24Z</updated>
    <id>tag:github.com,2024-04-23:/PacktPublishing/Polars-Cookbook</id>
    <link href="https://github.com/PacktPublishing/Polars-Cookbook" rel="alternate"></link>
    <summary type="html">&lt;p&gt;B21621 - Polars Cookbook&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Polars-Cookbook&lt;/h1&gt; &#xA;&lt;p&gt;B21621 - Polars Cookbook&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Polars-Cookbook/raw/main/Chapter01/ch01.ipynb&#34;&gt;Chapter 1: Getting Started with Python Polars&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Introducing Key Features in Polars&lt;/li&gt; &#xA; &lt;li&gt;The Polars DataFrame&lt;/li&gt; &#xA; &lt;li&gt;The Polars Series&lt;/li&gt; &#xA; &lt;li&gt;The Polars LazyFrame&lt;/li&gt; &#xA; &lt;li&gt;Selecting columns and filtering data&lt;/li&gt; &#xA; &lt;li&gt;Creating, modifying, and deleting columns&lt;/li&gt; &#xA; &lt;li&gt;Understanding method chaining&lt;/li&gt; &#xA; &lt;li&gt;Processing datasets larger than RAM&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Polars-Cookbook/raw/main/Chapter02/ch02.ipynb&#34;&gt;Chapter 2: Reading and Writing Files&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Reading and writing CSV files&lt;/li&gt; &#xA; &lt;li&gt;Reading and writing parquet files&lt;/li&gt; &#xA; &lt;li&gt;Reading and writing delta tables&lt;/li&gt; &#xA; &lt;li&gt;Reading and writing excel files&lt;/li&gt; &#xA; &lt;li&gt;Reading and writing JSON files&lt;/li&gt; &#xA; &lt;li&gt;Reading and writing other data file formats&lt;/li&gt; &#xA; &lt;li&gt;Reading and writing multiple files&lt;/li&gt; &#xA; &lt;li&gt;Working with databases&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Polars-Cookbook/raw/main/Chapter03/ch03.ipynb&#34;&gt;Chapter 3: Introduction to Data Analysis in Python Polars&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Inspecting the DataFrame&lt;/li&gt; &#xA; &lt;li&gt;Casting data types&lt;/li&gt; &#xA; &lt;li&gt;Handling duplicate values&lt;/li&gt; &#xA; &lt;li&gt;Masking sensitive data&lt;/li&gt; &#xA; &lt;li&gt;Visualizing data using Plotly&lt;/li&gt; &#xA; &lt;li&gt;Detecting and handling outliers&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Polars-Cookbook/raw/main/Chapter04/ch04.ipynb&#34;&gt;Chapter 4: Data Transformation Techniques&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Exploring basic aggregations&lt;/li&gt; &#xA; &lt;li&gt;Using group by aggregations&lt;/li&gt; &#xA; &lt;li&gt;Aggregating values across multiple columns&lt;/li&gt; &#xA; &lt;li&gt;Computing with window functions&lt;/li&gt; &#xA; &lt;li&gt;Applying UDFs&lt;/li&gt; &#xA; &lt;li&gt;Using SQL for data transformations&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Polars-Cookbook/raw/main/Chapter05/ch05.ipynb&#34;&gt;Chapter 5: Handling Missing Values&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Identifying missing data&lt;/li&gt; &#xA; &lt;li&gt;Deleting rows and columns containing missing data&lt;/li&gt; &#xA; &lt;li&gt;Filling missing data&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Polars-Cookbook/raw/main/Chapter06/ch06.ipynb&#34;&gt;Chapter 6: Performing String Manipulations&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Filtering strings&lt;/li&gt; &#xA; &lt;li&gt;Converting strings into a Date/Datetime/Time&lt;/li&gt; &#xA; &lt;li&gt;Extracting substrings&lt;/li&gt; &#xA; &lt;li&gt;Cleaning strings&lt;/li&gt; &#xA; &lt;li&gt;Splitting strings into lists and structs&lt;/li&gt; &#xA; &lt;li&gt;Concatenating and combining strings&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Polars-Cookbook/raw/main/Chapter07/ch07.ipynb&#34;&gt;Chapter 7: Working with Nested Data Structures&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Creating lists&lt;/li&gt; &#xA; &lt;li&gt;Aggregating elements in lists&lt;/li&gt; &#xA; &lt;li&gt;Accessing and selecting elements in lists&lt;/li&gt; &#xA; &lt;li&gt;Applying logic to each element in lists&lt;/li&gt; &#xA; &lt;li&gt;Working with structs and JSON data&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Polars-Cookbook/raw/main/Chapter08/ch08.ipynb&#34;&gt;Chapter 8: Reshaping and Tidying Data&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Turning columns into rows&lt;/li&gt; &#xA; &lt;li&gt;Turning rows into columns&lt;/li&gt; &#xA; &lt;li&gt;Joining DataFrames&lt;/li&gt; &#xA; &lt;li&gt;Concatenating DataFrames&lt;/li&gt; &#xA; &lt;li&gt;Other reshaping techniques&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Polars-Cookbook/raw/main/Chapter09/ch09.ipynb&#34;&gt;Chapter 9: Time Series Analysis&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Working with date and time&lt;/li&gt; &#xA; &lt;li&gt;Applying rolling windows calculations&lt;/li&gt; &#xA; &lt;li&gt;Resampling techniques&lt;/li&gt; &#xA; &lt;li&gt;Time series forecasting with the functime library&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Polars-Cookbook/raw/main/Chapter10/ch10.ipynb&#34;&gt;Chapter 10: Interoperability With Other Python Libraries&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Converting to and from a pandas DataFrame&lt;/li&gt; &#xA; &lt;li&gt;Converting to and from NumPy arrays&lt;/li&gt; &#xA; &lt;li&gt;Interoperating with PyArrow&lt;/li&gt; &#xA; &lt;li&gt;Integration with DuckDB&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Polars-Cookbook/raw/main/Chapter11/ch11.ipynb&#34;&gt;Chapter 11: Working With Common Cloud Sources&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Amazon S3&lt;/li&gt; &#xA; &lt;li&gt;Azure Blog Storage&lt;/li&gt; &#xA; &lt;li&gt;Google Cloud Storage&lt;/li&gt; &#xA; &lt;li&gt;BigQuery&lt;/li&gt; &#xA; &lt;li&gt;Snowflake&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Polars-Cookbook/raw/main/Chapter12/ch12.ipynb&#34;&gt;Chapter 12: Testing and Debugging in Polars&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Debugging chained operations&lt;/li&gt; &#xA; &lt;li&gt;Inspecting and optimizing the query plan&lt;/li&gt; &#xA; &lt;li&gt;Testing data quality with cuallee&lt;/li&gt; &#xA; &lt;li&gt;Running unit tests with Pytest&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>