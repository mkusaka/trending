<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-06-03T01:43:20Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>nicknochnack/TFODCourse</title>
    <updated>2022-06-03T01:43:20Z</updated>
    <id>tag:github.com,2022-06-03:/nicknochnack/TFODCourse</id>
    <link href="https://github.com/nicknochnack/TFODCourse" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Tensorflow Object Detection Walkthrough&lt;/h1&gt; &#xA;&lt;p&gt;This set of Notebooks provides a complete set of code to be able to train and leverage your own custom object detection model using the Tensorflow Object Detection API. This accompanies the Tensorflow Object Detection course on my &lt;a href=&#34;https://www.youtube.com/c/nicholasrenotte&#34;&gt;YouTube channel&lt;/a&gt;. &lt;img src=&#34;https://i.imgur.com/H3tUyKM.png&#34;&gt; &lt;/p&gt;&#xA;&lt;h2&gt;Steps&lt;/h2&gt; &#xA;&lt;br&gt; &#xA;&lt;b&gt;Step 1.&lt;/b&gt; Clone this repository: https://github.com/nicknochnack/TFODCourse &#xA;&lt;br&gt;&#xA;&lt;br&gt; &#xA;&lt;b&gt;Step 2.&lt;/b&gt; Create a new virtual environment &#xA;&lt;pre&gt;&#xA;python -m venv tfod&#xA;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;b&gt;Step 3.&lt;/b&gt; Activate your virtual environment &#xA;&lt;pre&gt;&#xA;source tfod/bin/activate # Linux&#xA;.\tfod\Scripts\activate # Windows &#xA;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;b&gt;Step 4.&lt;/b&gt; Install dependencies and add virtual environment to the Python Kernel &#xA;&lt;pre&gt;&#xA;python -m pip install --upgrade pip&#xA;pip install ipykernel&#xA;python -m ipykernel install --user --name=tfodj&#xA;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;b&gt;Step 5.&lt;/b&gt; Collect images using the Notebook &#xA;&lt;a href=&#34;https://github.com/nicknochnack/TFODCourse/raw/main/1.%20Image%20Collection.ipynb&#34;&gt;1. Image Collection.ipynb&lt;/a&gt; - ensure you change the kernel to the virtual environment as shown below &#xA;&lt;img src=&#34;https://i.imgur.com/8yac6Xl.png&#34;&gt; &#xA;&lt;br&gt; &#xA;&lt;b&gt;Step 6.&lt;/b&gt; Manually divide collected images into two folders train and test. So now all folders and annotations should be split between the following two folders. &#xA;&lt;br&gt; \TFODCourse\Tensorflow\workspace\images\train&#xA;&lt;br&gt; \TFODCourse\Tensorflow\workspace\images\test &#xA;&lt;br&gt;&#xA;&lt;br&gt; &#xA;&lt;b&gt;Step 7.&lt;/b&gt; Begin training process by opening &#xA;&lt;a href=&#34;https://github.com/nicknochnack/TFODCourse/raw/main/2.%20Training%20and%20Detection.ipynb&#34;&gt;2. Training and Detection.ipynb&lt;/a&gt;, this notebook will walk you through installing Tensorflow Object Detection, making detections, saving and exporting your model. &#xA;&lt;br&gt;&#xA;&lt;br&gt; &#xA;&lt;b&gt;Step 8.&lt;/b&gt; During this process the Notebook will install Tensorflow Object Detection. You should ideally receive a notification indicating that the API has installed successfully at Step 8 with the last line stating OK. &#xA;&lt;img src=&#34;https://i.imgur.com/FSQFo16.png&#34;&gt; If not, resolve installation errors by referring to the &#xA;&lt;a href=&#34;https://github.com/nicknochnack/TFODCourse/raw/main/README.md&#34;&gt;Error Guide.md&lt;/a&gt; in this folder. &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;b&gt;Step 9.&lt;/b&gt; Once you get to step 6. Train the model, inside of the notebook, you may choose to train the model from within the notebook. I have noticed however that training inside of a separate terminal on a Windows machine you&#39;re able to display live loss metrics. &#xA;&lt;img src=&#34;https://i.imgur.com/K0wLO57.png&#34;&gt; &#xA;&lt;br&gt; &#xA;&lt;b&gt;Step 10.&lt;/b&gt; You can optionally evaluate your model inside of Tensorboard. Once the model has been trained and you have run the evaluation command under Step 7. Navigate to the evaluation folder for your trained model e.g. &#xA;&lt;pre&gt; cd Tensorlfow/workspace/models/my_ssd_mobnet/eval&lt;/pre&gt; and open Tensorboard with the following command &#xA;&lt;pre&gt;tensorboard --logdir=. &lt;/pre&gt; Tensorboard will be accessible through your browser and you will be able to see metrics including mAP - mean Average Precision, and Recall. &#xA;&lt;br&gt;</summary>
  </entry>
  <entry>
    <title>NVIDIA/NeMo</title>
    <updated>2022-06-03T01:43:20Z</updated>
    <id>tag:github.com,2022-06-03:/NVIDIA/NeMo</id>
    <link href="https://github.com/NVIDIA/NeMo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;NeMo: a toolkit for conversational AI&lt;/p&gt;&lt;hr&gt;&lt;p&gt;|status| |documentation| |license| |lgtm_grade| |lgtm_alerts| |black|&lt;/p&gt; &#xA;&lt;p&gt;.. |status| image:: &lt;a href=&#34;http://www.repostatus.org/badges/latest/active.svg&#34;&gt;http://www.repostatus.org/badges/latest/active.svg&lt;/a&gt; :target: &lt;a href=&#34;http://www.repostatus.org/#active&#34;&gt;http://www.repostatus.org/#active&lt;/a&gt; :alt: Project Status: Active â€“ The project has reached a stable, usable state and is being actively developed.&lt;/p&gt; &#xA;&lt;p&gt;.. |documentation| image:: &lt;a href=&#34;https://readthedocs.com/projects/nvidia-nemo/badge/?version=main&#34;&gt;https://readthedocs.com/projects/nvidia-nemo/badge/?version=main&lt;/a&gt; :alt: Documentation :target: &lt;a href=&#34;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/&#34;&gt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;.. |license| image:: &lt;a href=&#34;https://img.shields.io/badge/License-Apache%202.0-brightgreen.svg&#34;&gt;https://img.shields.io/badge/License-Apache%202.0-brightgreen.svg&lt;/a&gt; :target: &lt;a href=&#34;https://github.com/NVIDIA/NeMo/raw/master/LICENSE&#34;&gt;https://github.com/NVIDIA/NeMo/blob/master/LICENSE&lt;/a&gt; :alt: NeMo core license and license for collections in this repo&lt;/p&gt; &#xA;&lt;p&gt;.. |lgtm_grade| image:: &lt;a href=&#34;https://img.shields.io/lgtm/grade/python/g/NVIDIA/NeMo.svg?logo=lgtm&amp;amp;logoWidth=18&#34;&gt;https://img.shields.io/lgtm/grade/python/g/NVIDIA/NeMo.svg?logo=lgtm&amp;amp;logoWidth=18&lt;/a&gt; :target: &lt;a href=&#34;https://lgtm.com/projects/g/NVIDIA/NeMo/context:python&#34;&gt;https://lgtm.com/projects/g/NVIDIA/NeMo/context:python&lt;/a&gt; :alt: Language grade: Python&lt;/p&gt; &#xA;&lt;p&gt;.. |lgtm_alerts| image:: &lt;a href=&#34;https://img.shields.io/lgtm/alerts/g/NVIDIA/NeMo.svg?logo=lgtm&amp;amp;logoWidth=18&#34;&gt;https://img.shields.io/lgtm/alerts/g/NVIDIA/NeMo.svg?logo=lgtm&amp;amp;logoWidth=18&lt;/a&gt; :target: &lt;a href=&#34;https://lgtm.com/projects/g/NVIDIA/NeMo/alerts/&#34;&gt;https://lgtm.com/projects/g/NVIDIA/NeMo/alerts/&lt;/a&gt; :alt: Total alerts&lt;/p&gt; &#xA;&lt;p&gt;.. |black| image:: &lt;a href=&#34;https://img.shields.io/badge/code%20style-black-000000.svg&#34;&gt;https://img.shields.io/badge/code%20style-black-000000.svg&lt;/a&gt; :target: &lt;a href=&#34;https://github.com/psf/black&#34;&gt;https://github.com/psf/black&lt;/a&gt; :alt: Code style: black&lt;/p&gt; &#xA;&lt;p&gt;.. _main-readme:&lt;/p&gt; &#xA;&lt;h1&gt;&lt;strong&gt;NVIDIA NeMo&lt;/strong&gt;&lt;/h1&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;NVIDIA NeMo is a conversational AI toolkit built for researchers working on automatic speech recognition (ASR), natural language processing (NLP), and text-to-speech synthesis (TTS). The primary objective of NeMo is to help researchers from industry and academia to reuse prior work (code and pretrained models) and make it easier to create new &lt;code&gt;conversational AI models &amp;lt;https://developer.nvidia.com/conversational-ai#started&amp;gt;&lt;/code&gt;_.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Pre-trained NeMo models. &amp;lt;https://catalog.ngc.nvidia.com/models?query=nemo&amp;amp;orderBy=weightPopularDESC&amp;gt;&lt;/code&gt;_&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Introductory video. &amp;lt;https://www.youtube.com/embed/wBgpMf_KQVw&amp;gt;&lt;/code&gt;_&lt;/p&gt; &#xA;&lt;h2&gt;Key Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speech processing &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;Automatic Speech Recognition (ASR) &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/intro.html&amp;gt;&lt;/code&gt;_ &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Supported models: Jasper, QuartzNet, CitriNet, Conformer-CTC, Conformer-Transducer, ContextNet, LSTM-Transducer (RNNT), LSTM-CTC, ...&lt;/li&gt; &#xA;     &lt;li&gt;Supports CTC and Transducer/RNNT losses/decoders&lt;/li&gt; &#xA;     &lt;li&gt;Beam Search decoding&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;Language Modelling for ASR &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/asr_language_modeling.html&amp;gt;&lt;/code&gt;_: N-gram LM in fusion with Beam Search decoding, Neural Rescoring with Transformer&lt;/li&gt; &#xA;     &lt;li&gt;Streaming and Buffered ASR (CTC/Transducer) - &lt;code&gt;Chunked Inference Examples &amp;lt;https://github.com/NVIDIA/NeMo/tree/main/examples/asr/asr_chunked_inference&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Speech Classification and Speech Command Recognition &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/speech_classification/intro.html&amp;gt;&lt;/code&gt;_: MatchboxNet (Command Recognition)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Voice activity Detection (VAD) &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/asr/speech_classification/models.html#marblenet-vad&amp;gt;&lt;/code&gt;_: MarbleNet&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Speaker Recognition &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/speaker_recognition/intro.html&amp;gt;&lt;/code&gt;_: TitaNet, ECAPA_TDNN, SpeakerNet&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Speaker Diarization &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/speaker_diarization/intro.html&amp;gt;&lt;/code&gt;_: TitaNet, ECAPA_TDNN, SpeakerNet&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Pretrained models on different languages. &amp;lt;https://ngc.nvidia.com/catalog/collections/nvidia:nemo_asr&amp;gt;&lt;/code&gt;_: English, Spanish, German, Russian, Chinese, French, Italian, Polish, ...&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;NGC collection of pre-trained speech processing models. &amp;lt;https://ngc.nvidia.com/catalog/collections/nvidia:nemo_asr&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Natural Language Processing &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;NeMo Megatron pre-training of Large Language Models &amp;lt;https://developer.nvidia.com/nemo-megatron-early-access&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Neural Machine Translation (NMT) &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/machine_translation.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Punctuation and Capitalization &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/punctuation_and_capitalization.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Token classification (named entity recognition) &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/token_classification.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Text classification &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/text_classification.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Joint Intent and Slot Classification &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/joint_intent_slot.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Question answering &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/question_answering.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;GLUE benchmark &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/glue_benchmark.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Information retrieval &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/information_retrieval.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Entity Linking &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/entity_linking.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Dialogue State Tracking &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/sgd_qa.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Prompt Tuning &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/prompt_learning.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;NGC collection of pre-trained NLP models. &amp;lt;https://ngc.nvidia.com/catalog/collections/nvidia:nemo_nlp&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Speech synthesis (TTS) &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/tts/intro.html#&amp;gt;&lt;/code&gt;_ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Spectrogram generation: Tacotron2, GlowTTS, TalkNet, FastPitch, FastSpeech2, Mixer-TTS, Mixer-TTS-X&lt;/li&gt; &#xA;   &lt;li&gt;Vocoders: WaveGlow, SqueezeWave, UniGlow, MelGAN, HiFiGAN, UnivNet&lt;/li&gt; &#xA;   &lt;li&gt;End-to-end speech generation: FastPitch_HifiGan_E2E, FastSpeech2_HifiGan_E2E&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;NGC collection of pre-trained TTS models. &amp;lt;https://ngc.nvidia.com/catalog/collections/nvidia:nemo_tts&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Tools &amp;lt;https://github.com/NVIDIA/NeMo/tree/main/tools&amp;gt;&lt;/code&gt;_ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;Text Processing (text normalization and inverse text normalization) &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/text_normalization/intro.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;CTC-Segmentation tool &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/tools/ctc_segmentation.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Speech Data Explorer &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/tools/speech_data_explorer.html&amp;gt;&lt;/code&gt;_: a dash-based tool for interactive exploration of ASR/TTS datasets&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Built for speed, NeMo can utilize NVIDIA&#39;s Tensor Cores and scale out training to multiple GPUs and multiple nodes.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Python 3.8 or above&lt;/li&gt; &#xA; &lt;li&gt;Pytorch 1.10.0 or above&lt;/li&gt; &#xA; &lt;li&gt;NVIDIA GPU for training&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;.. |main| image:: &lt;a href=&#34;https://readthedocs.com/projects/nvidia-nemo/badge/?version=main&#34;&gt;https://readthedocs.com/projects/nvidia-nemo/badge/?version=main&lt;/a&gt; :alt: Documentation Status :scale: 100% :target: &lt;a href=&#34;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/&#34;&gt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;.. |stable| image:: &lt;a href=&#34;https://readthedocs.com/projects/nvidia-nemo/badge/?version=stable&#34;&gt;https://readthedocs.com/projects/nvidia-nemo/badge/?version=stable&lt;/a&gt; :alt: Documentation Status :scale: 100% :target: &lt;a href=&#34;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/&#34;&gt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;+---------+-------------+------------------------------------------------------------------------------------------------------------------------------------------+ | Version | Status | Description | +=========+=============+==========================================================================================================================================+ | Latest | |main| | &lt;code&gt;Documentation of the latest (i.e. main) branch. &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/&amp;gt;&lt;/code&gt;_ | +---------+-------------+------------------------------------------------------------------------------------------------------------------------------------------+ | Stable | |stable| | &lt;code&gt;Documentation of the stable (i.e. most recent release) branch. &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/&amp;gt;&lt;/code&gt;_ | +---------+-------------+------------------------------------------------------------------------------------------------------------------------------------------+&lt;/p&gt; &#xA;&lt;h2&gt;Tutorials&lt;/h2&gt; &#xA;&lt;p&gt;A great way to start with NeMo is by checking &lt;code&gt;one of our tutorials &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/starthere/tutorials.html&amp;gt;&lt;/code&gt;_.&lt;/p&gt; &#xA;&lt;h2&gt;Getting help with NeMo&lt;/h2&gt; &#xA;&lt;p&gt;FAQ can be found on NeMo&#39;s &lt;code&gt;Discussions board &amp;lt;https://github.com/NVIDIA/NeMo/discussions&amp;gt;&lt;/code&gt;_. You are welcome to ask questions or start discussions there.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Conda&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#xA;We recommend installing NeMo in a fresh Conda environment.&#xA;&#xA;.. code-block:: bash&#xA;&#xA;    conda create --name nemo python==3.8&#xA;    conda activate nemo&#xA;&#xA;Install PyTorch using their `configurator &amp;lt;https://pytorch.org/get-started/locally/&amp;gt;`_. &#xA;&#xA;.. code-block:: bash&#xA;&#xA;    conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch&#xA;&#xA;.. note::&#xA;&#xA;  The command used to install PyTorch may depend on your system.&#xA;&#xA;Pip&#xA;~~~&#xA;Use this installation mode if you want the latest released version.&#xA;&#xA;.. code-block:: bash&#xA;&#xA;    apt-get update &amp;amp;&amp;amp; apt-get install -y libsndfile1 ffmpeg&#xA;    pip install Cython&#xA;    pip install nemo_toolkit[&#39;all&#39;]&#xA;&#xA;.. note::&#xA;&#xA;    Depending on the shell used, you may need to use ``&#34;nemo_toolkit[all]&#34;`` instead in the above command.&#xA;&#xA;Pip from source&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Use this installation mode if you want the a version from particular GitHub branch (e.g main).&lt;/p&gt; &#xA;&lt;p&gt;.. code-block:: bash&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;apt-get update &amp;amp;&amp;amp; apt-get install -y libsndfile1 ffmpeg&#xA;pip install Cython&#xA;python -m pip install git+https://github.com/NVIDIA/NeMo.git@{BRANCH}#egg=nemo_toolkit[all]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;From source&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Use this installation mode if you are contributing to NeMo.&#xA;&#xA;.. code-block:: bash&#xA;&#xA;    apt-get update &amp;amp;&amp;amp; apt-get install -y libsndfile1 ffmpeg&#xA;    git clone https://github.com/NVIDIA/NeMo&#xA;    cd NeMo&#xA;    ./reinstall.sh&#xA;&#xA;.. note::&#xA;&#xA;    If you only want the toolkit without additional conda-based dependencies, you may replace ``reinstall.sh``&#xA;    with ``pip install -e .`` when your PWD is the root of the NeMo repository.&#xA;&#xA;RNNT&#xA;~~~~&#xA;Note that RNNT requires numba to be installed from conda.&#xA;&#xA;.. code-block:: bash&#xA;&#xA;  conda remove numba&#xA;  pip uninstall numba&#xA;  conda install -c conda-forge numba&#xA;&#xA;Megatron GPT&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Megatron GPT training requires NVIDIA Apex to be installed.&lt;/p&gt; &#xA;&lt;p&gt;.. code-block:: bash&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/NVIDIA/apex&#xA;cd apex&#xA;git checkout 9263bc8c6c16555bd55dd759f1a1b8c0cd187d10&#xA;pip install -v --disable-pip-version-check --no-cache-dir --global-option=&#34;--cpp_ext&#34; --global-option=&#34;--cuda_ext&#34; --global-option=&#34;--fast_layer_norm&#34; ./&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Docker containers:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;To build a nemo container with Dockerfile from a branch, please run &#xA;&#xA;.. code-block:: bash&#xA;&#xA;    DOCKER_BUILDKIT=1 docker build -f Dockerfile -t nemo:latest .&#xA;&#xA;&#xA;If you chose to work with main branch, we recommend using NVIDIA&#39;s PyTorch container version 22.04-py3 and then installing from GitHub.&#xA;&#xA;.. code-block:: bash&#xA;&#xA;    docker run --gpus all -it --rm -v &amp;lt;nemo_github_folder&amp;gt;:/NeMo --shm-size=8g \&#xA;    -p 8888:8888 -p 6006:6006 --ulimit memlock=-1 --ulimit \&#xA;    stack=67108864 --device=/dev/snd nvcr.io/nvidia/pytorch:22.04-py3&#xA;&#xA;Examples&#xA;--------&#xA;&#xA;Many examples can be found under `&#34;Examples&#34; &amp;lt;https://github.com/NVIDIA/NeMo/tree/stable/examples&amp;gt;`_ folder.&#xA;&#xA;&#xA;Contributing&#xA;------------&#xA;&#xA;We welcome community contributions! Please refer to the  `CONTRIBUTING.md &amp;lt;https://github.com/NVIDIA/NeMo/blob/stable/CONTRIBUTING.md&amp;gt;`_ CONTRIBUTING.md for the process.&#xA;&#xA;Publications&#xA;------------&#xA;&#xA;We provide an ever growing list of publications that utilize the NeMo framework. Please refer to `PUBLICATIONS.md &amp;lt;https://github.com/NVIDIA/NeMo/blob/main/PUBLICATIONS.md&amp;gt;`_. We welcome the addition of your own articles to this list !&#xA;&#xA;Citation&#xA;--------&#xA;&#xA;.. code-block:: bash&#xA;&#xA;  @article{kuchaiev2019nemo,&#xA;    title={Nemo: a toolkit for building ai applications using neural modules},&#xA;    author={Kuchaiev, Oleksii and Li, Jason and Nguyen, Huyen and Hrinchuk, Oleksii and Leary, Ryan and Ginsburg, Boris and Kriman, Samuel and Beliaev, Stanislav and Lavrukhin, Vitaly and Cook, Jack and others},&#xA;    journal={arXiv preprint arXiv:1909.09577},&#xA;    year={2019}&#xA;  }&#xA;&#xA;License&#xA;-------&#xA;NeMo is under `Apache 2.0 license &amp;lt;https://github.com/NVIDIA/NeMo/blob/stable/LICENSE&amp;gt;`_.&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>datawhalechina/thorough-pytorch</title>
    <updated>2022-06-03T01:43:20Z</updated>
    <id>tag:github.com,2022-06-03:/datawhalechina/thorough-pytorch</id>
    <link href="https://github.com/datawhalechina/thorough-pytorch" rel="alternate"></link>
    <summary type="html">&lt;p&gt;PyTorchå…¥é—¨æ•™ç¨‹ï¼Œåœ¨çº¿é˜…è¯»åœ°å€ï¼šhttps://datawhalechina.github.io/thorough-pytorch/&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;æ·±å…¥æµ…å‡ºPyTorch&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;åœ¨çº¿é˜…è¯»åœ°å€&lt;/strong&gt;ï¼š&lt;a href=&#34;https://datawhalechina.github.io/thorough-pytorch/&#34;&gt;https://datawhalechina.github.io/thorough-pytorch/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;é…å¥—è§†é¢‘æ•™ç¨‹&lt;/strong&gt;ï¼š&lt;a href=&#34;https://www.bilibili.com/video/BV1L44y1472Z&#34;&gt;https://www.bilibili.com/video/BV1L44y1472Z&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ä¸€ã€é¡¹ç›®åˆè¡·&lt;/h2&gt; &#xA;&lt;p&gt;PyTorchæ˜¯åˆ©ç”¨æ·±åº¦å­¦ä¹ è¿›è¡Œæ•°æ®ç§‘å­¦ç ”ç©¶çš„é‡è¦å·¥å…·ï¼Œåœ¨çµæ´»æ€§ã€å¯è¯»æ€§å’Œæ€§èƒ½ä¸Šéƒ½å…·å¤‡ç›¸å½“çš„ä¼˜åŠ¿ï¼Œè¿‘å¹´æ¥å·²æˆä¸ºå­¦æœ¯ç•Œå®ç°æ·±åº¦å­¦ä¹ ç®—æ³•æœ€å¸¸ç”¨çš„æ¡†æ¶ã€‚&lt;/p&gt; &#xA;&lt;p&gt;è€ƒè™‘åˆ°PyTorchçš„å­¦ä¹ å…¼å…·ç†è®ºå‚¨å¤‡å’ŒåŠ¨æ‰‹è®­ç»ƒï¼Œä¸¤æ‰‹éƒ½è¦æŠ“ä¸¤æ‰‹éƒ½è¦ç¡¬çš„ç‰¹ç‚¹ï¼Œæˆ‘ä»¬å¼€å‘äº†ã€Šæ·±å…¥æµ…å‡ºPyTorchã€‹è¯¾ç¨‹ï¼ŒæœŸæœ›ä»¥ç»„é˜Ÿå­¦ä¹ çš„å½¢å¼ï¼Œå¸®åŠ©å¤§å®¶ä»å…¥é—¨åˆ°ç†Ÿç»ƒæŒæ¡PyTorchå·¥å…·ï¼Œè¿›è€Œå®ç°è‡ªå·±çš„æ·±åº¦å­¦ä¹ ç®—æ³•ã€‚&lt;/p&gt; &#xA;&lt;p&gt;æˆ‘ä»¬çš„æ„¿æ™¯æ˜¯ï¼šé€šè¿‡ç»„é˜Ÿå­¦ä¹ ï¼Œå¤§å®¶èƒ½å¤ŸæŒæ¡ç”±æµ…å…¥æ·±åœ°PyTorchçš„åŸºæœ¬çŸ¥è¯†å’Œå†…å®¹ï¼Œç»è¿‡è‡ªå·±çš„åŠ¨æ‰‹å®è·µåŠ æ·±æ“ä½œçš„ç†Ÿç»ƒåº¦ã€‚åŒæ—¶é€šè¿‡é¡¹ç›®å®æˆ˜ï¼Œå……åˆ†é”»ç‚¼ç¼–ç¨‹èƒ½åŠ›ï¼ŒæŒæ¡PyTorchè¿›è¡Œæ·±åº¦å­¦ä¹ çš„åŸºæœ¬æµç¨‹ï¼Œæå‡è§£å†³å®é™…é—®é¢˜çš„èƒ½åŠ›ã€‚&lt;/p&gt; &#xA;&lt;p&gt;å­¦ä¹ çš„å…ˆä¿®è¦æ±‚æ˜¯ï¼Œä¼šä½¿ç”¨Pythonç¼–ç¨‹ï¼Œäº†è§£åŒ…æ‹¬ç¥ç»ç½‘ç»œåœ¨å†…çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œå‹¤äºåŠ¨æ‰‹å®è·µã€‚&lt;/p&gt; &#xA;&lt;p&gt;ã€Šæ·±å…¥æµ…å‡ºPyTorchã€‹æ˜¯ä¸€ä¸ªç³»åˆ—ï¼Œä¸€å…±æœ‰ä¸‰ä¸ªéƒ¨åˆ†ã€‚å·²ç»ä¸Šçº¿çš„æ˜¯æœ¬ç³»åˆ—çš„ç¬¬ä¸€ã€äºŒéƒ¨åˆ†ï¼Œåç»­ä¼šä¸æ–­æ›´æ–°ã€Šæ·±å…¥æµ…å‡ºPyTorchã€‹ï¼ˆä¸‹ï¼‰ï¼Œç»™å‡ºæ›´è´´åˆå®é™…åº”ç”¨çš„å®æˆ˜æ¡ˆä¾‹ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;äºŒã€å†…å®¹è®¾ç½®ï¼ˆå·²ä¸Šçº¿éƒ¨åˆ†ï¼‰&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ç¬¬ä¸€ç« ï¼šPyTorchçš„ç®€ä»‹å’Œå®‰è£… &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;PyTorchç®€ä»‹&lt;/li&gt; &#xA;   &lt;li&gt;PyTorchçš„å®‰è£…&lt;/li&gt; &#xA;   &lt;li&gt;PyTorchç›¸å…³èµ„æºç®€ä»‹&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;ç¬¬äºŒç« ï¼šPyTorchåŸºç¡€çŸ¥è¯† &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;å¼ é‡åŠå…¶è¿ç®—&lt;/li&gt; &#xA;   &lt;li&gt;è‡ªåŠ¨æ±‚å¯¼ç®€ä»‹&lt;/li&gt; &#xA;   &lt;li&gt;å¹¶è¡Œè®¡ç®—ã€CUDAå’ŒcuDNNç®€ä»‹&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;ç¬¬ä¸‰ç« ï¼šPyTorchçš„ä¸»è¦ç»„æˆæ¨¡å— &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;æ€è€ƒï¼šå®Œæˆä¸€å¥—æ·±åº¦å­¦ä¹ æµç¨‹éœ€è¦å“ªäº›å…³é”®ç¯èŠ‚&lt;/li&gt; &#xA;   &lt;li&gt;åŸºæœ¬é…ç½®&lt;/li&gt; &#xA;   &lt;li&gt;æ•°æ®è¯»å…¥&lt;/li&gt; &#xA;   &lt;li&gt;æ¨¡å‹æ„å»º&lt;/li&gt; &#xA;   &lt;li&gt;æŸå¤±å‡½æ•°&lt;/li&gt; &#xA;   &lt;li&gt;ä¼˜åŒ–å™¨&lt;/li&gt; &#xA;   &lt;li&gt;è®­ç»ƒå’Œè¯„ä¼°&lt;/li&gt; &#xA;   &lt;li&gt;å¯è§†åŒ–&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;ç¬¬å››ç« ï¼šPyTorchåŸºç¡€å®æˆ˜ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;åŸºç¡€å®æˆ˜â€”â€”Fashion-MNISTæ—¶è£…åˆ†ç±»&lt;/li&gt; &#xA;   &lt;li&gt;åŸºç¡€å®æˆ˜â€”â€”æœè”¬åˆ†ç±»å®æˆ˜ï¼ˆnotebookï¼‰&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;ç¬¬äº”ç« ï¼šPyTorchæ¨¡å‹å®šä¹‰ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;æ¨¡å‹å®šä¹‰æ–¹å¼&lt;/li&gt; &#xA;   &lt;li&gt;åˆ©ç”¨æ¨¡å‹å—å¿«é€Ÿæ­å»ºå¤æ‚ç½‘ç»œ&lt;/li&gt; &#xA;   &lt;li&gt;æ¨¡å‹ä¿®æ”¹&lt;/li&gt; &#xA;   &lt;li&gt;æ¨¡å‹ä¿å­˜ä¸è¯»å–&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;ç¬¬å…­ç« ï¼šPyTorchè¿›é˜¶è®­ç»ƒæŠ€å·§ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;è‡ªå®šä¹‰æŸå¤±å‡½æ•°&lt;/li&gt; &#xA;   &lt;li&gt;åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡&lt;/li&gt; &#xA;   &lt;li&gt;æ¨¡å‹å¾®è°ƒ&lt;/li&gt; &#xA;   &lt;li&gt;åŠç²¾åº¦è®­ç»ƒ&lt;/li&gt; &#xA;   &lt;li&gt;æ•°æ®æ‰©å……&lt;/li&gt; &#xA;   &lt;li&gt;è¶…å‚æ•°çš„ä¿®æ”¹åŠä¿å­˜&lt;/li&gt; &#xA;   &lt;li&gt;PyTorchæ¨¡å‹å®šä¹‰ä¸è¿›é˜¶è®­ç»ƒæŠ€å·§&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;ç¬¬ä¸ƒç« ï¼šPyTorchå¯è§†åŒ– &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;å¯è§†åŒ–ç½‘ç»œç»“æ„&lt;/li&gt; &#xA;   &lt;li&gt;å¯è§†åŒ–CNNå·ç§¯å±‚&lt;/li&gt; &#xA;   &lt;li&gt;ä½¿ç”¨TensorBoardå¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;ç¬¬å…«ç« ï¼šPyTorchç”Ÿæ€ç®€ä»‹ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ç®€ä»‹&lt;/li&gt; &#xA;   &lt;li&gt;å›¾åƒâ€”torchvision&lt;/li&gt; &#xA;   &lt;li&gt;è§†é¢‘â€”PyTorchVideo&lt;/li&gt; &#xA;   &lt;li&gt;æ–‡æœ¬â€”torchtext&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;ç¬¬ä¹ç« ï¼šå¸¸è§ç½‘ç»œä»£ç çš„è§£è¯» &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;è®¡ç®—æœºè§†è§‰ &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;å›¾åƒåˆ†ç±»&lt;/li&gt; &#xA;     &lt;li&gt;ç›®æ ‡æ£€æµ‹&lt;/li&gt; &#xA;     &lt;li&gt;å›¾åƒåˆ†å‰²&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;è‡ªç„¶è¯­è¨€å¤„ç†&lt;/li&gt; &#xA;   &lt;li&gt;éŸ³é¢‘å¤„ç†&lt;/li&gt; &#xA;   &lt;li&gt;è§†é¢‘å¤„ç†&lt;/li&gt; &#xA;   &lt;li&gt;å…¶ä»–&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ä¸‰ã€äººå‘˜å®‰æ’&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;æˆå‘˜&amp;nbsp;&lt;/th&gt; &#xA;   &lt;th&gt;ä¸ªäººç®€ä»‹&lt;/th&gt; &#xA;   &lt;th&gt;ä¸ªäººä¸»é¡µ&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;æå˜‰éª&lt;/td&gt; &#xA;   &lt;td&gt;DataWhaleæˆå‘˜ï¼Œæ¸…åå¤§å­¦ç ”ç©¶ç”Ÿ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.zhihu.com/people/li-jia-qi-16-9/posts&#34;&gt;https://www.zhihu.com/people/li-jia-qi-16-9/posts&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ç‰›å¿—åº·&lt;/td&gt; &#xA;   &lt;td&gt;DataWhaleæˆå‘˜ï¼Œè¥¿å®‰ç”µå­ç§‘æŠ€å¤§å­¦æœ¬ç§‘ç”Ÿ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.zhihu.com/people/obeah-82&#34;&gt;https://www.zhihu.com/people/obeah-82&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;åˆ˜æ´‹&lt;/td&gt; &#xA;   &lt;td&gt;Datawhaleæˆå‘˜ï¼Œä¸­å›½ç§‘å­¦é™¢æ•°å­¦ä¸ç³»ç»Ÿç§‘å­¦ç ”ç©¶æ‰€ç ”ç©¶ç”Ÿ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.zhihu.com/people/ming-ren-19-34/asks&#34;&gt;https://www.zhihu.com/people/ming-ren-19-34/asks&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;é™ˆå®‰ä¸œ&lt;/td&gt; &#xA;   &lt;td&gt;DataWhaleæˆå‘˜ï¼Œä¸­å¤®æ°‘æ—å¤§å­¦ç ”ç©¶ç”Ÿ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://andongblue.github.io/chenandong.github.io/&#34;&gt;https://andongblue.github.io/chenandong.github.io/&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;æ•™ç¨‹è´¡çŒ®æƒ…å†µï¼ˆå·²ä¸Šçº¿è¯¾ç¨‹å†…å®¹ï¼‰ï¼š&lt;/p&gt; &#xA;&lt;p&gt;æå˜‰éªï¼šç¬¬ä¸‰ç« ï¼›ç¬¬å››ç« ï¼›ç¬¬äº”ç« ï¼›ç¬¬å…­ç« ï¼›ç¬¬ä¸ƒç« ï¼›ç¬¬å…«ç« ï¼›å†…å®¹æ•´åˆ&lt;/p&gt; &#xA;&lt;p&gt;ç‰›å¿—åº·ï¼šç¬¬ä¸€ç« ï¼›ç¬¬ä¸‰ç« ï¼›ç¬¬å…­ç« ï¼›ç¬¬ä¸ƒç« ï¼›ç¬¬å…«ç« ï¼Œæ–‡æ¡£éƒ¨ç½²&lt;/p&gt; &#xA;&lt;p&gt;åˆ˜æ´‹ï¼šç¬¬äºŒç« ï¼›ç¬¬ä¸‰ç« &lt;/p&gt; &#xA;&lt;p&gt;é™ˆå®‰ä¸œï¼šç¬¬äºŒç« ï¼›ç¬¬ä¸‰ç« ï¼›ç¬¬ä¸ƒç« &lt;/p&gt; &#xA;&lt;h2&gt;å››ã€ è¯¾ç¨‹ç¼–æ’ä¸ä½¿ç”¨æ–¹æ³•&lt;/h2&gt; &#xA;&lt;p&gt;éƒ¨åˆ†ç« èŠ‚ç›´æ’­è®²è§£è¯·è§‚çœ‹Bç«™å›æ”¾ï¼ˆæŒç»­æ›´æ–°ï¼‰ï¼š&lt;a href=&#34;https://www.bilibili.com/video/BV1L44y1472Z&#34;&gt;https://www.bilibili.com/video/BV1L44y1472Z&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;è¯¾ç¨‹ç¼–æ’ï¼š æ·±å…¥æµ…å‡ºPyTorchåˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼šPyTorchæ·±åº¦å­¦ä¹ åŸºç¡€çŸ¥è¯†ã€PyTorchè¿›é˜¶æ“ä½œã€PyTorchæ¡ˆä¾‹åˆ†æã€‚&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ä½¿ç”¨æ–¹æ³•:&lt;/p&gt; &lt;p&gt;æˆ‘ä»¬çš„è¯¾ç¨‹å†…å®¹éƒ½ä»¥markdownæ ¼å¼æˆ–jupyter notebookçš„å½¢å¼ä¿å­˜åœ¨æœ¬ä»“åº“å†…ã€‚é™¤äº†å¤šçœ‹åŠ æ·±è¯¾ç¨‹å†…å®¹çš„ç†è§£å¤–ï¼Œæœ€é‡è¦çš„è¿˜æ˜¯åŠ¨æ‰‹ç»ƒä¹ ã€ç»ƒä¹ ã€ç»ƒä¹ &lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ç»„é˜Ÿå­¦ä¹ å®‰æ’:&lt;/p&gt; &lt;p&gt;ç¬¬ä¸€éƒ¨åˆ†ï¼šç¬¬ä¸€ç« åˆ°ç¬¬å››ç« ï¼Œå­¦ä¹ å‘¨æœŸï¼š10å¤©ï¼›&lt;/p&gt; &lt;p&gt;ç¬¬äºŒéƒ¨åˆ†ï¼šç¬¬äº”ç« åˆ°ç¬¬å…«ç« ï¼Œå­¦ä¹ å‘¨æœŸï¼š11å¤©&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;äº”ã€å…³äºè´¡çŒ®&lt;/h2&gt; &#xA;&lt;p&gt;æœ¬é¡¹ç›®ä½¿ç”¨&lt;code&gt;Forking&lt;/code&gt;å·¥ä½œæµï¼Œå…·ä½“å‚è€ƒ&lt;a href=&#34;https://www.atlassian.com/git/tutorials/comparing-workflows/forking-workflow&#34;&gt;atlassianæ–‡æ¡£&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;å¤§è‡´æ­¥éª¤å¦‚ä¸‹ï¼š&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;åœ¨GitHubä¸ŠForkæœ¬ä»“åº“&lt;/li&gt; &#xA; &lt;li&gt;Clone Forkåçš„ä¸ªäººä»“åº“&lt;/li&gt; &#xA; &lt;li&gt;è®¾ç½®&lt;code&gt;upstream&lt;/code&gt;ä»“åº“åœ°å€ï¼Œå¹¶ç¦ç”¨&lt;code&gt;push&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;ä½¿ç”¨åˆ†æ”¯å¼€å‘ï¼Œè¯¾ç¨‹åˆ†æ”¯åä¸º&lt;code&gt;lecture{#NO}&lt;/code&gt;ï¼Œ&lt;code&gt;#NO&lt;/code&gt;ä¿æŒä¸¤ä½ï¼Œå¦‚&lt;code&gt;lecture07&lt;/code&gt;ï¼Œå¯¹åº”è¯¾ç¨‹ç›®å½•&lt;/li&gt; &#xA; &lt;li&gt;PRä¹‹å‰ä¿æŒä¸åŸå§‹ä»“åº“çš„åŒæ­¥ï¼Œä¹‹åå‘èµ·PRè¯·æ±‚&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;å‘½ä»¤ç¤ºä¾‹ï¼š&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# fork&#xA;# clone&#xA;git clone git@github.com:USERNAME/thorough-pytorch.git&#xA;# set upstream&#xA;git remote add upstream git@github.com:datawhalechina/thorough-pytorch.git&#xA;# disable upstream push&#xA;git remote set-url --push upstream DISABLE&#xA;# verify&#xA;git remote -v&#xA;# some sample output:&#xA;# origin&#x9;git@github.com:NoFish-528/thorough-pytorch.git (fetch)&#xA;# origin&#x9;git@github.com:NoFish-528/thorough-pytorch.git (push)&#xA;# upstream&#x9;git@github.com:datawhalechina/thorough-pytorch.git (fetch)&#xA;# upstream&#x9;DISABLE (push)&#xA;# do your work&#xA;git checkout -b lecture07&#xA;# edit and commit and push your changes&#xA;git push -u origin lecture07&#xA;# keep your fork up to date&#xA;## fetch upstream main and merge with forked main branch&#xA;git fetch upstream&#xA;git checkout main&#xA;git merge upstream/main&#xA;## rebase brach and force push&#xA;git checkout lecture07&#xA;git rebase main&#xA;git push -f&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Commit Message&lt;/h3&gt; &#xA;&lt;p&gt;æäº¤ä¿¡æ¯ä½¿ç”¨å¦‚ä¸‹æ ¼å¼ï¼š&lt;code&gt;&amp;lt;type&amp;gt;: &amp;lt;short summary&amp;gt;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;type&amp;gt;: &amp;lt;short summary&amp;gt;&#xA;  â”‚            â”‚&#xA;  â”‚            â””â”€â«¸ Summary in present tense. Not capitalized. No period at the end.&#xA;  â”‚&#xA;  â””â”€â«¸ Commit Type: lecture{#NO}|others&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;others&lt;/code&gt;åŒ…æ‹¬éè¯¾ç¨‹ç›¸å…³çš„æ”¹åŠ¨ï¼Œå¦‚æœ¬&lt;code&gt;README.md&lt;/code&gt;ä¸­çš„å˜åŠ¨ï¼Œ&lt;code&gt;.gitignore&lt;/code&gt;çš„è°ƒæ•´ç­‰ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;å…­ã€æ›´æ–°è®¡åˆ’&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;å†…å®¹&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;æ›´æ–°æ—¶é—´&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;å†…å®¹&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;æ¨¡å‹åˆå§‹åŒ–&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;âœ…&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;torch.nn.init&lt;/code&gt;çš„ä½¿ç”¨&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;visdomå¯è§†åŒ–&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2022.4.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;Visdom&lt;/code&gt;çš„ä½¿ç”¨&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;apex&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2022.5.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;apexçš„ç®€ä»‹å’Œä½¿ç”¨&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;è¶…å‚æ•°çš„ä¿å­˜&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;âœ…&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ä½¿ç”¨argparseè¿›è¡Œå‚æ•°çš„ä¿®æ”¹&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;onnx&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2022.5.31&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;è®²è¿°ONNXæ ¼å¼å’Œå®æˆ˜ä¾‹å­&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;æ¨¡å‹éƒ¨ç½²&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Flaskéƒ¨ç½²PyTorchæ¨¡å‹&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;TorchScript&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;TorchScript&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;æ•°æ®å¢å¼º&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;âœ…&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;imgaugçš„ä½¿ç”¨&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;å¹¶è¡Œè®­ç»ƒ&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;å¹¶è¡Œè®­ç»ƒ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;æ¨¡å‹é¢„è®­ç»ƒ - torchhub&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2022.4.16&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;torchhubçš„ç®€ä»‹å’Œä½¿ç”¨æ–¹æ³•&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;æ¨¡å‹é¢„è®­ç»ƒ - timm&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;âœ…&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;timmé¢„è®­ç»ƒæ¨¡å‹çš„ä½¿ç”¨æ–¹æ³•&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;æ¨¡å‹é¢„è®­ç»ƒ - openmmlab&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2022.4.27&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;openmmlabç³»åˆ—çš„ä½¿ç”¨&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ç›®æ ‡æ£€æµ‹ - yoloç³»åˆ—&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yoloç³»åˆ—ä»‹ç»ä¸trickå®ç°&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ç›®æ ‡æ£€æµ‹ - SSD&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SSDçš„ç®€ä»‹å’Œå®ç°&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ç›®æ ‡æ£€æµ‹ - RCNNç³»åˆ—&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Fast-RCNN &amp;amp; Mask-RCNN&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ç›®æ ‡æ£€æµ‹ - DETR&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;DETRçš„å®ç°&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;å›¾åƒåˆ†ç±» - GoogLeNet&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2022.5.11&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;GoogLeNetçš„ä»‹ç»ä¸å®ç°&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;å›¾åƒåˆ†ç±» - Vision transformer&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2022.5.18&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Vitä»‹ç»ä¸å®ç°&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;å›¾åƒåˆ†ç±» - MobileNetç³»åˆ—&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2022.4æœˆ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;MobileNetç³»åˆ—ä»‹ç»ä¸å®ç°&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;å›¾åƒåˆ†ç±» - GhostNet&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2022.4æœˆ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;GhostNetä»£ç è®²è§£&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ç”Ÿæˆå¼å¯¹æŠ—ç½‘ç»œ - ç”Ÿæˆæ‰‹å†™æ•°å­—å®æˆ˜&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2022.5.25&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ç”Ÿæˆæ•°å­—å¹¶å¯è§†åŒ–&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ç”Ÿæˆå¼å¯¹æŠ—ç½‘ç»œ - DCGAN&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;é£æ ¼è¿ç§» - StyleGAN&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ç”Ÿæˆç½‘ç»œ - VAE&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;å›¾åƒåˆ†å‰² Deeplabç³»åˆ—&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Deeplabç³»åˆ—ä»£ç è®²è§£&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;è‡ªç„¶è¯­è¨€å¤„ç† LSTM&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;LSTMæƒ…æ„Ÿåˆ†æå®æˆ˜&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;è‡ªç„¶è¯­è¨€å¤„ç† RNN&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;RNNåå­—åˆ†ç±»&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;è‡ªç„¶è¯­è¨€å¤„ç† Transformer&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;è‡ªç„¶è¯­è¨€å¤„ç† BERT&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;è§†é¢‘&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;å¾…å®š&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;éŸ³é¢‘&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;å¾…å®š&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;è‡ªå®šä¹‰CUDAæ‰©å±•å’Œç®—å­&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;ä¸ƒã€é¸£è°¢ä¸åé¦ˆ&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;éå¸¸æ„Ÿè°¢DataWhaleæˆå‘˜ å¶å‰å¤ @&lt;a href=&#34;https://github.com/PureBuckwheat&#34;&gt;PureBuckwheat&lt;/a&gt; å’Œ èƒ¡é”é”‹ @&lt;a href=&#34;https://github.com/Relph1119&#34;&gt;Relph1119&lt;/a&gt; å¯¹æ–‡æ¡£çš„ç»†è‡´æ ¡å¯¹ï¼&lt;/li&gt; &#xA; &lt;li&gt;å¦‚æœæœ‰ä»»ä½•æƒ³æ³•å¯ä»¥è”ç³»æˆ‘ä»¬DataWhaleä¹Ÿæ¬¢è¿å¤§å®¶å¤šå¤šæå‡ºissueã€‚&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;å…«ã€å…³æ³¨æˆ‘ä»¬&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/datawhalechina/easy-rl/master/docs/res/qrcode.jpeg&#34; width=&#34;250&#34; height=&#34;270&#34; alt=&#34;Datawhaleæ˜¯ä¸€ä¸ªä¸“æ³¨AIé¢†åŸŸçš„å¼€æºç»„ç»‡ï¼Œä»¥â€œfor the learnerï¼Œå’Œå­¦ä¹ è€…ä¸€èµ·æˆé•¿â€ä¸ºæ„¿æ™¯ï¼Œæ„å»ºå¯¹å­¦ä¹ è€…æœ€æœ‰ä»·å€¼çš„å¼€æºå­¦ä¹ ç¤¾åŒºã€‚å…³æ³¨æˆ‘ä»¬ï¼Œä¸€èµ·å­¦ä¹ æˆé•¿ã€‚&#34;&gt;&#xA;&lt;/div&gt; &#xA;&lt;h2&gt;LICENSE&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a rel=&#34;license&#34; href=&#34;http://creativecommons.org/licenses/by-nc-sa/4.0/&#34;&gt;&lt;img alt=&#34;çŸ¥è¯†å…±äº«è®¸å¯åè®®&#34; style=&#34;border-width:0&#34; src=&#34;https://img.shields.io/badge/license-CC%20BY--NC--SA%204.0-lightgrey&#34;&gt;&lt;/a&gt;&lt;br&gt;æœ¬ä½œå“é‡‡ç”¨&lt;a rel=&#34;license&#34; href=&#34;http://creativecommons.org/licenses/by-nc-sa/4.0/&#34;&gt;çŸ¥è¯†å…±äº«ç½²å-éå•†ä¸šæ€§ä½¿ç”¨-ç›¸åŒæ–¹å¼å…±äº« 4.0 å›½é™…è®¸å¯åè®®&lt;/a&gt;è¿›è¡Œè®¸å¯ã€‚&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>aws/amazon-sagemaker-examples</title>
    <updated>2022-06-03T01:43:20Z</updated>
    <id>tag:github.com,2022-06-03:/aws/amazon-sagemaker-examples</id>
    <link href="https://github.com/aws/amazon-sagemaker-examples" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Example ğŸ““ Jupyter notebooks that demonstrate how to build, train, and deploy machine learning models using ğŸ§  Amazon SageMaker.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://github.com/aws/amazon-sagemaker-examples/raw/main/_static/sagemaker-banner.png&#34; alt=&#34;SageMaker&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Amazon SageMaker Examples&lt;/h1&gt; &#xA;&lt;p&gt;Example Jupyter notebooks that demonstrate how to build, train, and deploy machine learning models using Amazon SageMaker.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;ğŸ“š&lt;/span&gt; Background&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://aws.amazon.com/sagemaker/&#34;&gt;Amazon SageMaker&lt;/a&gt; is a fully managed service for data science and machine learning (ML) workflows. You can use Amazon SageMaker to simplify the process of building, training, and deploying ML models.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://sagemaker-examples.readthedocs.io/en/latest/&#34;&gt;SageMaker example notebooks&lt;/a&gt; are Jupyter notebooks that demonstrate the usage of Amazon SageMaker.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;ğŸ› &lt;/span&gt; Setup&lt;/h2&gt; &#xA;&lt;p&gt;The quickest setup to run example notebooks includes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;An &lt;a href=&#34;http://docs.aws.amazon.com/sagemaker/latest/dg/gs-account.html&#34;&gt;AWS account&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Proper &lt;a href=&#34;http://docs.aws.amazon.com/sagemaker/latest/dg/authentication-and-access-control.html&#34;&gt;IAM User and Role&lt;/a&gt; setup&lt;/li&gt; &#xA; &lt;li&gt;An &lt;a href=&#34;http://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html&#34;&gt;Amazon SageMaker Notebook Instance&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;An &lt;a href=&#34;http://docs.aws.amazon.com/sagemaker/latest/dg/gs-config-permissions.html&#34;&gt;S3 bucket&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;span&gt;ğŸ’»&lt;/span&gt; Usage&lt;/h2&gt; &#xA;&lt;p&gt;These example notebooks are automatically loaded into SageMaker Notebook Instances. They can be accessed by clicking on the &lt;code&gt;SageMaker Examples&lt;/code&gt; tab in Jupyter or the SageMaker logo in JupyterLab.&lt;/p&gt; &#xA;&lt;p&gt;Although most examples utilize key Amazon SageMaker functionality like distributed, managed training or real-time hosted endpoints, these notebooks can be run outside of Amazon SageMaker Notebook Instances with minimal modification (updating IAM role definition and installing the necessary libraries).&lt;/p&gt; &#xA;&lt;p&gt;As of February 7, 2022, the default branch is named &#34;main&#34;. See our &lt;a href=&#34;https://github.com/aws/amazon-sagemaker-examples/discussions/3131&#34;&gt;announcement&lt;/a&gt; for details and how to update your existing clone.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;ğŸ““&lt;/span&gt; Examples&lt;/h2&gt; &#xA;&lt;h3&gt;Introduction to Ground Truth Labeling Jobs&lt;/h3&gt; &#xA;&lt;p&gt;These examples provide quick walkthroughs to get you up and running with the labeling job workflow for Amazon SageMaker Ground Truth.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/ground_truth_labeling_jobs/bring_your_own_model_for_sagemaker_labeling_workflows_with_active_learning&#34;&gt;Bring your own model for SageMaker labeling workflows with active learning&lt;/a&gt; is an end-to-end example that shows how to bring your custom training, inference logic and active learning to the Amazon SageMaker ecosystem.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/ground_truth_labeling_jobs/from_unlabeled_data_to_deployed_machine_learning_model_ground_truth_demo_image_classification&#34;&gt;From Unlabeled Data to a Deployed Machine Learning Model: A SageMaker Ground Truth Demonstration for Image Classification&lt;/a&gt; is an end-to-end example that starts with an unlabeled dataset, labels it using the Ground Truth API, analyzes the results, trains an image classification neural net using the annotated dataset, and finally uses the trained model to perform batch and online inference.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/ground_truth_labeling_jobs/ground_truth_object_detection_tutorial&#34;&gt;Ground Truth Object Detection Tutorial&lt;/a&gt; is a similar end-to-end example but for an object detection task.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/ground_truth_labeling_jobs/data_analysis_of_ground_truth_image_classification_output&#34;&gt;Basic Data Analysis of an Image Classification Output Manifest&lt;/a&gt; presents charts to visualize the number of annotations for each class, differentiating between human annotations and automatic labels (if your job used auto-labeling). It also displays sample images in each class, and creates a pdf which concisely displays the full results.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/ground_truth_labeling_jobs/object_detection_augmented_manifest_training&#34;&gt;Training a Machine Learning Model Using an Output Manifest&lt;/a&gt; introduces the concept of an &#34;augmented manifest&#34; and demonstrates that the output file of a labeling job can be immediately used as the input file to train a SageMaker machine learning model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/ground_truth_labeling_jobs/annotation_consolidation&#34;&gt;Annotation Consolidation&lt;/a&gt; demonstrates Amazon SageMaker Ground Truth annotation consolidation techniques for image classification for a completed labeling job.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Introduction to Applying Machine Learning&lt;/h3&gt; &#xA;&lt;p&gt;These examples provide a gentle introduction to machine learning concepts as they are applied in practical use cases across a variety of sectors.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_applying_machine_learning/xgboost_customer_churn&#34;&gt;Predicting Customer Churn&lt;/a&gt; uses customer interaction and service usage data to find those most likely to churn, and then walks through the cost/benefit trade-offs of providing retention incentives. This uses Amazon SageMaker&#39;s implementation of &lt;a href=&#34;https://github.com/dmlc/xgboost&#34;&gt;XGBoost&lt;/a&gt; to create a highly predictive model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_applying_machine_learning/breast_cancer_prediction&#34;&gt;Cancer Prediction&lt;/a&gt; predicts Breast Cancer based on features derived from images, using SageMaker&#39;s Linear Learner.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_applying_machine_learning/ensemble_modeling&#34;&gt;Ensembling&lt;/a&gt; predicts income using two Amazon SageMaker models to show the advantages in ensembling.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_applying_machine_learning/video_game_sales&#34;&gt;Video Game Sales&lt;/a&gt; develops a binary prediction model for the success of video games based on review scores.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_applying_machine_learning/gluon_recommender_system&#34;&gt;MXNet Gluon Recommender System&lt;/a&gt; uses neural network embeddings for non-linear matrix factorization to predict user movie ratings on Amazon digital reviews.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_applying_machine_learning/fair_linear_learner&#34;&gt;Fair Linear Learner&lt;/a&gt; is an example of an effective way to create fair linear models with respect to sensitive features.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_applying_machine_learning/US-census_population_segmentation_PCA_Kmeans&#34;&gt;Population Segmentation of US Census Data using PCA and Kmeans&lt;/a&gt; analyzes US census data and reduces dimensionality using PCA then clusters US counties using KMeans to identify segments of similar counties.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_applying_machine_learning/object2vec_document_embedding&#34;&gt;Document Embedding using Object2Vec&lt;/a&gt; is an example to embed a large collection of documents in a common low-dimensional space, so that the semantic distances between these documents are preserved.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_applying_machine_learning/deepar_chicago_traffic_violations&#34;&gt;Traffic violations forecasting using DeepAR&lt;/a&gt; is an example to use daily traffic violation data to predict pattern and seasonality to use Amazon DeepAR alogorithm.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;SageMaker Automatic Model Tuning&lt;/h3&gt; &#xA;&lt;p&gt;These examples introduce SageMaker&#39;s hyperparameter tuning functionality which helps deliver the best possible predictions by running a large number of training jobs to determine which hyperparameter values are the most impactful.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/hyperparameter_tuning/xgboost_direct_marketing&#34;&gt;XGBoost Tuning&lt;/a&gt; shows how to use SageMaker hyperparameter tuning to improve your model fit.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/hyperparameter_tuning/blazingtext_text_classification_20_newsgroups&#34;&gt;BlazingText Tuning&lt;/a&gt; shows how to use SageMaker hyperparameter tuning with the BlazingText built-in algorithm and 20_newsgroups dataset..&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/hyperparameter_tuning/tensorflow_mnist&#34;&gt;TensorFlow Tuning&lt;/a&gt; shows how to use SageMaker hyperparameter tuning with the pre-built TensorFlow container and MNIST dataset.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/hyperparameter_tuning/mxnet_mnist&#34;&gt;MXNet Tuning&lt;/a&gt; shows how to use SageMaker hyperparameter tuning with the pre-built MXNet container and MNIST dataset.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/hyperparameter_tuning/huggingface_multiclass_text_classification_20_newsgroups&#34;&gt;HuggingFace Tuning&lt;/a&gt; shows how to use SageMaker hyperparameter tuning with the pre-built HuggingFace container and 20_newsgroups dataset.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/hyperparameter_tuning/keras_bring_your_own&#34;&gt;Keras BYO Tuning&lt;/a&gt; shows how to use SageMaker hyperparameter tuning with a custom container running a Keras convolutional network on CIFAR-10 data.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/hyperparameter_tuning/r_bring_your_own&#34;&gt;R BYO Tuning&lt;/a&gt; shows how to use SageMaker hyperparameter tuning with the custom container from the &lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/r_bring_your_own&#34;&gt;Bring Your Own R Algorithm&lt;/a&gt; example.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/hyperparameter_tuning/analyze_results&#34;&gt;Analyzing Results&lt;/a&gt; is a shared notebook that can be used after each of the above notebooks to provide analysis on how training jobs with different hyperparameters performed.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;SageMaker Autopilot&lt;/h3&gt; &#xA;&lt;p&gt;These examples introduce SageMaker Autopilot. Autopilot automatically performs feature engineering, model selection, model tuning (hyperparameter optimization) and allows you to directly deploy the best model to an endpoint to serve inference requests.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/autopilot/&#34;&gt;Customer Churn AutoML&lt;/a&gt; shows how to use SageMaker Autopilot to automatically train a model for the &lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_applying_machine_learning/xgboost_customer_churn&#34;&gt;Predicting Customer Churn&lt;/a&gt; task.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/autopilot/&#34;&gt;Targeted Direct Marketing AutoML&lt;/a&gt; shows how to use SageMaker Autopilot to automatically train a model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-autopilot/housing_prices&#34;&gt;Housing Prices AutoML&lt;/a&gt; shows how to use SageMaker Autopilot for a linear regression problem (predict housing prices).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Introduction to Amazon Algorithms&lt;/h3&gt; &#xA;&lt;p&gt;These examples provide quick walkthroughs to get you up and running with Amazon SageMaker&#39;s custom developed algorithms. Most of these algorithms can train on distributed hardware, scale incredibly well, and are faster and cheaper than popular alternatives.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-python-sdk/1P_kmeans_highlevel&#34;&gt;k-means&lt;/a&gt; is our introductory example for Amazon SageMaker. It walks through the process of clustering MNIST images of handwritten digits using Amazon SageMaker k-means.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/factorization_machines_mnist&#34;&gt;Factorization Machines&lt;/a&gt; showcases Amazon SageMaker&#39;s implementation of the algorithm to predict whether a handwritten digit from the MNIST dataset is a 0 or not using a binary classifier.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/lda_topic_modeling&#34;&gt;Latent Dirichlet Allocation (LDA)&lt;/a&gt; introduces topic modeling using Amazon SageMaker Latent Dirichlet Allocation (LDA) on a synthetic dataset.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/linear_learner_mnist&#34;&gt;Linear Learner&lt;/a&gt; predicts whether a handwritten digit from the MNIST dataset is a 0 or not using a binary classifier from Amazon SageMaker Linear Learner.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/ntm_synthetic&#34;&gt;Neural Topic Model (NTM)&lt;/a&gt; uses Amazon SageMaker Neural Topic Model (NTM) to uncover topics in documents from a synthetic data source, where topic distributions are known.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/pca_mnist&#34;&gt;Principal Components Analysis (PCA)&lt;/a&gt; uses Amazon SageMaker PCA to calculate eigendigits from MNIST.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/seq2seq_translation_en-de&#34;&gt;Seq2Seq&lt;/a&gt; uses the Amazon SageMaker Seq2Seq algorithm that&#39;s built on top of &lt;a href=&#34;https://github.com/awslabs/sockeye&#34;&gt;Sockeye&lt;/a&gt;, which is a sequence-to-sequence framework for Neural Machine Translation based on MXNet. Seq2Seq implements state-of-the-art encoder-decoder architectures which can also be used for tasks like Abstractive Summarization in addition to Machine Translation. This notebook shows translation from English to German text.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/imageclassification_caltech&#34;&gt;Image Classification&lt;/a&gt; includes full training and transfer learning examples of Amazon SageMaker&#39;s Image Classification algorithm. This uses a ResNet deep convolutional neural network to classify images from the caltech dataset.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/xgboost_abalone&#34;&gt;XGBoost for regression&lt;/a&gt; predicts the age of abalone (&lt;a href=&#34;https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression.html&#34;&gt;Abalone dataset&lt;/a&gt;) using regression from Amazon SageMaker&#39;s implementation of &lt;a href=&#34;https://github.com/dmlc/xgboost&#34;&gt;XGBoost&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/xgboost_mnist&#34;&gt;XGBoost for multi-class classification&lt;/a&gt; uses Amazon SageMaker&#39;s implementation of &lt;a href=&#34;https://github.com/dmlc/xgboost&#34;&gt;XGBoost&lt;/a&gt; to classify handwritten digits from the MNIST dataset as one of the ten digits using a multi-class classifier. Both single machine and distributed use-cases are presented.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/deepar_synthetic&#34;&gt;DeepAR for time series forecasting&lt;/a&gt; illustrates how to use the Amazon SageMaker DeepAR algorithm for time series forecasting on a synthetically generated data set.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/blazingtext_word2vec_text8&#34;&gt;BlazingText Word2Vec&lt;/a&gt; generates Word2Vec embeddings from a cleaned text dump of Wikipedia articles using SageMaker&#39;s fast and scalable BlazingText implementation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/object_detection_birds&#34;&gt;Object detection for bird images&lt;/a&gt; demonstrates how to use the Amazon SageMaker Object Detection algorithm with a public dataset of Bird images.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/object2vec_movie_recommendation&#34;&gt;Object2Vec for movie recommendation&lt;/a&gt; demonstrates how Object2Vec can be used to model data consisting of pairs of singleton tokens using movie recommendation as a running example.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/object2vec_multilabel_genre_classification&#34;&gt;Object2Vec for multi-label classification&lt;/a&gt; shows how ObjectToVec algorithm can train on data consisting of pairs of sequences and singleton tokens using the setting of genre prediction of movies based on their plot descriptions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/object2vec_sentence_similarity&#34;&gt;Object2Vec for sentence similarity&lt;/a&gt; explains how to train Object2Vec using sequence pairs as input using sentence similarity analysis as the application.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/ipinsights_login&#34;&gt;IP Insights for suspicious logins&lt;/a&gt; shows how to train IP Insights on a login events for a web server to identify suspicious login attempts.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/semantic_segmentation_pascalvoc&#34;&gt;Semantic Segmentation&lt;/a&gt; shows how to train a semantic segmentation algorithm using the Amazon SageMaker Semantic Segmentation algorithm. It also demonstrates how to host the model and produce segmentation masks and probability of segmentation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/jumpstart_instance_segmentation&#34;&gt;JumpStart Instance Segmentation&lt;/a&gt; demonstrates how to use a pre-trained Instance Segmentation model available in JumpStart for inference.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/jumpstart_semantic_segmentation&#34;&gt;JumpStart Semantic Segmentation&lt;/a&gt; demonstrates how to use a pre-trained Semantic Segmentation model available in JumpStart for inference, how to finetune the pre-trained model on a custom dataset using JumpStart transfer learning algorithm, and how to use fine-tuned model for inference.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/jumpstart_text_generation&#34;&gt;JumpStart Text Generation&lt;/a&gt; shows how to use JumpStart to generate text that appears indistinguishable from the hand-written text.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/jumpstart_text_summarization&#34;&gt;JumpStart Text Summarization&lt;/a&gt; shows how to use JumpStart to summarize the text to contain only the important information.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/jumpstart_image_embedding&#34;&gt;JumpStart Image Embedding&lt;/a&gt; demonstrates how to use a pre-trained model available in JumpStart for image embedding.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/jumpstart_text_embedding&#34;&gt;JumpStart Text Embedding&lt;/a&gt; demonstrates how to use a pre-trained model available in JumpStart for text embedding.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/jumpstart_object_detection&#34;&gt;JumpStart Object Detection&lt;/a&gt; demonstrates how to use a pre-trained Object Detection model available in JumpStart for inference, how to finetune the pre-trained model on a custom dataset using JumpStart transfer learning algorithm, and how to use fine-tuned model for inference.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/jumpstart_machine_translation&#34;&gt;JumpStart Machine Translation&lt;/a&gt; demonstrates how to translate text from one language to another language in JumpStart.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/jumpstart_named_entity_recognition&#34;&gt;JumpStart Named Entity Recognition&lt;/a&gt; demonstrates how to identify named entities such as names, locations etc. in the text in JumpStart.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Amazon SageMaker RL&lt;/h3&gt; &#xA;&lt;p&gt;The following provide examples demonstrating different capabilities of Amazon SageMaker RL.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/reinforcement_learning/rl_cartpole_coach&#34;&gt;Cartpole using Coach&lt;/a&gt; demonstrates the simplest usecase of Amazon SageMaker RL using Intel&#39;s RL Coach.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/reinforcement_learning/rl_deepracer_robomaker_coach_gazebo&#34;&gt;AWS DeepRacer&lt;/a&gt; demonstrates AWS DeepRacer trainig using RL Coach in the Gazebo environment.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/reinforcement_learning/rl_hvac_coach_energyplus&#34;&gt;HVAC using EnergyPlus&lt;/a&gt; demonstrates the training of HVAC systems using the EnergyPlus environment.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/reinforcement_learning/rl_knapsack_coach_custom&#34;&gt;Knapsack Problem&lt;/a&gt; demonstrates how to solve the knapsack problem using a custom environment.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/reinforcement_learning/rl_mountain_car_coach_gymEnv&#34;&gt;Mountain Car&lt;/a&gt; Mountain car is a classic RL problem. This notebook explains how to solve this using the OpenAI Gym environment.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/reinforcement_learning/rl_network_compression_ray_custom&#34;&gt;Distributed Neural Network Compression&lt;/a&gt; This notebook explains how to compress ResNets using RL, using a custom environment and the RLLib toolkit.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/reinforcement_learning/rl_portfolio_management_coach_customEnv&#34;&gt;Portfolio Management&lt;/a&gt; This notebook uses a custom Gym environment to manage multiple financial investments.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/reinforcement_learning/rl_predictive_autoscaling_coach_customEnv&#34;&gt;Autoscaling&lt;/a&gt; demonstrates how to adjust load depending on demand. This uses RL Coach and a custom environment.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/reinforcement_learning/rl_roboschool_ray&#34;&gt;Roboschool&lt;/a&gt; is an open source physics simulator that is commonly used to train RL policies for robotic systems. This notebook demonstrates training a few agents using it.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/reinforcement_learning/rl_roboschool_stable_baselines&#34;&gt;Stable Baselines&lt;/a&gt; In this notebook example, we will make the HalfCheetah agent learn to walk using the stable-baselines, which are a set of improved implementations of Reinforcement Learning (RL) algorithms based on OpenAI Baselines.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/reinforcement_learning/rl_traveling_salesman_vehicle_routing_coach&#34;&gt;Travelling Salesman&lt;/a&gt; is a classic NP hard problem, which this notebook solves with AWS SageMaker RL.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/reinforcement_learning/rl_tic_tac_toe_coach_customEnv&#34;&gt;Tic-tac-toe&lt;/a&gt; is a simple implementation of a custom Gym environment to train and deploy an RL agent in Coach that then plays tic-tac-toe interactively in a Jupyter Notebook.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/reinforcement_learning/rl_unity_ray&#34;&gt;Unity Game Agent&lt;/a&gt; shows how to use RL algorithms to train an agent to play Unity3D game.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Scientific Details of Algorithms&lt;/h3&gt; &#xA;&lt;p&gt;These examples provide more thorough mathematical treatment on a select group of algorithms.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/scientific_details_of_algorithms/streaming_median&#34;&gt;Streaming Median&lt;/a&gt; sequentially introduces concepts used in streaming algorithms, which many SageMaker algorithms rely on to deliver speed and scalability.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/scientific_details_of_algorithms/lda_topic_modeling&#34;&gt;Latent Dirichlet Allocation (LDA)&lt;/a&gt; dives into Amazon SageMaker&#39;s spectral decomposition approach to LDA.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/scientific_details_of_algorithms/linear_learner_class_weights_loss_functions&#34;&gt;Linear Learner features&lt;/a&gt; shows how to use the class weights and loss functions features of the SageMaker Linear Learner algorithm to improve performance on a credit card fraud prediction task&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Amazon SageMaker Debugger&lt;/h3&gt; &#xA;&lt;p&gt;These examples provide and introduction to SageMaker Debugger which allows debugging and monitoring capabilities for training of machine learning and deep learning algorithms. Note that although these notebooks focus on a specific framework, the same approach works with all the frameworks that Amazon SageMaker Debugger supports. The notebooks below are listed in the order in which we recommend you review them.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-debugger/tensorflow_builtin_rule/&#34;&gt;Using a built-in rule with TensorFlow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-debugger/tensorflow_keras_custom_rule/&#34;&gt;Using a custom rule with TensorFlow Keras&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-debugger/mnist_tensor_analysis/&#34;&gt;Interactive tensor analysis in notebook with MXNet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-debugger/mnist_tensor_plot/&#34;&gt;Visualizing Debugging Tensors of MXNet training&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-debugger/mxnet_realtime_analysis/&#34;&gt;Real-time analysis in notebook with MXNet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-debugger/xgboost_builtin_rules/&#34;&gt;Using a built in rule with XGBoost&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-debugger/xgboost_realtime_analysis/&#34;&gt;Real-time analysis in notebook with XGBoost&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-debugger/mxnet_spot_training/&#34;&gt;Using SageMaker Debugger with Managed Spot Training and MXNet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-debugger/tensorflow_action_on_rule/&#34;&gt;Reacting to CloudWatch Events from Rules to take an action based on status with TensorFlow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-debugger/pytorch_custom_container/&#34;&gt;Using SageMaker Debugger with a custom PyTorch container&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Amazon SageMaker Clarify&lt;/h3&gt; &#xA;&lt;p&gt;These examples provide an introduction to SageMaker Clarify which provides machine learning developers with greater visibility into their training data and models so they can identify and limit bias and explain predictions.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker_processing/fairness_and_explainability&#34;&gt;Fairness and Explainability with SageMaker Clarify&lt;/a&gt; shows how to use SageMaker Clarify Processor API to measure the pre-training bias of a dataset and post-training bias of a model, and explain the importance of the input features on the model&#39;s decision.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker_model_monitor/fairness_and_explainability&#34;&gt;Amazon SageMaker Clarify Model Monitors&lt;/a&gt; shows how to use SageMaker Clarify Model Monitor API to schedule bias monitor to monitor predictions for bias drift on a regular basis, and schedule explainability monitor to monitor predictions for feature attribution drift on a regular basis.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Publishing content from RStudio on Amazon SageMaker to RStudio Connect&lt;/h3&gt; &#xA;&lt;p&gt;These examples show you how to run R examples, and publish applications in RStudio on Amazon SageMaker to RStudio Connect.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/r_examples/rsconnect_rmarkdown/&#34;&gt;Publishing R Markdown&lt;/a&gt; shows how you can author an R Markdown document (.Rmd, .Rpres) within RStudio on Amazon SageMaker and publish to RStudio Connect for wide consumption.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/r_examples/rsconnect_shiny/&#34;&gt;Publishing R Shiny Apps&lt;/a&gt; shows how you can author an R Shiny application within RStudio on Amazon SageMaker and publish to RStudio Connect for wide consumption.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/r_examples/rsconnect_streamlit/&#34;&gt;Publishing Streamlit Apps&lt;/a&gt; shows how you can author a streamlit application withing Amazon SageMaker Studio and publish to RStudio Connect for wide consumption.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Advanced Amazon SageMaker Functionality&lt;/h3&gt; &#xA;&lt;p&gt;These examples showcase unique functionality available in Amazon SageMaker. They cover a broad range of topics and utilize a variety of methods, but aim to provide the user with sufficient insight or inspiration to develop within Amazon SageMaker.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/data_distribution_types&#34;&gt;Data Distribution Types&lt;/a&gt; showcases the difference between two methods for sending data from S3 to Amazon SageMaker Training instances. This has particular implication for scalability and accuracy of distributed training.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/handling_kms_encrypted_data&#34;&gt;Encrypting Your Data&lt;/a&gt; shows how to use Server Side KMS encrypted data with Amazon SageMaker training. The IAM role used for S3 access needs to have permissions to encrypt and decrypt data with the KMS key.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/parquet_to_recordio_protobuf&#34;&gt;Using Parquet Data&lt;/a&gt; shows how to bring &lt;a href=&#34;https://parquet.apache.org/&#34;&gt;Parquet&lt;/a&gt; data sitting in S3 into an Amazon SageMaker Notebook and convert it into the recordIO-protobuf format that many SageMaker algorithms consume.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/working_with_redshift_data&#34;&gt;Connecting to Redshift&lt;/a&gt; demonstrates how to copy data from Redshift to S3 and vice-versa without leaving Amazon SageMaker Notebooks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/xgboost_bring_your_own_model&#34;&gt;Bring Your Own XGBoost Model&lt;/a&gt; shows how to use Amazon SageMaker Algorithms containers to bring a pre-trained model to a realtime hosted endpoint without ever needing to think about REST APIs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/kmeans_bring_your_own_model&#34;&gt;Bring Your Own k-means Model&lt;/a&gt; shows how to take a model that&#39;s been fit elsewhere and use Amazon SageMaker Algorithms containers to host it.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/r_bring_your_own&#34;&gt;Bring Your Own R Algorithm&lt;/a&gt; shows how to bring your own algorithm container to Amazon SageMaker using the R language.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/install_r_kernel&#34;&gt;Installing the R Kernel&lt;/a&gt; shows how to install the R kernel into an Amazon SageMaker Notebook Instance.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/scikit_bring_your_own&#34;&gt;Bring Your Own scikit Algorithm&lt;/a&gt; provides a detailed walkthrough on how to package a scikit learn algorithm for training and production-ready hosting.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/mxnet_mnist_byom&#34;&gt;Bring Your Own MXNet Model&lt;/a&gt; shows how to bring a model trained anywhere using MXNet into Amazon SageMaker.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/tensorflow_iris_byom&#34;&gt;Bring Your Own TensorFlow Model&lt;/a&gt; shows how to bring a model trained anywhere using TensorFlow into Amazon SageMaker.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/search&#34;&gt;Experiment Management Capabilities with Search&lt;/a&gt; shows how to organize Training Jobs into projects, and track relationships between Models, Endpoints, and Training Jobs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/multi_model_bring_your_own&#34;&gt;Host Multiple Models with Your Own Algorithm&lt;/a&gt; shows how to deploy multiple models to a realtime hosted endpoint with your own custom algorithm.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/multi_model_xgboost_home_value&#34;&gt;Host Multiple Models with XGBoost&lt;/a&gt; shows how to deploy multiple models to a realtime hosted endpoint using a multi-model enabled XGBoost container.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/multi_model_sklearn_home_value&#34;&gt;Host Multiple Models with SKLearn&lt;/a&gt; shows how to deploy multiple models to a realtime hosted endpoint using a multi-model enabled SKLearn container.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-script-mode&#34;&gt;SageMaker Training and Inference with Script Mode&lt;/a&gt; shows how to use custom training and inference scripts, similar to those you would use outside of SageMaker, with SageMaker&#39;s prebuilt containers for various frameworks like Scikit-learn, PyTorch, and XGBoost.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-triton&#34;&gt;Host Models with NVidia Triton Server&lt;/a&gt; shows how to deploy models to a realtime hosted endpoint using &lt;a href=&#34;https://developer.nvidia.com/nvidia-triton-inference-server&#34;&gt;Triton&lt;/a&gt; as the model inference server.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Amazon SageMaker Neo Compilation Jobs&lt;/h3&gt; &#xA;&lt;p&gt;These examples provide an introduction to how to use Neo to compile and optimize deep learning models.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker_neo_compilation_jobs/gluoncv_ssd_mobilenet&#34;&gt;GluonCV SSD Mobilenet&lt;/a&gt; shows how to train GluonCV SSD MobileNet and use Amazon SageMaker Neo to compile and optimize the trained model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker_neo_compilation_jobs/imageclassification_caltech&#34;&gt;Image Classification&lt;/a&gt; Adapts from &lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/imageclassification_caltech&#34;&gt;image classification&lt;/a&gt; including Neo API and comparison against the uncompiled baseline.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker_neo_compilation_jobs/mxnet_mnist&#34;&gt;MNIST with MXNet&lt;/a&gt; Adapts from &lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-python-sdk/mxnet_mnist&#34;&gt;MXNet MNIST&lt;/a&gt; including Neo API and comparison against the uncompiled baseline.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker_neo_compilation_jobs/pytorch_torchvision&#34;&gt;Deploying pre-trained PyTorch vision models&lt;/a&gt; shows how to use Amazon SageMaker Neo to compile and optimize pre-trained PyTorch models from TorchVision.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker_neo_compilation_jobs/tensorflow_distributed_mnist&#34;&gt;Distributed TensorFlow&lt;/a&gt; includes Neo API and comparison against the uncompiled baseline.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker_neo_compilation_jobs/xgboost_customer_churn&#34;&gt;Predicting Customer Churn&lt;/a&gt; Adapts from &lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_applying_machine_learning/xgboost_customer_churn&#34;&gt;XGBoost customer churn&lt;/a&gt; including Neo API and comparison against the uncompiled baseline.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Amazon SageMaker Processing&lt;/h3&gt; &#xA;&lt;p&gt;These examples show you how to use SageMaker Processing jobs to run data processing workloads.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker_processing/scikit_learn_data_processing_and_model_evaluation&#34;&gt;Scikit-Learn Data Processing and Model Evaluation&lt;/a&gt; shows how to use SageMaker Processing and the Scikit-Learn container to run data preprocessing and model evaluation workloads.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker_processing/feature_transformation_with_sagemaker_processing&#34;&gt;Feature transformation with Amazon SageMaker Processing and SparkML&lt;/a&gt; shows how to use SageMaker Processing to run data processing workloads using SparkML prior to training.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker_processing/feature_transformation_with_sagemaker_processing_dask&#34;&gt;Feature transformation with Amazon SageMaker Processing and Dask&lt;/a&gt; shows how to use SageMaker Processing to transform data using Dask distributed clusters&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker_processing/spark_distributed_data_processing&#34;&gt;Distributed Data Processing using Apache Spark and SageMaker Processing&lt;/a&gt; shows how to use the built-in Spark container on SageMaker Processing using the SageMaker Python SDK.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Amazon SageMaker Pipelines&lt;/h3&gt; &#xA;&lt;p&gt;These examples show you how to use &lt;a href=&#34;https://aws.amazon.com/sagemaker/pipelines&#34;&gt;SageMaker Pipelines&lt;/a&gt; to create, automate and manage end-to-end Machine Learning workflows.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-pipelines/nlp/amazon_comprehend_sagemaker_pipeline&#34;&gt;Amazon Comprehend with SageMaker Pipelines&lt;/a&gt; shows how to deploy a custom text classification using Amazon Comprehend and SageMaker Pipelines.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-pipelines/time_series_forecasting/amazon_forecast_pipeline&#34;&gt;Amazon Forecast with SageMaker Pipelines&lt;/a&gt; shows how you can create a dataset, dataset group and predictor with Amazon Forecast and SageMaker Pipelines.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Amazon SageMaker Pre-Built Framework Containers and the Python SDK&lt;/h3&gt; &#xA;&lt;h4&gt;Pre-Built Deep Learning Framework Containers&lt;/h4&gt; &#xA;&lt;p&gt;These examples show you how to train and host in pre-built deep learning framework containers using the SageMaker Python SDK.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-python-sdk/chainer_cifar10&#34;&gt;Chainer CIFAR-10&lt;/a&gt; trains a VGG image classification network on CIFAR-10 using Chainer (both single machine and multi-machine versions are included)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-python-sdk/chainer_mnist&#34;&gt;Chainer MNIST&lt;/a&gt; trains a basic neural network on MNIST using Chainer (shows how to use local mode)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-python-sdk/chainer_sentiment_analysis&#34;&gt;Chainer sentiment analysis&lt;/a&gt; trains a LSTM network with embeddings to predict text sentiment using Chainer&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-python-sdk/scikit_learn_iris&#34;&gt;IRIS with Scikit-learn&lt;/a&gt; trains a Scikit-learn classifier on IRIS data&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-python-sdk/scikit_learn_model_registry_batch_transform&#34;&gt;Model Registry and Batch Transform with Scikit-learn&lt;/a&gt; trains a Scikit-learn Random Forest model, registers it in Model Registry, and runs a Batch Transform Job.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-python-sdk/mxnet_gluon_mnist&#34;&gt;MNIST with MXNet Gluon&lt;/a&gt; trains a basic neural network on the MNIST handwritten digit dataset using MXNet Gluon&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-python-sdk/mxnet_mnist&#34;&gt;MNIST with MXNet&lt;/a&gt; trains a basic neural network on the MNIST handwritten digit data using MXNet&#39;s symbolic syntax&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-python-sdk/mxnet_gluon_sentiment&#34;&gt;Sentiment Analysis with MXNet Gluon&lt;/a&gt; trains a text classifier using embeddings with MXNet Gluon&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-python-sdk/tensorflow_script_mode_training_and_serving&#34;&gt;TensorFlow training and serving&lt;/a&gt; trains a basic neural network on MNIST&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-python-sdk/tensorflow_script_mode_horovod&#34;&gt;TensorFlow with Horovod&lt;/a&gt; trains on MNIST using Horovod for distributed training&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-python-sdk/tensorflow_script_mode_using_shell_commands&#34;&gt;TensorFlow using shell commands&lt;/a&gt; shows how to use a shell script for the container&#39;s entry point&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Pre-Built Machine Learning Framework Containers&lt;/h4&gt; &#xA;&lt;p&gt;These examples show you how to build Machine Learning models with frameworks like Apache Spark or Scikit-learn using SageMaker Python SDK.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-python-sdk/sparkml_serving_emr_mleap_abalone&#34;&gt;Inference with SparkML Serving&lt;/a&gt; shows how to build an ML model with Apache Spark using Amazon EMR on Abalone dataset and deploy in SageMaker with SageMaker SparkML Serving.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-python-sdk/scikit_learn_inference_pipeline&#34;&gt;Pipeline Inference with Scikit-learn and LinearLearner&lt;/a&gt; builds a ML pipeline using Scikit-learn preprocessing and LinearLearner algorithm in single endpoint&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Using Amazon SageMaker with Apache Spark&lt;/h3&gt; &#xA;&lt;p&gt;These examples show how to use Amazon SageMaker for model training, hosting, and inference through Apache Spark using &lt;a href=&#34;https://github.com/aws/sagemaker-spark&#34;&gt;SageMaker Spark&lt;/a&gt;. SageMaker Spark allows you to interleave Spark Pipeline stages with Pipeline stages that interact with Amazon SageMaker.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-spark/pyspark_mnist&#34;&gt;MNIST with SageMaker PySpark&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Using Amazon SageMaker with Amazon Keyspaces (for Apache Cassandra)&lt;/h3&gt; &#xA;&lt;p&gt;These examples show how to use Amazon SageMaker to read data from &lt;a href=&#34;https://docs.aws.amazon.com/keyspaces/&#34;&gt;Amazon Keyspaces&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/ingest_data/sagemaker-keyspaces&#34;&gt;Train Machine Learning Models using Amazon Keyspaces as a Data Source&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;AWS Marketplace&lt;/h3&gt; &#xA;&lt;h4&gt;Create algorithms/model packages for listing in AWS Marketplace for machine learning.&lt;/h4&gt; &#xA;&lt;p&gt;These example notebooks show you how to package a model or algorithm for listing in AWS Marketplace for machine learning.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/creating_marketplace_products&#34;&gt;Creating Marketplace Products&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/creating_marketplace_products/models&#34;&gt;Creating a Model Package - Listing on AWS Marketplace&lt;/a&gt; provides a detailed walkthrough on how to package a pre-trained model as a SageMaker Model Package that can be listed on AWS Marketplace.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/creating_marketplace_products/algorithms&#34;&gt;Creating Algorithm and Model Package - Listing on AWS Marketplace&lt;/a&gt; provides a detailed walkthrough on how to package a scikit learn algorithm to create SageMaker Algorithm and SageMaker Model Package entities that can be used with the enhanced SageMaker Train/Transform/Hosting/Tuning APIs and listed on AWS Marketplace.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Once you have created an algorithm or a model package to be listed in the AWS Marketplace, the next step is to list it in AWS Marketplace, and provide a sample notebook that customers can use to try your algorithm or model package.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/curating_aws_marketplace_listing_and_sample_notebook/ModelPackage&#34;&gt;Curate your AWS Marketplace model package listing and sample notebook&lt;/a&gt; provides instructions on how to craft a sample notebook to be associated with your listing and how to curate a good AWS Marketplace listing that makes it easy for AWS customers to consume your model package.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/curating_aws_marketplace_listing_and_sample_notebook/Algorithm&#34;&gt;Curate your AWS Marketplace algorithm listing and sample notebook&lt;/a&gt; provides instructions on how to craft a sample notebook to be associated with your listing and how to curate a good AWS Marketplace listing that makes it easy for your customers to consume your algorithm.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Use algorithms, data, and model packages from AWS Marketplace.&lt;/h4&gt; &#xA;&lt;p&gt;These examples show you how to use model-packages and algorithms from AWS Marketplace and dataset products from AWS Data Exchange, for machine learning.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/using_algorithms&#34;&gt;Using Algorithms&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/using_algorithms/amazon_demo_product&#34;&gt;Using Algorithm From AWS Marketplace&lt;/a&gt; provides a detailed walkthrough on how to use Algorithm with the enhanced SageMaker Train/Transform/Hosting/Tuning APIs by choosing a canonical product listed on AWS Marketplace.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/using_algorithms/automl&#34;&gt;Using AutoML algorithm&lt;/a&gt; provides a detailed walkthrough on how to use AutoML algorithm from AWS Marketplace.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/using_model_packages&#34;&gt;Using Model Packages&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/using_model_packages/generic_sample_notebook&#34;&gt;Using Model Packages From AWS Marketplace&lt;/a&gt; is a generic notebook which provides sample code snippets you can modify and use for performing inference on Model Packages from AWS Marketplace, using Amazon SageMaker.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/using_model_packages/amazon_demo_product&#34;&gt;Using Amazon Demo product From AWS Marketplace&lt;/a&gt; provides a detailed walkthrough on how to use Model Package entities with the enhanced SageMaker Transform/Hosting APIs by choosing a canonical product listed on AWS Marketplace.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/using_model_packages/auto_insurance&#34;&gt;Using models for extracting vehicle metadata&lt;/a&gt; provides a detailed walkthrough on how to use pre-trained models from AWS Marketplace for extracting metadata for a sample use-case of auto-insurance claim processing.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/using_model_packages/improving_industrial_workplace_safety&#34;&gt;Using models for identifying non-compliance at a workplace&lt;/a&gt; provides a detailed walkthrough on how to use pre-trained models from AWS Marketplace for extracting metadata for a sample use-case of generating summary reports for identifying non-compliance at a construction/industrial workplace.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/using_model_packages/creative-writing-using-gpt-2-text-generation&#34;&gt;Creative writing using GPT-2 Text Generation&lt;/a&gt; will show you how to use AWS Marketplace GPT-2-XL pre-trained model on Amazon SageMaker to generate text based on your prompt to help you author prose and poetry.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/using_model_packages/amazon_augmented_ai_with_aws_marketplace_ml_models&#34;&gt;Amazon Augmented AI with AWS Marketplace ML models&lt;/a&gt; will show you how to use AWS Marketplace pre-trained ML models with Amazon Augmented AI to implement human-in-loop workflow reviews with your ML model predictions.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/using_model_packages/data_quality_monitoring&#34;&gt;Monitoring data quality in third-party models from AWS Marketplace&lt;/a&gt; will show you how to perform Data Quality monitoring on a pre-trained third-party model from AWS Marketplace.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/using_model_packages/evaluating_aws_marketplace_models_for_person_counting_use_case&#34;&gt;Evaluating ML models from AWS Marketplace for person counting use case&lt;/a&gt; will show you how to use two AWS Marketplace GluonCV pre-trained ML models for person counting use case and evaluate each model for performance in different types of crowd images.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/using_model_packages/preprocessing-audio-data-using-a-machine-learning-model&#34;&gt;Preprocessing audio data using a pre-trained machine learning model&lt;/a&gt; demonstrates the usage of a pre-trained audio track separation model to create synthetic features and improve an acoustic classification model.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/using_data&#34;&gt;Using Dataset Products&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/using_data/using_data_with_ml_model&#34;&gt;Using Dataset Product from AWS Data Exchange with ML model from AWS Marketplace&lt;/a&gt; is a sample notebook which shows how a dataset from AWS Data Exchange can be used with an ML Model Package from AWS Marketplace.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/using_data/image_classification_with_shutterstock_image_datasets&#34;&gt;Using Shutterstock Image Datasets to train Image Classification Models&lt;/a&gt; provides a detailed walkthrough on how to use the &lt;a href=&#34;https://aws.amazon.com/marketplace/pp/prodview-y6xuddt42fmbu?qid=1623195111604&amp;amp;sr=0-1&amp;amp;ref_=srh_res_product_title#offers&#34;&gt;Free Sample: Images &amp;amp; Metadata of â€œWhole Foodsâ€ Shoppers&lt;/a&gt; from Shutterstock&#39;s Image Datasets to train a multi-label image classification model using Shutterstock&#39;s pre-labeled image assets. You can learn more about this implementation &lt;a href=&#34;https://aws.amazon.com/blogs/awsmarketplace/using-shutterstocks-image-datasets-to-train-your-computer-vision-models/&#34;&gt;from this blog post&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;span&gt;âš–&lt;/span&gt; License&lt;/h2&gt; &#xA;&lt;p&gt;This library is licensed under the &lt;a href=&#34;http://aws.amazon.com/apache2.0/&#34;&gt;Apache 2.0 License&lt;/a&gt;. For more details, please take a look at the &lt;a href=&#34;https://github.com/aws/amazon-sagemaker-examples/raw/master/LICENSE.txt&#34;&gt;LICENSE&lt;/a&gt; file.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;ğŸ¤&lt;/span&gt; Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Although we&#39;re extremely excited to receive contributions from the community, we&#39;re still working on the best mechanism to take in examples from external sources. Please bear with us in the short-term if pull requests take longer than expected or are closed. Please read our &lt;a href=&#34;https://github.com/aws/amazon-sagemaker-examples/raw/master/CONTRIBUTING.md&#34;&gt;contributing guidelines&lt;/a&gt; if you&#39;d like to open an issue or submit a pull request.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>The-Art-of-Hacking/h4cker</title>
    <updated>2022-06-03T01:43:20Z</updated>
    <id>tag:github.com,2022-06-03:/The-Art-of-Hacking/h4cker</id>
    <link href="https://github.com/The-Art-of-Hacking/h4cker" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This repository is primarily maintained by Omar Santos and includes thousands of resources related to ethical hacking / penetration testing, digital forensics and incident response (DFIR), vulnerability research, exploit development, reverse engineering, and more.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Cyber Security Resources&lt;/h1&gt; &#xA;&lt;center&gt;&#xA; &lt;img src=&#34;https://h4cker.org/img/h4cker2.PNG&#34; width=&#34;200&#34; height=&#34;300&#34;&gt; &#xA;&lt;/center&gt; &#xA;&lt;p&gt;This repository includes thousands of cybersecurity-related references and resources and it is maintained by &lt;a href=&#34;https://omarsantos.io/&#34;&gt;Omar Santos&lt;/a&gt;. This GitHub repository has been created to provide supplemental material to several books, video courses, and live training created by Omar Santos and other co-authors. It provides over 9,000 references, scripts, tools, code, and other resources that help offensive and defensive security professionals learn and develop new skills. This GitHub repository provides guidance on how build your own hacking environment, learn about offensive security (ethical hacking) techniques, vulnerability research, exploit development, reverse engineering, malware analysis, threat intelligence, threat hunting, digital forensics and incident response (DFIR), includes examples of real-life penetration testing reports, and more.&lt;/p&gt; &#xA;&lt;h2&gt;The Art of Hacking Series&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;http://theartofhacking.org&#34;&gt;Art of Hacking Series&lt;/a&gt; is a series of video courses and live training that help you get up and running with your cybersecurity career. The following are the different video courses that are part of the Art of Hacking series:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.safaribooksonline.com/library/view/security-penetration-testing/9780134833989&#34;&gt;Security Penetration Testing (The Art of Hacking Series)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.safaribooksonline.com/library/view/wireless-networks-iot/9780134854632/&#34;&gt;Wireless Networks, IoT, and Mobile Devices Hacking (The Art of Hacking Series)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.safaribooksonline.com/videos/enterprise-penetration-testing/9780134854779&#34;&gt;Enterprise Penetration Testing and Continuous Monitoring (The Art of Hacking Series)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.safaribooksonline.com/videos/hacking-web-applications/9780135261422&#34;&gt;Hacking Web Applications: Security Penetration Testing for Today&#39;s DevOps and Cloud Environments (The Art of Hacking Series)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;These courses serve as comprehensive guide for any network and security professional who is starting a career in ethical hacking and penetration testing. It also can help individuals preparing for the &lt;a href=&#34;https://www.offensive-security.com/information-security-certifications/&#34;&gt;Offensive Security Certified Professional (OSCP)&lt;/a&gt;, the &lt;a href=&#34;https://www.eccouncil.org/programs/certified-ethical-hacker-ceh/&#34;&gt;Certified Ethical Hacker (CEH)&lt;/a&gt;, &lt;a href=&#34;https://certification.comptia.org/certifications/pentest&#34;&gt;CompTIA PenTest+&lt;/a&gt; and any other ethical hacking certification. This course helps any cyber security professional that want to learn the skills required to becoming a professional ethical hacker or that want to learn more about general hacking methodologies and concepts.&lt;/p&gt; &#xA;&lt;p&gt;These video courses are published by Pearson, but this GitHub repository is maintained and supported by the lead author of the series &lt;a href=&#34;https://omarsantos.io/&#34;&gt;Omar Santos&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Live Training&lt;/h2&gt; &#xA;&lt;p&gt;Other Live Training and Video Courses: &lt;a href=&#34;https://h4cker.org/training&#34;&gt;https://h4cker.org/training&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>aimacode/aima-python</title>
    <updated>2022-06-03T01:43:20Z</updated>
    <id>tag:github.com,2022-06-03:/aimacode/aima-python</id>
    <link href="https://github.com/aimacode/aima-python" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Python implementation of algorithms from Russell And Norvig&#39;s &#34;Artificial Intelligence - A Modern Approach&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;code&gt;aima-python&lt;/code&gt; &lt;a href=&#34;https://travis-ci.org/aimacode/aima-python&#34;&gt;&lt;img src=&#34;https://travis-ci.org/aimacode/aima-python.svg?branch=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://mybinder.org/repo/aimacode/aima-python&#34;&gt;&lt;img src=&#34;http://mybinder.org/badge.svg?sanitize=true&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;Python code for the book &lt;em&gt;&lt;a href=&#34;http://aima.cs.berkeley.edu&#34;&gt;Artificial Intelligence: A Modern Approach&lt;/a&gt;.&lt;/em&gt; You can use this in conjunction with a course on AI, or for study on your own. We&#39;re looking for &lt;a href=&#34;https://github.com/aimacode/aima-python/raw/master/CONTRIBUTING.md&#34;&gt;solid contributors&lt;/a&gt; to help.&lt;/p&gt; &#xA;&lt;h1&gt;Updates for 4th Edition&lt;/h1&gt; &#xA;&lt;p&gt;The 4th edition of the book as out now in 2020, and thus we are updating the code. All code here will reflect the 4th edition. Changes include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Move from Python 3.5 to 3.7.&lt;/li&gt; &#xA; &lt;li&gt;More emphasis on Jupyter (Ipython) notebooks.&lt;/li&gt; &#xA; &lt;li&gt;More projects using external packages (tensorflow, etc.).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Structure of the Project&lt;/h1&gt; &#xA;&lt;p&gt;When complete, this project will have Python implementations for all the pseudocode algorithms in the book, as well as tests and examples of use. For each major topic, such as &lt;code&gt;search&lt;/code&gt;, we provide the following files:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;search.ipynb&lt;/code&gt; and &lt;code&gt;search.py&lt;/code&gt;: Implementations of all the pseudocode algorithms, and necessary support functions/classes/data. The &lt;code&gt;.py&lt;/code&gt; file is generated automatically from the &lt;code&gt;.ipynb&lt;/code&gt; file; the idea is that it is easier to read the documentation in the &lt;code&gt;.ipynb&lt;/code&gt; file.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;search_XX.ipynb&lt;/code&gt;: Notebooks that show how to use the code, broken out into various topics (the &lt;code&gt;XX&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;tests/test_search.py&lt;/code&gt;: A lightweight test suite, using &lt;code&gt;assert&lt;/code&gt; statements, designed for use with &lt;a href=&#34;http://pytest.org/latest/&#34;&gt;&lt;code&gt;py.test&lt;/code&gt;&lt;/a&gt;, but also usable on their own.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Python 3.7 and up&lt;/h1&gt; &#xA;&lt;p&gt;The code for the 3rd edition was in Python 3.5; the current 4th edition code is in Python 3.7. It should also run in later versions, but does not run in Python 2. You can &lt;a href=&#34;https://www.python.org/downloads&#34;&gt;install Python&lt;/a&gt; or use a browser-based Python interpreter such as &lt;a href=&#34;https://repl.it/languages/python3&#34;&gt;repl.it&lt;/a&gt;. You can run the code in an IDE, or from the command line with &lt;code&gt;python -i filename.py&lt;/code&gt; where the &lt;code&gt;-i&lt;/code&gt; option puts you in an interactive loop where you can run Python functions. All notebooks are available in a &lt;a href=&#34;http://mybinder.org/repo/aimacode/aima-python&#34;&gt;binder environment&lt;/a&gt;. Alternatively, visit &lt;a href=&#34;http://jupyter.org/&#34;&gt;jupyter.org&lt;/a&gt; for instructions on setting up your own Jupyter notebook environment.&lt;/p&gt; &#xA;&lt;p&gt;Features from Python 3.6 and 3.7 that we will be using for this version of the code:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.python.org/3.6/whatsnew/3.6.html#whatsnew36-pep498&#34;&gt;f-strings&lt;/a&gt;: all string formatting should be done with &lt;code&gt;f&#39;var = {var}&#39;&lt;/code&gt;, not with &lt;code&gt;&#39;var = {}&#39;.format(var)&lt;/code&gt; nor &lt;code&gt;&#39;var = %s&#39; % var&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.python.org/3.7/library/typing.html&#34;&gt;&lt;code&gt;typing&lt;/code&gt; module&lt;/a&gt;: declare functions with type hints: &lt;code&gt;def successors(state) -&amp;gt; List[State]:&lt;/code&gt;; that is, give type declarations, but omit them when it is obvious. I don&#39;t need to say &lt;code&gt;state: State&lt;/code&gt;, but in another context it would make sense to say &lt;code&gt;s: State&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Underscores in numerics: write a million as &lt;code&gt;1_000_000&lt;/code&gt; not as &lt;code&gt;1000000&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.python.org/3.7/library/dataclasses.html#module-dataclasses&#34;&gt;&lt;code&gt;dataclasses&lt;/code&gt; module&lt;/a&gt;: replace &lt;code&gt;namedtuple&lt;/code&gt; with &lt;code&gt;dataclass&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation Guide&lt;/h2&gt; &#xA;&lt;p&gt;To download the repository:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;git clone https://github.com/aimacode/aima-python.git&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Then you need to install the basic dependencies to run the project on your system:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd aima-python&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You also need to fetch the datasets from the &lt;a href=&#34;https://github.com/aimacode/aima-data&#34;&gt;&lt;code&gt;aima-data&lt;/code&gt;&lt;/a&gt; repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git submodule init&#xA;git submodule update&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Wait for the datasets to download, it may take a while. Once they are downloaded, you need to install &lt;code&gt;pytest&lt;/code&gt;, so that you can run the test suite:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install pytest&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Then to run the tests:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;py.test&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;And you are good to go!&lt;/p&gt; &#xA;&lt;h1&gt;Index of Algorithms&lt;/h1&gt; &#xA;&lt;p&gt;Here is a table of algorithms, the figure, name of the algorithm in the book and in the repository, and the file where they are implemented in the repository. This chart was made for the third edition of the book and is being updated for the upcoming fourth edition. Empty implementations are a good place for contributors to look for an issue. The &lt;a href=&#34;https://github.com/aimacode/aima-pseudocode&#34;&gt;aima-pseudocode&lt;/a&gt; project describes all the algorithms from the book. An asterisk next to the file name denotes the algorithm is not fully implemented. Another great place for contributors to start is by adding tests and writing on the notebooks. You can see which algorithms have tests and notebook sections below. If the algorithm you want to work on is covered, don&#39;t worry! You can still add more tests and provide some examples of use in the notebook!&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;strong&gt;Figure&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;strong&gt;Name (in 3&lt;sup&gt;rd&lt;/sup&gt; edition)&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;strong&gt;Name (in repository)&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;strong&gt;File&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;strong&gt;Tests&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;strong&gt;Notebook&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Random-Vacuum-Agent&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;RandomVacuumAgent&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/agents.py&#34;&gt;&lt;code&gt;agents.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Model-Based-Vacuum-Agent&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;ModelBasedVacuumAgent&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/agents.py&#34;&gt;&lt;code&gt;agents.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2.1&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Environment&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;Environment&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/agents.py&#34;&gt;&lt;code&gt;agents.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2.1&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Agent&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;Agent&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/agents.py&#34;&gt;&lt;code&gt;agents.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2.3&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Table-Driven-Vacuum-Agent&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;TableDrivenVacuumAgent&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/agents.py&#34;&gt;&lt;code&gt;agents.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2.7&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Table-Driven-Agent&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;TableDrivenAgent&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/agents.py&#34;&gt;&lt;code&gt;agents.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2.8&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Reflex-Vacuum-Agent&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;ReflexVacuumAgent&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/agents.py&#34;&gt;&lt;code&gt;agents.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2.10&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Simple-Reflex-Agent&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;SimpleReflexAgent&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/agents.py&#34;&gt;&lt;code&gt;agents.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2.12&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Model-Based-Reflex-Agent&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;ReflexAgentWithState&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/agents.py&#34;&gt;&lt;code&gt;agents.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Problem&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;Problem&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/search.py&#34;&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Node&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;Node&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/search.py&#34;&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Queue&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;Queue&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/utils.py&#34;&gt;&lt;code&gt;utils.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;No Need&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3.1&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Simple-Problem-Solving-Agent&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;SimpleProblemSolvingAgent&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/search.py&#34;&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3.2&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Romania&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;romania&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/search.py&#34;&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3.7&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Tree-Search&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;depth/breadth_first_tree_search&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/search.py&#34;&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3.7&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Graph-Search&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;depth/breadth_first_graph_search&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/search.py&#34;&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3.11&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Breadth-First-Search&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;breadth_first_graph_search&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/search.py&#34;&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3.14&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Uniform-Cost-Search&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;uniform_cost_search&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/search.py&#34;&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3.17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Depth-Limited-Search&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;depth_limited_search&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/search.py&#34;&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3.18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Iterative-Deepening-Search&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;iterative_deepening_search&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/search.py&#34;&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3.22&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Best-First-Search&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;best_first_graph_search&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/search.py&#34;&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3.24&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;A*-Search&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;astar_search&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/search.py&#34;&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3.26&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Recursive-Best-First-Search&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;recursive_best_first_search&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/search.py&#34;&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;4.2&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Hill-Climbing&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;hill_climbing&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/search.py&#34;&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;4.5&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Simulated-Annealing&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;simulated_annealing&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/search.py&#34;&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;4.8&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Genetic-Algorithm&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;genetic_algorithm&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/search.py&#34;&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;4.11&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;And-Or-Graph-Search&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;and_or_graph_search&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/search.py&#34;&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;4.21&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Online-DFS-Agent&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;online_dfs_agent&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/search.py&#34;&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;4.24&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;LRTA*-Agent&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;LRTAStarAgent&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/search.py&#34;&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;5.3&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Minimax-Decision&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;minimax_decision&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/games.py&#34;&gt;&lt;code&gt;games.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;5.7&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Alpha-Beta-Search&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;alphabeta_search&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/games.py&#34;&gt;&lt;code&gt;games.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;6&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;CSP&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;CSP&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/csp.py&#34;&gt;&lt;code&gt;csp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;6.3&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;AC-3&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;AC3&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/csp.py&#34;&gt;&lt;code&gt;csp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;6.5&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Backtracking-Search&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;backtracking_search&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/csp.py&#34;&gt;&lt;code&gt;csp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;6.8&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Min-Conflicts&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;min_conflicts&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/csp.py&#34;&gt;&lt;code&gt;csp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;6.11&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Tree-CSP-Solver&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;tree_csp_solver&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/csp.py&#34;&gt;&lt;code&gt;csp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;7&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;KB&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;KB&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/logic.py&#34;&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;7.1&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;KB-Agent&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;KB_AgentProgram&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/logic.py&#34;&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;7.7&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Propositional Logic Sentence&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;Expr&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/utils.py&#34;&gt;&lt;code&gt;utils.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;7.10&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;TT-Entails&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;tt_entails&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/logic.py&#34;&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;7.12&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;PL-Resolution&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;pl_resolution&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/logic.py&#34;&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;7.14&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Convert to CNF&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;to_cnf&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/logic.py&#34;&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;7.15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;PL-FC-Entails?&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;pl_fc_entails&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/logic.py&#34;&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;7.17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;DPLL-Satisfiable?&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;dpll_satisfiable&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/logic.py&#34;&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;7.18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;WalkSAT&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;WalkSAT&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/logic.py&#34;&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;7.20&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Hybrid-Wumpus-Agent&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;HybridWumpusAgent&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;7.22&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;SATPlan&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;SAT_plan&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/logic.py&#34;&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;9&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Subst&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;subst&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/logic.py&#34;&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;9.1&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Unify&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;unify&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/logic.py&#34;&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;9.3&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;FOL-FC-Ask&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;fol_fc_ask&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/logic.py&#34;&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;9.6&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;FOL-BC-Ask&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;fol_bc_ask&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/logic.py&#34;&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;10.1&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Air-Cargo-problem&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;air_cargo&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/planning.py&#34;&gt;&lt;code&gt;planning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;10.2&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Spare-Tire-Problem&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;spare_tire&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/planning.py&#34;&gt;&lt;code&gt;planning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;10.3&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Three-Block-Tower&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;three_block_tower&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/planning.py&#34;&gt;&lt;code&gt;planning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;10.7&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Cake-Problem&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;have_cake_and_eat_cake_too&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/planning.py&#34;&gt;&lt;code&gt;planning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;10.9&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Graphplan&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;GraphPlan&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/planning.py&#34;&gt;&lt;code&gt;planning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;10.13&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Partial-Order-Planner&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;PartialOrderPlanner&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/planning.py&#34;&gt;&lt;code&gt;planning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;11.1&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Job-Shop-Problem-With-Resources&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;job_shop_problem&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/planning.py&#34;&gt;&lt;code&gt;planning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;11.5&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Hierarchical-Search&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;hierarchical_search&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/planning.py&#34;&gt;&lt;code&gt;planning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;11.8&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Angelic-Search&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;angelic_search&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/planning.py&#34;&gt;&lt;code&gt;planning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;11.10&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Doubles-tennis&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;double_tennis_problem&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/planning.py&#34;&gt;&lt;code&gt;planning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;13&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Discrete Probability Distribution&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;ProbDist&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/probability.py&#34;&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;13.1&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;DT-Agent&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;DTAgent&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/probability.py&#34;&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;14.9&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Enumeration-Ask&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;enumeration_ask&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/probability.py&#34;&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;14.11&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Elimination-Ask&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;elimination_ask&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/probability.py&#34;&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;14.13&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Prior-Sample&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;prior_sample&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/probability.py&#34;&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;14.14&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Rejection-Sampling&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;rejection_sampling&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/probability.py&#34;&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;14.15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Likelihood-Weighting&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;likelihood_weighting&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/probability.py&#34;&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;14.16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Gibbs-Ask&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;gibbs_ask&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/probability.py&#34;&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;15.4&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Forward-Backward&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;forward_backward&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/probability.py&#34;&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;15.6&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Fixed-Lag-Smoothing&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;fixed_lag_smoothing&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/probability.py&#34;&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;15.17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Particle-Filtering&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;particle_filtering&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/probability.py&#34;&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;16.9&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Information-Gathering-Agent&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;InformationGatheringAgent&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/probability.py&#34;&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;17.4&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Value-Iteration&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;value_iteration&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/mdp.py&#34;&gt;&lt;code&gt;mdp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;17.7&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Policy-Iteration&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;policy_iteration&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/mdp.py&#34;&gt;&lt;code&gt;mdp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;17.9&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;POMDP-Value-Iteration&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;pomdp_value_iteration&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/mdp.py&#34;&gt;&lt;code&gt;mdp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;18.5&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Decision-Tree-Learning&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;DecisionTreeLearner&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/learning.py&#34;&gt;&lt;code&gt;learning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;18.8&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Cross-Validation&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;cross_validation&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/learning.py&#34;&gt;&lt;code&gt;learning.py&lt;/code&gt;&lt;/a&gt;*&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;18.11&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Decision-List-Learning&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;DecisionListLearner&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/learning.py&#34;&gt;&lt;code&gt;learning.py&lt;/code&gt;&lt;/a&gt;*&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;18.24&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Back-Prop-Learning&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;BackPropagationLearner&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/learning.py&#34;&gt;&lt;code&gt;learning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;18.34&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;AdaBoost&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;AdaBoost&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/learning.py&#34;&gt;&lt;code&gt;learning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;19.2&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Current-Best-Learning&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;current_best_learning&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/knowledge.py&#34;&gt;&lt;code&gt;knowledge.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;19.3&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Version-Space-Learning&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;version_space_learning&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/knowledge.py&#34;&gt;&lt;code&gt;knowledge.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;19.8&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Minimal-Consistent-Det&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;minimal_consistent_det&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/knowledge.py&#34;&gt;&lt;code&gt;knowledge.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;19.12&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;FOIL&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;FOIL_container&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/knowledge.py&#34;&gt;&lt;code&gt;knowledge.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;21.2&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Passive-ADP-Agent&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;PassiveADPAgent&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/rl.py&#34;&gt;&lt;code&gt;rl.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;21.4&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Passive-TD-Agent&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;PassiveTDAgent&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/rl.py&#34;&gt;&lt;code&gt;rl.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;21.8&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Q-Learning-Agent&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;QLearningAgent&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/rl.py&#34;&gt;&lt;code&gt;rl.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;22.1&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;HITS&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;HITS&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/nlp.py&#34;&gt;&lt;code&gt;nlp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;23&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Chart-Parse&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;Chart&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/nlp.py&#34;&gt;&lt;code&gt;nlp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;23.5&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;CYK-Parse&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;CYK_parse&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/nlp.py&#34;&gt;&lt;code&gt;nlp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;25.9&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Monte-Carlo-Localization&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;monte_carlo_localization&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/probability.py&#34;&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Index of data structures&lt;/h1&gt; &#xA;&lt;p&gt;Here is a table of the implemented data structures, the figure, name of the implementation in the repository, and the file where they are implemented.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;strong&gt;Figure&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;strong&gt;Name (in repository)&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;strong&gt;File&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3.2&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;romania_map&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/search.py&#34;&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;4.9&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;vacumm_world&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/search.py&#34;&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;4.23&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;one_dim_state_space&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/search.py&#34;&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;6.1&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;australia_map&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/search.py&#34;&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;7.13&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;wumpus_world_inference&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/logic.py&#34;&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;7.16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;horn_clauses_KB&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/logic.py&#34;&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;17.1&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;sequential_decision_environment&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/mdp.py&#34;&gt;&lt;code&gt;mdp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;18.2&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;waiting_decision_tree&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/learning.py&#34;&gt;&lt;code&gt;learning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Acknowledgements&lt;/h1&gt; &#xA;&lt;p&gt;Many thanks for contributions over the years. I got bug reports, corrected code, and other support from Darius Bacon, Phil Ruggera, Peng Shao, Amit Patil, Ted Nienstedt, Jim Martin, Ben Catanzariti, and others. Now that the project is on GitHub, you can see the &lt;a href=&#34;https://github.com/aimacode/aima-python/graphs/contributors&#34;&gt;contributors&lt;/a&gt; who are doing a great job of actively improving the project. Many thanks to all contributors, especially &lt;a href=&#34;https://github.com/darius&#34;&gt;@darius&lt;/a&gt;, &lt;a href=&#34;https://github.com/SnShine&#34;&gt;@SnShine&lt;/a&gt;, &lt;a href=&#34;https://github.com/reachtarunhere&#34;&gt;@reachtarunhere&lt;/a&gt;, &lt;a href=&#34;https://github.com/antmarakis&#34;&gt;@antmarakis&lt;/a&gt;, &lt;a href=&#34;https://github.com/Chipe1&#34;&gt;@Chipe1&lt;/a&gt;, &lt;a href=&#34;https://github.com/ad71&#34;&gt;@ad71&lt;/a&gt; and &lt;a href=&#34;https://github.com/MariannaSpyrakou&#34;&gt;@MariannaSpyrakou&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;!--Reference Links--&gt;</summary>
  </entry>
  <entry>
    <title>aws-samples/aws-machine-learning-university-accelerated-nlp</title>
    <updated>2022-06-03T01:43:20Z</updated>
    <id>tag:github.com,2022-06-03:/aws-samples/aws-machine-learning-university-accelerated-nlp</id>
    <link href="https://github.com/aws-samples/aws-machine-learning-university-accelerated-nlp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Machine Learning University: Accelerated Natural Language Processing Class&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aws-samples/aws-machine-learning-university-accelerated-nlp/master/data/MLU_Logo.png&#34; alt=&#34;logo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Machine Learning University: Accelerated Natural Language Processing Class&lt;/h2&gt; &#xA;&lt;p&gt;This repository contains &lt;strong&gt;slides&lt;/strong&gt;, &lt;strong&gt;notebooks&lt;/strong&gt; and &lt;strong&gt;datasets&lt;/strong&gt; for the &lt;strong&gt;Machine Learning University (MLU) Accelerated Natural Language Processing&lt;/strong&gt; class. Our mission is to make Machine Learning accessible to everyone. We have courses available across many topics of machine learning and believe knowledge of ML can be a key enabler for success. This class is designed to help you get started with Natural Language Processing (NLP), learn widely used techniques and apply them on real-world problems.&lt;/p&gt; &#xA;&lt;h2&gt;YouTube&lt;/h2&gt; &#xA;&lt;p&gt;Watch all NLP class video recordings in this &lt;a href=&#34;https://www.youtube.com/playlist?list=PL8P_Z6C4GcuWfAq8Pt6PBYlck4OprHXsw&#34;&gt;YouTube playlist&lt;/a&gt; from our &lt;a href=&#34;https://www.youtube.com/channel/UC12LqyqTQYbXatYS9AA7Nuw/playlists&#34;&gt;YouTube channel&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PL8P_Z6C4GcuWfAq8Pt6PBYlck4OprHXsw&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/0FXKbEgz-uU/0.jpg&#34; alt=&#34;Playlist&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Course Overview&lt;/h2&gt; &#xA;&lt;p&gt;There are three lectures and one final project in this class. Course overview is below.&lt;/p&gt; &#xA;&lt;p&gt;Lecture 1&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;title&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;studio lab&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to ML&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Intro to NLP and Text Processing&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/aws-samples/aws-machine-learning-university-accelerated-nlp/blob/master/notebooks/MLA-NLP-Lecture1-Text-Process.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Bag of Words (BoW)&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/aws-samples/aws-machine-learning-university-accelerated-nlp/blob/master/notebooks/MLA-NLP-Lecture1-BOW.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;K Nearest Neighbors (KNN)&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/aws-samples/aws-machine-learning-university-accelerated-nlp/blob/master/notebooks/MLA-NLP-Lecture1-KNN.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Final Project&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/aws-samples/aws-machine-learning-university-accelerated-nlp/blob/master/notebooks/MLA-NLP-Lecture1-Final-Project.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Lecture 2&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;title&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;studio lab&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Tree-based Models&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github//aws-samples/aws-machine-learning-university-accelerated-nlp/blob/master/notebooks/MLA-NLP-Lecture2-Tree-Models.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Regression Models&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;Linear Regression &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/aws-samples/aws-machine-learning-university-accelerated-nlp/blob/master/notebooks/MLA-NLP-Lecture2-Linear-Regression.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In Studio Lab&#34;&gt;&lt;/a&gt; &lt;br&gt; Logistic Regression &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/aws-samples/aws-machine-learning-university-accelerated-nlp/blob/master/notebooks/MLA-NLP-Lecture2-Logistic-Regression.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Optimization-Regularization&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Hyperparameter Tuning&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;AWS AI/ML Services&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/aws-samples/aws-machine-learning-university-accelerated-nlp/blob/master/notebooks/MLA-NLP-Lecture2-Sagemaker.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Final Project&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/aws-samples/aws-machine-learning-university-accelerated-nlp/blob/master/notebooks/MLA-NLP-Lecture2-Final-Project.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Lecture 3&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;title&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;studio lab&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Neural Networks&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/aws-samples/aws-machine-learning-university-accelerated-nlp/blob/master/notebooks/MLA-NLP-Lecture3-Neural-Networks.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Word Embeddings&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/aws-samples/aws-machine-learning-university-accelerated-nlp/blob/master/notebooks/MLA-NLP-Lecture3-Word-Vectors.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Recurrent Neural Networks (RNN)&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github//aws-samples/aws-machine-learning-university-accelerated-nlp/blob/master/notebooks/MLA-NLP-Lecture3-Recurrent-Neural-Networks.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Transformers&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/aws-samples/aws-machine-learning-university-accelerated-nlp/blob/master/notebooks/MLA-NLP-Lecture3-Neural-Networks.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Final Project&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/aws-samples/aws-machine-learning-university-accelerated-nlp/blob/master/notebooks/MLA-NLP-Lecture3-Final-Project.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Final Project:&lt;/strong&gt; Practice working with a &#34;real-world&#34; NLP dataset for the final project. Final project dataset is in the &lt;a href=&#34;https://github.com/aws-samples/aws-machine-learning-university-accelerated-nlp/tree/master/data/final_project&#34;&gt;data/final_project folder&lt;/a&gt;. For more details on the final project, check out &lt;a href=&#34;https://github.com/aws-samples/aws-machine-learning-university-accelerated-nlp/raw/master/notebooks/MLA-NLP-Lecture1-Final-Project.ipynb&#34;&gt;this notebook&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Interactives/Visuals&lt;/h2&gt; &#xA;&lt;p&gt;Interested in visual, interactive explanations of core machine learning concepts? Check out our &lt;a href=&#34;https://mlu-explain.github.io/&#34;&gt;MLU-Explain articles&lt;/a&gt; to learn at your own pace!&lt;/p&gt; &#xA;&lt;h2&gt;Contribute&lt;/h2&gt; &#xA;&lt;p&gt;If you would like to contribute to the project, see &lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-machine-learning-university-accelerated-nlp/master/CONTRIBUTING.md&#34;&gt;CONTRIBUTING&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The license for this repository depends on the section. Data set for the course is being provided to you by permission of Amazon and is subject to the terms of the &lt;a href=&#34;https://www.amazon.com/gp/help/customer/display.html?nodeId=201909000&#34;&gt;Amazon License and Access&lt;/a&gt;. You are expressly prohibited from copying, modifying, selling, exporting or using this data set in any way other than for the purpose of completing this course. The lecture slides are released under the CC-BY-SA-4.0 License. The code examples are released under the MIT-0 License. See each section&#39;s LICENSE file for details.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Azure/Azure-Sentinel</title>
    <updated>2022-06-03T01:43:20Z</updated>
    <id>tag:github.com,2022-06-03:/Azure/Azure-Sentinel</id>
    <link href="https://github.com/Azure/Azure-Sentinel" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Cloud-native SIEM for intelligent security analytics for your entire enterprise.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Microsoft Sentinel and Microsoft 365 Defender&lt;/h1&gt; &#xA;&lt;p&gt;Welcome to the unified Microsoft Sentinel and Microsoft 365 Defender repository! This repository contains out of the box detections, exploration queries, hunting queries, workbooks, playbooks and much more to help you get ramped up with Microsoft Sentinel and provide you security content to secure your environment and hunt for threats. The hunting queries also include Microsoft 365 Defender hunting queries for advanced hunting scenarios in both Microsoft 365 Defender and Microsoft Sentinel. You can also submit to &lt;a href=&#34;https://github.com/Azure/Azure-Sentinel/issues&#34;&gt;issues&lt;/a&gt; for any samples or resources you would like to see here as you onboard to Microsoft Sentinel. This repository welcomes contributions and refer to this repository&#39;s &lt;a href=&#34;https://aka.ms/threathunters&#34;&gt;wiki&lt;/a&gt; to get started. For questions and feedback, please contact &lt;a href=&#34;https://raw.githubusercontent.com/Azure/Azure-Sentinel/master/AzureSentinel@microsoft.com&#34;&gt;AzureSentinel@microsoft.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Resources&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://go.microsoft.com/fwlink/?linkid=2073774&amp;amp;clcid=0x409&#34;&gt;Microsoft Sentinel documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/microsoft-365/security/defender/microsoft-365-defender?view=o365-worldwide&#34;&gt;Microsoft 365 Defender documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/securitywebinars&#34;&gt;Security Community Webinars&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://help.github.com/en#dotcom&#34;&gt;Getting started with GitHub&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We value your feedback. Here are some channels to help surface your questions or feedback:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;General product specific Q&amp;amp;A for SIEM and SOAR - Join in the &lt;a href=&#34;https://techcommunity.microsoft.com/t5/microsoft-sentinel/bd-p/MicrosoftSentinel&#34;&gt;Microsoft Sentinel Tech Community conversations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;General product specific Q&amp;amp;A for XDR - Join in the &lt;a href=&#34;https://techcommunity.microsoft.com/t5/microsoft-365-defender/bd-p/MicrosoftThreatProtection&#34;&gt;Microsoft 365 Defender Tech Community conversations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Product specific feature requests - Upvote or post new on &lt;a href=&#34;https://feedback.azure.com/d365community/forum/37638d17-0625-ec11-b6e6-000d3a4f07b8&#34;&gt;Microsoft Sentinel feedback forums&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Report product or contribution bugs - File a GitHub Issue using &lt;a href=&#34;https://github.com/Azure/Azure-Sentinel/issues/new?assignees=&amp;amp;labels=&amp;amp;template=bug_report.md&amp;amp;title=&#34;&gt;Bug template&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;General feedback on community and contribution process - File a GitHub Issue using &lt;a href=&#34;https://github.com/Azure/Azure-Sentinel/issues/new?assignees=&amp;amp;labels=&amp;amp;template=feature_request.md&amp;amp;title=&#34;&gt;Feature Request template&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href=&#34;https://cla.microsoft.com&#34;&gt;https://cla.microsoft.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Add in your new or updated contributions to GitHub&lt;/h2&gt; &#xA;&lt;p&gt;Note: If you are a first time contributor to this repository, &lt;a href=&#34;https://docs.github.com/github/getting-started-with-github/fork-a-repo&#34;&gt;General GitHub Fork the repo guidance&lt;/a&gt; before cloning or &lt;a href=&#34;https://github.com/Azure/Azure-Sentinel/raw/master/GettingStarted.md&#34;&gt;Specific steps for the Sentinel repo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;General Steps&lt;/h2&gt; &#xA;&lt;p&gt;Brand new or update to a contribution via these methods:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Submit for review directly on GitHub website &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Browse to the folder you want to upload your file to&lt;/li&gt; &#xA;   &lt;li&gt;Choose Upload Files and browse to your file.&lt;/li&gt; &#xA;   &lt;li&gt;You will be required to create your own branch and then submit the Pull Request for review.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Use &lt;a href=&#34;https://help.github.com/en/desktop/getting-started-with-github-desktop&#34;&gt;GitHub Desktop&lt;/a&gt; or &lt;a href=&#34;https://visualstudio.microsoft.com/vs/&#34;&gt;Visual Studio&lt;/a&gt; or &lt;a href=&#34;https://code.visualstudio.com/?wt.mc_id=DX_841432&#34;&gt;VSCode&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://docs.github.com/github/getting-started-with-github/fork-a-repo&#34;&gt;Fork the repo&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://help.github.com/en/github/creating-cloning-and-archiving-repositories/cloning-a-repository&#34;&gt;Clone the repo&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://help.github.com/en/desktop/contributing-to-projects/creating-a-branch-for-your-work&#34;&gt;Create your own branch&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Do your additions/updates in GitHub Desktop&lt;/li&gt; &#xA;   &lt;li&gt;Be sure to merge master back to your branch before you push.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://help.github.com/en/github/using-git/pushing-commits-to-a-remote-repository&#34;&gt;Push your changes to GitHub&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Pull Request&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;After you push your changes, you will need to submit the &lt;a href=&#34;https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/about-pull-requests&#34;&gt;Pull Request (PR)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Details about the Proposed Changes are required, be sure to include a minimal level of detail so a review can clearly understand the reason for the change and what he change is related to in the code.&lt;/li&gt; &#xA; &lt;li&gt;After submission, check the &lt;a href=&#34;https://github.com/Azure/Azure-Sentinel/pulls&#34;&gt;Pull Request&lt;/a&gt; for comments&lt;/li&gt; &#xA; &lt;li&gt;Make changes as suggested and update your branch or explain why no change is needed. Resolve the comment when done.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Pull Request Detection Template Structure Validation Check&lt;/h3&gt; &#xA;&lt;p&gt;As part of the PR checks we run a structure validation to make sure all required parts of the YAML structure are included. For Detections, there is a new section that must be included. See the &lt;a href=&#34;https://github.com/Azure/Azure-Sentinel/wiki/Contribute-to-Sentinel-GitHub-Community-of-Queries#now-onto-the-how&#34;&gt;contribution guidelines&lt;/a&gt; for more information. If this section or any other required section is not included, then a validation error will occur similar to the below. The example is specifically if the YAML is missing the entityMappings section:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;A total of 1 test files matched the specified pattern.&#xA;[xUnit.net 00:00:00.95]     Kqlvalidations.Tests.DetectionTemplateStructureValidationTests.Validate_DetectionTemplates_HaveValidTemplateStructure(detectionsYamlFileName: &#34;ExcessiveBlockedTrafficGeneratedbyUser.yaml&#34;) [FAIL]&#xA;  X Kqlvalidations.Tests.DetectionTemplateStructureValidationTests.Validate_DetectionTemplates_HaveValidTemplateStructure(detectionsYamlFileName: &#34;ExcessiveBlockedTrafficGeneratedbyUser.yaml&#34;) [104ms]&#xA;  Error Message:&#xA;   Expected object to be &amp;lt;null&amp;gt;, but found System.ComponentModel.DataAnnotations.ValidationException with message &#34;An old mapping for entity &#39;AccountCustomEntity&#39; does not have a matching new mapping entry.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Pull Request Kql Validation Check&lt;/h3&gt; &#xA;&lt;p&gt;As part of the PR checks we run a syntax validation of the kql queries defined in the template. If this check fails go to Azure Pipeline (by pressing on the errors link on the checks tab in your PR) &lt;img src=&#34;https://raw.githubusercontent.com/Azure/Azure-Sentinel/master/.github/Media/Azurepipeline.png&#34; alt=&#34;Azurepipeline&#34;&gt; In the pipeline you can see which test failed and what is the cause: &lt;img src=&#34;https://raw.githubusercontent.com/Azure/Azure-Sentinel/master/.github/Media/PipelineTestsTab.png&#34; alt=&#34;Pipeline Tests Tab&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Example error message:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;A total of 1 test files matched the specified pattern.&#xA;[xUnit.net 00:00:01.81]     Kqlvalidations.Tests.KqlValidationTests.Validate_DetectionQueries_HaveValidKql(detectionsYamlFileName: &#34;ExcessiveBlockedTrafficGeneratedbyUser.yaml&#34;) [FAIL]&#xA;  X Kqlvalidations.Tests.KqlValidationTests.Validate_DetectionQueries_HaveValidKql(detectionsYamlFileName: &#34;ExcessiveBlockedTrafficGeneratedbyUser.yaml&#34;) [21ms]&#xA;  Error Message:&#xA;   Template Id:fa0ab69c-7124-4f62-acdd-61017cf6ce89 is not valid Errors:The name &#39;SymantecEndpointProtection&#39; does not refer to any known table, tabular variable or function., Code: &#39;KS204&#39;, Severity: &#39;Error&#39;, Location: &#39;67..93&#39;,The name &#39;SymantecEndpointProtection&#39; does not refer to any known table, tabular variable or function., Code: &#39;KS204&#39;, Severity: &#39;Error&#39;, Location: &#39;289..315&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are using custom logs table (a table which is not defined on all workspaces by default) you should verify your table schema is defined in json file in the folder &lt;em&gt;Azure-Sentinel\.script\tests\KqlvalidationsTests\CustomTables&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example for table tablexyz.json&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;Name&#34;: &#34;tablexyz&#34;,&#xA;  &#34;Properties&#34;: [&#xA;    {&#xA;      &#34;Name&#34;: &#34;SomeDateTimeColumn&#34;,&#xA;      &#34;Type&#34;: &#34;DateTime&#34;&#xA;    },&#xA;    {&#xA;      &#34;Name&#34;: &#34;SomeStringColumn&#34;,&#xA;      &#34;Type&#34;: &#34;String&#34;&#xA;    },&#xA;    {&#xA;      &#34;Name&#34;: &#34;SomeDynamicColumn&#34;,&#xA;      &#34;Type&#34;: &#34;Dynamic&#34;&#xA;    }&#xA;  ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Run Kql Validation Locally&lt;/h3&gt; &#xA;&lt;p&gt;In order to run the kql validation before submitting Pull Request in you local machine:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You need to have &lt;strong&gt;.Net Core 3.1 SDK&lt;/strong&gt; installed &lt;a href=&#34;https://dotnet.microsoft.com/download&#34;&gt;How to download .Net&lt;/a&gt; (Supports all platforms)&lt;/li&gt; &#xA; &lt;li&gt;Open Shell and navigate to &lt;code&gt;Azure-Sentinel\\.script\tests\KqlvalidationsTests\&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Execute &lt;code&gt;dotnet test&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Example of output (in Ubuntu):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Welcome to .NET Core 3.1!&#xA;---------------------&#xA;SDK Version: 3.1.403&#xA;&#xA;Telemetry&#xA;---------&#xA;The .NET Core tools collect usage data in order to help us improve your experience. The data is anonymous. It is collected by Microsoft and shared with the community. You can opt-out of telemetry by setting the DOTNET_CLI_TELEMETRY_OPTOUT environment variable to &#39;1&#39; or &#39;true&#39; using your favorite shell.&#xA;&#xA;Read more about .NET Core CLI Tools telemetry: https://aka.ms/dotnet-cli-telemetry&#xA;&#xA;----------------&#xA;Explore documentation: https://aka.ms/dotnet-docs&#xA;Report issues and find source on GitHub: https://github.com/dotnet/core&#xA;Find out what&#39;s new: https://aka.ms/dotnet-whats-new&#xA;Learn about the installed HTTPS developer cert: https://aka.ms/aspnet-core-https&#xA;Use &#39;dotnet --help&#39; to see available commands or visit: https://aka.ms/dotnet-cli-docs&#xA;Write your first app: https://aka.ms/first-net-core-app&#xA;--------------------------------------------------------------------------------------&#xA;Test run for /mnt/c/git/Azure-Sentinel/.script/tests/KqlvalidationsTests/bin/Debug/netcoreapp3.1/Kqlvalidations.Tests.dll(.NETCoreApp,Version=v3.1)&#xA;Microsoft (R) Test Execution Command Line Tool Version 16.7.0&#xA;Copyright (c) Microsoft Corporation.  All rights reserved.&#xA;&#xA;Starting test execution, please wait...&#xA;&#xA;A total of 1 test files matched the specified pattern.&#xA;&#xA;Test Run Successful.&#xA;Total tests: 171&#xA;     Passed: 171&#xA; Total time: 25.7973 Seconds&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Detection schema validation tests&lt;/h3&gt; &#xA;&lt;p&gt;Similarly to KQL Validation, there is an automatic validation of the schema of a detection. The schema validation includes the detection&#39;s frequency and period, the detection&#39;s trigger type and threshold, validity of connectors Ids (&lt;a href=&#34;https://github.com/Azure/Azure-Sentinel/raw/master/.script/tests/detectionTemplateSchemaValidation/ValidConnectorIds.json&#34;&gt;valid connectors Ids list&lt;/a&gt;), etc. A wrong format or missing attributes will result with an informative check failure, which should guide you through the resolution of the issue, but make sure to look into the format of already approved detection.&lt;/p&gt; &#xA;&lt;h3&gt;Run Detection Schema Validation Locally&lt;/h3&gt; &#xA;&lt;p&gt;In order to run the kql validation before submitting Pull Request in you local machine:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You need to have &lt;strong&gt;.Net Core 3.1 SDK&lt;/strong&gt; installed &lt;a href=&#34;https://dotnet.microsoft.com/download&#34;&gt;How to download .Net&lt;/a&gt; (Supports all platforms)&lt;/li&gt; &#xA; &lt;li&gt;Open Shell and navigate to &lt;code&gt;Azure-Sentinel\\.script\tests\DetectionTemplateSchemaValidation\&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Execute &lt;code&gt;dotnet test&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;When you submit a pull request, a CLA-bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; &#xA;&lt;p&gt;For information on what you can contribute and further details, refer to the &lt;a href=&#34;https://github.com/Azure/Azure-Sentinel/wiki#get-started&#34;&gt;&#34;get started&#34;&lt;/a&gt; section on the project&#39;s &lt;a href=&#34;https://aka.ms/threathunters&#34;&gt;wiki&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>EssayKillerBrain/EssayTopicPredict</title>
    <updated>2022-06-03T01:43:20Z</updated>
    <id>tag:github.com,2022-06-03:/EssayKillerBrain/EssayTopicPredict</id>
    <link href="https://github.com/EssayKillerBrain/EssayTopicPredict" rel="alternate"></link>
    <summary type="html">&lt;p&gt;é«˜è€ƒä½œæ–‡é¢˜ç›®é¢„æµ‹æ¨¡å‹ v1.0&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;EssayTopicPredict&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache--2.0-green&#34; alt=&#34;image&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/License-MIT-orange&#34; alt=&#34;image&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/License-Anti--996-red&#34; alt=&#34;image&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/pypi-v0.0.1a4-yellowgreen&#34; alt=&#34;image&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/stars-%3C%201k-blue&#34; alt=&#34;image&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/issues-1%20open-brightgreen&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;é€šç”¨å‹é«˜è€ƒä½œæ–‡é¢˜ç›®é¢„æµ‹æ¨¡å‹ v1.0 -äººå·¥æ™ºèƒ½æ¡†æ¶ï¼Œä»…é™äº¤æµä¸ç§‘æ™®ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;é¡¹ç›®ç®€ä»‹&lt;/h2&gt; &#xA;&lt;p&gt;EssayTopicPredictæ˜¯åŸºäºæ— ç›‘ç£å­¦ä¹ ã€æ¨¡å¼è¯†åˆ«ä¸NLPé¢†åŸŸçš„æœ€æ–°æ¨¡å‹æ‰€æ„å»ºçš„ç”Ÿæˆå¼è€ƒè¯•é¢˜ç›®AIæ¡†æ¶ï¼Œç›®å‰ç¬¬ä¸€ç‰ˆfinetuneæ¨¡å‹é’ˆå¯¹é«˜è€ƒä½œæ–‡ï¼Œå¯ä»¥æœ‰æ•ˆç”Ÿæˆç¬¦åˆäººç±»è®¤çŸ¥çš„æ–‡ç« é¢˜ç›®ã€‚&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;é¡¹ç›®ä½œè€…&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;ä¸»é¡µ1&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;ä¸»é¡µ2&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;ä¸»é¡µ3&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;å›¾çµçš„çŒ«&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.zhihu.com/people/dong-xi-97-29&#34;&gt;çŸ¥ä¹&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://space.bilibili.com/371846699&#34;&gt;Bç«™&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.youtube.com/channel/UCoEVP6iTw5sfozUGLLWJyDg/featured&#34;&gt;Youtube&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;è‡´è°¢&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;æ„Ÿè°¢å¼€æºä½œè€…&lt;a href=&#34;https://github.com/imcaspar&#34;&gt;@imcaspar&lt;/a&gt; æä¾›GPT-2ä¸­æ–‡é¢„è®­ç»ƒæ¡†æ¶ä¸æ•°æ®æ”¯æŒã€‚ &lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;æ¡†æ¶è¯´æ˜&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; åŸºäºå“ˆå·¥å¤§RoBerta-WWM-EXTã€Bertopicã€GANæ¨¡å‹çš„é«˜è€ƒé¢˜ç›®é¢„æµ‹AI&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; æ”¯æŒbert tokenizerï¼Œå½“å‰ç‰ˆæœ¬åŸºäºclue chinese vocab&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 17äº¿å‚æ•°å¤šæ¨¡å—å¼‚æ„æ·±åº¦ç¥ç»ç½‘ç»œï¼Œè¶…2äº¿æ¡é¢„è®­ç»ƒæ•°æ®&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; å¯ç»“åˆä½œæ–‡ç”Ÿæˆå™¨ä¸€èµ·ä½¿ç”¨ï¼š&lt;a href=&#34;https://colab.research.google.com/github/EssayKillerBrain/EssayKiller_V2/blob/master/colab_online.ipynb&#34;&gt;17äº¿å‚æ•°ä½œæ–‡æ€æ‰‹&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; ç«¯åˆ°ç«¯ç”Ÿæˆï¼Œä»è¯•å·è¯†åˆ«åˆ°ç­”é¢˜å¡è¾“å‡ºä¸€æ¡é¾™æœåŠ¡&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;æœ¬åœ°ç¯å¢ƒ&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ubuntu 18.04.2/ Windows10 x86&lt;/li&gt; &#xA; &lt;li&gt;Pandas 0.24.2&lt;/li&gt; &#xA; &lt;li&gt;Regex 2019.4.14&lt;/li&gt; &#xA; &lt;li&gt;h5py 2.9.0&lt;/li&gt; &#xA; &lt;li&gt;Numpy 1.16.2&lt;/li&gt; &#xA; &lt;li&gt;Tensorboard 1.15.2&lt;/li&gt; &#xA; &lt;li&gt;Tensorflow-gpu 1.15.2&lt;/li&gt; &#xA; &lt;li&gt;Requests 2.22.0&lt;/li&gt; &#xA; &lt;li&gt;OpenCV 3.4.2&lt;/li&gt; &#xA; &lt;li&gt;CUDA &amp;gt;= 10.0&lt;/li&gt; &#xA; &lt;li&gt;CuDNN &amp;gt;= 7.6.0&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;å¼€å‘æ—¥å¿—&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2022.04.23 æœ¬åœ°Gité¡¹ç›®å»ºç«‹&lt;/li&gt; &#xA; &lt;li&gt;2022.05.03 æ•´ä½“æ¨¡å‹æ¶æ„æ­å»ºï¼Œå¼€å§‹è¯­æ–™æ”¶é›†&lt;/li&gt; &#xA; &lt;li&gt;2022.05.13 æ•°æ®é›†æ¸…æ´—ã€è¯­æ–™å¤„ç†&lt;/li&gt; &#xA; &lt;li&gt;2022.05.21 Bertopic+DBSCANèšç±»ç®—æ³•&lt;/li&gt; &#xA; &lt;li&gt;2022.05.31 RoBertaä¸æ‘˜è¦æ¨¡å‹è°ƒæ•´&lt;/li&gt; &#xA; &lt;li&gt;2022.05.30 ä»£ç Reviewä¸å¼€æºå‘å¸ƒ&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;æ¨¡å‹ç»“æ„&lt;/h2&gt; &#xA;&lt;p&gt;æ•´ä¸ªæ¡†æ¶åˆ†ä¸ºProprocessã€Bertã€DNSCAN 3ä¸ªæ¨¡å—ï¼Œæ¯ä¸ªæ¨¡å—çš„ç½‘ç»œå•ç‹¬è®­ç»ƒï¼Œå‚æ•°ç›¸äº’ç‹¬ç«‹ã€‚&lt;/p&gt; &#xA;&lt;h3&gt;1. ä¾‹å­&lt;/h3&gt; &#xA;&lt;p&gt;é«˜è€ƒè¯­æ–‡è¯•å·ä½œæ–‡é¢˜&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://images.shobserver.com/img/2020/7/7/37b2224ee3de441a8a040cb4f5576c2d.jpg&#34; alt=&#34;æµ™æ±Ÿå·&#34;&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;æ•°æ®å‡†å¤‡&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;äººæ°‘æ—¥æŠ¥ã€å¤®è§†æ–°é—»ã€å¾®åšå®¢æˆ·ç«¯ã€äººæ°‘ç½‘4ä¸ªä¸»è¦çˆ¬è™«æ¸ é“ï¼Œé€šè¿‡ä¸åŒAPIè¿›è¡Œçˆ¬å–ï¼ˆæ—¶é—´ä¸ºè¿‡å»12ä¸ªæœˆå†…ï¼‰&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;ä¿®æ”¹/train/config.pyä¸­train_data_rootï¼Œvalidation_data_rootä»¥åŠimage_path&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;è®­ç»ƒ&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd train  &#xA;python train.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;è®­ç»ƒç»“æœ&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Epoch 3/100&#xA;25621/25621 [==============================] - 15856s 619ms/step - loss: 0.1035 - acc: 0.9816 - val_loss: 0.1060 - val_acc: 0.9823&#xA;Epoch 4/100&#xA;25621/25621 [==============================] - 15651s 611ms/step - loss: 0.0798 - acc: 0.9879 - val_loss: 0.0848 - val_acc: 0.9878&#xA;Epoch 5/100&#xA;25621/25621 [==============================] - 16510s 644ms/step - loss: 0.0732 - acc: 0.9889 - val_loss: 0.0815 - val_acc: 0.9881&#xA;Epoch 6/100&#xA;25621/25621 [==============================] - 15621s 610ms/step - loss: 0.0691 - acc: 0.9895 - val_loss: 0.0791 - val_acc: 0.9886&#xA;Epoch 7/100&#xA;25621/25621 [==============================] - 15782s 616ms/step - loss: 0.0666 - acc: 0.9899 - val_loss: 0.0787 - val_acc: 0.9887&#xA;Epoch 8/100&#xA;25621/25621 [==============================] - 15560s 607ms/step - loss: 0.0645 - acc: 0.9903 - val_loss: 0.0771 - val_acc: 0.9888&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;h3&gt;2. ç½‘ç»œç»“æ„&lt;/h3&gt; &#xA;&lt;h4&gt;2.1 BERT&lt;/h4&gt; &#xA;&lt;p&gt;&lt;strong&gt;Whole Word Masking (wwm)&lt;/strong&gt;ï¼Œæš‚ç¿»è¯‘ä¸º&lt;code&gt;å…¨è¯Mask&lt;/code&gt;æˆ–&lt;code&gt;æ•´è¯Mask&lt;/code&gt;ï¼Œæ˜¯è°·æ­Œåœ¨2019å¹´5æœˆ31æ—¥å‘å¸ƒçš„ä¸€é¡¹BERTçš„å‡çº§ç‰ˆæœ¬ï¼Œä¸»è¦æ›´æ”¹äº†åŸé¢„è®­ç»ƒé˜¶æ®µçš„è®­ç»ƒæ ·æœ¬ç”Ÿæˆç­–ç•¥ã€‚ ç®€å•æ¥è¯´ï¼ŒåŸæœ‰åŸºäºWordPieceçš„åˆ†è¯æ–¹å¼ä¼šæŠŠä¸€ä¸ªå®Œæ•´çš„è¯åˆ‡åˆ†æˆè‹¥å¹²ä¸ªå­è¯ï¼Œåœ¨ç”Ÿæˆè®­ç»ƒæ ·æœ¬æ—¶ï¼Œè¿™äº›è¢«åˆ†å¼€çš„å­è¯ä¼šéšæœºè¢«maskã€‚ åœ¨&lt;code&gt;å…¨è¯Mask&lt;/code&gt;ä¸­ï¼Œå¦‚æœä¸€ä¸ªå®Œæ•´çš„è¯çš„éƒ¨åˆ†WordPieceå­è¯è¢«maskï¼Œåˆ™åŒå±è¯¥è¯çš„å…¶ä»–éƒ¨åˆ†ä¹Ÿä¼šè¢«maskï¼Œå³&lt;code&gt;å…¨è¯Mask&lt;/code&gt;ã€‚&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™é‡Œçš„maskæŒ‡çš„æ˜¯å¹¿ä¹‰çš„maskï¼ˆæ›¿æ¢æˆ[MASK]ï¼›ä¿æŒåŸè¯æ±‡ï¼›éšæœºæ›¿æ¢æˆå¦å¤–ä¸€ä¸ªè¯ï¼‰ï¼Œå¹¶éåªå±€é™äºå•è¯æ›¿æ¢æˆ&lt;code&gt;[MASK]&lt;/code&gt;æ ‡ç­¾çš„æƒ…å†µã€‚ æ›´è¯¦ç»†çš„è¯´æ˜åŠæ ·ä¾‹è¯·å‚è€ƒï¼š&lt;a href=&#34;https://github.com/ymcui/Chinese-BERT-wwm/issues/4&#34;&gt;#4&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;åŒç†ï¼Œç”±äºè°·æ­Œå®˜æ–¹å‘å¸ƒçš„&lt;code&gt;BERT-base, Chinese&lt;/code&gt;ä¸­ï¼Œä¸­æ–‡æ˜¯ä»¥&lt;strong&gt;å­—&lt;/strong&gt;ä¸ºç²’åº¦è¿›è¡Œåˆ‡åˆ†ï¼Œæ²¡æœ‰è€ƒè™‘åˆ°ä¼ ç»ŸNLPä¸­çš„ä¸­æ–‡åˆ†è¯ï¼ˆCWSï¼‰ã€‚ æˆ‘ä»¬å°†å…¨è¯Maskçš„æ–¹æ³•åº”ç”¨åœ¨äº†ä¸­æ–‡ä¸­ï¼Œä½¿ç”¨äº†ä¸­æ–‡ç»´åŸºç™¾ç§‘ï¼ˆåŒ…æ‹¬ç®€ä½“å’Œç¹ä½“ï¼‰è¿›è¡Œè®­ç»ƒï¼Œå¹¶ä¸”ä½¿ç”¨äº†&lt;a href=&#34;http://ltp.ai&#34;&gt;å“ˆå·¥å¤§LTP&lt;/a&gt;ä½œä¸ºåˆ†è¯å·¥å…·ï¼Œå³å¯¹ç»„æˆåŒä¸€ä¸ª&lt;strong&gt;è¯&lt;/strong&gt;çš„æ±‰å­—å…¨éƒ¨è¿›è¡ŒMaskã€‚&lt;/p&gt; &#xA;&lt;p&gt;ä¸‹è¿°æ–‡æœ¬å±•ç¤ºäº†&lt;code&gt;å…¨è¯Mask&lt;/code&gt;çš„ç”Ÿæˆæ ·ä¾‹ã€‚ &lt;strong&gt;æ³¨æ„ï¼šä¸ºäº†æ–¹ä¾¿ç†è§£ï¼Œä¸‹è¿°ä¾‹å­ä¸­åªè€ƒè™‘æ›¿æ¢æˆ[MASK]æ ‡ç­¾çš„æƒ…å†µã€‚&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;è¯´æ˜&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;æ ·ä¾‹&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;åŸå§‹æ–‡æœ¬&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ä½¿ç”¨è¯­è¨€æ¨¡å‹æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªè¯çš„probabilityã€‚&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;åˆ†è¯æ–‡æœ¬&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ä½¿ç”¨ è¯­è¨€ æ¨¡å‹ æ¥ é¢„æµ‹ ä¸‹ ä¸€ä¸ª è¯ çš„ probability ã€‚&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;åŸå§‹Maskè¾“å…¥&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ä½¿ ç”¨ è¯­ è¨€ [MASK] å‹ æ¥ [MASK] æµ‹ ä¸‹ ä¸€ ä¸ª è¯ çš„ pro [MASK] ##lity ã€‚&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;å…¨è¯Maskè¾“å…¥&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ä½¿ ç”¨ è¯­ è¨€ [MASK] [MASK] æ¥ [MASK] [MASK] ä¸‹ ä¸€ ä¸ª è¯ çš„ [MASK] [MASK] [MASK] ã€‚&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;2.2 DBSCAN&lt;/h4&gt; &#xA;&lt;p&gt;åŸºäºå¯†åº¦çš„å™ªå£°åº”ç”¨ç©ºé—´èšç±»(DBSCAN)æ˜¯ä¸€ç§æ— ç›‘ç£çš„MLèšç±»ç®—æ³•ã€‚æ— ç›‘ç£çš„æ„æ€æ˜¯å®ƒä¸ä½¿ç”¨é¢„å…ˆæ ‡è®°çš„ç›®æ ‡æ¥èšç±»æ•°æ®ç‚¹ã€‚èšç±»æ˜¯æŒ‡è¯•å›¾å°†ç›¸ä¼¼çš„æ•°æ®ç‚¹åˆ†ç»„åˆ°äººå·¥ç¡®å®šçš„ç»„æˆ–ç°‡ä¸­ã€‚å®ƒå¯ä»¥æ›¿ä»£KMeanså’Œå±‚æ¬¡èšç±»ç­‰æµè¡Œçš„èšç±»ç®—æ³•ã€‚&lt;/p&gt; &#xA;&lt;p&gt;KMeans vs DBSCANï¼š KMeanså°¤å…¶å®¹æ˜“å—åˆ°å¼‚å¸¸å€¼çš„å½±å“ã€‚å½“ç®—æ³•éå†è´¨å¿ƒæ—¶ï¼Œåœ¨è¾¾åˆ°ç¨³å®šæ€§å’Œæ”¶æ•›æ€§ä¹‹å‰ï¼Œç¦»ç¾¤å€¼å¯¹è´¨å¿ƒçš„ç§»åŠ¨æ–¹å¼æœ‰æ˜¾è‘—çš„å½±å“ã€‚æ­¤å¤–ï¼ŒKMeansåœ¨é›†ç¾¤å¤§å°å’Œå¯†åº¦ä¸åŒçš„æƒ…å†µä¸‹è¿˜å­˜åœ¨æ•°æ®ç²¾ç¡®èšç±»çš„é—®é¢˜ã€‚K-Meansåªèƒ½åº”ç”¨çƒå½¢ç°‡ï¼Œå¦‚æœæ•°æ®ä¸æ˜¯çƒå½¢çš„ï¼Œå®ƒçš„å‡†ç¡®æ€§å°±ä¼šå—åˆ°å½±å“ã€‚æœ€åï¼ŒKMeansè¦æ±‚æˆ‘ä»¬é¦–å…ˆé€‰æ‹©å¸Œæœ›æ‰¾åˆ°çš„é›†ç¾¤çš„æ•°é‡ã€‚&lt;/p&gt; &#xA;&lt;p&gt;å¦ä¸€æ–¹é¢ï¼ŒDBSCANä¸è¦æ±‚æˆ‘ä»¬æŒ‡å®šé›†ç¾¤çš„æ•°é‡ï¼Œé¿å…äº†å¼‚å¸¸å€¼ï¼Œå¹¶ä¸”åœ¨ä»»æ„å½¢çŠ¶å’Œå¤§å°çš„é›†ç¾¤ä¸­å·¥ä½œå¾—éå¸¸å¥½ã€‚å®ƒæ²¡æœ‰è´¨å¿ƒï¼Œèšç±»ç°‡æ˜¯é€šè¿‡å°†ç›¸é‚»çš„ç‚¹è¿æ¥åœ¨ä¸€èµ·çš„è¿‡ç¨‹å½¢æˆçš„ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;ä¸­æ–‡æ¨¡å‹ä¸‹è½½&lt;/h2&gt; &#xA;&lt;p&gt;æœ¬ç›®å½•ä¸­ä¸»è¦åŒ…å«baseæ¨¡å‹ï¼Œæ•…æˆ‘ä»¬ä¸åœ¨æ¨¡å‹ç®€ç§°ä¸­æ ‡æ³¨&lt;code&gt;base&lt;/code&gt;å­—æ ·ã€‚å¯¹äºå…¶ä»–å¤§å°çš„æ¨¡å‹ä¼šæ ‡æ³¨å¯¹åº”çš„æ ‡è®°ï¼ˆä¾‹å¦‚largeï¼‰ã€‚&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;BERT-largeæ¨¡å‹&lt;/code&gt;&lt;/strong&gt;ï¼š24-layer, 1024-hidden, 16-heads, 330M parameters&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;BERT-baseæ¨¡å‹&lt;/code&gt;&lt;/strong&gt;ï¼š12-layer, 768-hidden, 12-heads, 110M parameters&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;æ³¨æ„ï¼šå¼€æºç‰ˆæœ¬ä¸åŒ…å«MLMä»»åŠ¡çš„æƒé‡ï¼›å¦‚éœ€åšMLMä»»åŠ¡ï¼Œè¯·ä½¿ç”¨é¢å¤–æ•°æ®è¿›è¡ŒäºŒæ¬¡é¢„è®­ç»ƒï¼ˆå’Œå…¶ä»–ä¸‹æ¸¸ä»»åŠ¡ä¸€æ ·ï¼‰ã€‚&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;æ¨¡å‹ç®€ç§°&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;è¯­æ–™&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Googleä¸‹è½½&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;ç™¾åº¦ç½‘ç›˜ä¸‹è½½&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;&lt;code&gt;RBT6, Chinese&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;EXTæ•°æ®&lt;sup&gt;[1]&lt;/sup&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;&lt;a href=&#34;https://pan.baidu.com/s/1_MDAIYIGVgDovWkSs51NDA?pwd=hniy&#34;&gt;TensorFlowï¼ˆå¯†ç hniyï¼‰&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;&lt;code&gt;RBT4, Chinese&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;EXTæ•°æ®&lt;sup&gt;[1]&lt;/sup&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;&lt;a href=&#34;https://pan.baidu.com/s/1MUrmuTULnMn3L1aw_dXxSA?pwd=sjpt&#34;&gt;TensorFlowï¼ˆå¯†ç sjptï¼‰&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;&lt;code&gt;RBTL3, Chinese&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;EXTæ•°æ®&lt;sup&gt;[1]&lt;/sup&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;&lt;a href=&#34;https://drive.google.com/open?id=1Jzn1hYwmv0kXkfTeIvNT61Rn1IbRc-o8&#34;&gt;TensorFlow&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;strong&gt;&lt;a href=&#34;https://drive.google.com/open?id=1qs5OasLXXjOnR2XuGUh12NanUl0pkjEv&#34;&gt;PyTorch&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;&lt;a href=&#34;https://pan.baidu.com/s/1vV9ClBMbsSpt8wUpfQz62Q?pwd=s6cu&#34;&gt;TensorFlowï¼ˆå¯†ç s6cuï¼‰&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;&lt;code&gt;RBT3, Chinese&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;EXTæ•°æ®&lt;sup&gt;[1]&lt;/sup&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;&lt;a href=&#34;https://drive.google.com/open?id=1-rvV0nBDvRCASbRz8M9Decc3_8Aw-2yi&#34;&gt;TensorFlow&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;strong&gt;&lt;a href=&#34;https://drive.google.com/open?id=1_LqmIxm8Nz1Abvlqb8QFZaxYo-TInOed&#34;&gt;PyTorch&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;&lt;a href=&#34;https://pan.baidu.com/s/1AnapwWj1YBZ_4E6AAtj2lg?pwd=5a57&#34;&gt;TensorFlowï¼ˆå¯†ç 5a57ï¼‰&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;&lt;code&gt;RoBERTa-wwm-ext-large, Chinese&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;EXTæ•°æ®&lt;sup&gt;[1]&lt;/sup&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;&lt;a href=&#34;https://drive.google.com/open?id=1dtad0FFzG11CBsawu8hvwwzU2R0FDI94&#34;&gt;TensorFlow&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;strong&gt;&lt;a href=&#34;https://drive.google.com/open?id=1-2vEZfIFCdM1-vJ3GD6DlSyKT4eVXMKq&#34;&gt;PyTorch&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;&lt;a href=&#34;https://pan.baidu.com/s/1F68xzCLWEonTEVP7HQ0Ciw?pwd=dqqe&#34;&gt;TensorFlowï¼ˆå¯†ç dqqeï¼‰&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;&lt;code&gt;RoBERTa-wwm-ext, Chinese&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;EXTæ•°æ®&lt;sup&gt;[1]&lt;/sup&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;&lt;a href=&#34;https://drive.google.com/open?id=1jMAKIJmPn7kADgD3yQZhpsqM-IRM1qZt&#34;&gt;TensorFlow&lt;/a&gt;&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;&lt;a href=&#34;https://drive.google.com/open?id=1eHM3l4fMo6DsQYGmey7UZGiTmQquHw25&#34;&gt;PyTorch&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;&lt;a href=&#34;https://pan.baidu.com/s/1oR0cgSXE3Nz6dESxr98qVA?pwd=vybq&#34;&gt;TensorFlowï¼ˆå¯†ç vybqï¼‰&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;&lt;code&gt;BERT-wwm-ext, Chinese&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;EXTæ•°æ®&lt;sup&gt;[1]&lt;/sup&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;&lt;a href=&#34;https://drive.google.com/open?id=1buMLEjdtrXE2c4G1rpsNGWEx7lUQ0RHi&#34;&gt;TensorFlow&lt;/a&gt;&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;&lt;a href=&#34;https://drive.google.com/open?id=1iNeYFhCBJWeUsIlnW_2K6SMwXkM4gLb_&#34;&gt;PyTorch&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;&lt;a href=&#34;https://pan.baidu.com/s/1x-jIw1X2yNYHGak2yiq4RQ?pwd=wgnt&#34;&gt;TensorFlowï¼ˆå¯†ç wgntï¼‰&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;&lt;code&gt;BERT-wwm, Chinese&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;ä¸­æ–‡ç»´åŸº&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;&lt;a href=&#34;https://drive.google.com/open?id=1RoTQsXp2hkQ1gSRVylRIJfQxJUgkfJMW&#34;&gt;TensorFlow&lt;/a&gt;&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;&lt;a href=&#34;https://drive.google.com/open?id=1AQitrjbvCWc51SYiLN-cJq4e0WiNN4KY&#34;&gt;PyTorch&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;&lt;a href=&#34;https://pan.baidu.com/s/1HDdDXiYxGT5ub5OeO7qdWw?pwd=qfh8&#34;&gt;TensorFlowï¼ˆå¯†ç qfh8ï¼‰&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;BERT-base, Chinese&lt;/code&gt;&lt;sup&gt;Google&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ä¸­æ–‡ç»´åŸº&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip&#34;&gt;Google Cloud&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;BERT-base, Multilingual Cased&lt;/code&gt;&lt;sup&gt;Google&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;å¤šè¯­ç§ç»´åŸº&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip&#34;&gt;Google Cloud&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;BERT-base, Multilingual Uncased&lt;/code&gt;&lt;sup&gt;Google&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;å¤šè¯­ç§ç»´åŸº&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/bert_models/2018_11_03/multilingual_L-12_H-768_A-12.zip&#34;&gt;Google Cloud&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[1] EXTæ•°æ®åŒ…æ‹¬ï¼šä¸­æ–‡ç»´åŸºç™¾ç§‘ï¼Œå…¶ä»–ç™¾ç§‘ã€æ–°é—»ã€é—®ç­”ç­‰æ•°æ®ï¼Œæ€»è¯æ•°è¾¾5.4Bã€‚&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;æŸ¥çœ‹æ›´å¤šå“ˆå·¥å¤§è®¯é£è”åˆå®éªŒå®¤ï¼ˆHFLï¼‰å‘å¸ƒçš„èµ„æºï¼š&lt;a href=&#34;https://github.com/ymcui/HFL-Anthology&#34;&gt;https://github.com/ymcui/HFL-Anthology&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python run.py --model bert&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/EssayKillerBrain/EssayKiller_V2/raw/master/References/attachments/Clipboard_2020-09-29-16-40-19.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;æµ‹è¯•æ—¶ï¼Œéœ€è¦ç”¨æ­£åˆ™è¡¨è¾¾å¼è¿‡æ»¤è€ƒè¯•ä¸“ç”¨è¯ï¼ŒåŒ…æ‹¬â€œé˜…è¯»ä¸‹é¢çš„ææ–™ï¼Œæ ¹æ®è¦æ±‚å†™ä½œâ€ï¼Œâ€œè¦æ±‚ï¼šxxxâ€ï¼Œâ€œè¯·å®Œæˆ/è¯·ç»“åˆ/è¯·ç»¼åˆxxâ€ã€‚&lt;/p&gt; &#xA;&lt;p&gt;æ¯”å¦‚&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://github.com/EssayKillerBrain/EssayKiller_V2/raw/master/References/attachments/Clipboard_2020-09-29-17-17-30.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code&gt;äººä»¬ç”¨çœ¼ç›çœ‹ä»–äººã€çœ‹ä¸–ç•Œï¼Œå´æ— æ³•ç›´æ¥çœ‹åˆ°å®Œæ•´çš„è‡ªå·±ã€‚æ‰€ä»¥ï¼Œåœ¨äººç”Ÿçš„æ—…ç¨‹ä¸­ï¼Œæˆ‘ä»¬éœ€è¦å¯»æ‰¾å„ç§â€œé•œå­â€ã€ä¸æ–­ç»˜åˆ¶â€œè‡ªç”»åƒâ€æ¥å®¡è§†è‡ªæˆ‘ï¼Œå°è¯•å›ç­”â€œæˆ‘æ˜¯æ€æ ·çš„äººâ€â€œæˆ‘æƒ³è¿‡æ€æ ·çš„ç”Ÿæ´»â€â€œæˆ‘èƒ½åšäº›ä»€ä¹ˆâ€â€œå¦‚ä½•ç”Ÿæ´»å¾—æ›´æœ‰æ„ä¹‰â€ç­‰é‡è¦çš„é—®é¢˜ã€‚&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{EssayKillerBrain,&#xA;  author = {Turing&#39;s Cat},&#xA;  title = {Autowritting Ai Framework},&#xA;  year = {2020},&#xA;  publisher = {GitHub},&#xA;  journal = {GitHub repository},&#xA;  howpublished = {\url{https://github.com/AlanTur1ng/EssayTopicPredict}},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;å‚è€ƒèµ„æ–™&lt;/h2&gt; &#xA;&lt;p&gt;[1] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&lt;br&gt; [2] ERNIE: Enhanced Representation through Knowledge Integration&lt;br&gt; [3] Fine-tune BERT for Extractive Summarization&lt;br&gt; [4] EAST: An Efficient and Accurate Scene Text Detector&lt;br&gt; [5] An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition&lt;br&gt; [6] Language Models are Unsupervised Multitask Learners&lt;br&gt; [7] &lt;a href=&#34;https://github.com/Morizeyao/GPT2-Chinese&#34;&gt;https://github.com/Morizeyao/GPT2-Chinese&lt;/a&gt;&lt;br&gt; [8] &lt;a href=&#34;https://github.com/argman/EAST&#34;&gt;https://github.com/argman/EAST&lt;/a&gt;&lt;br&gt; [9] &lt;a href=&#34;https://github.com/bgshih/crnn&#34;&gt;https://github.com/bgshih/crnn&lt;/a&gt;&lt;br&gt; [10] &lt;a href=&#34;https://github.com/zhiyou720/chinese_summarizer&#34;&gt;https://github.com/zhiyou720/chinese_summarizer&lt;/a&gt;&lt;br&gt; [11] &lt;a href=&#34;https://zhuanlan.zhihu.com/p/64737915&#34;&gt;https://zhuanlan.zhihu.com/p/64737915&lt;/a&gt;&lt;br&gt; [12] &lt;a href=&#34;https://github.com/ouyanghuiyu/chineseocr_lite&#34;&gt;https://github.com/ouyanghuiyu/chineseocr_lite&lt;/a&gt;&lt;br&gt; [13] &lt;a href=&#34;https://github.com/google-research/bert&#34;&gt;https://github.com/google-research/bert&lt;/a&gt;&lt;br&gt; [14] &lt;a href=&#34;https://github.com/rowanz/grover&#34;&gt;https://github.com/rowanz/grover&lt;/a&gt;&lt;br&gt; [15] &lt;a href=&#34;https://github.com/wind91725/gpt2-ml-finetune-&#34;&gt;https://github.com/wind91725/gpt2-ml-finetune-&lt;/a&gt;&lt;br&gt; [16] &lt;a href=&#34;https://github.com/guodongxiaren/README&#34;&gt;https://github.com/guodongxiaren/README&lt;/a&gt;&lt;br&gt; [17] &lt;a href=&#34;https://www.jianshu.com/p/55560d3e0e8a&#34;&gt;https://www.jianshu.com/p/55560d3e0e8a&lt;/a&gt;&lt;br&gt; [18] &lt;a href=&#34;https://github.com/YCG09/chinese_ocr&#34;&gt;https://github.com/YCG09/chinese_ocr&lt;/a&gt;&lt;br&gt; [19] &lt;a href=&#34;https://github.com/xiaomaxiao/keras_ocr&#34;&gt;https://github.com/xiaomaxiao/keras_ocr&lt;/a&gt;&lt;br&gt; [20] &lt;a href=&#34;https://github.com/nghuyong/ERNIE-Pytorch&#34;&gt;https://github.com/nghuyong/ERNIE-Pytorch&lt;/a&gt;&lt;br&gt; [21] &lt;a href=&#34;https://zhuanlan.zhihu.com/p/43534801&#34;&gt;https://zhuanlan.zhihu.com/p/43534801&lt;/a&gt;&lt;br&gt; [22] &lt;a href=&#34;https://blog.csdn.net/xuxunjie147/article/details/87178774/&#34;&gt;https://blog.csdn.net/xuxunjie147/article/details/87178774/&lt;/a&gt;&lt;br&gt; [23] &lt;a href=&#34;https://github.com/JiangYanting/Pre-modern_Chinese_corpus_dataset&#34;&gt;https://github.com/JiangYanting/Pre-modern_Chinese_corpus_dataset&lt;/a&gt;&lt;br&gt; [24] &lt;a href=&#34;https://github.com/brightmart/nlp_chinese_corpus&#34;&gt;https://github.com/brightmart/nlp_chinese_corpus&lt;/a&gt;&lt;br&gt; [25] &lt;a href=&#34;https://github.com/SophonPlus/ChineseNlpCorpus&#34;&gt;https://github.com/SophonPlus/ChineseNlpCorpus&lt;/a&gt;&lt;br&gt; [26] &lt;a href=&#34;https://github.com/THUNLP-AIPoet/Resources&#34;&gt;https://github.com/THUNLP-AIPoet/Resources&lt;/a&gt;&lt;br&gt; [27] &lt;a href=&#34;https://github.com/OYE93/Chinese-NLP-Corpus&#34;&gt;https://github.com/OYE93/Chinese-NLP-Corpus&lt;/a&gt;&lt;br&gt; [28] &lt;a href=&#34;https://github.com/CLUEbenchmark/CLUECorpus2020&#34;&gt;https://github.com/CLUEbenchmark/CLUECorpus2020&lt;/a&gt;&lt;br&gt; [29] &lt;a href=&#34;https://github.com/zhiyou720/chinese_summarizer&#34;&gt;https://github.com/zhiyou720/chinese_summarizer&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;å…è´£å£°æ˜&lt;/h2&gt; &#xA;&lt;p&gt;è¯¥é¡¹ç›®ä¸­çš„å†…å®¹ä»…ä¾›æŠ€æœ¯ç ”ç©¶ä¸ç§‘æ™®ï¼Œä¸ä½œä¸ºä»»ä½•ç»“è®ºæ€§ä¾æ®ï¼Œä¸æä¾›ä»»ä½•å•†ä¸šåŒ–åº”ç”¨æˆæƒ&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>CompVis/latent-diffusion</title>
    <updated>2022-06-03T01:43:20Z</updated>
    <id>tag:github.com,2022-06-03:/CompVis/latent-diffusion</id>
    <link href="https://github.com/CompVis/latent-diffusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;High-Resolution Image Synthesis with Latent Diffusion Models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Latent Diffusion Models&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.10752&#34;&gt;arXiv&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/#bibtex&#34;&gt;BibTeX&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/assets/results.gif&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.10752&#34;&gt;&lt;strong&gt;High-Resolution Image Synthesis with Latent Diffusion Models&lt;/strong&gt;&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/rromb&#34;&gt;Robin Rombach&lt;/a&gt;*, &lt;a href=&#34;https://github.com/ablattmann&#34;&gt;Andreas Blattmann&lt;/a&gt;*, &lt;a href=&#34;https://github.com/qp-qp&#34;&gt;Dominik Lorenz&lt;/a&gt;, &lt;a href=&#34;https://github.com/pesser&#34;&gt;Patrick Esser&lt;/a&gt;, &lt;a href=&#34;https://hci.iwr.uni-heidelberg.de/Staff/bommer&#34;&gt;BjÃ¶rn Ommer&lt;/a&gt;&lt;br&gt; * equal contribution&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/assets/modelfigure.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;h3&gt;April 2022&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Thanks to &lt;a href=&#34;https://github.com/crowsonkb&#34;&gt;Katherine Crowson&lt;/a&gt;, classifier-free guidance received a ~2x speedup and the &lt;a href=&#34;https://arxiv.org/abs/2202.09778&#34;&gt;PLMS sampler&lt;/a&gt; is available. See also &lt;a href=&#34;https://github.com/CompVis/latent-diffusion/pull/51&#34;&gt;this PR&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Our 1.45B &lt;a href=&#34;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/#text-to-image&#34;&gt;latent diffusion LAION model&lt;/a&gt; was integrated into &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces ğŸ¤—&lt;/a&gt; using &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;. Try out the Web Demo: &lt;a href=&#34;https://huggingface.co/spaces/multimodalart/latentdiffusion&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;More pre-trained LDMs are available:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;A 1.45B &lt;a href=&#34;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/#text-to-image&#34;&gt;model&lt;/a&gt; trained on the &lt;a href=&#34;https://arxiv.org/abs/2111.02114&#34;&gt;LAION-400M&lt;/a&gt; database.&lt;/li&gt; &#xA;   &lt;li&gt;A class-conditional model on ImageNet, achieving a FID of 3.6 when using &lt;a href=&#34;https://openreview.net/pdf?id=qw8AKxfYbI&#34;&gt;classifier-free guidance&lt;/a&gt; Available via a &lt;a href=&#34;https://colab.research.google.com/github/CompVis/latent-diffusion/blob/main/scripts/latent_imagenet_diffusion.ipynb&#34;&gt;colab notebook&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/CompVis/latent-diffusion/blob/main/scripts/latent_imagenet_diffusion.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;A suitable &lt;a href=&#34;https://conda.io/&#34;&gt;conda&lt;/a&gt; environment named &lt;code&gt;ldm&lt;/code&gt; can be created and activated with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env create -f environment.yaml&#xA;conda activate ldm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Pretrained Models&lt;/h1&gt; &#xA;&lt;p&gt;A general list of all available checkpoints is available in via our &lt;a href=&#34;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/#model-zoo&#34;&gt;model zoo&lt;/a&gt;. If you use any of these models in your work, we are always happy to receive a &lt;a href=&#34;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/#bibtex&#34;&gt;citation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Text-to-Image&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/assets/txt2img-preview.png&#34; alt=&#34;text2img-figure&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Download the pre-trained weights (5.7GB)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mkdir -p models/ldm/text2img-large/&#xA;wget -O models/ldm/text2img-large/model.ckpt https://ommer-lab.com/files/latent-diffusion/nitro/txt2img-f8-large/model.ckpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and sample with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/txt2img.py --prompt &#34;a virus monster is playing guitar, oil on canvas&#34; --ddim_eta 0.0 --n_samples 4 --n_iter 4 --scale 5.0  --ddim_steps 50&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will save each sample individually as well as a grid of size &lt;code&gt;n_iter&lt;/code&gt; x &lt;code&gt;n_samples&lt;/code&gt; at the specified output location (default: &lt;code&gt;outputs/txt2img-samples&lt;/code&gt;). Quality, sampling speed and diversity are best controlled via the &lt;code&gt;scale&lt;/code&gt;, &lt;code&gt;ddim_steps&lt;/code&gt; and &lt;code&gt;ddim_eta&lt;/code&gt; arguments. As a rule of thumb, higher values of &lt;code&gt;scale&lt;/code&gt; produce better samples at the cost of a reduced output diversity.&lt;br&gt; Furthermore, increasing &lt;code&gt;ddim_steps&lt;/code&gt; generally also gives higher quality samples, but returns are diminishing for values &amp;gt; 250. Fast sampling (i.e. low values of &lt;code&gt;ddim_steps&lt;/code&gt;) while retaining good quality can be achieved by using &lt;code&gt;--ddim_eta 0.0&lt;/code&gt;.&lt;br&gt; Faster sampling (i.e. even lower values of &lt;code&gt;ddim_steps&lt;/code&gt;) while retaining good quality can be achieved by using &lt;code&gt;--ddim_eta 0.0&lt;/code&gt; and &lt;code&gt;--plms&lt;/code&gt; (see &lt;a href=&#34;https://arxiv.org/abs/2202.09778&#34;&gt;Pseudo Numerical Methods for Diffusion Models on Manifolds&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;h4&gt;Beyond 256Â²&lt;/h4&gt; &#xA;&lt;p&gt;For certain inputs, simply running the model in a convolutional fashion on larger features than it was trained on can sometimes result in interesting results. To try it out, tune the &lt;code&gt;H&lt;/code&gt; and &lt;code&gt;W&lt;/code&gt; arguments (which will be integer-divided by 8 in order to calculate the corresponding latent size), e.g. run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/txt2img.py --prompt &#34;a sunset behind a mountain range, vector image&#34; --ddim_eta 1.0 --n_samples 1 --n_iter 1 --H 384 --W 1024 --scale 5.0  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to create a sample of size 384x1024. Note, however, that controllability is reduced compared to the 256x256 setting.&lt;/p&gt; &#xA;&lt;p&gt;The example below was generated using the above command. &lt;img src=&#34;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/assets/txt2img-convsample.png&#34; alt=&#34;text2img-figure-conv&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Inpainting&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/assets/inpainting.png&#34; alt=&#34;inpainting&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Download the pre-trained weights&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;wget -O models/ldm/inpainting_big/last.ckpt https://heibox.uni-heidelberg.de/f/4d9ac7ea40c64582b7c9/?dl=1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and sample with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/inpaint.py --indir data/inpainting_examples/ --outdir outputs/inpainting_results&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;indir&lt;/code&gt; should contain images &lt;code&gt;*.png&lt;/code&gt; and masks &lt;code&gt;&amp;lt;image_fname&amp;gt;_mask.png&lt;/code&gt; like the examples provided in &lt;code&gt;data/inpainting_examples&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Class-Conditional ImageNet&lt;/h2&gt; &#xA;&lt;p&gt;Available via a &lt;a href=&#34;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/scripts/latent_imagenet_diffusion.ipynb&#34;&gt;notebook&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/CompVis/latent-diffusion/blob/main/scripts/latent_imagenet_diffusion.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;. &lt;img src=&#34;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/assets/birdhouse.png&#34; alt=&#34;class-conditional&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Unconditional Models&lt;/h2&gt; &#xA;&lt;p&gt;We also provide a script for sampling from unconditional LDMs (e.g. LSUN, FFHQ, ...). Start it via&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=&amp;lt;GPU_ID&amp;gt; python scripts/sample_diffusion.py -r models/ldm/&amp;lt;model_spec&amp;gt;/model.ckpt -l &amp;lt;logdir&amp;gt; -n &amp;lt;\#samples&amp;gt; --batch_size &amp;lt;batch_size&amp;gt; -c &amp;lt;\#ddim steps&amp;gt; -e &amp;lt;\#eta&amp;gt; &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Train your own LDMs&lt;/h1&gt; &#xA;&lt;h2&gt;Data preparation&lt;/h2&gt; &#xA;&lt;h3&gt;Faces&lt;/h3&gt; &#xA;&lt;p&gt;For downloading the CelebA-HQ and FFHQ datasets, proceed as described in the &lt;a href=&#34;https://github.com/CompVis/taming-transformers#celeba-hq&#34;&gt;taming-transformers&lt;/a&gt; repository.&lt;/p&gt; &#xA;&lt;h3&gt;LSUN&lt;/h3&gt; &#xA;&lt;p&gt;The LSUN datasets can be conveniently downloaded via the script available &lt;a href=&#34;https://github.com/fyu/lsun&#34;&gt;here&lt;/a&gt;. We performed a custom split into training and validation images, and provide the corresponding filenames at &lt;a href=&#34;https://ommer-lab.com/files/lsun.zip&#34;&gt;https://ommer-lab.com/files/lsun.zip&lt;/a&gt;. After downloading, extract them to &lt;code&gt;./data/lsun&lt;/code&gt;. The beds/cats/churches subsets should also be placed/symlinked at &lt;code&gt;./data/lsun/bedrooms&lt;/code&gt;/&lt;code&gt;./data/lsun/cats&lt;/code&gt;/&lt;code&gt;./data/lsun/churches&lt;/code&gt;, respectively.&lt;/p&gt; &#xA;&lt;h3&gt;ImageNet&lt;/h3&gt; &#xA;&lt;p&gt;The code will try to download (through &lt;a href=&#34;http://academictorrents.com/&#34;&gt;Academic Torrents&lt;/a&gt;) and prepare ImageNet the first time it is used. However, since ImageNet is quite large, this requires a lot of disk space and time. If you already have ImageNet on your disk, you can speed things up by putting the data into &lt;code&gt;${XDG_CACHE}/autoencoders/data/ILSVRC2012_{split}/data/&lt;/code&gt; (which defaults to &lt;code&gt;~/.cache/autoencoders/data/ILSVRC2012_{split}/data/&lt;/code&gt;), where &lt;code&gt;{split}&lt;/code&gt; is one of &lt;code&gt;train&lt;/code&gt;/&lt;code&gt;validation&lt;/code&gt;. It should have the following structure:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;${XDG_CACHE}/autoencoders/data/ILSVRC2012_{split}/data/&#xA;â”œâ”€â”€ n01440764&#xA;â”‚   â”œâ”€â”€ n01440764_10026.JPEG&#xA;â”‚   â”œâ”€â”€ n01440764_10027.JPEG&#xA;â”‚   â”œâ”€â”€ ...&#xA;â”œâ”€â”€ n01443537&#xA;â”‚   â”œâ”€â”€ n01443537_10007.JPEG&#xA;â”‚   â”œâ”€â”€ n01443537_10014.JPEG&#xA;â”‚   â”œâ”€â”€ ...&#xA;â”œâ”€â”€ ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you haven&#39;t extracted the data, you can also place &lt;code&gt;ILSVRC2012_img_train.tar&lt;/code&gt;/&lt;code&gt;ILSVRC2012_img_val.tar&lt;/code&gt; (or symlinks to them) into &lt;code&gt;${XDG_CACHE}/autoencoders/data/ILSVRC2012_train/&lt;/code&gt; / &lt;code&gt;${XDG_CACHE}/autoencoders/data/ILSVRC2012_validation/&lt;/code&gt;, which will then be extracted into above structure without downloading it again. Note that this will only happen if neither a folder &lt;code&gt;${XDG_CACHE}/autoencoders/data/ILSVRC2012_{split}/data/&lt;/code&gt; nor a file &lt;code&gt;${XDG_CACHE}/autoencoders/data/ILSVRC2012_{split}/.ready&lt;/code&gt; exist. Remove them if you want to force running the dataset preparation again.&lt;/p&gt; &#xA;&lt;h2&gt;Model Training&lt;/h2&gt; &#xA;&lt;p&gt;Logs and checkpoints for trained models are saved to &lt;code&gt;logs/&amp;lt;START_DATE_AND_TIME&amp;gt;_&amp;lt;config_spec&amp;gt;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Training autoencoder models&lt;/h3&gt; &#xA;&lt;p&gt;Configs for training a KL-regularized autoencoder on ImageNet are provided at &lt;code&gt;configs/autoencoder&lt;/code&gt;. Training can be started by running&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=&amp;lt;GPU_ID&amp;gt; python main.py --base configs/autoencoder/&amp;lt;config_spec&amp;gt;.yaml -t --gpus 0,    &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where &lt;code&gt;config_spec&lt;/code&gt; is one of {&lt;code&gt;autoencoder_kl_8x8x64&lt;/code&gt;(f=32, d=64), &lt;code&gt;autoencoder_kl_16x16x16&lt;/code&gt;(f=16, d=16), &lt;code&gt;autoencoder_kl_32x32x4&lt;/code&gt;(f=8, d=4), &lt;code&gt;autoencoder_kl_64x64x3&lt;/code&gt;(f=4, d=3)}.&lt;/p&gt; &#xA;&lt;p&gt;For training VQ-regularized models, see the &lt;a href=&#34;https://github.com/CompVis/taming-transformers&#34;&gt;taming-transformers&lt;/a&gt; repository.&lt;/p&gt; &#xA;&lt;h3&gt;Training LDMs&lt;/h3&gt; &#xA;&lt;p&gt;In &lt;code&gt;configs/latent-diffusion/&lt;/code&gt; we provide configs for training LDMs on the LSUN-, CelebA-HQ, FFHQ and ImageNet datasets. Training can be started by running&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=&amp;lt;GPU_ID&amp;gt; python main.py --base configs/latent-diffusion/&amp;lt;config_spec&amp;gt;.yaml -t --gpus 0,&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where &lt;code&gt;&amp;lt;config_spec&amp;gt;&lt;/code&gt; is one of {&lt;code&gt;celebahq-ldm-vq-4&lt;/code&gt;(f=4, VQ-reg. autoencoder, spatial size 64x64x3),&lt;code&gt;ffhq-ldm-vq-4&lt;/code&gt;(f=4, VQ-reg. autoencoder, spatial size 64x64x3), &lt;code&gt;lsun_bedrooms-ldm-vq-4&lt;/code&gt;(f=4, VQ-reg. autoencoder, spatial size 64x64x3), &lt;code&gt;lsun_churches-ldm-vq-4&lt;/code&gt;(f=8, KL-reg. autoencoder, spatial size 32x32x4),&lt;code&gt;cin-ldm-vq-8&lt;/code&gt;(f=8, VQ-reg. autoencoder, spatial size 32x32x4)}.&lt;/p&gt; &#xA;&lt;h1&gt;Model Zoo&lt;/h1&gt; &#xA;&lt;h2&gt;Pretrained Autoencoding Models&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/assets/reconstruction2.png&#34; alt=&#34;rec2&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;All models were trained until convergence (no further substantial improvement in rFID).&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;rFID vs val&lt;/th&gt; &#xA;   &lt;th&gt;train steps&lt;/th&gt; &#xA;   &lt;th&gt;PSNR&lt;/th&gt; &#xA;   &lt;th&gt;PSIM&lt;/th&gt; &#xA;   &lt;th&gt;Link&lt;/th&gt; &#xA;   &lt;th&gt;Comments&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;f=4, VQ (Z=8192, d=3)&lt;/td&gt; &#xA;   &lt;td&gt;0.58&lt;/td&gt; &#xA;   &lt;td&gt;533066&lt;/td&gt; &#xA;   &lt;td&gt;27.43 +/- 4.26&lt;/td&gt; &#xA;   &lt;td&gt;0.53 +/- 0.21&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/vq-f4.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/vq-f4.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;f=4, VQ (Z=8192, d=3)&lt;/td&gt; &#xA;   &lt;td&gt;1.06&lt;/td&gt; &#xA;   &lt;td&gt;658131&lt;/td&gt; &#xA;   &lt;td&gt;25.21 +/- 4.17&lt;/td&gt; &#xA;   &lt;td&gt;0.72 +/- 0.26&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://heibox.uni-heidelberg.de/f/9c6681f64bb94338a069/?dl=1&#34;&gt;https://heibox.uni-heidelberg.de/f/9c6681f64bb94338a069/?dl=1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;no attention&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;f=8, VQ (Z=16384, d=4)&lt;/td&gt; &#xA;   &lt;td&gt;1.14&lt;/td&gt; &#xA;   &lt;td&gt;971043&lt;/td&gt; &#xA;   &lt;td&gt;23.07 +/- 3.99&lt;/td&gt; &#xA;   &lt;td&gt;1.17 +/- 0.36&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/vq-f8.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/vq-f8.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;f=8, VQ (Z=256, d=4)&lt;/td&gt; &#xA;   &lt;td&gt;1.49&lt;/td&gt; &#xA;   &lt;td&gt;1608649&lt;/td&gt; &#xA;   &lt;td&gt;22.35 +/- 3.81&lt;/td&gt; &#xA;   &lt;td&gt;1.26 +/- 0.37&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/vq-f8-n256.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/vq-f8-n256.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;f=16, VQ (Z=16384, d=8)&lt;/td&gt; &#xA;   &lt;td&gt;5.15&lt;/td&gt; &#xA;   &lt;td&gt;1101166&lt;/td&gt; &#xA;   &lt;td&gt;20.83 +/- 3.61&lt;/td&gt; &#xA;   &lt;td&gt;1.73 +/- 0.43&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://heibox.uni-heidelberg.de/f/0e42b04e2e904890a9b6/?dl=1&#34;&gt;https://heibox.uni-heidelberg.de/f/0e42b04e2e904890a9b6/?dl=1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;f=4, KL&lt;/td&gt; &#xA;   &lt;td&gt;0.27&lt;/td&gt; &#xA;   &lt;td&gt;176991&lt;/td&gt; &#xA;   &lt;td&gt;27.53 +/- 4.54&lt;/td&gt; &#xA;   &lt;td&gt;0.55 +/- 0.24&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/kl-f4.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/kl-f4.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;f=8, KL&lt;/td&gt; &#xA;   &lt;td&gt;0.90&lt;/td&gt; &#xA;   &lt;td&gt;246803&lt;/td&gt; &#xA;   &lt;td&gt;24.19 +/- 4.19&lt;/td&gt; &#xA;   &lt;td&gt;1.02 +/- 0.35&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/kl-f8.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/kl-f8.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;f=16, KL (d=16)&lt;/td&gt; &#xA;   &lt;td&gt;0.87&lt;/td&gt; &#xA;   &lt;td&gt;442998&lt;/td&gt; &#xA;   &lt;td&gt;24.08 +/- 4.22&lt;/td&gt; &#xA;   &lt;td&gt;1.07 +/- 0.36&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/kl-f16.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/kl-f16.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;f=32, KL (d=64)&lt;/td&gt; &#xA;   &lt;td&gt;2.04&lt;/td&gt; &#xA;   &lt;td&gt;406763&lt;/td&gt; &#xA;   &lt;td&gt;22.27 +/- 3.93&lt;/td&gt; &#xA;   &lt;td&gt;1.41 +/- 0.40&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/kl-f32.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/kl-f32.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Get the models&lt;/h3&gt; &#xA;&lt;p&gt;Running the following script downloads und extracts all available pretrained autoencoding models.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;bash scripts/download_first_stages.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The first stage models can then be found in &lt;code&gt;models/first_stage_models/&amp;lt;model_spec&amp;gt;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Pretrained LDMs&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Datset&lt;/th&gt; &#xA;   &lt;th&gt;Task&lt;/th&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;FID&lt;/th&gt; &#xA;   &lt;th&gt;IS&lt;/th&gt; &#xA;   &lt;th&gt;Prec&lt;/th&gt; &#xA;   &lt;th&gt;Recall&lt;/th&gt; &#xA;   &lt;th&gt;Link&lt;/th&gt; &#xA;   &lt;th&gt;Comments&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CelebA-HQ&lt;/td&gt; &#xA;   &lt;td&gt;Unconditional Image Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;LDM-VQ-4 (200 DDIM steps, eta=0)&lt;/td&gt; &#xA;   &lt;td&gt;5.11 (5.11)&lt;/td&gt; &#xA;   &lt;td&gt;3.29&lt;/td&gt; &#xA;   &lt;td&gt;0.72&lt;/td&gt; &#xA;   &lt;td&gt;0.49&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/celeba.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/celeba.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FFHQ&lt;/td&gt; &#xA;   &lt;td&gt;Unconditional Image Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;LDM-VQ-4 (200 DDIM steps, eta=1)&lt;/td&gt; &#xA;   &lt;td&gt;4.98 (4.98)&lt;/td&gt; &#xA;   &lt;td&gt;4.50 (4.50)&lt;/td&gt; &#xA;   &lt;td&gt;0.73&lt;/td&gt; &#xA;   &lt;td&gt;0.50&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/ffhq.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/ffhq.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LSUN-Churches&lt;/td&gt; &#xA;   &lt;td&gt;Unconditional Image Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;LDM-KL-8 (400 DDIM steps, eta=0)&lt;/td&gt; &#xA;   &lt;td&gt;4.02 (4.02)&lt;/td&gt; &#xA;   &lt;td&gt;2.72&lt;/td&gt; &#xA;   &lt;td&gt;0.64&lt;/td&gt; &#xA;   &lt;td&gt;0.52&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/lsun_churches.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/lsun_churches.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LSUN-Bedrooms&lt;/td&gt; &#xA;   &lt;td&gt;Unconditional Image Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;LDM-VQ-4 (200 DDIM steps, eta=1)&lt;/td&gt; &#xA;   &lt;td&gt;2.95 (3.0)&lt;/td&gt; &#xA;   &lt;td&gt;2.22 (2.23)&lt;/td&gt; &#xA;   &lt;td&gt;0.66&lt;/td&gt; &#xA;   &lt;td&gt;0.48&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/lsun_bedrooms.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/lsun_bedrooms.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ImageNet&lt;/td&gt; &#xA;   &lt;td&gt;Class-conditional Image Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;LDM-VQ-8 (200 DDIM steps, eta=1)&lt;/td&gt; &#xA;   &lt;td&gt;7.77(7.76)* /15.82**&lt;/td&gt; &#xA;   &lt;td&gt;201.56(209.52)* /78.82**&lt;/td&gt; &#xA;   &lt;td&gt;0.84* / 0.65**&lt;/td&gt; &#xA;   &lt;td&gt;0.35* / 0.63**&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/cin.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/cin.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;*: w/ guiding, classifier_scale 10 **: w/o guiding, scores in bracket calculated with script provided by &lt;a href=&#34;https://github.com/openai/guided-diffusion&#34;&gt;ADM&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Conceptual Captions&lt;/td&gt; &#xA;   &lt;td&gt;Text-conditional Image Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;LDM-VQ-f4 (100 DDIM steps, eta=0)&lt;/td&gt; &#xA;   &lt;td&gt;16.79&lt;/td&gt; &#xA;   &lt;td&gt;13.89&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/text2img.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/text2img.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;finetuned from LAION&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenImages&lt;/td&gt; &#xA;   &lt;td&gt;Super-resolution&lt;/td&gt; &#xA;   &lt;td&gt;LDM-VQ-4&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/sr_bsr.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/sr_bsr.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;BSR image degradation&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenImages&lt;/td&gt; &#xA;   &lt;td&gt;Layout-to-Image Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;LDM-VQ-4 (200 DDIM steps, eta=0)&lt;/td&gt; &#xA;   &lt;td&gt;32.02&lt;/td&gt; &#xA;   &lt;td&gt;15.92&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/layout2img_model.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/layout2img_model.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Landscapes&lt;/td&gt; &#xA;   &lt;td&gt;Semantic Image Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;LDM-VQ-4&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/semantic_synthesis256.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/semantic_synthesis256.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Landscapes&lt;/td&gt; &#xA;   &lt;td&gt;Semantic Image Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;LDM-VQ-4&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/semantic_synthesis.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/semantic_synthesis.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;finetuned on resolution 512x512&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Get the models&lt;/h3&gt; &#xA;&lt;p&gt;The LDMs listed above can jointly be downloaded and extracted via&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;bash scripts/download_models.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The models can then be found in &lt;code&gt;models/ldm/&amp;lt;model_spec&amp;gt;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Coming Soon...&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;More inference scripts for conditional LDMs.&lt;/li&gt; &#xA; &lt;li&gt;In the meantime, you can play with our colab notebook &lt;a href=&#34;https://colab.research.google.com/drive/1xqzUi2iXQXDqXBHQGP9Mqt2YrYW6cx-J?usp=sharing&#34;&gt;https://colab.research.google.com/drive/1xqzUi2iXQXDqXBHQGP9Mqt2YrYW6cx-J?usp=sharing&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Comments&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Our codebase for the diffusion models builds heavily on &lt;a href=&#34;https://github.com/openai/guided-diffusion&#34;&gt;OpenAI&#39;s ADM codebase&lt;/a&gt; and &lt;a href=&#34;https://github.com/lucidrains/denoising-diffusion-pytorch&#34;&gt;https://github.com/lucidrains/denoising-diffusion-pytorch&lt;/a&gt;. Thanks for open-sourcing!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The implementation of the transformer encoder is from &lt;a href=&#34;https://github.com/lucidrains/x-transformers&#34;&gt;x-transformers&lt;/a&gt; by &lt;a href=&#34;https://github.com/lucidrains?tab=repositories&#34;&gt;lucidrains&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;BibTeX&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{rombach2021highresolution,&#xA;      title={High-Resolution Image Synthesis with Latent Diffusion Models}, &#xA;      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and BjÃ¶rn Ommer},&#xA;      year={2021},&#xA;      eprint={2112.10752},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>amueller/introduction_to_ml_with_python</title>
    <updated>2022-06-03T01:43:20Z</updated>
    <id>tag:github.com,2022-06-03:/amueller/introduction_to_ml_with_python</id>
    <link href="https://github.com/amueller/introduction_to_ml_with_python" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Notebooks and code for the book &#34;Introduction to Machine Learning with Python&#34;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://mybinder.org/v2/gh/amueller/introduction_to_ml_with_python/master&#34;&gt;&lt;img src=&#34;https://mybinder.org/badge.svg?sanitize=true&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Introduction to Machine Learning with Python&lt;/h1&gt; &#xA;&lt;p&gt;This repository holds the code for the forthcoming book &#34;Introduction to Machine Learning with Python&#34; by &lt;a href=&#34;http://amueller.io&#34;&gt;Andreas Mueller&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/sarah_guido&#34;&gt;Sarah Guido&lt;/a&gt;. You can find details about the book on the &lt;a href=&#34;http://shop.oreilly.com/product/0636920030515.do&#34;&gt;O&#39;Reilly website&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The books requires the current stable version of scikit-learn, that is 0.20.0. Most of the book can also be used with previous versions of scikit-learn, though you need to adjust the import for everything from the &lt;code&gt;model_selection&lt;/code&gt; module, mostly &lt;code&gt;cross_val_score&lt;/code&gt;, &lt;code&gt;train_test_split&lt;/code&gt; and &lt;code&gt;GridSearchCV&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This repository provides the notebooks from which the book is created, together with the &lt;code&gt;mglearn&lt;/code&gt; library of helper functions to create figures and datasets.&lt;/p&gt; &#xA;&lt;p&gt;For the curious ones, the cover depicts a &lt;a href=&#34;https://en.wikipedia.org/wiki/Hellbender&#34;&gt;hellbender&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;All datasets are included in the repository, with the exception of the aclImdb dataset, which you can download from the page of &lt;a href=&#34;http://ai.stanford.edu/~amaas/data/sentiment/&#34;&gt;Andrew Maas&lt;/a&gt;. See the book for details.&lt;/p&gt; &#xA;&lt;p&gt;If you get &lt;code&gt;ImportError: No module named mglearn&lt;/code&gt; you can try to install mglearn into your python environment using the command &lt;code&gt;pip install mglearn&lt;/code&gt; in your terminal or &lt;code&gt;!pip install mglearn&lt;/code&gt; in Jupyter Notebook.&lt;/p&gt; &#xA;&lt;h2&gt;Errata&lt;/h2&gt; &#xA;&lt;p&gt;Please note that the first print of the book is missing the following line when listing the assumed imports:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from IPython.display import display&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please add this line if you see an error involving &lt;code&gt;display&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The first print of the book used a function called &lt;code&gt;plot_group_kfold&lt;/code&gt;. This has been renamed to &lt;code&gt;plot_label_kfold&lt;/code&gt; because of a rename in scikit-learn.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;To run the code, you need the packages &lt;code&gt;numpy&lt;/code&gt;, &lt;code&gt;scipy&lt;/code&gt;, &lt;code&gt;scikit-learn&lt;/code&gt;, &lt;code&gt;matplotlib&lt;/code&gt;, &lt;code&gt;pandas&lt;/code&gt; and &lt;code&gt;pillow&lt;/code&gt;. Some of the visualizations of decision trees and neural networks structures also require &lt;code&gt;graphviz&lt;/code&gt;. The chapter on text processing also requirs &lt;code&gt;nltk&lt;/code&gt; and &lt;code&gt;spacy&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The easiest way to set up an environment is by installing &lt;a href=&#34;https://www.continuum.io/downloads&#34;&gt;Anaconda&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Installing packages with conda:&lt;/h3&gt; &#xA;&lt;p&gt;If you already have a Python environment set up, and you are using the &lt;code&gt;conda&lt;/code&gt; package manager, you can get all packages by running&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda install numpy scipy scikit-learn matplotlib pandas pillow graphviz python-graphviz&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For the chapter on text processing you also need to install &lt;code&gt;nltk&lt;/code&gt; and &lt;code&gt;spacy&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda install nltk spacy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Installing packages with pip&lt;/h3&gt; &#xA;&lt;p&gt;If you already have a Python environment and are using pip to install packages, you need to run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install numpy scipy scikit-learn matplotlib pandas pillow graphviz&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You also need to install the graphiz C-library, which is easiest using a package manager. If you are using OS X and homebrew, you can &lt;code&gt;brew install graphviz&lt;/code&gt;. If you are on Ubuntu or debian, you can &lt;code&gt;apt-get install graphviz&lt;/code&gt;. Installing graphviz on Windows can be tricky and using conda / anaconda is recommended. For the chapter on text processing you also need to install &lt;code&gt;nltk&lt;/code&gt; and &lt;code&gt;spacy&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install nltk spacy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Downloading English language model&lt;/h3&gt; &#xA;&lt;p&gt;For the text processing chapter, you need to download the English language model for spacy using&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m spacy download en&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Submitting Errata&lt;/h2&gt; &#xA;&lt;p&gt;If you have errata for the (e-)book, please submit them via the &lt;a href=&#34;http://www.oreilly.com/catalog/errata.csp?isbn=0636920030515&#34;&gt;O&#39;Reilly Website&lt;/a&gt;. You can submit fixes to the code as pull-requests here, but I&#39;d appreciate it if you would also submit them there, as this repository doesn&#39;t hold the &#34;master notebooks&#34;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/amueller/introduction_to_ml_with_python/master/cover.jpg&#34; alt=&#34;cover&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>darkprinx/break-the-ice-with-python</title>
    <updated>2022-06-03T01:43:20Z</updated>
    <id>tag:github.com,2022-06-03:/darkprinx/break-the-ice-with-python</id>
    <link href="https://github.com/darkprinx/break-the-ice-with-python" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The repository is about 100+ python programming exercise problem discussed, explained, and solved in different ways&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Break The Ice With Python&lt;/h1&gt; &#xA;&lt;h3&gt;A journey of 100+ simple yet interesting problems which are explained, solved, discussed in different pythonic ways&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://mybinder.org/v2/gh/darkprinx/100-plus-Python-programming-exercises-extended/master?filepath=notebooks%2F&#34;&gt;&lt;img src=&#34;https://mybinder.org/badge_logo.svg?sanitize=true&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://deepnote.com/launch?url=https%3A%2F%2Fgithub.com%2Fdarkprinx%2F100-plus-Python-programming-exercises-extended%2Fblob%2Fmaster%2Fnotebooks%2FDay_01.ipynb&#34;&gt;&lt;img src=&#34;https://deepnote.com/buttons/try-in-a-jupyter-notebook.svg?sanitize=true&#34; alt=&#34;Deepnote&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;The exercise text contents of this repository was collected from GitHub account of &lt;a href=&#34;https://github.com/zhiwehu/Python-programming-exercises&#34;&gt;zhiwehu&lt;/a&gt;. I collected it to practice and solve all the listed problems with python. Even after these collected problems are all set up, I will try to add more problems in near future. If you are a very beginner with python then I hope this 100+ exercise will help you a lot to get your hands free with python.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;One will find the given problems very simple and easy to understand. A beginner can try 3-5 problems a day which will take a little time to solve but definitely will learn a couple of new stuff (no matter how lazy you are :P ). And after regular practice of only a month, one can find himself solved more than 100++ problems which are obviously not a deniable achievement.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;In this repository, I will be gradually updating the codebase of the given problems with my own solutions. Also, I may try to explain the code and tell my opinion about the problem if needed. Main Authors solutions are in python 2 &amp;amp; my solutions will be in python 3. Every problem is divided into a template format which is discussed below. There is a &lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/issues/3&#34;&gt;discussion&lt;/a&gt; section so don&#39;t forget to share your opinion, ideas and feel free to discuss anything wrong or mistake&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;A Big Thanks to &lt;a href=&#34;https://github.com/apurvmishra99&#34;&gt;apurvmishra99&lt;/a&gt; for contributing the repository by cleaning up the formatting of all Days_.md files. fixing some random errors, fixing some variable naming with PEP8 conventions, and adding a whole new folder of &lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/tree/master/notebooks&#34;&gt;jupyter&lt;/a&gt; notebook of all 24 days.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;100+ Python challenging programming exercises&lt;/h1&gt; &#xA;&lt;h2&gt;1. Problem Template&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;strong&gt;Question&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;strong&gt;Hints&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;strong&gt;Solution&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;2. Practice Status&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day%201.md&#34; title=&#34;Day 1 Status&#34;&gt;Day 1&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 1-3&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day%202.md&#34; title=&#34;Day 2 Status&#34;&gt;Day 2&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 4-9&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day%203.md&#34; title=&#34;Day 3 Status&#34;&gt;Day 3&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 10-13&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day%204.md&#34; title=&#34;Day 4 Status&#34;&gt;Day 4&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 14-15&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day%205.md&#34; title=&#34;Day 5 Status&#34;&gt;Day 5&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 16-17&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day%206.md&#34; title=&#34;Day 6 Status&#34;&gt;Day 6&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 18-19&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day%207.md&#34; title=&#34;Day 7 Status&#34;&gt;Day 7&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 20-21&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day%208.md&#34; title=&#34;Day 8 Status&#34;&gt;Day 8&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 22-25&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day%209.md&#34; title=&#34;Day 9 Status&#34;&gt;Day 9&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 26-30&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day_10.md&#34; title=&#34;Day 10 Status&#34;&gt;Day 10&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 31-37&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day_11.md&#34; title=&#34;Day 11 Status&#34;&gt;Day 11&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 38-43&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day_12.md&#34; title=&#34;Day 12 Status&#34;&gt;Day 12&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 44-46&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day_13.md&#34; title=&#34;Day 13 Status&#34;&gt;Day 13&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 47-50&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day_14.md&#34; title=&#34;Day 14 Status&#34;&gt;Day 14&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 51-53&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day_15.md&#34; title=&#34;Day 15 Status&#34;&gt;Day 15&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 54-59&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day_16.md&#34; title=&#34;Day 16 Status&#34;&gt;Day 16&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 60-64&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day_17.md&#34; title=&#34;Day 17 Status&#34;&gt;Day 17&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 65-69&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day_18.md&#34; title=&#34;Day 18 Status&#34;&gt;Day 18&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 70-74&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day_19.md&#34; title=&#34;Day 19 Status&#34;&gt;Day 19&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 75-79&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day_20.md&#34; title=&#34;Day 20 Status&#34;&gt;Day 20&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 80-84&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day_21.md&#34; title=&#34;Day 21 Status&#34;&gt;Day 21&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 85-89&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day_22.md&#34; title=&#34;Day 22 Status&#34;&gt;Day 22&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 90-94&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day_23.md&#34; title=&#34;Day 23 Status&#34;&gt;Day 23&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 95-99&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day_24.md&#34; title=&#34;Day 24 Status&#34;&gt;Day 24&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 100-103&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>yidao620c/python3-cookbook</title>
    <updated>2022-06-03T01:43:20Z</updated>
    <id>tag:github.com,2022-06-03:/yidao620c/python3-cookbook</id>
    <link href="https://github.com/yidao620c/python3-cookbook" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ã€ŠPython Cookbookã€‹ 3rd Edition Translation&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/yidao620c/python3-cookbook/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/yidao620c/python3-cookbook.svg?sanitize=true&#34; alt=&#34;GitHub issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://www.apache.org/licenses/LICENSE-2.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/hexpm/l/plug.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/yidao620c/python3-cookbook/releases/latest&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/downloads/yidao620c/python3-cookbook/total.svg?sanitize=true&#34; alt=&#34;Github downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/yidao620c/python3-cookbook/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/release/yidao620c/python3-cookbook.svg?sanitize=true&#34; alt=&#34;GitHub release&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;ã€ŠPython Cookbook in Chineseã€‹ 3rd Edition ç¿»è¯‘&lt;/h1&gt; &#xA;&lt;p&gt;ã€ŠPython Cookbookã€‹3rd ä¸­æ–‡ç‰ˆ3.0.0æ­£å¼å‘å¸ƒå•¦ ^_^ï¼ â€”â€”2017/12/07&lt;/p&gt; &#xA;&lt;p&gt;åœ¨çº¿é˜…è¯»åœ°å€ï¼š&lt;a href=&#34;http://python3-cookbook.readthedocs.org/zh_CN/latest/&#34;&gt;http://python3-cookbook.readthedocs.org/zh_CN/latest/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;æœ€æ–°ç‰ˆ(3.0.0)ä¸‹è½½&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ä¸­æ–‡ç®€ä½“ç‰ˆPDFä¸‹è½½åœ°å€ï¼š&lt;a href=&#34;https://pan.baidu.com/s/1pL1cI9d&#34;&gt;https://pan.baidu.com/s/1pL1cI9d&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ä¸­æ–‡ç¹ä½“ç‰ˆPDFä¸‹è½½åœ°å€ï¼š&lt;a href=&#34;https://pan.baidu.com/s/1qX97VJI&#34;&gt;https://pan.baidu.com/s/1qX97VJI&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;å…³äºä½œè€…David Beazley&lt;/h2&gt; &#xA;&lt;p&gt;æœ¬ä¹¦ä½œè€…æ˜¯David Beazleyå¤§ç¥ï¼Œä¸€ä½ç‹¬ç«‹çš„è®¡ç®—æœºç§‘å­¦å®¶ã€æ•™è‚²å®¶ï¼Œä»¥åŠæœ‰ç€35å¹´å¼€å‘ç»éªŒçš„è½¯ä»¶å¼€å‘è€…ã€‚ ä»–åœ¨Pythonç¤¾åŒºä¸€ç›´éƒ½å¾ˆæ´»è·ƒï¼Œç¼–å†™äº†å¾ˆå¤šçš„&lt;a href=&#34;http://www.dabeaz.com/software.html&#34;&gt;pythonåŒ…&lt;/a&gt;ï¼Œ å‘è¡¨äº†å¾ˆå¤šçš„å…¬å¼€&lt;a href=&#34;http://www.dabeaz.com/talks.html&#34;&gt;æ¼”è®²è§†é¢‘&lt;/a&gt; ä»¥åŠ &lt;a href=&#34;http://www.dabeaz.com/tutorials.html&#34;&gt;ç¼–ç¨‹æ•™ç¨‹&lt;/a&gt;ã€‚ åŒæ—¶è¿˜æ˜¯&lt;a href=&#34;http://www.dabeaz.com/per.html&#34;&gt;Python Essential Reference&lt;/a&gt; ä»¥åŠ &lt;a href=&#34;http://www.dabeaz.com/cookbook.html&#34;&gt; Python Cookbook (O&#39;Reilly Media)&lt;/a&gt;çš„ä½œè€…ã€‚&lt;/p&gt; &#xA;&lt;p&gt;David Beazleyå¤§ç¥çš„åšå®¢åœ°å€ï¼š&lt;a href=&#34;http://www.dabeaz.com/&#34;&gt;http://www.dabeaz.com/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;è¯‘è€…çš„è¯&lt;/h2&gt; &#xA;&lt;p&gt;äººç”Ÿè‹¦çŸ­ï¼Œæˆ‘ç”¨Pythonï¼&lt;/p&gt; &#xA;&lt;p&gt;è¯‘è€…ä¸€ç›´åšæŒä½¿ç”¨Python3ï¼Œå› ä¸ºå®ƒä»£è¡¨äº†Pythonçš„æœªæ¥ã€‚è™½ç„¶å‘åå…¼å®¹æ˜¯å®ƒçš„ç¡¬ä¼¤ï¼Œä½†æ˜¯è¿™ä¸ªå±€é¢è¿Ÿæ—©ä¼šæ”¹å˜çš„ï¼Œ è€Œä¸”Python3çš„æœªæ¥éœ€è¦æ¯ä¸ªäººçš„å¸®åŠ©å’Œæ”¯æŒã€‚ ç›®å‰å¸‚é¢ä¸Šçš„æ•™ç¨‹ä¹¦ç±ï¼Œç½‘ä¸Šçš„æ‰‹å†Œå¤§éƒ¨åˆ†åŸºæœ¬éƒ½æ˜¯2.xç³»åˆ—çš„ï¼Œä¸“é—¨åŸºäº3.xç³»åˆ—çš„ä¹¦ç±å°‘çš„å¯æ€œã€‚&lt;/p&gt; &#xA;&lt;p&gt;æœ€è¿‘çœ‹åˆ°ä¸€æœ¬ã€ŠPython Cookbookã€‹3rd Editionï¼Œå®Œå…¨åŸºäºPython3ï¼Œå†™çš„ä¹Ÿå¾ˆä¸é”™ã€‚ ä¸ºäº†Python3çš„æ™®åŠï¼Œæˆ‘ä¹Ÿä¸è‡ªé‡åŠ›ï¼Œæƒ³åšç‚¹ä»€ä¹ˆäº‹æƒ…ã€‚äºæ˜¯ä¹ï¼Œå°±æœ‰äº†ç¿»è¯‘è¿™æœ¬ä¹¦çš„å†²åŠ¨äº†ï¼ è¿™ä¸æ˜¯ä¸€é¡¹è½»æ¾çš„å·¥ä½œï¼Œå´æ˜¯ä¸€ä»¶å€¼å¾—åšçš„å·¥ä½œï¼šä¸ä»…æ–¹ä¾¿äº†åˆ«äººï¼Œè€Œä¸”å¯¹è‡ªå·±ç¿»è¯‘èƒ½åŠ›ä¹Ÿæ˜¯ä¸€ç§é”»ç‚¼å’Œæå‡ã€‚&lt;/p&gt; &#xA;&lt;p&gt;è¯‘è€…ä¼šåšæŒå¯¹è‡ªå·±æ¯ä¸€å¥çš„ç¿»è¯‘è´Ÿè´£ï¼ŒåŠ›æ±‚é«˜è´¨é‡ã€‚ä½†å—èƒ½åŠ›é™åˆ¶ï¼Œä¹Ÿéš¾å…æœ‰ç–æ¼æˆ–è€…è¡¨æ„ä¸å½“çš„åœ°æ–¹ã€‚ å¦‚æœè¯‘æ–‡ä¸­æœ‰ä»€ä¹ˆé”™æ¼çš„åœ°æ–¹è¯·å¤§å®¶è§è°…ï¼Œä¹Ÿæ¬¢è¿å¤§å®¶éšæ—¶æŒ‡æ­£ã€‚&lt;/p&gt; &#xA;&lt;p&gt;ç›®å‰å·²ç»æ­£å¼å®Œæˆäº†æ•´æœ¬ä¹¦çš„ç¿»è¯‘å·¥ä½œï¼Œå†æ—¶2å¹´ï¼Œä¸ç®¡æ€æ ·è¿˜æ˜¯åšæŒä¸‹æ¥äº†ã€‚ç°åœ¨å…±äº«ç»™pythonç¤¾åŒºã€‚&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;æ¬¢è¿å…³æ³¨æˆ‘çš„ä¸ªäººå…¬ä¼—å·â€œé£æ±¡ç†Šâ€ï¼Œæˆ‘ä¼šå®šæœŸåˆ†äº«ä¸€äº›è‡ªå·±çš„Pythonå­¦ä¹ ç¬”è®°å’Œå¿ƒå¾—ã€‚&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/yidao620c/python3-cookbook/raw/master/exts/wuxiong.jpg&#34; alt=&#34;å…¬ä¼—å·&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;é¡¹ç›®è¯´æ˜&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;æ‰€æœ‰æ–‡æ¡£å‡ä½¿ç”¨reStructuredTextç¼–è¾‘ï¼Œå‚è€ƒ &lt;a href=&#34;http://docutils.sourceforge.net/docs/user/rst/quickref.html&#34;&gt;reStructuredText&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;å½“å‰æ–‡æ¡£ç”Ÿæˆæ‰˜ç®¡åœ¨ &lt;a href=&#34;https://readthedocs.org/&#34;&gt;readthedocs&lt;/a&gt; ä¸Š&lt;/li&gt; &#xA; &lt;li&gt;ç”Ÿæˆçš„æ–‡æ¡£é¢„è§ˆåœ°å€ï¼š &lt;a href=&#34;http://python3-cookbook.readthedocs.org/zh_CN/latest/&#34;&gt;python3-cookbook&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ä½¿ç”¨äº†pythonå®˜æ–¹æ–‡æ¡£ä¸»é¢˜ &lt;a href=&#34;https://github.com/snide/sphinx_rtd_theme&#34;&gt;sphinx-rtd-theme&lt;/a&gt; ï¼Œä¹Ÿæ˜¯é»˜è®¤çš„ä¸»é¢˜default.&lt;/li&gt; &#xA; &lt;li&gt;ä¹¦ä¸­æ‰€æœ‰ä»£ç å‡åœ¨python 3.6ç‰ˆæœ¬ä¸‹é¢è¿è¡Œé€šè¿‡ï¼Œæ‰€æœ‰æºç æ”¾åœ¨cookbookåŒ…ä¸‹é¢&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;# on_rtd is whether we are on readthedocs.org, this line of code grabbed from docs.readthedocs.org&#xA;on_rtd = os.environ.get(&#39;READTHEDOCS&#39;, None) == &#39;True&#39;&#xA;&#xA;if not on_rtd:  # only import and set the theme if we&#39;re building docs locally&#xA;    import sphinx_rtd_theme&#xA;    html_theme = &#39;sphinx_rtd_theme&#39;&#xA;    html_theme_path = [sphinx_rtd_theme.get_html_theme_path()]&#xA;&#xA;# otherwise, readthedocs.org uses their theme by default, so no need to specify it&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;å…¶ä»–è´¡çŒ®è€…&lt;/h2&gt; &#xA;&lt;p&gt;æ’åä¸åˆ†å…ˆåï¼š&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Yu Longjun (&lt;a href=&#34;https://github.com/yulongjun&#34;&gt;https://github.com/yulongjun&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;tylinux (&lt;a href=&#34;https://github.com/tylinux&#34;&gt;https://github.com/tylinux&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Kevin Guan (&lt;a href=&#34;https://github.com/K-Guan&#34;&gt;https://github.com/K-Guan&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;littlezz (&lt;a href=&#34;https://github.com/littlezz&#34;&gt;https://github.com/littlezz&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;cclauss (&lt;a href=&#34;https://github.com/cclauss&#34;&gt;https://github.com/cclauss&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Yan Zhang (&lt;a href=&#34;https://github.com/Eskibear&#34;&gt;https://github.com/Eskibear&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;xiuyanduan (&lt;a href=&#34;https://github.com/xiuyanduan&#34;&gt;https://github.com/xiuyanduan&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;FPlust (&lt;a href=&#34;https://github.com/fplust&#34;&gt;https://github.com/fplust&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;lambdaplus (&lt;a href=&#34;https://github.com/lambdaplus&#34;&gt;https://github.com/lambdaplus&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Tony Yang (&lt;a href=&#34;mailto:liuliu036@gmail.com&#34;&gt;liuliu036@gmail.com&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/yidao620c/python3-cookbook/graphs/contributors&#34;&gt;æ›´å¤šè´¡çŒ®è€…&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;å…³äºæºç ç”ŸæˆPDFæ–‡ä»¶&lt;/h2&gt; &#xA;&lt;p&gt;æœ‰ç½‘å‹æé—®æ€æ ·é€šè¿‡æºç ç”ŸæˆPDFæ–‡ä»¶ï¼Œç”±äºè¿™ä¸ªæ­¥éª¤ä»‹ç»æœ‰ç‚¹é•¿ï¼Œä¸é€‚åˆæ”¾åœ¨READMEé‡Œé¢ï¼Œ æˆ‘ä¸“é—¨å†™äº†ç¯‡åšå®¢ä¸“é—¨ä»‹ç»æ€æ ·é€šè¿‡ReadtheDocsæ‰˜ç®¡æ–‡æ¡£ï¼Œæ€æ ·è‡ªå·±ç”ŸæˆPDFæ–‡ä»¶ï¼Œå¤§å®¶å¯ä»¥å‚è€ƒä¸€ä¸‹ã€‚&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.xncoding.com/2017/01/22/fullstack/readthedoc.html&#34;&gt;https://www.xncoding.com/2017/01/22/fullstack/readthedoc.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;å¦å¤–å…³äºç”Ÿæˆçš„PDFæ–‡ä»¶ä¸­ä¼šè‡ªåŠ¨ç”Ÿæˆæ ‡é¢˜ç¼–å·çš„é—®é¢˜ï¼Œæœ‰çƒ­å¿ƒç½‘å‹ &lt;a href=&#34;https://github.com/CarlKing5019&#34;&gt;CarlKing5019&lt;/a&gt;æå‡ºäº†è§£å†³æ–¹æ¡ˆï¼Œ è¯·å‚è€ƒissues108ï¼š&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/yidao620c/python3-cookbook/issues/108&#34;&gt;https://github.com/yidao620c/python3-cookbook/issues/108&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;å†æ¬¡æ„Ÿè°¢æ¯ä¸€ä½è´¡çŒ®è€…ã€‚&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;How to Contribute&lt;/h2&gt; &#xA;&lt;p&gt;You are welcome to contribute to the project as follow&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;fork project and commit pull requests&lt;/li&gt; &#xA; &lt;li&gt;add/edit wiki&lt;/li&gt; &#xA; &lt;li&gt;report/fix issue&lt;/li&gt; &#xA; &lt;li&gt;code review&lt;/li&gt; &#xA; &lt;li&gt;commit new feature&lt;/li&gt; &#xA; &lt;li&gt;add testcase&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Meanwhile you&#39;d better follow the rules below&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;It&#39;s &lt;em&gt;NOT&lt;/em&gt; recommended to submit a pull request directly to &lt;code&gt;master&lt;/code&gt; branch. &lt;code&gt;develop&lt;/code&gt; branch is more appropriate&lt;/li&gt; &#xA; &lt;li&gt;Follow common Python coding conventions&lt;/li&gt; &#xA; &lt;li&gt;Add the following &lt;a href=&#34;http://www.apache.org/licenses/LICENSE-2.0&#34;&gt;license&lt;/a&gt; in each source file&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;(The Apache License)&lt;/p&gt; &#xA;&lt;p&gt;Copyright (c) 2014-2018 &lt;a href=&#34;https://www.xncoding.com/&#34;&gt;Xiong Neng&lt;/a&gt; and other contributors&lt;/p&gt; &#xA;&lt;p&gt;Licensed under the Apache License, Version 2.0 (the &#34;License&#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;   http://www.apache.org/licenses/LICENSE-2.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &#34;AS IS&#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>dotnet/csharp-notebooks</title>
    <updated>2022-06-03T01:43:20Z</updated>
    <id>tag:github.com,2022-06-03:/dotnet/csharp-notebooks</id>
    <link href="https://github.com/dotnet/csharp-notebooks" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Get started learning C# with C# notebooks powered by .NET Interactive and VS Code.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;.NET Interactive Notebooks for C#&lt;/h1&gt; &#xA;&lt;p&gt;Welcome to the home of .NET interactive notebooks for C#!&lt;/p&gt; &#xA;&lt;h2&gt;How to Install&lt;/h2&gt; &#xA;&lt;h3&gt;VS Code&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download the .NET Coding Pack for VS Code for &lt;a href=&#34;https://aka.ms/dotnet-coding-pack-win&#34;&gt;Windows&lt;/a&gt; or &lt;a href=&#34;https://aka.ms/dotnet-coding-pack-mac&#34;&gt;macOS&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Install the &lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=ms-dotnettools.dotnet-interactive-vscode&#34;&gt;.NET Interactive Notebooks&lt;/a&gt; extension.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Visual Studio&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download and install &lt;a href=&#34;https://visualstudio.microsoft.com/downloads/&#34;&gt;Visual Studio 2022&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Download and install &lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=MLNET.notebook&#34;&gt;Notebook Editor Extension&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For more information and resources, visit &lt;a href=&#34;https://dotnet.microsoft.com/learntocode&#34;&gt;Learn to code C#&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;C# 101&lt;/h2&gt; &#xA;&lt;p&gt;Download or clone this repo and open the &lt;code&gt;csharp-101&lt;/code&gt; folder in VS Code to get started with the C# 101 notebooks. Or, if you want just tap on one of the Notebook links below and automatically have it open in VS Code!&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;#&lt;/th&gt; &#xA;   &lt;th&gt;Topic&lt;/th&gt; &#xA;   &lt;th&gt;Notebook Link&lt;/th&gt; &#xA;   &lt;th&gt;Video Link&lt;/th&gt; &#xA;   &lt;th&gt;Documentation&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;Hello World&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/csharp101-notebook01&#34;&gt;01 Notebook&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=KT2VR7m19So&amp;amp;list=PLdo4fOcmZ0oVxKLQCHpiUWun7vlJJvUiN&amp;amp;index=2&#34;&gt;01 Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.microsoft.com/dotnet/csharp/tour-of-csharp/tutorials/hello-world?WT.mc_id=csharpnotebook-35129-website&#34;&gt;Intro to C#&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;The Basics of Strings&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/csharp101-notebook02&#34;&gt;02 Notebook&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=JSpC7Cz64h0&amp;amp;list=PLdo4fOcmZ0oVxKLQCHpiUWun7vlJJvUiN&amp;amp;index=3&#34;&gt;02 Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.microsoft.com/dotnet/csharp/tour-of-csharp/tutorials/hello-world?WT.mc_id=csharpnotebook-35129-website&#34;&gt;Intro to C#&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;Searching Strings&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/csharp101-notebook03&#34;&gt;03 Notebook&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=JL30gSE3WaQ&amp;amp;list=PLdo4fOcmZ0oVxKLQCHpiUWun7vlJJvUiN&amp;amp;index=4&#34;&gt;03 Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.microsoft.com/dotnet/csharp/tour-of-csharp/tutorials/hello-world?WT.mc_id=csharpnotebook-35129-website&#34;&gt;Intro to C#&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;Numbers and Integers Math&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/csharp101-notebook04&#34;&gt;04 Notebook&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=jEE0pWTq54U&amp;amp;list=PLdo4fOcmZ0oVxKLQCHpiUWun7vlJJvUiN&amp;amp;index=5&#34;&gt;04 Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.microsoft.com/dotnet/csharp/tour-of-csharp/tutorials/numbers-in-csharp?WT.mc_id=csharpnotebook-35129-website&#34;&gt;Numbers in C#&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;Numbers and Integer Precision&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/csharp101-notebook05&#34;&gt;05 Notebook&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=31EmPADtv4w&amp;amp;list=PLdo4fOcmZ0oVxKLQCHpiUWun7vlJJvUiN&amp;amp;index=6&#34;&gt;05 Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.microsoft.com/dotnet/csharp/tour-of-csharp/tutorials/numbers-in-csharp?WT.mc_id=csharpnotebook-35129-website&#34;&gt;Numbers in C#&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;6&lt;/td&gt; &#xA;   &lt;td&gt;Numbers and Decimals&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/csharp101-notebook06&#34;&gt;06 Notebook&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=kdKcpF9roeU&amp;amp;list=PLdo4fOcmZ0oVxKLQCHpiUWun7vlJJvUiN&amp;amp;index=7&#34;&gt;06 Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.microsoft.com/dotnet/csharp/tour-of-csharp/tutorials/numbers-in-csharp?WT.mc_id=csharpnotebook-35129-website&#34;&gt;Numbers in C#&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;Branches (if)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/csharp101-notebook07&#34;&gt;07 Notebook&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=y4OTe8LSokg&amp;amp;list=PLdo4fOcmZ0oVxKLQCHpiUWun7vlJJvUiN&amp;amp;index=8&#34;&gt;07 Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.microsoft.com/dotnet/csharp/tour-of-csharp/tutorials/branches-and-loops-local?WT.mc_id=csharpnotebook-35129-website&#34;&gt;Branches and Loops in C#&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;What Are Loops?&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/csharp101-notebook08&#34;&gt;08 Notebook&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=z31m5Up_gSQ&amp;amp;list=PLdo4fOcmZ0oVxKLQCHpiUWun7vlJJvUiN&amp;amp;index=10&#34;&gt;08 Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.microsoft.com/dotnet/csharp/tour-of-csharp/tutorials/branches-and-loops-local?WT.mc_id=csharpnotebook-35129-website&#34;&gt;Branches and Loops in C#&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;9&lt;/td&gt; &#xA;   &lt;td&gt;Combining Branches and Loops&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/csharp101-notebook09&#34;&gt;09 Notebook&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=qK7tUpaOXi8&amp;amp;list=PLdo4fOcmZ0oVxKLQCHpiUWun7vlJJvUiN&amp;amp;index=11&#34;&gt;09 Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.microsoft.com/dotnet/csharp/tour-of-csharp/tutorials/branches-and-loops-local?WT.mc_id=csharpnotebook-35129-website&#34;&gt;Branches and Loops in C#&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;10&lt;/td&gt; &#xA;   &lt;td&gt;Arrays, Lists, and Collections&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/csharp101-notebook10&#34;&gt;10 Notebook&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=qLeF_wpnVto&amp;amp;list=PLdo4fOcmZ0oVxKLQCHpiUWun7vlJJvUiN&amp;amp;index=12&#34;&gt;10 Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.microsoft.com/dotnet/csharp/tour-of-csharp/tutorials/arrays-and-collections?WT.mc_id=csharpnotebook-35129-website&#34;&gt;Arrays, Lists, and Collections in C#&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;11&lt;/td&gt; &#xA;   &lt;td&gt;Search, Sort, and Index Lists&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/csharp101-notebook11&#34;&gt;11 Notebook&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=NJ5ghiutzfY&amp;amp;list=PLdo4fOcmZ0oVxKLQCHpiUWun7vlJJvUiN&amp;amp;index=13&#34;&gt;11 Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.microsoft.com/dotnet/csharp/tour-of-csharp/tutorials/arrays-and-collections?WT.mc_id=csharpnotebook-35129-website&#34;&gt;Arrays, Lists, and Collections in C#&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;   &lt;td&gt;Lists of Other Types&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/csharp101-notebook12&#34;&gt;12 Notebook&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=oIQdb93xewE&amp;amp;list=PLdo4fOcmZ0oVxKLQCHpiUWun7vlJJvUiN&amp;amp;index=14&#34;&gt;12 Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.microsoft.com/dotnet/csharp/tour-of-csharp/tutorials/arrays-and-collections?WT.mc_id=csharpnotebook-35129-website&#34;&gt;Arrays, Lists, and Collections in C#&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;13&lt;/td&gt; &#xA;   &lt;td&gt;Objects and Classes&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/csharp101-notebook13&#34;&gt;13 Notebook&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=TzgxcAiHCWA&amp;amp;list=PLdo4fOcmZ0oVxKLQCHpiUWun7vlJJvUiN&amp;amp;index=16&#34;&gt;13 Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.microsoft.com/dotnet/csharp/fundamentals/tutorials/classes?WT.mc_id=csharpnotebook-35129-website&#34;&gt;Object Oriented Coding in C#&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;14&lt;/td&gt; &#xA;   &lt;td&gt;Methods and Members&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/csharp101-notebook14&#34;&gt;14 Notebook&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=xLhm3bEG__c&amp;amp;list=PLdo4fOcmZ0oVxKLQCHpiUWun7vlJJvUiN&amp;amp;index=17&#34;&gt;14 Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.microsoft.com/dotnet/csharp/fundamentals/tutorials/classes?WT.mc_id=csharpnotebook-35129-website&#34;&gt;Object Oriented Coding in C#&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;15&lt;/td&gt; &#xA;   &lt;td&gt;Methods and Exceptions&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/csharp101-notebook15&#34;&gt;15 Notebook&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=8YsoBBiVVzQ&amp;amp;list=PLdo4fOcmZ0oVxKLQCHpiUWun7vlJJvUiN&amp;amp;index=18&#34;&gt;15 Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.microsoft.com/dotnet/csharp/fundamentals/tutorials/classes?WT.mc_id=csharpnotebook-35129-website&#34;&gt;Object Oriented Coding in C#&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Machine Learning&lt;/h2&gt; &#xA;&lt;p&gt;Download or clone this repo and open the &lt;code&gt;machine-learning&lt;/code&gt; folder in Visual Studio 2022 to get started with the machine-learning notebooks. Or, if you want just tap on one of the Notebook links below and automatically have it open in Visual Studio!&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Links below require &lt;a href=&#34;https://visualstudio.microsoft.com/downloads/&#34;&gt;Visual Studio 2022&lt;/a&gt; and &lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=MLNET.notebook&#34;&gt;Notebook Editor Extension&lt;/a&gt; 0.3.4 or greater&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Getting Started Series&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;#&lt;/th&gt; &#xA;   &lt;th&gt;Topic&lt;/th&gt; &#xA;   &lt;th&gt;Notebook Link&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;Intro to Machine Learning&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/ml-01-intro&#34;&gt;01 Notebook&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;Data Prep and Feature Engineering&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/ml-02-data&#34;&gt;02 Notebook&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;Training and AutoML&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/ml-03-training&#34;&gt;03 Notebook&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;Model Evaluation&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/ml-04-evaluation&#34;&gt;04 Notebook&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;End to End (E2E) Notebooks - examples of the entire ML process.&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;#&lt;/th&gt; &#xA;   &lt;th&gt;Topic&lt;/th&gt; &#xA;   &lt;th&gt;Notebook Link&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;E2E&lt;/td&gt; &#xA;   &lt;td&gt;Classification using AutoML (Iris Dataset)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/ml-e2e-iris&#34;&gt;Iris E2E AutoML&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;E2E&lt;/td&gt; &#xA;   &lt;td&gt;Forecasting using Regression (Luna Dataset)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/ml-e2e-luna-regression&#34;&gt;Luna E2E Regression&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;E2E&lt;/td&gt; &#xA;   &lt;td&gt;Forecasting using SSA (Luna Dataset)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/ml-e2e-luna-ssa&#34;&gt;Luna E2E SSA&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;E2E&lt;/td&gt; &#xA;   &lt;td&gt;Regression using AutoML (Taxi Dataset)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/ml-e2e-taxi&#34;&gt;Taxi E2E AutoML&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Reference Notebooks&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;#&lt;/th&gt; &#xA;   &lt;th&gt;Topic&lt;/th&gt; &#xA;   &lt;th&gt;Notebook Link&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;REF&lt;/td&gt; &#xA;   &lt;td&gt;Data Processing with DataFrame&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/ml-ref-data-frame&#34;&gt;Data Frame&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;REF&lt;/td&gt; &#xA;   &lt;td&gt;Graphs and Visualizations&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/ml-ref-visualizations&#34;&gt;Visualizations&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;REF&lt;/td&gt; &#xA;   &lt;td&gt;Kaggle Competitions (Titanic Dataset)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/ml-ref-kaggle-titanic&#34;&gt;Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;.NET Foundation&lt;/h2&gt; &#xA;&lt;p&gt;.NET Interative Notebooks for C# is a &lt;a href=&#34;https://www.dotnetfoundation.org/projects&#34;&gt;.NET Foundation&lt;/a&gt; project.&lt;/p&gt; &#xA;&lt;p&gt;There are many .NET related projects on GitHub.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Microsoft/dotnet&#34;&gt;.NET home repo&lt;/a&gt;&amp;nbsp;- links to 100s of .NET projects, from Microsoft and the community.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/aspnet/core/?view=aspnetcore-3.1&#34;&gt;ASP.NET Core home&lt;/a&gt; - the best place to start learning about ASP.NET Core.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This project has adopted the code of conduct defined by the &lt;a href=&#34;http://contributor-covenant.org/&#34;&gt;Contributor Covenant&lt;/a&gt; to clarify expected behavior in our community. For more information, see the &lt;a href=&#34;http://www.dotnetfoundation.org/code-of-conduct&#34;&gt;.NET Foundation Code of Conduct&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;.NET (including the csharp-notebooks repo) is licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/dotnet/csharp-notebooks/main/LICENSE&#34;&gt;MIT&lt;/a&gt; license.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>NVIDIA-AI-IOT/deepstream_python_apps</title>
    <updated>2022-06-03T01:43:20Z</updated>
    <id>tag:github.com,2022-06-03:/NVIDIA-AI-IOT/deepstream_python_apps</id>
    <link href="https://github.com/NVIDIA-AI-IOT/deepstream_python_apps" rel="alternate"></link>
    <summary type="html">&lt;p&gt;DeepStream SDK Python bindings and sample applications&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DeepStream Python Apps&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains Python bindings and sample applications for the &lt;a href=&#34;https://developer.nvidia.com/deepstream-sdk&#34;&gt;DeepStream SDK&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;SDK version supported: 6.1&lt;/p&gt; &#xA;&lt;p&gt;&lt;b&gt;The bindings sources along with build instructions are now available under &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/bindings&#34;&gt;bindings&lt;/a&gt;! &lt;/b&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;b&gt;This release comes with Operating System upgrades (from Ubuntu 18.04 to Ubuntu 20.04) for DeepStreamSDK 6.1 support. This translates to upgrade in Python version to 3.8 and &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/3rdparty/gst-python/&#34;&gt;gst-python&lt;/a&gt; version has also been upgraded to 1.16.2 !&lt;/b&gt;&lt;/p&gt; &#xA;&lt;p&gt;Download the latest release package complete with bindings and sample applications from the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/releases&#34;&gt;release section&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Please report any issues or bugs on the &lt;a href=&#34;https://devtalk.nvidia.com/default/board/209&#34;&gt;DeepStream SDK Forums&lt;/a&gt;. This enables the DeepStream community to find help at a central location.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/#deepstream-python-apps&#34;&gt;DeepStream Python Apps&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/#python-bindings&#34;&gt;Python Bindings&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/#sample-applications&#34;&gt;Sample Applications&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a name=&#34;metadata_bindings&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Python Bindings&lt;/h2&gt; &#xA;&lt;p&gt;DeepStream pipelines can be constructed using Gst Python, the GStreamer framework&#39;s Python bindings. For accessing DeepStream MetaData, Python &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/bindings&#34;&gt;bindings&lt;/a&gt; are provided as part of this repository. This module is generated using &lt;a href=&#34;https://github.com/pybind/pybind11&#34;&gt;Pybind11&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/.python-app-pipeline.png&#34; alt=&#34;bindings pipeline&#34; height=&#34;600px&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;These bindings support a Python interface to the MetaData structures and functions. Usage of this interface is documented in the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/HOWTO.md&#34;&gt;HOW-TO Guide&lt;/a&gt; and demonstrated in the sample applications.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;sample_applications&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Sample Applications&lt;/h2&gt; &#xA;&lt;p&gt;Sample applications provided here demonstrate how to work with DeepStream pipelines using Python.&lt;br&gt; The sample applications require &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/#metadata_bindings&#34;&gt;MetaData Bindings&lt;/a&gt; to work.&lt;/p&gt; &#xA;&lt;p&gt;To run the sample applications or write your own, please consult the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/HOWTO.md&#34;&gt;HOW-TO Guide&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/.test3-app.png&#34; alt=&#34;deepstream python app screenshot&#34; height=&#34;400px&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;We currently provide the following sample applications:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps/deepstream-test1&#34;&gt;deepstream-test1&lt;/a&gt; -- 4-class object detection pipeline&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps/deepstream-test2&#34;&gt;deepstream-test2&lt;/a&gt; -- 4-class object detection, tracking and attribute classification pipeline&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;UPDATE&lt;/b&gt; &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps/deepstream-test3&#34;&gt;deepstream-test3&lt;/a&gt; -- multi-stream pipeline performing 4-class object detection - now also supports triton inference server, no-display mode, file-loop and silent mode&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps/deepstream-test4&#34;&gt;deepstream-test4&lt;/a&gt; -- msgbroker for sending analytics results to the cloud&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps/deepstream-imagedata-multistream&#34;&gt;deepstream-imagedata-multistream&lt;/a&gt; -- multi-stream pipeline with access to image buffers&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps/deepstream-ssd-parser&#34;&gt;deepstream-ssd-parser&lt;/a&gt; -- SSD model inference via Triton server with output parsing in Python&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps/deepstream-test1-usbcam&#34;&gt;deepstream-test1-usbcam&lt;/a&gt; -- deepstream-test1 pipelien with USB camera input&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps/deepstream-test1-rtsp-out&#34;&gt;deepstream-test1-rtsp-out&lt;/a&gt; -- deepstream-test1 pipeline with RTSP output&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps/deepstream-opticalflow&#34;&gt;deepstream-opticalflow&lt;/a&gt; -- optical flow and visualization pipeline with flow vectors returned in NumPy array&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps/deepstream-segmentation&#34;&gt;deepstream-segmentation&lt;/a&gt; -- segmentation and visualization pipeline with segmentation mask returned in NumPy array&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps/deepstream-nvdsanalytics&#34;&gt;deepstream-nvdsanalytics&lt;/a&gt; -- multistream pipeline with analytics plugin&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps/runtime_source_add_delete&#34;&gt;runtime_source_add_delete&lt;/a&gt; -- add/delete source streams at runtime&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps/deepstream-imagedata-multistream-redaction&#34;&gt;deepstream-imagedata-multistream-redaction&lt;/a&gt; -- multi-stream pipeline with face detection and redaction&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps/deepstream-rtsp-in-rtsp-out&#34;&gt;deepstream-rtsp-in-rtsp-out&lt;/a&gt; -- multi-stream pipeline with RTSP input/output&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;NEW&lt;/b&gt; &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps/deepstream-preprocess-test&#34;&gt;deepstream-preprocess-test&lt;/a&gt; -- multi-stream pipeline using nvdspreprocess plugin with custom ROIs&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Detailed application information is provided in each application&#39;s subdirectory under &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps&#34;&gt;apps&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>aamini/introtodeeplearning</title>
    <updated>2022-06-03T01:43:20Z</updated>
    <id>tag:github.com,2022-06-03:/aamini/introtodeeplearning</id>
    <link href="https://github.com/aamini/introtodeeplearning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Lab Materials for MIT 6.S191: Introduction to Deep Learning&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;http://introtodeeplearning.com&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aamini/introtodeeplearning/master/assets/banner.png&#34; alt=&#34;banner&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository contains all of the code and software labs for &lt;a href=&#34;http://introtodeeplearning.com&#34;&gt;MIT 6.S191: Introduction to Deep Learning&lt;/a&gt;! All lecture slides and videos are available on the course website.&lt;/p&gt; &#xA;&lt;h1&gt;Instructions&lt;/h1&gt; &#xA;&lt;p&gt;6.S191 software labs are designed to be completed at your own pace. At the end of each of the labs, there will be instructions on how you can submit your notebook for grade. Additionally, if you would like to submit your lab as part of the 6.S191 lab competitions, instructions regarding what information must be submitted is also provided at the end of each lab.&lt;/p&gt; &#xA;&lt;h2&gt;Opening the labs in Google Colaboratory:&lt;/h2&gt; &#xA;&lt;p&gt;The 2022 6.S191 labs will be run in Google&#39;s Colaboratory, a Jupyter notebook environment that runs entirely in the cloud, you don&#39;t need to download anything. To run these labs, you must have a Google account.&lt;/p&gt; &#xA;&lt;p&gt;On this Github repo, navigate to the lab folder you want to run (&lt;code&gt;lab1&lt;/code&gt;, &lt;code&gt;lab2&lt;/code&gt;, &lt;code&gt;lab3&lt;/code&gt;) and open the appropriate python notebook (*.ipynb). Click the &#34;Run in Colab&#34; link on the top of the lab. That&#39;s it!&lt;/p&gt; &#xA;&lt;h2&gt;Running the labs&lt;/h2&gt; &#xA;&lt;p&gt;Now, to run the labs, open the Jupyter notebook on Colab. Navigate to the &#34;Runtime&#34; tab --&amp;gt; &#34;Change runtime type&#34;. In the pop-up window, under &#34;Runtime type&#34; select &#34;Python 3&#34;, and under &#34;Hardware accelerator&#34; select &#34;GPU&#34;. Go through the notebooks and fill in the &lt;code&gt;#TODO&lt;/code&gt; cells to get the code to compile for yourself!&lt;/p&gt; &#xA;&lt;h3&gt;MIT Deep Learning package&lt;/h3&gt; &#xA;&lt;p&gt;You might notice that inside the labs we install the &lt;code&gt;mitdeeplearning&lt;/code&gt; python package from the Python Package repository:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install mitdeeplearning&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;This package contains convienence functions that we use throughout the course and can be imported like any other Python package.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import mitdeeplearning as mdl&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;We do this for you in each of the labs, but the package is also open source under the same license so you can also use it outside the class.&lt;/p&gt; &#xA;&lt;h2&gt;Lecture Videos&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=njKP3FqW3Sk&amp;amp;list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&amp;amp;index=1&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aamini/introtodeeplearning/master/assets/video_play.png&#34; width=&#34;500&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;All lecture videos are available publicly online and linked above! Use and/or modification of lecture slides outside of 6.S191 must reference:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Â© MIT 6.S191: Introduction to Deep Learning&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;http://introtodeeplearning.com&#34;&gt;http://introtodeeplearning.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;All code in this repository is copyright 2022 &lt;a href=&#34;http://introtodeeplearning.com&#34;&gt;MIT 6.S191 Introduction to Deep Learning&lt;/a&gt;. All Rights Reserved.&lt;/p&gt; &#xA;&lt;p&gt;Licensed under the MIT License. You may not use this file except in compliance with the License. Use and/or modification of this code outside of 6.S191 must reference:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Â© MIT 6.S191: Introduction to Deep Learning&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;http://introtodeeplearning.com&#34;&gt;http://introtodeeplearning.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt;</summary>
  </entry>
</feed>