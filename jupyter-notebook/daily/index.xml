<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-02-01T01:32:46Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>RAIVNLab/MRL</title>
    <updated>2024-02-01T01:32:46Z</updated>
    <id>tag:github.com,2024-02-01:/RAIVNLab/MRL</id>
    <link href="https://github.com/RAIVNLab/MRL" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code repository for the paper - &#34;Matryoshka Representation Learning&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;a href=&#34;https://arxiv.org/abs/2205.13147&#34;&gt;Matryoshka Representation Learning&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;Aditya Kusupati*, Gantavya Bhatt*, Aniket Rege*, Matthew Wallingford, Aditya Sinha, Vivek Ramanujan, William Howard-Snyder, Kaifeng Chen, Sham Kakade, Prateek Jain, Ali Farhadi&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Learned representations are used in multiple downstream tasks like web-scale search &amp;amp; classification. However, they are flat &amp;amp; rigid -- Information is diffused across dimensions and cannot be adaptively deployed without large post-hoc overhead. We fix both of these issues with &lt;strong&gt;Matryoshka Representation Learning&lt;/strong&gt; (MRL)ðŸª†.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/RAIVNLab/MRL/main/images/mrl-teaser.jpeg&#34; width=&#34;512&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;This repository contains code to train, evaluate, and analyze Matryoshka Representations with a ResNet50 backbone. The training pipeline utilizes efficient &lt;a href=&#34;https://github.com/libffcv/ffcv-imagenet&#34;&gt;FFCV&lt;/a&gt; dataloaders modified for MRL. The repository is organized as follows:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Set up&lt;/li&gt; &#xA; &lt;li&gt;Matryoshka Linear Layer&lt;/li&gt; &#xA; &lt;li&gt;Training ResNet50 Models&lt;/li&gt; &#xA; &lt;li&gt;Inference&lt;/li&gt; &#xA; &lt;li&gt;Model Analysis&lt;/li&gt; &#xA; &lt;li&gt;Retrieval&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Set Up&lt;/h2&gt; &#xA;&lt;p&gt;Pip install the requirements file in this directory. Note that a python3 distribution is required:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip3 install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Preparing the Dataset&lt;/h3&gt; &#xA;&lt;p&gt;Following the ImageNet training pipeline of &lt;a href=&#34;https://github.com/libffcv/ffcv-imagenet&#34;&gt;FFCV&lt;/a&gt; for ResNet50, generate the dataset with the following command (&lt;code&gt;IMAGENET_DIR&lt;/code&gt; should point to a PyTorch style &lt;a href=&#34;https://github.com/MadryLab/pytorch-imagenet-dataset&#34;&gt;ImageNet dataset&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Required environmental variables for the script:&#xA;cd train/&#xA;export IMAGENET_DIR=/path/to/pytorch/format/imagenet/directory/&#xA;export WRITE_DIR=/your/path/here/&#xA;&#xA;# Serialize images with:&#xA;# - 500px side length maximum&#xA;# - 50% JPEG encoded, 90% raw pixel values&#xA;# - quality=90 JPEGs&#xA;./write_imagenet.sh 500 0.50 90&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that we prepare the dataset with the following FFCV configuration:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ResNet-50 training: 50% JPEG 500px side length (&lt;em&gt;train_500_0.50_90.ffcv&lt;/em&gt;)&lt;/li&gt; &#xA; &lt;li&gt;ResNet-50 evaluation: 0% JPEG 500px side length (&lt;em&gt;val_500_uncompressed.ffcv&lt;/em&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Matryoshka Linear Layer&lt;/h2&gt; &#xA;&lt;p&gt;We make only a minor modification to the ResNet50 architecture via the MRL linear layer, defined in &lt;code&gt;MRL.py&lt;/code&gt;, which can be instantiated as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;nesting_list = [8, 16, 32, 64, 128, 256, 512, 1024, 2048]&#xA;fc_layer = MRL_Linear_Layer(nesting_list, num_classes=1000, efficient=efficient)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Where &lt;code&gt;nesting_list&lt;/code&gt; is the list of representation sizes we wish to train on, &lt;code&gt;num_classes&lt;/code&gt; is the number of output features, and the &lt;code&gt;efficient&lt;/code&gt; flag is to train MRL-E.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RAIVNLab/MRL/main/train/&#34;&gt;Training ResNet50 models&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/RAIVNLab/MRL/main/images/mrl-r50-accuracy.jpeg&#34; width=&#34;784&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;We use PyTorch Distributed Data Parallel shared over 2 A100 GPUs and FFCV dataloaders. FFCV utilizes 8 A100 GPUs, therefore we linearly downscale the learning rate by 4 to compensate. We utilize the &lt;code&gt;rn50_40_epochs.yaml&lt;/code&gt; configuration file provided by FFCV to train MRL ResNet50 models for 40 epochs. While training, we dump model ckpts and training logs by default. &lt;code&gt;$WRITE_DIR&lt;/code&gt; is same variable used to create the dataset.&lt;/p&gt; &#xA;&lt;h3&gt;Training Fixed Feature Baseline&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CUDA_VISIBLE_DEVICES=0,1&#xA;&#xA;python train_imagenet.py --config-file rn50_configs/rn50_40_epochs.yaml --model.fixed_feature=2048 \&#xA;--data.train_dataset=$WRITE_DIR/train_500_0.50_90.ffcv --data.val_dataset=$WRITE_DIR/val_500_uncompressed.ffcv \&#xA;--data.num_workers=12 --data.in_memory=1 --logging.folder=trainlogs --logging.log_level=1 \&#xA;--dist.world_size=2 --training.distributed=1 --lr.lr=0.425&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Training MRL model&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CUDA_VISIBLE_DEVICES=0,1&#xA;&#xA;python train_imagenet.py --config-file rn50_configs/rn50_40_epochs.yaml --model.mrl=1 \&#xA;--data.train_dataset=$WRITE_DIR/train_500_0.50_90.ffcv --data.val_dataset=$WRITE_DIR/val_500_uncompressed.ffcv \&#xA;--data.num_workers=12 --data.in_memory=1 --logging.folder=trainlogs --logging.log_level=1 \&#xA;--dist.world_size=2 --training.distributed=1 --lr.lr=0.425&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Training MRL-E model&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CUDA_VISIBLE_DEVICES=0,1&#xA;&#xA;python train_imagenet.py --config-file rn50_configs/rn50_40_epochs.yaml --model.efficient=1 \&#xA;--data.train_dataset=$WRITE_DIR/train_500_0.50_90.ffcv --data.val_dataset=$WRITE_DIR/val_500_uncompressed.ffcv \&#xA;--data.num_workers=12 --data.in_memory=1 --logging.folder=trainlogs --logging.log_level=1 \&#xA;--dist.world_size=2 --training.distributed=1 --lr.lr=0.425&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, we start nesting from rep. size = 8 (i.e. $2^3$). We provide flexibility in starting nesting, for example from rep. size = 16, with the &lt;code&gt;nesting_start&lt;/code&gt; flag as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# to start nesting from d=16&#xA;--model.nesting_start=4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RAIVNLab/MRL/main/inference/&#34;&gt;Inference on Trained Models&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h3&gt;Classification performance&lt;/h3&gt; &#xA;&lt;p&gt;To evaluate our models, we utilize the &lt;code&gt;pytorch_inference.py&lt;/code&gt; script; arguments in brackets are optional. This script is also able to evaluate the standard Imagenet-1K validation set (V1). To evaluate the Fixed Feature (FF) Baseline, pass &lt;code&gt;--rep_size &amp;lt;dim&amp;gt;&lt;/code&gt; flag to evaluate a particular representation size. For example, to evaluate an FF model with rep. size = 512:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cd inference&#xA;&#xA;python pytorch_inference.py --path &amp;lt;final_weight.pt&amp;gt; --dataset &amp;lt;V2/A/Sketch/R/V1&amp;gt; --rep_size 512&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Similarly, to evaluate MRL models, pass the &lt;code&gt;--mrl&lt;/code&gt; flag (add &lt;code&gt;--efficient&lt;/code&gt; for MRL-E). Note that for MRL models, the &lt;code&gt;rep_size&lt;/code&gt; flag is not required. The general form of the command to evaluate trained models is:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cd inference&#xA;&#xA;python pytorch_inference.py --path &amp;lt;final_weight.pt&amp;gt; --dataset &amp;lt;V2/A/Sketch/R/V1&amp;gt; \&#xA;[--tta] [--mrl] [--efficient] [--rep_size &amp;lt;dim&amp;gt;] [--old_ckpt] [--save_logits] \&#xA;[--save_softmax] [--save_gt] [--save_predictions]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;save_*&lt;/code&gt; flags are useful for downstream &lt;a href=&#34;https://raw.githubusercontent.com/RAIVNLab/MRL/main/model_analysis&#34;&gt;model analysis&lt;/a&gt;. Our script is able to perform &#34;test time augmentation (tta)&#34; during evaluation with the &lt;code&gt;--tta&lt;/code&gt; flag. Note that the classification results reported in the paper are without tta, and tta is only used for adaptive classification using model cascades. Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/RAIVNLab/MRL/main/model_analysis&#34;&gt;model analysis&lt;/a&gt; for further details.&lt;/p&gt; &#xA;&lt;p&gt;Lastly, to evaluate our uploaded checkpoints (ResNet50), please additionally use the &lt;code&gt;--old_ckpt&lt;/code&gt; flag. Our model checkpoints can be found &lt;a href=&#34;https://drive.google.com/drive/folders/1IEfJk4xp-sPEKvKn6eKAUzvoRV8ho2vq?usp=sharing&#34;&gt;here&lt;/a&gt;, and are arranged according to the training routine. The model naming convention is such that &lt;code&gt;r50_mrl1_e0_ff2048.pt&lt;/code&gt; corresponds to the model trained with MRL (here &#34;e&#34; refers to efficient) and &lt;code&gt;r50_mrl0_e0_ff256.pt&lt;/code&gt; corresponds to the model with rep. size = 256 and trained without MRL. In the paper we only consider $rep. size \in [8, 16, 32, 64, 128, 256, 512, 1024, 2048]$. To evaluate on other rep. sizes, change the variable &lt;code&gt;NESTING_LIST&lt;/code&gt; in &lt;code&gt;pytorch_eval.py&lt;/code&gt;. For a detailed description, please run &lt;code&gt;python pytorch_inference.py --help&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Robustness Datasets&lt;/h4&gt; &#xA;&lt;p&gt;We also evaluate our trained models on four robustness datasets: ImageNetV2/A/R/Sketch. Note that for evaluation, we utilized PyTorch dataloaders. Please refer to their respective repositories for additional documentation and download the datasets in the root directory.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/modestyachts/ImageNetV2_pytorch&#34;&gt;ImageNetV2_pytorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hendrycks/natural-adv-examples&#34;&gt;ImageNetA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hendrycks/imagenet-r&#34;&gt;ImageNetR&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/HaohanWang/ImageNet-Sketch&#34;&gt;ImageNet-Sketch&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RAIVNLab/MRL/main/model_analysis/&#34;&gt;Model Analysis&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;cd model_analysis&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;We provide four Jupyter notebooks which contain performance visualization via GradCAM images (for checkpoint models), superclass performance, model cascades and oracle upper bound. Please refer to detailed documentation &lt;a href=&#34;https://raw.githubusercontent.com/RAIVNLab/MRL/main/model_analysis/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RAIVNLab/MRL/main/retrieval/&#34;&gt;Retrieval&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;We carry out image retrieval on ImageNet-1K with two query sets, ImageNet-1K validation set and ImageNetV2. We also created &lt;a href=&#34;https://raw.githubusercontent.com/RAIVNLab/MRL/main/imagenet-4k&#34;&gt;ImageNet-4K&lt;/a&gt; to evaluate MRL image retrieval in an out-of-distribution setting, with its validation set used as query set. A detailed description of the retrieval pipeline is provided &lt;a href=&#34;https://raw.githubusercontent.com/RAIVNLab/MRL/main/retrieval/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In an attempt to achieve optimal compute-accuracy tradeoff, we carry out &lt;strong&gt;Adaptive Retrieval&lt;/strong&gt; by retrieving a $k=$ 200 length neighbors shortlist with lower dimension $D_s$ and reranking with higher dimension $D_r$. We also provide a simple cascading policy to automate the choice of appropriate $D_s$ and $D_r$, which we call &lt;strong&gt;Funnel Retrieval&lt;/strong&gt;. We retrieve a shortlist at $D_s$ and then re-rank the shortlist five times while simultaneously increasing $D_r$ (rerank cascade) and decreasing the shortlist length $k$ (shortlist cascade), which resembles a funnel structure. With both of these techniques, we are able to match the Top-1 accuracy (%) of retrieval with $D_s=$ 2048 with 128$\times$ less MFLOPs/Query on ImageNet-1K.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this project useful in your research, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{kusupati2022matryoshka,&#xA;  title     = {Matryoshka Representation Learning},&#xA;  author    = {Kusupati, Aditya and Bhatt, Gantavya and Rege, Aniket and Wallingford, Matthew and Sinha, Aditya and Ramanujan, Vivek and Howard-Snyder, William and Chen, Kaifeng and Kakade, Sham and Jain, Prateek and others},&#xA;  title     = {Matryoshka Representation Learning.},&#xA;  booktitle = {Advances in Neural Information Processing Systems},&#xA;  month     = {December},&#xA;  year      = {2022},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>aigeek0x0/rag-with-langchain-colbert-and-ragatouille</title>
    <updated>2024-02-01T01:32:46Z</updated>
    <id>tag:github.com,2024-02-01:/aigeek0x0/rag-with-langchain-colbert-and-ragatouille</id>
    <link href="https://github.com/aigeek0x0/rag-with-langchain-colbert-and-ragatouille" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Build a Streamlit Chatbot using Langchain, ColBERT, Ragatouille, and ChromaDB&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Build a Streamlit Chatbot using Langchain, ColBERT, Ragatouille, and ChromaDB&lt;/h1&gt; &#xA;&lt;p&gt;This is an implementation of advanced RAG system using Langchain&#39;s EnsembleRetriever and ColBERT. It efficiently pulls all the relevant context required for Mixtral 8x7B to generate high-quality answers for us.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/12b9-R32q39WrNpwW7C3uGRNdm0Tg_elK?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/aigeek0x0/rag-with-langchain-colbert-and-ragatouille/raw/main/lets-rag.png&#34; alt=&#34;Build a Streamlit Chatbot using Langchain, ColBERT, Ragatouille, and ChromaDB&#34; width=&#34;800&#34; style=&#34;margin-left:&#39;auto&#39; margin-right:&#39;auto&#39; display:&#39;block&#39;&#34;&gt;</summary>
  </entry>
</feed>