<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-06-05T01:33:29Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Lifelong-Robot-Learning/LIBERO</title>
    <updated>2025-06-05T01:33:29Z</updated>
    <id>tag:github.com,2025-06-05:/Lifelong-Robot-Learning/LIBERO</id>
    <link href="https://github.com/Lifelong-Robot-Learning/LIBERO" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Benchmarking Knowledge Transfer in Lifelong Robot Learning&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/Lifelong-Robot-Learning/LIBERO/raw/master/images/libero_logo.png&#34; width=&#34;360&#34;&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/Lifelong-Robot-Learning/LIBERO/actions&#34;&gt; &lt;img alt=&#34;Tests Passing&#34; src=&#34;https://github.com/anuraghazra/github-readme-stats/workflows/Test/badge.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/Lifelong-Robot-Learning/LIBERO/graphs/contributors&#34;&gt; &lt;img alt=&#34;GitHub Contributors&#34; src=&#34;https://img.shields.io/github/contributors/Lifelong-Robot-Learning/LIBERO&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/Lifelong-Robot-Learning/LIBERO/issues&#34;&gt; &lt;img alt=&#34;Issues&#34; src=&#34;https://img.shields.io/github/issues/Lifelong-Robot-Learning/LIBERO?color=0088ff&#34;&gt; &lt;/a&gt;&lt;/p&gt;&#xA; &lt;h2&gt;&lt;a href=&#34;https://github.com/Lifelong-Robot-Learning/LIBERO/issues&#34;&gt;&lt;strong&gt;Benchmarking Knowledge Transfer for Lifelong Robot Learning&lt;/strong&gt;&lt;/a&gt;&lt;/h2&gt;&#xA; &lt;a href=&#34;https://github.com/Lifelong-Robot-Learning/LIBERO/issues&#34;&gt; &lt;p&gt;Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, Peter Stone&lt;/p&gt; &lt;/a&gt;&#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/Lifelong-Robot-Learning/LIBERO/issues&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://libero-project.github.io&#34;&gt;[Website]&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/pdf/2306.03310.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://lifelong-robot-learning.github.io/LIBERO/&#34;&gt;[Docs]&lt;/a&gt;&lt;/p&gt; &#xA; &lt;hr&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://github.com/Lifelong-Robot-Learning/LIBERO/raw/master/images//fig1.png&#34; alt=&#34;pull_figure&#34;&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;LIBERO&lt;/strong&gt; is designed for studying knowledge transfer in multitask and lifelong robot learning problems. Successfully resolving these problems require both declarative knowledge about objects/spatial relationships and procedural knowledge about motion/behaviors. &lt;strong&gt;LIBERO&lt;/strong&gt; provides:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;a procedural generation pipeline that could in principle generate an infinite number of manipulation tasks.&lt;/li&gt; &#xA; &lt;li&gt;130 tasks grouped into four task suites: &lt;strong&gt;LIBERO-Spatial&lt;/strong&gt;, &lt;strong&gt;LIBERO-Object&lt;/strong&gt;, &lt;strong&gt;LIBERO-Goal&lt;/strong&gt;, and &lt;strong&gt;LIBERO-100&lt;/strong&gt;. The first three task suites have controlled distribution shifts, meaning that they require the transfer of a specific type of knowledge. In contrast, &lt;strong&gt;LIBERO-100&lt;/strong&gt; consists of 100 manipulation tasks that require the transfer of entangled knowledge. &lt;strong&gt;LIBERO-100&lt;/strong&gt; is further splitted into &lt;strong&gt;LIBERO-90&lt;/strong&gt; for pretraining a policy and &lt;strong&gt;LIBERO-10&lt;/strong&gt; for testing the agent&#39;s downstream lifelong learning performance.&lt;/li&gt; &#xA; &lt;li&gt;five research topics.&lt;/li&gt; &#xA; &lt;li&gt;three visuomotor policy network architectures.&lt;/li&gt; &#xA; &lt;li&gt;three lifelong learning algorithms with the sequential finetuning and multitask learning baselines.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Contents&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lifelong-Robot-Learning/LIBERO/master/#Installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lifelong-Robot-Learning/LIBERO/master/#Dataset&#34;&gt;Datasets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lifelong-Robot-Learning/LIBERO/master/#Getting-Started&#34;&gt;Getting Started&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lifelong-Robot-Learning/LIBERO/master/#Task&#34;&gt;Task&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lifelong-Robot-Learning/LIBERO/master/#Training&#34;&gt;Training&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lifelong-Robot-Learning/LIBERO/master/#Evaluation&#34;&gt;Evaluation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lifelong-Robot-Learning/LIBERO/master/#Citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lifelong-Robot-Learning/LIBERO/master/#License&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Installtion&lt;/h1&gt; &#xA;&lt;p&gt;Please run the following commands in the given order to install the dependency for &lt;strong&gt;LIBERO&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n libero python=3.8.13&#xA;conda activate libero&#xA;git clone https://github.com/Lifelong-Robot-Learning/LIBERO.git&#xA;cd LIBERO&#xA;pip install -r requirements.txt&#xA;pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0 --extra-index-url https://download.pytorch.org/whl/cu113&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then install the &lt;code&gt;libero&lt;/code&gt; package:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Datasets&lt;/h1&gt; &#xA;&lt;p&gt;We provide high-quality human teleoperation demonstrations for the four task suites in &lt;strong&gt;LIBERO&lt;/strong&gt;. To download the demonstration dataset, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python benchmark_scripts/download_libero_datasets.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, the dataset will be stored under the &lt;code&gt;LIBERO&lt;/code&gt; folder and all four datasets will be downloaded. To download a specific dataset, use&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python benchmark_scripts/download_libero_datasets.py --datasets DATASET&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where &lt;code&gt;DATASET&lt;/code&gt; is chosen from &lt;code&gt;[libero_spatial, libero_object, libero_100, libero_goal&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NEW!!!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Alternatively, you can download the dataset from HuggingFace by using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python benchmark_scripts/download_libero_datasets.py --use-huggingface&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This option can also be combined with the specific dataset selection:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python benchmark_scripts/download_libero_datasets.py --datasets DATASET --use-huggingface&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The datasets hosted on HuggingFace are available at &lt;a href=&#34;https://huggingface.co/datasets/yifengzhu-hf/LIBERO-datasets&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Getting Started&lt;/h1&gt; &#xA;&lt;p&gt;For a detailed walk-through, please either refer to the documentation or the notebook examples provided under the &lt;code&gt;notebooks&lt;/code&gt; folder. In the following, we provide example scripts for retrieving a task, training and evaluation.&lt;/p&gt; &#xA;&lt;h2&gt;Task&lt;/h2&gt; &#xA;&lt;p&gt;The following is a minimal example of retrieving a specific task from a specific task suite.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from libero.libero import benchmark&#xA;from libero.libero.envs import OffScreenRenderEnv&#xA;&#xA;&#xA;benchmark_dict = benchmark.get_benchmark_dict()&#xA;task_suite_name = &#34;libero_10&#34; # can also choose libero_spatial, libero_object, etc.&#xA;task_suite = benchmark_dict[task_suite_name]()&#xA;&#xA;# retrieve a specific task&#xA;task_id = 0&#xA;task = task_suite.get_task(task_id)&#xA;task_name = task.name&#xA;task_description = task.language&#xA;task_bddl_file = os.path.join(get_libero_path(&#34;bddl_files&#34;), task.problem_folder, task.bddl_file)&#xA;print(f&#34;[info] retrieving task {task_id} from suite {task_suite_name}, the &#34; + \&#xA;      f&#34;language instruction is {task_description}, and the bddl file is {task_bddl_file}&#34;)&#xA;&#xA;# step over the environment&#xA;env_args = {&#xA;    &#34;bddl_file_name&#34;: task_bddl_file,&#xA;    &#34;camera_heights&#34;: 128,&#xA;    &#34;camera_widths&#34;: 128&#xA;}&#xA;env = OffScreenRenderEnv(**env_args)&#xA;env.seed(0)&#xA;env.reset()&#xA;init_states = task_suite.get_task_init_states(task_id) # for benchmarking purpose, we fix the a set of initial states&#xA;init_state_id = 0&#xA;env.set_init_state(init_states[init_state_id])&#xA;&#xA;dummy_action = [0.] * 7&#xA;for step in range(10):&#xA;    obs, reward, done, info = env.step(dummy_action)&#xA;env.close()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Currently, we only support sparse reward function (i.e., the agent receives &lt;code&gt;+1&lt;/code&gt; when the task is finished). As sparse-reward RL is extremely hard to learn, currently we mainly focus on lifelong imitation learning.&lt;/p&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;To start a lifelong learning experiment, please choose:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;BENCHMARK&lt;/code&gt; from &lt;code&gt;[LIBERO_SPATIAL, LIBERO_OBJECT, LIBERO_GOAL, LIBERO_90, LIBERO_10]&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;POLICY&lt;/code&gt; from &lt;code&gt;[bc_rnn_policy, bc_transformer_policy, bc_vilt_policy]&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ALGO&lt;/code&gt; from &lt;code&gt;[base, er, ewc, packnet, multitask]&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;then run the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export CUDA_VISIBLE_DEVICES=GPU_ID &amp;amp;&amp;amp; \&#xA;export MUJOCO_EGL_DEVICE_ID=GPU_ID &amp;amp;&amp;amp; \&#xA;python libero/lifelong/main.py seed=SEED \&#xA;                               benchmark_name=BENCHMARK \&#xA;                               policy=POLICY \&#xA;                               lifelong=ALGO&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please see the documentation for the details of reproducing the study results.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;By default the policies will be evaluated on the fly during training. If you have limited computing resource of GPUs, we offer an evaluation script for you to evaluate models separately.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python libero/lifelong/evaluate.py --benchmark BENCHMARK_NAME \&#xA;                                   --task_id TASK_ID \ &#xA;                                   --algo ALGO_NAME \&#xA;                                   --policy POLICY_NAME \&#xA;                                   --seed SEED \&#xA;                                   --ep EPOCH \&#xA;                                   --load_task LOAD_TASK \&#xA;                                   --device_id CUDA_ID&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Citation&lt;/h1&gt; &#xA;&lt;p&gt;If you find &lt;strong&gt;LIBERO&lt;/strong&gt; to be useful in your own research, please consider citing our paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{liu2023libero,&#xA;  title={LIBERO: Benchmarking Knowledge Transfer for Lifelong Robot Learning},&#xA;  author={Liu, Bo and Zhu, Yifeng and Gao, Chongkai and Feng, Yihao and Liu, Qiang and Zhu, Yuke and Stone, Peter},&#xA;  journal={arXiv preprint arXiv:2306.03310},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Component&lt;/th&gt; &#xA;   &lt;th&gt;License&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Codebase&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lifelong-Robot-Learning/LIBERO/master/LICENSE&#34;&gt;MIT License&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Datasets&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://creativecommons.org/licenses/by/4.0/legalcode&#34;&gt;Creative Commons Attribution 4.0 International (CC BY 4.0)&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt;</summary>
  </entry>
  <entry>
    <title>Liuziyu77/Visual-RFT</title>
    <updated>2025-06-05T01:33:29Z</updated>
    <id>tag:github.com,2025-06-05:/Liuziyu77/Visual-RFT</id>
    <link href="https://github.com/Liuziyu77/Visual-RFT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official repository of &#39;Visual-RFT: Visual Reinforcement Fine-Tuning&#39; &amp; &#39;Visual-ARFT: Visual Agentic Reinforcement Fine-Tuning&#39;‚Äô&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;!--   &lt;h1 align=&#34;center&#34;&gt;&lt;img src=&#34;assets/logo.png&#34; width=&#34;256&#34;&gt;&lt;/h1&gt; --&gt; &lt;/p&gt;&#xA;&lt;h1 align=&#34;center&#34;&gt;Visual-RFT: Visual Reinforcement Fine-Tuning&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/Liuziyu77&#34;&gt;&lt;strong&gt;Ziyu Liu*&lt;/strong&gt;&lt;/a&gt; ¬∑ &lt;a href=&#34;https://github.com/SunzeY&#34;&gt;&lt;strong&gt;Zeyi Sun*&lt;/strong&gt;&lt;/a&gt; ¬∑ &lt;a href=&#34;https://yuhangzang.github.io/&#34;&gt;&lt;strong&gt;Yuhang Zang&lt;/strong&gt;&lt;/a&gt; ¬∑ &lt;a href=&#34;https://lightdxy.github.io/&#34;&gt;&lt;strong&gt;Xiaoyi Dong&lt;/strong&gt;&lt;/a&gt; ¬∑ &lt;a href=&#34;https://scholar.google.com/citations?user=sJkqsqkAAAAJ&#34;&gt;&lt;strong&gt;Yuhang Cao&lt;/strong&gt;&lt;/a&gt; ¬∑ &lt;a href=&#34;https://kennymckormick.github.io/&#34;&gt;&lt;strong&gt;Haodong Duan&lt;/strong&gt;&lt;/a&gt; ¬∑ &lt;a href=&#34;http://dahua.site/&#34;&gt;&lt;strong&gt;Dahua Lin&lt;/strong&gt;&lt;/a&gt; ¬∑ &lt;a href=&#34;https://myownskyw7.github.io/&#34;&gt;&lt;strong&gt;Jiaqi Wang&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;!--   &lt;h2 align=&#34;center&#34;&gt;Accepted By ICLR 2025!&lt;/h2&gt; --&gt; &#xA;&lt;!-- üè†&lt;a href=&#34;https://liuziyu77.github.io/MIA-DPO/&#34;&gt;Homepage&lt;/a&gt;&lt;/h3&gt;| --&gt; üìñ&#xA;&lt;a href=&#34;https://arxiv.org/abs/2503.01785&#34;&gt;Paper&lt;/a&gt; | ü§ó&#xA;&lt;a href=&#34;https://huggingface.co/collections/laolao77/virft-datasets-67bc271b6f2833eccc0651df&#34;&gt;Datasets&lt;/a&gt; | ü§ó&#xA;&lt;a href=&#34;https://huggingface.co/papers/2503.01785&#34;&gt;Daily Paper&lt;/a&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; üåàWe introduce &lt;strong&gt;Visual Reinforcement Fine-tuning (Visual-RFT)&lt;/strong&gt;, the first comprehensive adaptation of &lt;strong&gt;Deepseek-R1&#39;s RL strategy&lt;/strong&gt; to the &lt;strong&gt;multimodal field&lt;/strong&gt;. We use the Qwen2-VL-2/7B model as our base model and design a &lt;strong&gt;rule-based verifiable reward&lt;/strong&gt;, which is integrated into a &lt;strong&gt;GRPO-based reinforcement fine-tuning framework&lt;/strong&gt; to enhance the performance of LVLMs across various visual perception tasks. &lt;strong&gt;ViRFT&lt;/strong&gt; extends R1&#39;s reasoning capabilities to multiple visual perception tasks, including various detection tasks like &lt;strong&gt;Open Vocabulary Detection, Few-shot Detection, Reasoning Grounding, and Fine-grained Image Classification&lt;/strong&gt;. &lt;/p&gt; &#xA;&lt;!--     &lt;a href=&#34;&#34;&gt;&#xA;      &lt;img src=&#34;assets/teaser.png&#34; alt=&#34;Logo&#34; width=&#34;100%&#34;&gt; &#xA;    &lt;/a&gt; --&gt; &#xA;&lt;br&gt; &#xA;&lt;a href=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Liuziyu77/Visual-RFT/main/assets/radar.png&#34; alt=&#34;Logo&#34;&gt; &lt;/a&gt; &#xA;&lt;h2&gt;üî•üî•üî• Visual-RFT: Visual Reinforcement Fine-Tuning&lt;/h2&gt; &#xA;&lt;p&gt;We introduce &lt;em&gt;Visual Reinforcement Fine-tuning (Visual-RFT)&lt;/em&gt;, the first comprehensive adaptation of Deepseek-R1‚Äôs RL strategy to the multimodal field. We use the Qwen2-VL-2/7B model as our base model and design a rule-based verifiable reward, which is integrated into a GRPO-based reinforcement fine-tuning framework to enhance the performance of LVLMs across various visual perception tasks.&lt;/p&gt; &#xA;&lt;p&gt;üìñ&lt;a href=&#34;https://arxiv.org/abs/2503.01785&#34;&gt;Paper&lt;/a&gt; | ü§ó&lt;a href=&#34;https://huggingface.co/collections/laolao77/virft-datasets-67bc271b6f2833eccc0651df&#34;&gt;Datasets&lt;/a&gt; | ü§ó&lt;a href=&#34;https://huggingface.co/papers/2503.01785&#34;&gt;Daily Paper&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üî•üî•üî• Visual-ARFT: Visual Agentic Reinforcement Fine-Tuning&lt;/h2&gt; &#xA;&lt;p&gt;Our new work &lt;em&gt;Visual Agentic Reinforcement Fine-Tuning (Visual-ARFT)&lt;/em&gt; is designed for enabling flexible and adaptive agentic abilities for Large Vision-Language Models (LVLMs). With Visual-ARFT, open-source LVLMs gain the ability to browse websites for real-time information updates and write code to manipulate and analyze input images through cropping, rotation, and other image processing techniques. We also present a Multi-modal Agentic Tool Bench (MAT) with two settings (MAT-Search and MAT-Coding) designed to evaluate LVLMs‚Äô agentic search and coding abilities.&lt;/p&gt; &#xA;&lt;p&gt;üìñ&lt;a href=&#34;https://arxiv.org/abs/2505.14246&#34;&gt;Paper&lt;/a&gt; | ü§ó&lt;a href=&#34;https://huggingface.co/datasets/laolao77/MAT&#34;&gt;Datasets&lt;/a&gt; | ü§ó&lt;a href=&#34;https://huggingface.co/collections/laolao77/visual-arft-682c601d0e35ac6470adfe9f&#34;&gt;Models&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üì¢ News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üöÄ [05/21/2025] We support both &lt;strong&gt;HuggingFace Dataset&lt;/strong&gt; format and &lt;strong&gt;JSON&lt;/strong&gt; file format as input datasets for training.&lt;/li&gt; &#xA; &lt;li&gt;üöÄ [05/21/2025] We updata the trainer of &lt;strong&gt;Visual-RFT&lt;/strong&gt; to support both Qwen2-VL and Qwen2.5-VL. And we support multi-image inputs with &lt;code&gt;grpo_trainer_mp.py&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üöÄ [05/20/2025] We release &lt;strong&gt;Visual-ARFT&lt;/strong&gt; repository &lt;a href=&#34;https://github.com/Liuziyu77/Visual-RFT/tree/main/Visual-ARFT&#34;&gt;Repo-URL&lt;/a&gt;: A RFT framework dedicated to enhancing the &lt;strong&gt;multimodal agentic capabilities of LVLMs&lt;/strong&gt;. (Support Qwen2-VL and Qwen2.5-VL)&lt;/li&gt; &#xA; &lt;li&gt;üöÄ [03/12/2025] We release the code of &lt;strong&gt;Visual-RFT&lt;/strong&gt; to build the &lt;a href=&#34;https://github.com/Liuziyu77/Visual-RFT/tree/main/dataset&#34;&gt;dataset&lt;/a&gt; on your own data.&lt;/li&gt; &#xA; &lt;li&gt;üöÄ [03/04/2025] We release our &lt;strong&gt;Visual-RFT&#39;s&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2503.01785&#34;&gt;Paper&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üöÄ [03/04/2025] We upload our training datasets of &lt;strong&gt;Visual-RFT&lt;/strong&gt; to &lt;a href=&#34;https://huggingface.co/collections/laolao77/virft-datasets-67bc271b6f2833eccc0651df&#34;&gt;Huggingface&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üöÄ [03/04/2025] We release &lt;strong&gt;Visual-RFT&lt;/strong&gt; repository and our training code.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üí° Highlights&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üî• &lt;strong&gt;Visual Reinforcement Fine-tuning (Visual-RFT)&lt;/strong&gt;: We introduce Visual Reinforcement Fine-tuning (&lt;strong&gt;Visual-RFT&lt;/strong&gt;), which extends reinforcement learning with verified rewards on visual perception tasks that are effective with limited data for fine-tuning.&lt;/li&gt; &#xA; &lt;li&gt;üî• &lt;strong&gt;Verified Rewards&lt;/strong&gt;: We design different &lt;strong&gt;verified rewards&lt;/strong&gt; for different visual tasks that enable efficient, high-quality reward computation at a negligible cost. This allows the seamless transfer of DeepSeek R1&#39;s style reinforcement learning strategy to the multi-modal domain.&lt;/li&gt; &#xA; &lt;li&gt;üî• &lt;strong&gt;Extensive Experiments&lt;/strong&gt;: We conduct &lt;strong&gt;extensive experiments&lt;/strong&gt; on various visual perception tasks, including fine-grained image classification, open vocabulary object detection, few-shot object detection, and reasoning grounding.&lt;/li&gt; &#xA; &lt;li&gt;üî• &lt;strong&gt;Open Source&lt;/strong&gt;: We fully &lt;strong&gt;open-source&lt;/strong&gt; the training code, training data, and evaluation scripts on Github to facilitate further research.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;a href=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Liuziyu77/Visual-RFT/main/assets/teaser.png&#34; alt=&#34;Logo&#34;&gt; &lt;/a&gt; &#xA;&lt;h2&gt;Framework&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Visual-RFT&lt;/strong&gt; framework is shown below. The policy model generates a group of responses based on the input. Each response is passed through a verifiable reward function to compute the reward. After group computation of the rewards for each output, the quality of each response is evaluated and used to update the policy model. To ensure the stability of the policy model training, &lt;strong&gt;Visual-RFT&lt;/strong&gt; use KL divergence to limit the difference between the policy model and the reference model. For &lt;em&gt;&lt;strong&gt;more implementation details&lt;/strong&gt;&lt;/em&gt;, including data generation, the design of the &lt;em&gt;&lt;strong&gt;verifiable reward&lt;/strong&gt;&lt;/em&gt;, and other aspects, please refer to our paper.&lt;/p&gt; &#xA;&lt;a href=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Liuziyu77/Visual-RFT/main/assets/framework.png&#34; alt=&#34;Logo&#34;&gt; &lt;/a&gt; &#xA;&lt;h2&gt;üõ†Ô∏è Setup&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/Liuziyu77/Visual-RFT.git&#xA;conda create -n Visual-RFT python=3.10&#xA;conda activate Visual-RFT&#xA;bash setup.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;We have uploaded the model trained on 200+ samples from the LISA dataset (&lt;a href=&#34;https://huggingface.co/Zery/Qwen2-VL-7B_visual_rft_lisa_IoU_reward&#34;&gt;ü§óHuggingface&lt;/a&gt;). You can use it to evaluate the inference performance of &lt;strong&gt;Reasoning Grounding&lt;/strong&gt;. More details refer to &lt;code&gt;demo&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;h3&gt;Datasets&lt;/h3&gt; &#xA;&lt;p&gt;To train on our various visual perception tasks, first visit &lt;a href=&#34;https://huggingface.co/collections/laolao77/virft-datasets-67bc271b6f2833eccc0651df&#34;&gt;Huggingface Datasets&lt;/a&gt; to download the datasets. We have uploaded different datasets for different tasks.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Datasets&lt;/th&gt; &#xA;   &lt;th&gt;Task&lt;/th&gt; &#xA;   &lt;th&gt;Setting&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;laolao77/ViRFT_COCO&lt;/td&gt; &#xA;   &lt;td&gt;Detection&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;It includes all categories from COCO, with a total of 6k entries.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;laolao77/ViRFT_COCO_base65&lt;/td&gt; &#xA;   &lt;td&gt;Detection&lt;/td&gt; &#xA;   &lt;td&gt;Open Vocabulary&lt;/td&gt; &#xA;   &lt;td&gt;It includes 65 basic categories from COCO, with a total of 6k entries.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;laolao77/ViRFT_COCO_8_cate_4_shot&lt;/td&gt; &#xA;   &lt;td&gt;Detection&lt;/td&gt; &#xA;   &lt;td&gt;Few-shot&lt;/td&gt; &#xA;   &lt;td&gt;It includes 8 selected categories from COCO.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;laolao77/ViRFT_LVIS_few_shot&lt;/td&gt; &#xA;   &lt;td&gt;Detection&lt;/td&gt; &#xA;   &lt;td&gt;Few-shot&lt;/td&gt; &#xA;   &lt;td&gt;It includes 6 selected categories from COCO.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;laolao77/ViRFT_CLS_flower_4_shot&lt;/td&gt; &#xA;   &lt;td&gt;Classification&lt;/td&gt; &#xA;   &lt;td&gt;Few-shot&lt;/td&gt; &#xA;   &lt;td&gt;It includes the 102 categories from the Flower102 dataset, with 4 images per category.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;laolao77/ViRFT_CLS_fgvc_aircraft_4_shot&lt;/td&gt; &#xA;   &lt;td&gt;Classification&lt;/td&gt; &#xA;   &lt;td&gt;Few-shot&lt;/td&gt; &#xA;   &lt;td&gt;It includes the 100 categories from the FGVC-Aircraft dataset, with 4 images per category.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;laolao77/ViRFT_CLS_car196_4shot&lt;/td&gt; &#xA;   &lt;td&gt;Classification&lt;/td&gt; &#xA;   &lt;td&gt;Few-shot&lt;/td&gt; &#xA;   &lt;td&gt;It includes the 196 categories from the Stanford Cars dataset, with 4 images per category.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;laolao77/ViRFT_CLS_pets37_4shot&lt;/td&gt; &#xA;   &lt;td&gt;Classification&lt;/td&gt; &#xA;   &lt;td&gt;Few-shot&lt;/td&gt; &#xA;   &lt;td&gt;It includes the 37 categories from the Pets37 dataset, with 4 images per category.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LISA dataset&lt;/td&gt; &#xA;   &lt;td&gt;Grounding&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;Reasoning Grounding&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;üîî If your want to build a dataset on your own data, you can refere to &lt;code&gt;dataset/build_dataset.ipynb&lt;/code&gt;. Just provide a &lt;code&gt;json&lt;/code&gt; file with &lt;code&gt;image&lt;/code&gt;, &lt;code&gt;promble&lt;/code&gt; and &#39;solution&#39;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;Datasets Formats&lt;/strong&gt; üî¶ We support both &lt;strong&gt;HuggingFace Dataset&lt;/strong&gt; format and &lt;strong&gt;JSON&lt;/strong&gt; file format as input datasets for training.&lt;/p&gt; &#xA;&lt;p&gt;Refer to &lt;a href=&#34;https://github.com/Liuziyu77/Visual-RFT/raw/main/src/virft/src/open_r1/grpo.py&#34;&gt;grpo.py&lt;/a&gt; for &lt;strong&gt;HuggingFace Dataset&lt;/strong&gt; format example.&lt;/p&gt; &#xA;&lt;p&gt;Refer to &lt;a href=&#34;https://github.com/Liuziyu77/Visual-RFT/raw/main/Visual-ARFT/src/visual_arft/src/open_r1/grpo_agent_search.py&#34;&gt;grpo.py&lt;/a&gt; for &lt;strong&gt;JSON&lt;/strong&gt; format example.&lt;/p&gt; &#xA;&lt;h3&gt;GRPO&lt;/h3&gt; &#xA;&lt;p&gt;After downloading the dataset, you can start training using the following example bash script. Our bash scripts are in &lt;code&gt;/src/scripts&lt;/code&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;üîî There&#39;s no need for prolonged training. For a dataset with only a few hundred samples, 200 steps should be sufficient.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code&gt;# There&#39;s no need for prolonged training. For a dataset with only a few hundred samples, 200 steps should be sufficient.&#xA;export DEBUG_MODE=&#34;true&#34;&#xA;export LOG_PATH=&#34;./debug_log_2b_GRPO_coco_base65cate_6k.txt&#34;&#xA;&#xA;export DATA_PATH=./share_data/ViRFT_COCO_base65   ### your local dataset downloading from huggingface&#xA;export CKPT_PATH=./share_models/Qwen2-VL-2B-Instruct    ### Qwen2-VL-2B checkpoint path&#xA;export SAVE_PATH=./share_models/Qwen2-VL-2B-Instruct_GRPO_coco_base65cate_6k    ### save path&#xA;&#xA;torchrun --nproc_per_node=&#34;8&#34; \&#xA;    --nnodes=&#34;1&#34; \&#xA;    --node_rank=&#34;0&#34; \&#xA;    --master_addr=&#34;127.0.0.1&#34; \&#xA;    --master_port=&#34;12345&#34; \&#xA;    src/open_r1/grpo.py \&#xA;    --output_dir ${SAVE_PATH}  \&#xA;    --model_name_or_path ${CKPT_PATH} \&#xA;    --dataset_name ${DATA_PATH} \&#xA;    --deepspeed local_scripts/zero3.json \&#xA;    --max_prompt_length 1024 \&#xA;    --per_device_train_batch_size 1 \&#xA;    --gradient_accumulation_steps 2 \&#xA;    --logging_steps 1 \&#xA;    --bf16 \&#xA;    --report_to wandb \&#xA;    --gradient_checkpointing false \&#xA;    --attn_implementation flash_attention_2 \&#xA;    --max_pixels 401408 \&#xA;    --num_train_epochs 1 \&#xA;    --run_name Qwen2-VL-2B_GRPO_coco_base65cate_6k \&#xA;    --save_steps 100 \&#xA;    --save_only_model true \&#xA;    --num_generations 8 &#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;OOM Tips&lt;/h3&gt; &#xA;&lt;p&gt;‚è∞ Running into OOM (Out-Of-Memory) issues during training is quite common, especially when using GPUs with limited memory.&lt;/p&gt; &#xA;&lt;p&gt;üî¶ But no worries ‚Äî here are some helpful &lt;strong&gt;OOM tips&lt;/strong&gt; for you:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;About distributed training:&lt;/strong&gt; You can alleviate memory pressure by specifying the &lt;code&gt;--deepspeed&lt;/code&gt; argument, e.g. &lt;code&gt;--deepspeed /src/visual_arft/local_scripts/zero3.json&lt;/code&gt;. If memory is still insufficient, you can further reduce the load by using: &lt;code&gt;--deepspeed /src/visual_arft/local_scripts/zero3_offload.json&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;About the number of generations per group in GRPO:&lt;/strong&gt; You can reduce GPU memory usage by lowering the &lt;code&gt;--num_generation parameter&lt;/code&gt;. In the example script, the default value is &lt;code&gt;--num_generation 8&lt;/code&gt;, but you can try setting it to 4 to save memory. Keep in mind, though, that a smaller &lt;code&gt;--num_generation&lt;/code&gt; may lead to worse performance.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;About gradient_checkpointing:&lt;/strong&gt; Moreover, setting &lt;code&gt;--gradient_checkpointing&lt;/code&gt; to &lt;code&gt;true&lt;/code&gt; can save memory, allowing for a higher &lt;code&gt;--num_generations&lt;/code&gt; limit, which leads to better training performance. However, it will slow down the training process.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;About Image resolution:&lt;/strong&gt; If you&#39;re still encountering OOM issues, you can also reduce the resolution of the images in the training dataset!&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;SFT&lt;/h3&gt; &#xA;&lt;p&gt;We use &lt;a href=&#34;https://github.com/hiyouga/LLaMA-Factory&#34;&gt;LLaMa-Factory&lt;/a&gt; for supervised fine-tuning (SFT) of the model. You can convert the downloaded dataset into the corresponding Qwen SFT format for training.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;We conducted extensive experiments on various visual perception tasks, including &lt;strong&gt;fine-grained image classification&lt;/strong&gt;, &lt;strong&gt;open vocabulary object detection&lt;/strong&gt;, &lt;strong&gt;few-shot object detection&lt;/strong&gt;, and &lt;strong&gt;reasoning grounding&lt;/strong&gt;. &lt;strong&gt;ViRFT&lt;/strong&gt; achieves remarkable performance improvements across these tasks with minimal data and computational cost, significantly surpassing supervised fine-tuning baselines.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;We provide a step-by-step tutorial for using the evaluation code. If you encounter any issues, feel free to open an issue.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;COCO Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;You can use the files in the &lt;code&gt;coco_evaluation&lt;/code&gt; directory for model inference and obtain evaluation results. Our code supports multi-GPU evaluation, and it requires at least two GPUs.&lt;/p&gt; &#xA;&lt;p&gt;For &lt;em&gt;&lt;strong&gt;inference&lt;/strong&gt;&lt;/em&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd ./coco_evaluation&#xA;python Qwen2_VL_coco_infere.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please note that some file paths and model paths in &lt;code&gt;Qwen2_VL_coco_infere.py&lt;/code&gt; need to be modified.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;### line 167-168, change for your model path and model base.&#xA;model_path = &#34;./share_models/Qwen2-VL-2B-Instruct_RL/&#34;  # RL model&#xA;model_base = &#34;./share_models/Qwen2-VL-2B-Instruct/&#34;  # original Qwen2-VL model&#xA;### line 182, change for your coco val annnotation path&#xA;with open(&#39;./data/coco/annotations/instances_val2017.json&#39;, &#39;r&#39;) as json_file:&#xA;### line 224, Modify according to your own image path.&#xA;image_path = &#39;./data/coco/val2017/&#39;+image[&#39;file_name&#39;]    &#xA;### line 231-241, selecte the categories you want to evaluation&#xA;selected_cate = [&#39;bus&#39;, &#39;train&#39;, &#39;fire hydrant&#39;, &#39;stop sign&#39;, &#39;cat&#39;, &#39;dog&#39;, &#39;bed&#39;, &#39;toilet&#39;]&#xA;### line 350, results save path&#xA;with open(f&#39;prediction_results.json&#39;, &#39;w&#39;) as json_file:&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The inference results will be saved in &lt;code&gt;JSON&lt;/code&gt; format and later used for evaluation.&lt;/p&gt; &#xA;&lt;p&gt;For &lt;em&gt;&lt;strong&gt;evaluation&lt;/strong&gt;&lt;/em&gt;, just run &lt;code&gt;./coco_evaluation/evaluation.ipynb&lt;/code&gt; step by step.&lt;/p&gt; &#xA;&lt;h3&gt;LVIS Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;You can use the files in the &lt;code&gt;lvis_evaluation&lt;/code&gt; directory for model inference and obtain evaluation results. Our code supports multi-GPU evaluation, and it requires at least two GPUs.&lt;/p&gt; &#xA;&lt;p&gt;For &lt;em&gt;&lt;strong&gt;inference&lt;/strong&gt;&lt;/em&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd ./lvis_evaluation&#xA;python Qwen2_VL_lvis_infere.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please note that some file paths and model paths in &lt;code&gt;Qwen2_VL_lvis_infere.py&lt;/code&gt; need to be modified.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;### line 169-170, change for your model path and model base&#xA;model_path = &#34;./share_models/Qwen2-VL-2B-Instruct_RL/&#34;  # RL model&#xA;model_base = &#34;./share_models/Qwen2-VL-2B-Instruct/&#34;  # original Qwen2-VL model&#xA;### line 184, change for your lvis val annnotation path&#xA;with open(&#39;./data/lvis/annotations/lvis_v1_val.json&#39;, &#39;r&#39;) as json_file:&#xA;### line 228, Modify according to your own image path.&#xA;image_path = &#39;./data/lvis/&#39; + &#34;/&#34;.join(parts[-2:])   &#xA;### line 234-242, selecte the categories you want to evaluation&#xA;selected_cate = [&#39;horse_buggy&#39;, &#39;die&#39;, &#39;kitchen_table&#39;, &#39;omelet&#39;, &#39;papaya&#39;, &#39;stepladder&#39;]&#xA;### line 346, results save path&#xA;with open(f&#39;prediction_results.json&#39;, &#39;w&#39;) as json_file:&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The inference results will be saved in &lt;code&gt;JSON&lt;/code&gt; format and later used for evaluation.&lt;/p&gt; &#xA;&lt;p&gt;For &lt;em&gt;&lt;strong&gt;evaluation&lt;/strong&gt;&lt;/em&gt;, just run &lt;code&gt;./lvis_evaluation/lvis_evaluation.ipynb&lt;/code&gt; step by step.&lt;/p&gt; &#xA;&lt;h3&gt;Classification Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;You can use the files in the &lt;code&gt;classification&lt;/code&gt; directory for model inference and obtain evaluation results. Our code supports multi-GPU evaluation, and it requires at least two GPUs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd ./classification&#xA;python Qwen2_VL_classification_infere.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please note that the model paths in &lt;code&gt;Qwen2_VL_classification_infere.py&lt;/code&gt; need to be modified.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;### line 61-63, change for your model path and model base&#xA;model_path = &#34;./share_models/Qwen2-VL-2B-Instruct_RL/&#34;  # after RL&#xA;model_base = &#34;./share_models/Qwen2-VL-2B-Instruct/&#34;  # original Qwen2-VL&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Inference and result computation are performed simultaneously. After the program finishes running, the number of correctly classified items will be displayed in the command line, and the accuracy is obtained by dividing it by the length of the validation set. (Flower102: 2463, Pets37: 3669, stanford cars: 8041, fgvc-aircraft: 3333)&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;üîî Sometimes, due to environment issues, the model may produce incorrect inferences when &lt;code&gt;use_cache = None&lt;/code&gt;. You might consider explicitly setting &lt;code&gt;use_cache = True&lt;/code&gt;. &lt;code&gt;generated_ids = model.generate(**inputs, max_new_tokens=1024, use_cache=True)&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Evaluation Results&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;We have conducted &lt;strong&gt;extensive experiments&lt;/strong&gt;; please refer to our paper for further details&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Case Study&lt;/h3&gt; &#xA;&lt;p&gt;In the following figure, we present some inference examples from &lt;strong&gt;ViRFT&lt;/strong&gt;. We observe that the thinking process significantly enhances the reasoning and grounding ability with &lt;strong&gt;ViRFT&lt;/strong&gt;. Through &lt;strong&gt;ViRFT&lt;/strong&gt;, Qwen2-VL learns to think critically and carefully examine the image to produce accurate grounding results. &lt;a href=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Liuziyu77/Visual-RFT/main/assets/case_lisa.png&#34; alt=&#34;Logo&#34;&gt; &lt;/a&gt; We also present some inference cases of the model when handling &lt;em&gt;fine-grained classification tasks&lt;/em&gt;. These results not demonstrate the strong generalization ability of &lt;strong&gt;ViRFT&lt;/strong&gt; across various visual tasks. &lt;a href=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Liuziyu77/Visual-RFT/main/assets/case_cls.png&#34; alt=&#34;Logo&#34;&gt; &lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;‚úíÔ∏èCitation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{liu2025visual,&#xA;  title={Visual-RFT: Visual Reinforcement Fine-Tuning},&#xA;  author={Liu, Ziyu and Sun, Zeyi and Zang, Yuhang and Dong, Xiaoyi and Cao, Yuhang and Duan, Haodong and Lin, Dahua and Wang, Jiaqi},&#xA;  journal={arXiv preprint arXiv:2503.01785},&#xA;  year={2025}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üìÑ License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg?sanitize=true&#34; alt=&#34;Code License&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Data%20License-CC%20By%20NC%204.0-red.svg?sanitize=true&#34; alt=&#34;Data License&#34;&gt; &lt;strong&gt;Usage and License Notices&lt;/strong&gt;: The data and code are intended and licensed for research use only. License: Attribution-NonCommercial 4.0 International It should abide by the policy of OpenAI: &lt;a href=&#34;https://openai.com/policies/terms-of-use&#34;&gt;https://openai.com/policies/terms-of-use&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;We sincerely thank projects &lt;a href=&#34;https://github.com/Deep-Agent/R1-V&#34;&gt;R1-V&lt;/a&gt;, &lt;a href=&#34;https://github.com/huggingface/open-r1&#34;&gt;Open-R1&lt;/a&gt;, and &lt;a href=&#34;https://github.com/EvolvingLMMs-Lab/open-r1-multimodal&#34;&gt;Open-r1-multimodal&lt;/a&gt; for providing their open-source resources.&lt;/p&gt;</summary>
  </entry>
</feed>