<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-12-01T01:33:17Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Zefan-Cai/KVCache-Factory</title>
    <updated>2024-12-01T01:33:17Z</updated>
    <id>tag:github.com,2024-12-01:/Zefan-Cai/KVCache-Factory</id>
    <link href="https://github.com/Zefan-Cai/KVCache-Factory" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Unified KV Cache Compression Methods for Auto-Regressive Models&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Zefan-Cai/KVCache-Factory/main/assets/logo.png&#34; alt=&#34;# KVCache-Facroty&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;[2024-11-28] Çhange the name to KVCache-Factory! The target of our project is now a unified framework of KV Cache compression of diverse models.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2024-06-25] Support multi-GPUs inference with big LLMs now! Try out PyramidKV on LlaMa-3-70B-Instruct!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2024-06-10] Support PyramidKV, SnapKV, H2O and StreamingLLM at Flash Attention v2, Sdpa Attention now! If your devices (i.e., V100, 3090) does not support Flash Attention v2, you can set attn_implementation=sdpa to try PyramidKV at Sdpa Attention!&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TODO:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;Support implementation of Streaming LLM, H2O and SnapKV&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;Support Mistral model&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;Support implementation of Needle&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;Support KV cache compression without Flash Attention v2 (i.e. Sdpa Attention) for V100&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;Support multi-GPU inference for 70B LlaMa-3&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;Introduce new functions to support kv cache budget allocation (i.e., supports for percentage.)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;Support Mixtral&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;Support Batch Inference&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;Support KV cache compression at decoding stage&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Performence&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Zefan-Cai/KVCache-Factory/main/figs/Result.png&#34; width=&#34;100%&#34;&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Zefan-Cai/KVCache-Factory/main/figs/Needle.png&#34; width=&#34;80%&#34;&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Visualization: Inefficient Attention&lt;/h2&gt; &#xA;&lt;p&gt;The Llama model attention map with 3 documents is represented as follows:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Zefan-Cai/KVCache-Factory/main/figs/attention_pattern.png&#34; width=&#34;100%&#34;&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;p&gt;we provide a notebook &lt;code&gt;visualization.ipynb&lt;/code&gt; to reproduce the visualization result of each Llama-2-7b-hf model layer for a given 3 document.&lt;/p&gt; &#xA;&lt;p&gt;Model attention maps for different layers would be stored at &lt;code&gt;./attention&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;transformers &amp;gt;= 4.41&#xA;flash-attn &amp;gt;= 2.4.0.post1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;git clone https://github.com/Zefan-Cai/PyramidKV.git&#xA;cd PyramidKV&#xA;pip install -r requirements.txt .&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;We support inference code on &lt;code&gt;LongBench&lt;/code&gt; to repuduce our result.&lt;/p&gt; &#xA;&lt;p&gt;Please refer to &lt;code&gt;scripts/scripts_longBench/eval.sh&lt;/code&gt; to modify the parameters according to your requirements.&lt;/p&gt; &#xA;&lt;p&gt;Our codebase support Flash Attention v2, Sdpa Attention, etc. The results presented in our paper in based on Flash Attention v2.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CUDA_VISIBLE_DEVICES=$1&#xA;&#xA;method=$2 # Support PyramidKV, SnapKV, H2O, StreamingLLM&#xA;max_capacity_prompts=64 # 128,2048 in paper&#xA;attn_implementation=$3 # Support &#34;flash_attention_2&#34;, &#34;sdpa&#34;, &#34;eager&#34;.&#xA;source_path=$4&#xA;model_path=$5&#xA;save_dir=${source_path}&#34;results_long_bench&#34; # path to result save_dir&#xA;&#xA;python3 run_longbench.py \&#xA;    --method ${method} \&#xA;    --model_path ${model_path} \&#xA;    --max_capacity_prompts ${max_capacity_prompts} \&#xA;    --attn_implementation ${attn_implementation} \&#xA;    --save_dir ${save_dir} \&#xA;    --use_cache True&#xA;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CUDA_VISIBLE_DEVICES: For multi-GPU inference for big LLMs, just need to specify CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7. For single GPU inference, just need to specify CUDA_VISIBLE_DEVICES=0.&lt;/li&gt; &#xA; &lt;li&gt;model_path: Path to your model. Support &#34;Llama-3-8B-Instruct&#34; for now.&lt;/li&gt; &#xA; &lt;li&gt;method: Support &lt;code&gt;PyramidKV&lt;/code&gt;, &lt;code&gt;SnapKV&lt;/code&gt;, &lt;code&gt;StreamingLLM&lt;/code&gt;, &lt;code&gt;H2O&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;max_capacity_prompts: Selected KV Size in each layer. （e.g. 128, 2048 in paper）. When method is &#34;PyramidKV&#34;, given that the total number of KV remains unchanged, the specific KV length for each layer will be modified accordingly&lt;/li&gt; &#xA; &lt;li&gt;save_dir: Path to your dir to save LongBench result.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;After modifying parameters, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&#xA;sh scripts/scripts_longBench/eval.sh&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Needle in haystack&lt;/h2&gt; &#xA;&lt;p&gt;We support inference code on &lt;code&gt;Needle in haystack&lt;/code&gt; to repuduce our result.&lt;/p&gt; &#xA;&lt;p&gt;Please refer to &lt;code&gt;scripts/scripts_needle/eval.sh&lt;/code&gt; to modify the parameters according to your requirements.&lt;/p&gt; &#xA;&lt;p&gt;Our codebase support Flash Attention v2, Sdpa Attention, etc. The results presented in our paper in based on Flash Attention v2.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#xA;METHOD=&#39;pyramidkv&#39;       # [&#39;full&#39;, &#39;pyramidkv&#39;, &#39;snapkv&#39;, &#39;streamingllm&#39;, &#39;h2o&#39;]&#xA;MAX_CAPACITY_PROMPT=96  # [64, 96, 128, 256, 512, 1024, 2048, ...]&#xA;attn_implementation=&#34;flash_attention_2&#34; # Support &#34;flash_attention_2&#34;, &#34;sdpa&#34;, &#34;&#34;.&#xA;TAG=test&#xA;&#xA;&#xA;# For Llama3-8b&#xA;&#xA;(&#xA;python -u run_needle_in_haystack.py --s_len 1000 --e_len 8001\&#xA;    --model_provider LLaMA3 \&#xA;    --model_name /mnt/workspace/zhiyuanhu/yuliang/models/llama3-8b_raw \&#xA;    --attn_implementation ${attn_implementation} \&#xA;    --step 100 \&#xA;    --method $METHOD \&#xA;    --max_capacity_prompt $MAX_CAPACITY_PROMPT \&#xA;    --model_version LlaMA3_${METHOD}_${MAX_CAPACITY_PROMPT}_${TAG}&#xA;) 2&amp;gt;&amp;amp;1  | tee results_needle/logs/LlaMA3_${METHOD}_${MAX_CAPACITY_PROMPT}_${TAG}.log&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Both LLaMA3 and Mistral2 inference support on single GPU.&lt;/li&gt; &#xA; &lt;li&gt;model_provider: LLaMA3 or Mistral2&lt;/li&gt; &#xA; &lt;li&gt;model_name: Path to your model. Support &#34;Llama-3-8B-Instruct&#34; &#34;Mistral-7B-Instruct-v0.2&#34; and for now.&lt;/li&gt; &#xA; &lt;li&gt;step: The increase of context length.&lt;/li&gt; &#xA; &lt;li&gt;method: Support &lt;code&gt;PyramidKV&lt;/code&gt;, &lt;code&gt;SnapKV&lt;/code&gt;, &lt;code&gt;StreamingLLM&lt;/code&gt;, &lt;code&gt;H2O&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;max_capacity_prompt: Selected KV Size in each layer. （e.g. 128, 2048 in paper）. When method is &#34;PyramidKV&#34;, given that the total number of KV remains unchanged, the specific KV length for each layer will be modified accordingly&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To reproduce our results, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash scripts/scripts_needle/eval.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After inference, run&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;python scripts/scripts_needle/visualize.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;to draw the img, you should change &lt;code&gt;FOLDER_PATH&lt;/code&gt; in &lt;code&gt;visualize.py&lt;/code&gt; to your output path (the argument of &lt;code&gt;--model_version&lt;/code&gt; in &lt;code&gt;eval.sh&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find &lt;strong&gt;PyramidKV&lt;/strong&gt; useful for your research and applications, please kindly cite using this BibTeX:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-latex&#34;&gt;@article{cai2024pyramidkv,&#xA;  title={Pyramidkv: Dynamic kv cache compression based on pyramidal information funneling},&#xA;  author={Cai, Zefan and Zhang, Yichi and Gao, Bofei and Liu, Yuliang and Liu, Tianyu and Lu, Keming and Xiong, Wayne and Dong, Yue and Chang, Baobao and Hu, Junjie and Xiao Wen},&#xA;  journal={arXiv preprint arXiv:2406.02069},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-latex&#34;&gt;@article{fu2024not,&#xA;  title={Not All Heads Matter: A Head-Level KV Cache Compression Method with Integrated Retrieval and Reasoning},&#xA;  author={Fu, Yu and Cai, Zefan and Asi, Abedelkadir and Xiong, Wayne and Dong, Yue and Xiao, Wen},&#xA;  journal={arXiv preprint arXiv:2410.19258},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;Thanks &lt;strong&gt;[SnapKV]&lt;/strong&gt; &lt;a href=&#34;https://github.com/FasterDecoding/SnapKV&#34;&gt;SnapKV: LLM Knows What You are Looking for Before Generation&lt;/a&gt; for providing open-source code to support the expansion of this project.&lt;/p&gt;</summary>
  </entry>
</feed>