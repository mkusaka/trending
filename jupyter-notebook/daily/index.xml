<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-16T01:32:01Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>chongzhou96/EdgeSAM</title>
    <updated>2023-12-16T01:32:01Z</updated>
    <id>tag:github.com,2023-12-16:/chongzhou96/EdgeSAM</id>
    <link href="https://github.com/chongzhou96/EdgeSAM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official PyTorch implementation of &#34;EdgeSAM: Prompt-In-the-Loop Distillation for On-Device Deployment of SAM&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;EdgeSAM&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt-In-the-Loop Distillation for On-Device Deployment of SAM&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://chongzhou96.github.io/&#34;&gt;Chong Zhou&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, &lt;a href=&#34;https://lxtgh.github.io/&#34;&gt;Xiangtai Li&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, &lt;a href=&#34;https://www.mmlab-ntu.com/person/ccloy/&#34;&gt;Chen Change Loy&lt;sup&gt;1*&lt;/sup&gt;&lt;/a&gt;, &lt;a href=&#34;https://daibo.info/&#34;&gt;Bo Dai&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;(*corresponding author)&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.mmlab-ntu.com/&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;S-Lab, Nanyang Technological University&lt;/a&gt;, &lt;a href=&#34;https://www.shlab.org.cn/&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;Shanghai Artificial Intelligence Laboratory&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://arxiv.org/abs/2312.06660&#34;&gt;&lt;code&gt;Paper&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://www.mmlab-ntu.com/project/edgesam/&#34;&gt;&lt;code&gt;Project Page&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/spaces/chongzhou/EdgeSAM&#34;&gt;&lt;code&gt;Hugging Face Demo&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;&#34;&gt;&lt;code&gt;iOS App (TBA)&lt;/code&gt;&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/chongzhou96/EdgeSAM/assets/15973859/fe1cd104-88dc-4690-a5ea-ff48ae013db3&#34;&gt;https://github.com/chongzhou96/EdgeSAM/assets/15973859/fe1cd104-88dc-4690-a5ea-ff48ae013db3&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Watch the full live demo video: [&lt;a href=&#34;https://www.youtube.com/watch?v=YYsEQ2vleiE&#34;&gt;YouTube&lt;/a&gt;] [&lt;a href=&#34;https://www.bilibili.com/video/BV1294y1P7TC/&#34;&gt;Bilibili&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;2023/12/16&lt;/strong&gt;: EdgeSAM is now supported in &lt;a href=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything&#34;&gt;Grounded-Segment-Anything&lt;/a&gt;. Check out the &lt;a href=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything/raw/main/EfficientSAM/grounded_edge_sam.py&#34;&gt;grounded-edge-sam demo&lt;/a&gt;. Thanks to the IDEA Research team!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2023/12/14&lt;/strong&gt;: &lt;a href=&#34;https://github.com/autodistill/autodistill-grounded-edgesam&#34;&gt;autodistill-grounded-edgesam&lt;/a&gt; combines Grounding DINO and EdgeSAM to create Grounded EdgeSAM [&lt;a href=&#34;https://blog.roboflow.com/how-to-use-grounded-edgesam/&#34;&gt;blog&lt;/a&gt;]. Thanks to the Roboflow team!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2023/12/13&lt;/strong&gt;: Add ONNX export and speed up the web demo with ONNX as the backend.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;EdgeSAM&lt;/strong&gt; is an accelerated variant of the Segment Anything Model (SAM), optimized for efficient execution on edge devices with minimal compromise in performance. It achieves a &lt;strong&gt;40-fold speed increase&lt;/strong&gt; compared to the original SAM, and outperforms MobileSAM, being &lt;strong&gt;14 times as fast&lt;/strong&gt; when deployed on edge devices while enhancing the mIoUs on COCO and LVIS by 2.3 and 3.2 respectively. EdgeSAM is also the first SAM variant that can run at &lt;strong&gt;over 30 FPS&lt;/strong&gt; on an iPhone 14.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;900&#34; alt=&#34;compare&#34; src=&#34;https://github.com/chongzhou96/EdgeSAM/assets/15973859/95a6f308-7300-4cb4-8b1b-b711cdea3f64&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;In this figure, we show the encoder throughput of EdgeSAM compared with SAM and MobileSAM as well as the mIoU performance on the SA-1K dataset (sampled from SA-1B) with box and point prompts.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; &lt;strong&gt;Approach&lt;/strong&gt; &lt;/summary&gt; &#xA; &lt;p&gt;Our approach involves distilling the original ViT-based SAM image encoder into a purely CNN-based architecture, better suited for edge devices. We carefully benchmark various distillation strategies and demonstrate that task-agnostic encoder distillation fails to capture the full knowledge embodied in SAM. To overcome this bottleneck, we include both the prompt encoder and mask decoder in the distillation process, with box and point prompts in the loop, so that the distilled model can accurately capture the intricate dynamics between user input and mask generation.&lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;612&#34; alt=&#34;arch&#34; src=&#34;https://github.com/chongzhou96/EdgeSAM/assets/15973859/e706101a-c3d5-4d99-bea5-c6735ce25237&#34;&gt; &lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; &lt;strong&gt;Performance&lt;/strong&gt; &lt;/summary&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Method&lt;/th&gt; &#xA;    &lt;th&gt;Train Set&lt;/th&gt; &#xA;    &lt;th&gt;COCO AP&lt;/th&gt; &#xA;    &lt;th&gt;COCO AP&lt;sub&gt;s&lt;/sub&gt;&lt;/th&gt; &#xA;    &lt;th&gt;COCO AP&lt;sub&gt;m&lt;/sub&gt;&lt;/th&gt; &#xA;    &lt;th&gt;COCO AP&lt;sub&gt;l&lt;/sub&gt;&lt;/th&gt; &#xA;    &lt;th&gt;GFLops&lt;/th&gt; &#xA;    &lt;th&gt;MParam.&lt;/th&gt; &#xA;    &lt;th&gt;FPS iPhone 14&lt;/th&gt; &#xA;    &lt;th&gt;FPS 2080 Ti&lt;/th&gt; &#xA;    &lt;th&gt;FPS 3090&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;SAM&lt;/td&gt; &#xA;    &lt;td&gt;SA-1B&lt;/td&gt; &#xA;    &lt;td&gt;46.1&lt;/td&gt; &#xA;    &lt;td&gt;33.6&lt;/td&gt; &#xA;    &lt;td&gt;51.9&lt;/td&gt; &#xA;    &lt;td&gt;57.7&lt;/td&gt; &#xA;    &lt;td&gt;2734.8&lt;/td&gt; &#xA;    &lt;td&gt;641.1&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;td&gt;4.3&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;FastSAM&lt;/td&gt; &#xA;    &lt;td&gt;2% SA-1B&lt;/td&gt; &#xA;    &lt;td&gt;37.9&lt;/td&gt; &#xA;    &lt;td&gt;23.9&lt;/td&gt; &#xA;    &lt;td&gt;43.4&lt;/td&gt; &#xA;    &lt;td&gt;50.0&lt;/td&gt; &#xA;    &lt;td&gt;887.6&lt;/td&gt; &#xA;    &lt;td&gt;68.2&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;td&gt;25.0*&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;MobileSAM&lt;/td&gt; &#xA;    &lt;td&gt;1% SA-1B&lt;/td&gt; &#xA;    &lt;td&gt;39.4&lt;/td&gt; &#xA;    &lt;td&gt;26.9&lt;/td&gt; &#xA;    &lt;td&gt;44.4&lt;/td&gt; &#xA;    &lt;td&gt;52.2&lt;/td&gt; &#xA;    &lt;td&gt;38.2&lt;/td&gt; &#xA;    &lt;td&gt;9.8&lt;/td&gt; &#xA;    &lt;td&gt;4.9&lt;/td&gt; &#xA;    &lt;td&gt;103.5&lt;/td&gt; &#xA;    &lt;td&gt;100.0*&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;EdgeSAM&lt;/td&gt; &#xA;    &lt;td&gt;1% SA-1B&lt;/td&gt; &#xA;    &lt;td&gt;42.2&lt;/td&gt; &#xA;    &lt;td&gt;29.6&lt;/td&gt; &#xA;    &lt;td&gt;47.6&lt;/td&gt; &#xA;    &lt;td&gt;53.9&lt;/td&gt; &#xA;    &lt;td&gt;22.1&lt;/td&gt; &#xA;    &lt;td&gt;9.6&lt;/td&gt; &#xA;    &lt;td&gt;38.7&lt;/td&gt; &#xA;    &lt;td&gt;164.3&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;EdgeSAM-3x&lt;/td&gt; &#xA;    &lt;td&gt;3% SA-1B&lt;/td&gt; &#xA;    &lt;td&gt;42.7&lt;/td&gt; &#xA;    &lt;td&gt;30.0&lt;/td&gt; &#xA;    &lt;td&gt;48.6&lt;/td&gt; &#xA;    &lt;td&gt;54.5&lt;/td&gt; &#xA;    &lt;td&gt;22.1&lt;/td&gt; &#xA;    &lt;td&gt;9.6&lt;/td&gt; &#xA;    &lt;td&gt;38.7&lt;/td&gt; &#xA;    &lt;td&gt;164.3&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;EdgeSAM-10x&lt;/td&gt; &#xA;    &lt;td&gt;10% SA-1B&lt;/td&gt; &#xA;    &lt;td&gt;43.0&lt;/td&gt; &#xA;    &lt;td&gt;30.3&lt;/td&gt; &#xA;    &lt;td&gt;48.9&lt;/td&gt; &#xA;    &lt;td&gt;55.1&lt;/td&gt; &#xA;    &lt;td&gt;22.1&lt;/td&gt; &#xA;    &lt;td&gt;9.6&lt;/td&gt; &#xA;    &lt;td&gt;38.7&lt;/td&gt; &#xA;    &lt;td&gt;164.3&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;p&gt;&lt;em&gt;In this table, we report the mask mAP on the COCO dataset. ViTDet-H is used as the detector, whose box mAP is 58.7, to provide box prompts. For speed benchmarking, we infer both the encoder and decoder (with a single prompt). FLOPs are calculated based on the 1024x1024 input resolution. Numbers denoted by * are copied from MobileSAM. 3x and 10x represent training with more data. Here, we do not apply an additional mask refinement iteration per the setting of the original SAM paper.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chongzhou96/EdgeSAM/master/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chongzhou96/EdgeSAM/master/#usage&#34;&gt;Usage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chongzhou96/EdgeSAM/master/#demo&#34;&gt;Web Demo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chongzhou96/EdgeSAM/master/#export&#34;&gt;CoreML / ONNX Export&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chongzhou96/EdgeSAM/master/#checkpoints&#34;&gt;Checkpoints&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chongzhou96/EdgeSAM/master/#ios&#34;&gt;iOS App&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chongzhou96/EdgeSAM/master/#acknowledgement&#34;&gt;Acknowledgements&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chongzhou96/EdgeSAM/master/#cite&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chongzhou96/EdgeSAM/master/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation &lt;a name=&#34;installation&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;The code requires &lt;code&gt;python&amp;gt;=3.8&lt;/code&gt; and we use &lt;code&gt;torch==2.0.0&lt;/code&gt; and &lt;code&gt;torchvision==0.15.1&lt;/code&gt;. Please refer to the &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;official PyTorch installation instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the repository locally:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone git@github.com:chongzhou96/EdgeSAM.git; cd EdgeSAM&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Install additional dependencies:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Install EdgeSAM:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage &lt;a name=&#34;usage&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download checkpoints (please refer to &lt;a href=&#34;https://raw.githubusercontent.com/chongzhou96/EdgeSAM/master/#checkpoints&#34;&gt;Checkpoints&lt;/a&gt; for more details about the PyTorch and CoreML checkpoints):&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;mkdir weights&#xA;wget -P weights/ https://huggingface.co/spaces/chongzhou/EdgeSAM/resolve/main/weights/edge_sam.pth&#xA;wget -P weights/ https://huggingface.co/spaces/chongzhou/EdgeSAM/resolve/main/weights/edge_sam_3x.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;You can easily incorporate EdgeSAM into your Python code with following lines:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;from segment_anything import SamPredictor, sam_model_registry&#xA;sam = sam_model_registry[&#34;edge_sam&#34;](checkpoint=&#34;&amp;lt;path/to/checkpoint&amp;gt;&#34;)&#xA;predictor = SamPredictor(sam)&#xA;predictor.set_image(&amp;lt;your_image&amp;gt;)&#xA;masks, _, _ = predictor.predict(&amp;lt;input_prompts&amp;gt;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Since EdgeSAM follows the same encoder-decoder architecture as SAM, their usages are very similar. One minor difference is that EdgeSAM allows outputting 1, 3, and 4 mask candidates for each prompt, while SAM yields either 1 or 3 masks. For more details, please refer to the &lt;a href=&#34;https://github.com/chongzhou96/EdgeSAM/raw/master/notebooks/predictor_example.ipynb&#34;&gt;example Jupyter Notebook&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Web Demo &lt;a name=&#34;demo&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;After installing EdgeSAM and downloading the checkpoints. You can start an interactive web demo with the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python web_demo/gradio_app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, the demo is hosted on &lt;code&gt;http://0.0.0.0:8080/&lt;/code&gt; and expects &lt;code&gt;edge_sam_3x.pth&lt;/code&gt; to be stored in the &lt;code&gt;weights/&lt;/code&gt; folder. You can change the default behavior by:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python web_demo/gradio_app.py --checkpoint [CHECKPOINT] --server-name [SERVER_NAME] --port [PORT]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Since EdgeSAM can run smoothly on a mobile phone, it&#39;s fine if you don&#39;t have a GPU.&lt;/p&gt; &#xA;&lt;p&gt;We&#39;ve deployed the same web demo in the Hugging Face Space [&lt;a href=&#34;https://huggingface.co/spaces/chongzhou/EdgeSAM&#34;&gt;link&lt;/a&gt;]. &lt;del&gt; However, since it uses the CPU as the backend and is shared by all users, the experience might not be as good as a local deployment. &lt;/del&gt; Really appreciate the Hugging Face team for supporting us with the GPU!&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Speed up the web demo with ONNX backend&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install the onnxruntime with &lt;code&gt;pip install onnxruntime&lt;/code&gt; if your machine doesn&#39;t have a GPU or &lt;code&gt;pip install onnxruntime-gpu&lt;/code&gt; if it does (but don&#39;t install both of them). Our implementation is tested under version &lt;code&gt;1.16.3&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Download the ONNX models to the &lt;code&gt;weights/&lt;/code&gt; folder:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;wget -P weights/ https://huggingface.co/spaces/chongzhou/EdgeSAM/resolve/main/weights/edge_sam_3x_encoder.onnx&#xA;wget -P weights/ https://huggingface.co/spaces/chongzhou/EdgeSAM/resolve/main/weights/edge_sam_3x_decoder.onnx&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Start the demo:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python web_demo/gradio_app.py --enable-onnx&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Navigate to &lt;a href=&#34;http://0.0.0.0:8080&#34;&gt;http://0.0.0.0:8080&lt;/a&gt; in your browser.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;CoreML / ONNX Export &lt;a name=&#34;export&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;CoreML&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We provide a script that can export a trained EdgeSAM PyTorch model to two CoreML model packages, one for the encoder and another for the decoder. You can also download the exported CoreML models at &lt;a href=&#34;https://raw.githubusercontent.com/chongzhou96/EdgeSAM/master/#checkpoints&#34;&gt;Checkpoints&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For encoder:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/export_coreml_model.py [CHECKPOINT]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For decoder:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/export_coreml_model.py [CHECKPOINT] --decoder --use-stability-score&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Since EdgeSAM doesn&#39;t perform knowledge distillation on the IoU token of the original SAM, its IoU predictions might not be reliable. Therefore, we use the stability score for mask selection instead. You can stick to the IoU predictions by removing &lt;code&gt;--use-stability-score&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The following shows the performance reports of the EdgeSAM CoreML models measured by Xcode on an iPhone 14 (left: encoder, right: decoder):&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/chongzhou96/EdgeSAM/assets/15973859/8df54f76-24c9-4ad2-af6d-086b971d073b&#34; alt=&#34;xcode&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; &lt;strong&gt; Known issues and model descriptions &lt;/strong&gt; &lt;/summary&gt; &#xA; &lt;p&gt;As of &lt;code&gt;coremltools==7.1&lt;/code&gt;, you may encounter the assertion error during the export, e.g., &lt;code&gt;assert len(inputs) &amp;lt;= 3 or inputs[3] is None&lt;/code&gt;. One workaround is to comment out this assertion following the traceback path, e.g., &lt;code&gt;/opt/anaconda3/envs/EdgeSAM/lib/python3.8/site-packages/coremltools/converters/mil/frontend/torch/ops.py line 1573&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;p&gt;Since CoreML doesn&#39;t support interpolation with dynamic target sizes, the converted CoreML models do not contain the pre-processing, i.e., resize-norm-pad, and the post-processing, i.e., resize back to the original size.&lt;/p&gt; &#xA; &lt;p&gt;The encoder takes a &lt;code&gt;1x3x1024x1024&lt;/code&gt; image as the input and outputs a &lt;code&gt;1x256x64x64&lt;/code&gt; image embedding. The decoder then takes the image embedding together with point coordinates and point labels as the input. The point coordinates follow the &lt;code&gt;(height, width)&lt;/code&gt; format with the top-left corner as the &lt;code&gt;(0, 0)&lt;/code&gt;. The choices of point labels are &lt;code&gt;0: negative point&lt;/code&gt;, &lt;code&gt;1: positive point&lt;/code&gt;, &lt;code&gt;2: top-left corner of box&lt;/code&gt;, and &lt;code&gt;3: bottom-right corner of box&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;&lt;strong&gt;ONNX&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Similar to the CoreML export, you can use the following commands to export the encoder and the decoder to ONNX models respectively:&lt;/p&gt; &#xA;&lt;p&gt;For encoder:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/export_onnx_model.py [CHECKPOINT]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For decoder:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/export_onnx_model.py [CHECKPOINT] --decoder --use-stability-score&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Checkpoints &lt;a name=&#34;checkpoints&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Please download the checkpoints of EdgeSAM from its Hugging Face Space (all the EdgeSAM variants only differ in the number of training images):&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;COCO mAP&lt;/th&gt; &#xA;   &lt;th&gt;PyTorch&lt;/th&gt; &#xA;   &lt;th&gt;CoreML&lt;/th&gt; &#xA;   &lt;th&gt;ONNX&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SAM&lt;/td&gt; &#xA;   &lt;td&gt;46.1&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EdgeSAM&lt;/td&gt; &#xA;   &lt;td&gt;42.1&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/chongzhou/EdgeSAM/resolve/main/weights/edge_sam.pth&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;[&lt;a href=&#34;https://huggingface.co/spaces/chongzhou/EdgeSAM/resolve/main/weights/edge_sam_encoder.mlpackage.zip&#34;&gt;Encoder&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/spaces/chongzhou/EdgeSAM/resolve/main/weights/edge_sam_decoder.mlpackage.zip&#34;&gt;Decoder&lt;/a&gt;]&lt;/td&gt; &#xA;   &lt;td&gt;[&lt;a href=&#34;https://huggingface.co/spaces/chongzhou/EdgeSAM/resolve/main/weights/edge_sam_encoder.onnx&#34;&gt;Encoder&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/spaces/chongzhou/EdgeSAM/resolve/main/weights/edge_sam_decoder.onnx&#34;&gt;Decoder&lt;/a&gt;]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EdgeSAM-3x&lt;/td&gt; &#xA;   &lt;td&gt;42.7&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/chongzhou/EdgeSAM/resolve/main/weights/edge_sam_3x.pth&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;[&lt;a href=&#34;https://huggingface.co/spaces/chongzhou/EdgeSAM/resolve/main/weights/edge_sam_3x_encoder.mlpackage.zip&#34;&gt;Encoder&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/spaces/chongzhou/EdgeSAM/resolve/main/weights/edge_sam_3x_decoder.mlpackage.zip&#34;&gt;Decoder&lt;/a&gt;]&lt;/td&gt; &#xA;   &lt;td&gt;[&lt;a href=&#34;https://huggingface.co/spaces/chongzhou/EdgeSAM/resolve/main/weights/edge_sam_3x_encoder.onnx&#34;&gt;Encoder&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/spaces/chongzhou/EdgeSAM/resolve/main/weights/edge_sam_3x_decoder.onnx&#34;&gt;Decoder&lt;/a&gt;]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EdgeSAM-10x&lt;/td&gt; &#xA;   &lt;td&gt;43&lt;/td&gt; &#xA;   &lt;td&gt;TBA&lt;/td&gt; &#xA;   &lt;td&gt;TBA&lt;/td&gt; &#xA;   &lt;td&gt;TBA&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Note: You need to unzip the CoreML model packages before usage.&lt;/p&gt; &#xA;&lt;h2&gt;iOS App &lt;a name=&#34;ios&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;We are planning to release the iOS app that we used in the live demo to the App Store. Please stay tuned!&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements &lt;a name=&#34;acknowledgement&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;This study is supported under the RIE2020 Industry Alignment Fund Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s). We are grateful to &lt;a href=&#34;https://www.linkedin.com/in/hansoong-choong-0493a5155/&#34;&gt;Han Soong Chong&lt;/a&gt; for his effort in the demonstration application.&lt;/p&gt; &#xA;&lt;p&gt;We appreciate the following projects, which enable EdgeSAM: &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;SAM&lt;/a&gt;, &lt;a href=&#34;https://github.com/ChaoningZhang/MobileSAM&#34;&gt;MobileSAM&lt;/a&gt;, &lt;a href=&#34;https://github.com/CASIA-IVA-Lab/FastSAM&#34;&gt;FastSAM&lt;/a&gt;, &lt;a href=&#34;https://github.com/microsoft/Cream&#34;&gt;TinyViT&lt;/a&gt;, and &lt;a href=&#34;https://github.com/THU-MIG/RepViT&#34;&gt;RepViT&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citation &lt;a name=&#34;cite&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{zhou2023edgesam,&#xA;  title={EdgeSAM: Prompt-In-the-Loop Distillation for On-Device Deployment of SAM},&#xA;  author={Zhou, Chong and Li, Xiangtai and Loy, Chen Change and Dai, Bo},&#xA;  journal={arXiv preprint arXiv:2312.06660},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License &lt;a name=&#34;license&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under &lt;a rel=&#34;license&#34; href=&#34;https://github.com/chongzhou96/EdgeSAM/raw/master/LICENSE&#34;&gt;NTU S-Lab License 1.0&lt;/a&gt;. Redistribution and use should follow this license.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Azure/azure-search-vector-samples</title>
    <updated>2023-12-16T01:32:01Z</updated>
    <id>tag:github.com,2023-12-16:/Azure/azure-search-vector-samples</id>
    <link href="https://github.com/Azure/azure-search-vector-samples" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A repository of code samples for Vector search capabilities in Azure AI Search.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Vector search samples - Azure AI Search&lt;/h1&gt; &#xA;&lt;p&gt;This repository has code samples for &lt;a href=&#34;https://learn.microsoft.com/azure/search/vector-search-overview&#34;&gt;vector support&lt;/a&gt; in Azure AI Search. Vector search is generally available, but it also has capabilities still in preview, under &lt;a href=&#34;https://azure.microsoft.com/support/legal/preview-supplemental-terms/&#34;&gt;Supplemental Terms of Use&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Vector indexing and queries are generally available.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://learn.microsoft.com/azure/search/vector-search-integrated-vectorization&#34;&gt;Integrated data chunking and vectorization&lt;/a&gt;, which takes a dependency on indexers and skillsets, is in preview.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Content&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Sample&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Status&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Azure/azure-search-vector-samples/main/demo-dotnet/DotNetVectorDemo/readme.md&#34;&gt;DotNetVectorDemo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A .NET console app that calls Azure OpenAI to vectorize data. It then calls Azure AI Search to create, load, and query vector data.&lt;/td&gt; &#xA;   &lt;td&gt;Generally available (GA)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Azure/azure-search-vector-samples/main/demo-dotnet/DotNetIntegratedVectorizationDemo/readme.md&#34;&gt;DotNetIntegratedVectorizationDemo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A .NET console app that calls Azure AI Search to create an index, indexer, data source, and skillset. An Azure Storage account provides the data. Azure OpenAI is called by the skillset during indexing, and again during query execution to vectorize text queries.&lt;/td&gt; &#xA;   &lt;td&gt;Public preview&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Azure/azure-search-vector-samples/main/demo-javascript/JavaScriptVectorDemo/readme.md&#34;&gt;JavaScriptVectorDemo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;There are three code samples. One is an end-to-end code sample that calls Azure OpenAI for embeddings and Azure AI Seach to create, load, and query an index that contains vectors. Another sample calls just Azure OpenAI and is used to generate embeddings for fields in an index. The last one also calls just Azure OpenAI and is used to generate an embedding for a vector query.&lt;/td&gt; &#xA;   &lt;td&gt;GA&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Azure/azure-search-vector-samples/main/demo-python/readme.md&#34;&gt;demo-python/*.ipynb&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A collection of notebooks that demonstrate aspects of vector search, including data chunking and vectorization of both text and image content.&lt;/td&gt; &#xA;   &lt;td&gt;GA and preview&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Azure/azure-search-vector-samples/main/postman-collection/README.md&#34;&gt;postman-collection&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Two separate Postman collections of REST API calls for generally available (2023-11-01) and preview (2023-10-01-preview) versions. GA version shows you how to create, load, and query vector and non-vector content in an index. Preview version demonstrates integrated data chunking and vectorization through indexers and skillsets. Use the &lt;a href=&#34;https://www.postman.com/downloads/&#34;&gt;Postman app&lt;/a&gt; for these samples.&lt;/td&gt; &#xA;   &lt;td&gt;GA and preview&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Related samples and tools&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Azure-Samples/chat-with-your-data-solution-accelerator&#34;&gt;chat-with-your-data-solution-accelerator&lt;/a&gt; A template that deploys multiple Azure resources for a custom chat-with-your-data solution. Use this accelerator to create a production-ready solution that implements coding best practices.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Azure-Samples/azure-search-openai-demo/tree/vectors&#34;&gt;Azure Search OpenAI Demo&lt;/a&gt; A sample app for the Retrieval-Augmented Generation pattern running in Azure, using Azure AI Search for retrieval and Azure OpenAI large language models to power ChatGPT-style and Q&amp;amp;A experiences. Use the &#34;vectors&#34; branch to leverage Vector retrieval.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Azure-Samples/azure-search-openai-demo-csharp/tree/feature/embeddingSearch&#34;&gt;Azure Search OpenAI Demo - C#&lt;/a&gt; A sample app for the Retrieval-Augmented Generation pattern running in Azure, using Azure AI Search for retrieval and Azure OpenAI large language models to power ChatGPT-style and Q&amp;amp;A experiences using C#.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ruoccofabrizio/azure-open-ai-embeddings-qna&#34;&gt;Azure OpenAI Embeddings QnA with Azure Search as a Vector Store&lt;/a&gt; (github.com) A simple web application for a OpenAI-enabled document search. This repo uses Azure OpenAI Service for creating embeddings vectors from documents. For answering the question of a user, using Azure AI Search for retrieval and Azure OpenAI large language models to power ChatGPT-style and Q&amp;amp;A experiences.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/chatgpt-retrieval-plugin/raw/main/README.md#azure-cognitive-search&#34;&gt;ChatGPT Retreival Plugin Azure Search Vector Database&lt;/a&gt; The ChatGPT Retrieval Plugin lets you easily find personal or work documents by asking questions in natural language. Azure AI Search now supported as an official vector database.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/farzad528/azure-search-vector-search-demo&#34;&gt;Azure Search Vector Search Demo Web App Template&lt;/a&gt; A Vector Search Demo React Web App Template using Azure OpenAI for Text Search and Cognitive Services Florence Vision API for Image Search.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Azure-Samples/azure-search-comparison-tool&#34;&gt;Azure Cognitive Search Comparison Tool&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://learn.microsoft.com/azure/search/&#34;&gt;Azure AI Search Documentation&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://learn.microsoft.com/azure/search/retrieval-augmented-generation-overview&#34;&gt;Retrieval Augmented Generation (RAG) in Azure AI Search&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://learn.microsoft.com/azure/search/vector-search-overview&#34;&gt;Vector search overview&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://learn.microsoft.com/azure/search/hybrid-search-overview&#34;&gt;Hybrid search overview&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://learn.microsoft.com/azure/search/vector-search-how-to-create-index&#34;&gt;Create a vector index&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://learn.microsoft.com/azure/search/vector-search-how-to-query&#34;&gt;Query a vector index&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://learn.microsoft.com/azure/search/vector-search-ranking&#34;&gt;Vector search algorithms&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://learn.microsoft.com/rest/api/searchservice/&#34;&gt;REST API reference&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://learn.microsoft.com/azure/cognitive-services/openai/&#34;&gt;Azure OpenAI Service Documentation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://learn.microsoft.com/azure/cognitive-services/computer-vision/&#34;&gt;Azure AI Vision Documentation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>