<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-30T01:37:43Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ChaoningZhang/MobileSAM</title>
    <updated>2023-06-30T01:37:43Z</updated>
    <id>tag:github.com,2023-06-30:/ChaoningZhang/MobileSAM</id>
    <link href="https://github.com/ChaoningZhang/MobileSAM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This is the offiicial code for Faster Segment Anything (MobileSAM) project that makes SAM lightweight&lt;/p&gt;&lt;hr&gt;&lt;p float=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ChaoningZhang/MobileSAM/master/assets/logo2.png?raw=true&#34; width=&#34;99.1%&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Faster Segment Anything (MobileSAM)&lt;/h1&gt; &#xA;&lt;p&gt;&lt;span&gt;üìå&lt;/span&gt; MobileSAM paper is available at &lt;a href=&#34;https://www.researchgate.net/publication/371851844_Faster_Segment_Anything_Towards_Lightweight_SAM_for_Mobile_Applications&#34;&gt;ResearchGate&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/pdf/2306.14289.pdf&#34;&gt;arXiv&lt;/a&gt;. The latest version will first appear on &lt;a href=&#34;https://arxiv.org/pdf/2306.14289.pdf&#34;&gt;ResearchGate&lt;/a&gt;, since it takes time for arXiv to update the content.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;üìå&lt;/span&gt; A &lt;strong&gt;demo of MobileSAM&lt;/strong&gt; running on &lt;strong&gt;CPU&lt;/strong&gt; is open at &lt;a href=&#34;https://huggingface.co/spaces/dhkim2810/MobileSAM&#34;&gt;demo link&lt;/a&gt; (A new version with other features will come soon, stay tuned!). On our own Mac i5 CPU, it takes around 3s. On the hugging face demo, the interface and inferior CPUs make it slower but still works fine.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;üçá&lt;/span&gt; Media coverage and Projects that adapt from SAM to MobileSAM (Updates)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;2023/06/30&lt;/strong&gt;: MobileSAM has been featured by &lt;a href=&#34;https://twitter.com/_akhaliq?lang=en&#34;&gt;AK&lt;/a&gt; for the second time, see the link &lt;a href=&#34;https://twitter.com/_akhaliq/status/1674410573075718145&#34;&gt;AK&#39;s MobileSAM tweet&lt;/a&gt;. Welcome to retweet.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;2023/06/29&lt;/strong&gt;: &lt;a href=&#34;https://github.com/vietanhdev/anylabeling&#34;&gt;AnyLabeling&lt;/a&gt; supports MobileSAM for auto-labeling. Thanks for their effort.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;2023/06/29&lt;/strong&gt;: &lt;a href=&#34;https://github.com/wangsssky/SonarSAM&#34;&gt;SonarSAM&lt;/a&gt; supports MobileSAM for Image encoder full-finetuing. Thanks for their effort.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;2023/06/29&lt;/strong&gt;: &lt;a href=&#34;https://github.com/continue-revolution/sd-webui-segment-anything&#34;&gt;Stable Diffusion WebUIv&lt;/a&gt; supports MobileSAM. Thanks for their effort.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;2023/06/28&lt;/strong&gt;: &lt;a href=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything&#34;&gt;Grounding-SAM&lt;/a&gt; supports MobileSAM with &lt;a href=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything/tree/main/EfficientSAM&#34;&gt;Grounded-MobileSAM&lt;/a&gt;. Thanks for their effort.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;2023/06/27&lt;/strong&gt;: MobileSAM has been featured by &lt;a href=&#34;https://twitter.com/_akhaliq?lang=en&#34;&gt;AK&lt;/a&gt;, see the link &lt;a href=&#34;https://twitter.com/_akhaliq/status/1673585099097636864&#34;&gt;AK&#39;s MobileSAM tweet&lt;/a&gt;. Welcome to retweet. &lt;img src=&#34;https://raw.githubusercontent.com/ChaoningZhang/MobileSAM/master/assets/model_diagram.jpg?raw=true&#34; alt=&#34;MobileSAM&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;span&gt;‚≠ê&lt;/span&gt; &lt;strong&gt;How is MobileSAM trained?&lt;/strong&gt; MobileSAM is trained on a single GPU with 100k datasets (1% of the original images) for less than a day. The training code will be available soon.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;‚≠ê&lt;/span&gt; &lt;strong&gt;How to Adapt from SAM to MobileSAM?&lt;/strong&gt; Since MobileSAM keeps exactly the same pipeline as the original SAM, we inherit pre-processing, post-processing, and all other interfaces from the original SAM. Therefore, by assuming everything is exactly the same except for a smaller image encoder, those who use the original SAM for their projects can &lt;strong&gt;adapt to MobileSAM with almost zero effort&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;‚≠ê&lt;/span&gt; &lt;strong&gt;MobileSAM performs on par with the original SAM (at least visually)&lt;/strong&gt; and keeps exactly the same pipeline as the original SAM except for a change on the image encoder. Specifically, we replace the original heavyweight ViT-H encoder (632M) with a much smaller Tiny-ViT (5M). On a single GPU, MobileSAM runs around 12ms per image: 8ms on the image encoder and 4ms on the mask decoder.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;The comparison of ViT-based image encoder is summarzed as follows:&lt;/p&gt; &#xA;  &lt;table&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;Image Encoder&lt;/th&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;Original SAM&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;MobileSAM&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Paramters&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;611M&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;5M&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Speed&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;452ms&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;8ms&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Original SAM and MobileSAM have exactly the same prompt-guided mask decoder:&lt;/p&gt; &#xA;  &lt;table&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;Mask Decoder&lt;/th&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;Original SAM&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;MobileSAM&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Paramters&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;3.876M&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;3.876M&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Speed&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;4ms&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;4ms&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The comparison of the whole pipeline is summarized as follows:&lt;/p&gt; &#xA;  &lt;table&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;Whole Pipeline (Enc+Dec)&lt;/th&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;Original SAM&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;MobileSAM&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Paramters&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;615M&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;9.66M&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Speed&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;456ms&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;12ms&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;span&gt;‚≠ê&lt;/span&gt; &lt;strong&gt;Original SAM and MobileSAM with a (single) point as the prompt.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p float=&#34;left&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ChaoningZhang/MobileSAM/master/assets/mask_point.jpg?raw=true&#34; width=&#34;99.1%&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;‚≠ê&lt;/span&gt; &lt;strong&gt;Original SAM and MobileSAM with a box as the prompt.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p float=&#34;left&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ChaoningZhang/MobileSAM/master/assets/mask_box.jpg?raw=true&#34; width=&#34;99.1%&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;üí™&lt;/span&gt; &lt;strong&gt;Is MobileSAM faster and smaller than FastSAM? Yes!&lt;/strong&gt; MobileSAM is around 7 times smaller and around 5 times faster than the concurrent FastSAM. The comparison of the whole pipeline is summarzed as follows:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Whole Pipeline (Enc+Dec)&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;FastSAM&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;MobileSAM&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Paramters&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;68M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;9.66M&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Speed&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;64ms&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12ms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;span&gt;üí™&lt;/span&gt; &lt;strong&gt;Does MobileSAM aign better with the original SAM than FastSAM? Yes!&lt;/strong&gt; FastSAM is suggested to work with multiple points, thus we compare the mIoU with two prompt points (with different pixel distances) and show the resutls as follows. Higher mIoU indicates higher alignment.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;mIoU&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;FastSAM&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;MobileSAM&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;100&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;0.27&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.73&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;200&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;0.33&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.71&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;300&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;0.37&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.74&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;400&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;0.41&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.73&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;500&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;0.41&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.73&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;The code requires &lt;code&gt;python&amp;gt;=3.8&lt;/code&gt;, as well as &lt;code&gt;pytorch&amp;gt;=1.7&lt;/code&gt; and &lt;code&gt;torchvision&amp;gt;=0.8&lt;/code&gt;. Please follow the instructions &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;here&lt;/a&gt; to install both PyTorch and TorchVision dependencies. Installing both PyTorch and TorchVision with CUDA support is strongly recommended.&lt;/p&gt; &#xA;&lt;p&gt;Install Mobile Segment Anything:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install git+https://github.com/ChaoningZhang/MobileSAM.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or clone the repository locally and install with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone git@github.com:ChaoningZhang/MobileSAM.git&#xA;cd MobileSAM; pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;GettingStarted&#34;&gt;&lt;/a&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;The MobileSAM can be loaded in the following ways:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;from mobile_encoder.setup_mobile_sam import setup_model&#xA;checkpoint = torch.load(&#39;../weights/mobile_sam.pt&#39;)&#xA;mobile_sam = setup_model()&#xA;mobile_sam.load_state_dict(checkpoint,strict=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then the model can be easily used in just a few lines to get masks from a given prompt:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;from segment_anything import SamPredictor&#xA;device = &#34;cuda&#34;&#xA;mobile_sam.to(device=device)&#xA;mobile_sam.eval()&#xA;predictor = SamPredictor(mobile_sam)&#xA;predictor.set_image(&amp;lt;your_image&amp;gt;)&#xA;masks, _, _ = predictor.predict(&amp;lt;input_prompts&amp;gt;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or generate masks for an entire image:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;from segment_anything import SamAutomaticMaskGenerator&#xA;&#xA;mask_generator = SamAutomaticMaskGenerator(mobile_sam)&#xA;masks = mask_generator.generate(&amp;lt;your_image&amp;gt;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;BibTex of our MobileSAM&lt;/h2&gt; &#xA;&lt;p&gt;If you use MobileSAM in your research, please use the following BibTeX entry. &lt;span&gt;üì£&lt;/span&gt; Thank you!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{mobile_sam,&#xA;  title={Faster Segment Anything: Towards Lightweight SAM for Mobile Applications},&#xA;  author={Zhang, Chaoning and Han, Dongshen and Qiao, Yu and Kim, Jung Uk and Bae, Sung Ho and Lee, Seungkyu and Hong, Choong Seon},&#xA;  journal={arXiv preprint arXiv:2306.14289},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;SAM&lt;/a&gt; (Segment Anything) [&lt;b&gt;bib&lt;/b&gt;] &lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{kirillov2023segany,&#xA;  title={Segment Anything}, &#xA;  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Doll{\&#39;a}r, Piotr and Girshick, Ross},&#xA;  journal={arXiv:2304.02643},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; &lt;a href=&#34;https://github.com/microsoft/Cream/tree/main/TinyViT&#34;&gt;TinyViT&lt;/a&gt; (TinyViT: Fast Pretraining Distillation for Small Vision Transformers) [&lt;b&gt;bib&lt;/b&gt;] &lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{tiny_vit,&#xA;  title={TinyViT: Fast Pretraining Distillation for Small Vision Transformers},&#xA;  author={Wu, Kan and Zhang, Jinnian and Peng, Houwen and Liu, Mengchen and Xiao, Bin and Fu, Jianlong and Yuan, Lu},&#xA;  booktitle={European conference on computer vision (ECCV)},&#xA;  year={2022}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt;</summary>
  </entry>
</feed>