<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-09-08T01:39:00Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>MagnusPetersen/EvoGen-Prompt-Evolution</title>
    <updated>2022-09-08T01:39:00Z</updated>
    <id>tag:github.com,2022-09-08:/MagnusPetersen/EvoGen-Prompt-Evolution</id>
    <link href="https://github.com/MagnusPetersen/EvoGen-Prompt-Evolution" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;EvoGen-Prompt-Evolution&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MagnusPetersen/EvoGen-Prompt-Evolution/main/Media/banner.png&#34; alt=&#34;Evolved Prompt Output&#34; title=&#34;Evolved Prompt Output&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Description &amp;amp; Method&lt;/h2&gt; &#xA;&lt;p&gt;EvoGen is an evolutionary algorithm that optimizes prompts for text-to-image models for aesthetics, as assessed by @rivershavewings aesthetics model.&lt;/p&gt; &#xA;&lt;h3&gt;Notebook:&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/MagnusPetersen/EvoGen-Prompt-Evolution/blob/main/Notebook/EvoGen.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The algorithm is composed of three different AI models, stable diffusion as the generative model, using the &lt;a href=&#34;https://discord.gg/upmXXsrwZc&#34;&gt;deforum&lt;/a&gt; generation code, which generates images from prompts, @rivershavewings aesthetics model that rates the resulting images and the evolutionary algorithm that created the prompts at random and then optimizes the prompts based on the rating by using the highest rated prompts to generate the next generation of prompts.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MagnusPetersen/EvoGen-Prompt-Evolution/main/Media/model_schema.png&#34; alt=&#34;Model Schema&#34; title=&#34;[Model Schema]&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Setup&lt;/h3&gt; &#xA;&lt;p&gt;To run this notebook or any Stablediffusion notebook you need to get the ckpt file of the model and put it on your Google Drive. It can be downloaded from &lt;a href=&#34;https://huggingface.co/CompVis/stable-diffusion&#34;&gt;HuggingFace&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Hyperparameters&lt;/h3&gt; &#xA;&lt;h4&gt;Evolutionary Algorithm Settings&lt;/h4&gt; &#xA;&lt;p&gt;General population settings, such as how many generations the algorithm runs for, how many prompts there are in each generation, and the word length range of the prompts.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;generations:&lt;/strong&gt; How many generations the algorithm runs for&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;population_count:&lt;/strong&gt; How many prompts there are in each generation/how large each generation is&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;prompt_length_max:&lt;/strong&gt; Maximum prompt length when the population is initialized, during evolution this limit can be surpassed.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;prompt_length_min:&lt;/strong&gt; Minimum prompt length when the population is initialized, during evolution this limit can be surpassed.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;artist_prop:&lt;/strong&gt; Probability to sample from the artist list when adding or mutating a word.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;genre_prop:&lt;/strong&gt; Probability to sample from the genre list when adding or mutating a word.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;custom_prop:&lt;/strong&gt; Probability to sample from the custom user-defined list when adding or mutating a word.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The difference between the sum of the three custom lists and 1 is the probability to sample from the English dictionary word list.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;delete_prop:&lt;/strong&gt; probability to delete a word per word in each prompt after each generation&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;add_prop:&lt;/strong&gt; probability to add a new word per word in each prompt after each generation from the word lists&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;mutate_prop:&lt;/strong&gt; probability to swap out a word per word in each prompt after each generation for a new word from the word lists&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;shuffle_prop:&lt;/strong&gt; The probability to shuffle the order of words per prompt&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;cross_prop:&lt;/strong&gt; The cross-over probability is the probability of the parents of the next generation swapping prompt parts per parent pair&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;k:&lt;/strong&gt; K denotes the rounds in the tournament selection process. A higher K value means fewer parents generate the next generation, this means a higher score increase but less diversity in the prompts.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;cutoff:&lt;/strong&gt; Cutoff aesthetics score to save the image and prompt&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Image Generation Settings&lt;/h4&gt; &#xA;&lt;p&gt;Image generation settings have an impact on the speed and behavior of the evolutionary algorithm, the euler_a sampler in conjunction with low step size and resolution is advisable for quick prompt evolution. Good prompts can then later be used to generate higher-quality images. The parameter n_samples determines how many images are generated per prompt, the scores are then averaged. A higher n_samples slows down the generation but stabilizes the optimization.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;n_samples:&lt;/strong&gt; The number of samples/seeds that will be evaluated per prompt in each generation. A higher n_samples slows down the generation but stabilizes the optimization.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;Stable Diffusion&lt;/a&gt; by Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer and the &lt;a href=&#34;https://stability.ai/&#34;&gt;Stability.ai&lt;/a&gt; Team. &lt;a href=&#34;https://github.com/crowsonkb/k-diffusion&#34;&gt;K Diffusion&lt;/a&gt; by &lt;a href=&#34;https://twitter.com/RiversHaveWings&#34;&gt;Katherine Crowson&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The aesthetics model that is an integral part of this method was made by &lt;a href=&#34;https://twitter.com/RiversHaveWings&#34;&gt;Katherine Crowson&lt;/a&gt; and can be found on her &lt;a href=&#34;https://github.com/crowsonkb/simulacra-aesthetic-models&#34;&gt;Github account&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The baseline of the notebook, setup, description, and image generation, is based on the &lt;a href=&#34;https://discord.gg/upmXXsrwZc&#34;&gt;deforum&lt;/a&gt; notebook.&lt;/p&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;h3&gt;1. trireme Fantasy Art nondefiling rhinopharyngitis canaliculization cricotracheotomy conure atheology beret Aestheticism Vicente Juan Masip Ashley Bickerton aplustria pelodytoid&lt;/h3&gt; &#xA;&lt;p float=&#34;middle&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MagnusPetersen/EvoGen-Prompt-Evolution/main/Media/1.png&#34; width=&#34;204&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MagnusPetersen/EvoGen-Prompt-Evolution/main/Media/2.png&#34; width=&#34;204&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MagnusPetersen/EvoGen-Prompt-Evolution/main/Media/3.png&#34; width=&#34;204&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MagnusPetersen/EvoGen-Prompt-Evolution/main/Media/4.png&#34; width=&#34;204&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;2. rimal disputants nugget Antoine Blanchard chemosynthesis mentocondylial calculatory supertragical Magic Realism&lt;/h3&gt; &#xA;&lt;p float=&#34;middle&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MagnusPetersen/EvoGen-Prompt-Evolution/main/Media/5.png&#34; width=&#34;204&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MagnusPetersen/EvoGen-Prompt-Evolution/main/Media/6.png&#34; width=&#34;204&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MagnusPetersen/EvoGen-Prompt-Evolution/main/Media/7.png&#34; width=&#34;204&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MagnusPetersen/EvoGen-Prompt-Evolution/main/Media/8.png&#34; width=&#34;204&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;3. harmonics omegoid Peter Blake Vladimir Tretchikoff&lt;/h3&gt; &#xA;&lt;p float=&#34;middle&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MagnusPetersen/EvoGen-Prompt-Evolution/main/Media/9.png&#34; width=&#34;204&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MagnusPetersen/EvoGen-Prompt-Evolution/main/Media/10.png&#34; width=&#34;204&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MagnusPetersen/EvoGen-Prompt-Evolution/main/Media/11.png&#34; width=&#34;204&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MagnusPetersen/EvoGen-Prompt-Evolution/main/Media/12.png&#34; width=&#34;204&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;To Do&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Hyperparameter Tuning:&lt;/strong&gt; The hyperparameters are barely tuned and there is a lot of performance to be had when it comes to aesthetics optimization, prompt diversity, and optimization speed.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Evolutionary Algorithm Tuning:&lt;/strong&gt; The algorithm is very basic and I have not read much on evolutionary algorithms. Here there is still a lot of room for improvement. Especially the crossover stage of the algorithm might be suboptimal the way it is done now for text.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;General Features:&lt;/strong&gt; There are some features I would like to add, especially having a &#34;base&#34; prompt that is in each population member so that you can take your favorite prompt and build on it.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>yandexdataschool/nlp_course</title>
    <updated>2022-09-08T01:39:00Z</updated>
    <id>tag:github.com,2022-09-08:/yandexdataschool/nlp_course</id>
    <link href="https://github.com/yandexdataschool/nlp_course" rel="alternate"></link>
    <summary type="html">&lt;p&gt;YSDA course in Natural Language Processing&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;YSDA Natural Language Processing course&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This is the 2021 version. For previous year&#39; course materials, go to &lt;a href=&#34;https://github.com/yandexdataschool/nlp_course/tree/2020&#34;&gt;this branch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Lecture and seminar materials for each week are in ./week* folders, see README.md for materials and instructions&lt;/li&gt; &#xA; &lt;li&gt;YSDA homework deadlines will be listed in Anytask (&lt;a href=&#34;https://github.com/yandexdataschool/nlp_course/wiki/Homeworks-and-grading&#34;&gt;read more&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Any technical issues, ideas, bugs in course materials, contribution ideas - add an &lt;a href=&#34;https://github.com/yandexdataschool/nlp_course/issues&#34;&gt;issue&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Installing libraries and troubleshooting: &lt;a href=&#34;https://github.com/yandexdataschool/nlp_course/issues/1&#34;&gt;this thread&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;&lt;a href=&#34;https://lena-voita.github.io/nlp_course.html&#34;&gt;NLP Course For You&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;h1&gt;Syllabus&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/yandexdataschool/nlp_course/2021/week01_embeddings&#34;&gt;&lt;strong&gt;week01&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;Word Embeddings&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Lecture: Word embeddings. Distributional semantics. Count-based (pre-neural) methods. Word2Vec: learn vectors. GloVe: count, then learn. Evaluation: intrinsic vs extrinsic. Analysis and Interpretability. &lt;a href=&#34;https://lena-voita.github.io/nlp_course.html#preview_word_emb&#34;&gt;Interactive lecture materials and more.&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Seminar: Playing with word and sentence embeddings&lt;/li&gt; &#xA;   &lt;li&gt;Homework: Embedding-based machine translation system&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/yandexdataschool/nlp_course/2021/week02_classification&#34;&gt;&lt;strong&gt;week02&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;Text Classification&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Lecture: Text classification: introduction and datasets. General framework: feature extractor + classifier. Classical approaches: Naive Bayes, MaxEnt (Logistic Regression), SVM. Neural Networks: General View, Convolutional Models, Recurrent Models. Practical Tips: Data Augmentation. Analysis and Interpretability. &lt;a href=&#34;https://lena-voita.github.io/nlp_course.html#preview_text_clf&#34;&gt;Interactive lecture materials and more.&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Seminar: Text classification with convolutional NNs.&lt;/li&gt; &#xA;   &lt;li&gt;Homework: Statistical &amp;amp; neural text classification.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/yandexdataschool/nlp_course/2021/week03_lm&#34;&gt;&lt;strong&gt;week03&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;Language Modeling&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Lecture: Language Modeling: what does it mean? Left-to-right framework. N-gram language models. Neural Language Models: General View, Recurrent Models, Convolutional Models. Evaluation. Practical Tips: Weight Tying. Analysis and Interpretability. &lt;a href=&#34;https://lena-voita.github.io/nlp_course.html#preview_lang_models&#34;&gt;Interactive lecture materials and more.&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Seminar: Build a N-gram language model from scratch&lt;/li&gt; &#xA;   &lt;li&gt;Homework: Neural LMs &amp;amp; smoothing in count-based models.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/yandexdataschool/nlp_course/2021/week04_seq2seq&#34;&gt;&lt;strong&gt;week04&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;Seq2seq and Attention&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Lecture: Seq2seq Basics: Encoder-Decoder framework, Training, Simple Models, Inference (e.g., beam search). Attention: general, score functions, models. Transformer: self-attention, masked self-attention, multi-head attention; model architecture. Subword Segmentation (BPE). Analysis and Interpretability: functions of attention heads; probing for linguistic structure. &lt;a href=&#34;https://lena-voita.github.io/nlp_course.html#preview_seq2seq_attn&#34;&gt;Interactive lecture materials and more.&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Seminar: Basic sequence to sequence model&lt;/li&gt; &#xA;   &lt;li&gt;Homework: Machine translation with attention&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/yandexdataschool/nlp_course/2021/week05_transfer&#34;&gt;&lt;strong&gt;week05&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;Transfer Learning&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Lecture: What is Transfer Learning? Great idea 1: From Words to Words-in-Context (CoVe, ELMo). Great idea 2: From Replacing Embeddings to Replacing Models (GPT, BERT). (A Bit of) Adaptors. Analysis and Interpretability. &lt;a href=&#34;https://lena-voita.github.io/nlp_course.html#preview_transfer&#34;&gt;Interactive lecture materials and more.&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/yandexdataschool/nlp_course/2021/week06_da&#34;&gt;&lt;strong&gt;week06&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;Domain Adaptation&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Lecture: General theory. Instance weighting. Proxy-labels methods. Feature matching methods. Distillation-like methods.&lt;/li&gt; &#xA;   &lt;li&gt;Seminar+Homework: BERT-based NER domain adaptation&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/yandexdataschool/nlp_course/2021/week07_compression&#34;&gt;&lt;strong&gt;week07&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;Model compression and acceleration&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/yandexdataschool/nlp_course/2021/week08_em&#34;&gt;&lt;strong&gt;week08&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;Probabilistic inference, generative models and hidden variables&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/yandexdataschool/nlp_course/2021/week09_mt&#34;&gt;&lt;strong&gt;week09&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;Machine translation&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/yandexdataschool/nlp_course/2021/week10_relation_extraction&#34;&gt;&lt;strong&gt;week10&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;Relation extraction&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/yandexdataschool/nlp_course/2021/week11_summarization&#34;&gt;&lt;strong&gt;week11&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;Summarization&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/yandexdataschool/nlp_course/2021/week12_style&#34;&gt;&lt;strong&gt;week12&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;Style Transfer&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/yandexdataschool/nlp_course/2021/week13_conversation&#34;&gt;&lt;strong&gt;week13&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;Dialogue systems&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/yandexdataschool/nlp_course/2021/week14_ai_ml_generated_art&#34;&gt;&lt;strong&gt;week14&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;AI &amp;amp; ML generated art&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Contributors &amp;amp; course staff&lt;/h1&gt; &#xA;&lt;p&gt;Course materials and teaching performed by&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lena-voita.github.io&#34;&gt;Elena Voita&lt;/a&gt; - course admin, lectures, seminars, homeworks&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kovarsky&#34;&gt;Boris Kovarsky&lt;/a&gt; - lectures, seminars, homeworks&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/drt7&#34;&gt;David Talbot&lt;/a&gt; - lectures, seminars, homeworks&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/justheuristic&#34;&gt;Just Heuristic&lt;/a&gt; - lectures, seminars, homeworks&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/altsoph&#34;&gt;Alexey Tikhonov @altsoph&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://michschli.github.io&#34;&gt;Michael Sejr Schlichtkrull&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/abrazinskas/&#34;&gt;Arthur Bražinskas&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/kr0niker&#34;&gt;Ivan Yamshchikov&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/nzinov&#34;&gt;Nikolay Zinov&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Authors and contributors of previous years&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/esgv&#34;&gt;Sergey Gubanov&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://research.yandex.com/people/301832&#34;&gt;Vyacheslav Alipov&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Vladimir Kirichenko&lt;/li&gt; &#xA; &lt;li&gt;Andrey Zhigunov&lt;/li&gt; &#xA; &lt;li&gt;Pavel Bogomolov&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>girafe-ai/ml-course</title>
    <updated>2022-09-08T01:39:00Z</updated>
    <id>tag:github.com,2022-09-08:/girafe-ai/ml-course</id>
    <link href="https://github.com/girafe-ai/ml-course" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Open Machine Learning course&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;em&gt;Warning, repository has been renamed to represent its current status.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Open Machine Learning course&lt;/h1&gt; &#xA;&lt;h2&gt;Basic (first semester) and advanced (second semester) tracks.&lt;/h2&gt; &#xA;&lt;p&gt;This course aims to introduce students to modern state of Machine Learning and Artificial Intelligence. It is designed to take one full year - approximately 2 * 15 lectures and seminars.&lt;/p&gt; &#xA;&lt;p&gt;All learning materials are available here, full list of topics considered in the course are listed in &lt;code&gt;program_*.pdf&lt;/code&gt; files.&lt;/p&gt; &#xA;&lt;h2&gt;Repository structure&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;on &lt;code&gt;master&lt;/code&gt; branch previous term materials are stored to give a quick and comprehensive overview&lt;/li&gt; &#xA; &lt;li&gt;on &lt;code&gt;*_basic&lt;/code&gt; and &lt;code&gt;*_advanced&lt;/code&gt; branches materials for current launches are being published&lt;/li&gt; &#xA; &lt;li&gt;tags (e.g. &lt;code&gt;2021_spring&lt;/code&gt;) contain previous launches materials for convenience&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Video lectures&lt;/h2&gt; &#xA;&lt;p&gt;All lecture materials are currently in Russian language&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Track&lt;/th&gt; &#xA;   &lt;th&gt;Launch&lt;/th&gt; &#xA;   &lt;th&gt;Lectures&lt;/th&gt; &#xA;   &lt;th&gt;Practice&lt;/th&gt; &#xA;   &lt;th&gt;Language&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;basic&lt;/td&gt; &#xA;   &lt;td&gt;Spring 2020&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PL4_hYwCyhAvZyW6qS58x4uElZgAkMVUvj&#34;&gt;youtube&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PL4_hYwCyhAvYPOWn6e44RKxEfRWEsPA1z&#34;&gt;youtube&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ru&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;advanced&lt;/td&gt; &#xA;   &lt;td&gt;Fall 2020&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PL4_hYwCyhAvY7k32D65q3xJVo8X8dc3Ye&#34;&gt;youtube&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PL4_hYwCyhAvZLp0CTIDVQr9FtDR_7DaUr&#34;&gt;youtube&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ru&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;basic&lt;/td&gt; &#xA;   &lt;td&gt;Spring 2019&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PL4_hYwCyhAvasRqzz4w562ce0esEwS0Mt&#34;&gt;youtube&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;ru&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;advanced&lt;/td&gt; &#xA;   &lt;td&gt;Fall 2019&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PL4_hYwCyhAvZeq93ssEUaR47xhvs7IhJM&#34;&gt;youtube&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PL4_hYwCyhAvYvuHz_PKlEV-kOsK2bwUBg&#34;&gt;youtube&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ru&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;p&gt;We are expecting our students to have a basic knowlege of:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;calculus, especially matrix calculus, differentiation&lt;/li&gt; &#xA; &lt;li&gt;linear algebra&lt;/li&gt; &#xA; &lt;li&gt;probability theory and statistics&lt;/li&gt; &#xA; &lt;li&gt;programming, especially on Python&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Although if you don&#39;t have any of this, you could substitude it with your diligence because the course provides additional materials to study requirements yourself.&lt;/p&gt; &#xA;&lt;h2&gt;Materials for self-study&lt;/h2&gt; &#xA;&lt;p&gt;A lot of great materials are available online. See &lt;a href=&#34;https://raw.githubusercontent.com/girafe-ai/ml-course/master/extra_materials.md&#34;&gt;extra materials&lt;/a&gt; file for the whole list.Also lectures and seminars contains references to more detailed materials on topics.&lt;/p&gt; &#xA;&lt;p&gt;Lectures conspects are available for both &lt;a href=&#34;https://raw.githubusercontent.com/girafe-ai/ml-course/master/lecture_notes_basic__ru.pdf&#34;&gt;basic&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/girafe-ai/ml-course/master/lecture_notes_advanced__ru.pdf&#34;&gt;advanced&lt;/a&gt;. Especially useful for fast and furious exam passing. (Previous version of informal basic conspect is &lt;a href=&#34;https://github.com/girafe-ai/ml-mipt/raw/spring_2019/ML_informal_notes.pdf&#34;&gt;available&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;Docker image&lt;/h2&gt; &#xA;&lt;p&gt;Using docker for tasks evaluation is a good idea, prebuilt image is under cunstruction.&lt;/p&gt;</summary>
  </entry>
</feed>