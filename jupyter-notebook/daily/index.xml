<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-02-15T01:38:38Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>hongfz16/EVA3D</title>
    <updated>2023-02-15T01:38:38Z</updated>
    <id>tag:github.com,2023-02-15:/hongfz16/EVA3D</id>
    <link href="https://github.com/hongfz16/EVA3D" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[ICLR 2023 Spotlight] EVA3D: Compositional 3D Human Generation from 2D Image Collections&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;EVA3D: Compositional 3D Human Generation from 2D Image Collections&lt;/h1&gt; &#xA; &lt;div&gt; &#xA;  &lt;a href=&#34;https://hongfz16.github.io/&#34; target=&#34;_blank&#34;&gt;Fangzhou Hong&lt;/a&gt;‚ÄÉ &#xA;  &lt;a href=&#34;https://frozenburning.github.io/&#34; target=&#34;_blank&#34;&gt;Zhaoxi Chen&lt;/a&gt;‚ÄÉ &#xA;  &lt;a href=&#34;https://github.com/NIRVANALAN&#34; target=&#34;_blank&#34;&gt;Yushi Lan&lt;/a&gt;‚ÄÉ &#xA;  &lt;a href=&#34;https://scholar.google.com/citations?user=lSDISOcAAAAJ&amp;amp;hl=zh-CN&#34; target=&#34;_blank&#34;&gt;Liang Pan&lt;/a&gt;‚ÄÉ &#xA;  &lt;a href=&#34;https://liuziwei7.github.io/&#34; target=&#34;_blank&#34;&gt;Ziwei Liu&lt;/a&gt;&#xA;  &lt;sup&gt;*&lt;/sup&gt; &#xA; &lt;/div&gt; &#xA; &lt;div&gt;&#xA;   S-Lab, Nanyang Technological University‚ÄÉ &#xA;  &lt;sup&gt;*&lt;/sup&gt;corresponding author &#xA; &lt;/div&gt; &#xA; &lt;div&gt; &#xA;  &lt;span&gt;ü§©&lt;/span&gt; &#xA;  &lt;strong&gt;Accepted to ICLR 2023 as Spotlight&lt;/strong&gt; &#xA; &lt;/div&gt; &#xA; &lt;div&gt; &#xA;  &lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/hongfz16/EVA3D/blob/main/notebook/EVA3D_Demo.ipynb&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt; &lt;/a&gt; &#xA;  &lt;a target=&#34;_blank&#34; href=&#34;https://huggingface.co/spaces/hongfz16/EVA3D&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Demo-%F0%9F%A4%97%20Hugging%20Face-blue&#34; alt=&#34;HuggingFace&#34;&gt; &lt;/a&gt; &#xA;  &lt;a href=&#34;https://hits.seeyoufarm.com&#34;&gt;&lt;img src=&#34;https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2Fhongfz16%2FEVA3D&amp;amp;count_bg=%2379C83D&amp;amp;title_bg=%23555555&amp;amp;icon=&amp;amp;icon_color=%23E7E7E7&amp;amp;title=hits&amp;amp;edge_flat=false&#34;&gt;&lt;/a&gt; &#xA; &lt;/div&gt; &#xA; &lt;p&gt;&lt;strong&gt;EVA3D is a high-quality unconditional 3D human generative model that only requires 2D image collections for training.&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;table&gt; &#xA;  &lt;tbody&gt;&#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hongfz16/EVA3D/main/assets/0032_rgb.gif&#34; width=&#34;100%&#34;&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hongfz16/EVA3D/main/assets/0032_geo.gif&#34; width=&#34;100%&#34;&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hongfz16/EVA3D/main/assets/0067_rgb.gif&#34; width=&#34;100%&#34;&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hongfz16/EVA3D/main/assets/0067_geo.gif&#34; width=&#34;100%&#34;&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hongfz16/EVA3D/main/assets/0021_rgb_dancing.gif&#34; width=&#34;98%&#34;&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hongfz16/EVA3D/main/assets/0001_rgb_interpolation.gif&#34; width=&#34;88%&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34; width=&#34;14%&#34;&gt;Sample 1 RGB&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34; width=&#34;14%&#34;&gt;Sample 1 Geo&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34; width=&#34;14%&#34;&gt;Sample 2 RGB&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34; width=&#34;14%&#34;&gt;Sample 2 Geo&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34; width=&#34;19%&#34;&gt;Novel Pose Generation&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34; width=&#34;19%&#34;&gt;Latent Space Interpolation&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt;&#xA; &lt;/table&gt; &#xA; &lt;p&gt;&lt;span&gt;üìñ&lt;/span&gt; For more visual results, go checkout our &lt;a href=&#34;https://hongfz16.github.io/projects/EVA3D.html&#34; target=&#34;_blank&#34;&gt;project page&lt;/a&gt;&lt;/p&gt; &#xA; &lt;!-- This repository will contain the official implementation of _EVA3D: Compositional 3D Human Generation from 2D Image Collections_. --&gt; &#xA; &lt;p&gt;&lt;span&gt;üçª&lt;/span&gt; Training and Inference codes released&lt;/p&gt; &#xA; &lt;hr&gt; &#xA; &lt;h4 align=&#34;center&#34;&gt; &lt;a href=&#34;https://hongfz16.github.io/projects/EVA3D.html&#34; target=&#34;_blank&#34;&gt;[Project Page]&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://arxiv.org/abs/2210.04888&#34; target=&#34;_blank&#34;&gt;[arXiv]&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://youtu.be/JNV0FJ0aDWM&#34; target=&#34;_blank&#34;&gt;[Demo Video]&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://colab.research.google.com/drive/1k6-Sc_EsIT292hNgu-7haC5ghggImQ7f?usp=sharing&#34; target=&#34;_blank&#34;&gt;[Colab Demo]&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;&#34; target=&#34;_blank&#34;&gt;[Hugging Face &lt;span&gt;ü§ó&lt;/span&gt;]&lt;/a&gt; &lt;/h4&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;&lt;span&gt;üì£&lt;/span&gt; Updates&lt;/h2&gt; &#xA;&lt;p&gt;[02/2023] Training codes for DeepFashion with our processed dataset are released.&lt;/p&gt; &#xA;&lt;p&gt;[02/2023] Inference codes (512x256 generation on DeepFashion) are released, including colab and huggingface demos.&lt;/p&gt; &#xA;&lt;p&gt;[01/2023] EVA3D is accepted to ICLR 2023 (Spotlight)&lt;span&gt;ü•≥&lt;/span&gt;!&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;ü§ü&lt;/span&gt; Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work useful for your research, please consider citing the paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{&#xA;    hong2023evad,&#xA;    title={{EVA}3D: Compositional 3D Human Generation from 2D Image Collections},&#xA;    author={Fangzhou Hong and Zhaoxi Chen and Yushi LAN and Liang Pan and Ziwei Liu},&#xA;    booktitle={International Conference on Learning Representations},&#xA;    year={2023},&#xA;    url={https://openreview.net/forum?id=g7U9jD_2CUr}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;span&gt;üñ•&lt;/span&gt; Requirements&lt;/h2&gt; &#xA;&lt;p&gt;NVIDIA GPUs are required for this project. We have test the inference codes on NVIDIA RTX2080Ti, NVIDIA V100, NVIDIA A100, NVIDIA T4. The training codes have been tested on NVIDIA V100, NVIDIA A100. We recommend using anaconda to manage the python environments.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create --name eva3d python=3.8&#xA;conda install pytorch==1.7.1 torchvision==0.8.2 torchaudio==0.7.2 cudatoolkit=10.1 -c pytorch&#xA;conda install -c fvcore -c iopath -c conda-forge fvcore iopath&#xA;conda install pytorch3d -c pytorch3d&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;span&gt;üèÉ‚ôÄ&lt;/span&gt; Inference&lt;/h2&gt; &#xA;&lt;h3&gt;Download Models&lt;/h3&gt; &#xA;&lt;p&gt;The pretrain model and SMPL model are needed for inference.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python download_models.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Commands&lt;/h3&gt; &#xA;&lt;p&gt;We provide a script for inference the model trained on DeepFashion with the resolution of 512x256.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/demo_deepfashion_512x256.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;span&gt;üöã&lt;/span&gt; Training&lt;/h2&gt; &#xA;&lt;h3&gt;DeepFashion&lt;/h3&gt; &#xA;&lt;h4&gt;Download SMPL Models &amp;amp; Processed Datasets&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python download_models.py&#xA;python download_datasets.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Commands&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/train_deepfashion_512x256.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Intermediate results will be saved under &lt;code&gt;checkpoint/train_deepfashion_512x256/volume_renderer/samples&lt;/code&gt; every 100 iterations. The first line presents inference images from EMA generator. The second line present one inference sample of the training generator and one sample from the training dataset.&lt;/p&gt; &#xA;&lt;p&gt;To inference the trained models, please refer to the &lt;strong&gt;Inference&lt;/strong&gt; section.&lt;/p&gt; &#xA;&lt;p&gt;Support for more datasets coming soon...&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;üóû&lt;/span&gt; License&lt;/h2&gt; &#xA;&lt;p&gt;Distributed under the S-Lab License. See &lt;code&gt;LICENSE&lt;/code&gt; for more information.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;üôå&lt;/span&gt; Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;This study is supported by NTU NAP, MOE AcRF Tier 2 (T2EP20221-0033), and under the RIE2020 Industry Alignment Fund ‚Äì Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s).&lt;/p&gt; &#xA;&lt;p&gt;This project is built on source codes shared by &lt;a href=&#34;https://github.com/royorel/StyleSDF&#34;&gt;StyleSDF&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>camenduru/controlnet-colab</title>
    <updated>2023-02-15T01:38:38Z</updated>
    <id>tag:github.com,2023-02-15:/camenduru/controlnet-colab</id>
    <link href="https://github.com/camenduru/controlnet-colab" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;üê£ Please follow me for new updates &lt;a href=&#34;https://twitter.com/camenduru&#34;&gt;https://twitter.com/camenduru&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ü¶í Colab&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/camenduru/controlnet-colab/blob/main/controlnet-colab.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ü§ó Hugging Face&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/spaces/camenduru/controlnet&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Main Repo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/lllyasviel/ControlNet&#34;&gt;https://github.com/lllyasviel/ControlNet&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Paper&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/lllyasviel/ControlNet/raw/main/github_page/control.pdf&#34;&gt;https://github.com/lllyasviel/ControlNet/raw/main/github_page/control.pdf&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Output&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/54370274/218286109-0106cac8-8b03-4027-a687-192baf556efa.jpg&#34; alt=&#34;Screenshot 2023-02-12 023250&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>true-grue/kispython</title>
    <updated>2023-02-15T01:38:38Z</updated>
    <id>tag:github.com,2023-02-15:/true-grue/kispython</id>
    <link href="https://github.com/true-grue/kispython" rel="alternate"></link>
    <summary type="html">&lt;p&gt;–ö—É—Ä—Å –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ —è–∑—ã–∫–µ Python –≤ –†–¢–£ –ú–ò–†–≠–ê&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;–ü—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ —è–∑—ã–∫–µ Python. –í–µ—Å–µ–Ω–Ω–∏–π —Å–µ–º–µ—Å—Ç—Ä 2023&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;–í–≤–µ–¥–µ–Ω–∏–µ&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;–õ–µ–∫—Ü–∏—è: &lt;a href=&#34;https://colab.research.google.com/github/true-grue/kispython/blob/main/lect1.ipynb&#34;&gt;co&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –∑–∞–Ω—è—Ç–∏–µ: &lt;a href=&#34;https://github.com/true-grue/kispython/raw/main/pract1.ipynb&#34;&gt;gh&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/true-grue/kispython/blob/main/pract1.ipynb&#34;&gt;co&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;–ü—Ä–æ—Å—Ç—ã–µ —Å–∫—Ä–∏–ø—Ç—ã&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;–õ–µ–∫—Ü–∏—è: &lt;a href=&#34;https://colab.research.google.com/github/true-grue/kispython/blob/main/lect2.ipynb&#34;&gt;co&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
</feed>