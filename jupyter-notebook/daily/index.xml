<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-29T01:37:59Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>zsylvester/segmenteverygrain</title>
    <updated>2023-05-29T01:37:59Z</updated>
    <id>tag:github.com,2023-05-29:/zsylvester/segmenteverygrain</id>
    <link href="https://github.com/zsylvester/segmenteverygrain" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A SAM-based model for instance segmentation of images of grains&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Segment Every Grain&lt;/h1&gt; &#xA;&lt;img src=&#34;https://github.com/zsylvester/segmenteverygrain/raw/main/gravel_example_mask.jpg&#34; width=&#34;600&#34;&gt; &#xA;&lt;h2&gt;Description&lt;/h2&gt; &#xA;&lt;p&gt;&#39;segmenteverygrain&#39; is a Python package that aims to detect grains (or grain-like objects) in images. The goal is to develop an ML model that does a reasonably good job at detecting most of the grains in a photo, so that it will be useful for determining grain size and grain shape, a common task in geomorphology and sedimentary geology. &#39;segmenteverygrain&#39; relies on the &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;Segment Anything Model (SAM)&lt;/a&gt;, developed by Meta, for getting high-quality outlines of the grains. However, SAM requires prompts for every object detected and, when used in &#39;everything&#39; mode, it tends to be slow and results in many overlapping masks and non-grain (background) objects. To deal with these issues, &#39;segmenteverygrain&#39; relies on a Unet-style, patch-based convolutional neural network to create a first-pass segmentation which is then used as a set of prompts for the SAM-based segmentation. Some of the grains will be missed with this approach, but the segmentations that are created tend to be of high quality.&lt;/p&gt; &#xA;&lt;p&gt;&#39;segmenteverygrain&#39; also includes a set of functions that make it possible to clean up the segmentation results: delete and merge objects by clicking on them, and adding grains that were not segmented automatically. The QC-d masks can be saved and added to a dataset of grain images (see the &#39;images&#39; folder). These images then can be used to improve the Unet model. Many of the images used in the dataset are from the &lt;a href=&#34;https://github.com/DigitalGrainSize/SediNet&#34;&gt;sedinet&lt;/a&gt; project.&lt;/p&gt; &#xA;&lt;p&gt;This is &lt;em&gt;work in progress&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;numpy&lt;/li&gt; &#xA; &lt;li&gt;matplotlib&lt;/li&gt; &#xA; &lt;li&gt;scipy&lt;/li&gt; &#xA; &lt;li&gt;pillow&lt;/li&gt; &#xA; &lt;li&gt;scikit-image&lt;/li&gt; &#xA; &lt;li&gt;opencv-python&lt;/li&gt; &#xA; &lt;li&gt;networkx&lt;/li&gt; &#xA; &lt;li&gt;rasterio&lt;/li&gt; &#xA; &lt;li&gt;shapely&lt;/li&gt; &#xA; &lt;li&gt;tensorflow&lt;/li&gt; &#xA; &lt;li&gt;segment-anything&lt;/li&gt; &#xA; &lt;li&gt;tqdm&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install segmenteverygrain&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://github.com/zsylvester/segmenteverygrain/raw/main/segmenteverygrain/Segment_every_grain.ipynb&#34;&gt;Segment_every_grain.ipynb&lt;/a&gt; notebook for an example of how the models can be loaded and used for segmenting an image and QC-ing the result.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://github.com/zsylvester/segmenteverygrain/raw/main/segmenteverygrain/Train_seg_unet_model.ipynb&#34;&gt;Train_seg_unet_model.ipynb&lt;/a&gt; notebook goes through the steps needed to create, train, and test the Unet model.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://github.com/zsylvester/segmenteverygrain/raw/main/segmenteverygrain/Segment_every_grain_colab.ipynb&#34;&gt;Segment_every_grain_colab.ipynb&lt;/a&gt; has been adjusted so that the segmentation can be tested in Google Colab. That said, the interactivity in Colab is not as smooth as in a local notebook.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;Thanks to Danny Stockli, Nick Howes, Jake Covault, Matt Malkowski, Raymond Luong, and Sergey Fomel for discussions and/or helping with generating training data. Funding for this work came from the &lt;a href=&#34;http://www.beg.utexas.edu/qcl&#34;&gt;Quantitative Clastics Laboratory industrial consortium&lt;/a&gt; at the Bureau of Economic Geology, The University of Texas at Austin.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;segmenteverygrain is licensed under the &lt;a href=&#34;https://github.com/zsylvester/segmenteverygrain/raw/master/LICENSE.txt&#34;&gt;Apache License 2.0&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>