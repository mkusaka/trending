<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-09-08T01:30:56Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>iusztinpaul/hands-on-llms</title>
    <updated>2023-09-08T01:30:56Z</updated>
    <id>tag:github.com,2023-09-08:/iusztinpaul/hands-on-llms</id>
    <link href="https://github.com/iusztinpaul/hands-on-llms" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Learn how to engineer your end-to-end LLM ecosystem: training, streaming, and inference pipelines | deploy &amp; automate | work in progress...&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h2&gt;Hands-on LLMOps&lt;/h2&gt; &#xA; &lt;h1&gt;Train and Deploy a Real-Time Financial Advisor&lt;/h1&gt; &#xA; &lt;i&gt;by &lt;a href=&#34;https://github.com/iusztinpaul&#34;&gt;Paul Iusztin&lt;/a&gt; and &lt;a href=&#34;https://github.com/Paulescu&#34;&gt;Pau Labarta Bajo&lt;/a&gt;&lt;/i&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iusztinpaul/hands-on-llms/main/#1-building-blocks&#34;&gt;1. Building Blocks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iusztinpaul/hands-on-llms/main/#2-setup-external-services&#34;&gt;2. Setup External Services&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iusztinpaul/hands-on-llms/main/#3-install--usage&#34;&gt;3. Install &amp;amp; Usage&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;1. Building Blocks&lt;/h2&gt; &#xA;&lt;h3&gt;Training pipeline&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Fine-tune Falcon 7B using our own &lt;a href=&#34;https://raw.githubusercontent.com/iusztinpaul/hands-on-llms/main/modules/q_and_a_dataset_generator/&#34;&gt;Q&amp;amp;A generated dataset&lt;/a&gt; containing investing questions and answers based on Alpaca News. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;It seems that 1 GPU is enough if we use &lt;a href=&#34;https://lightning.ai/pages/blog/falcon-a-guide-to-finetune-and-inference/&#34;&gt;Lit-Parrot&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Real-time data pipeline&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Build real-time feature pipeline, that ingests data form Alpaca, computes embeddings, and stores them into a serverless Vector DB.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Inference pipeline&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; REST API for inference, that &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;receives a question (e.g. &#34;Is it a good time to invest in renewable energy?&#34;),&lt;/li&gt; &#xA;   &lt;li&gt;finds the most relevant documents in the VectorDB (aka context)&lt;/li&gt; &#xA;   &lt;li&gt;sends a prompt with question and context to our fine-tuned Falcon and return response.&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;2. Setup External Services&lt;/h2&gt; &#xA;&lt;p&gt;Before diving into the modules, you have to set up a couple of additional tools for the course.&lt;/p&gt; &#xA;&lt;h3&gt;2.1. Comet ML&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;ML platform&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Go to &lt;a href=&#34;https://www.comet.com/signup?utm_source=thepauls&amp;amp;utm_medium=partner&amp;amp;utm_content=github&#34;&gt;Comet ML&lt;/a&gt;, create an account, a project, and an API KEY. We will show you in every module how to add these credentials.&lt;/p&gt; &#xA;&lt;h3&gt;2.2. Beam&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;cloud compute&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Go to &lt;a href=&#34;https://www.beam.cloud?utm_source=thepauls&amp;amp;utm_medium=partner&amp;amp;utm_content=github&#34;&gt;Beam&lt;/a&gt; and follow their quick setup/get started tutorial. You must install their CLI and configure your credentials on your local machine.&lt;/p&gt; &#xA;&lt;p&gt;When using Poetry, we had issues locating the Beam CLI when using the Poetry virtual environment. To fix this, create a symlink using the following command - replace &lt;code&gt;&amp;lt;your-poetry-env-name&amp;gt;&lt;/code&gt; with your Poetry env name:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export POETRY_ENV_NAME=&amp;lt;your-poetry-env-name&amp;gt;&#xA; ln -s /usr/local/bin/beam ~/.cache/pypoetry/virtualenvs/${POETRY_ENV_NAME}/bin/beam&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;3. Install &amp;amp; Usage&lt;/h2&gt; &#xA;&lt;p&gt;Every module has its dependencies and scripts. In a production setup, every module would have its repository, but in this use case, for learning purposes, we put everything in one place:&lt;/p&gt; &#xA;&lt;p&gt;Thus, check out the README for every module individually to see how to install &amp;amp; use it:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iusztinpaul/hands-on-llms/main/modules/q_and_a_dataset_generator/&#34;&gt;q_and_a_dataset_generator&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iusztinpaul/hands-on-llms/main/modules/training_pipeline/&#34;&gt;training_pipeline&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iusztinpaul/hands-on-llms/main/modules/streaming_pipeline/&#34;&gt;streaming_pipeline&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;&#34;&gt;inference_pipeline&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;3.1 Run Notebooks Server&lt;/h3&gt; &#xA;&lt;p&gt;If you want to run a notebook server inside a virtual environment, follow the next steps.&lt;/p&gt; &#xA;&lt;p&gt;First, expose the virtual environment as a notebook kernel:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m ipykernel install --user --name hands-on-llms --display-name &#34;hands-on-llms&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now run the notebook server:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;jupyter notebook notebooks/ --ip 0.0.0.0 --port 8888&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>