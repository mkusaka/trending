<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-26T01:32:14Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>nexusflowai/NexusRaven-V2</title>
    <updated>2023-12-26T01:32:14Z</updated>
    <id>tag:github.com,2023-12-26:/nexusflowai/NexusRaven-V2</id>
    <link href="https://github.com/nexusflowai/NexusRaven-V2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;NexusRaven-V2&lt;/h1&gt; &#xA;&lt;h3&gt;Pushing The Boundaries of Open Source Function Calling Models&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/nexusflowai/NexusRaven-V2/master/CODE_LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg?sanitize=true&#34; alt=&#34;Code License&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/Model%20License-LLaMa-red.svg?sanitize=true&#34; alt=&#34;Model License&#34;&gt; &lt;a href=&#34;https://www.python.org/downloads/release/python-3100/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.10+-blue.svg?sanitize=true&#34; alt=&#34;Python 3.10+&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://huggingface.co/Nexusflow&#34; target=&#34;_blank&#34;&gt;Nexusflow HF&lt;/a&gt; - &lt;a href=&#34;https://discord.gg/HDSVmNAs3y&#34; target=&#34;_blank&#34;&gt;Nexusflow Discord&lt;/a&gt; - &lt;a href=&#34;http://nexusflow.ai/blogs/ravenv2&#34; target=&#34;_blank&#34;&gt;NexusRaven-V2 blog post&lt;/a&gt; - &lt;a href=&#34;https://colab.research.google.com/drive/19JYixRPPlanmW5q49WYi_tU8rhHeCEKW?usp=sharing&#34; target=&#34;_blank&#34;&gt;Prompting Notebook CoLab&lt;/a&gt; - &lt;a href=&#34;https://huggingface.co/spaces/Nexusflow/Nexus_Function_Calling_Leaderboard&#34; target=&#34;_blank&#34;&gt;Leaderboard&lt;/a&gt; - &lt;a href=&#34;https://huggingface.co/spaces/Nexusflow/NexusRaven-V2-Demo&#34; target=&#34;_blank&#34;&gt;Read-World Demo&lt;/a&gt; - &lt;a href=&#34;https://huggingface.co/Nexusflow/NexusRaven-V2-13B&#34; target=&#34;_blank&#34;&gt;NexusRaven-V2-13B HF&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;a&gt;&lt;img src=&#34;https://raw.githubusercontent.com/nexusflowai/NexusRaven-V2/master/imgs/raven.png&#34; alt=&#34;NexusRaven&#34; style=&#34;width: 40%; min-width: 300px; display: block; margin: auto;&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;Welcome to NexusRaven-V2! We want to provide the evaluation data, evaluation code, and a prompting guide to help you reproduce our results, as well as use NexusRaven-V2 to the best of its ability!&lt;/p&gt; &#xA;&lt;h2&gt;Introducing NexusRaven-V2&lt;/h2&gt; &#xA;&lt;p&gt;NexusRaven-V2 is an open-source and commercially viable function calling LLM that surpasses the state-of-the-art in function calling capabilities. It has 13B parameters. NexusRaven-V2 is capable of doing single function calls, parallel function calls (where multiple &#34;disconnected&#34; function calls are necessary to answer the user query), and nested function calls (where the argument for the necessary function requires a chain of other function calls).&lt;/p&gt; &#xA;&lt;p&gt;💪 &lt;strong&gt;Versatile Function Calling Capability&lt;/strong&gt;: NexusRaven-V2 is capable of generating single function calls, nested calls, and parallel calls in many challenging cases.&lt;/p&gt; &#xA;&lt;p&gt;🤓 &lt;strong&gt;Fully Explainable&lt;/strong&gt;: NexusRaven-V2 is capable of generating very detailed explanations for the function calls it generates. This behavior can be turned off, to save tokens during inference.&lt;/p&gt; &#xA;&lt;p&gt;📊 &lt;strong&gt;Performance Highlights&lt;/strong&gt;: NexusRaven-V2 surpasses GPT-4 by up to 7% in function calling success rates in human-generated use cases involving nested and composite functions.&lt;/p&gt; &#xA;&lt;p&gt;🔧 &lt;strong&gt;Generalization to the Unseen&lt;/strong&gt;: NexusRaven-V2 has never been trained on the functions used in evaluation.&lt;/p&gt; &#xA;&lt;p&gt;🔥 &lt;strong&gt;Commercially Permissive&lt;/strong&gt;: The training of NexusRaven-V2 does not involve any data generated by proprietary LLMs such as GPT-4. You have full control of the model when deployed in commercial applications.&lt;/p&gt; &#xA;&lt;p&gt;Please checkout the following links!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/19JYixRPPlanmW5q49WYi_tU8rhHeCEKW?usp=sharing&#34;&gt;Prompting Notebook CoLab&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/Nexusflow/Nexus_Function_Calling_Leaderboard&#34;&gt;Evaluation Leaderboard&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/Nexusflow/NexusRaven-V2-Demo&#34;&gt;NexusRaven-V2 Real-World Demo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;NexusRaven-V2 model usage&lt;/h2&gt; &#xA;&lt;p&gt;NexusRaven-V2 accepts a list of python functions. These python functions can do anything (including sending GET/POST requests to external APIs!). The two requirements include the python function signature and the appropriate docstring to generate the function call.&lt;/p&gt; &#xA;&lt;h3&gt;NexusRaven-V2&#39;s Capabilities&lt;/h3&gt; &#xA;&lt;p&gt;NexusRaven-V2 is capable of generating deeply nested function calls, parallel function calls, and simple single calls. It can also justify the function calls it generated. If you would like to generate the call only, please set a stop criteria of &#34;&amp;lt;bot_end&amp;gt;&#34;. Otherwise, please allow NexusRaven-V2 to run until its stop token (i.e. &#34;&amp;lt;/s&amp;gt;&#34;).&lt;/p&gt; &#xA;&lt;h3&gt;Quick Start Prompting Guide&lt;/h3&gt; &#xA;&lt;p&gt;Please refer to our notebook, &lt;a href=&#34;https://raw.githubusercontent.com/nexusflowai/NexusRaven-V2/master/How-To-Prompt.ipynb&#34;&gt;How-To-Prompt.ipynb&lt;/a&gt;, for more advanced tutorials on using NexusRaven-V2!&lt;/p&gt; &#xA;&lt;h3&gt;Quickstart&lt;/h3&gt; &#xA;&lt;p&gt;You can run the model on a GPU using the following code.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Please `pip install transformers accelerate`&#xA;from transformers import pipeline&#xA;&#xA;&#xA;pipeline = pipeline(&#xA;    &#34;text-generation&#34;,&#xA;    model=&#34;Nexusflow/NexusRaven-V2-13B&#34;,&#xA;    torch_dtype=&#34;auto&#34;,&#xA;    device_map=&#34;auto&#34;,&#xA;)&#xA;&#xA;prompt_template = \&#xA;&#39;&#39;&#39;&#xA;Function:&#xA;def get_weather_data(coordinates):&#xA;    &#34;&#34;&#34;&#xA;    Fetches weather data from the Open-Meteo API for the given latitude and longitude.&#xA;&#xA;    Args:&#xA;    coordinates (tuple): The latitude of the location.&#xA;&#xA;    Returns:&#xA;    float: The current temperature in the coordinates you&#39;ve asked for&#xA;    &#34;&#34;&#34;&#xA;&#xA;Function:&#xA;def get_coordinates_from_city(city_name):&#xA;    &#34;&#34;&#34;&#xA;    Fetches the latitude and longitude of a given city name using the Maps.co Geocoding API.&#xA;&#xA;    Args:&#xA;    city_name (str): The name of the city.&#xA;&#xA;    Returns:&#xA;    tuple: The latitude and longitude of the city.&#xA;    &#34;&#34;&#34;&#xA;&#xA;User Query: {query}&amp;lt;human_end&amp;gt;&#xA;&#xA;&#39;&#39;&#39;&#xA;&#xA;prompt = prompt_template.format(query=&#34;What&#39;s the weather like in Seattle right now?&#34;)&#xA;&#xA;result = pipeline(prompt, max_new_tokens=2048, return_full_text=False, do_sample=False, temperature=0.001)[0][&#34;generated_text&#34;]&#xA;print (result)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This should generate the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Call: get_weather_data(coordinates=get_coordinates_from_city(city_name=&#39;Seattle&#39;))&amp;lt;bot_end&amp;gt;&#xA;Thought: The function call `get_weather_data(coordinates=get_coordinates_from_city(city_name=&#39;Seattle&#39;))` answers the question &#34;What&#39;s the weather like in Seattle right now?&#34; by following these steps:&#xA;&#xA;1. `get_coordinates_from_city(city_name=&#39;Seattle&#39;)`: This function call fetches the latitude and longitude of the city &#34;Seattle&#34; using the Maps.co Geocoding API.&#xA;2. `get_weather_data(coordinates=...)`: This function call fetches the current weather data for the coordinates returned by the previous function call.&#xA;&#xA;Therefore, the function call `get_weather_data(coordinates=get_coordinates_from_city(city_name=&#39;Seattle&#39;))` answers the question &#34;What&#39;s the weather like in Seattle right now?&#34; by first fetching the coordinates of the city &#34;Seattle&#34; and then fetching the current weather data for those coordinates.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you would like to prevent the generation of the explanation of the function call (for example, to save on inference tokens), please set a stopping criteria of &#34;&amp;lt;bot_end&amp;gt;&#34;.&lt;/p&gt; &#xA;&lt;p&gt;Please follow this prompting template to maximize the performance of RavenV2.&lt;/p&gt; &#xA;&lt;h3&gt;Using with OpenAI FC Schematics&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/nexusflowai/nexusraven-pip&#34;&gt;If you currently have a workflow that is built around OpenAI&#39;s function calling and you want to try NexusRaven-V2, we have a package that helps you drop in NexusRaven-V2.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Please give it a try!&lt;/p&gt; &#xA;&lt;h2&gt;NexusRaven-V2 on Nexus Function Calling Benchmark&lt;/h2&gt; &#xA;&lt;h3&gt;Benchmarks&lt;/h3&gt; &#xA;&lt;p&gt;We curated 9 tasks within the Nexus Function Calling Benchmark specifically around function calling. We release 8 of them, but keep one internally to avoid situations where new community models overfit to these benchmarks.&lt;/p&gt; &#xA;&lt;p&gt;They are based on real-world APIs. Here are the 8 datasets for evaluation:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/datasets/Nexusflow/Function_Call_Definitions&#34;&gt;The function definitions for all benchmarks can be found here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/Nexusflow/NVDLibraryBenchmark&#34; target=&#34;_blank&#34;&gt; NVDLibrary &lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/Nexusflow/VirusTotalBenchmark&#34; target=&#34;_blank&#34;&gt; VirusTotal &lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/Nexusflow/OTXAPIBenchmark&#34; target=&#34;_blank&#34;&gt; OTX &lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/Nexusflow/PlacesAPIBenchmark&#34; target=&#34;_blank&#34;&gt; Places API &lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/Nexusflow/ClimateAPIBenchmark&#34; target=&#34;_blank&#34;&gt; Climate API &lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/Nexusflow/VirusTotalMultiple&#34; target=&#34;_blank&#34;&gt; VirusTotal-Parallel Calls &lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/Nexusflow/VirusTotalMultiple&#34; target=&#34;_blank&#34;&gt; VirusTotal-Nested Calls &lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/Nexusflow/CVECPEAPIBenchmark&#34; target=&#34;_blank&#34;&gt; NVDLibrary-Nested Calls &lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We report accuracy on The Stack API but do not release the data. We keep it internal as a means of benchmarking new models to ensure generalizability.&lt;/p&gt; &#xA;&lt;h3&gt;Benchmark Classifications&lt;/h3&gt; &#xA;&lt;p&gt;We classify the benchmarks above into three categories, based on the type of API usage they require to get the answer correctly. These include: single calls, parallel calls, and nested calls.&lt;/p&gt; &#xA;&lt;h4&gt;Single Calls&lt;/h4&gt; &#xA;&lt;p&gt;Single Calls are defined as API usage that only require a single function call. Here is an example of this:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;a&gt;&lt;img src=&#34;https://raw.githubusercontent.com/nexusflowai/NexusRaven-V2/master/imgs/singleapi.png&#34; alt=&#34;singleapi_example&#34; style=&#34;width: 100%; min-width: 300px; display: block; margin: auto;&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h4&gt;Nested Calls&lt;/h4&gt; &#xA;&lt;p&gt;Nested Calls are defined as API usage that require multiple calls all at once to get the right answer, where the argument for the outer call depends on the call graph. Here is an example of this:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;a&gt;&lt;img src=&#34;https://raw.githubusercontent.com/nexusflowai/NexusRaven-V2/master/imgs/multiapi.png&#34; alt=&#34;multiapi_example&#34; style=&#34;width: 100%; min-width: 300px; display: block; margin: auto;&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h4&gt;Parallel Calls&lt;/h4&gt; &#xA;&lt;p&gt;Parallel calls are when seperate calls are generated, but they do not connect into each other.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;a&gt;&lt;img src=&#34;https://raw.githubusercontent.com/nexusflowai/NexusRaven-V2/master/imgs/parallelapi.png&#34; alt=&#34;parallelapi_example&#34; style=&#34;width: 100%; min-width: 300px; display: block; margin: auto;&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h4&gt;Task Classification&lt;/h4&gt; &#xA;&lt;p&gt;We classify the tasks within Nexus Function Calling Benchmark into the following categories:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;API&lt;/th&gt; &#xA;    &lt;th&gt;Category&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;OTX&lt;/td&gt; &#xA;    &lt;td&gt;Single Calls&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;NVDLibrary&lt;/td&gt; &#xA;    &lt;td&gt;Single Calls&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;VirusTotal&lt;/td&gt; &#xA;    &lt;td&gt;Single Calls&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;VirusTotal-Nested Calls&lt;/td&gt; &#xA;    &lt;td&gt;Nested Calls&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Climate&lt;/td&gt; &#xA;    &lt;td&gt;Nested and Parallel Calls&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;VirusTotal-Parallel Calls&lt;/td&gt; &#xA;    &lt;td&gt;Parallel Calls&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Places API&lt;/td&gt; &#xA;    &lt;td&gt;Nested Calls&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;NVDLibrary-Nested Calls&lt;/td&gt; &#xA;    &lt;td&gt;Nested Calls&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;The Stack API&lt;/td&gt; &#xA;    &lt;td&gt;Mostly Single Calls&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;NexusRaven-V2 Capability Breakdown&lt;/h3&gt; &#xA;&lt;p&gt;We benchmark NexusRaven-V2 against GPT4 primarily. Our benchmarking effort consists of several categories of benchmarks. This includes: single calls, parallel calls, and nested calls.&lt;/p&gt; &#xA;&lt;h4&gt;Capability Performance&lt;/h4&gt; &#xA;&lt;p&gt;The performance of NexusRaven-V2 on each category is reflected below:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;a&gt;&lt;img src=&#34;https://raw.githubusercontent.com/nexusflowai/NexusRaven-V2/master/docs/radar-2.png&#34; alt=&#34;NexusRaven&#34; style=&#34;width: 80%; min-width: 300px; display: block; margin: auto;&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Overall Scores&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;a&gt;&lt;img src=&#34;https://raw.githubusercontent.com/nexusflowai/NexusRaven-V2/master/docs/blog2-fc.png&#34; alt=&#34;Scores&#34; style=&#34;width: 80%; min-width: 300px; display: block; margin: auto;&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Reproducing The Results&lt;/h3&gt; &#xA;&lt;p&gt;Please see the &lt;a href=&#34;https://raw.githubusercontent.com/nexusflowai/NexusRaven-V2/master/evaluation_notebook/Benchmark_Models.ipynb&#34;&gt;Benchmarking Notebook&lt;/a&gt; for reproducing Raven&#39;s results. The cells have already been run and the output has been recorded in the notebook itself. The output of each cell is what we report in the leaderboard. However, you are welcome to rerun the cells to verify. The NexusRaven-V2 endpoint will be accessible for this effort, and the link is present in the notebook itself.&lt;/p&gt; &#xA;&lt;p&gt;NexusRaven-V2&#39;s benchmark performance should be identical to the reported accuracy in leaderboard.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;IMPORTANT NOTE:&lt;/strong&gt; However, it is currently impossible to generate deterministic outputs for GPT4, so we see WILD swings in the per-task accuracy for GPT4 (but, the average accuracy across task categories are relatively stable within a few percent). Please note this when rerunning the GPT4 cells in the notebook.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The code in this repository for running the NexusRaven-V2 model, the evaluation notebook, the prompting notebook, and the evaluation data are licensed under &lt;a href=&#34;https://raw.githubusercontent.com/nexusflowai/NexusRaven-V2/master/CODE_LICENSE&#34;&gt;Apache 2.0&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;p&gt;We thank the CodeLLaMa Team for their great foundational models that made NexusRaven-V2 possible.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{rozière2023code,&#xA;      title={Code Llama: Open Foundation Models for Code}, &#xA;      author={Baptiste Rozière and Jonas Gehring and Fabian Gloeckle and Sten Sootla and Itai Gat and Xiaoqing Ellen Tan and Yossi Adi and Jingyu Liu and Tal Remez and Jérémy Rapin and Artyom Kozhevnikov and Ivan Evtimov and Joanna Bitton and Manish Bhatt and Cristian Canton Ferrer and Aaron Grattafiori and Wenhan Xiong and Alexandre Défossez and Jade Copet and Faisal Azhar and Hugo Touvron and Louis Martin and Nicolas Usunier and Thomas Scialom and Gabriel Synnaeve},&#xA;      year={2023},&#xA;      eprint={2308.12950},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{nexusraven,&#xA;      title={NexusRaven-V2: Surpassing GPT-4 for Zero-shot Function Calling},&#xA;      author={Nexusflow.ai team},&#xA;      year={2023},&#xA;      url={https://nexusflow.ai/blogs/ravenv2}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;Please join our &lt;a href=&#34;https://discord.gg/HDSVmNAs3y&#34;&gt;Discord Channel&lt;/a&gt; to reach out for any issues and comments!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ytzfhqs/AAAMLP-CN</title>
    <updated>2023-12-26T01:32:14Z</updated>
    <id>tag:github.com,2023-12-26:/ytzfhqs/AAAMLP-CN</id>
    <link href="https://github.com/ytzfhqs/AAAMLP-CN" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Approaching (Almost) Any Machine Learning Problem中译版，在线文档地址：https://ytzfhqs.github.io/AAAMLP-CN/&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AAAMLP-CN&lt;/h1&gt; &#xA;&lt;h2&gt;新特性&lt;/h2&gt; &#xA;&lt;h3&gt;2023.10.25&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;😎完成全部翻译&lt;/li&gt; &#xA; &lt;li&gt;📝计划对kaggle游乐园系列优秀解决方案代码进行解析&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;2023.09.07&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;⚡修正部分已知文字错误和代码错误&lt;/li&gt; &#xA; &lt;li&gt;🤗添加&lt;a href=&#34;https://ytzfhqs.github.io/AAAMLP-CN/&#34;&gt;在线文档&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;简介&lt;/h2&gt; &#xA;&lt;p&gt;Abhishek Thakur，很多 kaggler 对他都非常熟悉，2017 年，他在 Linkedin 发表了一篇名为&lt;strong&gt;Approaching (Almost) Any Machine Learning Problem&lt;/strong&gt;的文章，介绍他建立的一个自动的机器学习框架，几乎可以解决任何机器学习问题，这篇文章曾火遍 Kaggle。&lt;/p&gt; &#xA;&lt;p&gt;Abhishek 在 Kaggle 上的成就：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Competitions Grandmaster（17 枚金牌，世界排名第 3）&lt;/li&gt; &#xA; &lt;li&gt;Kernels Expert （Kagglers 排名前 1％）&lt;/li&gt; &#xA; &lt;li&gt;Discussion Grandmaster（65 枚金牌，世界排名第 2）&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;目前，Abhishek 在挪威 boost 公司担任首席数据科学家的职位，这是一家专门从事会话人工智能的软件公司。&lt;/p&gt; &#xA;&lt;p&gt;本文对&lt;strong&gt;Approaching (Almost) Any Machine Learning Problem&lt;/strong&gt;进行了&lt;strong&gt;中文翻译&lt;/strong&gt;，由于本人水平有限，且未使用机器翻译，可能有部分言语不通顺或本土化程度不足，也请大家在阅读过程中多提供宝贵意见。另附上书籍原&lt;a href=&#34;https://github.com/abhishekkrthakur/approachingalmost&#34;&gt;项目地址&lt;/a&gt;，&lt;strong&gt;转载请一定标明出处！&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;本项目&lt;strong&gt;支持在线阅读&lt;/strong&gt;，方便您随时随地进行查阅。&lt;/p&gt; &#xA;&lt;p&gt;因为有几章内容太过基础，所以未进行翻译，详细情况请参照书籍目录：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;准备环境（已翻译）&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;无监督和有监督学习（已翻译）&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;交叉检验（已翻译）&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;评估指标（已翻译）&lt;/strong&gt; -&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;组织机器学习（已翻译）&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;处理分类变量（已翻译）&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;特征工程（已翻译）&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;特征选择（已翻译）&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;超参数优化（已翻译）&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;图像分类和分割方法（已翻译）&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;文本分类或回归方法（已翻译）&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;组合和堆叠方法（已翻译）&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;可重复代码和模型方法（已翻译）&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;我将会把完整的翻译版 &lt;code&gt;Markdown&lt;/code&gt; 文件上传到 GitHub，以供大家免费下载和阅读。为了最佳的阅读体验，推荐使用 PDF 格式或是在线阅读进行查看&lt;/p&gt; &#xA;&lt;p&gt;若您在阅读过程中发现任何错误或不准确之处，非常欢迎通过提交 Issue 或 Pull Request 来协助我进行修正。&lt;/p&gt; &#xA;&lt;p&gt;如果您觉得这个项目对您有帮助，请不吝给予 Star 或者进行关注。&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>rohan-paul/LLM-FineTuning-Large-Language-Models</title>
    <updated>2023-12-26T01:32:14Z</updated>
    <id>tag:github.com,2023-12-26:/rohan-paul/LLM-FineTuning-Large-Language-Models</id>
    <link href="https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models" rel="alternate"></link>
    <summary type="html">&lt;p&gt;LLM (Large Language Model) FineTuning&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Multiple LLM (Large Language Models) FineTuning Projects&lt;/h1&gt; &#xA;&lt;h2&gt;For almost all of these I have detailed video in my &lt;a href=&#34;https://www.youtube.com/channel/UC0_a8SNpTFkmVv5SLMs1CIA/featured&#34;&gt;YouTube Channel&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/channel/UC0_a8SNpTFkmVv5SLMs1CIA/videos&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/LLM-FineTuning-Large-Language-Models/main/assets/Youtube_Cover.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Find me here..&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;🐦 TWITTER: &lt;a href=&#34;https://twitter.com/rohanpaul_ai&#34;&gt;https://twitter.com/rohanpaul_ai&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;🟠 YouTube: &lt;a href=&#34;https://www.youtube.com/@RohanPaul-AI/featured&#34;&gt;https://www.youtube.com/@RohanPaul-AI/featured&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;👨🏻‍💼 LINKEDIN: &lt;a href=&#34;https://www.linkedin.com/in/rohan-paul-b27285129/&#34;&gt;https://www.linkedin.com/in/rohan-paul-b27285129/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;​👨‍🔧​ KAGGLE: &lt;a href=&#34;https://www.kaggle.com/paulrohan2020&#34;&gt;https://www.kaggle.com/paulrohan2020&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=815NpXvniIg&amp;amp;list=PLxqBkZuBynVTzqUQCQFgetR97y1X_1uCI&amp;amp;index=16&amp;amp;ab_channel=Rohan-Paul-AI&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/raw/main/CodeLLaMA_34B_Conversation_with_Streamlit.py&#34;&gt;CodeLLaMA-34B - Conversational Agent | Large Language Models&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=RYTOQERqVsg&amp;amp;list=PLxqBkZuBynVTzqUQCQFgetR97y1X_1uCI&amp;amp;index=14&amp;amp;ab_channel=Rohan-Paul-AI&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/raw/main/Inference_Yarn-Llama-2-13b-128k_Github.ipynb&#34;&gt;Inference Yarn-Llama-2-13b-128k with KV Cache to answer quiz on very long textbook&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=6DGYj1EEWOw&amp;amp;list=PLxqBkZuBynVTzqUQCQFgetR97y1X_1uCI&amp;amp;index=13&amp;amp;ab_channel=Rohan-Paul-AI&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/raw/main/Mistral_FineTuning_with_PEFT_and_QLORA.ipynb&#34;&gt;Mistral 7B FineTuning with_PEFT and QLORA&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=fEzuBFi35J4&amp;amp;list=PLxqBkZuBynVTzqUQCQFgetR97y1X_1uCI&amp;amp;index=11&amp;amp;ab_channel=Rohan-Paul-AI&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/raw/main/Falcon-7B_FineTuning_with_PEFT_and_QLORA.ipynb&#34;&gt;Falcon finetuning on openassistant-guanaco&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=J0RbOtLrJhQ&amp;amp;list=PLxqBkZuBynVTzqUQCQFgetR97y1X_1uCI&amp;amp;index=10&amp;amp;ab_channel=Rohan-Paul-AI&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/raw/main/FineTuning_phi-1_5_with_PRFT_LoRA.ipynb&#34;&gt;Fine Tuning Phi 1_5 with PEFT and QLoRA&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/raw/main/Finetune_codellama-34B-with-QLoRA.ipynb&#34;&gt;Finetune codellama-34B with QLoRA&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=gNSw9JwGv4w&amp;amp;list=PLxqBkZuBynVTzqUQCQFgetR97y1X_1uCI&amp;amp;index=16&amp;amp;t=486s&amp;amp;ab_channel=Rohan-Paul-AI&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/raw/main/Finetune_opt_bnb_peft.ipynb&#34;&gt;Fine-tune opt-6.7b with QLoRA while keeping some of the Model layers at full-precision of float32&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=QAY82UvrsHg&amp;amp;list=PLxqBkZuBynVTiTEvP6-GYf35yA6OqIN7Y&amp;amp;index=2&amp;amp;ab_channel=Rohan-Paul-AI&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/raw/main/Web%20scraping%20with%20Large%20Language%20Models%20(LLM)-AnthropicAI%20%2B%20LangChainAI.ipynb&#34;&gt;Web scraping with Large Language Models (LLM)-AnthropicAI + LangChainAI&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/raw/main/Mixtral_Chatbot_with_Gradio&#34;&gt;Mixtral Chatbot with Gradio&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/raw/main/togetherai-api-with_Mixtral.ipynb&#34;&gt;togetherai api to run Mixtral&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Other Language Models (Which are technically NOT LLM )&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=-rqmj_tfQLo&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=34&amp;amp;ab_channel=Rohan-Paul-AI&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/DeBERTa%20Fine%20Tuning-for%20Amazon%20Review%20Dataset%20Pytorch.ipynb&#34;&gt;DeBERTa Fine Tuning for Amazon Review Dataset Pytorch&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=4nNbg4bWDrQ&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=32&amp;amp;ab_channel=Rohan-Paul-AI&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/FineTuning_BERT_for_Multi_Class_Classification_Turkish&#34;&gt;FineTuning BERT for Multi-Class Classification on custom Dataset&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=91msLyGC-LI&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=28&amp;amp;ab_channel=Rohan-Paul-AI&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=91msLyGC-LI&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=28&amp;amp;ab_channel=Rohan-Paul-AI&#34;&gt;Document STRIDE when Tokenizing with HuggingFace Transformer for NLP Projects&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=cplo2UyNw24&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=31&amp;amp;ab_channel=Rohan-Paul-AI&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;Fine-tuning of a PreTrained Transformer model - what really happens to the weights (parameters)&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=pqpaHeCsuVI&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=30&amp;amp;ab_channel=Rohan-Paul-AI&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=pqpaHeCsuVI&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=30&amp;amp;ab_channel=Rohan-Paul-AI&#34;&gt;Cerebras-GPT New Large Language Model Open Sourced with Apache 2.0 License&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=6X0xfXMKCjM&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=29&amp;amp;ab_channel=Rohan-Paul-AI&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/Roberta-Large-NER-on-Kaggle-NLP%20Competition&#34;&gt;Roberta-Large Named Entity Recognizition on Kaggle NLP Competition with PyTorch&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=EHtHF9Kvm0Y&amp;amp;list=PLxqBkZuBynVTn2lkHNAcw6lgm1MD5QiMK&amp;amp;index=28&amp;amp;ab_channel=Rohan-Paul-AI&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/Longformer%20end%20to%20end%20with%20Kaggle%20NLP%20competition&#34;&gt;Longformer end to end with Kaggle NLP competition&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=tvdIF1FU7fg&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=24&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/zero_shot_multilingual_sentiment_classification_with_USEm&#34;&gt;Zero Shot Multilingual Sentiment Classification with PyTorch Lightning&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=CwLPglxw1WA&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=23&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/Fine_Tuning_HuggingFace_Transformer_BERT_Yelp_Customer_Review_Predictions&#34;&gt;Fine Tuning Transformer (BERT) for Customer Review Prediction | NLP | HuggingFace &lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=30zPz5Xz-8g&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=21&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/Understing_BERT_Embedding_Vector&#34;&gt;Understanding BERT Embeddings and Tokenization | NLP | HuggingFace&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=fl0ow-nD8FM&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=20&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/Topic-modeling-with-bertopic-arxiv-abstract&#34;&gt;Topic Modeling with BERTopic | arxiv-abstract dataset&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=vrDdnQfav0s&amp;amp;list=PLxqBkZuBynVTn2lkHNAcw6lgm1MD5QiMK&amp;amp;index=21&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/Topic_Modeling_with_LDA.ipynb&#34;&gt;Latent Dirichlet Allocation (LDA) for Topic Modeling&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=iCL1TmRQ0sk&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=19&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/Add-task_specific_custom_layer_to_model.ipynb&#34;&gt;Adding a custom task-specific Layer to a HuggingFace Pretrained Model&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=ZvsH09XGuZ0&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=18&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/Multi-class-text-classifica_fine-tuning-distilbert.ipynb&#34;&gt;Fine Tuning DistilBERT for Multiclass Text Classification&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=dzyDHMycx_c&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=18&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/YT_Fine_tuning_BERT_NER_v1.ipynb&#34;&gt;Fine Tuning BERT for Named Entity Recognition (NER)&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=fLqiPks4neU&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=15&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/Fine_Tuning_Pegasus_for_Text_Summarization.ipynb&#34;&gt;Text Summarization by Fine Tuning Transformer Model | NLP &lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=HDSNjrxSwqw&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=14&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/Text_Summarization_%20BART%20_T5_Pegasus.ipynb&#34;&gt;Text Summarization with Transformer - BART + T5 + Pegasus &lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=oxEXBJQG27A&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=13&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/raw/main/Other-Language_Models_BERT_related/Deberta-v3-large-For_Kaggle_Competition_Feedback-Prize/deberta-v3-large-For_Kaggle_Competition_Feedback-Prize.ipynb&#34;&gt;Debarta-v3-large model fine tuning for Kaggle Competition Feedback-Prize | NLP&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=SmWbKiueYVU&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=12&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/Topic_Modeling_with_BERT_and_Automatic_cluster_labeling/Topic_Modeling.ipynb&#34;&gt;Topic Modeling with BERT and Automatic Cluster Labeling&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Ua_ToM-CG5Q&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=11&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/Decoding_Strategies_for_text_generation/Decoding_Strategies_for_text_generation.ipynb&#34;&gt;Decoding strategies while generating text with GPT-2&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=VrJwKdls6d4&amp;amp;list=PLxqBkZuBynVTn2lkHNAcw6lgm1MD5QiMK&amp;amp;index=12&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/Fake_News_Classification_with_LSTM_Tensorflow.ipynb&#34;&gt;Fake News Classification with LSTM and Tensorflow&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=hgg2GAgDLzA&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=11&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/FinBERT_Long_Text_Part_2.ipynb&#34;&gt;FinBERT Sentiment Analysis for very Long Text (more than 512 Tokens) | PART 2&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=WEAAs_0etJQ&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=9&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/FinBERT_Long_Text_Part_2.ipynb&#34;&gt;FinBERT Sentiment Analysis for very Long Text Corpus (more than 512 Tokens) | PART-1&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=fwDTLQDKJTE&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=8&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/Cosine_Similarity_between_sentences_with_Transformers.ipynb&#34;&gt;Cosine Similarity between sentences with Transformers HuggingFace&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=urMUa4Nw_B8&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=7&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/Zero_Shot_Learning_multilingual-NER.ipynb&#34;&gt;Zero Shot Learning - Cross Lingual Named Entity Recognition with XLM-Roberta&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Hp8_Enwzdxk&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=6&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/BERT_HuggingFace_Basic_Usages.ipynb&#34;&gt;BERT from Hugging Face - Few Baseline Application | NLP&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=CHFiTTPeyUw&amp;amp;list=PLxqBkZuBynVTn2lkHNAcw6lgm1MD5QiMK&amp;amp;index=9&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/Transformer_From_Scratch/Transformer_From_Scratch.ipynb&#34;&gt;Transformer Encoder with Scaled Dot Product from Scratch&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=_IGdekeBCoE&amp;amp;list=PLxqBkZuBynVTn2lkHNAcw6lgm1MD5QiMK&amp;amp;index=7&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/Fuzzy-String-Matching.ipynb&#34;&gt;Fuzzy String Matching in Natural Language Processing | NLP&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=SzSANHjYhfg&amp;amp;list=PLxqBkZuBynVTn2lkHNAcw6lgm1MD5QiMK&amp;amp;index=6&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/Word-Vectors-Understanding-with-Spacy.ipynb&#34;&gt;Understanding Word Vectors usage with Spacy Word and Sentence Similarity&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=TxTxWAohW7E&amp;amp;list=PLxqBkZuBynVTn2lkHNAcw6lgm1MD5QiMK&amp;amp;index=5&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/Named_Entity_Recognition_NER_using_spaCy%20-%20Extracting_Subject_Verb_Action.ipynb&#34;&gt;Named Entity Recognition NER using spaCy - Extracting Subject Verb Action&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=zcW2HouIIQg&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=5&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/raw/main/Other-Language_Models_BERT_related/Fine_Tuning_DistilBert_Poem_Sentiments.ipynb&#34;&gt;Fine-Tuning-DistilBert - Hugging Face Transformer for Poem Sentiment Prediction | NLP&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=0Y03waAL4Gw&amp;amp;list=PLxqBkZuBynVTn2lkHNAcw6lgm1MD5QiMK&amp;amp;index=4&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/bert-base-uncased-fine-tuned-kaggle-hate-speech-dataset.ipynb&#34;&gt;Fine Tuning BERT-Based-Uncased Hugging Face Model on Kaggle Hate Speech Dataset&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=DpzQNQI-S3s&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=3&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/Text%20Analytics%20of%20Tweet%20Emotion%20-%20EDA%20with%20Plotly.ipynb&#34;&gt;Text Analytics of Tweet Emotion - EDA with Plotly&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://bit.ly/3Nk0zRA&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/sentiment_analysis_textblob_Vader.ipynb&#34;&gt;Sentiment analysis using TextBlob and Vader&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>