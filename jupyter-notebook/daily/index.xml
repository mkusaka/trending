<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-01T01:32:05Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>krishnaik06/Updated-Langchain</title>
    <updated>2024-05-01T01:32:05Z</updated>
    <id>tag:github.com,2024-05-01:/krishnaik06/Updated-Langchain</id>
    <link href="https://github.com/krishnaik06/Updated-Langchain" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Updated-Langchain&lt;/h1&gt;</summary>
  </entry>
  <entry>
    <title>kingjulio8238/memary</title>
    <updated>2024-05-01T01:32:05Z</updated>
    <id>tag:github.com,2024-05-01:/kingjulio8238/memary</id>
    <link href="https://github.com/kingjulio8238/memary" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Longterm Memory for Autonomous Agents.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;memary: Open-Source Longterm Memory for Autonomous Agents&lt;/h1&gt; &#xA;&lt;img width=&#34;1410&#34; alt=&#34;memary logo&#34; src=&#34;https://raw.githubusercontent.com/kingjulio8238/memary/main/diagrams/memary_logo.png&#34;&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/GnUU3_xK6bg&#34;&gt;memary demo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Why use memary?&lt;/h2&gt; &#xA;&lt;p&gt;Agents use LLMs that are currently constrained to finite context windows. memary overcomes this limitation by allowing your agents to store a large corpus of information in knowledge graphs, infer user knowledge through our memory modules, and only retrieve relevant information for meaningful responses.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Routing Agent:&lt;/strong&gt; Leverage a ReAct agent to route a query for execution amongst many tools.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Knowledge Graph Creation &amp;amp; Retrieval:&lt;/strong&gt; Leverage Neo4j to create knowledge graphs storing agent responses for later retrieval.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Memory Stream:&lt;/strong&gt; Track all entities stored in the knowledge graph using entity extraction. This stream reflects the user&#39;s breadth of knowledge.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Entity Knowledge Store:&lt;/strong&gt; Group and order all the entities in the memory stream and pass the top N entities into the context window. This knowledge store reflects the user&#39;s depth of knowledge.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How it works&lt;/h2&gt; &#xA;&lt;p&gt;The current structure of memary is detailed in the diagram below.&lt;/p&gt; &#xA;&lt;img width=&#34;1410&#34; alt=&#34;memary overview&#34; src=&#34;https://raw.githubusercontent.com/kingjulio8238/memary/main/diagrams/system.png&#34;&gt; &#xA;&lt;p&gt;The above process includes the routing agent, knoweldge graph and memory module are all integrated into the &lt;code&gt;ChatAgent&lt;/code&gt; class located in the &lt;code&gt;src/agent&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;p&gt;Raw source code for these components can also be found in their respective directories including benchmarks, notebooks, and updates.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Create your &lt;a href=&#34;https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/#create-and-use-virtual-environments&#34;&gt;virtual environment&lt;/a&gt; and activate it.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install Python dependencies:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;To run the Streamlit app:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Ensure that a &lt;code&gt;.env&lt;/code&gt; exists with necessary API keys and Neo4j credentials.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;OPENAI_API_KEY=&#34;YOUR_API_KEY&#34;&#xA;NEO4J_PW=&#34;YOUR_NEO4J_PW&#34;&#xA;NEO4J_URL=&#34;YOUR_NEO4J_URL&#34;&#xA;PERPLEXITY_API_KEY=&#34;YOUR_API_KEY&#34;&#xA;GOOGLEMAPS_API_KEY=&#34;YOUR_API_KEY&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Run: &lt;pre&gt;&lt;code&gt;streamlit run streamlit_app/app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Detailed Component Breakdown&lt;/h2&gt; &#xA;&lt;h3&gt;Routing Agent&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/kingjulio8238/memary/assets/120517860/e5be38db-8c7a-4df2-8b1d-b578fa9c827f&#34; alt=&#34;agent diagram&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Uses the &lt;a href=&#34;https://react-lm.github.io/&#34;&gt;ReAct agent&lt;/a&gt; to plan and execute a query given the tools provided. This type of agent can reason over which of the tools to use next to further the response, feed inputs into the selected tool, and repeat the process with the output until it determines that the answer is satisfactory.&lt;/li&gt; &#xA; &lt;li&gt;Current tool suite: While we didn&#39;t emphasize equipping the agent with many tools, we hope to see memary help agents in the community equipped with a vast array of tools covering multi-modalities. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Location&lt;/strong&gt; - determines the user&#39;s current location and nearby surroundings using geocoder and googlemaps.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;CV&lt;/strong&gt; - answers a query based on a provided image using gpt-4-vision-preview.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Search&lt;/strong&gt; - queries the knowledge graph for a response based on existing nodes and executes an external search if no related entities exist.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;How does it work? &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Takes in each query → selects a tool → executes and finds an answer to current step → repeats this process until it reaches a satisfactory answer.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Purpose in larger system &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Each response from the agent is saved in the knowledge graph. You can view responses from various tools as distinct elements that contribute to the user&#39;s knowledge.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Future contributions &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Make your own agent and add as many tools as possible! Each tool expands the agent&#39;s ability to answer a wide variety of queries.&lt;/li&gt; &#xA;   &lt;li&gt;Create an LLM Judge that scores the routing agent and provides feedback.&lt;/li&gt; &#xA;   &lt;li&gt;Integrate multiprocessing so that the agent can process multiple sub-queries simultaneously. We have open-sourced the query decomposition and reranking code to help with this!&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Knowledge Graph&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/kingjulio8238/memary/main/diagrams/kg.png&#34; alt=&#34;KG diagram&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;What are knowledge graphs (KG)? &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;KGs are databases that store information in the form of entities, which can be anything from objects to more abstract concepts and their relationships with one another.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;KGs vs other knowledge stores &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;KGs provide more depth of essential context that can be easily retrieved.&lt;/li&gt; &#xA;   &lt;li&gt;The knowledge store&#39;s graph structure allows information to be centered around certain entities and their relationships with other entities, thus ensuring that the context of the information is relevant.&lt;/li&gt; &#xA;   &lt;li&gt;KGs are more adept at handling complex queries, as the varying relationships between different entities in the query can provide insight into how to join multiple subgraphs together.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Knowledge graphs ↔ LLMs &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;memary uses a Neo4j graph database to store knoweldge.&lt;/li&gt; &#xA;   &lt;li&gt;Llamaindex was used to add nodes into the graph store based on documents.&lt;/li&gt; &#xA;   &lt;li&gt;Perplexity (mistral-7b-instruct model) was used for external queries.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;What can one do with the KG? &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Inject the final agent responses into existing KGs.&lt;/li&gt; &#xA;   &lt;li&gt;memary uses a &lt;a href=&#34;https://arxiv.org/pdf/2401.18059.pdf&#34;&gt;recursive retrieval&lt;/a&gt; approach to search the KG, which involves determining what the key entities are in the query, building a subgraph of those entities with a maximum depth of 2 away, and finally using that subgraph to build up the context.&lt;/li&gt; &#xA;   &lt;li&gt;When faced with multiple key entities in a query, memary uses &lt;a href=&#34;https://neo4j.com/developer-blog/knowledge-graphs-llms-multi-hop-question-answering/&#34;&gt;multi-hop&lt;/a&gt; reasoning to join multiple subgraphs into a larger subgraph to search through.&lt;/li&gt; &#xA;   &lt;li&gt;These techniques reduce latency compared to searching the entire knowledge graph at once.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Purpose in larger system &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Continuously update the memory module with each node insertion.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Future contributions &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Expand the graph’s capabilities to support multiple modalities, i.e., images.&lt;/li&gt; &#xA;   &lt;li&gt;Graph optimizations to reduce latency of search times.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Memory Module&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/kingjulio8238/memary/assets/120517860/5bf361a4-84e5-4a93-bc9b-aa9e42c3dac3&#34; alt=&#34;Memory Module&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;What is the memory module?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The memory module comprises the Memory Stream and Entity Knowledge Store. The memory module was influenced by the design of &lt;a href=&#34;https://arxiv.org/pdf/2311.06318.pdf&#34;&gt;K-LaMP&lt;/a&gt; proposed by Microsoft Research.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;The Memory Stream captures all entities inserted into the KG and their associated timestamps. This stream reflects the breadth of the users&#39; knowledge, i.e., concepts users have had exposure to but no depth of exposure is inferred. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Timeline Analysis: Map out a timeline of interactions, highlighting moments of high engagement or shifts in topic focus. This helps in understanding the evolution of the user&#39;s interests over time.&lt;/li&gt; &#xA;   &lt;li&gt;Extract Themes: Look for recurring themes or topics within the interactions. This thematic analysis can help anticipate user interests or questions even before they are explicitly stated.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;The Entity Knowledge Store tracks the frequency and recency of references to each entity stored in the memory stream. This knowledge store reflects users&#39; depth of knowledge, i.e., concepts they are more familiar with than others. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Rank Entities by Relevance: Use both frequency and recency to rank entities. An entity frequently mentioned (high count) and referenced recently is likely of high importance, and the user is well aware of this concept.&lt;/li&gt; &#xA;   &lt;li&gt;Categorize Entities: Group entities into categories based on their nature or the context in which they&#39;re mentioned (e.g., technical terms, personal interests). This categorization aids in quickly accessing relevant information tailored to the user&#39;s inquiries.&lt;/li&gt; &#xA;   &lt;li&gt;Highlight Changes Over Time: Identify any significant changes in the entities&#39; ranking or categorization over time. A shift in the most frequently mentioned entities could indicate a change in the user&#39;s interests or knowledge.&lt;/li&gt; &#xA;   &lt;li&gt;Additional information on the memory modules can be found &lt;a href=&#34;https://github.com/seyeong-han/KnowledgeGraphRAG&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Purpose in larger system &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Compress/summarize the top N ranked entities in the entity knowledge store and pass to the LLM’s finite context window alongside the agent&#39;s response and chat history for inference.&lt;/li&gt; &#xA;   &lt;li&gt;Personalize Responses: Use the key categorized entities and themes associated with the user to tailor agent responses more closely to the user&#39;s current interests and knowledge level/expertise.&lt;/li&gt; &#xA;   &lt;li&gt;Anticipate Needs: Leverage trends and shifts identified in the summaries to anticipate users&#39; future questions or needs.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Future contributions &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;We currently extract the top N entities from the entitiy knowledge store and pass these entities into the context window for inference. memary can future benefit from more advanced memory compression techniques such as passing only entities that are in the agent&#39;s response to the context window. We look forward to related community contributions.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/kingjulio8238/memary/assets/120517860/eb911941-9ec0-492f-a47d-5b4196508a1b&#34; alt=&#34;Memory Compression&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Future Integrations&lt;/h2&gt; &#xA;&lt;p&gt;Currently memary is structured so that the ReAct agent can only process one query at a time. We hope to see &lt;strong&gt;multiprocessing&lt;/strong&gt; integrated so that the agent can process many subqueries simultaneously. We expect this to improve the relevancy and accuracy of responses. The source code for both decomposing the query and reranking the many agent responses has been provided, and once multiprocessing has been added to the system, these components can easily be integrated into the main &lt;code&gt;ChatAgent&lt;/code&gt; class. The diagram below shows how the newly integrated system would work.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/kingjulio8238/memary/main/diagrams/final.png&#34; alt=&#34;Future Integrations&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Query Decomposition&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/kingjulio8238/memary/assets/120517860/e8663b07-66c4-4c08-82d3-cef8eb9c2554&#34; alt=&#34;QD Diagram&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;What is query decomposition?&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;A preprocessing technique that breaks down complex queries into simpler queries to expedite the LLM’s ability to answer the prompt. It is important to note that this process leaves simple queries unchanged.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Why decompose?&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;User queries are complex and multifaceted, and base-model LLMs are often unable to fully understand all aspects of the query in order to create a succinct and accurate response.&lt;/li&gt; &#xA;   &lt;li&gt;Allows an LLM of similar capabilities to answer easier questions and synthesize those answers to provide an improved response.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;How it works&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Initially, a LlamaIndex fine-tuned query engine approach was taken. However, the LangChain query engine was found to be faster and easier to use. LangChain’s &lt;code&gt;PydanticToolsParser&lt;/code&gt; framework was used. The query_engine_with_examples has been given 87 pre-decomposed queries (complex query + set of subqueries) to determine a pattern. Users can invoke the engine with individual queries or collate them into a list and invoke them by batch.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Individual Invocation: &lt;code&gt;sub_qs = query_analyzer_with_examples.invoke( {&#34;question&#34;: &#34;What is 2 + 2? Why is it not 3?&#34;} )&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Batch Invocation: &lt;code&gt;questions = [ &#34;Where can I buy a Macbook Pro with an M3 chip? What is the difference to the M2 chip? How much more expensive is the M3?&#34;, &#34;How can I buy tickets to the upcoming NBA game? What is the price of lower bowl seats versus nosebleeds? What is the view like at either seat?&#34;, &#34;Between a Macbook and a Windows machine, which is better for systems engineering? Which chips are most ideal? What is the price difference between the two?&#34;,] &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;responses = [] for question in questions: responses.append(query_analyzer_with_examples.invoke({&#34;question&#34;: question}))&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Purpose in larger system&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;In a parallel system, the agent will be able to parse multiple queries at once. The query decomposer (QD) will pass all subqueries (or original query if no subqueries exist) to the agent at once.&lt;/li&gt; &#xA;   &lt;li&gt;Simultaneously, QD will pass the original query to the reranking module to rerank the agent responses based on their relevance to the pre-decomposed query.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Future contributions&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Once agent multiprocessing is integrated, QD will be valuable to leverage. All user queries will be passed to QD, and the (sub)queries wil be passed to the routing agent for parallel processing.&lt;/li&gt; &#xA;   &lt;li&gt;Self-Learning: Whenever queries are decomposed, those examples will be appended to the engine’s example store as a feedback loop for improved future performance.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Reranking&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/kingjulio8238/memary/assets/120517860/3f15b40f-c591-43ab-aa10-727b6997727d&#34; alt=&#34;Reranking Diagram&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;What is reranking? &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Reranking is the process of scoring nodes based on their relevancy.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Why rerank agent responses? &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Ensure that the various responses to subqueries, when merged, are relevant to the original query prior to decomposition.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Our Approach &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;We benchmarked three models to determine which one would best work for reranking: BM25 Reranking Fusion, Cohere Rerank, and ColBERT Rerank. After testing BM25, it was clear that the model was not able to classify the different responses and provide a simple merged answer. Instead of answering the question, it combined all the information on the page, introducing irrelevant information.&lt;/li&gt; &#xA;   &lt;li&gt;Next, when testing out Cohere, the model performed better than BM25 but was still not classifying the paragraphs well. The reranking was not always accurate, as it performed well for some questions but was not able to rank others. Furthermore, the ranking was still pretty inaccurate, performing between 0.25 - 0.5 out of 1.&lt;/li&gt; &#xA;   &lt;li&gt;Finally, we tested ColBERT rerank, and it was found that this model performed best compared to the other two. ColBERT was able to synthesize results from the given data and ranked them very accurately, with reranking scores between 0.6 - 0.7 out of 1. With this, ColBERT had the most potential, being able to determine which responses were most related and important to answering the query.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Purpose in larger system &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Passes the reranking result to the knowledge graph for storage and to the model as one source of context for inference.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Future contributions &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Once agent multiprocessing is integrated, reranking can be integrated into the &lt;code&gt;ChatAgent&lt;/code&gt; class.&lt;/li&gt; &#xA;   &lt;li&gt;Future Benchmarking: Include the Cohere Rerank 3 model and others in the &lt;a href=&#34;https://docs.google.com/document/d/1gHzvgktqnHcg7wbIuKHr6W5NMYk6UVlJkRQfSqzk9e4/edit&#34;&gt;reranking analysis&lt;/a&gt;. The data used for benchmarking can be found &lt;a href=&#34;https://docs.google.com/document/d/1knfJRsoEzjKziilmF_ZwSwMRBvYbF0yNlRdpDteDiW4/edit?usp=sharing&#34;&gt;here&lt;/a&gt;. Add to it!&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions from the community and hope to see memary advance as agents do!&lt;/p&gt; &#xA;&lt;p&gt;Initial Contributors: &lt;a href=&#34;https://www.linkedin.com/in/juliansaks/&#34;&gt;Julian Saks&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/kevin-li8/&#34;&gt;Kevin Li&lt;/a&gt;, &lt;a href=&#34;https://github.com/seyeong-han&#34;&gt;Seyeong Han&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/arnav-chopra/&#34;&gt;Arnav Chopra&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/aishwarya--balaji/&#34;&gt;Aishwarya Balaji&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/anshusiripurapu/&#34;&gt;Anshu Siripurapu&lt;/a&gt; (Hook &#39;em!)&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>StableFluffy/EasyLLMFeaturePorter</title>
    <updated>2024-05-01T01:32:05Z</updated>
    <id>tag:github.com,2024-05-01:/StableFluffy/EasyLLMFeaturePorter</id>
    <link href="https://github.com/StableFluffy/EasyLLMFeaturePorter" rel="alternate"></link>
    <summary type="html">&lt;p&gt;1-Click is all you need.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;EasyLLMFeaturePorter&lt;/h1&gt; &#xA;&lt;p&gt;1-Click is all you need.&lt;/p&gt; &#xA;&lt;h2&gt;Methodology&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Get diff between base model and context expanded model.&lt;/li&gt; &#xA; &lt;li&gt;Get diff between base model and finetuned model(model to expand context). Then use this diff to calculate activation ratio.&lt;/li&gt; &#xA; &lt;li&gt;Add diff * (1 - activation ratio) to finetuned model.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Done !&lt;/p&gt; &#xA;&lt;h3&gt;Explanation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arca.live/b/alpaca/104827551&#34;&gt;Korean&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/blog/maywell/llm-feature-transfer&#34;&gt;English&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Special Thanks&lt;/h3&gt; &#xA;&lt;p&gt;Thanks to kuotient for letting me know issue on initial commit. &lt;a href=&#34;https://huggingface.co/kuotient&#34;&gt;Huggingface&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Thanks to &lt;a href=&#34;https://sionic.ai/&#34;&gt;Sionic Ai&lt;/a&gt; for providing A100 cluster.&lt;/p&gt;</summary>
  </entry>
</feed>