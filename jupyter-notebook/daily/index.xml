<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-21T01:39:33Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Beomi/KoAlpaca</title>
    <updated>2023-03-21T01:39:33Z</updated>
    <id>tag:github.com,2023-03-21:/Beomi/KoAlpaca</id>
    <link href="https://github.com/Beomi/KoAlpaca" rel="alternate"></link>
    <summary type="html">&lt;p&gt;KoAlpaca: Korean Alpaca Model based on Stanford Alpaca (feat. LLAMA and Polyglot-ko)&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Beomi/KoAlpaca/main/assets/KoAlpaca.png&#34; alt=&#34;KoAlpaca icon&#34; style=&#34;width: 200px; height:200px; display: block; margin: auto; border-radius: 50%;&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;KoAlpaca: Korean Alpaca Model based on Stanford Alpaca (feat. LLAMA and Polyglot-ko)&lt;/h1&gt; &#xA;&lt;p&gt;Stanford Alpaca 모델을 학습한 방식과 동일한 방식으로 학습을 진행한, 한국어 Alpaca 모델입니다.&lt;/p&gt; &#xA;&lt;h2&gt;바로 써보기: Telegram Bot으로 만나보세요!&lt;/h2&gt; &#xA;&lt;p&gt;아래 QR코드를 찍거나, 혹은 &lt;a href=&#34;https://t.me/KoAlpacaBot&#34;&gt;https://t.me/KoAlpacaBot&lt;/a&gt;에서 만나보세요!&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Beomi/KoAlpaca/main/assets/koalpaca_telegram.jpg&#34; alt=&#34;KoAlpaca Telegram Bot QR Code&#34; style=&#34;width: 50%; max-width: 300px; display: block; margin: auto;&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;한국어 모델과 영한 모델 모두 변경해가며 체험하실 수 있습니다!&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Beomi/KoAlpaca/main/assets/telegram_example_1.png&#34; alt=&#34;KoAlpaca Telegram Bot QR Code&#34; style=&#34;width: 45%; max-width: 300px; display: inline-block; margin: auto;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Beomi/KoAlpaca/main/assets/telegram_example_2.png&#34; alt=&#34;KoAlpaca Telegram Bot QR Code&#34; style=&#34;width: 45%; max-width: 300px; display: inline-block; margin: auto;&#34;&gt; &lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;📣 각 모델별 한 대의 GPU로 서빙되고 있어서, 생성에 시간이 걸릴 수 있습니다. 속도를 위해 생성 최대 토큰은 요청당 128토큰으로 제한되어있습니다.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;완성된 모델: 한국어 모델(Polyglot-ko) &amp;amp; 영한 모델(LLAMA)&lt;/h2&gt; &#xA;&lt;p&gt;KoAlpaca는 백본 모델로 두 가지 모델을 사용했습니다.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Polyglot-ko 5.8B 기반 -&amp;gt; &lt;a href=&#34;https://huggingface.co/beomi/KoAlpaca-Polyglot&#34;&gt;https://huggingface.co/beomi/KoAlpaca-Polyglot&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Meta LLAMA 7B 기반 -&amp;gt; &lt;a href=&#34;https://huggingface.co/beomi/KoAlpaca&#34;&gt;https://huggingface.co/beomi/KoAlpaca&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Meta의 LLAMA 모델은 한국어 데이터셋을 충분히 학습하지 않아서, 실제 Inference를 돌려보았을 때 한국어 성능이 낮게 나오는 이슈가 있습니다.&lt;/p&gt; &#xA;&lt;p&gt;보다 한국어에 나은 퀄리티를 얻어보고자 Polyglot-ko 5.8B 모델을 백본으로 사용한 두 가지 모델을 학습시켰습니다.&lt;/p&gt; &#xA;&lt;h2&gt;LLAMA 모델 Inference 예시 코드&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Beomi/KoAlpaca/main/Inference%20Test.ipynb&#34;&gt;Inference Test.ipynb&lt;/a&gt; 파일을 참고해주세요.&lt;/p&gt; &#xA;&lt;h2&gt;데이터셋 제작 방법&lt;/h2&gt; &#xA;&lt;p&gt;데이터셋은 기본적으로 &lt;a href=&#34;https://raw.githubusercontent.com/Beomi/KoAlpaca/main/en_alpaca_data.json&#34;&gt;Stanford Alpaca에서 제공한 5만2천개 데이터셋&lt;/a&gt;을 기반으로 합니다.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;데이터셋 번역&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Alpaca 데이터셋은 다음과 같이 Instruct 부분과 Input, 그리고 Output 부분으로 구성되어있습니다.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;...&#xA;{&#xA;    &#34;instruction&#34;: &#34;Describe a time when you had to make a difficult decision.&#34;,&#xA;    &#34;input&#34;: &#34;&#34;,&#xA;    &#34;output&#34;: &#34;I had to make a difficult decision when I was working as a project manager at a construction company. I was in charge of a project that needed to be completed by a certain date in order to meet the client\u2019s expectations. However, due to unexpected delays, we were not able to meet the deadline and so I had to make a difficult decision. I decided to extend the deadline, but I had to stretch the team\u2019s resources even further and increase the budget. Although it was a risky decision, I ultimately decided to go ahead with it to ensure that the project was completed on time and that the client\u2019s expectations were met. The project was eventually successfully completed and this was seen as a testament to my leadership and decision-making abilities.&#34;&#xA;},&#xA;{&#xA;    &#34;instruction&#34;: &#34;Identify the odd one out.&#34;,&#xA;    &#34;input&#34;: &#34;Twitter, Instagram, Telegram&#34;,&#xA;    &#34;output&#34;: &#34;Telegram&#34;&#xA;},&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Output의 경우 OpenAI의 &lt;code&gt;text-davinci-003&lt;/code&gt; 모델의 생성 결과이기 때문에, 해당 부분은 번역하지 않았습니다.&lt;/p&gt; &#xA;&lt;p&gt;따라서 Instruct와 Input 컬럼을 번역하고, 해당 Input부분에 에러가 있는 일부를 제외하였습니다.&lt;/p&gt; &#xA;&lt;p&gt;번역을 하기 위해 DeepL API 서비스를 사용하였고, 번역된 데이터는 다음과 같은 형태가 됩니다.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;...&#xA;{&#xA;    &#34;instruction&#34;: &#34;홀수 중 하나를 밝히세요.&#34;,&#xA;    &#34;input&#34;: &#34;트위터, 인스타그램, 텔레그램&#34;&#xA;}&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Output 데이터 생성&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;이후 해당 Instruct와 Input은 다음과 같이 Batch Decoding 기법을 통해서 OpenAI ChatGPT API(&lt;code&gt;gpt-3.5-turbo&lt;/code&gt;)를 통해 데이터를 생성합니다.&lt;/p&gt; &#xA;&lt;p&gt;Alpaca 연구와 동일하게, 한 Instruct/Input 셋에 대해서 단일 응답만을 생성해 데이터셋을 구성하였습니다.&lt;/p&gt; &#xA;&lt;p&gt;기존 연구에서는 20개의 데이터셋을 한번에 생성했지만, 한국어로 생성을 진행해보았을 때 10개를 동시에 생성할 때 안정적으로 생성이 진행되어 10개씩 생성하였습니다.&lt;/p&gt; &#xA;&lt;p&gt;아래는 답변을 생성하는데 사용한 Prompt입니다.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;PROMPT = &#34;&#34;&#34;\&#xA;다양한 작업에 대한 답변을 생성해주세요. 이러한 작업 지침은 ChatGPT 모델에 주어지며, ChatGPT 모델이 지침을 완료하는지 평가합니다.&#xA;&#xA;요구 사항은 다음과 같습니다:&#xA;1. 다양성을 극대화하기 위해 각 지시에 대해 동사를 반복하지 않도록 하세요.&#xA;2. 지시에 사용되는 언어도 다양해야 합니다. 예를 들어, 질문과 명령형 지시를 결합해야 합니다.&#xA;3. 지시 사항의 유형이 다양해야 합니다. 목록에는 개방형 생성, 분류, 편집 등과 같은 다양한 유형의 작업이 포함되어야 합니다.&#xA;2. GPT 언어 모델은 지시를 완료할 수 있어야 합니다. 예를 들어 어시스턴트에게 시각적 또는 오디오 출력을 생성하도록 요청하지 마세요. 또 다른 예로, 어시스턴트가 어떤 작업도 수행할 수 없으므로 오후 5시에 깨우거나 미리 알림을 설정하도록 요청하지 마세요.&#xA;3. 답변은 한국어로 작성해야 합니다.&#xA;4. 답변을 1~2문장으로 작성하세요. 명령문이나 질문도 허용됩니다.&#xA;5. 지시 사항에 대한 적절한 입력을 생성해야 합니다. 입력 필드에는 지시에 대한 구체적인 예가 포함되어야 합니다. 실제 데이터를 포함해야 하며 단순한 자리 표시자를 포함해서는 안 됩니다. 입력은 지시 사항을 어렵게 만들 수 있는 상당한 내용을 제공해야 하지만 100단어를 넘지 않는 것이 이상적입니다.&#xA;6. 일부 지시사항은 추가 입력이 있고, 일부 지시에는 입력 필드가 비어있습니다. 예를 들어 &#34;세계에서 가장 높은 봉우리는 무엇인가?&#34;라는 일반적인 정보를 묻는 지시의 경우 구체적인 맥락을 제공할 필요가 없어, 입력 필드가 비어있을 수 있습니다.&#xA;7. 출력은 명령어와 입력에 대한 적절한 응답이어야 합니다.&#xA;&#xA;아래에 10개의 명령어와 입력(옵션)에 따라 적절한 응답을 생성하세요. &#xA;응답은 아래와 같은 형식으로 10가지를 0번 부터 9번 까지, 번호에 따라 해당 번호의 명령어와 입력에 알맞게 작성하세요.&#xA;&#xA;각 응답 사이는 ### 으로 내용을 분리해주세요.&#xA;&#xA;응답0: 첫 번째 응답내용###&#xA;응답1: 두 번째 응답내용###&#xA;...&#xA;응답9: 마지막 응답내용&#34;&#34;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;추가적으로, 아래와 같이 ChatGPT API의 system prompt를 추가하였습니다.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#34;content&#34;: &#34;아래는 작업을 설명하는 명령어입니다. 입력이 없으면 입력을 제외하고, 명령어에 따른 요청을 적절히 완료하는 응답을 작성하세요. 추가적 입력이 있다면 작업을 설명하는 명령어와 추가 컨텍스트를 제공하는 입력에 따라 요청을 적절히 완료하는 응답을 작성하세요.&#34;,&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;한국어로 생성이 완료된 데이터셋은 &lt;code&gt;ko_alpaca_data.json&lt;/code&gt;에 저장되어 있습니다.&lt;/p&gt; &#xA;&lt;h2&gt;모델 학습 방법&lt;/h2&gt; &#xA;&lt;h3&gt;LLAMA 7B 모델 학습&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Beomi/KoAlpaca/main/assets/ko_alpaca_llama_train_02.jpeg&#34; alt=&#34;KoAlpaca LLAMA Train Loss Graph&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;모델 학습은 A100 80GB 4대로 학습을 진행하였습니다. 학습에 사용한 스크립트는 &lt;code&gt;train.py&lt;/code&gt;에 저장되어 있고, 학습에 사용한 스크립트는 &lt;code&gt;train.sh&lt;/code&gt;에 저장되어 있습니다.&lt;/p&gt; &#xA;&lt;h4&gt;모델 학습시 유의사항&lt;/h4&gt; &#xA;&lt;p&gt;아래에도 적혀있지만, 현재(2023.03.18기준) LLAMA 모델은 Huggingface &lt;code&gt;main&lt;/code&gt; 브랜치에만 있습니다. 또한, Layer 명칭이 지속적으로 바뀌고 있어 주의가 필요합니다. 현재는 &lt;code&gt;LlamaDecoderLayer&lt;/code&gt;로 되어있습니다.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# train.sh&#xA;--fsdp_transformer_layer_cls_to_wrap &#39;LlamaDecoderLayer&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Polyglot-ko 5.8B 모델 학습&lt;/h3&gt; &#xA;&lt;p&gt;모델 학습은 A100 80GB 1대로 학습을 진행하였습니다.&lt;/p&gt; &#xA;&lt;p&gt;*내용보충 예정&lt;/p&gt; &#xA;&lt;h2&gt;NSMC Benchmark Test&lt;/h2&gt; &#xA;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/Beomi/KoAlpaca/blob/master/nsmc_polyglot_and_koalpaca_by_few_shot.ipynb&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt; &lt;/a&gt; &#xA;&lt;h3&gt;Benchmark Test 방식&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;polyglot-ko 5.8b&lt;/code&gt;와 &lt;code&gt;KoAlpaca-polyglot&lt;/code&gt;과 Benchmark 성능 비교&lt;/li&gt; &#xA; &lt;li&gt;Few-shot Learning으로 2 가지 프롬프트 유형으로 구분하여 테스트 진행&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;1. Few-shot Learning 구성&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;few-shot 구성&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;k=10&lt;/code&gt; 설정&lt;/li&gt; &#xA;   &lt;li&gt;텍스트 토큰 길이가 25 이하인 문장만 구성&lt;/li&gt; &#xA;   &lt;li&gt;데이터 토큰 길이의 약 75 퍼센타일에 해당하는 데이터만 포함&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;NSMC 데이터 토큰 길이 분포&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;Few shot 케이스 토큰 평균 길이:  20.229&#xA;Few shot 케이스 토큰 최대 길이:  280&#xA;Few shot 케이스 토큰 길이 표준편차:  16.488&#xA;Few shot 케이스 토큰 길이 80 퍼센타일:  27.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;2. 프롬프트 구성&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;프롬프트 유형 1: 매우 단순한 프롬프트 구성. 분석 Task에 대한 내용을 명시적으로 하지 않음&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def build_prompt_text(sent):&#xA;    return &#34;문장: &#34; + sent + &#39;\n감정:&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;프롬프트 유형 2: 프롬프트 유형 1에 비해 Task 의도 내용을 포함. 분석 Task에 대한 내용을 의문형으로 표현&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def build_prompt_text(sent):&#xA;    return &#39;다음 문장은 긍정일까요 부정일까요?\n&#39; + sent + &#39;\n정답:&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;3. Benchmark Test 결과&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;acc.&lt;/th&gt; &#xA;   &lt;th&gt;프롬프트 유형 1&lt;/th&gt; &#xA;   &lt;th&gt;프롬프트 유형 2&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;polyglot-ko&lt;/td&gt; &#xA;   &lt;td&gt;0.5752&lt;/td&gt; &#xA;   &lt;td&gt;0.7223&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;koalpaca-polyglot&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.6939&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.7683&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Stanford Alpaca: An Instruction-following LLaMA Model&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache_2.0-green.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.python.org/downloads/release/python-390/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.9+-blue.svg?sanitize=true&#34; alt=&#34;Python 3.9+&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/psf/black&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true&#34; alt=&#34;Code style: black&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is the repo for the Stanford Alpaca project, which aims to build and share an instruction-following LLaMA model. The repo contains:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A &lt;a href=&#34;https://crfm.stanford.edu/alpaca/&#34;&gt;&lt;strong&gt;web demo&lt;/strong&gt;&lt;/a&gt; to interact with our Alpaca model&lt;/li&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/Beomi/KoAlpaca/main/#data-release&#34;&gt;52K data&lt;/a&gt; used for fine-tuning the model&lt;/li&gt; &#xA; &lt;li&gt;The code for &lt;a href=&#34;https://raw.githubusercontent.com/Beomi/KoAlpaca/main/#data-generation-process&#34;&gt;generating the data&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;The code for &lt;a href=&#34;https://raw.githubusercontent.com/Beomi/KoAlpaca/main/#fine-tuning&#34;&gt;fine-tuning the model&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;The current Alpaca model is fine-tuned from a 7B LLaMA model [1] on 52K instruction-following data generated by the techniques in the Self-Instruct [2] paper, with some modifications that we discuss in the next section. In a preliminary human evaluation, we found that the Alpaca 7B model behaves similarly to the &lt;code&gt;text-davinci-003&lt;/code&gt; model on the Self-Instruct instruction-following evaluation suite [2].&lt;/p&gt; &#xA;&lt;p&gt;Alpaca is still under development, and there are many limitations that have to be addressed. Importantly, we have not yet fine-tuned the Alpaca model to be safe and harmless. We thus encourage users to be cautious when interacting with Alpaca, and to report any concerning behavior to help improve the safety and ethical considerations of the model.&lt;/p&gt; &#xA;&lt;p&gt;Our initial release contains the data generation procedure, dataset, and training recipe. We intend to release the model weights if we are given permission to do so by the creators of LLaMA. For now, we have chosen to host a live demo to help readers better understand the capabilities and limits of Alpaca, as well as a way to help us better evaluate Alpaca&#39;s performance on a broader audience.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Please read our release &lt;a href=&#34;https://crfm.stanford.edu/2023/03/13/alpaca.html&#34;&gt;blog post&lt;/a&gt; for more details about the model, our discussion of the potential harm and limitations of Alpaca models, and our thought process for releasing a reproducible model.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;[1]: LLaMA: Open and Efficient Foundation Language Models. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample. &lt;a href=&#34;https://arxiv.org/abs/2302.13971v1&#34;&gt;https://arxiv.org/abs/2302.13971v1&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[2]: Self-Instruct: Aligning Language Model with Self Generated Instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, Hannaneh Hajishirzi. &lt;a href=&#34;https://arxiv.org/abs/2212.10560&#34;&gt;https://arxiv.org/abs/2212.10560&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Data Release&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Beomi/KoAlpaca/main/alpaca_data.json&#34;&gt;&lt;code&gt;alpaca_data.json&lt;/code&gt;&lt;/a&gt; contains 52K instruction-following data we used for fine-tuning the Alpaca model. This JSON file is a list of dictionaries, each dictionary contains the following fields:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;instruction&lt;/code&gt;: &lt;code&gt;str&lt;/code&gt;, describes the task the model should perform. Each of the 52K instructions is unique.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;input&lt;/code&gt;: &lt;code&gt;str&lt;/code&gt;, optional context or input for the task. For example, when the instruction is &#34;Summarize the following article&#34;, the input is the article. Around 40% of the examples have an input.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;output&lt;/code&gt;: &lt;code&gt;str&lt;/code&gt;, the answer to the instruction as generated by &lt;code&gt;text-davinci-003&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We used the following prompts for fine-tuning the Alpaca model:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;for examples with a non-empty input field:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.&#xA;&#xA;### Instruction:&#xA;{instruction}&#xA;&#xA;### Input:&#xA;{input}&#xA;&#xA;### Response:&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;for examples with an empty input field:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;Below is an instruction that describes a task. Write a response that appropriately completes the request.&#xA;&#xA;### Instruction:&#xA;{instruction}&#xA;&#xA;### Response:&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;During inference (eg for the web demo), we use the user instruction with an empty input field (second option).&lt;/p&gt; &#xA;&lt;h2&gt;Data Generation Process&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; &lt;strong&gt; Running the code &lt;/strong&gt; &lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Set environment variables &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; to your OpenAI API key.&lt;/li&gt; &#xA;  &lt;li&gt;Install the dependencies with &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;Run &lt;code&gt;python -m generate_instruction generate_instruction_following_data&lt;/code&gt; to generate the data.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;We built on the data generation pipeline from &lt;a href=&#34;https://github.com/yizhongw/self-instruct&#34;&gt;self-instruct&lt;/a&gt; and made the following modifications:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We used &lt;code&gt;text-davinci-003&lt;/code&gt; to generate the instruction data instead of &lt;code&gt;davinci&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;We wrote a new prompt (&lt;code&gt;prompt.txt&lt;/code&gt;) that explicitly gave the requirement of instruction generation to &lt;code&gt;text-davinci-003&lt;/code&gt;. Note: there is a slight error in the prompt we used, and future users should incorporate the edit in &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca/pull/24&#34;&gt;https://github.com/tatsu-lab/stanford_alpaca/pull/24&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;We adopted much more aggressive batch decoding, i.e., generating 20 instructions at once, which significantly reduced the cost of data generation.&lt;/li&gt; &#xA; &lt;li&gt;We simplified the data generation pipeline by discarding the difference between classification and non-classification instructions.&lt;/li&gt; &#xA; &lt;li&gt;We only generated a single instance for each instruction, instead of 2 to 3 instances as in [1].&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This produced an instruction-following dataset with 52K examples obtained at a much lower cost (less than $500). In a preliminary study, we also find our 52K generated data to be much more diverse than the data released by &lt;a href=&#34;https://github.com/yizhongw/self-instruct/raw/main/data/seed_tasks.jsonl&#34;&gt;self-instruct&lt;/a&gt;. We plot the below figure (in the style of Figure 2 in the &lt;a href=&#34;https://arxiv.org/abs/2212.10560&#34;&gt;self-instruct paper&lt;/a&gt; to demonstrate the diversity of our data. The inner circle of the plot represents the root verb of the instructions, and the outer circle represents the direct objects.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Beomi/KoAlpaca/main/assets/parse_analysis.png&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Beomi/KoAlpaca/main/assets/parse_analysis.png&#34; width=&#34;750&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Fine-tuning&lt;/h2&gt; &#xA;&lt;p&gt;We fine-tune our models using standard Hugging Face training code with the following hyperparameters:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Hyperparameter&lt;/th&gt; &#xA;   &lt;th&gt;Value&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Batch size&lt;/td&gt; &#xA;   &lt;td&gt;128&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Learning rate&lt;/td&gt; &#xA;   &lt;td&gt;2e-5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Epochs&lt;/td&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Max length&lt;/td&gt; &#xA;   &lt;td&gt;512&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Weight decay&lt;/td&gt; &#xA;   &lt;td&gt;0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Given Hugging Face hasn&#39;t officially supported the LLaMA models, we fine-tuned LLaMA with Hugging Face&#39;s transformers library by installing it from a particular fork (i.e. this &lt;a href=&#34;https://github.com/huggingface/transformers/pull/21955&#34;&gt;PR&lt;/a&gt; to be merged). The hash of the specific commit we installed was &lt;code&gt;68d640f7c368bcaaaecfc678f11908ebbd3d6176&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To reproduce our fine-tuning runs for LLaMA, first install the requirements&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, install the particular fork of Hugging Face&#39;s transformers library.&lt;/p&gt; &#xA;&lt;p&gt;Below is a command that fine-tunes LLaMA-7B with our dataset on a machine with 4 A100 80G GPUs in FSDP &lt;code&gt;full_shard&lt;/code&gt; mode. We were able to reproduce a model of similar quality as the one we hosted in our demo with the following command using &lt;strong&gt;Python 3.10&lt;/strong&gt;. Replace &lt;code&gt;&amp;lt;your_random_port&amp;gt;&lt;/code&gt; with a port of your own, &lt;code&gt;&amp;lt;your_path_to_hf_converted_llama_ckpt_and_tokenizer&amp;gt;&lt;/code&gt; with the path to your converted checkpoint and tokenizer (following instructions in the PR), and &lt;code&gt;&amp;lt;your_output_dir&amp;gt;&lt;/code&gt; with where you want to store your outputs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;torchrun --nproc_per_node=4 --master_port=&amp;lt;your_random_port&amp;gt; train.py \&#xA;    --model_name_or_path &amp;lt;your_path_to_hf_converted_llama_ckpt_and_tokenizer&amp;gt; \&#xA;    --data_path ./alpaca_data.json \&#xA;    --bf16 True \&#xA;    --output_dir &amp;lt;your_output_dir&amp;gt; \&#xA;    --num_train_epochs 3 \&#xA;    --per_device_train_batch_size 4 \&#xA;    --per_device_eval_batch_size 4 \&#xA;    --gradient_accumulation_steps 8 \&#xA;    --evaluation_strategy &#34;no&#34; \&#xA;    --save_strategy &#34;steps&#34; \&#xA;    --save_steps 2000 \&#xA;    --save_total_limit 1 \&#xA;    --learning_rate 2e-5 \&#xA;    --weight_decay 0. \&#xA;    --warmup_ratio 0.03 \&#xA;    --lr_scheduler_type &#34;cosine&#34; \&#xA;    --logging_steps 1 \&#xA;    --fsdp &#34;full_shard auto_wrap&#34; \&#xA;    --fsdp_transformer_layer_cls_to_wrap &#39;LLaMADecoderLayer&#39; \&#xA;    --tf32 True&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Warning&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;fsdp_transformer_layer_cls_to_wrap&lt;/code&gt; must be set to the name of the specific decoder layer. The LLaMA Hugging Face PR is not stable. Earlier commits used the name &lt;code&gt;LLaMADecoderLayer&lt;/code&gt; for their decoder layer (the commit hash our code is based on this). More recent commits use &lt;code&gt;LlamaDecoderLayer&lt;/code&gt; (notice the small case difference). Not setting &lt;code&gt;fsdp_transformer_layer_cls_to_wrap&lt;/code&gt; to the correct name will lead to drastic slowdowns in training.&lt;/p&gt; &#xA;&lt;h3&gt;Side notes&lt;/h3&gt; &#xA;&lt;p&gt;The same script also works for OPT fine-tuning. Here&#39;s an example for fine-tuning OPT-6.7B&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;torchrun --nproc_per_node=4 --master_port=&amp;lt;your_random_port&amp;gt; train.py \&#xA;    --model_name_or_path &#34;facebook/opt-6.7b&#34; \&#xA;    --data_path ./alpaca_data.json \&#xA;    --bf16 True \&#xA;    --output_dir &amp;lt;your_output_dir&amp;gt; \&#xA;    --num_train_epochs 3 \&#xA;    --per_device_train_batch_size 4 \&#xA;    --per_device_eval_batch_size 4 \&#xA;    --gradient_accumulation_steps 8 \&#xA;    --evaluation_strategy &#34;no&#34; \&#xA;    --save_strategy &#34;steps&#34; \&#xA;    --save_steps 2000 \&#xA;    --save_total_limit 1 \&#xA;    --learning_rate 2e-5 \&#xA;    --weight_decay 0. \&#xA;    --warmup_ratio 0.03 \&#xA;    --lr_scheduler_type &#34;cosine&#34; \&#xA;    --logging_steps 1 \&#xA;    --fsdp &#34;full_shard auto_wrap&#34; \&#xA;    --fsdp_transformer_layer_cls_to_wrap &#39;OPTDecoderLayer&#39; \&#xA;    --tf32 True&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note the given training script is meant to be simple and easy to use, and is not particularly optimized. To run on more gpus, you may prefer to turn down &lt;code&gt;gradient_accumulation_steps&lt;/code&gt; to keep a global batch size of 128. Global batch size has not been tested for optimality.&lt;/p&gt; &#xA;&lt;h3&gt;Authors&lt;/h3&gt; &#xA;&lt;p&gt;All grad students below contributed equally and the order is determined by random draw.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.rohantaori.com/&#34;&gt;Rohan Taori&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ishaan.io/&#34;&gt;Ishaan Gulrajani&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tiiiger.github.io/&#34;&gt;Tianyi Zhang&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://yanndubs.github.io/&#34;&gt;Yann Dubois&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.lxuechen.com/&#34;&gt;Xuechen Li&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;All advised by &lt;a href=&#34;https://thashim.github.io/&#34;&gt;Tatsunori B. Hashimoto&lt;/a&gt;. Yann is also advised by &lt;a href=&#34;https://cs.stanford.edu/~pliang/&#34;&gt;Percy Liang&lt;/a&gt; and Xuechen is also advised by &lt;a href=&#34;https://guestrin.su.domains/&#34;&gt;Carlos Guestrin&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Citation&lt;/h3&gt; &#xA;&lt;p&gt;Please cite the repo if you use the data or code in this repo.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{alpaca,&#xA;  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },&#xA;  title = {Stanford Alpaca: An Instruction-following LLaMA model},&#xA;  year = {2023},&#xA;  publisher = {GitHub},&#xA;  journal = {GitHub repository},&#xA;  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Naturally, you should also cite the original LLaMA paper [1] and the Self-Instruct paper [2].&lt;/p&gt; &#xA;&lt;h3&gt;Acknowledgements&lt;/h3&gt; &#xA;&lt;p&gt;We thank Yizhong Wang for his help in explaining the data generation pipeline in Self-Instruct and providing the code for the parse analysis plot. We thank Yifan Mai for helpful support, and members of the Stanford NLP Group as well as the Center for Research on Foundation Models (CRFM) for their helpful feedback.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>22-hours/cabrita</title>
    <updated>2023-03-21T01:39:33Z</updated>
    <id>tag:github.com,2023-03-21:/22-hours/cabrita</id>
    <link href="https://github.com/22-hours/cabrita" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Finetuning InstructLLaMA with portuguese data&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/22-hours/cabrita/main/assets/cabrita.png&#34; alt=&#34;Cabrita&#34; style=&#34;width: 20%; min-width: 300px; display: block; margin: auto;&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Cabrita: A portuguese finetuned instruction LLaMA&lt;/h1&gt; &#xA;&lt;p&gt;This repository is intended to share all the steps and resources that we used to finetune our version of LLaMA.&lt;/p&gt; &#xA;&lt;p&gt;This model is designed for research use only, i.e., cannot be used for commercial purposes or entertainment.&lt;/p&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;If I have seen further it is by standing on the sholders [sic] of Giants. -- &lt;cite&gt;Isaac Newton&lt;/cite&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;We started this section with this citation because everything we did was only possible due to the strong community and works that other people and groups did. For our work, we rely mainly in the works developed by: &lt;a href=&#34;https://ai.facebook.com/blog/large-language-model-llama-meta-ai/&#34;&gt;LLaMA&lt;/a&gt;, &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca&lt;/a&gt;, &lt;a href=&#34;https://github.com/tloen/alpaca-lora&#34;&gt;Alpaca Lora&lt;/a&gt;, &lt;a href=&#34;https://openai.com/blog/chatgpt&#34;&gt;ChatGPT&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/&#34;&gt;Hugging Face&lt;/a&gt;. So, thank you all for the great work and open this to the world!&lt;/p&gt; &#xA;&lt;h2&gt;Data&lt;/h2&gt; &#xA;&lt;p&gt;We translated the &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca/raw/main/alpaca_data.json&#34;&gt;alpaca_data.json&lt;/a&gt; to portuguese using ChatGPT. Even this translation was not the best, the tradeoff between costs and results were. We paid around US$ 8.00 to translate the full dataset to portuguese. If you want to know more about how the dataset was built go to: &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Finetuning&lt;/h2&gt; &#xA;&lt;p&gt;To finetuned the LLaMA model we used the code available on &lt;a href=&#34;https://github.com/tloen/alpaca-lora&#34;&gt;Alpaca Lora&lt;/a&gt;, which provides code to finetune the LLaMA model using PEFT from Hugging Face. With this, we could run our finetuning step using 1 A100 at Colab on top of LLaMA-7B. We trained during 1 hour and we found the results pretty incredible with just that much time. The notebook we used is avaible &lt;a href=&#34;https://raw.githubusercontent.com/22-hours/cabrita/main/notebooks/train_lora.ipynb&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Example outputs&lt;/h2&gt; &#xA;&lt;p&gt;Stanford Alpaca:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Instruction: Invente uma desculpa criativa pra dizer que não preciso ir à festa.&#xA;Response: Eu preciso se concentrar em trabalho importante para mim, por isso eu não posso ir à festa.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Cabrita:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Instrução: Invente uma desculpa criativa pra dizer que não preciso ir à festa.&#xA;Resposta: &#34;Eu preciso ficar em casa para cuidar dos meus animais de estimação.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Stanford Alpaca:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Instruction: Como posso começar a carreira como cientista de dados? Escreva na forma de lista.&#xA;Response:&#xA;1. Comece por se especializar em um campo específico, como big data, computação, bioinformática ou estatística.&#xA;2. Obtivere um diploma em um programa de graduação relacionado a ciência de dados, como informática, matemática ou estatística.&#xA;3. Aquisite experiência prática através de internships ou projetos em empresas ou instituições.&#xA;4. Ameize sua resumo e currículo com resultados relevantes e experiência em trabalho relacionado.&#xA;5. Leia artigos, livros e atualizações sobre ciência de dados.&#xA;6. Crie um portfólio com projetos e experimentos que tenha realizado.&#xA;7. Receba um certificado de qualificação em alguma área de ciência de dados.&#xA;8. Tente obter contato com profissionais da área para conversar sobre oportunidades de emprego.&#xA;9. Acesse redes sociais, como LinkedIn, para encontrar empregos e conexões.&#xA;10. Continui sua formação atualizando-se com tecnologias e ferramentas de ciência de dados.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Cabrita:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Instrução: Como posso começar a carreira como cientista de dados? Escreva na forma de lista.&#xA;Resposta: &#xA;1. Obtenha um mestrado em ciência de dados ou engenharia de dados.&#xA;2. Encontre um emprego como cientista de dados ou engenheiro de dados.&#xA;3. Conheça as principais ferramentas de análise de dados, como Python, R e SQL.&#xA;4. Conheça as principais técnicas de análise de dados, como regressão linear, regressão logística e análise de clustering.&#xA;5. Conheça as principais abordagens de análise de dados, como análise exploratória, análise de associação e análise de regressão.&#xA;6. Conheça as principais ferramentas de visualização de dados, como Tableau, QlikView e Power BI.&#xA;7. Conheça as principais abordagens de análise de dados, como análise de associação, análise de regressão e análise exploratória.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can test it using the eval notebook &lt;a href=&#34;https://raw.githubusercontent.com/22-hours/cabrita/main/notebooks/cabrita-lora.ipynb&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Next steps&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create a better portuguese dataset&lt;/li&gt; &#xA; &lt;li&gt;Evaluate the toxicity&lt;/li&gt; &#xA; &lt;li&gt;Finetune large models&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Authors&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/piEsposito&#34;&gt;piEsposito&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pedrogengo&#34;&gt;pedrogengo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>lewangdev/gost-install.ipynb</title>
    <updated>2023-03-21T01:39:33Z</updated>
    <id>tag:github.com,2023-03-21:/lewangdev/gost-install.ipynb</id>
    <link href="https://github.com/lewangdev/gost-install.ipynb" rel="alternate"></link>
    <summary type="html">&lt;p&gt;通过 Jupyter Notebook 安装 GOST&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;通过 Jupyter Notebook 安装 GOST&lt;/h1&gt; &#xA;&lt;h2&gt;安装 Python 环境&lt;/h2&gt; &#xA;&lt;p&gt;如果不会安装，请问 ChatGPT&lt;/p&gt; &#xA;&lt;h2&gt;安装 jupyterlab&lt;/h2&gt; &#xA;&lt;h3&gt;Windows 上&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python -m venv .venv&#xA;.\.venv\Scripts\activate&#xA;pip install jupyterlab&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;MacOS 或者 Linux 上&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python -m venv .venv&#xA;. .venv/bin/activate&#xA;pip install jupyterlab&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;运行 jupyterlab&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;jupyter-lab&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;操作&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;浏览器打开 &lt;a href=&#34;http://localhost:8888/lab/tree/gost_install.ipynb&#34;&gt;http://localhost:8888/lab/tree/gost_install.ipynb&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;根据提示一步一步执行即可&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>