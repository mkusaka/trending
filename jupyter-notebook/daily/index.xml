<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-27T01:27:24Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>baegwangbin/DSINE</title>
    <updated>2024-04-27T01:27:24Z</updated>
    <id>tag:github.com,2024-04-27:/baegwangbin/DSINE</id>
    <link href="https://github.com/baegwangbin/DSINE" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[CVPR 2024 Oral] Rethinking Inductive Biases for Surface Normal Estimation&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Rethinking Inductive Biases for Surface Normal Estimation&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;20%&#34; src=&#34;https://github.com/baegwangbin/DSINE/raw/main/docs/img/dsine/logo_with_outline.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Official implementation of the paper&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Rethinking Inductive Biases for Surface Normal Estimation&lt;/strong&gt; &lt;br&gt; CVPR 2024 [oral] &lt;br&gt; &lt;a href=&#34;https://baegwangbin.com&#34; target=&#34;_blank&#34;&gt;Gwangbin Bae&lt;/a&gt; and &lt;a href=&#34;https://www.doc.ic.ac.uk/~ajd/&#34; target=&#34;_blank&#34;&gt;Andrew J. Davison&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://github.com/baegwangbin/DSINE/raw/main/paper.pdf&#34; target=&#34;_blank&#34;&gt;[paper.pdf]&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2403.00712&#34; target=&#34;_blank&#34;&gt;[arXiv]&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=2y9-35c719Y&amp;amp;t=5s&#34; target=&#34;_blank&#34;&gt;[youtube]&lt;/a&gt; &lt;a href=&#34;https://baegwangbin.github.io/DSINE/&#34; target=&#34;_blank&#34;&gt;[project page]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Abstract&lt;/h2&gt; &#xA;&lt;p&gt;Despite the growing demand for accurate surface normal estimation models, existing methods use general-purpose dense prediction models, adopting the same inductive biases as other tasks. In this paper, we discuss the &lt;strong&gt;inductive biases&lt;/strong&gt; needed for surface normal estimation and propose to &lt;strong&gt;(1) utilize the per-pixel ray direction&lt;/strong&gt; and &lt;strong&gt;(2) encode the relationship between neighboring surface normals by learning their relative rotation&lt;/strong&gt;. The proposed method can generate &lt;strong&gt;crisp ‚Äî yet, piecewise smooth ‚Äî predictions&lt;/strong&gt; for challenging in-the-wild images of arbitrary resolution and aspect ratio. Compared to a recent ViT-based state-of-the-art model, our method shows a stronger generalization ability, despite being trained on an orders of magnitude smaller dataset.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;100%&#34; src=&#34;https://github.com/baegwangbin/DSINE/raw/main/docs/img/fig_comparison.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;We provide the instructions in &lt;strong&gt;four steps&lt;/strong&gt; (click &#34;‚ñ∏&#34; to expand). For example, if you just want to test DSINE on some images, you can stop after &lt;strong&gt;Step 1&lt;/strong&gt;. This would minimize the amount of installation/downloading.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;Step 1. Test DSINE on some images&lt;/b&gt; (requires minimal dependencies)&lt;/summary&gt; &#xA; &lt;p&gt;Start by installing dependencies.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;conda create --name DSINE python=3.10&#xA;conda activate DSINE&#xA;&#xA;conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia&#xA;python -m pip install geffnet&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Then, download the model weights from &lt;a href=&#34;https://drive.google.com/drive/folders/1t3LMJIIrSnCGwOEf53Cyg0lkSXd3M4Hm?usp=drive_link&#34; target=&#34;_blank&#34;&gt;this link&lt;/a&gt; and save it under &lt;code&gt;projects/dsine/checkpoints/&lt;/code&gt;. Note that it should maintain the same folder structure as the google drive. For example, &lt;code&gt;checkpoints/exp001_cvpr2024/dsine.pt&lt;/code&gt; (in google drive) is our best model. It should be saved as &lt;code&gt;projects/dsine/checkpoints/exp001_cvpr2024/dsine.pt&lt;/code&gt;. The corresponding config file is &lt;code&gt;projects/dsine/experiments/exp001_cvpr2024/dsine.txt&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;p&gt;The models under &lt;code&gt;checkpoints/exp002_kappa/&lt;/code&gt; (in google drive) are the ones that can also estimate uncertainty.&lt;/p&gt; &#xA; &lt;p&gt;Then, move to the folder &lt;code&gt;projects/dsine/&lt;/code&gt;, and run&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;python test_minimal.py ./experiments/exp001_cvpr2024/dsine.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;This will generate predictions for the images under &lt;code&gt;projects/dsine/samples/img/&lt;/code&gt;. The result will be saved under &lt;code&gt;projects/dsine/samples/output/&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;p&gt;Our model assumes known camera intrinsics, but providing approximate intrinsics still gives good results. For some images in &lt;code&gt;projects/dsine/samples/img/&lt;/code&gt;, the corresponding camera intrinsics (fx, fy, cx, cy - assuming perspective camera with no distortion) is provided as a &lt;code&gt;.txt&lt;/code&gt; file. If such a file does not exist, the intrinsics will be approximated, by assuming $60^\circ$ field-of-view.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;Step 2. Test DSINE on benchmark datasets &amp;amp; run a real-time demo&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt;Install additional dependencies.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;python -m pip install tensorboard&#xA;python -m pip install opencv-python&#xA;python -m pip install matplotlib&#xA;&#xA;python -m pip install pyrealsense2    # needed only for demo using a realsense camera&#xA;python -m pip install vidgear         # needed only for demo on YouTube videos&#xA;python -m pip install yt_dlp          # needed only for demo on YouTube videos&#xA;python -m pip install mss             # needed only for demo on screen capture&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Download the evaluation datasets (&lt;code&gt;dsine_eval.zip&lt;/code&gt;) from &lt;a href=&#34;https://drive.google.com/drive/folders/1t3LMJIIrSnCGwOEf53Cyg0lkSXd3M4Hm?usp=drive_link&#34; target=&#34;_blank&#34;&gt;this link&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; By downloading the dataset, you are agreeing to the respective LICENSE of each dataset. The link to the dataset can be found in the respective &lt;code&gt;readme.txt&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;p&gt;If you go to &lt;code&gt;projects/__init__.py&lt;/code&gt;, there is a variable called &lt;code&gt;DATASET_DIR&lt;/code&gt; and &lt;code&gt;EXPERIMENT_DIR&lt;/code&gt;:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;DATASET_DIR&lt;/code&gt; is where your dataset should be stored. For example, the &lt;code&gt;dsine_eval&lt;/code&gt; dataset (downloaded from the link above) should be saved under &lt;code&gt;DATASET_DIR/dsine_eval&lt;/code&gt;. Update this variable.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;EXPERIMENT_DIR&lt;/code&gt; is where the experiments (e.g. model weights, log, etc) will be saved. Update this variable.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;Then, move to the folder &lt;code&gt;projects/dsine/&lt;/code&gt;, and run:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# getting benchmark performance on the six evaluation datasets&#xA;python test.py ./experiments/exp001_cvpr2024/dsine.txt --mode benchmark&#xA;&#xA;# getting benchmark performance on the six evaluation datasets (with visualization)&#xA;# it will be saved under EXPERIMENT_DIR/dsine/exp001_cvpr2024/dsine/test/&#xA;python test.py ./experiments/exp001_cvpr2024/dsine.txt --mode benchmark --visualize&#xA;&#xA;# generate predictions for the images in `projects/dsine/samples/img/&#xA;python test.py ./experiments/exp001_cvpr2024/dsine.txt --mode samples&#xA;&#xA;# measure the throughput (inference speed) on your device&#xA;python test.py ./experiments/exp001_cvpr2024/dsine.txt --mode throughput&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;You can also run a real-time demo by running:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# captures your screen and makes prediction&#xA;python test.py ./experiments/exp001_cvpr2024/dsine.txt --mode screen&#xA;&#xA;# demo using webcam&#xA;python test.py ./experiments/exp001_cvpr2024/dsine.txt --mode webcam&#xA;&#xA;# demo using a realsense camera&#xA;python test.py ./experiments/exp001_cvpr2024/dsine.txt --mode rs&#xA;&#xA;# demo on a Youtube video (replace with a different link)&#xA;python test.py ./experiments/exp001_cvpr2024/dsine.txt --mode https://www.youtube.com/watch?v=X-iEq8hWd6k&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;For each input option, there are some additional parameters. See &lt;code&gt;projects/dsine/test.py&lt;/code&gt; for more information.&lt;/p&gt; &#xA; &lt;p&gt;You can also try building your own real-time demo. Please see &lt;a href=&#34;https://github.com/baegwangbin/DSINE/raw/main/notes/real_time_demo.ipynb&#34;&gt;this notebook&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;Step 3. Train DSINE&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt;In &lt;code&gt;projects/dsine/&lt;/code&gt;, run:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python train.py ./experiments/exp000_test/test.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;And do &lt;code&gt;tensorboard --logdir EXPERIMENT_DIR/dsine/exp000_test/test/log&lt;/code&gt; to open the tensorboard.&lt;/p&gt; &#xA; &lt;p&gt;This will train the model on the train split of the NYUv2 dataset, which should be under &lt;code&gt;DATASET_DIR/dsine_eval/nyuv2/train/&lt;/code&gt;. There are only 795 images here, and the performance will not be good. To get better results you need to:&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;(1) Create a custom dataloader&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;We are checking if we can release the entire training dataset (~400GB). Before the release, you can try building your custom dataloader. You need to define a &lt;code&gt;get_sample(args, sample_path, info)&lt;/code&gt; function and provide a data split in &lt;code&gt;data/datasets&lt;/code&gt;. Check how they are defined/provided for other datasets. You also need to update &lt;code&gt;projects/baseline_normal/dataloader.py&lt;/code&gt; so the newly defined &lt;code&gt;get_sample&lt;/code&gt; function can be used.&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;(2) Generate GT surface normals&lt;/strong&gt; (optional)&lt;/p&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;In case your dataset does not come with ground truth surface normal maps, you can try generating them from the ground truth depth maps. Please see &lt;a href=&#34;https://github.com/baegwangbin/DSINE/raw/main/notes/depth_to_normal.ipynb&#34;&gt;this notebook&lt;/a&gt; for more information.&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;(3) Customize data augmentation&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;In case you are using synthetic images, you need the right set of data augmentation functions to minimize the synthetic-to-real domain gap. We provide a wide range of augmentation functions, but the hyperparameters are not finetuned and you can potentially get better results by finetuning them. Please see &lt;a href=&#34;https://github.com/baegwangbin/DSINE/raw/main/notes/visualize_augmentation.ipynb&#34;&gt;this notebook&lt;/a&gt; for more information.&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;Step 4. Start your own surface normal estimation project&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt;If you want to start your own surface normal estimation project, you can do so very easily.&lt;/p&gt; &#xA; &lt;p&gt;First of all, have a look at &lt;code&gt;projects/baseline_normal&lt;/code&gt;. This is a place where you can try different CNN architectures without worrying about the camera intrinsics and rotation estimation. You can try popular architectures like U-Net, and try different backbones. In this folder, you can run:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python train.py ./experiments/exp000_test/test.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;The project-specific &lt;code&gt;config&lt;/code&gt; is defined in &lt;code&gt;projects/baseline_normal/config.py&lt;/code&gt;. Default config, which is shared across all projects are in &lt;code&gt;projects/__init__.py&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;p&gt;The dataloaders are in &lt;code&gt;projects/baseline_normal/dataloader.py&lt;/code&gt;. We use the same dataloaders in &lt;code&gt;dsine&lt;/code&gt; project, so we don&#39;t have &lt;code&gt;projects/dsine/dataloader.py&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;p&gt;The losses are defined in &lt;code&gt;projects/baseline_normal/losses.py&lt;/code&gt;. These are building blocks for your custom loss functions in your own project. For example, in the DSINE project, we produce a list of predictions and the loss is the weighted sum of the losses computed for each prediction. You can see how this is done in &lt;code&gt;projects/dsine/losses.py&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;p&gt;You can start a new project by copying the folder &lt;code&gt;projects/dsine&lt;/code&gt; to create &lt;code&gt;projects/NEW_PROJECT_NAME&lt;/code&gt;. Then, update the &lt;code&gt;config.py&lt;/code&gt; and &lt;code&gt;losses.py&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;p&gt;Lastly, you can should &lt;code&gt;train.py&lt;/code&gt; and &lt;code&gt;test.py&lt;/code&gt;. For things that should be different in different projects, we made a note like following:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#‚Üì‚Üì‚Üì‚Üì&#xA;#NOTE: forward pass&#xA;img = data_dict[&#39;img&#39;].to(device)&#xA;intrins = data_dict[&#39;intrins&#39;].to(device)&#xA;...&#xA;pred_list = model(img, intrins=intrins, mode=&#39;test&#39;)&#xA;norm_out = pred_list[-1]&#xA;#‚Üë‚Üë‚Üë‚Üë&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Search for the arrows (‚Üì‚Üì‚Üì‚Üì/‚Üë‚Üë‚Üë‚Üë) to see where things should be modified in different projects.&lt;/p&gt; &#xA; &lt;p&gt;The test commands above (e.g. for getting the benchmark performance &amp;amp; running real-time demo) should apply the same for all projects.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Additional instructions&lt;/h2&gt; &#xA;&lt;p&gt;If you want to make contributions to this repo, please make a pull request and add instructions in the following format.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;Using torch hub to predict normal&lt;/b&gt; (contribution by &lt;a href=&#34;https://github.com/hugoycj&#34; target=&#34;_blank&#34;&gt;hugoycj&lt;/a&gt;)&lt;/summary&gt; &#xA; &lt;p&gt;NOTE: the code below is deprecated and should be modified (as the folder structure has changed).&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;import torch&#xA;import cv2&#xA;import numpy as np&#xA;&#xA;# Load the normal predictor model from torch hub&#xA;normal_predictor = torch.hub.load(&#34;hugoycj/DSINE-hub&#34;, &#34;DSINE&#34;, trust_repo=True)&#xA;&#xA;# Load the input image using OpenCV&#xA;image = cv2.imread(args.input, cv2.IMREAD_COLOR)&#xA;h, w = image.shape[:2]&#xA;&#xA;# Use the model to infer the normal map from the input image&#xA;with torch.inference_mode():&#xA;    normal = normal_predictor.infer_cv2(image)[0]  # Output shape: (H, W, 3)&#xA;    normal = (normal + 1) / 2  # Convert values to the range [0, 1]&#xA;&#xA;# Convert the normal map to a displayable format&#xA;normal = (normal * 255).cpu().numpy().astype(np.uint8).transpose(1, 2, 0)&#xA;normal = cv2.cvtColor(normal, cv2.COLOR_RGB2BGR)&#xA;&#xA;# Save the output normal map to a file&#xA;cv2.imwrite(args.output, normal)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;If the network is unavailable to retrieve weights, you can use local weights for torch hub as shown below:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;normal_predictor = torch.hub.load(&#34;hugoycj/DSINE-hub&#34;, &#34;DSINE&#34;, local_file_path=&#39;./checkpoints/dsine.pt&#39;, trust_repo=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;Generating ground truth surface normals&lt;/b&gt;&lt;/summary&gt; We provide the code used to generate the ground truth surface normals from ground truth depth maps. See &#xA; &lt;a href=&#34;https://github.com/baegwangbin/DSINE/raw/main/notes/depth_to_normal.ipynb&#34; target=&#34;_blank&#34;&gt;this notebook&lt;/a&gt; for more information. &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;About the coordinate system&lt;/b&gt;&lt;/summary&gt; We use the right-handed coordinate system with (X, Y, Z) = (right, down, front). An important thing to note is that both the ground truth normals and our prediction are the &#xA; &lt;b&gt;outward normals&lt;/b&gt;. For example, in the case of a fronto-parallel wall facing the camera, the normals would be (0, 0, 1), not (0, 0, -1). If you instead need to use the &#xA; &lt;b&gt;inward normals&lt;/b&gt;, please do &#xA; &lt;code&gt;normals = -normals&lt;/code&gt;. &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;Sharing your model weights&lt;/b&gt;&lt;/summary&gt; If you wish to share your model weights, please make a pull request by providing the corresponding config file and the link to the weights. &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work useful in your research please consider citing our paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{bae2024dsine,&#xA;    title     = {Rethinking Inductive Biases for Surface Normal Estimation},&#xA;    author    = {Gwangbin Bae and Andrew J. Davison},&#xA;    booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},&#xA;    year      = {2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you use the models that also estimate the uncertainty, please also cite the following paper, where we introduced the loss function:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@InProceedings{bae2021eesnu,&#xA;    title     = {Estimating and Exploiting the Aleatoric Uncertainty in Surface Normal Estimation}&#xA;    author    = {Gwangbin Bae and Ignas Budvytis and Roberto Cipolla},&#xA;    booktitle = {International Conference on Computer Vision (ICCV)},&#xA;    year      = {2021}                         &#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>megvii-research/HiDiffusion</title>
    <updated>2024-04-27T01:27:24Z</updated>
    <id>tag:github.com,2024-04-27:/megvii-research/HiDiffusion</id>
    <link href="https://github.com/megvii-research/HiDiffusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/megvii-research/HiDiffusion/main/assets/hidiffusion_logo.jpg&#34; height=&#34;120&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;&#xA; &lt;div align=&#34;center&#34;&gt;&#xA;  üí° HiDiffusion: Unlocking Higher-Resolution Creativity and Efficiency in Pretrained Diffusion Models&#xA; &lt;/div&gt;&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; Shen Zhang, Zhaowei Chen, Zhenyu Zhao, Yuhao Chen, Yao Tang, Jiajun Liang&#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://hidiffusion.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Project%20Page&amp;amp;message=Github&amp;amp;color=blue&amp;amp;logo=github-pages&#34;&gt;&lt;/a&gt; ‚ÄÇ &#xA; &lt;a href=&#34;https://arxiv.org/abs/2311.17528&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Paper&amp;amp;message=Arxiv:HiDiffusion&amp;amp;color=red&amp;amp;logo=arxiv&#34;&gt;&lt;/a&gt; ‚ÄÇ &#xA; &lt;a href=&#34;https://colab.research.google.com/drive/1EiBn9lSnPZTU4cikRRaBBexs429M-qty?usp=sharing&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Demo&amp;amp;message=Colab&amp;amp;color=purple&amp;amp;logo=googlecolab&#34;&gt;&lt;/a&gt; ‚ÄÇ &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/megvii-research/HiDiffusion/main/assets/image_gallery.jpg&#34; width=&#34;800&#34;&gt; &#xA; &lt;br&gt; &#xA; &lt;em&gt; (Select HiDiffusion samples for various diffusion models, resolutions, and aspect ratios.) &lt;/em&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;üëâ Why HiDiffusion&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A &lt;strong&gt;training-free method that increases the resolution and speed of pretrained diffusion models.&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Designed as a &lt;strong&gt;plug-and-play implementation&lt;/strong&gt;. It can be integrated into diffusion pipelines by &lt;strong&gt;only adding a single line of code&lt;/strong&gt;!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/megvii-research/HiDiffusion/main/assets/quality_efficiency.jpg&#34; width=&#34;800&#34;&gt; &#xA; &lt;br&gt; &#xA; &lt;em&gt; (Faster, and better image details.) &lt;/em&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;üì¢ Supported Models&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;‚úÖ &lt;a href=&#34;https://huggingface.co/papers/2307.01952&#34;&gt;Stable Diffusion XL&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ &lt;a href=&#34;https://huggingface.co/stabilityai/sdxl-turbo&#34;&gt;Stable Diffusion XL Turbo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ &lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-2-1&#34;&gt;Stable Diffusion v2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ &lt;a href=&#34;https://huggingface.co/runwayml/stable-diffusion-v1-5&#34;&gt;Stable Diffusion v1&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: HiDiffusion also supports the downstream diffusion models based on these repositories, such as &lt;a href=&#34;https://huggingface.co/nitrosocke/Ghibli-Diffusion&#34;&gt;Ghibli-Diffusion&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic&#34;&gt;Playground&lt;/a&gt;, etc.&lt;/p&gt; &#xA;&lt;h2&gt;üí£ Supported Tasks&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;‚úÖ Text-to-image&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ ControlNet&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ Inpainting&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üîé Main Requirements&lt;/h2&gt; &#xA;&lt;p&gt;This repository is tested on&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python==3.8&lt;/li&gt; &#xA; &lt;li&gt;torch==1.13.1&lt;/li&gt; &#xA; &lt;li&gt;diffusers==0.27.0&lt;/li&gt; &#xA; &lt;li&gt;transformers==4.27.4&lt;/li&gt; &#xA; &lt;li&gt;accelerate==0.18.0&lt;/li&gt; &#xA; &lt;li&gt;xformers==0.0.16rc425&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üîë Install HiDiffusion&lt;/h2&gt; &#xA;&lt;p&gt;After installing the packages in the &lt;a href=&#34;https://raw.githubusercontent.com/megvii-research/HiDiffusion/main/#-main-requirements&#34;&gt;main requirements&lt;/a&gt;, install HiDiffusion:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip3 install hidiffusion&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Installing from source&lt;/h3&gt; &#xA;&lt;p&gt;Alternatively, you can install from github source. Clone the repository and install:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/megvii-model/HiDiffusion.git&#xA;cd HiDiffusion&#xA;python3 setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üöÄ Usage&lt;/h2&gt; &#xA;&lt;p&gt;Generating outputs with HiDiffusion is super easy based on ü§ó &lt;a href=&#34;https://github.com/huggingface/diffusers/tree/main&#34;&gt;diffusers&lt;/a&gt;. &lt;strong&gt;You just need to add a single line of code&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Text-to-image generation&lt;/h2&gt; &#xA;&lt;h3&gt;Stable Diffusion XL&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from hidiffusion import apply_hidiffusion, remove_hidiffusion&#xA;from diffusers import StableDiffusionXLPipeline, DDIMScheduler&#xA;import torch&#xA;pretrain_model = &#34;stabilityai/stable-diffusion-xl-base-1.0&#34;&#xA;scheduler = DDIMScheduler.from_pretrained(pretrain_model, subfolder=&#34;scheduler&#34;)&#xA;pipe = StableDiffusionXLPipeline.from_pretrained(pretrain_model, scheduler = scheduler, torch_dtype=torch.float16, variant=&#34;fp16&#34;).to(&#34;cuda&#34;)&#xA;&#xA;# # Optional. enable_xformers_memory_efficient_attention can save memory usage and increase inference speed. enable_model_cpu_offload and enable_vae_tiling can save memory usage.&#xA;# pipe.enable_xformers_memory_efficient_attention()&#xA;# pipe.enable_model_cpu_offload()&#xA;# pipe.enable_vae_tiling()&#xA;&#xA;# Apply hidiffusion with a single line of code.&#xA;apply_hidiffusion(pipe)&#xA;&#xA;prompt = &#34;Standing tall amidst the ruins, a stone golem awakens, vines and flowers sprouting from the crevices in its body.&#34;&#xA;negative_prompt = &#34;blurry, ugly, duplicate, poorly drawn face, deformed, mosaic, artifacts, bad limbs&#34;&#xA;image = pipe(prompt, guidance_scale=7.5, height=2048, width=2048, eta=1.0, negative_prompt=negative_prompt).images[0]&#xA;image.save(f&#34;golem.jpg&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Output:&lt;/summary&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/megvii-research/HiDiffusion/main/assets/sdxl.jpg&#34; width=&#34;800&#34;&gt; &#xA; &lt;/div&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;Set height = 4096, width = 4096, and you can get output with 4096x4096 resolution.&lt;/p&gt; &#xA;&lt;h3&gt;Stable Diffusion XL Turbo&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from hidiffusion import apply_hidiffusion, remove_hidiffusion&#xA;from diffusers import AutoPipelineForText2Image&#xA;import torch&#xA;pretrain_model = &#34;stabilityai/sdxl-turbo&#34;&#xA;pipe = AutoPipelineForText2Image.from_pretrained(pretrain_model, torch_dtype=torch.float16, variant=&#34;fp16&#34;).to(&#39;cuda&#39;)&#xA;&#xA;# # Optional. enable_xformers_memory_efficient_attention can save memory usage and increase inference speed. enable_model_cpu_offload and enable_vae_tiling can save memory usage.&#xA;# pipe.enable_xformers_memory_efficient_attention()&#xA;# pipe.enable_model_cpu_offload()&#xA;# pipe.enable_vae_tiling()&#xA;&#xA;# Apply hidiffusion with a single line of code.&#xA;apply_hidiffusion(pipe)&#xA;&#xA;prompt = &#34;In the depths of a mystical forest, a robotic owl with night vision lenses for eyes watches over the nocturnal creatures.&#34;&#xA;image = pipe(prompt, num_inference_steps=4, height=1024, width=1024, guidance_scale=0.0).images[0]&#xA;image.save(f&#34;./owl.jpg&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Output:&lt;/summary&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/megvii-research/HiDiffusion/main/assets/sdxl_turbo.jpg&#34; width=&#34;800&#34;&gt; &#xA; &lt;/div&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Stable Diffusion v2-1&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from hidiffusion import apply_hidiffusion, remove_hidiffusion&#xA;from diffusers import DiffusionPipeline, DDIMScheduler&#xA;import torch&#xA;pretrain_model = &#34;stabilityai/stable-diffusion-2-1-base&#34;&#xA;scheduler = DDIMScheduler.from_pretrained(pretrain_model, subfolder=&#34;scheduler&#34;)&#xA;pipe = DiffusionPipeline.from_pretrained(pretrain_model, scheduler = scheduler, torch_dtype=torch.float16).to(&#34;cuda&#34;)&#xA;&#xA;# # Optional. enable_xformers_memory_efficient_attention can save memory usage and increase inference speed. enable_model_cpu_offload and enable_vae_tiling can save memory usage.&#xA;# pipe.enable_xformers_memory_efficient_attention()&#xA;# pipe.enable_model_cpu_offload()&#xA;# pipe.enable_vae_tiling()&#xA;&#xA;# Apply hidiffusion with a single line of code.&#xA;apply_hidiffusion(pipe)&#xA;&#xA;prompt = &#34;An adorable happy brown border collie sitting on a bed, high detail.&#34;&#xA;negative_prompt = &#34;ugly, tiling, out of frame, poorly drawn face, extra limbs, disfigured, deformed, body out of frame, blurry, bad anatomy, blurred, artifacts, bad proportions.&#34;&#xA;image = pipe(prompt, guidance_scale=7.5, height=1024, width=1024, eta=1.0, negative_prompt=negative_prompt).images[0]&#xA;image.save(f&#34;collie.jpg&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Output:&lt;/summary&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/megvii-research/HiDiffusion/main/assets/sd21.jpg&#34; width=&#34;800&#34;&gt; &#xA; &lt;/div&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;Set height = 2048, width = 2048, and you can get output with 2048x2048 resolution.&lt;/p&gt; &#xA;&lt;h3&gt;Stable Diffusion v1-5&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from hidiffusion import apply_hidiffusion, remove_hidiffusion&#xA;from diffusers import DiffusionPipeline, DDIMScheduler&#xA;import torch&#xA;pretrain_model = &#34;runwayml/stable-diffusion-v1-5&#34;&#xA;scheduler = DDIMScheduler.from_pretrained(pretrain_model, subfolder=&#34;scheduler&#34;)&#xA;pipe = DiffusionPipeline.from_pretrained(pretrain_model, scheduler = scheduler, torch_dtype=torch.float16).to(&#34;cuda&#34;)&#xA;&#xA;# # Optional. enable_xformers_memory_efficient_attention can save memory usage and increase inference speed. enable_model_cpu_offload and enable_vae_tiling can save memory usage.&#xA;# pipe.enable_xformers_memory_efficient_attention()&#xA;# pipe.enable_model_cpu_offload()&#xA;# pipe.enable_vae_tiling()&#xA;&#xA;# Apply hidiffusion with a single line of code.&#xA;apply_hidiffusion(pipe)&#xA;&#xA;prompt = &#34;thick strokes, bright colors, an exotic fox, cute, chibi kawaii. detailed fur, hyperdetailed , big reflective eyes, fairytale, artstation,centered composition, perfect composition, centered, vibrant colors, muted colors, high detailed, 8k.&#34;&#xA;negative_prompt = &#34;ugly, tiling, poorly drawn face, out of frame, disfigured, deformed, blurry, bad anatomy, blurred.&#34;&#xA;image = pipe(prompt, guidance_scale=7.5, height=1024, width=1024, eta=1.0, negative_prompt=negative_prompt).images[0]&#xA;image.save(f&#34;fox.jpg&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Output:&lt;/summary&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/megvii-research/HiDiffusion/main/assets/sd15.jpg&#34; width=&#34;800&#34;&gt; &#xA; &lt;/div&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;Set height = 2048, width = 2048, and you can get output with 2048x2048 resolution.&lt;/p&gt; &#xA;&lt;h3&gt;Remove HiDiffusion&lt;/h3&gt; &#xA;&lt;p&gt;If you want to remove HiDiiffusion, simply use &lt;code&gt;remove_hidiffusion(pipe)&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;ControlNet&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, DDIMScheduler&#xA;import numpy as np&#xA;import torch&#xA;import cv2&#xA;from PIL import Image&#xA;from hidiffusion import apply_hidiffusion, remove_hidiffusion&#xA;&#xA;# load Yoshua_Bengio.jpg in the assets file.&#xA;path = &#39;Yoshua_Bengio.jpg&#39;&#xA;image = Image.open(path)&#xA;# get canny image&#xA;image = np.array(image)&#xA;image = cv2.Canny(image, 100, 200)&#xA;image = image[:, :, None]&#xA;image = np.concatenate([image, image, image], axis=2)&#xA;canny_image = Image.fromarray(image)&#xA;&#xA;# initialize the models and pipeline&#xA;controlnet_conditioning_scale = 0.5  # recommended for good generalization&#xA;controlnet = ControlNetModel.from_pretrained(&#xA;    &#34;diffusers/controlnet-canny-sdxl-1.0&#34;, torch_dtype=torch.float16, variant=&#34;fp16&#34;&#xA;)&#xA;scheduler = DDIMScheduler.from_pretrained(&#34;stabilityai/stable-diffusion-xl-base-1.0&#34;, subfolder=&#34;scheduler&#34;)&#xA;pipe = StableDiffusionXLControlNetPipeline.from_pretrained(&#xA;    &#34;stabilityai/stable-diffusion-xl-base-1.0&#34;, controlnet=controlnet, torch_dtype=torch.float16,&#xA;    scheduler = scheduler&#xA;)&#xA;&#xA;# Apply hidiffusion with a single line of code.&#xA;apply_hidiffusion(pipe)&#xA;&#xA;pipe.enable_model_cpu_offload()&#xA;pipe.enable_xformers_memory_efficient_attention()&#xA;&#xA;prompt = &#34;The Joker, high face detail, high detail, muted color, 8k&#34;&#xA;negative_prompt = &#34;blurry, ugly, duplicate, poorly drawn, deformed, mosaic.&#34;&#xA;&#xA;image = pipe(&#xA;    prompt, controlnet_conditioning_scale=controlnet_conditioning_scale, image=canny_image,&#xA;    height=2048, width=2048, guidance_scale=7.5, negative_prompt = negative_prompt, eta=1.0&#xA;).images[0]&#xA;&#xA;image.save(&#39;joker.jpg&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Output:&lt;/summary&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/megvii-research/HiDiffusion/main/assets/controlnet_result.jpg&#34; width=&#34;800&#34;&gt; &#xA; &lt;/div&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Inpainting&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from diffusers import AutoPipelineForInpainting, DDIMScheduler&#xA;from diffusers.utils import load_image&#xA;from hidiffusion import apply_hidiffusion, remove_hidiffusion&#xA;from PIL import Image&#xA;&#xA;scheduler = DDIMScheduler.from_pretrained(&#34;stabilityai/stable-diffusion-xl-base-1.0&#34;, subfolder=&#34;scheduler&#34;)&#xA;pipeline = AutoPipelineForInpainting.from_pretrained(&#xA;    &#34;diffusers/stable-diffusion-xl-1.0-inpainting-0.1&#34;, torch_dtype=torch.float16, variant=&#34;fp16&#34;, &#xA;    scheduler=scheduler&#xA;)&#xA;&#xA;# Apply hidiffusion with a single line of code.&#xA;apply_hidiffusion(pipeline)&#xA;&#xA;pipeline.enable_model_cpu_offload()&#xA;# remove following line if xFormers is not installed&#xA;pipeline.enable_xformers_memory_efficient_attention()&#xA;&#xA;# load base and mask image&#xA;img_url = &#34;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/sdxl-text2img.png&#34;&#xA;init_image = load_image(img_url)&#xA;# load mask_image.jpg in the assets file.&#xA;mask_image = Image.open(&#34;mask_image.png&#34;)&#xA;&#xA;prompt =  &#34;A steampunk explorer in a leather aviator cap and goggles, with a brass telescope in hand, stands amidst towering ancient trees, their roots entwined with intricate gears and pipes.&#34;&#xA;&#xA;negative_prompt = &#34;blurry, ugly, duplicate, poorly drawn, deformed, mosaic&#34;&#xA;image = pipeline(prompt=prompt, image=init_image, mask_image=mask_image, height=2048, width=2048, strength=0.85, guidance_scale=12.5, negative_prompt = negative_prompt, eta=1.0).images[0]&#xA;image.save(&#39;steampunk_explorer.jpg&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Output:&lt;/summary&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/megvii-research/HiDiffusion/main/assets/inpainting_result.jpg&#34; width=&#34;800&#34;&gt; &#xA; &lt;/div&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Integration into downstream models&lt;/h2&gt; &#xA;&lt;p&gt;HiDiffusion supports models based on &lt;a href=&#34;https://raw.githubusercontent.com/megvii-research/HiDiffusion/main/#-supported-models&#34;&gt;supported models&lt;/a&gt;, such as &lt;a href=&#34;https://huggingface.co/nitrosocke/Ghibli-Diffusion&#34;&gt;Ghibli-Diffusion&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic&#34;&gt;Playground&lt;/a&gt;, etc.&lt;/p&gt; &#xA;&lt;h3&gt;Ghibli-Diffusion&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from diffusers import StableDiffusionPipeline&#xA;import torch&#xA;from hidiffusion import apply_hidiffusion, remove_hidiffusion&#xA;&#xA;model_id = &#34;nitrosocke/Ghibli-Diffusion&#34;&#xA;pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)&#xA;pipe = pipe.to(&#34;cuda&#34;)&#xA;&#xA;# Apply hidiffusion with a single line of code.&#xA;apply_hidiffusion(pipe)&#xA;&#xA;prompt = &#34;ghibli style magical princess with golden hair&#34;&#xA;negative_prompt=&#34;blurry, ugly, duplicate, poorly drawn face, deformed, mosaic, artifacts, bad limbs&#34;&#xA;image = pipe(prompt, height=1024, width=1024, eta=1.0, negative_prompt=negative_prompt).images[0]&#xA;&#xA;image.save(&#34;./magical_princess.jpg&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Output:&lt;/summary&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/megvii-research/HiDiffusion/main/assets/ghibli_diffusion.jpg&#34; width=&#34;800&#34;&gt; &#xA; &lt;/div&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Playground&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from diffusers import DiffusionPipeline&#xA;import torch&#xA;from hidiffusion import apply_hidiffusion, remove_hidiffusion&#xA;&#xA;pipe = DiffusionPipeline.from_pretrained(&#xA;    &#34;playgroundai/playground-v2-1024px-aesthetic&#34;,&#xA;    torch_dtype=torch.float16,&#xA;    use_safetensors=True,&#xA;    add_watermarker=False,&#xA;    variant=&#34;fp16&#34;&#xA;)&#xA;pipe.to(&#34;cuda&#34;)&#xA;pipe.enable_xformers_memory_efficient_attention()&#xA;&#xA;# Apply hidiffusion with a single line of code.&#xA;apply_hidiffusion(pipe)&#xA;&#xA;prompt = &#34;The little girl riding a bike, in a beautiful anime scene by Hayao Miyazaki: a snowy Tokyo city with massive Miyazaki clouds floating in the blue sky, enchanting snowscapes of the city with bright sunlight, Miyazaki&#39;s landscape imagery, Japanese art&#34;&#xA;negative_prompt=&#34;blurry, ugly, duplicate, poorly drawn, deformed, mosaic&#34;&#xA;image  = pipe(prompt=prompt, guidance_scale=3.0, height=2048, width=2048, negative_prompt=negative_prompt).images[0]&#xA;image.save(&#39;girl.jpg&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: You may change guidance scale from 3.0 to 5.0 and design appropriate negative prompt to generate satisfactory results.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Output:&lt;/summary&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/megvii-research/HiDiffusion/main/assets/playground_result.jpg&#34; width=&#34;800&#34;&gt; &#xA; &lt;/div&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;üôè Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;This codebase is based on &lt;a href=&#34;https://github.com/dbolya/tomesd&#34;&gt;tomesd&lt;/a&gt; and &lt;a href=&#34;https://github.com/huggingface/diffusers/tree/main&#34;&gt;diffusers&lt;/a&gt;. Thanks!&lt;/p&gt; &#xA;&lt;h2&gt;üéì Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{zhang2023hidiffusion,&#xA;  title={HiDiffusion: Unlocking Higher-Resolution Creativity and Efficiency in Pretrained Diffusion Models},&#xA;  author={Zhang, Shen and Chen, Zhaowei and Zhao, Zhenyu and Chen, Yuhao and Tang, Yao and Liang, Jiajun},&#xA;  journal={arXiv preprint arXiv:2311.17528},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>