<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-01-05T01:33:19Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>karpathy/nanoGPT</title>
    <updated>2023-01-05T01:33:19Z</updated>
    <id>tag:github.com,2023-01-05:/karpathy/nanoGPT</id>
    <link href="https://github.com/karpathy/nanoGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The simplest, fastest repository for training/finetuning medium-sized GPTs.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;nanoGPT&lt;/h1&gt; &#xA;&lt;p&gt;The simplest, fastest repository for training/finetuning medium-sized GPTs. It&#39;s a re-write of &lt;a href=&#34;https://github.com/karpathy/minGPT&#34;&gt;minGPT&lt;/a&gt;, which I think became too complicated, and which I am hesitant to now touch. Still under active development, currently working to reproduce GPT-2 on OpenWebText dataset. The code itself aims by design to be plain and readable: &lt;code&gt;train.py&lt;/code&gt; is a ~300-line boilerplate training loop and &lt;code&gt;model.py&lt;/code&gt; a ~300-line GPT model definition, which can optionally load the GPT-2 weights from OpenAI. That&#39;s it.&lt;/p&gt; &#xA;&lt;h2&gt;install&lt;/h2&gt; &#xA;&lt;p&gt;Dependencies:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org&#34;&gt;pytorch&lt;/a&gt; &amp;lt;3&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://numpy.org/install/&#34;&gt;numpy&lt;/a&gt; &amp;lt;3&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;pip install datasets&lt;/code&gt; for huggingface datasets &amp;lt;3 (if you want to download + preprocess OpenWebText)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;pip install tiktoken&lt;/code&gt; for OpenAI&#39;s fast bpe code &amp;lt;3&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;pip install wandb&lt;/code&gt; for optional logging &amp;lt;3&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;pip install tqdm&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;pip install networkx&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;usage&lt;/h2&gt; &#xA;&lt;p&gt;To render a dataset we first tokenize some documents into one simple long 1D array of indices. E.g. for OpenWebText see:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ cd data/openwebtext&#xA;$ python prepare.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To download and tokenize the &lt;a href=&#34;https://huggingface.co/datasets/openwebtext&#34;&gt;OpenWebText&lt;/a&gt; dataset. This will create a &lt;code&gt;train.bin&lt;/code&gt; and &lt;code&gt;val.bin&lt;/code&gt; which holds the GPT2 BPE token ids in one sequence, stored as raw uint16 bytes. Then we&#39;re ready to kick off training. The training script currently by default tries to reproduce the smallest GPT-2 released by OpenAI, i.e. the 124M version of GPT-2. We can demo train as follows on a single device, though I encourage you to read the code and see all of the settings and paths up top in the file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ python train.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To train using PyTorch Distributed Data Parallel (DDP) run the script with torchrun. For example to train on a node with 4 GPUs run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ torchrun --standalone --nproc_per_node=4 train.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To my knowledge, running this with the current script with the GPT-2 hyperparameters should reproduce the GPT-2 result, provided that OpenWebText ~= WebText. I&#39;d like to make the code more efficient before attempting to go there. Once some checkpoints are written to the output directory (e.g. &lt;code&gt;./out&lt;/code&gt; by default), we can sample from the model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ python sample.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Training on 1 A100 40GB GPU overnight currently gets loss ~3.74, training on 4 gets ~3.60. Training on an 8 x A100 40GB node for 400,000 iters (~1 day) atm gets down to 3.1. Random chance at init is -ln(1/50257) = 10.82. Which brings us to baselines.&lt;/p&gt; &#xA;&lt;h2&gt;baselines&lt;/h2&gt; &#xA;&lt;p&gt;OpenAI GPT-2 checkpoints allow us to get some baselines in place for openwebtext. We can get the numbers as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ python train.py eval_gpt2&#xA;$ python train.py eval_gpt2_medium&#xA;$ python train.py eval_gpt2_large&#xA;$ python train.py eval_gpt2_xl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and observe the following losses on train and val:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;model&lt;/th&gt; &#xA;   &lt;th&gt;params&lt;/th&gt; &#xA;   &lt;th&gt;train loss&lt;/th&gt; &#xA;   &lt;th&gt;val loss&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gpt2&lt;/td&gt; &#xA;   &lt;td&gt;124M&lt;/td&gt; &#xA;   &lt;td&gt;3.11&lt;/td&gt; &#xA;   &lt;td&gt;3.12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gpt2-medium&lt;/td&gt; &#xA;   &lt;td&gt;350M&lt;/td&gt; &#xA;   &lt;td&gt;2.85&lt;/td&gt; &#xA;   &lt;td&gt;2.84&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gpt2-large&lt;/td&gt; &#xA;   &lt;td&gt;774M&lt;/td&gt; &#xA;   &lt;td&gt;2.66&lt;/td&gt; &#xA;   &lt;td&gt;2.67&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gpt2-xl&lt;/td&gt; &#xA;   &lt;td&gt;1558M&lt;/td&gt; &#xA;   &lt;td&gt;2.56&lt;/td&gt; &#xA;   &lt;td&gt;2.54&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;I briefly tried finetuning gpt2 a bit more on our OWT and didn&#39;t notice dramatic improvements, suggesting that OWT is not much much different from WT in terms of the data distribution, but this needs a bit more thorough attempt once the code is in a better place.&lt;/p&gt; &#xA;&lt;h2&gt;finetuning&lt;/h2&gt; &#xA;&lt;p&gt;For an example of how to finetune a GPT on new text go to &lt;code&gt;data/shakespeare&lt;/code&gt; and look at &lt;code&gt;prepare.py&lt;/code&gt; to download the tiny shakespeare dataset and render it into a &lt;code&gt;train.bin&lt;/code&gt; and &lt;code&gt;val.bin&lt;/code&gt;. Unlike OpenWebText this will run in seconds. Finetuning takes very little time, e.g. on a single GPU just a few minutes. Run an example finetuning like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ python train.py config/finetune_shakespeare.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will load the config parameter overrides in &lt;code&gt;config/finetune_shakespeare.py&lt;/code&gt; (I didn&#39;t tune them much though). Basically, we initialize from a GPT2 checkpoint with &lt;code&gt;init_from&lt;/code&gt; and train as normal, except shorter and with a small learning rate. The best checkpoint (lowest validation loss) will be in the &lt;code&gt;out_dir&lt;/code&gt; directory, e.g. in &lt;code&gt;out-shakespeare&lt;/code&gt; by default, per the config file. You can then run the code in &lt;code&gt;sample.py&lt;/code&gt; to generate infinite Shakespeare. Note that you&#39;ll have to edit it to point to the correct &lt;code&gt;out_dir&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;benchmarking&lt;/h2&gt; &#xA;&lt;p&gt;For model benchmarking &lt;code&gt;bench.py&lt;/code&gt; might be useful. It&#39;s identical what happens in the meat of the training loop of &lt;code&gt;train.py&lt;/code&gt;, but omits much of the other complexities.&lt;/p&gt; &#xA;&lt;h2&gt;efficiency notes&lt;/h2&gt; &#xA;&lt;p&gt;Code by default now uses &lt;a href=&#34;https://pytorch.org/get-started/pytorch-2.0/&#34;&gt;PyTorch 2.0&lt;/a&gt;. At the time of writing (Dec 29, 2022) this makes &lt;code&gt;torch.compile()&lt;/code&gt; available in the nightly release. The improvement from the one line of code is noticeable, e.g. cutting down iteration time from ~250ms / iter to 135ms / iter. Nice work PyTorch team!&lt;/p&gt; &#xA;&lt;h2&gt;todos&lt;/h2&gt; &#xA;&lt;p&gt;A few todos I&#39;m aware of:&lt;/p&gt; &#xA;&lt;p&gt;Optimizations&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Additional optimizations to the running time&lt;/li&gt; &#xA; &lt;li&gt;Investigate need for an actual Data Loader with a dedicated worker process for data&lt;/li&gt; &#xA; &lt;li&gt;Look into more efficient fused optimizers (e.g. apex)&lt;/li&gt; &#xA; &lt;li&gt;Re-evaluate use of flash attention (previously I wasn&#39;t able to get the forward pass to match up so I took it out)&lt;/li&gt; &#xA; &lt;li&gt;CUDA Graphs?&lt;/li&gt; &#xA; &lt;li&gt;Investigate potential speedups from Lightning or huggingface Accelerate&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Features / APIs&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add back fp16 support? (would need to also add back gradient scaler)&lt;/li&gt; &#xA; &lt;li&gt;Add CPU support&lt;/li&gt; &#xA; &lt;li&gt;Finetune the finetuning script, I think the hyperparams are not great&lt;/li&gt; &#xA; &lt;li&gt;Report and track other metrics e.g. perplexity, num_tokens, MFU, ...&lt;/li&gt; &#xA; &lt;li&gt;Eval zero-shot perplexities on PTB, WikiText, other related benchmarks&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Suspiciousness&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Current initialization (PyTorch default) departs from GPT-2. In a very quick experiment I found it to be superior to the one suggested in the papers, but that can&#39;t be right?&lt;/li&gt; &#xA; &lt;li&gt;I don&#39;t currently seem to need gradient clipping but it is very often used (?). Nothing is exploding so far at these scales but maybe I&#39;m laeving performance on the table. Evaluate with/without.&lt;/li&gt; &#xA; &lt;li&gt;I am still not 100% confident that my GPT-2 small reproduction hyperparameters are good, if someone has reproduced GPT-2 I&#39;d be eager to exchange notes ty&lt;/li&gt; &#xA; &lt;li&gt;I keep seeing different values cited for weight decay and AdamW betas, look into&lt;/li&gt; &#xA; &lt;li&gt;I can&#39;t exactly reproduce Chinchilla paper results, see &lt;a href=&#34;https://raw.githubusercontent.com/karpathy/nanoGPT/master/scaling_laws.ipynb&#34;&gt;scaling_laws.ipynb&lt;/a&gt; notebook&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Results&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Actually reproduce GPT-2 results and have clean configs that reproduce the result. It was estimated ~3 years ago that the training cost of 1.5B model was ~$50K (?). Sounds a bit too high.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;acknowledgements&lt;/h1&gt; &#xA;&lt;p&gt;Thank you &lt;a href=&#34;https://lambdalabs.com&#34;&gt;Lambda labs&lt;/a&gt; for supporting the training costs of nanoGPT experiments.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>huggingface/blog</title>
    <updated>2023-01-05T01:33:19Z</updated>
    <id>tag:github.com,2023-01-05:/huggingface/blog</id>
    <link href="https://github.com/huggingface/blog" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Public repo for HF blog posts&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;The Hugging Face Blog Repository 🤗&lt;/h1&gt; &#xA;&lt;p&gt;This is the official repository of the &lt;a href=&#34;https://hf.co/blog&#34;&gt;Hugging Face Blog&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;How to write an article? 📝&lt;/h2&gt; &#xA;&lt;p&gt;1️⃣ Create a branch &lt;code&gt;YourName/Title&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;2️⃣ Create a md (markdown) file, &lt;strong&gt;use a short file name&lt;/strong&gt;. For instance, if your title is &#34;Introduction to Deep Reinforcement Learning&#34;, the md file name could be &lt;code&gt;intro-rl.md&lt;/code&gt;. This is important because the &lt;strong&gt;file name will be the blogpost&#39;s URL&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;3️⃣ Create a new folder in &lt;code&gt;assets&lt;/code&gt;. Use the same name as the name of the md file. Optionally you may add a numerical prefix to that folder, using the number that hasn&#39;t been used yet. But this is no longer required. i.e. the asset folder in this example will be &lt;code&gt;123_intro-rl&lt;/code&gt; or &lt;code&gt;intro-rl&lt;/code&gt;. This folder will contain &lt;strong&gt;all your illustrations and thumbnail&lt;/strong&gt;. The folder number is mostly for (rough) ordering purposes, so it&#39;s no big deal if two concurrent articles use the same number.&lt;/p&gt; &#xA;&lt;p&gt;🖼️: In terms of images, &lt;strong&gt;try to have small files&lt;/strong&gt; to avoid having a slow loading user experience:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Use compressed images, you can use this website: &lt;a href=&#34;https://www.iloveimg.com/compress-image&#34;&gt;https://www.iloveimg.com/compress-image&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;4️⃣ Copy and paste this to your md file and change the elements&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;title&lt;/li&gt; &#xA; &lt;li&gt;thumbnail&lt;/li&gt; &#xA; &lt;li&gt;Published (change the date)&lt;/li&gt; &#xA; &lt;li&gt;Change the author card &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;href =&#34;/ your huggingface username&#34;&lt;/li&gt; &#xA;   &lt;li&gt;src : your huggingface picture, for that right click to the huggingface picture and copy the link&lt;/li&gt; &#xA;   &lt;li&gt;&lt;span class=&#34;fullname&#34;&gt; : your name&lt;/span&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;---&#xA;title: &#34;PUT YOUR TITLE HERE&#34; &#xA;thumbnail: /blog/assets/101_decision-transformers-train/thumbnail.gif&#xA;---&#xA;&#xA;# Train your first Decision Transformer&#xA;&#xA;&amp;lt;div class=&#34;blog-metadata&#34;&amp;gt;&#xA;    &amp;lt;small&amp;gt;Published September 02, 2022.&amp;lt;/small&amp;gt;&#xA;    &amp;lt;a target=&#34;_blank&#34; class=&#34;btn no-underline text-sm mb-5 font-sans&#34; href=&#34;https://github.com/huggingface/blog/blob/main/decision-transformers-train.md&#34;&amp;gt;&#xA;        Update on GitHub&#xA;    &amp;lt;/a&amp;gt;&#xA;&amp;lt;/div&amp;gt;&#xA;&#xA;&amp;lt;div class=&#34;author-card&#34;&amp;gt;&#xA;    &amp;lt;a href=&#34;/edbeeching&#34;&amp;gt; &#xA;        &amp;lt;img class=&#34;avatar avatar-user&#34; src=&#34;https://aeiljuispo.cloudimg.io/v7/https://s3.amazonaws.com/moonup/production/uploads/1644220542819-noauth.jpeg?w=200&amp;amp;h=200&amp;amp;f=face&#34; title=&#34;Gravatar&#34;&amp;gt;&#xA;        &amp;lt;div class=&#34;bfc&#34;&amp;gt;&#xA;            &amp;lt;code&amp;gt;edbeeching&amp;lt;/code&amp;gt;&#xA;            &amp;lt;span class=&#34;fullname&#34;&amp;gt;Edward Beeching&amp;lt;/span&amp;gt;&#xA;        &amp;lt;/div&amp;gt;&#xA;    &amp;lt;/a&amp;gt;&#xA;    &amp;lt;a href=&#34;/ThomasSimonini&#34;&amp;gt; &#xA;        &amp;lt;img class=&#34;avatar avatar-user&#34; src=&#34;https://aeiljuispo.cloudimg.io/v7/https://s3.amazonaws.com/moonup/production/uploads/1632748593235-60cae820b1c79a3e4b436664.jpeg?w=200&amp;amp;h=200&amp;amp;f=face&#34; title=&#34;Gravatar&#34;&amp;gt;&#xA;        &amp;lt;div class=&#34;bfc&#34;&amp;gt;&#xA;            &amp;lt;code&amp;gt;ThomasSimonini&amp;lt;/code&amp;gt;&#xA;            &amp;lt;span class=&#34;fullname&#34;&amp;gt;Thomas Simonini&amp;lt;/span&amp;gt;&#xA;        &amp;lt;/div&amp;gt;&#xA;    &amp;lt;/a&amp;gt;&#xA;&amp;lt;/div&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;5️⃣ Then, you can add your content. It&#39;s markdown system so if you wrote your text on notion just control shift v to copy/paste as markdown.&lt;/p&gt; &#xA;&lt;p&gt;6️⃣ Modify &lt;code&gt;_blog.yml&lt;/code&gt; to add your blogpost.&lt;/p&gt; &#xA;&lt;p&gt;7️⃣ When your article is ready, &lt;strong&gt;open a pull request&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;8️⃣ The article will be &lt;strong&gt;published automatically when you merge your pull request&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;How to get a responsive thumbnail?&lt;/h2&gt; &#xA;&lt;p&gt;1️⃣ Create a &lt;code&gt;1300x650&lt;/code&gt; image&lt;/p&gt; &#xA;&lt;p&gt;2️⃣ Use &lt;a href=&#34;https://github.com/huggingface/blog/raw/main/assets/thumbnail-template.svg&#34;&gt;this template&lt;/a&gt; and fill the content part.&lt;/p&gt; &#xA;&lt;h2&gt;Using LaTeX&lt;/h2&gt; &#xA;&lt;p&gt;Just add:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;\\(your_latex_here\\)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For instance:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;\\( Q(S_t, A_t) \\)&lt;/code&gt; ➡️ $Q(S_t, A_t)$&lt;/p&gt;</summary>
  </entry>
</feed>