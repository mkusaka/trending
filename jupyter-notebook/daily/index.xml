<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-06-09T01:32:06Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>deepset-ai/haystack-cookbook</title>
    <updated>2024-06-09T01:32:06Z</updated>
    <id>tag:github.com,2024-06-09:/deepset-ai/haystack-cookbook</id>
    <link href="https://github.com/deepset-ai/haystack-cookbook" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üë©üèª‚Äçüç≥ A collection of example notebooks&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üë©üèª‚Äçüç≥ Haystack Cookbook&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://haystack.deepset.ai/&#34;&gt;&lt;img src=&#34;https://github.com/deepset-ai/haystack/raw/main/docs/img/banner_20.png&#34; alt=&#34;Green logo of a stylized white &#39;H&#39; with the text &#39;Haystack, by deepset. Haystack 2.0 is live üéâ&#39;&amp;nbsp;Abstract green and yellow diagrams in the background.&#34;&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;A collection of example notebooks using &lt;a href=&#34;https://github.com/deepset-ai/haystack&#34;&gt;Haystack&lt;/a&gt; üëá&lt;/p&gt; &#xA;&lt;p&gt;You can use these examples as guidelines on how to make use of different model providers, vector databases, retrieval techniques and more with Haystack. Most of them showcase a specific, small demo.&lt;/p&gt; &#xA;&lt;p&gt;To learn more about &lt;em&gt;how&lt;/em&gt; to use Haystack, please visit our &lt;a href=&#34;https://docs.haystack.deepset.ai/docs&#34;&gt;Docs&lt;/a&gt; and official &lt;a href=&#34;https://haystack.deepset.ai/tutorials&#34;&gt;Tutorials&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For more examples, you may also find our &lt;a href=&#34;https://haystack.deepset.ai/blog&#34;&gt;Blog&lt;/a&gt; useful.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Unless &#39;(Haystack 1.x)&#39; is mentioned in the title, all of these examples use Haystack 2.0 onwards.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Colab&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Speaker Diarization with AssemblyAI&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/using_speaker_diarization_with_assemblyai.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Advance Prompt Customization for Anthropic&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/prompt_customization_for_Anthropic.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Techcrunch News Digest with Local LLMs using TitanML Takeoff&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/10EralM_8pCJ5nXnGIZYr6atqefmi8r2z?usp=sharing&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Use Gemini Models with Vertex AI&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/vertexai-gemini-examples.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Gradient AI Embedders and Generators for RAG&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/gradient-embeders-and-generators-for-notion-rag.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mixtral 8x7B with Hugging Face TGI for Web QA&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/mixtral-8x7b-for-web-qa.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Amazon Bedrock and OpenSearch for PDF QA&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/amazon_bedrock_for_documentation_qa.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Use Zephyr 7B Beta with Hugging Face for RAG&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/zephyr-7b-beta-for-rag.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hacker News RAG with Custom Component&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/hackernews-custom-component-rag.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Use Chroma for RAG and Indexing&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/chroma-indexing-and-rag-examples.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Using the Jina-embeddings-v2-base-en model in a Haystack RAG pipeline for legal document analsysis&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/jina-embeddings-v2-legal-analysis-rag.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Multilingual RAG from a podcast with Whisper, Qdrant and Mistral&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/multilingual_rag_podcast.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Improve retrieval by embedding meaningful metadata&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/improve-retrieval-by-embedding-metadata.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Information extraction via LLMs (Gorilla OpenFunctions)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/information-extraction-gorilla.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Information extraction via LLMs (NexusRaven)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/information_extraction_raven.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Using AstraDB as a data store in your Haystack pipelines&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/astradb_haystack_integration.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Streaming model explorer: compare how different models handle the same prompt.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/model_explorer_streaming.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Function Calling with OpenAIChatGenerator&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/function_calling_with_OpenAIChatGenerator.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Use the vLLM inference engine in Haystack 2.x&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/vllm_inference_engine.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Build with Google Gemma: chat and RAG&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/gemma_chat_rag.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Optimizing Retrieval with HyDE&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/using_hyde_for_improved_retrieval.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RAG pipeline using FastEmbed for embeddings generation&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/rag_fastembed.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Air-Gapped RAG pipelines with NVIDIA NIMs&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/rag-with-nims.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Evaluate a RAG pipeline using Haystack-UpTrain integration&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/rag_eval_uptrain.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RAG on the Oscars using Llama 3 models&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/llama3_rag.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chatting with SQL Databases&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/chat_with_SQL_3_ways.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Evaluate a RAG pipeline using DeepEval integration&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/rag_eval_deep_eval.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Evaluate a RAG pipeline using Ragas integration&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/rag_eval_ragas.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Sparse Embedding Retrieval with Qdrant and FastEmbed&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/sparse_embedding_retrieval.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Extract Metadata Filters from a Query&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/extracting_metadata_filters_from_a_user_query.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Run tasks concurrently within a custom component&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/concurrent_tasks.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Prompt Optimization with DSPy&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/prompt_optimization_with_dspy.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Cohere for Multilingual QA (Haystack 1.x)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/haystack-1.x/cohere-for-multilingual-qa.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-4 and Weaviate for Custom Documentation QA (Haystack 1.x)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/haystack-1.x/gpt4-weaviate-custom-documentation-qa.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Whisper Transcriber and Weaviate for YouTube video QA (Haystack 1.x)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/haystack-1.x/whisper-and-weaviate-for-youtube-rag.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;How to Contribute to this repository&lt;/h2&gt; &#xA;&lt;p&gt;If you have an example that uses Haystack, you can add it to this repository by creating a PR. You can also create a PR from Colab by creating a Fork of this repository and selecting &#34;Save a Copy to GitHub&#34;. Once you add your example to your fork, you can create a PR onto this repository.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Add your Notebook&lt;/li&gt; &#xA; &lt;li&gt;Give a descriptive name to your file that includes the names of (if applicable) the model providers, databases the technologies you use in your example and/or the task you are completing in the example.&lt;/li&gt; &#xA; &lt;li&gt;Make sure to add a row in the table above üéâ&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>Portkey-AI/gateway</title>
    <updated>2024-06-09T01:32:06Z</updated>
    <id>tag:github.com,2024-06-09:/Portkey-AI/gateway</id>
    <link href="https://github.com/Portkey-AI/gateway" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Blazing Fast AI Gateway. Route to 200+ LLMs with 1 fast &amp; friendly API.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p align=&#34;right&#34;&gt; &lt;strong&gt;English&lt;/strong&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/Portkey-AI/gateway/main/README.cn.md&#34;&gt;‰∏≠Êñá&lt;/a&gt; &lt;/p&gt; &#xA; &lt;h1&gt;AI Gateway&lt;/h1&gt; &#xA; &lt;h4&gt;Reliably route to 200+ LLMs with 1 fast &amp;amp; friendly API&lt;/h4&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Portkey-AI/gateway/main/docs/images/demo.gif&#34; width=&#34;650&#34; alt=&#34;Gateway Demo&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Portkey-AI/gateway/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/Ileriayo/markdown-badges&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://portkey.ai/community&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1143393887742861333&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/portkeyai&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/url/https/twitter/follow/portkeyai?style=social&amp;amp;label=Follow%20%40PortkeyAI&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.npmjs.com/package/@portkey-ai/gateway&#34;&gt;&lt;img src=&#34;https://badge.fury.io/js/%40portkey-ai%2Fgateway.svg?sanitize=true&#34; alt=&#34;npm version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://replit.com/@portkey/AI-Gateway?v=1&#34;&gt;&lt;img src=&#34;https://replit.com/badge?caption=Deploy%20on%20Replit&#34; width=&#34;99&#34; style=&#34;display:block;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Gateway streamlines requests to 200+ open &amp;amp; closed source models with a unified API. It is also production-ready with support for caching, fallbacks, retries, timeouts, loadbalancing, and can be edge-deployed for minimum latency.&lt;/p&gt; &#xA;&lt;p&gt;‚úÖ&amp;nbsp; &lt;strong&gt;Blazing fast&lt;/strong&gt; (9.9x faster) with a &lt;strong&gt;tiny footprint&lt;/strong&gt; (~45kb installed) &lt;br&gt; ‚úÖ&amp;nbsp; &lt;strong&gt;Load balance&lt;/strong&gt; across multiple models, providers, and keys &lt;br&gt; ‚úÖ&amp;nbsp; &lt;strong&gt;Fallbacks&lt;/strong&gt; make sure your app stays resilient &lt;br&gt; ‚úÖ&amp;nbsp; &lt;strong&gt;Automatic Retries&lt;/strong&gt; with exponential fallbacks come by default &lt;br&gt; ‚úÖ&amp;nbsp; &lt;strong&gt;Configurable Request Timeouts&lt;/strong&gt; to easily handle unresponsive LLM requests &lt;br&gt; ‚úÖ&amp;nbsp; &lt;strong&gt;Multimodal&lt;/strong&gt; to support routing between Vision, TTS, STT, Image Gen, and more models &lt;br&gt; ‚úÖ&amp;nbsp; &lt;strong&gt;Plug-in&lt;/strong&gt; middleware as needed &lt;br&gt; ‚úÖ&amp;nbsp; Battle tested over &lt;strong&gt;300B tokens&lt;/strong&gt; &lt;br&gt; ‚úÖ&amp;nbsp; &lt;strong&gt;Enterprise-ready&lt;/strong&gt; for enhanced security, scale, and custom deployments &lt;br&gt; &lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How to Run the Gateway?&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Portkey-AI/gateway/main/#run-it-locally&#34;&gt;Run it Locally&lt;/a&gt; for complete control &amp;amp; customization&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Portkey-AI/gateway/main/#gateway-hosted-by-portkey&#34;&gt;Hosted by Portkey&lt;/a&gt; for quick setup without infrastructure concerns&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Portkey-AI/gateway/main/#gateway-enterprise-version&#34;&gt;Enterprise On-Prem&lt;/a&gt; for advanced features and dedicated support&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Run it Locally&lt;/h3&gt; &#xA;&lt;p&gt;Run the following command in your terminal and it will spin up the Gateway on your local system:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npx @portkey-ai/gateway&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;sup&gt;Your AI Gateway is now running on &lt;a href=&#34;http://localhost:8787&#34;&gt;http://localhost:8787&lt;/a&gt; üöÄ&lt;/sup&gt;&lt;/p&gt; &#xA;&lt;p&gt;Gateway is also edge-deployment ready. Explore Cloudflare, Docker, AWS etc. deployment &lt;a href=&#34;https://raw.githubusercontent.com/Portkey-AI/gateway/main/#deploying-the-ai-gateway&#34;&gt;guides here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Gateway Hosted by Portkey&lt;/h3&gt; &#xA;&lt;p&gt;This same open-source Gateway powers Portkey API that processes &lt;strong&gt;billions of tokens&lt;/strong&gt; daily and is in production with companies like Postman, Haptik, Turing, MultiOn, SiteGPT, and more.&lt;/p&gt; &#xA;&lt;p&gt;Sign up for the free developer plan (10K request/month) &lt;a href=&#34;https://app.portkey.ai/&#34;&gt;here&lt;/a&gt; or &lt;a href=&#34;https://calendly.com/rohit-portkey/noam&#34;&gt;discuss here&lt;/a&gt; for enterprise deployments.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;How to Use the Gateway?&lt;/h2&gt; &#xA;&lt;h3&gt;Compatible with OpenAI API &amp;amp; SDK&lt;/h3&gt; &#xA;&lt;p&gt;Gateway is fully compatible with the OpenAI API &amp;amp; SDK, and extends them to call 200+ LLMs and makes them reliable. To use the Gateway through OpenAI, you only need to update your &lt;code&gt;base_URL&lt;/code&gt; and pass the provider name in headers.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To use through Portkey, set your &lt;code&gt;base_URL&lt;/code&gt; to: &lt;code&gt;https://api.portkey.ai/v1&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;To run locally, set: &lt;code&gt;http://localhost:8787/v1&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Let&#39;s see how we can use the Gateway to make an Anthropic request in OpenAI spec below - the same will follow for all the other providers.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Python-logo-notext.svg/1869px-Python-logo-notext.svg.png&#34; height=&#34;20&#34;&gt; Python&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install portkey-ai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1hLvoq_VdGlJ_92sPPiwTznSra5Py0FuW?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;While instantiating your OpenAI client,&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Set the &lt;code&gt;base_URL&lt;/code&gt; to &lt;code&gt;http://localhost:8787/v1&lt;/code&gt; (or &lt;code&gt;PORTKEY_GATEWAY_URL&lt;/code&gt; through the Portkey SDK if you&#39;re using the hosted version)&lt;/li&gt; &#xA; &lt;li&gt;Pass the provider name in the &lt;code&gt;default_headers&lt;/code&gt; param (here we are using &lt;code&gt;createHeaders&lt;/code&gt; method with the Portkey SDK to auto-create the full header)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from openai import OpenAI&#xA;from portkey_ai import PORTKEY_GATEWAY_URL, createHeaders&#xA;&#xA;gateway = OpenAI(&#xA;    api_key=&#34;ANTHROPIC_API_KEY&#34;,&#xA;    base_url=PORTKEY_GATEWAY_URL, # Or http://localhost:8787/v1 when running locally&#xA;    default_headers=createHeaders(&#xA;        provider=&#34;anthropic&#34;,&#xA;        api_key=&#34;PORTKEY_API_KEY&#34; # Grab from https://app.portkey.ai # Not needed when running locally&#xA;    )&#xA;)&#xA;&#xA;chat_complete = gateway.chat.completions.create(&#xA;    model=&#34;claude-3-sonnet-20240229&#34;,&#xA;    messages=[{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;What&#39;s a fractal?&#34;}],&#xA;    max_tokens=512&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to run the Gateway locally, don&#39;t forget to run &lt;code&gt;npx @portkey-ai/gateway&lt;/code&gt; in your terminal before this! Otherwise just &lt;a href=&#34;https://app.portkey.ai/&#34;&gt;sign up on Portkey&lt;/a&gt; and keep your Portkey API Key handy.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;img src=&#34;https://cdn-icons-png.flaticon.com/512/5968/5968322.png&#34; height=&#34;20&#34;&gt; Node.JS&lt;/h3&gt; &#xA;&lt;p&gt;Works the same as in Python. Add &lt;code&gt;baseURL&lt;/code&gt; &amp;amp; &lt;code&gt;defaultHeaders&lt;/code&gt; while instantiating your OpenAI client and pass the relevant provider details.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm install portkey-ai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;import OpenAI from &#39;openai&#39;;&#xA;import { PORTKEY_GATEWAY_URL, createHeaders } from &#39;portkey-ai&#39;&#xA; &#xA;const gateway = new OpenAI({&#xA;   apiKey: &#34;ANTHROPIC_API_KEY&#34;,&#xA;    baseURL: PORTKEY_GATEWAY_URL, // Or http://localhost:8787/v1 when running locally&#xA;    defaultHeaders: createHeaders({&#xA;        provider: &#34;anthropic&#34;,&#xA;        apiKey: &#34;PORTKEY_API_KEY&#34; // Grab from https://app.portkey.ai / Not needed when running locally&#xA;  })&#xA;});&#xA;&#xA;async function main(){&#xA;   const chatCompletion = await portkey.chat.completions.create({&#xA;      messages: [{ role: &#39;user&#39;, content: &#39;Who are you?&#39; }],&#xA;      model: &#39;claude-3-sonnet-20240229&#39;,&#xA;      maxTokens:512&#xA;   });&#xA;   console.log(chatCompletion.choices[0].message.content);&#xA;}&#xA;&#xA;main()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;img src=&#34;https://www.svgrepo.com/show/305922/curl.svg?sanitize=true&#34; height=&#34;20&#34;&gt; REST&lt;/h3&gt; &#xA;&lt;p&gt;In your OpenAI REST request,&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Change the request URL to &lt;code&gt;https://api.portkey.ai/v1&lt;/code&gt; (or &lt;code&gt;http://localhost:8787/v1&lt;/code&gt; if you&#39;re hosting locally)&lt;/li&gt; &#xA; &lt;li&gt;Pass an additional &lt;code&gt;x-portkey-provider&lt;/code&gt; header with the provider&#39;s name&lt;/li&gt; &#xA; &lt;li&gt;Change the model&#39;s name to &lt;code&gt;claude-3&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl &#39;http://localhost:8787/v1/chat/completions&#39; \&#xA;  -H &#39;x-portkey-provider: anthropic&#39; \&#xA;  -H &#34;Authorization: Bearer $ANTHROPIC_API_KEY&#34; \&#xA;  -H &#39;Content-Type: application/json&#39; \&#xA;  -d &#39;{ &#34;model&#34;: &#34;claude-3-haiku-20240229&#34;, &#34;messages&#34;: [{&#34;role&#34;: &#34;user&#34;,&#34;content&#34;: &#34;Hi&#34;}] }&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For other providers, change the &lt;code&gt;provider&lt;/code&gt; &amp;amp; &lt;code&gt;model&lt;/code&gt; to their respective values.&lt;/p&gt; &#xA;&lt;h2&gt;Gateway Cookbooks&lt;/h2&gt; &#xA;&lt;h3&gt;Trending Cookbooks&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Portkey-AI/gateway/main/cookbook/use-cases/run-gateway-on-prompts-from-langchain-hub.md&#34;&gt;Run Gateway on prompts from Langchain hub&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Portkey-AI/gateway/main/cookbook/integrations/vercel-ai.md&#34;&gt;Use Porkey Gateway with Vercel&#39;s AI SDK&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Portkey-AI/gateway/main/cookbook/getting-started/fallback-from-stable-diffusion-to-dall-e.ipynb&#34;&gt;Set up fallback from SDXL to Dall-E-3&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Latest Cookbooks&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Portkey-AI/gateway/main/cookbook/use-cases/LMSYS%20Series/comparing-top10-LMSYS-models-with-Portkey.ipynb&#34;&gt;Comparing Top 10 LMSYS Models with Portkey&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Portkey-AI/gateway/main/cookbook/getting-started/fallback-from-openai-to-azure.ipynb&#34;&gt;Fallback from OpenAI to Azure OpenAI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Portkey-AI/gateway/main/cookbook/getting-started/automatic-retries-on-failures.md&#34;&gt;Set up automatic retries for failed requests&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Portkey-AI/gateway/main/cookbook/use-cases/llama-3-on-groq.ipynb&#34;&gt;Call Llama 3 on Groq&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Portkey-AI/gateway/main/examples/&#34;&gt;More Examples&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h2&gt;Supported Providers&lt;/h2&gt; &#xA;&lt;p&gt;Explpore Gateway integrations with &lt;a href=&#34;https://portkey.ai/docs/welcome/integration-guides&#34;&gt;20+ providers&lt;/a&gt; and &lt;a href=&#34;https://portkey.ai/docs/welcome/integration-guides&#34;&gt;6+ frameworks&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Provider&lt;/th&gt; &#xA;   &lt;th&gt;Support&lt;/th&gt; &#xA;   &lt;th&gt;Stream&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Portkey-AI/gateway/main/docs/images/openai.png&#34; width=&#34;35&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://portkey.ai/docs/welcome/integration-guides/openai&#34;&gt;OpenAI&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Portkey-AI/gateway/main/docs/images/azure.png&#34; width=&#34;35&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://portkey.ai/docs/welcome/integration-guides/azure-openai&#34;&gt;Azure OpenAI&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Portkey-AI/gateway/main/docs/images/anyscale.png&#34; width=&#34;35&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://portkey.ai/docs/welcome/integration-guides/anyscale-llama2-mistral-zephyr&#34;&gt;Anyscale&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/2/2d/Google-favicon-2015.png&#34; width=&#34;35&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://portkey.ai/docs/welcome/integration-guides/gemini&#34;&gt;Google Gemini &amp;amp; Palm&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Portkey-AI/gateway/main/docs/images/anthropic.png&#34; width=&#34;35&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://portkey.ai/docs/welcome/integration-guides/anthropic&#34;&gt;Anthropic&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Portkey-AI/gateway/main/docs/images/cohere.png&#34; width=&#34;35&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://portkey.ai/docs/welcome/integration-guides/cohere&#34;&gt;Cohere&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://assets-global.website-files.com/64f6f2c0e3f4c5a91c1e823a/654693d569494912cfc0c0d4_favicon.svg?sanitize=true&#34; width=&#34;35&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://portkey.ai/docs/welcome/integration-guides/together-ai&#34;&gt;Together AI&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://www.perplexity.ai/favicon.svg?sanitize=true&#34; width=&#34;35&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://portkey.ai/docs/welcome/integration-guides/perplexity-ai&#34;&gt;Perplexity&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://docs.mistral.ai/img/favicon.ico&#34; width=&#34;35&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://portkey.ai/docs/welcome/integration-guides/mistral-ai&#34;&gt;Mistral&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://docs.nomic.ai/img/nomic-logo.png&#34; width=&#34;35&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://portkey.ai/docs/welcome/integration-guides/nomic&#34;&gt;Nomic&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://files.readme.io/d38a23e-small-studio-favicon.png&#34; width=&#34;35&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://portkey.ai/docs/welcome/integration-guides&#34;&gt;AI21&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://platform.stability.ai/small-logo-purple.svg?sanitize=true&#34; width=&#34;35&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://portkey.ai/docs/welcome/integration-guides/stability-ai&#34;&gt;Stability AI&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://deepinfra.com/_next/static/media/logo.4a03fd3d.svg?sanitize=true&#34; width=&#34;35&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://portkey.ai/docs/welcome/integration-guides&#34;&gt;DeepInfra&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://ollama.com/public/ollama.png&#34; width=&#34;35&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://portkey.ai/docs/welcome/integration-guides/ollama&#34;&gt;Ollama&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://novita.ai/favicon.ico&#34; width=&#34;35&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Novita AI&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://portkey.ai/docs/welcome/what-is-portkey#ai-providers-supported&#34;&gt;View the complete list of 200+ supported models here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Reliability Features&lt;/h2&gt; &#xA;&lt;table width=&#34;100%&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;50%&#34;&gt; &lt;h4&gt;&lt;a href=&#34;https://portkey.ai/docs/product/ai-gateway-streamline-llm-integrations/fallbacks&#34;&gt;Fallbacks&lt;/a&gt;&lt;/h4&gt; This feature allows you to specify a prioritized list of LLMs. If the primary LLM fails, Portkey will automatically fallback to the next LLM in the list to ensure reliability. &lt;br&gt;&lt;br&gt; &lt;img src=&#34;https://framerusercontent.com/images/gmlOW8yeKP2pGuIsObM6gKLzeMI.png&#34; height=&#34;100&#34;&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;50%&#34;&gt; &lt;h4&gt;&lt;a href=&#34;https://portkey.ai/docs/product/ai-gateway-streamline-llm-integrations/automatic-retries&#34;&gt;Automatic Retries&lt;/a&gt;&lt;/h4&gt; AI Gateway can automatically retry failed requests up to 5 times. A backoff strategy spaces out retry attempts to prevent network overload. &lt;br&gt;&lt;br&gt; &lt;img src=&#34;https://github.com/roh26it/Rubeus/assets/971978/8a6e653c-94b2-4ba7-95c7-93544ee476b1&#34; height=&#34;100&#34;&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;table width=&#34;100%&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;50%&#34;&gt; &lt;h4&gt;&lt;a href=&#34;https://portkey.ai/docs/product/ai-gateway-streamline-llm-integrations/load-balancing&#34;&gt;Load Balancing&lt;/a&gt;&lt;/h4&gt; Distribute load effectively across multiple API keys or providers based on custom weights to ensure high availability and optimal performance. &lt;br&gt;&lt;br&gt; &lt;img src=&#34;https://framerusercontent.com/images/6EWuq3FWhqrPe3kKLqVspevi4.png&#34; height=&#34;100&#34;&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;50%&#34;&gt; &lt;h4&gt;&lt;a href=&#34;https://portkey.ai/docs/product/ai-gateway-streamline-llm-integrations/request-timeouts&#34;&gt;Request Timeouts&lt;/a&gt;&lt;/h4&gt; Manage unruly LLMs &amp;amp; latencies by setting up granular request timeouts, allowing automatic termination of requests that exceed a specified duration. &lt;br&gt;&lt;br&gt; &lt;img src=&#34;https://github.com/vrushankportkey/gateway/assets/134934501/b23b98b2-6451-4747-8898-6847ad8baed4&#34; height=&#34;100&#34;&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h4&gt;Reliability features are set by passing a relevant Gateway Config (JSON) with the &lt;code&gt;x-portkey-config&lt;/code&gt; header or with the &lt;code&gt;config&lt;/code&gt; param in the SDKs&lt;/h4&gt; &#xA;&lt;h3&gt;Example: Setting up Fallback from OpenAI to Anthropic&lt;/h3&gt; &#xA;&lt;h4&gt;Write the fallback logic&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;strategy&#34;: { &#34;mode&#34;: &#34;fallback&#34; },&#xA;  &#34;targets&#34;: [&#xA;    { &#34;provider&#34;: &#34;openai&#34;, &#34;api_key&#34;: &#34;OPENAI_API_KEY&#34; },&#xA;    { &#34;provider&#34;: &#34;anthropic&#34;, &#34;api_key&#34;: &#34;ANTHROPIC_API_KEY&#34; }&#xA;  ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Use it while making your request&lt;/h4&gt; &#xA;&lt;p&gt;Portkey Gateway will automatically trigger Anthropic if the OpenAI request fails:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;REST&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl &#39;http://localhost:8787/v1/chat/completions&#39; \&#xA;  -H &#39;x-portkey-provider: google&#39; \&#xA;  -H &#39;x-portkey-config: $CONFIG&#39; \&#xA;  -H &#34;Authorization: Bearer $GOOGLE_AI_STUDIO_KEY&#34; \&#xA;  -H &#39;Content-Type: application/json&#39; \&#xA;  -d &#39;{ &#34;model&#34;: &#34;gemini-1.5-pro-latest&#34;, &#34;messages&#34;: [{&#34;role&#34;: &#34;user&#34;,&#34;content&#34;: &#34;Hi&#34;}] }&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also trigger Fallbacks only on specific status codes by passing an array of status codes with the &lt;code&gt;on_status_codes&lt;/code&gt; param in &lt;code&gt;strategy&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://portkey.ai/docs/product/ai-gateway-streamline-llm-integrations/fallbacks&#34;&gt;Read the full Fallback documentation here.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Example: Loadbalance Requests across 3 Accounts&lt;/h3&gt; &#xA;&lt;h4&gt;Write the loadbalancer config&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;strategy&#34;: { &#34;mode&#34;: &#34;loadbalance&#34; },&#xA;  &#34;targets&#34;: [&#xA;    { &#34;provider&#34;: &#34;openai&#34;, &#34;api_key&#34;: &#34;ACCOUNT_1_KEY&#34;, &#34;weight&#34;: 1 },&#xA;    { &#34;provider&#34;: &#34;openai&#34;, &#34;api_key&#34;: &#34;ACCOUNT_2_KEY&#34;, &#34;weight&#34;: 1 },&#xA;    { &#34;provider&#34;: &#34;openai&#34;, &#34;api_key&#34;: &#34;ACCOUNT_3_KEY&#34;, &#34;weight&#34;: 1 }&#xA;  ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Pass the config while instantiating OpenAI client&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import OpenAI from &#39;openai&#39;;&#xA;import { PORTKEY_GATEWAY_URL, createHeaders } from &#39;portkey-ai&#39;&#xA; &#xA;const gateway = new OpenAI({&#xA;  baseURL: PORTKEY_GATEWAY_URL,&#xA;  defaultHeaders: createHeaders({&#xA;    apiKey: &#34;PORTKEY_API_KEY&#34;,&#xA;    config: &#34;CONFIG_ID&#34;&#xA;  })&#xA;});&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://portkey.ai/docs/product/ai-gateway-streamline-llm-integrations/load-balancing&#34;&gt;Read the Loadbalancing docs here.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Automatic Retries&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Similarly, you can write a Config that will attempt retries up to 5 times&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;    &#34;retry&#34;: { &#34;attempts&#34;: 5 }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://portkey.ai/docs/product/ai-gateway-streamline-llm-integrations/automatic-retries&#34;&gt;Read the full Retries documentation here.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Request Timeouts&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Here, the request timeout of 10 seconds will be applied to *all* the targets.&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;strategy&#34;: { &#34;mode&#34;: &#34;fallback&#34; },&#xA;  &#34;request_timeout&#34;: 10000,&#xA;  &#34;targets&#34;: [&#xA;    { &#34;virtual_key&#34;: &#34;open-ai-xxx&#34; },&#xA;    { &#34;virtual_key&#34;: &#34;azure-open-ai-xxx&#34; }&#xA;  ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://portkey.ai/docs/product/ai-gateway-streamline-llm-integrations/request-timeouts&#34;&gt;Read the full Request Timeouts documentation here.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Using Gateway Configs&lt;/h3&gt; &#xA;&lt;p&gt;Here&#39;s a guide to &lt;a href=&#34;https://portkey.ai/docs/api-reference/config-object&#34;&gt;use the config object in your request&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Supported SDKs&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Language&lt;/th&gt; &#xA;   &lt;th&gt;Supported SDKs&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Node.js / JS / TS&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.npmjs.com/package/portkey-ai&#34;&gt;Portkey SDK&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://www.npmjs.com/package/openai&#34;&gt;OpenAI SDK&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://www.npmjs.com/package/langchain&#34;&gt;LangchainJS&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://www.npmjs.com/package/llamaindex&#34;&gt;LlamaIndex.TS&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Python&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pypi.org/project/portkey-ai/&#34;&gt;Portkey SDK&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://portkey.ai/docs/welcome/integration-guides/openai&#34;&gt;OpenAI SDK&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://portkey.ai/docs/welcome/integration-guides/langchain-python&#34;&gt;Langchain&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://portkey.ai/docs/welcome/integration-guides/llama-index-python&#34;&gt;LlamaIndex&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Go&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/sashabaranov/go-openai&#34;&gt;go-openai&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Java&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/TheoKanning/openai-java&#34;&gt;openai-java&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Rust&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.rs/async-openai/latest/async_openai/&#34;&gt;async-openai&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ruby&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/alexrudall/ruby-openai&#34;&gt;ruby-openai&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Deploying the AI Gateway&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Portkey-AI/gateway/main/docs/installation-deployments.md&#34;&gt;See docs&lt;/a&gt; on installing the AI Gateway locally or deploying it on popular locations.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Deploy to &lt;a href=&#34;https://raw.githubusercontent.com/Portkey-AI/gateway/main/docs/installation-deployments.md#deploy-to-app-stack&#34;&gt;App Stack&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Deploy to &lt;a href=&#34;https://github.com/Portkey-AI/gateway/raw/main/docs/installation-deployments.md#deploy-to-cloudflare-workers&#34;&gt;Cloudflare Workers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Deploy using &lt;a href=&#34;https://github.com/Portkey-AI/gateway/raw/main/docs/installation-deployments.md#deploy-using-docker&#34;&gt;Docker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Deploy using &lt;a href=&#34;https://github.com/Portkey-AI/gateway/raw/main/docs/installation-deployments.md#deploy-using-docker-compose&#34;&gt;Docker Compose&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Deploy to &lt;a href=&#34;https://github.com/Portkey-AI/gateway/raw/main/docs/installation-deployments.md#deploy-to-zeabur&#34;&gt;Zeabur&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run a &lt;a href=&#34;https://github.com/Portkey-AI/gateway/raw/main/docs/installation-deployments.md#run-a-nodejs-server&#34;&gt;Node.js server&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Gateway Enterprise Version&lt;/h2&gt; &#xA;&lt;p&gt;Make your AI app more &lt;ins&gt;reliable&lt;/ins&gt; and &lt;ins&gt;forward compatible&lt;/ins&gt;, while ensuring complete &lt;ins&gt;data security&lt;/ins&gt; and &lt;ins&gt;privacy&lt;/ins&gt;.&lt;/p&gt; &#xA;&lt;p&gt;‚úÖ&amp;nbsp; Secure Key Management - for role-based access control and tracking &lt;br&gt; ‚úÖ&amp;nbsp; Simple &amp;amp; Semantic Caching - to serve repeat queries faster &amp;amp; save costs &lt;br&gt; ‚úÖ&amp;nbsp; Access Control &amp;amp; Inbound Rules - to control which IPs and Geos can connect to your deployments &lt;br&gt; ‚úÖ&amp;nbsp; PII Redaction - to automatically remove sensitive data from your requests to prevent indavertent exposure &lt;br&gt; ‚úÖ&amp;nbsp; SOC2, ISO, HIPAA, GDPR Compliances - for best security practices &lt;br&gt; ‚úÖ&amp;nbsp; Professional Support - along with feature prioritization &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://calendly.com/rohit-portkey/noam&#34;&gt;Schedule a call to discuss enterprise deployments&lt;/a&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;The easiest way to contribute is to pick any issue with the &lt;code&gt;good first issue&lt;/code&gt; tag üí™. Read the Contributing guidelines &lt;a href=&#34;https://raw.githubusercontent.com/Portkey-AI/gateway/main/CONTRIBUTING.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Bug Report? &lt;a href=&#34;https://github.com/Portkey-AI/gateway/issues&#34;&gt;File here&lt;/a&gt; | Feature Request? &lt;a href=&#34;https://github.com/Portkey-AI/gateway/issues&#34;&gt;File here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;p&gt;Join our growing community around the world, for help, ideas, and discussions on AI.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;View our official &lt;a href=&#34;https://portkey.ai/blog&#34;&gt;Blog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Chat with us on &lt;a href=&#34;https://portkey.ai/community&#34;&gt;Discord&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Follow us on &lt;a href=&#34;https://twitter.com/PortkeyAI&#34;&gt;Twitter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Connect with us on &lt;a href=&#34;https://www.linkedin.com/company/portkey-ai/&#34;&gt;LinkedIn&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- - Visit us on [YouTube](https://www.youtube.com/channel/UCZph50gLNXAh1DpmeX8sBdw) --&gt; &#xA;&lt;!-- - Join our [Dev community](https://dev.to/portkeyai) --&gt; &#xA;&lt;!-- - Questions tagged #portkey on [Stack Overflow](https://stackoverflow.com/questions/tagged/portkey) --&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/Portkey-AI/gateway/assets/971978/89d6f0af-a95d-4402-b451-14764c40d03f&#34; alt=&#34;Rubeus Social Share (4)&#34;&gt;&lt;/p&gt;</summary>
  </entry>
</feed>