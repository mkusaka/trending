<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-02-17T01:29:15Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>gkamradt/LLMTest_NeedleInAHaystack</title>
    <updated>2024-02-17T01:29:15Z</updated>
    <id>tag:github.com,2024-02-17:/gkamradt/LLMTest_NeedleInAHaystack</id>
    <link href="https://github.com/gkamradt/LLMTest_NeedleInAHaystack" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Doing simple retrieval from LLM models at various context lengths to measure accuracy&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Needle In A Haystack - Pressure Testing LLMs&lt;/h1&gt; &#xA;&lt;p&gt;Supported model providers: OpenAI, Anthropic&lt;/p&gt; &#xA;&lt;p&gt;A simple &#39;needle in a haystack&#39; analysis to test in-context retrieval ability of long context LLMs.&lt;/p&gt; &#xA;&lt;p&gt;Get the behind the scenes on the &lt;a href=&#34;https://youtu.be/KwRRuiCCdmc&#34;&gt;overview video&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gkamradt/LLMTest_NeedleInAHaystack/main/img/NeedleHaystackCodeSnippet.png&#34; alt=&#34;GPT-4-128 Context Testing&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/gkamradt/LLMTest_NeedleInAHaystack.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;The Test&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Place a random fact or statement (the &#39;needle&#39;) in the middle of a long context window (the &#39;haystack&#39;)&lt;/li&gt; &#xA; &lt;li&gt;Ask the model to retrieve this statement&lt;/li&gt; &#xA; &lt;li&gt;Iterate over various document depths (where the needle is placed) and context lengths to measure performance&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;This is the code that backed &lt;a href=&#34;https://twitter.com/GregKamradt/status/1722386725635580292&#34;&gt;this OpenAI&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/GregKamradt/status/1727018183608193393&#34;&gt;Anthropic analysis&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If ran and &lt;code&gt;save_results = True&lt;/code&gt;, then this script will populate a &lt;code&gt;result/&lt;/code&gt; directory with evaluation information. Due to potential concurrent requests each new test will be saved as a few file.&lt;/p&gt; &#xA;&lt;p&gt;I&#39;ve put the results from the original tests in &lt;code&gt;/original_results&lt;/code&gt;. I&#39;ve upgraded the script since those test were ran so the data formats may not match your script results.&lt;/p&gt; &#xA;&lt;p&gt;The key parameters:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;needle&lt;/code&gt; - The statement or fact which will be placed in your context (&#39;haystack&#39;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;haystack_dir&lt;/code&gt; - The directory which contains the text files to load as background context. Only text files are supported&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;retrieval_question&lt;/code&gt; - The question with which to retrieve your needle in the background context&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;results_version&lt;/code&gt; - You may want to run your test multiple times for the same combination of length/depth, change the version number if so&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;context_lengths_min&lt;/code&gt; - The starting point of your context lengths list to iterate&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;context_lengths_max&lt;/code&gt; - The ending point of your context lengths list to iterate&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;context_lengths_num_intervals&lt;/code&gt; - The number of intervals between your min/max to iterate through&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;document_depth_percent_min&lt;/code&gt; - The starting point of your document depths. Should be int &amp;gt; 0&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;document_depth_percent_max&lt;/code&gt; - The ending point of your document depths. Should be int &amp;lt; 100&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;document_depth_percent_intervals&lt;/code&gt; - The number of iterations to do between your min/max points&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;document_depth_percent_interval_type&lt;/code&gt; - Determines the distribution of depths to iterate over. &#39;linear&#39; or &#39;sigmoid&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;model_provider&lt;/code&gt; - &#39;OpenAI&#39; or &#39;Anthropic&#39;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;model_name&lt;/code&gt; - The name of the model you&#39;d like to test. Should match the exact value which needs to be passed to the api. Ex: &lt;code&gt;gpt-4-1106-preview&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;save_results&lt;/code&gt; - Whether or not you&#39;d like to save your results to file. They will be temporarily saved in the object regardless. True/False&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;save_contexts&lt;/code&gt; - Whether or not you&#39;d like to save your contexts to file. &lt;strong&gt;Warning&lt;/strong&gt; these will get very long. True/False&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Other Parameters:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;context_lengths&lt;/code&gt; - A custom set of context lengths. This will override the values set for &lt;code&gt;context_lengths_min&lt;/code&gt;, max, and intervals if set&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;document_depth_percents&lt;/code&gt; - A custom set of document depths lengths. This will override the values set for &lt;code&gt;document_depth_percent_min&lt;/code&gt;, max, and intervals if set&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;openai_api_key&lt;/code&gt; - Must be supplied. GPT-4 is used for evaluation. Can either be passed when creating the object or an environment variable&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;anthropic_api_key&lt;/code&gt; - Only needed if testing Anthropic models. Can either be passed when creating the object or an environment variable&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;num_concurrent_requests&lt;/code&gt; - Default: 1. Set higher if you&#39;d like to run more requests in parallel. Keep in mind rate limits.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;final_context_length_buffer&lt;/code&gt; - The amount of context to take off each input to account for system messages and output tokens. This can be more intelligent but using a static value for now. Default 200 tokens.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;seconds_to_sleep_between_completions&lt;/code&gt; - Default: None, set # of seconds if you&#39;d like to slow down your requests&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;print_ongoing_status&lt;/code&gt; - Default: True, whether or not to print the status of test as they complete&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Results Visualization&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;LLMNeedleInHaystackVisualization.ipynb&lt;/code&gt; holds the code to make the pivot table visualization. The pivot table was then transferred to Google Slides for custom annotations and formatting. See the &lt;a href=&#34;https://docs.google.com/presentation/d/15JEdEBjm32qBbqeYM6DK6G-3mUJd7FAJu-qEzj8IYLQ/edit?usp=sharing&#34;&gt;google slides version&lt;/a&gt;. See an overview of how this viz was created &lt;a href=&#34;https://twitter.com/GregKamradt/status/1729573848893579488&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;OpenAI&#39;s GPT-4-128K (Run 11/8/2023)&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/gkamradt/LLMTest_NeedleInAHaystack/main/img/GPT_4_testing.png&#34; alt=&#34;GPT-4-128 Context Testing&#34; width=&#34;800&#34;&gt; &#xA;&lt;h2&gt;Anthropic&#39;s Claude 2.1 (Run 11/21/2023)&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/gkamradt/LLMTest_NeedleInAHaystack/main/img/Claude_2_1_testing.png&#34; alt=&#34;GPT-4-128 Context Testing&#34; width=&#34;800&#34;&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the MIT License - see the &lt;a href=&#34;https://raw.githubusercontent.com/gkamradt/LLMTest_NeedleInAHaystack/main/LICENSE.txt&#34;&gt;LICENSE&lt;/a&gt; file for details. Use of this software requires attribution to the original author and project, as detailed in the license.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>facefusion/facefusion-colab</title>
    <updated>2024-02-17T01:29:15Z</updated>
    <id>tag:github.com,2024-02-17:/facefusion/facefusion-colab</id>
    <link href="https://github.com/facefusion/facefusion-colab" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Next generation face swapper and enhancer&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;FaceFusion Colab&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Next generation face swapper and enhancer.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/facefusion/facefusion-colab/blob/master/facefusion.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/open-colab-blue.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/license-MIT-green&#34; alt=&#34;License&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Read the &lt;a href=&#34;https://docs.facefusion.io&#34;&gt;documentation&lt;/a&gt; for a deep dive.&lt;/p&gt;</summary>
  </entry>
</feed>