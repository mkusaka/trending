<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-10-15T01:32:46Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>brevdev/notebooks</title>
    <updated>2023-10-15T01:32:46Z</updated>
    <id>tag:github.com,2023-10-15:/brevdev/notebooks</id>
    <link href="https://github.com/brevdev/notebooks" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://uohmivykqgnnbiouffke.supabase.co/storage/v1/object/public/landingpage/brevdevnotebooks.png&#34; width=&#34;100%&#34;&gt; &#xA;&lt;!-- Links --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://console.brev.dev&#34; style=&#34;color: #06b6d4;&#34;&gt;Console&lt;/a&gt; • &lt;a href=&#34;https://brev.dev&#34; style=&#34;color: #06b6d4;&#34;&gt;Docs&lt;/a&gt; • &lt;a href=&#34;/&#34; style=&#34;color: #06b6d4;&#34;&gt;Templates&lt;/a&gt; • &lt;a href=&#34;https://discord.gg/NVDyv7TUgJ&#34; style=&#34;color: #06b6d4;&#34;&gt;Discord&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Brev.dev Notebooks&lt;/h1&gt; &#xA;&lt;p&gt;This repo contains helpful AI/ML notebook templates. Each notebook has been coupled with the minimum GPU specs required to use them + setup scripts making a Brev template. Click the deploy badge on any notebook to deploy it.&lt;/p&gt; &#xA;&lt;h2&gt;Notebooks&lt;/h2&gt; &#xA;&lt;!-- make a table  --&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Notebook&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Min. GPU&lt;/th&gt; &#xA;   &lt;th&gt;Deploy&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/brevdev/notebooks/raw/main/llama2-finetune.ipynb&#34;&gt;Fine-tune Llama 2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Fine-tune Llama 2 on your own dataset&lt;/td&gt; &#xA;   &lt;td&gt;1x A10G&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/file/d/1RB0xrrb1GuaTRZ2qjCKuoySmRn5d-lky/view?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://console.brev.dev/environment/new?instance=A10G:g5.xlarge&amp;amp;name=fine-tune-llama2&#34;&gt;&lt;img src=&#34;https://uohmivykqgnnbiouffke.supabase.co/storage/v1/object/public/landingpage/brevdeploynavy.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/brevdev/notebooks/raw/main/mistral-finetune.ipynb&#34;&gt;Fine-tune Mistral&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A Guide to Cheaply Fine-tuning Mistral&lt;/td&gt; &#xA;   &lt;td&gt;1x A10G&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/file/d/1mlr4apb3zM9mxkQMBlbC1CdjorqK6pIx/view?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://console.brev.dev/environment/new?instance=A10G:g5.xlarge&amp;amp;name=mistral-finetune&#34;&gt;&lt;img src=&#34;https://uohmivykqgnnbiouffke.supabase.co/storage/v1/object/public/landingpage/brevdeploynavy.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/brevdev/notebooks/raw/main/mistral-finetune-own-data.ipynb&#34;&gt;Fine-tune Mistral - Own Data&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Fine-tune Mistral on your own dataset&lt;/td&gt; &#xA;   &lt;td&gt;1x A10G&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/file/d/1n_XpTFn10_64NkcHmEF8CUm3E43sPQUo/view?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://console.brev.dev/environment/new?instance=A10G:g5.xlarge&amp;amp;name=mistral-finetune&#34;&gt;&lt;img src=&#34;https://uohmivykqgnnbiouffke.supabase.co/storage/v1/object/public/landingpage/brevdeploynavy.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/brevdev/notebooks/raw/main/julia-install.ipynb&#34;&gt;Julia Install&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Easily Install Julia + Notebooks&lt;/td&gt; &#xA;   &lt;td&gt;any || CPU&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/file/d/1RDnIu87b6a7Uu6Kc8EkJoenq8toVknkj/view?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://console.brev.dev/environment/new?instance=t3a.medium&amp;amp;name=julia&#34;&gt;&lt;img src=&#34;https://uohmivykqgnnbiouffke.supabase.co/storage/v1/object/public/landingpage/brevdeploynavy.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;What is Brev.dev?&lt;/h3&gt; &#xA;&lt;p&gt;Brev is a dev tool that makes it really easy to code on a GPU in the cloud. Brev does 3 things: provision, configure, and connect.&lt;/p&gt; &#xA;&lt;h4&gt;Provision:&lt;/h4&gt; &#xA;&lt;p&gt;Brev provisions a GPU for you. You don&#39;t have to worry about setting up a cloud account. We have solid GPU supply, but if you do have AWS or GCP, you can link them.&lt;/p&gt; &#xA;&lt;h4&gt;Configure:&lt;/h4&gt; &#xA;&lt;p&gt;Brev configures your GPU with the right drivers and libraries. Use our open source tool Verb to point and click the right python and CUDA versions.&lt;/p&gt; &#xA;&lt;h4&gt;Connect:&lt;/h4&gt; &#xA;&lt;p&gt;Brev.dev CLI automatically edits your ssh config so you can &lt;code&gt;ssh gpu-name&lt;/code&gt; or run &lt;code&gt;brev open gpu-name&lt;/code&gt; to open VS Code to the remote machine&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>dscripka/openWakeWord</title>
    <updated>2023-10-15T01:32:46Z</updated>
    <id>tag:github.com,2023-10-15:/dscripka/openWakeWord</id>
    <link href="https://github.com/dscripka/openWakeWord" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An open-source audio wake word (or phrase) detection framework with a focus on performance and simplicity.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://github.com/dscripka/openWakeWord/actions/workflows/tests.yml/badge.svg?sanitize=true&#34; alt=&#34;Github CI&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;openWakeWord&lt;/h1&gt; &#xA;&lt;p&gt;openWakeWord is an open-source wakeword library that can be used to create voice-enabled applications and interfaces. It includes pre-trained models for common words &amp;amp; phrases that work well in real-world environments.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Quick Links&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dscripka/openWakeWord/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dscripka/openWakeWord/main/#training-new-models&#34;&gt;Training New Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dscripka/openWakeWord/main/#faq&#34;&gt;FAQ&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Updates&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023/10/11&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Significant improvements to the process of &lt;a href=&#34;https://raw.githubusercontent.com/dscripka/openWakeWord/main/#training-new-models&#34;&gt;training new models&lt;/a&gt;, including an example Google Colab notebook demonstrating how to train a basic wake word model in &amp;lt;1 hour.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023/06/15&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;v0.5.0 of openWakeWord released. See the &lt;a href=&#34;https://raw.githubusercontent.com/dscripka/openWakeWord/main/CHANGELOG.md&#34;&gt;changelog&lt;/a&gt; for a full descriptions of new features and changes.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Demo&lt;/h1&gt; &#xA;&lt;p&gt;You can try an online demo of the included pre-trained models via HuggingFace Spaces &lt;a href=&#34;https://huggingface.co/spaces/davidscripka/openWakeWord&#34;&gt;right here!&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Note that real-time detection of a microphone stream can occasionally behave strangely in Spaces. For the most reliable testing, perform a local installation as described below.&lt;/p&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;p&gt;Installing openWakeWord is simple and has minimal dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install openwakeword&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On Linux systems, both the &lt;a href=&#34;https://pypi.org/project/onnxruntime/&#34;&gt;onnxruntime&lt;/a&gt; package and &lt;a href=&#34;https://pypi.org/project/tflite-runtime/&#34;&gt;tflite-runtime&lt;/a&gt; packages will be installed as dependencies since both inference frameworks are supported. On Windows, only onnxruntime is installed due to a lack of support for modern versions of tflite.&lt;/p&gt; &#xA;&lt;p&gt;To (optionally) use &lt;a href=&#34;https://www.speex.org/&#34;&gt;Speex&lt;/a&gt; noise suppression on Linux systems to improve performance in noisy environments, install the Speex dependencies and then the pre-built Python package (see the assets &lt;a href=&#34;https://github.com/dscripka/openWakeWord/releases/tag/v0.1.1&#34;&gt;here&lt;/a&gt; for all .whl versions), adjusting for your python version and system architecture as needed.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt-get install libspeexdsp-dev&#xA;pip install https://github.com/dscripka/openWakeWord/releases/download/v0.1.1/speexdsp_ns-0.1.2-cp38-cp38-linux_x86_64.whl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Many thanks to &lt;a href=&#34;https://github.com/TeaPoly/speexdsp-ns-python&#34;&gt;TeaPoly&lt;/a&gt; for their Python wrapper of the Speex noise suppression libraries.&lt;/p&gt; &#xA;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;p&gt;For quick local testing, clone this repository and use the included &lt;a href=&#34;https://raw.githubusercontent.com/dscripka/openWakeWord/main/examples/detect_from_microphone.py&#34;&gt;example script&lt;/a&gt; to try streaming detection from a local microphone. You can individually download pre-trained models from current and past &lt;a href=&#34;https://github.com/dscripka/openWakeWord/releases/&#34;&gt;releases&lt;/a&gt;, or you can download them using Python (see below).&lt;/p&gt; &#xA;&lt;p&gt;Adding openWakeWord to your own Python code requires just a few lines:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import openwakeword&#xA;from openwakeword.model import Model&#xA;&#xA;# One-time download of all pre-trained models (or only select models)&#xA;openwakeword.utils.download_models()&#xA;&#xA;# Instantiate the model(s)&#xA;model = Model(&#xA;    wakeword_models=[&#34;path/to/model.tflite&#34;],  # can also leave this argument empty to load all of the included pre-trained models&#xA;)&#xA;&#xA;# Get audio data containing 16-bit 16khz PCM audio data from a file, microphone, network stream, etc.&#xA;# For the best efficiency and latency, audio frames should be multiples of 80 ms, with longer frames&#xA;# increasing overall efficiency at the cost of detection latency&#xA;frame = my_function_to_get_audio_frame()&#xA;&#xA;# Get predictions for the frame&#xA;prediction = model.predict(frame)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Additionally, openWakeWord provides other useful utility functions. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get predictions for individual WAV files (16-bit 16khz PCM)&#xA;from openwakeword.model import Model&#xA;&#xA;model = Model()&#xA;model.predict_clip(&#34;path/to/wav/file&#34;)&#xA;&#xA;# Get predictions for a large number of files using multiprocessing&#xA;from openwakeword.utils import bulk_predict&#xA;&#xA;bulk_predict(&#xA;    file_paths = [&#34;path/to/wav/file/1&#34;, &#34;path/to/wav/file/2&#34;],&#xA;    wakeword_models = [&#34;hey jarvis&#34;],&#xA;    ncpu=2&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;code&gt;openwakeword/utils.py&lt;/code&gt; and &lt;code&gt;openwakeword/model.py&lt;/code&gt; for the full specification of class methods and utility functions.&lt;/p&gt; &#xA;&lt;h1&gt;Recommendations for Usage&lt;/h1&gt; &#xA;&lt;h2&gt;Noise Suppression and Voice Activity Detection (VAD)&lt;/h2&gt; &#xA;&lt;p&gt;While the default settings for openWakeWord will work well in many cases, there are adjustable parameters in openWakeWord that can improve performance in some deployment scenarios.&lt;/p&gt; &#xA;&lt;p&gt;On supported platforms (currently only X86 and Arm64 linux), Speex noise suppression can be enabled by setting the &lt;code&gt;enable_speex_noise_suppression=True&lt;/code&gt; when instantiating an openWakeWord model. This can improve performance when relatively constant background noise is present.&lt;/p&gt; &#xA;&lt;p&gt;Second, a voice activity detection (VAD) model from &lt;a href=&#34;https://github.com/snakers4/silero-vad&#34;&gt;Silero&lt;/a&gt; is included with openWakeWord, and can be enabled by setting the &lt;code&gt;vad_threshold&lt;/code&gt; argument to a value between 0 and 1 when instantiating an openWakeWord model. This will only allow a positive prediction from openWakeWord when the VAD model simultaneously has a score above the specified threshold, which can significantly reduce false-positive activations in the present of non-speech noise.&lt;/p&gt; &#xA;&lt;h2&gt;Threshold Scores for Activation&lt;/h2&gt; &#xA;&lt;p&gt;All of the included openWakeWord models were trained to work well with a default threshold of &lt;code&gt;0.5&lt;/code&gt; for a positive prediction, but you are encouraged to determine the best threshold for your environment and use-case through testing. For certain deployments, using a lower or higher threshold in practice may result in significantly better performance.&lt;/p&gt; &#xA;&lt;h2&gt;User-specific models&lt;/h2&gt; &#xA;&lt;p&gt;If the baseline performance of openWakeWord models is not sufficient for a given application (specifically, if the false activation rate is unacceptably high), it is possible to train &lt;a href=&#34;https://raw.githubusercontent.com/dscripka/openWakeWord/main/docs/custom_verifier_models.md&#34;&gt;custom verifier models&lt;/a&gt; for specific voices that act as a second-stage filter on predictions (i.e., only allow activations through that were likely spoken by a known set of voices). This can greatly improve performance, at the cost of making the openWakeWord system less likely to respond to new voices.&lt;/p&gt; &#xA;&lt;h1&gt;Project Goals&lt;/h1&gt; &#xA;&lt;p&gt;openWakeWord has four high-level goals, which combine to (hopefully!) produce a framework that is simple to use &lt;em&gt;and&lt;/em&gt; extend.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Be fast &lt;em&gt;enough&lt;/em&gt; for real-world usage, while maintaining ease of use and development. For example, a single core of a Raspberry Pi 3 can run 15-20 openWakeWord models simultaneously in real-time. However, the models are likely still too large for less powerful systems or micro-controllers. Commercial options like &lt;a href=&#34;https://picovoice.ai/platform/porcupine/&#34;&gt;Picovoice Porcupine&lt;/a&gt; or &lt;a href=&#34;https://fluent.ai/products/wakeword/&#34;&gt;Fluent Wakeword&lt;/a&gt; are likely better suited for highly constrained hardware environments.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Be accurate &lt;em&gt;enough&lt;/em&gt; for real-world usage. The included models are typically have false-accept and false-reject rates below the annoyance threshold for the average user. This is obviously subjective, by a false-accept rate of &amp;lt;0.5 per hour and a false-reject rate of &amp;lt;5% is often reasonable in practice. See the &lt;a href=&#34;https://raw.githubusercontent.com/dscripka/openWakeWord/main/#performance-and-evaluation&#34;&gt;Performance &amp;amp; Evaluation&lt;/a&gt; section for details about how well the included models can be expected to perform in practice.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Have a simple model architecture and inference process. Models process a stream of audio data in 80 ms frames, and return a score between 0 and 1 for each frame indicating the confidence that a wake word/phrase has been detected. All models also have a shared feature extraction backbone, so that each additional model only has a small impact to overall system complexity and resource requirements.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Require &lt;strong&gt;little to no manual data collection&lt;/strong&gt; to train new models. The included models (see the &lt;a href=&#34;https://raw.githubusercontent.com/dscripka/openWakeWord/main/#pre-trained-models&#34;&gt;Pre-trained Models&lt;/a&gt; section for more details) were all trained with &lt;em&gt;100% synthetic&lt;/em&gt; speech generated from text-to-speech models. Training new models is a simple as generating new clips for the target wake word/phrase and training a small model on top of of the frozen shared feature extractor. See the &lt;a href=&#34;https://raw.githubusercontent.com/dscripka/openWakeWord/main/#training-new-models&#34;&gt;Training New Models&lt;/a&gt; section for more details.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Future releases of openWakeWord will aim to stay aligned with these goals, even when adding new functionality.&lt;/p&gt; &#xA;&lt;h1&gt;Pre-Trained Models&lt;/h1&gt; &#xA;&lt;p&gt;openWakeWord comes with pre-trained models for common words &amp;amp; phrases. Currently, only English models are supported, but they should be reasonably robust across different types speaker accents and pronunciation.&lt;/p&gt; &#xA;&lt;p&gt;The table below lists each model, examples of the word/phrases it is trained to recognize, and the associated documentation page for additional detail. Many of these models are trained on multiple variations of the same word/phrase; see the individual documentation pages for each model to see all supported word &amp;amp; phrase variations.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Detected Speech&lt;/th&gt; &#xA;   &lt;th&gt;Documentation Page&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;alexa&lt;/td&gt; &#xA;   &lt;td&gt;&#34;alexa&#34;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dscripka/openWakeWord/main/docs/models/alexa.md&#34;&gt;docs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;hey mycroft&lt;/td&gt; &#xA;   &lt;td&gt;&#34;hey mycroft&#34;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dscripka/openWakeWord/main/docs/models/hey_mycroft.md&#34;&gt;docs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;hey jarvis&lt;/td&gt; &#xA;   &lt;td&gt;&#34;hey jarvis&#34;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dscripka/openWakeWord/main/docs/models/hey_jarvis.md&#34;&gt;docs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;hey rhasspy&lt;/td&gt; &#xA;   &lt;td&gt;&#34;hey rhasspy&#34;&lt;/td&gt; &#xA;   &lt;td&gt;TBD&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;current weather&lt;/td&gt; &#xA;   &lt;td&gt;&#34;what&#39;s the weather&#34;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dscripka/openWakeWord/main/docs/models/weather.md&#34;&gt;docs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;timers&lt;/td&gt; &#xA;   &lt;td&gt;&#34;set a 10 minute timer&#34;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dscripka/openWakeWord/main/docs/models/timers.md&#34;&gt;docs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Based on the methods discussed in &lt;a href=&#34;https://raw.githubusercontent.com/dscripka/openWakeWord/main/#performance-and-evaluation&#34;&gt;performance testing&lt;/a&gt;, each included model aims to meet the target performance criteria of &amp;lt;5% false-reject rates and &amp;lt;0.5/hour false-accept rates with appropriate threshold tuning. These levels are subjective, but hopefully are below the annoyance threshold where the average user becomes frustrated with a system that often misses intended activations and/or causes disruption by activating too frequently at undesired times. For example, at these performance levels a user could expect to have the model process continuous mixed content audio of several hours with at most a few false activations, and have a failed intended activation in only 1/20 attempts (and a failed retry in only 1/400 attempts).&lt;/p&gt; &#xA;&lt;p&gt;If you have a new wake word or phrase that you would like to see included in the next release, please open an issue, and we&#39;ll do a best to train a model! The focus of these requests and future release will be on words and phrases that have broad general usage versus highly specific application.&lt;/p&gt; &#xA;&lt;h1&gt;Model Architecture&lt;/h1&gt; &#xA;&lt;p&gt;openWakeword models are composed of three separate components:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;A pre-processing function that computes &lt;a href=&#34;https://pytorch.org/audio/main/generated/torchaudio.transforms.MelSpectrogram.html&#34;&gt;melspectrogram&lt;/a&gt; of the input audio data. For openWakeword, an ONNX implementation of Torch&#39;s melspectrogram function with fixed parameters is used to enable efficient performance across devices.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;A shared feature extraction backbone model that converts melspectrogram inputs into general-purpose speech audio embeddings. This &lt;a href=&#34;https://arxiv.org/abs/2002.01322&#34;&gt;model&lt;/a&gt; is provided by &lt;a href=&#34;https://tfhub.dev/google/speech_embedding/1&#34;&gt;Google&lt;/a&gt; as a TFHub module under an &lt;a href=&#34;https://opensource.org/licenses/Apache-2.0&#34;&gt;Apache-2.0&lt;/a&gt; license. For openWakeWord, this model was manually re-implemented to separate out different functionality and allow for more control of architecture modifications compared to a TFHub module. The model itself is series of relatively simple convolutional blocks, and gains its strong performance from extensive pre-training on large amounts of data. This model is the core component of openWakeWord, and enables the strong performance that is seen even when training on fully-synthetic data.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;A classification model that follows the shared (and frozen) feature extraction model. The structure of this classification model is arbitrary, but in practice a simple fully-connected network or 2 layer RNN works well.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Performance and Evaluation&lt;/h1&gt; &#xA;&lt;p&gt;Evaluating wake word/phrase detection models is challenging, and it is often very difficult to assess how different models presented in papers or other projects will perform &lt;em&gt;when deployed&lt;/em&gt; with respect to two critical metrics: false-reject rates and false-accept rates. For clarity in definitions:&lt;/p&gt; &#xA;&lt;p&gt;A &lt;em&gt;false-reject&lt;/em&gt; is when the model fails to detect an intended activation from a user.&lt;/p&gt; &#xA;&lt;p&gt;A &lt;em&gt;false-accept&lt;/em&gt; is when the model inadvertently activates when the user did not intend for it to do so.&lt;/p&gt; &#xA;&lt;p&gt;For openWakeWord, evaluation follows two principles:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;The &lt;em&gt;false-reject&lt;/em&gt; rate should be determined from wakeword/phrases that represent realistic recording environments, including those with background noise and reverberation. This can be accomplished by directly collected data from these environments, or simulating them with data augmentation methods.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The &lt;em&gt;false-accept&lt;/em&gt; rate should be determined from audio that represents the types of environments that would be expected for the deployed model, not just on the training/evaluation data. In practice, this means that the model should only rarely activate in error, even in the presence of hours of continuous speech and background noise.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;While other wakeword evaluation standards &lt;a href=&#34;https://github.com/Picovoice/wake-word-benchmark&#34;&gt;do exist&lt;/a&gt;, for openWakeWord it was decided that a custom evaluation would better indicate what performance users can expect for real-world deployments. Specifically:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;em&gt;false-reject&lt;/em&gt; rates are calculated from either clean recordings of the wakeword that are mixed with background noise at realistic signal-to-noise ratios (e.g., 5-10 dB) &lt;em&gt;and&lt;/em&gt; reverberated with room Impulse Response Functions (RIRs) to better simulate far-field audio, &lt;em&gt;or&lt;/em&gt; manually collected data from realistic deployment environments (e.g., far-field capture with normal environment noise).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;em&gt;false-accept&lt;/em&gt; rates are determined by using the &lt;a href=&#34;https://www.amazon.science/publications/dipco-dinner-party-corpus&#34;&gt;Dinner Party Corpus&lt;/a&gt; dataset, which represents ~5.5 hours of far-field speech, background music, and miscellaneous noise. This dataset sets a realistic (if challenging) goal for how many false activations might occur in a similar situation.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;To illustrate how openWakeWord can produce capable models, the false-accept/false-reject curves for the included &lt;code&gt;&#34;alexa&#34;&lt;/code&gt; model is shown below along with the performance of a strong commercial competitor, &lt;a href=&#34;https://picovoice.ai/platform/porcupine/&#34;&gt;Picovoice Porcupine&lt;/a&gt;. Other existing open-source wakeword engines (e.g., &lt;a href=&#34;https://github.com/Kitt-AI/snowboy&#34;&gt;Snowboy&lt;/a&gt;, &lt;a href=&#34;https://github.com/cmusphinx/pocketsphinx&#34;&gt;PocketSphinx&lt;/a&gt;, etc.) are not included as they are either no longer maintained or demonstrate performance significantly below that of Porcupine. The positive test examples used were those included in &lt;a href=&#34;https://github.com/Picovoice/wake-word-benchmark&#34;&gt;Picovoice&#39;s&lt;/a&gt; repository, a fantastic resource that they have freely provided to the community. Note, however, that the test data was prepared differently compared to Picovoice&#39;s implementation (see the &lt;a href=&#34;https://raw.githubusercontent.com/dscripka/openWakeWord/main/docs/models/alexa.md&#34;&gt;Alexa model documentation&lt;/a&gt; for more details).&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dscripka/openWakeWord/main/docs/models/images/alexa_performance_plot.png&#34; alt=&#34;FPR/FRR curve for &amp;quot;alexa&amp;quot; pre-trained model&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;For at least this test data and preparation, openWakeWord produces a model that is more accurate than Porcupine.&lt;/p&gt; &#xA;&lt;p&gt;As a second illustration, the false-accept/false-reject rate of the included &lt;code&gt;&#34;hey mycroft&#34;&lt;/code&gt; model is shown below along with the performance of a &lt;a href=&#34;https://picovoice.ai/docs/quick-start/porcupine-python/#custom-keywords&#34;&gt;custom&lt;/a&gt; Picovoice Porcupine model and &lt;a href=&#34;https://mycroft-ai.gitbook.io/docs/mycroft-technologies/precise&#34;&gt;Mycroft Precise&lt;/a&gt;. In this case, the positive test examples were manually collected from a male speaker with a relatively neutral American english accent in realistic home recording scenarios (see the &lt;a href=&#34;https://raw.githubusercontent.com/dscripka/openWakeWord/main/docs/models/hey_mycroft.md&#34;&gt;Hey Mycroft model documentation&lt;/a&gt; for more details).&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dscripka/openWakeWord/main/docs/models/images/hey_mycroft_performance.png&#34; alt=&#34;FPR/FRR curve for &amp;quot;hey mycroft&amp;quot; pre-trained model&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Again, for at least this test data and preparation, openWakeWord produces a model at least as good as existing solutions.&lt;/p&gt; &#xA;&lt;p&gt;However, in should noted that for both of these tests sample sizes are small and there are issues (&lt;a href=&#34;https://github.com/Picovoice/wake-word-benchmark/issues/13&#34;&gt;1&lt;/a&gt;, &lt;a href=&#34;https://github.com/MycroftAI/mycroft-precise/issues/237&#34;&gt;2&lt;/a&gt;) with the evaluation of the other libraries that suggest these results should be interpreted cautiously. As such, the only claim being made is that openWakeWord models are broadly competitive with comparable offerings. You are strongly encouraged to &lt;a href=&#34;https://raw.githubusercontent.com/dscripka/openWakeWord/main/#installation--usage&#34;&gt;test openWakeWord&lt;/a&gt; to determine if it will meet the requirements of your use-case.&lt;/p&gt; &#xA;&lt;p&gt;Finally, to give evidence that the core methods behind openWakeWord (i.e., pre-trained speech embeddings and high-quality synthetic speech) are effective across a wider range of wake word/phrase structure and length, the table below shows the performance on the &lt;a href=&#34;https://paperswithcode.com/sota/spoken-language-understanding-on-fluent&#34;&gt;Fluent Speech Commands&lt;/a&gt; test set using an openWakeWord model and the baseline method shown in a &lt;a href=&#34;https://arxiv.org/abs/1910.09463&#34;&gt;related paper by the dataset authors&lt;/a&gt;. While both models were trained on fully-synthetic data, due to fundamentally different data synthesis &amp;amp; preparation, training, and evaluation approaches, the numbers below are likely not directly comparable. Rather, the important conclusion is that openWakeWord is a viable approach for the task of spoken language understanding (SLU).&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Test Set Accuracy&lt;/th&gt; &#xA;   &lt;th&gt;Link&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;openWakeWord&lt;/td&gt; &#xA;   &lt;td&gt;~97.5%&lt;/td&gt; &#xA;   &lt;td&gt;NA&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;encoder-decoder&lt;/td&gt; &#xA;   &lt;td&gt;~94.9%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1910.09463&#34;&gt;paper&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;If you are aware of other open-source wakeword/phrase libraries that should be added to these comparisons, or have suggestions on how to improve the evaluation more generally, please open an issue! We are eager to continue improving openWakeWord by learning how others are approaching this problem.&lt;/p&gt; &#xA;&lt;h2&gt;Other Performance Details&lt;/h2&gt; &#xA;&lt;h3&gt;Model Robustness&lt;/h3&gt; &#xA;&lt;p&gt;Due to a combination of variability in the generated speech and the extensive pre-training from Google, openWakeWord models also demonstrate some additional performance benefits that are useful for real-world applications. In testing, three in particular have been observed.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;The trained models seem to respond reasonably well to wakewords and phrases that are &lt;a href=&#34;https://en.wikipedia.org/wiki/Whispering&#34;&gt;whispered&lt;/a&gt;. This is somewhat surprising behavior, as the text-to-speech models used for producing training data generally do not create synthetic speech that has acoustic qualities similar to whispering.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The models also respond relatively well to wakewords and phrases spoken at different speeds (within reason).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The models are able to handle some variability in the phrasing of a given command. This behavior was not entirely a surprise, given that &lt;a href=&#34;https://arxiv.org/abs/1904.03670&#34;&gt;others&lt;/a&gt; have reported similar benefits when training end-to-end spoken language understanding systems. For example, the included &lt;a href=&#34;https://raw.githubusercontent.com/dscripka/openWakeWord/main/docs/models/weather.md&#34;&gt;pre-trained weather model&lt;/a&gt; will typically still respond correctly to a phrase like &#34;how is the weather today&#34; despite not training directly on that phrase (though false rejections rates will likely be higher, on average, compared to phrases closer to the training data).&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Background Noise&lt;/h3&gt; &#xA;&lt;p&gt;While the models are trained with background noise to increase robustness, in some cases additional noise suppression can improve performance. Setting the &lt;code&gt;enable_speex_noise_suppression=True&lt;/code&gt; argument during openWakeWord model initialization will use the efficient Speex noise suppression algorithm to pre-process the audio data prior to prediction. This can reduce both false-reject rates and false-accept rates, though testing in a realistic deployment environment is strongly recommended.&lt;/p&gt; &#xA;&lt;h1&gt;Training New Models&lt;/h1&gt; &#xA;&lt;p&gt;openWakeWord includes an automated utility that greatly simplifies the process of training custom models. This can be used in two ways:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;A simple &lt;a href=&#34;https://colab.research.google.com/drive/1q1oe2zOyZp7UsB3jJiQ1IFn8z5YfjwEb?usp=sharing&#34;&gt;Google Colab&lt;/a&gt; notebook with an easy to use interface and simple end-to-end process. This allows anyone to produce a custom model very quickly (&amp;lt;1 hour) and doesn&#39;t require any development experience, but the performance of the model may be low in some deployment scenarios.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;A more detailed &lt;a href=&#34;https://raw.githubusercontent.com/dscripka/openWakeWord/main/notebooks/automatic_model_training.ipynb&#34;&gt;notebook&lt;/a&gt; (also on &lt;a href=&#34;https://colab.research.google.com/drive/1yyFH-fpguX2BTAW8wSQxTrJnJTM-0QAd?usp=sharing&#34;&gt;Google Colab&lt;/a&gt;) that describes the training process in more details, and enables more customization. This can produce high quality models, but requires more development experience.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For users interested in understanding the fundamental concepts behind model training there is a more detailed, educational &lt;a href=&#34;https://raw.githubusercontent.com/dscripka/openWakeWord/main/notebooks/training_models.ipynb&#34;&gt;tutorial notebook&lt;/a&gt; also available. However, this specific notebook is not intended for training production models, and the automated process above is recommended for that purpose.&lt;/p&gt; &#xA;&lt;p&gt;Fundamentally, a new model requires two data generation and collection steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Generate new training data for the desired wakeword/phrase using open-source speech-to-text systems (see &lt;a href=&#34;https://raw.githubusercontent.com/dscripka/openWakeWord/main/docs/synthetic_data_generation.md&#34;&gt;Synthetic Data Generation&lt;/a&gt; for more details). These models and the generation code are hosted in a separate &lt;a href=&#34;https://github.com/dscripka/synthetic_speech_dataset_generation&#34;&gt;repository&lt;/a&gt;. The number of generated examples required can vary, a minimum of several thousand is recommended and performance seems to increase smoothly with increasing dataset size.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Collect negative data (e.g., audio where the wakeword/phrase is not present) to help the model have a low false-accept rate. This also benefits from scale, and the &lt;a href=&#34;https://raw.githubusercontent.com/dscripka/openWakeWord/main/#pre-trained-models&#34;&gt;included models&lt;/a&gt; were all trained with ~30,000 hours of negative data representing speech, noise, and music. See the individual model documentation pages for more details on training data curation and preparation.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Language Support&lt;/h1&gt; &#xA;&lt;p&gt;Currently, openWakeWord only supports English, primarily because the pre-trained text-to-speech models used to generate training data are all based on english datasets. It&#39;s likely that speech-to-text models trained on other languages would also work well, but non-english models &amp;amp; datasets are less commonly available.&lt;/p&gt; &#xA;&lt;p&gt;Future release road maps may have non-english support. In particular, &lt;a href=&#34;https://github.com/MycroftAI/mimic3-voices&#34;&gt;Mycroft.AIs Mimic 3&lt;/a&gt; TTS engine may work well to help extend some support to other languages.&lt;/p&gt; &#xA;&lt;h1&gt;FAQ&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Is there a Docker implementation for openWakeWord?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;While there isn&#39;t an official Docker implementation, &lt;a href=&#34;https://github.com/dalehumby&#34;&gt;@dalehumby&lt;/a&gt; &lt;a href=&#34;https://github.com/dalehumby/openWakeWord-rhasspy&#34;&gt;has created one&lt;/a&gt; that works very well!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Can openWakeWord be run in a browser with javascript?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;While the ONNX runtime &lt;a href=&#34;https://onnxruntime.ai/docs/get-started/with-javascript.html&#34;&gt;does support javascript&lt;/a&gt;, much of the other functionality required for openWakeWord models would need to be ported. This is not currently on the roadmap, but please open an issue/start a discussion if this feature is of particular interest.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Is there a C++ version of openWakeWord?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;While the ONNX runtime &lt;a href=&#34;https://onnxruntime.ai/docs/get-started/with-cpp.html&#34;&gt;also has a C++ API&lt;/a&gt;, there isn&#39;t an official C++ implementation of the full openWakeWord library. However, &lt;a href=&#34;https://github.com/synesthesiam&#34;&gt;@synesthesiam&lt;/a&gt; has created a &lt;a href=&#34;https://github.com/rhasspy/openWakeWord-cpp&#34;&gt;C++ version&lt;/a&gt; of openWakeWord with basic functionality implemented.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Why are there three separate models instead of just one?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Separating the models was an intentional choice to provide flexibility and optimize the efficiency of the end-to-end prediction process. For example, with separate melspectrogram, embedding, and prediction models, each one can operate on different size inputs of audio to optimize overall latency and share computations between models. It certainly is possible to make a combined model with all of the steps integrated, though, if that was a requirement of a particular use case.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;I still get a large number of false activations when I use the pre-trained models, how can I reduce these?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;First, review the &lt;a href=&#34;https://raw.githubusercontent.com/dscripka/openWakeWord/main/#recommendations-for-usage&#34;&gt;recommendations for usage&lt;/a&gt; and ensure that these options do not improve overall system accuracy. Second, experiment with &lt;a href=&#34;https://raw.githubusercontent.com/dscripka/openWakeWord/main/#user-specific-models&#34;&gt;custom verifier models&lt;/a&gt;, if possible. If neither of these approaches are helping, please open an issue with details of the deployment environment and the types of false activations that you are experiencing. We certainly appreciate feedback &amp;amp; requests on how to improve the base pre-trained models!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Acknowledgements&lt;/h1&gt; &#xA;&lt;p&gt;I am very grateful for the encouraging and positive response from the open-source community since the release of openWakeWord in January 2023. In particular, I want to acknowledge and thank the following individuals and groups for their feedback, collaboration, and development support:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/synesthesiam&#34;&gt;synesthesiam&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/secretsauceai&#34;&gt;SecretSauceAI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/OpenVoiceOS&#34;&gt;OpenVoiceOS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NabuCasa&#34;&gt;Nabu Casa&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/home-assistant&#34;&gt;Home Assistant&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;All of the code in this repository is licensed under the &lt;strong&gt;Apache 2.0&lt;/strong&gt; license. All of the included pre-trained models are licensed under the &lt;a href=&#34;https://creativecommons.org/licenses/by-nc-sa/4.0/&#34;&gt;Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International&lt;/a&gt; license due to the inclusion of datasets with unknown or restrictive licensing as part of the training data. If you are interested in pre-trained models with more permissive licensing, please raise an issue and we will try to add them to a future release.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>google-deepmind/deepmind-research</title>
    <updated>2023-10-15T01:32:46Z</updated>
    <id>tag:github.com,2023-10-15:/google-deepmind/deepmind-research</id>
    <link href="https://github.com/google-deepmind/deepmind-research" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This repository contains implementations and illustrative code to accompany DeepMind publications&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DeepMind Research&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains implementations and illustrative code to accompany DeepMind publications. Along with publishing papers to accompany research conducted at DeepMind, we release open-source &lt;a href=&#34;https://deepmind.com/research/open-source/open-source-environments/&#34;&gt;environments&lt;/a&gt;, &lt;a href=&#34;https://deepmind.com/research/open-source/open-source-datasets/&#34;&gt;data sets&lt;/a&gt;, and &lt;a href=&#34;https://deepmind.com/research/open-source/open-source-code/&#34;&gt;code&lt;/a&gt; to enable the broader research community to engage with our work and build upon it, with the ultimate goal of accelerating scientific progress to benefit society. For example, you can build on our implementations of the &lt;a href=&#34;https://github.com/deepmind/dqn&#34;&gt;Deep Q-Network&lt;/a&gt; or &lt;a href=&#34;https://github.com/deepmind/dnc&#34;&gt;Differential Neural Computer&lt;/a&gt;, or experiment in the same environments we use for our research, such as &lt;a href=&#34;https://github.com/deepmind/lab&#34;&gt;DeepMind Lab&lt;/a&gt; or &lt;a href=&#34;https://github.com/deepmind/pysc2&#34;&gt;StarCraft II&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you enjoy building tools, environments, software libraries, and other infrastructure of the kind listed below, you can view open positions to work in related areas on our &lt;a href=&#34;https://deepmind.com/careers/&#34;&gt;careers page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For a full list of our publications, please see &lt;a href=&#34;https://deepmind.com/research/publications/&#34;&gt;https://deepmind.com/research/publications/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Projects&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/fusion_tcv&#34;&gt;Magnetic control of tokamak plasmas through deep reinforcement learning&lt;/a&gt;, Nature 2022&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/density_functional_approximation_dm21&#34;&gt;Pushing the Frontiers of Density Functionals by Solving the Fractional Electron Problem&lt;/a&gt;, Science 2021&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/pitfalls_static_language_models&#34;&gt;Mind the Gap: Assessing Temporal Generalization in Neural Language Models&lt;/a&gt;, NeurIPS 2021&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/tandem_dqn&#34;&gt;The Difficulty of Passive Learning in Deep Reinforcement Learning&lt;/a&gt;, NeurIPS 2021&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/nowcasting&#34;&gt;Skilful precipitation nowcasting using deep generative models of radar&lt;/a&gt;, Nature 2021&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/cadl&#34;&gt;Compute-Aided Design as Language&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/continual_learning&#34;&gt;Encoders and ensembles for continual learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/hierarchical_transformer_memory&#34;&gt;Towards mental time travel: a hierarchical memory for reinforcement learning agents&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/perceiver&#34;&gt;Perceiver IO: A General Architecture for Structured Inputs &amp;amp; Outputs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/neural_mip_solving&#34;&gt;Solving Mixed Integer Programs Using Neural Networks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/noisy_label&#34;&gt;A Realistic Simulation Framework for Learning with Label Noise&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/rapid_task_solving&#34;&gt;Rapid Task-Solving in Novel Environments&lt;/a&gt;, ICLR 2021&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/wikigraphs&#34;&gt;WikiGraphs: A Wikipedia - Knowledge Graph Paired Dataset&lt;/a&gt;, TextGraphs 2021&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/box_arrangement&#34;&gt;Behavior Priors for Efficient Reinforcement Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/meshgraphnets&#34;&gt;Learning Mesh-Based Simulation with Graph Networks&lt;/a&gt;, ICLR 2021&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/ogb_lsc&#34;&gt;Open Graph Benchmark - Large-Scale Challenge (OGB-LSC)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/synthetic_returns&#34;&gt;Synthetic Returns for Long-Term Credit Assignment&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/galaxy_mergers&#34;&gt;A Deep Learning Approach for Characterizing Major Galaxy Mergers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/kfac_ferminet_alpha&#34;&gt;Better, Faster Fermionic Neural Networks&lt;/a&gt; (KFAC implementation)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/object_attention_for_reasoning&#34;&gt;Object-based attention for spatio-temporal reasoning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/enformer&#34;&gt;Effective gene expression prediction from sequence by integrating long-range interactions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/satore&#34;&gt;Satore: First-order logic saturation with atom rewriting&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/nfnets&#34;&gt;Characterizing signal propagation to close the performance gap in unnormalized ResNets&lt;/a&gt;, ICLR 2021&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/adversarial_robustness&#34;&gt;Uncovering the Limits of Adversarial Training against Norm-Bounded Adversarial Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/cmtouch&#34;&gt;Learning rich touch representations through cross-modal self-supervision&lt;/a&gt;, CoRL 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/functional_regularisation_for_continual_learning&#34;&gt;Functional Regularisation for Continual Learning&lt;/a&gt;, ICLR 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/avae&#34;&gt;The Autoencoding Variational Autoencoder&lt;/a&gt;, NeurIPS 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/mmv&#34;&gt;Self-Supervised MultiModal Versatile Networks&lt;/a&gt;, NeurIPS 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/ode_gan&#34;&gt;ODE-GAN: Training GANs by Solving Ordinary Differential Equations&lt;/a&gt;, NeurIPS 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/causal_reasoning&#34;&gt;Algorithms for Causal Reasoning in Probability Trees&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/gated_linear_networks&#34;&gt;Gated Linear Networks&lt;/a&gt;, NeurIPS 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/himo&#34;&gt;Value-driven Hindsight Modelling&lt;/a&gt;, NeurIPS 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/learned_free_energy_estimation&#34;&gt;Targeted free energy estimation via learned mappings&lt;/a&gt;, Journal of Chemical Physics 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/learning_to_simulate&#34;&gt;Learning to Simulate Complex Physics with Graph Networks&lt;/a&gt;, ICML 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/physics_planning_games&#34;&gt;Physically Embedded Planning Problems&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/polygen&#34;&gt;PolyGen: PolyGen: An Autoregressive Generative Model of 3D Meshes&lt;/a&gt;, ICML 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/byol&#34;&gt;Bootstrap Your Own Latent&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/catch_carry&#34;&gt;Catch &amp;amp; Carry: Reusable Neural Controllers for Vision-Guided Whole-Body Tasks&lt;/a&gt;, SIGGRAPH 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/memo&#34;&gt;MEMO: A Deep Network For Flexible Combination Of Episodic Memories&lt;/a&gt;, ICLR 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/rl_unplugged&#34;&gt;RL Unplugged: Benchmarks for Offline Reinforcement Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/geomancer&#34;&gt;Disentangling by Subspace Diffusion (GEOMANCER)&lt;/a&gt;, NeurIPS 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/affordances_theory&#34;&gt;What can I do here? A theory of affordances in reinforcement learning&lt;/a&gt;, ICML 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/sketchy&#34;&gt;Scaling data-driven robotics with reward sketching and batch reinforcement learning&lt;/a&gt;, RSS 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/counterfactual_fairness&#34;&gt;Path-Specific Counterfactual Fairness&lt;/a&gt;, AAAI 2019&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/option_keyboard&#34;&gt;The Option Keyboard: Combining Skills in Reinforcement Learning&lt;/a&gt;, NeurIPS 2019&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/visr&#34;&gt;VISR - Fast Task Inference with Variational Intrinsic Successor Features&lt;/a&gt;, ICLR 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/glassy_dynamics&#34;&gt;Unveiling the predictive power of static structure in glassy systems&lt;/a&gt;, Nature Physics 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/iodine&#34;&gt;Multi-Object Representation Learning with Iterative Variational Inference (IODINE)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/alphafold_casp13&#34;&gt;AlphaFold CASP13&lt;/a&gt;, Nature 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/unrestricted_advx&#34;&gt;Unrestricted Adversarial Challenge&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/hierarchical_probabilistic_unet&#34;&gt;Hierarchical Probabilistic U-Net (HPU-Net)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/scratchgan&#34;&gt;Training Language GANs from Scratch&lt;/a&gt;, NeurIPS 2019&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/tvt&#34;&gt;Temporal Value Transport&lt;/a&gt;, Nature Communications 2019&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/curl&#34;&gt;Continual Unsupervised Representation Learning (CURL)&lt;/a&gt;, NeurIPS 2019&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/transporter&#34;&gt;Unsupervised Learning of Object Keypoints (Transporter)&lt;/a&gt;, NeurIPS 2019&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/bigbigan&#34;&gt;BigBiGAN&lt;/a&gt;, NeurIPS 2019&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/cs_gan&#34;&gt;Deep Compressed Sensing&lt;/a&gt;, ICML 2019&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/side_effects_penalties&#34;&gt;Side Effects Penalties&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/PrediNet&#34;&gt;PrediNet Architecture and Relations Game Datasets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/unsupervised_adversarial_training&#34;&gt;Unsupervised Adversarial Training&lt;/a&gt;, NeurIPS 2019&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/graph_matching_networks&#34;&gt;Graph Matching Networks for Learning the Similarity of Graph Structured Objects&lt;/a&gt;, ICML 2019&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/regal&#34;&gt;REGAL: Transfer Learning for Fast Optimization of Computation Graphs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/ensemble_loss_landscape&#34;&gt;Deep Ensembles: A Loss Landscape Perspective&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/powerpropagation&#34;&gt;Powerpropagation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/physics_inspired_models&#34;&gt;Physics Inspired Models&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;This is not an official Google product.&lt;/em&gt;&lt;/p&gt;</summary>
  </entry>
</feed>