<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-20T01:36:22Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>pyannote/pyannote-audio</title>
    <updated>2023-04-20T01:36:22Z</updated>
    <id>tag:github.com,2023-04-20:/pyannote/pyannote-audio</id>
    <link href="https://github.com/pyannote/pyannote-audio" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Neural building blocks for speaker diarization: speech activity detection, speaker change detection, overlapped speech detection, speaker embedding&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Neural speaker diarization with &lt;code&gt;pyannote.audio&lt;/code&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;code&gt;pyannote.audio&lt;/code&gt; is an open-source toolkit written in Python for speaker diarization. Based on &lt;a href=&#34;https://raw.githubusercontent.com/pyannote/pyannote-audio/develop/pytorch.org&#34;&gt;PyTorch&lt;/a&gt; machine learning framework, it provides a set of trainable end-to-end neural building blocks that can be combined and jointly optimized to build speaker diarization pipelines.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=37R_R82lfwA&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/37R_R82lfwA/0.jpg&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;TL;DR &lt;a href=&#34;https://colab.research.google.com/github/pyannote/pyannote-audio/blob/develop/tutorials/intro.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 1. visit hf.co/pyannote/speaker-diarization and hf.co/pyannote/segmentation and accept user conditions (only if requested)&#xA;# 2. visit hf.co/settings/tokens to create an access token (only if you had to go through 1.)&#xA;# 3. instantiate pretrained speaker diarization pipeline&#xA;from pyannote.audio import Pipeline&#xA;pipeline = Pipeline.from_pretrained(&#34;pyannote/speaker-diarization&#34;,&#xA;                                    use_auth_token=&#34;ACCESS_TOKEN_GOES_HERE&#34;)&#xA;&#xA;# 4. apply pretrained pipeline&#xA;diarization = pipeline(&#34;audio.wav&#34;)&#xA;&#xA;# 5. print the result&#xA;for turn, _, speaker in diarization.itertracks(yield_label=True):&#xA;    print(f&#34;start={turn.start:.1f}s stop={turn.end:.1f}s speaker_{speaker}&#34;)&#xA;# start=0.2s stop=1.5s speaker_0&#xA;# start=1.8s stop=3.9s speaker_1&#xA;# start=4.2s stop=5.7s speaker_0&#xA;# ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Highlights&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;span&gt;ü§ó&lt;/span&gt; pretrained &lt;a href=&#34;https://hf.co/models?other=pyannote-audio-pipeline&#34;&gt;pipelines&lt;/a&gt; (and &lt;a href=&#34;https://hf.co/models?other=pyannote-audio-model&#34;&gt;models&lt;/a&gt;) on &lt;a href=&#34;https://huggingface.co/pyannote&#34;&gt;&lt;span&gt;ü§ó&lt;/span&gt; model hub&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;ü§Ø&lt;/span&gt; state-of-the-art performance (see &lt;a href=&#34;https://raw.githubusercontent.com/pyannote/pyannote-audio/develop/#benchmark&#34;&gt;Benchmark&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;üêç&lt;/span&gt; Python-first API&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;‚ö°&lt;/span&gt; multi-GPU training with &lt;a href=&#34;https://pytorchlightning.ai/&#34;&gt;pytorch-lightning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;üéõ&lt;/span&gt; data augmentation with &lt;a href=&#34;https://github.com/asteroid-team/torch-audiomentations&#34;&gt;torch-audiomentations&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Only Python 3.8+ is supported.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# install from develop branch&#xA;pip install -qq https://github.com/pyannote/pyannote-audio/archive/refs/heads/develop.zip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pyannote/pyannote-audio/develop/CHANGELOG.md&#34;&gt;Changelog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pyannote/pyannote-audio/develop/FAQ.md&#34;&gt;Frequently asked questions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Models &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Available tasks explained&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pyannote/pyannote-audio/develop/tutorials/applying_a_model.ipynb&#34;&gt;Applying a pretrained model&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pyannote/pyannote-audio/develop/tutorials/training_a_model.ipynb&#34;&gt;Training, fine-tuning, and transfer learning&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Pipelines &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Available pipelines explained&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pyannote/pyannote-audio/develop/tutorials/applying_a_pipeline.ipynb&#34;&gt;Applying a pretrained pipeline&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pyannote/pyannote-audio/develop/tutorials/adapting_pretrained_pipeline.ipynb&#34;&gt;Adapting a pretrained pipeline to your own data&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pyannote/pyannote-audio/develop/tutorials/voice_activity_detection.ipynb&#34;&gt;Training a pipeline&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Contributing &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pyannote/pyannote-audio/develop/tutorials/add_your_own_model.ipynb&#34;&gt;Adding a new model&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pyannote/pyannote-audio/develop/tutorials/add_your_own_task.ipynb&#34;&gt;Adding a new task&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Adding a new pipeline&lt;/li&gt; &#xA;   &lt;li&gt;Sharing pretrained models and pipelines&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Blog &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;2022-12-02 &amp;gt; &lt;a href=&#34;https://raw.githubusercontent.com/pyannote/pyannote-audio/develop/tutorials/adapting_pretrained_pipeline.ipynb&#34;&gt;&#34;How I reached 1st place at Ego4D 2022, 1st place at Albayzin 2022, and 6th place at VoxSRC 2022 speaker diarization challenges&#34;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;2022-10-23 &amp;gt; &lt;a href=&#34;https://herve.niderb.fr/fastpages/2022/10/23/One-speaker-segmentation-model-to-rule-them-all&#34;&gt;&#34;One speaker segmentation model to rule them all&#34;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;2021-08-05 &amp;gt; &lt;a href=&#34;https://herve.niderb.fr/fastpages/2021/08/05/Streaming-voice-activity-detection-with-pyannote.html&#34;&gt;&#34;Streaming voice activity detection with pyannote.audio&#34;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Miscellaneous &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pyannote/pyannote-audio/develop/tutorials/training_with_cli.md&#34;&gt;Training with &lt;code&gt;pyannote-audio-train&lt;/code&gt; command line tool&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pyannote/pyannote-audio/develop/tutorials/prodigy.md&#34;&gt;Annotating your own data with Prodigy&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pyannote/pyannote-audio/develop/tutorials/speaker_verification.ipynb&#34;&gt;Speaker verification&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Visualization and debugging&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Benchmark&lt;/h2&gt; &#xA;&lt;p&gt;Out of the box, &lt;code&gt;pyannote.audio&lt;/code&gt; default speaker diarization &lt;a href=&#34;https://hf.co/pyannote/speaker-diarization&#34;&gt;pipeline&lt;/a&gt; is expected to be much better (and faster) in v2.x than in v1.1. Those numbers are diarization error rates (in %)&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Dataset \ Version&lt;/th&gt; &#xA;   &lt;th&gt;v1.1&lt;/th&gt; &#xA;   &lt;th&gt;v2.0&lt;/th&gt; &#xA;   &lt;th&gt;v2.1.1 (finetuned)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AISHELL-4&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;14.6&lt;/td&gt; &#xA;   &lt;td&gt;14.1 (14.5)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AliMeeting (channel 1)&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;27.4 (23.8)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AMI (IHM)&lt;/td&gt; &#xA;   &lt;td&gt;29.7&lt;/td&gt; &#xA;   &lt;td&gt;18.2&lt;/td&gt; &#xA;   &lt;td&gt;18.9 (18.5)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AMI (SDM)&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;29.0&lt;/td&gt; &#xA;   &lt;td&gt;27.1 (22.2)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CALLHOME (part2)&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;30.2&lt;/td&gt; &#xA;   &lt;td&gt;32.4 (29.3)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DIHARD 3 (full)&lt;/td&gt; &#xA;   &lt;td&gt;29.2&lt;/td&gt; &#xA;   &lt;td&gt;21.0&lt;/td&gt; &#xA;   &lt;td&gt;26.9 (21.9)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VoxConverse (v0.3)&lt;/td&gt; &#xA;   &lt;td&gt;21.5&lt;/td&gt; &#xA;   &lt;td&gt;12.6&lt;/td&gt; &#xA;   &lt;td&gt;11.2 (10.7)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;REPERE (phase2)&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;12.6&lt;/td&gt; &#xA;   &lt;td&gt;8.2 ( 8.3)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;This American Life&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;20.8 (15.2)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Citations&lt;/h2&gt; &#xA;&lt;p&gt;If you use &lt;code&gt;pyannote.audio&lt;/code&gt; please use the following citations:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{Bredin2020,&#xA;  Title = {{pyannote.audio: neural building blocks for speaker diarization}},&#xA;  Author = {{Bredin}, Herv{\&#39;e} and {Yin}, Ruiqing and {Coria}, Juan Manuel and {Gelly}, Gregory and {Korshunov}, Pavel and {Lavechin}, Marvin and {Fustes}, Diego and {Titeux}, Hadrien and {Bouaziz}, Wassim and {Gill}, Marie-Philippe},&#xA;  Booktitle = {ICASSP 2020, IEEE International Conference on Acoustics, Speech, and Signal Processing},&#xA;  Year = {2020},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{Bredin2021,&#xA;  Title = {{End-to-end speaker segmentation for overlap-aware resegmentation}},&#xA;  Author = {{Bredin}, Herv{\&#39;e} and {Laurent}, Antoine},&#xA;  Booktitle = {Proc. Interspeech 2021},&#xA;  Year = {2021},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;p&gt;For commercial enquiries and scientific consulting, please contact &lt;a href=&#34;mailto:herve@niderb.fr&#34;&gt;me&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;p&gt;The commands below will setup pre-commit hooks and packages needed for developing the &lt;code&gt;pyannote.audio&lt;/code&gt; library.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e .[dev,testing]&#xA;pre-commit install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Tests rely on a set of debugging files available in &lt;a href=&#34;https://raw.githubusercontent.com/pyannote/pyannote-audio/develop/test/data&#34;&gt;&lt;code&gt;test/data&lt;/code&gt;&lt;/a&gt; directory. Set &lt;code&gt;PYANNOTE_DATABASE_CONFIG&lt;/code&gt; environment variable to &lt;code&gt;test/data/database.yml&lt;/code&gt; before running tests:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;PYANNOTE_DATABASE_CONFIG=tests/data/database.yml pytest&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>z-x-yang/Segment-and-Track-Anything</title>
    <updated>2023-04-20T01:36:22Z</updated>
    <id>tag:github.com,2023-04-20:/z-x-yang/Segment-and-Track-Anything</id>
    <link href="https://github.com/z-x-yang/Segment-and-Track-Anything" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An open-source project dedicated to tracking and segmenting any objects in videos, either automatically or interactively. The primary algorithms utilized include the Segment Anything Model (SAM) for key-frame segmentation and Associating Objects with Transformers (AOT) for efficient tracking and propagation purposes.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Segment and Track Anything (SAM-Track)&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/z-x-yang/Segment-and-Track-Anything/main/assets/demo_3x2.gif&#34; width=&#34;880&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Segment and Track Anything&lt;/strong&gt; is an open-source project that focuses on the segmentation and tracking of any objects in videos, utilizing both automatic and interactive methods. The primary algorithms utilized include the &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;&lt;strong&gt;SAM&lt;/strong&gt; (Segment Anything Models)&lt;/a&gt; for automatic/interactive key-frame segmentation and the &lt;a href=&#34;https://github.com/yoxu515/aot-benchmark&#34;&gt;&lt;strong&gt;DeAOT&lt;/strong&gt; (Decoupling features in Associating Objects with Transformers)&lt;/a&gt; (NeurIPS2022) for efficient multi-object tracking and propagation. The SAM-Track pipeline enables dynamic and automatic detection and segmentation of new objects by SAM, while DeAOT is responsible for tracking all identified objects.&lt;/p&gt; &#xA;&lt;h2&gt;Demos&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://youtu.be/UPhtpf1k6HA&#34; title=&#34;Segment-and-Track-Anything Versatile Demo&#34;&gt;&lt;img src=&#34;https://res.cloudinary.com/marcomontalbano/image/upload/v1681713095/video_to_markdown/images/youtube--UPhtpf1k6HA-c05b58ac6eb4c4700831b2b3070cd403.jpg&#34; alt=&#34;Segment-and-Track-Anything Versatile Demo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;This video showcases the segmentation and tracking capabilities of SAM-Track in various scenarios, such as street views, AR, cells, animations, aerial shots, and more.&lt;/p&gt; &#xA;&lt;p&gt;Bilibili Video Link: &lt;a href=&#34;https://www.bilibili.com/video/BV1M24y1c77y/&#34;&gt;Versatile Demo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Colab notebook&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 1.0-Version Interactive WebUI &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;We will create a feature that enables users to interactively modify the mask for the initial video frame according to their needs. The interactive segmentation capabilities of Segment-and-Track-Anything is demonstrated in &lt;a href=&#34;https://www.youtube.com/watch?v=ctnD03w-5VA&#34;&gt;Demo1&lt;/a&gt; and &lt;a href=&#34;https://www.youtube.com/watch?v=DfCUGUxALYo&#34;&gt;Demo2&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Bilibili Video Link: &lt;a href=&#34;https://www.bilibili.com/video/BV1JL411v7uE/&#34;&gt;Demo1&lt;/a&gt;, &lt;a href=&#34;https://www.bilibili.com/video/BV1Qs4y1w763/&#34;&gt;Demo2&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Demo1&lt;/strong&gt; showcases SAM-Track&#39;s ability to interactively segment and track individual objects. The user specified that SAM-Track tracked a man playing street basketball.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Xyd54AngvV8&#34; title=&#34;Interactive Segment-and-Track-Anything Demo1&#34;&gt;&lt;img src=&#34;https://res.cloudinary.com/marcomontalbano/image/upload/v1681712022/video_to_markdown/images/youtube--Xyd54AngvV8-c05b58ac6eb4c4700831b2b3070cd403.jpg&#34; alt=&#34;Interactive Segment-and-Track-Anything Demo1&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;Demo2&lt;/strong&gt; showcases SAM-Track&#39;s ability to interactively add specified objects for tracking.The user customized the addition of objects to be tracked on top of the segmentation of everything in the scene using SAM-Track.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=eZrdna8JkoQ&#34; title=&#34;Interactive Segment-and-Track-Anything Demo2&#34;&gt;&lt;img src=&#34;https://res.cloudinary.com/marcomontalbano/image/upload/v1681712071/video_to_markdown/images/youtube--eZrdna8JkoQ-c05b58ac6eb4c4700831b2b3070cd403.jpg&#34; alt=&#34;Interactive Segment-and-Track-Anything Demo2&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;Segment-Anything&lt;/a&gt; repository has been cloned and renamed as sam, and the &lt;a href=&#34;https://github.com/yoxu515/aot-benchmark&#34;&gt;aot-benchmark&lt;/a&gt; repository has been cloned and renamed as aot.&lt;/p&gt; &#xA;&lt;p&gt;Please check the dependency requirements in &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;SAM&lt;/a&gt; and &lt;a href=&#34;https://github.com/yoxu515/aot-benchmark&#34;&gt;DeAOT&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The implementation is tested under python 3.9, as well as pytorch 1.10 and torchvision 0.11. &lt;strong&gt;We recommend equivalent or higher pytorch version&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Use the &lt;code&gt;install.sh&lt;/code&gt; to install the necessary libs for SAM-Track&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash script/install.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Model Preparation&lt;/h3&gt; &#xA;&lt;p&gt;Download SAM model to ckpt, the default model is SAM-VIT-B (&lt;a href=&#34;https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth&#34;&gt;sam_vit_b_01ec64.pth&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;Download DeAOT/AOT model to ckpt, the default model is R50-DeAOT-L (&lt;a href=&#34;https://drive.google.com/file/d/1QoChMkTVxdYZ_eBlZhK2acq9KMQZccPJ/view&#34;&gt;R50_DeAOTL_PRE_YTB_DAV.pth&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;You can download the default weights using the command line as shown below.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash script/download_ckpt.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Run Demo&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The video to be processed can be put in ./assets.&lt;/li&gt; &#xA; &lt;li&gt;Then run demo.ipynb step by step to generate results.&lt;/li&gt; &#xA; &lt;li&gt;The results will be saved as masks for each frame and a gif file for visualization.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The arguments for SAM-Track, DeAOT and SAM can be manually modified in model_args.py for purpose of using other models or controling the behavior of each model.&lt;/p&gt; &#xA;&lt;h3&gt;WebUI App&lt;/h3&gt; &#xA;&lt;p&gt;Our user-friendly visual interface allows you to easily obtain the results of your experiments. Simply initiate it using the command line.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Users can upload the video directly on the UI and use Segtracker to track all objects within that video. We use the depth-map video as a example.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/z-x-yang/Segment-and-Track-Anything/main/assets/gradio.jpg&#34; alt=&#34;Gradio&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Parameters:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;aot_model&lt;/strong&gt;: used to select which version of DeAOT/AOT to use for tracking and propagation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;sam_gap&lt;/strong&gt;: used to control how often SAM is used to add newly appearing objects at specified frame intervals. Increase to decrease the frequency of discovering new targets, but significantly improve speed of inference.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;points_per_side&lt;/strong&gt;: used to control the number of points per side used for generating masks by sampling a grid over the image. Increasing the size enhances the ability to detect small objects, but larger targets may be segmented into finer granularity.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;max_obj_num&lt;/strong&gt;: used to limit the maximum number of objects that SAM-Track can detect and track. A larger number of objects necessitates a greater utilization of memory, with approximately 16GB of memory capable of processing a maximum of 255 objects.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Usage:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Start app, use your browser to open the web-link.&lt;/li&gt; &#xA; &lt;li&gt;Click on the &lt;code&gt;input-video window&lt;/code&gt; to upload a video.&lt;/li&gt; &#xA; &lt;li&gt;Adjust SAM-Track parameters as needed.&lt;/li&gt; &#xA; &lt;li&gt;Click &lt;code&gt;Seg and Track&lt;/code&gt; to get the experiment results.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Credits&lt;/h3&gt; &#xA;&lt;p&gt;Licenses for borrowed code can be found in &lt;code&gt;licenses.md&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;DeAOT/AOT - &lt;a href=&#34;https://github.com/yoxu515/aot-benchmark&#34;&gt;https://github.com/yoxu515/aot-benchmark&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;SAM - &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;https://github.com/facebookresearch/segment-anything&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Gradio (for building WebUI) - &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;https://github.com/gradio-app/gradio&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;About us&lt;/h3&gt; &#xA;&lt;p&gt;Thank you for your interest in this project. The project is supervised by the ReLER Lab at Zhejiang University‚Äôs College of Computer Science and Technology. ReLER was established by Yang Yi, a Qiu Shi Distinguished Professor at Zhejiang University. Our dedicated team of contributors includes &lt;a href=&#34;https://github.com/yoxu515&#34;&gt;Yuanyou Xu&lt;/a&gt;, &lt;a href=&#34;https://github.com/yamy-cheng&#34;&gt;Yangming Cheng&lt;/a&gt;, &lt;a href=&#34;https://github.com/lingorX&#34;&gt;Liulei Li&lt;/a&gt;, &lt;a href=&#34;https://z-x-yang.github.io/&#34;&gt;Zongxin Yang&lt;/a&gt;, &lt;a href=&#34;https://sites.google.com/view/wenguanwang&#34;&gt;Wenguan Wang&lt;/a&gt; and &lt;a href=&#34;https://scholar.google.com/citations?user=RMSuNFwAAAAJ&amp;amp;hl=en&#34;&gt;Yi Yang&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mklarqvist/ai-playground</title>
    <updated>2023-04-20T01:36:22Z</updated>
    <id>tag:github.com,2023-04-20:/mklarqvist/ai-playground</id>
    <link href="https://github.com/mklarqvist/ai-playground" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ai-playground&lt;/h1&gt; &#xA;&lt;p&gt;Various AI fun stuff&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Title&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Colab&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mklarqvist/ai-playground/main/perplexity-clone/&#34;&gt;PerplexityAI clone&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A simple implementation to retrieve real-time data to feed to your large LLMs for QA tasks with references.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/mklarqvist/ai-playground/blob/main/perplexity-clone/perplexity_clone.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open in Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mklarqvist/ai-playground/main/google-magi-clone/&#34;&gt;Google Magi clone&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A clone of Google &#34;Magi&#34; in Nuxt3 + Vuetify based on their marketing material.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt;</summary>
  </entry>
</feed>