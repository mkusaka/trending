<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-18T01:30:08Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>bigcode-project/octopack</title>
    <updated>2023-08-18T01:30:08Z</updated>
    <id>tag:github.com,2023-08-18:/bigcode-project/octopack</id>
    <link href="https://github.com/bigcode-project/octopack" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üêô OctoPack: Instruction Tuning Code Large Language Models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OctoPack: Instruction Tuning Code Large Language Models&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bigcode-project/octopack/main/visuals/banner.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository provides an overview of all components from the paper &lt;a href=&#34;https://arxiv.org/abs/2308.07124&#34;&gt;OctoPack: Instruction Tuning Code Large Language Models&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;!-- TOC --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bigcode-project/octopack/main/#overview&#34;&gt;Overview&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bigcode-project/octopack/main/#data&#34;&gt;Data&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bigcode-project/octopack/main/#commitpack&#34;&gt;CommitPack&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bigcode-project/octopack/main/#commitpackft&#34;&gt;CommitPackFT&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bigcode-project/octopack/main/#other&#34;&gt;Other&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bigcode-project/octopack/main/#evaluation&#34;&gt;Evaluation&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bigcode-project/octopack/main/#run&#34;&gt;Run&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bigcode-project/octopack/main/#creation&#34;&gt;Creation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bigcode-project/octopack/main/#training&#34;&gt;Training&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bigcode-project/octopack/main/#octocoder&#34;&gt;OctoCoder&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bigcode-project/octopack/main/#octogeex&#34;&gt;OctoGeeX&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bigcode-project/octopack/main/#santacoder-finetuning&#34;&gt;SantaCoder Finetuning&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bigcode-project/octopack/main/#santacoder-pretraining-santacoderpack&#34;&gt;SantaCoder Pretraining (SantaCoderPack)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bigcode-project/octopack/main/#other-1&#34;&gt;Other&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bigcode-project/octopack/main/#visuals&#34;&gt;Visuals&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bigcode-project/octopack/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- /TOC --&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Data &lt;/th&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/bigcode/commitpack&#34;&gt;CommitPack&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;4TB of GitHub commits across 350 programming languages&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt; &lt;/th&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/bigcode/commitpackft&#34;&gt;CommitPackFT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Filtered version of CommitPack for high-quality commit messages that resemble instructions&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model &lt;/th&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/bigcode/octocoder&#34;&gt;OctoCoder&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;StarCoder (16B parameters) instruction tuned on CommitPackFT + OASST&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;th&gt; &lt;/th&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/bigcode/octogeex&#34;&gt;OctoGeeX&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;CodeGeeX2 (6B parameters) instruction tuned on CommitPackFT + OASST&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Evaluation &lt;/th&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/bigcode/humanevalpack&#34;&gt;HumanEvalPack&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Extension of OpenAI&#39;s HumanEval to cover 3 scenarios across 6 languages&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Data&lt;/h2&gt; &#xA;&lt;h3&gt;CommitPack&lt;/h3&gt; &#xA;&lt;p&gt;CommitPack is uploaded &lt;a href=&#34;https://huggingface.co/datasets/bigcode/commitpack&#34;&gt;here&lt;/a&gt;. To recreate:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;BigQuery SQL:&lt;/strong&gt; Use BigQuery to select the commit data from the GitHub action data. All SQL commands can be found in &lt;code&gt;dataset/commitpack/sql&lt;/code&gt;. They are executed in order starting with the first one to to the fifth one. They are separated and executed one-by-one as BigQuery was raising &lt;code&gt;Resources exceeded&lt;/code&gt; errors during query execution when running all in a single statement. After each SQL query a dataset is created and named as indicated in the filename. E.g. after executing &lt;code&gt;sql_1_commits_table_base.sql&lt;/code&gt;, you would name the output dataset &lt;code&gt;commits_table_base&lt;/code&gt;, which is then referenced in the 2nd statement.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Export:&lt;/strong&gt; From BigQuery export the dataset after the final SQL statement inside GCP to a bucket as parquet files.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Upload to HF:&lt;/strong&gt; Use a GCP compute instance to copy all the parquet files into a Hugging Face dataset and push it. The resulting dataset contains metadata on the commits, &lt;a href=&#34;https://huggingface.co/datasets/bigcode/commitpackmeta&#34;&gt;CommitPackMeta&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Scrape GitHub:&lt;/strong&gt; Run the script at &lt;code&gt;dataset/commitpack/scrape_github.py&lt;/code&gt; to download the files prior and after each git commit from GitHub. It contains some basic filters to remove noise files (relying on the extensions file at &lt;code&gt;dataset/commitpack/programming_languages.json&lt;/code&gt;) and then uses multi-threading and multi-processing for scraping. It is recommended to run it on a very large instance.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Shard (optional):&lt;/strong&gt; Depending on the size of your files, you may want to shard them at this point using the script at &lt;code&gt;dataset/commitpack/shard.sh&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Opt-out &amp;amp; languages:&lt;/strong&gt; Run the script at &lt;code&gt;dataset/commitpack/licenses_langs.py&lt;/code&gt; to remove repositories from users who opted out of the step (first part with &lt;code&gt;__main__&lt;/code&gt;, needs to be uncommented) and split the large files from the prior step into files for each programming language (second part with &lt;code&gt;__main__&lt;/code&gt;, currently uncommented). You will likely have to change some of the path names and uncomment parts as necessary&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Shard (optional):&lt;/strong&gt; Using the script at &lt;code&gt;dataset/commitpack/shard.py&lt;/code&gt; you can shard the large jsonl files for each language into smaller chunks with a specified size limit.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;CommitPackFT&lt;/h3&gt; &#xA;&lt;p&gt;CommitPackFT is uploaded &lt;a href=&#34;https://huggingface.co/datasets/bigcode/commitpackft&#34;&gt;here&lt;/a&gt;. To recreate:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Prepare:&lt;/strong&gt; Download &lt;a href=&#34;https://huggingface.co/datasets/bigcode/commitpack&#34;&gt;CommitPack&lt;/a&gt; via e.g. &lt;code&gt;git clone bigcode/commitpack&lt;/code&gt; or follow all the steps above to recreate it.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Filter:&lt;/strong&gt; Run &lt;code&gt;python dataset/commitpackft/commitpackft_filters1.py&lt;/code&gt; followed by &lt;code&gt;python dataset/commitpackft/commitpackft_filters2.py&lt;/code&gt;. You may want to modify some of the global variables defined in the scripts.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Other&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;StarCoder Self-Instruct: Uploaded &lt;a href=&#34;https://huggingface.co/datasets/codeparrot/self-instruct-starcoder&#34;&gt;here&lt;/a&gt;, to recreate see &lt;a href=&#34;https://github.com/ArmelRandy/Self-instruct&#34;&gt;this repository&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;xP3x: Uploaded &lt;a href=&#34;https://huggingface.co/datasets/bigcode/xp3x-octopack&#34;&gt;here&lt;/a&gt;, to recreate see the script at &lt;code&gt;dataset/xp3x/filter_xp3x.py&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;OASST: Uploaded &lt;a href=&#34;https://huggingface.co/datasets/bigcode/oasst-octopack&#34;&gt;here&lt;/a&gt;, to recreate see the script at &lt;code&gt;dataset/oasst/filter_oasst.py&lt;/code&gt;. Each line in the jsonl file is a conversation tree. We only keep the first two messages of each conversation tree, which are the question and answer.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;h3&gt;Run&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Setup:&lt;/strong&gt; Run the below bash code to setup the evaluation repository. If you want the repository in exactly the state we used it for the paper you can add the the flag &lt;code&gt;-b octopack&lt;/code&gt; to clone the branch we used for the paper. Generally, we recommend using the latest version of the code.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/bigcode-project/bigcode-evaluation-harness&#xA;# If you want the exact paper branch: git clone -b octopack https://github.com/bigcode-project/bigcode-evaluation-harness&#xA;cd bigcode-evaluation-harness&#xA;pip install -q -r requirements.txt&#xA;accelerate config&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Run:&lt;/strong&gt; You can then run a task via e.g.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;accelerate launch main.py \&#xA;--model bigcode/octocoder  \&#xA;--tasks humanevalfixtests-python \&#xA;--do_sample True \&#xA;--temperature 0.2 \&#xA;--n_samples 20 \&#xA;--batch_size 5 \&#xA;--allow_code_execution \&#xA;--save_generations \&#xA;--trust_remote_code \&#xA;--prompt octocoder \&#xA;--save_generations_path generations_humanevalfixpython_octocoder.json \&#xA;--metric_output_path evaluation_humanevalfixpython_octocoder.json \&#xA;--max_length_generation 2048 \&#xA;--precision bf16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Notes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;accelerate&lt;/code&gt;: You can also directly use &lt;code&gt;python main.py&lt;/code&gt;. Accelerate has the advantage of automatically handling mixed precision &amp;amp; devices.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;prompt&lt;/code&gt;: This defines the prompt. Example values are &lt;code&gt;octocoder&lt;/code&gt;, &lt;code&gt;wizardcoder&lt;/code&gt;, &lt;code&gt;instructcodet5p&lt;/code&gt;, &lt;code&gt;starchat&lt;/code&gt; which use the prompting format that is put forth by the respective model creators. You can refer to the actual &lt;a href=&#34;https://raw.githubusercontent.com/bigcode-project/bigcode-evaluation-harness/parity/lm_eval/tasks/humanevalpack.py&#34;&gt;evaluation file&lt;/a&gt; for how the prompt looks like.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;allow_code_execution&lt;/code&gt;: This will directly execute the evaluation and save results on your current machine. If you only want to create the generations and evaluate them later, you can add the flag &lt;code&gt;--generation_only&lt;/code&gt; and then evaluate them using e.g. the Colab notebook we provide in the next section. This is practical for languages you may not have installed on your machine, such as Rust.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;tasks&lt;/code&gt;: For HumanEvalPack, the tasks are the following:&lt;code&gt;&#39;humanevalfixdocs-cpp&#39;, &#39;humanevalfixdocs-go&#39;, &#39;humanevalfixdocs-java&#39;, &#39;humanevalfixdocs-js&#39;, &#39;humanevalfixdocs-python&#39;, &#39;humanevalfixdocs-rust&#39;, &#39;humanevalfixtests-cpp&#39;, &#39;humanevalfixtests-go&#39;, &#39;humanevalfixtests-java&#39;, &#39;humanevalfixtests-js&#39;, &#39;humanevalfixtests-python&#39;, &#39;humanevalfixtests-rust&#39;, &#39;humanevalexplaindescribe-cpp&#39;, &#39;humanevalexplaindescribe-go&#39;, &#39;humanevalexplaindescribe-java&#39;, &#39;humanevalexplaindescribe-js&#39;, &#39;humanevalexplaindescribe-python&#39;, &#39;humanevalexplaindescribe-rust&#39;, &#39;humanevalexplainsynthesize-cpp&#39;, &#39;humanevalexplainsynthesize-go&#39;, &#39;humanevalexplainsynthesize-java&#39;, &#39;humanevalexplainsynthesize-js&#39;, &#39;humanevalexplainsynthesize-python&#39;, &#39;humanevalexplainsynthesize-rust&#39;, &#39;humanevalsynthesize-cpp&#39;, &#39;humanevalsynthesize-go&#39;, &#39;humanevalsynthesize-java&#39;, &#39;humanevalsynthesize-js&#39;, &#39;humanevalsynthesize-python&#39;, &#39;humanevalsynthesize-rust&#39;&lt;/code&gt;. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;HumanEvalFix is divided into two parts: One where only tests are provided and no docstrings (main focus of the paper) and one where instead of tests docstrings are provided as the source of truth (appendix).&lt;/li&gt; &#xA;   &lt;li&gt;HumanEvalExplain consists of describing first and then synthesizing given the descriptions. You need to run these tasks sequentially. For the describing you can activate &lt;code&gt;--generation_only&lt;/code&gt; as there is no evaluation yet. For the synthesizing part, you need to provide the descriptions via &lt;code&gt;--load_data_path&lt;/code&gt;, which will then be used to synthesize answers. &lt;code&gt;n_samples&lt;/code&gt; is set to 1 for synthesis as we generate 1 answer for each description (multiple samples have already been generated for the descriptions via &lt;code&gt;n_samples&lt;/code&gt;). See below for an example:&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;accelerate launch main.py \&#xA;--model bigcode/octocoder  \&#xA;--tasks humanevalexplaindescribe-python \&#xA;--generation_only \&#xA;--do_sample True \&#xA;--temperature 0.2 \&#xA;--n_samples 20 \&#xA;--batch_size 5 \&#xA;--allow_code_execution \&#xA;--save_generations \&#xA;--trust_remote_code \&#xA;--prompt octocoder \&#xA;--save_generations_path generations_humanevalexplaindescribepython_octocoder.json \&#xA;--max_length_generation 2048 \&#xA;--precision bf16&#xA;&#xA;accelerate launch main.py \&#xA;--model bigcode/octocoder  \&#xA;--tasks humanevalexplaindescribe-python \&#xA;--generation_only \&#xA;--do_sample True \&#xA;--temperature 0.2 \&#xA;--n_samples 1 \&#xA;--batch_size 1 \&#xA;--allow_code_execution \&#xA;--save_generations \&#xA;--trust_remote_code \&#xA;--prompt octocoder \&#xA;--load_data_path generations_humanevalexplaindescribepython_octocoder.json \&#xA;--save_generations_path generations_humanevalexplainsynthesizepython_octocoder.json \&#xA;--metric_output_path evaluation_humanevalexplainpython_octocoder.json \&#xA;--max_length_generation 2048 \&#xA;--precision bf16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;HumanEvalSynthesize is an extension of HumanEval. If you would like to run with the original HumanEval prompt that relies on pure function continuation you can use the flag &lt;code&gt;--prompt continue&lt;/code&gt;. OctoCoder uses &lt;code&gt;--prompt octocoder&lt;/code&gt; as shown in the below script. The below script should reproduce the pass@1 HumanEval score of 46.2% for OctoCoder:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;accelerate launch main.py \&#xA;--model bigcode/octocoder  \&#xA;--tasks humanevalsynthesize-python \&#xA;--do_sample True \&#xA;--temperature 0.2 \&#xA;--n_samples 20 \&#xA;--batch_size 5 \&#xA;--allow_code_execution \&#xA;--save_generations \&#xA;--trust_remote_code \&#xA;--prompt octocoder \&#xA;--save_generations_path generations_humanevalsynthesizepython_octocoder.json \&#xA;--metric_output_path evaluation_humanevalsynthesizepython_octocoder.json \&#xA;--max_length_generation 2048 \&#xA;--precision bf16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Unfortunately, there is some randomness depending on the Python version you use for evaluation and the &lt;code&gt;batch_size&lt;/code&gt;. We use &lt;code&gt;batch_size=5&lt;/code&gt; and Python 3.9.13&lt;/li&gt; &#xA; &lt;li&gt;We provide the exact scripts we used in &lt;code&gt;evaluation/run/eval_scripts&lt;/code&gt; for each model. There is also a &lt;code&gt;_range.sh&lt;/code&gt; script for each task (e.g. &lt;code&gt;evaluation/run/eval_scripts/eval_humanevalfix_range.sh&lt;/code&gt;), which runs each sample individually. This is much faster if you have multiple GPUs available. In the &lt;code&gt;_range.sh&lt;/code&gt; scripts you need to specify the model and language you would like to run. After running it, you will have 164 generation files, which you need to merge with &lt;code&gt;python evaluation/run/merge_generations.py &#34;generations_*json&#34;&lt;/code&gt;. Subsequently, you need to run the evaluation as explained in the next step.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Evaluate:&lt;/strong&gt; If you have only created generations without evaluating them (e.g. by adding the &lt;code&gt;--generation_only&lt;/code&gt; flag or using &lt;code&gt;_range.sh&lt;/code&gt; scripts), you can use the notebook at &lt;code&gt;evaluation/run/humanevalpack_evaluation&lt;/code&gt; or &lt;a href=&#34;https://colab.research.google.com/drive/1tlpGcDPdKKMDqDS0Ihwh2vR_MGlzAPC_?usp=sharing&#34;&gt;this colab&lt;/a&gt; to evaluate the generations. It contains a section for each programming language where it installs the language first and then given the path to your generations evaluates them providing you with the pass@k scores.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Creation&lt;/h3&gt; &#xA;&lt;p&gt;To create HumanEvalPack, we follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;We use the upper commented out part of the script at &lt;code&gt;evaluation/create/prepare_humaneval.py&lt;/code&gt; to create a JSON with the solution for each humaneval language in &lt;code&gt;evaluation/create/humaneval-x/data&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;We then manually go through each JSON file (e.g. &lt;code&gt;evaluation/create/humaneval-x/data/cpp/data/humanevalpack.json&lt;/code&gt;) to introduce a bug across all languages in parallel.&lt;/li&gt; &#xA; &lt;li&gt;We also make several fixes to the humaneval-x dataset, all of which are documented at the top of &lt;code&gt;evaluation/create/humaneval-x/README.md&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;We run the lower part of &lt;code&gt;evaluation/create/prepare_humaneval.py&lt;/code&gt; to turn the JSON files back into JSONL files with the buggy solution, an instruction column and some other metadata. These JSONL files located at e.g. &lt;code&gt;evaluation/create/humaneval-x/data/cpp/data/humanevalpack.jsonl&lt;/code&gt; are then uploaded into the HF dataset at &lt;a href=&#34;https://huggingface.co/datasets/bigcode/humanevalpack&#34;&gt;https://huggingface.co/datasets/bigcode/humanevalpack&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;h3&gt;OctoCoder&lt;/h3&gt; &#xA;&lt;p&gt;The finetuning script to create OctoCoder is at &lt;code&gt;finetuning/starcoder/finetune.py&lt;/code&gt;. The folder contains a &lt;code&gt;README.md&lt;/code&gt; with instructions.&lt;/p&gt; &#xA;&lt;h3&gt;OctoGeeX&lt;/h3&gt; &#xA;&lt;p&gt;OctoGeeX is finetuned based on &lt;a href=&#34;https://huggingface.co/THUDM/codegeex2-6b&#34;&gt;CodeGeeX2-6B&lt;/a&gt; using an internal training framework. The hyperparameters are as follows:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Parameter&lt;/th&gt; &#xA;   &lt;th&gt;Value&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;tp_size&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;global_batch_size&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;48&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;lr&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;5e-5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;train_step&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;50&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;seq_length&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;8192&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;precision&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;bf16&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;It is also compatible with &lt;code&gt;finetuning/finetune.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;SantaCoder Finetuning&lt;/h3&gt; &#xA;&lt;p&gt;The finetuning script for santacoder is at &lt;code&gt;finetuning/santacoder/finetune.py&lt;/code&gt;. The default hyperparameters are set for the &lt;code&gt;line diff&lt;/code&gt; format, as described in the Appendix H.&lt;/p&gt; &#xA;&lt;h3&gt;SantaCoder Pretraining (SantaCoderPack)&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Obtain Megatron-LM by executing &lt;code&gt;git clone https://github.com/bigcode-project/Megatron-LM&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Download the dataset: Download a pretraining dataset (commitpack-subset-cf) using the &lt;code&gt;git clone https://huggingface.co/datasets/bigcode/commitpack-subset-cf&lt;/code&gt;, and merge all jsonl files into one jsonl file. You can name it as you prefer, such as &lt;code&gt;commitpack_cf.jsonl&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Move the files &lt;code&gt;training/preprocess_santacoderpack.sh&lt;/code&gt; and &lt;code&gt;training/pretraining_santacoderpack.sh&lt;/code&gt; to the &lt;code&gt;Megatron-LM&lt;/code&gt; directory.&lt;/li&gt; &#xA; &lt;li&gt;Tokenize the pretraining dataset by modifying &lt;code&gt;preprocess_santacoderpack.sh&lt;/code&gt; to point to your jsonl file. Also, change the path of the tokenizer to point to StarCoder&#39;s &lt;code&gt;tokenizer.json&lt;/code&gt; by using &lt;code&gt;wget https://huggingface.co/bigcode/starcoderbase/raw/main/tokenizer.json&lt;/code&gt;. Finally, specify an output prefix where the tokenized data will be stored, and run the script using &lt;code&gt;bash preprocess_santacoderpack.sh&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Modify &lt;code&gt;pretraining_santacoderpack.sh&lt;/code&gt; to adjust the &lt;code&gt;CHECKPOINT_PATH&lt;/code&gt; so that it points to the saved Megatron-LM checkpoint, and set the &lt;code&gt;TOKENIZER_FILE&lt;/code&gt; to StarCoder&#39;s &lt;code&gt;tokenizer.json&lt;/code&gt;. Make sure to point to the correct environment and cache locations, and alter any custom settings to fit your setup. Run the script by executing &lt;code&gt;bash pretraining_santacoderpack.sh&lt;/code&gt;!&lt;/li&gt; &#xA; &lt;li&gt;Convert the saved checkpoint using the script located at &lt;code&gt;convert_large.sh&lt;/code&gt;. It contains instructions which repos to download.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Other&lt;/h3&gt; &#xA;&lt;p&gt;We did not end up using Megatron-LM fine-tuning for the model in the paper, but implemented it nevertheless. Feel free to follow these instructions to use it:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Get the StarCoderBase Megatron-LM checkpoint: &lt;code&gt;git clone https://huggingface.co/bigcode/starcoderbase-megatron&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Get Megatron-LM: &lt;code&gt;git clone -b mtf https://github.com/bigcode-project/Megatron-LM&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Prepare a Python environment with PyTorch. (TODO: There may be some other packages needed that you will find out about when training fails)&lt;/li&gt; &#xA; &lt;li&gt;Prepare dataset: Preapre a finetuning dataset in the form of a single jsonl file with two keys: &lt;code&gt;inputs&lt;/code&gt; &amp;amp; &lt;code&gt;outputs&lt;/code&gt;. &lt;code&gt;inputs&lt;/code&gt; should contain the prompt and instruction while &lt;code&gt;outputs&lt;/code&gt; contains the targets. Loss will only be computed over &lt;code&gt;outputs&lt;/code&gt;. See &lt;code&gt;dataset/commits_to_jsonl.py&lt;/code&gt; for an example of doing this. In that example we put the instruction (commit message) in the target, but it&#39;s better to put it in the input.&lt;/li&gt; &#xA; &lt;li&gt;Tokenize the fine-tuning dataset by modifying &lt;code&gt;dataset/preprocess.sh&lt;/code&gt; to point to your jsonl dataset. Also modify the path of the tokenizer, in our case point to the StarCoder&#39;s &lt;code&gt;tokenizer.json&lt;/code&gt; (&lt;code&gt;wget https://huggingface.co/bigcode/starcoderbase/raw/main/tokenizer.json&lt;/code&gt;). Finally specify an output prefix where the tokenized data will be stored. Then run it with &lt;code&gt;bash dataset/preprocess.sh&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Create two files &lt;code&gt;train_data_paths.txt.tmp&lt;/code&gt; and &lt;code&gt;valid_data_paths.txt.tmp&lt;/code&gt; that contain the paths to the above created tokenized dataset. For example they could look like &lt;code&gt;&#34;train: 1.0 0:0.95 output_prefix&#34;&lt;/code&gt; and &lt;code&gt;&#34;valid: 1.0 0.95:1.0 output_prefix&lt;/code&gt;. In this case the dataset is split into 95% training and 5% validation. The first number is the weight of the dataset, the second number is the start of the dataset and the third number is the end of the dataset.&lt;/li&gt; &#xA; &lt;li&gt;Rename the checkpoint downloaded to &lt;code&gt;release&lt;/code&gt; i.e. &lt;code&gt;mv starcoderbase-megatron/iter* starcoderbase-megatron/release&lt;/code&gt; and create a file &lt;code&gt;starcoderbase-megatron/latest_checkpointed_iteration.txt&lt;/code&gt; that contains simply &lt;code&gt;release&lt;/code&gt; (&lt;code&gt;echo release &amp;gt; starcoderbase-megatron/latest_checkpointed_iteration.txt&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Modify &lt;code&gt;training/finetune_starcoderbase.sh&lt;/code&gt; to adapt &lt;code&gt;CHECKPOINT_PATH&lt;/code&gt; to point to the downloaded Megatron-LM checkpoint, &lt;code&gt;WEIGHTS_TRAIN&lt;/code&gt; &amp;amp; &lt;code&gt;WEIGHTS_VALID&lt;/code&gt; to point to the above created txt files, &lt;code&gt;TOKENIZER_FILE&lt;/code&gt; to StarCoder&#39;s &lt;code&gt;tokenizer.json&lt;/code&gt;, point to your environment and cache locations, and modify the SBATCH settings to suit your setup. Then run it with &lt;code&gt;bash training/finetune_starcoderbase.sh&lt;/code&gt;. You can interrupt and resume training, however, if you resume, you need to remove &lt;code&gt;--no_load_optim&lt;/code&gt; and &lt;code&gt;--no_load_rng&lt;/code&gt; from the command line arguments in the script to load the optimizer and random number generator state from the newly saved checkpoint (we only do not want to load them from starcoderbase).&lt;/li&gt; &#xA; &lt;li&gt;Convert the saved checkpoint using the script at &lt;code&gt;convert_large.sh&lt;/code&gt;. It contains instructions which repos to download.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Visuals&lt;/h2&gt; &#xA;&lt;p&gt;Figures:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Figure 1: &lt;code&gt;visuals/main.pdf&lt;/code&gt;, create the main plot in &lt;code&gt;visuals/plots.ipynb&lt;/code&gt; or via &lt;a href=&#34;https://colab.research.google.com/drive/17OIf7rzAeetH9JchYT2RMyPUop1Z12JJ?usp=sharing&#34;&gt;this colab&lt;/a&gt; and then add it to the correct tab in &lt;code&gt;visuals/visuals.drawio&lt;/code&gt; which can be opened with &lt;a href=&#34;https://app.diagrams.net/&#34;&gt;drawio&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Figure 2 (Upper): &lt;code&gt;visuals/distribution.pdf&lt;/code&gt;, create via &lt;code&gt;visuals/plots.ipynb&lt;/code&gt; or &lt;a href=&#34;https://colab.research.google.com/drive/17OIf7rzAeetH9JchYT2RMyPUop1Z12JJ?usp=sharing&#34;&gt;colab&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Figure 2 (Lower): &lt;code&gt;visuals/tasks.pdf&lt;/code&gt;, create via &lt;code&gt;visuals/distribution_tasks.py&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Figure 3: &lt;code&gt;visuals/humanevalpack.pdf&lt;/code&gt;, create via &lt;code&gt;visuals/visuals.drawio&lt;/code&gt; which can be opened with &lt;a href=&#34;https://app.diagrams.net/&#34;&gt;drawio&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Figure 4: &lt;code&gt;visuals/ablations.pdf&lt;/code&gt;, create via &lt;code&gt;visuals/plots.ipynb&lt;/code&gt; or &lt;a href=&#34;https://colab.research.google.com/drive/17OIf7rzAeetH9JchYT2RMyPUop1Z12JJ?usp=sharing&#34;&gt;this colab&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Other Figures: Manual&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Tables:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Table 4: Create via &lt;code&gt;visual/distribution_languages.py&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Other Tables: Manual&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{muennighoff2023octopack,&#xA;      title={OctoPack: Instruction Tuning Code Large Language Models}, &#xA;      author={Niklas Muennighoff and Qian Liu and Armel Zebaze and Qinkai Zheng and Binyuan Hui and Terry Yue Zhuo and Swayam Singh and Xiangru Tang and Leandro von Werra and Shayne Longpre},&#xA;      journal={arXiv preprint arXiv:2308.07124},&#xA;      year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>evidentlyai/evidently</title>
    <updated>2023-08-18T01:30:08Z</updated>
    <id>tag:github.com,2023-08-18:/evidentlyai/evidently</id>
    <link href="https://github.com/evidentlyai/evidently" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Evaluate and monitor ML models from validation to production. Join our Discord: https://discord.com/invite/xZjKRaNp8b&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt;Evidently&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;b&gt;An open-source framework to evaluate, test and monitor ML models in production.&lt;/b&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://docs.evidentlyai.com&#34;&gt;Docs&lt;/a&gt; | &lt;a href=&#34;https://discord.gg/xZjKRaNp8b&#34;&gt;Discord&lt;/a&gt; | &lt;a href=&#34;https://www.evidentlyai.com/user-newsletter&#34;&gt;User Newsletter&lt;/a&gt; | &lt;a href=&#34;https://evidentlyai.com/blog&#34;&gt;Blog&lt;/a&gt; | &lt;a href=&#34;https://twitter.com/EvidentlyAI&#34;&gt;Twitter&lt;/a&gt; | &lt;a href=&#34;https://www.evidentlyai.com/product/cloud&#34;&gt;Evidently Cloud&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h1&gt;&lt;span&gt;üÜï&lt;/span&gt; New release&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Evidently 0.4.0&lt;/strong&gt;. Self-host an ML Monitoring interface -&amp;gt; &lt;a href=&#34;https://docs.evidentlyai.com/get-started/tutorial-monitoring&#34;&gt;QuickStart&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;&lt;span&gt;üìä&lt;/span&gt; What is Evidently?&lt;/h1&gt; &#xA;&lt;p&gt;Evidently is an open-source Python library for data scientists and ML engineers. It helps evaluate, test, and monitor ML models from validation to production. It works with tabular, text data and embeddings.&lt;/p&gt; &#xA;&lt;p&gt;Evidently has a modular approach with 3 components on top of the shared &lt;code&gt;metrics&lt;/code&gt; functionality.&lt;/p&gt; &#xA;&lt;h2&gt;1. Tests: batch model checks&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/evidentlyai/evidently/main/docs/images/evidently_tests_main-min.png&#34; alt=&#34;Tests example&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Tests perform structured data and ML model checks. They verify a condition and return an explicit &lt;strong&gt;pass&lt;/strong&gt; or &lt;strong&gt;fail&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can create a custom Test Suite from 50+ tests or run a preset (for example, &lt;strong&gt;Data Drift&lt;/strong&gt; or &lt;strong&gt;Regression Performance&lt;/strong&gt;). You can get results as a &lt;strong&gt;JSON&lt;/strong&gt;, Python dictionary, exportable HTML, &lt;strong&gt;visual report&lt;/strong&gt; inside Jupyter notebook, or as Evidently JSON &lt;code&gt;snapshot&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Tests are best for automated checks. You can integrate them as a pipeline step using tools like Airflow.&lt;/p&gt; &#xA;&lt;h2&gt;2. Reports: interactive visualizations&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Old dashboards API was deprecated in v0.1.59. Here is the &lt;a href=&#34;https://raw.githubusercontent.com/evidentlyai/evidently/main/docs/book/support/migration.md&#34;&gt;migration guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/evidentlyai/evidently/main/docs/images/evidently_reports_main-min.png&#34; alt=&#34;Report example&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Reports calculate various data and ML &lt;strong&gt;metrics&lt;/strong&gt; and render rich &lt;strong&gt;visualizations&lt;/strong&gt;. You can create a custom Report or run a preset to evaluate a specific aspect of the model or data performance. For example, a &lt;a href=&#34;https://docs.evidentlyai.com/presets/data-quality&#34;&gt;&lt;strong&gt;Data Quality&lt;/strong&gt;&lt;/a&gt; or &lt;a href=&#34;https://docs.evidentlyai.com/presets/class-performance&#34;&gt;&lt;strong&gt;Classification Performance&lt;/strong&gt;&lt;/a&gt; report.&lt;/p&gt; &#xA;&lt;p&gt;You can get an &lt;strong&gt;HTML report&lt;/strong&gt; (best for exploratory analysis and debugging), &lt;strong&gt;JSON&lt;/strong&gt; or Python dictionary output (best for logging, documentation or to integrate with BI tools), or as Evidently JSON &lt;code&gt;snapshot&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;3. ML monitoring dashboard&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;This functionality is available from v0.4.0.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/evidentlyai/evidently/main/docs/images/evidently_ml_monitoring_main.png&#34; alt=&#34;Dashboard example&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can self-host an ML monitoring dashboard to visualize metrics and test results over time. This functionality sits on top of Reports and Test Suites. You must store their outputs as Evidently JSON &lt;code&gt;snapshots&lt;/code&gt; that serve as a data source for the Evidently Monitoring UI.&lt;/p&gt; &#xA;&lt;p&gt;You can track 100+ metrics available in Evidently, from the number of nulls to text sentiment and embedding drift.&lt;/p&gt; &#xA;&lt;h1&gt;&lt;span&gt;üë©üíª&lt;/span&gt; Install Evidently&lt;/h1&gt; &#xA;&lt;h3&gt;MAC OS and Linux&lt;/h3&gt; &#xA;&lt;p&gt;Evidently is available as a PyPI package. To install it using pip package manager, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install evidently&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Evidently is also available on Anaconda distribution platform. To install Evidently using conda installer, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;conda install -c conda-forge evidently&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want visualize the Reports or Test Suites inside Jupyter notebook, you need &lt;code&gt;jupyter nbextension&lt;/code&gt;. After installing &lt;code&gt;evidently&lt;/code&gt;, run the two following commands in the terminal from the evidently directory. This is not required if you want to use Evidently Monitoring UI.&lt;/p&gt; &#xA;&lt;p&gt;To install &lt;code&gt;jupyter nbextension&lt;/code&gt;, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;jupyter nbextension install --sys-prefix --symlink --overwrite --py evidently&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To enable it, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;jupyter nbextension enable evidently --py --sys-prefix&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;That&#39;s it! A single run after the installation is enough.&lt;/p&gt; &#xA;&lt;h3&gt;Windows&lt;/h3&gt; &#xA;&lt;p&gt;Evidently is available as a PyPI package. To install it using pip package manager, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install evidently&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To install Evidently using conda installer, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;conda install -c conda-forge evidently&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you run Jupyter notebook on Windows, you will use a different method to display Reports and Test Suites. You must add the argument &lt;code&gt;inline&lt;/code&gt; when calling the Report: &lt;code&gt;report.show(mode=&#39;inline&#39;)&lt;/code&gt;. Read more about different environments in the &lt;a href=&#34;https://docs.evidentlyai.com/integrations/notebook-environments&#34;&gt;docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;&lt;span&gt;‚ñ∂&lt;/span&gt; Getting started&lt;/h1&gt; &#xA;&lt;h3&gt;Option 1: Test Suites&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;This is a simple Hello World example. Head to docs for a complete &lt;a href=&#34;https://docs.evidentlyai.com/get-started/tutorial&#34;&gt;Quickstart for Reports and Test Suites&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Prepare your data as two pandas &lt;code&gt;DataFrames&lt;/code&gt;. The first is your reference data, and the second is current production data.&amp;nbsp;The structure of both datasets should be identical. You need input features only to run some evaluations (e.g., Data Drift). In other cases (e.g., Target Drift, Classification Performance), you need Target and/or Prediction.&lt;/p&gt; &#xA;&lt;p&gt;After installing the tool, import the Evidently &lt;strong&gt;Test Suite&lt;/strong&gt; and required presets. We&#39;ll use a simple toy dataset:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd&#xA;&#xA;from sklearn import datasets&#xA;&#xA;from evidently.test_suite import TestSuite&#xA;from evidently.test_preset import DataStabilityTestPreset&#xA;from evidently.test_preset import DataQualityTestPreset&#xA;&#xA;iris_data = datasets.load_iris(as_frame=&#39;auto&#39;)&#xA;iris_frame = iris_data.frame&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run the &lt;strong&gt;Data Stability&lt;/strong&gt; Test Suite and display the output in the notebook:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_stability= TestSuite(tests=[&#xA;    DataStabilityTestPreset(),&#xA;])&#xA;data_stability.run(current_data=iris_frame.iloc[:60], reference_data=iris_frame.iloc[60:], column_mapping=None)&#xA;data_stability &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also save an HTML file. You&#39;ll need to open it from the destination folder.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_stability.save_html(&#34;file.html&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To get the output as JSON:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_stability.json()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Option 2: Reports&lt;/h3&gt; &#xA;&lt;p&gt;After installing the tool, import the Evidently &lt;strong&gt;Report&lt;/strong&gt; and required presets:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd&#xA;&#xA;from sklearn import datasets&#xA;&#xA;from evidently.report import Report&#xA;from evidently.metric_preset import DataDriftPreset&#xA;&#xA;iris_data = datasets.load_iris(as_frame=&#39;auto&#39;)&#xA;iris_frame = iris_data.frame&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To generate the &lt;strong&gt;Data Drift&lt;/strong&gt; report, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_drift_report = Report(metrics=[&#xA;    DataDriftPreset(),&#xA;])&#xA;&#xA;data_drift_report.run(current_data=iris_frame.iloc[:60], reference_data=iris_frame.iloc[60:], column_mapping=None)&#xA;data_drift_report&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Save the report as HTML. You&#39;ll later need to open it from the destination folder.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_drift_report.save_html(&#34;file.html&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To get the output as JSON:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_drift_report.json()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Option 3: ML monitoring dashboard&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;This will launch a demo project in the Evidently UI. Head to docs for a complete &lt;a href=&#34;https://docs.evidentlyai.com/get-started/tutorial-monitoring&#34;&gt;ML Monitoring Quickstart&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Recommended step: create a virtual environment and activate it.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install virtualenv&#xA;virtualenv venv&#xA;source venv/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After installing Evidently (&lt;code&gt;pip install evidently&lt;/code&gt;), run the Evidently UI with the demo project:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;evidently ui --demo-project&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Access Evidently UI service in your browser. Go to the &lt;strong&gt;localhost:8000&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;&lt;span&gt;üíª&lt;/span&gt; Contributions&lt;/h1&gt; &#xA;&lt;p&gt;We welcome contributions! Read the &lt;a href=&#34;https://raw.githubusercontent.com/evidentlyai/evidently/main/CONTRIBUTING.md&#34;&gt;Guide&lt;/a&gt; to learn more.&lt;/p&gt; &#xA;&lt;h1&gt;&lt;span&gt;üìö&lt;/span&gt; Documentation&lt;/h1&gt; &#xA;&lt;p&gt;For more information, refer to a complete &lt;a href=&#34;https://docs.evidentlyai.com&#34;&gt;Documentation&lt;/a&gt;. You can start with the tutorials:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.evidentlyai.com/get-started/tutorial&#34;&gt;Get Started with Reports and Test Suites&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.evidentlyai.com/get-started/tutorial-monitoring&#34;&gt;Get Started with ML monitoring&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;&lt;span&gt;üóÇ&lt;/span&gt; Examples&lt;/h1&gt; &#xA;&lt;p&gt;Simple examples on toy datasets to quickly explore what Evidently can do out of the box.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Title&lt;/th&gt; &#xA;   &lt;th&gt;Code example&lt;/th&gt; &#xA;   &lt;th&gt;Tutorial&lt;/th&gt; &#xA;   &lt;th&gt;Contents&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;QuickStart Tutorial: ML Monitoring&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/evidentlyai/evidently/raw/main/examples/sample_notebooks/get_started_monitoring.py&#34;&gt;Example&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.evidentlyai.com/get-started/tutorial-monitoring&#34;&gt;Tutorial&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Pre-built ML monitoring dashboard.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;QuickStart Tutorial: Tests and Reports&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/evidentlyai/evidently/raw/main/examples/sample_notebooks/getting_started_tutorial.ipynb&#34;&gt;Jupyter notebook&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1j0Wh4LM0mgMuDY7LQciLaUV4G1khB-zb&#34;&gt;Colab&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.evidentlyai.com/get-started/tutorial&#34;&gt;Tutorial&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Data Stability and custom Test Suites, Data Drift and Target Drift Reports&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Evidently Metric Presets&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/evidentlyai/evidently/raw/main/examples/sample_notebooks/evidently_metric_presets.ipynb&#34;&gt;Jupyter notebook&lt;/a&gt; &lt;br&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1wmHWipPd6iEy9Ce8NWBcxs_BSa9hgKgk&#34;&gt;Colab&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;Data Drift, Target Drift, Data Quality, Regression, Classification Reports&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Evidently Metrics&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/evidentlyai/evidently/raw/main/examples/sample_notebooks/evidently_metrics.ipynb&#34;&gt;Jupyter notebook&lt;/a&gt; &lt;br&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1IpfQsq5dmjuG_Qbn6BNtghq6aubZBP5A&#34;&gt;Colab&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;All individual Metrics&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Evidently Test Presets&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/evidentlyai/evidently/raw/main/examples/sample_notebooks/evidently_test_presets.ipynb&#34;&gt;Jupyter notebook&lt;/a&gt; &lt;br&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1CBAFY1qmHHV_72SC7YBeaD4c6LLpPQan&#34;&gt;Colab&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;NoTargetPerformance, Data Stability, Data Quality, Data Drift Regression, Multi-class Classification, Binary Classification, Binary Classification top-K test suites&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Evidently Tests&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/evidentlyai/evidently/raw/main/examples/sample_notebooks/evidently_tests.ipynb&#34;&gt;Jupyter notebook&lt;/a&gt; &lt;br&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1nQhfXft4VZ3G7agvXgH_LqVHdCh-WaMl&#34;&gt;Colab&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;All individual Tests&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;There are more example in the &lt;a href=&#34;https://github.com/evidentlyai/community-examples&#34;&gt;Community Examples&lt;/a&gt; repository.&lt;/p&gt; &#xA;&lt;h2&gt;Integrations&lt;/h2&gt; &#xA;&lt;p&gt;Explore &lt;a href=&#34;https://github.com/evidentlyai/evidently/tree/main/examples/integrations&#34;&gt;Integrations&lt;/a&gt; to see how to integrate Evidently in the prediction pipelines and with other tools.&lt;/p&gt; &#xA;&lt;h2&gt;How-to guides&lt;/h2&gt; &#xA;&lt;p&gt;Explore the &lt;a href=&#34;https://github.com/evidentlyai/evidently/tree/main/examples/how_to_questions&#34;&gt;How-to guides&lt;/a&gt; to understand specific features in Evidently, such as working with text data.&lt;/p&gt; &#xA;&lt;h1&gt;&lt;span&gt;‚òé&lt;/span&gt; User Newsletter&lt;/h1&gt; &#xA;&lt;p&gt;To get updates on new features, integrations and code tutorials, sign up for the &lt;a href=&#34;https://www.evidentlyai.com/user-newsletter&#34;&gt;Evidently User Newsletter&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;&lt;span&gt;‚úÖ&lt;/span&gt; Discord Community&lt;/h1&gt; &#xA;&lt;p&gt;If you want to chat and connect, join our &lt;a href=&#34;https://discord.gg/xZjKRaNp8b&#34;&gt;Discord community&lt;/a&gt;!&lt;/p&gt;</summary>
  </entry>
</feed>