<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-22T01:38:05Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>lchen001/LLMDrift</title>
    <updated>2023-07-22T01:38:05Z</updated>
    <id>tag:github.com,2023-07-22:/lchen001/LLMDrift</id>
    <link href="https://github.com/lchen001/LLMDrift" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üéì LLM Drifts: How Is ChatGPT‚Äôs Behavior Changing over Time?&lt;/h1&gt; &#xA;&lt;p&gt;Large language models (LLM) services such as GPT-4 and GPT-3.5 are widely being used. However, when and how these models are updated over time is opaque. Towards filling in this gap, this repository contains (i) a diverse set of &lt;em&gt;datasets&lt;/em&gt;, and (ii) &lt;em&gt;generations&lt;/em&gt; from popular LLMs (including GPT-4 and GPT-3.5) on these datasets over time.&lt;/p&gt; &#xA;&lt;h2&gt;üîç Main Findings&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img align=&#34;center&#34; src=&#34;https://raw.githubusercontent.com/lchen001/LLMDrift/main/asset/intro.png&#34; width=&#34;460px&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;b&gt;Figure 1:&lt;/b&gt; Performance of the March 2023 and June 2023 versions of GPT-4 and GPT-3.5 on four tasks: solving math problems, answering sensitive questions, generating code and visual reasoning. The performances of GPT-4 and GPT-3.5 can vary substantially over time, and for the worse in some tasks. &lt;/p&gt; &#xA;&lt;p&gt;What are the main findings? In a nutshell, there are many interesting performance shifts over time. For example, GPT-4 (March 2023) was very good at identifying prime numbers (accuracy 97.6%) but GPT-4 (June 2023) was very poor on these same questions (accuracy 2.4%). Interestingly GPT-3.5 (June 2023) was much better than GPT-3.5 (March 2023) in this task. We hope releasing the datasets and generations can help the community to understand how LLM services drift better. The above figure gives a quantatitive summary.&lt;/p&gt; &#xA;&lt;h2&gt;üöÄ Reproducing the Results&lt;/h2&gt; &#xA;&lt;p&gt;You can directly run the &lt;a href=&#34;https://colab.research.google.com/github/lchen001/LLMDrift/blob/main/Intro.ipynb&#34;&gt;Google Colab Notebook&lt;/a&gt; to reproduce the monitored performance drifts in &lt;a href=&#34;https://arxiv.org/pdf/2307.09009.pdf&#34;&gt;our paper&lt;/a&gt;. You don&#39;t need API keys to get started. You can also use the &lt;a href=&#34;https://raw.githubusercontent.com/lchen001/LLMDrift/main/Intro.ipynb&#34;&gt;local intro notebook&lt;/a&gt; directly.&lt;/p&gt; &#xA;&lt;h2&gt;üíæ Datasets and Generations&lt;/h2&gt; &#xA;&lt;p&gt;The datasets and generations can be found under &lt;code&gt;generation/&lt;/code&gt;. Each csv file corresponds to one dataset. One record/row corresponds to one query and the generation from one LLM service.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img align=&#34;center&#34; src=&#34;https://raw.githubusercontent.com/lchen001/LLMDrift/main/asset/generationexample.png&#34; width=&#34;460px&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;b&gt;Figure 2:&lt;/b&gt; The first few rows in the LLM generations on PRIME dataset. &lt;/p&gt; &#xA;&lt;p&gt;The above figure shows the first few rows in the &lt;code&gt;generation/PRIME_EVAL.csv&lt;/code&gt;. It includes the model, query parameters (such as temperature), the query, the reference answer, the generated answer, and latency. Such information could be leverage to study various aspects of LLM services.&lt;/p&gt; &#xA;&lt;h2&gt;üìö Read More&lt;/h2&gt; &#xA;&lt;p&gt;You can find more details in the academic paper:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2307.09009.pdf&#34;&gt;&lt;strong&gt;How Is ChatGPT‚Äôs Behavior Changing over Time?&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üéØ Reference&lt;/h2&gt; &#xA;&lt;p&gt;If you use our findings and/or datasets in a research paper, please cite our work as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{chen2023LLMDrift,&#xA;  title={How Is ChatGPT‚Äôs Behavior Changing over Time?},&#xA;  author={Chen, Lingjiao and Zaharia, Matei and Zou, James},&#xA;  journal={arXiv preprint arXiv:2307.09009},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>Deepandreinforcement/Counting-People-YOLOv8</title>
    <updated>2023-07-22T01:38:05Z</updated>
    <id>tag:github.com,2023-07-22:/Deepandreinforcement/Counting-People-YOLOv8</id>
    <link href="https://github.com/Deepandreinforcement/Counting-People-YOLOv8" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Counting-People-YOLOv8&lt;/h1&gt;</summary>
  </entry>
</feed>