<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-17T01:37:57Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>kw2828/Dolly-2.0-Series</title>
    <updated>2023-04-17T01:37:57Z</updated>
    <id>tag:github.com,2023-04-17:/kw2828/Dolly-2.0-Series</id>
    <link href="https://github.com/kw2828/Dolly-2.0-Series" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Resources for running, fine-tuning, and inferencing Dolly 2.0&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Dolly 2.0 Series&lt;/h1&gt; &#xA;&lt;p&gt;Democratizing resources for running, fine-tuning, and inferencing Dolly 2.0. Changelog below:&lt;/p&gt; &#xA;&lt;p&gt;04/14/23 Update:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1n5U13L0Bzhs32QO_bls5jwuZR62GPSwE?usp=sharing&#34;&gt;Fine-tuning Dolly with LoRa 2.0 Colab Notebook&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;04/12/23 Update:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=5sNJpRgZh-s&amp;amp;ab_channel=GenerativeAIEntrepreneurs&#34;&gt;Dolly 2.0 Update Video on YouTube&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1A8Prplbjr16hy9eGfWd3-r34FOuccB2c?usp=sharing&#34;&gt;Dolly 2.0 Google Colab (Modified for Colab Free)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm&#34;&gt;DataBricks Blog post&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/databrickslabs/dolly/tree/master/data&#34;&gt;&lt;code&gt;databricks-dolly-15k&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;WIP - Each with a seperate Colab Notebook&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Colab Notebook to Run and Inference Dolly 2.0&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Colab Notebook to Fine-Tune Dolly 2.0 on Alpaca&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Colab Notebook on Fine-Tuning on LAION&#39;s Dataset&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Fine-tuning Dolly for Specific Use Cases&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Evaluation Metrics of LLMs&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Reinforcement Learning with Human Feedback (RLHF)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Generative Agents&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Deployment Challenges of LLMs&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>krishnaik06/FSDSRegression</title>
    <updated>2023-04-17T01:37:57Z</updated>
    <id>tag:github.com,2023-04-17:/krishnaik06/FSDSRegression</id>
    <link href="https://github.com/krishnaik06/FSDSRegression" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;End To End ML Project&lt;/h2&gt; &#xA;&lt;h3&gt;created a environment&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -p venv python==3.8&#xA;&#xA;conda activate venv/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Install all necessary libraries&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>dvlab-research/3D-Box-Segment-Anything</title>
    <updated>2023-04-17T01:37:57Z</updated>
    <id>tag:github.com,2023-04-17:/dvlab-research/3D-Box-Segment-Anything</id>
    <link href="https://github.com/dvlab-research/3D-Box-Segment-Anything" rel="alternate"></link>
    <summary type="html">&lt;p&gt;We extend Segment Anything to 3D perception by combining it with VoxelNeXt.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;3D-Box via Segment Anything&lt;/h1&gt; &#xA;&lt;p&gt;We extend &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;Segment Anything&lt;/a&gt; to 3D perception by combining it with &lt;a href=&#34;https://github.com/dvlab-research/VoxelNeXt&#34;&gt;VoxelNeXt&lt;/a&gt;. Note that this project is still in progress. We are improving it and dveloping more examples. Any issue or pull request is welcome!&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/dvlab-research/3D-Box-Segment-Anything/main/images/sam-voxelnext.png&#34; width=&#34;100%&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Why this project?&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;Segment Anything&lt;/a&gt; and its following projects focus on 2D images. In this project, we extend the scope to 3D world by combining &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;Segment Anything&lt;/a&gt; and &lt;a href=&#34;https://github.com/dvlab-research/VoxelNeXt&#34;&gt;VoxelNeXt&lt;/a&gt;. When we provide a prompt (e.g., a point / box), the result is not only 2D segmentation mask, but also 3D boxes.&lt;/p&gt; &#xA;&lt;p&gt;The core idea is that &lt;a href=&#34;https://github.com/dvlab-research/VoxelNeXt&#34;&gt;VoxelNeXt&lt;/a&gt; is a fully sparse 3D detector. It predicts 3D object upon each sparse voxel. We project 3D sparse voxels onto 2D images. And then 3D boxes can be generated for voxels in the SAM mask.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This project makes 3D object detection to be promptable.&lt;/li&gt; &#xA; &lt;li&gt;VoxelNeXt is based on sparse voxels that are easy to be related to the mask generated from segment anything.&lt;/li&gt; &#xA; &lt;li&gt;This project could facilitate 3D box labeling. 3D box can be obtained via a simple click on image. It might largely save human efforts, especially on autonuous driving scenes.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Basic requirements &lt;code&gt;pip install -r requirements.txt &lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Segment anything &lt;code&gt;pip install git+https://github.com/facebookresearch/segment-anything.git &lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;spconv &lt;code&gt;pip install spconv &lt;/code&gt; or cuda version spconv &lt;code&gt;pip install spconv-cu111&lt;/code&gt; based on your cuda version.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Please try it via &lt;a href=&#34;https://raw.githubusercontent.com/dvlab-research/3D-Box-Segment-Anything/main/seg_anything_and_3D.ipynb&#34;&gt;seg_anything_and_3D.ipynb&lt;/a&gt;. We provide this example on nuScenes dataset. You can use other image-points pairs. The point to image translation infos on nuScenes val can be download &lt;a href=&#34;https://drive.google.com/file/d/1nJqdfs0gMTIo4fjOwytSbM0fdBOJ4IGb/view?usp=share_link&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/dvlab-research/3D-Box-Segment-Anything/main/images/mask_box.png&#34; width=&#34;100%&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/dvlab-research/3D-Box-Segment-Anything/main/images/image_boxes1.png&#34; width=&#34;100%&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/dvlab-research/3D-Box-Segment-Anything/main/images/image_boxes2.png&#34; width=&#34;100%&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/dvlab-research/3D-Box-Segment-Anything/main/images/image_boxes3.png&#34; width=&#34;100%&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;TODO List&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;[ ] Zero-shot version VoxelNeXt.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;[ ] Examples on more datasets.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;[ ] Indoor scenes.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this project useful in your research, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{kirillov2023segany,&#xA;  title={Segment Anything}, &#xA;  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Doll{\&#39;a}r, Piotr and Girshick, Ross},&#xA;  journal={arXiv:2304.02643},&#xA;  year={2023}&#xA;}&#xA;&#xA;@inproceedings{chen2023voxenext,&#xA;  title={VoxelNeXt: Fully Sparse VoxelNet for 3D Object Detection and Tracking},&#xA;  author={Yukang Chen and Jianhui Liu and Xiangyu Zhang and Xiaojuan Qi and Jiaya Jia},&#xA;  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},&#xA;  year={2023}&#xA;}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;Segment Anything&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dvlab-research/VoxelNeXt&#34;&gt;VoxelNeXt&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dvlab-research/UVTR&#34;&gt;UVTR&lt;/a&gt; for 3D to 2D translation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Our Works in 3D Perception&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;VoxelNeXt (CVPR 2023)&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2303.11301&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/dvlab-research/VoxelNeXt&#34;&gt;[Code]&lt;/a&gt; Fully Sparse VoxelNet for 3D Object Detection and Tracking.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Focal Sparse Conv (CVPR 2022 Oral)&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2204.12463&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/dvlab-research/FocalsConv&#34;&gt;[Code]&lt;/a&gt; Dynamic sparse convolution for high performance.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Spatial Pruned Conv (NeurIPS 2022)&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2209.14201&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/CVMI-Lab/SPS-Conv&#34;&gt;[Code]&lt;/a&gt; 50% FLOPs saving for efficient 3D object detection.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;LargeKernel3D (CVPR 2023)&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2206.10555&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/dvlab-research/LargeKernel3D&#34;&gt;[Code]&lt;/a&gt; Large-kernel 3D sparse CNN backbone.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;SphereFormer (CVPR 2023)&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2303.12766&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/dvlab-research/SphereFormer&#34;&gt;[Code]&lt;/a&gt; Spherical window 3D transformer backbone.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dvlab-research/spconv-plus&#34;&gt;spconv-plus&lt;/a&gt; A library where we combine our works into &lt;a href=&#34;https://github.com/traveller59/spconv&#34;&gt;spconv&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dvlab-research/SparseTransformer&#34;&gt;SparseTransformer&lt;/a&gt; A library that includes high-efficiency transformer implementations for sparse point cloud or voxel data.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>