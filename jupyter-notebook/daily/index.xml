<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-09-13T01:36:17Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>karpathy/nn-zero-to-hero</title>
    <updated>2022-09-13T01:36:17Z</updated>
    <id>tag:github.com,2022-09-13:/karpathy/nn-zero-to-hero</id>
    <link href="https://github.com/karpathy/nn-zero-to-hero" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Neural Networks: Zero to Hero&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Neural Networks: Zero to Hero&lt;/h2&gt; &#xA;&lt;p&gt;A course on neural networks that starts all the way at the basics. The course is a series of YouTube videos where we code and train neural networks together. The Jupyter notebooks we build in the videos are then captured here inside the &lt;a href=&#34;https://raw.githubusercontent.com/karpathy/nn-zero-to-hero/master/lectures/&#34;&gt;lectures&lt;/a&gt; directory. Every lecture also has a set of exercises included in the video description. (This may grow into something more respectable).&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Lecture 1: The spelled-out intro to neural networks and backpropagation: building micrograd&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Backpropagation and training of neural networks. Assumes basic knowledge of Python and a vague recollection of calculus from high school.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=VMj-3S1tku0&#34;&gt;YouTube video lecture&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/karpathy/nn-zero-to-hero/master/lectures/micrograd&#34;&gt;Jupyter notebook files&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/karpathy/micrograd&#34;&gt;micrograd Github repo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Lecture 2: The spelled-out intro to language modeling: building makemore&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We implement a bigram character-level language model, which we will further complexify in followup videos into a modern Transformer language model, like GPT. In this video, the focus is on (1) introducing torch.Tensor and its subtleties and use in efficiently evaluating neural networks and (2) the overall framework of language modeling that includes model training, sampling, and the evaluation of a loss (e.g. the negative log likelihood for classification).&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=PaCmpygFfXo&#34;&gt;YouTube video lecture&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/karpathy/nn-zero-to-hero/master/lectures/makemore/makemore_part1_bigrams.ipynb&#34;&gt;Jupyter notebook files&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/karpathy/makemore&#34;&gt;makemore Github repo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Lecture 3: The spelled-out intro to language modeling: building makemore. PART 2: MLP&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We implement a multilayer perceptron (MLP) character-level language model. In this video we also introduce many basics of machine learning (e.g. model training, learning rate tuning, hyperparameters, evaluation, train/dev/test splits, under/overfitting, etc.).&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtu.be/TCH_1BHY58I&#34;&gt;YouTube video lecture&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/karpathy/nn-zero-to-hero/master/lectures/makemore/makemore_part2_mlp.ipynb&#34;&gt;Jupyter notebook files&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/karpathy/makemore&#34;&gt;makemore Github repo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;ongoing...&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;License&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;MIT&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>bloc97/CrossAttentionControl</title>
    <updated>2022-09-13T01:36:17Z</updated>
    <id>tag:github.com,2022-09-13:/bloc97/CrossAttentionControl</id>
    <link href="https://github.com/bloc97/CrossAttentionControl" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Unofficial implementation of &#34;Prompt-to-Prompt Image Editing with Cross Attention Control&#34; with Stable Diffusion&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Cross Attention Control with Stable Diffusion&lt;/h1&gt; &#xA;&lt;p&gt;Unofficial implementation of &#34;Prompt-to-Prompt Image Editing with Cross Attention Control&#34; with Stable Diffusion, some modifications were made to the methods described in the paper in order to make them work with Stable Diffusion.&lt;/p&gt; &#xA;&lt;p&gt;Paper: &lt;a href=&#34;https://arxiv.org/abs/2208.01626&#34;&gt;https://arxiv.org/abs/2208.01626&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;What is Cross Attention Control?&lt;/h2&gt; &#xA;&lt;p&gt;Large-scale language-image models (eg. Stable Diffusion) are usually hard to control just with editing the prompts alone and can be very unpredictable and unintuitive for users. Most existing methods require the user to input a mask which is cumbersome and might not yield good results if the mask has an inadequate shape. Cross Attention Control allows much finer control of the prompt by modifying the internal attention maps of the diffusion model during inference without the need for the user to input a mask and does so with minimal performance penalities (compared to clip guidance) and no additional training or fine-tuning of the diffusion model.&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;This notebook uses the following libraries: &lt;code&gt;torch transformers diffusers numpy PIL tqdm difflib&lt;/code&gt;&lt;br&gt; Simply install the required libraries using &lt;code&gt;pip&lt;/code&gt; and run the jupyter notebook, some examples are given inside.&lt;br&gt; A description of the parameters are given at the end of the readme.&lt;/p&gt; &#xA;&lt;h1&gt;Results/Demonstrations&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;All images shown below are generated using the same seed. The initial and target images must be generated with the same seed for cross attention control to work.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Target replacement&lt;/h2&gt; &#xA;&lt;p&gt;Top left prompt: &lt;code&gt;[a cat] sitting on a car&lt;/code&gt;&lt;br&gt; Clockwise: &lt;code&gt;a smiling dog...&lt;/code&gt;, &lt;code&gt;a hamster...&lt;/code&gt;, &lt;code&gt;a tiger...&lt;/code&gt;&lt;br&gt; Note: different strength values for &lt;code&gt;prompt_edit_spatial_start&lt;/code&gt; were used, clockwise: &lt;code&gt;0.7&lt;/code&gt;, &lt;code&gt;0.5&lt;/code&gt;, &lt;code&gt;1.0&lt;/code&gt; &lt;img src=&#34;https://github.com/bloc97/CrossAttentionControl/raw/main/images/fouranimals.png?raw=true&#34; alt=&#34;Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Style injection&lt;/h2&gt; &#xA;&lt;p&gt;Top left prompt: &lt;code&gt;a fantasy landscape with a maple forest&lt;/code&gt;&lt;br&gt; Clockwise: &lt;code&gt;a watercolor painting of...&lt;/code&gt;, &lt;code&gt;a van gogh painting of...&lt;/code&gt;, &lt;code&gt;a charcoal pencil sketch of...&lt;/code&gt;&lt;br&gt; &lt;img src=&#34;https://github.com/bloc97/CrossAttentionControl/raw/main/images/fourstyles.png?raw=true&#34; alt=&#34;Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Global editing&lt;/h2&gt; &#xA;&lt;p&gt;Top left prompt: &lt;code&gt;a fantasy landscape with a pine forest&lt;/code&gt;&lt;br&gt; Clockwise: &lt;code&gt;..., autumn&lt;/code&gt;, &lt;code&gt;..., winter&lt;/code&gt;, &lt;code&gt;..., spring, green&lt;/code&gt;&lt;br&gt; &lt;img src=&#34;https://github.com/bloc97/CrossAttentionControl/raw/main/images/fourseasons.png?raw=true&#34; alt=&#34;Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Reducing unpredictability when modifying prompts&lt;/h2&gt; &#xA;&lt;p&gt;Left image prompt: &lt;code&gt;a fantasy landscape with a pine forest&lt;/code&gt;&lt;br&gt; Right image prompt: &lt;code&gt;a winter fantasy landscape with a pine forest&lt;/code&gt;&lt;br&gt; Middle image: Cross attention enabled prompt editing (left image -&amp;gt; right image)&lt;br&gt; &lt;img src=&#34;https://github.com/bloc97/CrossAttentionControl/raw/main/images/a%20fantasy%20landscape%20with%20a%20pine%20forest%20-%20a%20winter%20fantasy%20landscape%20with%20a%20pine%20forest.png?raw=true&#34; alt=&#34;Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Left image prompt: &lt;code&gt;a fantasy landscape with a pine forest&lt;/code&gt;&lt;br&gt; Right image prompt: &lt;code&gt;a watercolor painting of a landscape with a pine forest&lt;/code&gt;&lt;br&gt; Middle image: Cross attention enabled prompt editing (left image -&amp;gt; right image)&lt;br&gt; &lt;img src=&#34;https://github.com/bloc97/CrossAttentionControl/raw/main/images/a%20fantasy%20landscape%20with%20a%20pine%20forest%20-%20a%20watercolor%20painting%20of%20a%20landscape%20with%20a%20pine%20forest.png?raw=true&#34; alt=&#34;Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Left image prompt: &lt;code&gt;a fantasy landscape with a pine forest&lt;/code&gt;&lt;br&gt; Right image prompt: &lt;code&gt;a fantasy landscape with a pine forest and a river&lt;/code&gt;&lt;br&gt; Middle image: Cross attention enabled prompt editing (left image -&amp;gt; right image)&lt;br&gt; &lt;img src=&#34;https://github.com/bloc97/CrossAttentionControl/raw/main/images/a%20fantasy%20landscape%20with%20a%20pine%20forest%20-%20A%20fantasy%20landscape%20with%20a%20pine%20forest%20and%20a%20river.png?raw=true&#34; alt=&#34;Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Direct token attention control&lt;/h2&gt; &#xA;&lt;p&gt;Left image prompt: &lt;code&gt;a fantasy landscape with a pine forest&lt;/code&gt;&lt;br&gt; Towards the right: &lt;code&gt;-fantasy&lt;/code&gt; &lt;img src=&#34;https://github.com/bloc97/CrossAttentionControl/raw/main/images/a%20fantasy%20landscape%20with%20a%20pine%20forest%20-%20decrease%20fantasy.png?raw=true&#34; alt=&#34;Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Left image prompt: &lt;code&gt;a fantasy landscape with a pine forest&lt;/code&gt;&lt;br&gt; Towards the right: &lt;code&gt;+fantasy&lt;/code&gt; and &lt;code&gt;+forest&lt;/code&gt; &lt;img src=&#34;https://github.com/bloc97/CrossAttentionControl/raw/main/images/a%20fantasy%20landscape%20with%20a%20pine%20forest%20-%20increase%20fantasy%20and%20forest.png?raw=true&#34; alt=&#34;Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Left image prompt: &lt;code&gt;a fantasy landscape with a pine forest&lt;/code&gt;&lt;br&gt; Towards the right: &lt;code&gt;-fog&lt;/code&gt; &lt;img src=&#34;https://github.com/bloc97/CrossAttentionControl/raw/main/images/a%20fantasy%20landscape%20with%20a%20pine%20forest%20-%20decrease%20fog.png?raw=true&#34; alt=&#34;Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Left image: from previous example&lt;br&gt; Towards the right: &lt;code&gt;-rocks&lt;/code&gt; &lt;img src=&#34;https://github.com/bloc97/CrossAttentionControl/raw/main/images/a%20fantasy%20landscape%20with%20a%20pine%20forest%20-%20decrease%20rocks.png?raw=true&#34; alt=&#34;Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Comparison to standard prompt editing&lt;/h2&gt; &#xA;&lt;p&gt;Let&#39;s compare our results above where we removed fog and rocks from our fantasy landscape using cross attention maps against what people usually do, by editing the prompt alone.&lt;br&gt; We can first try adding &#34;without fog and without rocks&#34; to our prompt.&lt;/p&gt; &#xA;&lt;p&gt;Image prompt: &lt;code&gt;A fantasy landscape with a pine forest without fog and without rocks&lt;/code&gt;&lt;br&gt; However, we still see fog and rocks.&lt;br&gt; &lt;img src=&#34;https://github.com/bloc97/CrossAttentionControl/raw/main/images/A%20fantasy%20landscape%20with%20a%20pine%20forest%20without%20fog%20and%20without%20rocks.png?raw=true&#34; alt=&#34;Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;We can try adding words like dry, sunny and grass.&lt;br&gt; Image prompt: &lt;code&gt;A fantasy landscape with a pine forest without fog and rocks, dry sunny day, grass&lt;/code&gt;&lt;br&gt; There are less rocks and fog, but the image&#39;s composition and style is completely different from before and we still haven&#39;t obtained our desired fog and rock-free image...&lt;br&gt; &lt;img src=&#34;https://github.com/bloc97/CrossAttentionControl/raw/main/images/A%20fantasy%20landscape%20with%20a%20pine%20forest%20without%20fog%20and%20rocks%2C%20dry%20sunny%20day%2C%20grass.png?raw=true&#34; alt=&#34;Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Two functions are included, &lt;code&gt;stablediffusion(...)&lt;/code&gt; which generates images and &lt;code&gt;prompt_token(...)&lt;/code&gt; that is used to help the user find the token index for words in the prompt, which is used to tweak token weights in &lt;code&gt;prompt_edit_token_weights&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Parameters of &lt;code&gt;stabledifusion(...)&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name = Default Value&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Example&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;prompt=&#34;&#34;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;the prompt as a string&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;&#34;a cat riding a bicycle&#34;&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;prompt_edit=None&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;the second prompt as a string, used to edit the first prompt using cross attention, set &lt;code&gt;None&lt;/code&gt; to disable&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;&#34;a dog riding a bicycle&#34;&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;prompt_edit_token_weights=[]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;values to scale the importance of the tokens in cross attention layers, as a list of tuples representing &lt;code&gt;(token id, strength)&lt;/code&gt;, this is used to increase or decrease the importance of a word in the prompt, it is applied to &lt;code&gt;prompt_edit&lt;/code&gt; when possible (if &lt;code&gt;prompt_edit&lt;/code&gt; is &lt;code&gt;None&lt;/code&gt;, weights are applied to &lt;code&gt;prompt&lt;/code&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;[(2, 2.5), (6, -5.0)]&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;prompt_edit_tokens_start=0.0&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;how strict is the generation with respect to the initial prompt, increasing this will let the network be more creative for smaller details/textures, should be smaller than &lt;code&gt;prompt_edit_tokens_end&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;0.0&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;prompt_edit_tokens_end=1.0&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;how strict is the generation with respect to the initial prompt, decreasing this will let the network be more creative for larger features/general scene composition, should be bigger than &lt;code&gt;prompt_edit_tokens_start&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;1.0&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;prompt_edit_spatial_start=0.0&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;how strict is the generation with respect to the initial image &lt;em&gt;(generated from the first prompt, not from img2img)&lt;/em&gt;, increasing this will let the network be more creative for smaller details/textures, should be smaller than &lt;code&gt;prompt_edit_spatial_end&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;0.0&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;prompt_edit_spatial_end=1.0&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;how strict is the generation with respect to the initial image &lt;em&gt;(generated from the first prompt, not from img2img)&lt;/em&gt;, decreasing this will let the network be more creative for larger features/general scene composition, should be bigger than &lt;code&gt;prompt_edit_spatial_start&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;1.0&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;guidance_scale=7.5&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;standard classifier-free guidance strength for stable diffusion&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;7.5&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;steps=50&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;number of diffusion steps as an integer, higher usually produces better images but is slower&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;50&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;seed=None&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;random seed as an integer, set &lt;code&gt;None&lt;/code&gt; to use a random seed&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;126794873&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;width=512&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;image width&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;512&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;height=512&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;image height&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;512&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;init_image=None&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;init image for image to image generation, as a PIL image, it will be resized to &lt;code&gt;width x height&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;PIL.Image()&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;init_image_strength=0.5&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;strength of the noise added for image to image generation, higher will make the generation care less about the initial image&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;0.5&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt;</summary>
  </entry>
  <entry>
    <title>vijishmadhavan/Chehara-GAN</title>
    <updated>2022-09-13T01:36:17Z</updated>
    <id>tag:github.com,2022-09-13:/vijishmadhavan/Chehara-GAN</id>
    <link href="https://github.com/vijishmadhavan/Chehara-GAN" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Simple à¤šà¥‡à¤¹à¤°à¤¾(face) restoration&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;à¤šà¥‡à¤¹à¤°à¤¾-GAN&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Beta model, might have issues.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Demo v0.2:&lt;/strong&gt; &lt;a href=&#34;https://colab.research.google.com/github/vijishmadhavan/Chehara-GAN/blob/master/%E0%A4%9A%E0%A5%87%E0%A4%B9%E0%A4%B0%E0%A4%BE_GAN.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; align=&#34;center&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Demo v0.1:&lt;/strong&gt; &lt;a href=&#34;https://colab.research.google.com/github/vijishmadhavan/Chehara-GAN/blob/master/%E0%A4%9A%E0%A5%87%E0%A4%B9%E0%A4%B0%E0%A4%BE_GAN_v1.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; align=&#34;center&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://visitor-badge.glitch.me/badge?page_id=vijishmadhavan.Chehara-GAN&#34; alt=&#34;visitors&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you like what I&#39;m doing you can:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Follow me on &lt;a href=&#34;https://twitter.com/Vijish68859437&#34;&gt;twitter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Check my other projects &lt;a href=&#34;https://github.com/vijishmadhavan&#34;&gt;GitHub&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;You can sponsor me to support my open source work ðŸ’– &lt;a href=&#34;https://github.com/sponsors/vijishmadhavan?o=sd&amp;amp;sc=t&#34;&gt;sponsor&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Changelog&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;2022-09-010 Tested it on 64Ã—64 -&amp;gt; 512Ã—512 on FFHQ-CelebaHQ - &lt;a href=&#34;https://raw.githubusercontent.com/vijishmadhavan/Chehara-GAN/master/#Example-Images&#34;&gt;Example Images&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2022-08-026 Use this &lt;a href=&#34;https://www.dropbox.com/s/1818xtxblsyrv45/c2.pth?dl=1&#34;&gt;weights&lt;/a&gt; to train it further.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2022-08-026 &lt;strong&gt;Colab Demo v0.2:&lt;/strong&gt; &lt;a href=&#34;https://colab.research.google.com/github/vijishmadhavan/Chehara-GAN/blob/master/%E0%A4%9A%E0%A5%87%E0%A4%B9%E0%A4%B0%E0%A4%BE_GAN.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; align=&#34;center&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2022-08-026 &lt;a href=&#34;https://www.dropbox.com/s/iiqvfu58as8unz1/p6500.pkl?dl=1&#34;&gt;beta-v0.2&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2022-08-023 &lt;a href=&#34;https://www.dropbox.com/s/d43p26ikzlxuyix/p4500.pkl?dl=1&#34;&gt;beta-v0.1&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2022-08-022 Colab demo v0.1 &lt;a href=&#34;https://colab.research.google.com/github/vijishmadhavan/Chehara-GAN/blob/master/%E0%A4%9A%E0%A5%87%E0%A4%B9%E0%A4%B0%E0%A4%BE_GAN.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; align=&#34;center&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2022-08-022 Fast.ai model training &lt;a href=&#34;https://github.com/aarcosg/fastai-course-v3-notes/raw/master/refactored_by_topics/CNN_L7_gan_feature-loss.md&#34;&gt;url&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Example Images&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;64Ã—64 -&amp;gt; 512Ã—512&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/vijishmadhavan/Chehara-GAN/raw/master/compare/0_41_inf-side.png&#34; alt=&#34;Superstar&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;64Ã—64 -&amp;gt; 512Ã—512&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/vijishmadhavan/Chehara-GAN/raw/master/compare/0_21_inf-side.png&#34; alt=&#34;Superstar&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Image samples&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/vijishmadhavan/Chehara-GAN/raw/master/compare/sidee.png&#34; alt=&#34;Superstar&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/vijishmadhavan/Chehara-GAN/raw/master/compare/1_7-side.jpg&#34; alt=&#34;Superstar&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/vijishmadhavan/Chehara-GAN/raw/master/compare/Adele_crop-side.png&#34; alt=&#34;Superstar&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/vijishmadhavan/Chehara-GAN/raw/master/compare/90050137-4da1-44cc-b64b-0b9efc813148-side.jpg&#34; alt=&#34;Superstar&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/vijishmadhavan/Chehara-GAN/raw/master/compare/ami-side.jpg&#34; alt=&#34;Superstar&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/vijishmadhavan/Chehara-GAN/raw/master/compare/0941881e-1b87-46d3-9b4e-10e9b8b4137b-side.jpg&#34; alt=&#34;Superstar&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Dataset&lt;/h3&gt; &#xA;&lt;p&gt;Dataset was generated using the below models.&lt;/p&gt; &#xA;&lt;p&gt;-&lt;a href=&#34;https://github.com/yangxy/GPEN&#34;&gt;GPEN&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;-&lt;a href=&#34;https://github.com/TencentARC/GFPGAN&#34;&gt;GFPGAN&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;-&lt;a href=&#34;https://github.com/NVlabs/ffhq-dataset&#34;&gt;FFHQ&lt;/a&gt; can be used, but generating a dataset would help in dealing with real image problems.&lt;/p&gt; &#xA;&lt;p&gt;-Generated dataset was cropped to facial features and then trained.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/vijishmadhavan/Chehara-GAN/raw/master/compare/facial%20feature-side.jpg&#34; alt=&#34;Superstar&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Model&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.dropbox.com/s/d43p26ikzlxuyix/p4500.pkl?dl=1&#34;&gt;beta-v0.1&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.dropbox.com/s/iiqvfu58as8unz1/p6500.pkl?dl=1&#34;&gt;beta-v0.2&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Use this &lt;a href=&#34;https://www.dropbox.com/s/1818xtxblsyrv45/c2.pth?dl=1&#34;&gt;weights&lt;/a&gt; to train it further.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;works well with color photos.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Implementation Details&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Model architecture: &lt;a href=&#34;https://fastai1.fast.ai/vision.models.unet.html&#34;&gt;fastai v1 u-net&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Paired image2image training: &lt;a href=&#34;https://github.com/aarcosg/fastai-course-v3-notes/raw/master/refactored_by_topics/CNN_L7_gan_feature-loss.md&#34;&gt;fastai v1 superres notebook&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Limitation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;I call this a beta as it needs lot of improvement, But with minimum effort the results are good.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>