<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-08-02T01:45:56Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>xianhu/LearnPython</title>
    <updated>2022-08-02T01:45:56Z</updated>
    <id>tag:github.com,2022-08-02:/xianhu/LearnPython</id>
    <link href="https://github.com/xianhu/LearnPython" rel="alternate"></link>
    <summary type="html">&lt;p&gt;以撸代码的形式学习Python&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LearnPython&lt;/h1&gt; &#xA;&lt;p&gt;以撸代码的形式学习Python, 具体说明在&lt;a href=&#34;https://zhuanlan.zhihu.com/pythoner&#34;&gt;知乎专栏-撸代码,学知识&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;===================================================================================================&lt;/p&gt; &#xA;&lt;h3&gt;python_base.py: 千行代码入门Python&lt;/h3&gt; &#xA;&lt;h3&gt;python_visual.py: 15张图入门Matplotlib&lt;/h3&gt; &#xA;&lt;h3&gt;python_visual_animation.py: 使用Matplotlib画动态图实例&lt;/h3&gt; &#xA;&lt;h3&gt;python_spider.py: 一个很“水”的Python爬虫入门代码文件&lt;/h3&gt; &#xA;&lt;h3&gt;python_weibo.py: “史上最详细”的Python模拟登录新浪微博流程&lt;/h3&gt; &#xA;&lt;h3&gt;python_lda.py: 玩点高级的--带你入门Topic模型LDA（小改进+附源码）&lt;/h3&gt; &#xA;&lt;h3&gt;python_sqlalchemy.py: 作为一个Pythoner, 不会SQLAlchemy都不好意思跟同行打招呼！&lt;/h3&gt; &#xA;&lt;h3&gt;python_oneline.py: 几个小例子告诉你, 一行Python代码能干哪些事&lt;/h3&gt; &#xA;&lt;h3&gt;python_requests.py: Python中最好用的爬虫库Requests代码实例&lt;/h3&gt; &#xA;&lt;h3&gt;python_functional.py: Python进阶: 函数式编程实例（附代码）&lt;/h3&gt; &#xA;&lt;h3&gt;python_decorator.py: Python进阶: 通过实例详解装饰器（附代码）&lt;/h3&gt; &#xA;&lt;h3&gt;python_datetime.py: 你真的了解Python中的日期时间处理吗？&lt;/h3&gt; &#xA;&lt;h3&gt;python_metaclass.py: Python进阶: 一步步理解Python中的元类metaclass&lt;/h3&gt; &#xA;&lt;h3&gt;python_coroutine.py: Python进阶: 理解Python中的异步IO和协程(Coroutine), 并应用在爬虫中&lt;/h3&gt; &#xA;&lt;h3&gt;python_aiohttp.py: Python中最好用的异步爬虫库Aiohttp代码实例&lt;/h3&gt; &#xA;&lt;h3&gt;python_thread_multiprocess.py: Python进阶: 聊聊IO密集型任务、计算密集型任务，以及多线程、多进程&lt;/h3&gt; &#xA;&lt;h3&gt;python_version36.py: Python3.6正式版要来了, 你期待哪些新特性？&lt;/h3&gt; &#xA;&lt;h3&gt;python_magic_methods: Python进阶: 实例讲解Python中的魔法函数(Magic Methods)&lt;/h3&gt; &#xA;&lt;h3&gt;python_restful_api.py: 利用Python和Flask快速开发RESTful API&lt;/h3&gt; &#xA;&lt;h3&gt;python_restful_api.py: RESTful API进阶: 连接数据库、添加参数、Token认证、返回代码说明等&lt;/h3&gt; &#xA;&lt;h3&gt;python_context.py: With语句和上下文管理器ContextManager&lt;/h3&gt; &#xA;&lt;h3&gt;python_flask.py: Flask相关说明&lt;/h3&gt; &#xA;&lt;h3&gt;MyShow: 玩点好玩的--知乎全部话题关系可视化&lt;/h3&gt; &#xA;&lt;h3&gt;python_markov_chain.py: 玩点好玩的--使用马尔可夫模型自动生成文章&lt;/h3&gt; &#xA;&lt;h3&gt;python_wechat.py: 玩点好玩的--自己写一个微信小助手&lt;/h3&gt; &#xA;&lt;h3&gt;python_csv.py: Python中CSV文件的简单读写&lt;/h3&gt; &#xA;&lt;h3&gt;python_numpy.py: 使用numpy进行矩阵操作&lt;/h3&gt; &#xA;&lt;h3&gt;python_mail.py: 使用Python自动发送邮件，包括发送HTML以及图片、附件等&lt;/h3&gt; &#xA;&lt;h3&gt;python_redis.py: Python操作Redis实现消息的发布与订阅&lt;/h3&gt; &#xA;&lt;h3&gt;python_schedule.py: Python进行调度开发&lt;/h3&gt; &#xA;&lt;h3&gt;python_socket.py: Python的socket开发实例&lt;/h3&gt; &#xA;&lt;h3&gt;Plotly目录: 一些plotly画图的实例，使用jupyter notebook编写&lt;/h3&gt; &#xA;&lt;p&gt;===================================================================================================&lt;/p&gt; &#xA;&lt;h3&gt;您可以fork该项目, 并在修改后提交Pull request, 看到后会尽量进行代码合并&lt;/h3&gt;</summary>
  </entry>
  <entry>
    <title>algorithmica-org/algorithmica</title>
    <updated>2022-08-02T01:45:56Z</updated>
    <id>tag:github.com,2022-08-02:/algorithmica-org/algorithmica</id>
    <link href="https://github.com/algorithmica-org/algorithmica" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A computer science textbook&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Algorithmica v3&lt;/h1&gt; &#xA;&lt;p&gt;Algorithmica is an open-access web book dedicated to the art and science of computing.&lt;/p&gt; &#xA;&lt;p&gt;You can contribute via &lt;a href=&#34;https://prose.io/&#34;&gt;Prose&lt;/a&gt; by clicking on the pencil icon on the top right on any page or by editing its source directly on GitHub. We use a slightly different Markdown dialect, so if you are not sure that the change is correct (for example, editing an intricate LaTeX formula), you can install &lt;a href=&#34;https://gohugo.io/&#34;&gt;Hugo&lt;/a&gt; and build the site locally — or just create a pull request, and a preview link will be automatically generated for you.&lt;/p&gt; &#xA;&lt;p&gt;If you happen to speak Russian, please also read the &lt;a href=&#34;https://ru.algorithmica.org/contributing/&#34;&gt;contributing guidelines&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Key technical changes from the &lt;a href=&#34;https://github.com/algorithmica-org/articles&#34;&gt;previous version&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;pandoc -&amp;gt; Hugo&lt;/li&gt; &#xA; &lt;li&gt;CSS -&amp;gt; Sass&lt;/li&gt; &#xA; &lt;li&gt;Github Pages -&amp;gt; Netlify&lt;/li&gt; &#xA; &lt;li&gt;Yandex.Metrica -&amp;gt; &lt;del&gt;Google Analytics&lt;/del&gt; went back to Metrica&lt;/li&gt; &#xA; &lt;li&gt;algorithmica.org/{lang}/* -&amp;gt; {lang}.algorithmica.org/*&lt;/li&gt; &#xA; &lt;li&gt;Rich metadata support (language, sections, TOCs, authors...)&lt;/li&gt; &#xA; &lt;li&gt;Automated global table of contents&lt;/li&gt; &#xA; &lt;li&gt;Theming support&lt;/li&gt; &#xA; &lt;li&gt;Search support (Lunr)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Short-term todo list:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Style adjustments for mobile and print versions&lt;/li&gt; &#xA; &lt;li&gt;A pdf version of the whole website&lt;/li&gt; &#xA; &lt;li&gt;Meta-information support (for Google Scholar and social media)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://css-tricks.com/table-of-contents-with-intersectionobserver/&#34;&gt;Sticky table of contents&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>xuebinqin/DIS</title>
    <updated>2022-08-02T01:45:56Z</updated>
    <id>tag:github.com,2022-08-02:/xuebinqin/DIS</id>
    <link href="https://github.com/xuebinqin/DIS" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This is the repo for our new project Highly Accurate Dichotomous Image Segmentation&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;420&#34; height=&#34;320&#34; src=&#34;https://raw.githubusercontent.com/xuebinqin/DIS/main/figures/dis-logo-official.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xuebinqin/DIS/main/figures/dis5k-v1-sailship.jpeg&#34; alt=&#34;dis5k-v1-sailship&#34;&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://arxiv.org/pdf/2203.03041.pdf&#34;&gt;Highly Accurate Dichotomous Image Segmentation （ECCV 2022）&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://xuebinqin.github.io/&#34;&gt;Xuebin Qin&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.co.uk/citations?user=6yvjpQQAAAAJ&amp;amp;hl=en&#34;&gt;Hang Dai&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.de/citations?user=3lMuodUAAAAJ&amp;amp;hl=en&#34;&gt;Xiaobin Hu&lt;/a&gt;, &lt;a href=&#34;https://dengpingfan.github.io/&#34;&gt;Deng-Ping Fan*&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=z84rLjoAAAAJ&amp;amp;hl=en&#34;&gt;Ling Shao&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=TwMib_QAAAAJ&amp;amp;hl=en&#34;&gt;Luc Van Gool&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;This is the official repo for our newly formulated DIS task:&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://xuebinqin.github.io/dis/index.html&#34;&gt;&lt;strong&gt;Project Page&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/2203.03041.pdf&#34;&gt;&lt;strong&gt;Arxiv&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;PLEASE STAY TUNED FOR OUR DIS V2.0 (Jul. 30th, 2022)&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xuebinqin/DIS/main/figures/peacock.jpg&#34; alt=&#34;disv2-peacock&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Updates !!!&lt;/h1&gt; &#xA;&lt;p&gt;** (2022-Jul.-30)** Thank &lt;a href=&#34;https://github.com/AK391&#34;&gt;&lt;strong&gt;AK391&lt;/strong&gt;&lt;/a&gt; for the implementaiton of a Web Demo: Integrated into &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces 🤗&lt;/a&gt; using &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;. Try out the Web Demo &lt;a href=&#34;https://huggingface.co/spaces/doevent/dis-background-removal&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;. &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;Notes for official DIS group: Currently, the released DIS deep model is the academic version that was trained with DIS V1.0, which includes very few animal, human, cars, etc. So it may not work well on these targets. We will release another version for general use and test. In addition, our DIS V2.0 will cover more categories with extremely well-annotated samples. Please stay tuned. &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;** (2022-Jul.-17)** Our paper, code and dataset are now officially released!!! Please check our project page for more details: &lt;a href=&#34;https://xuebinqin.github.io/dis/index.html&#34;&gt;&lt;strong&gt;Project Page&lt;/strong&gt;&lt;/a&gt;.&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;** (2022-Jul.-5)** Our DIS work is now accepted by ECCV 2022, the code and dataset will be released before July 17th, 2022. Please be aware of our updates.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;1. Our Dichotomous Image Segmentation (DIS) Dataset&lt;/h2&gt; &#xA;&lt;h3&gt;1.1 &lt;a href=&#34;https://xuebinqin.github.io/dis/index.html&#34;&gt;DIS dataset V1.0: DIS5K&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;br&gt; &#xA;&lt;h3&gt;Download： &lt;a href=&#34;https://drive.google.com/file/d/1jOC2zK0GowBvEt03B7dGugCRDVAoeIqq/view?usp=sharing&#34;&gt;Google Drive&lt;/a&gt; or &lt;a href=&#34;https://pan.baidu.com/s/1y6CQJYledfYyEO0C_Gejpw?pwd=rtgw&#34;&gt;Baidu Pan 提取码：rtgw&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xuebinqin/DIS/main/figures/DIS5k-dataset-v1-sailship.png&#34; alt=&#34;dis5k-dataset-v1-sailship&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/xuebinqin/DIS/main/figures/complexities-qual.jpeg&#34; alt=&#34;complexities-qual&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/xuebinqin/DIS/main/figures/categories.jpeg&#34; alt=&#34;categories&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;1.2 &lt;a href=&#34;https://github.com/xuebinqin/DIS&#34;&gt;DIS dataset V2.0&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;Although our DIS5K V1.0 includes samples from more than 200 categories, many categories, such as human, animals, cars and so on, in real world are not included. &lt;a href=&#34;&#34;&gt;So the current version (v1.0) of our dataset may limit the robustness of the trained models.&lt;/a&gt; To build the comprehensive and large-scale highly accurate dichotomous image segmentation dataset, we are building our DIS dataset V2.0. The V2.0 will be released soon. Please stay tuned.&lt;/p&gt; &#xA;&lt;p&gt;Samples from DIS dataset V2.0. &lt;img src=&#34;https://raw.githubusercontent.com/xuebinqin/DIS/main/figures/dis-v2.jpg&#34; alt=&#34;dis-v2&#34;&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;2. APPLICATIONS of Our DIS5K Dataset&lt;/h2&gt; &#xA;&lt;br&gt; &#xA;&lt;h3&gt;3D Modeling&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xuebinqin/DIS/main/figures/3d-modeling.png&#34; alt=&#34;3d-modeling&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Image Editing&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xuebinqin/DIS/main/figures/ship-demo.gif&#34; alt=&#34;ship-demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Art Design Materials&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xuebinqin/DIS/main/figures/bg-removal.gif&#34; alt=&#34;bg-removal&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Still Image Animation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xuebinqin/DIS/main/figures/view-move.gif&#34; alt=&#34;view-move&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;AR&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xuebinqin/DIS/main/figures/motor-demo.gif&#34; alt=&#34;motor-demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;3D Rendering&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xuebinqin/DIS/main/figures/video-3d.gif&#34; alt=&#34;video-3d&#34;&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;3. Architecture of Our IS-Net&lt;/h2&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xuebinqin/DIS/main/figures/is-net.png&#34; alt=&#34;is-net&#34;&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;4. Human Correction Efforts (HCE)&lt;/h2&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xuebinqin/DIS/main/figures/hce-metric.png&#34; alt=&#34;hce-metric&#34;&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;5. Experimental Results&lt;/h2&gt; &#xA;&lt;br&gt; &#xA;&lt;h3&gt;Predicted Maps, &lt;a href=&#34;https://drive.google.com/file/d/1FMtDLFrL6xVc41eKlLnuZWMBAErnKv0Y/view?usp=sharing&#34;&gt;(Google Drive)&lt;/a&gt;, &lt;a href=&#34;https://pan.baidu.com/s/1WUk2RYYpii2xzrvLna9Fsg?pwd=ph1d&#34;&gt;(Baidu Pan 提取码：ph1d)&lt;/a&gt;, of Our IS-Net and Other SOTAs&lt;/h3&gt; &#xA;&lt;h3&gt;Qualitative Comparisons Against SOTAs&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xuebinqin/DIS/main/figures/qual-comp.jpg&#34; alt=&#34;qual-comp&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Quantitative Comparisons Against SOTAs&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xuebinqin/DIS/main/figures/quan-comp.png&#34; alt=&#34;qual-comp&#34;&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;6. Run Our Code&lt;/h2&gt; &#xA;&lt;br&gt; &#xA;&lt;h3&gt;(1) Clone this repo&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/xuebinqin/DIS.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;(2) Configuring the environment: go to the root &lt;code&gt;DIS&lt;/code&gt; folder and run&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env create -f pytorch18.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or you can check the &lt;code&gt;requirements.txt&lt;/code&gt; to configure the dependancies.&lt;/p&gt; &#xA;&lt;h3&gt;(3) Train:&lt;/h3&gt; &#xA;&lt;p&gt;(a) Open &lt;code&gt;train_valid_inference_main.py&lt;/code&gt;, set the path of your to-be-inferenced &lt;code&gt;train_datasets&lt;/code&gt; and &lt;code&gt;valid_datasets&lt;/code&gt;, e.g., &lt;code&gt;valid_datasets=[dataset_vd]&lt;/code&gt; &lt;br&gt; (b) Set the &lt;code&gt;hypar[&#34;mode&#34;]&lt;/code&gt; to &lt;code&gt;&#34;train&#34;&lt;/code&gt; &lt;br&gt; (c) Create a new folder &lt;code&gt;your_model_weights&lt;/code&gt; in the directory &lt;code&gt;saved_models&lt;/code&gt; and set it as the &lt;code&gt;hypar[&#34;model_path&#34;] =&#34;../saved_models/your_model_weights&#34;&lt;/code&gt; and make sure &lt;code&gt;hypar[&#34;valid_out_dir&#34;]&lt;/code&gt;(line 668) is set to &lt;code&gt;&#34;&#34;&lt;/code&gt;, otherwise the prediction maps of the validation stage will be saved to that directory, which will slow the training speed down &lt;br&gt; (d) Run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python train_valid_inference_main.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;(4) Inference&lt;/h3&gt; &#xA;&lt;p&gt;(a). Download the pre-trained weights (for fair academic comparisons only, the optimized model for engineering or common use will be released soon) &lt;code&gt;isnet.pth&lt;/code&gt; from &lt;a href=&#34;https://drive.google.com/file/d/1KyMpRjewZdyYfxHPYcd-ZbanIXtin0Sn/view?usp=sharing&#34;&gt;(Google Drive)&lt;/a&gt; or &lt;a href=&#34;https://pan.baidu.com/s/1-X2WutiBkWPt-oakuvZ10w?pwd=xbfk&#34;&gt;(Baidu Pan 提取码：xbfk)&lt;/a&gt; and store &lt;code&gt;isnet.pth&lt;/code&gt; in &lt;code&gt;saved_models/IS-Net&lt;/code&gt; &lt;br&gt; (b) Open &lt;code&gt;train_valid_inference_main.py&lt;/code&gt;, set the path of your to-be-inferenced &lt;code&gt;valid_datasets&lt;/code&gt;, e.g., &lt;code&gt;valid_datasets=[dataset_te1, dataset_te2, dataset_te3, dataset_te4]&lt;/code&gt; &lt;br&gt; (c) Set the &lt;code&gt;hypar[&#34;mode&#34;]&lt;/code&gt; to &lt;code&gt;&#34;valid&#34;&lt;/code&gt; &lt;br&gt; (d) Set the output directory of your predicted maps, e.g., &lt;code&gt;hypar[&#34;valid_out_dir&#34;] = &#34;../DIS5K-Results-test&#34;&lt;/code&gt; &lt;br&gt; (e) Run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python train_valid_inference_main.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;(5) Use of our Human Correction Efforts(HCE) metric&lt;/h3&gt; &#xA;&lt;p&gt;Set the ground truth directory &lt;code&gt;gt_root&lt;/code&gt; and the prediction directory &lt;code&gt;pred_root&lt;/code&gt;. To reduce the time costs for computing HCE, the skeletion of the DIS5K dataset can be pre-computed and stored in &lt;code&gt;gt_ske_root&lt;/code&gt;. If &lt;code&gt;gt_ske_root=&#34;&#34;&lt;/code&gt;, the HCE code will compute the skeleton online which usually takes a lot for time for large size ground truth. Then, run &lt;code&gt;python hce_metric_main.py&lt;/code&gt;. Other metrics are evaluated based on the &lt;a href=&#34;https://github.com/mczhuge/SOCToolbox&#34;&gt;SOCToolbox&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;7. Term of Use&lt;/h2&gt; &#xA;&lt;p&gt;Our code and evaluation metric use Apache License 2.0. The Terms of use for our DIS5K dataset is provided as &lt;a href=&#34;https://raw.githubusercontent.com/xuebinqin/DIS/main/DIS5K-Dataset-Terms-of-Use.pdf&#34;&gt;DIS5K-Dataset-Terms-of-Use.pdf&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;We would like to thank Dr. &lt;a href=&#34;https://scholar.google.co.uk/citations?user=T9MTcK0AAAAJ&amp;amp;hl=en&#34;&gt;Ibrahim Almakky&lt;/a&gt; for his helps in implementing the dataloader cache machanism of loading large-size training samples and Jiayi Zhu for his efforts in re-organizing our code and dataset.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;br&gt; &#xA;&lt;pre&gt;&lt;code&gt;@InProceedings{qin2022,&#xA;      author={Xuebin Qin and Hang Dai and Xiaobin Hu and Deng-Ping Fan and Ling Shao and Luc Van Gool},&#xA;      title={Highly Accurate Dichotomous Image Segmentation},&#xA;      booktitle={ECCV},&#xA;      year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Our Previous Works: &lt;a href=&#34;https://github.com/xuebinqin/U-2-Net&#34;&gt;U&lt;sup&gt;2&lt;/sup&gt;-Net&lt;/a&gt;, &lt;a href=&#34;https://github.com/xuebinqin/BASNet&#34;&gt;BASNet&lt;/a&gt;.&lt;/h2&gt; &#xA;&lt;br&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#xA;@InProceedings{Qin_2020_PR,&#xA;      title = {U2-Net: Going Deeper with Nested U-Structure for Salient Object Detection},&#xA;      author = {Qin, Xuebin and Zhang, Zichen and Huang, Chenyang and Dehghan, Masood and Zaiane, Osmar and Jagersand, Martin},&#xA;      journal = {Pattern Recognition},&#xA;      volume = {106},&#xA;      pages = {107404},&#xA;      year = {2020}&#xA;}&#xA;&#xA;@InProceedings{Qin_2019_CVPR,&#xA;        author = {Qin, Xuebin and Zhang, Zichen and Huang, Chenyang and Gao, Chao and Dehghan, Masood and Jagersand, Martin},&#xA;        title = {BASNet: Boundary-Aware Salient Object Detection},&#xA;        booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},&#xA;        month = {June},&#xA;        year = {2019}&#xA;}&#xA;&#xA;@article{qin2021boundary,&#xA;       title={Boundary-aware segmentation network for mobile and web applications},&#xA;       author={Qin, Xuebin and Fan, Deng-Ping and Huang, Chenyang and Diagne, Cyril and Zhang, Zichen and Sant&#39;Anna, Adri{\`a} Cabeza and Suarez, Albert and Jagersand, Martin and Shao, Ling},&#xA;       journal={arXiv preprint arXiv:2101.04704},&#xA;       year={2021}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>