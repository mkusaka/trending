<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-09-05T01:30:43Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>OpenGVLab/SAM-Med2D</title>
    <updated>2023-09-05T01:30:43Z</updated>
    <id>tag:github.com,2023-09-05:/OpenGVLab/SAM-Med2D</id>
    <link href="https://github.com/OpenGVLab/SAM-Med2D" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official implementation of SAM-Med2D&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SAM-Med2D [&lt;a href=&#34;https://arxiv.org/abs/2308.16184&#34;&gt;Paper&lt;/a&gt;]&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://openxlab.org.cn/apps/detail/litianbin/SAM-Med2D&#34;&gt;&lt;img src=&#34;https://cdn-static.openxlab.org.cn/app-center/openxlab_app.svg?sanitize=true&#34; alt=&#34;Open in OpenXLab&#34;&gt;&lt;/a&gt;  &lt;a src=&#34;https://img.shields.io/badge/cs.CV-2308.16184-b31b1b?logo=arxiv&amp;amp;logoColor=red&#34; href=&#34;https://arxiv.org/abs/2308.16184&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/cs.CV-2308.16184-b31b1b?logo=arxiv&amp;amp;logoColor=red&#34;&gt; &lt;/a&gt;&lt;a src=&#34;https://img.shields.io/badge/WeChat-Group-green?logo=wechat&#34; href=&#34;https://github.com/OpenGVLab/SAM-Med2D/raw/main/assets/SAM-Med2D_wechat_group.jpeg&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/WeChat-Group-green?logo=wechat&#34;&gt; &lt;/a&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/OpenGVLab/SAM-Med2D/blob/main/predictor_example.ipynb&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/OpenGVLab/SAM-Med2D&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/OpenGVLab/SAM-Med2D.svg?style=social&amp;amp;label=Star&amp;amp;maxAge=60&#34; alt=&#34;GitHub Stars&#34;&gt;&lt;/a&gt;üî•üî•üî•&lt;/p&gt; &#xA;&lt;!-- ## Description --&gt; &#xA;&lt;h2&gt;üå§Ô∏è Highlights&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üèÜ Collected and curated the largest medical image segmentation dataset (4.6M images and 19.7M masks) to date for training models.&lt;/li&gt; &#xA; &lt;li&gt;üèÜ The most comprehensive fine-tuning based on Segment Anything Model (SAM).&lt;/li&gt; &#xA; &lt;li&gt;üèÜ Comprehensive evaluation of SAM-Med2D on large-scale datasets.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üî• Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;(2023.09.02) Test code release&lt;/li&gt; &#xA; &lt;li&gt;(2023.08.31) Pre-trained model release&lt;/li&gt; &#xA; &lt;li&gt;(2023.08.31) Paper release&lt;/li&gt; &#xA; &lt;li&gt;(2023.08.26) Online Demo release&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üëâ Dataset&lt;/h2&gt; &#xA;&lt;p&gt;SAM-Med2D is trained and tested on a dataset that includes &lt;strong&gt;4.6M images&lt;/strong&gt; and &lt;strong&gt;19.7M masks&lt;/strong&gt;. This dataset covers 10 medical data modalities, 4 anatomical structures + lesions, and 31 major human organs. To our knowledge, this is currently the largest and most diverse medical image segmentation dataset in terms of quantity and coverage of categories.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img width=&#34;800&#34; alt=&#34;image&#34; src=&#34;https://github.com/OpenGVLab/SAM-Med2D/raw/main/assets/dataset.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üëâ Framework&lt;/h2&gt; &#xA;&lt;p&gt;The pipeline of SAM-Med2D. We freeze the image encoder and incorporate learnable adapter layers in each Transformer block to acquire domain-specific knowledge in the medical field. We fine-tune the prompt encoder using point, Bbox, and mask information, while updating the parameters of the mask decoder through interactive training.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img width=&#34;800&#34; alt=&#34;image&#34; src=&#34;https://github.com/OpenGVLab/SAM-Med2D/raw/main/assets/framwork.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üëâ Results&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;caption align=&#34;center&#34;&gt;&#xA;  Quantitative comparison of different methods on the test set: &#xA; &lt;/caption&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Resolution&lt;/th&gt; &#xA;   &lt;th&gt;Bbox (%)&lt;/th&gt; &#xA;   &lt;th&gt;1 pt (%)&lt;/th&gt; &#xA;   &lt;th&gt;3 pts (%)&lt;/th&gt; &#xA;   &lt;th&gt;5 pts (%)&lt;/th&gt; &#xA;   &lt;th&gt;FPS&lt;/th&gt; &#xA;   &lt;th&gt;Checkpoint&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SAM&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;$256\times256$&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;61.63&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18.94&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28.28&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.47&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1_U26MIJhWnWVwmI5JkGg2cd2J6MvkqU-/view?usp=drive_link&#34;&gt;Offical&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SAM&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;$1024\times1024$&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;74.49&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36.88&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.00&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.57&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1_U26MIJhWnWVwmI5JkGg2cd2J6MvkqU-/view?usp=drive_link&#34;&gt;Offical&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FT-SAM&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;$256\times256$&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;73.56&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;60.11&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;70.95&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;75.51&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1J4qQt9MZZYdv1eoxMTJ4FL8Fz65iUFM8/view?usp=drive_link&#34;&gt;FT-SAM&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SAM-Med2D&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;$256\times256$&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;79.30&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;70.01&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;76.35&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;78.68&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1ARiB5RkSsWmAB_8mqWnwDF8ZKTtFwsjl/view?usp=drive_link&#34;&gt;SAM-Med2D&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;caption align=&#34;center&#34;&gt;&#xA;  Generalization validation on 9 MICCAI2023 datasets, where &#34;*&#34; denotes that we drop adapter layer of SAM-Med2D in test phase: &#xA; &lt;/caption&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th rowspan=&#34;2&#34;&gt;Datasets&lt;/th&gt; &#xA;   &lt;th colspan=&#34;3&#34;&gt;Bbox prompt (%)&lt;/th&gt; &#xA;   &lt;th colspan=&#34;3&#34;&gt;1 point prompt (%)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;SAM&lt;/th&gt; &#xA;   &lt;th&gt;SAM-Med2D&lt;/th&gt; &#xA;   &lt;th&gt;SAM-Med2D*&lt;/th&gt; &#xA;   &lt;th&gt;SAM&lt;/th&gt; &#xA;   &lt;th&gt;SAM-Med2D&lt;/th&gt; &#xA;   &lt;th&gt;SAM-Med2D*&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.synapse.org/#!Synapse:syn51236108/wiki/621615&#34;&gt;CrossMoDA23&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;78.98&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;70.51&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84.62&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18.49&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.08&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;73.98&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://kits-challenge.org/kits23/&#34;&gt;KiTS23&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84.80&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;76.32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;87.93&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38.93&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48.81&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;79.87&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://codalab.lisn.upsaclay.fr/competitions/12239#learn_the_details&#34;&gt;FLARE23&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;86.11&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83.51&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;90.95&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51.05&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62.86&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;85.10&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://atlas-challenge.u-bourgogne.fr/&#34;&gt;ATLAS2023&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;82.98&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;73.70&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;86.56&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.89&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34.72&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;70.42&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://multicenteraorta.grand-challenge.org/&#34;&gt;SEG2023&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;75.98&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;68.02&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84.31&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;11.75&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48.05&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;69.85&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://lnq2023.grand-challenge.org/lnq2023/&#34;&gt;LNQ2023&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;72.31&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.84&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;81.33&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.81&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.81&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.84&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://codalab.lisn.upsaclay.fr/competitions/9804&#34;&gt;CAS2023&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;52.34&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.11&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;60.38&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.45&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28.79&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15.19&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://tdsc-abus2023.grand-challenge.org/Dataset/&#34;&gt;TDSC-ABUS2023&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;71.66&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64.65&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;76.65&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12.11&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35.99&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;61.84&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://toothfairy.grand-challenge.org/toothfairy/&#34;&gt;ToothFairy2023&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;65.86&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.45&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;75.29&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.01&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.12&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.32&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Weighted sum&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;85.35&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;81.93&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;90.12&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48.08&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;60.31&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83.41&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;üëâ Visualization&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img width=&#34;800&#34; alt=&#34;image&#34; src=&#34;https://github.com/OpenGVLab/SAM-Med2D/raw/main/assets/visualization.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üëâ Test&lt;/h2&gt; &#xA;&lt;p&gt;Prepare your own dataset and refer to the samples in &lt;code&gt;SAM-Med2D/data_demo&lt;/code&gt; to replace them according to your specific scenario. You need to generate the &#34;label2image_test.json&#34; file before running &#34;test.py&#34;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd ./SAM-Med2d&#xA;python test.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;work_dir: Specifies the working directory for the testing process. Default value is &#34;workdir&#34;.&lt;/li&gt; &#xA; &lt;li&gt;batch_size: 1.&lt;/li&gt; &#xA; &lt;li&gt;image_size: Default value is 256.&lt;/li&gt; &#xA; &lt;li&gt;boxes_prompt: Use Bbox prompt to get segmentation results.&lt;/li&gt; &#xA; &lt;li&gt;point_num: Specifies the number of points. Default value is 1.&lt;/li&gt; &#xA; &lt;li&gt;iter_point: Specifies the number of iterations for point prompts.&lt;/li&gt; &#xA; &lt;li&gt;sam_checkpoint: Load sam or sammed checkpoint.&lt;/li&gt; &#xA; &lt;li&gt;encoder_adapter: Set to True if using SAM-Med2D&#39;s pretrained weights.&lt;/li&gt; &#xA; &lt;li&gt;save_pred: Whether to save the prediction results.&lt;/li&gt; &#xA; &lt;li&gt;prompt_path: Is there a fixed Prompt file? If not, the value is None, and it will be automatically generated in the latest prediction.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üöÄ Try SAM-Med2D&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üèÜ &lt;strong&gt;Gradio Online:&lt;/strong&gt; Online Demo can be found on &lt;a href=&#34;https://openxlab.org.cn/apps/detail/litianbin/SAM-Med2D&#34;&gt;OpenXLab&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üèÜ &lt;strong&gt;Notebook Demo:&lt;/strong&gt; You can use &lt;a href=&#34;https://github.com/OpenGVLab/SAM-Med2D/raw/main/predictor_example.ipynb&#34;&gt;predictor_example.ipynb&lt;/a&gt; to run it locally to view the prediction results generated by different prompts.&lt;/li&gt; &#xA; &lt;li&gt;üèÜ &lt;strong&gt;Gradio Local:&lt;/strong&gt; You can deploy &lt;a href=&#34;https://github.com/OpenGVLab/SAM-Med2D/raw/main/app.ipynb&#34;&gt;app.ipynb&lt;/a&gt; locally and upload test cases.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Notes:&lt;/strong&gt; Welcome to feedback &lt;a href=&#34;https://github.com/OpenGVLab/SAM-Med2D/issues/2&#34;&gt;good caseüëç&lt;/a&gt; and &lt;a href=&#34;https://github.com/OpenGVLab/SAM-Med2D/issues/1&#34;&gt;bad caseüëé&lt;/a&gt; in issue.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üóìÔ∏è Ongoing&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Train code release&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Test code release&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Pre-trained model release&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Paper release&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Online Demo release&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üé´ License&lt;/h2&gt; &#xA;&lt;p&gt;This project is released under the &lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/SAM-Med2D/main/LICENSE&#34;&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üí¨ Discussion Group&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions about SAM-Med2D, feel free to join our WeChat group discussion:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img width=&#34;300&#34; alt=&#34;image&#34; src=&#34;https://github.com/OpenGVLab/SAM-Med2D/raw/main/assets/SAM-Med2D_wechat_group.jpeg&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ü§ù Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We thank all medical workers and dataset owners for making public datasets available to the community.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to the open-source of the following projects: &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;Segment Anything&lt;/a&gt; ‚ÄÇ&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üëã Hiring &amp;amp; Global Collaboration&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hiring:&lt;/strong&gt; We are hiring researchers, engineers, and interns in General Vision Group, Shanghai AI Lab. If you are interested in Medical Foundation Models and General Medical AI, including designing benchmark datasets, general models, evaluation systems, and efficient tools, please contact us.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Global Collaboration:&lt;/strong&gt; We&#39;re on a mission to redefine medical research, aiming for a more universally adaptable model. Our passionate team is delving into foundational healthcare models, promoting the development of the medical community. Collaborate with us to increase competitiveness, reduce risk, and expand markets.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Contact:&lt;/strong&gt; Junjun He(&lt;a href=&#34;mailto:hejunjun@pjlab.org.cn&#34;&gt;hejunjun@pjlab.org.cn&lt;/a&gt;), Jin Ye(&lt;a href=&#34;mailto:yejin@pjlab.org.cn&#34;&gt;yejin@pjlab.org.cn&lt;/a&gt;), and Tianbin Li (&lt;a href=&#34;mailto:litianbin@pjlab.org.cn&#34;&gt;litianbin@pjlab.org.cn&lt;/a&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Reference&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{cheng2023sammed2d,&#xA;      title={SAM-Med2D}, &#xA;      author={Junlong Cheng and Jin Ye and Zhongying Deng and Jianpin Chen and Tianbin Li and Haoyu Wang and Yanzhou Su and&#xA;              Ziyan Huang and Jilong Chen and Lei Jiangand Hui Sun and Junjun He and Shaoting Zhang and Min Zhu and Yu Qiao},&#xA;      year={2023},&#xA;      eprint={2308.16184},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>