<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-31T01:37:25Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>EleutherAI/pythia</title>
    <updated>2023-03-31T01:37:25Z</updated>
    <id>tag:github.com,2023-03-31:/EleutherAI/pythia</id>
    <link href="https://github.com/EleutherAI/pythia" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Pythia: Interpreting Autoregressive Transformers Across Time and Scale&lt;/h1&gt; &#xA;&lt;p&gt;This repository is for EleutherAI&#39;s work-in-progress project &lt;em&gt;Pythia&lt;/em&gt; which combines interpretability analysis and scaling laws to understand how knowledge develops and evolves during training in autoregressive transformers.&lt;/p&gt; &#xA;&lt;h2&gt;Models&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Params&lt;/th&gt; &#xA;   &lt;th&gt;n_layers&lt;/th&gt; &#xA;   &lt;th&gt;d_model&lt;/th&gt; &#xA;   &lt;th&gt;n_heads&lt;/th&gt; &#xA;   &lt;th&gt;d_head&lt;/th&gt; &#xA;   &lt;th&gt;Batch Size&lt;/th&gt; &#xA;   &lt;th&gt;Learning Rate&lt;/th&gt; &#xA;   &lt;th&gt;Checkpoints&lt;/th&gt; &#xA;   &lt;th&gt;Evaluations&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pythia-70M&lt;/td&gt; &#xA;   &lt;td&gt;6&lt;/td&gt; &#xA;   &lt;td&gt;512&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;64&lt;/td&gt; &#xA;   &lt;td&gt;2M&lt;/td&gt; &#xA;   &lt;td&gt;1e-3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/pythia-70m&#34;&gt;Here&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Ready&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pythia-70M-Deduped&lt;/td&gt; &#xA;   &lt;td&gt;6&lt;/td&gt; &#xA;   &lt;td&gt;512&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;64&lt;/td&gt; &#xA;   &lt;td&gt;2M&lt;/td&gt; &#xA;   &lt;td&gt;1e-3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/pythia-70m-deduped&#34;&gt;Here&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Ready&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pythia-160M&lt;/td&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;   &lt;td&gt;768&lt;/td&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;   &lt;td&gt;64&lt;/td&gt; &#xA;   &lt;td&gt;4M&lt;/td&gt; &#xA;   &lt;td&gt;6e-4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/pythia-160m&#34;&gt;Here&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Ready&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pythia-160M-Deduped&lt;/td&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;   &lt;td&gt;768&lt;/td&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;   &lt;td&gt;64&lt;/td&gt; &#xA;   &lt;td&gt;4M&lt;/td&gt; &#xA;   &lt;td&gt;6e-4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/pythia-160m-deduped&#34;&gt;Here&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Ready&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pythia-410M&lt;/td&gt; &#xA;   &lt;td&gt;24&lt;/td&gt; &#xA;   &lt;td&gt;1024&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;64&lt;/td&gt; &#xA;   &lt;td&gt;4M&lt;/td&gt; &#xA;   &lt;td&gt;3e-4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/pythia-410m&#34;&gt;Here&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Ready&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pythia-410M-Deduped&lt;/td&gt; &#xA;   &lt;td&gt;24&lt;/td&gt; &#xA;   &lt;td&gt;1024&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;64&lt;/td&gt; &#xA;   &lt;td&gt;4M&lt;/td&gt; &#xA;   &lt;td&gt;3e-4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/pythia-410m-deduped&#34;&gt;Here&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Ready&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pythia-1B&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;2048&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;256&lt;/td&gt; &#xA;   &lt;td&gt;2M&lt;/td&gt; &#xA;   &lt;td&gt;3e-4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/pythia-1b&#34;&gt;Here&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Ready&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pythia-1B-Deduped&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;2048&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;256&lt;/td&gt; &#xA;   &lt;td&gt;2M&lt;/td&gt; &#xA;   &lt;td&gt;3e-4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/pythia-1b-deduped&#34;&gt;Here&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Ready&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pythia-1.4B&lt;/td&gt; &#xA;   &lt;td&gt;24&lt;/td&gt; &#xA;   &lt;td&gt;2048&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;128&lt;/td&gt; &#xA;   &lt;td&gt;4M&lt;/td&gt; &#xA;   &lt;td&gt;2e-4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/pythia-1.4b&#34;&gt;Here&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Ready&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pythia-1.4B-Deduped&lt;/td&gt; &#xA;   &lt;td&gt;24&lt;/td&gt; &#xA;   &lt;td&gt;2048&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;128&lt;/td&gt; &#xA;   &lt;td&gt;4M&lt;/td&gt; &#xA;   &lt;td&gt;2e-4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/pythia-1.4b-deduped&#34;&gt;Here&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Ready&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pythia-2.8B&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;2560&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;80&lt;/td&gt; &#xA;   &lt;td&gt;2M&lt;/td&gt; &#xA;   &lt;td&gt;1.6e-4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/pythia-2.8b&#34;&gt;Here&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Ready&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pythia-2.8B-Deduped&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;2560&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;80&lt;/td&gt; &#xA;   &lt;td&gt;2M&lt;/td&gt; &#xA;   &lt;td&gt;1.6e-4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/pythia-2.8b-deduped&#34;&gt;Here&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Ready&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pythia-6.9B&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;4096&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;128&lt;/td&gt; &#xA;   &lt;td&gt;2M&lt;/td&gt; &#xA;   &lt;td&gt;1.2e-4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/pythia-6.9b&#34;&gt;Here&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Ready&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pythia-6.9B-Deduped&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;4096&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;128&lt;/td&gt; &#xA;   &lt;td&gt;2M&lt;/td&gt; &#xA;   &lt;td&gt;1.2e-4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/pythia-6.9b-deduped&#34;&gt;Here&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Ready&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pythia-12B&lt;/td&gt; &#xA;   &lt;td&gt;36&lt;/td&gt; &#xA;   &lt;td&gt;5120&lt;/td&gt; &#xA;   &lt;td&gt;40&lt;/td&gt; &#xA;   &lt;td&gt;128&lt;/td&gt; &#xA;   &lt;td&gt;2M&lt;/td&gt; &#xA;   &lt;td&gt;1.2e-4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/pythia-12b&#34;&gt;Here&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Ready&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pythia-12B-Deduped&lt;/td&gt; &#xA;   &lt;td&gt;36&lt;/td&gt; &#xA;   &lt;td&gt;5120&lt;/td&gt; &#xA;   &lt;td&gt;40&lt;/td&gt; &#xA;   &lt;td&gt;128&lt;/td&gt; &#xA;   &lt;td&gt;2M&lt;/td&gt; &#xA;   &lt;td&gt;1.2e-4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/pythia-12b-deduped&#34;&gt;Here&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Ready&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;We train and release a suite of 8 model sizes on 2 different datasets: &lt;a href=&#34;https://pile.eleuther.ai/&#34;&gt;the Pile&lt;/a&gt;, as well as the Pile with deduplication applied.&lt;/p&gt; &#xA;&lt;p&gt;All 8 model sizes are trained on the exact same data, in the exact same order. Each model saw 299,892,736,000 ~= 299.9B tokens during training, and &lt;em&gt;143 checkpoints&lt;/em&gt; for each model are saved every 2,097,152,000 ~= 2B tokens, evenly spaced throughout training. This corresponds to just under 1 epoch on the Pile for non-&#34;deduped&#34; models, and ~= 1.5 epochs on the deduped Pile (which contains 207B tokens in 1 epoch).&lt;/p&gt; &#xA;&lt;p&gt;Config files used to train these models within the &lt;a href=&#34;https://github.com/EleutherAI/gpt-neox&#34;&gt;GPT-NeoX library&lt;/a&gt; can be found at the &lt;code&gt;models/&lt;/code&gt; directory within this repository.&lt;/p&gt; &#xA;&lt;p&gt;We also upload the pre-tokenized data files and a script to reconstruct the dataloader as seen during training for all models. See &lt;strong&gt;Reproducing Training&lt;/strong&gt; section for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;All Pythia models are hosted on &lt;a href=&#34;https://huggingface.co/EleutherAI&#34;&gt;the Huggingface hub&lt;/a&gt;. They can be loaded and used via the following code (shown for the 3rd &lt;code&gt;pythia-70M-deduped&lt;/code&gt; model checkpoint):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import GPTNeoXForCausalLM, AutoTokenizer&#xA;&#xA;model = GPTNeoXForCausalLM.from_pretrained(&#xA;  &#34;EleutherAI/pythia-70m-deduped&#34;,&#xA;  revision=&#34;step3000&#34;,&#xA;  cache_dir=&#34;./pythia-70m-deduped/step3000&#34;,&#xA;)&#xA;&#xA;tokenizer = AutoTokenizer.from_pretrained(&#xA;  &#34;EleutherAI/pythia-70m-deduped&#34;,&#xA;  revision=&#34;step3000&#34;,&#xA;  cache_dir=&#34;./pythia-70m-deduped/step3000&#34;,&#xA;)&#xA;&#xA;inputs = tokenizer(&#34;Hello, I am&#34;, return_tensors=&#34;pt&#34;)&#xA;tokens = model.generate(**inputs)&#xA;tokenizer.decode(tokens[0])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;All models were trained for the equivalent of 143000 steps at a batch size of 2,097,152 tokens. Revision/branch &lt;code&gt;step143000&lt;/code&gt; (e.g. &lt;a href=&#34;https://huggingface.co/EleutherAI/pythia-19m-deduped/tree/step143000&#34;&gt;https://huggingface.co/EleutherAI/pythia-19m-deduped/tree/step143000&lt;/a&gt;) corresponds exactly to the model checkpoint on the &lt;code&gt;main&lt;/code&gt; branch of each model.&lt;/p&gt; &#xA;&lt;p&gt;Models with a batch size of 4M tokens listed were originally trained for 71500 steps instead, and checkpointed every 500 steps. The checkpoints on Huggingface are renamed for consistency with all 2M batch models, so &lt;code&gt;step1000&lt;/code&gt; is the first checkpoint for the 1.3B that was saved (corresponding to step 500 in training), and &lt;code&gt;step1000&lt;/code&gt; is likewise the first 6.9B checkpoint that was saved (corresponding to 1000 &#34;actual&#34; steps.)&lt;/p&gt; &#xA;&lt;p&gt;We additionally have all model checkpoints in the format accepted by the &lt;a href=&#34;https://github.com/EleutherAI/gpt-neox&#34;&gt;GPT-NeoX library&lt;/a&gt;, but do not serve them at scale due to size of optimizer states and anticipated lower demand. If you would like to perform analysis using the models within the GPT-NeoX codebase, or would like the optimizer states, please email &lt;a href=&#34;mailto:hailey@eleuther.ai&#34;&gt;hailey@eleuther.ai&lt;/a&gt; and &lt;a href=&#34;mailto:stella@eleuther.ai&#34;&gt;stella@eleuther.ai&lt;/a&gt; to arrange access.&lt;/p&gt; &#xA;&lt;h2&gt;Reproducing Training&lt;/h2&gt; &#xA;&lt;p&gt;We provide the training data for replication of our training runs. The &lt;a href=&#34;https://github.com/EleutherAI/gpt-neox&#34;&gt;GPT-NeoX library&lt;/a&gt; requires the pre-tokenized training data in the form of 2 memory-mapped numpy arrays: a &lt;code&gt;.bin&lt;/code&gt; and &lt;code&gt;.idx&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;p&gt;We provide these files, hosted on the Hugging Face hub.&lt;/p&gt; &#xA;&lt;p&gt;To download and use the deduplicated Pile training data, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git lfs clone https://huggingface.co/datasets/EleutherAI/pythia_pile_idxmaps&#xA;&#xA;python utils/unshard_memmap.py --input_file ./pythia_pile_idxmaps/pile_0.87_deduped_text_document-00000-of-00082.bin --num_shards 83 --output_dir ./pythia_pile_idxmaps/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will take over a day to run, though it should not require more than 5 GB of RAM. We recommend downloading this rather than retokenizing the Pile from scratch, in order to preserve the data order seen by the Pythia models.&lt;/p&gt; &#xA;&lt;p&gt;TODO: forthcoming: more information on how to replicate + relaunch the Pythia training runs, once the data is actually downloaded.&lt;/p&gt; &#xA;&lt;h3&gt;Dataset Viewer&lt;/h3&gt; &#xA;&lt;p&gt;We provide a tool to view particular portions of the training dataloader used by all models during training, at &lt;code&gt;utils/batch_viewer.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To run, first substitute the filepath to the downloaded &lt;code&gt;.bin&lt;/code&gt; and &lt;code&gt;.idx&lt;/code&gt; files for either the Pile or deduplicated Pile in &lt;code&gt;utils/dummy_config.yml&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;PYTHONPATH=utils/gpt-neox/ python utils/batch_viewer.py \&#xA;  --start_iteration 0 \&#xA;  --end_iteration 1000 \&#xA;  --mode save \&#xA;  --conf_dir utils/dummy_config.yml &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Passing &lt;code&gt;--mode save&lt;/code&gt; will save a separate file containing each batch as a numpy array.&lt;/p&gt; &#xA;&lt;p&gt;Passing &lt;code&gt;--mode custom&lt;/code&gt; will save a dictionary for each batch to a JSONL file--it can be used to compute arbitrary statistics over each batch seen during training.&lt;/p&gt; &#xA;&lt;h2&gt;Benchmark Scores&lt;/h2&gt; &#xA;&lt;p&gt;We also provide benchmark 0-shot and 5-shot results on a variety of NLP datasets:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Lambada (&lt;code&gt;lambada_openai&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Wikitext (&lt;code&gt;wikitext&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;PiQA (&lt;code&gt;piqa&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;SciQ (&lt;code&gt;sciq&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;WSC (&lt;code&gt;wsc&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Winogrande (&lt;code&gt;winogrande&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;ARC-challenge (&lt;code&gt;arc_challenge&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;ARC-easy (&lt;code&gt;arc_easy&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;LogiQA (&lt;code&gt;logiqa&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;BLiMP (&lt;code&gt;blimp_*&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;MMLU (&lt;code&gt;hendrycksTest*&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Evaluations were performed in GPT-NeoX using the &lt;a href=&#34;https://github.com/EleutherAI/lm-evaluation-harness&#34;&gt;LM Evaluation Harness&lt;/a&gt;, and are viewable by model and step at &lt;code&gt;results/json/*&lt;/code&gt; in this repository.&lt;/p&gt; &#xA;&lt;p&gt;TODO: 160m, 410m, and 1.4b models&#39; step numbers from these evaluations correspond to .5x what is listed on Hugging Face. (e.g. here the final step is 71500, as it was during training. on the HF model hub these models&#39; checkpoints were relabeled such that pythia-1.4b&#39;s &#34;step143000&#34; checkpoint is the same as the step 71500 checkpoint in these evals.)&lt;/p&gt; &#xA;&lt;h3&gt;Plotting Results&lt;/h3&gt; &#xA;&lt;p&gt;We will also provide utilities for creating plots based on the dumped zero and few-shot results. Sample notebook and data format forthcoming.&lt;/p&gt; &#xA;&lt;h2&gt;Experiments&lt;/h2&gt; &#xA;&lt;h3&gt;Training Order and Memorization&lt;/h3&gt; &#xA;&lt;p&gt;A common explanation for language model training dynamics is that LMs have a mass of knowledge and when they come across new information they glom that knowledge on and slowly integrate it into the mass over time. One prediction that this mental model makes is that tokens encountered later in training will be more likely to be memorized than ones encountered earlier in training, as the model will not have time to adjust its representations to store the info without memorization. The primary goal of this experiment is to &lt;strong&gt;disprove&lt;/strong&gt; this prediction and demonstrate that training order doesn&#39;t influence memorization.&lt;/p&gt; &#xA;&lt;h3&gt;Gender Bias and Dataset Interventions&lt;/h3&gt;</summary>
  </entry>
  <entry>
    <title>alura-es-cursos/inmersion-en-datos</title>
    <updated>2023-03-31T01:37:25Z</updated>
    <id>tag:github.com,2023-03-31:/alura-es-cursos/inmersion-en-datos</id>
    <link href="https://github.com/alura-es-cursos/inmersion-en-datos" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Repositorio con la inmersión en datos de Alura Latam&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;inmersion-en-datos&lt;/h1&gt; &#xA;&lt;p&gt;Repositorio con la inmersión en datos de Alura Latam&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>tensorflow/tpu</title>
    <updated>2023-03-31T01:37:25Z</updated>
    <id>tag:github.com,2023-03-31:/tensorflow/tpu</id>
    <link href="https://github.com/tensorflow/tpu" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Reference models and tools for Cloud TPUs.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Cloud TPUs&lt;/h1&gt; &#xA;&lt;p&gt;This repository is a collection of reference models and tools used with &lt;a href=&#34;https://cloud.google.com/tpu/&#34;&gt;Cloud TPUs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The fastest way to get started training a model on a Cloud TPU is by following the tutorial. Click the button below to launch the tutorial using Google Cloud Shell.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://console.cloud.google.com/cloudshell/open?git_repo=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftpu&amp;amp;page=shell&amp;amp;tutorial=tools%2Fctpu%2Ftutorial.md&#34;&gt;&lt;img src=&#34;http://gstatic.com/cloudssh/images/open-btn.svg?sanitize=true&#34; alt=&#34;Open in Cloud Shell&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; This repository is a public mirror, pull requests will not be accepted. Please file an issue if you have a feature or bug request.&lt;/p&gt; &#xA;&lt;h2&gt;Running Models&lt;/h2&gt; &#xA;&lt;p&gt;To run models in the &lt;code&gt;models&lt;/code&gt; subdirectory, you may need to add the top-level &lt;code&gt;/models&lt;/code&gt; folder to the Python path with the command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export PYTHONPATH=&#34;$PYTHONPATH:/path/to/models&#34;&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>