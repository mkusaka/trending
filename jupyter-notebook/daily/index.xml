<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-22T01:37:32Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Letscode-br/lc-challenges-python</title>
    <updated>2023-05-22T01:37:32Z</updated>
    <id>tag:github.com,2023-05-22:/Letscode-br/lc-challenges-python</id>
    <link href="https://github.com/Letscode-br/lc-challenges-python" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Repositório que reúne os Challenges de python/dados anunciados no Let&#39;s Code Pass.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Reúne os desafios lançados no Let&#39;s Code Pass&lt;/h1&gt; &#xA;&lt;p&gt;Para mais informações, entre em nossa comunidade se inscrevendo em nosso &lt;a href=&#34;https://letscode.com.br/lets-code-pass&#34;&gt;site&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;python-challenge1&lt;/h2&gt; &#xA;&lt;p&gt;O primeiro desafio é aplicação do &lt;strong&gt;Método Twist&lt;/strong&gt; para codificação e decodificação de mensagens. Mais informações em &lt;code&gt;Challenge_Dados_-_Fevereiro.pdf&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;nb_challenge1_python.ipynb&lt;/code&gt;: notebook com enunciado e solução passo a passo.&lt;br&gt; &lt;code&gt;script_solution1.py&lt;/code&gt;: script com a solução do problema.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>gitkarma/quantum_computing_course2023</title>
    <updated>2023-05-22T01:37:32Z</updated>
    <id>tag:github.com,2023-05-22:/gitkarma/quantum_computing_course2023</id>
    <link href="https://github.com/gitkarma/quantum_computing_course2023" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Course Material for the online short course on Quantum Computing Using QSim&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;quantum_computing_course2023&lt;/h1&gt; &#xA;&lt;h2&gt;Links to Video Lectures&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLt_nrfusQeEdYto6Qh2hkD7EcCQ-7QHFk&#34;&gt;PLAYLIST&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=XYv7PnQmv2c&amp;amp;list=PLt_nrfusQeEdYto6Qh2hkD7EcCQ-7QHFk&amp;amp;index=1&amp;amp;pp=iAQB&#34;&gt;Week 1 - Session1 (06/05/2023) starttime 36:00&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=aTg4lBrR3EQ&amp;amp;list=PLt_nrfusQeEdYto6Qh2hkD7EcCQ-7QHFk&amp;amp;index=2&amp;amp;t=60s&amp;amp;pp=iAQB&#34;&gt;Week 1 - Session2 (07/05/2023) starttime 00.00&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=A9m-4i35HLE&amp;amp;list=PLt_nrfusQeEdYto6Qh2hkD7EcCQ-7QHFk&amp;amp;index=3&amp;amp;pp=iAQB&#34;&gt;Week 2 - Session1 (13/05/2023) starttime 07:00&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=IjP-9kt9EYo&amp;amp;list=PLt_nrfusQeEdYto6Qh2hkD7EcCQ-7QHFk&amp;amp;index=4&#34;&gt;Week 2 - Session2 (14/05/2023) starttime 31:30&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=SiRIhrEz7GY&amp;amp;list=PLt_nrfusQeEdYto6Qh2hkD7EcCQ-7QHFk&amp;amp;index=5&amp;amp;pp=iAQB&#34;&gt;Week 3 - Session1 (20/05/2023) starttime 00:00&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=kagvuQVYnnQ&amp;amp;list=PLt_nrfusQeEdYto6Qh2hkD7EcCQ-7QHFk&amp;amp;index=6&amp;amp;pp=iAQB&#34;&gt;Week 3 - Session2 (21/05/2023) starttime 08:00&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>sdatkinson/neural-amp-modeler</title>
    <updated>2023-05-22T01:37:32Z</updated>
    <id>tag:github.com,2023-05-22:/sdatkinson/neural-amp-modeler</id>
    <link href="https://github.com/sdatkinson/neural-amp-modeler" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Neural network emulator for guitar amplifiers.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;NAM: neural amp modeler&lt;/h1&gt; &#xA;&lt;p&gt;This repository handles training, reamping, and exporting the weights of a model. For playing trained models in real time in a standalone application or plugin, see the partner repo, &lt;a href=&#34;https://github.com/sdatkinson/NeuralAmpModelerPlugin&#34;&gt;NeuralAmpModelerPlugin&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;How to use (Google Colab)&lt;/h2&gt; &#xA;&lt;p&gt;If you don&#39;t have a good computer for training ML models, you use Google Colab to train in the cloud using the pre-made notebooks under &lt;code&gt;bin\train&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For the very easiest experience, open &lt;a href=&#34;https://colab.research.google.com/github/sdatkinson/neural-amp-modeler/blob/d7c95f5/bin/train/easy_colab.ipynb&#34;&gt;&lt;code&gt;easy_colab.ipynb&lt;/code&gt; on Google Colab&lt;/a&gt; and follow the steps!&lt;/p&gt; &#xA;&lt;p&gt;For a little more visibility under the hood, you can use &lt;a href=&#34;https://colab.research.google.com/github/sdatkinson/neural-amp-modeler/blob/main/bin/train/colab.ipynb&#34;&gt;colab.ipynb&lt;/a&gt; instead.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;No local installation required!&lt;/li&gt; &#xA; &lt;li&gt;Decent GPUs are available if you don&#39;t have one on your computer.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Uploading your data can take a long time.&lt;/li&gt; &#xA; &lt;li&gt;The session will time out after a few hours (for free accounts), so extended training runs aren&#39;t really feasible. Also, there&#39;s a usage limit so you can&#39;t hang out all day. I&#39;ve tried to set you up with a good model that should train reasonably quickly!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to use (Local)&lt;/h2&gt; &#xA;&lt;p&gt;Alternatively, you can clone this repo to your computer and use it locally.&lt;/p&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;Installation uses &lt;a href=&#34;https://www.anaconda.com/&#34;&gt;Anaconda&lt;/a&gt; for package management.&lt;/p&gt; &#xA;&lt;p&gt;For computers with a CUDA-capable GPU (recommended):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda env create -f environment_gpu.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Otherwise, for a CPU-only install (will train much more slowly):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda env create -f environment_cpu.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then activate the environment you&#39;ve created with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda activate nam&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Train models (GUI)&lt;/h3&gt; &#xA;&lt;p&gt;After installing, you can open a GUI trainer by running&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;nam&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;from the terminal.&lt;/p&gt; &#xA;&lt;h3&gt;Train models (Python script)&lt;/h3&gt; &#xA;&lt;p&gt;For users looking to get more fine-grained control over the modeling process, NAM includes a training script that can be run from the terminal. In order to run it&lt;/p&gt; &#xA;&lt;h4&gt;Download audio files&lt;/h4&gt; &#xA;&lt;p&gt;Download the &lt;a href=&#34;https://drive.google.com/file/d/1v2xFXeQ9W2Ks05XrqsMCs2viQcKPAwBk/view?usp=share_link&#34;&gt;v1_1_1.wav&lt;/a&gt; and &lt;a href=&#34;https://drive.google.com/file/d/14w2utgL16NozmESzAJO_I0_VCt-5Wgpv/view?usp=share_link&#34;&gt;overdrive.wav&lt;/a&gt; to a folder of your choice&lt;/p&gt; &#xA;&lt;h4&gt;Update data configuration&lt;/h4&gt; &#xA;&lt;p&gt;Edit &lt;code&gt;bin/train/data/single_pair.json&lt;/code&gt; to point to relevant audio files&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;    &#34;common&#34;: {&#xA;        &#34;x_path&#34;: &#34;C:\\path\\to\\v1_1_1.wav&#34;,&#xA;        &#34;y_path&#34;: &#34;C:\\path\\to\\overdrive.wav&#34;,&#xA;        &#34;delay&#34;: 0&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Run training script&lt;/h4&gt; &#xA;&lt;p&gt;Open up a terminal. Activate your nam environment and call the training with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python bin/train/main.py \&#xA;bin/train/inputs/data/single_pair.json \&#xA;bin/train/inputs/models/demonet.json \&#xA;bin/train/inputs/learning/demo.json \&#xA;bin/train/outputs/MyAmp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;data/single_pair.json&lt;/code&gt; contains the information about the data you&#39;re training on&lt;br&gt; &lt;code&gt;models/demonet.json&lt;/code&gt; contains information about the model architecture that is being trained. The example used here uses a &lt;code&gt;feather&lt;/code&gt; configured &lt;code&gt;wavenet&lt;/code&gt;.&lt;br&gt; &lt;code&gt;learning/demo.json&lt;/code&gt; contains information about the training run itself (e.g. number of epochs).&lt;/p&gt; &#xA;&lt;p&gt;The configuration above runs a short (demo) training. For a real training you may prefer to run something like,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python bin/train/main.py \&#xA;bin/train/inputs/data/single_pair.json \&#xA;bin/train/inputs/models/wavenet.json \&#xA;bin/train/inputs/learning/default.json \&#xA;bin/train/outputs/MyAmp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;As a side note, NAM uses &lt;a href=&#34;https://lightning.ai/pages/open-source/&#34;&gt;PyTorch Lightning&lt;/a&gt; under the hood as a modeling framework, and you can control many of the Pytorch Lightning configuration options from &lt;code&gt;bin/train/inputs/learning/default.json&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Export a model (to use with &lt;a href=&#34;https://github.com/sdatkinson/NeuralAmpModelerPlugin&#34;&gt;the plugin&lt;/a&gt;)&lt;/h4&gt; &#xA;&lt;p&gt;Exporting the trained model to a &lt;code&gt;.nam&lt;/code&gt; file for use with the plugin can be done with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python bin/export.py \&#xA;path/to/config_model.json \&#xA;path/to/checkpoints/epoch=123_val_loss=0.000010.ckpt \&#xA;path/to/exported_models/MyAmp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, point the plugin at the exported &lt;code&gt;model.nam&lt;/code&gt; file and you&#39;re good to go!&lt;/p&gt; &#xA;&lt;h3&gt;Other utilities&lt;/h3&gt; &#xA;&lt;h4&gt;Run a model on an input signal (&#34;reamping&#34;)&lt;/h4&gt; &#xA;&lt;p&gt;Handy if you want to just check it out without needing to use the plugin:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python bin/run.py \&#xA;path/to/source.wav \&#xA;path/to/config_model.json \&#xA;path/to/checkpoints/epoch=123_val_loss=0.000010.ckpt \&#xA;path/to/output.wav&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>