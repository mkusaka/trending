<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-15T01:30:35Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>hamzafarooq/advanced-llms-course</title>
    <updated>2024-03-15T01:30:35Z</updated>
    <id>tag:github.com,2024-03-15:/hamzafarooq/advanced-llms-course</id>
    <link href="https://github.com/hamzafarooq/advanced-llms-course" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code files for advanced LLM Course&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;advanced-llms&lt;/h1&gt; &#xA;&lt;p&gt;Welcome to the comprehensive course on advancing your skills in building sophisticated Large Language Model (LLM) applications! Details about the course can be found here: &lt;a href=&#34;https://maven.com/boring-bot/advanced-llm&#34;&gt;https://maven.com/boring-bot/advanced-llm&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;We have tried to build the most advanced LLM course currently being offered in the world. No pun intended.&lt;/p&gt; &#xA;&lt;p&gt;If you have already acquired knowledge about RAG, cosine similarity, vector databases, and Langchain, it&#39;s time to delve into the practical aspects of packaging and deploying these models in production environments.&lt;/p&gt; &#xA;&lt;p&gt;This course builds upon the fundamental building blocks of LLMs and covers the following key topics:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Fine-tuning: Learn advanced techniques for fine-tuning LLMs (ChatGPT and Open-source LLMs) to enhance their performance and adapt them to specific tasks or domains.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Model merging: Explore methods to merge multiple models, optimizing their collective capabilities for more robust and versatile language processing.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Inference speed exploration: Understand strategies to optimize and accelerate inference speeds, ensuring efficient real-time processing of language model outputs.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Quantization methods: Dive into techniques for model quantization, reducing model size while maintaining performance, crucial for deployment in resource-constrained environments.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Model hosting and deployments: Gain insights into best practices for hosting and deploying LLMs in production settings, ensuring seamless integration into diverse applications.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Semantic Caching: Learn how to build it all from scratch and implement it with GCP and REDIS&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Guardrail and DSPy: Implement State of the Art Guardrail and learn how you can build applications with minimal prompting&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Throughout the course, we will analyze state-of-the-art AI products, reverse-engineering some through Python.&lt;/p&gt; &#xA;&lt;p&gt;Additionally, my collaboration with experienced Software Engineers on our team will provide valuable insights into integrating LLMs with Node.js for web application development.&lt;/p&gt; &#xA;&lt;p&gt;As a bonus, you&#39;ll have access to experimental products being developed at Traversaal.ai, my startup, allowing you to stay at the forefront of cutting-edge advancements in the field.&lt;/p&gt; &#xA;&lt;p&gt;Prerequisites for this course include proficiency in Python and a solid understanding of RAGs, as well as Encoder and Decoder models.&lt;/p&gt; &#xA;&lt;p&gt;If you feel the need for a more foundational course, consider checking out my other offering on LLMs: &lt;a href=&#34;https://maven.com/boring-bot/ml-system-design&#34;&gt;https://maven.com/boring-bot/ml-system-design&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Tools utilized in this course include VS Code, UNIX terminal, Jupyter Notebooks, and Conda package management, ensuring a hands-on and practical learning experience.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Azure-Samples/contoso-chat</title>
    <updated>2024-03-15T01:30:35Z</updated>
    <id>tag:github.com,2024-03-15:/Azure-Samples/contoso-chat</id>
    <link href="https://github.com/Azure-Samples/contoso-chat" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;End to End LLM App development with Azure AI Studio and Prompt Flow&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Table Of Contents&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Azure-Samples/contoso-chat/main/#1-learning-objectives&#34;&gt;Learning Objectives&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Azure-Samples/contoso-chat/main/#2-pre-requisites&#34;&gt;Pre-Requisites&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Azure-Samples/contoso-chat/main/#3-development-environment&#34;&gt;Setup Development Environment&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;3.1 &lt;a href=&#34;https://raw.githubusercontent.com/Azure-Samples/contoso-chat/main/#31-pre-built-environment-in-cloud-github-codespaces&#34;&gt;Pre-built Container, GitHub Codespaces&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;3.2 &lt;a href=&#34;https://raw.githubusercontent.com/Azure-Samples/contoso-chat/main/#32-pre-built-environment-on-device-docker-desktop&#34;&gt;Pre-built Container, Docker Desktop&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;3.3 &lt;a href=&#34;https://raw.githubusercontent.com/Azure-Samples/contoso-chat/main/#33-manual-setup-environment-on-device-anaconda-or-venv&#34;&gt;Manual Python env, Anaconda or venv&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Azure-Samples/contoso-chat/main/#4-create-azure-resources&#34;&gt;Provision Azure Resources&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;4.1 &lt;a href=&#34;https://raw.githubusercontent.com/Azure-Samples/contoso-chat/main/#41-authenticate-with-azure&#34;&gt;Authenticate With Azure&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;4.2 &lt;a href=&#34;https://raw.githubusercontent.com/Azure-Samples/contoso-chat/main/#42-run-provisioning-script&#34;&gt;Run Provisioning Script&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;4.3 &lt;a href=&#34;https://raw.githubusercontent.com/Azure-Samples/contoso-chat/main/#43-verify-configjson-setup&#34;&gt;Verify config.json setup&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;4.4 &lt;a href=&#34;https://raw.githubusercontent.com/Azure-Samples/contoso-chat/main/#44-verify-env-setup&#34;&gt;Verify .env setup&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;4.5 &lt;a href=&#34;https://raw.githubusercontent.com/Azure-Samples/contoso-chat/main/#45-verify-local-connections-for-prompt-flow&#34;&gt;Verify local Connections&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;4.6 &lt;a href=&#34;https://raw.githubusercontent.com/Azure-Samples/contoso-chat/main/#46-verify-cloud-connections-for-prompt-flow&#34;&gt;Verify cloud Connections&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Azure-Samples/contoso-chat/main/#5-populate-with-sample-data&#34;&gt;Populate With Your Data&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Azure-Samples/contoso-chat/main/#6-building-a-prompt-flow&#34;&gt;Build Your Prompt Flow&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;6.1 &lt;a href=&#34;https://raw.githubusercontent.com/Azure-Samples/contoso-chat/main/#61-explore-the-contoso-chat-prompt-flow&#34;&gt;Explore contoso-chat Prompt Flow&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;6.2 &lt;a href=&#34;https://raw.githubusercontent.com/Azure-Samples/contoso-chat/main/#62-understand-prompt-flow-components&#34;&gt;Understand Prompt Flow Components&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;6.3 &lt;a href=&#34;https://raw.githubusercontent.com/Azure-Samples/contoso-chat/main/#63-run-the-prompt-flow&#34;&gt;Run The Prompt Flow&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Azure-Samples/contoso-chat/main/#7-evaluating-prompt-flow-results&#34;&gt;Evaluate Your Prompt Flow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Azure-Samples/contoso-chat/main/#8-deployment-with-sdk&#34;&gt;Deploy Using Azure AI SDK&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Azure-Samples/contoso-chat/main/#9-deploy-with-github-actions&#34;&gt;Deploy with GitHub Actions&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;em&gt;If you find this sample useful, consider giving us a star on GitHub! If you have any questions or comments, consider filing an Issue on the &lt;a href=&#34;https://github.com/Azure-Samples/contoso-chat&#34;&gt;source repo&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;1. Learning Objectives&lt;/h2&gt; &#xA;&lt;p&gt;Learn to build an Large Language Model (LLM) Application with a RAG (Retrieval Augmented Generation) architecture using &lt;strong&gt;Azure AI Studio&lt;/strong&gt; and &lt;strong&gt;Prompt Flow&lt;/strong&gt;. By the end of this workshop you should be able to:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Describe what Azure AI Studio and Prompt Flow provide&lt;/li&gt; &#xA; &lt;li&gt;Explain the RAG Architecture for building LLM Apps&lt;/li&gt; &#xA; &lt;li&gt;Build, run, evaluate, and deploy, a RAG-based LLM App to Azure.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;2. Pre-Requisites&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Azure Subscription&lt;/strong&gt; - &lt;a href=&#34;https://azure.microsoft.com/free/&#34;&gt;Signup for a free account.&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Visual Studio Code&lt;/strong&gt; - &lt;a href=&#34;https://code.visualstudio.com/download&#34;&gt;Download it for free.&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;GitHub Account&lt;/strong&gt; - &lt;a href=&#34;https://github.com/signup&#34;&gt;Signup for a free account.&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Access to Azure Open AI Services&lt;/strong&gt; - &lt;a href=&#34;https://learn.microsoft.com/legal/cognitive-services/openai/limited-access&#34;&gt;Learn about getting access.&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Ability to provision Azure AI Search (Paid)&lt;/strong&gt; - Required for Semantic Ranker&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;3. Development Environment&lt;/h2&gt; &#xA;&lt;p&gt;The repository is instrumented with a &lt;code&gt;devcontainer.json&lt;/code&gt; configuration that can provide you with a &lt;em&gt;pre-built&lt;/em&gt; environment that can be launched locally, or in the cloud. You can also elect to do a &lt;em&gt;manual&lt;/em&gt; environment setup locally, if desired. Here are the three options in increasing order of complexity and effort on your part. &lt;strong&gt;Pick one!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Pre-built environment, in cloud&lt;/strong&gt; with GitHub Codespaces&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Pre-built environment, on device&lt;/strong&gt; with Docker Desktop&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Manual setup environment, on device&lt;/strong&gt; with Anaconda or venv&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The first approach is &lt;em&gt;recommended&lt;/em&gt; for minimal user effort in startup and maintenance. The third approach will require you to manually update or maintain your local environment, to reflect any future updates to the repo.&lt;/p&gt; &#xA;&lt;p&gt;To setup the development environment you can leverage either GitHub Codespaces, a local Python environment (using Anaconda or venv), or a VS Code Dev Container environment (using Docker).&lt;/p&gt; &#xA;&lt;h3&gt;3.1 Pre-Built Environment, in cloud (GitHub Codespaces)&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;This is the recommended option.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fork the repo into your personal profile.&lt;/li&gt; &#xA; &lt;li&gt;In your fork, click the green &lt;code&gt;Code&lt;/code&gt; button on the repository&lt;/li&gt; &#xA; &lt;li&gt;Select the &lt;code&gt;Codespaces&lt;/code&gt; tab and click &lt;code&gt;Create codespace...&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This should open a new browser tab with a Codespaces container setup process running. On completion, this will launch a Visual Studio Code editor in the browser, with all relevant dependencies already installed in the running development container beneath. &lt;strong&gt;Congratulations! Your cloud dev environment is ready!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;3.2 Pre-Built Environment, on device (Docker Desktop)&lt;/h3&gt; &#xA;&lt;p&gt;This option uses the same &lt;code&gt;devcontainer.json&lt;/code&gt; configuration, but launches the development container in your local device using Docker Desktop. To use this approach, you need to have the following tools pre-installed in your local device:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Visual Studio Code (with Dev Containers Extension)&lt;/li&gt; &#xA; &lt;li&gt;Docker Desktop (community or free version is fine)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Make sure your Docker Desktop daemon is running on your local device.&lt;/strong&gt; Then,&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fork this repo to your personal profile&lt;/li&gt; &#xA; &lt;li&gt;Clone that fork to your local device&lt;/li&gt; &#xA; &lt;li&gt;Open the cloned repo using Visual Studio Code&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If your Dev Containers extension is installed correctly, you will be prompted to &#34;re-open the project in a container&#34; - just confirm to launch the container locally. Alternatively, you may need to trigger this step manually. See the &lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers&#34;&gt;Dev Containers Extension&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;p&gt;Once your project launches in the local Docker desktop container, you should see the Visual Studio Code editor reflect that connection in the status bar (blue icon, bottom left). &lt;strong&gt;Congratulations! Your local dev environment is ready!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;3.3 Manual Setup Environment, on device (Anaconda or venv)&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone the repo&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/azure/contoso-chat&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Open the repo in VS Code&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd contoso-chat&#xA;code .&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install the &lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=prompt-flow.prompt-flow&#34;&gt;Prompt Flow Extension&lt;/a&gt; in VS Code&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Open the VS Code Extensions tab&lt;/li&gt; &#xA;   &lt;li&gt;Search for &#34;Prompt Flow&#34;&lt;/li&gt; &#xA;   &lt;li&gt;Install the extension&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install the &lt;a href=&#34;https://learn.microsoft.com/cli/azure/install-azure-cli&#34;&gt;Azure CLI&lt;/a&gt; for your device OS&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create a new local Python environment using &lt;strong&gt;either&lt;/strong&gt; &lt;a href=&#34;https://www.anaconda.com/products/individual&#34;&gt;anaconda&lt;/a&gt; &lt;strong&gt;or&lt;/strong&gt; &lt;a href=&#34;https://docs.python.org/3/library/venv.html&#34;&gt;venv&lt;/a&gt; for a managed environment.&lt;/p&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Option 1&lt;/strong&gt;: Using anaconda&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n contoso-chat python=3.11&#xA;conda activate contoso-chat&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Option 2:&lt;/strong&gt; Using venv&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 -m venv .venv&#xA;source .venv/bin/activate&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;4. Create Azure resources&lt;/h2&gt; &#xA;&lt;p&gt;We setup our development ennvironment in the previous step. In this step, we&#39;ll &lt;strong&gt;provision Azure resources&lt;/strong&gt; for our project, ready to use for developing our LLM Application.&lt;/p&gt; &#xA;&lt;h3&gt;4.1 Authenticate with Azure&lt;/h3&gt; &#xA;&lt;p&gt;Start by connecting your Visual Studio Code environment to your Azure account:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Open the terminal in VS Code and use command &lt;code&gt;az login&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Complete the authentication flow.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;If you are running within a dev container, use these instructions to login instead:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Open the terminal in VS Code and use command &lt;code&gt;az login --use-device-code&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;The console message will give you an alphanumeric code&lt;/li&gt; &#xA; &lt;li&gt;Navigate to &lt;em&gt;&lt;a href=&#34;https://microsoft.com/devicelogin&#34;&gt;https://microsoft.com/devicelogin&lt;/a&gt;&lt;/em&gt; in a new tab&lt;/li&gt; &#xA; &lt;li&gt;Enter the code from step 2 and complete the flow.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;In either case, verify that the console shows a message indicating a successful authentication. &lt;strong&gt;Congratulations! Your VS Code session is now connected to your Azure subscription!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;4.2 Run Provisioning Script&lt;/h3&gt; &#xA;&lt;p&gt;The project requires a number of Azure resources to be set up, in a specified order. To simplify this, an auto-provisioning script has been provided. (NOTE: It will use the current active subscription to create the resource. If you have multiple subscriptions, use &lt;code&gt;az account set --subscription &#34;&amp;lt;SUBSCRIPTION-NAME&amp;gt;&#34;&lt;/code&gt; first to set the desired active subscription.)&lt;/p&gt; &#xA;&lt;p&gt;Run the provisioning script as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./provision.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The script should &lt;strong&gt;set up a dedicated resource group&lt;/strong&gt; with the following resources:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Azure AI services&lt;/strong&gt; resource&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Azure Machine Learning workspace&lt;/strong&gt; (Azure AI Project) resource&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Search service&lt;/strong&gt; (Azure AI Search) resource&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Azure Cosmos DB account&lt;/strong&gt; resource&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The script will set up an &lt;strong&gt;Azure AI Studio&lt;/strong&gt; project with the following model deployments created by default, in a relevant region that supports them. &lt;em&gt;Your Azure subscription must be &lt;a href=&#34;https://learn.microsoft.com/azure/ai-services/openai/overview#how-do-i-get-access-to-azure-openai&#34;&gt;enabled for Azure OpenAI access&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;gpt-3.5-turbo&lt;/li&gt; &#xA; &lt;li&gt;text-embeddings-ada-002&lt;/li&gt; &#xA; &lt;li&gt;gpt-4&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The Azure AI Search resource will have &lt;strong&gt;Semantic Ranker&lt;/strong&gt; enabled for this project, which requires the use of a paid tier of that service. It may also be created in a different region, based on availability of that feature.&lt;/p&gt; &#xA;&lt;h3&gt;4.3 Verify &lt;code&gt;config.json&lt;/code&gt; setup&lt;/h3&gt; &#xA;&lt;p&gt;The script should automatically create a &lt;code&gt;config.json&lt;/code&gt; in your root directory, with the relevant Azure subscription, resource group, and AI workspace properties defined. &lt;em&gt;These will be made use of by the Azure AI SDK for relevant API interactions with the Azure AI platform later&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If the config.json file is not created, simply download it from your Azure portal by visiting the &lt;em&gt;Azure AI project&lt;/em&gt; resource created, and looking at its Overview page.&lt;/p&gt; &#xA;&lt;h3&gt;4.4 Verify &lt;code&gt;.env&lt;/code&gt; setup&lt;/h3&gt; &#xA;&lt;p&gt;The default sample has an &lt;code&gt;.env.sample&lt;/code&gt; file that shows the relevant environment variables that need to be configured in this project. The script should create a &lt;code&gt;.env&lt;/code&gt; file that has these same variables &lt;em&gt;but populated with the right values&lt;/em&gt; for your Azure resources.&lt;/p&gt; &#xA;&lt;p&gt;If the file is not created, simply copy over &lt;code&gt;.env.sample&lt;/code&gt; to &lt;code&gt;.env&lt;/code&gt; - then populate those values manually from the respective Azure resource pages using the Azure Portal (for Azure CosmosDB and Azure AI Search) and the Azure AI Studio (for the Azure OpenAI values)&lt;/p&gt; &#xA;&lt;h3&gt;4.5 Verify local connections for Prompt Flow&lt;/h3&gt; &#xA;&lt;p&gt;You will need to have your local Prompt Flow extension configured to have the following &lt;em&gt;connection&lt;/em&gt; objects set up:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;contoso-cosmos&lt;/code&gt; to Azure Cosmos DB endpoint&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;contoso-search&lt;/code&gt; to Azure AI Search endpoint&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;aoai-connection&lt;/code&gt; to Azure OpenAI endpoint&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Verify if these were created by using the &lt;a href=&#34;https://microsoft.github.io/promptflow/reference/pf-command-reference.html#pf-connection&#34;&gt;pf tool&lt;/a&gt; from the VS Code terminal as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pf connection list&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If the connections are &lt;em&gt;not&lt;/em&gt; visible, create them by running the &lt;code&gt;connections/create-connections.ipynb&lt;/code&gt; notebook. Then run the above command to verify they were created correctly.&lt;/p&gt; &#xA;&lt;h3&gt;4.6 Verify cloud connections for Prompt Flow&lt;/h3&gt; &#xA;&lt;p&gt;The auto-provisioning will have setup 2 of the 3 connections for you by default. First, verify this by&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;going to &lt;a href=&#34;https://ai.azure.com&#34;&gt;Azure AI Studio&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;signing in with your Azure account, then clicking &#34;Build&#34;&lt;/li&gt; &#xA; &lt;li&gt;selecting the Azure AI project for this repo, from that list&lt;/li&gt; &#xA; &lt;li&gt;clicking &#34;Settings&#34; in the sidebar for the project&lt;/li&gt; &#xA; &lt;li&gt;clicking &#34;View All&#34; in the Connections panel in Settings&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You should see &lt;code&gt;contoso-search&lt;/code&gt; and &lt;code&gt;aoai-connection&lt;/code&gt; pre-configured, else create them from the Azure AI Studio interface using the &lt;strong&gt;Create Connection&lt;/strong&gt; workflow (and using the relevant values from your &lt;code&gt;.env&lt;/code&gt; file).&lt;/p&gt; &#xA;&lt;p&gt;You will however need to &lt;strong&gt;create &lt;code&gt;contoso-cosmos&lt;/code&gt; manually from Azure ML Studio&lt;/strong&gt;. This is a temporary measure for &lt;em&gt;custom connections&lt;/em&gt; and may be automated in future. For now, do this:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Visit &lt;a href=&#34;https://ai.azure.com&#34;&gt;https://ai.azure.com&lt;/a&gt; and sign in if necessary&lt;/li&gt; &#xA; &lt;li&gt;Under Recent Projects, click your Azure AI project (e.g., contoso-chat-aiproj)&lt;/li&gt; &#xA; &lt;li&gt;Select Settings (on sidebar), scroll down to the Connections pane, and click &#34;View All&#34;&lt;/li&gt; &#xA; &lt;li&gt;Click &#34;+ New connection&#34;, modify the Service field, and select Custom from dropdown&lt;/li&gt; &#xA; &lt;li&gt;Enter &#34;Connection Name&#34;: contoso-cosmos, &#34;Access&#34;: Project.&lt;/li&gt; &#xA; &lt;li&gt;Click &#34;+ Add key value pairs&#34; &lt;strong&gt;four&lt;/strong&gt; times. Fill in the following details found in the &lt;code&gt;.env&lt;/code&gt; file: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;key=key, value=.env value for COSMOS_KEY, is-secret=checked&lt;/li&gt; &#xA;   &lt;li&gt;key=endpoint, value=.env value for COSMOS_ENDPOINT&lt;/li&gt; &#xA;   &lt;li&gt;key=containerId, value=customers&lt;/li&gt; &#xA;   &lt;li&gt;key=databaseId, value=contoso-outdoor&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Click &#34;Save&#34; to finish setup.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Refresh main Connections list screen to verify that you now have all three required connections listed.&lt;/p&gt; &#xA;&lt;h2&gt;5. Populate with sample data&lt;/h2&gt; &#xA;&lt;p&gt;In this step we want to populate the required data for our application use case.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Populate Search Index&lt;/strong&gt; in Azure AI Search &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Run the code in the &lt;code&gt;data/product_info/create-azure-search.ipynb&lt;/code&gt; notebook.&lt;/li&gt; &#xA;   &lt;li&gt;Visit the Azure AI Search resource in the Azure Portal&lt;/li&gt; &#xA;   &lt;li&gt;Click on &#34;Indexes&#34; and verify that a new index was created&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Populate Customer Data&lt;/strong&gt; in Azure Cosmos DB &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Run the code in the &lt;code&gt;data/customer_info/create-cosmos-db.ipynb&lt;/code&gt; notebook.&lt;/li&gt; &#xA;   &lt;li&gt;Visit the Azure Cosmos DB resource in the Azure Portal&lt;/li&gt; &#xA;   &lt;li&gt;Click on &#34;Data Explorer&#34; and verify tat the container and database were created!&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;6. Building a prompt flow&lt;/h2&gt; &#xA;&lt;p&gt;We are now ready to begin building our prompt flow! The repository comes with a number of pre-written flows that provide the starting points for this project. In the following section, we&#39;ll explore what these are and how they work.&lt;/p&gt; &#xA;&lt;h3&gt;6.1. Explore the &lt;code&gt;contoso-chat&lt;/code&gt; Prompt Flow&lt;/h3&gt; &#xA;&lt;p&gt;A prompt flow is a DAG (directed acyclic graph) that is made up of nodes that are connected together to form a flow. Each node in the flow is a python function tool that can be edited and customized to fit your needs.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Click on the &lt;code&gt;contoso-chat/flow.dag.yaml&lt;/code&gt; file in the Visual Studio Code file explorer.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;You should get a view &lt;em&gt;similar to&lt;/em&gt; what is shown below.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Click the &lt;code&gt;Visual editor&lt;/code&gt; text line shown underlined below. &lt;img src=&#34;https://raw.githubusercontent.com/Azure-Samples/contoso-chat/main/images/visualeditorbutton.png&#34; alt=&#34;Visual editor button&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;This will open up the prompt flow in the visual editor as shown: - &lt;img src=&#34;https://raw.githubusercontent.com/Azure-Samples/contoso-chat/main/images/promptflow.png&#34; alt=&#34;Alt text&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;6.2 Understand Prompt Flow components&lt;/h3&gt; &#xA;&lt;p&gt;The prompt flow is a directed acyclic graph (DAG) of nodes, with a starting node (input), a terminating node (output), and an intermediate sub-graph of connected nodes as follows:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Node&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;em&gt;input&lt;/em&gt;s&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;This node is used to start the flow and is the entry point for the flow. It has the input parameters &lt;code&gt;customer_id&lt;/code&gt; and &lt;code&gt;question&lt;/code&gt;, and &lt;code&gt;chat_history&lt;/code&gt;. The &lt;code&gt;customer_id&lt;/code&gt; is used to look up the customer information in the Cosmos DB. The &lt;code&gt;question&lt;/code&gt; is the question the customer is asking. The &lt;code&gt;chat_history&lt;/code&gt; is the chat history of the conversation with the customer.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;em&gt;question_embedding&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;This node is used to embed the question text using the &lt;code&gt;text-embedding-ada-002&lt;/code&gt; model. The embedding is used to find the most relevant documents from the AI Search index.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;em&gt;retrieve_documents&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;This node is used to retrieve the most relevant documents from the AI Search index with the question vector.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;em&gt;customer_lookup&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;This node is used to get the customer information from the Cosmos DB.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;em&gt;customer_prompt&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;This node is used to generate the prompt with the information retrieved and added to the &lt;code&gt;customer_prompt.jinja2&lt;/code&gt; template.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;em&gt;llm_response&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;This node is used to generate the response to the customer using the &lt;code&gt;GPT-35-Turbo&lt;/code&gt; model.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;em&gt;outputs&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;This node is used to end the flow and return the response to the customer.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;6.3 Run the prompt flow&lt;/h3&gt; &#xA;&lt;p&gt;Let&#39;s run the flow to see what happens. &lt;strong&gt;Note that the input node is pre-configured with a question.&lt;/strong&gt; By running the flow, we anticipate that the output node should now provide the result obtained from the LLM when presented with the &lt;em&gt;customer prompt&lt;/em&gt; that was created from the initial question with enhanced customer data and retrieved product context.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To run the flow, click the &lt;code&gt;Run All&lt;/code&gt; (play icon) at the top. When prompted, select &#34;Run it with standard mode&#34;.&lt;/li&gt; &#xA; &lt;li&gt;Watch the console output for execution progress updates&lt;/li&gt; &#xA; &lt;li&gt;On completion, the visual graph nodes should light up (green=success, red=failure).&lt;/li&gt; &#xA; &lt;li&gt;Click any node to open the declarative version showing details of execution&lt;/li&gt; &#xA; &lt;li&gt;Click the &lt;code&gt;Prompt Flow&lt;/code&gt; tab in the Visual Studio Code terminal window for execution times&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For more details on running the prompt flow, &lt;a href=&#34;https://microsoft.github.io/promptflow/how-to-guides/init-and-test-a-flow.html#test-a-flow&#34;&gt;follow the instructions here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Congratulations!! You ran the prompt flow and verified it works!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;6.4 Try other customer inputs (optional)&lt;/h3&gt; &#xA;&lt;p&gt;If you like, you can try out other possible customer inputs to see what the output of the Prompt Flow might be. (This step is optional, and you can skip it if you like.)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;As before, run the flow by clicking the &lt;code&gt;Run All&lt;/code&gt; (play icon) at the top. This time when prompted, select &#34;Run it with interactive mode (text only).&#34;&lt;/li&gt; &#xA; &lt;li&gt;Watch the console output, and when the &#34;User: &#34; prompt appears, enter a question of your choice. The &#34;Bot&#34; response (from the output node) will then appear.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Here are some questions you can try:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;What have I purchased before?&lt;/li&gt; &#xA; &lt;li&gt;What is a good sleeping bag for summer use?&lt;/li&gt; &#xA; &lt;li&gt;How do you clean the CozyNights Sleeping Bag?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;7. Evaluating prompt flow results&lt;/h2&gt; &#xA;&lt;p&gt;Now, we need to understand how well our prompt flow performs using defined metrics like &lt;strong&gt;groundedness&lt;/strong&gt;, &lt;strong&gt;coherence&lt;/strong&gt; etc. To evaluate the prompt flow, we need to be able to compare it to what we see as &#34;good results&#34; in order to understand how well it aligns with our expectations.&lt;/p&gt; &#xA;&lt;p&gt;We may be able to evaluate the flow manually (e.g., using Azure AI Studio) but for now, we&#39;ll evaluate this by running the prompt flow using &lt;strong&gt;gpt-4&lt;/strong&gt; and comparing our performance to the results obtained there. To do this, follow the instructions and steps in the notebook &lt;code&gt;evaluate-chat-prompt-flow.ipynb&lt;/code&gt; under the &lt;code&gt;eval&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;h2&gt;8. Deployment with SDK&lt;/h2&gt; &#xA;&lt;p&gt;At this point, we&#39;ve built, run, and evaluated, the prompt flow &lt;strong&gt;locally&lt;/strong&gt; in our Visual Studio Code environment. We are now ready to deploy the prompt flow to a hosted endpoint on Azure, allowing others to use that endpoint to send &lt;em&gt;user questions&lt;/em&gt; and receive relevant responses.&lt;/p&gt; &#xA;&lt;p&gt;This process consists of the following steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;We push the prompt flow to Azure (effectively uploading flow assets to Azure AI Studio)&lt;/li&gt; &#xA; &lt;li&gt;We activate an automatic runtime and run the uploaded flow once, to verify it works.&lt;/li&gt; &#xA; &lt;li&gt;We deploy the flow, triggering a series of actions that results in a hosted endpoint.&lt;/li&gt; &#xA; &lt;li&gt;We can now use built-in tests on Azure AI Studio to validate the endpoint works as desired.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Just follow the instructions and steps in the notebook &lt;code&gt;push_and_deploy_pf.ipynb&lt;/code&gt; under the &lt;code&gt;deployment&lt;/code&gt; folder. Once this is done, the deployment endpoint and key can be used in any third-party application to &lt;em&gt;integrate&lt;/em&gt; with the deployed flow for real user experiences.&lt;/p&gt; &#xA;&lt;h2&gt;9. Deploy with GitHub Actions&lt;/h2&gt; &#xA;&lt;h3&gt;9.1. Create Connection to Azure in GitHub&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Login to &lt;a href=&#34;https://shell.azure.com/&#34;&gt;Azure Shell&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Follow the instructions to &lt;a href=&#34;hhttps://github.com/microsoft/llmops-promptflow-template/raw/main/docs/github_workflows_how_to_setup.md#create-azure-service-principal&#34;&gt;create a service principal here&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Follow the &lt;a href=&#34;https://github.com/microsoft/llmops-promptflow-template/raw/main/docs/github_workflows_how_to_setup.md#steps&#34;&gt;instructions in steps 1 - 8 here&lt;/a&gt; to add create and add the user-assigned managed identity to the subscription and workspace.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Assign &lt;code&gt;Data Science Role&lt;/code&gt; and the &lt;code&gt;Azure Machine Learning Workspace Connection Secrets Reader&lt;/code&gt; to the service principal. Complete this step in the portal under the IAM.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Setup authentication with Github &lt;a href=&#34;https://github.com/microsoft/llmops-promptflow-template/raw/main/docs/github_workflows_how_to_setup.md#set-up-authentication-with-azure-and-github&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;{&#xA;  &#34;clientId&#34;: &amp;lt;GUID&amp;gt;,&#xA;  &#34;clientSecret&#34;: &amp;lt;GUID&amp;gt;,&#xA;  &#34;subscriptionId&#34;: &amp;lt;GUID&amp;gt;,&#xA;  &#34;tenantId&#34;: &amp;lt;GUID&amp;gt;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add &lt;code&gt;SUBSCRIPTION&lt;/code&gt; (this is the subscription) , &lt;code&gt;GROUP&lt;/code&gt; (this is the resource group name), &lt;code&gt;WORKSPACE&lt;/code&gt; (this is the project name), and &lt;code&gt;KEY_VAULT_NAME&lt;/code&gt; to GitHub.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;9.2. Create a custom environment for endpoint&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Follow the instructions to create a custom env with the packages needed &lt;a href=&#34;https://learn.microsoft.com/en-us/azure/machine-learning/how-to-manage-environments-in-studio?view=azureml-api-2#create-an-environment&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Select the &lt;code&gt;upload existing docker&lt;/code&gt; option&lt;/li&gt; &#xA;   &lt;li&gt;Upload from the folder &lt;code&gt;runtime\docker&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Update the deployment.yml image to the newly created environemnt. You can find the name under &lt;code&gt;Azure container registry&lt;/code&gt; in the environment details page.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href=&#34;https://cla.opensource.microsoft.com&#34;&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; &#xA;&lt;h2&gt;Trademarks&lt;/h2&gt; &#xA;&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href=&#34;https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general&#34;&gt;Microsoft&#39;s Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party&#39;s policies.&lt;/p&gt;</summary>
  </entry>
</feed>