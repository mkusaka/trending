<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-06-14T01:50:40Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>lukepolson/youtube_channel</title>
    <updated>2022-06-14T01:50:40Z</updated>
    <id>tag:github.com,2022-06-14:/lukepolson/youtube_channel</id>
    <link href="https://github.com/lukepolson/youtube_channel" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Notebooks for the python tutorials of my youtube channel. See specific youtube video for link to specifc notebook.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Youtube Channel Repository&lt;/h1&gt; &#xA;&lt;p&gt;Contained here is all the code contained in my videos on youtube. I have a docker image that will make things simple (no need to worry about dependencies when running my code)!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://hub.docker.com/r/lukepolson/psolver_base&#34;&gt;https://hub.docker.com/r/lukepolson/psolver_base&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;To run this Docker image using Docker Desktop (I&#39;m assuming the linux users are already proficient/can follow this and use terminal commands)&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Pull the repository using &lt;code&gt;docker pull lukepolson/psolver_base&lt;/code&gt;. You need to use windows powershell or a MAC terminal&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Open Docker Desktop, go to images, and click run: &lt;img src=&#34;https://raw.githubusercontent.com/lukepolson/youtube_channel/main/images/step1.PNG?raw=true&#34; alt=&#34;Alt text&#34; title=&#34;1&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Configure the optional settings as follows:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lukepolson/youtube_channel/main/images/step1.5.PNG?raw=true&#34; alt=&#34;Alt text&#34; title=&#34;2&#34;&gt; 4. Go to windows powershell, use the command &lt;code&gt;docker ps -a&lt;/code&gt; to see all running containers and look at the ID. Then use &lt;code&gt;docker logs &amp;lt;ID&amp;gt;&lt;/code&gt; to get the logs for that container. Copy the token for the jupyter login. &lt;img src=&#34;https://raw.githubusercontent.com/lukepolson/youtube_channel/main/images/step2.PNG?raw=true&#34; alt=&#34;Alt text&#34; title=&#34;3&#34;&gt;&lt;/p&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;Open up google chrome, type in the search bar &lt;code&gt;localhost:8888/lab&lt;/code&gt; and you will be in the correct environment!&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>fastai/fastai</title>
    <updated>2022-06-14T01:50:40Z</updated>
    <id>tag:github.com,2022-06-14:/fastai/fastai</id>
    <link href="https://github.com/fastai/fastai" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The fastai deep learning library&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Welcome to fastai&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;fastai simplifies training fast and accurate neural nets using modern best practices&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/fastai/fastai/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;CI&#34;&gt; &lt;a href=&#34;https://pypi.org/project/fastai/#description&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/fastai?color=blue&amp;amp;label=pypi%20version&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://anaconda.org/fastai/fastai&#34;&gt;&lt;img src=&#34;https://img.shields.io/conda/vn/fastai/fastai?color=seagreen&amp;amp;label=conda%20version&#34; alt=&#34;Conda (channel only)&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/fastai/docker-containers&#34;&gt;&lt;img src=&#34;https://github.com/fastai/docker-containers/workflows/Build%20fastai%20images/badge.svg?sanitize=true&#34; alt=&#34;Build fastai images&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://github.com/fastai/fastai/workflows/docs/badge.svg?sanitize=true&#34; alt=&#34;docs&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installing&lt;/h2&gt; &#xA;&lt;p&gt;You can use fastai without any installation by using &lt;a href=&#34;https://colab.research.google.com/&#34;&gt;Google Colab&lt;/a&gt;. In fact, every page of this documentation is also available as an interactive notebook - click &#34;Open in colab&#34; at the top of any page to open it (be sure to change the Colab runtime to &#34;GPU&#34; to have it run fast!) See the fast.ai documentation on &lt;a href=&#34;https://course.fast.ai/start_colab&#34;&gt;Using Colab&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;p&gt;You can install fastai on your own machines with conda (highly recommended), as long as you&#39;re running Linux or Windows (NB: Mac is not supported). For Windows, please see the &#34;Running on Windows&#34; for important notes.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;re using &lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;miniconda&lt;/a&gt; (recommended) then run (note that if you replace &lt;code&gt;conda&lt;/code&gt; with &lt;a href=&#34;https://github.com/mamba-org/mamba&#34;&gt;mamba&lt;/a&gt; the install process will be much faster and more reliable):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda install -c fastchan fastai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;...or if you&#39;re using &lt;a href=&#34;https://www.anaconda.com/products/individual&#34;&gt;Anaconda&lt;/a&gt; then run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda install -c fastchan fastai anaconda&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To install with pip, use: &lt;code&gt;pip install fastai&lt;/code&gt;. If you install with pip, you should install PyTorch first by following the PyTorch &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;installation instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you plan to develop fastai yourself, or want to be on the cutting edge, you can use an editable install (if you do this, you should also use an editable install of &lt;a href=&#34;https://github.com/fastai/fastcore&#34;&gt;fastcore&lt;/a&gt; to go with it.) First install PyTorch, and then:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/fastai/fastai&#xA;pip install -e &#34;fastai[dev]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Learning fastai&lt;/h2&gt; &#xA;&lt;p&gt;The best way to get started with fastai (and deep learning) is to read &lt;a href=&#34;https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527&#34;&gt;the book&lt;/a&gt;, and complete &lt;a href=&#34;https://course.fast.ai&#34;&gt;the free course&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To see what&#39;s possible with fastai, take a look at the &lt;a href=&#34;https://docs.fast.ai/quick_start.html&#34;&gt;Quick Start&lt;/a&gt;, which shows how to use around 5 lines of code to build an image classifier, an image segmentation model, a text sentiment model, a recommendation system, and a tabular model. For each of the applications, the code is much the same.&lt;/p&gt; &#xA;&lt;p&gt;Read through the &lt;a href=&#34;https://docs.fast.ai/tutorial&#34;&gt;Tutorials&lt;/a&gt; to learn how to train your own models on your own datasets. Use the navigation sidebar to look through the fastai documentation. Every class, function, and method is documented here.&lt;/p&gt; &#xA;&lt;p&gt;To learn about the design and motivation of the library, read the &lt;a href=&#34;https://www.mdpi.com/2078-2489/11/2/108/htm&#34;&gt;peer reviewed paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;About fastai&lt;/h2&gt; &#xA;&lt;p&gt;fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches. It aims to do both things without substantial compromises in ease of use, flexibility, or performance. This is possible thanks to a carefully layered architecture, which expresses common underlying patterns of many deep learning and data processing techniques in terms of decoupled abstractions. These abstractions can be expressed concisely and clearly by leveraging the dynamism of the underlying Python language and the flexibility of the PyTorch library. fastai includes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A new type dispatch system for Python along with a semantic type hierarchy for tensors&lt;/li&gt; &#xA; &lt;li&gt;A GPU-optimized computer vision library which can be extended in pure Python&lt;/li&gt; &#xA; &lt;li&gt;An optimizer which refactors out the common functionality of modern optimizers into two basic pieces, allowing optimization algorithms to be implemented in 4â€“5 lines of code&lt;/li&gt; &#xA; &lt;li&gt;A novel 2-way callback system that can access any part of the data, model, or optimizer and change it at any point during training&lt;/li&gt; &#xA; &lt;li&gt;A new data block API&lt;/li&gt; &#xA; &lt;li&gt;And much more...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;fastai is organized around two main design goals: to be approachable and rapidly productive, while also being deeply hackable and configurable. It is built on top of a hierarchy of lower-level APIs which provide composable building blocks. This way, a user wanting to rewrite part of the high-level API or add particular behavior to suit their needs does not have to learn how to use the lowest level.&lt;/p&gt; &#xA;&lt;img alt=&#34;Layered API&#34; src=&#34;https://raw.githubusercontent.com/fastai/fastai/master/nbs/images/layered.png&#34; width=&#34;345&#34; style=&#34;max-width: 345px&#34;&gt; &#xA;&lt;h2&gt;Migrating from other libraries&lt;/h2&gt; &#xA;&lt;p&gt;It&#39;s very easy to migrate from plain PyTorch, Ignite, or any other PyTorch-based library, or even to use fastai in conjunction with other libraries. Generally, you&#39;ll be able to use all your existing data processing code, but will be able to reduce the amount of code you require for training, and more easily take advantage of modern best practices. Here are migration guides from some popular libraries to help you on your way:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.fast.ai/migrating_pytorch&#34;&gt;Plain PyTorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.fast.ai/migrating_ignite&#34;&gt;Ignite&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.fast.ai/migrating_lightning&#34;&gt;Lightning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.fast.ai/migrating_catalyst&#34;&gt;Catalyst&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Windows Support&lt;/h2&gt; &#xA;&lt;p&gt;When installing with &lt;code&gt;mamba&lt;/code&gt; or &lt;code&gt;conda&lt;/code&gt; replace &lt;code&gt;-c fastchan&lt;/code&gt; in the installation with &lt;code&gt;-c pytorch -c nvidia -c fastai&lt;/code&gt;, since fastchan is not currently supported on Windows.&lt;/p&gt; &#xA;&lt;p&gt;Due to python multiprocessing issues on Jupyter and Windows, &lt;code&gt;num_workers&lt;/code&gt; of &lt;code&gt;Dataloader&lt;/code&gt; is reset to 0 automatically to avoid Jupyter hanging. This makes tasks such as computer vision in Jupyter on Windows many times slower than on Linux. This limitation doesn&#39;t exist if you use fastai from a script.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/fastai/fastai/raw/master/nbs/examples/dataloader_spawn.py&#34;&gt;this example&lt;/a&gt; to fully leverage the fastai API on Windows.&lt;/p&gt; &#xA;&lt;h2&gt;Tests&lt;/h2&gt; &#xA;&lt;p&gt;To run the tests in parallel, launch:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;nbdev_test_nbs&lt;/code&gt; or &lt;code&gt;make test&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;For all the tests to pass, you&#39;ll need to install the dependencies specified as part of dev_requirements in settings.ini&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install -e .[dev]&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Tests are written using &lt;code&gt;nbdev&lt;/code&gt;, for example see the documentation for &lt;code&gt;test_eq&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;After you clone this repository, please run &lt;code&gt;nbdev_install_git_hooks&lt;/code&gt; in your terminal. This sets up git hooks, which clean up the notebooks to remove the extraneous stuff stored in the notebooks (e.g. which cells you ran) which causes unnecessary merge conflicts.&lt;/p&gt; &#xA;&lt;p&gt;Before submitting a PR, check that the local library and notebooks match. The script &lt;code&gt;nbdev_diff_nbs&lt;/code&gt; can let you know if there is a difference between the local library and the notebooks.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you made a change to the notebooks in one of the exported cells, you can export it to the library with &lt;code&gt;nbdev_build_lib&lt;/code&gt; or &lt;code&gt;make fastai&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;If you made a change to the library, you can export it back to the notebooks with &lt;code&gt;nbdev_update_lib&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Docker Containers&lt;/h2&gt; &#xA;&lt;p&gt;For those interested in official docker containers for this project, they can be found &lt;a href=&#34;https://github.com/fastai/docker-containers#fastai&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>bloc97/Anime4K</title>
    <updated>2022-06-14T01:50:40Z</updated>
    <id>tag:github.com,2022-06-14:/bloc97/Anime4K</id>
    <link href="https://github.com/bloc97/Anime4K" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A High-Quality Real Time Upscaler for Anime Video&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Anime4K&lt;/h1&gt; &#xA;&lt;p&gt;Anime4K is a set of open-source, high-quality real-time anime upscaling/denoising algorithms that can be implemented in any programming language.&lt;/p&gt; &#xA;&lt;p&gt;The simplicity and speed of Anime4K allows the user to watch upscaled anime in real time, as we believe in preserving original content and promoting freedom of choice for all anime fans. Re-encoding anime into 4K should be avoided as it is non-reversible, potentially damages original content by introducing artifacts, takes up to O(n&lt;sup&gt;2&lt;/sup&gt;) more disk space and more importantly, does so without any meaningful decrease in entropy (lost information is lost).&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Disclaimer: All art assets used are for demonstration and educational purposes. All rights are reserved to their original owners. If you (as a person or a company) own the art and do not wish it to be associated with this project, please contact us at &lt;a href=&#34;mailto:anime4k.upscale@gmail.com&#34;&gt;anime4k.upscale@gmail.com&lt;/a&gt; and we will gladly take it down.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Foreword&lt;/h2&gt; &#xA;&lt;p&gt;Anime4K is optimized for &lt;strong&gt;native 1080p anime encoded with h.264, h.265 or VC-1&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Even if it might work, it is &lt;strong&gt;not&lt;/strong&gt; optimized for downscaled 720p, 480p or standard definition anime (eg. DVDs). Older anime (especially pre-digital era production) have artifacts that are very difficult to remove, such as bad deinterlacing, camera blur during production, severe ringing, film grain, older MPEG compression artifacts, etc.&lt;/p&gt; &#xA;&lt;p&gt;This is also not replacement for SRGANs, as they perform much better on low-resolution images or images with lots of degradation (albeit not in real time).&lt;/p&gt; &#xA;&lt;p&gt;What Anime4K does provide is a way to upscale, in real time, 1080p anime for 4K screens while providing a similar &lt;em&gt;effect&lt;/em&gt; to SRGANs and being much better than waifu2x (See &lt;a href=&#34;https://github.com/bloc97/Anime4K/tree/master/results/Comparisons/Screenshots&#34;&gt;comparisons&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;Currently, research is being done on better real-time upscaling for lower resolution or older content.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;strong&gt;Installation Instructions&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bloc97/Anime4K/master/md/GLSL_Instructions_Windows.md&#34;&gt;Windows (GLSL/MPV)&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bloc97/Anime4K/master/md/GLSL_Instructions_Linux.md&#34;&gt;Linux (GLSL/MPV)&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h2&gt;v4.1 Low resolution experiment&lt;/h2&gt; &#xA;&lt;p&gt;Results from the &lt;a href=&#34;https://github.com/bloc97/Anime4K/commit/5f9294d847e724b67f941d1742e3565a0a106291&#34;&gt;experimental SRGAN shaders&lt;/a&gt; for 360p -&amp;gt; 4K: (zoom in to view details)&lt;/p&gt; &#xA;&lt;p&gt;The images are sorted by algorithm speed, bicubic being the fastest. &lt;a href=&#34;https://github.com/igv/FSRCNN-TensorFlow&#34;&gt;FSRCNNX&lt;/a&gt; and Anime4K are real-time while &lt;a href=&#34;https://github.com/nagadomi/waifu2x&#34;&gt;waifu2x&lt;/a&gt; and &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN&#34;&gt;Real-ESRGAN&lt;/a&gt; are not. &lt;img src=&#34;https://raw.githubusercontent.com/bloc97/Anime4K/master/results/Comparisons/Cropped_Screenshots/Magia_360p_4K.png?raw=true&#34; alt=&#34;Comparison&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/bloc97/Anime4K/master/results/Comparisons/Cropped_Screenshots/Higurashi_360p_4K.png?raw=true&#34; alt=&#34;Comparison&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;v4&lt;/h2&gt; &#xA;&lt;p&gt;We introduce a line reconstruction algorithm that aims to tackle the distribution shift problem seen in 1080p anime. In the wild anime exhibit a surprising amount of variance caused by low quality compositing due to budget and time constraints that traditional super-resolution algorithms cannot handle. GANs can implicitly encode this distribution shift but are slow to use and hard to train. Our algorithm explicitly corrects this distribution shift and allows traditional &#34;MSE&#34; SR algorithms to work with a wide variety of anime.&lt;/p&gt; &#xA;&lt;p&gt;Source: &lt;a href=&#34;https://fancaps.net/anime/picture.php?/14728493&#34;&gt;https://fancaps.net/anime/picture.php?/14728493&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/bloc97/Anime4K/master/md/GLSL_Instructions_Advanced.md#modes&#34;&gt;Mode&lt;/a&gt;: &lt;code&gt;B&lt;/code&gt;&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/bloc97/Anime4K/master/results/Comparisons/Cropped_Screenshots/Maxed.png?raw=true&#34; alt=&#34;Comparison&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Source: &lt;a href=&#34;https://fancaps.net/anime/picture.php?/13365760&#34;&gt;https://fancaps.net/anime/picture.php?/13365760&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/bloc97/Anime4K/master/md/GLSL_Instructions_Advanced.md#modes&#34;&gt;Mode&lt;/a&gt;: &lt;code&gt;A&lt;/code&gt;&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/bloc97/Anime4K/master/results/Comparisons/Cropped_Screenshots/Slime.png?raw=true&#34; alt=&#34;Comparison&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Performance numbers are obtained using a Vega64 GPU and are tested using &lt;code&gt;UL&lt;/code&gt; shader variants. The fast version is for &lt;code&gt;M&lt;/code&gt; variants.&lt;br&gt; &lt;em&gt;Note that CUDA accelerated SRGANs/Waifu2x using tensor cores can be much faster and close to realtime (~80ms), but their large size severely hampers non-CUDA implementations.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;v3&lt;/h2&gt; &#xA;&lt;p&gt;The monolithic Anime4K shader is broken into modular components, allowing customization for specific types of anime and/or personal taste. What&#39;s new:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A complete overhaul of the algorithm(s) for speed, quality and efficiency.&lt;/li&gt; &#xA; &lt;li&gt;Real-time, high quality line art CNN upscalers. &lt;em&gt;(6 variants)&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;Line art deblurring shaders. &lt;em&gt;(&#34;blind deconvolution&#34; and DTD shader)&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;Denoising algorithms. &lt;em&gt;(Bilateral Mode and CNN variants)&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;Blind resampling artifact reduction algorithms. &lt;em&gt;(For badly resampled anime.)&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;Experimental line darkening and line thinning algorithm. &lt;em&gt;(For perceptual quality. We perceive thinner/darker lines as perceptually higher quality, even if it might not be the case.)&lt;/em&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/bloc97/Anime4K/wiki&#34;&gt;More information about each shader (OUTDATED)&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Visits&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://count.getloli.com/&#34;&gt;&lt;img src=&#34;https://count.getloli.com/get/@72276803-0571-4e62-b0a7-9880fcd0244f?theme=gelbooru&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;Counting since &lt;code&gt;2021-09-19T16:02:06Z&lt;/code&gt; (ISO 8601)&lt;/p&gt; &#xA;&lt;h2&gt;Projects that use Anime4K&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Blinue/Magpie&#34;&gt;https://github.com/Blinue/Magpie&lt;/a&gt; (General-purpose real-time upscaler for any program/game running on Windows 10)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;Note that the following might be using an outdated version of Anime4K. There have been significant quality improvements since v3.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yeataro/TD-Anime4K&#34;&gt;https://github.com/yeataro/TD-Anime4K&lt;/a&gt; (Anime4K for TouchDesigner)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/keijiro/UnityAnime4K&#34;&gt;https://github.com/keijiro/UnityAnime4K&lt;/a&gt; (Anime4K for Unity)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/net2cn/Anime4KSharp&#34;&gt;https://github.com/net2cn/Anime4KSharp&lt;/a&gt; (Anime4K Re-Implemented in C#)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/andraantariksa/Anime4K-rs&#34;&gt;https://github.com/andraantariksa/Anime4K-rs&lt;/a&gt; (Anime4K Re-Implemented in Rust)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/TianZerL/Anime4KCPP&#34;&gt;https://github.com/TianZerL/Anime4KCPP&lt;/a&gt; (Anime4K &amp;amp; more algorithms implemented in C++)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/k4yt3x/video2x&#34;&gt;https://github.com/k4yt3x/video2x&lt;/a&gt; (Anime Video Upscaling Pipeline)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;OpenCV&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;TensorFlow&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Keras&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Torch&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;mpv&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;MPC&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/opencv.png&#34; alt=&#34;OpenCV&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/tensorflow.png&#34; alt=&#34;TensorFlow&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/keras-team.png&#34; alt=&#34;Keras&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/torch.png&#34; alt=&#34;Torch&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/mpv-player.png&#34; alt=&#34;mpv&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/mpc-hc.png&#34; alt=&#34;MPC&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Many thanks to the &lt;a href=&#34;https://github.com/opencv/opencv&#34;&gt;OpenCV&lt;/a&gt;, &lt;a href=&#34;https://github.com/tensorflow/tensorflow&#34;&gt;TensorFlow&lt;/a&gt;, &lt;a href=&#34;https://github.com/keras-team/keras&#34;&gt;Keras&lt;/a&gt; and &lt;a href=&#34;https://github.com/torch/torch7&#34;&gt;Torch&lt;/a&gt; groups and contributors. This project would not have been possible without the existence of high quality, open source machine learning libraries.&lt;/p&gt; &#xA;&lt;p&gt;I would also want to specially thank the creators of &lt;a href=&#34;https://cv.snu.ac.kr/research/VDSR/&#34;&gt;VDSR&lt;/a&gt; and &lt;a href=&#34;http://mmlab.ie.cuhk.edu.hk/projects/FSRCNN.html&#34;&gt;FSRCNN&lt;/a&gt;, in addition to the open source projects &lt;a href=&#34;https://github.com/nagadomi/waifu2x&#34;&gt;waifu2x&lt;/a&gt; and &lt;a href=&#34;https://github.com/igv/FSRCNN-TensorFlow&#34;&gt;FSRCNNX&lt;/a&gt; for sparking my interest in creating this project. I am also extending my gratitude to the contributors of &lt;a href=&#34;https://github.com/mpv-player/mpv&#34;&gt;mpv&lt;/a&gt; and &lt;a href=&#34;https://mpc-hc.org/&#34;&gt;MPC-HC&lt;/a&gt;/&lt;a href=&#34;https://sourceforge.net/projects/mpcbe/&#34;&gt;BE&lt;/a&gt; for their efforts on creating excellent media players with endless customization options.&lt;br&gt; Furthermore, I want to thank the people who contributed to this project in any form, be it by reporting bugs, submitting suggestions, helping others&#39; issues or submitting code. I will forever hold you in high regard.&lt;/p&gt; &#xA;&lt;p&gt;I also wish to express my sincere gratitude to the people of &lt;a href=&#34;https://www.umontreal.ca/&#34;&gt;UniversitÃ© de MontrÃ©al&lt;/a&gt;, &lt;a href=&#34;https://diro.umontreal.ca/accueil/&#34;&gt;DIRO&lt;/a&gt;, &lt;a href=&#34;http://www.ligum.umontreal.ca/&#34;&gt;LIGUM&lt;/a&gt; and &lt;a href=&#34;https://mila.quebec/en/&#34;&gt;MILA&lt;/a&gt; for providing so many opportunities to students (including me), providing the necessary infrastructure and fostering an excellent learning environment.&lt;/p&gt; &#xA;&lt;p&gt;I would also like to thank the greater open source community, in which the assortment of concrete examples and code were of great help.&lt;/p&gt; &#xA;&lt;p&gt;Finally, but not least, infinite thanks to my family, friends and professors for providing financial, technical, social support and expertise for my ongoing learning journey during these hard times. Your help has been beyond description, really.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;This list is not final, as the project is far from done. Any future acknowledgements will be promptly added.&lt;/em&gt;&lt;/p&gt;</summary>
  </entry>
</feed>