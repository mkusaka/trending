<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-07-05T01:32:24Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>QwenLM/Qwen2.5-Omni</title>
    <updated>2025-07-05T01:32:24Z</updated>
    <id>tag:github.com,2025-07-05:/QwenLM/Qwen2.5-Omni</id>
    <link href="https://github.com/QwenLM/Qwen2.5-Omni" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Qwen2.5-Omni is an end-to-end multimodal model by Qwen team at Alibaba Cloud, capable of understanding text, audio, vision, video, and performing real-time speech generation.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Qwen2.5-Omni&lt;/h1&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/README_CN.md&#34;&gt;‰∏≠Êñá&lt;/a&gt; &amp;nbsp;ÔΩú &amp;nbsp; English&amp;nbsp;&amp;nbsp; &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/Omni_logo.png&#34; width=&#34;400&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; üíú &lt;a href=&#34;https://chat.qwenlm.ai/&#34;&gt;&lt;b&gt;Qwen Chat&lt;/b&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü§ó &lt;a href=&#34;https://huggingface.co/collections/Qwen/qwen25-omni-67de1e5f0f9464dc6314b36e&#34;&gt;Hugging Face&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü§ñ &lt;a href=&#34;https://modelscope.cn/collections/Qwen25-Omni-a2505ce0d5514e&#34;&gt;ModelScope&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üìë &lt;a href=&#34;https://qwenlm.github.io/blog/qwen2.5-omni/&#34;&gt;Blog&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üìö &lt;a href=&#34;https://github.com/QwenLM/Qwen2.5-Omni/tree/main/cookbooks&#34;&gt;Cookbooks&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üìë &lt;a href=&#34;https://arxiv.org/abs/2503.20215&#34;&gt;Paper&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;br&gt; üñ•Ô∏è &lt;a href=&#34;https://huggingface.co/spaces/Qwen/Qwen2.5-Omni-7B-Demo%20&#34;&gt;Demo&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üí¨ &lt;a href=&#34;https://github.com/QwenLM/Qwen/raw/main/assets/wechat.png&#34;&gt;WeChat (ÂæÆ‰ø°)&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü´® &lt;a href=&#34;https://discord.gg/CV4E9rpNSD&#34;&gt;Discord&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üìë &lt;a href=&#34;https://help.aliyun.com/zh/model-studio/user-guide/qwen-omni&#34;&gt;API&lt;/a&gt; &#xA; &lt;!-- &amp;nbsp&amp;nbsp | &amp;nbsp&amp;nbspüñ•Ô∏è &lt;a href=&#34;https://gallery.pai-ml.com/#/preview/deepLearning/cv/qwen2.5-vl&#34;&gt;PAI-DSW&lt;/a&gt; --&gt; &lt;/p&gt; &#xA;&lt;p&gt;We release &lt;strong&gt;Qwen2.5-Omni&lt;/strong&gt;, the new flagship end-to-end multimodal model in the Qwen series. Designed for comprehensive multimodal perception, it seamlessly processes diverse inputs including text, images, audio, and video, while delivering real-time streaming responses through both text generation and natural speech synthesis. Let&#39;s click the video below for more information üòÉ&lt;/p&gt; &#xA;&lt;a href=&#34;https://youtu.be/yKcANdkRuNI&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/video_cover.png&#34; alt=&#34;Open Video&#34;&gt; &lt;/a&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2025.06.12: Qwen2.5-Omni-7B ranked first among open source models in the spoken language understanding and reasoning benchmark &lt;a href=&#34;https://arxiv.org/abs/2506.04779&#34;&gt;MMSU&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;2025.06.09: Congratulations to our open source Qwen2.5-Omni-7B for ranking first in the &lt;a href=&#34;https://sakshi113.github.io/mmau_homepage/#leaderboard&#34;&gt;MMAU&lt;/a&gt; leaderboard, and first in the &lt;a href=&#34;https://github.com/ddlBoJack/MMAR&#34;&gt;MMAR&lt;/a&gt; of open source models in the audio understanding and reasoning evaluation!&lt;/li&gt; &#xA; &lt;li&gt;2025.05.16: We release 4-bit quantized Qwen2.5-Omni-7B (GPTQ-Int4/AWQ) models that maintain comparable performance to the original version on multimodal evaluations while reducing GPU VRAM consumption by over 50%+. See &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#gptq-int4-and-awq-usage&#34;&gt;GPTQ-Int4 and AWQ Usage&lt;/a&gt; for details, and models can be obtained from Hugging Face (&lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Omni-7B-GPTQ-Int4&#34;&gt;GPTQ-Int4&lt;/a&gt;|&lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Omni-7B-AWQ&#34;&gt;AWQ&lt;/a&gt;) and ModelScope (&lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Omni-7B-GPTQ-Int4&#34;&gt;GPTQ-Int4&lt;/a&gt;|&lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Omni-7B-AWQ&#34;&gt;AWQ&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;2025.05.13: &lt;a href=&#34;https://github.com/alibaba/MNN/raw/master/apps/Android/MnnLlmChat/README.md#releases&#34;&gt;MNN Chat App&lt;/a&gt; support Qwen2.5-Omni now, let&#39;s experience Qwen2.5-Omni on the edge devices! Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#deployment-with-mnn&#34;&gt;Deployment with MNN&lt;/a&gt; for information about memory consumption and inference speed benchmarks.&lt;/li&gt; &#xA; &lt;li&gt;2025.04.30: Exciting! We We have released Qwen2.5-Omni-3B to enable more platforms to run Qwen2.5-Omni. The model can be downloaded from &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Omni-3B&#34;&gt;Hugging Face&lt;/a&gt;. The &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#performance&#34;&gt;performance&lt;/a&gt; of this model is updated, and please refer to &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#minimum-gpu-memory-requirements&#34;&gt;Minimum GPU memory requirements&lt;/a&gt; for information about resource consumption. And for best experience, &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#--transformers-usage&#34;&gt;transformers&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#deployment-with-vllm&#34;&gt;vllm&lt;/a&gt; code have update, you can pull the &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#-docker&#34;&gt;official docker&lt;/a&gt; again to get them.&lt;/li&gt; &#xA; &lt;li&gt;2025.04.11: We release the new vllm version which support audio ouput now! Please experience it from source or our docker image.&lt;/li&gt; &#xA; &lt;li&gt;2025.04.02: ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Qwen2.5-Omni reaches top-1 on Hugging Face Trending!&lt;/li&gt; &#xA; &lt;li&gt;2025.03.29: ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Qwen2.5-Omni reaches top-2 on Hugging Face Trending!&lt;/li&gt; &#xA; &lt;li&gt;2025.03.26: Real-time interaction with Qwen2.5-Omni is available on &lt;a href=&#34;https://chat.qwen.ai/&#34;&gt;Qwen Chat&lt;/a&gt;. Let&#39;s start this amazing journey now!&lt;/li&gt; &#xA; &lt;li&gt;2025.03.26: We have released the &lt;a href=&#34;https://huggingface.co/collections/Qwen/qwen25-omni-67de1e5f0f9464dc6314b36e&#34;&gt;Qwen2.5-Omni&lt;/a&gt;. For more details, please check our &lt;a href=&#34;https://qwenlm.github.io/blog/qwen2.5-omni/&#34;&gt;blog&lt;/a&gt;!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contents &#xA; &lt;!-- omit in toc --&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#overview&#34;&gt;Overview&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#key-features&#34;&gt;Key Features&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#model-architecture&#34;&gt;Model Architecture&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#performance&#34;&gt;Performance&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#quickstart&#34;&gt;Quickstart&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#--transformers-usage&#34;&gt;Transformers Usage&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#-modelscope-usage&#34;&gt;ModelScope Usage&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#gptq-int4-and-awq-usage&#34;&gt;GPTQ-Int4 and AWQ Usage&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#usage-tips&#34;&gt;Usage Tips&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#cookbooks-for-more-usage-cases&#34;&gt;Cookbooks for More Usage Cases&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#api-inference&#34;&gt;API inference&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#customization-settings&#34;&gt;Customization Settings&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#chat-with-qwen25-omni&#34;&gt;Chat with Qwen2.5-Omni&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#online-demo&#34;&gt;Online Demo&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#launch-local-web-ui-demo&#34;&gt;Launch Local Web UI Demo&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#real-time-interaction&#34;&gt;Real-Time Interaction&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#deployment-with-vllm&#34;&gt;Deployment with vLLM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#deployment-with-mnn&#34;&gt;Deployment with MNN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#-docker&#34;&gt;Docker&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- - [Citation](#citation) --&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;h3&gt;Introduction&lt;/h3&gt; &#xA;&lt;p&gt;Qwen2.5-Omni is an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/qwen_omni.png&#34; width=&#34;80%&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h3&gt;Key Features&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Omni and Novel Architecture&lt;/strong&gt;: We propose Thinker-Talker architecture, an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner. We propose a novel position embedding, named TMRoPE (Time-aligned Multimodal RoPE), to synchronize the timestamps of video inputs with audio.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Real-Time Voice and Video Chat&lt;/strong&gt;: Architecture designed for fully real-time interactions, supporting chunked input and immediate output.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Natural and Robust Speech Generation&lt;/strong&gt;: Surpassing many existing streaming and non-streaming alternatives, demonstrating superior robustness and naturalness in speech generation.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Strong Performance Across Modalities&lt;/strong&gt;: Exhibiting exceptional performance across all modalities when benchmarked against similarly sized single-modality models. Qwen2.5-Omni outperforms the similarly sized Qwen2-Audio in audio capabilities and achieves comparable performance to Qwen2.5-VL-7B.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Excellent End-to-End Speech Instruction Following&lt;/strong&gt;: Qwen2.5-Omni shows performance in end-to-end speech instruction following that rivals its effectiveness with text inputs, evidenced by benchmarks such as MMLU and GSM8K.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Model Architecture&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/overview.png&#34; width=&#34;80%&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h3&gt;Performance&lt;/h3&gt; &#xA;&lt;p&gt;We conducted a comprehensive evaluation of Qwen2.5-Omni, which demonstrates strong performance across all modalities when compared to similarly sized single-modality models and closed-source models like Qwen2.5-VL-7B, Qwen2-Audio, and Gemini-1.5-pro. In tasks requiring the integration of multiple modalities, such as OmniBench, Qwen2.5-Omni achieves state-of-the-art performance. Furthermore, in single-modality tasks, it excels in areas including speech recognition (Common Voice), translation (CoVoST2), audio understanding (MMAU), image reasoning (MMMU, MMStar), video understanding (MVBench), and speech generation (Seed-tts-eval and subjective naturalness).&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/bar.png&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;details&gt; &#xA; &lt;summary&gt;Multimodality -&amp;gt; Text&lt;/summary&gt; &#xA; &lt;table class=&#34;tg&#34;&gt;&#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th class=&#34;tg-0lax&#34;&gt;Datasets&lt;/th&gt; &#xA;    &lt;th class=&#34;tg-0lax&#34;&gt;Model&lt;/th&gt; &#xA;    &lt;th class=&#34;tg-0lax&#34;&gt;Performance&lt;/th&gt; &#xA;   &lt;/tr&gt;&#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34; rowspan=&#34;10&#34;&gt;OmniBench&lt;br&gt;Speech | Sound Event | Music | Avg&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Gemini-1.5-Pro&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;42.67%|42.26%|46.23%|42.91%&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;MIO-Instruct&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;36.96%|33.58%|11.32%|33.80%&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;AnyGPT (7B)&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;17.77%|20.75%|13.21%|18.04%&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;video-SALMONN&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;34.11%|31.70%|&lt;strong&gt;56.60%&lt;/strong&gt;|35.64%&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;UnifiedIO2-xlarge&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;39.56%|36.98%|29.25%|38.00%&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;UnifiedIO2-xxlarge&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;34.24%|36.98%|24.53%|33.98%&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;MiniCPM-o&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;-|-|-|40.50%&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Baichuan-Omni-1.5&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;-|-|-|42.90%&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen2.5-Omni-3B&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;52.14%|52.08%|52.83%|52.19%&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen2.5-Omni-7B&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;strong&gt;55.25%&lt;/strong&gt;|&lt;strong&gt;60.00%&lt;/strong&gt;|52.83%|&lt;strong&gt;56.13%&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt;&#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Audio -&amp;gt; Text&lt;/summary&gt; &#xA; &lt;table class=&#34;tg&#34;&gt;&#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th class=&#34;tg-0lax&#34;&gt;Datasets&lt;/th&gt; &#xA;    &lt;th class=&#34;tg-0lax&#34;&gt;Model&lt;/th&gt; &#xA;    &lt;th class=&#34;tg-0lax&#34;&gt;Performance&lt;/th&gt; &#xA;   &lt;/tr&gt;&#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-9j4x&#34; colspan=&#34;3&#34;&gt;ASR&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34; rowspan=&#34;12&#34;&gt;Librispeech&lt;br&gt;dev-clean | dev other | test-clean | test-other&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;SALMONN&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;-|-|2.1|4.9&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;SpeechVerse&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;-|-|2.1|4.4&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Whisper-large-v3&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;-|-|1.8|3.6&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Llama-3-8B&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;-|-|-|3.4&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Llama-3-70B&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;-|-|-|3.1&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Seed-ASR-Multilingual&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;-|-|&lt;strong&gt;1.6&lt;/strong&gt;|&lt;strong&gt;2.8&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;MiniCPM-o&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;-|-|1.7|-&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;MinMo&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;-|-|1.7|3.9&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen-Audio&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;1.8|4.0|2.0|4.2&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen2-Audio&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;strong&gt;1.3&lt;/strong&gt;|&lt;strong&gt;3.4&lt;/strong&gt;|&lt;strong&gt;1.6&lt;/strong&gt;|3.6&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen2.5-Omni-3B&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;2.0|4.1|2.2|4.5&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen2.5-Omni-7B&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;1.6|3.5|1.8|3.4&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34; rowspan=&#34;5&#34;&gt;Common Voice 15&lt;br&gt;en | zh | yue | fr&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Whisper-large-v3&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;9.3|12.8|10.9|10.8&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;MinMo&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;7.9|6.3|6.4|8.5&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen2-Audio&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;8.6|6.9|&lt;strong&gt;5.9&lt;/strong&gt;|9.6&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen2.5-Omni-3B&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;9.1|6.0|11.6|9.6&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen2.5-Omni-7B&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;strong&gt;7.6&lt;/strong&gt;|&lt;strong&gt;5.2&lt;/strong&gt;|7.3|&lt;strong&gt;7.5&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34; rowspan=&#34;8&#34;&gt;Fleurs&lt;br&gt;zh | en&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Whisper-large-v3&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;7.7|4.1&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Seed-ASR-Multilingual&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;-|&lt;strong&gt;3.4&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Megrez-3B-Omni&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;10.8|-&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;MiniCPM-o&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;4.4|-&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;MinMo&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;3.0|3.8&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen2-Audio&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;7.5|-&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen2.5-Omni-3B&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;3.2|5.4&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen2.5-Omni-7B&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;strong&gt;3.0&lt;/strong&gt;|4.1&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34; rowspan=&#34;6&#34;&gt;Wenetspeech&lt;br&gt;test-net | test-meeting&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Seed-ASR-Chinese&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;strong&gt;4.7|5.7&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Megrez-3B-Omni&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;-|16.4&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;MiniCPM-o&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;6.9|-&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;MinMo&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;6.8|7.4&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen2.5-Omni-3B&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;6.3|8.1&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen2.5-Omni-7B&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;5.9|7.7&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34; rowspan=&#34;4&#34;&gt;Voxpopuli-V1.0-en&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Llama-3-8B&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;6.2&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Llama-3-70B&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;strong&gt;5.7&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen2.5-Omni-3B&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;6.6&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen2.5-Omni-7B&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;5.8&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-9j4x&#34; colspan=&#34;3&#34;&gt;S2TT&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34; rowspan=&#34;9&#34;&gt;CoVoST2&lt;br&gt;en-de | de-en | en-zh | zh-en&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;SALMONN&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;18.6|-|33.1|-&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;SpeechLLaMA&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;-|27.1|-|12.3&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;BLSP&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;14.1|-|-|-&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;MiniCPM-o&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;-|-|&lt;strong&gt;48.2&lt;/strong&gt;|27.2&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;MinMo&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;-|&lt;strong&gt;39.9&lt;/strong&gt;|46.7|26.0&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen-Audio&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;25.1|33.9|41.5|15.7&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen2-Audio&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;29.9|35.2|45.2|24.4&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen2.5-Omni-3B&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;28.3|38.1|41.4|26.6&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen2.5-Omni-7B&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;strong&gt;30.2&lt;/strong&gt;|37.7|41.4|&lt;strong&gt;29.4&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-9j4x&#34; colspan=&#34;3&#34;&gt;SER&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34; rowspan=&#34;6&#34;&gt;Meld&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;WavLM-large&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;0.542&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;MiniCPM-o&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;0.524&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen-Audio&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;0.557&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen2-Audio&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;0.553&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen2.5-Omni-3B&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;0.558&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen2.5-Omni-7B&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;strong&gt;0.570&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-9j4x&#34; colspan=&#34;3&#34;&gt;VSC&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34; rowspan=&#34;6&#34;&gt;VocalSound&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;CLAP&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;0.495&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Pengi&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;0.604&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen-Audio&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;0.929&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen2-Audio&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;strong&gt;0.939&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen2.5-Omni-3B&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;0.936&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen2.5-Omni-7B&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;strong&gt;0.939&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-9j4x&#34; colspan=&#34;3&#34;&gt;Music&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34; rowspan=&#34;3&#34;&gt;GiantSteps Tempo&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Llark-7B&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;0.86&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen2.5-Omni-3B&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;strong&gt;0.88&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen2.5-Omni-7B&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;strong&gt;0.88&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34; rowspan=&#34;3&#34;&gt;MusicCaps&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;LP-MusicCaps&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;0.291|0.149|0.089|&lt;strong&gt;0.061&lt;/strong&gt;|0.129|0.130&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen2.5-Omni-3B&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;0.325|&lt;strong&gt;0.163&lt;/strong&gt;|&lt;strong&gt;0.093&lt;/strong&gt;|0.057|&lt;strong&gt;0.132&lt;/strong&gt;|&lt;strong&gt;0.229&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen2.5-Omni-7B&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;strong&gt;0.328&lt;/strong&gt;|0.162|0.090|0.055|0.127|0.225&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-9j4x&#34; colspan=&#34;3&#34;&gt;Audio Reasoning&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34; rowspan=&#34;4&#34;&gt;MMAU&lt;br&gt;Sound | Music | Speech | Avg&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Gemini-Pro-V1.5&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;56.75|49.40|58.55|54.90&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen2-Audio&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;54.95|50.98|42.04|49.20&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen2.5-Omni-3B&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;strong&gt;70.27&lt;/strong&gt;|60.48|59.16|63.30&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen2.5-Omni-7B&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;67.87|&lt;strong&gt;69.16|59.76|65.60&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-9j4x&#34; colspan=&#34;3&#34;&gt;Voice Chatting&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34; rowspan=&#34;9&#34;&gt;VoiceBench&lt;br&gt;AlpacaEval | CommonEval | SD-QA | MMSU&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Ultravox-v0.4.1-LLaMA-3.1-8B&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;strong&gt;4.55&lt;/strong&gt;|3.90|53.35|47.17&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;MERaLiON&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;4.50|3.77|55.06|34.95&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Megrez-3B-Omni&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;3.50|2.95|25.95|27.03&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Lyra-Base&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;3.85|3.50|38.25|49.74&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;MiniCPM-o&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;4.42|&lt;strong&gt;4.15&lt;/strong&gt;|50.72|54.78&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Baichuan-Omni-1.5&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;4.50|4.05|43.40|57.25&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen2-Audio&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;3.74|3.43|35.71|35.72&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen2.5-Omni-3B&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;4.32|4.00|49.37|50.23&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen2.5-Omni-7B&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;4.49|3.93|&lt;strong&gt;55.71&lt;/strong&gt;|&lt;strong&gt;61.32&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34; rowspan=&#34;9&#34;&gt;VoiceBench&lt;br&gt;OpenBookQA | IFEval | AdvBench | Avg&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Ultravox-v0.4.1-LLaMA-3.1-8B&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;65.27|&lt;strong&gt;66.88&lt;/strong&gt;|98.46|71.45&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;MERaLiON&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;27.23|62.93|94.81|62.91&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Megrez-3B-Omni&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;28.35|25.71|87.69|46.25&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Lyra-Base&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;72.75|36.28|59.62|57.66&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;MiniCPM-o&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;78.02|49.25|97.69|71.69&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Baichuan-Omni-1.5&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;74.51|54.54|97.31|71.14&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen2-Audio&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;49.45|26.33|96.73|55.35&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen2.5-Omni-3B&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;74.73|42.10|98.85|68.81&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen2.5-Omni-7B&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;strong&gt;81.10&lt;/strong&gt;|52.87|&lt;strong&gt;99.42&lt;/strong&gt;|&lt;strong&gt;74.12&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt;&#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Image -&amp;gt; Text&lt;/summary&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Dataset&lt;/th&gt; &#xA;    &lt;th&gt;Qwen2.5-Omni-7B&lt;/th&gt; &#xA;    &lt;th&gt;Qwen2.5-Omni-3B&lt;/th&gt; &#xA;    &lt;th&gt;Other Best&lt;/th&gt; &#xA;    &lt;th&gt;Qwen2.5-VL-7B&lt;/th&gt; &#xA;    &lt;th&gt;GPT-4o-mini&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;MMMU&lt;sub&gt;val&lt;/sub&gt;&lt;/td&gt; &#xA;    &lt;td&gt;59.2&lt;/td&gt; &#xA;    &lt;td&gt;53.1&lt;/td&gt; &#xA;    &lt;td&gt;53.9&lt;/td&gt; &#xA;    &lt;td&gt;58.6&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;60.0&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;MMMU-Pro&lt;sub&gt;overall&lt;/sub&gt;&lt;/td&gt; &#xA;    &lt;td&gt;36.6&lt;/td&gt; &#xA;    &lt;td&gt;29.7&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;38.3&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;37.6&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;MathVista&lt;sub&gt;testmini&lt;/sub&gt;&lt;/td&gt; &#xA;    &lt;td&gt;67.9&lt;/td&gt; &#xA;    &lt;td&gt;59.4&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;71.9&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;68.2&lt;/td&gt; &#xA;    &lt;td&gt;52.5&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;MathVision&lt;sub&gt;full&lt;/sub&gt;&lt;/td&gt; &#xA;    &lt;td&gt;25.0&lt;/td&gt; &#xA;    &lt;td&gt;20.8&lt;/td&gt; &#xA;    &lt;td&gt;23.1&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;25.1&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;MMBench-V1.1-EN&lt;sub&gt;test&lt;/sub&gt;&lt;/td&gt; &#xA;    &lt;td&gt;81.8&lt;/td&gt; &#xA;    &lt;td&gt;77.8&lt;/td&gt; &#xA;    &lt;td&gt;80.5&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;82.6&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;76.0&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;MMVet&lt;sub&gt;turbo&lt;/sub&gt;&lt;/td&gt; &#xA;    &lt;td&gt;66.8&lt;/td&gt; &#xA;    &lt;td&gt;62.1&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;67.5&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;67.1&lt;/td&gt; &#xA;    &lt;td&gt;66.9&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;MMStar&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;64.0&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;55.7&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;64.0&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;63.9&lt;/td&gt; &#xA;    &lt;td&gt;54.8&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;MME&lt;sub&gt;sum&lt;/sub&gt;&lt;/td&gt; &#xA;    &lt;td&gt;2340&lt;/td&gt; &#xA;    &lt;td&gt;2117&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;2372&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;2347&lt;/td&gt; &#xA;    &lt;td&gt;2003&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;MuirBench&lt;/td&gt; &#xA;    &lt;td&gt;59.2&lt;/td&gt; &#xA;    &lt;td&gt;48.0&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;59.2&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;CRPE&lt;sub&gt;relation&lt;/sub&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;76.5&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;73.7&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;td&gt;76.4&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;RealWorldQA&lt;sub&gt;avg&lt;/sub&gt;&lt;/td&gt; &#xA;    &lt;td&gt;70.3&lt;/td&gt; &#xA;    &lt;td&gt;62.6&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;71.9&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;68.5&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;MME-RealWorld&lt;sub&gt;en&lt;/sub&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;61.6&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;55.6&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;td&gt;57.4&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;MM-MT-Bench&lt;/td&gt; &#xA;    &lt;td&gt;6.0&lt;/td&gt; &#xA;    &lt;td&gt;5.0&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;6.3&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;AI2D&lt;/td&gt; &#xA;    &lt;td&gt;83.2&lt;/td&gt; &#xA;    &lt;td&gt;79.5&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;85.8&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;83.9&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;TextVQA&lt;sub&gt;val&lt;/sub&gt;&lt;/td&gt; &#xA;    &lt;td&gt;84.4&lt;/td&gt; &#xA;    &lt;td&gt;79.8&lt;/td&gt; &#xA;    &lt;td&gt;83.2&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;84.9&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;DocVQA&lt;sub&gt;test&lt;/sub&gt;&lt;/td&gt; &#xA;    &lt;td&gt;95.2&lt;/td&gt; &#xA;    &lt;td&gt;93.3&lt;/td&gt; &#xA;    &lt;td&gt;93.5&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;95.7&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;ChartQA&lt;sub&gt;test Avg&lt;/sub&gt;&lt;/td&gt; &#xA;    &lt;td&gt;85.3&lt;/td&gt; &#xA;    &lt;td&gt;82.8&lt;/td&gt; &#xA;    &lt;td&gt;84.9&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;87.3&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;OCRBench_V2&lt;sub&gt;en&lt;/sub&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;57.8&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;51.7&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;td&gt;56.3&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Dataset&lt;/th&gt; &#xA;    &lt;th&gt;Qwen2.5-Omni-7B&lt;/th&gt; &#xA;    &lt;th&gt;Qwen2.5-Omni-3B&lt;/th&gt; &#xA;    &lt;th&gt;Qwen2.5-VL-7B&lt;/th&gt; &#xA;    &lt;th&gt;Grounding DINO&lt;/th&gt; &#xA;    &lt;th&gt;Gemini 1.5 Pro&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Refcoco&lt;sub&gt;val&lt;/sub&gt;&lt;/td&gt; &#xA;    &lt;td&gt;90.5&lt;/td&gt; &#xA;    &lt;td&gt;88.7&lt;/td&gt; &#xA;    &lt;td&gt;90.0&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;90.6&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;73.2&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Refcoco&lt;sub&gt;textA&lt;/sub&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;93.5&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;91.8&lt;/td&gt; &#xA;    &lt;td&gt;92.5&lt;/td&gt; &#xA;    &lt;td&gt;93.2&lt;/td&gt; &#xA;    &lt;td&gt;72.9&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Refcoco&lt;sub&gt;textB&lt;/sub&gt;&lt;/td&gt; &#xA;    &lt;td&gt;86.6&lt;/td&gt; &#xA;    &lt;td&gt;84.0&lt;/td&gt; &#xA;    &lt;td&gt;85.4&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;88.2&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;74.6&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Refcoco+&lt;sub&gt;val&lt;/sub&gt;&lt;/td&gt; &#xA;    &lt;td&gt;85.4&lt;/td&gt; &#xA;    &lt;td&gt;81.1&lt;/td&gt; &#xA;    &lt;td&gt;84.2&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;88.2&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;62.5&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Refcoco+&lt;sub&gt;textA&lt;/sub&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;91.0&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;87.5&lt;/td&gt; &#xA;    &lt;td&gt;89.1&lt;/td&gt; &#xA;    &lt;td&gt;89.0&lt;/td&gt; &#xA;    &lt;td&gt;63.9&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Refcoco+&lt;sub&gt;textB&lt;/sub&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;79.3&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;73.2&lt;/td&gt; &#xA;    &lt;td&gt;76.9&lt;/td&gt; &#xA;    &lt;td&gt;75.9&lt;/td&gt; &#xA;    &lt;td&gt;65.0&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Refcocog+&lt;sub&gt;val&lt;/sub&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;87.4&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;85.0&lt;/td&gt; &#xA;    &lt;td&gt;87.2&lt;/td&gt; &#xA;    &lt;td&gt;86.1&lt;/td&gt; &#xA;    &lt;td&gt;75.2&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Refcocog+&lt;sub&gt;test&lt;/sub&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;87.9&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;85.1&lt;/td&gt; &#xA;    &lt;td&gt;87.2&lt;/td&gt; &#xA;    &lt;td&gt;87.0&lt;/td&gt; &#xA;    &lt;td&gt;76.2&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;ODinW&lt;/td&gt; &#xA;    &lt;td&gt;42.4&lt;/td&gt; &#xA;    &lt;td&gt;39.2&lt;/td&gt; &#xA;    &lt;td&gt;37.3&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;55.0&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;36.7&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;PointGrounding&lt;/td&gt; &#xA;    &lt;td&gt;66.5&lt;/td&gt; &#xA;    &lt;td&gt;46.2&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;67.3&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Video(without audio) -&amp;gt; Text&lt;/summary&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Dataset&lt;/th&gt; &#xA;    &lt;th&gt;Qwen2.5-Omni-7B&lt;/th&gt; &#xA;    &lt;th&gt;Qwen2.5-Omni-3B&lt;/th&gt; &#xA;    &lt;th&gt;Other Best&lt;/th&gt; &#xA;    &lt;th&gt;Qwen2.5-VL-7B&lt;/th&gt; &#xA;    &lt;th&gt;GPT-4o-mini&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Video-MME&lt;sub&gt;w/o sub&lt;/sub&gt;&lt;/td&gt; &#xA;    &lt;td&gt;64.3&lt;/td&gt; &#xA;    &lt;td&gt;62.0&lt;/td&gt; &#xA;    &lt;td&gt;63.9&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;65.1&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;64.8&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Video-MME&lt;sub&gt;w sub&lt;/sub&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;72.4&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;68.6&lt;/td&gt; &#xA;    &lt;td&gt;67.9&lt;/td&gt; &#xA;    &lt;td&gt;71.6&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;MVBench&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;70.3&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;68.7&lt;/td&gt; &#xA;    &lt;td&gt;67.2&lt;/td&gt; &#xA;    &lt;td&gt;69.6&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;EgoSchema&lt;sub&gt;test&lt;/sub&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;68.6&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;61.4&lt;/td&gt; &#xA;    &lt;td&gt;63.2&lt;/td&gt; &#xA;    &lt;td&gt;65.0&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Zero-shot Speech Generation&lt;/summary&gt; &#xA; &lt;table class=&#34;tg&#34;&gt;&#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th class=&#34;tg-0lax&#34;&gt;Datasets&lt;/th&gt; &#xA;    &lt;th class=&#34;tg-0lax&#34;&gt;Model&lt;/th&gt; &#xA;    &lt;th class=&#34;tg-0lax&#34;&gt;Performance&lt;/th&gt; &#xA;   &lt;/tr&gt;&#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-9j4x&#34; colspan=&#34;3&#34;&gt;Content Consistency&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34; rowspan=&#34;11&#34;&gt;SEED&lt;br&gt;test-zh | test-en | test-hard &lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Seed-TTS_ICL&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;1.11 | 2.24 | 7.58&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Seed-TTS_RL&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;strong&gt;1.00&lt;/strong&gt; | 1.94 | &lt;strong&gt;6.42&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;MaskGCT&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;2.27 | 2.62 | 10.27&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;E2_TTS&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;1.97 | 2.19 | -&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;F5-TTS&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;1.56 | &lt;strong&gt;1.83&lt;/strong&gt; | 8.67&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;CosyVoice 2&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;1.45 | 2.57 | 6.83&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;CosyVoice 2-S&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;1.45 | 2.38 | 8.08&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen2.5-Omni-3B_ICL&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;1.95 | 2.87 | 9.92&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen2.5-Omni-3B_RL&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;1.58 | 2.51 | 7.86&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen2.5-Omni-7B_ICL&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;1.70 | 2.72 | 7.97&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen2.5-Omni-7B_RL&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;1.42 | 2.32 | 6.54&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-9j4x&#34; colspan=&#34;3&#34;&gt;Speaker Similarity&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34; rowspan=&#34;11&#34;&gt;SEED&lt;br&gt;test-zh | test-en | test-hard &lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Seed-TTS_ICL&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;0.796 | 0.762 | 0.776&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Seed-TTS_RL&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;&lt;strong&gt;0.801&lt;/strong&gt; | &lt;strong&gt;0.766&lt;/strong&gt; | &lt;strong&gt;0.782&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;MaskGCT&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;0.774 | 0.714 | 0.748&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;E2_TTS&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;0.730 | 0.710 | -&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;F5-TTS&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;0.741 | 0.647 | 0.713&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;CosyVoice 2&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;0.748 | 0.652 | 0.724&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;CosyVoice 2-S&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;0.753 | 0.654 | 0.732&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen2.5-Omni-3B_ICL&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;0.741 | 0.635 | 0.748&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen2.5-Omni-3B_RL&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;0.744 | 0.635 | 0.746&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen2.5-Omni-7B_ICL&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;0.752 | 0.632 | 0.747&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;Qwen2.5-Omni-7B_RL&lt;/td&gt; &#xA;    &lt;td class=&#34;tg-0lax&#34;&gt;0.754 | 0.641 | 0.752&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt;&#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Text -&amp;gt; Text&lt;/summary&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Dataset&lt;/th&gt; &#xA;    &lt;th&gt;Qwen2.5-Omni-7B&lt;/th&gt; &#xA;    &lt;th&gt;Qwen2.5-Omni-3B&lt;/th&gt; &#xA;    &lt;th&gt;Qwen2.5-7B&lt;/th&gt; &#xA;    &lt;th&gt;Qwen2.5-3B&lt;/th&gt; &#xA;    &lt;th&gt;Qwen2-7B&lt;/th&gt; &#xA;    &lt;th&gt;Llama3.1-8B&lt;/th&gt; &#xA;    &lt;th&gt;Gemma2-9B&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;MMLU-Pro&lt;/td&gt; &#xA;    &lt;td&gt;47.0&lt;/td&gt; &#xA;    &lt;td&gt;40.4&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;56.3&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;43.7&lt;/td&gt; &#xA;    &lt;td&gt;44.1&lt;/td&gt; &#xA;    &lt;td&gt;48.3&lt;/td&gt; &#xA;    &lt;td&gt;52.1&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;MMLU-redux&lt;/td&gt; &#xA;    &lt;td&gt;71.0&lt;/td&gt; &#xA;    &lt;td&gt;60.9&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;75.4&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;64.4&lt;/td&gt; &#xA;    &lt;td&gt;67.3&lt;/td&gt; &#xA;    &lt;td&gt;67.2&lt;/td&gt; &#xA;    &lt;td&gt;72.8&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;LiveBench&lt;sub&gt;0831&lt;/sub&gt;&lt;/td&gt; &#xA;    &lt;td&gt;29.6&lt;/td&gt; &#xA;    &lt;td&gt;22.3&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;35.9&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;26.8&lt;/td&gt; &#xA;    &lt;td&gt;29.2&lt;/td&gt; &#xA;    &lt;td&gt;26.7&lt;/td&gt; &#xA;    &lt;td&gt;30.6&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;GPQA&lt;/td&gt; &#xA;    &lt;td&gt;30.8&lt;/td&gt; &#xA;    &lt;td&gt;34.3&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;36.4&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;30.3&lt;/td&gt; &#xA;    &lt;td&gt;34.3&lt;/td&gt; &#xA;    &lt;td&gt;32.8&lt;/td&gt; &#xA;    &lt;td&gt;32.8&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;MATH&lt;/td&gt; &#xA;    &lt;td&gt;71.5&lt;/td&gt; &#xA;    &lt;td&gt;63.6&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;75.5&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;65.9&lt;/td&gt; &#xA;    &lt;td&gt;52.9&lt;/td&gt; &#xA;    &lt;td&gt;51.9&lt;/td&gt; &#xA;    &lt;td&gt;44.3&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;GSM8K&lt;/td&gt; &#xA;    &lt;td&gt;88.7&lt;/td&gt; &#xA;    &lt;td&gt;82.6&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;91.6&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;86.7&lt;/td&gt; &#xA;    &lt;td&gt;85.7&lt;/td&gt; &#xA;    &lt;td&gt;84.5&lt;/td&gt; &#xA;    &lt;td&gt;76.7&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;HumanEval&lt;/td&gt; &#xA;    &lt;td&gt;78.7&lt;/td&gt; &#xA;    &lt;td&gt;70.7&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;84.8&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;74.4&lt;/td&gt; &#xA;    &lt;td&gt;79.9&lt;/td&gt; &#xA;    &lt;td&gt;72.6&lt;/td&gt; &#xA;    &lt;td&gt;68.9&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;MBPP&lt;/td&gt; &#xA;    &lt;td&gt;73.2&lt;/td&gt; &#xA;    &lt;td&gt;70.4&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;79.2&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;72.7&lt;/td&gt; &#xA;    &lt;td&gt;67.2&lt;/td&gt; &#xA;    &lt;td&gt;69.6&lt;/td&gt; &#xA;    &lt;td&gt;74.9&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;MultiPL-E&lt;/td&gt; &#xA;    &lt;td&gt;65.8&lt;/td&gt; &#xA;    &lt;td&gt;57.6&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;70.4&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;60.2&lt;/td&gt; &#xA;    &lt;td&gt;59.1&lt;/td&gt; &#xA;    &lt;td&gt;50.7&lt;/td&gt; &#xA;    &lt;td&gt;53.4&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;LiveCodeBench&lt;sub&gt;2305-2409&lt;/sub&gt;&lt;/td&gt; &#xA;    &lt;td&gt;24.6&lt;/td&gt; &#xA;    &lt;td&gt;16.5&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;28.7&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;19.9&lt;/td&gt; &#xA;    &lt;td&gt;23.9&lt;/td&gt; &#xA;    &lt;td&gt;8.3&lt;/td&gt; &#xA;    &lt;td&gt;18.9&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;Below, we provide simple examples to show how to use Qwen2.5-Omni with ü§ñ ModelScope and ü§ó Transformers.&lt;/p&gt; &#xA;&lt;p&gt;The codes of Qwen2.5-Omni has been in the latest Hugging face transformers and we advise you to install with command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install transformers==4.52.3&#xA;pip install accelerate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or you might encounter the following error:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;KeyError: &#39;qwen2_5_omni&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and you can also use our &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#-docker&#34;&gt;official docker image&lt;/a&gt; to start without building from source.&lt;/p&gt; &#xA;&lt;p&gt;We offer a toolkit to help you handle various types of audio and visual input more conveniently, as if you were using an API. This includes base64, URLs, and interleaved audio, images and videos. You can install it using the following command and make sure your system has &lt;code&gt;ffmpeg&lt;/code&gt; installed:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# It&#39;s highly recommended to use `[decord]` feature for faster video loading.&#xA;pip install qwen-omni-utils[decord] -U&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are not using Linux, you might not be able to install &lt;code&gt;decord&lt;/code&gt; from PyPI. In that case, you can use &lt;code&gt;pip install qwen-omni-utils -U&lt;/code&gt; which will fall back to using torchvision for video processing. However, you can still &lt;a href=&#34;https://github.com/dmlc/decord?tab=readme-ov-file#install-from-source&#34;&gt;install decord from source&lt;/a&gt; to get decord used when loading video.&lt;/p&gt; &#xA;&lt;p&gt;We are preparing &lt;a href=&#34;https://github.com/QwenLM/Qwen2.5-Omni/tree/main/cookbooks&#34;&gt;cookbooks&lt;/a&gt; for many capabilities, including audio understanding, voice chatting, screen recording interaction, video information extracting, omni chatting and more. Welcome to learn more!&lt;/p&gt; &#xA;&lt;h3&gt;ü§ó Transformers Usage&lt;/h3&gt; &#xA;&lt;p&gt;Here we show a code snippet to show you how to use the chat model with &lt;code&gt;transformers&lt;/code&gt; and &lt;code&gt;qwen_omni_utils&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import soundfile as sf&#xA;&#xA;from transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor&#xA;from qwen_omni_utils import process_mm_info&#xA;&#xA;# default: Load the model on the available device(s)&#xA;model = Qwen2_5OmniForConditionalGeneration.from_pretrained(&#34;Qwen/Qwen2.5-Omni-7B&#34;, torch_dtype=&#34;auto&#34;, device_map=&#34;auto&#34;)&#xA;&#xA;# We recommend enabling flash_attention_2 for better acceleration and memory saving.&#xA;# model = Qwen2_5OmniForConditionalGeneration.from_pretrained(&#xA;#     &#34;Qwen/Qwen2.5-Omni-7B&#34;,&#xA;#     torch_dtype=&#34;auto&#34;,&#xA;#     device_map=&#34;auto&#34;,&#xA;#     attn_implementation=&#34;flash_attention_2&#34;,&#xA;# )&#xA;&#xA;processor = Qwen2_5OmniProcessor.from_pretrained(&#34;Qwen/Qwen2.5-Omni-7B&#34;)&#xA;&#xA;conversation = [&#xA;    {&#xA;        &#34;role&#34;: &#34;system&#34;,&#xA;        &#34;content&#34;: [&#xA;            {&#34;type&#34;: &#34;text&#34;, &#34;text&#34;: &#34;You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.&#34;}&#xA;        ],&#xA;    },&#xA;    {&#xA;        &#34;role&#34;: &#34;user&#34;,&#xA;        &#34;content&#34;: [&#xA;            {&#34;type&#34;: &#34;video&#34;, &#34;video&#34;: &#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/draw.mp4&#34;},&#xA;        ],&#xA;    },&#xA;]&#xA;&#xA;# set use audio in video&#xA;USE_AUDIO_IN_VIDEO = True&#xA;&#xA;# Preparation for inference&#xA;text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)&#xA;audios, images, videos = process_mm_info(conversation, use_audio_in_video=USE_AUDIO_IN_VIDEO)&#xA;inputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors=&#34;pt&#34;, padding=True, use_audio_in_video=USE_AUDIO_IN_VIDEO)&#xA;inputs = inputs.to(model.device).to(model.dtype)&#xA;&#xA;# Inference: Generation of the output text and audio&#xA;text_ids, audio = model.generate(**inputs, use_audio_in_video=USE_AUDIO_IN_VIDEO)&#xA;&#xA;text = processor.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)&#xA;print(text)&#xA;sf.write(&#xA;    &#34;output.wav&#34;,&#xA;    audio.reshape(-1).detach().cpu().numpy(),&#xA;    samplerate=24000,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Minimum GPU memory requirements&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Precision&lt;/th&gt; &#xA;   &lt;th&gt;15(s) Video&lt;/th&gt; &#xA;   &lt;th&gt;30(s) Video&lt;/th&gt; &#xA;   &lt;th&gt;60(s) Video&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-Omni-3B&lt;/td&gt; &#xA;   &lt;td&gt;FP32&lt;/td&gt; &#xA;   &lt;td&gt;89.10 GB&lt;/td&gt; &#xA;   &lt;td&gt;Not Recommend&lt;/td&gt; &#xA;   &lt;td&gt;Not Recommend&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-Omni-3B&lt;/td&gt; &#xA;   &lt;td&gt;BF16&lt;/td&gt; &#xA;   &lt;td&gt;18.38 GB&lt;/td&gt; &#xA;   &lt;td&gt;22.43 GB&lt;/td&gt; &#xA;   &lt;td&gt;28.22 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-Omni-7B&lt;/td&gt; &#xA;   &lt;td&gt;FP32&lt;/td&gt; &#xA;   &lt;td&gt;93.56 GB&lt;/td&gt; &#xA;   &lt;td&gt;Not Recommend&lt;/td&gt; &#xA;   &lt;td&gt;Not Recommend&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-Omni-7B&lt;/td&gt; &#xA;   &lt;td&gt;BF16&lt;/td&gt; &#xA;   &lt;td&gt;31.11 GB&lt;/td&gt; &#xA;   &lt;td&gt;41.85 GB&lt;/td&gt; &#xA;   &lt;td&gt;60.19 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Note: The table above presents the theoretical minimum memory requirements for inference with &lt;code&gt;transformers&lt;/code&gt; and &lt;code&gt;BF16&lt;/code&gt; is test with &lt;code&gt;attn_implementation=&#34;flash_attention_2&#34;&lt;/code&gt;. However, in practice, the actual memory usage is typically at least 1.2 times higher. For more information, see the linked resource &lt;a href=&#34;https://huggingface.co/docs/accelerate/main/en/usage_guides/model_size_estimator&#34;&gt;here&lt;/a&gt;. We are currently planning to develop a version that can perform inference with lower resource consumption requirements so that Qwen2.5-Omni can run on most platforms. Stay tuned!&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Video URL resource usage&lt;/summary&gt; &#xA; &lt;p&gt;Video URL compatibility largely depends on the third-party library version. The details are in the table below. Change the backend by &lt;code&gt;FORCE_QWENVL_VIDEO_READER=torchvision&lt;/code&gt; or &lt;code&gt;FORCE_QWENVL_VIDEO_READER=decord&lt;/code&gt; if you prefer not to use the default one.&lt;/p&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Backend&lt;/th&gt; &#xA;    &lt;th&gt;HTTP&lt;/th&gt; &#xA;    &lt;th&gt;HTTPS&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;torchvision &amp;gt;= 0.19.0&lt;/td&gt; &#xA;    &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;    &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;torchvision &amp;lt; 0.19.0&lt;/td&gt; &#xA;    &lt;td&gt;‚ùå&lt;/td&gt; &#xA;    &lt;td&gt;‚ùå&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;decord&lt;/td&gt; &#xA;    &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;    &lt;td&gt;‚ùå&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Batch inference&lt;/summary&gt; &#xA; &lt;p&gt;The model can batch inputs composed of mixed samples of various types such as text, images, audio and videos as input when &lt;code&gt;return_audio=False&lt;/code&gt; is set. Here is an example.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Sample messages for batch inference&#xA;&#xA;# Conversation with video only&#xA;conversation1 = [&#xA;    {&#xA;        &#34;role&#34;: &#34;system&#34;,&#xA;        &#34;content&#34;: [&#xA;            {&#34;type&#34;: &#34;text&#34;, &#34;text&#34;: &#34;You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.&#34;}&#xA;        ],&#xA;    },&#xA;    {&#xA;        &#34;role&#34;: &#34;user&#34;,&#xA;        &#34;content&#34;: [&#xA;            {&#34;type&#34;: &#34;video&#34;, &#34;video&#34;: &#34;/path/to/video.mp4&#34;},&#xA;        ]&#xA;    }&#xA;]&#xA;&#xA;# Conversation with audio only&#xA;conversation2 = [&#xA;    {&#xA;        &#34;role&#34;: &#34;system&#34;,&#xA;        &#34;content&#34;: [&#xA;            {&#34;type&#34;: &#34;text&#34;, &#34;text&#34;: &#34;You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.&#34;}&#xA;        ],&#xA;    },&#xA;    {&#xA;        &#34;role&#34;: &#34;user&#34;,&#xA;        &#34;content&#34;: [&#xA;            {&#34;type&#34;: &#34;audio&#34;, &#34;audio&#34;: &#34;/path/to/audio.wav&#34;},&#xA;        ]&#xA;    }&#xA;]&#xA;&#xA;# Conversation with pure text&#xA;conversation3 = [&#xA;    {&#xA;        &#34;role&#34;: &#34;system&#34;,&#xA;        &#34;content&#34;: [&#xA;            {&#34;type&#34;: &#34;text&#34;, &#34;text&#34;: &#34;You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.&#34;}&#xA;        ],&#xA;    },&#xA;    {&#xA;        &#34;role&#34;: &#34;user&#34;,&#xA;        &#34;content&#34;: &#34;who are you?&#34;&#xA;    }&#xA;]&#xA;&#xA;&#xA;# Conversation with mixed media&#xA;conversation4 = [&#xA;    {&#xA;        &#34;role&#34;: &#34;system&#34;,&#xA;        &#34;content&#34;: [&#xA;            {&#34;type&#34;: &#34;text&#34;, &#34;text&#34;: &#34;You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.&#34;}&#xA;        ],&#xA;    },&#xA;    {&#xA;        &#34;role&#34;: &#34;user&#34;,&#xA;        &#34;content&#34;: [&#xA;            {&#34;type&#34;: &#34;image&#34;, &#34;image&#34;: &#34;/path/to/image.jpg&#34;},&#xA;            {&#34;type&#34;: &#34;video&#34;, &#34;video&#34;: &#34;/path/to/video.mp4&#34;},&#xA;            {&#34;type&#34;: &#34;audio&#34;, &#34;audio&#34;: &#34;/path/to/audio.wav&#34;},&#xA;            {&#34;type&#34;: &#34;text&#34;, &#34;text&#34;: &#34;What are the elements can you see and hear in these medias?&#34;},&#xA;        ],&#xA;    }&#xA;]&#xA;&#xA;# Combine messages for batch processing&#xA;conversations = [conversation1, conversation2, conversation3, conversation4]&#xA;&#xA;# set use audio in video&#xA;USE_AUDIO_IN_VIDEO = True&#xA;&#xA;# Preparation for batch inference&#xA;text = processor.apply_chat_template(conversations, add_generation_prompt=True, tokenize=False)&#xA;audios, images, videos = process_mm_info(conversations, use_audio_in_video=USE_AUDIO_IN_VIDEO)&#xA;&#xA;inputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors=&#34;pt&#34;, padding=True, use_audio_in_video=USE_AUDIO_IN_VIDEO)&#xA;inputs = inputs.to(model.device).to(model.dtype)&#xA;&#xA;# Batch Inference&#xA;text_ids = model.generate(**inputs, use_audio_in_video=USE_AUDIO_IN_VIDEO, return_audio=False)&#xA;text = processor.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)&#xA;print(text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;ü§ñ ModelScope Usage&lt;/h3&gt; &#xA;&lt;p&gt;We strongly advise users especially those in mainland China to use ModelScope, &lt;code&gt;snapshot_download&lt;/code&gt; can help you solve issues concerning downloading checkpoints.&lt;/p&gt; &#xA;&lt;h3&gt;GPTQ-Int4 and AWQ Usage&lt;/h3&gt; &#xA;&lt;p&gt;To improve the Qwen2.5-Omni-7B&#39;s operability on devices with constrained GPU memory, we implemented 4-bit quantization of the Thinker&#39;s weights using GPTQ and AWQ, effectively reducing GPU VRAM usage. Ohter key optimizations include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Enhanced the inference pipeline to load model weights on-demand for each module and offload them to CPU memory once inference is complete, preventing peak VRAM usage from becoming excessive.&lt;/li&gt; &#xA; &lt;li&gt;Converted the code2wav module to support streaming inference, thereby avoiding the pre-allocation of excessive GPU memory.&lt;/li&gt; &#xA; &lt;li&gt;Adjusted the ODE solver from a second-order (RK4) to a first-order (Euler) method to further decrease computational overhead.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;These improvements aim to ensure efficient performance of Qwen2.5-Omni across a range of hardware configurations, particularly those with lower GPU memory availability (RTX3080, 4080, 5070, etc). Currently, the relevant models and usage methods can be obtained from Hugging Face (&lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Omni-7B-GPTQ-Int4&#34;&gt;GPTQ-Int4&lt;/a&gt;|&lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Omni-7B-AWQ&#34;&gt;AWQ&lt;/a&gt;) and ModelScope (&lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Omni-7B-GPTQ-Int4&#34;&gt;GPTQ-Int4&lt;/a&gt;|&lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Omni-7B-AWQ&#34;&gt;AWQ&lt;/a&gt;). As below, we provide simple example to show how to use Qwen2.5-Omni-7B-GPTQ-Int4 with &lt;code&gt;gptqmodel&lt;/code&gt; as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install transformers==4.52.3&#xA;pip install accelerate&#xA;pip install gptqmodel==2.0.0&#xA;pip install numpy==2.0.0&#xA;&#xA;git clone https://github.com/QwenLM/Qwen2.5-Omni.git&#xA;&#xA;cd Qwen2.5-Omni/low-VRAM-mode/&#xA;&#xA;CUDA_VISIBLE_DEVICES=0 python3 low_VRAM_demo_gptq.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use Qwen2.5-Omni-7B-AWQ with &lt;code&gt;autoawq&lt;/code&gt; please run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install transformers==4.52.3&#xA;pip install accelerate&#xA;pip install autoawq==0.2.9&#xA;&#xA;git clone https://github.com/QwenLM/Qwen2.5-Omni.git&#xA;&#xA;cd Qwen2.5-Omni/low-VRAM-mode/&#xA;&#xA;CUDA_VISIBLE_DEVICES=0 python3 low_VRAM_demo_awq.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The following two tables present a performance comparison and GPU memory consumption between Qwen2.5-Omni-7B-GPTQ-Int4/Qwen2.5-Omni-7B-AWQ and Qwen2.5-Omni-7B on specific evaluation benchmarks. The data demonstrates that the GPTQ-Int4/AWQ model maintains comparable performance while reducing GPU memory requirements by over 50%+, enabling a broader range of devices to run and experience the high-performance Qwen2.5-Omni-7B model. Notably, the GPTQ-Int4/AWQ variant exhibits slightly slower inference speeds compared to the native Qwen2.5-Omni-7B model due to quantization techniques and CPU offload mechanisms.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Evaluation Set&lt;/th&gt; &#xA;   &lt;th&gt;Task&lt;/th&gt; &#xA;   &lt;th&gt;Metrics&lt;/th&gt; &#xA;   &lt;th&gt;Qwen2.5-Omni-7B&lt;/th&gt; &#xA;   &lt;th&gt;Qwen2.5-Omni-7B-GPTQ-Int4&lt;/th&gt; &#xA;   &lt;th&gt;Qwen2.5-Omni-7B-AWQ&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LibriSpeech test-other&lt;/td&gt; &#xA;   &lt;td&gt;ASR&lt;/td&gt; &#xA;   &lt;td&gt;WER ‚¨áÔ∏è&lt;/td&gt; &#xA;   &lt;td&gt;3.4&lt;/td&gt; &#xA;   &lt;td&gt;3.71&lt;/td&gt; &#xA;   &lt;td&gt;3.91&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WenetSpeech test-net&lt;/td&gt; &#xA;   &lt;td&gt;ASR&lt;/td&gt; &#xA;   &lt;td&gt;WER ‚¨áÔ∏è&lt;/td&gt; &#xA;   &lt;td&gt;5.9&lt;/td&gt; &#xA;   &lt;td&gt;6.62&lt;/td&gt; &#xA;   &lt;td&gt;6.31&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Seed-TTS test-hard&lt;/td&gt; &#xA;   &lt;td&gt;TTS (Speaker: Chelsie)&lt;/td&gt; &#xA;   &lt;td&gt;WER ‚¨áÔ∏è&lt;/td&gt; &#xA;   &lt;td&gt;8.7&lt;/td&gt; &#xA;   &lt;td&gt;10.3&lt;/td&gt; &#xA;   &lt;td&gt;8.88&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MMLU-Pro&lt;/td&gt; &#xA;   &lt;td&gt;Text -&amp;gt; Text&lt;/td&gt; &#xA;   &lt;td&gt;Accuracy ‚¨ÜÔ∏è&lt;/td&gt; &#xA;   &lt;td&gt;47.0&lt;/td&gt; &#xA;   &lt;td&gt;43.76&lt;/td&gt; &#xA;   &lt;td&gt;45.66&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OmniBench&lt;/td&gt; &#xA;   &lt;td&gt;Speech -&amp;gt; Text&lt;/td&gt; &#xA;   &lt;td&gt;Accuracy ‚¨ÜÔ∏è&lt;/td&gt; &#xA;   &lt;td&gt;56.13&lt;/td&gt; &#xA;   &lt;td&gt;53.59&lt;/td&gt; &#xA;   &lt;td&gt;54.64&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VideoMME&lt;/td&gt; &#xA;   &lt;td&gt;Multimodality -&amp;gt; Text&lt;/td&gt; &#xA;   &lt;td&gt;Accuracy ‚¨ÜÔ∏è&lt;/td&gt; &#xA;   &lt;td&gt;72.4&lt;/td&gt; &#xA;   &lt;td&gt;68.0&lt;/td&gt; &#xA;   &lt;td&gt;72.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Precision&lt;/th&gt; &#xA;   &lt;th&gt;15(s) Video&lt;/th&gt; &#xA;   &lt;th&gt;30(s) Video&lt;/th&gt; &#xA;   &lt;th&gt;60(s) Video&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-Omni-7B&lt;/td&gt; &#xA;   &lt;td&gt;FP32&lt;/td&gt; &#xA;   &lt;td&gt;93.56 GB&lt;/td&gt; &#xA;   &lt;td&gt;Not Recommend&lt;/td&gt; &#xA;   &lt;td&gt;Not Recommend&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-Omni-7B&lt;/td&gt; &#xA;   &lt;td&gt;BF16&lt;/td&gt; &#xA;   &lt;td&gt;31.11 GB&lt;/td&gt; &#xA;   &lt;td&gt;41.85 GB&lt;/td&gt; &#xA;   &lt;td&gt;60.19 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-Omni-7B&lt;/td&gt; &#xA;   &lt;td&gt;GPTQ-Int4&lt;/td&gt; &#xA;   &lt;td&gt;11.64 GB&lt;/td&gt; &#xA;   &lt;td&gt;17.43 GB&lt;/td&gt; &#xA;   &lt;td&gt;29.51 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-Omni-7B&lt;/td&gt; &#xA;   &lt;td&gt;AWQ&lt;/td&gt; &#xA;   &lt;td&gt;11.77 GB&lt;/td&gt; &#xA;   &lt;td&gt;17.84 GB&lt;/td&gt; &#xA;   &lt;td&gt;30.31 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Usage Tips&lt;/h3&gt; &#xA;&lt;h4&gt;Prompt for audio output&lt;/h4&gt; &#xA;&lt;p&gt;If users need audio output, the system prompt must be set as &#34;You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.&#34;, otherwise the audio output may not work as expected.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;{&#xA;    &#34;role&#34;: &#34;system&#34;,&#xA;    &#34;content&#34;: [&#xA;          {&#34;type&#34;: &#34;text&#34;, &#34;text&#34;: &#34;You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.&#34;}&#xA;    ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Use audio in video&lt;/h4&gt; &#xA;&lt;p&gt;In the process of multimodal interaction, the videos provided by users are often accompanied by audio (such as questions about the content in the video, or sounds generated by certain events in the video). This information is conducive to the model providing a better interactive experience. So we provide the following options for users to decide whether to use audio in video.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# first place, in data preprocessing&#xA;audios, images, videos = process_mm_info(conversations, use_audio_in_video=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# second place, in model processor&#xA;inputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors=&#34;pt&#34;, &#xA;                   padding=True, use_audio_in_video=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#  third place, in model inference&#xA;text_ids, audio = model.generate(**inputs, use_audio_in_video=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It is worth noting that during a multi-round conversation, the &lt;code&gt;use_audio_in_video&lt;/code&gt; parameter in these places must be set to the same, otherwise unexpected results will occur.&lt;/p&gt; &#xA;&lt;h4&gt;Use audio output or not&lt;/h4&gt; &#xA;&lt;p&gt;The model supports both text and audio outputs, if users do not need audio outputs, they can call &lt;code&gt;model.disable_talker()&lt;/code&gt; after init the model. This option will save about &lt;code&gt;2GB&lt;/code&gt; of GPU memory but the &lt;code&gt;return_audio&lt;/code&gt; option for &lt;code&gt;generate&lt;/code&gt; function will only allow to be set at &lt;code&gt;False&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = Qwen2_5OmniForConditionalGeneration.from_pretrained(&#xA;    &#34;Qwen/Qwen2.5-Omni-7B&#34;,&#xA;    torch_dtype=&#34;auto&#34;,&#xA;    device_map=&#34;auto&#34;&#xA;)&#xA;model.disable_talker()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In order to obtain a flexible experience, we recommend that users can decide whether to return audio when &lt;code&gt;generate&lt;/code&gt; function is called. If &lt;code&gt;return_audio&lt;/code&gt; is set to &lt;code&gt;False&lt;/code&gt;, the model will only return text outputs to get text responses faster.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = Qwen2_5OmniForConditionalGeneration.from_pretrained(&#xA;    &#34;Qwen/Qwen2.5-Omni-7B&#34;,&#xA;    torch_dtype=&#34;auto&#34;,&#xA;    device_map=&#34;auto&#34;&#xA;)&#xA;...&#xA;text_ids = model.generate(**inputs, return_audio=False)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Change voice type of output audio&lt;/h4&gt; &#xA;&lt;p&gt;Qwen2.5-Omni supports the ability to change the voice of the output audio. The &lt;code&gt;&#34;Qwen/Qwen2.5-Omni-7B&#34;&lt;/code&gt; checkpoint supports two voice types as follows:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Voice Type&lt;/th&gt; &#xA;   &lt;th&gt;Gender&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chelsie&lt;/td&gt; &#xA;   &lt;td&gt;Female&lt;/td&gt; &#xA;   &lt;td&gt;A honeyed, velvety voice that carries a gentle warmth and luminous clarity.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ethan&lt;/td&gt; &#xA;   &lt;td&gt;Male&lt;/td&gt; &#xA;   &lt;td&gt;A bright, upbeat voice with infectious energy and a warm, approachable vibe.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Users can use the &lt;code&gt;speaker&lt;/code&gt; parameter of &lt;code&gt;generate&lt;/code&gt; function to specify the voice type. By defalut, if &lt;code&gt;speaker&lt;/code&gt; is not specified, the default voice type is &lt;code&gt;Chelsie&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;text_ids, audio = model.generate(**inputs, speaker=&#34;Chelsie&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;text_ids, audio = model.generate(**inputs, speaker=&#34;Ethan&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Flash-Attention 2 to speed up generation&lt;/h4&gt; &#xA;&lt;p&gt;First, make sure to install the latest version of Flash Attention 2:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -U flash-attn --no-build-isolation&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Also, you should have hardware that is compatible with FlashAttention 2. Read more about it in the official documentation of the &lt;a href=&#34;https://github.com/Dao-AILab/flash-attention&#34;&gt;flash attention repository&lt;/a&gt;. FlashAttention-2 can only be used when a model is loaded in &lt;code&gt;torch.float16&lt;/code&gt; or &lt;code&gt;torch.bfloat16&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To load and run a model using FlashAttention-2, add &lt;code&gt;attn_implementation=&#34;flash_attention_2&#34;&lt;/code&gt; when loading the model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import Qwen2_5OmniForConditionalGeneration&#xA;&#xA;model = Qwen2_5OmniForConditionalGeneration.from_pretrained(&#xA;    &#34;Qwen/Qwen2.5-Omni-7B&#34;,&#xA;    device_map=&#34;auto&#34;,&#xA;    torch_dtype=torch.bfloat16,&#xA;    attn_implementation=&#34;flash_attention_2&#34;,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Cookbooks for More Usage Cases&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Cookbook&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Open&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/QwenLM/Qwen2.5-Omni/raw/main/cookbooks/universal_audio_understanding.ipynb&#34;&gt;Universal Audio Understanding&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Speech recongnition, speech-to-text translation and audio analysis.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/universal_audio_understanding.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/QwenLM/Qwen2.5-Omni/raw/main/cookbooks/voice_chatting.ipynb&#34;&gt;Voice Chatting&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Chatting with Qwen2.5-Omni by voice input and output.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/voice_chatting.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/QwenLM/Qwen2.5-Omni/raw/main/cookbooks/screen_recording_interaction.ipynb&#34;&gt;Screen Recording Interaction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Get the information and content you want to know by asking questions in real time on the recording screen.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/screen_recording_interaction.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/QwenLM/Qwen2.5-Omni/raw/main/cookbooks/video_information_extracting.ipynb&#34;&gt;Video Information Extracting&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Obtaining information from the video stream.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/video_information_extracting.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/QwenLM/Qwen2.5-Omni/raw/main/cookbooks/omni_chatting_for_music.ipynb&#34;&gt;Omni Chatting for Music&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Chat with Qwen2.5-Omni about music content in a audio and video stream.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/omni_chatting_for_music.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/QwenLM/Qwen2.5-Omni/raw/main/cookbooks/omni_chatting_for_math.ipynb&#34;&gt;Omni Chatting for Math&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Chat with Qwen2.5-Omni about math content in a audio and video stream.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/omni_chatting_for_math.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/QwenLM/Qwen2.5-Omni/raw/main/cookbooks/multi_round_omni_chatting.ipynb&#34;&gt;Multi Round Omni Chatting&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Conducted multiple rounds of audio and video dialogues with Qwen2.5-Omni to provide the most comprehensive ability demonstration.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/multi_round_omni_chatting.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;API Inference&lt;/h3&gt; &#xA;&lt;p&gt;To explore Qwen2.5-Omni, we encourage you to test our cutting-edge API service for a faster and efficient experience.&lt;/p&gt; &#xA;&lt;h4&gt;Installation&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install openai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Examples&lt;/h4&gt; &#xA;&lt;p&gt;You can use the OpenAI API service to interact with Qwen2.5-Omni like below. And for more usage, please refer to the tutorial at &lt;a href=&#34;https://help.aliyun.com/zh/model-studio/user-guide/qwen-omni&#34;&gt;aliyun&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import base64&#xA;import numpy as np&#xA;import soundfile as sf&#xA;&#xA;from openai import OpenAI&#xA;&#xA;client = OpenAI(&#xA;    api_key=&#34;your_api_key&#34;,&#xA;    base_url=&#34;https://dashscope.aliyuncs.com/compatible-mode/v1&#34;,&#xA;)&#xA;&#xA;messages = [&#xA;    {&#xA;        &#34;role&#34;: &#34;system&#34;,&#xA;        &#34;content&#34;: &#34;You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.&#34;,&#xA;    },&#xA;    {&#xA;        &#34;role&#34;: &#34;user&#34;,&#xA;        &#34;content&#34;: [&#xA;            {&#34;type&#34;: &#34;video_url&#34;, &#34;video_url&#34;: &#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/draw.mp4&#34;},&#xA;        ],&#xA;    },&#xA;]&#xA;&#xA;# Qwen-Omni only supports stream mode&#xA;completion = client.chat.completions.create(&#xA;    model=&#34;qwen-omni-turbo&#34;,&#xA;    messages=messages,&#xA;    modalities=[&#34;text&#34;, &#34;audio&#34;],&#xA;    audio={&#xA;        &#34;voice&#34;: &#34;Cherry&#34;, # Cherry, Ethan, Serena, Chelsie is available&#xA;        &#34;format&#34;: &#34;wav&#34;&#xA;    },&#xA;    stream=True,&#xA;    stream_options={&#34;include_usage&#34;: True}&#xA;)&#xA;&#xA;text = []&#xA;audio_string = &#34;&#34;&#xA;for chunk in completion:&#xA;    if chunk.choices:&#xA;        if hasattr(chunk.choices[0].delta, &#34;audio&#34;):&#xA;            try:&#xA;                audio_string += chunk.choices[0].delta.audio[&#34;data&#34;]&#xA;            except Exception as e:&#xA;                text.append(chunk.choices[0].delta.audio[&#34;transcript&#34;])&#xA;    else:&#xA;        print(chunk.usage)&#xA;&#xA;print(&#34;&#34;.join(text))&#xA;wav_bytes = base64.b64decode(audio_string)&#xA;wav_array = np.frombuffer(wav_bytes, dtype=np.int16)&#xA;sf.write(&#34;output.wav&#34;, wav_array, samplerate=24000)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Customization Settings&lt;/h3&gt; &#xA;&lt;p&gt;Since Qwen2.5-Omni does not support prompt settings when using &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#prompt-for-audio-output&#34;&gt;audio output&lt;/a&gt; (including local deployment and API inference), we suggest that if you need to control the output of the model or modify the personality settings of the model, you can try adding similar content to the conversation template as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;conversation = [&#xA;    {&#xA;        &#34;role&#34;: &#34;user&#34;,&#xA;        &#34;content&#34;: [&#xA;            {&#34;type&#34;: &#34;text&#34;, &#34;text&#34;: &#34;You are a shopping guide, now responsible for introducing various products.&#34;},&#xA;        ],&#xA;    },&#xA;    {&#xA;        &#34;role&#34;: &#34;assistant&#34;,&#xA;        &#34;content&#34;: [&#xA;            {&#34;type&#34;: &#34;text&#34;, &#34;text&#34;: &#34;Sure, I got it.&#34;},&#xA;        ],&#xA;    },&#xA;    {&#xA;        &#34;role&#34;: &#34;user&#34;,&#xA;        &#34;content&#34;: [&#xA;            {&#34;type&#34;: &#34;text&#34;, &#34;text&#34;: &#34;Who are you?&#34;},&#xA;        ],&#xA;    },&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Chat with Qwen2.5-Omni&lt;/h2&gt; &#xA;&lt;h3&gt;Online Demo&lt;/h3&gt; &#xA;&lt;p&gt;Without deployment, you can experience online web demo directly by visiting our &lt;a href=&#34;https://huggingface.co/spaces/Qwen/Qwen2.5-Omni-7B-Demo&#34;&gt;Hugginface Spaces&lt;/a&gt; and &lt;a href=&#34;https://modelscope.cn/studios/Qwen/Qwen2.5-Omni-Demo&#34;&gt;Modelscope Studio&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Launch Local Web UI Demo&lt;/h3&gt; &#xA;&lt;p&gt;In this section, we provide instructions for users to build a web-based user interface (UI) demo. This UI demo allows users to interact with a predefined model or application through a web browser. Follow the steps below to get started or you can launch the web demo directly from our &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#-docker&#34;&gt;official docker image&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Installation&lt;/h4&gt; &#xA;&lt;p&gt;Before you begin, ensure that you have the required dependencies installed on your system. You can install them by running the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements_web_demo.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Running the Demo with FlashAttention-2&lt;/h4&gt; &#xA;&lt;p&gt;Once the required packages are installed, you can launch the web demo using the following command. This command will start a web server and provide you with a link to access the UI in your web browser.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Recommended&lt;/strong&gt;: For enhanced performance and efficiency, especially in multi-image and video processing scenarios, we strongly recommend using &lt;a href=&#34;https://github.com/Dao-AILab/flash-attention&#34;&gt;FlashAttention-2&lt;/a&gt;. FlashAttention-2 provides significant improvements in memory usage and speed, making it ideal for handling large-scale models and data processing.&lt;/p&gt; &#xA;&lt;p&gt;To enable FlashAttention-2, use the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# default for Qwen2.5-Omni-7B&#xA;python web_demo.py --flash-attn2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# for Qwen2.5-Omni-3B&#xA;python web_demo.py --flash-attn2 -c Qwen/Qwen2.5-Omni-3B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will load the model with FlashAttention-2 enabled.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Default Usage&lt;/strong&gt;: If you prefer to run the demo without FlashAttention-2 or if you do not specify the &lt;code&gt;--flash-attn2&lt;/code&gt; option, the demo will load the model using the standard attention implementation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# default for Qwen2.5-Omni-7B&#xA;python web_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# for Qwen2.5-Omni-3B&#xA;python web_demo.py -c Qwen/Qwen2.5-Omni-3B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After running the command, you‚Äôll see a link generated in the terminal similar to this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Running on local: http://127.0.0.1:7860/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Copy this link and paste it into your browser to access the web UI, where you can interact with the model by inputting text, uploading audios/images/videos, changing voice type or using any other provided functionalities.&lt;/p&gt; &#xA;&lt;h3&gt;Real-Time Interaction&lt;/h3&gt; &#xA;&lt;p&gt;The streaming Real-time interaction with Qwen2.5-Omni is available now, please visit &lt;a href=&#34;https://chat.qwen.ai/&#34;&gt;Qwen Chat&lt;/a&gt; and select the voice/video calls in the chat box to experience.&lt;/p&gt; &#xA;&lt;h2&gt;Deployment with vLLM&lt;/h2&gt; &#xA;&lt;p&gt;We recommend using vLLM for fast Qwen2.5-Omni deployment and inference. You need to install from our provided &lt;a href=&#34;https://github.com/fyabc/vllm/tree/qwen2_omni_public&#34;&gt;source&lt;/a&gt; to get vLLM support for Qwen2.5-Omni or use our &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#-docker&#34;&gt;official docker image&lt;/a&gt;. You can also check &lt;a href=&#34;https://docs.vllm.ai/en/latest/serving/multimodal_inputs.html&#34;&gt;vLLM official documentation&lt;/a&gt; for more details about online serving and offline inference.&lt;/p&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone -b qwen2_omni_public https://github.com/fyabc/vllm.git&#xA;cd vllm&#xA;git checkout de8f43fbe9428b14d31ac5ec45d065cd3e5c3ee0&#xA;pip install setuptools_scm torchdiffeq resampy x_transformers qwen-omni-utils accelerate&#xA;pip install -r requirements/cuda.txt&#xA;pip install --upgrade setuptools wheel&#xA;pip install .&#xA;pip install transformers==4.52.3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Inference Local&lt;/h3&gt; &#xA;&lt;p&gt;You can use vLLM to inference Qwen2.5-Omni locally, we provide example in &lt;a href=&#34;https://github.com/fyabc/vllm/raw/qwen2_omni_public/examples/offline_inference/qwen2_5_omni/end2end.py&#34;&gt;vLLM repo&lt;/a&gt; which can generate audio output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# git clone -b qwen2_omni_public https://github.com/fyabc/vllm.git&#xA;# cd vllm&#xA;# git checkout de8f43fbe9428b14d31ac5ec45d065cd3e5c3ee0&#xA;# cd examples/offline_inference/qwen2_5_omni/&#xA;&#xA;# only text output for single GPU&#xA;python end2end.py --model Qwen/Qwen2.5-Omni-7B --prompt audio-in-video-v2 --enforce-eager --thinker-only&#xA;&#xA;# only text output for multi GPUs (example in 4 GPUs)&#xA;python end2end.py --model Qwen/Qwen2.5-Omni-7B --prompt audio-in-video-v2 --enforce-eager --thinker-only --thinker-devices [0,1,2,3] --thinker-gpu-memory-utilization 0.9 &#xA;&#xA;# audio output for single GPU&#xA;python end2end.py --model Qwen/Qwen2.5-Omni-7B --prompt audio-in-video-v2 --enforce-eager --do-wave --voice-type Chelsie --warmup-voice-type Chelsie --output-dir output_wav&#xA;&#xA;# audio output for multi GPUs (example in 4 GPUs)&#xA;python end2end.py --model Qwen/Qwen2.5-Omni-7B --prompt audio-in-video-v2 --enforce-eager --do-wave --voice-type Chelsie --warmup-voice-type Chelsie --thinker-devices [0,1] --talker-devices [2] --code2wav-devices [3] --thinker-gpu-memory-utilization 0.9 --talker-gpu-memory-utilization 0.9 --output-dir output_wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;vLLM Serve usage&lt;/h3&gt; &#xA;&lt;p&gt;You can also use vLLM serve through &lt;code&gt;pip install vllm&amp;gt;=0.8.5.post1&lt;/code&gt;, and vLLM serve for Qwen2.5-Omni only supports thinker now, meaning only text output is supported. You can start vLLM servev through the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# for single GPU&#xA;vllm serve /path/to/Qwen2.5-Omni-7B/ --port 8000 --host 127.0.0.1 --dtype bfloat16&#xA;# for multi GPUs (example in 4 GPUs)&#xA;vllm serve /path/to/Qwen2.5-Omni-7B/ --port 8000 --host 127.0.0.1 --dtype bfloat16 -tp 4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you can use the chat API as below (via curl for example):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl http://localhost:8000/v1/chat/completions \&#xA;    -H &#34;Content-Type: application/json&#34; \&#xA;    -d &#39;{&#xA;    &#34;messages&#34;: [&#xA;    {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: &#34;You are a helpful assistant.&#34;},&#xA;    {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: [&#xA;        {&#34;type&#34;: &#34;image_url&#34;, &#34;image_url&#34;: {&#34;url&#34;: &#34;https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png&#34;}},&#xA;        {&#34;type&#34;: &#34;audio_url&#34;, &#34;audio_url&#34;: {&#34;url&#34;: &#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/cough.wav&#34;}},&#xA;        {&#34;type&#34;: &#34;text&#34;, &#34;text&#34;: &#34;What is the text in the illustrate ans what it the sound in the audio?&#34;}&#xA;    ]}&#xA;    ]&#xA;    }&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Deployment with MNN&lt;/h2&gt; &#xA;&lt;p&gt;Qwen2.5-Omni is now supported in MNN, enabling deployment on edge devices. The MNN models for Qwen2.5-Omni are available for download through Hugging Face (&lt;a href=&#34;https://huggingface.co/taobao-mnn/Qwen2.5-Omni-7B-MNN&#34;&gt;7B&lt;/a&gt;|&lt;a href=&#34;https://huggingface.co/taobao-mnn/Qwen2.5-Omni-3B-MNN&#34;&gt;3B&lt;/a&gt;) and ModelScope (&lt;a href=&#34;https://modelscope.cn/models/MNN/Qwen2.5-Omni-7B-MNN&#34;&gt;7B&lt;/a&gt;|&lt;a href=&#34;https://modelscope.cn/models/MNN/Qwen2.5-Omni-3B-MNN&#34;&gt;3B&lt;/a&gt;), along with usage instructions. For detailed information, you can visit &lt;a href=&#34;https://github.com/alibaba/MNN&#34;&gt;MNN&lt;/a&gt; to learn about it.&lt;/p&gt; &#xA;&lt;p&gt;The table below shows memory consumption and inference speed benchmarks for the Qwen2.5-Omni MNN implementation across various mobile SoC platforms.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Platform&lt;/th&gt; &#xA;   &lt;th&gt;Snapdragon 8 Gen 1&lt;/th&gt; &#xA;   &lt;th&gt;Snapdragon 8 Elite&lt;/th&gt; &#xA;   &lt;th&gt;Snapdragon 8 Gen 1&lt;/th&gt; &#xA;   &lt;th&gt;Snapdragon 8 Elite&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Model Size&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;3B&lt;/td&gt; &#xA;   &lt;td&gt;3B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Memory Peak&lt;/td&gt; &#xA;   &lt;td&gt;5.8G&lt;/td&gt; &#xA;   &lt;td&gt;5.8G&lt;/td&gt; &#xA;   &lt;td&gt;3.6G&lt;/td&gt; &#xA;   &lt;td&gt;3.6G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Thinker Prefill Speed&lt;/td&gt; &#xA;   &lt;td&gt;25.58 tok/s&lt;/td&gt; &#xA;   &lt;td&gt;46.32 tok/s&lt;/td&gt; &#xA;   &lt;td&gt;54.31 tok/s&lt;/td&gt; &#xA;   &lt;td&gt;55.16 tok/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Thinker Decode Speed&lt;/td&gt; &#xA;   &lt;td&gt;8.35 tok/s&lt;/td&gt; &#xA;   &lt;td&gt;11.52 tok/s&lt;/td&gt; &#xA;   &lt;td&gt;15.84 tok/s&lt;/td&gt; &#xA;   &lt;td&gt;23.31 tok/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Talker Prefill Speed&lt;/td&gt; &#xA;   &lt;td&gt;17.21 tok/s&lt;/td&gt; &#xA;   &lt;td&gt;97.77 tok/s&lt;/td&gt; &#xA;   &lt;td&gt;34.58 tok/s&lt;/td&gt; &#xA;   &lt;td&gt;217.82 tok/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Talker Decode Speed&lt;/td&gt; &#xA;   &lt;td&gt;18.75 tok/s&lt;/td&gt; &#xA;   &lt;td&gt;38.65 tok/s&lt;/td&gt; &#xA;   &lt;td&gt;51.90 tok/s&lt;/td&gt; &#xA;   &lt;td&gt;62.34 tok/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Code2Wav Speed&lt;/td&gt; &#xA;   &lt;td&gt;20.83 tok/s&lt;/td&gt; &#xA;   &lt;td&gt;27.36 tok/s&lt;/td&gt; &#xA;   &lt;td&gt;28.45 tok/s&lt;/td&gt; &#xA;   &lt;td&gt;27.36 tok/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;üê≥ Docker&lt;/h2&gt; &#xA;&lt;p&gt;To simplify the deploy process, we provide docker images with pre-build environments: &lt;a href=&#34;https://hub.docker.com/r/qwenllm/qwen-omni&#34;&gt;qwenllm/qwen-omni&lt;/a&gt;. You only need to install the driver and download model files to launch demos.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run --gpus all --ipc=host --network=host --rm --name qwen2.5-omni -it qwenllm/qwen-omni:2.5-cu121 bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And you can also launch the web demo by:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash docker/docker_web_demo.sh --checkpoint /path/to/Qwen2.5-Omni-7B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To enable FlashAttention-2, use the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash docker/docker_web_demo.sh --checkpoint /path/to/Qwen2.5-Omni-7B --flash-attn2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our paper and code useful in your research, please consider giving a star &lt;span&gt;‚≠ê&lt;/span&gt; and citation &lt;span&gt;üìù&lt;/span&gt; :)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-BibTeX&#34;&gt;&#xA;@article{Qwen2.5-Omni,&#xA;  title={Qwen2.5-Omni Technical Report},&#xA;  author={Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, Bin Zhang, Xiong Wang, Yunfei Chu, Junyang Lin},&#xA;  journal={arXiv preprint arXiv:2503.20215},&#xA;  year={2025}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt;</summary>
  </entry>
</feed>