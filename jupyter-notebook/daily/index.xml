<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-21T01:29:53Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>topoteretes/cognee</title>
    <updated>2024-03-21T01:29:53Z</updated>
    <id>tag:github.com,2024-03-21:/topoteretes/cognee</id>
    <link href="https://github.com/topoteretes/cognee" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Memory management for the AI Applications and AI Agents&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;cognee&lt;/h1&gt; &#xA;&lt;p&gt;Make data processing for LLMs easy&lt;/p&gt; &#xA;&lt;p&gt; &lt;a href=&#34;https://cognee.ai&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/topoteretes/cognee/main/assets/cognee-logo.png&#34; width=&#34;160px&#34; alt=&#34;Cognee logo&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt; &lt;i&gt;Open-source framework for creating knowledge graphs and data models for LLMs.&lt;/i&gt; &lt;/p&gt; &#xA;&lt;p&gt; &lt;a href=&#34;https://github.com/topoteretes/cognee/fork&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/forks/topoteretes/cognee?style=for-the-badge&#34; alt=&#34;cognee forks&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/topoteretes/cognee/stargazers&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/stars/topoteretes/cognee?style=for-the-badge&#34; alt=&#34;cognee stars&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/topoteretes/cognee/pulls&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/issues-pr/topoteretes/cognee?style=for-the-badge&#34; alt=&#34;cognee pull-requests&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/topoteretes/cognee/releases&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/release/topoteretes/cognee?&amp;amp;label=Latest&amp;amp;style=for-the-badge&#34; alt=&#34;cognee releases&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;🚀 It&#39;s alive&lt;/h2&gt; &#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p&gt;Try it yourself on Whatsapp with one of our &lt;a href=&#34;https://keepi.ai&#34;&gt;partners&lt;/a&gt; by typing &lt;code&gt;/save {content you want to save}&lt;/code&gt; followed by &lt;code&gt;/query {knowledge you saved previously}&lt;/code&gt; For more info here are the &lt;a href=&#34;https://topoteretes.github.io/cognee/&#34;&gt;docs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h2&gt;📦 Installation&lt;/h2&gt; &#xA;&lt;p&gt;With pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install cognee&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;With poetry:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry add cognee&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;💻 Usage&lt;/h2&gt; &#xA;&lt;p&gt;Check out our demo notebook &lt;a href=&#34;https://raw.githubusercontent.com/topoteretes/cognee/main/cognee%20-%20Get%20Started.ipynb&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Set OpenAI API Key as an environment variable&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;import os&#xA;&#xA;# Setting an environment variable&#xA;os.environ[&#39;OPENAI_API_KEY&#39;] = &#39;&#39;&#xA;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add a new piece of information to storage&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;import cognee&#xA;cognee.add(absolute_data_path, dataset_name)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Use LLMs and cognee to create graphs&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;cognee.cognify(dataset_name)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Render the graph after adding your Graphistry credentials to .env&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;graph_url = await render_graph(graph, graph_type = &#34;networkx&#34;)&#xA;print(graph_url)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Query the graph for a piece of information&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;query_params = {&#xA;    SearchType.SIMILARITY: {&#39;query&#39;: &#39;your search query here&#39;}&#xA;}&#xA;cognee.search(graph, query_params) &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/-ARUfIzhzC4&#34; title=&#34;Learn about cognee: 55&#34;&gt;&lt;img src=&#34;https://i3.ytimg.com/vi/-ARUfIzhzC4/maxresdefault.jpg&#34; width=&#34;100%&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Architecture&lt;/h2&gt; &#xA;&lt;h3&gt;How Cognee Enhances Your Contextual Memory&lt;/h3&gt; &#xA;&lt;p&gt;Our framework for the OpenAI, Graph (Neo4j) and Vector (Weaviate) databases introduces three key enhancements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Query Classifiers: Navigate information graph using Pydantic OpenAI classifiers.&lt;/li&gt; &#xA; &lt;li&gt;Document Topology: Structure and store documents in public and private domains.&lt;/li&gt; &#xA; &lt;li&gt;Personalized Context: Provide a context object to the LLM for a better response.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/topoteretes/cognee/main/assets/architecture.png&#34; alt=&#34;Image&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>datawhalechina/llm-cookbook</title>
    <updated>2024-03-21T01:29:53Z</updated>
    <id>tag:github.com,2024-03-21:/datawhalechina/llm-cookbook</id>
    <link href="https://github.com/datawhalechina/llm-cookbook" rel="alternate"></link>
    <summary type="html">&lt;p&gt;面向开发者的 LLM 入门教程，吴恩达大模型系列课程中文版&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/datawhalechina/llm-cookbook/main/figures/readme.jpg&#34; alt=&#34;figures/readme.jpg&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;面向开发者的大模型手册 - LLM Cookbook&lt;/h1&gt; &#xA;&lt;h2&gt;项目简介&lt;/h2&gt; &#xA;&lt;p&gt;本项目是一个面向开发者的大模型手册，针对国内开发者的实际需求，主打 LLM 全方位入门实践。本项目基于吴恩达老师大模型系列课程内容，对原课程内容进行筛选、翻译、复现和调优，覆盖从 Prompt Engineering 到 RAG 开发、模型微调的全部流程，用最适合国内学习者的方式，指导国内开发者如何学习、入门 LLM 相关项目。&lt;/p&gt; &#xA;&lt;p&gt;针对不同内容的特点，我们对共计 11 门吴恩达老师的大模型课程进行了翻译复现，并结合国内学习者的实际情况，对不同课程进行了分级和排序，初学者可以先系统学习我们的必修类课程，掌握入门 LLM 所有方向都需要掌握的基础技能和概念，再选择性地学习我们的选修类课程，在自己感兴趣的方向上不断探索和学习。&lt;/p&gt; &#xA;&lt;p&gt;如果有你非常喜欢但我们还没有进行复现的吴恩达老师大模型课程，我们欢迎每一位开发者参考我们已有课程的格式和写法来对课程进行复现并提交 PR，在 PR 审核通过后，我们会根据课程内容将课程进行分级合并。欢迎每一位开发者的贡献！&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;在线阅读地址：&lt;a href=&#34;https://datawhalechina.github.io/llm-cookbook/&#34;&gt;面向开发者的 LLM 入门课程-在线阅读&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PDF下载地址：&lt;a href=&#34;https://datawhalechina.github.io/llm-cookbook/releases&#34;&gt;面向开发者的 LLM 入门教程-PDF&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;英文原版地址：&lt;a href=&#34;https://learn.deeplearning.ai&#34;&gt;吴恩达关于大模型的系列课程&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;项目意义&lt;/h2&gt; &#xA;&lt;p&gt;LLM 正在逐步改变人们的生活，而对于开发者，如何基于 LLM 提供的 API 快速、便捷地开发一些具备更强能力、集成LLM 的应用，来便捷地实现一些更新颖、更实用的能力，是一个急需学习的重要能力。&lt;/p&gt; &#xA;&lt;p&gt;由吴恩达老师与 OpenAI 合作推出的大模型系列教程，从大模型时代开发者的基础技能出发，深入浅出地介绍了如何基于大模型 API、LangChain 架构快速开发结合大模型强大能力的应用。其中，《Prompt Engineering for Developers》教程面向入门 LLM 的开发者，深入浅出地介绍了对于开发者，如何构造 Prompt 并基于 OpenAI 提供的 API 实现包括总结、推断、转换等多种常用功能，是入门 LLM 开发的经典教程；《Building Systems with the ChatGPT API》教程面向想要基于 LLM 开发应用程序的开发者，简洁有效而又系统全面地介绍了如何基于 ChatGPT API 打造完整的对话系统；《LangChain for LLM Application Development》教程结合经典大模型开源框架 LangChain，介绍了如何基于 LangChain 框架开发具备实用功能、能力全面的应用程序，《LangChain Chat With Your Data》教程则在此基础上进一步介绍了如何使用 LangChain 架构结合个人私有数据开发个性化大模型应用；《Building Generative AI Applications with Gradio》、《Evaluating and Debugging Generative AI》教程分别介绍了两个实用工具 Gradio 与 W&amp;amp;B，指导开发者如何结合这两个工具来打造、评估生成式 AI 应用。&lt;/p&gt; &#xA;&lt;p&gt;上述教程非常适用于开发者学习以开启基于 LLM 实际搭建应用程序之路。因此，我们将该系列课程翻译为中文，并复现其范例代码，也为其中一个视频增加了中文字幕，支持国内中文学习者直接使用，以帮助中文学习者更好地学习 LLM 开发；我们也同时实现了效果大致相当的中文 Prompt，支持学习者感受中文语境下 LLM 的学习使用，对比掌握多语言语境下的 Prompt 设计与 LLM 开发。未来，我们也将加入更多 Prompt 高级技巧，以丰富本课程内容，帮助开发者掌握更多、更巧妙的 Prompt 技能。&lt;/p&gt; &#xA;&lt;h2&gt;项目受众&lt;/h2&gt; &#xA;&lt;p&gt;所有具备基础 Python 能力，想要入门 LLM 的开发者。&lt;/p&gt; &#xA;&lt;h2&gt;项目亮点&lt;/h2&gt; &#xA;&lt;p&gt;《ChatGPT Prompt Engineering for Developers》、《Building Systems with the ChatGPT API》等教程作为由吴恩达老师与 OpenAI 联合推出的官方教程，在可预见的未来会成为 LLM 的重要入门教程，但是目前还只支持英文版且国内访问受限，打造中文版且国内流畅访问的教程具有重要意义；同时，GPT 对中文、英文具有不同的理解能力，本教程在多次对比、实验之后确定了效果大致相当的中文 Prompt，支持学习者研究如何提升 ChatGPT 在中文语境下的理解与生成能力。&lt;/p&gt; &#xA;&lt;h2&gt;学习指南&lt;/h2&gt; &#xA;&lt;p&gt;本教程适用于所有具备基础 Python 能力，想要入门 LLM 的开发者。&lt;/p&gt; &#xA;&lt;p&gt;如果你想要开始学习本教程，你需要提前具备：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;至少一个 LLM API（最好是 OpenAI，如果是其他 API，你可能需要参考&lt;a href=&#34;https://github.com/datawhalechina/llm-universe&#34;&gt;其他教程&lt;/a&gt;对 API 调用代码进行修改）&lt;/li&gt; &#xA; &lt;li&gt;能够使用 Python Jupyter Notebook&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;本教程共包括 11 门课程，分为必修类、选修类两个类别。必修类课程是我们认为最适合初学者学习以入门 LLM 的课程，包括了入门 LLM 所有方向都需要掌握的基础技能和概念，我们也针对必修类课程制作了适合阅读的在线阅读和 PDF 版本，在学习必修类课程时，我们建议学习者按照我们列出的顺序进行学习；选修类课程是在必修类课程上的拓展延伸，包括了 RAG 开发、模型微调、模型评估等多个方面，适合学习者在掌握了必修类课程之后选择自己感兴趣的方向和课程进行学习。&lt;/p&gt; &#xA;&lt;p&gt;必修类课程包括：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;面向开发者的 Prompt Engineering。基于吴恩达老师《ChatGPT Prompt Engineering for Developers》课程打造，面向入门 LLM 的开发者，深入浅出地介绍了对于开发者，如何构造 Prompt 并基于 OpenAI 提供的 API 实现包括总结、推断、转换等多种常用功能，是入门 LLM 开发的第一步。&lt;/li&gt; &#xA; &lt;li&gt;搭建基于 ChatGPT 的问答系统。基于吴恩达老师《Building Systems with the ChatGPT API》课程打造，指导开发者如何基于 ChatGPT 提供的 API 开发一个完整的、全面的智能问答系统。通过代码实践，实现了基于 ChatGPT 开发问答系统的全流程，介绍了基于大模型开发的新范式，是大模型开发的实践基础。&lt;/li&gt; &#xA; &lt;li&gt;使用 LangChain 开发应用程序。基于吴恩达老师《LangChain for LLM Application Development》课程打造，对 LangChain 展开深入介绍，帮助学习者了解如何使用 LangChain，并基于 LangChain 开发完整的、具备强大能力的应用程序。&lt;/li&gt; &#xA; &lt;li&gt;使用 LangChain 访问个人数据。基于吴恩达老师《LangChain Chat with Your Data》课程打造，深入拓展 LangChain 提供的个人数据访问能力，指导开发者如何使用 LangChain 开发能够访问用户个人数据、提供个性化服务的大模型应用。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;选修类课程包括：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;使用 Gradio 搭建生成式 AI 应用。基于吴恩达老师《Building Generative AI Applications with Gradio》课程打造，指导开发者如何使用 Gradio 通过 Python 接口程序快速、高效地为生成式 AI 构建用户界面。&lt;/li&gt; &#xA; &lt;li&gt;评估改进生成式 AI。基于吴恩达老师《Evaluating and Debugging Generative AI》课程打造，结合 wandb，提供一套系统化的方法和工具，帮助开发者有效地跟踪和调试生成式 AI 模型。&lt;/li&gt; &#xA; &lt;li&gt;微调大语言模型。基于吴恩达老师《Finetuning Large Language Model》课程打造，结合 lamini 框架，讲述如何便捷高效地在本地基于个人数据微调开源大语言模型。&lt;/li&gt; &#xA; &lt;li&gt;大模型与语义检索。基于吴恩达老师《Large Language Models with Semantic Search》课程打造，针对检索增强生成，讲述了多种高级检索技巧以实现更准确、高效的检索增强 LLM 生成效果。&lt;/li&gt; &#xA; &lt;li&gt;基于 Chroma 的高级检索。基于吴恩达老师《Advanced Retrieval for AI with Chroma》课程打造，旨在介绍基于 Chroma 的高级检索技术，提升检索结果的准确性。&lt;/li&gt; &#xA; &lt;li&gt;搭建和评估高级 RAG 应用。基于吴恩达老师《Building and Evaluating Advanced RAG Applications》课程打造，介绍构建和实现高质量RAG系统所需的关键技术和评估框架。&lt;/li&gt; &#xA; &lt;li&gt;LangChain 的 Functions、Tools 和 Agents。基于吴恩达老师《Functions, Tools and Agents with LangChain》课程打造，介绍如何基于 LangChain 的新语法构建 Agent。&lt;/li&gt; &#xA; &lt;li&gt;Prompt 高级技巧。原创内容，创作中。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;其他资料包括：&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;双语字幕视频地址：&lt;a href=&#34;https://www.bilibili.com/video/BV1Bo4y1A7FU/?share_source=copy_web&#34;&gt;吴恩达 x OpenAI的Prompt Engineering课程专业翻译版&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;中英双语字幕下载：&lt;a href=&#34;https://github.com/GitHubDaily/ChatGPT-Prompt-Engineering-for-Developers-in-Chinese&#34;&gt;《ChatGPT提示工程》非官方版中英双语字幕&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;视频讲解：&lt;a href=&#34;https://www.bilibili.com/video/BV1PN4y1k7y2/?spm_id_from=333.999.0.0&#34;&gt;面向开发者的 Prompt Engineering 讲解（数字游民大会）&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;目录结构说明：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;content：基于原课程复现的双语版代码，可运行的 Notebook，更新频率最高，更新速度最快。&#xA;&#xA;docs：必修类课程文字教程版在线阅读源码，适合阅读的 md。&#xA;&#xA;figures：图片文件。&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;致谢&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;核心贡献者&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/logan-zou&#34;&gt;邹雨衡-项目负责人&lt;/a&gt;（Datawhale成员-对外经济贸易大学研究生）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://yam.gift/&#34;&gt;长琴-项目发起人&lt;/a&gt;（内容创作者-Datawhale成员-AI算法工程师）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Sophia-Huang&#34;&gt;玉琳-项目发起人&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/xuhu0115&#34;&gt;徐虎-教程编撰者&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Weihong-Liu&#34;&gt;刘伟鸿-教程编撰者&lt;/a&gt;（内容创作者-江南大学非全研究生）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://Joyenjoye.com&#34;&gt;Joye-教程编撰者&lt;/a&gt;（内容创作者-数据科学家）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/0-yy-0&#34;&gt;高立业&lt;/a&gt;（内容创作者-DataWhale成员-算法工程师）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/GKDGKD&#34;&gt;邓宇文&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/wisdom-pan&#34;&gt;魂兮&lt;/a&gt;（内容创作者-前端工程师）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KMnO4-zx&#34;&gt;宋志学&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/YikunHan42&#34;&gt;韩颐堃&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/6forwater29&#34;&gt;陈逸涵&lt;/a&gt; (内容创作者-Datawhale意向成员-AI爱好者)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ztgg0228&#34;&gt;仲泰&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/leason-wan&#34;&gt;万礼行&lt;/a&gt;（内容创作者-视频翻译者）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Bald0Wang&#34;&gt;王熠明&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://yetingyun.blog.csdn.net&#34;&gt;曾浩龙&lt;/a&gt;（内容创作者-Datawhale 意向成员-JLU AI 研究生）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/xinqi-fan&#34;&gt;小饭同学&lt;/a&gt;（内容创作者）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sunhanyu714%5D&#34;&gt;孙韩玉&lt;/a&gt;（内容创作者-算法量化部署工程师）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/YinHan-Zhang&#34;&gt;张银晗&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/LinChentang&#34;&gt;左春生&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Jin-Zhang-Yaoguang&#34;&gt;张晋&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Aphasia0515&#34;&gt;李娇娇&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Kedreamix&#34;&gt;邓恺俊&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Zhiyuan-Fan&#34;&gt;范致远&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Beyondzjl&#34;&gt;周景林&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/very-very-very&#34;&gt;诸世纪&lt;/a&gt;（内容创作者-算法工程师）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/YixinZ-NUS&#34;&gt;Zhang Yixin&lt;/a&gt;（内容创作者-IT爱好者）&lt;/li&gt; &#xA; &lt;li&gt;Sarai（内容创作者-AI应用爱好者）&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;其他&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;特别感谢 &lt;a href=&#34;https://github.com/Sm1les&#34;&gt;@Sm1les&lt;/a&gt;、&lt;a href=&#34;https://github.com/LSGOMYP&#34;&gt;@LSGOMYP&lt;/a&gt; 对本项目的帮助与支持；&lt;/li&gt; &#xA; &lt;li&gt;感谢 &lt;a href=&#34;https://github.com/GitHubDaily&#34;&gt;GithubDaily&lt;/a&gt; 提供的双语字幕；&lt;/li&gt; &#xA; &lt;li&gt;如果有任何想法可以联系我们 DataWhale 也欢迎大家多多提出 issue；&lt;/li&gt; &#xA; &lt;li&gt;特别感谢以下为教程做出贡献的同学！&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;a href=&#34;https://datawhalechina.github.io/llm-cookbook/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=datawhalechina/llm-cookbook&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;Made with &lt;a href=&#34;https://contrib.rocks&#34;&gt;contrib.rocks&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#datawhalechina/llm-cookbook&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=datawhalechina/llm-cookbook&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;关注我们&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;扫描下方二维码关注公众号：Datawhale&lt;/p&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/datawhalechina/llm-cookbook/main/figures/qrcode.jpeg&#34; width=&#34;180&#34; height=&#34;180&#34;&gt; &#xA;&lt;/div&gt; Datawhale 是一个专注于数据科学与 AI 领域的开源组织，汇集了众多领域院校和知名企业的优秀学习者，聚合了一群有开源精神和探索精神的团队成员。微信搜索公众号Datawhale可以加入我们。 &#xA;&lt;h2&gt;LICENSE&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a rel=&#34;license&#34; href=&#34;http://creativecommons.org/licenses/by-nc-sa/4.0/&#34;&gt;&lt;img alt=&#34;知识共享许可协议&#34; style=&#34;border-width:0&#34; src=&#34;https://img.shields.io/badge/license-CC%20BY--NC--SA%204.0-lightgrey&#34;&gt;&lt;/a&gt;&lt;br&gt;本作品采用&lt;a rel=&#34;license&#34; href=&#34;http://creativecommons.org/licenses/by-nc-sa/4.0/&#34;&gt;知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议&lt;/a&gt;进行许可。&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mrdbourke/simple-local-rag</title>
    <updated>2024-03-21T01:29:53Z</updated>
    <id>tag:github.com,2024-03-21:/mrdbourke/simple-local-rag</id>
    <link href="https://github.com/mrdbourke/simple-local-rag" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Build a RAG (Retrieval Augmented Generation) pipeline from scratch and have it all run locally.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Simple Local RAG Tutorial&lt;/h1&gt; &#xA;&lt;p&gt;Local RAG pipeline we&#39;re going to build:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mrdbourke/simple-local-rag/main/images/simple-local-rag-workflow-flowchart.png&#34; alt=&#34;&amp;quot;This is a flowchart describing a simple local retrieval-augmented generation (RAG) workflow for document processing and embedding creation, followed by search and answer functionality. The process begins with a collection of documents, such as PDFs or a 1200-page nutrition textbook, which are preprocessed into smaller chunks, for example, groups of 10 sentences each. These chunks are used as context for the Large Language Model (LLM). A cool person (potentially the user) asks a query such as &amp;quot;What are the macronutrients? And what do they do?&amp;quot; This query is then transformed by an embedding model into a numerical representation using sentence transformers or other options from Hugging Face, which are stored in a torch.tensor format for efficiency, especially with large numbers of embeddings (around 100k+). For extremely large datasets, a vector database/index may be used. The numerical query and relevant document passages are processed on a local GPU, specifically an RTX 4090. The LLM generates output based on the context related to the query, which can be interacted with through an optional chat web app interface. All of this processing happens on a local GPU. The flowchart includes icons for documents, processing steps, and hardware, with arrows indicating the flow from document collection to user interaction with the generated text and resources.&amp;quot;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;All designed to run locally on a NVIDIA GPU.&lt;/p&gt; &#xA;&lt;p&gt;All the way from PDF ingestion to &#34;chat with PDF&#34; style features.&lt;/p&gt; &#xA;&lt;p&gt;All using open-source tools.&lt;/p&gt; &#xA;&lt;p&gt;In our specific example, we&#39;ll build NutriChat, a RAG workflow that allows a person to query a 1200 page PDF version of a Nutrition Textbook and have an LLM generate responses back to the query based on passages of text from the textbook.&lt;/p&gt; &#xA;&lt;p&gt;PDF source: &lt;a href=&#34;https://pressbooks.oer.hawaii.edu/humannutrition2/&#34;&gt;https://pressbooks.oer.hawaii.edu/humannutrition2/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can also run notebook &lt;code&gt;00-simple-local-rag.ipynb&lt;/code&gt; directly in &lt;a href=&#34;https://colab.research.google.com/github/mrdbourke/simple-local-rag/blob/main/00-simple-local-rag.ipynb&#34;&gt;Google Colab&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;TODO:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Finish setup instructions&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Make header image of workflow&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add intro to RAG info in README?&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add extensions to README&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Record video of code writing/walkthrough - DONE, follow along with each line of code on YouTube: &lt;a href=&#34;https://youtu.be/qN_2fnOPY-M&#34;&gt;https://youtu.be/qN_2fnOPY-M&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Two main options:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;If you have a local NVIDIA GPU with 5GB+ VRAM, follow the steps below to have this pipeline run locally on your machine.&lt;/li&gt; &#xA; &lt;li&gt;If you don’t have a local NVIDIA GPU, you can follow along in Google Colab and have it run on a NVIDIA GPU there.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Comfortable writing Python code.&lt;/li&gt; &#xA; &lt;li&gt;1-2 beginner machine learning/deep learning courses.&lt;/li&gt; &#xA; &lt;li&gt;Familiarity with PyTorch, see my &lt;a href=&#34;https://youtu.be/Z_ikDlimN6A?si=NIkrslkvHaNdlYgx&#34;&gt;beginner PyTorch video&lt;/a&gt; for more.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;Note: Tested in Python 3.11, running on Windows 11 with a NVIDIA RTX 4090 with CUDA 12.1.&lt;/p&gt; &#xA;&lt;h3&gt;Clone repo&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/mrdbourke/simple-local-rag.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd simple-local-rag&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Create environment&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m venv venv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Activate environment&lt;/h3&gt; &#xA;&lt;p&gt;Linux/macOS:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;source venv/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Windows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;.\venv\Scripts\activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Install requirements&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; I found I had to install &lt;code&gt;torch&lt;/code&gt; manually (&lt;code&gt;torch&lt;/code&gt; 2.1.1+ is required for newer versions of attention for faster inference) with CUDA, see: &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;https://pytorch.org/get-started/locally/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;On Windows I used:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip3 install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Launch notebook&lt;/h3&gt; &#xA;&lt;p&gt;VS Code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;code .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Jupyter Notebook&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;jupyter notebook&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Setup notes:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you run into any install/setup troubles, please leave an issue.&lt;/li&gt; &#xA; &lt;li&gt;To get access to the Gemma LLM models, you will have to &lt;a href=&#34;https://huggingface.co/google/gemma-7b-it&#34;&gt;agree to the terms &amp;amp; conditions&lt;/a&gt; on the Gemma model page on Hugging Face. You will then have to authorize your local machine via the &lt;a href=&#34;https://huggingface.co/docs/huggingface_hub/en/quick-start#authentication&#34;&gt;Hugging Face CLI/Hugging Face Hub &lt;code&gt;login()&lt;/code&gt; function&lt;/a&gt;. Once you&#39;ve done this, you&#39;ll be able to download the models. If you&#39;re using Google Colab, you can add a &lt;a href=&#34;https://huggingface.co/docs/hub/en/security-tokens&#34;&gt;Hugging Face token&lt;/a&gt; to the &#34;Secrets&#34; tab.&lt;/li&gt; &#xA; &lt;li&gt;For speedups, installing and compiling Flash Attention 2 (faster attention implementation) can take ~5 minutes to 3 hours depending on your system setup. See the &lt;a href=&#34;https://github.com/Dao-AILab/flash-attention/tree/main&#34;&gt;Flash Attention 2 GitHub&lt;/a&gt; for more. In particular, if you&#39;re running on Windows, see this &lt;a href=&#34;https://github.com/Dao-AILab/flash-attention/issues/595&#34;&gt;GitHub issue thread&lt;/a&gt;. I&#39;ve commented out &lt;code&gt;flash-attn&lt;/code&gt; in the requirements.txt due to compile time, feel free to uncomment if you&#39;d like use it or run &lt;code&gt;pip install flash-attn&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What is RAG?&lt;/h2&gt; &#xA;&lt;p&gt;RAG stands for Retrieval Augmented Generation.&lt;/p&gt; &#xA;&lt;p&gt;It was introduced in the paper &lt;a href=&#34;https://arxiv.org/abs/2005.11401&#34;&gt;&lt;em&gt;Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Each step can be roughly broken down to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Retrieval&lt;/strong&gt; - Seeking relevant information from a source given a query. For example, getting relevant passages of Wikipedia text from a database given a question.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Augmented&lt;/strong&gt; - Using the relevant retrieved information to modify an input to a generative model (e.g. an LLM).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Generation&lt;/strong&gt; - Generating an output given an input. For example, in the case of an LLM, generating a passage of text given an input prompt.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Why RAG?&lt;/h2&gt; &#xA;&lt;p&gt;The main goal of RAG is to improve the generation outptus of LLMs.&lt;/p&gt; &#xA;&lt;p&gt;Two primary improvements can be seen as:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Preventing hallucinations&lt;/strong&gt; - LLMs are incredible but they are prone to potential hallucination, as in, generating something that &lt;em&gt;looks&lt;/em&gt; correct but isn&#39;t. RAG pipelines can help LLMs generate more factual outputs by providing them with factual (retrieved) inputs. And even if the generated answer from a RAG pipeline doesn&#39;t seem correct, because of retrieval, you also have access to the sources where it came from.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Work with custom data&lt;/strong&gt; - Many base LLMs are trained with internet-scale text data. This means they have a great ability to model language, however, they often lack specific knowledge. RAG systems can provide LLMs with domain-specific data such as medical information or company documentation and thus customized their outputs to suit specific use cases.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The authors of the original RAG paper mentioned above outlined these two points in their discussion.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;This work offers several positive societal benefits over previous work: the fact that it is more strongly grounded in real factual knowledge (in this case Wikipedia) makes it “hallucinate” less with generations that are more factual, and offers more control and interpretability. RAG could be employed in a wide variety of scenarios with direct benefit to society, for example by endowing it with a medical index and asking it open-domain questions on that topic, or by helping people be more effective at their jobs.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;RAG can also be a much quicker solution to implement than fine-tuning an LLM on specific data.&lt;/p&gt; &#xA;&lt;h2&gt;What kind of problems can RAG be used for?&lt;/h2&gt; &#xA;&lt;p&gt;RAG can help anywhere there is a specific set of information that an LLM may not have in its training data (e.g. anything not publicly accessible on the internet).&lt;/p&gt; &#xA;&lt;p&gt;For example you could use RAG for:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Customer support Q&amp;amp;A chat&lt;/strong&gt; - By treating your existing customer support documentation as a resource, when a customer asks a question, you could have a system retrieve relevant documentation snippets and then have an LLM craft those snippets into an answer. Think of this as a &#34;chatbot for your documentation&#34;. Klarna, a large financial company, &lt;a href=&#34;https://www.klarna.com/international/press/klarna-ai-assistant-handles-two-thirds-of-customer-service-chats-in-its-first-month/&#34;&gt;uses a system like this&lt;/a&gt; to save $40M per year on customer support costs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Email chain analysis&lt;/strong&gt; - Let&#39;s say you&#39;re an insurance company with long threads of emails between customers and insurance agents. Instead of searching through each individual email, you could retrieve relevant passages and have an LLM create strucutred outputs of insurance claims.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Company internal documentation chat&lt;/strong&gt; - If you&#39;ve worked at a large company, you know how hard it can be to get an answer sometimes. Why not let a RAG system index your company information and have an LLM answer questions you may have? The benefit of RAG is that you will have references to resources to learn more if the LLM answer doesn&#39;t suffice.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Textbook Q&amp;amp;A&lt;/strong&gt; - Let&#39;s say you&#39;re studying for your exams and constantly flicking through a large textbook looking for answers to your quesitons. RAG can help provide answers as well as references to learn more.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;All of these have the common theme of retrieving relevant resources and then presenting them in an understandable way using an LLM.&lt;/p&gt; &#xA;&lt;p&gt;From this angle, you can consider an LLM a calculator for words.&lt;/p&gt; &#xA;&lt;h2&gt;Why local?&lt;/h2&gt; &#xA;&lt;p&gt;Privacy, speed, cost.&lt;/p&gt; &#xA;&lt;p&gt;Running locally means you use your own hardware.&lt;/p&gt; &#xA;&lt;p&gt;From a privacy standpoint, this means you don&#39;t have send potentially sensitive data to an API.&lt;/p&gt; &#xA;&lt;p&gt;From a speed standpoint, it means you won&#39;t necessarily have to wait for an API queue or downtime, if your hardware is running, the pipeline can run.&lt;/p&gt; &#xA;&lt;p&gt;And from a cost standpoint, running on your own hardware often has a heavier starting cost but little to no costs after that.&lt;/p&gt; &#xA;&lt;p&gt;Performance wise, LLM APIs may still perform better than an open-source model running locally on general tasks but there are more and more examples appearing of smaller, focused models outperforming larger models.&lt;/p&gt; &#xA;&lt;h2&gt;Key terms&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Term&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Token&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A sub-word piece of text. For example, &#34;hello, world!&#34; could be split into [&#34;hello&#34;, &#34;,&#34;, &#34;world&#34;, &#34;!&#34;]. A token can be a whole word,&lt;br&gt; part of a word or group of punctuation characters. 1 token ~= 4 characters in English, 100 tokens ~= 75 words.&lt;br&gt; Text gets broken into tokens before being passed to an LLM.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Embedding&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A learned numerical representation of a piece of data. For example, a sentence of text could be represented by a vector with&lt;br&gt; 768 values. Similar pieces of text (in meaning) will ideally have similar values.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Embedding model&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A model designed to accept input data and output a numerical representation. For example, a text embedding model may take in 384 &lt;br&gt;tokens of text and turn it into a vector of size 768. An embedding model can and often is different to an LLM model.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Similarity search/vector search&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Similarity search/vector search aims to find two vectors which are close together in high-demensional space. For example, &lt;br&gt;two pieces of similar text passed through an embedding model should have a high similarity score, whereas two pieces of text about&lt;br&gt; different topics will have a lower similarity score. Common similarity score measures are dot product and cosine similarity.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Large Language Model (LLM)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A model which has been trained to numerically represent the patterns in text. A generative LLM will continue a sequence when given a sequence. &lt;br&gt;For example, given a sequence of the text &#34;hello, world!&#34;, a genertive LLM may produce &#34;we&#39;re going to build a RAG pipeline today!&#34;.&lt;br&gt; This generation will be highly dependant on the training data and prompt.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;LLM context window&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The number of tokens a LLM can accept as input. For example, as of March 2024, GPT-4 has a default context window of 32k tokens&lt;br&gt; (about 96 pages of text) but can go up to 128k if needed. A recent open-source LLM from Google, Gemma (March 2024) has a context&lt;br&gt; window of 8,192 tokens (about 24 pages of text). A higher context window means an LLM can accept more relevant information&lt;br&gt; to assist with a query. For example, in a RAG pipeline, if a model has a larger context window, it can accept more reference items&lt;br&gt; from the retrieval system to aid with its generation.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Prompt&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A common term for describing the input to a generative LLM. The idea of &#34;&lt;a href=&#34;https://en.wikipedia.org/wiki/Prompt_engineering&#34;&gt;prompt engineering&lt;/a&gt;&#34; is to structure a text-based&lt;br&gt; (or potentially image-based as well) input to a generative LLM in a specific way so that the generated output is ideal. This technique is&lt;br&gt; possible because of a LLMs capacity for in-context learning, as in, it is able to use its representation of language to breakdown &lt;br&gt;the prompt and recognize what a suitable output may be (note: the output of LLMs is probable, so terms like &#34;may output&#34; are used).&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;TK - Extensions&lt;/h1&gt; &#xA;&lt;p&gt;Coming soon.&lt;/p&gt;</summary>
  </entry>
</feed>