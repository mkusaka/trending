<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-24T01:39:41Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>SamurAIGPT/langchain-course</title>
    <updated>2023-05-24T01:39:41Z</updated>
    <id>tag:github.com,2023-05-24:/SamurAIGPT/langchain-course</id>
    <link href="https://github.com/SamurAIGPT/langchain-course" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Learn to build and deploy AI apps.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Langchain Course for Beginners&lt;/h1&gt; &#xA;&lt;p&gt;Welcome to the LangChain Beginners Course repository! This course is designed to help you get started with LangChain, a powerful open-source framework for developing applications using large language models (LLMs) like ChatGPT.&lt;/p&gt; &#xA;&lt;p&gt;I have built 12 AI apps in 12 weeks using Langchain hosted on &lt;a href=&#34;https://raw.githubusercontent.com/SamurAIGPT/langchain-course/main/thesamur.ai&#34;&gt;SamurAI&lt;/a&gt; and have onboarded million visitors a month.&lt;/p&gt; &#xA;&lt;h3&gt;Topics (May be updated)&lt;/h3&gt; &#xA;&lt;h2&gt;Course Structure&lt;/h2&gt; &#xA;&lt;p&gt;This course is divided into four modules, each focusing on a particular aspect of LangChain:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Introduction to LangChain&lt;/li&gt; &#xA; &lt;li&gt;Langchain Fundamentals&lt;/li&gt; &#xA; &lt;li&gt;Building Applications with LangChain&lt;/li&gt; &#xA; &lt;li&gt;Project and Conclusion&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Each module contains lessons with theory and practical exercises to help solidify your understanding of LangChain.&lt;/p&gt; &#xA;&lt;h3&gt;Getting Started&lt;/h3&gt; &#xA;&lt;p&gt;Videos coming soon &lt;a href=&#34;https://www.youtube.com/@AnilChandraNaiduMatcha&#34;&gt;https://www.youtube.com/@AnilChandraNaiduMatcha&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://www.youtube.com/@ankursingh9507&#34;&gt;https://www.youtube.com/@ankursingh9507&lt;/a&gt; .Subscribe to the channel to get latest content&lt;/p&gt; &#xA;&lt;p&gt;Follow &lt;a href=&#34;https://twitter.com/matchaman11&#34;&gt;Anil Chandra Naidu Matcha&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://twitter.com/ankur_maker&#34;&gt;Ankur Singh&lt;/a&gt; on twitter for updates&lt;/p&gt; &#xA;&lt;p&gt;Join our discord server for support &lt;a href=&#34;https://discord.gg/FBpafqbbYF&#34;&gt;https://discord.gg/FBpafqbbYF&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;p&gt;This course assumes you have a basic understanding of Python and JavaScript, as LangChain is written in these languages. Familiarity with machine learning concepts and language models is beneficial but not required.&lt;/p&gt; &#xA;&lt;h3&gt;Contributions&lt;/h3&gt; &#xA;&lt;p&gt;Contributions to this course are welcome! If you have suggestions for improvements, please open an issue or create a pull request.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Anil-matcha/Chat-With-Excel</title>
    <updated>2023-05-24T01:39:41Z</updated>
    <id>tag:github.com,2023-05-24:/Anil-matcha/Chat-With-Excel</id>
    <link href="https://github.com/Anil-matcha/Chat-With-Excel" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Chat with your tabular data. No need of remembering any formulas or learning pandas. Train machine learning models in natural language&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Chat-With-Excel&lt;/h1&gt; &#xA;&lt;p&gt;Chat with your tabular data. No need of remembering any formulas or learning pandas. Train machine learning models in natural language&lt;/p&gt; &#xA;&lt;h3&gt;Getting Started&lt;/h3&gt; &#xA;&lt;p&gt;Code is up now, ‚≠ê (Star) the repo to receive updates&lt;/p&gt; &#xA;&lt;p&gt;Replit and streamlit version coming soon&lt;/p&gt; &#xA;&lt;p&gt;Follow &lt;a href=&#34;https://twitter.com/matchaman11&#34;&gt;Anil Chandra Naidu Matcha&lt;/a&gt; on twitter for updates&lt;/p&gt; &#xA;&lt;p&gt;Subscribe to &lt;a href=&#34;https://www.youtube.com/@AnilChandraNaiduMatcha&#34;&gt;https://www.youtube.com/@AnilChandraNaiduMatcha&lt;/a&gt; for more such video tutorials&lt;/p&gt; &#xA;&lt;h3&gt;How to run ?&lt;/h3&gt; &#xA;&lt;p&gt;Click on open in Google colab from the file Data analysis with Langchain and run all the steps one by one&lt;/p&gt; &#xA;&lt;p&gt;Make sure to setup the openai key in create_csv_agent function&lt;/p&gt; &#xA;&lt;h3&gt;Demo link&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://chatcsv.thesamur.ai/&#34;&gt;https://chatcsv.thesamur.ai/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Also check&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Anil-matcha/Website-to-Chatbot&#34;&gt;Chat with Website code&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Anil-matcha/ChatPDF&#34;&gt;Chat with PDF code&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Anil-matcha/Chat-Youtube&#34;&gt;Chat with Youtube code&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Anil-matcha/DiscordGPT&#34;&gt;ChatGPT in Discord code&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>stanfordnlp/string2string</title>
    <updated>2023-05-24T01:39:41Z</updated>
    <id>tag:github.com,2023-05-24:/stanfordnlp/string2string</id>
    <link href="https://github.com/stanfordnlp/string2string" rel="alternate"></link>
    <summary type="html">&lt;p&gt;String-to-String Algorithms for Natural Language Processing&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/stanfordnlp/string2string/raw/main/fables/string2string-overview.png&#34; class=&#34;center&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;string2string&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://badge.fury.io/py/string2string&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/string2string&#34; alt=&#34;PyPI - Python Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/py/string2string&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/string2string.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/string2string&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/string2string&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/stanfordnlp/string2string/raw/main/LICENSE.txt&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/stanfordnlp/string2string.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2304.14395&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2304.14395-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stanfordnlp/string2string/main/#getting-started&#34;&gt;&lt;strong&gt;Getting Started&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/stanfordnlp/string2string/main/#tutorials&#34;&gt;&lt;strong&gt;Tutorials&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/stanfordnlp/string2string/main/#example-usage&#34;&gt;&lt;strong&gt;Example Usage&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://string2string.readthedocs.io/en/latest/&#34;&gt;&lt;strong&gt;Documentation&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://github.com/stanfordnlp/string2string/tree/main/tests&#34;&gt;&lt;strong&gt;Tests&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/stanfordnlp/string2string/main/#citation&#34;&gt;&lt;strong&gt;Citation&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/stanfordnlp/string2string/main/#thanks&#34;&gt;&lt;strong&gt;Thanks&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/stanfordnlp/string2string/main/#contributing&#34;&gt;&lt;strong&gt;Contributing&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Abstract&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;strong&gt;string2string&lt;/strong&gt; library is an open-source tool that offers a comprehensive suite of efficient algorithms for a broad range of string-to-string problems. It includes both traditional algorithmic solutions and recent advanced neural approaches to address various problems in pairwise string alignment, distance measurement, lexical and semantic search, and similarity analysis. Additionally, the library provides several helpful visualization tools and metrics to facilitate the interpretation and analysis of these methods.&lt;/p&gt; &#xA;&lt;p&gt;The library features notable algorithms such as the &lt;a href=&#34;https://en.wikipedia.org/wiki/Smith%E2%80%93Waterman_algorithm&#34;&gt;Smith-Waterman algorithm&lt;/a&gt; for pairwise local alignment, the &lt;a href=&#34;https://en.wikipedia.org/wiki/Hirschberg%27s_algorithm&#34;&gt;Hirschberg algorithm&lt;/a&gt; for global alignment, the &lt;a href=&#34;https://en.wikipedia.org/wiki/Wagner%E2%80%93Fischer_algorithm&#34;&gt;Wagner-Fisher algorithm&lt;/a&gt; for &lt;a href=&#34;https://en.wikipedia.org/wiki/Edit_distance&#34;&gt;edit distance&lt;/a&gt;, &lt;a href=&#34;https://github.com/neulab/BARTScore&#34;&gt;BARTScore&lt;/a&gt; and &lt;a href=&#34;https://github.com/Tiiiger/bert_score&#34;&gt;BERTScore&lt;/a&gt; for similarity analysis, the &lt;a href=&#34;https://en.wikipedia.org/wiki/Knuth%E2%80%93Morris%E2%80%93Pratt_algorithm&#34;&gt;Knuth-Morris-Pratt&lt;/a&gt; algorithm for lexical search, and &lt;a href=&#34;https://github.com/facebookresearch/faiss&#34;&gt;Faiss&lt;/a&gt; for &lt;a href=&#34;https://en.wikipedia.org/wiki/Semantic_search&#34;&gt;semantic search&lt;/a&gt;. Moreover, it wraps existing highly efficient and widely-used implementations of certain frameworks and metrics, such as &lt;a href=&#34;https://github.com/mjpost/sacrebleu&#34;&gt;sacreBLEU&lt;/a&gt; and &lt;a href=&#34;https://github.com/google-research/google-research/tree/master/rouge&#34;&gt;ROUGE&lt;/a&gt;, whenever it is appropriate and suitable.&lt;/p&gt; &#xA;&lt;p&gt;In general, the &lt;a href=&#34;https://string2string.readthedocs.io/en/latest/&#34;&gt;&lt;strong&gt;string2string&lt;/strong&gt;&lt;/a&gt; library seeks to provide extensive coverage and increased flexibility compared to existing libraries for strings. It can be used for many downstream applications, tasks, and problems in natural-language processing, bioinformatics, and computational social sciences. With its comprehensive suite of algorithms, visualization tools, and metrics, the string2string library is a valuable resource for researchers and practitioners in various fields.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Install the string2string library by running the following command in your terminal:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install string2string&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once the installation is complete, you can import the library and start using its functionalities.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Remark&lt;/strong&gt;: We recommend using Python 3.7+ for the library.&lt;/p&gt; &#xA;&lt;h2&gt;Tutorials&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/11dKisbukdDMaZwp_Tnx_64Z7sn0uQD9c?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/11dKisbukdDMaZwp_Tnx_64Z7sn0uQD9c?usp=sharing&#34;&gt;Tutorial: Alignment Tasks and Algorithms&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1e8iwBkA7Q4XpmHtxst8_XA-APx4Vsb4j?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1e8iwBkA7Q4XpmHtxst8_XA-APx4Vsb4j?usp=sharing&#34;&gt;Tutorial: Distance Tasks and Algorithms&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1wu-JOyivxn_52SreF2ukYY7xi4uFVuAx?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1wu-JOyivxn_52SreF2ukYY7xi4uFVuAx?usp=sharing&#34;&gt;Tutorial: Search Tasks and Algorithms&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1qNDIkVCEMOVW4WySmzQBvrNAzZ4-zORT?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1qNDIkVCEMOVW4WySmzQBvrNAzZ4-zORT?usp=sharing&#34;&gt;Tutorial: Similarity Tasks and Algorithms&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1lpXNQn2DSuJB-0iQ-x3h_jx-6-laGpNk?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1lpXNQn2DSuJB-0iQ-x3h_jx-6-laGpNk?usp=sharing&#34;&gt;Hands-On Tutorial: Semantic Search and Visualization of USPTO Patents&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1TsMT3DESGY4BNkk-ZRDaL70CRTqrQAtB?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1TsMT3DESGY4BNkk-ZRDaL70CRTqrQAtB?usp=sharing&#34;&gt;Hands-On Tutorial: Plagiarism Detection of Essays&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Example Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Alignment&lt;/h3&gt; &#xA;&lt;p&gt;In the following example, we illustrate how to align two sequences of strings globally by using the &lt;a href=&#34;https://en.wikipedia.org/wiki/Needleman%E2%80%93Wunsch_algorithm&#34;&gt;Needleman-Wunsch algorithm&lt;/a&gt;. This algorithm, along with other alignment techniques, can be found in the alignment module of the library. The code snippet below demonstrates how to apply the Needleman-Wunsch algorithm to perform a global alignment of two given strings.&lt;/p&gt; &#xA;&lt;p&gt;This example provides a practical illustration of how to use the Needleman-Wunsch algorithm to solve the problem of sequence alignment, which is a fundamental problem in bioinformatics.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; # Import the NeedlemanWunsch class from the alignment module&#xA;&amp;gt;&amp;gt;&amp;gt; from string2string.alignment import NeedlemanWunsch&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; # Create an instance of the NeedlemanWunsch class&#xA;&amp;gt;&amp;gt;&amp;gt; nw = NeedlemanWunsch()&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; # Let&#39;s create a list of strings (resembling DNA sequences), but they can be any strings (e.g., words), of course.&#xA;&amp;gt;&amp;gt;&amp;gt; seq1 = [&#39;X&#39;, &#39;ATT&#39;, &#39;GC&#39;, &#39;GC&#39;, &#39;A&#39;, &#39;A&#39;, &#39;G&#39;]&#xA;&amp;gt;&amp;gt;&amp;gt; seq2 = [&#39;ATT&#39;, &#39;G&#39;, &#39;GC&#39;, &#39;GC&#39;, &#39;A&#39;, &#39;C&#39;, &#39;G&#39;]&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; # Compute the alignment between two strings&#xA;&amp;gt;&amp;gt;&amp;gt; aligned_seq1, aligned_seq2 = nw.get_alignment(seq1, seq2)&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; # Print the alignment between the sequences, as computed by the Needleman-Wunsch algorithm.&#xA;&amp;gt;&amp;gt;&amp;gt; nw.print_alignment(aligned_seq1, aligned_seq2)&#xA;X | ATT | - | GC | GC | A | A | G&#xA;- | ATT | G | GC | GC | A | C | G&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; alg_path, alg_seq1_parts, alg_seq2_parts = nw.get_alignment_strings_and_indices(aligned_seq1, aligned_seq2)&#xA;&amp;gt;&amp;gt;&amp;gt; from string2string.misc.plotting_functions import plot_pairwise_alignment&#xA;&amp;gt;&amp;gt;&amp;gt; plot_pairwise_alignment(seq1_pieces = alg_seq1_parts, seq2_pieces = alg_seq2_parts, alignment = alg_path, str2colordict = {&#39;-&#39;: &#39;lightgray&#39;, &#39;ATT&#39;: &#39;indianred&#39;, &#39;GC&#39;: &#39;darkseagreen&#39;, &#39;A&#39;: &#39;skyblue&#39;, &#39;G&#39;: &#39;palevioletred&#39;, &#39;C&#39;: &#39;steelblue&#39;}, title = &#39;Global Alignment Between Two Sequences of Strings&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/stanfordnlp/string2string/raw/main/fables/alignment-example.png&#34; class=&#34;center&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Distance&lt;/h3&gt; &#xA;&lt;p&gt;The following code snippet demonstrates how to use the &lt;a href=&#34;https://en.wikipedia.org/wiki/Levenshtein_distance&#34;&gt;Levenshtein edit distance algorithm&lt;/a&gt; to compute the edit distance between two strings, at the character level and at the word level.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; # Let&#39;s create a Levenshtein edit distance class instance, with the default (unit cost) weights, from the distance module&#xA;&amp;gt;&amp;gt;&amp;gt; from string2string.distance import LevenshteinEditDistance&#xA;&amp;gt;&amp;gt;&amp;gt; edit_dist = LevenshteinEditDistance()&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; # Let&#39;s also create a Tokenizer class instance with the default word delimiter (i.e., space)&#xA;&amp;gt;&amp;gt;&amp;gt; from string2string.misc import Tokenizer&#xA;&amp;gt;&amp;gt;&amp;gt; tokenizer = Tokenizer(word_delimiter=&#39; &#39;)&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; # Let&#39;s create two strings&#xA;&amp;gt;&amp;gt;&amp;gt; text1 = &#34;The quick brown fox jumps over the lazy dog&#34;&#xA;&amp;gt;&amp;gt;&amp;gt; text2 = &#34;The kuack brown box jumps over the lazy dog&#34;&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; # Get the edit distance between them at the character level&#xA;&amp;gt;&amp;gt;&amp;gt; edit_dist_score  = edit_dist.compute(text1, text2)&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; print(f&#34;Edit distance between these two texts at the character level is {edit_dist_score}&#34;)&#xA;# Edit distance between these two texts at the character level is 3.0&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; # Tokenize the two texts&#xA;&amp;gt;&amp;gt;&amp;gt; text1_tokens = tokenizer.tokenize(text1)&#xA;&amp;gt;&amp;gt;&amp;gt; text2_tokens = tokenizer.tokenize(text2)&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; # Get the distance between them at the word level&#xA;&amp;gt;&amp;gt;&amp;gt; edit_dist_score  = edit_dist.compute(text1_tokens, text2_tokens)&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; print(f&#34;Edit distance between these two texts at the word level is {edit_dist_score}&#34;)&#xA;# Edit distance between these two texts at the word level is 2.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Search&lt;/h3&gt; &#xA;&lt;p&gt;The following code snippet demonstrates how to use the &lt;a href=&#34;https://en.wikipedia.org/wiki/Knuth%E2%80%93Morris%E2%80%93Pratt_algorithm&#34;&gt;Knuth-Morrs-Pratt (KMP) search algorithm&lt;/a&gt; to find the index of a pattern in a text.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; # Let&#39;s create a KMPSearch class instance from the search module&#xA;&amp;gt;&amp;gt;&amp;gt; from string2string.search import KMPSearch&#xA;&amp;gt;&amp;gt;&amp;gt; knuth_morris_pratt = KMPSearch()&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; # Let&#39;s define a pattern and a text&#xA;&amp;gt;&amp;gt;&amp;gt; pattern = Jane Austen&#39;&#xA;&amp;gt;&amp;gt;&amp;gt; text = &#39;Sense and Sensibility, Pride and Prejudice, Emma, Mansfield Park, Northanger Abbey, Persuasion, and Lady Susan were written by Jane Austen and are important works of English literature.&#39;&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; # Now let&#39;s find the index of the pattern in the text, if it exists (otherwise, -1 is returned).&#xA;&amp;gt;&amp;gt;&amp;gt; idx = knuth_morris_pratt.search(pattern=pattern,text=text)&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; print(f&#39;The index of the pattern in the text is {idx}.&#39;)&#xA;# The index of the pattern in the text is 127.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Faiss Semantic Search&lt;/h3&gt; &#xA;&lt;p&gt;The example below demonstrates how to use the &lt;a href=&#34;https://github.com/facebookresearch/faiss&#34;&gt;Faiss&lt;/a&gt; tool developed by FAIR to perform semantic search. First, we use the &lt;a href=&#34;https://huggingface.co/facebook/bart-large&#34;&gt;BART-Large model from Hugging Face&lt;/a&gt; to generate embeddings for a small corpus of 25 sentences. To perform the search, we first encode a query sentence using the same BART model and use it to search the corpus. Specifically, we search for sentences that are most similar in meaning to the query sentence. After performing the search, we print the top thre sentences from the corpus that are most similar to the query sentence.&lt;/p&gt; &#xA;&lt;p&gt;This approach can be useful in a variety of natural-processing applications, such as question-answering and information retrieval, where it is essential to find relevant information quickly and accurately.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; # Let&#39;s create a FaissSearch class instance from the search module to perform semantic search&#xA;&amp;gt;&amp;gt;&amp;gt; from string2string.search import FaissSearch&#xA;&amp;gt;&amp;gt;&amp;gt; faiss_search = FaissSearch(model_name_or_path = &#39;facebook/bart-large&#39;)&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; # Let&#39;s create a corpus of strings (e.g., sentences)&#xA;&amp;gt;&amp;gt;&amp;gt; corpus = {&#xA;        &#39;text&#39;: [&#xA;            &#34;Coffee is my go-to drink in the morning.&#34;, &#xA;            &#34;I always try to make time for exercise.&#34;, &#xA;            &#34;Learning something new every day keeps me motivated.&#34;, &#xA;            &#34;The sunsets in my hometown are breathtaking.&#34;, &#xA;            &#34;I am grateful for the support of my friends and family.&#34;, &#xA;            &#34;The book I&#39;m reading is incredibly captivating.&#34;, &#xA;            &#34;I love listening to music while I work.&#34;, &#xA;            &#34;I&#39;m excited to try the new restaurant in town.&#34;, &#xA;            &#34;Taking a walk in nature always clears my mind.&#34;, &#xA;            &#34;I believe that kindness is the most important trait.&#34;, &#xA;            &#34;It&#39;s important to take breaks throughout the day.&#34;, &#xA;            &#34;I&#39;m looking forward to the weekend.&#34;, &#xA;            &#34;Reading before bed helps me relax.&#34;, &#xA;            &#34;I try to stay positive even in difficult situations.&#34;, &#xA;            &#34;Cooking is one of my favorite hobbies.&#34;, &#xA;            &#34;I&#39;m grateful for the opportunity to learn and grow every day.&#34;, &#xA;            &#34;I love traveling and experiencing new cultures.&#34;, &#xA;            &#34;I&#39;m proud of the progress I&#39;ve made so far.&#34;, &#xA;            &#34;A good night&#39;s sleep is essential for my well-being.&#34;, &#xA;            &#34;Spending time with loved ones always brings me joy.&#34;, &#xA;            &#34;I&#39;m grateful for the beauty of nature around me.&#34;, &#xA;            &#34;I try to live in the present moment and appreciate what I have.&#34;, &#xA;            &#34;I believe that honesty is always the best policy.&#34;, &#xA;            &#34;I enjoy challenging myself and pushing my limits.&#34;, &#xA;            &#34;I&#39;m excited to see what the future holds.&#34;&#xA;        ],&#xA;    }&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; # Next we need to initialize and encode the corpus&#xA;&amp;gt;&amp;gt;&amp;gt; faiss_search.initialize_corpus(&#xA;    corpus=corpus,&#xA;    section=&#39;text&#39;, &#xA;    embedding_type=&#39;mean_pooling&#39;,&#xA;    )&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; # Let&#39;s define a query, and the number of top results we want to retrieve; then, let&#39;s perform the semantic search.&#xA;&amp;gt;&amp;gt;&amp;gt; query = &#39;I like going for a run in the morning.&#39;&#xA;&amp;gt;&amp;gt;&amp;gt; top_k = 5&#xA;&amp;gt;&amp;gt;&amp;gt; top_k_results = faiss_search.search(query=query, k = top_k)&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; # Let&#39;s define a function to print the results of the search.&#xA;&amp;gt;&amp;gt;&amp;gt; def print_results(query, results, top_k):&#xA;        # Let&#39;s first print the query.&#xA;        print(f&#39;Query: &#34;{query}&#34;\n&#39;)&#xA;&#xA;        # Let&#39;s now print the top k results.&#xA;        print(f&#39;Top {top_k} most similar sentences in the corpus to the query (smallest score is most similar):&#39;)&#xA;        for i in range(top_k):&#xA;            print(f&#39; - {i+1}: &#34;{results[&#34;text&#34;][i]}&#34; with a similarity score of {top_k_results[&#34;score&#34;][i]:.2f}&#39;)&#xA;&amp;gt;&amp;gt;&amp;gt; print_results(query=query, results=top_k_results, top_k=top_k)&#xA;# Query: &#34;I like going for a run in the morning.&#34;&#xA;&#xA;# Top 3 most similar sentences in the corpus to the query (smallest score is most similar):&#xA;#  - 1: &#34;I always try to make time for exercise.&#34; with a similarity score of 170.65&#xA;#  - 2: &#34;The sunsets in my hometown are breathtaking.&#34; with a similarity score of 238.20&#xA;#  - 3: &#34;Coffee is my go-to drink in the morning.&#34; with a similarity score of 238.85&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Cosine Similarity (with Glove and fastText Embeddings)&lt;/h3&gt; &#xA;&lt;p&gt;The following example demonstrates how to use &lt;a href=&#34;https://nlp.stanford.edu/projects/glove/&#34;&gt;pre-trained GloVe embeddings&lt;/a&gt; to calculate the cosine similarity between different pairs of words. Specifically, we compute the cosine similarity between the embeddings of four words: &#34;cat&#34;, &#34;dog&#34;, &#34;phone&#34;, and &#34;computer&#34;. We then create a similarity matrix and use the plot_heatmap function in the visualization module to plot this matrix.&lt;/p&gt; &#xA;&lt;p&gt;Overall, this example provides a practical demonstration of how to use pre-trained embeddings such as &lt;a href=&#34;https://nlp.stanford.edu/projects/glove/&#34;&gt;GloVe&lt;/a&gt; and &lt;a href=&#34;https://fasttext.cc/&#34;&gt;fastText&lt;/a&gt; to quantify the semantic similarity between pairs of words, which can be useful in a variety of natural-language processing tasks.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; # Let&#39;s create a Cosine Similarity class instance from the similarity module&#xA;&amp;gt;&amp;gt;&amp;gt; from string2string.similarity import CosineSimilarity&#xA;&amp;gt;&amp;gt;&amp;gt; cosine_similarity = CosineSimilarity()&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; # Let&#39;s also create an instance of the GloVeEmbeddings class from the misc module to compute the embeddings of words&#xA;&amp;gt;&amp;gt;&amp;gt; from string2string.misc import GloVeEmbeddings&#xA;&amp;gt;&amp;gt;&amp;gt; glove = GloVeEmbeddings(model=&#39;glove.6B.200d&#39;, dim=50, force_download=True, dir=&#39;./models/glove-model/&#39;)&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; # Let&#39;s define a list of words&#xA;&amp;gt;&amp;gt;&amp;gt; words = [&#39;cat&#39;, &#39;dog&#39;, &#39;phone&#39;, &#39;computer&#39;]&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; # Let&#39;s create a list to store the embeddings of the words and compute them&#xA;&amp;gt;&amp;gt;&amp;gt; embeds = []&#xA;&amp;gt;&amp;gt;&amp;gt; for word in words:&#xA;&amp;gt;&amp;gt;&amp;gt;     embedding = glove.get_embedding(word)&#xA;&amp;gt;&amp;gt;&amp;gt;     embeds.append(embedding)&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; # Let&#39;s create a similarity matrix to store the cosine similarity between each pair of embeddings&#xA;&amp;gt;&amp;gt;&amp;gt; similarity_matrix = np.zeros((len(words), len(words)))&#xA;&amp;gt;&amp;gt;&amp;gt; for i in range(len(embeds)):&#xA;        similarity_matrix[i, i] = 1&#xA;        for j in range(i + 1, len(embeds)):&#xA;            result = cosine_similarity.compute(embeds[i], embeds[j], dim=1).item()&#xA;            similarity_matrix[i, j] = result&#xA;            similarity_matrix[j, i] = result&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; # Let&#39;s visualize the similarity matrix&#xA;&amp;gt;&amp;gt;&amp;gt; from string2string.misc.plotting_functions import plot_heatmap&#xA;&amp;gt;&amp;gt;&amp;gt; plot_heatmap(&#xA;        similarity_matrix, &#xA;        title=&#39;Similarity Between GloVe Embeddings&#39;,&#xA;        x_ticks = words,&#xA;        y_ticks = words,&#xA;        x_label = &#39;Words&#39;,&#xA;        y_label = &#39;Words&#39;,&#xA;        valfmt = &#39;{x:.2f}&#39;,&#xA;        cmap=&#34;Blues&#34;,&#xA;    )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/stanfordnlp/string2string/raw/main/fables/similarity-example.png&#34; class=&#34;center&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Metrics (sacreBLEU and ROUGE)&lt;/h3&gt; &#xA;&lt;p&gt;In the code snippet below, you can see an example of how to employ the sacreBLEU metric for calculating the BLEU score. This metric is implemented as a wrapper around the &lt;a href=&#34;https://github.com/mjpost/sacrebleu&#34;&gt;sacreBLEU library&lt;/a&gt;. The computation is performed by providing a list of candidate sentences and a list of reference sentences as input and calling the compute method of the metric instance.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; # Let&#39;s import the sacreBLEU metric from the metrics module and create an instance of it&#xA;&amp;gt;&amp;gt;&amp;gt; from string2string.metrics import sacreBLEU&#xA;&amp;gt;&amp;gt;&amp;gt; sbleu = sacreBLEU()&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; # Let&#39;s define a list of candidate sentences and a list of reference sentences&#xA;&amp;gt;&amp;gt;&amp;gt; candidates = [&#39;The sun is shining.&#39;, &#39;The birds are chirping.&#39;, &#39;She is playing the guitar.&#39;, &#39;He is cooking dinner.&#39;]&#xA;&amp;gt;&amp;gt;&amp;gt; references = [[&#39;The sun is shining.&#39;, &#39;The sun is bright.&#39;], [&#39;The birds are singing.&#39;, &#39;The harold is singing.&#39;], [&#39;Julie is playing the flute.&#39;, &#39;She is playing the piano.&#39;], [&#39;Chef is cooking dinner.&#39;, &#39;He is cooking lunch.&#39;]]&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; # Compute the sacreBLEU score&#xA;&amp;gt;&amp;gt;&amp;gt; result = sbleu.compute(candidates, references)&#xA;&amp;gt;&amp;gt;&amp;gt; print(result)&#xA;# {&#39;score&#39;: 67.92604743211312, &#39;counts&#39;: [19, 13, 9, 4], &#39;totals&#39;: [21, 17, 13, 9], &#39;precisions&#39;: [90.47619047619048, 76.47058823529412, 69.23076923076923, 44.44444444444444], &#39;bp&#39;: 1.0, &#39;sys_len&#39;: 21, &#39;ref_len&#39;: 21}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Similarly, we can use the ROUGE metric to calculate the ROUGE score. This metric is implemented as a wrapper around the &lt;a href=&#34;https://github.com/google-research/google-research/tree/master/rouge&#34;&gt;ROUGE library&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; # Let&#39;s import the ROUGE metric from the metrics module and create an instance of it&#xA;&amp;gt;&amp;gt;&amp;gt; from string2string.metrics import ROUGE&#xA;&amp;gt;&amp;gt;&amp;gt; rogue = ROUGE()&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; # Let&#39;s define a list of candidate sentences and a list of reference sentences&#xA;&amp;gt;&amp;gt;&amp;gt; candidates = [&#34;The cat is sitting on the mat.&#34;, &#34;The dog is barking at the mailman.&#34;, &#34;The bird is singing in the tree.&#34;] &#xA;&amp;gt;&amp;gt;&amp;gt; references = [[&#34;The cat is sitting on the mat.&#34;], [&#34;The dog is barking at the postman.&#34;], [&#34;The bird sings on the tree.&#34;]]&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; # Compute the ROUGE score&#xA;&amp;gt;&amp;gt;&amp;gt; result = rogue.compute(candidates, references)&#xA;&amp;gt;&amp;gt;&amp;gt; print(result)&#xA;# {&#39;rouge1&#39;: 0.8241758241758242, &#39;rouge2&#39;: 0.7323232323232324, &#39;rougeL&#39;: 0.8241758241758242, &#39;rougeLsum&#39;: 0.8241758241758242}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;BARTScore and BERTScore&lt;/h3&gt; &#xA;&lt;p&gt;This code snippet shows how to utilize the BARTScore and BERTScore metrics to compute their corresponding scores. These similarity metrics serve as wrappers around the BARTScore and BERTScore libraries.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; # Importing the BERTScore and BARTScore classes from the similarity module&#xA;&amp;gt;&amp;gt;&amp;gt; from string2string.similarity import BERTScore, BARTScore&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; # Initializing the BARTScore metric&#xA;&amp;gt;&amp;gt;&amp;gt; bart_scorer = BARTScore(model_name_or_path=&#39;facebook/bart-large-cnn&#39;)&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; # Initializing the BERTScore metric&#xA;&amp;gt;&amp;gt;&amp;gt; bert_scorer = BERTScore(lang=&#34;en&#34;)&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; # Define the source and target sentences&#xA;&amp;gt;&amp;gt;&amp;gt; source_sentences = [&#34;I&#39;m super happy today.&#34;, &#34;John von Neumann was a mathematician and physicist.&#34;]&#xA;&amp;gt;&amp;gt;&amp;gt; target_sentences = [&#34;I feel pretty wonderful today.&#34;, &#34;Kurt Vonnegut&#39;s Slaughterhouse Five had a profound impact on many people.&#34;]&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; # Compute the BARTScore&#xA;&amp;gt;&amp;gt;&amp;gt; score = bart_scorer.compute(source_sentences, target_sentences, agg=&#34;mean&#34;, batch_size=4)&#xA;&amp;gt;&amp;gt;&amp;gt; print(score)&#xA;# {&#39;score&#39;: array([-2.77911878, -4.60774326])}&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; # Compute the BERTScore&#xA;&amp;gt;&amp;gt;&amp;gt; score = bert_scorer.compute(source_sentences, target_sentences)&#xA;&amp;gt;&amp;gt;&amp;gt; print(score)&#xA;# {&#39;precision&#39;: array([0.943804 , 0.8559195], dtype=float32), &#39;recall&#39;: array([0.943804  , 0.85145634], dtype=float32), &#39;f1&#39;: array([0.943804 , 0.8536821], dtype=float32)}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Potential Future Enhancements and Improvements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Use the &lt;a href=&#34;https://numba.pydata.org/&#34;&gt;Numba library&lt;/a&gt; to accelerate the execution time of the algorithms.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Include additional string-based metrics in the miscellaneous module.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Implement &lt;a href=&#34;https://blast.ncbi.nlm.nih.gov/Blast.cgi&#34;&gt;BLAST&lt;/a&gt; and &lt;a href=&#34;https://www.ebi.ac.uk/Tools/sss/fasta/&#34;&gt;FASTA&lt;/a&gt; algorithms. [Work-in-Progress]&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add more customizable and useful visualization features.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{suzgun2023string2string,&#xA;  title={string2string: A Modern Python Library for String-to-String Algorithms},&#xA;  author={Suzgun, Mirac and Shieber, Stuart M and Jurafsky, Dan},&#xA;  journal={arXiv preprint arXiv:2304.14395},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Thanks&lt;/h2&gt; &#xA;&lt;p&gt;Our project owes a debt of gratitude to the following individuals for their contributions, comments, and feedback: Federico Bianchi, Corinna Coupette, Sebastian Gehrmann, Tayfun G√ºr, ≈ûule Kahraman, Deniz Kele≈ü, Luke Melas-Kyriazi, Christopher Manning, Tol√∫lop√© √íg√∫nr√®m√≠, Alexander &#34;Sasha&#34; Rush, Kyle Swanson, and Garrett Tanzer.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;If you are interested in contributing to this library, we encourage you to review our &lt;a href=&#34;https://string2string.readthedocs.io/en/latest/&#34;&gt;documentation&lt;/a&gt; first to gain a better understanding of the codebase&#39;s format and structure. Once you are familiar with the codebase, you are welcome to submit a pull request. We are looking for new contributors to help us drive our efforts forward!&lt;/p&gt;</summary>
  </entry>
</feed>