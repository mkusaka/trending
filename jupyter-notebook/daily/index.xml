<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-22T01:40:57Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Anil-matcha/openai-functions</title>
    <updated>2023-06-22T01:40:57Z</updated>
    <id>tag:github.com,2023-06-22:/Anil-matcha/openai-functions</id>
    <link href="https://github.com/Anil-matcha/openai-functions" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Demonstrate new Openai functions ability&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Openai functions&lt;/h1&gt; &#xA;&lt;p&gt;Demonstrate new Openai functions ability&lt;/p&gt; &#xA;&lt;p&gt;Open the notebook, click on open in colab Make a copy Add your openai key and run the notebook to get results&lt;/p&gt; &#xA;&lt;p&gt;You can change the default query to anything else&lt;/p&gt; &#xA;&lt;p&gt;Follow &lt;a href=&#34;https://twitter.com/matchaman11&#34;&gt;Anil Chandra Naidu Matcha&lt;/a&gt; on twitter for updates&lt;/p&gt; &#xA;&lt;p&gt;Subscribe to &lt;a href=&#34;https://www.youtube.com/@AnilChandraNaiduMatcha&#34;&gt;https://www.youtube.com/@AnilChandraNaiduMatcha&lt;/a&gt; for more such video tutorials&lt;/p&gt; &#xA;&lt;h3&gt;Also check&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Anil-matcha/Website-to-Chatbot&#34;&gt;Chat with Website code&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Anil-matcha/Chat-With-Excel&#34;&gt;Chat with CSV code&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Anil-matcha/Chat-Youtube&#34;&gt;Chat with Youtube code&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Anil-matcha/DiscordGPT&#34;&gt;ChatGPT in Discord code&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>dataflowr/notebooks</title>
    <updated>2023-06-22T01:40:57Z</updated>
    <id>tag:github.com,2023-06-22:/dataflowr/notebooks</id>
    <link href="https://github.com/dataflowr/notebooks" rel="alternate"></link>
    <summary type="html">&lt;p&gt;code for deep learning courses&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;a href=&#34;https://www.dataflowr.com/&#34;&gt;Dataflowr: Deep Learning DIY&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://dataflowr.github.io/website/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dataflowr/website/master/_assets/dataflowr_logo.png&#34; alt=&#34;Dataflowr&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Code and notebooks for the deep learning course &lt;a href=&#34;https://www.dataflowr.com/&#34;&gt;dataflowr&lt;/a&gt;. Here is the schedule followed at Ã©cole polytechnique in 2023:&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;ðŸŒ»&lt;/span&gt;Session&lt;span&gt;1âƒ£&lt;/span&gt; Finetuning VGG&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://dataflowr.github.io/website/modules/1-intro-general-overview/&#34;&gt;Module 1 - Introduction &amp;amp; General Overview&lt;/a&gt; Slides + notebook Dogs and Cats with VGG + Practicals (more dogs and cats)&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Things to remember&lt;/summary&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;you do not need to understand everything to run a deep learning model! But the main goal of this course will be to come back to each step done today and understand them...&lt;/li&gt; &#xA;   &lt;li&gt;to use the dataloader from Pytorch, you need to follow the API (i.e. for classification store your dataset in folders)&lt;/li&gt; &#xA;   &lt;li&gt;using a pretrained model and modifying it to adapt it to a similar task is easy.&lt;/li&gt; &#xA;   &lt;li&gt;if you do not understand why we take this loss, that&#39;s fine, we&#39;ll cover that in Module 3.&lt;/li&gt; &#xA;   &lt;li&gt;even with a GPU, avoid unnecessary computations!&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA; &lt;/blockquote&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;&lt;span&gt;ðŸŒ»&lt;/span&gt;Session&lt;span&gt;2âƒ£&lt;/span&gt; PyTorch tensors and Autodiff&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://dataflowr.github.io/website/modules/2a-pytorch-tensors/&#34;&gt;Module 2a - PyTorch tensors&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://dataflowr.github.io/website/modules/2b-automatic-differentiation/&#34;&gt;Module 2b - Automatic differentiation&lt;/a&gt; + Practicals&lt;/li&gt; &#xA;  &lt;li&gt;MLP from scratch start of &lt;a href=&#34;https://dataflowr.github.io/website/homework/1-mlp-from-scratch/&#34;&gt;HW1&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module2/AD_with_dual_numbers_Julia.ipynb&#34;&gt;another look at autodiff with dual numbers and Julia&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Things to remember&lt;/summary&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Pytorch tensors = Numpy on GPU + gradients!&lt;/li&gt; &#xA;   &lt;li&gt;in deep learning, &lt;a href=&#34;https://numpy.org/doc/stable/user/basics.broadcasting.html&#34;&gt;broadcasting&lt;/a&gt; is used everywhere. The rules are the same as for Numpy.&lt;/li&gt; &#xA;   &lt;li&gt;Automatic differentiation is not only the chain rule! Backpropagation algorithm (or dual numbers) is a clever algorithm to implement automatic differentiation...&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA; &lt;/blockquote&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;&lt;span&gt;ðŸŒ»&lt;/span&gt;Session&lt;span&gt;3âƒ£&lt;/span&gt;&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://dataflowr.github.io/website/modules/3-loss-functions-for-classification/&#34;&gt;Module 3 - Loss function for classification&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://dataflowr.github.io/website/modules/4-optimization-for-deep-learning/&#34;&gt;Module 4 - Optimization for deep learning&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://dataflowr.github.io/website/modules/5-stacking-layers/&#34;&gt;Module 5 - Stacking layers&lt;/a&gt; and overfitting a MLP on CIFAR10: &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module5/Stacking_layers_MLP_CIFAR10.ipynb&#34;&gt;Stacking_layers_MLP_CIFAR10.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://dataflowr.github.io/website/modules/6-convolutional-neural-network/&#34;&gt;Module 6: Convolutional neural network&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;how to regularize with dropout and uncertainty estimation with MC Dropout: &lt;a href=&#34;https://dataflowr.github.io/website/modules/15-dropout/&#34;&gt;Module 15 - Dropout&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Things to remember&lt;/summary&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Loss vs Accuracy. Know your loss for a classification task!&lt;/li&gt; &#xA;   &lt;li&gt;know your optimizer (Module 4)&lt;/li&gt; &#xA;   &lt;li&gt;know how to build a neural net with torch.nn.module (Module 5)&lt;/li&gt; &#xA;   &lt;li&gt;know how to use convolution and pooling layers (kernel, stride, padding)&lt;/li&gt; &#xA;   &lt;li&gt;know how to use dropout&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA; &lt;/blockquote&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;&lt;span&gt;ðŸŒ»&lt;/span&gt;Session&lt;span&gt;4âƒ£&lt;/span&gt;&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://dataflowr.github.io/website/modules/7-dataloading/&#34;&gt;Module 7 - Dataloading&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://dataflowr.github.io/website/modules/8a-embedding-layers/&#34;&gt;Module 8a - Embedding layers&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://dataflowr.github.io/website/modules/8b-collaborative-filtering/&#34;&gt;Module 8b - Collaborative filtering&lt;/a&gt; and build your own recommender system: &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module8/08_collaborative_filtering_empty.ipynb&#34;&gt;08_collaborative_filtering_empty.ipynb&lt;/a&gt; (on a larger dataset &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module8/08_collaborative_filtering_1M.ipynb&#34;&gt;08_collaborative_filtering_1M.ipynb&lt;/a&gt;)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://dataflowr.github.io/website/modules/8c-word2vec/&#34;&gt;Module 8c - Word2vec&lt;/a&gt; and build your own word embedding &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module8/08_Word2vec_pytorch_empty.ipynb&#34;&gt;08_Word2vec_pytorch_empty.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://dataflowr.github.io/website/modules/16-batchnorm/&#34;&gt;Module 16 - Batchnorm&lt;/a&gt; and check your understanding with &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module16/16_simple_batchnorm_eval.ipynb&#34;&gt;16_simple_batchnorm_eval.ipynb&lt;/a&gt; and more &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module16/16_batchnorm_simple.ipynb&#34;&gt;16_batchnorm_simple.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://dataflowr.github.io/website/modules/17-resnets/&#34;&gt;Module 17 - Resnets&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;start of &lt;a href=&#34;https://dataflowr.github.io/website/homework/2-CAM-adversarial/&#34;&gt;Homework 2: Class Activation Map and adversarial examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Things to remember&lt;/summary&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;know how to use dataloader&lt;/li&gt; &#xA;   &lt;li&gt;to deal with categorical variables in deep learning, use embeddings&lt;/li&gt; &#xA;   &lt;li&gt;in the case of word embedding, starting in an unsupervised setting, we built a supervised task (i.e. predicting central / context words in a window) and learned the representation thanks to negative sampling&lt;/li&gt; &#xA;   &lt;li&gt;know your batchnorm&lt;/li&gt; &#xA;   &lt;li&gt;architectures with skip connections allows deeper models&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA; &lt;/blockquote&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;&lt;span&gt;ðŸŒ»&lt;/span&gt;Session&lt;span&gt;5âƒ£&lt;/span&gt;&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://dataflowr.github.io/website/modules/9a-autoencoders/&#34;&gt;Module 9a: Autoencoders&lt;/a&gt; and code your noisy autoencoder &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module9/09_AE_NoisyAE.ipynb&#34;&gt;09_AE_NoisyAE.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;&#34;&gt;Module 10: Generative Adversarial Networks&lt;/a&gt; and code your GAN, Conditional GAN and InfoGAN &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module10/10_GAN_double_moon.ipynb&#34;&gt;10_GAN_double_moon.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://dataflowr.github.io/website/modules/13-siamese/&#34;&gt;Module 13: Siamese Networks and Representation Learning&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;start of &lt;a href=&#34;https://dataflowr.github.io/website/homework/3-VAE/&#34;&gt;Homework 3: VAE for MNIST clustering and generation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;&lt;span&gt;ðŸŒ»&lt;/span&gt;Session&lt;span&gt;6âƒ£&lt;/span&gt;&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://dataflowr.github.io/website/modules/11a-recurrent-neural-networks-theory/&#34;&gt;Module 11a - Recurrent Neural Networks theory&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://dataflowr.github.io/website/modules/11b-recurrent-neural-networks-practice/&#34;&gt;Module 11b - Recurrent Neural Networks practice&lt;/a&gt; and predict engine failure with &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module11/11_predicitions_RNN_empty.ipynb&#34;&gt;11_predicitions_RNN_empty.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://dataflowr.github.io/website/modules/11c-batches-with-sequences/&#34;&gt;Module 11c - Batches with sequences in Pytorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;&lt;span&gt;ðŸŒ»&lt;/span&gt;Session&lt;span&gt;7âƒ£&lt;/span&gt;&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://dataflowr.github.io/website/modules/12-attention/&#34;&gt;Module 12 - Attention and Transformers&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Correcting the PyTorch tutorial on attention in seq2seq: &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module12/12_seq2seq_attention.ipynb&#34;&gt;12_seq2seq_attention.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Build your own microGPT: &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module12/GPT_hist.ipynb&#34;&gt;GPT_hist.ipynb&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;&lt;span&gt;ðŸŒ»&lt;/span&gt;Session&lt;span&gt;8âƒ£&lt;/span&gt;&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://dataflowr.github.io/website/modules/9b-unet/&#34;&gt;Module 9b - UNets&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://dataflowr.github.io/website/modules/9c-flows/&#34;&gt;Module 9c - Flows&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Build your own Real NVP: &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module9/Normalizing_flows_empty.ipynb&#34;&gt;Normalizing_flows_empty.ipynb&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;&lt;span&gt;ðŸŒ»&lt;/span&gt;Session&lt;span&gt;9âƒ£&lt;/span&gt;&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://dataflowr.github.io/website/modules/18a-diffusion/&#34;&gt;Module 18a - Denoising Diffusion Probabilistic Models&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Train your own DDPM on MNIST: &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module18/ddpm_nano_empty.ipynb&#34;&gt;ddpm_nano_empty.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Finetuning on CIFAR10: &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module18/ddpm_micro_sol.ipynb&#34;&gt;ddpm_micro_sol.ipynb&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;For more updates: &lt;a href=&#34;https://twitter.com/marc_lelarge&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/url/https/twitter.com/marc_lelarge.svg?style=social&amp;amp;label=Follow%20%40marc_lelarge&#34; alt=&#34;Twitter URL&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;&lt;span&gt;ðŸŒ»&lt;/span&gt; All notebooks&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://dataflowr.github.io/website/modules/1-intro-general-overview/&#34;&gt;&lt;strong&gt;Module 1: Introduction &amp;amp; General Overview&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Intro: finetuning VGG for dogs vs cats &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module1/01_intro.ipynb&#34;&gt;01_intro.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Practical: Using CNN for more dogs and cats &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module1/01_practical_empty.ipynb&#34;&gt;01_practical_empty.ipynb&lt;/a&gt; and its solution &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module1/sol/01_practical_sol.ipynb&#34;&gt;01_practical_sol.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://dataflowr.github.io/website/modules/2a-pytorch-tensors/&#34;&gt;&lt;strong&gt;Module 2: Pytorch tensors and automatic differentiation&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Basics on PyTorch tensors and automatic differentiation &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module2/02a_basics.ipynb&#34;&gt;02a_basics.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Linear regression from numpy to pytorch &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module2/02b_linear_reg.ipynb&#34;&gt;02b_linear_reg.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Practical: implementing backprop from scratch &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module2/02_backprop.ipynb&#34;&gt;02_backprop.ipynb&lt;/a&gt; and its solution &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module2/sol/02_backprop_sol.ipynb&#34;&gt;02_backprop_sol.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Bonus: intro to JAX: autodiff the functional way &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module2/autodiff_functional_empty.ipynb&#34;&gt;autodiff_functional_empty.ipynb&lt;/a&gt; and its solution &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module2/autodiff_functional_sol.ipynb&#34;&gt;autodiff_functional_sol.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Bonus: Linear regression in JAX &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module2/linear_regression_jax.ipynb&#34;&gt;linear_regression_jax.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Bonus: automatic differentiation with dual numbers &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module2/AD_with_dual_numbers_Julia.ipynb&#34;&gt;AD_with_dual_numbers_Julia.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://dataflowr.github.io/website/homework/1-mlp-from-scratch/&#34;&gt;&lt;strong&gt;Homework 1: MLP from scratch&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/HW1/hw1_mlp.ipynb&#34;&gt;hw1_mlp.ipynb&lt;/a&gt; and its solution &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/HW1/sol/hw1_mlp_sol.ipynb&#34;&gt;hw1_mlp_sol.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://dataflowr.github.io/website/modules/3-loss-functions-for-classification/&#34;&gt;&lt;strong&gt;Module 3: Loss functions for classification&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;An explanation of underfitting and overfitting with polynomial regression &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module3/03_polynomial_regression.ipynb&#34;&gt;03_polynomial_regression.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://dataflowr.github.io/website/modules/4-optimization-for-deep-learning/&#34;&gt;&lt;strong&gt;Module 4: Optimization for deep leaning&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Practical: code Adagrad, RMSProp, Adam, AMSGrad &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module4/04_gradient_descent_optimization_algorithms_empty.ipynb&#34;&gt;04_gradient_descent_optimization_algorithms_empty.ipynb&lt;/a&gt; and its solution &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module4/sol/04_gradient_descent_optimization_algorithms_sol.ipynb&#34;&gt;04_gradient_descent_optimization_algorithms_sol.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://dataflowr.github.io/website/modules/5-stacking-layers/&#34;&gt;&lt;strong&gt;Module 5: Stacking layers&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Practical: overfitting a MLP on CIFAR10 &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module5/Stacking_layers_MLP_CIFAR10.ipynb&#34;&gt;Stacking_layers_MLP_CIFAR10.ipynb&lt;/a&gt; and its solution &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module5/sol/MLP_CIFAR10.ipynb&#34;&gt;MLP_CIFAR10.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://dataflowr.github.io/website/modules/6-convolutional-neural-network/&#34;&gt;&lt;strong&gt;Module 6: Convolutional neural network&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Practical: build a simple digit recognizer with CNN &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module6/06_convolution_digit_recognizer.ipynb&#34;&gt;06_convolution_digit_recognizer.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://dataflowr.github.io/website/homework/2-CAM-adversarial/&#34;&gt;&lt;strong&gt;Homework 2: Class Activation Map and adversarial examples&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/HW2/HW2_CAM_Adversarial.ipynb&#34;&gt;HW2_CAM_Adversarial.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://dataflowr.github.io/website/modules/8a-embedding-layers/&#34;&gt;&lt;strong&gt;Module 8: Embedding layers&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://dataflowr.github.io/website/modules/8b-collaborative-filtering/&#34;&gt;&lt;strong&gt;Collaborative filtering&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://dataflowr.github.io/website/modules/8c-word2vec/&#34;&gt;&lt;strong&gt;Word2vec&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Practical: Collaborative filtering with Movielens 100k dataset &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module8/08_collaborative_filtering_empty.ipynb&#34;&gt;08_collaborative_filtering_empty.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Practical: Refactoring code, collaborative filtering with Movielens 1M dataset &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module8/08_collaborative_filtering_1M.ipynb&#34;&gt;08_collaborative_filtering_1M.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Practical: Word Embedding (word2vec) in PyTorch &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module8/08_Word2vec_pytorch_empty.ipynb&#34;&gt;08_Word2vec_pytorch_empty.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Finding Synonyms and Analogies with Glove &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module8/08_Playing_with_word_embedding.ipynb&#34;&gt;08_Playing_with_word_embedding.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://dataflowr.github.io/website/modules/9-autoencoders/&#34;&gt;&lt;strong&gt;Module 9a: Autoencoders&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Practical: denoising autoencoder (with convolutions and transposed convolutions) &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module9/09_AE_NoisyAE.ipynb&#34;&gt;09_AE_NoisyAE.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://dataflowr.github.io/website/modules/9b-unet/&#34;&gt;&lt;strong&gt;Module 9b - UNets&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;UNet for image segmentation &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module9/UNet_image_seg.ipynb&#34;&gt;UNet_image_seg.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://dataflowr.github.io/website/modules/9c-flows/&#34;&gt;&lt;strong&gt;Module 9c - Flows&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;implementing Real NVP &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module9/Normalizing_flows_empty.ipynb&#34;&gt;Normalizing_flows_empty.ipynb&lt;/a&gt; and its solution &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module9/Normalizing_flows_sol.ipynb&#34;&gt;Normalizing_flows_sol.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://dataflowr.github.io/website/modules/10-generative-adversarial-networks/&#34;&gt;&lt;strong&gt;Module 10 - Generative Adversarial Networks&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Conditional GAN and InfoGAN &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module10/10_GAN_double_moon.ipynb&#34;&gt;10_GAN_double_moon.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://dataflowr.github.io/website/modules/11b-recurrent-neural-networks-practice/&#34;&gt;&lt;strong&gt;Module 11 - Recurrent Neural Networks&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://dataflowr.github.io/website/modules/11c-batches-with-sequences/&#34;&gt;&lt;strong&gt;Batches with sequences in Pytorch&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;notebook used in the theory course: &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module11/11_RNN.ipynb&#34;&gt;11_RNN.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;predicting engine failure with RNN &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module11/11_predicitions_RNN_empty.ipynb&#34;&gt;11_predicitions_RNN_empty.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://dataflowr.github.io/website/modules/12-attention/&#34;&gt;&lt;strong&gt;Module 12 - Attention and Transformers&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Correcting the &lt;a href=&#34;https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html&#34;&gt;PyTorch tutorial&lt;/a&gt; on attention in seq2seq: &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module12/12_seq2seq_attention.ipynb&#34;&gt;12_seq2seq_attention.ipynb&lt;/a&gt; and its &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module12/12_seq2seq_attention_solution.ipynb&#34;&gt;solution&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;building a simple transformer block and thinking like transformers: &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module12/GPT_hist.ipynb&#34;&gt;GPT_hist.ipynb&lt;/a&gt; and its &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module12/GPT_hist_sol.ipynb&#34;&gt;solution&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://dataflowr.github.io/website/modules/13-siamese/&#34;&gt;&lt;strong&gt;Module 13 - Siamese Networks and Representation Learning&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;learning embeddings with contrastive loss: &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module13/13_siamese_triplet_mnist_empty.ipynb&#34;&gt;13_siamese_triplet_mnist_empty.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://dataflowr.github.io/website/modules/15-dropout/&#34;&gt;&lt;strong&gt;Module 15 - Dropout&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Dropout on a toy dataset: &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module15/15a_dropout_intro.ipynb&#34;&gt;15a_dropout_intro.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;playing with dropout on MNIST: &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module15/15b_dropout_mnist.ipynb&#34;&gt;15b_dropout_mnist.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://dataflowr.github.io/website/modules/16-batchnorm/&#34;&gt;&lt;strong&gt;Module 16 - Batchnorm&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;impact of batchnorm: &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module16/16_batchnorm_simple.ipynb&#34;&gt;16_batchnorm_simple.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Playing with batchnorm without any training: &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module16/16_simple_batchnorm_eval.ipynb&#34;&gt;16_simple_batchnorm_eval.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://dataflowr.github.io/website/modules/18a-diffusion/&#34;&gt;&lt;strong&gt;Module 18a - Denoising Diffusion Probabilistic Models&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Denoising Diffusion Probabilistic Models for MNIST: &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module18/ddpm_nano_empty.ipynb&#34;&gt;ddpm_nano_empty.ipynb&lt;/a&gt; and its solution &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module18/ddpm_nano_sol.ipynb&#34;&gt;ddpm_nano_sol.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Denoising Diffusion Probabilistic Models for CIFAR10: &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/Module18/ddpm_micro_sol.ipynb&#34;&gt;ddpm_micro_sol.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://dataflowr.github.io/website/modules/graph0/&#34;&gt;&lt;strong&gt;Module - Deep Learning on graphs&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Inductive bias in GCN: a spectral perspective &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/graphs/GCN_inductivebias_spectral.ipynb&#34;&gt;GCN_inductivebias_spectral.ipynb&lt;/a&gt; and for colab &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/graphs/GCN_inductivebias_spectral-colab.ipynb&#34;&gt;GCN_inductivebias_spectral-colab.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Graph ConvNets in PyTorch &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/graphs/spectral_gnn.ipynb&#34;&gt;spectral_gnn.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;NERF&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;PyTorch Tiny NERF &lt;a href=&#34;https://github.com/dataflowr/notebooks/raw/master/nerf/tiny_nerf_extended.ipynb&#34;&gt;tiny_nerf_extended.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;If you want to run locally, follow the instructions of &lt;a href=&#34;https://dataflowr.github.io/website/modules/0-sotfware-installation/&#34;&gt;Module 0 - Running the notebooks locally&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;2020 version of the course&lt;/h2&gt; &#xA;&lt;p&gt;Archives are available on the archive-2020 branch.&lt;/p&gt;</summary>
  </entry>
</feed>