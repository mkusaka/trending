<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-01-23T01:40:12Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>neelnanda-io/TransformerLens</title>
    <updated>2023-01-23T01:40:12Z</updated>
    <id>tag:github.com,2023-01-23:/neelnanda-io/TransformerLens</id>
    <link href="https://github.com/neelnanda-io/TransformerLens" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TransformerLens&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/transformer-lens/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/transformer-lens&#34; alt=&#34;Pypi&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;(Formerly known as EasyTransformer)&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://neelnanda.io/transformer-lens-demo&#34;&gt;Start Here&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h2&gt;A Library for Mechanistic Interpretability of Generative Language Models&lt;/h2&gt; &#xA;&lt;p&gt;This is a library for doing &lt;a href=&#34;https://distill.pub/2020/circuits/zoom-in/&#34;&gt;mechanistic interpretability&lt;/a&gt; of GPT-2 Style language models. The goal of mechanistic interpretability is to take a trained model and reverse engineer the algorithms the model learned during training from its weights. It is a fact about the world today that we have computer programs that can essentially speak English at a human level (GPT-3, PaLM, etc), yet we have no idea how they work nor how to write one ourselves. This offends me greatly, and I would like to solve this!&lt;/p&gt; &#xA;&lt;p&gt;TransformerLens lets you load in an open source language model, like GPT-2, and exposes the internal activations of the model to you. You can cache any internal activation in the model, and add in functions to edit, remove or replace these activations as the model runs. The core design principle I&#39;ve followed is to enable exploratory analysis. One of the most fun parts of mechanistic interpretability compared to normal ML is the extremely short feedback loops! The point of this library is to keep the gap between having an experiment idea and seeing the results as small as possible, to make it easy for &lt;strong&gt;research to feel like play&lt;/strong&gt; and to enter a flow state. Part of what I aimed for is to make &lt;em&gt;my&lt;/em&gt; experience of doing research easier and more fun, hopefully this transfers to you!&lt;/p&gt; &#xA;&lt;p&gt;I used to work for the &lt;a href=&#34;https://raw.githubusercontent.com/neelnanda-io/TransformerLens/main/transformer-circuits.pub&#34;&gt;Anthropic interpretability team&lt;/a&gt;, and I wrote this library because after I left and tried doing independent research, I got extremely frustrated by the state of open source tooling. There&#39;s a lot of excellent infrastructure like HuggingFace and DeepSpeed to &lt;em&gt;use&lt;/em&gt; or &lt;em&gt;train&lt;/em&gt; models, but very little to dig into their internals and reverse engineer how they work. &lt;strong&gt;This library tries to solve that&lt;/strong&gt;, and to make it easy to get into the field even if you don&#39;t work at an industry org with real infrastructure! One of the great things about mechanistic interpretability is that you don&#39;t need large models or tons of compute. There are lots of important open problems that can be solved with a small model in a Colab notebook!&lt;/p&gt; &#xA;&lt;p&gt;The core features were heavily inspired by the interface to &lt;a href=&#34;https://transformer-circuits.pub/2021/garcon/index.html&#34;&gt;Anthropic&#39;s excellent Garcon tool&lt;/a&gt;. Credit to Nelson Elhage and Chris Olah for building Garcon and showing me the value of good infrastructure for enabling exploratory research!&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Start with the &lt;a href=&#34;https://neelnanda.io/transformer-lens-demo&#34;&gt;main demo&lt;/a&gt; to learn how the library works, and the basic features&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To see what using it for exploratory analysis in practice looks like, check out &lt;a href=&#34;https://neelnanda.io/exploratory-analysis-demo&#34;&gt;my notebook analysing Indirect Objection Identification&lt;/a&gt; or &lt;a href=&#34;https://www.youtube.com/watch?v=yo4QvDn-vsU&#34;&gt;my recording of myself doing research&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;p&gt;Mechanistic interpretability is a very young and small field, and there are a &lt;em&gt;lot&lt;/em&gt; of open problems - if you would like to help, please try working on one! &lt;strong&gt;Check out my &lt;a href=&#34;https://docs.google.com/document/d/1WONBzNqfKIxERejrrPlQMyKqg7jSFW92x5UMXNrMdPo/edit&#34;&gt;list of concrete open problems&lt;/a&gt; to figure out where to start.&lt;/strong&gt;. It begins with advice on skilling up, and key resources to check out.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;re new to transformers, check out my &lt;a href=&#34;https://neelnanda.io/transformer-tutorial&#34;&gt;what is a transformer tutorial&lt;/a&gt; and &lt;a href=&#34;https://neelnanda.io/transformer-tutorial-2&#34;&gt;tutorial on coding GPT-2 from scratch&lt;/a&gt; (with &lt;a href=&#34;https://neelnanda.io/transformer-template&#34;&gt;an accompanying template&lt;/a&gt; to write one yourself!&lt;/p&gt; &#xA;&lt;h2&gt;Advice for Reading the Code&lt;/h2&gt; &#xA;&lt;p&gt;One significant design decision made was to have a single transformer implementation that could support a range of subtly different GPT-style models. This has the upside of interpretability code just working for arbitrary models when you change the model name in &lt;code&gt;HookedTransformer.from_pretrained&lt;/code&gt;! But it has the significant downside that the code implementing the model (in &lt;code&gt;HookedTransformer.py&lt;/code&gt; and &lt;code&gt;components.py&lt;/code&gt;) can be difficult to read. I recommend starting with my &lt;a href=&#34;https://neelnanda.io/transformer-solution&#34;&gt;Clean Transformer Demo&lt;/a&gt;, which is a clean, minimal implementation of GPT-2 with the same internal architecture and activation names as HookedTransformer, but is significantly clearer and better documented.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install git+https://github.com/neelnanda-io/TransformerLens&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Import the library with &lt;code&gt;import transformer_lens&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;(Note: This library used to be known as EasyTransformer, and some breaking changes have been made since the rename. If you need to use the old version with some legacy code, run &lt;code&gt;pip install git+https://github.com/neelnanda-io/TransformerLens@v1&lt;/code&gt;.)&lt;/p&gt; &#xA;&lt;h2&gt;Local Development&lt;/h2&gt; &#xA;&lt;h3&gt;DevContainer&lt;/h3&gt; &#xA;&lt;p&gt;For a one-click setup of your development environment, this project includes a &lt;a href=&#34;https://containers.dev/&#34;&gt;DevContainer&lt;/a&gt;. It can be used locally with &lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers&#34;&gt;VS Code&lt;/a&gt; or with &lt;a href=&#34;https://github.com/features/codespaces&#34;&gt;GitHub Codespaces&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Manual Setup&lt;/h3&gt; &#xA;&lt;p&gt;This project uses &lt;a href=&#34;https://python-poetry.org/docs/#installation&#34;&gt;Poetry&lt;/a&gt; for package management. Install as follows (this will also setup your virtual environment):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry config virtualenvs.in-project true&#xA;poetry install --with dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Optionally, if you want Jupyter Lab you can run &lt;code&gt;poetry run pip install jupyterlab&lt;/code&gt; (to install in the same virtual environment), and then run with &lt;code&gt;poetry run jupyter lab&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Then the library can be imported as &lt;code&gt;import transformer_lens&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Testing&lt;/h3&gt; &#xA;&lt;p&gt;If adding a feature, please add unit tests for it to the tests folder, and check that it hasn&#39;t broken anything major using the existing tests (install pytest and run it in the root TransformerLens/ directory)&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Please cite this library as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{nandatransformerlens2022,&#xA;    title  = {TransformerLens},&#xA;    author = {Nanda, Neel},&#xA;    url    = {https://github.com/neelnanda-io/TransformerLens},&#xA;    year   = {2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(This is my best guess for how citing software works, feel free to send a correction!) Also, if you&#39;re actually using this for your research, I&#39;d love to chat! Reach out at &lt;a href=&#34;mailto:neelnanda27@gmail.com&#34;&gt;neelnanda27@gmail.com&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>PeiranLi0930/DL-Algorithms</title>
    <updated>2023-01-23T01:40:12Z</updated>
    <id>tag:github.com,2023-01-23:/PeiranLi0930/DL-Algorithms</id>
    <link href="https://github.com/PeiranLi0930/DL-Algorithms" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The Research Questions in Machine Learning&lt;/p&gt;&lt;hr&gt;</summary>
  </entry>
</feed>