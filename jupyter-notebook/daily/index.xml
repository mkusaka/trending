<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-25T01:37:08Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>LC1332/Chinese-alpaca-lora</title>
    <updated>2023-03-25T01:37:08Z</updated>
    <id>tag:github.com,2023-03-25:/LC1332/Chinese-alpaca-lora</id>
    <link href="https://github.com/LC1332/Chinese-alpaca-lora" rel="alternate"></link>
    <summary type="html">&lt;p&gt;éª†é©¼:A Chinese finetuned instruction LLaMA. Developed by é™ˆå¯æº @ åä¸­å¸ˆèŒƒå¤§å­¦ &amp; æé²é² @ å•†æ±¤ç§‘æŠ€ &amp; å†·å­æ˜‚ @ å•†æ±¤ç§‘æŠ€&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;éª†é©¼(Luotuo): Chinese-alpaca-lora&lt;/h1&gt; &#xA;&lt;p&gt;éª†é©¼(Luotuo) is the Chinese pinyin(pronunciation) of camel&lt;/p&gt; &#xA;&lt;p&gt;A Chinese finetuned instruction LLaMA. Developed by å†·å­æ˜‚ @ å•†æ±¤ç§‘æŠ€, é™ˆå¯æº @ åä¸­å¸ˆèŒƒå¤§å­¦(Third year undergraduate student) and æé²é² @ å•†æ±¤ç§‘æŠ€&lt;/p&gt; &#xA;&lt;p&gt;(email: &lt;a href=&#34;mailto:chengli@sensetime.com&#34;&gt;chengli@sensetime.com&lt;/a&gt;, &lt;a href=&#34;mailto:zaleng@bu.edu&#34;&gt;zaleng@bu.edu&lt;/a&gt;, &lt;a href=&#34;mailto:chenqiyuan1012@foxmail.com&#34;&gt;chenqiyuan1012@foxmail.com&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/LC1332/Chinese-alpaca-lora/main/image/camel_back.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;This is NOT an official product of SenseTime&lt;/p&gt; &#xA;&lt;p&gt;We named project in Luotuo(Camel) because both LLaMA and alpaca are all belongs to Artiodactyla-Camelidae(å¶è¹„ç›®-éª†é©¼ç§‘)&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;p&gt;[2023-3-24] We&#39;ve just released CamelBell(é©¼é“ƒ): tuning Chinese LLM with very few data on GLM-6B via LoRA, try &lt;a href=&#34;https://colab.research.google.com/github/LC1332/Chinese-alpaca-lora/blob/main/notebook/TuoLing_evaluation_code.ipynb&#34;&gt;here&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/LC1332/Chinese-alpaca-lora/blob/main/notebook/TuoLing_evaluation_code.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; , we may create a new repo soon&lt;/p&gt; &#xA;&lt;p&gt;[2023-3-24] The Luotuo proj aim to study whether an En model cross-language learning to Ch via LoRA. We will soon release a Chinese LoRA project, CamelBell, which can be trained with less data based on a Chinese base model.&lt;/p&gt; &#xA;&lt;h2&gt;A Quick Start&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Colab Link&lt;/th&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;detail&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CamelBell quick evaluation&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/LC1332/Chinese-alpaca-lora/blob/main/notebook/TuoLing_evaluation_code.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Tuoling specific Evaluation Code&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;A quick evaluation&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/LC1332/Chinese-alpaca-lora/blob/main/notebook/evaluation_code.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Evaluation code with standard HuggingFace pipeline&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Bot with Interface&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/LC1332/Chinese-alpaca-lora/blob/main/notebook/ChatLuotuo.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Interactive Chatting Bot using Gradio&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Training Code&lt;/td&gt; &#xA;   &lt;td&gt;To be released&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Training code, run on colab&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Data Translation&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/LC1332/Chinese-alpaca-lora/blob/main/notebook/translate_json_data.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Translation alpaca.json into Chinese&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Trained Model&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model Name&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Training Data and Setting&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/qychen/luotuo-lora-7b-0.1/tree/main&#34;&gt;luotuo-lora-7b-0.1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Trained on Translated alpaca 52k data&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/silk-road/luotuo-lora-7b-0.3&#34;&gt;luotuo-lora-7b-0.3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Trained on Translated alpaca 52k data + guanaco, 1 epoch&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;luotuo-lora-7b-0.9&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;(In Plan) cleaned alpaca 52k + full guanaco&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;luotuo-lora-7b-0.3 the performance seems have significant improvement than 0.1, even just after 1 epoch.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/LC1332/Chinese-alpaca-lora/main/image/0.3result_1epoch.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;But 1 epoch cost us 7 hours to train, which cost more than $10. We will keep continue tuning it later.&lt;/p&gt; &#xA;&lt;h2&gt;Qualitative Result(0.3)&lt;/h2&gt; &#xA;&lt;p&gt;Evaluation code are now in notebook/evaluation_code.ipynb&lt;/p&gt; &#xA;&lt;h3&gt;Good Examples&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;Input: ä¸­å›½çš„é¦–éƒ½åœ¨å“ªé‡Œï¼Ÿ&#xA;Luotuo-Output: ä¸­å›½çš„é¦–éƒ½æ˜¯åŒ—äº¬ã€‚&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;Input: éº¦å½“åŠ³æ˜¯å“ªä¸ªå›½å®¶çš„ï¼Ÿ&#xA;Luotuo-Output: ç¾å›½ã€‚&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;Input: 2017å¹´ç¾å›½æ€»ç»Ÿæ˜¯è°&#xA;Luotuo-Output: 2017å¹´ç¾å›½æ€»ç»Ÿæ˜¯ç‰¹æœ—æ™®&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;Input: åä¸­å¸ˆèŒƒå¤§å­¦åœ¨å“ªé‡Œ&#xA;Luotuo-0.1-Output: åä¸­å¸ˆèŒƒå¤§å­¦ä½äºåŒ—äº¬&#xA;Luotuo-0.3-Output: åä¸­å¸ˆèŒƒå¤§å­¦åœ¨æ­¦æ±‰å¸‚ã€‚&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;this example failed in 0.1&lt;/p&gt; &#xA;&lt;h3&gt;Bad Examples&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;Input: å•†æ±¤ç§‘æŠ€æ˜¯åšä»€ä¹ˆçš„ï¼Ÿ&#xA;Luotuo-0.1-Output: å•†æ±¤ç§‘æŠ€æ˜¯ä¸€ç§æŠ€æœ¯ï¼Œå®ƒç”¨äºåˆ›å»ºäº’è”ç½‘åº”ç”¨ç¨‹åºå’ŒæœåŠ¡ã€‚&#xA;Luotuo-0.3-Output: å•†æ±¤ç§‘æŠ€æ˜¯ä¸€ç§æŠ€æœ¯ï¼Œå®ƒå¯ä»¥ç”¨äºåˆ›å»ºæ–°çš„äº§å“å’ŒæœåŠ¡ï¼Œä»¥å¢åŠ äº§å“å’ŒæœåŠ¡çš„å¸å¼•åŠ›ã€‚å®ƒå¯ä»¥ç”¨äºåˆ›å»ºæ–°çš„äº§å“å’ŒæœåŠ¡ï¼Œä»¥å¢åŠ äº§å“å’ŒæœåŠ¡çš„å¸å¼•åŠ›ã€‚å®ƒå¯ä»¥ç”¨äºåˆ›å»ºæ–°çš„äº§å“å’ŒæœåŠ¡ï¼Œä»¥å¢åŠ äº§å“å’ŒæœåŠ¡çš„å¸å¼•åŠ›ã€‚å®ƒå¯ä»¥ç”¨äºåˆ›å»ºæ–°çš„äº§å“å’ŒæœåŠ¡ï¼Œä»¥å¢åŠ äº§å“å’ŒæœåŠ¡çš„å¸å¼•åŠ›ã€‚å®ƒå¯ä»¥ç”¨äºåˆ›å»ºæ–°çš„äº§å“å’ŒæœåŠ¡ï¼Œä»¥å¢åŠ äº§å“å’ŒæœåŠ¡çš„å¸å¼•åŠ›ã€‚å®ƒå¯ä»¥ç”¨äºåˆ›å»ºæ–°çš„äº§å“å’ŒæœåŠ¡ï¼Œä»¥å¢åŠ äº§å“å’ŒæœåŠ¡çš„å¸å¼•åŠ›ã€‚å®ƒå¯ä»¥ç”¨äºåˆ›å»ºæ–°çš„&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;Input: çŒ«å’Œç‹—çš„å…³ç³»æ˜¯ä»€ä¹ˆ&#xA;Luotuo-Output: çŒ«å’Œç‹—ä¹‹é—´çš„å…³ç³»æ˜¯å‹å¥½çš„ã€‚å®ƒä»¬é€šå¸¸åœ¨ä¸€èµ·ä¼‘æ¯æˆ–ç©è€ã€‚çŒ«å’Œç‹—ä¹‹é—´çš„å…³ç³»é€šå¸¸å¾ˆå¥½ï¼Œå®ƒä»¬é€šå¸¸åœ¨ä¸€èµ·ä¼‘æ¯æˆ–ç©è€ã€‚çŒ«å’Œç‹—ä¹‹é—´çš„å…³ç³»é€šå¸¸å¾ˆå¥½ï¼Œå®ƒä»¬é€šå¸¸åœ¨ä¸€èµ·ä¼‘æ¯æˆ–ç©è€ã€‚çŒ«å’Œç‹—ä¹‹é—´çš„å…³ç³»é€šå¸¸å¾ˆå¥½ï¼Œå®ƒä»¬é€šå¸¸åœ¨ä¸€èµ·ä¼‘æ¯æˆ–ç©è€ã€‚çŒ«å’Œç‹—ä¹‹é—´çš„å…³ç³»é€šå¸¸å¾ˆå¥½ï¼Œå®ƒä»¬é€šå¸¸åœ¨ä¸€èµ·ä¼‘æ¯æˆ–ç©è€ã€‚çŒ«å’Œç‹—ä¹‹é—´çš„å…³ç³»é€šå¸¸å¾ˆå¥½ï¼Œå®ƒä»¬é€šå¸¸åœ¨ä¸€èµ·ä¼‘æ¯æˆ–ç©è€ã€‚çŒ«å’Œç‹—ä¹‹é—´çš„å…³ç³»é€šå¸¸&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;We have tuned a Chinese LLaMA model baed on &lt;a href=&#34;https://ai.facebook.com/blog/large-language-model-llama-meta-ai/&#34;&gt;LLaMA&lt;/a&gt;, &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca&lt;/a&gt;, &lt;a href=&#34;https://github.com/tloen/alpaca-lora&#34;&gt;Alpaca LoRA&lt;/a&gt;, &lt;a href=&#34;https://github.com/22-hours/cabrita&#34;&gt;cabrita&lt;/a&gt;, &lt;a href=&#34;https://github.com/masa3141/japanese-alpaca-lora&#34;&gt;Japanese-Alpaca-LoRA&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The training code in in cleaning, if you are in very hurry, check the Japanese project and simply change the json training data file name.&lt;/p&gt; &#xA;&lt;h2&gt;Data&lt;/h2&gt; &#xA;&lt;p&gt;This is an inbuilding project&lt;/p&gt; &#xA;&lt;p&gt;The training code only made a slightly change on the Japanese-Alpaca-LoRA&lt;/p&gt; &#xA;&lt;p&gt;A. &lt;a href=&#34;https://huggingface.co/qychen/luotuo-lora-7b-0.1/tree/main&#34;&gt;0.1 version model&lt;/a&gt; was trained on translated data, which translate the &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca/raw/main/alpaca_data.json&#34;&gt;alpaca_data.json&lt;/a&gt; to Chinese using ChatGPT API. We paid around US $30-45 to translate the full dataset to chinese. Translated data is available. (&lt;a href=&#34;https://raw.githubusercontent.com/LC1332/Chinese-alpaca-lora/main/data/trans_chinese_alpaca_data.json&#34;&gt;trans_chinese_alpaca_data.json&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;B. We are also plan to consider the data in &lt;a href=&#34;https://guanaco-model.github.io/&#34;&gt;Guanaco&lt;/a&gt; hikariming&#39;s &lt;a href=&#34;https://github.com/hikariming/alpaca_chinese_dataset&#34;&gt;alpaca_chinese_dataset&lt;/a&gt; and carbonz0â€˜s &lt;a href=&#34;https://github.com/carbonz0/alpaca-chinese-dataset&#34;&gt;alpaca-chinese-dataset&lt;/a&gt;, may updated it into later version.&lt;/p&gt; &#xA;&lt;p&gt;We plan to upload two different models A and B, because the provider of B claim the clean data will bring significant improvement.&lt;/p&gt; &#xA;&lt;h2&gt;Sponsorships(èµåŠ©)&lt;/h2&gt; &#xA;&lt;p&gt;Top 3 Sponsors&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Time&lt;/th&gt; &#xA;   &lt;th&gt;Sponsor&lt;/th&gt; &#xA;   &lt;th&gt;Amount&lt;/th&gt; &#xA;   &lt;th&gt;Balance&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2023/3/24&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/pandodao/botastic&#34;&gt;yiplee&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;512&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2023/3/24&lt;/td&gt; &#xA;   &lt;td&gt;Hijun&lt;/td&gt; &#xA;   &lt;td&gt;500&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2023/3/24&lt;/td&gt; &#xA;   &lt;td&gt;å€ª**&lt;/td&gt; &#xA;   &lt;td&gt;500&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;balance = 1706 now. Detailed balance see in &lt;a href=&#34;https://raw.githubusercontent.com/LC1332/Chinese-alpaca-lora/main/data/Sponsorship_and_balance.md&#34;&gt;sponsorship_and_balance.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;è¿™åŸæœ¬æ˜¯æˆ‘ä»¬çš„ä¸€ä¸ªä½œä¸šé¡¹ç›®ï¼Œæˆ‘ä»¬åŸæœ¬è®¡åˆ’è®­ç»ƒåˆ°1.0ä¸ºæ­¢ã€‚ä½†æ˜¯ç¤¾åŒºçš„çƒ­æƒ…è¶…è¿‡äº†æˆ‘ä»¬çš„æƒ³è±¡ã€‚å¦‚æœæ‚¨æ„¿æ„èµåŠ©æˆ‘ä»¬çš„é¡¹ç›®ï¼Œå¯ä»¥&lt;/p&gt; &#xA;&lt;p&gt;æ‰«æè¿™ä¸ª&lt;a href=&#34;https://s1.imagehub.cc/images/2023/03/23/fba44d198f0bb887089b4d8739363c0b.jpeg&#34;&gt;äºŒç»´ç &lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;å¹¶ä¸”åŠ è¿™ä¸ª&lt;a href=&#34;https://s1.imagehub.cc/images/2023/03/23/b69e4e47759132dd3d4bbafa7bd602aa.jpeg&#34;&gt;æ”¯ä»˜å®&lt;/a&gt;è´¦å·ï¼Œç•™ä¸‹æ‚¨çš„å§“å&lt;/p&gt; &#xA;&lt;p&gt;é¡¹ç›®çš„èµ„é‡‘æµå‘å°†è¢«å…¬å¼€ï¼Œæ‰€æœ‰çš„èµ„é‡‘å°†è¢«ç”¨äºæ•°æ®çš„æ ‡æ³¨ï¼Œè®­ç»ƒç®—åŠ›çš„è´­ä¹°æˆ–è€…åç»­å‘¨è¾¹äº§å“çš„å‘æ”¾ã€‚æ•°æ®å’Œç®—åŠ›çš„æçŒ®ä¹Ÿä¼šä¸€åŒæ€»ç»“åœ¨sponsorshipçš„è¡¨æ ¼ä¸­ã€‚å¤‡ç”¨é“¾æ¥ &lt;a href=&#34;https://raw.githubusercontent.com/LC1332/Chinese-alpaca-lora/main/image/sponser_QR_code.jpeg&#34;&gt;äºŒç»´ç &lt;/a&gt; , &lt;a href=&#34;https://raw.githubusercontent.com/LC1332/Chinese-alpaca-lora/main/image/alipay_friend.jpeg&#34;&gt;æ”¯ä»˜å®&lt;/a&gt;è´¦å·&lt;/p&gt; &#xA;&lt;p&gt;This was originally an exercise project for us, and we originally planned to train until version 1.0. However, the enthusiasm of the community exceeded our expectations. If you are willing to sponsor our project, you can scan this &lt;a href=&#34;https://raw.githubusercontent.com/LC1332/Chinese-alpaca-lora/main/image/sponser_QR_code.jpeg&#34;&gt;QR code&lt;/a&gt; and add &lt;a href=&#34;https://raw.githubusercontent.com/LC1332/Chinese-alpaca-lora/main/image/alipay_friend.jpeg&#34;&gt;this Alipay account&lt;/a&gt;, leaving your name.&lt;/p&gt; &#xA;&lt;p&gt;All funds will be used for data annotation, purchase of training computing power, or distribution of subsequent peripheral products.&lt;/p&gt; &#xA;&lt;h2&gt;TODO and Be a Contributor&lt;/h2&gt; &#xA;&lt;p&gt;It seems that there are many follow-up tasks to be done after the basic version is completed. Many developers in the community have put forward more friendly suggestions, and I have put a longer TODO list in &lt;a href=&#34;https://raw.githubusercontent.com/LC1332/Chinese-alpaca-lora/main/data/TODO_list.md&#34;&gt;TODO_list.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;inbuilding project&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; translate alpaca json data into Chinese&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; finetuning with lora(model 0.1)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; release 0.1 model (model A)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; model to hugging face, GUI demo&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; train lora with more alpaca data(model 0.3)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; train lora with more alpaca data(model 0.9)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We plan to use this Luotuo project as the git repository for the entire Chinese LLM project. After the completion of the original Luotuo: LLaMA-LoRA, it will be migrated to Luotuo-vanilla. The CamelBell, Loulan, Silk-Road and other derivative Chinese language model projects will gradually be added to the Luotuo project.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this project useful in your research, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{leng2023luotuo-ch-alpaca,&#xA;  title={Luotuo: Evaluating Cross-En-Ch-lingual training of LLM via Low Rank Adaption},&#xA;  publisher = {GitHub},&#xA;  author={Ziang Leng, Qiyuan Chen and Cheng Li},&#xA;  url={https://github.com/LC1332/Chinese-alpaca-lora},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>wangxuqi/Prompt-Engineering-Guide-Chinese</title>
    <updated>2023-03-25T01:37:08Z</updated>
    <id>tag:github.com,2023-03-25:/wangxuqi/Prompt-Engineering-Guide-Chinese</id>
    <link href="https://github.com/wangxuqi/Prompt-Engineering-Guide-Chinese" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Promptå·¥ç¨‹å¸ˆæŒ‡å—ï¼Œæºè‡ªè‹±æ–‡ç‰ˆï¼Œä½†å¢åŠ äº†AIGCçš„promptéƒ¨åˆ†ï¼Œä¸ºäº†é™ä½åŒå­¦ä»¬çš„å­¦ä¹ é—¨æ§›ï¼Œç¿»è¯‘æ›´æ–°&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Prompt-Engineering-Guide-Chinese&lt;/h1&gt; &#xA;&lt;p&gt;Promptå·¥ç¨‹å¸ˆæŒ‡å—ï¼Œæºè‡ªäºgithubä¸Šæœ€ç«çš„è‹±æ–‡æŒ‡å—ï¼Œä¸ºäº†é™ä½åŒå­¦ä»¬çš„å­¦ä¹ é—¨æ§› å®æ—¶æŒæ¡æœ€æ–°å­¦ä¹ å†…å®¹ï¼ŒæŒç»­æ›´æ–°ï¼Œæ¬¢è¿å…±åŒæ·»åŠ æ›´å¤šçš„promptæŒ‡å—&lt;/p&gt; &#xA;&lt;p&gt;Promptå·¥ç¨‹æ˜¯ä¸€ç§ç›¸å¯¹è¾ƒæ–°çš„å­¦ç§‘ï¼Œç”¨äºå¼€å‘å’Œä¼˜åŒ–æç¤ºï¼Œä»¥æœ‰æ•ˆåœ°ä½¿ç”¨è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰è¿›è¡Œå„ç§åº”ç”¨å’Œç ”ç©¶ä¸»é¢˜ã€‚Promptå·¥ç¨‹æŠ€èƒ½æœ‰åŠ©äºæ›´å¥½åœ°ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„èƒ½åŠ›å’Œå±€é™æ€§ã€‚ç ”ç©¶äººå‘˜ä½¿ç”¨Promptå·¥ç¨‹æ¥æ”¹å–„LLMsåœ¨å„ç§å¸¸è§å’Œå¤æ‚ä»»åŠ¡ä¸Šçš„èƒ½åŠ›ï¼Œä¾‹å¦‚é—®ç­”å’Œç®—æœ¯æ¨ç†ã€‚å¼€å‘äººå‘˜ä½¿ç”¨Promptå·¥ç¨‹æ¥è®¾è®¡å¼ºå¤§ä¸”æœ‰æ•ˆçš„æç¤ºæŠ€æœ¯ï¼Œä¸LLMså’Œå…¶ä»–å·¥å…·è¿›è¡Œæ¥å£ã€‚&lt;/p&gt; &#xA;&lt;p&gt;å‡ºäºå¯¹å¼€å‘LLMsçš„é«˜åº¦å…´è¶£ï¼Œæˆ‘ä»¬åˆ›å»ºäº†è¿™ä¸ªæ–°çš„Promptå·¥ç¨‹æŒ‡å—ï¼Œå…¶ä¸­åŒ…å«æ‰€æœ‰ä¸Promptå·¥ç¨‹ç›¸å…³çš„æœ€æ–°è®ºæ–‡ã€å­¦ä¹ æŒ‡å—ã€è®²åº§ã€å‚è€ƒèµ„æ–™å’Œå·¥å…·ã€‚&lt;/p&gt; &#xA;&lt;p&gt;ç¥æ‚¨æ„‰å¿«åœ°è¿›è¡ŒPromptå·¥ç¨‹ï¼&lt;/p&gt; &#xA;&lt;h2&gt;&lt;/h2&gt; &#xA;&lt;h2&gt;æŒ‡å—&lt;/h2&gt; &#xA;&lt;p&gt;ä»¥ä¸‹æ˜¯æˆ‘ä»¬å¼€å‘çš„ä¸€ç³»åˆ—Promptå·¥ç¨‹æŒ‡å—ã€‚è¿™äº›æŒ‡å—ä»åœ¨ä¸æ–­å®Œå–„ä¸­ã€‚&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wangxuqi/Prompt-Engineering-Guide-Chinese/main/guides/prompts-intro.md&#34;&gt;Promptå·¥ç¨‹-ç®€ä»‹&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wangxuqi/Prompt-Engineering-Guide-Chinese/main/guides/prompts-basic-usage.md&#34;&gt;Promptå·¥ç¨‹-åŸºæœ¬æç¤º&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wangxuqi/Prompt-Engineering-Guide-Chinese/main/guides/prompts-advanced-usage.md&#34;&gt;Promptå·¥ç¨‹-é«˜çº§æç¤º&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wangxuqi/Prompt-Engineering-Guide-Chinese/main/guides/prompts-applications.md&#34;&gt;Promptå·¥ç¨‹-åº”ç”¨&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wangxuqi/Prompt-Engineering-Guide-Chinese/main/guides/prompts-chatgpt.md&#34;&gt;Promptå·¥ç¨‹-ChatGPT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wangxuqi/Prompt-Engineering-Guide-Chinese/main/guides/prompts-adversarial.md&#34;&gt;Promptå·¥ç¨‹-å¯¹æŠ—æ€§æç¤º&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wangxuqi/Prompt-Engineering-Guide-Chinese/main/guides/prompts-reliability.md&#34;&gt;Promptå·¥ç¨‹-å¯é æ€§&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wangxuqi/Prompt-Engineering-Guide-Chinese/main/guides/prompts-miscellaneous.md&#34;&gt;Promptå·¥ç¨‹-å…¶ä»–ä¸»é¢˜&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wangxuqi/Prompt-Engineering-Guide-Chinese/main/pages/papers.mdx&#34;&gt;Promptå·¥ç¨‹-è®ºæ–‡&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wangxuqi/Prompt-Engineering-Guide-Chinese/main/pages/tools.mdx&#34;&gt;Promptå·¥ç¨‹-å·¥å…·&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wangxuqi/Prompt-Engineering-Guide-Chinese/main/pages/datasets.mdx&#34;&gt;Promptå·¥ç¨‹-æ•°æ®é›†&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wangxuqi/Prompt-Engineering-Guide-Chinese/main/pages/readings.mdx&#34;&gt;Promptå·¥ç¨‹-é™„åŠ é˜…è¯»&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>lxe/simple-llama-finetuner</title>
    <updated>2023-03-25T01:37:08Z</updated>
    <id>tag:github.com,2023-03-25:/lxe/simple-llama-finetuner</id>
    <link href="https://github.com/lxe/simple-llama-finetuner" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Simple UI for LLaMA Model Finetuning&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ğŸ¦™ Simple LLaMA Finetuner&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/lxe/simple-llama-finetuner/blob/master/Simple_LLaMA_FineTuner.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/lxe/no-bugs&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/no-bugs-brightgreen.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/lxe/onehundred/tree/master&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/coverage-%F0%9F%92%AF-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Simple LLaMA Finetuner is a beginner-friendly interface designed to facilitate fine-tuning the &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;LLaMA-7B&lt;/a&gt; language model using &lt;a href=&#34;https://arxiv.org/abs/2106.09685&#34;&gt;LoRA&lt;/a&gt; method via the &lt;a href=&#34;https://github.com/huggingface/peft&#34;&gt;PEFT library&lt;/a&gt; on commodity NVIDIA GPUs. With small dataset and sample lengths of 256, you can even run this on a regular Colab Tesla T4 instance.&lt;/p&gt; &#xA;&lt;p&gt;With this intuitive UI, you can easily manage your dataset, customize parameters, train, and evaluate the model&#39;s inference capabilities.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zphang/minimal-llama/&#34;&gt;https://github.com/zphang/minimal-llama/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tloen/alpaca-lora&#34;&gt;https://github.com/tloen/alpaca-lora&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/peft&#34;&gt;https://github.com/huggingface/peft&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/Anthropic/hh-rlhf&#34;&gt;https://huggingface.co/datasets/Anthropic/hh-rlhf&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Simply paste datasets in the UI, separated by double blank lines&lt;/li&gt; &#xA; &lt;li&gt;Adjustable parameters for fine-tuning and inference&lt;/li&gt; &#xA; &lt;li&gt;Beginner-friendly UI with explanations for each parameter&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Accelerate / DeepSpeed&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Load other models&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; More dataset preparation tools&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Linux or WSL&lt;/li&gt; &#xA; &lt;li&gt;Modern NVIDIA GPU with &amp;gt;16 GB of VRAM (but it might be possible to run with less for smaller sample lengths)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Usage&lt;/h3&gt; &#xA;&lt;p&gt;I recommend using a virtual environment to install the required packages. Conda preferred.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n llama-finetuner python=3.10&#xA;conda activate llama-finetuner&#xA;conda install -y cuda -c nvidia/label/cuda-11.7.0&#xA;conda install -y pytorch=1.13.1 pytorch-cuda=11.7 -c pytorch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On WSL, you might need to install CUDA manually by following &lt;a href=&#34;https://developer.nvidia.com/cuda-downloads?target_os=Linux&amp;amp;target_arch=x86_64&amp;amp;Distribution=WSL-Ubuntu&amp;amp;target_version=2.0&amp;amp;target_type=deb_local&#34;&gt;these steps&lt;/a&gt;, then running the following before you launch:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export LD_LIBRARY_PATH=/usr/lib/wsl/lib&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Clone the repository and install the required packages.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/lxe/simple-llama-finetuner.git&#xA;cd simple-llama-finetuner&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Launch it&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python main.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Open &lt;a href=&#34;http://127.0.0.1:7860/&#34;&gt;http://127.0.0.1:7860/&lt;/a&gt; in your browser. Prepare your training data by separating each sample with 2 blank lines. Paste the whole training dataset into the textbox. Specify the model name in the &#34;LoRA Model Name&#34; textbox, then click train. You might need to adjust the max sequence length and batch size to fit your GPU memory. The model will be saved in the &lt;code&gt;lora-{your model name}&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;p&gt;After training is done, navigate to &#34;Inference&#34; tab, click &#34;Reload Models&#34;, select your model, and play with it.&lt;/p&gt; &#xA;&lt;p&gt;Have fun!&lt;/p&gt; &#xA;&lt;h2&gt;Screenshots&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/1486609/226793136-84531388-4081-49bb-b982-3f47e6ec25cd.png&#34; alt=&#34;Image1&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/1486609/226809466-b1eb6f3f-4049-4a41-a2e3-52b06a6e1230.png&#34; alt=&#34;Image2&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;MIT License&lt;/p&gt; &#xA;&lt;p&gt;Copyright (c) 2023 Aleksey Smolenchuk&lt;/p&gt; &#xA;&lt;p&gt;Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the &#34;Software&#34;), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:&lt;/p&gt; &#xA;&lt;p&gt;The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.&lt;/p&gt; &#xA;&lt;p&gt;THE SOFTWARE IS PROVIDED &#34;AS IS&#34;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.&lt;/p&gt;</summary>
  </entry>
</feed>