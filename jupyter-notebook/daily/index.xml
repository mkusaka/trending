<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-02T01:38:49Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>FurkanGozukara/Stable-Diffusion</title>
    <updated>2023-06-02T01:38:49Z</updated>
    <id>tag:github.com,2023-06-02:/FurkanGozukara/Stable-Diffusion</id>
    <link href="https://github.com/FurkanGozukara/Stable-Diffusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Best Stable Diffusion and AI Tutorials, Guides, News, Tips and Tricks&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://discord.com/servers/software-engineering-courses-secourses-772774097734074388&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/772774097734074388?label=Discord&amp;amp;logo=discord&#34; alt=&#34;image&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hits.seeyoufarm.com&#34;&gt;&lt;img src=&#34;https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2FFurkanGozukara%2FStable-Diffusion&amp;amp;count_bg=%2379C83D&amp;amp;title_bg=%239E0F0F&amp;amp;icon=apachespark.svg&amp;amp;icon_color=%23E7E7E7&amp;amp;title=views&amp;amp;edge_flat=false&#34; alt=&#34;Hits&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/GozukaraFurkan&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/GozukaraFurkan?label=Follow&amp;amp;style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/SECourses&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/YouTube-Channel-red?style=for-the-badge&amp;amp;logo=youtube&#34; alt=&#34;YouTube Channel&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.patreon.com/your_patreon_page&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Patreon-Support%20Me-f96854?style=for-the-badge&amp;amp;logo=patreon&#34; alt=&#34;Patreon&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Expert-Level Tutorials on Stable Diffusion: Master Advanced Techniques and Strategies&lt;/h1&gt; &#xA;&lt;p&gt;Greetings everyone. I am Dr. Furkan Gözükara. I am an Assistant Professor in Software Engineering department of a private university (have PhD in Computer Engineering). My professional programming skill is unfortunately C# not Python :)&lt;/p&gt; &#xA;&lt;p&gt;My linkedin : &lt;a href=&#34;https://www.linkedin.com/in/furkangozukara/&#34;&gt;&lt;strong&gt;https://www.linkedin.com/in/furkangozukara&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Our channel address if you like to subscribe : &lt;a href=&#34;https://www.youtube.com/@SECourses&#34;&gt;&lt;strong&gt;https://www.youtube.com/@SECourses&lt;/strong&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;Our discord to get more help : &lt;a href=&#34;https://discord.com/servers/software-engineering-courses-secourses-772774097734074388&#34;&gt;&lt;strong&gt;https://discord.com/servers/software-engineering-courses-secourses-772774097734074388&lt;/strong&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;I am keeping this list up-to-date. I got upcoming new awesome video ideas. Trying to find time to do that.&lt;/p&gt; &#xA;&lt;h3&gt;I am open to any criticism you have. I am constantly trying to improve the quality of my tutorial guide videos. Please leave comments with both your suggestions and what you would like to see in future videos.&lt;/h3&gt; &#xA;&lt;h3&gt;All videos have manually fixed subtitles and properly prepared video chapters. You can watch with these perfect subtitles or look for the chapters you are interested in.&lt;/h3&gt; &#xA;&lt;p&gt;Since my profession is teaching, I usually do not skip any of the important parts. Therefore, you may find my videos a little bit longer.&lt;/p&gt; &#xA;&lt;p&gt;Playlist link on YouTube: &lt;a href=&#34;https://www.youtube.com/watch?v=mnCY8uM7E50&amp;amp;list=PL_pbwdIyffsmclLl0O144nQRnezKlNdx3&#34;&gt;&lt;strong&gt;Stable Diffusion Tutorials, Automatic1111 Web UI &amp;amp; Google Colab Guides, DreamBooth, Textual Inversion / Embedding, LoRA, AI Upscaling, Video to Anime&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;1.) Automatic1111 Web UI - PC - Free&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/B5U7LJOvH6g&#34;&gt;&lt;strong&gt;How To Install Python, Setup Virtual Environment VENV, Set Default Python System Path &amp;amp; Install Git&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/B5U7LJOvH6g&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/19240467/235155922-c9ebf609-e1d3-4bbf-8f00-bc181fa4a10b.png&#34; alt=&#34;image&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;2.) Automatic1111 Web UI - PC - Free&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=AZg6vzWHOTA&#34;&gt;&lt;strong&gt;Easiest Way to Install &amp;amp; Run Stable Diffusion Web UI on PC by Using Open Source Automatic Installer&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=AZg6vzWHOTA&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/19240467/218344261-aa236e18-152f-4287-b4fd-fa09c8f57a3f.png&#34; alt=&#34;image&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;3.) Automatic1111 Web UI - PC - Free&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=aAyvsX-EpG4&#34;&gt;&lt;strong&gt;How to use Stable Diffusion V2.1 and Different Models in the Web UI - SD 1.5 vs 2.1 vs Anything V3&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=aAyvsX-EpG4&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/19240467/218344276-af8f2fa2-4fdb-4454-9c8f-bd3ef4d2c92a.png&#34; alt=&#34;image&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;4.) Automatic1111 Web UI - PC - Free&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Bdl-jWR3Ukc&#34;&gt;&lt;strong&gt;Zero To Hero Stable Diffusion DreamBooth Tutorial By Using Automatic1111 Web UI - Ultra Detailed&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Bdl-jWR3Ukc&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/19240467/218344301-04f91cf4-fa35-4975-8c3d-9951c765839a.png&#34; alt=&#34;image&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;5.) Automatic1111 Web UI - PC - Free&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=KwxNcGhHuLY&#34;&gt;&lt;strong&gt;DreamBooth Got Buffed - 22 January Update - Much Better Success Train Stable Diffusion Models Web UI&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=KwxNcGhHuLY&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/19240467/218344369-97b68dd7-732d-4ca3-9acc-a87984ebe0f0.png&#34; alt=&#34;image&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;6.) Automatic1111 Web UI - PC - Free&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=s25hcW4zq4M&#34;&gt;&lt;strong&gt;How to Inject Your Trained Subject e.g. Your Face Into Any Custom Stable Diffusion Model By Web UI&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=s25hcW4zq4M&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/19240467/218344509-01d70965-aeea-4096-bc29-7a005b4d47a6.png&#34; alt=&#34;image&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;7.) Automatic1111 Web UI - PC - Free&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=mfaqqL5yOO4&#34;&gt;&lt;strong&gt;How To Do Stable Diffusion LORA Training By Using Web UI On Different Models - Tested SD 1.5, SD 2.1&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=mfaqqL5yOO4&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/19240467/218344459-bd4554b0-b57b-4079-aaea-ed93d8be95ed.png&#34; alt=&#34;image&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;8.) Automatic1111 Web UI - PC - Free&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=O01BrQwOd-Q&#34;&gt;&lt;strong&gt;8 GB LoRA Training - Fix CUDA &amp;amp; xformers For DreamBooth and Textual Inversion in Automatic1111 SD UI&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=O01BrQwOd-Q&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/19240467/218344491-52ac51d8-6556-4abc-b2fb-d640a46c48a2.png&#34; alt=&#34;image&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;9.) Automatic1111 Web UI - PC - Free&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=dNOpWt-epdQ&#34;&gt;&lt;strong&gt;How To Do Stable Diffusion Textual Inversion (TI) / Text Embeddings By Automatic1111 Web UI Tutorial&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=dNOpWt-epdQ&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/19240467/218344538-d5f0329d-b0e9-44ed-aaf0-5e4bb134afb7.png&#34; alt=&#34;image&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;10.) Automatic1111 Web UI - PC - Free&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=TBq1bhY8BOc&#34;&gt;&lt;strong&gt;How To Generate Stunning Epic Text By Stable Diffusion AI - No Photoshop - For Free - Depth-To-Image&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=TBq1bhY8BOc&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/19240467/218344579-fda1e9b8-a810-48af-9dcb-f47e87afee9e.png&#34; alt=&#34;image&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;11.) Python Code - Hugging Face Diffusers Script - PC - Free&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=-6CA18MS0pY&#34;&gt;&lt;strong&gt;How to Run and Convert Stable Diffusion Diffusers (.bin Weights) &amp;amp; Dreambooth Models to CKPT File&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=-6CA18MS0pY&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/19240467/218344677-3f812cf3-db37-4ccb-8f81-99b8a1d5ef00.png&#34; alt=&#34;image&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;12.) NMKD Stable Diffusion GUI - Open Source - PC - Free&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=EPRa8EZl9Os&#34;&gt;&lt;strong&gt;Forget Photoshop - How To Transform Images With Text Prompts using InstructPix2Pix Model in NMKD GUI&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=EPRa8EZl9Os&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/19240467/218344868-3232f875-b2c5-4caa-b59b-9d0fd683c06b.png&#34; alt=&#34;image&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;13.) Google Colab Free - Cloud - No PC Is Required&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=mnCY8uM7E50&#34;&gt;&lt;strong&gt;Transform Your Selfie into a Stunning AI Avatar with Stable Diffusion - Better than Lensa for Free&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=mnCY8uM7E50&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/19240467/218344900-286cded5-0171-4b9e-9354-7adf4bada612.png&#34; alt=&#34;image&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;14.) Google Colab Free - Cloud - No PC Is Required&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=kIyqAdd_i10&#34;&gt;&lt;strong&gt;Stable Diffusion Google Colab, Continue, Directory, Transfer, Clone, Custom Models, CKPT SafeTensors&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=kIyqAdd_i10&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/19240467/218344930-95956805-6a6e-46ee-8885-64043246d79b.png&#34; alt=&#34;image&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;15.) Automatic1111 Web UI - PC - Free&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=XiKyEKJrTLQ&#34;&gt;&lt;strong&gt;Become A Stable Diffusion Prompt Master By Using DAAM - Attention Heatmap For Each Used Token - Word&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=XiKyEKJrTLQ&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/19240467/218345146-54076e5d-230a-4774-8d6a-8358cbd15f78.png&#34; alt=&#34;image&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;16.) Python Script - Gradio Based - ControlNet - PC - Free&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=YJebdQ30UZQ&#34;&gt;&lt;strong&gt;Transform Your Sketches into Masterpieces with Stable Diffusion ControlNet AI - How To Use Tutorial&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=YJebdQ30UZQ&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/19240467/218345328-ada437bf-5eb4-478e-a951-84486a42995d.png&#34; alt=&#34;image&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;17.) Automatic1111 Web UI - PC - Free&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=vhqqmkTBMlU&#34;&gt;&lt;strong&gt;Sketches into Epic Art with 1 Click: A Guide to Stable Diffusion ControlNet in Automatic1111 Web UI&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=vhqqmkTBMlU&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/19240467/218806127-c84d1ff8-d5bb-41b0-bdef-6922568792b9.png&#34; alt=&#34;image&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;18.) RunPod - Automatic1111 Web UI - Cloud - Paid - No PC Is Required&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=QN1vdGhjcRc&#34;&gt;&lt;strong&gt;Ultimate RunPod Tutorial For Stable Diffusion - Automatic1111 - Data Transfers, Extensions, CivitAI&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=QN1vdGhjcRc&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/19240467/219958249-82ecb925-901b-4f87-b776-f592b0f5eaad.png&#34; alt=&#34;image&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;19.) RunPod - Automatic1111 Web UI - Cloud - Paid - No PC Is Required&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=zA4LksIVas8&#34;&gt;&lt;strong&gt;RunPod Fix For DreamBooth &amp;amp; xFormers - How To Use Automatic1111 Web UI Stable Diffusion on RunPod&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=zA4LksIVas8&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/19240467/228829128-e32d2900-0162-4de7-ba78-887b9083b090.png&#34; alt=&#34;image&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;20.) Automatic1111 Web UI - PC - Free&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/iFRdrRyAQdQ&#34;&gt;&lt;strong&gt;Fantastic New ControlNet OpenPose Editor Extension &amp;amp; Image Mixing - Stable Diffusion Web UI Tutorial&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/iFRdrRyAQdQ&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/19240467/220776337-3abce5a3-bb17-4240-8400-4e633562ecc8.png&#34; alt=&#34;image&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;21.) Automatic1111 Web UI - PC - Free&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/Tb4IYIYm4os&#34;&gt;&lt;strong&gt;Automatic1111 Stable Diffusion DreamBooth Guide: Optimal Classification Images Count Comparison Test&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/Tb4IYIYm4os&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/19240467/221384116-e42d6f37-a068-4a2a-9bda-11ac47f33faa.png&#34; alt=&#34;image&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;22.) Automatic1111 Web UI - PC - Free&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/sRdtVanSRl4&#34;&gt;&lt;strong&gt;Epic Web UI DreamBooth Update - New Best Settings - 10 Stable Diffusion Training Compared on RunPods&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/sRdtVanSRl4&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/19240467/222991604-ceed12bc-0bc9-4f16-82fe-e6779132e00c.png&#34; alt=&#34;image&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;23.) Automatic1111 Web UI - PC - Free&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/tXaQAkOgezQ&#34;&gt;&lt;strong&gt;New Style Transfer Extension, ControlNet of Automatic1111 Stable Diffusion T2I-Adapter Color Control&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/tXaQAkOgezQ&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/19240467/223283277-eaaf6e53-df43-40ac-8096-c08f9a14cc8d.png&#34; alt=&#34;image&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;24.) Automatic1111 Web UI - PC - Free&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/C_mJI4U23nQ&#34;&gt;&lt;strong&gt;Generate Text Arts &amp;amp; Fantastic Logos By Using ControlNet Stable Diffusion Web UI For Free Tutorial&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/C_mJI4U23nQ&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/19240467/224442765-ba241f71-b412-4f5b-bf39-506e9682e336.png&#34; alt=&#34;image&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;25.) Automatic1111 Web UI - PC - Free&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/pom3nQejaTs&#34;&gt;&lt;strong&gt;How To Install New DREAMBOOTH &amp;amp; Torch 2 On Automatic1111 Web UI PC For Epic Performance Gains Guide&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/pom3nQejaTs&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/19240467/226115542-72db7e7e-cee0-4e3a-82c4-12348e2b237e.png&#34; alt=&#34;image&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;26.) Automatic1111 Web UI - PC - Free&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/m-UVVY_syP0&#34;&gt;&lt;strong&gt;Training Midjourney Level Style And Yourself Into The SD 1.5 Model via DreamBooth Stable Diffusion&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/m-UVVY_syP0&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/19240467/226378438-fe70f09e-94a8-4d1d-9468-e44dca99aac7.png&#34; alt=&#34;image&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;27.) Automatic1111 Web UI - PC - Free&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/kmT-z2lqEPQ&#34;&gt;&lt;strong&gt;Video To Anime - Generate An EPIC Animation From Your Phone Recording By Using Stable Diffusion AI&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/kmT-z2lqEPQ&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/19240467/228096548-5f6add70-ca04-4bec-8c33-24d243227532.png&#34; alt=&#34;image&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;28.) Python Script - Jupyter Based - PC - Free&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/dYt9xJ7dnpU&#34;&gt;&lt;strong&gt;Midjourney Level NEW Open Source Kandinsky 2.1 Beats Stable Diffusion - Installation And Usage Guide&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/dYt9xJ7dnpU&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/19240467/230183162-8a6f7e84-dcd9-45b5-a94c-b93a10778f42.png&#34; alt=&#34;image&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;29.) Automatic1111 Web UI - PC - Free&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/lgP1LNnaUaQ&#34;&gt;&lt;strong&gt;RTX 3090 vs RTX 3060 Ultimate Showdown for Stable Diffusion, ML, AI &amp;amp; Video Rendering Performance&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/lgP1LNnaUaQ&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/19240467/231303430-63d801cf-3c5a-4c20-b445-bb682febfa4e.png&#34; alt=&#34;image&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;30.) Kohya Web UI - Automatic1111 Web UI - PC - Free&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/TpuDOsuKIBo&#34;&gt;&lt;strong&gt;Generate Studio Quality Realistic Photos By Kohya LoRA Stable Diffusion Training - Full Tutorial&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/TpuDOsuKIBo&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/19240467/235155355-83ff14e5-a3c8-4ae8-83a5-6d2573189a22.png&#34; alt=&#34;image&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;31.) Kaggle NoteBook - Free&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/R2fEocf-MU8&#34;&gt;&lt;strong&gt;DeepFloyd IF By Stability AI - Is It Stable Diffusion XL or Version 3? We Review and Show How To Use&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/R2fEocf-MU8&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/19240467/235505544-2ba77ef2-3928-4c44-aba8-2536aebbfb60.png&#34; alt=&#34;image&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;32.) Python Script - Automatic1111 Web UI - PC - Free&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/343I11mhnXs&#34;&gt;&lt;strong&gt;How To Find Best Stable Diffusion Generated Images By Using DeepFace AI - DreamBooth / LoRA Training&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/343I11mhnXs&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/19240467/236293388-6254ff84-0866-4bd4-a5d4-2db3c42be3f0.png&#34; alt=&#34;image&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;33.) Kohya Web UI - RunPod - Paid&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/3uzCNrQao3o&#34;&gt;&lt;strong&gt;How To Install And Use Kohya LoRA GUI / Web UI on RunPod IO With Stable Diffusion &amp;amp; Automatic1111&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/3uzCNrQao3o&#34;&gt;&lt;img src=&#34;https://i.imgur.com/q7FPRIO.png&#34; alt=&#34;image&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;34.) PC - Google Colab - Free&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/OI1LEN-SgLM&#34;&gt;&lt;strong&gt;Mind-Blowing Deepfake Tutorial: Turn Anyone into Your Favorite Movie Star! PC &amp;amp; Google Colab - roop&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/OI1LEN-SgLM&#34;&gt;&lt;img src=&#34;https://i.imgur.com/ICWeeV0.png&#34; alt=&#34;image&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;35.) Automatic1111 Web UI - PC - Free&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/ot5GkaxHPzk&#34;&gt;&lt;strong&gt;Stable Diffusion Now Has The Photoshop Generative Fill Feature With ControlNet Extension - Tutorial&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/ot5GkaxHPzk&#34;&gt;&lt;img src=&#34;https://i.imgur.com/Cf4z9J7.png&#34; alt=&#34;image&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>bowang-lab/MedSAM</title>
    <updated>2023-06-02T01:38:49Z</updated>
    <id>tag:github.com,2023-06-02:/bowang-lab/MedSAM</id>
    <link href="https://github.com/bowang-lab/MedSAM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The official repository for MedSAM: Segment Anything in Medical Images.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MedSAM&lt;/h1&gt; &#xA;&lt;p&gt;This is the official repository for MedSAM: Segment Anything in Medical Images.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create a virtual environment &lt;code&gt;conda create -n medsam python=3.10 -y&lt;/code&gt; and activate it &lt;code&gt;conda activate medsam&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;Pytorch 2.0&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;git clone https://github.com/bowang-lab/MedSAM&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Enter the MedSAM folder &lt;code&gt;cd MedSAM&lt;/code&gt; and run &lt;code&gt;pip install -e .&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Fine-tune SAM on customized dataset&lt;/h2&gt; &#xA;&lt;p&gt;We provide a step-by-step tutorial with a small dataset to help you quickly start the training process.&lt;/p&gt; &#xA;&lt;h3&gt;Data preparation and preprocessing&lt;/h3&gt; &#xA;&lt;p&gt;Download &lt;a href=&#34;https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth&#34;&gt;SAM checkpoint&lt;/a&gt; and place it at &lt;code&gt;work_dir/SAM/sam_vit_b_01ec64.pth&lt;/code&gt; .&lt;/p&gt; &#xA;&lt;p&gt;Download the demo &lt;a href=&#34;https://zenodo.org/record/7860267&#34;&gt;dataset&lt;/a&gt; and unzip.&lt;/p&gt; &#xA;&lt;p&gt;This dataset contains 50 abdomen CT scans and each scan contain an annotation mask with 13 organs. The names of the organ label are available at &lt;a href=&#34;https://flare22.grand-challenge.org/&#34;&gt;MICCAI FLARE2022&lt;/a&gt;. In this tutorial, we will fine-tune SAM for gallbladder segmentation.&lt;/p&gt; &#xA;&lt;p&gt;Run pre-processing&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python pre_CT.py -i path_to_image_folder -gt path_to_gt_folder -o path_to_output&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;split dataset: 80% for training and 20% for testing&lt;/li&gt; &#xA; &lt;li&gt;image normalization&lt;/li&gt; &#xA; &lt;li&gt;pre-compute image embedding&lt;/li&gt; &#xA; &lt;li&gt;save the normalized images, ground truth masks, and image embedding as a &lt;code&gt;npz&lt;/code&gt; file&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: Medical images have various data formats. Thus, it&#39;s impossible that one script can handle all these different formats. Here, we provide two typical examples for CT and non-CT (e.g., various MR sequences, PET images) image preprocessing. You can adapt the preprocessing code to your own datasets.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Model Training (&lt;a href=&#34;https://drive.google.com/file/d/1EvVBTSa9L7pDTmUOp-MHXxGD1lrU9Txk/view?usp=share_link&#34;&gt;Video Tutorial&lt;/a&gt;)&lt;/h3&gt; &#xA;&lt;p&gt;Please check the step-by-step tutorial: finetune_and_inference_tutorial_3D_dataset.ipynb&lt;/p&gt; &#xA;&lt;p&gt;We also provide a tutorial on 2D dataset (png format): finetune_and_inference_tutorial_2D_dataset.ipynb&lt;/p&gt; &#xA;&lt;p&gt;You can also train the model on the whole dataset.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download the training set (&lt;a href=&#34;https://drive.google.com/drive/folders/1pwpAkWPe6czxkATG9SmVV0TP62NZiKld?usp=share_link&#34;&gt;GoogleDrive&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: For the convenience of file sharing, we compress each image and mask pair in a &lt;code&gt;npz&lt;/code&gt; file. The pre-computed image embedding is too large (require ~1 TB space). You can generate it with the following command&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Pre-compute the image embedding and save the image embedding and ground truth as &lt;code&gt;.npy&lt;/code&gt; files.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python utils/precompute_img_embed.py -i path_to_train_folder -o ./data/Tr_npy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Train the model&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python train -i ./data/Tr_npy --task_name SAM-ViT-B --num_epochs 1000 --batch_size 8 --lr 1e-5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you find this dataset valuable in your research, kindly acknowledge and credit the original data sources: &lt;a href=&#34;https://zenodo.org/record/7262581&#34;&gt;AMOS&lt;/a&gt;, &lt;a href=&#34;http://braintumorsegmentation.org/&#34;&gt;BraTS2021&lt;/a&gt;, &lt;a href=&#34;https://www.creatis.insa-lyon.fr/Challenge/acdc/&#34;&gt;ACDC&lt;/a&gt;, &lt;a href=&#34;https://www.ub.edu/mnms/&#34;&gt;M&amp;amp;Ms&lt;/a&gt;, &lt;a href=&#34;https://promise12.grand-challenge.org/&#34;&gt;PROMISE12&lt;/a&gt; &lt;a href=&#34;https://abcs.mgh.harvard.edu/&#34;&gt;ABCs&lt;/a&gt;, &lt;a href=&#34;https://ieeexplore.ieee.org/document/9497733&#34;&gt;AbdomenCT-1K&lt;/a&gt;, &lt;a href=&#34;http://medicaldecathlon.com/&#34;&gt;MSD&lt;/a&gt;, &lt;a href=&#34;https://kits19.grand-challenge.org/&#34;&gt;KiTS19&lt;/a&gt;, &lt;a href=&#34;https://competitions.codalab.org/competitions/17094&#34;&gt;LiTS&lt;/a&gt;, &lt;a href=&#34;https://github.com/JunMa11/COVID-19-CT-Seg-Benchmark&#34;&gt;COVID-19 CT-Seg&lt;/a&gt;, &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S1361841521003819&#34;&gt;HECKTOR&lt;/a&gt; &lt;a href=&#34;https://drive.grand-challenge.org/&#34;&gt;DRIVE&lt;/a&gt;, &lt;a href=&#34;https://www.kaggle.com/datasets/sani84/glasmiccai2015-gland-segmentation&#34;&gt;Colon gland&lt;/a&gt;, &lt;a href=&#34;https://www.nature.com/articles/s41597-023-01981-y&#34;&gt;polyp&lt;/a&gt;, &lt;a href=&#34;https://www.synapse.org/#!Synapse:syn22427422&#34;&gt;instruments&lt;/a&gt;, &lt;a href=&#34;https://www.kaggle.com/datasets/ignaciorlando/ussimandsegm&#34;&gt;Abdomen Ultrasound&lt;/a&gt;, &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S2352340919312181&#34;&gt;Breast Ultrasound&lt;/a&gt;, &lt;a href=&#34;http://imgcom.jsrt.or.jp/minijsrtdb/&#34;&gt;JSRT&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Train the model without pre-computed embeddings, run the following command:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python train_no_npz.py --csv &amp;lt;path-to-csv-file&amp;gt;  --image_col &amp;lt;csv-image-column-name&amp;gt; --mask_col &amp;lt;csv-mask-column-name&amp;gt; --model_type vit_b --checkpoint ../SAM_weights/sam_vit_b_01ec64.pth [--image &amp;lt;image-file-dir-path&amp;gt;] [--mask &amp;lt;mask-file-dir-path&amp;gt;]--num_epochs 100 --batch_size 4 --lr 1e-4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;--image&lt;/code&gt; and &lt;code&gt;--mask&lt;/code&gt; arguments can be used to specify the paths to the input and mask images, respectively. If these arguments are not specified, the paths to the images will be taken from the CSV file.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;--image_col&lt;/code&gt; and &lt;code&gt;--mask_col&lt;/code&gt; arguments can be used to specify the names of the columns in the CSV file that contain the paths to the input and mask images&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;-k&lt;/code&gt; argument can be used to specify the number of folds for cross-validation. If this argument is not specified, the model will be trained on the entire dataset.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: This method is slower and requires more memory than training the model using pre-computed embeddings.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;Download the model checkpoint (&lt;a href=&#34;https://drive.google.com/drive/folders/1bWv_Zs5oYLpGMAvbotnlNXJPq7ltRUvF?usp=share_link&#34;&gt;GoogleDrive&lt;/a&gt;) and testing data (&lt;a href=&#34;https://drive.google.com/drive/folders/1Qx-4EM0MoarzAfvSIp9fkpk8UBrWM6EP?usp=share_link&#34;&gt;GoogleDrive&lt;/a&gt;) and put them to &lt;code&gt;data/Test&lt;/code&gt; and &lt;code&gt;work_dir/MedSAM&lt;/code&gt; respectively.&lt;/p&gt; &#xA;&lt;p&gt;Run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python MedSAM_Inference.py -i ./data/Test -o ./ -chk work_dir/MedSAM/medsam_20230423_vit_b_0.0.1.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The segmentation results are available at &lt;a href=&#34;https://drive.google.com/drive/folders/1I8sgCRi30QtMix8DbDBIBTGDM_1FmSaO?usp=sharing&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The implementation code of DSC and NSD can be obtained at &lt;a href=&#34;http://medicaldecathlon.com/files/Surface_distance_based_measures.ipynb&#34;&gt;http://medicaldecathlon.com/files/Surface_distance_based_measures.ipynb&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;To-do-list&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Train the ViT-H model&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Explore other fine-tuning methods, e.g., fine-tune the image encoder as well, lora fine-tuning&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support scribble prompts&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support IoU/DSC regression&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Enlarge the dataset&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 3D slicer and napari support&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We are excited about the potential of segmentation foundation models in the medical image domain. However, training such models requires extensive computing resources. Therefore, we have made all the pre-processed training and images publicly available for research purposes. To prevent duplication of effort (e.g., conduct the same experiemnts), we encourage sharing of results and trained models on the discussion page. We look forward to working with the community to advance this exciting research area.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We highly appreciate all the challenge organizers and dataset owners for providing the public dataset to the community.&lt;/li&gt; &#xA; &lt;li&gt;We thank Meta AI for making the source code of &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;segment anything&lt;/a&gt; publicly available.&lt;/li&gt; &#xA; &lt;li&gt;We also thank Alexandre Bonnet for sharing this great &lt;a href=&#34;https://encord.com/blog/learn-how-to-fine-tune-the-segment-anything-model-sam/&#34;&gt;blog&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Reference&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{MedSAM,&#xA;  title={Segment Anything in Medical Images},&#xA;  author={Ma, Jun and Wang, Bo},&#xA;  journal={arXiv preprint arXiv:2304.12306},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>sanchit-gandhi/whisper-jax</title>
    <updated>2023-06-02T01:38:49Z</updated>
    <id>tag:github.com,2023-06-02:/sanchit-gandhi/whisper-jax</id>
    <link href="https://github.com/sanchit-gandhi/whisper-jax" rel="alternate"></link>
    <summary type="html">&lt;p&gt;JAX implementation of OpenAI&#39;s Whisper model for up to 70x speed-up on TPU.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Whisper JAX&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains optimised JAX code for OpenAI&#39;s &lt;a href=&#34;https://arxiv.org/abs/2212.04356&#34;&gt;Whisper Model&lt;/a&gt;, largely built on the 🤗 Hugging Face Transformers Whisper implementation. Compared to OpenAI&#39;s PyTorch code, Whisper JAX runs over &lt;strong&gt;70x&lt;/strong&gt; faster, making it the fastest Whisper implementation available.&lt;/p&gt; &#xA;&lt;p&gt;The JAX code is compatible on CPU, GPU and TPU, and can be run standalone (see &lt;a href=&#34;https://raw.githubusercontent.com/sanchit-gandhi/whisper-jax/main/#pipeline-usage&#34;&gt;Pipeline Usage&lt;/a&gt;) or as an inference endpoint (see &lt;a href=&#34;https://raw.githubusercontent.com/sanchit-gandhi/whisper-jax/main/#creating-an-endpoint&#34;&gt;Creating an Endpoint&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;For a quick-start guide to running Whisper JAX on a Cloud TPU, refer to the following Kaggle notebook, where we transcribe 30 mins of audio in approx 30 sec:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/code/sgandhi99/whisper-jax-tpu&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The Whisper JAX model is also running as a demo on the Hugging Face Hub:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/spaces/sanchit-gandhi/whisper-jax&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Whisper JAX was tested using Python 3.9 and JAX version 0.4.5. Installation assumes that you already have the latest version of the JAX package installed on your device. You can do so using the official JAX installation guide: &lt;a href=&#34;https://github.com/google/jax#installation&#34;&gt;https://github.com/google/jax#installation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Once the appropriate version of JAX has been installed, Whisper JAX can be installed through pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install git+https://github.com/sanchit-gandhi/whisper-jax.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To update the Whisper JAX package to the latest version, simply run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install --upgrade --no-deps --force-reinstall git+https://github.com/sanchit-gandhi/whisper-jax.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Pipeline Usage&lt;/h2&gt; &#xA;&lt;p&gt;The recommended way of running Whisper JAX is through the &lt;a href=&#34;https://github.com/sanchit-gandhi/whisper-jax/raw/main/whisper_jax/pipeline.py#L57&#34;&gt;&lt;code&gt;FlaxWhisperPipline&lt;/code&gt;&lt;/a&gt; abstraction class. This class handles all the necessary pre- and post-processing, as well as wrapping the generate method for data parallelism across accelerator devices.&lt;/p&gt; &#xA;&lt;p&gt;Whisper JAX makes use of JAX&#39;s &lt;a href=&#34;https://jax.readthedocs.io/en/latest/_autosummary/jax.pmap.html&#34;&gt;&lt;code&gt;pmap&lt;/code&gt;&lt;/a&gt; function for data parallelism across GPU/TPU devices. This function is &lt;em&gt;Just In Time (JIT)&lt;/em&gt; compiled the first time it is called. Thereafter, the function will be &lt;em&gt;cached&lt;/em&gt;, enabling it to be run in super-fast time:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from whisper_jax import FlaxWhisperPipline&#xA;&#xA;# instantiate pipeline&#xA;pipeline = FlaxWhisperPipline(&#34;openai/whisper-large-v2&#34;)&#xA;&#xA;# JIT compile the forward call - slow, but we only do once&#xA;text = pipeline(&#34;audio.mp3&#34;)&#xA;&#xA;# used cached function thereafter - super fast!!&#xA;text = pipeline(&#34;audio.mp3&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Half-Precision&lt;/h3&gt; &#xA;&lt;p&gt;The model computation can be run in half-precision by passing the dtype argument when instantiating the pipeline. This will speed-up the computation quite considerably by storing intermediate tensors in half-precision. There is no change to the precision of the model weights.&lt;/p&gt; &#xA;&lt;p&gt;For most GPUs, the dtype should be set to &lt;code&gt;jnp.float16&lt;/code&gt;. For A100 GPUs or TPUs, the dtype should be set to &lt;code&gt;jnp.bfloat16&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from whisper_jax import FlaxWhisperPipline&#xA;import jax.numpy as jnp&#xA;&#xA;# instantiate pipeline in bfloat16&#xA;pipeline = FlaxWhisperPipline(&#34;openai/whisper-large-v2&#34;, dtype=jnp.bfloat16)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Batching&lt;/h3&gt; &#xA;&lt;p&gt;Whisper JAX also provides the option of &lt;em&gt;batching&lt;/em&gt; a single audio input across accelerator devices. The audio is first chunked into 30 second segments, and then chunks dispatched to the model to be transcribed in parallel. The resulting transcriptions are stitched back together at the boundaries to give a single, uniform transcription. In practice, batching provides a 10x speed-up compared to transcribing the audio samples sequentially, with a less than 1% penalty to the WER[^1], provided the batch size is selected large enough.&lt;/p&gt; &#xA;&lt;p&gt;To enable batching, pass the &lt;code&gt;batch_size&lt;/code&gt; parameter when you instantiate the pipeline:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from whisper_jax import FlaxWhisperPipline&#xA;&#xA;# instantiate pipeline with batching&#xA;pipeline = FlaxWhisperPipline(&#34;openai/whisper-large-v2&#34;, batch_size=16)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Task&lt;/h3&gt; &#xA;&lt;p&gt;By default, the pipeline transcribes the audio file in the language it was spoken in. For speech translation, set the &lt;code&gt;task&lt;/code&gt; argument to &lt;code&gt;&#34;translate&#34;&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# translate&#xA;text = pipeline(&#34;audio.mp3&#34;, task=&#34;translate&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Timestamps&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://github.com/sanchit-gandhi/whisper-jax/raw/main/whisper_jax/pipeline.py#L57&#34;&gt;&lt;code&gt;FlaxWhisperPipline&lt;/code&gt;&lt;/a&gt; also supports timestamp prediction. Note that enabling timestamps will require a second JIT compilation of the forward call, this time including the timestamp outputs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# transcribe and return timestamps&#xA;outputs = pipeline(&#34;audio.mp3&#34;,  task=&#34;transcribe&#34;, return_timestamps=True)&#xA;text = outputs[&#34;text&#34;]  # transcription&#xA;chunks = outputs[&#34;chunks&#34;]  # transcription + timestamps&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Putting it all together&lt;/h3&gt; &#xA;&lt;p&gt;In the following code snippet, we instantiate the model in bfloat16 precision with batching enabled, and transcribe the audio file returning timestamps tokens:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from whisper_jax import FlaxWhisperPipline&#xA;import jax.numpy as jnp&#xA;&#xA;# instantiate pipeline with bfloat16 and enable batching&#xA;pipeline = FlaxWhisperPipline(&#34;openai/whisper-large-v2&#34;, dtype=jnp.bfloat16, batch_size=16)&#xA;&#xA;# transcribe and return timestamps&#xA;outputs = pipeline(&#34;audio.mp3&#34;,  task=&#34;transcribe&#34;, return_timestamps=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Model Usage&lt;/h2&gt; &#xA;&lt;p&gt;The Whisper JAX model can use on a more granular level in much the same way as the original Hugging Face Transformers implementation. This requires the Whisper processor to be loaded separately to the model to handle the pre- and post-processing, and the generate function to be wrapped using &lt;code&gt;pmap&lt;/code&gt; by hand:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import jax.numpy as jnp&#xA;from datasets import load_dataset&#xA;from flax.jax_utils import replicate&#xA;from flax.training.common_utils import shard&#xA;from jax import device_get, pmap&#xA;from transformers import WhisperProcessor&#xA;&#xA;from whisper_jax import FlaxWhisperForConditionalGeneration&#xA;&#xA;# load the processor and model&#xA;processor = WhisperProcessor.from_pretrained(&#34;openai/whisper-large-v2&#34;)&#xA;model, params = FlaxWhisperForConditionalGeneration.from_pretrained(&#xA;    &#34;openai/whisper-large-v2&#34;, dtype=jnp.bfloat16, _do_init=False,&#xA;)&#xA;&#xA;def generate_fn(input_features):&#xA;    pred_ids = model.generate(&#xA;        input_features, task=&#34;transcribe&#34;, return_timestamps=False, max_length=model.config.max_length, params=params,&#xA;    )&#xA;    return pred_ids.sequences&#xA;&#xA;# pmap the generate function for data parallelism&#xA;p_generate = pmap(generate_fn, &#34;input_features&#34;)&#xA;# replicate the parameters across devices&#xA;params = replicate(params)&#xA;&#xA;# load a dummy sample from the LibriSpeech dataset&#xA;ds = load_dataset(&#34;hf-internal-testing/librispeech_asr_dummy&#34;, &#34;clean&#34;, split=&#34;validation&#34;)&#xA;sample = ds[0][&#34;audio&#34;]&#xA;&#xA;# pre-process: convert the audio array to log-mel input features&#xA;input_features = processor(sample[&#34;array&#34;], sampling_rate=sample[&#34;sampling_rate&#34;], return_tensors=&#34;np&#34;).input_features&#xA;# replicate the input features across devices for DP&#xA;input_features = shard(input_features)&#xA;&#xA;# run the forward pass (JIT compiled the first time it is called)&#xA;pred_ids = p_generate(input_features)&#xA;output_ids = device_get(pred_ids.reshape(-1, model.config.max_length))&#xA;&#xA;# post-process: convert tokens ids to text string&#xA;transcription = processor.batch_decode(pred_ids, skip_special_tokens=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Available Models and Languages&lt;/h2&gt; &#xA;&lt;p&gt;All Whisper models on the Hugging Face Hub with Flax weights are compatible with Whisper JAX. This includes, but is not limited to, the official OpenAI Whisper checkpoints:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Size&lt;/th&gt; &#xA;   &lt;th&gt;Parameters&lt;/th&gt; &#xA;   &lt;th&gt;English-only&lt;/th&gt; &#xA;   &lt;th&gt;Multilingual&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;tiny&lt;/td&gt; &#xA;   &lt;td&gt;39 M&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/openai/whisper-tiny.en&#34;&gt;✓&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/openai/whisper-tiny&#34;&gt;✓&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;base&lt;/td&gt; &#xA;   &lt;td&gt;74 M&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/openai/whisper-base.en&#34;&gt;✓&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/openai/whisper-base&#34;&gt;✓&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;small&lt;/td&gt; &#xA;   &lt;td&gt;244 M&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/openai/whisper-small.en&#34;&gt;✓&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/openai/whisper-small&#34;&gt;✓&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;medium&lt;/td&gt; &#xA;   &lt;td&gt;769 M&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/openai/whisper-medium.en&#34;&gt;✓&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/openai/whisper-medium&#34;&gt;✓&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;large&lt;/td&gt; &#xA;   &lt;td&gt;1550 M&lt;/td&gt; &#xA;   &lt;td&gt;x&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/openai/whisper-large&#34;&gt;✓&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;large-v2&lt;/td&gt; &#xA;   &lt;td&gt;1550 M&lt;/td&gt; &#xA;   &lt;td&gt;x&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/openai/whisper-large-v2&#34;&gt;✓&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Should you wish to use a fine-tuned Whisper checkpoint in Whisper JAX, you should first convert the PyTorch weights to Flax. This is straightforward through use of the &lt;code&gt;from_pt&lt;/code&gt; argument, which will convert the PyTorch state dict to a frozen Flax parameter dictionary on the fly. You can then push the converted Flax weights to the Hub to be used directly in Flax the next time they are required. Note that converting weights from PyTorch to Flax requires both PyTorch and Flax to be installed.&lt;/p&gt; &#xA;&lt;p&gt;For example, to convert the fine-tuned checkpoint &lt;a href=&#34;https://huggingface.co/sanchit-gandhi/whisper-small-hi&#34;&gt;&lt;code&gt;sanchit-gandhi/whisper-small-hi&lt;/code&gt;&lt;/a&gt; from the blog post &lt;a href=&#34;https://huggingface.co/blog/fine-tune-whisper&#34;&gt;Fine-Tuning Whisper&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from whisper_jax import FlaxWhisperForConditionalGeneration, FlaxWhisperPipline&#xA;import jax.numpy as jnp&#xA;&#xA;checkpoint_id = &#34;sanchit-gandhi/whisper-small-hi&#34;&#xA;# convert PyTorch weights to Flax&#xA;model = FlaxWhisperForConditionalGeneration.from_pretrained(checkpoint_id, from_pt=True)&#xA;# push converted weights to the Hub&#xA;model.push_to_hub(checkpoint_id)&#xA;&#xA;# now we can load the Flax weights directly as required&#xA;pipeline = FlaxWhisperPipline(checkpoint_id, dtype=jnp.bfloat16, batch_size=16)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Advanced Usage&lt;/h2&gt; &#xA;&lt;p&gt;More advanced users may wish to explore different parallelisation techniques. The Whisper JAX code is built on-top of the &lt;a href=&#34;https://github.com/google-research/t5x&#34;&gt;T5x codebase&lt;/a&gt;, meaning it can be run using model, activation, and data parallelism using the T5x partitioning convention. To use T5x partitioning, the logical axis rules and number of model partitions must be defined. For more details, the user is referred to the official T5x partitioning guide: &lt;a href=&#34;https://github.com/google-research/t5x/raw/main/docs/usage/partitioning.md&#34;&gt;https://github.com/google-research/t5x/blob/main/docs/usage/partitioning.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Pipeline&lt;/h3&gt; &#xA;&lt;p&gt;The following code snippet demonstrates how data parallelism can be achieved using the pipeline &lt;code&gt;shard_params&lt;/code&gt; method in an entirely equivalent way to &lt;code&gt;pmap&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from whisper_jax import FlaxWhisperPipline&#xA;import jax.numpy as jnp&#xA;&#xA;# 2D parameter and activation partitioning for DP&#xA;logical_axis_rules_dp = (&#xA;    (&#34;batch&#34;, &#34;data&#34;),&#xA;    (&#34;mlp&#34;, None),&#xA;    (&#34;heads&#34;, None),&#xA;    (&#34;vocab&#34;, None),&#xA;    (&#34;embed&#34;, None),&#xA;    (&#34;embed&#34;, None),&#xA;    (&#34;joined_kv&#34;, None),&#xA;    (&#34;kv&#34;, None),&#xA;    (&#34;length&#34;, None),&#xA;    (&#34;num_mel&#34;, None),&#xA;    (&#34;channels&#34;, None),&#xA;)&#xA;&#xA;pipeline = FlaxWhisperPipline(&#34;openai/whisper-large-v2&#34;, dtype=jnp.bfloat16, batch_size=16)&#xA;pipeline.shard_params(num_mp_partitions=1, logical_axis_rules=logical_axis_rules_dp)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Model&lt;/h3&gt; &#xA;&lt;p&gt;It is also possible to use the Whisper JAX model with T5x partitioning by defining a T5x inference state and T5x partitioner:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import jax&#xA;import jax.numpy as jnp&#xA;from flax.core.frozen_dict import freeze&#xA;from jax.sharding import PartitionSpec as P&#xA;&#xA;from whisper_jax import FlaxWhisperForConditionalGeneration, InferenceState, PjitPartitioner&#xA;&#xA;&#xA;# 2D parameter and activation partitioning for DP&#xA;logical_axis_rules_dp = [&#xA;    (&#34;batch&#34;, &#34;data&#34;),&#xA;    (&#34;mlp&#34;, None),&#xA;    (&#34;heads&#34;, None),&#xA;    (&#34;vocab&#34;, None),&#xA;    (&#34;embed&#34;, None),&#xA;    (&#34;embed&#34;, None),&#xA;    (&#34;joined_kv&#34;, None),&#xA;    (&#34;kv&#34;, None),&#xA;    (&#34;length&#34;, None),&#xA;    (&#34;num_mel&#34;, None),&#xA;    (&#34;channels&#34;, None),&#xA;]&#xA;&#xA;model, params = FlaxWhisperForConditionalGeneration.from_pretrained(&#xA;    &#34;openai/whisper-large-v2&#34;,&#xA;    _do_init=False,&#xA;    dtype=jnp.bfloat16,&#xA;)&#xA;&#xA;&#xA;def init_fn():&#xA;    input_shape = (1, 80, 3000)&#xA;&#xA;    input_features = jnp.zeros(input_shape, dtype=&#34;f4&#34;)&#xA;    input_features = input_features.at[(..., -1)].set(model.config.eos_token_id)&#xA;&#xA;    decoder_input_ids = jnp.zeros((input_shape[0], 1), dtype=&#34;i4&#34;)&#xA;    decoder_attention_mask = jnp.ones_like(decoder_input_ids)&#xA;&#xA;    batch_size, sequence_length = decoder_input_ids.shape&#xA;    decoder_position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[None, :], (batch_size, sequence_length))&#xA;&#xA;    rng = jax.random.PRNGKey(0)&#xA;    init_params = model.module.init(&#xA;        rng,&#xA;        input_features=input_features,&#xA;        decoder_input_ids=decoder_input_ids,&#xA;        decoder_attention_mask=decoder_attention_mask,&#xA;        decoder_position_ids=decoder_position_ids,&#xA;        return_dict=False,&#xA;    )&#xA;    return init_params&#xA;&#xA;&#xA;# Axis names metadata&#xA;param_axes = jax.eval_shape(init_fn)[&#34;params_axes&#34;]&#xA;&#xA;# Create InferenceState, since the partitioner expects it&#xA;state = InferenceState(&#xA;    step=jnp.array(0),&#xA;    params=freeze(model.params_shape_tree),&#xA;    params_axes=freeze(param_axes),&#xA;    flax_mutables=None,&#xA;    flax_mutables_axes=param_axes,&#xA;)&#xA;&#xA;# Define the pjit partitioner with 1 model partition&#xA;partitioner = PjitPartitioner(&#xA;    num_partitions=1,&#xA;    logical_axis_rules=logical_axis_rules_dp,&#xA;)&#xA;&#xA;mesh_axes = partitioner.get_mesh_axes(state)&#xA;params_spec = mesh_axes.params&#xA;&#xA;p_shard_params = partitioner.partition(model.to_bf16, (params_spec,), params_spec)&#xA;&#xA;&#xA;def generate(params, input_features):&#xA;    output_ids = model.generate(input_features, params=params, max_length=model.config.max_length).sequences&#xA;    return output_ids&#xA;&#xA;&#xA;p_generate = partitioner.partition(&#xA;    generate,&#xA;    in_axis_resources=(params_spec, P(&#34;data&#34;)),&#xA;    out_axis_resources=P(&#34;data&#34;),&#xA;)&#xA;&#xA;# This will auto-magically run in mesh context&#xA;params = p_shard_params(freeze(params))&#xA;&#xA;# you can now run the forward pass with: &#xA;# pred_ids = p_generate(input_features)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Benchmarks&lt;/h2&gt; &#xA;&lt;p&gt;We compare Whisper JAX to the official &lt;a href=&#34;https://github.com/openai/whisper&#34;&gt;OpenAI implementation&lt;/a&gt; and the &lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/whisper&#34;&gt;🤗 Transformers implementation&lt;/a&gt;. We benchmark the models on audio samples of increasing length and report the average inference time in seconds over 10 repeat runs. For all three systems, we pass a pre-loaded audio file to the model and measure the time for the forward pass. Leaving the task of loading the audio file to the systems adds an equal offset to all the benchmark times, so the actual time for loading &lt;strong&gt;and&lt;/strong&gt; transcribing an audio file will be higher than the reported numbers.&lt;/p&gt; &#xA;&lt;p&gt;OpenAI and Transformers both run in PyTorch on GPU. Whisper JAX runs in JAX on GPU and TPU. OpenAI transcribes the audio sequentially in the order it is spoken. Both Transformers and Whisper JAX use a batching algorithm, where chunks of audio are batched together and transcribed in parallel (see section &lt;a href=&#34;https://raw.githubusercontent.com/sanchit-gandhi/whisper-jax/main/#batching&#34;&gt;Batching&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Table 1:&lt;/strong&gt; Average inference time in seconds for audio files of increasing length. GPU device is a single A100 40GB GPU. TPU device is a single TPU v4-8.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;&lt;/th&gt; &#xA;    &lt;th&gt;OpenAI&lt;/th&gt; &#xA;    &lt;th&gt;Transformers&lt;/th&gt; &#xA;    &lt;th&gt;Whisper JAX&lt;/th&gt; &#xA;    &lt;th&gt;Whisper JAX&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Framework&lt;/td&gt; &#xA;    &lt;td&gt;PyTorch&lt;/td&gt; &#xA;    &lt;td&gt;PyTorch&lt;/td&gt; &#xA;    &lt;td&gt;JAX&lt;/td&gt; &#xA;    &lt;td&gt;JAX&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Backend&lt;/td&gt; &#xA;    &lt;td&gt;GPU&lt;/td&gt; &#xA;    &lt;td&gt;GPU&lt;/td&gt; &#xA;    &lt;td&gt;GPU&lt;/td&gt; &#xA;    &lt;td&gt;TPU&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;1 min&lt;/td&gt; &#xA;    &lt;td&gt;13.8&lt;/td&gt; &#xA;    &lt;td&gt;4.54&lt;/td&gt; &#xA;    &lt;td&gt;1.72&lt;/td&gt; &#xA;    &lt;td&gt;0.45&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;10 min&lt;/td&gt; &#xA;    &lt;td&gt;108.3&lt;/td&gt; &#xA;    &lt;td&gt;20.2&lt;/td&gt; &#xA;    &lt;td&gt;9.38&lt;/td&gt; &#xA;    &lt;td&gt;2.01&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;1 hour&lt;/td&gt; &#xA;    &lt;td&gt;1001.0&lt;/td&gt; &#xA;    &lt;td&gt;126.1&lt;/td&gt; &#xA;    &lt;td&gt;75.3&lt;/td&gt; &#xA;    &lt;td&gt;13.8&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Creating an Endpoint&lt;/h2&gt; &#xA;&lt;p&gt;The Whisper JAX model is running as a demo on the Hugging Face Hub:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/spaces/sanchit-gandhi/whisper-jax&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;However, at peak times there may be a queue of users that limit how quickly your audio input is transcribed. In this case, you may benefit from running the model yourself, such that you have unrestricted access to the Whisper JAX model.&lt;/p&gt; &#xA;&lt;p&gt;If you are just interested in running the model in a standalone Python script, refer to the Kaggle notebook Whisper JAX TPU:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/code/sgandhi99/whisper-jax-tpu&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Otherwise, we provide all the necessary code for creating an inference endpoint. To obtain this code, first clone the repository on the GPU/TPU on which you want to host the endpoint:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/sanchit-gandhi/whisper-jax&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And then install Whisper JAX from source, with the required additional endpoint dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd whisper-jax&#xA;pip install -e .[&#34;endpoint&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We recommend that you set-up an endpoint in the same zone/region as the one you are based in. This reduces the communication time between your local machine and the remote one, which can significantly reduce the overall request time.&lt;/p&gt; &#xA;&lt;h3&gt;Gradio App&lt;/h3&gt; &#xA;&lt;p&gt;The Python script &lt;a href=&#34;https://raw.githubusercontent.com/sanchit-gandhi/whisper-jax/main/app/app.py&#34;&gt;&lt;code&gt;app.py&lt;/code&gt;&lt;/a&gt; contains the code to launch a Gradio app with the Whisper large-v2 model. By default, it uses a batch size of 16 and bfloat16 half-precision. You should update these parameters depending on your GPU/TPU device (as explained in the sections on &lt;a href=&#34;https://raw.githubusercontent.com/sanchit-gandhi/whisper-jax/main/#half-precision&#34;&gt;Half-precision&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/sanchit-gandhi/whisper-jax/main/#batching&#34;&gt;Batching&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;We can launch the Gradio app on port 7860 (default) on our GPU/TPU device through the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python app/app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will launch a Gradio demo with the same interface as the official Whisper JAX demo. To view the Gradio app remotely, we have two options:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Open the port 7860 on the GPU/TPU device to listen to all requests&lt;/li&gt; &#xA; &lt;li&gt;Start an ngrok server on the GPU/TPU that redirects requests to port 7860&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;To open the port 7860 on your GPU/TPU, refer to your hardware provider&#39;s firewall instructions (for GCP, these can be found &lt;a href=&#34;https://cloud.google.com/firewall/docs/using-firewalls&#34;&gt;here&lt;/a&gt;). Once you have opened port 7860, you should be able to access the gradio demo through the http address:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;http://DEVICE-IP:7860&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where &lt;code&gt;DEVICE-IP&lt;/code&gt; is the public IP address of your GPU/TPU. We can verify this address is accessible by opening this http address in a browser window on our local machine.&lt;/p&gt; &#xA;&lt;p&gt;Alternatively, we can direct network requests to the Gradio app using ngrok. By using ngrok, we don&#39;t need to open the port 7860 on our GPU/TPU - ngrok will provide us with a public http address that will automatically redirect requests to port 7860 on our accelerator. However, in our experience, using ngrok was less reliable than a direct tunnel to port 7860, thus we recommend option 1 here where possible.&lt;/p&gt; &#xA;&lt;p&gt;To set-up ngrok on your GPU/TPU, first install ngrok according to the official &lt;a href=&#34;https://ngrok.com/download&#34;&gt;installation guide&lt;/a&gt;. You should authenticate your ngrok account if you have one, otherwise your ngrok server will be time-limited to 2 hours. Once installed and authenticated, you can launch an ngrok server on port 7860:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ngrok http 7860&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The ngrok http address will be of the form:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://NGROK-ADDRESS.ngrok.io&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;which can be used to access the Gradio demo through a web browser.&lt;/p&gt; &#xA;&lt;h3&gt;Sending Requests&lt;/h3&gt; &#xA;&lt;p&gt;Independent of whether you&#39;ve chosen to open the port 7860 or use ngrok, we&#39;re now ready to send audio file requests to our endpoint. To do this, we&#39;ll make use of the &lt;code&gt;gradio_client&lt;/code&gt; library. If you already have a recent version of Gradio, then the &lt;code&gt;gradio_client&lt;/code&gt; library is included as a dependency.&lt;/p&gt; &#xA;&lt;p&gt;Otherwise, the lightweight &lt;code&gt;gradio_client&lt;/code&gt; package can be installed from pip and is tested to work with Python versions 3.9 or higher:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install --upgrade gradio_client&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We can now send json requests to our endpoint using ngrok. The function &lt;code&gt;transcribe_audio&lt;/code&gt; sends an audio file to our endpoint and returns the transcription:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from gradio_client import Client&#xA;&#xA;# make sure this URL matches your http web address&#xA;API_URL = &#34;http://DEVICE-IP:7860/&#34; # if using port 7860&#xA;API_URL = &#34;https://NGROK-ADDRESS.ngrok.io/&#34; # if using ngrok&#xA;&#xA;# set up the Gradio client&#xA;client = Client(API_URL)&#xA;&#xA;def transcribe_audio(audio_path, task=&#34;transcribe&#34;, return_timestamps=False):&#xA;    &#34;&#34;&#34;Function to transcribe an audio file using our endpoint&#34;&#34;&#34;&#xA;    text, runtime = client.predict(&#xA;        audio_path,&#xA;        task,&#xA;        return_timestamps,&#xA;        api_name=&#34;/predict_1&#34;,&#xA;    )&#xA;    return text&#xA;&#xA;# transcribe an audio file using our endpoint&#xA;output = transcribe_audio(&#34;audio.mp3&#34;)&#xA;&#xA;# transcribe with timestamps&#xA;output_with_timestamps = transcribe_audio(&#34;audio.mp3&#34;, return_timestamps=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;🤗 Hugging Face Transformers for the base Whisper implementation, particularly to &lt;a href=&#34;https://github.com/andyehrenberg&#34;&gt;andyehrenberg&lt;/a&gt; for the &lt;a href=&#34;https://github.com/huggingface/transformers/pull/20479&#34;&gt;Flax Whisper PR&lt;/a&gt; and &lt;a href=&#34;https://github.com/ArthurZucker&#34;&gt;ArthurZucker&lt;/a&gt; for the batching algorithm&lt;/li&gt; &#xA; &lt;li&gt;Gradio for their easy-to-use package for building ML demos, and &lt;a href=&#34;https://github.com/pcuenca&#34;&gt;pcuenca&lt;/a&gt; for the help in hooking the demo up to the TPU&lt;/li&gt; &#xA; &lt;li&gt;Google&#39;s &lt;a href=&#34;https://sites.research.google/trc/about/&#34;&gt;TPU Research Cloud (TRC)&lt;/a&gt; programme for Cloud TPUs&lt;/li&gt; &#xA; &lt;li&gt;Google&#39;s &lt;a href=&#34;https://github.com/google-research/t5x&#34;&gt;t5x Repository&lt;/a&gt; for the model partitioning framework&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;[^1]: See WER results from Colab: &lt;a href=&#34;https://colab.research.google.com/drive/1rS1L4YSJqKUH_3YxIQHBI982zso23wor?usp=sharing&#34;&gt;https://colab.research.google.com/drive/1rS1L4YSJqKUH_3YxIQHBI982zso23wor?usp=sharing&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>