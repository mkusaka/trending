<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-02-24T01:38:36Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>phodal/ai-research</title>
    <updated>2023-02-24T01:38:36Z</updated>
    <id>tag:github.com,2023-02-24:/phodal/ai-research</id>
    <link href="https://github.com/phodal/ai-research" rel="alternate"></link>
    <summary type="html">&lt;p&gt;【🔞🔞🔞 内含不适合未成年人阅读的图片】基于我擅长的编程、绘画、写作展开的 AI 探索和总结：StableDiffusion 是一种强大的图像生成模型，能够通过对一张图片进行演化来生成新的图片。ChatGPT 是一个基于 Transformer 的语言生成模型，它能够自动为输入的主题生成合适的文章。而 Github Copilot 是一个智能编程助手，能够加速日常编程活动。&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;基于编程、绘画、写作的 AI 探索与总结（18 禁） —— 2023 年 2 月版&lt;/h1&gt; &#xA;&lt;p&gt;PS：本文的图形部分因 “真实世界原因”，包含一些年龄受限的词汇，建议未成年人在监护人陪同下阅读。&lt;/p&gt; &#xA;&lt;p&gt;就当前节点（2023.02.22）而言，我虽然研究过一段时间传统的机器学习，但是并不擅长深度学习等领域，所以很多 AI 领域相关的词汇，我是不擅长的，只为自己总结一下，方便在未来更新自己的认识。&lt;/p&gt; &#xA;&lt;p&gt;内容主要是结合我过去擅长的编程、写作、绘画展开的：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;绘画：text 2 image。结合 Stable Diffusion 讲述一张图片的演化。&lt;/li&gt; &#xA; &lt;li&gt;写作：chat 2 article。结合 ChatGPT 讲述围绕特定主题的内容创作。&lt;/li&gt; &#xA; &lt;li&gt;编程：comment 2 code。结合 GitHub Copilot 如何加速日常的编程活动。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;总体来说，&lt;strong&gt;AI 相当于美国队长的实力，遇强则强，遇弱则弱&lt;/strong&gt;，关键其实在于 Prompt。所以，如何更好地完善 prompt，实现 prompt 工程就是：如何更好地使用 AI 的基础。&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Prompt 是一种文本输入方式，用于与自然语言处理模型（例如GPT）进行交互。Prompt 是一个短语或一句话，可以启动模型并产生与 Prompt 相关的输出。这个输出可以是一个单独的单词、一句话，或者是一段完整的段落。它可以控制模型的行为，使其产生特定的结果。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;所以，如何更好地完善 prompt，实现 prompt 工程就是：如何更好地使用 AI 的基础。所以，在这篇文章里，我将介绍：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;用 text 2 image 来展示如何演进 prompt&lt;/li&gt; &#xA; &lt;li&gt;用 text 2 article 来展示充足 prompt 的优点&lt;/li&gt; &#xA; &lt;li&gt;结合 GitHub Copliot 来实现 prompt 完成工作&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;PS：感谢我老婆提供的照片。&lt;/p&gt; &#xA;&lt;h2&gt;图形生成：如何描述一张照片？？&lt;/h2&gt; &#xA;&lt;p&gt;人类需要思考第一个问题：如何描述一张照片？在不关心感受的情况下，我们关注于主体、背景、颜色、构图等。&lt;/p&gt; &#xA;&lt;p&gt;现在，我们脑海中出现一个场景：&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/phodal/ai-research/master/output/sample.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;接着，用文本来抽象它。在这里，我结合一下 Stable Diffusion 来讲述一下过程。&lt;/p&gt; &#xA;&lt;h3&gt;文本描述：第一个示例&lt;/h3&gt; &#xA;&lt;p&gt;生成 prompt：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;phodal: 我想写一段话描述一张照片，帮我美化一下：一年轻女子穿着裙子，背对着镜头，在海边看着日落，太阳已经到地平线下面了，她的头发在风中飘动。&lt;/li&gt; &#xA; &lt;li&gt;ChatGPT: 这张照片中的场景令人心醉，一位年轻女子穿着一条裙子，站在海边，背对着镜头，凝视着地平线下的夕阳。太阳已经沉入了海面，一片金黄色的光芒在海面上弥漫，照亮了整片海洋，仿佛这个时刻被定格在了时间的某个角落里。女子的头发随着微风轻轻飘动，轮廓被余晖映照得柔和而温暖。她的目光凝视着远方，仿佛在思考着生命的意义和无尽的未来。这张照片中蕴含着一种深刻的内涵，让人感受到时间和自然的力量，也让人思考着自己在这个宏大而美丽的世界中的角色和意义。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;如果转换成英语的话：&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;A young woman wearing a flowing dress stands at the edge of the sea, her back turned to the camera as she gazes upon the glow of the setting sun. The ocean stretches out before her in all its grandeur, and the sun slowly sinks below the horizon, casting a golden light over the water. The woman&#39;s hair sways gently in the breeze, her features softened by the warm hues of the sunset. Her gaze is fixed on the distance, as if pondering the meaning of life and the endless possibilities of the future.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;重复了 N 次之后，你会得到你想要的成功的照片，当然也会有失败的出现（因为没有配置 negative prompt 过滤失败的情况）：&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/phodal/ai-research/master/output/failure-seeds.jpeg&#34; alt=&#34;Failure&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;但是，熟悉各种搜索引擎的关键词的或者 NLP 的你，肯定知道，上面的大部分可能是废话，可以减化为如下的词，并添加一些专用的模型词汇，如&lt;code&gt;black hair&lt;/code&gt;, &lt;code&gt;hand before body&lt;/code&gt;, &lt;code&gt;no hand&lt;/code&gt;, &lt;code&gt;bodycon dress&lt;/code&gt;：&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;women back view without face, flowing dress, edge of the sea, backview, back turned to the camera, upon the glow of the setting sun, sun below the horizon, golden light over the water, hair sways gently, Chinese style clothes, black hair,&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;随后，添加一些反向 prompt，意思是不要这些内容，诸如于 AI 不擅长的：&lt;code&gt;bad hands&lt;/code&gt;、&lt;code&gt;morbid&lt;/code&gt; 等等。随后，不断调整 prompt，比如生成更好的 Prompt：&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/phodal/ai-research/master/output/women-backview-1.jpeg&#34; width=&#34;256px&#34; height=&#34;384px&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/phodal/ai-research/master/output/women-backview-3.jpeg&#34; width=&#34;256px&#34; height=&#34;384px&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/phodal/ai-research/master/output/women-backview-4.jpeg&#34; width=&#34;256px&#34; height=&#34;384px&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;诸如于采用 &lt;a href=&#34;https://huggingface.co/spaces/Gustavosta/MagicPrompt-Stable-Diffusion&#34;&gt;Magic Prompt&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h3&gt;精准控图：结合 ControlNet&lt;/h3&gt; &#xA;&lt;p&gt;由于生成的姿势是随机的、无法控制，所以我就引入了 ControlNet 插件 ——用来实现骨骼绑定、精准控线、线稿上色、依据深度图结构透视精准重绘等。现在，就能结合我们做的创作来完善生成的图形，诸如于我们绘制草图、或者输入一张原始图片，就可以生成我们预期的效果：&lt;/p&gt; &#xA;&lt;p&gt;最后，生成的图片如下：&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/phodal/ai-research/master/output/image-with-hed.png&#34; alt=&#34;Openpose&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;在我写完文章的时候，又修改了一下 prompt：&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;women back view without face standing on the sandy beach, bodycov full skirt, edge of the sea, back turned to the camera, upon the glow of the setting sun, black hair, sunset red to blue gradient sky&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;对应的 negative prompt 是：&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;(((simple background))),monochrome ,lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, lowres, bad anatomy, bad hands, text, error, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, ugly,pregnant,vore,duplicate,morbid,mut ilated,tran nsexual, hermaphrodite,long neck,mutated hands,poorly drawn hands,poorly drawn face,mutation,deformed,blurry,bad anatomy,bad proportions,malformed limbs,extra limbs,cloned face,disfigured,gross proportions, (((missing arms))),(((missing legs))), (((extra arms))),(((extra legs))),pubic hair, plump,bad legs,error legs,username,blurry,bad feet&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;效果凑合着，还是不错的。然后，我们可以做更多的尝试，配合一下参数调整（俗称炼丹）：&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/phodal/ai-research/master/output/samples/01.jpeg&#34; width=&#34;256px&#34; height=&#34;384px&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/phodal/ai-research/master/output/samples/02.jpeg&#34; width=&#34;256px&#34; height=&#34;384px&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/phodal/ai-research/master/output/samples/04.jpeg&#34; width=&#34;256px&#34; height=&#34;384px&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/phodal/ai-research/master/output/samples/05.jpeg&#34; width=&#34;256px&#34; height=&#34;384px&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/phodal/ai-research/master/output/samples/06.jpeg&#34; width=&#34;256px&#34; height=&#34;384px&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/phodal/ai-research/master/output/samples/08.jpeg&#34; width=&#34;256px&#34; height=&#34;384px&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;也可以结合 inpaint 对失真的部分进行修复。&lt;/p&gt; &#xA;&lt;h3&gt;更多的模型集：与二次元世界的照片（18 禁）&lt;/h3&gt; &#xA;&lt;p&gt;众所周知，AI 的生成质量是与模型息息相关的，所以好的质量需要有好的模型。&lt;/p&gt; &#xA;&lt;p&gt;我们可以在 &lt;a href=&#34;https://civitai.com/&#34;&gt;https://civitai.com/&lt;/a&gt; 上找到更多的模型，不过因为年龄限制等原因，只建议你在安全的场所打开，不建议在公共场所打开。&lt;/p&gt; &#xA;&lt;p&gt;真实的人物由于版权的种种原因，所以这个软件在二次元世界相当的流行。如下是常见的提示词来描述模型的质量，里面可能包含一些不适词语，为了体现真实世界，这里并不打算屏蔽。&lt;/p&gt; &#xA;&lt;p&gt;提示词 ：&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;modelshoot style, (wavy blue hair), ((half body portrait)), ((showing boobs, giant boobs, humongous breasts)), (( beautiful light makeup female sorceress in majestic blue dress)), photo realistic game cg, 8k, epic, (blue diamond necklace hyper intricate fine detail), symetrical features, joyful, majestic oil painting by Mikhail Vrubel, Atey Ghailan, by Jeremy Mann, Greg Manchess, WLOP, Charlie Bowater, trending on ArtStation, trending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic, black background, epic volumetric lighting, fine details, illustration, (masterpiece, best quality, highres), standing in majestic castle&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;负面提示词&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;(((simple background))),monochrome ,lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, lowres, bad anatomy, bad hands, text, error, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, ugly,pregnant,vore,duplicate,morbid,mut ilated,tran nsexual, hermaphrodite,long neck,mutated hands,poorly drawn hands,poorly drawn face,mutation,deformed,blurry,bad anatomy,bad proportions,malformed limbs,extra limbs,cloned face,disfigured,gross proportions, (((missing arms))),((( missing legs))), (((extra arms))),(((extra legs))),pubic hair, plump,bad legs,error legs,username,blurry,bad feet, lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;足够丰富的描述，可以帮助 AI 理解描述我们的需求：&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/phodal/ai-research/master/output/better.jpeg&#34; alt=&#34;Better Quality&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;上图为 AI 生成，也是同样场景下，不屏蔽的结果 —— 其原因是大部分的模型库加入了很多 18 禁的内容。&lt;/p&gt; &#xA;&lt;h3&gt;小结&lt;/h3&gt; &#xA;&lt;p&gt;现在，让我们来思考一下，如何描述一张带人物的图片：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;详细的描述词。&lt;/li&gt; &#xA; &lt;li&gt;人物的姿势。&lt;/li&gt; &#xA; &lt;li&gt;丰富的模型。&lt;/li&gt; &#xA; &lt;li&gt;反复修改的参数。&lt;/li&gt; &#xA; &lt;li&gt;持续迭代。（基于 inpaint 等进行修改）&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;那么，文章呢？&lt;/p&gt; &#xA;&lt;h2&gt;文章：如何围绕特定主题思考？&lt;/h2&gt; &#xA;&lt;p&gt;结合一下先前 Stable Diffusion 的无数次的失败经验，我们应该先找好一个合适的框架来跑 ChatGPT。&lt;/p&gt; &#xA;&lt;p&gt;在编写内容的内容，我们会有各种思路和原则：STAR、金字塔原理、5W1H等等。&lt;/p&gt; &#xA;&lt;p&gt;我们可以以 STAR （Situation（情景）Task（任务）、Action（行动）和 Result（结果））可以作为与 ChatGPT 会话的基础：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;描述一个情境或背景，使读者对问题有更好的了解。&lt;/li&gt; &#xA; &lt;li&gt;说明任务或目标，告诉读者需要完成什么。&lt;/li&gt; &#xA; &lt;li&gt;描述你所采取的行动，例如使用什么方法、工具、策略等等。&lt;/li&gt; &#xA; &lt;li&gt;解释结果，包括遇到的挑战和取得的成果。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;这样，对于我们展开思路会有一些帮助。&lt;/p&gt; &#xA;&lt;h3&gt;情境：如何用 ChatGPT 写一篇文章，以如何用 ChatGPT 写一篇文章?&lt;/h3&gt; &#xA;&lt;p&gt;phodal：我想写一篇文章，主题是《如何用 ChatGPT 写一篇文章，以如何用 ChatGPT 写一篇文章 》&lt;/p&gt; &#xA;&lt;p&gt;示例：&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/phodal/ai-research/master/output/chatgpt-writing-article.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;但是，这样的内容并非我们想要的。&lt;/p&gt; &#xA;&lt;p&gt;而在这个标题里，其实 ChatGPT 已经理解了，如何写一篇文章，换作是一个冷门的话题，他就不知道了。在此时，我们需要构建一个更丰富的上下文：&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/phodal/ai-research/master/images/focus-driven.png&#34; alt=&#34;关注点驱动&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;在这时，ChatGPT 给出的步骤依旧是错的，我们需要继续纠正。所以，我们可以考虑在一开始给一个完整的上下文：&lt;/p&gt; &#xA;&lt;h3&gt;构建更完整的上下文&lt;/h3&gt; &#xA;&lt;p&gt;对于 ChatGPT 来说，我们需要明确的 prompt：：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#xA;我想写一篇文章，主题是《如何用 ChatGPT 写一篇文章，以如何用 ChatGPT 写一篇文章》，文章的大纲是：&#xA;&#xA;1. 先用 100 字介绍一下 ChatGPT&#xA;2. 以 Julia 作为可以以具体的案例为例，介绍如何使用 ChatGPT 写一篇博客大赛的参赛作品。这包括如何确定主题、如何撰写开头和结尾、如何组织文章结构等等。主题是：《Julia Dataframe 在大数据分析领域的使用》，大纲中包含：用 200 字介绍一下 Julia 语言历史；介绍如何使用 DataFrames.jl，并有个代码示例；对比一下 Python Pandas、Apache Spark Dataframe；小结。&#xA;3. 对本文的内容进行总结，并再次强调使用 ChatGPT 进行写作的好处。&#xA;&#xA;帮我写一篇文章？&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;如此一来，我们提供的上下文就会更加准确，就可以看到一个范例：&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/phodal/ai-research/master/images/chatgpt-article-demo.png&#34; alt=&#34;Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;基于这个范例，可以思考我们的内容如何编写。&lt;/p&gt; &#xA;&lt;h3&gt;构建框架（frameworks）&lt;/h3&gt; &#xA;&lt;p&gt;ChatGPT 本身构建了一个框架，我们可以再输入一个有用的框架，以构建简单的反馈回路。于是，我开始教他 DDD，然后相互学习：&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/phodal/ai-research/master/images/ddd-copilot-1.png&#34; alt=&#34;DDD Copilot&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;不过，ChatGPT 和上面的图片一样，经常丢失一些上下文。&lt;/p&gt; &#xA;&lt;p&gt;phodal: 在这个 TicketOrder 聚合里，他应该包含哪些 API ？&lt;/p&gt; &#xA;&lt;p&gt;在 TicketOrder 聚合里，可以包含以下一些 API：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;createOrder()：创建一个新的订单，可以接收顾客信息、场次信息、座位信息等参数，并返回一个唯一的订单号。&lt;/li&gt; &#xA; &lt;li&gt;cancelOrder(orderId)：取消某个订单，需要提供订单号。&lt;/li&gt; &#xA; &lt;li&gt;getOrder(orderId)：获取某个订单的详细信息，需要提供订单号。&lt;/li&gt; &#xA; &lt;li&gt;reserveSeats(showTimeId, seatIds)：在某个场次中预留座位，需要提供场次号和座位号。&lt;/li&gt; &#xA; &lt;li&gt;confirmOrder(orderId)：确认订单，此时座位被锁定并且订单状态被改为已确认。&lt;/li&gt; &#xA; &lt;li&gt;payOrder(orderId, paymentInfo)：支付订单，需要提供订单号和支付信息。&lt;/li&gt; &#xA; &lt;li&gt;refundOrder(orderId)：退票，需要提供订单号并且只能在订单状态为已确认时进行。&lt;/li&gt; &#xA; &lt;li&gt;updateOrder(orderId, updateInfo)：修改订单信息，可以更新顾客信息、场次信息、座位信息等，需要提供订单号和更新内容。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;基本的 API 可以满足电影院订票业务的需求，但具体实现还需要根据业务场景和需求进行细化和调整。&lt;/p&gt; &#xA;&lt;h3&gt;其它小建议：&lt;/h3&gt; &#xA;&lt;p&gt;相似的，与 Stable Diffusion 类似的是，AI 生成的内容经常出现各种问题，所以我们要注意：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;保持自然的表达。ChatGPT 能够生成流畅的语句，但有时可能会出现奇怪或不合适的表达。在使用 ChatGPT 进行写作时，需要注意保持自然的表达方式。&lt;/li&gt; &#xA; &lt;li&gt;适时修正错误。如果 ChatGPT 生成的内容不准确或有错误，应该及时进行修正，以保证文章的准确性和可读性。&lt;/li&gt; &#xA; &lt;li&gt;多次修改和调整。ChatGPT 的输出结果虽然会自动进行语法纠错和编辑，但是为了保证文章质量，还是需要经过多次修改和调整，不断完善文章结构和内容。&lt;/li&gt; &#xA; &lt;li&gt;保持简洁明了。在使用 ChatGPT 进行写作时，需要尽量保持简洁明了，避免使用过于复杂的句式和词汇，以便读者更好地理解文章内容。&lt;/li&gt; &#xA; &lt;li&gt;注重原创性和独特性。尽管 ChatGPT 能够生成大量的文章，但是为了在博客大赛中脱颖而出，还需要注重文章的原创性和独特性，尽可能地突出自己的风格和思想。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;考虑到 ChatGPT 的代码并不那么准确，让我们使用 GitHub Copilot 生成更多的内容。&lt;/p&gt; &#xA;&lt;h2&gt;代码生成：GitHub Copilot&lt;/h2&gt; &#xA;&lt;p&gt;GitHub Copilot 是 GitHub 2021 年 8 月推出的一个 AI 代码生成工具，它可以根据你的代码，生成更多的代码。&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/phodal/ai-research/master/images/github-copilot-1.png&#34; alt=&#34;Samples&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;所以，对于 Markdown 本身来说，它也能提供一些帮助。当然了，在这方向它没有 ChatGPT 那么强大，经常会出现一些奇怪的重复代码。&lt;/p&gt; &#xA;&lt;p&gt;但是，也并非不可用，我们可以使用它来生成一些代码片段，以便于我们进行修改。&lt;/p&gt; &#xA;&lt;h3&gt;完整示例：DDD + ChatGPT + Copilot&lt;/h3&gt; &#xA;&lt;p&gt;如结合 Chat GPT 我们可以完成代码的前置设计（当然有些不靠谱）：&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/phodal/ai-research/master/images/chatgpt-ddd.png&#34; alt=&#34;ChatGPT DDD&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;复制生成的 API，写行注释告诉 Copilot：&lt;code&gt;convert to markdown table&lt;/code&gt;，然后我们就有了：&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;API&lt;/th&gt; &#xA;   &lt;th&gt;描述&lt;/th&gt; &#xA;   &lt;th&gt;HTTP方法&lt;/th&gt; &#xA;   &lt;th&gt;URL&lt;/th&gt; &#xA;   &lt;th&gt;参数&lt;/th&gt; &#xA;   &lt;th&gt;返回值&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;创建订单&lt;/td&gt; &#xA;   &lt;td&gt;创建一个新的订单&lt;/td&gt; &#xA;   &lt;td&gt;POST&lt;/td&gt; &#xA;   &lt;td&gt;/ticket-orders&lt;/td&gt; &#xA;   &lt;td&gt;创建订单信息&lt;/td&gt; &#xA;   &lt;td&gt;订单ID&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;查询订单&lt;/td&gt; &#xA;   &lt;td&gt;查询一个已存在的订单&lt;/td&gt; &#xA;   &lt;td&gt;GET&lt;/td&gt; &#xA;   &lt;td&gt;/ticket-orders/{orderId}&lt;/td&gt; &#xA;   &lt;td&gt;订单ID&lt;/td&gt; &#xA;   &lt;td&gt;订单信息&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;取消订单&lt;/td&gt; &#xA;   &lt;td&gt;取消一个已存在的订单&lt;/td&gt; &#xA;   &lt;td&gt;DELETE&lt;/td&gt; &#xA;   &lt;td&gt;/ticket-orders/{orderId}&lt;/td&gt; &#xA;   &lt;td&gt;订单ID&lt;/td&gt; &#xA;   &lt;td&gt;取消成功或失败信息&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;添加座位&lt;/td&gt; &#xA;   &lt;td&gt;向一个已存在的订单添加座位&lt;/td&gt; &#xA;   &lt;td&gt;POST&lt;/td&gt; &#xA;   &lt;td&gt;/ticket-orders/{orderId}/seats&lt;/td&gt; &#xA;   &lt;td&gt;座位信息&lt;/td&gt; &#xA;   &lt;td&gt;座位添加成功或失败信息&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;移除座位&lt;/td&gt; &#xA;   &lt;td&gt;从一个已存在的订单中移除座位&lt;/td&gt; &#xA;   &lt;td&gt;DELETE&lt;/td&gt; &#xA;   &lt;td&gt;/ticket-orders/{orderId}/seats/{seatId}&lt;/td&gt; &#xA;   &lt;td&gt;座位ID&lt;/td&gt; &#xA;   &lt;td&gt;座位移除成功或失败信息&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;查询可用座位&lt;/td&gt; &#xA;   &lt;td&gt;查询某个场次电影的可用座位&lt;/td&gt; &#xA;   &lt;td&gt;GET&lt;/td&gt; &#xA;   &lt;td&gt;/cinema-schedule/{scheduleId}/available-seats&lt;/td&gt; &#xA;   &lt;td&gt;场次ID&lt;/td&gt; &#xA;   &lt;td&gt;可用座位信息&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;接下来，我们可以使用 Spring + Copilot 来生成一些代码片段，以便于我们进行修改：&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/phodal/ai-research/master/images/github-sample-2.png&#34; alt=&#34;github-copilot Sample&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;在有足够类相关信息的情况下，诸如于，我们提供一个注释：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-kotlin&#34;&gt;@PostMapping&#xA;fun createTicketOrder(@RequestBody ticketOrder: TicketOrder): String {&#xA;    // check the schedule is valid, the seats are available, then create the order&#xA;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;它生成的代码会更加准确：&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/phodal/ai-research/master/images/github-sample-4.png&#34; alt=&#34;GitHub sample 4&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;而如果我们添加了对应的 Exception 的处理注释：&lt;code&gt;check the schedule is valid, the seats are available, then create the order, if the schedule is not valid, throw NotScheduleAvailableException&lt;/code&gt;，代码进一步转化为：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-kotlin&#34;&gt;@PostMapping&#xA;fun createTicketOrder(@RequestBody ticketOrder: TicketOrder): String {&#xA;    // check the schedule is valid, the seats are available, then create the order, if the schedule is not valid, throw NotScheduleAvailableException&#xA;    // check the schedule is valid&#xA;    val schedule = cinemaScheduleService.getSchedule(ticketOrder.scheduleId)&#xA;    if (schedule == null) {&#xA;        throw NotScheduleAvailableException()&#xA;    }&#xA;    // check the seats are available&#xA;    val availableSeats = cinemaScheduleService.getAvailableSeats(ticketOrder.scheduleId)&#xA;    if (!availableSeats.containsAll(ticketOrder.seats)) {&#xA;        throw NotSeatAvailableException()&#xA;    }&#xA;    // create order&#xA;    return tickerOrderApplicationService.createOrder(ticketOrder)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;在另外一些场景下，如果你的函数名和参数名都是英文，那么 Copilot 生成的代码也会更加准确：&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/phodal/ai-research/master/images/github-sample-5.png&#34; alt=&#34;GitHub sample 5&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;而在单元测试的场景下，Copilot 更加强大和智能，它能够根据你的测试代码，生成更多的测试代码。&lt;/p&gt; &#xA;&lt;h3&gt;其它：代码生成开源模型&lt;/h3&gt; &#xA;&lt;p&gt;我也尝试了下 Saleforce 的代码模型：&lt;a href=&#34;https://huggingface.co/Salesforce/codegen-6B-mono&#34;&gt;https://huggingface.co/Salesforce/codegen-6B-mono&lt;/a&gt;，但是不能在我的 MBP 上跑上来，有些遗憾。&lt;/p&gt; &#xA;&lt;h2&gt;总结：AI 就像占卜&lt;/h2&gt; &#xA;&lt;p&gt;你可能无法预期输出的是啥，总会有各种惊喜。&lt;/p&gt; &#xA;&lt;p&gt;而其实，与 AI 的 prompt 相似的是：和人类聊天的时候，我们也需要提供足够的上下文。所以，你需要一下 ChatGPT，他并不总是能理解你的上下文。&lt;/p&gt; &#xA;&lt;h2&gt;其它：Macbook Pro 的吐槽&lt;/h2&gt; &#xA;&lt;p&gt;公司配置的 Macbook Pro 2019 虽然有 AMD 显卡，但是没有 CUDA、ROCm 的支持，所以只能用 CPU 来跑了。&lt;/p&gt; &#xA;&lt;h3&gt;AI 生态不友好&lt;/h3&gt; &#xA;&lt;p&gt;诸如于 PyTorch，以前是支持 ROCm 加速的：&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://pytorch.org/assets/images/amd_rocm_blog.png&#34; alt=&#34;ROCm&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;现在的版本不行了：&lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;https://pytorch.org/get-started/locally/&lt;/a&gt; 。&lt;/p&gt; &#xA;&lt;h2&gt;本文相关资源&lt;/h2&gt; &#xA;&lt;p&gt;AI 生成图片：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Stable Diffusion Webui GitHub： &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui&#34;&gt;https://github.com/AUTOMATIC1111/stable-diffusion-webui&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;通用的 AI 模型社区：&lt;a href=&#34;https://huggingface.co/&#34;&gt;https://huggingface.co/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Stable Diffusion AI 艺术模型社区：&lt;a href=&#34;https://civitai.com/&#34;&gt;https://civitai.com/&lt;/a&gt; （18 禁）&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;代码模型相关：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Salesforce 模型：&lt;a href=&#34;https://huggingface.co/Salesforce/codegen-6B-mono&#34;&gt;https://huggingface.co/Salesforce/codegen-6B-mono&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;CarpserAI： &lt;a href=&#34;https://huggingface.co/CarperAI/diff-codegen-6b-v2&#34;&gt;https://huggingface.co/CarperAI/diff-codegen-6b-v2&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;ControlNet 加强：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;模型：&lt;a href=&#34;https://huggingface.co/lllyasviel/ControlNet&#34;&gt;https://huggingface.co/lllyasviel/ControlNet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;预编译：&lt;a href=&#34;https://huggingface.co/kohya-ss/ControlNet-diff-modules&#34;&gt;https://huggingface.co/kohya-ss/ControlNet-diff-modules&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>wibus-wee/stable_diffusion_chilloutmix_ipynb</title>
    <updated>2023-02-24T01:38:36Z</updated>
    <id>tag:github.com,2023-02-24:/wibus-wee/stable_diffusion_chilloutmix_ipynb</id>
    <link href="https://github.com/wibus-wee/stable_diffusion_chilloutmix_ipynb" rel="alternate"></link>
    <summary type="html">&lt;p&gt;AUTOMATIC1111 Stable Diffusion WebUI 1.5 + ChilloutMix&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AUTOMATIC1111 Stable Diffusion WebUI 1.5 + ChilloutMix Checkpoint&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/wibus-wee/stable_diffusion_chilloutmix_colab/blob/main/stable_diffusion_1_5_webui.ipynb&#34;&gt;Go To Colab&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/wibus-wee/stable_diffusion_chilloutmix_ipynb/main/prompts.md&#34;&gt;Prompts Collection&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;不附带任何 Loras，需要在 &lt;strong&gt;CivitAI&lt;/strong&gt; 中输入自行下载，记住勾选保存加入 Additional Network 选项。默认使用 ChilloutMix Checkpoint，可另外选择 SunshineMix Checkpoint&lt;/p&gt; &#xA;&lt;p&gt;目前已经支持 Paperspace 平台，但仍建议使用 Colab，因为目前免费使用的 Paperspace M4000 GPU 暂无 Xformers 支持，在生成图片时速度会慢于 Colab。&lt;/p&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Paperspace 平台支持&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; LoRA 训练 Jupyter Notebook 支持&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 使用 aria2 优化模型下载速度&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 国际化 (i18n)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;训练模型安装启动与一般安装启动区别&lt;/h2&gt; &#xA;&lt;p&gt;训练模型将会加载 &lt;code&gt;Dreambooth Extension for Stable-Diffusion-WebUI&lt;/code&gt; 插件，该插件将会在生成图片时自动保存到本地，而不是直接在网页上显示。&lt;/p&gt; &#xA;&lt;p&gt;与一般的启动不同也就只在于这里，由于 &lt;code&gt;Dreambooth Extension for Stable-Diffusion-WebUI&lt;/code&gt; 的特殊性，我需要在启动前加入一些额外的代码。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export REQS_FILE=&#34;./extensions/sd_dreambooth_extension/requirements.txt&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;但一般情况下我们并不需要训练模型，所以我将其放在了一个单独的区块中，如果你不需要训练模型，可以不使用它，它会拖慢启动速度 ( Git Clone, Install Requirements ), 当然，这不会影响生成图片的速度。&lt;/p&gt; &#xA;&lt;p&gt;训练模型我推荐使用 Colab，因为 Paperspace 平台的 M4000 GPU 暂无 Xformers 支持，所以在训练时速度会慢于 Colab。&lt;/p&gt; &#xA;&lt;h3&gt;⚠️ 注意&lt;/h3&gt; &#xA;&lt;p&gt;根据目前我所了解的信息，sd_dreambooth_extension 似乎无法很好地工作，训练效果并不理想。现在更推荐使用 &lt;a href=&#34;https://github.com/bmaltais/kohya_ss&#34;&gt;bmaltais/kohya_ss&lt;/a&gt; / &lt;a href=&#34;https://github.com/Akegarasu/lora-scripts&#34;&gt;Akegarasu/lora-scripts&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;新的 Jupyter Notebook 正在来的路上 (Power by &lt;a href=&#34;https://github.com/Akegarasu/lora-scripts&#34;&gt;Akegarasu/lora-scripts&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;由于 Colab 无法使用 Docker，因此大概率无法使用现存的 kohya_ss gui ( &lt;a href=&#34;https://github.com/googlecolab/colabtools/issues/299&#34;&gt;googlecolab/colabtools - #299&lt;/a&gt; )&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Loras&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;(x1)&lt;/code&gt; 「兼容性极佳」St. Louis (Luxurious Wheels) (Azur Lane): &lt;a href=&#34;https://civitai.com/models/6669/st-louis-luxurious-wheels-azur-lane&#34;&gt;https://civitai.com/models/6669/st-louis-luxurious-wheels-azur-lane&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;(x1)&lt;/code&gt; 「兼容性良好」Girls&#39; Frontline-OTs-14&#34;lightning&#34;: &lt;a href=&#34;https://civitai.com/models/6525/girls-frontline-ots-14lightning&#34;&gt;https://civitai.com/models/6525/girls-frontline-ots-14lightning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;(x0.5)&lt;/code&gt; 「兼容性优秀」@kbr/Korean Doll Likeness: &lt;a href=&#34;https://civitai.com/models/7448/korean-doll-likeness&#34;&gt;https://civitai.com/models/7448/korean-doll-likeness&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;(x0.5)&lt;/code&gt; 「兼容性中等」@kbr/Japanese Doll Likeness: &lt;a href=&#34;https://civitai.com/models/10135&#34;&gt;https://civitai.com/models/10135&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;(x0.5)&lt;/code&gt; 「兼容性中等」@kbr/Taiwan Doll Likeness : &lt;a href=&#34;https://civitai.com/models/7716/taiwan-doll-likeness&#34;&gt;https://civitai.com/models/7716/taiwan-doll-likeness&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;(x0.5)&lt;/code&gt; 「兼容性中等」Yae Miko | Realistic Genshin (Mixed): &lt;a href=&#34;https://civitai.com/models/8484/yae-miko-or-realistic-genshin&#34;&gt;https://civitai.com/models/8484/yae-miko-or-realistic-genshin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;(x0.5)&lt;/code&gt; 「兼容性良好」Gakki | Aragaki Yui | 新垣結衣: &lt;a href=&#34;https://civitai.com/models/8416/gakki-or-aragaki-yui-or&#34;&gt;https://civitai.com/models/8416/gakki-or-aragaki-yui-or&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Checkpoints&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[三次元] ChilloutMix: &lt;a href=&#34;https://civitai.com/models/6424/chilloutmix&#34;&gt;https://civitai.com/models/6424/chilloutmix&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[二点五次元] SunshineMix (偏向插画类): &lt;a href=&#34;https://civitai.com/models/9291/sunshinemix&#34;&gt;https://civitai.com/models/9291/sunshinemix&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[二次元] Grapefruit (hentai model): &lt;a href=&#34;https://civitai.com/models/2583/grapefruit-hentai-model&#34;&gt;https://civitai.com/models/2583/grapefruit-hentai-model&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;试验性 Lora&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;: Please be aware of the legal issues&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;「质量一般」liuyifei: &lt;a href=&#34;https://civitai.com/models/8453/liuyifei&#34;&gt;https://civitai.com/models/8453/liuyifei&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;「兼容性较为一般」Lisa Blackpink: &lt;a href=&#34;https://civitai.com/models/8605/lisa-blackpink&#34;&gt;https://civitai.com/models/8605/lisa-blackpink&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;「与描述不匹配」Jisoo Blackpink: &lt;a href=&#34;https://civitai.com/models/8311/jisoo-blackpink&#34;&gt;https://civitai.com/models/8311/jisoo-blackpink&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;「与描述不匹配，但不看描述还算可以，兼容性良好」Rosè Blackpink: &lt;a href=&#34;https://civitai.com/models/8600/rose-blackpink&#34;&gt;https://civitai.com/models/8600/rose-blackpink&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;试验性 CheckPoints&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;None&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;更新日志&lt;/h2&gt; &#xA;&lt;h3&gt;2023.2.21&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;更好的选项选择机制&lt;/li&gt; &#xA; &lt;li&gt;细化区分 Paperspace 与 Colab 平台而并非靠单一 Free GPU 型号&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;2023.2.19&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;修复错误识别 Quadro M4000 GPU 的问题&lt;/li&gt; &#xA; &lt;li&gt;新增&lt;strong&gt;训练模型&lt;/strong&gt;安装方案 ( Power by &lt;a href=&#34;https://github.com/d8ahazard/sd_dreambooth_extension&#34;&gt;d8ahazard/sd_dreambooth_extension&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;优化 checkpoint 选择&lt;/li&gt; &#xA; &lt;li&gt;修复无法读取变量的问题&lt;/li&gt; &#xA; &lt;li&gt;自动检查 webUI 与 训练扩展是否被安装&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;2023.2.18&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;更新上游 ChilloutMix 最新版本&lt;/li&gt; &#xA; &lt;li&gt;更新 Prompts Collection&lt;/li&gt; &#xA; &lt;li&gt;兼容 Paperspace 平台（ Free GPU ）&lt;/li&gt; &#xA; &lt;li&gt;导出全部生成图片&lt;/li&gt; &#xA; &lt;li&gt;使用 &lt;code&gt;nvidia-smi&lt;/code&gt; 查看 GPU 信息&lt;/li&gt; &#xA; &lt;li&gt;检查 GPU 是否支持 Xformers&lt;/li&gt; &#xA; &lt;li&gt;优化变量传递&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;2023.2.17&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;新增 SunshineMix Checkpoint 作为 2.5D 作画的第二选择&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;2023.2.16&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;由于中途可能会出现突然退出的情况，所以提供多了一个再次启动的方案&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Created by &lt;a href=&#34;https://github.com/wibus-wee&#34;&gt;@wibus-wee&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;Reference: &lt;a href=&#34;https://github.com/camenduru/stable-diffusion-webui-colab&#34;&gt;camenduru/stable-diffusion-webui-colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt;</summary>
  </entry>
  <entry>
    <title>cross-domain-compositing/cross-domain-compositing</title>
    <updated>2023-02-24T01:38:36Z</updated>
    <id>tag:github.com,2023-02-24:/cross-domain-compositing/cross-domain-compositing</id>
    <link href="https://github.com/cross-domain-compositing/cross-domain-compositing" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Cross-domain Compositing with Pretrained Diffusion Models&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.10167&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2302.10167-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;: &lt;br&gt; Diffusion models have enabled high-quality, conditional image editing capabilities. We propose to expand their arsenal, and demonstrate that off-the-shelf diffusion models can be used for a wide range of cross-domain compositing tasks. Among numerous others, these include image blending, object immersion, texture-replacement and even CG2Real translation or stylization. We employ a localized, iterative refinement scheme which infuses the injected objects with contextual information derived from the background scene, and enables control over the degree and types of changes the object may undergo. We conduct a range of qualitative and quantitative comparisons to prior work, and exhibit that our method produces higher quality and realistic results without requiring any annotations or training. Finally, we demonstrate how our method may be used for data augmentation of downstream tasks.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;This is the official implementation of Cross-domain Compositing (CDC), a local, inference-time, image editing method which utilizes pretrained diffusion models for image compositing in various domains.&lt;br&gt; We base our method on previous work in global image editing in inference-time, and propose a localized extension which enables applications such as: guided image inpainting, cross-domain image compositing and object-guided Sim2Real. The &lt;em&gt;fidelity-realism&lt;/em&gt; tradeoff is controlled by our parameters.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cross-domain-compositing/cross-domain-compositing/main/assets/cdc_teaser.jpg&#34; alt=&#34;teaser&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;This code builds on the &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;Stable Diffusion&lt;/a&gt; codebase.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the repo:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone --recursive https://github.com/cross-domain-compositing/cross-domain-compositing.git&#xA;cd cross-domain-compositing/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Create a new environment:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env create -f environment.yaml&#xA;conda activate ldm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Or&lt;/strong&gt; install additional requirements to existing Stable Diffusion environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda activate ldm&#xA;conda install -c anaconda scikit-learn&#xA;conda install -c conda-forge h5py&#xA;conda install -c conda-forge plyfile&#xA;conda install -c conda-forge trimesh&#xA;conda install -c conda-forge natsort&#xA;pip install PyMCubes&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that these are needed only if intended to run SVR. 3. Add submodules path and set PYTHONPATH to point to the root of the repository (we used &lt;a href=&#34;https://github.com/assafshocher/ResizeRight.git&#34;&gt;ResizeRight&lt;/a&gt; for image resizing):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export PYTHONPATH=$PYTHONPATH:$(pwd):$(pwd)/ResizeRight&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Download pretrained Stable Diffusion checkpoints:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;wget -P models/ldm/stable-diffusion-v1 https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/resolve/main/sd-v1-4.ckpt&#xA;wget -P models/ldm/stable-diffusion-v1 https://huggingface.co/runwayml/stable-diffusion-inpainting/resolve/main/sd-v1-5-inpainting.ckpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;Stable Diffusion&lt;/a&gt; for more information on available model checkpoints.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;On top of the original img2img arguments (see &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;Stable Diffusion&lt;/a&gt;), we add the following for controlled levels of local image editing:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--mask&lt;/code&gt; - Path to mask or directory containing masks (1 is for FG, 0 is for BG).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--T_in&lt;/code&gt; - Editing strength for inner region (1 in mask). 0 is for no conditioning at all, 1 for full guidance. Controls the amount of similarity to the reference image, using a high T allows for adding finer details to the reference at expense of similarity.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--T_out&lt;/code&gt; - Editing strength for outer region (0 in mask). 0 is for no conditioning at all, 1 for full guidance. Controls the amount of similarity to the reference image, using a high T allows for adding finer details to the reference at expense of similarity.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--down_N_in&lt;/code&gt; - Scaling (downsampling) factor for inner region (1 in mask). Has a similar effect to &lt;code&gt;T_in&lt;/code&gt; but relates more to structure.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--down_N_in&lt;/code&gt; - Scaling (downsampling) factor for outer region (0 in mask). Has a similar effect to &lt;code&gt;T_out&lt;/code&gt; but relates more to structure.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--blend_pix&lt;/code&gt; - Number of pixels for mask smoothing (see paper).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--repaint_start&lt;/code&gt; - When to start resampling for increased receptive field (see paper). 0 is for no resampling, 1 to start from first step.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--mask_dilate&lt;/code&gt; - Dilate mask by number of pixels.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We also implement Paint-By-Word from &lt;a href=&#34;https://arxiv.org/abs/2211.01324&#34;&gt;eDiff-I&lt;/a&gt; which enables localized text guidance to some degree, to use it:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--prompt_in/prompt_out&lt;/code&gt; - Prompts for inner/outer regions (must appear in &lt;code&gt;--prompt&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--prompt_amplifier_in/prompt_amplifier_out&lt;/code&gt; - Prompt weight for inner/outer regions.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Our arguments are also sweepable! To do so simply supply multiple parameters to the desired arguments, and the script will sweep all permutations. To define specific sets of sweeps use &lt;code&gt;--sweep_tuples&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;h4&gt;Locally Guided Image Editing&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts_cdc/img2img.py --config configs/stable-diffusion/v1-inference.yaml --ckpt models/ldm/stable-diffusion-v1/sd-v1-4.ckpt --init_img examples/scribbles/images/ --mask examples/scribbles/masks/ --from_file examples/scribbles/prompts.txt --batch_size 1 --n_samples 1 --outdir outputs/scribbles --ddim_steps 50 --strength 1.0 --T_out 1.0 --T_in 0.0 0.2 0.4 0.6 0.8 --down_N_out 1 --down_N_in 1 2 4 --seed 42 --repaint_start 0 0.2 0.4 0.6 0.8 --skip_grid&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also supply a config.yaml from a previous run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts_cdc/img2img.py --config [config]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Background augmentation&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts_cdc/img2img_inpaint.py --config configs/stable-diffusion/v1-inpainting-inference.yaml --ckpt models/ldm/stable-diffusion-v1/sd-v1-5-inpainting.ckpt --prompt &#34;A photograph of a sofa in a living room&#34; --init_img examples/sofas/images/ --mask examples/sofas/masks/ --n_samples 1 --outdir outputs/sofas --ddim_steps 50 --strength 1 --T_in 0 --T_out 0.5 --down_N_in 1 --down_N_out 1 --blend_pix 0 --seed 42 --repaint_start 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Single View 3D Reconstruction&lt;/h3&gt; &#xA;&lt;p&gt;We adopted &lt;a href=&#34;https://github.com/ManyiLi12345/D2IM-Net&#34;&gt;D^2IMNet&lt;/a&gt; for SVR model architecture, &lt;a href=&#34;https://github.com/Xharlie/DISN&#34;&gt;DISN&lt;/a&gt; data for training and &lt;a href=&#34;https://github.com/autonomousvision/occupancy_networks&#34;&gt;OccNet&lt;/a&gt; data for 3D evaluation. The model training/testing/preprocessing scripts are forked from &lt;a href=&#34;https://github.com/ManyiLi12345/D2IM-Net&#34;&gt;D^2IMNet&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Environment Configuration&lt;/h4&gt; &#xA;&lt;p&gt;We use &lt;a href=&#34;https://github.com/ThibaultGROUEIX/ChamferDistancePytorch.git&#34;&gt;ChamferDistancePytorch&lt;/a&gt; for chamfer distance evaluation.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git submodule add https://github.com/ThibaultGROUEIX/ChamferDistancePytorch.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Data Preparation&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download and unzip rendered &lt;a href=&#34;https://github.com/Xharlie/ShapenetRender_more_variation&#34;&gt;image data&lt;/a&gt; and &lt;a href=&#34;https://drive.google.com/file/d/1cHDickPLKLz3smQNpOGXD2W5mkXcy1nq/view?usp=sharing&#34;&gt;GT ShapeNet SDF models&lt;/a&gt; from &lt;a href=&#34;https://github.com/Xharlie/DISN&#34;&gt;DISN&lt;/a&gt; to &lt;code&gt;./SVR/data/&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Preprocess image data by running scripts in &lt;code&gt;./SVR/preprocessing&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Optional] Download processed ShapeNet data from &lt;a href=&#34;https://github.com/autonomousvision/occupancy_networks/raw/master/scripts/download_data.sh&#34;&gt;OccNet&lt;/a&gt; for 3D test result evaluation&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;Data Augmentation&lt;/h4&gt; &#xA;&lt;p&gt;Follow the steps below to perform background augmentation on the ShapeNet dataset.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Configure &lt;code&gt;./SVR/utils.py&lt;/code&gt;. Specify ShapeNet category ID of interest, articulate on text prompt, choose camera views and set foreground conditioning strength.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;python ./SVR/augment_ShapeNet_data.py&lt;/code&gt;. Change the save directory in the script if needed.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;Training and testing&lt;/h4&gt; &#xA;&lt;p&gt;Follow the steps below to train D^2IMNet on the augmented dataset.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;python ./SVR/train_test_split.py&lt;/code&gt; - Create train test split lst files.&lt;/li&gt; &#xA; &lt;li&gt;Configure &lt;code&gt;./SVR/utils.py&lt;/code&gt;. Specify the category of interest, the path to training images, and file names to save the trained model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;python ./SVR/train/train.py&lt;/code&gt; - Train SDFNet.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;python ./SVR/train/train_cam.py&lt;/code&gt; - Train CamNet.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;python ./SVR/test/test.py&lt;/code&gt; - Test on in-domain images.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;Test on in-the-wild images&lt;/h4&gt; &#xA;&lt;p&gt;Follow the steps below to test and evaluate in-the-wild images.:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Source in-the-wild images and preprocess them to 224 x 224. Save the folder of test images to &lt;code&gt;./SVR/data&lt;/code&gt;. Alternatively, we provide our processed sofa samples &lt;a href=&#34;https://drive.google.com/drive/folders/1Tp1ogEfZz9HmQakTTnSMrYWEgmC4oiYG?usp=sharing&#34;&gt;here&lt;/a&gt;, please save them to &lt;code&gt;./SVR/data/sofa_samples&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;python ./SVR/test/test_external_cam.py&lt;/code&gt; and &lt;code&gt;python ./SVR/test/test_external_images.py&lt;/code&gt; to get the predicted camera pose and 3D model. Remember to specify the input/output paths.&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;python ./SVR/blender_render_trans_bg.py&lt;/code&gt; to render the reconstructed model under the predicted camera pose. Remember to specify the input/output paths. We used blender 3.4.1 python API to run this script.&lt;/li&gt; &#xA; &lt;li&gt;Extract the reference mask of the input images, we used &lt;a href=&#34;https://github.com/xuebinqin/U-2-Net&#34;&gt;U^2Net&lt;/a&gt;. Save the folder of extracted reference masks in &lt;code&gt;./SVR/result&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;python ./SVR/eval/eval_2D_iou.py&lt;/code&gt; to evaluate 2D-IOU on in the wild images. Remember to specify the input/output paths.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Results&lt;/h2&gt; &#xA;&lt;h4&gt;Image Modification via Scribbles&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cross-domain-compositing/cross-domain-compositing/main/assets/scribbles.jpg&#34; alt=&#34;scribbles&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Object Immersion in Paintings&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cross-domain-compositing/cross-domain-compositing/main/assets/immersion.jpg&#34; alt=&#34;immersion&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Background Augmentation&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cross-domain-compositing/cross-domain-compositing/main/assets/augmentation.jpg&#34; alt=&#34;augmentation&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Parameters Configuration Effects&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cross-domain-compositing/cross-domain-compositing/main/assets/TNR.jpg&#34; alt=&#34;TNR&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use our work, please cite our &lt;a href=&#34;https://arxiv.org/abs/2302.10167&#34;&gt;paper&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>