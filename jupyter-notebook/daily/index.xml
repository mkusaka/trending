<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-11-17T01:35:53Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>SHI-Labs/OneFormer</title>
    <updated>2022-11-17T01:35:53Z</updated>
    <id>tag:github.com,2022-11-17:/SHI-Labs/OneFormer</id>
    <link href="https://github.com/SHI-Labs/OneFormer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[Preprint] OneFormer: One Transformer to Rule Universal Image Segmentation, 2022&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OneFormer: One Transformer to Rule Universal Image Segmentation&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pytorch.org/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Framework-PyTorch-orange.svg?sanitize=true&#34; alt=&#34;Framework: PyTorch&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/SHI-Labs/OneFormer/blob/main/colab/oneformer_colab.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-red.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://paperswithcode.com/sota/panoptic-segmentation-on-ade20k-val?p=oneformer-one-transformer-to-rule-universal&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/oneformer-one-transformer-to-rule-universal/panoptic-segmentation-on-ade20k-val&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/instance-segmentation-on-ade20k-val?p=oneformer-one-transformer-to-rule-universal&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/oneformer-one-transformer-to-rule-universal/instance-segmentation-on-ade20k-val&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/instance-segmentation-on-cityscapes-val?p=oneformer-one-transformer-to-rule-universal&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/oneformer-one-transformer-to-rule-universal/instance-segmentation-on-cityscapes-val&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/semantic-segmentation-on-coco-1?p=oneformer-one-transformer-to-rule-universal&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/oneformer-one-transformer-to-rule-universal/semantic-segmentation-on-coco-1&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/instance-segmentation-on-coco-val-panoptic?p=oneformer-one-transformer-to-rule-universal&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/oneformer-one-transformer-to-rule-universal/instance-segmentation-on-coco-val-panoptic&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/panoptic-segmentation-on-cityscapes-val?p=oneformer-one-transformer-to-rule-universal&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/oneformer-one-transformer-to-rule-universal/panoptic-segmentation-on-cityscapes-val&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/semantic-segmentation-on-ade20k-val?p=oneformer-one-transformer-to-rule-universal&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/oneformer-one-transformer-to-rule-universal/semantic-segmentation-on-ade20k-val&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/panoptic-segmentation-on-coco-minival?p=oneformer-one-transformer-to-rule-universal&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/oneformer-one-transformer-to-rule-universal/panoptic-segmentation-on-coco-minival&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/semantic-segmentation-on-cityscapes-val?p=oneformer-one-transformer-to-rule-universal&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/oneformer-one-transformer-to-rule-universal/semantic-segmentation-on-cityscapes-val&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- [![Huggingface space](https://img.shields.io/badge/🤗-Huggingface%20Space-cyan.svg)](https://huggingface.co/spaces/shi-labs/OneFormer) --&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://praeclarumjj3.github.io/&#34;&gt;Jitesh Jain&lt;/a&gt;, &lt;a href=&#34;https://chrisjuniorli.github.io/&#34;&gt;Jiachen Li&lt;/a&gt;&lt;sup&gt;†&lt;/sup&gt;, &lt;a href=&#34;https://www.linkedin.com/in/mtchiu/&#34;&gt;MangTik Chiu&lt;/a&gt;&lt;sup&gt;†&lt;/sup&gt;, &lt;a href=&#34;https://alihassanijr.com/&#34;&gt;Ali Hassani&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/nukich74/&#34;&gt;Nikita Orlov&lt;/a&gt;, &lt;a href=&#34;https://www.humphreyshi.com/home&#34;&gt;Humphrey Shi&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;sup&gt;†&lt;/sup&gt; Equal Contribution&lt;/p&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://praeclarumjj3.github.io/oneformer/&#34;&gt;&lt;code&gt;Project Page&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/2211.06220&#34;&gt;&lt;code&gt;arXiv&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/pdf/2211.06220.pdf&#34;&gt;&lt;code&gt;pdf&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/SHI-Labs/OneFormer/main/#4citation&#34;&gt;&lt;code&gt;BibTeX&lt;/code&gt;&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;This repo contains the code for our paper &lt;strong&gt;OneFormer: One Transformer to Rule Universal Image Segmentation&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/SHI-Labs/OneFormer/main/images/teaser.png&#34; width=&#34;100%&#34;&gt; &#xA;&lt;h4&gt;Features&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OneFormer is the &lt;strong&gt;first&lt;/strong&gt; multi-task universal image segmentation framework based on transformers.&lt;/li&gt; &#xA; &lt;li&gt;OneFormer needs to be trained only once with a single universal architecture, a single model, and on a single dataset , to outperform existing frameworks across semantic, instance, and panoptic segmentation tasks.&lt;/li&gt; &#xA; &lt;li&gt;OneFormer uses a task-conditioned joint training strategy, uniformly sampling different ground truth domains (semantic instance, or panoptic) by deriving all labels from panoptic annotations to train its multi-task model.&lt;/li&gt; &#xA; &lt;li&gt;OneFormer uses a task token to condition the model on the task in focus, making our architecture task-guided for training, and task-dynamic for inference, all with a single model.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/SHI-Labs/OneFormer/main/images/oneformer.svg?sanitize=true&#34; alt=&#34;OneFormer&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SHI-Labs/OneFormer/main/#news&#34;&gt;News&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SHI-Labs/OneFormer/main/#installation-instructions&#34;&gt;Installation Instructions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SHI-Labs/OneFormer/main/#dataset-preparation&#34;&gt;Dataset Preparation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SHI-Labs/OneFormer/main/#execution-instructions&#34;&gt;Execution Instructions&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SHI-Labs/OneFormer/main/#training&#34;&gt;Training&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SHI-Labs/OneFormer/main/#evaluation&#34;&gt;Evaluation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SHI-Labs/OneFormer/main/#demo&#34;&gt;Demo&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SHI-Labs/OneFormer/main/#results&#34;&gt;Results&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SHI-Labs/OneFormer/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;h3&gt;November 10, 2022&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://praeclarumjj3.github.io/oneformer/&#34;&gt;&lt;strong&gt;Project Page&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://praeclarumjj3.github.io/oneformer/&#34;&gt;&lt;strong&gt;ArXiv Preprint&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://praeclarumjj3.github.io/oneformer/&#34;&gt;&lt;strong&gt;GitHub Repo&lt;/strong&gt;&lt;/a&gt; are public!&lt;/li&gt; &#xA; &lt;li&gt;OneFormer sets new SOTA on Cityscapes val with single-scale inference on Panoptic Segmentation with &lt;strong&gt;68.5&lt;/strong&gt; PQ score and Instance Segmentation with &lt;strong&gt;46.7&lt;/strong&gt; AP score!&lt;/li&gt; &#xA; &lt;li&gt;OneFormer sets new SOTA on ADE20K val on Panoptic Segmentation with &lt;strong&gt;50.2&lt;/strong&gt; PQ score and on Instance Segmentation with &lt;strong&gt;37.6&lt;/strong&gt; AP!&lt;/li&gt; &#xA; &lt;li&gt;OneFormer sets new SOTA on COCO val on Panoptic Segmentation with &lt;strong&gt;58.0&lt;/strong&gt; PQ score!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation Instructions&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We use Python 3.8, PyTorch 1.10.1 (CUDA 11.3 build).&lt;/li&gt; &#xA; &lt;li&gt;We use Detectron2-v0.6.&lt;/li&gt; &#xA; &lt;li&gt;For complete installation instructions, please see &lt;a href=&#34;https://raw.githubusercontent.com/SHI-Labs/OneFormer/main/INSTALL.md&#34;&gt;INSTALL.md&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Dataset Preparation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We experiment on three major benchmark dataset: ADE20K, Cityscapes and COCO 2017.&lt;/li&gt; &#xA; &lt;li&gt;Please see &lt;a href=&#34;https://raw.githubusercontent.com/SHI-Labs/OneFormer/main/datasets/README.md&#34;&gt;Preparing Datasets for OneFormer&lt;/a&gt; for complete instructions for preparing the datasets.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Execution Instructions&lt;/h2&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We train all our models using 8 A6000 (48 GB each) GPUs.&lt;/li&gt; &#xA; &lt;li&gt;We use 8 A100 (80 GB each) for training Swin-L&lt;sup&gt;†&lt;/sup&gt; OneFormer and DiNAT-L&lt;sup&gt;†&lt;/sup&gt; OneFormer on COCO and all models with ConvNeXt-XL&lt;sup&gt;†&lt;/sup&gt; backbone. We also train the 896x896 models on ADE20K on 8 A100 GPUs.&lt;/li&gt; &#xA; &lt;li&gt;Please see &lt;a href=&#34;https://raw.githubusercontent.com/SHI-Labs/OneFormer/main/GETTING_STARTED.md&#34;&gt;Getting Started with OneFormer&lt;/a&gt; for training commands.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Evaluation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Please see &lt;a href=&#34;https://raw.githubusercontent.com/SHI-Labs/OneFormer/main/GETTING_STARTED.md&#34;&gt;Getting Started with OneFormer&lt;/a&gt; for evaluation commands.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Demo&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We provide a quick to run demo on Colab &lt;a href=&#34;https://colab.research.google.com/github/SHI-Labs/OneFormer/blob/main/colab/oneformer_colab.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- and Hugging Face Spaces. [![Huggingface space](https://img.shields.io/badge/🤗-Huggingface%20Space-cyan.svg)](https://huggingface.co/spaces/shi-labs/OneFormer). --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Please see &lt;a href=&#34;https://raw.githubusercontent.com/SHI-Labs/OneFormer/main/demo/README.md&#34;&gt;OneFormer Demo&lt;/a&gt; for command line instructions on running the demo.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Results&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/SHI-Labs/OneFormer/main/images/plots.svg?sanitize=true&#34; alt=&#34;Results&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;† denotes the backbones were pretrained on ImageNet-22k.&lt;/li&gt; &#xA; &lt;li&gt;Pre-trained models can be downloaded following the instructions given &lt;a href=&#34;https://raw.githubusercontent.com/SHI-Labs/OneFormer/main/tools/README.md/#download-pretrained-weights&#34;&gt;under tools&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;ADE20K&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Method&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Backbone&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Crop Size&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;PQ&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AP&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;mIoU &lt;br&gt; (s.s)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;mIoU &lt;br&gt; (ms+flip)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;#params&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;config&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Checkpoint&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;OneFormer&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Swin-L&lt;sup&gt;†&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640×640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;219M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SHI-Labs/OneFormer/main/configs/ade20k/swin/oneformer_swin_large_bs16_160k.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://shi-labs.com/projects/oneformer/ade20k/250_16_swin_l_oneformer_ade20k_160k.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;OneFormer&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Swin-L&lt;sup&gt;†&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;896×896&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;58.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;219M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SHI-Labs/OneFormer/main/configs/ade20k/swin/oneformer_swin_large_bs16_160k_896x896.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://shi-labs.com/projects/oneformer/ade20k/896x896_250_16_swin_l_oneformer_ade20k_160k.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;OneFormer&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ConvNeXt-L&lt;sup&gt;†&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640×640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;56.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;220M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SHI-Labs/OneFormer/main/configs/ade20k/convnext/oneformer_convnext_large_bs16_160k.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://shi-labs.com/projects/oneformer/ade20k/250_16_convnext_l_oneformer_ade20k_160k.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;OneFormer&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;DiNAT-L&lt;sup&gt;†&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640×640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;58.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;223M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SHI-Labs/OneFormer/main/configs/ade20k/dinat/oneformer_dinat_large_bs16_160k.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://shi-labs.com/projects/oneformer/ade20k/250_16_dinat_l_oneformer_ade20k_160k.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;OneFormer&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;DiNAT-L&lt;sup&gt;†&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;896×896&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;58.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;58.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;223M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SHI-Labs/OneFormer/main/configs/ade20k/dinat/oneformer_dinat_large_bs16_160k_896x896.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://shi-labs.com/projects/oneformer/ade20k/896x896_250_16_dinat_l_oneformer_ade20k_160k.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;OneFormer&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ConvNeXt-XL&lt;sup&gt;†&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640×640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;58.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;372M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SHI-Labs/OneFormer/main/configs/ade20k/convnext/oneformer_convnext_xlarge_bs16_160k.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://shi-labs.com/projects/oneformer/ade20k/250_16_convnext_xl_oneformer_ade20k_160k.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Cityscapes&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Method&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Backbone&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;PQ&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AP&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;mIoU &lt;br&gt; (s.s)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;mIoU &lt;br&gt; (ms+flip)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;#params&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;config&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Checkpoint&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;OneFormer&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Swin-L&lt;sup&gt;†&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;67.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;45.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;219M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SHI-Labs/OneFormer/main/configs/cityscapes/swin/oneformer_swin_large_bs16_90k.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://shi-labs.com/projects/oneformer/cityscapes/250_16_swin_l_oneformer_cityscapes_90k.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;OneFormer&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ConvNeXt-L&lt;sup&gt;†&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;68.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;220M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SHI-Labs/OneFormer/main/configs/cityscapes/convnext/oneformer_convnext_large_bs16_90k.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://shi-labs.com/projects/oneformer/cityscapes/250_16_convnext_l_oneformer_cityscapes_90k.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;OneFormer&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;DiNAT-L&lt;sup&gt;†&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;67.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;45.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;223M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SHI-Labs/OneFormer/main/configs/cityscapes/dinat/oneformer_dinat_large_bs16_90k.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://shi-labs.com/projects/oneformer/cityscapes/250_16_dinat_l_oneformer_cityscapes_90k.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;OneFormer&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ConvNeXt-XL&lt;sup&gt;†&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;68.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;372M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SHI-Labs/OneFormer/main/configs/cityscapes/convnext/oneformer_convnext_xlarge_bs16_90k.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://shi-labs.com/projects/oneformer/cityscapes/250_16_convnext_xl_oneformer_cityscapes_90k.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;COCO&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Method&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Backbone&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;PQ&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;PQ&lt;sup&gt;Th&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;PQ&lt;sup&gt;St&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AP&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;mIoU&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;#params&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;config&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Checkpoint&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;OneFormer&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Swin-L&lt;sup&gt;†&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;67.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;219M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SHI-Labs/OneFormer/main/configs/coco/swin/oneformer_swin_large_bs16_100ep.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://shi-labs.com/projects/oneformer/coco/150_16_swin_l_oneformer_coco_100ep.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;OneFormer&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;DiNAT-L&lt;sup&gt;†&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;58.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;68.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;223M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SHI-Labs/OneFormer/main/configs/coco/dinat/oneformer_dinat_large_bs16_100ep.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://shi-labs.com/projects/oneformer/coco/150_16_dinat_l_oneformer_coco_100ep.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you found OneFormer useful in your research, please consider starring ⭐ us on GitHub and citing 📚 us in your research!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{jain2022oneformer,&#xA;      title={OneFormer: One Transformer to Rule Universal Image Segmentation},&#xA;      author={Jitesh Jain and Jiachen Li and MangTik Chiu and Ali Hassani and Nikita Orlov and Humphrey Shi},&#xA;      journal={arXiv}, &#xA;      year={2022}&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;We thank the authors of &lt;a href=&#34;https://github.com/facebookresearch/Mask2Former&#34;&gt;Mask2Former&lt;/a&gt;, &lt;a href=&#34;https://github.com/NVlabs/GroupViT&#34;&gt;GroupViT&lt;/a&gt;, and &lt;a href=&#34;https://github.com/SHI-Labs/Neighborhood-Attention-Transformer&#34;&gt;Neighborhood Attention Transformer&lt;/a&gt; for releasing their helpful codebases.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>probml/dynamax</title>
    <updated>2022-11-17T01:35:53Z</updated>
    <id>tag:github.com,2022-11-17:/probml/dynamax</id>
    <link href="https://github.com/probml/dynamax" rel="alternate"></link>
    <summary type="html">&lt;p&gt;State Space Models library in JAX&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Welcome to DYNAMAX!&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/probml/dynamax/main/logo/logo.gif&#34; alt=&#34;Logo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/probml/dynamax/actions/workflows/run_tests.yml/badge.svg?branch=main&#34; alt=&#34;Test Status&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Dynamax is a library for probabilistic state space models (SSMs) written in &lt;a href=&#34;https://github.com/google/jax&#34;&gt;JAX&lt;/a&gt;. It has code for inference (state estimation) and learning (parameter estimation) in a variety of SSMs, including:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Hidden Markov Models (HMMs)&lt;/li&gt; &#xA; &lt;li&gt;Linear Gaussian State Space Models (aka Linear Dynamical Systems)&lt;/li&gt; &#xA; &lt;li&gt;Nonlinear Gaussian State Space Models&lt;/li&gt; &#xA; &lt;li&gt;Generalized Gaussian State Space Models (with non-Gaussian emission models)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The library consists of a set of core, functionally pure, low-level inference algorithms, as well as a set of model classes which provide a more user-friendly, object-oriented interface. It is compatible with other libraries in the JAX ecosystem, such as &lt;a href=&#34;https://github.com/deepmind/optax&#34;&gt;optax&lt;/a&gt; (used for estimating parameters using stochastic gradient descent), and &lt;a href=&#34;https://github.com/blackjax-devs/blackjax&#34;&gt;Blackjax&lt;/a&gt; (used for computing the parameter posterior using Hamiltonian Monte Carlo (HMC) or sequential Monte Carlo (SMC)).&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;For tutorials and API documentation, see: &lt;a href=&#34;https://probml.github.io/dynamax/&#34;&gt;https://probml.github.io/dynamax/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation and Testing&lt;/h2&gt; &#xA;&lt;p&gt;To install the latest releast of dynamax from PyPi:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-{.console}&#34;&gt;pip install dynamax                 # Install dynamax and core dependencies, or&#xA;pip install dynamax[notebooks]      # Install with demo notebook dependencies&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To install the latest development branch:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-{.console}&#34;&gt;pip install git+https://github.com/probml/dynamax.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, if you&#39;re a developer, you can install dynamax along with the test and documentation dependencies with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-{.console}&#34;&gt;git clone git@github.com:probml/dynamax.git&#xA;cd dynamax&#xA;pip install -e &#39;.[dev]&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run the tests:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-{.console}&#34;&gt;pytest dynamax                         # Run all tests&#xA;pytest dynamax/hmm/inference_test.py   # Run a specific test&#xA;pytest -k lgssm                        # Run tests with lgssm in the name&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;What are state space models?&lt;/h2&gt; &#xA;&lt;p&gt;A state space model or SSM is a partially observed Markov model, in which the hidden state, $z_t$, evolves over time according to a Markov process, possibly conditional on external inputs / controls / covariates, $u_t$, and generates an observation, $y_t$. This is illustrated in the graphical model below.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/probml/dynamax/main/docs/figures/LDS-UZY.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;The corresponding joint distribution has the following form (in dynamax, we restrict attention to discrete time systems):&lt;/p&gt; &#xA;&lt;p&gt;$$p(y_{1:T}, z_{1:T} | u_{1:T}) = p(z_1 | u_1) p(y_1 | z_1, u_1) \prod_{t=1}^T p(z_t | z_{t-1}, u_t) p(y_t | z_t, u_t)$$&lt;/p&gt; &#xA;&lt;p&gt;Here $p(z_t | z_{t-1}, u_t)$ is called the transition or dynamics model, and $p(y_t | z_{t}, u_t)$ is called the observation or emission model. In both cases, the inputs $u_t$ are optional; furthermore, the observation model may have auto-regressive dependencies, in which case we write $p(y_t | z_{t}, u_t, y_{1:t-1})$.&lt;/p&gt; &#xA;&lt;p&gt;We assume that we see the observations $y_{1:T}$, and want to infer the hidden states, either using online filtering (i.e., computing $p(z_t|y_{1:t})$ ) or offline smoothing (i.e., computing $p(z_t|y_{1:T})$ ). We may also be interested in predicting future states, $p(z_{t+h}|y_{1:t})$, or future observations, $p(y_{t+h}|y_{1:t})$, where h is the forecast horizon. (Note that by using a hidden state to represent the past observations, the model can have &#34;infinite&#34; memory, unlike a standard auto-regressive model.) All of these computations can be done efficiently using our library, as we discuss below. In addition, we can estimate the parameters of the transition and emission models, as we discuss below.&lt;/p&gt; &#xA;&lt;p&gt;More information can be found in these books:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&#34;Machine Learning: Advanced Topics&#34;, K. Murphy, MIT Press 2023. Available at &lt;a href=&#34;https://probml.github.io/pml-book/book2.html&#34;&gt;https://probml.github.io/pml-book/book2.html&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;&#34;Bayesian Filtering and Smoothing&#34;, S. Särkkä, Cambridge University Press, 2013. Available at &lt;a href=&#34;https://users.aalto.fi/~ssarkka/pub/cup_book_online_20131111.pdf&#34;&gt;https://users.aalto.fi/~ssarkka/pub/cup_book_online_20131111.pdf&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Example usage&lt;/h2&gt; &#xA;&lt;p&gt;Dynamax includes classes for many kinds of SSM. You can use these models to simulate data, and you can fit the models using standard learning algorithms like expectation-maximization (EM) and stochastic gradient descent (SGD). Below we illustrate the high level (object-oriented) API for the case of an HMM with Gaussian emissions. (See &lt;a href=&#34;https://github.com/probml/dynamax/raw/main/docs/notebooks/hmm/gaussian_hmm.ipynb&#34;&gt;this notebook&lt;/a&gt; for a runnable version of this code.)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import jax.numpy as jnp&#xA;import jax.random as jr&#xA;import matplotlib.pyplot as plt&#xA;from dynamax.hidden_markov_model import GaussianHMM&#xA;&#xA;key1, key2, key3 = jr.split(jr.PRNGKey(0), 3)&#xA;num_states = 3&#xA;emission_dim = 2&#xA;num_timesteps = 1000&#xA;&#xA;# Make a Gaussian HMM and sample data from it&#xA;hmm = GaussianHMM(num_states, emission_dim)&#xA;true_params, _ = hmm.initialize(key1)&#xA;true_states, emissions = hmm.sample(true_params, key2, num_timesteps)&#xA;&#xA;# Make a new Gaussian HMM and fit it with EM&#xA;params, props = hmm.initialize(key3, method=&#34;kmeans&#34;, emissions=emissions)&#xA;params, lls = hmm.fit_em(params, props, emissions, num_iters=20)&#xA;&#xA;# Plot the marginal log probs across EM iterations&#xA;plt.plot(lls)&#xA;plt.xlabel(&#34;EM iterations&#34;)&#xA;plt.ylabel(&#34;marginal log prob.&#34;)&#xA;&#xA;# Use fitted model for posterior inference&#xA;post = hmm.smoother(params, emissions)&#xA;print(post.smoothed_probs.shape) # (1000, 3)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;JAX allows you to easily vectorize these operations with &lt;code&gt;vmap&lt;/code&gt;. For example, you can sample and fit to a batch of emissions as shown below.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from functools import partial&#xA;from jax import vmap&#xA;&#xA;num_seq = 200&#xA;batch_true_states, batch_emissions = \&#xA;    vmap(partial(hmm.sample, true_params, num_timesteps=num_timesteps))(&#xA;        jr.split(key2, num_seq))&#xA;print(batch_true_states.shape, batch_emissions.shape) # (200,1000) and (200,1000,2)&#xA;&#xA;# Make a new Gaussian HMM and fit it with EM&#xA;params, props = hmm.initialize(key3, method=&#34;kmeans&#34;, emissions=batch_emissions)&#xA;params, lls = hmm.fit_em(params, props, batch_emissions, num_iters=20)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;These examples demonstrate the dynamax models, but we can also call the low-level inference code directly.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://github.com/probml/dynamax/raw/main/CONTRIBUTING.md&#34;&gt;this page&lt;/a&gt; for details on how to contribute.&lt;/p&gt; &#xA;&lt;h2&gt;About&lt;/h2&gt; &#xA;&lt;p&gt;Core team: Peter Chang, Giles Harper-Donnelly, Aleyna Kara, Xinglong Li, Scott Linderman, Kevin Murphy.&lt;/p&gt; &#xA;&lt;p&gt;Other contributors: Adrien Corenflos, Elizabeth DuPre, Gerardo Duran-Martin, Colin Schlager, Libby Zhang and other people &lt;a href=&#34;https://github.com/probml/dynamax/graphs/contributors&#34;&gt;listed here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;MIT License. 2022&lt;/p&gt;</summary>
  </entry>
</feed>