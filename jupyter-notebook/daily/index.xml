<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-04-29T01:32:56Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>gpu-mode/lectures</title>
    <updated>2025-04-29T01:32:56Z</updated>
    <id>tag:github.com,2025-04-29:/gpu-mode/lectures</id>
    <link href="https://github.com/gpu-mode/lectures" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Material for gpu-mode lectures&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Supplementary Material for Lectures&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/gpumode&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/gpumode?style=flat&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/@GPUMODE&#34;&gt;YouTube Channel&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The PMPP Book: &lt;a href=&#34;https://a.co/d/2S2fVzt&#34;&gt;Programming Massively Parallel Processors: A Hands-on Approach&lt;/a&gt; (Amazon link)&lt;/p&gt; &#xA;&lt;h2&gt;Lecture 1: Profiling and Integrating CUDA kernels in PyTorch&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaker: &lt;a href=&#34;https://twitter.com/marksaroufim&#34;&gt;Mark Saroufim&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Notebook and slides in &lt;a href=&#34;https://raw.githubusercontent.com/gpu-mode/lectures/main/lecture_001/&#34;&gt;lecture_001&lt;/a&gt; folder&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Lecture 2: Recap Ch. 1-3 from the PMPP book&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaker: &lt;a href=&#34;https://twitter.com/neurosp1ke&#34;&gt;Andreas Koepf&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Slides: The powerpoint file &lt;a href=&#34;https://raw.githubusercontent.com/gpu-mode/lectures/main/lecture_002/cuda_mode_lecture2.pptx&#34;&gt;lecture_002/cuda_mode_lecture2.pptx&lt;/a&gt; can be found in the root directory of this repository. Alternatively &lt;a href=&#34;https://docs.google.com/presentation/d/1deqvEHdqEC4LHUpStO6z3TT77Dt84fNAvTIAxBJgDck/edit#slide=id.g2b1444253e5_1_75&#34;&gt;here&lt;/a&gt; as Google docs presentation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Lecture 3: Getting Started With CUDA&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaker: &lt;a href=&#34;https://twitter.com/jeremyphoward&#34;&gt;Jeremy Howard&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Notebook: See the &lt;a href=&#34;https://raw.githubusercontent.com/gpu-mode/lectures/main/lecture_003/&#34;&gt;lecture_003&lt;/a&gt; folder, or run the &lt;a href=&#34;https://colab.research.google.com/drive/180uk6frvMBeT4tywhhYXmz3PJaCIA_uk?usp=sharing&#34;&gt;Colab version&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Lecture 4: Intro to Compute and Memory Architecture&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaker: &lt;a href=&#34;https://lernapparat.de/&#34;&gt;Thomas Viehmann&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Notebook and slides in the &lt;a href=&#34;https://raw.githubusercontent.com/gpu-mode/lectures/main/lecture_004/&#34;&gt;lecture_004&lt;/a&gt; folder.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Lecture 5: Going Further with CUDA for Python Programmers&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaker: &lt;a href=&#34;https://twitter.com/jeremyphoward&#34;&gt;Jeremy Howard&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Notebook in the &lt;a href=&#34;https://raw.githubusercontent.com/gpu-mode/lectures/main/lecture_005/&#34;&gt;lecture_005&lt;/a&gt; folder.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Lecture 6: Optimizing PyTorch Optimizers&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaker: &lt;a href=&#34;https://github.com/janeyx99&#34;&gt;Jane Xu&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/13WLCuxXzwu5JRZo0tAfW0hbKHQMvFw4O/edit#slide=id.p1&#34;&gt;Slides&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Lecture 7: Advanced Quantization&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaker: &lt;a href=&#34;https://github.com/HDCharles&#34;&gt;Charles Hernandez&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.dropbox.com/scl/fi/hzfx1l267m8gwyhcjvfk4/Quantization-Cuda-vs-Triton.pdf?rlkey=s4j64ivi2kpp2l0uq8xjdwbab&amp;amp;dl=0&#34;&gt;Slides&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Lecture 8: CUDA Performance Checklist&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaker: &lt;a href=&#34;https://github.com/msaroufim&#34;&gt;Mark Saroufim&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Code in the &lt;a href=&#34;https://raw.githubusercontent.com/gpu-mode/lectures/main/lecture_008/&#34;&gt;lecture_008&lt;/a&gt; folder&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/1cvVpf3ChFFiY4Kf25S4e4sPY6Y5uRUO-X-A4nJ7IhFE/edit?usp=sharing&#34;&gt;Slides&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Lecture 9: Reductions&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaker: &lt;a href=&#34;https://github.com/msaroufim&#34;&gt;Mark Saroufim&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Code in the &lt;a href=&#34;https://raw.githubusercontent.com/gpu-mode/lectures/main/lecture_009/&#34;&gt;lecture_009&lt;/a&gt; folder&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/1s8lRU8xuDn-R05p1aSP6P7T5kk9VYnDOCyN5bWKeg3U/edit?usp=drive_link&#34;&gt;Slides&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Lecture 10: Build a Prod Ready CUDA Library&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaker: &lt;a href=&#34;https://github.com/morousg&#34;&gt;Oscar Amoros Huguet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/158V8BzGj-IkdXXDAdHPNwUzDLNmr971_?usp=drive_link&#34;&gt;slides&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Lecture 11: Sparsity&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaker: &lt;a href=&#34;https://github.com/jcaip&#34;&gt;Jesse Cai&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/gpu-mode/lectures/main/lecture_011/sparsity.pptx&#34;&gt;Slides&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Lecture 12: Flash Attention&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaker: &lt;a href=&#34;https://lernapparat.de/&#34;&gt;Thomas Viehmann&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Lecture 13: Ring Attention&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaker: &lt;a href=&#34;https://twitter.com/neurosp1ke&#34;&gt;Andreas Koepf&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/gpu-mode/lectures/main/lecture_013/ring_attention.pptx&#34;&gt;Slides&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Lecture 14: Practitioner&#39;s Guide to Triton&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Date: 2024-04-13, Speaker: &lt;a href=&#34;https://twitter.com/UmerHAdil&#34;&gt;Umer Adil&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/gpu-mode/lectures/main/lecture_014/A_Practitioners_Guide_to_Triton.ipynb&#34;&gt;Notebook&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Lecture 15: CUTLASS&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaker: &lt;a href=&#34;https://github.com/ericauld&#34;&gt;Eric Auld&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Lecture 16: On Hands profiling&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaker: &lt;a href=&#34;https://www.linkedin.com/in/taylor-robie/&#34;&gt;Taylor Robbie&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Bonus Lecture: CUDA C++ llm.cpp&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaker: &lt;a href=&#34;&#34;&gt;Jake Hemstad &amp;amp; Georgii Evtushenko&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/1T-t0d_u0Xu8w_-1E5kAwmXNfF72x-HTA&#34;&gt;Slides&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Lecture 17: GPU Collective Communication (NCCL)&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaker: &lt;a href=&#34;https://physbam.stanford.edu/~dansj/&#34;&gt;Dan Johnson&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Code in the &lt;a href=&#34;https://raw.githubusercontent.com/gpu-mode/lectures/main/lecture_017/&#34;&gt;lecture_017&lt;/a&gt; folder&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Lecture 18: Fused Kernels&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaker: &lt;a href=&#34;https://www.kapilsharma.dev/&#34;&gt;Kapil Sharma&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Code in the &lt;a href=&#34;https://raw.githubusercontent.com/gpu-mode/lectures/main/lecture_018/&#34;&gt;lecture_018&lt;/a&gt; folder&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Lecture 19: Data Processing on GPUs&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaker: &lt;a href=&#34;https://github.com/devavret&#34;&gt;Devavret Makkar&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Lecture 20: Scan Algorithm&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaker: &lt;a href=&#34;https://ielhajj.github.io/&#34;&gt;Izzat El Haj&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/1MEMsE5LKi6ush_60hlYu3-cz4DUCFzSL/edit?usp=sharing&amp;amp;ouid=106222972308395582904&amp;amp;rtpof=true&amp;amp;sd=true&#34;&gt;Slides&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Lecture 21: Scan Algorithm Part 2&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaker: &lt;a href=&#34;https://ielhajj.github.io/&#34;&gt;Izzat El Haj&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/1MEMsE5LKi6ush_60hlYu3-cz4DUCFzSL/edit?usp=sharing&amp;amp;ouid=106222972308395582904&amp;amp;rtpof=true&amp;amp;sd=true&#34;&gt;Slides&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Lecture 22: Hacker&#39;s Guide to Speculative Decoding in VLLM&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaker: &lt;a href=&#34;https://x.com/cdnamz&#34;&gt;Cade Daniel&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/1p1xE-EbSAnXpTSiSI0gmy_wdwxN5XaULO3AnCWWoRe4/edit#slide=id.p&#34;&gt;Slides&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Lecture 23: Tensor Cores&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaker: Vijay Thakkar &amp;amp; Pradeep Ramani&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://drive.google.com/file/d/18sthk6IUOKbdtFphpm_jZNXoJenbWR8m/view&#34;&gt;Slides&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Lecture 24: Scan at the Speed of Light&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaker: Jake Hemstad &amp;amp; Georgii Evtushenko&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Lecture 25: Speaking Composable Kernel&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaker: Haocong Wang&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/gpu-mode/lectures/main/lecture_025/AMD_ROCm_Speaking_Composable_Kernel_July_20_2024.pdf&#34;&gt;Slides&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Lecture 26: SYCL MODE (Intel GPU)&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaker: Patric Zhao&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/1SW4XKomAJhhJSH5-jpZI9Qlwp7TEunbV/edit?usp=sharing&amp;amp;ouid=106222972308395582904&amp;amp;rtpof=true&amp;amp;sd=true&#34;&gt;Slides&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Lecture 27: gpu.cpp&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaker: &lt;a href=&#34;https://x.com/austinvhuang&#34;&gt;Austin Huang&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gpucpp-presentation.answer.ai/&#34;&gt;Slides&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Lecture 28: Liger Kernel&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaker: &lt;a href=&#34;https://x.com/hsu_byron&#34;&gt;Byron Hsu&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/1CGTV-uKw9crrBo13q1jAzAFCFzlpZFjeL4bnK67pTd8/edit?usp=sharing&#34;&gt;Slides&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Hands-on Notebooks &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1CQYhul7MVG5F0gmqTBbx1O1HgolPgF0M?usp=sharing&#34;&gt;RMSNorm: Verifying Correctness and Performance&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1Z2QtvaIiLm5MWOs7X6ZPS1MN3hcIJFbj?usp=sharing&#34;&gt;FusedLinearCrossEntropy: Verifying Memory Reduction&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1e52FH0BcE739GZaVp-3_Dv7mc4jF1aif?usp=sharing&#34;&gt;Convergence Comparison: Triton Kernel Patched vs. Original Model Layer-by-Layer&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1llnAdo0hc9FpxYRRnjih0l066NCp7Ylu?usp=sharing&#34;&gt;Contiguity is the hidden killer&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1WgaU_cmaxVzx8PcdKB5P9yHB6_WyGd4T?usp=sharing&#34;&gt;Address int32 overflow&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Lecture 29: Triton Internals&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaker: &lt;a href=&#34;https://www.kapilsharma.dev/&#34;&gt;Kapil Sharma&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Code/presentation in the &lt;a href=&#34;https://raw.githubusercontent.com/gpu-mode/lectures/main/lecture_029/&#34;&gt;lecture_029&lt;/a&gt; folder&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Lecture 30: Quantized training&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaker: &lt;a href=&#34;https://github.com/gau-nernst&#34;&gt;Thien Tran&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Code/presentation in the &lt;a href=&#34;https://raw.githubusercontent.com/gpu-mode/lectures/main/lecture_030/&#34;&gt;lecture_030&lt;/a&gt; folder&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Lecture 31: Beginners Guide to Metal Kernels&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaker: &lt;a href=&#34;https://github.com/gau-nernst&#34;&gt;Nikita Shulga&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Code/presentation in the &lt;a href=&#34;https://raw.githubusercontent.com/gpu-mode/lectures/main/lecture_031/&#34;&gt;lecture_031&lt;/a&gt; folder&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Lecture 32: Unsloth - LLM Systems Engineering&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaker: &lt;a href=&#34;https://x.com/danielhanchen&#34;&gt;Daniel Han&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/1BvgbDwvOY6Uy6jMuNXrmrz_6Km_CBW0f2espqeQaWfc/edit?usp=sharing&#34;&gt;Slides&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Lecture 33: BitBLAS&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaker: &lt;a href=&#34;https://github.com/LeiWang1999&#34;&gt;Wang Lei&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Code/presentation in the &lt;a href=&#34;https://raw.githubusercontent.com/gpu-mode/lectures/main/lecture_033/&#34;&gt;lecture_033&lt;/a&gt; folder&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Lecture 34: Low Bit Triton Kernels&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaker: &lt;a href=&#34;https://github.com/mobicham&#34;&gt;Hicham Badri&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/1R9B6RLOlAblyVVFPk9FtAq6MXR1ufj1NaT0bjjib7Vc/edit&#34;&gt;Slides&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Lecture 35: SGLang Performance Optimization&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaker: &lt;a href=&#34;https://linkedin.com/in/zhyncs&#34;&gt;Yineng Zhang&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zhyncs/lectures/raw/main/lecture_035/SGLang-Performance-Optimization-YinengZhang.pdf&#34;&gt;Slides&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Lecture 36: CUTLASS and Flash ATtention 3&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaker: &lt;a href=&#34;https://research.colfax-intl.com/blog/&#34;&gt;Jay Shah&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/gpu-mode/lectures/main/lecture_036/&#34;&gt;Slides&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Lecture 37: Introduction to SASS &amp;amp; GPU Microarchitecture&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaker: &lt;a href=&#34;https://github.com/ademeure&#34;&gt;Arun Demeure&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/gpu-mode/lectures/main/lecture_037/&#34;&gt;Slides&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Lecture 38: Lowbit kernels for ARM CPU&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaker: &lt;a href=&#34;https://github.com/metascroy&#34;&gt;Scott Roy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/gpu-mode/lectures/main/lecture_038/&#34;&gt;Slides&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Lecture 39: TorchTitan&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaker: Mark Saroufim and Tianyu Liu&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Lecture 40: Flash Infer&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaker: &lt;a href=&#34;https://homes.cs.washington.edu/~zhye/&#34;&gt;Zihao Ye&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Lecture 41: CUDA Docs for Humans&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaker: &lt;a href=&#34;https://x.com/charles_irl/status/1867306225706447023&#34;&gt;Charles Frye&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/15lTG6aqf72Hyk5_lqH7iSrc8aP1ElEYxCxch-tD37PE/edit#slide=id.g326210b960f_0_42&#34;&gt;Slides&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Lecture 42: Mosaic GPU&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaker: &lt;a href=&#34;https://x.com/apaszke&#34;&gt;Adam Paszke&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Lecture 43:&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaker: Erik Schultheis&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/gpu-mode/lectures/main/lecture_042&#34;&gt;Slides&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/audiocraft</title>
    <updated>2025-04-29T01:32:56Z</updated>
    <id>tag:github.com,2025-04-29:/facebookresearch/audiocraft</id>
    <link href="https://github.com/facebookresearch/audiocraft" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Audiocraft is a library for audio processing and generation with deep learning. It features the state-of-the-art EnCodec audio compressor / tokenizer, along with MusicGen, a simple and controllable music generation LM with textual and melodic conditioning.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AudioCraft&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/facebookresearch/audiocraft/workflows/audiocraft_docs/badge.svg?sanitize=true&#34; alt=&#34;docs badge&#34;&gt; &lt;img src=&#34;https://github.com/facebookresearch/audiocraft/workflows/audiocraft_linter/badge.svg?sanitize=true&#34; alt=&#34;linter badge&#34;&gt; &lt;img src=&#34;https://github.com/facebookresearch/audiocraft/workflows/audiocraft_tests/badge.svg?sanitize=true&#34; alt=&#34;tests badge&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;AudioCraft is a PyTorch library for deep learning research on audio generation. AudioCraft contains inference and training code for two state-of-the-art AI generative models producing high-quality audio: AudioGen and MusicGen.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;AudioCraft requires Python 3.9, PyTorch 2.1.0. To install AudioCraft, you can run the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Best to make sure you have torch installed first, in particular before installing xformers.&#xA;# Don&#39;t run this if you already have PyTorch installed.&#xA;python -m pip install &#39;torch==2.1.0&#39;&#xA;# You might need the following before trying to install the packages&#xA;python -m pip install setuptools wheel&#xA;# Then proceed to one of the following&#xA;python -m pip install -U audiocraft  # stable release&#xA;python -m pip install -U git+https://git@github.com/facebookresearch/audiocraft#egg=audiocraft  # bleeding edge&#xA;python -m pip install -e .  # or if you cloned the repo locally (mandatory if you want to train).&#xA;python -m pip install -e &#39;.[wm]&#39;  # if you want to train a watermarking model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We also recommend having &lt;code&gt;ffmpeg&lt;/code&gt; installed, either through your system or Anaconda:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt-get install ffmpeg&#xA;# Or if you are using Anaconda or Miniconda&#xA;conda install &#34;ffmpeg&amp;lt;5&#34; -c conda-forge&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Models&lt;/h2&gt; &#xA;&lt;p&gt;At the moment, AudioCraft contains the training code and inference code for:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/docs/MUSICGEN.md&#34;&gt;MusicGen&lt;/a&gt;: A state-of-the-art controllable text-to-music model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/docs/AUDIOGEN.md&#34;&gt;AudioGen&lt;/a&gt;: A state-of-the-art text-to-sound model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/docs/ENCODEC.md&#34;&gt;EnCodec&lt;/a&gt;: A state-of-the-art high fidelity neural audio codec.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/docs/MBD.md&#34;&gt;Multi Band Diffusion&lt;/a&gt;: An EnCodec compatible decoder using diffusion.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/docs/MAGNET.md&#34;&gt;MAGNeT&lt;/a&gt;: A state-of-the-art non-autoregressive model for text-to-music and text-to-sound.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/docs/WATERMARKING.md&#34;&gt;AudioSeal&lt;/a&gt;: A state-of-the-art audio watermarking.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/docs/MUSICGEN_STYLE.md&#34;&gt;MusicGen Style&lt;/a&gt;: A state-of-the-art text-and-style-to-music model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/docs/JASCO.md&#34;&gt;JASCO&lt;/a&gt;: &#34;High quality text-to-music model conditioned on chords, melodies and drum tracks&#34;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Training code&lt;/h2&gt; &#xA;&lt;p&gt;AudioCraft contains PyTorch components for deep learning research in audio and training pipelines for the developed models. For a general introduction of AudioCraft design principles and instructions to develop your own training pipeline, refer to the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/docs/TRAINING.md&#34;&gt;AudioCraft training documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For reproducing existing work and using the developed training pipelines, refer to the instructions for each specific model that provides pointers to configuration, example grids and model/task-specific information and FAQ.&lt;/p&gt; &#xA;&lt;h2&gt;API documentation&lt;/h2&gt; &#xA;&lt;p&gt;We provide some &lt;a href=&#34;https://facebookresearch.github.io/audiocraft/api_docs/audiocraft/index.html&#34;&gt;API documentation&lt;/a&gt; for AudioCraft.&lt;/p&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;h4&gt;Is the training code available?&lt;/h4&gt; &#xA;&lt;p&gt;Yes! We provide the training code for &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/docs/ENCODEC.md&#34;&gt;EnCodec&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/docs/MUSICGEN.md&#34;&gt;MusicGen&lt;/a&gt;,&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/docs/MBD.md&#34;&gt;Multi Band Diffusion&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/docs/JASCO.md&#34;&gt;JASCO&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Where are the models stored?&lt;/h4&gt; &#xA;&lt;p&gt;Hugging Face stored the model in a specific location, which can be overridden by setting the &lt;code&gt;AUDIOCRAFT_CACHE_DIR&lt;/code&gt; environment variable for the AudioCraft models. In order to change the cache location of the other Hugging Face models, please check out the &lt;a href=&#34;https://huggingface.co/docs/transformers/installation#cache-setup&#34;&gt;Hugging Face Transformers documentation for the cache setup&lt;/a&gt;. Finally, if you use a model that relies on Demucs (e.g. &lt;code&gt;musicgen-melody&lt;/code&gt;) and want to change the download location for Demucs, refer to the &lt;a href=&#34;https://pytorch.org/docs/stable/hub.html#where-are-my-downloaded-models-saved&#34;&gt;Torch Hub documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The code in this repository is released under the MIT license as found in the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/LICENSE&#34;&gt;LICENSE file&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The models weights in this repository are released under the CC-BY-NC 4.0 license as found in the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/LICENSE_weights&#34;&gt;LICENSE_weights file&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;For the general framework of AudioCraft, please cite the following.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{copet2023simple,&#xA;    title={Simple and Controllable Music Generation},&#xA;    author={Jade Copet and Felix Kreuk and Itai Gat and Tal Remez and David Kant and Gabriel Synnaeve and Yossi Adi and Alexandre Défossez},&#xA;    booktitle={Thirty-seventh Conference on Neural Information Processing Systems},&#xA;    year={2023},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When referring to a specific model, please cite as mentioned in the model specific README, e.g &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/docs/MUSICGEN.md&#34;&gt;./docs/MUSICGEN.md&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/docs/AUDIOGEN.md&#34;&gt;./docs/AUDIOGEN.md&lt;/a&gt;, etc.&lt;/p&gt;</summary>
  </entry>
</feed>