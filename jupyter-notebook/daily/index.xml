<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-10-09T01:32:40Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>FasterDecoding/Medusa</title>
    <updated>2023-10-09T01:32:40Z</updated>
    <id>tag:github.com,2023-10-09:/FasterDecoding/Medusa</id>
    <link href="https://github.com/FasterDecoding/Medusa" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Medusa: Simple Framework for Accelerating LLM Generation with Multiple Decoding Heads&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/FasterDecoding/Medusa/main/assets/logo.png&#34; alt=&#34;Medusa&#34; width=&#34;100&#34; align=&#34;left&#34;&gt;&lt;/p&gt;&#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;h1&gt;&amp;nbsp;Medusa: Simple Framework for Accelerating LLM Generation with Multiple Decoding Heads&lt;/h1&gt;&#xA;&lt;/div&gt;&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; | &lt;a href=&#34;https://sites.google.com/view/&#xA;medusa-llm&#34;&gt;&lt;b&gt;Blog&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/FasterDecoding/Medusa/main/ROADMAP.md&#34;&gt;&lt;b&gt;Roadmap&lt;/b&gt;&lt;/a&gt; | &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;em&gt;News&lt;/em&gt; ðŸ”¥&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2023/09] Medusa won the &lt;a href=&#34;https://twitter.com/tianle_cai/status/1703891335147897341&#34;&gt;Chai Prize Grant&lt;/a&gt;ðŸŽ‰ The prize will be used as a development bounty for those who help us achieve milestones in our &lt;a href=&#34;https://github.com/FasterDecoding/Medusa/issues/3&#34;&gt;roadmap&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;[2023/09] Medusa v0.1 is released!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;Medusa is a simple framework that democratizes the acceleration techniques for LLM generation with multiple decoding heads.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/FasterDecoding/Medusa/main/assets/medusa_demo.gif&#34; width=&#34;80%&#34;&gt; &#xA; &lt;/picture&gt; &#xA; &lt;br&gt; &#xA; &lt;div align=&#34;center&#34; width=&#34;80%&#34;&gt; &#xA;  &lt;em&gt;Medusa on Vicuna-7b.&lt;/em&gt; &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;We aim to tackle the three pain points of popular acceleration techniques like speculative decoding:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Requirement of a good draft model.&lt;/li&gt; &#xA; &lt;li&gt;System complexity.&lt;/li&gt; &#xA; &lt;li&gt;Inefficiency when using sampling-based generation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/FasterDecoding/Medusa/main/assets/medusa_pipeline.jpg&#34; width=&#34;60%&#34;&gt; &#xA; &lt;/picture&gt; &#xA; &lt;br&gt; &#xA; &lt;div align=&#34;left&#34; width=&#34;80%&#34;&gt; &#xA;  &lt;em&gt;Medusa adds extra &#34;heads&#34; to LLMs to predict multiple future tokens simultaneously. When augmenting a model with Medusa, the original model stays untouched, and only the new heads are fine-tuned during training. During generation, these heads each produce multiple likely words for the corresponding position. These options are then combined and processed using a tree-based attention mechanism. Finally, a typical acceptance scheme is employed to pick the longest plausible prefix from the candidates for further decoding.&lt;/em&gt; &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;We aim to solve the challenges associated with speculative decoding by implementing the following ideas:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Instead of introducing a new model, we train multiple decoding heads on the &lt;em&gt;same&lt;/em&gt; model.&lt;/li&gt; &#xA; &lt;li&gt;The training is parameter-efficient so that even the &#34;GPU-Poor&#34; can do it. And since there is no additional model, there is no need to adjust the distributed computing setup.&lt;/li&gt; &#xA; &lt;li&gt;Relaxing the requirement of matching the distribution of the original model makes the non-greedy generation even faster than greedy decoding.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/FasterDecoding/Medusa/main/assets/size_speedup.png&#34; width=&#34;45%&#34;&gt; &#xA; &lt;/picture&gt; &lt;/p&gt; In this initial release, our primary focus is on optimizing Medusa for a batch size of 1â€”a setting commonly utilized for local model hosting. In this configuration, Medusa delivers approximately a 2x speed increase across a range of Vicuna models. We are actively working to extend Medusa&#39;s capabilities by integrating it into additional inference frameworks, with the aim of achieving even greater performance gains and extending Medusa to broader settings. &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FasterDecoding/Medusa/main/#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FasterDecoding/Medusa/main/#contents&#34;&gt;Contents&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FasterDecoding/Medusa/main/#installation&#34;&gt;Installation&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FasterDecoding/Medusa/main/#method-1-with-pip&#34;&gt;Method 1: With pip&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FasterDecoding/Medusa/main/#method-2-from-source&#34;&gt;Method 2: From source&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FasterDecoding/Medusa/main/#model-weights&#34;&gt;Model Weights&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FasterDecoding/Medusa/main/#inference&#34;&gt;Inference&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FasterDecoding/Medusa/main/#training&#34;&gt;Training&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FasterDecoding/Medusa/main/#prepare-the-data&#34;&gt;Prepare the data&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FasterDecoding/Medusa/main/#train-the-model&#34;&gt;Train the model&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FasterDecoding/Medusa/main/#push-to-hugging-face-hub&#34;&gt;Push to Hugging Face Hub&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FasterDecoding/Medusa/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FasterDecoding/Medusa/main/#codebase-guide&#34;&gt;Codebase Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FasterDecoding/Medusa/main/#contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FasterDecoding/Medusa/main/#acknowledgements&#34;&gt;Acknowledgements&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Method 1: With pip&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install medusa-llm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Method 2: From the source&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/FasterDecoding/Medusa.git&#xA;cd Medusa&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Model Weights&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Size&lt;/th&gt; &#xA;   &lt;th&gt;Chat Command&lt;/th&gt; &#xA;   &lt;th&gt;Hugging Face Repo&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;python -m medusa.inference.cli --model FasterDecoding/medusa-vicuna-7b-v1.3&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/FasterDecoding/medusa-vicuna-7b-v1.3&#34;&gt;FasterDecoding/medusa-vicuna-7b-v1.3&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;python -m medusa.inference.cli --model FasterDecoding/medusa-vicuna-13b-v1.3&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/FasterDecoding/medusa-vicuna-13b-v1.3&#34;&gt;FasterDecoding/medusa-vicuna-13b-v1.3&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;33B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;python -m medusa.inference.cli --model FasterDecoding/medusa-vicuna-33b-v1.3&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/FasterDecoding/medusa-vicuna-33b-v1.3&#34;&gt;FasterDecoding/medusa-vicuna-33b-v1.3&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Inference&lt;/h3&gt; &#xA;&lt;p&gt;We currently support single-GPU inference with a batch size of 1, which is the most common setup for local model hosting. We are actively working to extend Medusa&#39;s capabilities by integrating it into other inference frameworks; please don&#39;t hesitate to reach out if you are interested in contributing to this effort.&lt;/p&gt; &#xA;&lt;p&gt;You can use the following command to launch a CLI interface:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_VISIBLE_DEVICES=0 python -m medusa.inference.cli --model [path of medusa model]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also pass &lt;code&gt;--load-in-8bit&lt;/code&gt; or &lt;code&gt;--load-in-4bit&lt;/code&gt; to load the base model in quantized format. If you download the base model elsewhere, you may override base model name or path with &lt;code&gt;--base-model [path of base model]&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;p&gt;For training, please install:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e &#34;.[train]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Prepare the data&lt;/h4&gt; &#xA;&lt;p&gt;We take a public version of the ShareGPT dataset, which is a subset of the Vicuna training data. For other models, you can use the corresponding training dataset.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://huggingface.co/datasets/Aeala/ShareGPT_Vicuna_unfiltered&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Remark: If you haven&#39;t installed &lt;code&gt;git-lfs&lt;/code&gt;, please install it before cloning:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git lfs install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Train the model&lt;/h4&gt; &#xA;&lt;p&gt;We follow the training setup from &lt;a href=&#34;https://github.com/lm-sys/FastChat#fine-tuning&#34;&gt;FastChat&lt;/a&gt;, but with a much larger learning rate because we freeze the original model and only train the new heads. Here is the training command for the Vicuna-7b model on 4 GPUs. Since we are only training the new heads, the training does not require a lot of memory, and only data parallelism is needed. You can modify the script to fit your own setup. For larger models, we use the same setup. You can also use &lt;code&gt;--load_in_8bit&lt;/code&gt; or &lt;code&gt;--load_in_4bit&lt;/code&gt; to load the base model in quantized format.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;torchrun --nproc_per_node=4 medusa/train/train.py --model_name_or_path lmsys/vicuna-7b-v1.3 \&#xA;    --data_path ShareGPT_Vicuna_unfiltered/ShareGPT_V4.3_unfiltered_cleaned_split.json \&#xA;    --bf16 True \&#xA;    --output_dir test \&#xA;    --num_train_epochs 1 \&#xA;    --per_device_train_batch_size 8 \&#xA;    --per_device_eval_batch_size 8 \&#xA;    --gradient_accumulation_steps 4 \&#xA;    --evaluation_strategy &#34;no&#34; \&#xA;    --save_strategy &#34;no&#34; \&#xA;    --learning_rate 1e-3 \&#xA;    --weight_decay 0.0 \&#xA;    --warmup_ratio 0.1 \&#xA;    --lr_scheduler_type &#34;cosine&#34; \&#xA;    --logging_steps 1 \&#xA;    --tf32 True \&#xA;    --model_max_length 2048 \&#xA;    --lazy_preprocess True \&#xA;    --medusa_num_heads 3 \&#xA;    --medusa_num_layers 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Push to Hugging Face Hub&lt;/h4&gt; &#xA;&lt;p&gt;You can use the following command to push your model to the Hugging Face Hub:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m medusa.hf_utils --folder [path of the model folder] --repo [name of the repo]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{medusa,&#xA;  author = {Tianle Cai and Yuhong Li and Zhengyang Geng and Hongwu Peng and Tri Dao},&#xA;  title = {Medusa: Simple Framework for Accelerating LLM Generation with Multiple Decoding Heads},&#xA;  year = {2023},&#xA;  publisher = {GitHub},&#xA;  journal = {GitHub repository},&#xA;  howpublished = {\url{https://github.com/FasterDecoding/Medusa}},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Codebase Guide&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;medusa/model/medusa_model.py&lt;/code&gt; is the key file for Medusa. It contains the &lt;code&gt;MedusaModel&lt;/code&gt; class, which is a wrapper of the original model and the new heads. This class also has an implementation of a streaming generation method. If you want to dive into the details of Medusa, this is the place to start.&lt;/p&gt; &#xA;&lt;p&gt;We also provide some illustrative notebooks in &lt;code&gt;notebooks/&lt;/code&gt; to help you understand the codebase.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome community contributions to Medusa. If you have an idea for how to improve it, please open an issue to discuss it with us. When submitting a pull request, please ensure that your changes are well-tested. Please split each major change into a separate pull request. We also have a &lt;a href=&#34;https://raw.githubusercontent.com/FasterDecoding/Medusa/main/ROADMAP.md&#34;&gt;Roadmap&lt;/a&gt; summarizing our future plans for Medusa. Don&#39;t hesitate to reach out if you are interested in contributing to any of the items on the roadmap.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;This codebase is influenced by remarkable projects from the LLM community, including &lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;FastChat&lt;/a&gt;, &lt;a href=&#34;https://github.com/mit-han-lab/llm-awq/tree/main/&#34;&gt;TinyChat&lt;/a&gt;, &lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;vllm&lt;/a&gt; and many others.&lt;/p&gt;</summary>
  </entry>
</feed>