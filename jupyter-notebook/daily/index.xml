<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-20T01:29:51Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ironjr/StreamMultiDiffusion</title>
    <updated>2024-03-20T01:29:51Z</updated>
    <id>tag:github.com,2024-03-20:/ironjr/StreamMultiDiffusion</id>
    <link href="https://github.com/ironjr/StreamMultiDiffusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official code for the paper &#34;StreamMultiDiffusion: Real-Time Interactive Generation with Region-Based Semantic Control.&#34;&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt; ü¶¶ü¶¶ StreamMultiDiffusion ü¶¶ü¶¶ &lt;/h1&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;üö®üö®üö® NEWS: Our first public demo is out at &lt;a href=&#34;https://huggingface.co/spaces/ironjr/SemanticPalette&#34;&gt;Hugging Face Space&lt;/a&gt;!&lt;/h2&gt; &#xA;&lt;p&gt;We demonstrate &lt;em&gt;semantic palette&lt;/em&gt;, a new drawing paradigm where users paint semantic meanings in addition to colors to create artworks. This is enabled by our acceleration technique for arbitrary-sized image generation from multiple region-based semantic controls. We give our huge thanks to the almighty &lt;a href=&#34;https://huggingface.co/&#34;&gt;Hugging Face ü§ó team&lt;/a&gt; and &lt;a href=&#34;https://www.gradio.app/&#34;&gt;Gradio team&lt;/a&gt; for their invaluable help in building this demo! ü§© The application can be run in your local, since we have provided the app &lt;a href=&#34;https://github.com/ironjr/StreamMultiDiffusion/raw/main/src/app_semantic_draw.py&#34;&gt;in this repository&lt;/a&gt;, too! Just run &lt;code&gt;python app_semantic_draw.py&lt;/code&gt; will do the job.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ironjr/StreamMultiDiffusion/main/assets/demo_semantic_draw_large.gif&#34; width=&#34;90%&#34;&gt; &lt;/p&gt; &#xA; &lt;hr&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ironjr/StreamMultiDiffusion/main/assets/demo.gif&#34; width=&#34;90%&#34;&gt; &lt;/p&gt; &#xA; &lt;h2&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.09055&#34;&gt;StreamMultiDiffusion: Real-Time Interactive Generation&lt;br&gt;with Region-Based Semantic Control&lt;/a&gt;&lt;/h2&gt; &#xA; &lt;p&gt;&lt;a href=&#34;http://jaerinlee.com/&#34;&gt;&lt;strong&gt;Jaerin Lee&lt;/strong&gt;&lt;/a&gt; ¬∑ &lt;a href=&#34;https://dqj5182.github.io/&#34;&gt;&lt;strong&gt;Daniel Sungho Jung&lt;/strong&gt;&lt;/a&gt; ¬∑ &lt;a href=&#34;https://github.com/dlrkdrjs97/&#34;&gt;&lt;strong&gt;Kanggeon Lee&lt;/strong&gt;&lt;/a&gt; ¬∑ &lt;a href=&#34;https://cv.snu.ac.kr/index.php/~kmlee/&#34;&gt;&lt;strong&gt;Kyoung Mu Lee&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ironjr/StreamMultiDiffusion/main/assets/logo_cvlab.png&#34; height=&#34;60&#34;&gt; &lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://jaerinlee.com/research/streammultidiffusion&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Page-green&#34; alt=&#34;Project&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2403.09055&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Arxiv-2403.09055-red&#34; alt=&#34;ArXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ironjr/StreamMultiDiffusion&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/ironjr/StreamMultiDiffusion&#34; alt=&#34;Github&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/_ironjr_&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/url?label=_ironjr_&amp;amp;url=https%3A%2F%2Ftwitter.com%2F_ironjr_&#34; alt=&#34;X&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ironjr/StreamMultiDiffusion/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-MIT-lightgrey&#34; alt=&#34;LICENSE&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/papers/2403.09055&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Paper-yellow&#34; alt=&#34;HFPaper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/ironjr/SemanticPalette&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Demo-yellow&#34; alt=&#34;HFDemo1&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ironjr/StreamMultiDiffusion/main/assets/figure_one.png&#34; width=&#34;100%&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;tl;dr: StreamMultiDiffusion is a &lt;em&gt;real-time&lt;/em&gt; &lt;em&gt;interactive&lt;/em&gt; &lt;em&gt;multiple&lt;/em&gt;-text-to-image generation from user-assigned &lt;em&gt;regional&lt;/em&gt; text prompts.&lt;/p&gt; &#xA;&lt;p&gt;In other words, &lt;strong&gt;you can now draw ‚úçÔ∏è using brushes üñåÔ∏è that paints &lt;em&gt;meanings&lt;/em&gt; üß† in addition to &lt;em&gt;colors&lt;/em&gt;&lt;/strong&gt; üåà!&lt;/p&gt; &#xA;&lt;p&gt;Our paper is mainly about establishing the compatibility between region-based controlling techniques of &lt;a href=&#34;https://multidiffusion.github.io/&#34;&gt;MultiDiffusion&lt;/a&gt; and acceleration techniques of &lt;a href=&#34;https://latent-consistency-models.github.io/&#34;&gt;LCM&lt;/a&gt; and &lt;a href=&#34;https://github.com/cumulo-autumn/StreamDiffusion&#34;&gt;StreamDiffusion&lt;/a&gt;. To our surprise, these works were not compatible before, limiting the possible applications from both branches of works. The effect of acceleration and stabilization of multiple region-based text-to-image generation technique is demonstrated using &lt;a href=&#34;https://huggingface.co/runwayml/stable-diffusion-v1-5&#34;&gt;StableDiffusion v1.5&lt;/a&gt; in the video below ‚¨áÔ∏è:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ironjr/MagicDraw/assets/12259041/9dda9740-58ba-4a96-b8c1-d40765979bd7&#34;&gt;https://github.com/ironjr/MagicDraw/assets/12259041/9dda9740-58ba-4a96-b8c1-d40765979bd7&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The video means that this project finally lets you work with &lt;strong&gt;large size image generation with fine-grained regional prompt control&lt;/strong&gt;. Previously, this was not feasible at all. Taking an hour per trial means that you cannot sample multiple times to pick the best generation you want or to tune the generation process to realize your intention. However, we have decreased the latency &lt;strong&gt;from an hour to a minute&lt;/strong&gt;, making the technology workable for creators (hopefully).&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;‚≠êÔ∏è Features&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ironjr/StreamMultiDiffusion/main/assets/feature1.gif&#34; alt=&#34;usage1&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ironjr/StreamMultiDiffusion/main/assets/feature3.gif&#34; alt=&#34;usage2&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ironjr/StreamMultiDiffusion/main/assets/feature2.gif&#34; alt=&#34;usage3&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA;&lt;/table&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Interactive image generation from scratch with fine-grained region control.&lt;/strong&gt; In other words, you paint images using meainings.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Prompt separation.&lt;/strong&gt; Be bothered no more by unintentional content mixing when generating two or more objects at the same time!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Real-time image inpainting and editing.&lt;/strong&gt; Basically, you draw upon any uploaded photo or a piece of art you want.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;ü§ñ Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n smd python=3.10 &amp;amp;&amp;amp; conda activate smd&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;‚ö° Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Overview&lt;/h3&gt; &#xA;&lt;p&gt;StreamMultiDiffusion is served in three different forms.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;The main interactive demo powered by Gradio is available at &lt;code&gt;src/app.py&lt;/code&gt;. Just type the below line in your command prompt and open &lt;code&gt;https://localhost:8000&lt;/code&gt; with any web browser will launch the app.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_VISIBLE_DEVICES=0 python app.py --model {your stable diffusion 1.5 checkpoint} --height 512 --width 512 --port 8000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;Jupyter Lab demos are available in the &lt;code&gt;notebooks&lt;/code&gt; directory. Simply type &lt;code&gt;jupyter lab&lt;/code&gt; in the command prompt will open a Jupyter server.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Command line prompts by importing the &lt;code&gt;model&lt;/code&gt; in &lt;code&gt;src&lt;/code&gt;. For detailed examples and interfaces, please see the Jupyter demos.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Basic Usage (Python)&lt;/h3&gt; &#xA;&lt;p&gt;The main python modules in our project is two-fold: (1) &lt;a href=&#34;https://github.com/ironjr/StreamMultiDiffusion/raw/main/src/model/stablemultidiffusion_pipeline.py&#34;&gt;&lt;code&gt;model.StableMultiDiffusionPipeline&lt;/code&gt;&lt;/a&gt; for single-call generation (might be more preferable for CLI users), and (2) &lt;a href=&#34;https://github.com/ironjr/StreamMultiDiffusion/raw/main/src/model/streammultidiffusion.py&#34;&gt;&lt;code&gt;model.StreamMultiDiffusion&lt;/code&gt;&lt;/a&gt; for streaming application such as the &lt;a href=&#34;https://github.com/ironjr/StreamMultiDiffusion/raw/main/src/app.py&#34;&gt;one&lt;/a&gt; in the main figure of this README page. We provide minimal examples for the possible applications below.&lt;/p&gt; &#xA;&lt;h3&gt;Streaming Generation Process&lt;/h3&gt; &#xA;&lt;p&gt;With &lt;a href=&#34;https://arxiv.org/abs/2403.09055&#34;&gt;multi-prompt stream batch&lt;/a&gt;, our modification to the &lt;a href=&#34;https://github.com/cumulo-autumn/StreamDiffusion&#34;&gt;original stream batch architecture&lt;/a&gt; by &lt;a href=&#34;https://twitter.com/cumulo_autumn&#34;&gt;@cumulo_autumn&lt;/a&gt;, we can stream this multi-prompt text-to-image generation process to generate images for ever.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Result:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ironjr/StreamMultiDiffusion/main/assets/zeus/prompt.png&#34; alt=&#34;mask&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ironjr/StreamMultiDiffusion/main/assets/athena_stream.gif&#34; alt=&#34;result&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Semantic Brush Input&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Generated Stream&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Code:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from util import seed_everything, Streamer&#xA;from model import StreamMultiDiffusion&#xA;&#xA;# The following packages are imported only for loading the images.&#xA;import torchvision.transforms as T&#xA;import requests&#xA;import time&#xA;import imageio # This is not included in our requirements.txt!&#xA;from functools import reduce&#xA;from io import BytesIO&#xA;from PIL import Image&#xA;&#xA;&#xA;seed = 2024&#xA;device = 0&#xA;height = 768&#xA;width = 512&#xA;&#xA;# Load the module.&#xA;device = torch.device(f&#39;cuda:{device}&#39;)&#xA;smd = StreamMultiDiffusion(&#xA;    device,&#xA;    hf_key=&#39;ironjr/BlazingDriveV11m&#39;,&#xA;    sd_version=&#39;1.5&#39;,&#xA;    height=height,&#xA;    width=width,&#xA;    cfg_type=&#39;none&#39;,&#xA;    autoflush=True,&#xA;    use_tiny_vae=True,&#xA;    mask_type=&#39;continuous&#39;,&#xA;    bootstrap_steps=2,&#xA;    bootstrap_mix_steps=1.5,&#xA;    seed=seed,&#xA;)&#xA;&#xA;# Load the masks.&#xA;masks = []&#xA;for i in range(1, 3):&#xA;    url = f&#39;https://raw.githubusercontent.com/ironjr/StreamMultiDiffusion/main/assets/zeus/prompt_p{i}.png&#39;&#xA;    response = requests.get(url)&#xA;    mask = Image.open(BytesIO(response.content)).convert(&#39;RGBA&#39;)&#xA;    mask = (T.ToTensor()(mask)[-1:] &amp;gt; 0.5).float()&#xA;    masks.append(mask)&#xA;# In this example, background is simply set as non-marked regions.&#xA;background = reduce(torch.logical_and, [m == 0 for m in masks])&#xA;&#xA;# Register a background, prompts, and masks (this can be called multiple times).&#xA;smd.update_background(Image.new(size=(width, height), mode=&#39;RGB&#39;, color=(255, 255, 255)))&#xA;smd.update_single_layer(&#xA;    idx=0,&#xA;    prompt=&#39;a photo of Mount Olympus&#39;,&#xA;    negative_prompt=&#39;worst quality, bad quality, normal quality, cropped, framed&#39;,&#xA;    mask=background,&#xA;    mask_strength=1.0,&#xA;    mask_std=0.0,&#xA;    prompt_strength=1.0,&#xA;)&#xA;smd.update_single_layer(&#xA;    idx=1,&#xA;    prompt=&#39;1girl, looking at viewer, lifts arm, smile, happy, Greek goddess Athena&#39;,&#xA;    negative_prompt=&#39;worst quality, bad quality, normal quality, cropped, framed&#39;,&#xA;    mask=masks[0],&#xA;    mask_strength=1.0,&#xA;    mask_std=0.0,&#xA;    prompt_strength=1.0,&#xA;)&#xA;smd.update_single_layer(&#xA;    idx=2,&#xA;    prompt=&#39;a small, sitting owl&#39;,&#xA;    negative_prompt=&#39;worst quality, bad quality, normal quality, cropped, framed&#39;,&#xA;    mask=masks[1],&#xA;    mask_strength=1.0,&#xA;    mask_std=0.0,&#xA;    prompt_strength=1.0,&#xA;)&#xA;&#xA;&#xA;# Generate images... forever.&#xA;# while True:&#xA;#     image = smd()&#xA;#     image.save(f&#39;{str(int(time.time() % 100000))}.png&#39;) # This will take up your hard drive pretty much soon.&#xA;#     display(image) # If `from IPython.display import display` is called.&#xA;#&#xA;#     You can also intercept the process in the middle of the generation by updating other background, prompts or masks.&#xA;#     smd.update_single_layer(&#xA;#         idx=2,&#xA;#         prompt=&#39;a small, sitting owl&#39;,&#xA;#         negative_prompt=&#39;worst quality, bad quality, normal quality, cropped, framed&#39;,&#xA;#         mask=masks[1],&#xA;#         mask_strength=1.0,&#xA;#         mask_std=0.0,&#xA;#         prompt_strength=1.0,&#xA;#     )&#xA;&#xA;# Or make a video/gif from your generation stream (requires `imageio`)&#xA;frames = []&#xA;for _ in range(50):&#xA;    image = smd()&#xA;    frames.append(image)&#xA;imageio.mimsave(&#39;my_beautiful_creation.gif&#39;, frames, loop=0)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Region-Based Multi-Text-to-Image Generation&lt;/h3&gt; &#xA;&lt;p&gt;We support arbitrary-sized image generation from arbitrary number of prompt-mask pairs. The first example is a simple example of generation Notice that &lt;strong&gt;our generation results also obeys strict prompt separation&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Result:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ironjr/StreamMultiDiffusion/main/assets/timessquare/timessquare_full.png&#34; alt=&#34;mask&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ironjr/StreamMultiDiffusion/main/assets/timessquare_generation.png&#34; alt=&#34;result&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Semantic Brush Input&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Generated Image (10 sec)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p align=&#34;center&#34;&gt; No more unwanted prompt mixing! Brown boy and pink girl generated simultaneously without a problem. &lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Code:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from model import StableMultiDiffusionPipeline&#xA;from util import seed_everything&#xA;&#xA;# The following packages are imported only for loading the images.&#xA;import torchvision.transforms as T&#xA;import requests&#xA;from functools import reduce&#xA;from io import BytesIO&#xA;from PIL import Image&#xA;&#xA;&#xA;seed = 2024&#xA;device = 0&#xA;&#xA;# Load the module.&#xA;seed_everything(seed)&#xA;device = torch.device(f&#39;cuda:{device}&#39;)&#xA;smd = StableMultiDiffusionPipeline(&#xA;    device,&#xA;    hf_key=&#39;ironjr/BlazingDriveV11m&#39;,&#xA;    sd_version=&#39;1.5&#39;,&#xA;)&#xA;&#xA;# Load prompts.&#xA;prompts = [&#xA;    # Background prompt.&#xA;    &#39;1girl, 1boy, times square&#39;,&#xA;    # Foreground prompts.&#xA;    &#39;1boy, looking at viewer, brown hair, casual shirt&#39;,&#xA;    &#39;1girl, looking at viewer, pink hair, leather jacket&#39;,&#xA;]&#xA;negative_prompts = [&#xA;    &#39;&#39;,&#xA;    &#39;1girl&#39;, # (Optional) The first prompt is a boy so we don&#39;t want a girl.&#xA;    &#39;1boy&#39;, # (Optional) The first prompt is a girl so we don&#39;t want a boy.&#xA;]&#xA;negative_prompt_prefix = &#39;worst quality, bad quality, normal quality, cropped, framed&#39;&#xA;negative_prompts = [negative_prompt_prefix + &#39;, &#39; + p for p in negative_prompts]&#xA;&#xA;# Load masks.&#xA;masks = []&#xA;for i in range(1, 3):&#xA;    url = f&#39;https://raw.githubusercontent.com/ironjr/StreamMultiDiffusion/main/assets/timessquare/timessquare_{i}.png&#39;&#xA;    response = requests.get(url)&#xA;    mask = Image.open(BytesIO(response.content)).convert(&#39;RGBA&#39;)&#xA;    mask = (T.ToTensor()(mask)[-1:] &amp;gt; 0.5).float()&#xA;    masks.append(mask)&#xA;# In this example, background is simply set as non-marked regions.&#xA;background = reduce(torch.logical_and, [m == 0 for m in masks])&#xA;masks = torch.stack([background] + masks, dim=0).float()&#xA;&#xA;height, width = masks.shape[-2:] # (768, 768) in this example.&#xA;&#xA;# Sample an image.&#xA;image = smd(&#xA;    prompts,&#xA;    negative_prompts,&#xA;    masks=masks,&#xA;    mask_strengths=1,&#xA;    mask_stds=0,&#xA;    height=height,&#xA;    width=width,&#xA;    bootstrap_steps=2,&#xA;)&#xA;image.save(&#39;my_beautiful_creation.png&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;&lt;em&gt;Larger&lt;/em&gt; Region-Based Multi-Text-to-Image Generation&lt;/h3&gt; &#xA;&lt;p&gt;The below code reproduces the results in the &lt;a href=&#34;https://github.com/ironjr/MagicDraw/assets/12259041/9dda9740-58ba-4a96-b8c1-d40765979bd7&#34;&gt;second video&lt;/a&gt; of this README page. The original MultiDiffusion pipeline using 50 step DDIM sampler takes roughly an hour to run the code, but we have reduced in down to &lt;strong&gt;a minute&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Result:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ironjr/StreamMultiDiffusion/main/assets/irworobongdo/irworobongdo_full.png&#34; alt=&#34;mask&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Semantic Brush Input&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ironjr/StreamMultiDiffusion/main/assets/irworobongdo_generation.png&#34; alt=&#34;result&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Generated Image (59 sec)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Code:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from model import StableMultiDiffusionPipeline&#xA;from util import seed_everything&#xA;&#xA;# The following packages are imported only for loading the images.&#xA;import torchvision.transforms as T&#xA;import requests&#xA;from functools import reduce&#xA;from io import BytesIO&#xA;from PIL import Image&#xA;&#xA;&#xA;seed = 2024&#xA;device = 0&#xA;&#xA;# Load the module.&#xA;seed_everything(seed)&#xA;device = torch.device(f&#39;cuda:{device}&#39;)&#xA;smd = StableMultiDiffusionPipeline(device)&#xA;&#xA;# Load prompts.&#xA;prompts = [&#xA;    # Background prompt.&#xA;    &#39;clear deep blue sky&#39;,&#xA;    # Foreground prompts.&#xA;    &#39;summer mountains&#39;,&#xA;    &#39;the sun&#39;,&#xA;    &#39;the moon&#39;,&#xA;    &#39;a giant waterfall&#39;,&#xA;    &#39;a giant waterfall&#39;,&#xA;    &#39;clean deep blue lake&#39;,&#xA;    &#39;a large tree&#39;,&#xA;    &#39;a large tree&#39;,&#xA;]&#xA;negative_prompts = [&#39;worst quality, bad quality, normal quality, cropped, framed&#39;] * len(prompts)&#xA;&#xA;# Load masks.&#xA;masks = []&#xA;for i in range(1, 9):&#xA;    url = f&#39;https://raw.githubusercontent.com/ironjr/StreamMultiDiffusion/main/assets/irworobongdo/irworobongdo_{i}.png&#39;&#xA;    response = requests.get(url)&#xA;    mask = Image.open(BytesIO(response.content)).convert(&#39;RGBA&#39;)&#xA;    mask = (T.ToTensor()(mask)[-1:] &amp;gt; 0.5).float()&#xA;    masks.append(mask)&#xA;# In this example, background is simply set as non-marked regions.&#xA;background = reduce(torch.logical_and, [m == 0 for m in masks])&#xA;masks = torch.stack([background] + masks, dim=0).float()&#xA;&#xA;height, width = masks.shape[-2:] # (768, 1920) in this example.&#xA;&#xA;# Sample an image.&#xA;image = smd(&#xA;    prompts,&#xA;    negative_prompts,&#xA;    masks=masks,&#xA;    mask_strengths=1,&#xA;    mask_stds=0,&#xA;    height=height,&#xA;    width=width,&#xA;    bootstrap_steps=2,&#xA;)&#xA;image.save(&#39;my_beautiful_creation.png&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Image Inpainting with Prompt Separation&lt;/h3&gt; &#xA;&lt;p&gt;Our pipeline also enables editing and inpainting existing images. We also support &lt;em&gt;any&lt;/em&gt; SD 1.5 checkpoint models. One exceptional advantage of ours is that we provide an easy separation of prompt You can additionally trade-off between prompt separation and overall harmonization by changing the argument &lt;code&gt;bootstrap_steps&lt;/code&gt; from 0 (full mixing) to 5 (full separation). We recommend &lt;code&gt;1-3&lt;/code&gt;. The following code is a minimal example of performing prompt separated multi-prompt image inpainting using our pipeline on a custom model.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Result:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ironjr/StreamMultiDiffusion/main/assets/timessquare/timessquare.jpeg&#34; alt=&#34;mask&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ironjr/StreamMultiDiffusion/main/assets/timessquare/timessquare_full.png&#34; alt=&#34;mask&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ironjr/StreamMultiDiffusion/main/assets/timessquare_inpainting.png&#34; alt=&#34;result&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Images to Inpaint&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Semantic Brush Input&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Inpainted Image (9 sec)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Code:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from model import StableMultiDiffusionPipeline&#xA;from util import seed_everything&#xA;&#xA;# The following packages are imported only for loading the images.&#xA;import torchvision.transforms as T&#xA;import requests&#xA;from io import BytesIO&#xA;from PIL import Image&#xA;&#xA;&#xA;seed = 2&#xA;device = 0&#xA;&#xA;# Load the module.&#xA;seed_everything(seed)&#xA;device = torch.device(f&#39;cuda:{device}&#39;)&#xA;smd = StableMultiDiffusionPipeline(&#xA;    device,&#xA;    hf_key=&#39;ironjr/BlazingDriveV11m&#39;,&#xA;    sd_version=&#39;1.5&#39;,&#xA;)&#xA;&#xA;# Load the background image you want to start drawing.&#xA;#   Although it works for any image, we recommend to use background that is generated&#xA;#   or at least modified by the same checkpoint model (e.g., preparing it by passing&#xA;#   it to the same checkpoint for an image-to-image pipeline with denoising_strength 0.2)&#xA;#   for the maximally harmonized results!&#xA;#   However, in this example, we choose to use a real-world image for the demo.&#xA;url = f&#39;https://raw.githubusercontent.com/ironjr/StreamMultiDiffusion/main/assets/timessquare/timessquare.jpeg&#39;&#xA;response = requests.get(url)&#xA;background_image = Image.open(BytesIO(response.content)).convert(&#39;RGB&#39;)&#xA;&#xA;# Load prompts and background prompts (explicitly).&#xA;background_prompt = &#39;1girl, 1boy, times square&#39;&#xA;prompts = [&#xA;    # Foreground prompts.&#xA;    &#39;1boy, looking at viewer, brown hair, casual shirt&#39;,&#xA;    &#39;1girl, looking at viewer, pink hair, leather jacket&#39;,&#xA;]&#xA;negative_prompts = [&#xA;    &#39;1girl&#39;,&#xA;    &#39;1boy&#39;,&#xA;]&#xA;negative_prompt_prefix = &#39;worst quality, bad quality, normal quality, cropped, framed&#39;&#xA;negative_prompts = [negative_prompt_prefix + &#39;, &#39; + p for p in negative_prompts]&#xA;background_negative_prompt = negative_prompt_prefix&#xA;&#xA;# Load masks.&#xA;masks = []&#xA;for i in range(1, 3):&#xA;    url = f&#39;https://raw.githubusercontent.com/ironjr/StreamMultiDiffusion/main/assets/timessquare/timessquare_{i}.png&#39;&#xA;    response = requests.get(url)&#xA;    mask = Image.open(BytesIO(response.content)).convert(&#39;RGBA&#39;)&#xA;    mask = (T.ToTensor()(mask)[-1:] &amp;gt; 0.5).float()&#xA;    masks.append(mask)&#xA;masks = torch.stack(masks, dim=0).float()&#xA;height, width = masks.shape[-2:] # (768, 768) in this example.&#xA;&#xA;# Sample an image.&#xA;image = smd(&#xA;    prompts,&#xA;    negative_prompts,&#xA;    masks=masks,&#xA;    mask_strengths=1,&#xA;    # Use larger standard deviation to harmonize the inpainting result (Recommended: 8-32)!&#xA;    mask_stds=16.0,&#xA;    height=height,&#xA;    width=width,&#xA;    bootstrap_steps=2,&#xA;    bootstrap_leak_sensitivity=0.1,&#xA;    # This is for providing the image input.&#xA;    background=background_image,&#xA;    background_prompt=background_prompt,&#xA;    background_negative_prompt=background_negative_prompt,&#xA;)&#xA;image.save(&#39;my_beautiful_inpainting.png&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Panorama Generation&lt;/h3&gt; &#xA;&lt;p&gt;Our &lt;a href=&#34;https://github.com/ironjr/StreamMultiDiffusion/raw/main/src/model/stablemultidiffusion_pipeline.py&#34;&gt;&lt;code&gt;model.StableMultiDiffusionPipeline&lt;/code&gt;&lt;/a&gt; supports x10 faster generation of irregularly large size images such as panoramas. For example, the following code runs in 10s with a single 2080 Ti GPU.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Result:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ironjr/StreamMultiDiffusion/main/assets/panorama_generation.png&#34; width=&#34;100%&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; 512x3072 image generated in 10 seconds. &lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Code:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from model import StableMultiDiffusionPipeline&#xA;&#xA;device = 0&#xA;&#xA;# Load the module.&#xA;device = torch.device(f&#39;cuda:{device}&#39;)&#xA;smd = StableMultiDiffusionPipeline(device)&#xA;&#xA;# Sample a panorama image.&#xA;smd.sample_panorama(&#39;A photo of Alps&#39;, height=512, width=3072)&#xA;image.save(&#39;my_panorama_creation.png&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Basic StableDiffusion&lt;/h3&gt; &#xA;&lt;p&gt;We also support standard single-prompt single-tile sampling of StableDiffusion checkpoint for completeness. This behaves exactly the same as calling &lt;a href=&#34;https://huggingface.co/docs/diffusers/en/index&#34;&gt;&lt;code&gt;diffuser&lt;/code&gt;&lt;/a&gt;&#39;s &lt;a href=&#34;https://github.com/huggingface/diffusers/raw/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py&#34;&gt;&lt;code&gt;StableDiffusionPipeline&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Result:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ironjr/StreamMultiDiffusion/main/assets/dolomites_generation.png&#34; width=&#34;50%&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Code:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from model import StableMultiDiffusionPipeline&#xA;&#xA;device = 0&#xA;&#xA;# Load the module.&#xA;device = torch.device(f&#39;cuda:{device}&#39;)&#xA;smd = StableMultiDiffusionPipeline(device)&#xA;&#xA;# Sample an image.&#xA;image = smd.sample(&#39;A photo of the dolomites&#39;)&#xA;image.save(&#39;my_creation.png&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Basic Usage (GUI)&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ironjr/StreamMultiDiffusion/main/assets/instruction1.png&#34; alt=&#34;usage1&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ironjr/StreamMultiDiffusion/main/assets/instruction2.png&#34; alt=&#34;usage2&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Upoad a background image&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Type some text prompts&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ironjr/StreamMultiDiffusion/main/assets/instruction3.png&#34; alt=&#34;usage3&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ironjr/StreamMultiDiffusion/main/assets/instruction4.png&#34; alt=&#34;usage4&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Draw&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Press the play button and enjoy ü§©&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;(top-left) &lt;strong&gt;Upload a background image.&lt;/strong&gt; You can start with a white background image, as well as any other images from your phone camera or other AI-generated artworks. You can also entirely cover the image editor with specific semantic brush to draw background image simultaneously from the text prompt.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;(top-right) &lt;strong&gt;Type some text prompts.&lt;/strong&gt; Click each semantic brush on the semantic palette on the left of the screen and type in text prompts in the interface below. This will create a new semantic brush for you.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;(bottom-left) &lt;strong&gt;Draw.&lt;/strong&gt; Select appropriate layer (&lt;em&gt;important&lt;/em&gt;) that matches the order of the semantic palette. That is, &lt;em&gt;&lt;strong&gt;Layer n&lt;/strong&gt;&lt;/em&gt; corresponds to &lt;em&gt;&lt;strong&gt;Prompt n&lt;/strong&gt;&lt;/em&gt;. I am not perfectly satisfied with the interface of the drawing interface. Importing professional Javascript-based online drawing tools instead of the default &lt;code&gt;gr.ImageEditor&lt;/code&gt; will enable more responsive interface. We have released our code with MIT License, so please feel free to fork this repo and build a better user interface upon it. üòÅ&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;(bottom-right) &lt;strong&gt;Press the play button and enjoy!&lt;/strong&gt; The buttons literally mean &#39;toggle stream/run single/run batch (4)&#39;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Basic Usage (CLI)&lt;/h3&gt; &#xA;&lt;p&gt;Coming Soon!&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;üíº Further Information&lt;/h2&gt; &#xA;&lt;p&gt;We have provided detailed explanation of the application design and the expected usages in appendices of our &lt;a href=&#34;https://arxiv.org/abs/2403.09055&#34;&gt;paper&lt;/a&gt;. This section is a summary of its contents. Although we expect everything to work fine, there may be unexpected bugs or missed features in the implementation. We are always welcoming issues and pull requests from you to improve this project! ü§ó&lt;/p&gt; &#xA;&lt;h3&gt;User Interface (GUI)&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ironjr/StreamMultiDiffusion/main/assets/user_interface.png&#34; width=&#34;90%&#34;&gt; &lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;No.&lt;/th&gt; &#xA;   &lt;th&gt;Component Name&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;&lt;em&gt;Semantic palette&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Creates and manages text prompt-mask pairs, a.k.a., &lt;em&gt;semantic brushes&lt;/em&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;Create new &lt;em&gt;semantic brush&lt;/em&gt; btn.&lt;/td&gt; &#xA;   &lt;td&gt;Creates a new text prompt-mask pair.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;Main drawing pad&lt;/td&gt; &#xA;   &lt;td&gt;User draws at each semantic layers with a brush tool.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;Layer selection&lt;/td&gt; &#xA;   &lt;td&gt;Each layer corresponds to each of the prompt mask in the &lt;em&gt;semantic palette&lt;/em&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;Background image upload&lt;/td&gt; &#xA;   &lt;td&gt;User uploads background image to start drawing.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;6&lt;/td&gt; &#xA;   &lt;td&gt;Drawing tools&lt;/td&gt; &#xA;   &lt;td&gt;Using brushes and erasers to interactively edit the prompt masks.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;Play button&lt;/td&gt; &#xA;   &lt;td&gt;Switches between streaming/step-by-step mode.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;Display&lt;/td&gt; &#xA;   &lt;td&gt;Generated images are streamed through this component.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;9&lt;/td&gt; &#xA;   &lt;td&gt;Mask alpha control&lt;/td&gt; &#xA;   &lt;td&gt;Changes the mask alpha value before quantization. Controls local content blending (simply means that you can use nonbinary masks for fine-grained controls), but extremely sensitive. Recommended: &amp;gt;0.95&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;10&lt;/td&gt; &#xA;   &lt;td&gt;Mask blur std. dev. control&lt;/td&gt; &#xA;   &lt;td&gt;Changes the standard deviation of the quantized mask of the current semantic brush. Less sensitive than mask alpha control.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;11&lt;/td&gt; &#xA;   &lt;td&gt;Seed control&lt;/td&gt; &#xA;   &lt;td&gt;Changes the seed of the application. May not be needed, since we generate infinite stream of images.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;   &lt;td&gt;Prompt edit&lt;/td&gt; &#xA;   &lt;td&gt;User can interactively change the positive/negative prompts at need.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;13&lt;/td&gt; &#xA;   &lt;td&gt;Prompt strength control&lt;/td&gt; &#xA;   &lt;td&gt;Prompt embedding mix ratio between the current &amp;amp; the background. Helps global content blending. Recommended: &amp;gt;0.75&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;14&lt;/td&gt; &#xA;   &lt;td&gt;Brush name edit&lt;/td&gt; &#xA;   &lt;td&gt;Adds convenience by changing the name of the brush. Does not affect the generation. Just for preference.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Demo Application Architecture&lt;/h3&gt; &#xA;&lt;p&gt;There are two types of transaction data between the front-end and the back-end (&lt;code&gt;model.streammultidiffusion_pipeline.StreamMultiDiffusion&lt;/code&gt;) of the application: a (1) background image object and a (2) list of text prompt-mask pairs. We choose to call a pair of the latter as a &lt;em&gt;semantic brush&lt;/em&gt;. Despite its fancy name, a &lt;em&gt;semantic brush&lt;/em&gt; is just a pair of a text prompt and a regional mask assigned to the prompt, possibly with additional mask-controlling parameters. Users interact with the application by registering and updating these two types of data to control the image generation stream. The interface is summarized in the image below ‚¨áÔ∏è:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ironjr/StreamMultiDiffusion/main/assets/app_design.png&#34; width=&#34;90%&#34;&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;üö© &lt;strong&gt;Updates&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üèÉ More public demos are expected!&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ March 19, 2023: Our first public demo of &lt;em&gt;semantic palette&lt;/em&gt; is out at &lt;a href=&#34;https://huggingface.co/spaces/ironjr/SemanticPalette&#34;&gt;Hugging Face Space&lt;/a&gt;! We would like to give our biggest thanks to the almighty Hugging Face ü§ó team for their help!&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ March 16, 2023: Added examples and instructions for region-based generation, panorama generation, and inpainting.&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ March 15, 2023: Added detailed instructions in this README for creators.&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ March 14, 2023: We have released our paper, StreamMultiDiffusion on &lt;a href=&#34;https://arxiv.org/abs/2403.09055&#34;&gt;arXiv&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ March 13, 2023: Code release!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üôã FAQ&lt;/h2&gt; &#xA;&lt;h3&gt;What is &lt;em&gt;Semantic Palette&lt;/em&gt; Anyway?&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Semantic palette&lt;/strong&gt; basically means that you paint things with semantics, i.e., text prompts, just like how you may use brush tools in commercial image editing software, such as Adobe Photoshop, etc. Our acceleration technique for region-based controlled image generation allows users to edit their prompt masks similarly to drawing. We couldn&#39;t find a good preexisting name for this type of user interface, so we named it as &lt;em&gt;semantic palette&lt;/em&gt;, hoping for it to make sense to you. üòÑ&lt;/p&gt; &#xA;&lt;h2&gt;üåè Citation&lt;/h2&gt; &#xA;&lt;p&gt;Please cite us if you find our project useful!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-latex&#34;&gt;@article{lee2024streammultidiffusion,&#xA;    title={{StreamMultiDiffusion:} Real-Time Interactive Generation with Region-Based Semantic Control},&#xA;    author={Lee, Jaerin and Jung, Daniel Sungho and Lee, Kanggeon and Lee, Kyoung Mu},&#xA;    journal={arXiv preprint arXiv:2403.09055},&#xA;    year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ü§ó Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;Our code is based on the projects: &lt;a href=&#34;https://github.com/cumulo-autumn/StreamDiffusion&#34;&gt;StreamDiffusion&lt;/a&gt;, &lt;a href=&#34;https://multidiffusion.github.io/&#34;&gt;MultiDiffusion&lt;/a&gt;, and &lt;a href=&#34;https://latent-consistency-models.github.io/&#34;&gt;Latent Consistency Model&lt;/a&gt;. Thank you for sharing such amazing works! We also give our huge thanks to &lt;a href=&#34;https://twitter.com/br_d&#34;&gt;@br_d&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/KBlueleaf&#34;&gt;@KBlueleaf&lt;/a&gt; for the wonderful models &lt;a href=&#34;https://civitai.com/models/121083?modelVersionId=236210&#34;&gt;BlazingDriveV11m&lt;/a&gt; and &lt;a href=&#34;https://civitai.com/models/136268/kohaku-v2&#34;&gt;Kohaku V2&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h2&gt;üìß Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions, please email &lt;code&gt;jarin.lee@gmail.com&lt;/code&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>sspaeti-com/practical-data-engineering</title>
    <updated>2024-03-20T01:29:51Z</updated>
    <id>tag:github.com,2024-03-20:/sspaeti-com/practical-data-engineering</id>
    <link href="https://github.com/sspaeti-com/practical-data-engineering" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Practical Data Engineering: A Hands-On Real-Estate Project Guide&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;left&#34;&gt; &lt;a href=&#34;https://www.ssp.sh/&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://ssp.sh/images/sspaeti_quadrat.png&#34; height=&#34;100&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Practical Data Engineering: A Hands-On Real-Estate Project Guide&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.ssp.sh/blog/data-engineering-project-in-twenty-minutes/&#34;&gt;&lt;img src=&#34;https://www.ssp.sh/blog/data-engineering-project-in-twenty-minutes/images/open-source-logos.png&#34; alt=&#34;Open Source Logos&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository containts a practical implementation of a data engineering project that spans across web-scraping real-estates, processing with Spark and Delta Lake, adding data science with Jupyter Notebooks, ingesting data into Apache Druid, visualizing with Apache Superset, and managing workflows with Dagster‚Äîall orchestrated on Kubernetes.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Built your own DE project or forked mine? Let me know in the comments; I&#39;d be curious to know more about.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üåü About This Project&lt;/h2&gt; &#xA;&lt;p&gt;This Practical Data Engineering project addresses common data engineering challenges while exploring innovative technologies. It should serve as a learning project but incorporate comprehensive real-world use cases. It&#39;s a guide to building a data application that collects real-estate data, enriches it with various metrics, and offers insights through machine learning and data visualization. This application helps you find your dream properties in your area and showcases how to handle a full-fledged data engineering pipeline using modern tools and frameworks.&lt;/p&gt; &#xA;&lt;h3&gt;Why this project?&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Real-World Application&lt;/strong&gt;: Tackling a genuine problem with real estate data to find the best properties.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Comprehensive Tech Stack&lt;/strong&gt;: Utilizes a wide range of technologies from web scraping, S3 storage, data processing, machine learning, to data visualization and orchestration.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hands-On Learning&lt;/strong&gt;: Offers a hands-on approach to understanding how different technologies integrate and complement each other in a real-world scenario.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Key Features &amp;amp; Learnings:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Scraping real estate listings with &lt;a href=&#34;https://beautiful-soup-4.readthedocs.io/en/latest/index.html&#34;&gt;Beautiful Soup&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Change Data Capture (CDC) mechanisms for efficient data updates&lt;/li&gt; &#xA; &lt;li&gt;Utilizing &lt;a href=&#34;https://github.com/minio/minio&#34;&gt;MinIO&lt;/a&gt; as an S3-Gateway for cloud-agnostic storage&lt;/li&gt; &#xA; &lt;li&gt;Implementing UPSERTs and ACID transactions with &lt;a href=&#34;https://delta.io/&#34;&gt;Delta Lake&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Integrating &lt;a href=&#34;https://github.com/jupyter/notebook&#34;&gt;Jupyter Notebooks&lt;/a&gt; for data science tasks Visualizing data with &lt;a href=&#34;https://github.com/apache/superset&#34;&gt;Apache Superset&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Orchestrating workflows with &lt;a href=&#34;https://github.com/dagster-io/dagster/&#34;&gt;Dagster&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Deploying on &lt;a href=&#34;https://github.com/kubernetes/kubernetes&#34;&gt;Kubernetes&lt;/a&gt; for scalability and cloud-agnostic architecture&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Technologies, Tools, and Frameworks:&lt;/h3&gt; &#xA;&lt;p&gt;This project leverages a vast array of open-source technologies including MinIO, Spark, Delta Lake, Jupyter Notebooks, Apache Druid, Apache Superset, and Dagster‚Äîall running on Kubernetes to ensure scalability and cloud-agnostic deployment.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://www.ssp.sh/blog/data-engineering-project-in-twenty-minutes/images/lakehouse-open-sourced.png&#34; height=&#34;500&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;üîÑ Project Evolution and Updates&lt;/h3&gt; &#xA;&lt;p&gt;This project started in November 2020 as a project for me to learn and teach about data engineering. I published the entire project in March 2021 (see the initial version on &lt;a href=&#34;https://github.com/sspaeti-com/practical-data-engineering/tree/v1&#34;&gt;branch &lt;code&gt;v1&lt;/code&gt;&lt;/a&gt;). Three years later, it&#39;s interesting that the tools used in this project are still used today. We always say how fast the Modern Data Stack changes, but if you choose wisely, you see that good tools will stay the time. Today, in &lt;code&gt;March 2024&lt;/code&gt;, I updated the project to the latest Dagster and representative tools versions. I kept most technologies, except Apache Spark. It was a nightmare to setup locally and to work with Delta Lake SQL APi. I replaced it with &lt;a href=&#34;https://github.com/delta-io/delta-rs&#34;&gt;delta-rs&lt;/a&gt; direct, which is implemented in Rust and can edit and write Delta Tables directly in Python.&lt;/p&gt; &#xA;&lt;p&gt;Next, I might add Rill Developer to the mix to have some fun analyzing the data powered by DuckDB. For a more production-ready dashboard, Superset would still be my choice tough.&lt;/p&gt; &#xA;&lt;h2&gt;üõ† Installation &amp;amp; Usage&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to individual component directories for detailed setup and usage instructions. The project is designed to run both locally and on cloud environments, offering flexibility in deployment and testing.&lt;/p&gt; &#xA;&lt;h3&gt;Prerequisites:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python and pip for installing dependencies&lt;/li&gt; &#xA; &lt;li&gt;MinIO running for cloud-agnostic S3 storage&lt;/li&gt; &#xA; &lt;li&gt;Docker Desktop &amp;amp; Kubernetes for running Jupyter Notebooks (optional, if you want ML capabilities)&lt;/li&gt; &#xA; &lt;li&gt;Basic understanding of Python and SQL for effective navigation and customization of the project&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Quick Start:&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;‚ö†Ô∏è &lt;strong&gt;Disclaimer: For Educational Use Only&lt;/strong&gt;&lt;br&gt; This project is designed for educational purposes, demonstrating web scraping and data engineering practices. Ensure you do not violate any website&#39;s copyright or terms of service, and approach scraping responsibly and respectfully.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone this repository.&lt;/li&gt; &#xA; &lt;li&gt;Install dependencies&lt;/li&gt; &#xA; &lt;li&gt;Install and start MinIO&lt;/li&gt; &#xA; &lt;li&gt;Explore the data with the provided Jupyter Notebooks and Superset dashboards.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;#change to the pipeline directory&#xA;cd src/pipelines/real-estate&#xA;&#xA;# installation&#xA;pip install -e &#34;.[dev]&#34;&#xA;&#xA;# run minio&#xA;minio server /tmp/minio/&#xA;&#xA;# create the bucket `real-estate` MinIO on 127.0.0.1:9000&#xA;# Create access key/passwords, defaults are MINIO_ROOT_USER/MINIO_ROOT_PASSWORD with default values that should work without any further configuration.&#xA;&#xA;# startup dagster&#xA;dagster dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üìà Visualizing the Pipeline&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/sspaeti-com/practical-data-engineering/main/images/dagster-practical-data-engineering-pipeline.png&#34; alt=&#34;Dagster UI ‚Äì Practical Data Engineering Pipeline&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üìö Resources &amp;amp; Further Reading&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtu.be/FfDOsgg2EEQ&#34;&gt;YouTube Video&lt;/a&gt;: Watch the video tutorial on building this project.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ssp.sh/blog/data-engineering-project-in-twenty-minutes/&#34;&gt;Building a Data Engineering Project in 20 Minutes&lt;/a&gt;: Access the full blog post detailing the project&#39;s development, challenges, and solutions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sspaeti-com/data-engineering-devops&#34;&gt;DevOps Repositories&lt;/a&gt;: Explore the setup for Druid, MinIO and other components.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ssp.sh/brain/open-source-data-engineering-projects/&#34;&gt;Open-Source Data Engineering Projects&lt;/a&gt;: A curated list of open-source data engineering projects to explore.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://vault.ssp.sh/&#34;&gt;Data Engineering Vault&lt;/a&gt;: A collection of resources, tutorials, and guides for data engineering projects.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.dedp.online&#34;&gt;Data Engineering Design Patterns&lt;/a&gt;: My book about the evolution of data engineering and its design patterns.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üì£ Feedback&lt;/h2&gt; &#xA;&lt;p&gt;Your feedback is invaluable to improve this project. If you&#39;ve built your project based on this repository or have suggestions, please let me know through creating an Issues or a Pull Request directly.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;em&gt;This project is part of my journey in exploring data engineering challenges and solutions. It&#39;s an open invitation for everyone interested in data engineering to learn, contribute, and share your experiences.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Below some impressions of the jupyter notebook used in this project.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://sspaeti.com/blog/the-location-independent-lifestyle/europe/sspaeti_com_todays_office_033.jpg&#34; width=&#34;600&#34;&gt; &lt;/p&gt;</summary>
  </entry>
</feed>