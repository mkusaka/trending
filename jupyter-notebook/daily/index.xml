<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-01-25T01:34:59Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>apoorvumang/prompt-lookup-decoding</title>
    <updated>2024-01-25T01:34:59Z</updated>
    <id>tag:github.com,2024-01-25:/apoorvumang/prompt-lookup-decoding</id>
    <link href="https://github.com/apoorvumang/prompt-lookup-decoding" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Prompt Lookup Decoding&lt;/h1&gt; &#xA;&lt;p&gt;Minimal implementation: See &lt;a href=&#34;https://raw.githubusercontent.com/apoorvumang/prompt-lookup-decoding/main/demo-pld.ipynb&#34;&gt;demo notebook&lt;/a&gt; or &lt;a href=&#34;https://colab.research.google.com/drive/1ovjH1sg3lXWdm5Rx5EEukB9H_PFJVpJ4?usp=sharing&#34;&gt;colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TLDR&lt;/strong&gt;: We modify speculative decoding where we replace the draft model with simple string matching in the prompt to generate candidate token sequences. This results in significant speedups (2x-4x) in input-grounded tasks, with no effect on output quality. This method can be used with any decoder model without model changes or external datastore, and with both greedy and sampling techniques.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/apoorvumang/prompt-lookup-decoding/assets/1957903/e908de89-ce5c-4156-8ef1-21f169dc1c8f&#34;&gt;https://github.com/apoorvumang/prompt-lookup-decoding/assets/1957903/e908de89-ce5c-4156-8ef1-21f169dc1c8f&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Coloured token indicate that multiple tokens were generated in a single step.&lt;/p&gt; &#xA;&lt;h2&gt;Method&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Intuition&lt;/strong&gt;: In several LLM use cases where you&#39;re doing &lt;em&gt;input grounded generation&lt;/em&gt; (summarization, document QA, multi-turn chat, code editing), there is high n-gram overlap between LLM input (prompt) and LLM output. This could be entity names, phrases, or code chunks that the LLM directly copies from the input while generating the output. Prompt lookup exploits this pattern to speed up autoregressive decoding in LLMs.&lt;/p&gt; &#xA;&lt;p&gt;Here&#39;s an animation explaining with an example (for information on how speculative decoding itself works/gives speedup, please see this excellent &lt;a href=&#34;https://huggingface.co/blog/assisted-generation&#34;&gt;huggingface blog&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/apoorvumang/prompt-lookup-decoding/assets/1957903/10c3728b-2d50-4205-a758-478e51425793&#34;&gt;https://github.com/apoorvumang/prompt-lookup-decoding/assets/1957903/10c3728b-2d50-4205-a758-478e51425793&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is the &#34;prompt lookup&#34; function:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;def find_candidate_pred_tokens(input_ids, max_ngram_size=3, num_pred_tokens=10):&#xA;    input_length = input_ids.size(1)&#xA;&#xA;    for ngram_size in range(max_ngram_size, 0, -1):&#xA;        # Extract the last n tokens as our search ngram&#xA;        ngram = input_ids[0, -ngram_size:].tolist()&#xA;&#xA;        # Create sliding windows of size ngram_size&#xA;        windows = input_ids.unfold(dimension=1, size=ngram_size, step=1)&#xA;&#xA;        # Convert ngram to a tensor for comparison&#xA;        ngram_tensor = torch.tensor(ngram, device=input_ids.device).unsqueeze(0)&#xA;&#xA;        # Find where the windows match the ngram&#xA;        matches = (windows == ngram_tensor).all(dim=2)&#xA;&#xA;        # Get the indices of matches&#xA;        match_indices = matches.nonzero(as_tuple=True)[1]&#xA;&#xA;        # Iterate through match indices to find a valid continuation&#xA;        for idx in match_indices:&#xA;            start_idx = idx + ngram_size&#xA;            end_idx = start_idx + num_pred_tokens&#xA;            # Ensure we don&#39;t go beyond the length of input_ids and avoid self-match&#xA;            if end_idx &amp;lt;= input_length and start_idx &amp;lt; input_length - ngram_size:&#xA;                return input_ids[0, start_idx:end_idx]&#xA;&#xA;    # If no match is found, return an empty tensor&#xA;    return torch.tensor([], dtype=torch.long, device=input_ids.device)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Implementation-wise, we modify &lt;a href=&#34;https://arxiv.org/pdf/2302.01318.pdf&#34;&gt;speculative decoding&lt;/a&gt; (aka assisted generation in hf transformers) by swapping out the “draft model” with this function.&lt;/p&gt; &#xA;&lt;p&gt;Input to this function is the same as to the draft model - all the tokens till the current generation step (&lt;code&gt;input_ids&lt;/code&gt;). It then tries to match last few tokens to somewhere earlier in the prompt. If found, it returns the next-k token continuation as &lt;code&gt;candidate_input_ids&lt;/code&gt; or candidate sequence. The 2 parameters are &lt;code&gt;max_ngram_size&lt;/code&gt;, which is the maximum ngram to use when looking for matches in the prompt. &lt;code&gt;num_pred_tokens&lt;/code&gt; is the candidate sequence length to return after match is found.&lt;/p&gt; &#xA;&lt;h2&gt;Experimental setup&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;GPU&lt;/strong&gt;: Single A100 40GB&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Model&lt;/strong&gt;: Mistral-7B-Instruct-v0.1&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Decoding Type&lt;/strong&gt;: Greedy decoding&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hyperparams&lt;/strong&gt;: Matching max n-gram size = 3, Continuation length = 10&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Datasets&lt;/h2&gt; &#xA;&lt;p&gt;We experiment on 3 datasets, and compare with simple greedy decoding as a baseline. We focus on &#34;input-grounded&#34; tasks where we expect high overlap between input and output - summarization, context-based QA and multi-turn chat.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Summarization&lt;/strong&gt;: CNN/Dailymail 100 examples&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Context-QA&lt;/strong&gt;: 100 examples from &lt;a href=&#34;https://github.com/project-miracl/hagrid&#34;&gt;HAGRID&lt;/a&gt;. We concatenate all the evidences to form the context and then do QA&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-turn chat&lt;/strong&gt;: &lt;a href=&#34;https://huggingface.co/datasets/HuggingFaceH4/mt_bench_prompts&#34;&gt;MT-bench&lt;/a&gt;, all 80 examples. This isn&#39;t exactly input-grounded generation but gives an idea of performance on regular chat&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Results&lt;/h2&gt; &#xA;&lt;h3&gt;Summarization and Context-QA&lt;/h3&gt; &#xA;&lt;p&gt;On both summarization and context-QA, we get a relatively consistent 2.4x speedup (on average). The error bar is stddev, which shows there is quite a bit of variation depending on the example. Throughput of PLD was always more than that of greedy (or within margin of error) - I never saw it giving worse throughput than greedy on any example.&lt;/p&gt; &#xA;&lt;img width=&#34;639&#34; alt=&#34;image&#34; src=&#34;https://github.com/apoorvumang/prompt-lookup-decoding/assets/1957903/df9aea26-3f49-473c-972a-0d9caa641b1e&#34;&gt; &#xA;&lt;h3&gt;Multi-turn chat&lt;/h3&gt; &#xA;&lt;p&gt;On MT-Bench, we see a similar gain on turn 1, but a much smaller gain on turn 0. This is expected - in the first turn, the algorithm can only match n-grams with its own output, since the prompt is pretty small. However this matching with self output gives measurable gains. Again, the error bars are stddev and I didn&#39;t see PLD giving worse throughput on any example.&lt;/p&gt; &#xA;&lt;img width=&#34;535&#34; alt=&#34;image&#34; src=&#34;https://github.com/apoorvumang/prompt-lookup-decoding/assets/1957903/518420ac-b3d2-44af-8f76-b5656b2be6f4&#34;&gt; &#xA;&lt;p&gt;MT-Bench also has prompt categories. Some observations:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;roleplay has the worst gain. This is probably because there isn&#39;t many ngrams to copy, since each generation is sort of unique.&lt;/li&gt; &#xA; &lt;li&gt;coding has very high gain in 2nd turn, because there is lots of code copying&lt;/li&gt; &#xA; &lt;li&gt;in first turn, extraction has highest gain. This agrees with our hypothesis - in extraction there is definitely n-gram copying, and PLD should help&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img width=&#34;709&#34; alt=&#34;image&#34; src=&#34;https://github.com/apoorvumang/prompt-lookup-decoding/assets/1957903/5bd0b126-3e3f-453c-b04c-05e46b3619ce&#34;&gt; &#xA;&lt;h2&gt;TODOs/Thoughts/Future work&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;There&#39;s probably better ways to do string matching than the current one, and there are several obvious things to improve eg. what to do when there are multiple matches? Whats the ideal length of continuation?&lt;/li&gt; &#xA; &lt;li&gt;We haven&#39;t yet tried sampling, although there&#39;s no reason it shouldn&#39;t work. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Here, one additional thing to test would be whether prompt lookup while sampling can affect hallucination rates, since this artifically increases probability of sampling exact sequences from input (this was suggest by my colleague Shwetha S)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Testing actual FLOPs impact and tradeoffs is needed&lt;/li&gt; &#xA; &lt;li&gt;Also need to figure out best hyperparams - 3 and 10 were chosen on very little testing&lt;/li&gt; &#xA; &lt;li&gt;It would be an interesting challenge to design the &#34;best lookup function&#34; for decoding, could even be a competition?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to cite&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{saxena2023prompt,&#xA;    title = {Prompt Lookup Decoding},&#xA;    author = {Apoorv Saxena},&#xA;    year = {2023},&#xA;    month = {November},&#xA;    url = {https://github.com/apoorvumang/prompt-lookup-decoding/}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>