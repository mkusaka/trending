<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-07T01:21:37Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>dhuynh95/LaVague</title>
    <updated>2024-03-07T01:21:37Z</updated>
    <id>tag:github.com,2024-03-07:/dhuynh95/LaVague</id>
    <link href="https://github.com/dhuynh95/LaVague" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Text2Action AI to automate browser interaction&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/dhuynh95/LaVague/main/static/logo.png&#34; alt=&#34;LaVague Logo&#34;&gt; &#xA; &lt;h1&gt;üåä Welcome to LaVague&lt;/h1&gt; &#xA; &lt;p&gt;Redefining internet surfing by transforming natural language instructions into seamless browser interactions.&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h2&gt;See LaVague in Action&lt;/h2&gt; &#xA; &lt;p&gt;Here are examples to show how LaVague can execute natural instructions on a browser to automate interactions with a website:&lt;/p&gt; &#xA; &lt;figure&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/dhuynh95/LaVague/main/static/lavague_hf-speedup.gif&#34; alt=&#34;LaVague Interaction Example&#34; style=&#34;margin-right: 20px;&#34;&gt; &#xA;  &lt;figcaption&gt;&#xA;   LaVague interacting with Hugging Face&#39;s website.&#xA;  &lt;/figcaption&gt; &#xA; &lt;/figure&gt; &#xA; &lt;figure&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/dhuynh95/LaVague/main/static/lavague_irs.gif&#34; alt=&#34;LaVague Workflow Example&#34;&gt; &#xA;  &lt;figcaption&gt;&#xA;   LaVague interacting with the IRS&#39;s website.&#xA;  &lt;/figcaption&gt; &#xA; &lt;/figure&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Motivations&lt;/h2&gt; &#xA;&lt;p&gt;LaVague is designed to automate menial tasks on behalf of its users. Many of these tasks are repetitive, time-consuming, and require little to no cognitive effort. By automating these tasks, LaVague aims to free up time for more meaningful endeavors, allowing users to focus on what truly matters to them.&lt;/p&gt; &#xA;&lt;p&gt;By providing an engine turning natural language queries into Selenium code, LaVague is designed to make it easy for users or other AIs to automate easily express web workflows and execute them on a browser.&lt;/p&gt; &#xA;&lt;p&gt;One of the key usages we see is to automate tasks that are personal to users and require them to be logged in, for instance automating the process of paying bills, filling out forms or pulling data from specific websites.&lt;/p&gt; &#xA;&lt;p&gt;LaVague is built on open-source projects and leverages open-sources models, either locally or remote, to ensure the transparency of the agent and ensures that it is aligned with users&#39; interests.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Natural Language Processing&lt;/strong&gt;: Understands instructions in natural language to perform browser interactions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Selenium Integration&lt;/strong&gt;: Seamlessly integrates with Selenium for automating web browsers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Open-Source&lt;/strong&gt;: Built on open-source projects such as transformers and llama-index, and leverages open-source models, either locally or remote, to ensure the transparency of the agent and ensures that it is aligned with users&#39; interests.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Local models for privacy and control&lt;/strong&gt;: Supports local models like &lt;code&gt;Gemma-7b&lt;/code&gt; so that users can fully control their AI assistant and have privacy guarantees.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Advanced AI techniques&lt;/strong&gt;: Uses a local embedding (&lt;code&gt;bge-small-en-v1.5&lt;/code&gt;) first to perform RAG to extract the most relevant HTML pieces to feed the LLM answering the query, as directly dropping the full HTML code would not fit in context. Then leverages Few-shot learning and Chain of Thought to elicit the most relevant Selenium code to perform the action without having to finetune the LLM (&lt;code&gt;Nous-Hermes-2-Mixtral-8x7B-DPO&lt;/code&gt;) for code generation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;You can try LaVague in the following Colab notebook:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/dhuynh95/LaVague/blob/main/LaVague.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;p&gt;This is an early project but could grow to democratize transparent and aligned AI models to undertake actions for the sake of users on the internet. We see the following key areas to explore:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fine-tune local models like a &lt;code&gt;gemma-7b-it&lt;/code&gt; to be expert in Text2Action&lt;/li&gt; &#xA; &lt;li&gt;Improve retrieval to make sure only relevant pieces of code are used for code generation&lt;/li&gt; &#xA; &lt;li&gt;Support other browser engines (playwright) or even other automation frameworks&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This is a one-man project done during a hackathon for now, so if you are interested please ping me on &lt;a href=&#34;https://twitter.com/dhuynh95&#34;&gt;Twitter&lt;/a&gt;!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>AILab-CVC/YOLO-World</title>
    <updated>2024-03-07T01:21:37Z</updated>
    <id>tag:github.com,2024-03-07:/AILab-CVC/YOLO-World</id>
    <link href="https://github.com/AILab-CVC/YOLO-World" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[CVPR 2024] Real-Time Open-Vocabulary Object Detection&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/assets/yolo_logo.png&#34; width=&#34;60%&#34;&gt; &#xA; &lt;br&gt; &#xA; &lt;a href=&#34;https://scholar.google.com/citations?hl=zh-CN&amp;amp;user=PH8rJHYAAAAJ&#34;&gt;Tianheng Cheng&lt;/a&gt;&#xA; &lt;sup&gt;&lt;span&gt;2,3,*&lt;/span&gt;&lt;/sup&gt;, &#xA; &lt;a href=&#34;https://linsong.info/&#34;&gt;Lin Song&lt;/a&gt;&#xA; &lt;sup&gt;&lt;span&gt;1,üìß,*&lt;/span&gt;&lt;/sup&gt;, &#xA; &lt;a href=&#34;https://yxgeee.github.io/&#34;&gt;Yixiao Ge&lt;/a&gt;&#xA; &lt;sup&gt;&lt;span&gt;1,üåü,2&lt;/span&gt;&lt;/sup&gt;, &#xA; &lt;a href=&#34;http://eic.hust.edu.cn/professor/liuwenyu/&#34;&gt; Wenyu Liu&lt;/a&gt;&#xA; &lt;sup&gt;&lt;span&gt;3&lt;/span&gt;&lt;/sup&gt;, &#xA; &lt;a href=&#34;https://xwcv.github.io/&#34;&gt;Xinggang Wang&lt;/a&gt;&#xA; &lt;sup&gt;&lt;span&gt;3,üìß&lt;/span&gt;&lt;/sup&gt;, &#xA; &lt;a href=&#34;https://scholar.google.com/citations?user=4oXBp9UAAAAJ&amp;amp;hl=en&#34;&gt;Ying Shan&lt;/a&gt;&#xA; &lt;sup&gt;&lt;span&gt;1,2&lt;/span&gt;&lt;/sup&gt; &#xA; &lt;br&gt; &#xA; &lt;p&gt;* Equal contribution üåü Project lead üìß Corresponding author&lt;/p&gt; &#xA; &lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; Tencent AI Lab, &lt;sup&gt;2&lt;/sup&gt; ARC Lab, Tencent PCG &lt;sup&gt;3&lt;/sup&gt; Huazhong University of Science and Technology &lt;br&gt;&lt;/p&gt; &#xA; &lt;div&gt; &#xA;  &lt;p&gt;&lt;a href=&#34;https://wondervictor.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Page-green&#34; alt=&#34;arxiv paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2401.17270&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-Paper-red&#34; alt=&#34;arxiv paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/AILab-CVC/YOLO-World/blob/master/inference.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/stevengrove/YOLO-World&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97HugginngFace-Spaces-orange&#34; alt=&#34;demo&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://replicate.com/zsxkib/yolo-world&#34;&gt;&lt;img src=&#34;https://replicate.com/zsxkib/yolo-world/badge&#34; alt=&#34;Replicate&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/papers/2401.17270&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97HugginngFace-Paper-yellow&#34; alt=&#34;hfpaper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-GPLv3.0-blue&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/SkalskiP/YOLO-World&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/YOLOWorldxEfficientSAM-%F0%9F%A4%97Spaces-orange&#34; alt=&#34;yoloworldseg&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://supervision.roboflow.com/develop/notebooks/zero-shot-object-detection-with-yolo-world&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%93%96Notebook-roboflow-purple&#34; alt=&#34;yologuide&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://inference.roboflow.com/foundation/yolo_world/&#34;&gt;&lt;img src=&#34;https://media.roboflow.com/deploy.svg?sanitize=true&#34; alt=&#34;deploy&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;/div&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Updates&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;üî•[2024-3-3]:&lt;/code&gt; We add the &lt;strong&gt;high-resolution YOLO-World&lt;/strong&gt;, which supports &lt;code&gt;1280x1280&lt;/code&gt; resolution with higher accuracy and better performance for small objects!&lt;br&gt; &lt;code&gt;üî•[2024-2-29]:&lt;/code&gt; We release the newest version of &lt;a href=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/docs/updates.md&#34;&gt; &lt;strong&gt;YOLO-World-v2&lt;/strong&gt;&lt;/a&gt; with higher accuracy and faster speed! We hope the community can join us to improve YOLO-World!&lt;br&gt; &lt;code&gt;üî•[2024-2-28]:&lt;/code&gt; Excited to announce that YOLO-World has been accepted by &lt;strong&gt;CVPR 2024&lt;/strong&gt;! We&#39;re continuing to make YOLO-World faster and stronger, as well as making it better to use for all.&lt;br&gt; &lt;code&gt;üî•[2024-2-22]:&lt;/code&gt; We sincerely thank &lt;a href=&#34;https://roboflow.com/&#34;&gt;RoboFlow&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/skalskip92&#34;&gt;@Skalskip92&lt;/a&gt; for the &lt;a href=&#34;https://www.youtube.com/watch?v=X7gKBGVz4vs&#34;&gt;&lt;strong&gt;Video Guide&lt;/strong&gt;&lt;/a&gt; about YOLO-World, nice work!&lt;br&gt; &lt;code&gt;üî•[2024-2-18]:&lt;/code&gt; We thank &lt;a href=&#34;https://twitter.com/skalskip92&#34;&gt;@Skalskip92&lt;/a&gt; for developing the wonderful segmentation demo via connecting YOLO-World and EfficientSAM. You can try it now at the &lt;a href=&#34;https://huggingface.co/spaces/SkalskiP/YOLO-World&#34;&gt;ü§ó HuggingFace Spaces&lt;/a&gt;.&lt;br&gt; &lt;code&gt;[2024-2-17]:&lt;/code&gt; The largest model &lt;strong&gt;X&lt;/strong&gt; of YOLO-World is released, which achieves better zero-shot performance!&lt;br&gt; &lt;code&gt;[2024-2-17]:&lt;/code&gt; We release the code &amp;amp; models for &lt;strong&gt;YOLO-World-Seg&lt;/strong&gt; now! YOLO-World now supports open-vocabulary / zero-shot object segmentation!&lt;br&gt; &lt;code&gt;[2024-2-15]:&lt;/code&gt; The pre-traind YOLO-World-L with CC3M-Lite is released!&lt;br&gt; &lt;code&gt;[2024-2-14]:&lt;/code&gt; We provide the &lt;a href=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/demo.py&#34;&gt;&lt;code&gt;image_demo&lt;/code&gt;&lt;/a&gt; for inference on images or directories.&lt;br&gt; &lt;code&gt;[2024-2-10]:&lt;/code&gt; We provide the &lt;a href=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/docs/finetuning.md&#34;&gt;fine-tuning&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/docs/data.md&#34;&gt;data&lt;/a&gt; details for fine-tuning YOLO-World on the COCO dataset or the custom datasets!&lt;br&gt; &lt;code&gt;[2024-2-3]:&lt;/code&gt; We support the &lt;code&gt;Gradio&lt;/code&gt; demo now in the repo and you can build the YOLO-World demo on your own device!&lt;br&gt; &lt;code&gt;[2024-2-1]:&lt;/code&gt; We&#39;ve released the code and weights of YOLO-World now!&lt;br&gt; &lt;code&gt;[2024-2-1]:&lt;/code&gt; We deploy the YOLO-World demo on &lt;a href=&#34;https://huggingface.co/spaces/stevengrove/YOLO-World&#34;&gt;HuggingFace ü§ó&lt;/a&gt;, you can try it now!&lt;br&gt; &lt;code&gt;[2024-1-31]:&lt;/code&gt; We are excited to launch &lt;strong&gt;YOLO-World&lt;/strong&gt;, a cutting-edge real-time open-vocabulary object detector.&lt;/p&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;p&gt;YOLO-World is under active development and please stay tuned ‚òïÔ∏è!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Gradio demo!&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Complete documents for pre-training YOLO-World.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; COCO &amp;amp; LVIS fine-tuning.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Extra pre-trained models on more data, such as CC3M.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Deployment toolkits, e.g., ONNX or TensorRT.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Inference acceleration and scripts for speed evaluation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Automatic labeling framework for image-text pairs, such as CC3M.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Highlights&lt;/h2&gt; &#xA;&lt;p&gt;This repo contains the PyTorch implementation, pre-trained weights, and pre-training/fine-tuning code for YOLO-World.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;YOLO-World is pre-trained on large-scale datasets, including detection, grounding, and image-text datasets.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;YOLO-World is the next-generation YOLO detector, with a strong open-vocabulary detection capability and grounding ability.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;YOLO-World presents a &lt;em&gt;prompt-then-detect&lt;/em&gt; paradigm for efficient user-vocabulary inference, which re-parameterizes vocabulary embeddings as parameters into the model and achieve superior inference speed. You can try to export your own detection model without extra training or fine-tuning in our &lt;a href=&#34;https://huggingface.co/spaces/stevengrove/YOLO-World&#34;&gt;online demo&lt;/a&gt;!&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;center&gt; &#xA; &lt;img width=&#34;800px&#34; src=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/assets/yolo_arch.png&#34;&gt; &#xA;&lt;/center&gt; &#xA;&lt;h2&gt;Abstract&lt;/h2&gt; &#xA;&lt;p&gt;The You Only Look Once (YOLO) series of detectors have established themselves as efficient and practical tools. However, their reliance on predefined and trained object categories limits their applicability in open scenarios. Addressing this limitation, we introduce YOLO-World, an innovative approach that enhances YOLO with open-vocabulary detection capabilities through vision-language modeling and pre-training on large-scale datasets. Specifically, we propose a new Re-parameterizable Vision-Language Path Aggregation Network (RepVL-PAN) and region-text contrastive loss to facilitate the interaction between visual and linguistic information. Our method excels in detecting a wide range of objects in a zero-shot manner with high efficiency. On the challenging LVIS dataset, YOLO-World achieves 35.4 AP with 52.0 FPS on V100, which outperforms many state-of-the-art methods in terms of both accuracy and speed. Furthermore, the fine-tuned YOLO-World achieves remarkable performance on several downstream tasks, including object detection and open-vocabulary instance segmentation.&lt;/p&gt; &#xA;&lt;h2&gt;Main Results&lt;/h2&gt; &#xA;&lt;p&gt;We&#39;ve pre-trained YOLO-World-S/M/L from scratch and evaluate on the &lt;code&gt;LVIS val-1.0&lt;/code&gt; and &lt;code&gt;LVIS minival&lt;/code&gt;. We provide the pre-trained model weights and training logs for applications/research or re-producing the results.&lt;/p&gt; &#xA;&lt;h3&gt;Zero-shot Inference on LVIS dataset&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;model&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Pre-train Data&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Size&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AP&lt;sup&gt;mini&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AP&lt;sub&gt;r&lt;/sub&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AP&lt;sub&gt;c&lt;/sub&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AP&lt;sub&gt;f&lt;/sub&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AP&lt;sup&gt;val&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AP&lt;sub&gt;r&lt;/sub&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AP&lt;sub&gt;c&lt;/sub&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AP&lt;sub&gt;f&lt;/sub&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;weights&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/configs/pretrain/yolo_world_s_dual_vlpan_l2norm_2e-3_100e_4x8gpus_obj365v1_goldg_train_lvis_minival.py&#34;&gt;YOLO-World-S&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;O365+GoldG&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;11.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/wondervictor/YOLO-World/resolve/main/yolo_world_s_clip_base_dual_vlpan_2e-3adamw_32xb16_100e_o365_goldg_train_pretrained-18bea4d2.pth&#34;&gt;HF Checkpoints ü§ó&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/configs/pretrain/yolo_world_v2_s_vlpan_bn_2e-3_100e_4x8gpus_obj365v1_goldg_train_lvis_minival.py&#34;&gt;YOLO-Worldv2-S&lt;/a&gt; üî•&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;O365+GoldG&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;11.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/wondervictor/YOLO-World/blob/main/yolo_world_v2_s_obj365v1_goldg_pretrain-55b943ea.pth&#34;&gt;HF Checkpoints ü§ó&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/configs/pretrain/yolo_world_m_dual_l2norm_2e-4_100e_4x8gpus_obj365v1_goldg_train_lvis_minival.py&#34;&gt;YOLO-World-M&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;O365+GoldG&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;31.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;19.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;19.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/wondervictor/YOLO-World/resolve/main/yolo_world_m_clip_base_dual_vlpan_2e-3adamw_32xb16_100e_o365_goldg_train_pretrained-2b7bd1be.pth&#34;&gt;HF Checkpoints ü§ó&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/configs/pretrain/yolo_world_v2_m_vlpan_bn_2e-3_100e_4x8gpus_obj365v1_goldg_train_lvis_minival.py&#34;&gt;YOLO-Worldv2-M&lt;/a&gt; üî•&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;O365+GoldG&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;23.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/wondervictor/YOLO-World/blob/main/yolo_world_v2_m_obj365v1_goldg_pretrain-c6237d5b.pth&#34;&gt;HF Checkpoints ü§ó&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/configs/pretrain/yolo_world_l_dual_vlpan_l2norm_2e-3_100e_4x8gpus_obj365v1_goldg_train_lvis_minival.py&#34;&gt;YOLO-World-L&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;O365+GoldG&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/wondervictor/YOLO-World/resolve/main/yolo_world_l_clip_base_dual_vlpan_2e-3adamw_32xb16_100e_o365_goldg_train_pretrained-0e566235.pth&#34;&gt;HF Checkpoints ü§ó&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/configs/pretrain/yolo_world_l_dual_vlpan_l2norm_2e-3_100e_4x8gpus_obj365v1_goldg_train_lvis_minival.py&#34;&gt;YOLO-World-L&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;O365+GoldG+CC3M-Lite&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;23.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/wondervictor/YOLO-World/blob/main/yolo_world_l_clip_base_dual_vlpan_2e-3adamw_32xb16_100e_o365_goldg_cc3mlite_train_pretrained-7a5eea3b.pth&#34;&gt;HF Checkpoints ü§ó&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/configs/pretrain/yolo_world_v2_l_vlpan_bn_2e-3_100e_4x8gpus_obj365v1_goldg_train_lvis_minival.py&#34;&gt;YOLO-Worldv2-L&lt;/a&gt; üî•&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;O365+GoldG&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;23.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/wondervictor/YOLO-World/blob/main/yolo_world_v2_l_obj365v1_goldg_pretrain-a82b1fe3.pth&#34;&gt;HF Checkpoints ü§ó&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/configs/pretrain/yolo_world_v2_l_vlpan_bn_2e-3_100e_4x8gpus_obj365v1_goldg_train_1280ft_lvis_minival.py&#34;&gt;YOLO-Worldv2-L&lt;/a&gt; üî•&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;O365+GoldG&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;1280 üî∏&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;29.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;21.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/wondervictor/YOLO-World/blob/main/yolo_world_v2_l_obj365v1_goldg_pretrain_1280ft-9babe3f6.pth&#34;&gt;HF Checkpoints ü§ó&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/configs/pretrain/yolo_world_v2_l_vlpan_bn_2e-3_100e_4x8gpus_obj365v1_goldg_train_lvis_minival.py&#34;&gt;YOLO-Worldv2-L&lt;/a&gt; üî•&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;O365+GoldG+CC3M-Lite&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/wondervictor/YOLO-World/blob/main/yolo_world_v2_l_obj365v1_goldg_cc3mlite_pretrain-ca93cd1f.pth&#34;&gt;HF Checkpoints ü§ó&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/configs/pretrain/yolo_world_x_dual_vlpan_l2norm_2e-3_100e_4x8gpus_obj365v1_goldg_train_lvis_minival.py&#34;&gt;YOLO-World-X&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;O365+GoldG+CC3M-Lite&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;19.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;23.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/wondervictor/YOLO-World/blob/main/yolo_world_x_clip_base_dual_vlpan_2e-3adamw_32xb16_100e_o365_goldg_cc3mlite_train_pretrained-8cf6b025.pth&#34;&gt;HF Checkpoints ü§ó&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/configs/pretrain/yolo_world_v2_x_vlpan_bn_2e-3_100e_4x8gpus_obj365v1_goldg_train_lvis_minival.py&#34;&gt;YOLO-Worldv2-X&lt;/a&gt; üî•&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;O365+GoldG+CC3M-Lite&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/wondervictor/YOLO-World/blob/main/yolo_world_v2_x_obj365v1_goldg_cc3mlite_pretrain-8698fbfa.pth&#34;&gt;HF Checkpoints ü§ó&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;The evaluation results of AP&lt;sup&gt;fixed&lt;/sup&gt; are tested on LVIS &lt;code&gt;minival&lt;/code&gt; with &lt;a href=&#34;https://github.com/achalddave/large-vocab-devil&#34;&gt;fixed AP&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The evaluation results of AP&lt;sup&gt;mini&lt;/sup&gt; are tested on LVIS &lt;code&gt;minival&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The evaluation results of AP&lt;sup&gt;val&lt;/sup&gt; are tested on LVIS &lt;code&gt;val 1.0&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hf-mirror.com/&#34;&gt;HuggingFace Mirror&lt;/a&gt; provides the mirror of HuggingFace, which is a choice for users who are unable to reach.&lt;/li&gt; &#xA; &lt;li&gt;üî∏: fine-tuning models with the pre-trained data.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pre-training Logs:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We provide the pre-training logs of &lt;code&gt;YOLO-World-v2&lt;/code&gt;. Due to the unexpected errors of the local machines, the training might be interrupted several times.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Pre-training Log&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;YOLO-World-v2-S&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1oib7pKfA2h1U_5-85H_s0Nz8jWd0R-WP/view?usp=drive_link&#34;&gt;Part-1&lt;/a&gt;, &lt;a href=&#34;https://drive.google.com/file/d/11cZ6OZy80VTvBlZy3kzLAHCxx5Iix5-n/view?usp=drive_link&#34;&gt;Part-2&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;YOLO-World-v2-L&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1Tola1QGJZTL6nGy3SBxKuknfNfREDm8J/view?usp=drive_link&#34;&gt;Part-1&lt;/a&gt;, &lt;a href=&#34;https://drive.google.com/file/d/1mTBXniioUb0CdctCG4ckIU6idGo0NnH8/view?usp=drive_link&#34;&gt;Part-2&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;YOLO-World-v2-M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1E6vYSS8kBipGc8oQnsjAfeUAx8I9yOX7/view?usp=drive_link&#34;&gt;Part-1&lt;/a&gt;, &lt;a href=&#34;https://drive.google.com/file/d/1fbM7vt2tgSeB8o_7tUDofWvpPNSViNj5/view?usp=drive_link&#34;&gt;Part-2&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;YOLO-World-v2-X&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1aEUA_EPQbXOrpxHTQYB6ieGXudb1PLpd/view?usp=drive_link&#34;&gt;Final part&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;YOLO-World-Seg: Open-Vocabulary Instance Segmentation&lt;/h3&gt; &#xA;&lt;p&gt;We fine-tune YOLO-World on LVIS (&lt;code&gt;LVIS-Base&lt;/code&gt;) with mask annotations for open-vocabulary (zero-shot) instance segmentation.&lt;/p&gt; &#xA;&lt;p&gt;We provide two fine-tuning strategies YOLO-World towards open-vocabulary instance segmentation:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;fine-tuning &lt;code&gt;all modules&lt;/code&gt;: leads to better LVIS segmentation accuracy but affects the zero-shot performance.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;fine-tuning the &lt;code&gt;segmentation head&lt;/code&gt;: maintains the zero-shot performanc but lowers LVIS segmentation accuracy.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Fine-tuning Data&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Fine-tuning Modules&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AP&lt;sup&gt;mask&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AP&lt;sub&gt;r&lt;/sub&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AP&lt;sub&gt;c&lt;/sub&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AP&lt;sub&gt;f&lt;/sub&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Weights&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/configs/segmentation/yolo_world_seg_m_dual_vlpan_2e-4_80e_8gpus_allmodules_finetune_lvis.py&#34;&gt;YOLO-World-Seg-M&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;LVIS-Base&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;all modules&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/wondervictor/YOLO-World/blob/main/yolo_world_seg_m_dual_vlpan_2e-4_80e_8gpus_allmodules_finetune_lvis-ca465825.pth&#34;&gt;HF Checkpoints ü§ó&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/configs/segmentation/yolo_world_seg_l_dual_vlpan_2e-4_80e_8gpus_allmodules_finetune_lvis.py&#34;&gt;YOLO-World-Seg-L&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;LVIS-Base&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;all modules&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/wondervictor/YOLO-World/blob/main/yolo_world_seg_l_dual_vlpan_2e-4_80e_8gpus_allmodules_finetune_lvis-8c58c916.pth&#34;&gt;HF Checkpoints ü§ó&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/configs/segmentation/yolo_seg_world_m_dual_vlpan_2e-4_80e_8gpus_seghead_finetune_lvis.py&#34;&gt;YOLO-World-Seg-M&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;LVIS-Base&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;seg head&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/wondervictor/YOLO-World/blob/main/yolo_world_seg_m_dual_vlpan_2e-4_80e_8gpus_seghead_finetune_lvis-7bca59a7.pth&#34;&gt;HF Checkpoints ü§ó&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/configs/segmentation/yolo_seg_world_l_dual_vlpan_2e-4_80e_8gpus_seghead_finetune_lvis.py&#34;&gt;YOLO-World-Seg-L&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;LVIS-Base&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;seg head&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;19.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;23.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/wondervictor/YOLO-World/blob/main/yolo_world_seg_l_dual_vlpan_2e-4_80e_8gpus_seghead_finetune_lvis-5a642d30.pth&#34;&gt;HF Checkpoints ü§ó&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;The mask AP are evaluated on the LVIS &lt;code&gt;val 1.0&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;All models are fine-tuned for 80 epochs on &lt;code&gt;LVIS-Base&lt;/code&gt; (866 categories, &lt;code&gt;common + frequent&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;The YOLO-World-Seg with only &lt;code&gt;seg head&lt;/code&gt; fine-tuned maintains the original zero-shot detection capability and segments objects.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;h3&gt;1. Installation&lt;/h3&gt; &#xA;&lt;p&gt;YOLO-World is developed based on &lt;code&gt;torch==1.11.0&lt;/code&gt; &lt;code&gt;mmyolo==0.6.0&lt;/code&gt; and &lt;code&gt;mmdetection==3.0.0&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Clone Project&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone --recursive https://github.com/AILab-CVC/YOLO-World.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Install&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install torch wheel -q&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. Preparing Data&lt;/h3&gt; &#xA;&lt;p&gt;We provide the details about the pre-training data in &lt;a href=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/docs/data.md&#34;&gt;docs/data&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Training &amp;amp; Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;We adopt the default &lt;a href=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/tools/train.py&#34;&gt;training&lt;/a&gt; or &lt;a href=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/tools/test.py&#34;&gt;evaluation&lt;/a&gt; scripts of &lt;a href=&#34;https://github.com/open-mmlab/mmyolo&#34;&gt;mmyolo&lt;/a&gt;. We provide the configs for pre-training and fine-tuning in &lt;code&gt;configs/pretrain&lt;/code&gt; and &lt;code&gt;configs/finetune_coco&lt;/code&gt;. Training YOLO-World is easy:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;chmod +x tools/dist_train.sh&#xA;# sample command for pre-training, use AMP for mixed-precision training&#xA;./tools/dist_train.sh configs/pretrain/yolo_world_l_t2i_bn_2e-4_100e_4x8gpus_obj365v1_goldg_train_lvis_minival.py 8 --amp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; YOLO-World is pre-trained on 4 nodes with 8 GPUs per node (32 GPUs in total). For pre-training, the &lt;code&gt;node_rank&lt;/code&gt; and &lt;code&gt;nnodes&lt;/code&gt; for multi-node training should be specified.&lt;/p&gt; &#xA;&lt;p&gt;Evaluating YOLO-World is also easy:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;chmod +x tools/dist_test.sh&#xA;./tools/dist_test.sh path/to/config path/to/weights 8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; We mainly evaluate the performance on LVIS-minival for pre-training.&lt;/p&gt; &#xA;&lt;h2&gt;Fine-tuning YOLO-World&lt;/h2&gt; &#xA;&lt;p&gt;We provide the details about fine-tuning YOLO-World in &lt;a href=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/docs/finetuning.md&#34;&gt;docs/fine-tuning&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Deployment&lt;/h2&gt; &#xA;&lt;p&gt;We provide the details about deployment for downstream applications in &lt;a href=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/docs/deploy.md&#34;&gt;docs/deployment&lt;/a&gt;. You can directly download the ONNX model through the online &lt;a href=&#34;https://huggingface.co/spaces/stevengrove/YOLO-World&#34;&gt;demo&lt;/a&gt; in Huggingface Spaces ü§ó.&lt;/p&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;h3&gt;Gradio Demo&lt;/h3&gt; &#xA;&lt;p&gt;We provide the &lt;a href=&#34;https://www.gradio.app/&#34;&gt;Gradio&lt;/a&gt; demo for local devices:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install gradio&#xA;python demo.py path/to/config path/to/weights&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Additionaly, you can use a Dockerfile to build an image with gradio. As a prerequisite, make sure you have respective drivers installed alongside &lt;a href=&#34;https://stackoverflow.com/questions/59691207/docker-build-with-nvidia-runtime&#34;&gt;nvidia-container-runtime&lt;/a&gt;. Replace MODEL_NAME and WEIGHT_NAME with the respective values or ommit this and use default values from the &lt;a href=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/Dockerfile#3&#34;&gt;Dockerfile&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build --build-arg=&#34;MODEL=MODEL_NAME&#34; --build-arg=&#34;WEIGHT=WEIGHT_NAME&#34; -t yolo_demo .&#xA;docker run --runtime nvidia -p 8080:8080&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Image Demo&lt;/h3&gt; &#xA;&lt;p&gt;We provide a simple image demo for inference on images with visualization outputs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python image_demo.py path/to/config path/to/weights image/path/directory &#39;person,dog,cat&#39; --topk 100 --threshold 0.005 --output-dir demo_outputs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Notes:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;code&gt;image&lt;/code&gt; can be a directory or a single image.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;texts&lt;/code&gt; can be a string of categories (noun phrases) which is separated by a comma. We also support &lt;code&gt;txt&lt;/code&gt; file in which each line contains a category ( noun phrases).&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;topk&lt;/code&gt; and &lt;code&gt;threshold&lt;/code&gt; control the number of predictions and the confidence threshold.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Google Golab Notebook&lt;/h3&gt; &#xA;&lt;p&gt;We sincerely thank &lt;a href=&#34;https://github.com/onuralpszr&#34;&gt;Onuralp&lt;/a&gt; for sharing the &lt;a href=&#34;https://colab.research.google.com/drive/1F_7S5lSaFM06irBCZqjhbN7MpUXo6WwO?usp=sharing&#34;&gt;Colab Demo&lt;/a&gt;, you can have a try üòäÔºÅ&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;We sincerely thank &lt;a href=&#34;https://github.com/open-mmlab/mmyolo&#34;&gt;mmyolo&lt;/a&gt;, &lt;a href=&#34;https://github.com/open-mmlab/mmdetection&#34;&gt;mmdetection&lt;/a&gt;, &lt;a href=&#34;https://github.com/microsoft/GLIP&#34;&gt;GLIP&lt;/a&gt;, and &lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;transformers&lt;/a&gt; for providing their wonderful code to the community!&lt;/p&gt; &#xA;&lt;h2&gt;Citations&lt;/h2&gt; &#xA;&lt;p&gt;If you find YOLO-World is useful in your research or applications, please consider giving us a star üåü and citing it.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{cheng2024yolow,&#xA;  title={YOLO-World: Real-Time Open-Vocabulary Object Detection},&#xA;  author={Cheng, Tianheng and Song, Lin and Ge, Yixiao and Liu, Wenyu and Wang, Xinggang and Shan, Ying},&#xA;  journal={arXiv preprint arXiv:2401.17270},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Licence&lt;/h2&gt; &#xA;&lt;p&gt;YOLO-World is under the GPL-v3 Licence and is supported for comercial usage.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>wilson1yan/VideoGPT</title>
    <updated>2024-03-07T01:21:37Z</updated>
    <id>tag:github.com,2024-03-07:/wilson1yan/VideoGPT</id>
    <link href="https://github.com/wilson1yan/VideoGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;VideoGPT: Video Generation using VQ-VAE and Transformers&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2104.10157&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://wilson1yan.github.io/videogpt/index.html&#34;&gt;[Website]&lt;/a&gt;&lt;a href=&#34;https://colab.research.google.com/github/wilson1yan/VideoGPT/blob/master/notebooks/Using_VideoGPT.ipynb&#34;&gt;[Colab]&lt;/a&gt; Integrated to &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces&lt;/a&gt; with &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;. See demo: &lt;a href=&#34;https://huggingface.co/spaces/akhaliq/VideoGPT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;We present VideoGPT: a conceptually simple architecture for scaling likelihood based generative modeling to natural videos. VideoGPT uses VQ-VAE that learns downsampled discrete latent representations of a raw video by employing 3D convolutions and axial self-attention. A simple GPT-like architecture is then used to autoregressively model the discrete latents using spatio-temporal position encodings. Despite the simplicity in formulation and ease of training, our architecture is able to generate samples competitive with state-of-the-art GAN models for video generation on the BAIR Robot dataset, and generate high fidelity natural images from UCF-101 and Tumbler GIF Dataset (TGIF). We hope our proposed architecture serves as a reproducible reference for a minimalistic implementation of transformer based video generation models.&lt;/p&gt; &#xA;&lt;h2&gt;Approach&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wilson1yan/VideoGPT/master/VideoGPT.png&#34; alt=&#34;VideoGPT&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Change the &lt;code&gt;cudatoolkit&lt;/code&gt; version compatible to your machine.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda install --yes -c conda-forge cudatoolkit=11.0 cudnn&#xA;pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html&#xA;pip install git+https://github.com/wilson1yan/VideoGPT.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Sparse Attention (Optional)&lt;/h3&gt; &#xA;&lt;p&gt;For limited compute scenarios, it may be beneficial to use &lt;a href=&#34;https://arxiv.org/abs/1904.10509&#34;&gt;sparse attention&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt-get install llvm-9-dev&#xA;DS_BUILD_SPARSE_ATTN=1 pip install deepspeed&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After installng &lt;code&gt;deepspeed&lt;/code&gt;, you can train a sparse transformer by setting the flag &lt;code&gt;--attn_type sparse&lt;/code&gt; in &lt;code&gt;scripts/train_videogpt.py&lt;/code&gt;. The default supported sparsity configuration is an N-d strided sparsity layout, however, you can write your own arbitrary layouts to use.&lt;/p&gt; &#xA;&lt;h2&gt;Dataset&lt;/h2&gt; &#xA;&lt;p&gt;The default code accepts data as an HDF5 file with the specified format in &lt;code&gt;videogpt/data.py&lt;/code&gt;. An example of such a dataset can be constructed from the BAIR Robot data by running the script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sh scripts/preprocess/bair/create_bair_dataset.sh datasets/bair&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, the code supports a dataset with the following directory structure:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;video_dataset/&#xA;    train/&#xA;        class_0/&#xA;            video1.mp4&#xA;            video2.mp4&#xA;            ...&#xA;        class_1/&#xA;            video1.mp4&#xA;            ...&#xA;        ...&#xA;        class_n/&#xA;            ...&#xA;    test/&#xA;        class_0/&#xA;            video1.mp4&#xA;            video2.mp4&#xA;            ...&#xA;        class_1/&#xA;            video1.mp4&#xA;            ...&#xA;        ...&#xA;        class_n/&#xA;            ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;An example of such a dataset can be constructed from &lt;a href=&#34;https://www.crcv.ucf.edu/data/UCF101.php&#34;&gt;UCF-101&lt;/a&gt; data by running the script&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sh scripts/preprocess/ucf101/create_ucf_dataset.sh datasets/ucf101&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You may need to install &lt;code&gt;unrar&lt;/code&gt; and &lt;code&gt;unzip&lt;/code&gt; for the code to work correctly.&lt;/p&gt; &#xA;&lt;p&gt;If you do not care about classes, the class folders are not necessary and the dataset file structure can be collapsed into &lt;code&gt;train&lt;/code&gt; and &lt;code&gt;test&lt;/code&gt; directories of just videos.&lt;/p&gt; &#xA;&lt;h2&gt;Using Pretrained VQ-VAEs&lt;/h2&gt; &#xA;&lt;p&gt;There are four available pre-trained VQ-VAE models. All strides listed with each model are downsampling amounts across THW for the encoders.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;bair_stride4x2x2&lt;/code&gt;: trained on 16 frame 64 x 64 videos from the BAIR Robot Pushing dataset&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ucf101_stride4x4x4&lt;/code&gt;: trained on 16 frame 128 x 128 videos from UCF-101&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;kinetics_stride4x4x4&lt;/code&gt;: trained on 16 frame 128 x 128 videos from Kinetics-600&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;kinetics_stride2x4x4&lt;/code&gt;: trained on 16 frame 128 x 128 videos from Kinetics-600, with 2x larger temporal latent codes (achieves slightly better reconstruction)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from torchvision.io import read_video&#xA;from videogpt import load_vqvae&#xA;from videogpt.data import preprocess&#xA;&#xA;video_filename = &#39;path/to/video_file.mp4&#39;&#xA;sequence_length = 16&#xA;resolution = 128&#xA;device = torch.device(&#39;cuda&#39;)&#xA;&#xA;vqvae = load_vqvae(&#39;kinetics_stride2x4x4&#39;)&#xA;video = read_video(video_filename, pts_unit=&#39;sec&#39;)[0]&#xA;video = preprocess(video, resolution, sequence_length).unsqueeze(0).to(device)&#xA;&#xA;encodings = vqvae.encode(video)&#xA;video_recon = vqvae.decode(encodings)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training VQ-VAE&lt;/h2&gt; &#xA;&lt;p&gt;Use the &lt;code&gt;scripts/train_vqvae.py&lt;/code&gt; script to train a VQ-VAE. Execute &lt;code&gt;python scripts/train_vqvae.py -h&lt;/code&gt; for information on all available training settings. A subset of more relevant settings are listed below, along with default values.&lt;/p&gt; &#xA;&lt;h3&gt;VQ-VAE Specific Settings&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--embedding_dim&lt;/code&gt;: number of dimensions for codebooks embeddings&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--n_codes 2048&lt;/code&gt;: number of codes in the codebook&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--n_hiddens 240&lt;/code&gt;: number of hidden features in the residual blocks&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--n_res_layers 4&lt;/code&gt;: number of residual blocks&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--downsample 4 4 4&lt;/code&gt;: T H W downsampling stride of the encoder&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Training Settings&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--gpus 2&lt;/code&gt;: number of gpus for distributed training&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--sync_batchnorm&lt;/code&gt;: uses &lt;code&gt;SyncBatchNorm&lt;/code&gt; instead of &lt;code&gt;BatchNorm3d&lt;/code&gt; when using &amp;gt; 1 gpu&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--gradient_clip_val 1&lt;/code&gt;: gradient clipping threshold for training&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--batch_size 16&lt;/code&gt;: batch size per gpu&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--num_workers 8&lt;/code&gt;: number of workers for each DataLoader&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Dataset Settings&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--data_path &amp;lt;path&amp;gt;&lt;/code&gt;: path to an &lt;code&gt;hdf5&lt;/code&gt; file or a folder containing &lt;code&gt;train&lt;/code&gt; and &lt;code&gt;test&lt;/code&gt; folders with subdirectories of videos&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--resolution 128&lt;/code&gt;: spatial resolution to train on&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--sequence_length 16&lt;/code&gt;: temporal resolution, or video clip length&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Using Pretrained VideoGPTs&lt;/h2&gt; &#xA;&lt;p&gt;There are two available pre-trained VideoGPT models&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;bair_gpt&lt;/code&gt;: single frame-conditional BAIR model using discrete encodings from &lt;code&gt;bair_stride4x2x2&lt;/code&gt; VQ-VAE&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ucf101_uncond_gpt&lt;/code&gt;: unconditional UCF101 model using discrete encodings from &lt;code&gt;ucf101_stride4x4x4&lt;/code&gt; VQ-VAE Note that both pre-trained models use sparse attention. For purposes of fine-tuning, you will need to install sparse attention, however, sampling does not required sparse attention to be installed.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Training VideoGPT&lt;/h2&gt; &#xA;&lt;p&gt;You can download a pretrained VQ-VAE, or train your own. Afterwards, use the &lt;code&gt;scripts/train_videogpt.py&lt;/code&gt; script to train an VideoGPT model for sampling. Execute &lt;code&gt;python scripts/train_videogpt.py -h&lt;/code&gt; for information on all available training settings. A subset of more relevant settings are listed below, along with default values.&lt;/p&gt; &#xA;&lt;h3&gt;VideoGPT Specific Settings&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--vqvae kinetics_stride4x4x4&lt;/code&gt;: path to a vqvae checkpoint file, OR a pretrained model name to download. Available pretrained models are: &lt;code&gt;bair_stride4x2x2&lt;/code&gt;, &lt;code&gt;ucf101_stride4x4x4&lt;/code&gt;, &lt;code&gt;kinetics_stride4x4x4&lt;/code&gt;, &lt;code&gt;kinetics_stride2x4x4&lt;/code&gt;. BAIR was trained on 64 x 64 videos, and the rest on 128 x 128 videos&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--n_cond_frames 0&lt;/code&gt;: number of frames to condition on. &lt;code&gt;0&lt;/code&gt; represents a non-frame conditioned model&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--class_cond&lt;/code&gt;: trains a class conditional model if activated&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--hidden_dim 576&lt;/code&gt;: number of transformer hidden features&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--heads 4&lt;/code&gt;: number of heads for multihead attention&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--layers 8&lt;/code&gt;: number of transformer layers&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--dropout 0.2&#39;&lt;/code&gt;: dropout probability applied to features after attention and positionwise feedforward layers&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--attn_type full&lt;/code&gt;: &lt;code&gt;full&lt;/code&gt; or &lt;code&gt;sparse&lt;/code&gt; attention. Refer to the Installation section for install sparse attention&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--attn_dropout 0.3&lt;/code&gt;: dropout probability applied to the attention weight matrix&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Training Settings&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--gpus 4&lt;/code&gt;: number of gpus for distributed training&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--gradient_clip_val 1&lt;/code&gt;: gradient clipping threshold for training&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--batch_size 8&lt;/code&gt;: batch size per gpu&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--num_workers 2&lt;/code&gt;: number of workers for each DataLoader&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--amp_level O1&lt;/code&gt;: for mixed precision training&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--precision 16&lt;/code&gt;: for mixed precision training&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Dataset Settings&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--data_path &amp;lt;path&amp;gt;&lt;/code&gt;: path to an &lt;code&gt;hdf5&lt;/code&gt; file or a folder containing &lt;code&gt;train&lt;/code&gt; and &lt;code&gt;test&lt;/code&gt; folders with subdirectories of videos&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--resolution 128&lt;/code&gt;: spatial resolution to train on&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--sequence_length 16&lt;/code&gt;: temporal resolution, or video clip length&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Sampling VideoGPT&lt;/h2&gt; &#xA;&lt;p&gt;VideoGPT models can be sampled using the &lt;code&gt;scripts/sample_videogpt.py&lt;/code&gt;. You can specify a path to a checkpoint during training, or the name of a pretrained model. You may need to install &lt;code&gt;ffmpeg&lt;/code&gt;: &lt;code&gt;sudo apt-get install ffmpeg&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;Evaluation is done primarily using &lt;a href=&#34;https://arxiv.org/abs/1812.01717&#34;&gt;Frechet Video Distance (FVD)&lt;/a&gt; for BAIR and Kinetics, and &lt;a href=&#34;https://arxiv.org/abs/1606.03498&#34;&gt;Inception Score&lt;/a&gt; for UCF-101. Inception Score can be computed by generating samples and using the code from the &lt;a href=&#34;https://github.com/pfnet-research/tgan2&#34;&gt;TGANv2 repo&lt;/a&gt;. FVD can be computed through &lt;code&gt;python scripts/compute_fvd.py&lt;/code&gt;, which runs a PyTorch-ported version of the &lt;a href=&#34;https://github.com/google-research/google-research/tree/master/frechet_video_distance&#34;&gt;original codebase&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Reproducing Paper Results&lt;/h2&gt; &#xA;&lt;p&gt;Note that this repo is primarily designed for simplicity and extending off of our method. Reproducing the full paper results can be done using code found at a &lt;a href=&#34;https://github.com/wilson1yan/VideoGPT-Paper&#34;&gt;separate repo&lt;/a&gt;. However, be aware that the code is not as clean.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Please consider using the follow citation when using our code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{yan2021videogpt,&#xA;      title={VideoGPT: Video Generation using VQ-VAE and Transformers}, &#xA;      author={Wilson Yan and Yunzhi Zhang and Pieter Abbeel and Aravind Srinivas},&#xA;      year={2021},&#xA;      eprint={2104.10157},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>