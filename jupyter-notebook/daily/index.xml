<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-23T01:39:46Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>mlabonne/llm-course</title>
    <updated>2023-06-23T01:39:46Z</updated>
    <id>tag:github.com,2023-06-23:/mlabonne/llm-course</id>
    <link href="https://github.com/mlabonne/llm-course" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Course with a roadmap and notebooks to get into Large Language Models (LLMs).&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üó£Ô∏è Large Language Model Course&lt;/h1&gt; &#xA;&lt;h2&gt;Notebooks&lt;/h2&gt; &#xA;&lt;p&gt;A list of notebooks and articles related to large language models.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Notebook&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Article&lt;/th&gt; &#xA;   &lt;th&gt;Notebook&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Decoding Strategies in Large Language Models&lt;/td&gt; &#xA;   &lt;td&gt;A guide to text generation from beam search to nucleus sampling&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://mlabonne.github.io/blog/posts/2022-06-07-Decoding_strategies.html&#34;&gt;Article&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/19CJlOS5lI29g-B3dziNn93Enez1yiHk2?usp=sharing&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mlabonne/llm-course/main/images/colab.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Visualizing GPT-2&#39;s Loss Landscape&lt;/td&gt; &#xA;   &lt;td&gt;3D plot of the loss landscape based on weight pertubations.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://twitter.com/maximelabonne/status/1667618081844219904&#34;&gt;Tweet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1Fu1jikJzFxnSPzR_V2JJyDVWWJNXssaL?usp=sharing&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mlabonne/llm-course/main/images/colab.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Improve ChatGPT with Knowledge Graphs&lt;/td&gt; &#xA;   &lt;td&gt;Augment ChatGPT&#39;s answers with knowledge graphs.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://mlabonne.github.io/blog/posts/Article_Improve_ChatGPT_with_Knowledge_Graphs.html&#34;&gt;Article&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1mwhOSw9Y9bgEaIFKT4CLi0n18pXRM4cj?usp=sharing&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mlabonne/llm-course/main/images/colab.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;p&gt;A step-by-step guide on how to get into large language models with learning resources.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mlabonne/llm-course/main/images/roadmap.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;1. Mathematics for Machine Learning&lt;/h3&gt; &#xA;&lt;p&gt;Before mastering machine learning, it is important to understand the fundamental mathematical concepts that power these algorithms.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Linear Algebra&lt;/strong&gt;: This is crucial for understanding many algorithms, especially those used in deep learning. Key concepts include vectors, matrices, determinants, eigenvalues and eigenvectors, vector spaces, and linear transformations.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Calculus&lt;/strong&gt;: Many machine learning algorithms involve the optimization of continuous functions, which requires an understanding of derivatives, integrals, limits, and series. Multivariable calculus and the concept of gradients are also important.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Probability and Statistics&lt;/strong&gt;: These are crucial for understanding how models learn from data and make predictions. Key concepts include probability theory, random variables, probability distributions, expectations, variance, covariance, correlation, hypothesis testing, confidence intervals, maximum likelihood estimation, and Bayesian inference.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìö Resources:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=fNk_zzaMoSs&amp;amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&#34;&gt;3Blue1Brown - The Essence of Linear Algebra&lt;/a&gt;: Series of videos that give a geometric intuition to these concepts.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=qBigTkBLU6g&amp;amp;list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9&#34;&gt;StatQuest with Josh Starmer - Statistics Fundamentals&lt;/a&gt;: Offers simple and clear explanations for many statistical concepts.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://automata88.medium.com/list/cacc224d5e7d&#34;&gt;AP Statistics Intuition by Ms Aerin&lt;/a&gt;: List of Medium articles that provide the intuition behind every probability distribution.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://immersivemath.com/ila/learnmore.html&#34;&gt;Immersive Linear Algebra&lt;/a&gt;: Another visual interpretation of linear algebra.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.khanacademy.org/math/linear-algebra&#34;&gt;Khan Academy - Linear Algebra&lt;/a&gt;: Great for beginners as it explains the concepts in a very intuitive way.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.khanacademy.org/math/calculus-1&#34;&gt;Khan Academy - Calculus&lt;/a&gt;: An interactive course that covers all the basics of calculus.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.khanacademy.org/math/statistics-probability&#34;&gt;Khan Academy - Probability and Statistics&lt;/a&gt;: Delivers the material in an easy-to-understand format.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;2. Python for Machine Learning&lt;/h3&gt; &#xA;&lt;p&gt;Python is a powerful and flexible programming language that&#39;s particularly good for machine learning, thanks to its readability, consistency, and robust ecosystem of data science libraries.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Python Basics&lt;/strong&gt;: Understanding of Python&#39;s basic syntax, data types, error handling, and object-oriented programming is crucial.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Data Science Libraries&lt;/strong&gt;: Familiarity with NumPy for numerical operations, Pandas for data manipulation and analysis, Matplotlib and Seaborn for data visualization is a must.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Data Preprocessing&lt;/strong&gt;: This involves feature scaling and normalization, handling missing data, outlier detection, categorical data encoding, and splitting data into training, validation, and test sets.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Machine Learning Libraries&lt;/strong&gt;: Proficiency with Scikit-learn, a library providing a wide selection of supervised and unsupervised learning algorithms, is vital. Understanding how to implement algorithms like linear regression, logistic regression, decision trees, random forests, k-nearest neighbors (K-NN), and K-means clustering is important. Dimensionality reduction techniques like PCA and t-SNE are also very helpful for visualizing high-dimensional data.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìö Resources:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://realpython.com/&#34;&gt;Real Python&lt;/a&gt;: A comprehensive resource with articles and tutorials for both beginner and advanced Python concepts.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=rfscVS0vtbw&#34;&gt;freeCodeCamp - Learn Python&lt;/a&gt;: Long video that provides a full introduction into all of the core concepts in Python.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jakevdp.github.io/PythonDataScienceHandbook/&#34;&gt;Python Data Science Handbook&lt;/a&gt;: Free digital book that is a great resource for learning pandas, NumPy, matplotlib, and Seaborn.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtu.be/i_LwzRVP7bg&#34;&gt;freeCodeCamp - Machine Learning for Everybody&lt;/a&gt;: Practical introduction to different machine learning algorithms for beginners.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.udacity.com/course/intro-to-machine-learning--ud120&#34;&gt;Udacity - Intro to Machine Learning&lt;/a&gt;: Free course that covers PCA and several other machine learning concepts.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;3. Neural Networks&lt;/h3&gt; &#xA;&lt;p&gt;Neural networks are a fundamental part of many machine learning models, particularly in the realm of deep learning. To utilize them effectively, a comprehensive understanding of their design and mechanics is essential.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fundamentals&lt;/strong&gt;: This includes understanding the structure of a neural network such as layers, weights, biases, activation functions (sigmoid, tanh, ReLU, etc.)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Training and Optimization&lt;/strong&gt;: Familiarize yourself with backpropagation and different types of loss functions, like Mean Squared Error (MSE) and Cross-Entropy. Understand various optimization algorithms like Gradient Descent, Stochastic Gradient Descent, RMSprop, and Adam.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Overfitting&lt;/strong&gt;: It&#39;s crucial to comprehend the concept of overfitting (where a model performs well on training data but poorly on unseen data) and various regularization techniques to prevent it. Techniques include dropout, L1/L2 regularization, early stopping, and data augmentation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Implement a Multilayer Perceptron (MLP)&lt;/strong&gt;: Build an MLP, also known as a fully connected network, using PyTorch.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìö Resources:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=aircAruvnKk&#34;&gt;3Blue1Brown - But what is a Neural Network?&lt;/a&gt;: This video gives an intuitive explanation of neural networks and their inner workings.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=VyWAvY2CF9c&#34;&gt;freeCodeCamp - Deep Learning Crash Course&lt;/a&gt;: This video efficiently introduces all the most important concepts in deep learning.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://course.fast.ai/&#34;&gt;Fast.ai - Practical Deep Learning&lt;/a&gt;: Free course designed for people with coding experience who want to learn about deep learning.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&#34;&gt;Patrick Loeber - PyTorch Tutorials&lt;/a&gt;: Series of videos for complete beginners to learn about PyTorch.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;4. Natural Language Processing (NLP)&lt;/h3&gt; &#xA;&lt;p&gt;NLP is a fascinating branch of artificial intelligence that bridges the gap between human language and machine understanding. From simple text processing to understanding linguistic nuances, NLP plays a crucial role in many applications like translation, sentiment analysis, chatbots, and much more.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Text Preprocessing&lt;/strong&gt;: Learn various text preprocessing steps like tokenization (splitting text into words or sentences), stemming (reducing words to their root form), lemmatization (similar to stemming but considers the context), stop word removal, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Feature Extraction Techniques&lt;/strong&gt;: Become familiar with techniques to convert text data into a format that can be understood by machine learning algorithms. Key methods include Bag-of-words (BoW), Term Frequency-Inverse Document Frequency (TF-IDF), and n-grams.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Word Embeddings&lt;/strong&gt;: Word embeddings are a type of word representation that allows words with similar meanings to have similar representations. Key methods include Word2Vec, GloVe, and FastText.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Recurrent Neural Networks (RNNs)&lt;/strong&gt;: Understand the working of RNNs, a type of neural network designed to work with sequence data. Explore LSTMs and GRUs, two RNN variants that are capable of learning long-term dependencies.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìö Resources:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://realpython.com/natural-language-processing-spacy-python/&#34;&gt;RealPython - NLP with spaCy in Python&lt;/a&gt;: Exhaustive guide about the spaCy library for NLP tasks in Python.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.kaggle.com/learn-guide/natural-language-processing&#34;&gt;Kaggle - NLP Guide&lt;/a&gt;: A few notebooks and resources for a hands-on explanation of NLP in Python.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jalammar.github.io/illustrated-word2vec/&#34;&gt;Jay Alammar - The Illustration Word2Vec&lt;/a&gt;: A good reference to understand the famous Word2Vec architecture.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jaketae.github.io/study/pytorch-rnn/&#34;&gt;Jake Tae - PyTorch RNN from Scratch&lt;/a&gt;: Practical and simple implementation of RNN, LSTM, and GRU models in PyTorch.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colah.github.io/posts/2015-08-Understanding-LSTMs/&#34;&gt;colah&#39;s blog - Understanding LSTM Networks&lt;/a&gt;: A more theoretical article about the LSTM network.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;5. The Transformer Architecture&lt;/h3&gt; &#xA;&lt;p&gt;The Transformer model, introduced in the &#34;Attention is All You Need&#34; paper, is the neural network architecture at the core of large language models. The original paper is difficult to read and eveb contains some mistakes, which is why alternative resources are recommended.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Attention Mechanisms&lt;/strong&gt;: Grasp the theory behind attention mechanisms, including self-attention and scaled dot-product attention, which allows the model to focus on different parts of the input when producing an output.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tokenization&lt;/strong&gt;: Understand how to convert raw text data into a format that the model can understand, which involves splitting the text into tokens (usually words or subwords).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Transformer Architecture&lt;/strong&gt;: Dive deeper into the architecture of Transformers, learning about their various components such as positional encoding, multi-head attention, feed-forward networks, and normalization layers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Decoding Strategies&lt;/strong&gt;: Learn about the different ways the model can generate output sequences. Common strategies include greedy decoding, beam search, and top-k sampling.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìö Resources:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jalammar.github.io/illustrated-transformer/&#34;&gt;The Illustrated Transformer by Jay Alammar&lt;/a&gt;: This is a visual and intuitive explanation of the Transformer model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/learn/nlp-course/&#34;&gt;Hugging Face - NLP Course&lt;/a&gt;: An excellent mini-course that goes beyond the Transformer architecture.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nlp.seas.harvard.edu/annotated-transformer/&#34;&gt;Harvard - The Annotated Transformer&lt;/a&gt;: An excellent in-depth article about the original Transformer paper.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=AFkGPmU16QA&#34;&gt;Introduction to the Transformer by Rachel Thomas&lt;/a&gt;: Provides a good intuition behind the main ideas of the Transformer architecture.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=ptuGllU5SQQ&#34;&gt;Stanford CS224N - Transformers&lt;/a&gt;: A more academic presentation of this architecture.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;6. Pre-trained Language Models&lt;/h3&gt; &#xA;&lt;p&gt;Pre-trained models like BERT, GPT-2, and T5 are powerful tools that can handle tasks like sequence classification, text generation, text summarization, and question answering.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;BERT&lt;/strong&gt;: Understand BERT&#39;s architecture, including the concept of bidirectional training, which distinguishes it from previous models. Learn about fine-tuning BERT for tasks like sequence classification, named entity recognition, and question answering.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;GPT-2&lt;/strong&gt;: Understand GPT-2&#39;s decoder-only architecture and its pre-training objective. Learn to use it for text generation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;T5&lt;/strong&gt;: the T5 model is a highly versatile model for tasks ranging from text classification to translation to summarization. Understand how to train and use T5 for multiple tasks, and learn about the &#34;prefix-tuning&#34; approach used with T5.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLM Evaluation&lt;/strong&gt;: Learn how to evaluate the performance of these models on your specific task, including appropriate metrics for different tasks such as accuracy, F1 score, BLEU score, or perplexity.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìö Resources:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jalammar.github.io/illustrated-bert/&#34;&gt;The Illustrated BERT by Jay Alammar&lt;/a&gt;: Another excellent visual guide to the BERT architecture.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/transformers/v4.30.0/en/model_doc/bert#overview&#34;&gt;Hugging Face - BERT&lt;/a&gt;: Overview and list of practical resources for various tasks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jalammar.github.io/illustrated-gpt2/&#34;&gt;The Illustrated GPT-2 by Jay Alammar&lt;/a&gt;: In-depth illustrated guide to the GPT-2 architecture.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1910.10683&#34;&gt;T5 paper&lt;/a&gt;: The original paper that introduced the T5 model and many essential concepts for modern NLP.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/transformers/notebooks&#34;&gt;Hugging Face - Transformers Notebooks&lt;/a&gt;: List of official notebooks provided by Hugging Face.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/metrics&#34;&gt;Hugging Face - Metrics&lt;/a&gt;: All metrics on the Hugging Face hub.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;7. Advanced Language Modeling&lt;/h3&gt; &#xA;&lt;p&gt;To fine-tune your skills, learn how to create embeddings with sentence transformers, store them in a vector database, and use parameter-efficient supervised learning or RLHF to fine-tune LLMs.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Sentence Transformers&lt;/strong&gt;: Sentence Transformers are models that can derive semantically meaningful embeddings for sentences, paragraphs, or texts. Learn how to store and retrieve these embeddings using an appropriate vector database for rapid similarity search.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fine-Tuning Language Models&lt;/strong&gt;: After understanding and using pre-trained models, the next step is fine-tuning them on a domain-specific dataset. It allows the model to be more accurate for certain tasks or domains, such as medical text analysis or sentiment analysis for movie reviews.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Parameter-Efficient Learning Techniques&lt;/strong&gt;: Explore more efficient ways to train or fine-tune your models without requiring massive amounts of data or computational resources, such as LoRA.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìö Resources:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.sbert.net/&#34;&gt;SBERT.net&lt;/a&gt;: Python library to implement sentence transformers, with a lot of examples.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.pinecone.io/learn/sentence-embeddings/&#34;&gt;Pinecone - Sentence Transformers&lt;/a&gt;: Mini-book about NLP for semantic search in general.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/blog/rlhf&#34;&gt;Hugging Face - RLHF&lt;/a&gt;: Blog post introducing the concept of RLHF.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/blog/peft&#34;&gt;Hugging Face - PEFT&lt;/a&gt;: Another library from Hugging Face implementing different techniques, such as LoRA.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.philschmid.de/fine-tune-flan-t5-peft&#34;&gt;Efficient LLM training by Phil Schmid&lt;/a&gt;: Implementation of LoRA to fine-tune a Flan-T5 model.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;8. LMOps&lt;/h3&gt; &#xA;&lt;p&gt;Finally, dive into Language Model Operations (LMOps), learn how to handle prompt engineering, build frameworks with Langchain and Llamaindex, and optimize inference with weight quantization, pruning, distillation, and more.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fine-tuning LLaMA&lt;/strong&gt;: Instruction fine-tuning has become extremely popular since the (accidental) release of LLaMA. The size of these models and the peculiarities of training them on instructions and answers introduce more complexity and often require parameter-efficient learning techniques such as QLoRA.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Build LLM Frameworks&lt;/strong&gt;: LLMs are a new building block in system design, where the rest of the architecture is handled by libraries such as LangChain and LlamaIndex, allowing you to query vector databases, improving the model&#39;s memory or providing various tools.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Optimization Techniques for Inference&lt;/strong&gt;: As the size of LLMs grows, it becomes increasingly important to apply optimization techniques to ensure that the models can be efficiently used for inference. Techniques include weight quantization (4-bit, 3-bit), pruning, knowledge distillation, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLM deployment&lt;/strong&gt;: These models can be deployed locally like &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; or in the cloud like Hugging Face&#39;s &lt;a href=&#34;https://github.com/huggingface/text-generation-inference&#34;&gt;text generation inference&lt;/a&gt; or &lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;vLLM&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìö Resources:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.mlexpert.io/machine-learning/tutorials/alpaca-fine-tuning&#34;&gt;MLExpert - Fine-tuning Alpaca&lt;/a&gt;: Guide to fine-tune LLaMA on a custom dataset.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/blog/hf-bitsandbytes-integration&#34;&gt;Hugging Face - LLM.int8()&lt;/a&gt;: Introduction to 8-bit matrix multiplication with LLM.int8().&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/blog/4bit-transformers-bitsandbytes&#34;&gt;Hugging Face - QLoRA&lt;/a&gt;: Blog post introducing QLoRA with notebooks to test it.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.kanaries.net/tutorials/ChatGPT/auto-gptq&#34;&gt;Kanaries - AutoGPTQ&lt;/a&gt;: Simple guide to use AutoGPTQ.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://a16z.com/2023/06/20/emerging-architectures-for-llm-applications/&#34;&gt;Emerging Architectures for LLM Applications&lt;/a&gt;: overview of the LLM app stack.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.pinecone.io/learn/langchain-intro/&#34;&gt;Pinecone - LangChain AI Handbook&lt;/a&gt;: Excellent free book on how to master the LangChain library.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gpt-index.readthedocs.io/en/latest/guides/primer.html&#34;&gt;A Primer to using LlamaIndex&lt;/a&gt;: Official guides to learn more about LlamaIndex.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Acknowledgements&lt;/h3&gt; &#xA;&lt;p&gt;This roadmap was inspired by the excellent &lt;a href=&#34;https://github.com/milanm/DevOps-Roadmap&#34;&gt;DevOps Roadmap&lt;/a&gt; from Milan Milanoviƒá and Romano Roth.&lt;/p&gt; &#xA;&lt;p&gt;Special thanks to Thomas Thelen for motivating me to create a roadmap, and Andr√© Frade for his input and review of the first draft.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Disclaimer: I am not affiliated with any sources listed here.&lt;/em&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/home-robot</title>
    <updated>2023-06-23T01:39:46Z</updated>
    <id>tag:github.com,2023-06-23:/facebookresearch/home-robot</id>
    <link href="https://github.com/facebookresearch/home-robot" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Mobile manipulation research tools for roboticists&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/home-robot/main/docs/HomeRobot_Logo_Horiz_Color_white_bg.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/facebookresearch/home-robot/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true&#34; alt=&#34;GitHub license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.python.org/downloads/release/python-370/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.9-blue.svg?sanitize=true&#34; alt=&#34;Python 3.9&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://dl.circleci.com/status-badge/redirect/gh/facebookresearch/home-robot/tree/main&#34;&gt;&lt;img src=&#34;https://dl.circleci.com/status-badge/img/gh/facebookresearch/home-robot/tree/main.svg?style=shield&#34; alt=&#34;CircleCI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/pre-commit/pre-commit&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&amp;amp;logoColor=white&#34; alt=&#34;pre-commit&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/psf/black&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true&#34; alt=&#34;Code style: black&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://timothycrosley.github.io/isort/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&#34; alt=&#34;Imports: isort&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Your open-source robotic mobile manipulation stack!&lt;/p&gt; &#xA;&lt;p&gt;Check out the &lt;a href=&#34;https://aihabitat.org/challenge/2023_homerobot_ovmm/&#34;&gt;Neurips 2023 HomeRobot Open-Vocabulary Mobile Manipulation Challenge!&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;HomeRobot lets you get started running a range of robotics tasks on a low-cost mobile manipulator, starting with &lt;em&gt;Open Vocabulary Mobile Manipulation&lt;/em&gt;, or OVMM. OVMM is a challenging task which means that, in an unknown environment, a robot must:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Explore its environment&lt;/li&gt; &#xA; &lt;li&gt;Find an object&lt;/li&gt; &#xA; &lt;li&gt;Find a receptacle -- a location on which it must place this object&lt;/li&gt; &#xA; &lt;li&gt;Put the object down on the receptacle.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Core Concepts&lt;/h2&gt; &#xA;&lt;p&gt;This package assumes you have a low-cost mobile robot with limited compute -- initially a &lt;a href=&#34;https://hello-robot.com/stretch-2&#34;&gt;Hello Robot Stretch&lt;/a&gt; -- and a &#34;workstation&#34; with more GPU compute. Both are assumed to be running on the same network.&lt;/p&gt; &#xA;&lt;p&gt;This is the recommended workflow for hardware robots:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Turn on your robot; for the Stretch, run &lt;code&gt;stretch_robot_home.py&lt;/code&gt; to get it ready to use.&lt;/li&gt; &#xA; &lt;li&gt;From your workstation, SSH into the robot and start a &lt;a href=&#34;http://wiki.ros.org/roslaunch&#34;&gt;ROS launch file&lt;/a&gt; which brings up necessary low-level control and hardware drivers.&lt;/li&gt; &#xA; &lt;li&gt;If desired, run &lt;a href=&#34;http://wiki.ros.org/rviz&#34;&gt;rviz&lt;/a&gt; on the workstation to see what the robot is seeing.&lt;/li&gt; &#xA; &lt;li&gt;Start running your AI code on the workstation - For example, you can run &lt;code&gt;python projects/stretch_grasping/eval_episode.py&lt;/code&gt; to run the OVMM task.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We provide a couple connections for useful perception libraries like &lt;a href=&#34;https://github.com/facebookresearch/Detic&#34;&gt;Detic&lt;/a&gt; and &lt;a href=&#34;https://github.com/NVlabs/contact_graspnet&#34;&gt;Contact Graspnet&lt;/a&gt;, which you can then use as a part of your methods.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Preliminary&lt;/h3&gt; &#xA;&lt;p&gt;HomeRobot requires Python 3.9. Installation on a workstation requires &lt;a href=&#34;https://docs.conda.io/projects/conda/en/latest/user-guide/install/linux.html&#34;&gt;conda&lt;/a&gt; and &lt;a href=&#34;https://mamba.readthedocs.io/en/latest/user_guide/mamba.html&#34;&gt;mamba&lt;/a&gt;. Installation on a robot assumes Ubuntu 20.04 and &lt;a href=&#34;http://wiki.ros.org/noetic&#34;&gt;ROS Noetic&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To set up the hardware stack on a Hello Robot Stretch, see the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/home-robot/main/docs/install_robot.md&#34;&gt;ROS installation instructions&lt;/a&gt; in &lt;code&gt;home_robot_hw&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You may need a calibrated URDF for our inverse kinematics code to work well; see &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/home-robot/main/docs/calibration.md&#34;&gt;calibration notes&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Network Setup&lt;/h4&gt; &#xA;&lt;p&gt;Follow the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/home-robot/main/docs/network.md&#34;&gt;network setup guide&lt;/a&gt; to set up your robot to use the network, and make sure that it can communicate between workstation and robot via ROS. On the robot side, start up the controllers with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;roslaunch home_robot_hw startup_stretch_hector_slam.launch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Workstation Instructions&lt;/h3&gt; &#xA;&lt;p&gt;To set up your workstation, follow these instructions. We will assume that your system supports CUDA 11.8 or better for pytorch; earlier versions should be fine, but may require some changes to the conda environment.&lt;/p&gt; &#xA;&lt;h4&gt;1. Create Your Environment&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Create a conda env - use the version in home_robot_hw if you want to run on the robot&#xA;mamba env create -n home-robot -f src/home_robot_hw/environment.yml&#xA;&#xA;# Otherwise, use the version in src/home_robot&#xA;mamba env create -n home-robot -f src/home_robot/environment.yml&#xA;&#xA;conda activate home-robot&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This should install pytorch; if you run into trouble, you may need to edit the installation to make sure you have the right CUDA version. See the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/home-robot/main/docs/install_pytorch.md&#34;&gt;pytorch install notes&lt;/a&gt; for more.&lt;/p&gt; &#xA;&lt;h4&gt;2. Install Home Robot Packages&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda activate home-robot&#xA;&#xA;# Install the core home_robot package&#xA;python -m pip install -e src/home_robot&#xA;&#xA;Skip to step 4 if you do not have a real robot setup or if you only want to use our simulation stack.&#xA;&#xA;# Install home_robot_hw&#xA;python -m pip install -e src/home_robot_hw&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Testing Real Robot Setup:&lt;/em&gt; Now you can run a couple commands to test your connection. If the &lt;code&gt;roscore&lt;/code&gt; and the robot controllers are running properly, you can run &lt;code&gt;rostopic list&lt;/code&gt; and should see a list of topics - streams of information coming from the robot. You can then run RVIZ to visualize the robot sensor output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;rviz -d $HOME_ROBOT_ROOT/src/home_robot_hw/launch/mapping_demo.rviz&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;3. Hardware Testing&lt;/h4&gt; &#xA;&lt;p&gt;Run the hardware manual test to make sure you can control the robot remotely. Ensure the robot has one meter of free space before running the script.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python tests/hw_manual_test.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Follow the on-screen instructions. The robot should move through a set of configurations.&lt;/p&gt; &#xA;&lt;h4&gt;4. Download third-party packages&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;git submodule update --init --recursive assets/hab_stretch src/home_robot/home_robot/perception/detection/detic/Detic src/third_party/detectron2 src/third_party/contact_graspnet&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;5. Install Detic&lt;/h4&gt; &#xA;&lt;p&gt;Install &lt;a href=&#34;https://detectron2.readthedocs.io/tutorials/install.html&#34;&gt;detectron2&lt;/a&gt;. If you installed our default environment above, you may need to &lt;a href=&#34;https://developer.nvidia.com/cuda-11-7-0-download-archive&#34;&gt;download CUDA11.7&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#xA;Download Detic checkpoint as per the instructions [on the Detic github page](https://github.com/facebookresearch/Detic):&#xA;```bash&#xA;cd $HOME_ROBOT_ROOT/src/home_robot/home_robot/perception/detection/detic/Detic/&#xA;mkdir models&#xA;wget https://dl.fbaipublicfiles.com/detic/Detic_LCOCOI21k_CLIP_SwinB_896b32_4x_ft4x_max-size.pth -O models/Detic_LCOCOI21k_CLIP_SwinB_896b32_4x_ft4x_max-size.pth --no-check-certificate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You should be able to run the Detic demo script as per the Detic instructions to verify your installation was correct:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget https://web.eecs.umich.edu/~fouhey/fun/desk/desk.jpg&#xA;python demo.py --config-file configs/Detic_LCOCOI21k_CLIP_SwinB_896b32_4x_ft4x_max-size.yaml --input desk.jpg --output out2.jpg --vocabulary custom --custom_vocabulary headphone,webcam,paper,coffe --confidence-threshold 0.3 --opts MODEL.WEIGHTS models/Detic_LCOCOI21k_CLIP_SwinB_896b32_4x_ft4x_max-size.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;6. Download pretrained skills&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;mkdir -p data/checkpoints&#xA;cd data/checkpoints&#xA;wget https://dl.fbaipublicfiles.com/habitat/data/baselines/v1/ovmm_baseline_home_robot_challenge_2023.zip&#xA;unzip ovmm_baseline_home_robot_challenge_2023.zip&#xA;cd ../../&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;7. Simulation Setup&lt;/h4&gt; &#xA;&lt;p&gt;To set up the simulation stack with Habitat, train DDPPO skills and run evaluations: see the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/home-robot/main/src/home_robot_sim/README.md&#34;&gt;installation instructions&lt;/a&gt; in &lt;code&gt;home_robot_sim&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For more details on the OVMM challenge, see the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/home-robot/main/projects/habitat_ovmm/README.md&#34;&gt;Habitat OVMM readme&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;8. Run Open Vocabulary Mobile Manipulation on Stretch&lt;/h4&gt; &#xA;&lt;p&gt;You should then be able to run the Stretch OVMM example.&lt;/p&gt; &#xA;&lt;p&gt;Run a grasping server; either Contact Graspnet or our simple grasp server.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# For contact graspnet&#xA;cd $HOME_ROBOT_ROOT/src/third_party/contact_graspnet&#xA;conda activate contact_graspnet_env&#xA;python contact_graspnet/graspnet_ros_server.py  --local_regions --filter_grasps&#xA;&#xA;# For simple grasping server&#xA;cd $HOME_ROBOT_ROOT&#xA;conda activate home-robot&#xA;python src/home_robot_hw/home_robot_hw/nodes/simple_grasp_server.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you can run the OVMM example script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd $HOME_ROBOT_ROOT&#xA;python projects/real_world_ovmm/eval_episode.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Code Contribution&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions to HomeRobot.&lt;/p&gt; &#xA;&lt;p&gt;There are two main classes in HomeRobot that you need to be concerned with:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;Environments&lt;/em&gt; extend the &lt;a href=&#34;https://github.com/facebookresearch/home-robot/raw/main/src/home_robot/home_robot/core/abstract_env.py&#34;&gt;abstract Environment class&lt;/a&gt; and provide &lt;em&gt;observations&lt;/em&gt; of the world, and a way to &lt;em&gt;apply actions&lt;/em&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Agents&lt;/em&gt; extend the &lt;a href=&#34;https://github.com/facebookresearch/home-robot/raw/main/src/home_robot/home_robot/core/abstract_agent.py&#34;&gt;abstract Agent class&lt;/a&gt;, which takes in an &lt;a href=&#34;https://github.com/facebookresearch/home-robot/raw/main/src/home_robot/home_robot/core/interfaces.py#L95&#34;&gt;observation&lt;/a&gt; and produces an &lt;a href=&#34;https://github.com/facebookresearch/home-robot/raw/main/src/home_robot/home_robot/core/interfaces.py#L50&#34;&gt;action&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Generally, new methods will be implemented as Agents.&lt;/p&gt; &#xA;&lt;h3&gt;Developing on Hardware&lt;/h3&gt; &#xA;&lt;p&gt;See the robot &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/home-robot/main/docs/hardware_development.md&#34;&gt;hardware development guide&lt;/a&gt; for some advice that may make developing code on the Stretch easier.&lt;/p&gt; &#xA;&lt;h3&gt;Organization&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/facebookresearch/home-robot/&#34;&gt;HomeRobot&lt;/a&gt; is broken up into three different packages:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Resource&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/home-robot/main/src/home_robot&#34;&gt;home_robot&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Core package containing agents and interfaces&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/home-robot/main/src/home_robot_sim&#34;&gt;home_robot_sim&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;OVMM simulation environment based on &lt;a href=&#34;https://aihabitat.org/&#34;&gt;AI Habitat&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/home-robot/main/src/home_robot_hw&#34;&gt;home_robot_hw&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ROS package containing hardware interfaces for the Hello Robot Stretch&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/home-robot/main/src/home_robot&#34;&gt;home_robot&lt;/a&gt; package contains embodiment-agnostic agent code, such as our &lt;a href=&#34;https://github.com/facebookresearch/home-robot/raw/main/src/home_robot/home_robot/agent/objectnav_agent/objectnav_agent.py&#34;&gt;ObjectNav agent&lt;/a&gt; (finds objects in scenes) and our &lt;a href=&#34;https://github.com/facebookresearch/home-robot/raw/main/src/home_robot/home_robot/agent/ovmm_agent/ovmm_agent.py&#34;&gt;hierarchical OVMM agent&lt;/a&gt;. These agents can be extended or modified to implement your own solution.&lt;/p&gt; &#xA;&lt;p&gt;Importantly, agents use a fixed set of &lt;a href=&#34;https://github.com/facebookresearch/home-robot/raw/main/src/home_robot/home_robot/core/interfaces.py&#34;&gt;interfaces&lt;/a&gt; which are overridden to provide access to&lt;/p&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/home-robot/main/src/home_robot_sim&#34;&gt;home_robot_sim&lt;/a&gt; package contains code for interface&lt;/p&gt; &#xA;&lt;h3&gt;Style&lt;/h3&gt; &#xA;&lt;p&gt;We use linters for enforcing good code style. The &lt;code&gt;lint&lt;/code&gt; test will not pass if your code does not conform.&lt;/p&gt; &#xA;&lt;p&gt;Install the git &lt;a href=&#34;https://pre-commit.com/&#34;&gt;pre-commit&lt;/a&gt; hooks by running&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m pip install pre-commit&#xA;cd $HOME_ROBOT_ROOT&#xA;pre-commit install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To format manually, run: &lt;code&gt;pre-commit run --show-diff-on-failure --all-files&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Home Robot is MIT licensed. See the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/home-robot/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;h2&gt;References (temp)&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hello-robot/stretch_body&#34;&gt;hello-robot/stretch_body&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Base API for interacting with the Stretch robot&lt;/li&gt; &#xA;   &lt;li&gt;Some scripts for interacting with the Stretch&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hello-robot/stretch_ros&#34;&gt;hello-robot/stretch_ros&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Builds on top of stretch_body&lt;/li&gt; &#xA;   &lt;li&gt;ROS-related code for Stretch&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/RoboStack/ros-noetic&#34;&gt;RoboStack/ros-noetic&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Conda stream with ROS binaries&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/i-Code</title>
    <updated>2023-06-23T01:39:46Z</updated>
    <id>tag:github.com,2023-06-23:/microsoft/i-Code</id>
    <link href="https://github.com/microsoft/i-Code" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Project i-Code&lt;/h1&gt; &#xA;&lt;p&gt;The ambition of the i-Code project is to build integrative and composable multimodal Artificial Intelligence. The &#34;i&#34; stands for integrative multimodal learning.&lt;/p&gt; &#xA;&lt;h2&gt;Multimodal Foundation Models&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/i-Code/tree/main/i-Code-V1&#34;&gt;i-Code V1&lt;/a&gt;: i-Code: An Integrative and Composable Multimodal Learning Framework. AAAI 2023, &lt;a href=&#34;https://arxiv.org/abs/2205.01818&#34;&gt;paper link&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/i-Code/tree/main/i-Code-V2&#34;&gt;i-Code V2&lt;/a&gt;: i-Code V2: An Autoregressive Generation Framework over Vision, Language, and Speech Data. &lt;a href=&#34;https://arxiv.org/abs/2305.12311&#34;&gt;Paper link&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/i-Code/tree/main/i-Code-V3&#34;&gt;i-Code V3 (CoDi)&lt;/a&gt;: Any-to-Any Generation via Composable Diffusion, &lt;a href=&#34;https://arxiv.org/abs/2305.11846&#34;&gt;paper link&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/i-Code/tree/main/i-Code-Studio&#34;&gt;i-Code Studio&lt;/a&gt;: A Configurable and Composable Framework for Integrative AI, &lt;a href=&#34;https://arxiv.org/abs/2305.13738&#34;&gt;paper link&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Multimodal Document Intelligence&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/i-Code/tree/main/i-Code-Doc&#34;&gt;i-Code Doc (UDOP)&lt;/a&gt;: Unifying Vision, Text, and Layout for Universal Document Processing. CVPR 2023 Highlight, &lt;a href=&#34;https://arxiv.org/abs/2212.02623&#34;&gt;paper link&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href=&#34;https://cla.opensource.microsoft.com&#34;&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; &#xA;&lt;h2&gt;Trademarks&lt;/h2&gt; &#xA;&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href=&#34;https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general&#34;&gt;Microsoft&#39;s Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party&#39;s policies.&lt;/p&gt;</summary>
  </entry>
</feed>