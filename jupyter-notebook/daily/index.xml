<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-10-31T01:31:08Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>mistralai/mistral-src</title>
    <updated>2023-10-31T01:31:08Z</updated>
    <id>tag:github.com,2023-10-31:/mistralai/mistral-src</id>
    <link href="https://github.com/mistralai/mistral-src" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Reference implementation of Mistral AI 7B v0.1 model.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Mistral Transformer&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains minimal code to run our 7B model.&lt;/p&gt; &#xA;&lt;p&gt;Blog: &lt;a href=&#34;https://mistral.ai/news/announcing-mistral-7b/&#34;&gt;https://mistral.ai/news/announcing-mistral-7b/&lt;/a&gt;&lt;br&gt; Discord: &lt;a href=&#34;https://discord.com/invite/mistralai&#34;&gt;https://discord.com/invite/mistralai&lt;/a&gt;&lt;br&gt; Documentation: &lt;a href=&#34;https://docs.mistral.ai/&#34;&gt;https://docs.mistral.ai/&lt;/a&gt;&lt;br&gt; Guardrailing: &lt;a href=&#34;https://docs.mistral.ai/usage/guardrailing&#34;&gt;https://docs.mistral.ai/usage/guardrailing&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Deployment&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;deploy&lt;/code&gt; folder contains code to build a &lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;vLLM&lt;/a&gt; image with the required dependencies to serve the Mistral AI model. In the image, the &lt;a href=&#34;https://github.com/huggingface/transformers/&#34;&gt;transformers&lt;/a&gt; library is used instead of the reference implementation. To build it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build deploy --build-arg MAX_JOBS=8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Instructions to run the image can be found in the &lt;a href=&#34;https://docs.mistral.ai/quickstart&#34;&gt;official documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Download the model&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;wget https://files.mistral-7b-v0-1.mistral.ai/mistral-7B-v0.1.tar&#xA;tar -xf mistral-7B-v0.1.tar&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Run the model&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m main demo /path/to/mistral-7B-v0.1/&#xA;# To give your own prompts&#xA;python -m main interactive /path/to/mistral-7B-v0.1/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Change &lt;code&gt;temperature&lt;/code&gt; or &lt;code&gt;max_tokens&lt;/code&gt; using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m main interactive /path/to/mistral-7B-v0.1/ --max_tokens 256 --temperature 1.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want a self-contained implementation, look at &lt;code&gt;one_file_ref.py&lt;/code&gt;, or run it with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m one_file_ref /path/to/mistral-7B-v0.1/&#xA;&#xA;This is a test of the emergency broadcast system. This is only a test.&#xA;&#xA;If this were a real emergency, you would be told what to do.&#xA;&#xA;This is a test&#xA;=====================&#xA;This is another test of the new blogging software. Iâ€™m not sure if Iâ€™m going to keep it or not. Iâ€™m not sure if Iâ€™m going to keep&#xA;=====================&#xA;This is a third test, mistral AI is very good at testing. ðŸ™‚&#xA;&#xA;This is a third test, mistral AI is very good at testing. ðŸ™‚&#xA;&#xA;This&#xA;=====================&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run logits equivalence through chunking and sliding window, launch&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m test_generate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Sliding window attention&lt;/h1&gt; &#xA;&lt;h2&gt;Vanilla attention&lt;/h2&gt; &#xA;&lt;p&gt;Attention is how information is shared between tokens in a sequence. In vanilla transformers, attention follows a causal mask: each token in the sequence can attend to itself and all the tokens in the past. This ensures that the model is causal, i.e. it can only use information from the past to predict the future.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mistralai/mistral-src/main/assets/full_attention.png&#34; alt=&#34;Causal attention mask&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Sliding window to speed-up inference and reduce memory pressure&lt;/h2&gt; &#xA;&lt;p&gt;The number of operations of attention is quadratic in the sequence length, and the memory pressure is linear in the sequence length. At inference time, this incurs higher latency and smaller throughput due to reduced cache availability. To alleviate this issue, we use a sliding window attention [1,2]: each token can attend to at most W tokens in the past (here, W=3).&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mistralai/mistral-src/main/assets/sliding_attention.png&#34; alt=&#34;Sliding window attention&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Note that tokens outside the sliding window still influence next word prediction. At each attention layer, information can move forward by W tokens at most: after two attention layers, information can move forward by 2W tokens, etc. For instance in a sequence of length 16K and a sliding window of 4K, after 4 layers, information has propagated to the full sequence length.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mistralai/mistral-src/main/assets/attention_through_layers.png&#34; alt=&#34;Attention through layers&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Empirically, we see that longer contexts do help &lt;em&gt;even outside the sliding window&lt;/em&gt; but when the sequence length becomes too large, the model does not use the full context anymore.&lt;/p&gt; &#xA;&lt;h2&gt;Rolling buffer cache&lt;/h2&gt; &#xA;&lt;p&gt;We implement a rolling buffer cache. The cache has a fixed size of W, and we store the (key, value) for position i in cache position i % W. When the position i is larger than W, past values in the cache are overwritten.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mistralai/mistral-src/main/assets/rolling_cache.png&#34; alt=&#34;Rolling cache&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Pre-fill and chunking&lt;/h2&gt; &#xA;&lt;p&gt;When generating a sequence, we need to predict tokens one-by-one, as each token is conditioned on the previous ones. However, the prompt is known in advance, and we can pre-fill the (k, v) cache with the prompt. If the prompt is very large, we can chunk it into smaller pieces, and pre-fill the cache with each chunk. For this we can choose as chunk size the window size. For each chunk, we thus need to compute the attention over the cache and over the chunk.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mistralai/mistral-src/main/assets/chunking.png&#34; alt=&#34;Chunking&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Integrations and related projects&lt;/h2&gt; &#xA;&lt;h3&gt;Model platforms&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Use Mistral AI in HuggingFace: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/mistralai/Mistral-7B-v0.1&#34;&gt;Mistral-7B-v0.1&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1&#34;&gt;Mistral-7B-Instruct-v0.1&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Use Mistral 7B on &lt;a href=&#34;https://github.com/GoogleCloudPlatform/vertex-ai-samples/raw/main/notebooks/community/model_garden/model_garden_pytorch_mistral.ipynb&#34;&gt;Vertex AI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Use Mistral 7B on &lt;a href=&#34;https://replicate.com/lucataco/mistral-7b-v0.1&#34;&gt;Replicate&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Use Mistral 7B on &lt;a href=&#34;https://aws.amazon.com/blogs/machine-learning/mistral-7b-foundation-models-from-mistral-ai-are-now-available-in-amazon-sagemaker-jumpstart/&#34;&gt;Sagemaker Jumpstart&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Use Mistral 7B on &lt;a href=&#34;https://app.baseten.co/explore/&#34;&gt;Baseten&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Applications&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Compare Mistral 7B to Llama 13B on &lt;a href=&#34;https://llmboxing.com/&#34;&gt;LLMBoxing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Compare Mistral 7B to 10+ LLMs on &lt;a href=&#34;https://chat.lmsys.org/&#34;&gt;Chatbot Arena&lt;/a&gt; or host it yourself with &lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;FastChat&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Use Mistral 7B in &lt;a href=&#34;https://dust.tt/&#34;&gt;Dust&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Speak to Mistral AI Instruct on &lt;a href=&#34;https://labs.perplexity.ai/&#34;&gt;Perplexity labs&lt;/a&gt; (warning: deployed version is not &lt;a href=&#34;https://docs.mistral.ai/usage/guardrailing&#34;&gt;guardrailed&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Use Mistral 7B in &lt;a href=&#34;https://blog.quivr.app/is-mistral-a-good-replacement-for-openai/&#34;&gt;Quivr&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Use Mistral 7B or its Zephyr derivate on &lt;a href=&#34;https://docs.llamaindex.ai/en/stable/core_modules/model_modules/llms/root.html#open-source-llms&#34;&gt;LlamaIndex&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Local deployment&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ollama.ai/library/mistral&#34;&gt;Ollama&lt;/a&gt; local deployment&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ggerganov/ggml&#34;&gt;GGML&lt;/a&gt; local deployment&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://textsynth.com/pricing.html&#34;&gt;TextSynth&lt;/a&gt; local deployment&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Derived models&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Multimodal: &lt;a href=&#34;https://huggingface.co/SkunkworksAI/BakLLaVA-1&#34;&gt;BakLLaVa-1&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Model fine-tuned on direct preferences: &lt;a href=&#34;https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha&#34;&gt;Zephyr-7B-alpha&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Model fine-tuned on generated data: &lt;a href=&#34;https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca&#34;&gt;OpenOrca&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;p&gt;[1] &lt;a href=&#34;https://arxiv.org/pdf/1904.10509.pdf&#34;&gt;Generating Long Sequences with Sparse Transformers, Child et al. 2019&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[2] &lt;a href=&#34;https://arxiv.org/pdf/2004.05150v2.pdf&#34;&gt;Longformer: The Long-Document Transformer, Beltagy et al. 2020&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>