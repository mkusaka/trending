<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-19T01:26:59Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>mshumer/gpt-oracle-trainer</title>
    <updated>2023-08-19T01:26:59Z</updated>
    <id>tag:github.com,2023-08-19:/mshumer/gpt-oracle-trainer</id>
    <link href="https://github.com/mshumer/gpt-oracle-trainer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;gpt-oracle-trainer&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://twitter.com/mattshumer_&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/mattshumer_?style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1xB0ZeiBAF78FAxNCz-TeRRwQtmmVjxOh?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open Main Version In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;Creating a chatbot that can accurately answer questions about a product or service&#39;s documentation is a complex task. You can fine-tune on the documentation itself (but results won&#39;t be conversational), use embeddings (risks losing relevant context), etc. &lt;code&gt;gpt-oracle-trainer&lt;/code&gt; is an experimental tool that aims to simplify this process and potentially produce better results.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Simply provide your product or service&#39;s documentation, describe your product or service, select a temperature for data generation, and choose the number of training examples to generate per document. The system will then generate the dataset in the correct format, train the model, and allow you to test it.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Data Generation&lt;/strong&gt;: The system generates a question-and-answer dataset based on your documentation, service description, temperature, and number of examples. The data is formatted correctly for model training.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Model Training&lt;/strong&gt;: The system trains the model using the generated dataset.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Model Testing&lt;/strong&gt;: Test the trained model with a custom prompt.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to Use&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1xB0ZeiBAF78FAxNCz-TeRRwQtmmVjxOh?usp=sharing&#34;&gt;Open the notebook in Google Colab &lt;/a&gt;or in a local Jupyter notebook.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add your OpenAI API key to the line &lt;code&gt;openai.api_key = &#34;OPENAI API KEY HERE&#34;&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Paste your documentation into the &lt;code&gt;docs&lt;/code&gt; list, each document as a separate string.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Define your &lt;code&gt;service_name_and_description&lt;/code&gt;, &lt;code&gt;temperature&lt;/code&gt;, and &lt;code&gt;number_of_examples_per_doc&lt;/code&gt;. For example:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;service_name_and_description = &#34;MosaicML, a platform that makes it easier to train and fine-tune large AI models&#34;&#xA;temperature = .7&#xA;number_of_examples_per_doc = 10&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the cells to generate the dataset, train the model, and test it.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The final trained model will be saved under the name you specify in &lt;code&gt;new_model&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Contributions are welcome! Some ideas:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;improve the data generation process&lt;/li&gt; &#xA; &lt;li&gt;add more customization options for the model training&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is &lt;a href=&#34;https://github.com/mshumer/gpt-oracle-trainer/raw/master/LICENSE&#34;&gt;MIT&lt;/a&gt; licensed.&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;Matt Shumer - &lt;a href=&#34;https://twitter.com/mattshumer_&#34;&gt;@mattshumer_&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Project Link: &lt;a href=&#34;https://raw.githubusercontent.com/mshumer/gpt-oracle-trainer/main/url&#34;&gt;https://github.com/mshumer/gpt-oracle-trainer&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Lastly, if you want to try something even cooler than this, sign up for &lt;a href=&#34;https://www.hyperwriteai.com/personal-assistant&#34;&gt;Personal Assistant&lt;/a&gt; (most of my time is spent on this). It&#39;s basically an AI that can operate your web browser to complete tasks for you.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>tencent-ailab/IP-Adapter</title>
    <updated>2023-08-19T01:26:59Z</updated>
    <id>tag:github.com,2023-08-19:/tencent-ailab/IP-Adapter</id>
    <link href="https://github.com/tencent-ailab/IP-Adapter" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The image prompt adapter is designed to enable a pretrained text-to-image diffusion model to generate images with image prompt.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://ip-adapter.github.io&#34;&gt;&lt;strong&gt;Project Page&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;|&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2308.06721&#34;&gt;&lt;strong&gt;Paper (ArXiv)&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;we present IP-Adapter, an effective and lightweight adapter to achieve image prompt capability for the pretrained text-to-image diffusion models. An IP-Adapter with only 22M parameters can achieve comparable or even better performance to a fine-tuned image prompt model. IP-Adapter can be generalized not only to other custom models fine-tuned from the same base model, but also to controllable generation using existing controllable tools. Moreover, the image prompt can also work well with the text prompt to accomplish multimodal image generation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/assets/figs/fig1.png&#34; alt=&#34;arch&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Release&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2023/8/18] ðŸ”¥ Add code and models for &lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0&#34;&gt;SDXL 1.0&lt;/a&gt;. Demo is &lt;a href=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/ip_adapter_sdxl_demo.ipynb&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2023/8/16] ðŸ”¥ We release the code and models.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Dependencies&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;diffusers &amp;gt;= 0.19.3&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Download Models&lt;/h2&gt; &#xA;&lt;p&gt;you can download models from &lt;a href=&#34;https://huggingface.co/h94/IP-Adapter&#34;&gt;here&lt;/a&gt;. To run the demo, you should also download the following models:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/runwayml/stable-diffusion-v1-5&#34;&gt;runwayml/stable-diffusion-v1-5&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/sd-vae-ft-mse&#34;&gt;stabilityai/sd-vae-ft-mse&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/SG161222/Realistic_Vision_V4.0_noVAE&#34;&gt;SG161222/Realistic_Vision_V4.0_noVAE&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/lllyasviel&#34;&gt;ControlNet models&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to Use&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/ip_adapter_demo.ipynb&#34;&gt;&lt;strong&gt;ip_adapter_demo&lt;/strong&gt;&lt;/a&gt;: image variations, image-to-image, and inpainting with image prompt.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/assets/demo/image_variations.jpg&#34; alt=&#34;image variations&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/assets/demo/image-to-image.jpg&#34; alt=&#34;image-to-image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/assets/demo/inpainting.jpg&#34; alt=&#34;inpainting&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/ip_adapter_controlnet_demo.ipynb&#34;&gt;&lt;strong&gt;ip_adapter_controlnet_demo&lt;/strong&gt;&lt;/a&gt;: structural generation with image prompt.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/assets/demo/structural_cond.jpg&#34; alt=&#34;structural_cond&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/ip_adapter_multimodal_prompts_demo.ipynb&#34;&gt;&lt;strong&gt;ip_adapter_multimodal_prompts_demo&lt;/strong&gt;&lt;/a&gt;: generation with multimodal prompts.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/assets/demo/multi_prompts.jpg&#34; alt=&#34;multi_prompts&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find IP-Adapter useful for your your research and applications, please cite using this BibTeX:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{ye2023ip-adapter,&#xA;  title={IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models},&#xA;  author={Ye, Hu and Zhang, Jun and Liu, Sibo and Han, Xiao and Yang, Wei},&#xA;  booktitle={arXiv preprint arxiv:2308.06721},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>