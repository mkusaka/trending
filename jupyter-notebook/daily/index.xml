<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-07-22T01:45:20Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>OpenMined/PySyft</title>
    <updated>2022-07-22T01:45:20Z</updated>
    <id>tag:github.com,2022-07-22:/OpenMined/PySyft</id>
    <link href="https://github.com/OpenMined/PySyft" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A library for answering questions using data you cannot see&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; &lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/OpenMined/PySyft/dev/packages/syft/docs/img/monorepo_logo2.png&#34; alt=&#34;Syft + Grid&#34; width=&#34;400&#34;&gt; &lt;br&gt; Code for computing on data&lt;br&gt; you do not own and cannot see &lt;br&gt; &lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://pypi.org/project/syft/&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/syft&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://pypi.org/project/syft/&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/syft.svg?sanitize=true&#34;&gt;&lt;/a&gt; &#xA; &lt;br&gt; &#xA; &lt;a href=&#34;https://github.com/OpenMined/PySyft/actions/workflows/syft-version_tests.yml&#34;&gt;&lt;img src=&#34;https://github.com/OpenMined/PySyft/actions/workflows/syft-version_tests.yml/badge.svg?branch=dev&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/OpenMined/PySyft/actions/workflows/nightlies-run.yml&#34;&gt;&lt;img src=&#34;https://github.com/OpenMined/PySyft/actions/workflows/nightlies-run.yml/badge.svg?branch=dev&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://openmined.slack.com/messages/support&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/chat-on%20slack-7A5979.svg?sanitize=true&#34;&gt;&lt;/a&gt; &#xA; &lt;br&gt;&#xA; &lt;br&gt; &#xA; &lt;div align=&#34;center&#34;&gt;&#xA;  &lt;a href=&#34;https://raw.githubusercontent.com/OpenMined/PySyft/dev/#&#34;&gt;&lt;img src=&#34;https://stars.medv.io/openmined/pysyft.svg?sanitize=true&#34;&gt;&lt;/a&gt;&#xA; &lt;/div&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;Syft + Grid provides secure and private Deep Learning in Python&lt;/h1&gt; &#xA;&lt;p&gt;Syft decouples private data from model training, using &lt;a href=&#34;https://ai.googleblog.com/2017/04/federated-learning-collaborative.html&#34;&gt;Federated Learning&lt;/a&gt;, &lt;a href=&#34;https://en.wikipedia.org/wiki/Differential_privacy&#34;&gt;Differential Privacy&lt;/a&gt;, and Encrypted Computation (like &lt;a href=&#34;https://en.wikipedia.org/wiki/Secure_multi-party_computation&#34;&gt;Multi-Party Computation (MPC)&lt;/a&gt; and &lt;a href=&#34;https://en.wikipedia.org/wiki/Homomorphic_encryption&#34;&gt;Homomorphic Encryption (HE)&lt;/a&gt;) within the main Deep Learning frameworks like PyTorch and TensorFlow. Join the movement on &lt;a href=&#34;http://slack.openmined.org/&#34;&gt;Slack&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Most software libraries let you compute over the information you own and see inside of machines you control. However, this means that you cannot compute on information without first obtaining (at least partial) ownership of that information. It also means that you cannot compute using machines without first obtaining control over those machines. This is very limiting to human collaboration and systematically drives the centralization of data, because you cannot work with a bunch of data without first putting it all in one (central) place.&lt;/p&gt; &#xA;&lt;p&gt;The Syft ecosystem seeks to change this system, allowing you to write software which can compute over information you do not own on machines you do not have (total) control over. This not only includes servers in the cloud, but also personal desktops, laptops, mobile phones, websites, and edge devices. Wherever your data wants to live in your ownership, the Syft ecosystem exists to help keep it there while allowing it to be used privately for computation.&lt;/p&gt; &#xA;&lt;h2&gt;Stable Release&lt;/h2&gt; &#xA;&lt;p&gt;The current stable release is &lt;code&gt;0.6.0&lt;/code&gt; which is available on:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/syft/&#34;&gt;PyPI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hub.docker.com/u/openmined&#34;&gt;Docker Hub&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For many use cases you can simply use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ pip install syft&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are doing the &lt;a href=&#34;https://courses.openmined.org/&#34;&gt;Private AI Series&lt;/a&gt; or you are an external party developing against Syft and Grid please use the &lt;a href=&#34;https://github.com/OpenMined/pysyft/tree/0.6.0&#34;&gt;&lt;code&gt;syft_0.6.0&lt;/code&gt;&lt;/a&gt; branch.&lt;/p&gt; &#xA;&lt;h2&gt;Development Branch&lt;/h2&gt; &#xA;&lt;p&gt;This is the &lt;code&gt;dev&lt;/code&gt; branch and to accommodate our need to experiment with new ideas and implementations we will be moving a few things around during the early stages of &lt;code&gt;0.7.0 beta&lt;/code&gt;. Currently the core &lt;code&gt;syft&lt;/code&gt; library and code will remain fairly stable, while we do some much needed quality improvements and refactors to the &lt;code&gt;grid&lt;/code&gt; codebase and its tooling for deployment and orchestration of nodes. During the process of development we will be moving examples from the &lt;code&gt;/packages/syft/examples&lt;/code&gt; folder down to the &lt;code&gt;/notebooks&lt;/code&gt; folder and ensuring they are working and tested with the latest &lt;code&gt;dev&lt;/code&gt; code.&lt;/p&gt; &#xA;&lt;h2&gt;Mono Repo üöù&lt;/h2&gt; &#xA;&lt;p&gt;This repo contains multiple sub-projects which work together.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;OpenMined/PySyft&#xA;‚îú‚îÄ‚îÄ README.md     &amp;lt;-- You are here üìå&#xA;‚îú‚îÄ‚îÄ notebooks     &amp;lt;-- Notebook Examples and Tutorials&#xA;‚îî‚îÄ‚îÄ packages&#xA;    ‚îú‚îÄ‚îÄ grid      &amp;lt;-- Grid - A network aware, persistent &amp;amp; containerized node running Syft&#xA;    ‚îî‚îÄ‚îÄ syft      &amp;lt;-- Syft - A package for doing remote data science on private data&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Syft&lt;/h2&gt; &#xA;&lt;p&gt;To read more about what Syft is please consult the current &lt;a href=&#34;https://openmined.github.io/PySyft/&#34;&gt;&lt;code&gt;0.6.0&lt;/code&gt; Documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Grid&lt;/h2&gt; &#xA;&lt;p&gt;To read more about what Grid is please consult the old &lt;a href=&#34;https://github.com/OpenMined/PyGrid&#34;&gt;PyGrid README&lt;/a&gt; until we finish writing the new one.&lt;/p&gt; &#xA;&lt;h2&gt;Dev Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;docker&lt;/li&gt; &#xA; &lt;li&gt;tox&lt;/li&gt; &#xA; &lt;li&gt;python 3.8+&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Docker&lt;/h3&gt; &#xA;&lt;p&gt;You will need &lt;code&gt;docker&lt;/code&gt; and &lt;code&gt;docker-compose&lt;/code&gt; to do development on the &lt;code&gt;monorepo&lt;/code&gt; tooling.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.docker.com/docker-for-mac/install/&#34;&gt;Get Docker for macOS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.docker.com/docker-for-windows/install/&#34;&gt;Get Docker for Windows&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.docker.com/engine/install/ubuntu/&#34;&gt;Get Docker for Ubuntu&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Dev Compose File&lt;/h3&gt; &#xA;&lt;p&gt;Run the &lt;a href=&#34;https://fastapi.tiangolo.com/&#34;&gt;FastAPI&lt;/a&gt; Dev environment using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ cd packages/grid&#xA;$ source .env &amp;amp;&amp;amp; docker compose up&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Rebuilding Docker Containers&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ cd packages/grid&#xA;$ docker compose build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Tox&lt;/h3&gt; &#xA;&lt;p&gt;You will need &lt;code&gt;tox&lt;/code&gt; to run some of our build and test tools.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ pip install tox&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;List Build Commands&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ tox -l&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You should see the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;syft.jupyter&#xA;syft.lint&#xA;syft.test.fast&#xA;syft.test.libs&#xA;syft.test.security&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;These commands can be run like so:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ tox -e syft.lint&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Single VM Deployment&lt;/h2&gt; &#xA;&lt;p&gt;We are providing a simple way to deploy all of our stack inside a single VM so that no matter where you want to run everything you can do so easily by thinking in terms of a single machine either bare metal or VM and have it provisioned and auto updated.&lt;/p&gt; &#xA;&lt;p&gt;To develop against this locally you will want the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;vagrant&lt;/li&gt; &#xA; &lt;li&gt;virtualbox&lt;/li&gt; &#xA; &lt;li&gt;ansible&lt;/li&gt; &#xA; &lt;li&gt;hagrid &amp;lt;-- in packages/hagrid&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;HAGrid Install&lt;/h2&gt; &#xA;&lt;p&gt;You can install HAGrid with pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ pip install hagrid&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;MacOS Instructions&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ brew install vagrant virtualbox ansible&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Hagrid the Grid deployment tool:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ cd packages/hagrid&#xA;$ pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Vagrant&lt;/h2&gt; &#xA;&lt;p&gt;Vagrant allows us to create and manage VMs locally for development. During the startup process of creating the VM the ansible provisioning scripts will be applied automatically to the VM. If you change the Vagrantfile which describes how the VM is defined you will need to either &lt;code&gt;vagrant reload&lt;/code&gt; or destroy and re-create it.&lt;/p&gt; &#xA;&lt;p&gt;Making changes to the VM state should be done through the &lt;code&gt;ansible&lt;/code&gt; scripts so that the state of the box is idempotent and re-running the ansible provisioning scripts should always result in the same working grid node state.&lt;/p&gt; &#xA;&lt;p&gt;To allow rapid development we mount the PySyft source repo into the VM at the path: &lt;code&gt;/home/om/PySyft&lt;/code&gt; which is where it would be if it was cloned down on a real remote VM.&lt;/p&gt; &#xA;&lt;p&gt;The configuration is done via a &lt;code&gt;Vagrantfile&lt;/code&gt; which is written in ruby.&lt;/p&gt; &#xA;&lt;h2&gt;Vagrant Networking&lt;/h2&gt; &#xA;&lt;h3&gt;Vagrant IP&lt;/h3&gt; &#xA;&lt;p&gt;The VM will be accessible on the IP &lt;code&gt;10.0.1.2&lt;/code&gt; which is defined in the &lt;code&gt;Vagrantfile&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Vagrant Landrush Plugin&lt;/h3&gt; &#xA;&lt;p&gt;The Landrush plugin for vagrant gives us an automatic dns service so we can access our local VM as though it were a real live domain on the internet.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ vagrant plugin install landrush&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;With this enabled you can access the box on: &lt;code&gt;http://node.openmined.grid&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Starting VM&lt;/h2&gt; &#xA;&lt;p&gt;NOTE: You may need your sudo password to enable the landrush DNS entry on startup.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ cd packages/grid&#xA;$ vagrant up --provision&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Provisioning the VM&lt;/h2&gt; &#xA;&lt;p&gt;You want to do this any time you are testing out your &lt;code&gt;ansible&lt;/code&gt; changes.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ cd packages/grid&#xA;$ vagrant provision&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to do a quick deploy where you skip the system provisioning you can run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ANSIBLE_ARGS=&#39;--extra-vars &#34;deploy_only=true&#34;&#39; vagrant provision&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Connecting to Vagrant VM&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ cd packages/grid&#xA;$ vagrant ssh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;TLS and Certificates&lt;/h2&gt; &#xA;&lt;p&gt;You can enable TLS in HAGrid by passing in the --tls param:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ hagrid launch domain to docker:8081+ --tls&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will go looking for a certificate and private key here:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;packages/grid/traefik/certs/key.pem&#xA;packages/grid/traefik/certs/cert.pem&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;These files and their settings are defined in:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;packages/grid/traefik/dynamic-configurations/certs.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;During development you will need to generate ones which match localhost and then enable --test mode so that these invalid self-signed certificates are accepted by various code and libraries.&lt;/p&gt; &#xA;&lt;p&gt;First, get the tool &lt;code&gt;mkcert&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;MacOS&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ brew install mkcert&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Generate Dev Cert&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd PySyft&#xA;export GRID=$(pwd)/packages/grid &amp;amp;&amp;amp; export CAROOT=$GRID/tls &amp;amp;&amp;amp; export CERTS=$GRID/traefik/certs&#xA;mkcert -cert-file=&#34;$CERTS/cert.pem&#34; -key-file=&#34;$CERTS/key.pem&#34; &#39;*.openmined.grid&#39; docker-host localhost 127.0.0.1 ::1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will have created the certificate and private key as well as placed the root signing certificate (think fake SSL authority who can charge for SSL certs) in the following dir:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;packages/grid/tls/rootCA-key.pem&#xA;packages/grid/tls/rootCA.pem&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To ensure that tailscale will accept these certs we mount the file into the tailscale container like so:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;version: &#34;3.8&#34;&#xA;services:&#xA;  tailscale:&#xA;    volumes:&#xA;      - ./tls/rootCA.pem:/usr/local/share/ca-certificates/rootCA.pem&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The startup script runs &lt;code&gt;update-ca-certificates&lt;/code&gt; so that the tailscale container is now aware of this fake authority and will accept the fake cert you have created for it.&lt;/p&gt; &#xA;&lt;h3&gt;Install Cert on Host&lt;/h3&gt; &#xA;&lt;p&gt;If you wish to visit web pages with &lt;code&gt;localhost:8081&lt;/code&gt; or &lt;code&gt;network1.openmined.grid&lt;/code&gt; and have the TLS certificate warning disappear you need to install the certificate with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ mkcert -install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Ignoring TLS Certs&lt;/h3&gt; &#xA;&lt;p&gt;Alternatively as we do in the integration tests you can pass it as an environment variable to programming languages like Python like so:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;REQUESTS_CA_BUNDLE=packages/grid/tls/rootCA.pem pytest tests/integration ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When you do this, python will also accept these certificates in libraries like requests. Alternatively you can also tell requests to ignore invalid certificates with the &lt;code&gt;verify&lt;/code&gt; kwarg like so:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import requests&#xA;request.get(url, verify=False)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To make this more convenient we have added an ENV called &lt;code&gt;IGNORE_TLS_ERRORS&lt;/code&gt; which we set using the &lt;code&gt;--test&lt;/code&gt; param in &lt;code&gt;hagrid&lt;/code&gt; like so:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ hagrid launch test_network_1 network to docker:9081 --tail=false --tls --test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can check for this with &lt;code&gt;sy.util.verify_tls()&lt;/code&gt; like so:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import requests&#xA;import syft as sy&#xA;&#xA;request.get(url, verify=sy.util.verify_tls())&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It is important not to run &lt;code&gt;IGNORE_TLS_ERRORS=true&lt;/code&gt; in production.&lt;/p&gt; &#xA;&lt;h3&gt;Ports&lt;/h3&gt; &#xA;&lt;p&gt;Normally web traffic is served over port &lt;code&gt;80&lt;/code&gt; for &lt;code&gt;http&lt;/code&gt; and port &lt;code&gt;443&lt;/code&gt; for &lt;code&gt;https&lt;/code&gt;. Naturally during development we need to use multiple ports for multiple stacks and bind them to &lt;code&gt;localhost&lt;/code&gt; and use things like &lt;code&gt;docker-host&lt;/code&gt; to resolve this global address space from within any isolated containers. Currently what we do is if the &lt;code&gt;port&lt;/code&gt; you supply to &lt;code&gt;hagrid&lt;/code&gt; is &lt;code&gt;80&lt;/code&gt; then we assume you are running in production and want port &lt;code&gt;443&lt;/code&gt; for &lt;code&gt;https&lt;/code&gt;. If you use any other port we will automatically find a port from &lt;code&gt;444&lt;/code&gt; onwards and this port will be included in the &lt;code&gt;http&lt;/code&gt; -&amp;gt; &lt;code&gt;https&lt;/code&gt; 301 redirect in traefik proxy. Because of this most browsers and network libraries will simply follow this redirect so you can continue to use the normal &lt;code&gt;http&lt;/code&gt; ports for everything.&lt;/p&gt; &#xA;&lt;h3&gt;Redirects&lt;/h3&gt; &#xA;&lt;p&gt;The Grid API inside Syft detects if a url and port combination provided gets redirected to &lt;code&gt;https&lt;/code&gt; and will change the &lt;code&gt;base_url&lt;/code&gt; which should mean that login credentials are sent over &lt;code&gt;https&lt;/code&gt; not &lt;code&gt;http&lt;/code&gt; where possible.&lt;/p&gt; &#xA;&lt;h3&gt;VPN&lt;/h3&gt; &#xA;&lt;p&gt;Due to the fact that TLS Certificates are only valid for domains and not IPs and the VPN is currently configured to use IPs only, we do not redirect or serve &lt;code&gt;https&lt;/code&gt; over the VPN. The traffic being sent by &lt;code&gt;wireguard&lt;/code&gt; over the VPN is already encrypted so there should be no need for &lt;code&gt;TLS&lt;/code&gt;. The way this works is, when Grid is using &lt;code&gt;TLS&lt;/code&gt; we forward external port &lt;code&gt;80&lt;/code&gt; traffic to port &lt;code&gt;81&lt;/code&gt; and then use this for the 301 redirect to the &lt;code&gt;TLS&lt;/code&gt; port &lt;code&gt;443+&lt;/code&gt;. If traffic arrives on port &lt;code&gt;80&lt;/code&gt; inside the cluster it does not get redirected which allows the VPN IPs to respond via &lt;code&gt;http&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Deploying Custom Certs&lt;/h3&gt; &#xA;&lt;p&gt;To install certs via &lt;code&gt;hagrid&lt;/code&gt; simply supply the two cert files as arguments:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ hagrid launch domain to azure --tls --upload_tls_key=/path/to/certs/key.pem --upload_tls_cert=/path/to/certs/cert.pem&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively if your machine is already setup without TLS you can simply reprovision with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ hagrid launch node_name domain to 123.x.x.x --tls --upload_tls_key=/path/to/certs/key.pem --upload_tls_cert=/path/to/certs/cert.pem&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Deploy to Cloud&lt;/h2&gt; &#xA;&lt;h3&gt;Azure Marketplace 1-click Deploy&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://azuremarketplace.microsoft.com/en-us/marketplace/apps/madhavajay1632269232059.openmined_mj_grid_domain_ubuntu_1?tab=Overview&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Azure/azure-quickstart-templates/master/1-CONTRIBUTION-GUIDE/images/deploytoazure.svg?sanitize=true&#34; alt=&#34;Deploy To Azure&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;We also have an example &lt;a href=&#34;https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2FOpenMined%2FPySyft%2Fdev%2Fpackages%2Fgrid%2Fquickstart%2Ftemplate.json&#34;&gt;Quickstart Template&lt;/a&gt; for use in your own automation pipelines.&lt;/p&gt; &#xA;&lt;h3&gt;Deploy from CLI to Azure&lt;/h3&gt; &#xA;&lt;p&gt;HAGrid supports creating VMs and provisioning with PyGrid directly using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ hagrid launch to azure&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Follow the prompts and enter resource groups, size, location, auth credentials etc.&lt;/p&gt; &#xA;&lt;h2&gt;Deploy from CLI to Google Cloud Platform (GCP)&lt;/h2&gt; &#xA;&lt;p&gt;HAGrid supports creating VMs and provisioning with PyGrid directly using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ hagrid launch to gcp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Follow the prompts and enter project id, machine type, zone, auth credentials etc. &lt;strong&gt;Note&lt;/strong&gt; Since we use the gcloud cli tool for provisioning we rely on the gcloud cli tool generated SSH keys so if you use a custom key path other than the default (~/.ssh/google_compute_engine) please specify the correct path.&lt;/p&gt; &#xA;&lt;h3&gt;HAGrid Deployment&lt;/h3&gt; &#xA;&lt;p&gt;Create a VM on your cloud provider with Ubuntu 20.04 with at least:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2x CPU&lt;/li&gt; &#xA; &lt;li&gt;4gb RAM&lt;/li&gt; &#xA; &lt;li&gt;40gb HDD&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Generate or supply a private key and note down the username.&lt;/p&gt; &#xA;&lt;p&gt;Run the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ hagrid launch node --type=domain --host=104.42.26.195 --username=ubuntu --key_path=~/.ssh/key.pem&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Deploy vs Provision&lt;/h3&gt; &#xA;&lt;p&gt;If you want to later skip the setup process of installing packages and docker engine etc you can pass in --mode=deploy which will skip those steps.&lt;/p&gt; &#xA;&lt;h3&gt;Use a Custom PySyft Fork&lt;/h3&gt; &#xA;&lt;p&gt;If you wish to use a different fork of PySyft you can pass in --repo=The-PET-Lab-at-the-UN-PPTTT/PySyft --branch=ungp_pet_lab&lt;/p&gt; &#xA;&lt;h2&gt;Switching to the OpenMined user&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ sudo su - om&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Cloud Images&lt;/h2&gt; &#xA;&lt;p&gt;We are using Packer to build cloud images in a very similar fashion to the dev Vagrant box.&lt;/p&gt; &#xA;&lt;p&gt;To build images you will need the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;packer&lt;/li&gt; &#xA; &lt;li&gt;vagrant&lt;/li&gt; &#xA; &lt;li&gt;virtualbox&lt;/li&gt; &#xA; &lt;li&gt;ansible&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;MacOS Instructions&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ brew install packer vagrant virtualbox ansible&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Build a Local Vagrant Box&lt;/h2&gt; &#xA;&lt;p&gt;Go to the following directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd packages/grid/packer&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./build_vagrant.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;What this does is first build the base image, by downloading a Ubuntu .iso and automating an install to a virtual machine. After the base image is created, the same ansible provisioning scripts that we use in HAGrid and the Vagrant Dev environment above are run against the image and finally a few shell scripts are executed to update some Ubuntu packages and clean out a lot of unused stuff to squeeze the image size down.&lt;/p&gt; &#xA;&lt;p&gt;To verify it worked you can start the Vagrant file like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd packages/grid/packer&#xA;vagrant up&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This system will start and automatically have the stack running and available on the local ip &lt;a href=&#34;http://10.0.1.3/&#34;&gt;http://10.0.1.3/&lt;/a&gt; you can also SSH into this box using the credentials in the Vagrantfile.&lt;/p&gt; &#xA;&lt;h2&gt;Azure Cloud Image&lt;/h2&gt; &#xA;&lt;p&gt;To create the azure cloud image you need to have the &lt;code&gt;az&lt;/code&gt; cli tool and make sure you are authenticated.&lt;/p&gt; &#xA;&lt;p&gt;Install the CLI tool:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ pip install az&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Authenticate your CLI tool:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ az login&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You will need to use a resource group and create a storage account within that resource group.&lt;/p&gt; &#xA;&lt;p&gt;Create a resource group called: &lt;code&gt;openmined-images&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ az group create -n openmined-images -l westus&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Create an app to use within the packer file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ az ad sp create-for-rbac --name openmined-images &amp;gt; azure_vars.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will create a file called &lt;code&gt;azure_vars.json&lt;/code&gt; which will look something like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;appId&#34;: &#34;21b92977-8ad0-467c-ae3a-47c864418126&#34;,&#xA;  &#34;displayName&#34;: &#34;openmined-images&#34;,&#xA;  &#34;password&#34;: &#34;xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&#34;,&#xA;  &#34;tenant&#34;: &#34;e3f9defa-1378-49b3-aed7-3dcacb468c41&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You need to know your &lt;code&gt;subscription_id&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ SUBSCRIPTION_ID=$(az account show --query id | tr -d &#39;&#34;&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can now build the image:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ ./build_azure.sh ${SUBSCRIPTION_ID}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Create a Shared Image Gallery&lt;/h3&gt; &#xA;&lt;p&gt;Create a Shared image gallery within Azure.&lt;/p&gt; &#xA;&lt;h2&gt;Kubernetes&lt;/h2&gt; &#xA;&lt;p&gt;We provide an option to deploy the stack using kubernetes. To test and run this locally we use &lt;code&gt;k3d&lt;/code&gt; and &lt;code&gt;devspace&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Local Dev&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;docker&lt;/li&gt; &#xA; &lt;li&gt;k3d&lt;/li&gt; &#xA; &lt;li&gt;devspace&lt;/li&gt; &#xA; &lt;li&gt;kubectl&lt;/li&gt; &#xA; &lt;li&gt;k9s&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;MacOS&lt;/h2&gt; &#xA;&lt;h3&gt;Docker&lt;/h3&gt; &#xA;&lt;p&gt;We will be using docker however you do not need to &lt;code&gt;enable kubernetes&lt;/code&gt; in your docker desktop app. If its enabled, disable it and click &lt;code&gt;Apply &amp;amp; Restart&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Instead we will use &lt;code&gt;k3d&lt;/code&gt; which will create and manage all the k8s resources we require as a normal container in docker engine.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ brew install k3d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;k3d&lt;/h3&gt; &#xA;&lt;p&gt;k3d lets you create a test registry and cluster like so&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ export NODE_NAME=test-network-1&#xA;$ export NODE_PORT=9081&#xA;$ k3d cluster create $NODE_NAME -p &#34;$NODE_PORT:80@loadbalancer&#34; --registry-use k3d-registry.localhost&#xA;$ k3d cluster start $NODE_NAME&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you ever need to reset things you can just kill the docker containers and volumes:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ docker rm $(docker ps -aq) --force&#xA;$ docker volume prune --force || true&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once k3s is running you should see the container in docker.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ docker ps&#xA;CONTAINER ID   IMAGE                                 COMMAND                  CREATED        STATUS              PORTS                                                                                                                                  NAMES&#xA;9b765411a2f2   rancher/k3s:v1.22.7-k3s1         &#34;/bin/k3d-entrypoint‚Ä¶&#34;   20 minutes ago   Up 20 minutes                                                   k3d-test-network-1-server-0&#xA;878bc1bd630c   registry:2                       &#34;/entrypoint.sh /etc‚Ä¶&#34;   20 minutes ago   Up 20 minutes   0.0.0.0:12345-&amp;gt;5000/tcp                         k3d-registry.localhost&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Kubectl&lt;/h3&gt; &#xA;&lt;p&gt;kubectl is the CLI tool for kubernetes. If you have ran k3s it should have configured your kubectl to have a context for the local k3s cluster by default.&lt;/p&gt; &#xA;&lt;p&gt;You should be able to run something like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ kubectl get all&#xA;NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE&#xA;service/kubernetes   ClusterIP   10.96.0.1    &amp;lt;none&amp;gt;        443/TCP   45h&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;k8s Namespaces&lt;/h3&gt; &#xA;&lt;p&gt;Think of a namespace as a grouping of resources and permissions which lets you easily create and destroy everything related to a single keyword.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ kubectl get namespaces&#xA;NAME                   STATUS   AGE&#xA;default                Active   45h&#xA;kube-node-lease        Active   45h&#xA;kube-public            Active   45h&#xA;kube-system            Active   45h&#xA;kubernetes-dashboard   Active   45h&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;All k8s have a default namespace and the other ones here are from kubernetes.&lt;/p&gt; &#xA;&lt;h3&gt;Helm Charts&lt;/h3&gt; &#xA;&lt;p&gt;The most popular way to deploy applications to k8s is with a tool called Helm. What helm aims to do is to provide another layer of abstraction over kubernetes yaml configuration with hierarchical variables, templates and a package definition which can be hosted over HTTP allowing custom applications to depend on other prefabricated helm charts or to provide consumable packages of your code as a helm chart itself.&lt;/p&gt; &#xA;&lt;h3&gt;devspace&lt;/h3&gt; &#xA;&lt;p&gt;To make development and deployment of our kubernetes code easier we use a tool called &lt;code&gt;devspace&lt;/code&gt; which aims to be like a hot reloading dev optimised version of &lt;code&gt;docker compose&lt;/code&gt; but for kubernetes. &lt;a href=&#34;https://devspace.sh/&#34;&gt;https://devspace.sh/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Additionally &lt;code&gt;devspace&lt;/code&gt; allows us to deploy using helm by auto-generating the values and charts from the devspace.yaml which means the single source of truth can be created which includes both production helm charts and kubernetes yaml configuration as well as local dev overrides.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ brew install devspace&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Deploy to local dev&lt;/h3&gt; &#xA;&lt;p&gt;Now run the &lt;code&gt;dev&lt;/code&gt; command with &lt;code&gt;devspace&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;p&gt;To run a network with headscale VPN:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ cd packages/grid&#xA;$ devspace dev -b -p network&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run a domain without the headscale VPN:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ cd packages/grid&#xA;$ devspace dev -b -p domain&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To deploy to your local k3s do the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ export NODE_NAME=test-network-1&#xA;$ export NODE_PORT=9081&#xA;$ devspace --no-warn --kube-context &#34;k3d-$NODE_NAME&#34; --namespace $NODE_NAME \&#xA;           --var DOMAIN_NAME=$NODE_NAME \&#xA;           --var NETWORK_CHECK_INTERVAL=5 \&#xA;           --var CONTAINER_REGISTRY=k3d-registry.localhost:12345/ \&#xA;           deploy -b -p network&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The important thing to note is the custom container registry. This will tag the images with that local registry and k3s will know how to pull them from its custom registry.&lt;/p&gt; &#xA;&lt;p&gt;You may need to add this to your &lt;code&gt;/etc/hosts&lt;/code&gt; file if you are not on linux:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;127.0.0.1 k3d-registry.localhost&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;k9s&lt;/h3&gt; &#xA;&lt;p&gt;There is a tool like ctop which lets you easily see the kubernetes cluster called k9s.&lt;/p&gt; &#xA;&lt;p&gt;Install it like so:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ brew install derailed/k9s/k9s&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When you run it you can select the local context k3d-test-network-1 and see the namespace you deployed.&lt;/p&gt; &#xA;&lt;h3&gt;Destroy the local deployment&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ devspace purge&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Delete persistent volumes&lt;/h3&gt; &#xA;&lt;p&gt;The database and the VPN containers have persistent volumes. You can check them with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ kubectl get persistentvolumeclaim&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And then delete PostgreSQL with something like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ kubectl delete persistentvolumeclaim app-db-data-db-0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Check which images / tags are being used&lt;/h3&gt; &#xA;&lt;p&gt;This will show all the unique images and their tags currently deployed which is useful when debugging which version is actually running in the cluster.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ kubectl get pods --all-namespaces -o jsonpath=&#34;{.items[*].spec.containers[*].image}&#34; | tr -s &#39;[[:space:]]&#39; &#39;\n&#39; | sort | uniq -c&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Restart a container / pod / deployment&lt;/h3&gt; &#xA;&lt;p&gt;Get all the deployments&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ kubectl get deployments&#xA;NAME             READY   UP-TO-DATE   AVAILABLE   AGE&#xA;backend          1/1     1            1           18m&#xA;backend-stream   1/1     1            1           18m&#xA;backend-worker   1/1     1            1           18m&#xA;frontend         1/1     1            1           18m&#xA;queue            1/1     1            1           19m&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Restart the backend-worker&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ kubectl rollout restart deployment backend-worker&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Deploy to Google Kubernetes Engine (GKE)&lt;/h3&gt; &#xA;&lt;p&gt;Configure kubectl context with GKE:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ gcloud container clusters get-credentials --region us-central1-c staging-cluster-1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Check that you have the correct context&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ kubectx&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Configure your Google Container Registry (GCR):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ gcloud auth configure-docker&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Check your settings with print&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ devspace print -p domain --var=CONTAINER_REGISTRY=gcr.io/reflected-space-315806/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You should see that you are creating a domain and that the container registry variable changes the image name to:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;images:&#xA;  backend:&#xA;    image: gcr.io/reflected-space-315806/openmined/grid-backend&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will tell &lt;code&gt;devspace&lt;/code&gt; to publish to the GCR for your active GCP project.&lt;/p&gt; &#xA;&lt;p&gt;Create the openmined namespace&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ kubectl create namespace openmined&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Tell devspace to use the openmined namespace&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ devspace use namespace openmined&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Deploy to GKE:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ devspace deploy -p domain --var=CONTAINER_REGISTRY=gcr.io/reflected-space-315806/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Access a container directly:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ devspace enter&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Attach to container stdout:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ devspace attach&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Use port forwarding to access an internal service:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ kubectl port-forward deployment/tailscale :4000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Deploy to Azure Kubernetes&lt;/h3&gt; &#xA;&lt;p&gt;Create a cluster and optional registry.&lt;/p&gt; &#xA;&lt;p&gt;I named my registry: &lt;code&gt;omazuretest&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Log into the registry with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ az acr login -n omazuretest.azurecr.io&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Check you have permission by tagging and uploading nginx:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ docker pull nginx:latest&#xA;$ docker tag nginx omazuretest.azurecr.io/nginx&#xA;$ docker push omazuretest.azurecr.io/nginx&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Build and Push Images&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;devspace --no-warn --kube-context &#34;azure-test&#34; --namespace azure-network-1 --var DOMAIN_NAME=azure-network-1 --var CONTAINER_REGISTRY=omazuretest.azurecr.io/ build -b&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Deploy&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;devspace --no-warn --kube-context &#34;azure-test&#34; --namespace azure-network-1 --var DOMAIN_NAME=azure-network-1 --var CONTAINER_REGISTRY=omazuretest.azurecr.io/ deploy -p network&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Render Plain Kubernetes Manifests&lt;/h3&gt; &#xA;&lt;p&gt;You can just output plain kubernetes manifests like so:&lt;/p&gt; &#xA;&lt;p&gt;For domains:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ devspace render -p domain --skip-build --silent &amp;gt; k8s/rendered/domain.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For networks:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ devspace render -p network --skip-build --silent &amp;gt; k8s/rendered/network.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Publish&lt;/h2&gt; &#xA;&lt;h3&gt;HAGrid&lt;/h3&gt; &#xA;&lt;h4&gt;PyPI&lt;/h4&gt; &#xA;&lt;p&gt;To publish hagrid to pypi create a virtualenv in the &lt;code&gt;/packages/hagrid&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ cd packages/hagrid&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Use a tool like &lt;code&gt;pipenv&lt;/code&gt; or manually create and source like so:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ python3 -m venv ./venv&#xA;$ source .venv/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install some tools:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ pip install --upgrade bandit safety setuptools wheel twine tox&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Bump the version inside &lt;code&gt;/packages/hagrid/hagrid/__init__.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Build a wheel:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ./build_wheel.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Check with twine:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ twine check `find -L ./ -name &#34;*.whl&#34;`&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Upload with twine:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ twine upload dist/hagrid-x.x.x-py3-none-any.whl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Docker&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ cd packages/hagrid&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Build and tag the images:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ./build_docker.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Publish to docker hub:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ docker push openmined/hagrid:latest&#xA;$ docker push openmined/hagrid:x.x.x&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Join Slack&lt;/h2&gt; &#xA;&lt;p&gt;Also, join the rapidly growing community of 12,000+ on &lt;a href=&#34;http://slack.openmined.org&#34;&gt;Slack&lt;/a&gt;. The Slack community is very friendly and great about quickly answering questions about the use and development of PySyft!&lt;/p&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This software is in beta. Use at your own risk.&lt;/p&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;p&gt;For support in using this library, please join the &lt;strong&gt;#support&lt;/strong&gt; Slack channel. &lt;a href=&#34;https://slack.openmined.org&#34;&gt;Click here to join our Slack community!&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Organizational Contributions&lt;/h2&gt; &#xA;&lt;p&gt;We are very grateful for contributions to Syft and Grid from the following organizations!&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://udacity.com/&#34;&gt;&lt;img src=&#34;https://github.com/udacity/private-ai/raw/master/udacity-logo-vert-white.png?raw=true&#34; alt=&#34;Udacity&#34; width=&#34;160&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/coMindOrg/federated-averaging-tutorials&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenMined/PySyft/main/packages/syft/docs/img/organizations/comind.png&#34; alt=&#34;coMind&#34; width=&#34;160&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://ark.hn&#34;&gt;&lt;img src=&#34;https://i.ibb.co/vYwcG9N/arkhn-logo.png&#34; alt=&#34;Arkhn&#34; width=&#34;160&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dropoutlabs.com/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dropoutlabs/files/master/dropout-labs-logo-white-2500.png&#34; alt=&#34;Dropout Labs&#34; width=&#34;160&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://genbu.ai/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenMined/PySyft/main/packages/syft/docs/img/organizations/genbu.png&#34; alt=&#34;GENBU AI&#34; width=&#34;160&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bitdefender.com/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenMined/PySyft/main/packages/syft/docs/img/organizations/bitdefender.png&#34; alt=&#34;Bitdefender&#34; width=&#34;160&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/OpenMined/PySyft/raw/main/packages/syft/LICENSE&#34;&gt;Apache License 2.0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>NeuromatchAcademy/course-content-dl</title>
    <updated>2022-07-22T01:45:20Z</updated>
    <id>tag:github.com,2022-07-22:/NeuromatchAcademy/course-content-dl</id>
    <link href="https://github.com/NeuromatchAcademy/course-content-dl" rel="alternate"></link>
    <summary type="html">&lt;p&gt;NMA deep learning course&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Neuromatch Academy Deep Learning (NMA-DL) syllabus&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;July 11-29, 2022&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Objectives&lt;/strong&gt;: Gain hands-on, code-first experience with deep learning theories, models, and skills that are useful for applications and for advancing science. We focus on how to decide which problems can be tackled with deep learning, how to determine what model is best, how to best implement a model, how to visualize / justify findings, and how neuroscience can inspire deep learning. And throughout we emphasize the ethical use of DL.&lt;/p&gt; &#xA;&lt;p&gt;Please check out &lt;a href=&#34;https://github.com/NeuromatchAcademy/precourse/raw/main/prereqs/DeepLearning.md&#34;&gt;expected prerequisites here&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;p&gt;The content should primarily be accessed from our ebook: &lt;a href=&#34;https://deeplearning.neuromatch.io/&#34;&gt;https://deeplearning.neuromatch.io/&lt;/a&gt; [under continuous development]&lt;/p&gt; &#xA;&lt;p&gt;Schedule for 2022: &lt;a href=&#34;https://github.com/NeuromatchAcademy/course-content-dl/raw/main/tutorials/Schedule/daily_schedules.md&#34;&gt;https://github.com/NeuromatchAcademy/course-content-dl/blob/main/tutorials/Schedule/daily_schedules.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Licensing&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://creativecommons.org/licenses/by/4.0/&#34;&gt;&lt;img src=&#34;https://i.creativecommons.org/l/by/4.0/88x31.png&#34; alt=&#34;CC BY 4.0&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://creativecommons.org/licenses/by/4.0/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg?sanitize=true&#34; alt=&#34;CC BY 4.0&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/BSD-3-Clause&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/9b9ea65d95c9ef878afa1987df65731d47681336/68747470733a2f2f696d672e736869656c64732e696f2f707970692f6c2f736561626f726e2e737667&#34; alt=&#34;BSD-3&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The contents of this repository are shared under under a &lt;a href=&#34;http://creativecommons.org/licenses/by/4.0/&#34;&gt;Creative Commons Attribution 4.0 International License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Software elements are additionally licensed under the &lt;a href=&#34;https://opensource.org/licenses/BSD-3-Clause&#34;&gt;BSD (3-Clause) License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Derivative works may use the license that is more appropriate to the relevant context.&lt;/p&gt;</summary>
  </entry>
</feed>