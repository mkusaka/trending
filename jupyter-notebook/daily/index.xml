<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-10-06T01:32:05Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>facebookresearch/maws</title>
    <updated>2023-10-06T01:32:05Z</updated>
    <id>tag:github.com,2023-10-06:/facebookresearch/maws</id>
    <link href="https://github.com/facebookresearch/maws" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code and models for the paper &#34;The effectiveness of MAE pre-pretraining for billion-scale pretraining&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MAWS&lt;/h1&gt; &#xA;&lt;p&gt;Models for the paper &lt;a href=&#34;https://arxiv.org/abs/2303.13496&#34;&gt;The effectiveness of MAE pre-pretraining for billion-scale pretraining&lt;/a&gt; for both MAE pre-pretraining and the follow up WSP pretraining, MAE→WSP, which we call MAWS (Masked Autoencoding → Weakly Supervised pretraining).&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;539&#34; alt=&#34;image&#34; src=&#34;https://github.com/facebookresearch/maws/assets/13458796/69afa2ca-9976-4c64-9814-1f906be05e36&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;To get started with playing with our models immediately, we have a notebook available to play with on &lt;a href=&#34;https://colab.research.google.com/github/facebookresearch/maws/blob/main/clip_example.ipynb&#34;&gt;Colab&lt;/a&gt;, or &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/maws/main/clip_example.ipynb&#34;&gt;locally&lt;/a&gt; for running our models in zero-shot mode.&lt;/p&gt; &#xA;&lt;p&gt;For building any of our models, select which model type you would like to build. We have models available for:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;model_type=&#34;maws&#34;&lt;/code&gt;: MAWS (MAE→WSP) pretraining, i.e. MAE pre-pretraining followed by WSP pretraining&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;model_type=&#34;maws_clip&#34;&lt;/code&gt;: MAWS pretrained models along with LiT aligned text encoders for CLIP style zero shot classification&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;model_type=&#34;mae&#34;&lt;/code&gt;: MAE pretrained models&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;model_type=&#34;mae_in1k&#34;&lt;/code&gt;: MAE pretrained on ImageNet-1k models&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;To access a model, specify the model architecture and the model type:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from maws.model import build_model&#xA;&#xA;# build a MAWS model with CLIP capabilities (via an aligned text encoder)&#xA;clip_model = build_model(&#34;vit_b16_xlmr_b&#34;, &#34;maws_clip&#34;)&#xA;&#xA;# build a MAWS model&#xA;maws_model = build_model(&#34;vit_b16&#34;, &#34;maws&#34;)&#xA;&#xA;# build an MAE model&#xA;mae_model = build_model(&#34;vit_b16&#34;, &#34;mae&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The models are also available via torch.hub:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# build a MAWS model with CLIP capabilities (via an aligned text encoder)&#xA;clip_model = torch.hub.load(&#34;facebookresearch/maws&#34;, model=&#34;vit_b16_xlmr_b_maws_clip&#34;)&#xA;&#xA;# build a MAWS model&#xA;maws_model = torch.hub.load(&#34;facebookresearch/maws&#34;, model=&#34;vit_b16_maws&#34;)&#xA;&#xA;# build an MAE model&#xA;mae_model = torch.hub.load(&#34;facebookresearch/maws&#34;, model=&#34;vit_b16_mae&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We list down all the available models and direct download links in the following section.&lt;/p&gt; &#xA;&lt;h3&gt;Installation instructions&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create --name maws python=3.10&#xA;conda activate maws&#xA;pip install torch torchvision torchtext&#xA;pip install timm==0.9.7&#xA;# for demo&#xA;pip install jupyter ipywidgets matplotlib&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Available models&lt;/h2&gt; &#xA;&lt;h3&gt;MAWS pretrained models&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Model name + weights&lt;/th&gt; &#xA;   &lt;th&gt;IN1k 224px linear&lt;/th&gt; &#xA;   &lt;th&gt;IN1k 512/518px finetuned&lt;/th&gt; &#xA;   &lt;th&gt;Text encoder&lt;/th&gt; &#xA;   &lt;th&gt;Model name + weights&lt;/th&gt; &#xA;   &lt;th&gt;IN1k 224px 0-shot&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/maws/pretrain/maws/vit_b16.pt&#34;&gt;vit_b16&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;83.3&lt;/td&gt; &#xA;   &lt;td&gt;86.4&lt;/td&gt; &#xA;   &lt;td&gt;XLMR-B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/maws/pretrain/clip/vit_b16_xlmr_b.pt&#34;&gt;vit_b16_xlmr_b&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;74.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-L&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/maws/pretrain/maws/vit_l16.pt&#34;&gt;vit_l16&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;86.1&lt;/td&gt; &#xA;   &lt;td&gt;88.8&lt;/td&gt; &#xA;   &lt;td&gt;XLMR-L&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/maws/pretrain/clip/vit_l16_xlmr_l.pt&#34;&gt;vit_l16_xlmr_l&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;79.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-H&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/maws/pretrain/maws/vit_h14.pt&#34;&gt;vit_h14&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;87.5&lt;/td&gt; &#xA;   &lt;td&gt;89.4&lt;/td&gt; &#xA;   &lt;td&gt;XLMR-L&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/maws/pretrain/clip/vit_h14_xlmr_l.pt&#34;&gt;vit_h14_xlmr_l&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;81.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-2B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/maws/pretrain/maws/vit_2b14.pt&#34;&gt;vit_2b14&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;88.1&lt;/td&gt; &#xA;   &lt;td&gt;89.7&lt;/td&gt; &#xA;   &lt;td&gt;XLMR-L&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/maws/pretrain/clip/vit_2b14_xlmr_l.pt&#34;&gt;vit_2b14_xlmr_l&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;82.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;MAE pretrained models&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Model name + weights&lt;/th&gt; &#xA;   &lt;th&gt;IN1k 224px finetuned&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/mae/pretrain/mae/vit_b16.pt&#34;&gt;vit_b16&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;83.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-L&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/maws/pretrain/mae/vit_l16.pt&#34;&gt;vit_l16&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;86.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-H&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/maws/pretrain/mae/vit_h14.pt&#34;&gt;vit_h14&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;87.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-2B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/maws/pretrain/mae/vit_2b14.pt&#34;&gt;vit_2b14&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;87.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;MAE pretrained on ImageNet-1k&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Model name + weights&lt;/th&gt; &#xA;   &lt;th&gt;IN1k 224px finetuned&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-2B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/maws/pretrain/mae_in1k/vit_2b14.pt&#34;&gt;vit_2b14&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;87.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Zero-shot evaluation on ImageNet-1k&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to all the available model names in the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/maws/main/#maws-pretrained-models&#34;&gt;MAWS Pretrained models&lt;/a&gt; section. &lt;code&gt;$IN1K_VAL_PATH&lt;/code&gt; should be the path to the ImageNet-1k val root folder.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python eval_zeroshot.py -m vit_b16_xlmr_b -p $IN1K_VAL_PATH&#xA;# You should get 74.888 top-1 accuracy&#xA;&#xA;# Trying the french language instead with a larger model on a 32GB V100&#xA;python eval_zeroshot.py -m vit_2b14_xlmr_l -p /datasets01/imagenet_full_size/061417/val/ -b 5 --language french&#xA;# You should get 62.622 top-1 accuracy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use our models or if the work is useful in your research, please give us a star and cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{singh2023effectiveness,&#xA;    title={The effectiveness of MAE pre-pretraining for billion-scale pretraining},&#xA;    author={Singh, Mannat and Duval, Quentin and Alwala, Kalyan Vasudev and Fan, Haoqi and Aggarwal, Vaibhav and Adcock, Aaron and Joulin, Armand and Doll{\&#39;a}r, Piotr and Feichtenhofer, Christoph and Girshick, Ross and Girdhar, Rohit and Misra, Ishan},&#xA;    booktitle={ICCV},&#xA;    year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Our models are released under the CC-BY-NC 4.0 license. See &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/maws/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; for additional details.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mttaggart/electron-app-tracker</title>
    <updated>2023-10-06T01:32:05Z</updated>
    <id>tag:github.com,2023-10-06:/mttaggart/electron-app-tracker</id>
    <link href="https://github.com/mttaggart/electron-app-tracker" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Electron Vulnerability Tracker&lt;/h1&gt; &#xA;&lt;p&gt;This Notebook tracks known &lt;a href=&#34;https://electronjs.org&#34;&gt;Electron&lt;/a&gt; apps for vulnerabilities. This knowledge is valuable given the frequent release of vulns in the browser technologies that comprise Electron, although these apps often fly under the radar for remediation.&lt;/p&gt; &#xA;&lt;p&gt;This work supersedes the lists hosted on &lt;a href=&#34;https://docs.google.com/spreadsheets/d/1QLLFYCO0FMAu1ob6mnYCapW8dnx-HXunbf_zc9QLXlM/edit?usp=sharing&#34;&gt;Google Sheets&lt;/a&gt; and &lt;a href=&#34;https://gist.github.com/mttaggart/02ed50c03c8283f4c343c3032dd2e7ec&#34;&gt;Github Gist&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;The Lists&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mttaggart/electron-app-tracker/raw/main/electron_apps.csv&#34;&gt;CSV&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mttaggart/electron-app-tracker/raw/main/electron_apps.json&#34;&gt;JSON&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Methodology&lt;/h2&gt; &#xA;&lt;p&gt;The original list of apps was sourced from &lt;a href=&#34;https://electron.js.org/apps&#34;&gt;Electron&#39;s own site&lt;/a&gt;, and has been amended with community support since then.&lt;/p&gt; &#xA;&lt;p&gt;This script both sources from and dynamically updates the CSV (and JSON) files holding the app info. If an app has a repo associated, then the repo is queried, using &lt;code&gt;requests&lt;/code&gt; and &lt;code&gt;BeautifulSoup&lt;/code&gt; for parsing. We&#39;re avoiding the GitHub API to avoid rate limiting. Each repo is then examined for &lt;code&gt;package.json&lt;/code&gt; files. If none is found in the repo root, immediate subdirectories are queries. For sanity, we only query immediate subdirs and we only query &lt;code&gt;main&lt;/code&gt; and &lt;code&gt;master&lt;/code&gt; branches. That means some apps will be missed in the scrape due to using version branches that are ahead of &lt;code&gt;main&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If a &lt;code&gt;package.json&lt;/code&gt; file is found, it is analyzed for the presence of &lt;code&gt;electron&lt;/code&gt; as a dependency. This version is then tested against known-patched versions of Electron for each listed vulnerability. If the semantic version is equal or greater, we show &lt;code&gt;patched&lt;/code&gt; for that vuln; otherwise, we show &lt;code&gt;vulnerable&lt;/code&gt;. Each scrape is timestamped, so new data will update existing data.&lt;/p&gt; &#xA;&lt;h2&gt;Additional CVEs&lt;/h2&gt; &#xA;&lt;p&gt;Electron will continue to inherit vulnerabilities, so this tracker is designed to add CVEs to its checks as time goes on. All that&#39;s required is adding to the defined dict in &lt;a href=&#34;https://github.com/mttaggart/electron-app-tracker/raw/main/electron_tracker/vulncheck.py&#34;&gt;&lt;code&gt;electron_tracker/vulncheck.py&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Contributing is easy! Simply open a Pull Request updating &lt;code&gt;electron_apps.csv&lt;/code&gt; with a &lt;code&gt;manual&lt;/code&gt; access type. Please provide evidence of the versions of the apps you&#39;re updating. We are particularly in need of any updates on the apps listed as &lt;code&gt;manual&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;And thank you for helping with the effort!&lt;/p&gt;</summary>
  </entry>
</feed>