<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-12-31T01:34:08Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>HMS-CardiacMR/DRAPR</title>
    <updated>2022-12-31T01:34:08Z</updated>
    <id>tag:github.com,2022-12-31:/HMS-CardiacMR/DRAPR</id>
    <link href="https://github.com/HMS-CardiacMR/DRAPR" rel="alternate"></link>
    <summary type="html">&lt;p&gt;We implemented a 3D (2D+time) convolutional neural network to suppress streaking artifacts from undersampled radial cine images. We trained the network using synthetic real-time radial cine images simulated using ECG-gated segmented Cartesian k-space data, which was acquired from 503 patients during breath-hold and at rest. Further, we implement…&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DRAPR: Deep-Learning Radial Acceleration with Parallel Reconstruction&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;We implemented a 3D (2D+time) convolutional neural network to suppress streaking artifacts from undersampled radial cine images. We trained the network using synthetic real-time radial cine images simulated using ECG-gated segmented Cartesian k-space data, which was acquired from 503 patients during breath-hold and at rest. Further, we implemented a prototype real-time radial sequence with acceleration rate = 12 on a 3T scanner, and used it to collect cine images with inline DL reconstruction whose total reconstruction time was 16.6 ms per frame. We evaluated the performance of the proposed approach by initially recruiting 9 healthy subjects in whom only rest images were collected. Subsequently, we recruited 14 subjects who participated in an exercise CMR imaging protocol in which both rest and post-exercise images were collected, including 8 patients with suspected coronary artery disease. Exercise was done using a CMR-compatible supine cycle ergometer positioned on the MR table.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Abstract&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Purpose&lt;/strong&gt;: To develop and evaluate a free-breathing and ECG-free real-time cine with deep learning (DL)-based radial acceleration for exercise CMR.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Method&lt;/strong&gt;: We implemented a 3D (2D+time) convolutional neural network to suppress streaking artifacts from undersampled radial cine images. We trained the network using synthetic real-time radial cine images simulated using ECG-gated segmented Cartesian k-space data, which was acquired from 503 patients during breath-hold and at rest. Further, we implemented a prototype real-time radial sequence with acceleration rate = 12 on a 3T scanner, and used it to collect cine images with inline DL reconstruction whose total reconstruction time was 16.6 ms per frame. We evaluated the performance of the proposed approach by initially recruiting 9 healthy subjects in whom only rest images were collected. Subsequently, we recruited 14 subjects who participated in an exercise CMR imaging protocol in which both rest and post-exercise images were collected, including 8 patients with suspected coronary artery disease. Exercise was done using a CMR-compatible supine cycle ergometer positioned on the MR table.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;: the DL model substantially suppressed streaking artifacts in free-breathing ECG-free real-time cine images acquired at rest and during post-exercise stress. Three readers evaluated residual artifact level in the entire field-of-view on a 3-point Likert scale (1-severe, 2-moderate, 3-minimal). In real-time images at rest, 89.4% of scores were moderate to minimal. The mean score was 2.3 ± 0.7, representing a significantly (P&amp;lt;0.001) higher amount of artifacts compared to ECG-gated (2.9 ± 0.3). In real-time images during post-exercise stress, 84.6% of scores were moderate to minimal, and the mean artifact level score was 2.1 ± 0.6. Comparison of left-ventricular measures derived from ECG-gated segmented and real-time cine images at rest showed differences in end-diastolic volume (-3.0 mL [-17.8 11.7], P=0.540) and mass (-0.6 g [-13.4 12.2], P=0.880) that were not significantly different from zero. Differences in measures of end-systolic volume (-7.0 mL [-15.3 1.3], P=020) and ejection fraction (5.0% [-1.0 11.1], P=0.037) were significant.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;: Our feasibility study demonstrated the potential of inline real-time cine with DL-based radial acceleration for Ex-CMR.&lt;/p&gt; &#xA;&lt;h2&gt;Off-Line Implenetation&lt;/h2&gt; &#xA;&lt;p&gt;Our code may be used to reconstruct raw k-space data offline. A Jupyter &lt;a href=&#34;https://github.com/HMS-CardiacMR/RealTimeCine/raw/main/notebooks/tutorial_basic.ipynb&#34;&gt;notebook&lt;/a&gt; tutorial is provided to demonstrate how to use our code.&lt;/p&gt; &#xA;&lt;h2&gt;In-Line Implenetation&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/HMS-CardiacMR/DRAPR/main/vids/figure_2.png&#34; width=&#34;100%&#34;&gt; &#xA;&lt;p&gt;Multi-coil raw radial k-space data acquired from the scanner is processed in the Image Reconstruction Environment (ICE) on the vendor reconstruction computer. Using the International Society for Magnetic Resonance in Medicine Raw Data (ISMRMRD) format, the collected data is transferred to the &lt;a href=&#34;https://github.com/kspaceKelvin/python-ismrmrd-server&#34;&gt;Framework for Image Reconstruction&lt;/a&gt; (FIRE) server using a FireEmitter functor. The FIRE server is located in the vendor reconstruction computer. The data is then transferred from the FIRE server to an external server via a connecting Secure Shell Protocol (SSH) tunnel. In the external sever, a Docker containing all Python dependencies such as PyTorch is used to process the raw k-space data in a single 32 GB Graphics Processing Unit (GPU). First, a non-uniform fast Fourier transform (NUFFT) is used to grid and reconstruct undersampled multi-coil radial k-space data. GPU parallelization is done in PyTorch by treating frames and coils as batch and channel dimensions. This approach enables application of the NUFFT at 10 ms per frame. Coil sensitivity and combination is subsequently performed in PyTorch at negligible computational cost. These coil-combined images are send to the U-Net for de-aliasing, which requires 6.6 ms per frame. The total processing time of 16.6 ms is about half the 37.7 ms temporal resolution of collected frames. Images are then returned to the FIRE server via the same SSH tunnel, and to ICE using a FireInjector functor. Finally, the reconstructed de-aliased images are finalized into DICOM format and returned to the scanner computer console for immediate display.&lt;/p&gt; &#xA;&lt;h3&gt;Launching FIRE Server&lt;/h3&gt; &#xA;&lt;p&gt;It is recommended to first create a virtual enviornment prior to running the code.&lt;/p&gt; &#xA;&lt;p&gt;Create Python venv with Python:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt; python -m venv drapr-venv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Activate Virtual Enviornment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;source drapr-venv/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install dependinces:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirments.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once the necessary libraries have been installed the FIRE server can be launched:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python main.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You will then see the following message appear in your console:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;2021-07-26 18:22:16,465 - Starting server and listening for data at 0.0.0.0:9002&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This indicates the FIRE is running and will process any data sent to it via port 9002.&lt;/p&gt; &#xA;&lt;h2&gt;Comparison to GRASP&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/HMS-CardiacMR/DRAPR/main/vids/video_1_lowres.gif&#34; align=&#34;left&#34; width=&#34;100%&#34;&gt; &#xA;&lt;h2&gt;Application to Exercise CMR&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/HMS-CardiacMR/DRAPR/main/vids/video_3_lowres.gif&#34; align=&#34;left&#34; width=&#34;100%&#34;&gt; &#xA;&lt;h2&gt;Application to Perfusion Imaging&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=2r85g0-Mwis&#34; title=&#34;Click to play video&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/2r85g0-Mwis/maxresdefault.jpg&#34; alt=&#34;Radial Perfusion Cardiac Magnetic Resonance Imaging Using Deep Learning Image Reconstruction&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Publications&lt;/h2&gt; &#xA;&lt;p&gt;If you use DRAPR or some part of the code, please cite:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;An Inline Deep Learning Based Free-Breathing ECG-Free Cine for Exercise CMR.&lt;/strong&gt; &lt;a href=&#34;https://cardiacmr.hms.harvard.edu/people/manuel-morales-phd&#34;&gt;Manuel A. Morales&lt;/a&gt;, &lt;a href=&#34;https://cardiacmr.hms.harvard.edu/people/salah-assana&#34;&gt;Salah Assana&lt;/a&gt;, &lt;a href=&#34;https://cardiacmr.hms.harvard.edu/people/xiaoying-cai-phd&#34;&gt;Xiaoying Cai&lt;/a&gt;, &lt;a href=&#34;https://marketing.webassets.siemens-healthineers.com/1800000007010698/f017dc5c4ecd/Siemens-Healthineers-Meet_Healthineers_Kelvin_Chow_1800000007010698.pdf&#34;&gt;Kelvin Chow&lt;/a&gt;, &lt;a href=&#34;https://cardiacmr.hms.harvard.edu/people/hassan-haji-valizadeh-phd&#34;&gt;Hassan Haji-valizadeh&lt;/a&gt;, &lt;a href=&#34;https://cardiacmr.hms.harvard.edu/people/eiryu-sai-md-phd&#34;&gt;Eiryu Sai&lt;/a&gt;, &lt;a href=&#34;https://cardiacmr.hms.harvard.edu/people/connie-tsao&#34;&gt;Connie Tsao&lt;/a&gt;, &lt;a href=&#34;https://cardiacmr.hms.harvard.edu/people/jason-matos-md&#34;&gt;Jason Matos&lt;/a&gt;, &lt;a href=&#34;https://cardiacmr.hms.harvard.edu/people/jennifer-rodriguez&#34;&gt;Jennifer Rodriguez&lt;/a&gt;, &lt;a href=&#34;https://cardiacmr.hms.harvard.edu/people/sophie-berg&#34;&gt;Sophie Berg&lt;/a&gt;, &lt;a href=&#34;https://cardiacmr.hms.harvard.edu/people/neal-whitehead-rn&#34;&gt;Neal Whitehead&lt;/a&gt;, &lt;a href=&#34;https://cardiacmr.hms.harvard.edu/people/patrick-pierce&#34;&gt;Patrick Pierce&lt;/a&gt;, &lt;a href=&#34;https://cardiacmr.hms.harvard.edu/people/beth-goddu&#34;&gt;Beth Goddu&lt;/a&gt;, &lt;a href=&#34;https://cardiacmr.hms.harvard.edu/people/warren-j-manning&#34;&gt;Warren J. Manning&lt;/a&gt;, &lt;a href=&#34;https://cardiacmr.hms.harvard.edu/people/reza-nezafat&#34;&gt;Reza Nezafat&lt;/a&gt;. Under revision.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Radial perfusion cardiac magnetic resonance imaging using deep learning image reconstruction.&lt;/strong&gt; &lt;a href=&#34;https://cardiacmr.hms.harvard.edu/people/salah-assana&#34;&gt;Salah Assana&lt;/a&gt;, &lt;a href=&#34;https://cardiacmr.hms.harvard.edu/people/manuel-morales-phd&#34;&gt;Manuel A. Morales&lt;/a&gt;, &lt;a href=&#34;https://cardiacmr.hms.harvard.edu/people/kei-nakata-md-phd&#34;&gt;Kei Nakata&lt;/a&gt;, &lt;a href=&#34;https://cardiacmr.hms.harvard.edu/people/eiryu-sai-md-phd&#34;&gt;Eiryu Sai&lt;/a&gt;, &lt;a href=&#34;https://cardiacmr.hms.harvard.edu/people/amine-amyar-phd&#34;&gt;Amine Amyar&lt;/a&gt;, &lt;a href=&#34;https://cardiacmr.hms.harvard.edu/people/hassan-haji-valizadeh-phd&#34;&gt;Hassan Haji-valizadeh&lt;/a&gt;, and &lt;a href=&#34;https://cardiacmr.hms.harvard.edu/people/reza-nezafat&#34;&gt;Reza Nezafat&lt;/a&gt;. Poster &lt;a href=&#34;https://submissions.mirasmart.com/ISMRM2022/Itinerary/ConferenceMatrixEventDetail.aspx?ses=G-169&#34;&gt;Presentation&lt;/a&gt; at ISMRM 2022. &lt;em&gt;Inline implementation for perfusion coming soon!&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>