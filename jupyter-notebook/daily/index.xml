<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-12-18T01:34:12Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ddPn08/automatic1111-colab</title>
    <updated>2022-12-18T01:34:12Z</updated>
    <id>tag:github.com,2022-12-18:/ddPn08/automatic1111-colab</id>
    <link href="https://github.com/ddPn08/automatic1111-colab" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Using StableDiffusion webui on Colab&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CoLab-compatible version of StableDiffusionWebui by Automatic1111&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/ddPn08/stable-diffusion-webui-colab/blob/main/automatic1111.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?message=Open%20in%20Colab&amp;amp;logo=googlecolab&amp;amp;labelColor=5c5c5c&amp;amp;color=0f80c1&amp;amp;label=%20&amp;amp;style=for-the-badge&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Features&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Xformers support &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Using Miniconda to make Xformers available.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Fast setup &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;If you have space in Google Drive, you can save your environment for faster setup.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Model download support &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Any model on the internet can be downloaded by entering the URL&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Google Drive support &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;You can also save output images, models, embeddings, configuration files, etc...&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Ngrok tunneling support &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;You can use Ngrok tunneling for more stable communication.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Advanced options &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;You can easily change advanced options by simply clicking a checkbox.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Respect&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui&#34;&gt;automatic1111/stable-diffusion-webui&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>deep-learning-with-pytorch/dlwpt-code</title>
    <updated>2022-12-18T01:34:12Z</updated>
    <id>tag:github.com,2022-12-18:/deep-learning-with-pytorch/dlwpt-code</id>
    <link href="https://github.com/deep-learning-with-pytorch/dlwpt-code" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code for the book Deep Learning with PyTorch by Eli Stevens, Luca Antiga, and Thomas Viehmann.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Deep Learning with PyTorch&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains code for the book Deep Learning with PyTorch by Eli Stevens, Luca Antiga, and Thomas Viehmann, published by Manning Publications.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/deep-learning-with-pytorch/dlwpt-code/master/data/Stevens-DLPy-HI.png&#34; alt=&#34;Image of the cover for Deep Learning with PyTorch&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The Manning site for the book is: &lt;a href=&#34;https://www.manning.com/books/deep-learning-with-pytorch&#34;&gt;https://www.manning.com/books/deep-learning-with-pytorch&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The book can also be purchased on Amazon: &lt;a href=&#34;https://amzn.to/38Iwrff&#34;&gt;https://amzn.to/38Iwrff&lt;/a&gt; (affiliate link; as per the rules: &#34;As an Amazon Associate I earn from qualifying purchases.&#34;)&lt;/p&gt; &#xA;&lt;p&gt;The errata for the book can be found on the manning website, or at &lt;a href=&#34;https://deep-learning-with-pytorch.github.io/dlwpt-code/errata.html&#34;&gt;https://deep-learning-with-pytorch.github.io/dlwpt-code/errata.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;About Deep Learning with PyTorch&lt;/h2&gt; &#xA;&lt;p&gt;This book has the aim of providing the foundations of deep learning with PyTorch and showing them in action in a real-life project. We strive to provide the key concepts underlying deep learning and show how PyTorch puts them in the hands of practitioners. In the book, we try to provide intuition that will support further exploration, and in doing so we selectively delve into details to show what is going on behind the curtain. Deep Learning with PyTorch doesn’t try to be a reference book; rather, it’s a conceptual companion that will allow you to independently explore more advanced material online. As such, we focus on a subset of the features offered by PyTorch. The most notable absence is recurrent neural networks, but the same is true for other parts of the PyTorch API.&lt;/p&gt; &#xA;&lt;h2&gt;Who should read this book&lt;/h2&gt; &#xA;&lt;p&gt;This book is meant for developers who are or aim to become deep learning practitioners and who want to get acquainted with PyTorch. We imagine our typical reader to be a computer scientist, data scientist, or software engineer, or an undergraduateor-later student in a related program. Since we don’t assume prior knowledge of deep learning, some parts in the first half of the book may be a repetition of concepts that are already known to experienced practitioners. For those readers, we hope the exposition will provide a slightly different angle to known topics. We expect readers to have basic knowledge of imperative and object-oriented programming. Since the book uses Python, you should be familiar with the syntax and operating environment. Knowing how to install Python packages and run scripts on your platform of choice is a prerequisite. Readers coming from C++, Java, JavaScript, Ruby, or other such languages should have an easy time picking it up but will need to do some catch-up outside this book. Similarly, being familiar with NumPy will be useful, if not strictly required. We also expect familiarity with some basic linear algebra, such as knowing what matrices and vectors are and what a dot product is.&lt;/p&gt; &#xA;&lt;h2&gt;About the authors&lt;/h2&gt; &#xA;&lt;p&gt;Eli Stevens has spent the majority of his career working at startups in Silicon Valley, with roles ranging from software engineer (making enterprise networking appliances) to CTO (developing software for radiation oncology). At publication, he is working on machine learning in the self-driving-car industry.&lt;/p&gt; &#xA;&lt;p&gt;Luca Antiga worked as a researcher in biomedical engineering in the 2000s, and spent the last decade as a cofounder and CTO of an AI engineering company. He has contributed to several open source projects, including the PyTorch core. He recently cofounded a US-based startup focused on infrastructure for data-defined software.&lt;/p&gt; &#xA;&lt;p&gt;Thomas Viehmann is a machine learning and PyTorch specialty trainer and consultant based in Munich, Germany, and a PyTorch core developer. With a PhD in mathematics, he is not scared by theory, but he is thoroughly practical when applying it to computing challenges.&lt;/p&gt;</summary>
  </entry>
</feed>