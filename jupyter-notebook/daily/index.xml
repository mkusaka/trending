<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-07T01:38:12Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>p0p4k/vits2_pytorch</title>
    <updated>2023-08-07T01:38:12Z</updated>
    <id>tag:github.com,2023-08-07:/p0p4k/vits2_pytorch</id>
    <link href="https://github.com/p0p4k/vits2_pytorch" rel="alternate"></link>
    <summary type="html">&lt;p&gt;unofficial vits2-TTS implementation in pytorch&lt;/p&gt;&lt;hr&gt;&lt;p&gt;[this is a work in progress, feel free to contribute! Model will be ready if this line is removed]&lt;/p&gt; &#xA;&lt;h1&gt;VITS2: Improving Quality and Efficiency of Single-Stage Text-to-Speech with Adversarial Learning and Architecture Design&lt;/h1&gt; &#xA;&lt;h3&gt;Jungil Kong, Jihoon Park, Beomjeong Kim, Jeongmin Kim, Dohee Kong, Sangjin Kim&lt;/h3&gt; &#xA;&lt;p&gt;Unofficial implementation of the &lt;a href=&#34;https://arxiv.org/abs/2307.16430&#34;&gt;VITS2 paper&lt;/a&gt;, sequel to &lt;a href=&#34;https://arxiv.org/abs/2106.06103&#34;&gt;VITS paper&lt;/a&gt;. (thanks to the authors for their work!)&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/p0p4k/vits2_pytorch/main/image.png&#34; alt=&#34;Alt text&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Single-stage text-to-speech models have been actively studied recently, and their results have outperformed two-stage pipeline systems. Although the previous single-stage model has made great progress, there is room for improvement in terms of its intermittent unnaturalness, computational efficiency, and strong dependence on phoneme conversion. In this work, we introduce VITS2, a single-stage text-to-speech model that efficiently synthesizes a more natural speech by improving several aspects of the previous work. We propose improved structures and training mechanisms and present that the proposed methods are effective in improving naturalness, similarity of speech characteristics in a multi-speaker model, and efficiency of training and inference. Furthermore, we demonstrate that the strong dependence on phoneme conversion in previous works can be significantly reduced with our method, which allows a fully end-toend single-stage approach.&lt;/p&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;p&gt;We will build this repo based on the &lt;a href=&#34;https://github.com/jaywalnut310/vits&#34;&gt;VITS repo&lt;/a&gt;. Currently I am adding vits2 changes in the &#39;notebooks&#39; folder. The goal is to make this model easier to transfer learning from VITS pretrained model!&lt;/p&gt; &#xA;&lt;h2&gt;Jupyter Notebook for initial experiments&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; check the &#39;notebooks&#39; folder&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to run (dry-run)&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;build monotonic alignment&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Cython-version Monotonoic Alignment Search&#xA;cd monotonic_align&#xA;python setup.py build_ext --inplace&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;model forward pass (dry-run)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from models import SynthesizerTrn&#xA;net_g = SynthesizerTrn(&#xA;    n_vocab=256,&#xA;    spec_channels=80, # &amp;lt;--- vits2 parameter (changed from 513 to 80)&#xA;    segment_size=8192,&#xA;    inter_channels=192,&#xA;    hidden_channels=192,&#xA;    filter_channels=768,&#xA;    n_heads=2,&#xA;    n_layers=6,&#xA;    kernel_size=3,&#xA;    p_dropout=0.1,&#xA;    resblock=&#34;1&#34;, &#xA;    resblock_kernel_sizes=[3, 7, 11],&#xA;    resblock_dilation_sizes=[[1, 3, 5], [1, 3, 5], [1, 3, 5]],&#xA;    upsample_rates=[8, 8, 2, 2],&#xA;    upsample_initial_channel=512,&#xA;    upsample_kernel_sizes=[16, 16, 4, 4],&#xA;    n_speakers=0,&#xA;    gin_channels=0,&#xA;    use_sdp=True, &#xA;    use_transformer_flows=True, # &amp;lt;--- vits2 parameter&#xA;    use_spk_conditioned_encoder=True, # &amp;lt;--- vits2 parameter&#xA;)&#xA;&#xA;x = torch.LongTensor([[1, 2, 3],[4, 5, 6]]) # token ids&#xA;x_lengths = torch.LongTensor([3, 2]) # token lengths&#xA;y = torch.randn(2, 80, 100) # mel spectrograms&#xA;y_lengths = torch.Tensor([100, 80]) # mel spectrogram lengths&#xA;&#xA;net_g(&#xA;    x=x,&#xA;    x_lengths=x_lengths,&#xA;    y=y,&#xA;    y_lengths=y_lengths,&#xA;)&#xA;&#xA;# calculate loss and backpropagate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;(08/07/2023 update - vits2_vctk_base.json and vits2_ljs_base.json are ready to train; multi-speaker and single-speaker models respectively)&lt;/li&gt; &#xA; &lt;li&gt;(08/06/2023 update - dry run is ready; duration predictor will complete within next week)&lt;/li&gt; &#xA; &lt;li&gt;(08/05/2023 update - everything except the duration predictor is ready to train and we can expect some improvement from VITS1)&lt;/li&gt; &#xA; &lt;li&gt;(08/04/2023 update - initial codebaase is ready; paper is being read)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Duration predictor (fig 1a)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Added LSTM discriminator to duration predictor in notebook.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Added adversarial loss to duration predictor&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Monotonic Alignment Search with Gaussian Noise added in &#39;notebooks&#39; folder; need expert verification (Section 2.2)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Update models.py/train.py/losses.py&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Transformer block in the normalizing flow (fig 1b)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Added transformer block to the normalizing flow in notebook.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Added layers and blocks in models.py (ResidualCouplingTransformersLayer, ResidualCouplingTransformersBlock)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add in config file (vits2_ljs_base.json; can be turned on using &#34;use_transformer_flows&#34; flag)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Speaker-conditioned text encoder (fig 1c)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Added speaker embedding to the text encoder in notebook.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Added speaker embedding to the text encoder in models.py (TextEncoder; backward compatible with VITS)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add in config file (vits2_ljs_base.json; can be turned on using &#34;use_spk_conditioned_encoder&#34; flag)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Mel spectrogram posterior encoder (Section 3)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Added mel spectrogram posterior encoder in notebook.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Added mel spectrogram posterior encoder in train.py&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Addded new config file (vits2_ljs_base.json; can be turned on using &#34;use_mel_posterior_encoder&#34; flag)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Training scripts&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Added vits flags to train.py (single-speaer model)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Added vits flags to train_ms.py (multi-speaker model)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Special mentions&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/erogol&#34;&gt;@erogol&lt;/a&gt; for quick feedback and guidance. (Please check his awesome &lt;a href=&#34;https://github.com/coqui-ai/TTS&#34;&gt;CoquiTTS&lt;/a&gt; repo).&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lexkoro&#34;&gt;@lexkoro&lt;/a&gt; for discussions and help with the prototype training.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/manmay-nakhashi&#34;&gt;@manmay-nakhashi&lt;/a&gt; for discussions and help with the code.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>