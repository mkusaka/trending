<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-02-04T01:38:25Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>rhiever/Data-Analysis-and-Machine-Learning-Projects</title>
    <updated>2023-02-04T01:38:25Z</updated>
    <id>tag:github.com,2023-02-04:/rhiever/Data-Analysis-and-Machine-Learning-Projects</id>
    <link href="https://github.com/rhiever/Data-Analysis-and-Machine-Learning-Projects" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Repository of teaching materials, code, and data for my data analysis and machine learning projects.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/python-2.7-blue.svg?sanitize=true&#34; alt=&#34;Python 2.7&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/python-3.5-blue.svg?sanitize=true&#34; alt=&#34;Python 3.5&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/license-MIT%20License-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Randy Olson&#39;s data analysis and machine learning projects&lt;/h1&gt; &#xA;&lt;p&gt;© 2016 - current, Randal S. Olson&lt;/p&gt; &#xA;&lt;p&gt;This is a repository of teaching materials, code, and data for my data analysis and machine learning projects.&lt;/p&gt; &#xA;&lt;p&gt;Each repository will (usually) correspond to one of the blog posts on my &lt;a href=&#34;http://www.randalolson.com/blog/&#34;&gt;web site&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Be sure to check the documentation (usually in IPython Notebook format) in the directory you&#39;re interested in for the notes on the analysis, data usage terms, etc.&lt;/p&gt; &#xA;&lt;p&gt;If you don&#39;t have the necessary software installed to run IPython Notebook, don&#39;t fret. You can use &lt;a href=&#34;http://nbviewer.ipython.org/&#34;&gt;nbviewer&lt;/a&gt; to view a notebook on the web.&lt;/p&gt; &#xA;&lt;p&gt;For example, if you want to view the notebook in the &lt;code&gt;wheres-waldo-path-optimization&lt;/code&gt; directory, copy the &lt;a href=&#34;https://github.com/rhiever/Data-Analysis-and-Machine-Learning-Projects/raw/master/wheres-waldo-path-optimization/Where&#39;s%20Waldo%20path%20optimization.ipynb&#34;&gt;full link&lt;/a&gt; to the notebook then paste it into &lt;a href=&#34;http://nbviewer.ipython.org/github/rhiever/Data-Analysis-and-Machine-Learning-Projects/blob/master/wheres-waldo-path-optimization/Where%27s%20Waldo%20path%20optimization.ipynb&#34;&gt;nbviewer&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;h3&gt;Instructional Material&lt;/h3&gt; &#xA;&lt;p&gt;All instructional material in this repository is made available under the &lt;a href=&#34;https://creativecommons.org/licenses/by/4.0/&#34;&gt;Creative Commons Attribution license&lt;/a&gt;. The following is a human-readable summary of (and not a substitute for) the &lt;a href=&#34;https://creativecommons.org/licenses/by/4.0/legalcode&#34;&gt;full legal text of the CC BY 4.0 license&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You are free to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Share&lt;/strong&gt;—copy and redistribute the material in any medium or format&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Adapt&lt;/strong&gt;—remix, transform, and build upon the material&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;for any purpose, even commercially.&lt;/p&gt; &#xA;&lt;p&gt;The licensor cannot revoke these freedoms as long as you follow the license terms.&lt;/p&gt; &#xA;&lt;p&gt;Under the following terms:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Attribution&lt;/strong&gt;—You must give appropriate credit (mentioning that your work is derived from work that is © Randal S. Olson and, where practical, linking to &lt;a href=&#34;http://www.randalolson.com/&#34;&gt;http://www.randalolson.com/&lt;/a&gt;), provide a &lt;a href=&#34;https://creativecommons.org/licenses/by/4.0/&#34;&gt;link to the license&lt;/a&gt;, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;No additional restrictions&lt;/strong&gt;—You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Notices:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation.&lt;/li&gt; &#xA; &lt;li&gt;No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Software&lt;/h3&gt; &#xA;&lt;p&gt;Except where otherwise noted, the example programs and other software provided in this repository are made available under the &lt;a href=&#34;http://opensource.org/&#34;&gt;OSI&lt;/a&gt;-approved &lt;a href=&#34;http://opensource.org/licenses/mit-license.html&#34;&gt;MIT license&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the &#34;Software&#34;), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:&lt;/p&gt; &#xA;&lt;p&gt;The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.&lt;/p&gt; &#xA;&lt;p&gt;THE SOFTWARE IS PROVIDED &#34;AS IS&#34;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>lukas/ml-class</title>
    <updated>2023-02-04T01:38:25Z</updated>
    <id>tag:github.com,2023-02-04:/lukas/ml-class</id>
    <link href="https://github.com/lukas/ml-class" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Machine learning lessons and teaching projects designed for engineers&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Projects&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://mybinder.org/v2/gh/lukas/ml-class/master?urlpath=%2Flab&#34;&gt;&lt;img src=&#34;https://mybinder.org/badge_logo.svg?sanitize=true&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;These are specific bite-sized projects to learn an aspect of deep learning, starting from scratch. There is an associated video around 10 minutes long. I assume no background in ML but some proficiency in python. The projects are in order from beginner to more advanced, but feel free to skip around.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Project&lt;/th&gt; &#xA;   &lt;th&gt;Starter Code&lt;/th&gt; &#xA;   &lt;th&gt;Video&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Build a simple image classifier for apparel&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/lukas/ml-class/tree/master/projects/1-fashion-mnist&#34;&gt;projects/1-fashion-mnist&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=CbXj7091OWA&#34;&gt;Build your first machine learning model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Improve your image classifier&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/lukas/ml-class/tree/master/projects/2-fashion-mnist-mlp&#34;&gt;projects/2-fashion-mnist-mlp&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=GVKDa5hxUZE&#34;&gt;Multi-Layer Perceptrons&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Build a convolutional image classifier&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/lukas/ml-class/tree/master/projects/3-fashion-mnist-cnn&#34;&gt;projects/3-fashion-mnist-cnn&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=wzy8jI-duEQ&#34;&gt;Convolutional Neural Networks&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Build a denoising autoencoder&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/lukas/ml-class/tree/master/projects/4-fashion-autoencoder&#34;&gt;projects/4-fashion-autoencoder&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=6maH8Lh3pK4&#34;&gt;Autoencoders&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Build a text classifier with Scikit-Learn&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/lukas/ml-class/tree/master/projects/5-sentiment-analysis&#34;&gt;projects/5-sentiment-analysis&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=qoyp8pBtCZ0&#34;&gt;Sentiment Analysis&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Predict the weather with an RNN&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/lukas/ml-class/tree/master/projects/6-rnn-timeseries&#34;&gt;projects/6-rnn-timeseries&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=8lbGjKhrJOo&#34;&gt;Recurrent Neural Networks&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Build a text generator&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/lukas/ml-class/tree/master/projects/7-text-generation&#34;&gt;projects/7-text-generation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=4F69m3krMHw&#34;&gt;Text Generation using LSTMs and GRUs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Build a sentiment classifier on Amazon reviews.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/lukas/ml-class/tree/master/projects/8-text-classification&#34;&gt;projects/8-text-classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=8YsZXTpFRO0&#34;&gt;Text Classification using CNNs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=NysY9FN9Uac&#34;&gt;Hybrid LSTM/CNNs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=MqugtGD605k&#34;&gt;Seq2seq Models&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=vbhEnEbj3JM&#34;&gt;Transfer Learning&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=H4MPIWX6ftE&#34;&gt;One Shot Learning&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Qf4YJcHXtcY&#34;&gt;Speech Recognition&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=yYqAvlkRwUQ&#34;&gt;Data Augmentation&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=ZBVwnoVIvZk&#34;&gt;Batch Size and Learning Rate&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;More Projects: Benchmarks&lt;/h1&gt; &#xA;&lt;p&gt;If you have done all of the tutorial projects, we have a few more that don&#39;t have associated lessons yet! You can learn by contributing to one of our collaborative &lt;a href=&#34;https://www.wandb.com/benchmarks&#34;&gt;Benchmarks&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Project&lt;/th&gt; &#xA;   &lt;th&gt;Link&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Japanese handwriting recognition&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://app.wandb.ai/wandb/kmnist/benchmark&#34;&gt;https://app.wandb.ai/wandb/kmnist/benchmark&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Video prediction with cat GIFs&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://app.wandb.ai/wandb/catz/benchmark&#34;&gt;https://app.wandb.ai/wandb/catz/benchmark&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Drought detection from satellite&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://app.wandb.ai/wandb/droughtwatch/benchmark&#34;&gt;https://app.wandb.ai/wandb/droughtwatch/benchmark&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Visual game playing&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://app.wandb.ai/wandb/witness/benchmark&#34;&gt;https://app.wandb.ai/wandb/witness/benchmark&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Image resolution enhancement&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://app.wandb.ai/wandb/superres/benchmark&#34;&gt;https://app.wandb.ai/wandb/superres/benchmark&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Photo colorization&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://app.wandb.ai/wandb/colorizer-applied-dl/benchmark&#34;&gt;https://app.wandb.ai/wandb/colorizer-applied-dl/benchmark&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Getting Started&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone this repository&lt;/li&gt; &#xA; &lt;li&gt;Get the python libraries (run &#39;pip install -r requirements.txt&#39;)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;You don&#39;t need a fancy computer to run most of the examples, but especially to do the later projects you may want to invest in a GPU.&lt;/p&gt; &#xA;&lt;h1&gt;Slides&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.dropbox.com/s/sdep4yralq4exq3/Oreilly%20LSTM%20-%20Sept%2010.pdf?dl=0&#34;&gt;O&#39;Reilly 9.10.2019 - Using Keras to classify text using LSTMs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Videos&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.wandb.com/classes/intro/overview&#34;&gt;Introduction to Machine Learning&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Examples&lt;/h1&gt; &#xA;&lt;p&gt;In my in-person classes, I typically use a lot of the examples in the directory &lt;em&gt;examples&lt;/em&gt;. This code is liable to change as I update things.&lt;/p&gt; &#xA;&lt;h1&gt;Reusing the materials&lt;/h1&gt; &#xA;&lt;p&gt;Please feel free to use these materials for your own classes/projects etc. If you do that, I would love it if you sent me a message and let me know what you&#39;re up to.&lt;/p&gt; &#xA;&lt;h3&gt;Windows&lt;/h3&gt; &#xA;&lt;h4&gt;Git&lt;/h4&gt; &#xA;&lt;p&gt;Install git if you don&#39;t have it: &lt;a href=&#34;https://git-scm.com/download/win&#34;&gt;https://git-scm.com/download/win&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Anaconda&lt;/h4&gt; &#xA;&lt;p&gt;Install &lt;a href=&#34;https://repo.continuum.io/archive/Anaconda3-4.4.0-Windows-x86_64.exe&#34;&gt;anaconda&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Try running the following from the command prompt:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python --version&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You should see something like&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Python 3.6.1 :: Anaconda 4.4.0 (64-bit)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If don&#39;t see &#34;Anaconda&#34; in the output, search for &#34;anaconda prompt&#34; from the start menu and enter your command prompt this way. It&#39;s also best to use a virtual environment to keep your packages silo&#39;ed. Do so with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n ml-class python=3.6&#xA;activate ml-class&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Whenever you start a new terminal, you will need to call &lt;code&gt;activate ml-class&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Clone this github repository&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/lukas/ml-class.git&#xA;cd ml-class&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;libraries&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install wandb&#xA;conda install -c conda-forge scikit-learn&#xA;conda install -c conda-forge tensorflow&#xA;conda install -c conda-forge keras&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Linux and Mac OS X&lt;/h3&gt; &#xA;&lt;h4&gt;Install python&lt;/h4&gt; &#xA;&lt;p&gt;You can download python from &lt;a href=&#34;https://www.python.org/downloads/&#34;&gt;https://www.python.org/downloads/&lt;/a&gt;. There are more detailed instructions for windows installation at &lt;a href=&#34;https://www.howtogeek.com/197947/how-to-install-python-on-windows/&#34;&gt;https://www.howtogeek.com/197947/how-to-install-python-on-windows/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The material should work with python 2 or 3. &lt;em&gt;On Windows, you need to install the 64 bit version of python 3.5 or 3.6 in order to install tensorflow&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Clone this github repository&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/lukas/ml-class.git&#xA;cd ml-class&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you get an error message here, most likely you don&#39;t have git installed. Go to &lt;a href=&#34;https://www.atlassian.com/git/tutorials/install-git&#34;&gt;https://www.atlassian.com/git/tutorials/install-git&lt;/a&gt; for instructions on installing git.&lt;/p&gt; &#xA;&lt;h4&gt;Install necessary pip libraries&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Reading material for people who haven&#39;t done a lot of programming&lt;/h3&gt; &#xA;&lt;p&gt;If you are uncomfortable opening up a terminal, I strongly recommend doing a quick tutorial before you take this class. Setting up your machine can be painful but once you&#39;re setup you can get a ton out of the class. I recommend getting started ahead of time.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;re on Windows I recommend checking out &lt;a href=&#34;http://thepythonguru.com/&#34;&gt;http://thepythonguru.com/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;re on a Mac check out &lt;a href=&#34;http://www.macworld.co.uk/how-to/mac/coding-with-python-on-mac-3635912/&#34;&gt;http://www.macworld.co.uk/how-to/mac/coding-with-python-on-mac-3635912/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;re on linux, you&#39;re probably already reasonably well setup :).&lt;/p&gt; &#xA;&lt;p&gt;If you run into trouble, the book Learn Python the Hard Way has installation steps in great detail: &lt;a href=&#34;https://learnpythonthehardway.org/book/ex0.html&#34;&gt;https://learnpythonthehardway.org/book/ex0.html&lt;/a&gt;. It also has a refresher on using a terminal in the appendix.&lt;/p&gt; &#xA;&lt;h3&gt;Reading material for people who are comfortable with programming, but haven&#39;t done a lot of python&lt;/h3&gt; &#xA;&lt;p&gt;If you are comfortable opening up a terminal but want a python intro/refresher check out &lt;a href=&#34;https://www.learnpython.org/&#34;&gt;https://www.learnpython.org/&lt;/a&gt; for a really nice introduction to Python.&lt;/p&gt; &#xA;&lt;h3&gt;Suggestions for people who have done a lot of programming in python&lt;/h3&gt; &#xA;&lt;p&gt;A lot of people like to follow along with ipython or jupyter notebooks and I think that&#39;s great! It makes data exploration easier. I also really appreciate pull requests to make the code clearer.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;ve never used pandas or numpy - they are great tools and I use them heavily in my work and for this class. I assume no knlowedge of pandas and numpy but you may want to do some learning on your own. You can get a quick overview of pandas at &lt;a href=&#34;http://pandas.pydata.org/pandas-docs/stable/10min.html&#34;&gt;http://pandas.pydata.org/pandas-docs/stable/10min.html&lt;/a&gt;. There is a great overview of numpy at &lt;a href=&#34;https://docs.scipy.org/doc/numpy/user/quickstart.html&#34;&gt;https://docs.scipy.org/doc/numpy/user/quickstart.html&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>AttendAndExcite/Attend-and-Excite</title>
    <updated>2023-02-04T01:38:25Z</updated>
    <id>tag:github.com,2023-02-04:/AttendAndExcite/Attend-and-Excite</id>
    <link href="https://github.com/AttendAndExcite/Attend-and-Excite" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official Implementation for &#34;Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Recent text-to-image generative models have demonstrated an unparalleled ability to generate diverse and creative imagery guided by a target text prompt. While revolutionary, current state-of-the-art diffusion models may still fail in generating images that fully convey the semantics in the given text prompt. We analyze the publicly available Stable Diffusion model and assess the existence of catastrophic neglect, where the model fails to generate one or more of the subjects from the input prompt. Moreover, we find that in some cases the model also fails to correctly bind attributes (e.g., colors) to their corresponding subjects. To help mitigate these failure cases, we introduce the concept of Generative Semantic Nursing (GSN), where we seek to intervene in the generative process on the fly during inference time to improve the faithfulness of the generated images. Using an attention-based formulation of GSN, dubbed Attend-and-Excite, we guide the model to refine the cross-attention units to attend to all subject tokens in the text prompt and strengthen — or excite — their activations, encouraging the model to generate all subjects described in the text prompt. We compare our approach to alternative approaches and demonstrate that it conveys the desired concepts more faithfully across a range of text prompts.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://attendandexcite.github.io/Attend-and-Excite/&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Project&amp;amp;message=Website&amp;amp;color=red&#34; height=&#34;20.5&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://youtu.be/9EWs2IX4cus&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=5-Minute&amp;amp;message=Video&amp;amp;color=darkgreen&#34; height=&#34;20.5&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/hysts/Attend-and-Excite&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/AttendAndExcite/Attend-and-Excite/main/docs/teaser.jpg&#34; width=&#34;800px&#34;&gt; &lt;br&gt; Given a pre-trained text-to-image diffusion model (e.g., Stable Diffusion) our method, Attend-and-Excite, guides the generative model to modify the cross-attention values during the image synthesis process to generate images that more faithfully depict the input text prompt. Stable Diffusion alone (top row) struggles to generate multiple objects (e.g., a horse and a dog). However, by incorporating Attend-and-Excite (bottom row) to strengthen the subject tokens (marked in blue), we achieve images that are more semantically faithful with respect to the input text prompts. &lt;/p&gt; &#xA;&lt;h2&gt;Description&lt;/h2&gt; &#xA;&lt;p&gt;Official implementation of our Attend-and-Excite paper.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;h3&gt;Environment&lt;/h3&gt; &#xA;&lt;p&gt;Our code builds on the requirement of the official &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;Stable Diffusion repository&lt;/a&gt;. To set up their environment, please run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env create -f environment/environment.yaml&#xA;conda activate ldm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On top of these requirements, we add several requirements which can be found in &lt;code&gt;environment/requirements.txt&lt;/code&gt;. These requirements will be installed in the above command.&lt;/p&gt; &#xA;&lt;h3&gt;Hugging Face Diffusers Library&lt;/h3&gt; &#xA;&lt;p&gt;Our code relies also on Hugging Face&#39;s &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;diffusers&lt;/a&gt; library for downloading the Stable Diffusion v1.4 model.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/AttendAndExcite/Attend-and-Excite/main/docs/results.jpg&#34; width=&#34;800px&#34;&gt; &lt;br&gt; Example generations outputted by Stable Diffusion with Attend-and-Excite. &lt;/p&gt; &#xA;&lt;p&gt;To generate an image, you can simply run the &lt;code&gt;run.py&lt;/code&gt; script. For example,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run.py --prompt &#34;a cat and a dog&#34; --seeds [0] --token_indices [2,5]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Notes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You may run multiple seeds by passing a list of seeds. For example, &lt;code&gt;--seeds [0,1,2,3]&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;If you do not provide a list of which token indices to alter using &lt;code&gt;--token_indices&lt;/code&gt;, we will split the text according to the Stable Diffusion&#39;s tokenizer and display the index of each token. You will then be able to input which indices you wish to alter.&lt;/li&gt; &#xA; &lt;li&gt;If you wish to run the standard Stable Diffusion model without Attend-and-Excite, you can do so by passing &lt;code&gt;--run_standard_sd True&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;All parameters are defined in &lt;code&gt;config.py&lt;/code&gt; and are set to their defaults according to the official paper.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;All generated images will be saved to the path &lt;code&gt;&#34;{config.output_path}/{prompt}&#34;&lt;/code&gt;. We will also save a grid of all images (in the case of multiple seeds) under &lt;code&gt;config.output_path&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Notebooks&lt;/h2&gt; &#xA;&lt;p&gt;We provide Jupyter notebooks to reproduce the results from the paper for image generation and explainability via the cross-attention maps.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/AttendAndExcite/Attend-and-Excite/main/docs/explainability.jpg&#34; width=&#34;450px&#34;&gt; &lt;br&gt; Example cross-attention visualizations. &lt;/p&gt; &#xA;&lt;h3&gt;Generation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;notebooks/generate_images.ipynb&lt;/code&gt; enables image generation using a free-form text prompt with and without Attend-and-Excite.&lt;/p&gt; &#xA;&lt;h3&gt;Explainability&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;notebooks/explain.ipynb&lt;/code&gt; produces a comparison of the cross-attention maps before and after applying Attend-and-Excite as seen in the illustration above. This notebook can be used to provide an explanation for the generations produced by Attend-and-Excite.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;This code is builds on the code from the &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;diffusers&lt;/a&gt; library as well as the &lt;a href=&#34;https://github.com/google/prompt-to-prompt/&#34;&gt;Prompt-to-Prompt&lt;/a&gt; codebase.&lt;/p&gt;</summary>
  </entry>
</feed>