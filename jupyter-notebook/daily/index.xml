<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-11T01:38:59Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>krishnaik06/mlproject</title>
    <updated>2023-03-11T01:38:59Z</updated>
    <id>tag:github.com,2023-03-11:/krishnaik06/mlproject</id>
    <link href="https://github.com/krishnaik06/mlproject" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;End to End MAchine Learning Project&lt;/h2&gt;</summary>
  </entry>
  <entry>
    <title>pytorch/botorch</title>
    <updated>2023-03-11T01:38:59Z</updated>
    <id>tag:github.com,2023-03-11:/pytorch/botorch</id>
    <link href="https://github.com/pytorch/botorch" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Bayesian optimization in PyTorch&lt;/p&gt;&lt;hr&gt;&lt;a href=&#34;https://botorch.org&#34;&gt; &lt;img width=&#34;350&#34; src=&#34;https://botorch.org/img/botorch_logo_lockup.png&#34; alt=&#34;BoTorch Logo&#34;&gt; &lt;/a&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://opensource.fb.com/support-ukraine&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Support-Ukraine-FFD500?style=flat&amp;amp;labelColor=005BBB&#34; alt=&#34;Support Ukraine&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/pytorch/botorch/actions?query=workflow%3ALint&#34;&gt;&lt;img src=&#34;https://github.com/pytorch/botorch/workflows/Lint/badge.svg?sanitize=true&#34; alt=&#34;Lint&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/pytorch/botorch/actions?query=workflow%3ATest&#34;&gt;&lt;img src=&#34;https://github.com/pytorch/botorch/workflows/Test/badge.svg?sanitize=true&#34; alt=&#34;Test&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/pytorch/botorch/actions?query=workflow%3ADocs&#34;&gt;&lt;img src=&#34;https://github.com/pytorch/botorch/workflows/Docs/badge.svg?sanitize=true&#34; alt=&#34;Docs&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/pytorch/botorch/actions?query=workflow%3ATutorials&#34;&gt;&lt;img src=&#34;https://github.com/pytorch/botorch/workflows/Tutorials/badge.svg?sanitize=true&#34; alt=&#34;Tutorials&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/github/pytorch/botorch&#34;&gt;&lt;img src=&#34;https://img.shields.io/codecov/c/github/pytorch/botorch.svg?sanitize=true&#34; alt=&#34;Codecov&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://anaconda.org/pytorch/botorch&#34;&gt;&lt;img src=&#34;https://img.shields.io/conda/v/pytorch/botorch.svg?sanitize=true&#34; alt=&#34;Conda&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/botorch&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/botorch.svg?sanitize=true&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/botorch/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-MIT-green.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;BoTorch is a library for Bayesian Optimization built on PyTorch.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;BoTorch is currently in beta and under active development!&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Why BoTorch ?&lt;/h4&gt; &#xA;&lt;p&gt;BoTorch&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Provides a modular and easily extensible interface for composing Bayesian optimization primitives, including probabilistic models, acquisition functions, and optimizers.&lt;/li&gt; &#xA; &lt;li&gt;Harnesses the power of PyTorch, including auto-differentiation, native support for highly parallelized modern hardware (e.g. GPUs) using device-agnostic code, and a dynamic computation graph.&lt;/li&gt; &#xA; &lt;li&gt;Supports Monte Carlo-based acquisition functions via the &lt;a href=&#34;https://arxiv.org/abs/1312.6114&#34;&gt;reparameterization trick&lt;/a&gt;, which makes it straightforward to implement new ideas without having to impose restrictive assumptions about the underlying model.&lt;/li&gt; &#xA; &lt;li&gt;Enables seamless integration with deep and/or convolutional architectures in PyTorch.&lt;/li&gt; &#xA; &lt;li&gt;Has first-class support for state-of-the art probabilistic models in &lt;a href=&#34;http://www.gpytorch.ai/&#34;&gt;GPyTorch&lt;/a&gt;, including support for multi-task Gaussian Processes (GPs) deep kernel learning, deep GPs, and approximate inference.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Target Audience&lt;/h4&gt; &#xA;&lt;p&gt;The primary audience for hands-on use of BoTorch are researchers and sophisticated practitioners in Bayesian Optimization and AI. We recommend using BoTorch as a low-level API for implementing new algorithms for &lt;a href=&#34;https://ax.dev&#34;&gt;Ax&lt;/a&gt;. Ax has been designed to be an easy-to-use platform for end-users, which at the same time is flexible enough for Bayesian Optimization researchers to plug into for handling of feature transformations, (meta-)data management, storage, etc. We recommend that end-users who are not actively doing research on Bayesian Optimization simply use Ax.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Installation Requirements&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python &amp;gt;= 3.8&lt;/li&gt; &#xA; &lt;li&gt;PyTorch &amp;gt;= 1.12&lt;/li&gt; &#xA; &lt;li&gt;gpytorch == 1.9.1&lt;/li&gt; &#xA; &lt;li&gt;linear_operator == 0.3.0&lt;/li&gt; &#xA; &lt;li&gt;pyro-ppl &amp;gt;= 1.8.4&lt;/li&gt; &#xA; &lt;li&gt;scipy&lt;/li&gt; &#xA; &lt;li&gt;multiple-dispatch&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Prerequisite only for MacOS users with Intel processors:&lt;/h3&gt; &#xA;&lt;p&gt;Before installing BoTorch, we recommend first manually installing PyTorch, a required dependency of BoTorch. Installing it according to the &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;PyTorch installation instructions&lt;/a&gt; ensures that it is properly linked against MKL, a library that optimizes mathematical computation for Intel processors. This will result in up to an order-of-magnitude speed-up for Bayesian optimization, as at the moment, installing PyTorch from pip does not link against MKL.&lt;/p&gt; &#xA;&lt;p&gt;The PyTorch installation instructions currently recommend:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://www.anaconda.com/distribution/#download-section&#34;&gt;Anaconda&lt;/a&gt;. Note that there are different installers for Intel and M1 Macs.&lt;/li&gt; &#xA; &lt;li&gt;Install PyTorch following the &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;PyTorch installation instructions&lt;/a&gt;. Currently, this suggests running &lt;code&gt;conda install pytorch torchvision -c pytorch&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;If you want to customize your installation, please follow the &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;PyTorch installation instructions&lt;/a&gt; to build from source.&lt;/p&gt; &#xA;&lt;h3&gt;Option 1: Installing the latest release&lt;/h3&gt; &#xA;&lt;p&gt;The latest release of BoTorch is easily installed either via &lt;a href=&#34;https://www.anaconda.com/distribution/#download-section&#34;&gt;Anaconda&lt;/a&gt; (recommended) or pip.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;To install BoTorch from Anaconda&lt;/strong&gt;, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda install botorch -c pytorch -c gpytorch -c conda-forge&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The above command installs BoTorch and any needed dependencies. &lt;code&gt; -c pytorch -c gpytorch -c conda-forge&lt;/code&gt; means that the most preferred source to install from is the PyTorch channel, the next most preferred is the GPyTorch channel, and the least preferred is conda-forge.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Alternatively, to install with &lt;code&gt;pip&lt;/code&gt;&lt;/strong&gt;, do&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install botorch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: Make sure the &lt;code&gt;pip&lt;/code&gt; being used is actually the one from the newly created Conda environment. If you&#39;re using a Unix-based OS, you can use &lt;code&gt;which pip&lt;/code&gt; to check.&lt;/p&gt; &#xA;&lt;h3&gt;Option 2: Installing from latest main branch&lt;/h3&gt; &#xA;&lt;p&gt;If you would like to try our bleeding edge features (and don&#39;t mind potentially running into the occasional bug here or there), you can install the latest development version directly from GitHub. If you want to also install the current &lt;code&gt;gpytorch&lt;/code&gt; and &lt;code&gt;linear_operator&lt;/code&gt; development versions, you will need to ensure that the &lt;code&gt;ALLOW_LATEST_GPYTORCH_LINOP&lt;/code&gt; environment variable is set:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install --upgrade git+https://github.com/cornellius-gp/linear_operator.git&#xA;pip install --upgrade git+https://github.com/cornellius-gp/gpytorch.git&#xA;export ALLOW_LATEST_GPYTORCH_LINOP=true&#xA;pip install --upgrade git+https://github.com/pytorch/botorch.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Option 3: Editable/dev install&lt;/h3&gt; &#xA;&lt;p&gt;If you want to &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/botorch/main/CONTRIBUTING.md&#34;&gt;contribute&lt;/a&gt; to BoTorch, you will want to install editably so that you can change files and have the changes reflected in your local install.&lt;/p&gt; &#xA;&lt;p&gt;If you want to install the current &lt;code&gt;gpytorch&lt;/code&gt; and &lt;code&gt;linear_operator&lt;/code&gt; development versions, as in Option 2, do that before proceeding.&lt;/p&gt; &#xA;&lt;h4&gt;Option 3a: Bare-bones editable install&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/pytorch/botorch.git&#xA;cd botorch&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Option 3b: Editable install with development and tutorials dependencies&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/pytorch/botorch.git&#xA;cd botorch&#xA;export ALLOW_BOTORCH_LATEST=true&#xA;pip install -e &#34;.[dev, tutorials]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;dev&lt;/code&gt;: Specifies tools necessary for development (testing, linting, docs building; see &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/botorch/main/#contributing&#34;&gt;Contributing&lt;/a&gt; below).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;tutorials&lt;/code&gt;: Also installs all packages necessary for running the tutorial notebooks.&lt;/li&gt; &#xA; &lt;li&gt;You can also install either the dev or tutorials dependencies without installing both, e.g. by changing the last command to &lt;code&gt;pip install -e &#34;.[dev]&#34;&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Here&#39;s a quick run down of the main components of a Bayesian optimization loop. For more details see our &lt;a href=&#34;https://botorch.org/docs/introduction&#34;&gt;Documentation&lt;/a&gt; and the &lt;a href=&#34;https://botorch.org/tutorials&#34;&gt;Tutorials&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Fit a Gaussian Process model to data&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from botorch.models import SingleTaskGP&#xA;from botorch.fit import fit_gpytorch_mll&#xA;from gpytorch.mlls import ExactMarginalLogLikelihood&#xA;&#xA;train_X = torch.rand(10, 2)&#xA;Y = 1 - (train_X - 0.5).norm(dim=-1, keepdim=True)  # explicit output dimension&#xA;Y += 0.1 * torch.rand_like(Y)&#xA;train_Y = (Y - Y.mean()) / Y.std()&#xA;&#xA;gp = SingleTaskGP(train_X, train_Y)&#xA;mll = ExactMarginalLogLikelihood(gp.likelihood, gp)&#xA;fit_gpytorch_mll(mll)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Construct an acquisition function&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botorch.acquisition import UpperConfidenceBound&#xA;&#xA;UCB = UpperConfidenceBound(gp, beta=0.1)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Optimize the acquisition function&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botorch.optim import optimize_acqf&#xA;&#xA;bounds = torch.stack([torch.zeros(2), torch.ones(2)])&#xA;candidate, acq_value = optimize_acqf(&#xA;    UCB, bounds=bounds, q=1, num_restarts=5, raw_samples=20,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citing BoTorch&lt;/h2&gt; &#xA;&lt;p&gt;If you use BoTorch, please cite the following paper:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1910.06403&#34;&gt;M. Balandat, B. Karrer, D. R. Jiang, S. Daulton, B. Letham, A. G. Wilson, and E. Bakshy. BoTorch: A Framework for Efficient Monte-Carlo Bayesian Optimization. Advances in Neural Information Processing Systems 33, 2020.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{balandat2020botorch,&#xA;  title={{BoTorch: A Framework for Efficient Monte-Carlo Bayesian Optimization}},&#xA;  author={Balandat, Maximilian and Karrer, Brian and Jiang, Daniel R. and Daulton, Samuel and Letham, Benjamin and Wilson, Andrew Gordon and Bakshy, Eytan},&#xA;  booktitle = {Advances in Neural Information Processing Systems 33},&#xA;  year={2020},&#xA;  url = {http://arxiv.org/abs/1910.06403}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://botorch.org/docs/papers&#34;&gt;here&lt;/a&gt; for an incomplete selection of peer-reviewed papers that build off of BoTorch.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/botorch/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING&lt;/a&gt; file for how to help out.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;BoTorch is MIT licensed, as found in the &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/botorch/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>benthecoder/ClassGPT</title>
    <updated>2023-03-11T01:38:59Z</updated>
    <id>tag:github.com,2023-03-11:/benthecoder/ClassGPT</id>
    <link href="https://github.com/benthecoder/ClassGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ChatGPT for lecture slides&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ClassGPT&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;ChatGPT for my lecture slides&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;img width=&#34;1251&#34; alt=&#34;SCR-20230307-isgj&#34; src=&#34;https://user-images.githubusercontent.com/49143413/223467346-473681e4-6203-4f31-a1f1-253829d4768a.png&#34;&gt; &#xA;&lt;p&gt;Built with &lt;a href=&#34;https://github.com/streamlit/streamlit&#34;&gt;Streamlit&lt;/a&gt;, powered by &lt;a href=&#34;https://github.com/jerryjliu/gpt_index&#34;&gt;LlamaIndex&lt;/a&gt; and &lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;LangChain&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Uses the latest &lt;a href=&#34;https://platform.openai.com/docs/guides/chat&#34;&gt;ChatGPT API&lt;/a&gt; from &lt;a href=&#34;https://openai.com/&#34;&gt;OpenAI&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Inspired by &lt;a href=&#34;http://athensgpt.com/&#34;&gt;AthensGPT&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;App Demo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/49143413/222878151-42354446-5234-41fa-ad36-002dd74a5408.mp4&#34;&gt;https://user-images.githubusercontent.com/49143413/222878151-42354446-5234-41fa-ad36-002dd74a5408.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How this works&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Parses pdf with &lt;a href=&#34;https://pypi.org/project/pypdf/&#34;&gt;pypdf&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Index Construction with LlamaIndex&#39;s &lt;code&gt;GPTSimpleVectorIndex&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;the &lt;code&gt;text-embedding-ada-002&lt;/code&gt; model is used to create embeddings&lt;/li&gt; &#xA;   &lt;li&gt;see &lt;a href=&#34;https://gpt-index.readthedocs.io/en/latest/guides/index_guide.html#vector-store-index&#34;&gt;vector store index&lt;/a&gt; page to learn more&lt;/li&gt; &#xA;   &lt;li&gt;here&#39;s a &lt;a href=&#34;https://raw.githubusercontent.com/benthecoder/ClassGPT/main/notebooks/index.json&#34;&gt;sample index&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;indexes and files are stored on s3&lt;/li&gt; &#xA; &lt;li&gt;Query the index &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;uses the latest ChatGPT model &lt;code&gt;gpt-3.5-turbo&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Configuration and secrets&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;configure aws (&lt;a href=&#34;https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html&#34;&gt;quickstart&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;    aws configure&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;create s3 bucket named &lt;code&gt;&#34;classgpt&#34;&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;rename [.env.local.example] to &lt;code&gt;.env&lt;/code&gt; and add your openai credentials&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Locally&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;create python env&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;    conda create -n classgpt python=3.9&#xA;    conda activate classgpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;install dependencies&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;    pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;run streamlit app&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;    cd app/&#xA;    streamlit run app/01_❓_Ask.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Docker&lt;/h3&gt; &#xA;&lt;p&gt;Alternative, you can use Docker&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;    docker compose up&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then open up a new tab and navigate to &lt;a href=&#34;http://localhost:8501/&#34;&gt;http://localhost:8501/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; update code to use new langchain abstractions &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; refer to &lt;a href=&#34;https://langchain.readthedocs.io/en/latest/modules/chat/examples/agent.html&#34;&gt;Agent — 🦜🔗 LangChain 0.0.103&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; memory now returns list of messages &lt;a href=&#34;https://langchain.readthedocs.io/en/latest/modules/memory/types/buffer.html&#34;&gt;ConversationBufferMemory — 🦜🔗 LangChain 0.0.103&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; test using only langchain to create documents &lt;a href=&#34;https://langchain.readthedocs.io/en/latest/modules/chat/examples/chat_vector_db.html&#34;&gt;Chat Vector DB — 🦜🔗 LangChain 0.0.103&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add ability to query on multiple files &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Compose indices of multiple lectures and query on all of them&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; loop through all existing index, create the ones that haven&#39;t been created, and compose them together&lt;/li&gt; &#xA;   &lt;li&gt;references &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://gpt-index.readthedocs.io/en/latest/how_to/composability.html&#34;&gt;Composability — LlamaIndex documentation&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jerryjliu/gpt_index/raw/main/examples/composable_indices/ComposableIndices.ipynb&#34;&gt;gpt_index/ComposableIndices.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1IJAKd1HIe-LvFRQmd3BCDDIsq6CpOwBj?usp=sharing&#34;&gt;Test Complex Queries over Multiple Documents&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Custom prompts and tweak settings &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; create a settings page for tweaking model parameters and provide custom prompts &lt;a href=&#34;https://github.com/hayabhay/whisper-ui&#34;&gt;example&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; choose local or cloud storage version, so users don&#39;t have to setup AWS s3 and everything is downloaded locally&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; deploy app on AWS&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;h3&gt;Tokens&lt;/h3&gt; &#xA;&lt;p&gt;Tokens can be thought of as pieces of words. Before the API processes the prompts, the input is broken down into tokens. These tokens are not cut up exactly where the words start or end - tokens can include trailing spaces and even sub-words. Here are some helpful rules of thumb for understanding tokens in terms of lengths:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;1 token ~= 4 chars in English&lt;/li&gt; &#xA; &lt;li&gt;1 token ~= ¾ words&lt;/li&gt; &#xA; &lt;li&gt;100 tokens ~= 75 words&lt;/li&gt; &#xA; &lt;li&gt;1-2 sentence ~= 30 tokens&lt;/li&gt; &#xA; &lt;li&gt;1 paragraph ~= 100 tokens&lt;/li&gt; &#xA; &lt;li&gt;1,500 words ~= 2048 tokens&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Try the &lt;a href=&#34;https://platform.openai.com/tokenizer&#34;&gt;OpenAI Tokenizer tool&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them&#34;&gt;Source&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Embeddings&lt;/h3&gt; &#xA;&lt;p&gt;An embedding is a vector (list) of floating point numbers. The distance between two vectors measures their relatedness. Small distances suggest high relatedness and large distances suggest low relatedness.&lt;/p&gt; &#xA;&lt;p&gt;For &lt;code&gt;text-embedding-ada-002&lt;/code&gt;, cost is $0.0004 / 1k tokens or 3000 pages/dollar&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://platform.openai.com/docs/guides/embeddings/use-cases&#34;&gt;Embeddings - OpenAI API&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://txt.cohere.ai/sentence-word-embeddings/&#34;&gt;What Are Word and Sentence Embeddings?&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Models&lt;/h3&gt; &#xA;&lt;p&gt;For &lt;code&gt;gpt-3.5-turbo&lt;/code&gt; model (ChatGPTAPI) cost is &lt;code&gt;$0.002 / 1K tokens&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;For &lt;code&gt;text-davinci-003&lt;/code&gt; model, cost is &lt;code&gt;$0.02 / 1K tokens&lt;/code&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://platform.openai.com/docs/guides/chat&#34;&gt;Chat completion - OpenAI API&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;h3&gt;Streamlit&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.streamlit.io/knowledge-base/deploy/increase-file-uploader-limit-streamlit-cloud&#34;&gt;Increase upload limit of st.file_uploader&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.streamlit.io/library/api-reference/performance/st.cache_resource&#34;&gt;st.cache_resource - Streamlit Docs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.streamlit.io/library/api-reference/session-state&#34;&gt;Session State&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hayabhay/whisper-ui&#34;&gt;hayabhay/whisper-ui: Streamlit UI for OpenAI&#39;s Whisper&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Deplyoment&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discuss.streamlit.io/t/streamlit-deployment-guide-wiki/5099&#34;&gt;Streamlit Deployment Guide (wiki) - 🚀 Deployment - Streamlit&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Jc5GI3v2jtE&#34;&gt;How to Deploy a streamlit application to AWS? Part-3&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;LlamaIndex&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gpt-index.readthedocs.io/en/latest/guides/usage_pattern.html#&#34;&gt;LlamaIndex Usage Pattern&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gpt-index.readthedocs.io/en/latest/guides/usage_pattern.html#optional-save-the-index-for-future-use&#34;&gt;Saving index&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Loading data&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://llamahub.ai/l/file-pdf&#34;&gt;PDF Loader&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/emptycrown/llama-hub/tree/main&#34;&gt;llama-hub github repo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jerryjliu/gpt_index/raw/f07050b84309d53842a3552d3546e765012d168c/gpt_index/readers/schema/base.py#L4&#34;&gt;document class&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/emptycrown/llama-hub/raw/main/loader_hub/file/pdf/base.py&#34;&gt;PDFReader class&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;ChatGPT&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jerryjliu/gpt_index/raw/main/examples/vector_indices/SimpleIndexDemo-ChatGPT.ipynb&#34;&gt;gpt_index/SimpleIndexDemo-ChatGPT.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Langchain&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jerryjliu/gpt_index/raw/main/examples/langchain_demo/LangchainDemo.ipynb&#34;&gt;gpt_index/LangchainDemo.ipynb&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://langchain.readthedocs.io/en/latest/modules/llms/integrations/openaichat.html&#34;&gt;OpenAIChat&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Boto3&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://stackoverflow.com/questions/44978426/boto3-file-upload-does-it-check-if-file-exists&#34;&gt;boto3 file_upload does it check if file exists&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.learnaws.org/2021/02/24/boto3-resource-client/&#34;&gt;Boto 3: Resource vs Client&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://stackoverflow.com/questions/46844263/writing-json-to-file-in-s3-bucket&#34;&gt;Writing json to file in s3 bucket&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Docker stuff&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://stackoverflow.com/questions/36354423/what-is-the-best-way-to-pass-aws-credentials-to-a-docker-container&#34;&gt;amazon web services - What is the best way to pass AWS credentials to a Docker container?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/acheong08/ChatGPT/issues/572&#34;&gt;docker-compose up failing due to: error: can&#39;t find Rust compiler · Issue #572 · acheong08/ChatGPT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://stackoverflow.com/questions/49676490/when-installing-rust-toolchain-in-docker-bash-source-command-doesnt-work&#34;&gt;linux - When installing Rust toolchain in Docker, Bash &lt;code&gt;source&lt;/code&gt; command doesn&#39;t work&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://askubuntu.com/questions/523962/how-to-install-a-package-with-apt-without-the-do-you-want-to-continue-y-n-p&#34;&gt;software installation - How to install a package with apt without the &#34;Do you want to continue [Y/n]?&#34; prompt? - Ask Ubuntu&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://stackoverflow.com/questions/25845538/how-to-use-sudo-inside-a-docker-container&#34;&gt;How to use sudo inside a docker container?&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>