<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-25T01:31:15Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>lm-sys/arena-hard</title>
    <updated>2024-04-25T01:31:15Z</updated>
    <id>tag:github.com,2024-04-25:/lm-sys/arena-hard</id>
    <link href="https://github.com/lm-sys/arena-hard" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Arena-Hard benchmark&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Arena-Hard&lt;/h1&gt; &#xA;&lt;p&gt;Arena-Hard is an evaluation tool for instruction-tuned LLMs. It contains 500 challenging user queries. We prompt GPT-4-Turbo as judge to compare the models&#39; responses against a baseline model (default: GPT-4-0314).&lt;/p&gt; &#xA;&lt;p&gt;Check out our blog post for more details about how Arena Hard v0.1 works -&amp;gt; &lt;a href=&#34;https://lmsys.org/blog/2024-04-19-arena-hard/&#34;&gt;Blog post link&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Install Dependencies&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/lm-sys/arena-hard.git&#xA;cd arena-hard&#xA;pip install -r requirements.txt&#xA;pip install -r requirements-optional.txt  # Optional dependencies (e.g., anthropic sdk)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Download dataset&lt;/h2&gt; &#xA;&lt;p&gt;We have pre-generated many popular models answers and judgments. You can browse them with an online &lt;a href=&#34;https://huggingface.co/spaces/lmsys/arena-hard-browser&#34;&gt;demo&lt;/a&gt; or download them by&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;&amp;gt; git clone https://huggingface.co/spaces/lmsys/arena-hard-browser&#xA;// copy answers/judgments to the data directory&#xA;&amp;gt; cp -r arena-hard-browser/data . &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;&amp;gt; python show_result.py&#xA;gpt-4-0125-preview             | score: 78.0  | 95% CI: (-1.8, 2.2)  | average #tokens: 619&#xA;claude-3-opus-20240229         | score: 60.4  | 95% CI: (-2.6, 2.1)  | average #tokens: 541&#xA;gpt-4-0314                     | score: 50.0  | 95% CI:  (0.0, 0.0)  | average #tokens: 423&#xA;claude-3-sonnet-20240229       | score: 46.8  | 95% CI: (-2.7, 2.3)  | average #tokens: 552&#xA;claude-3-haiku-20240307        | score: 41.5  | 95% CI: (-2.4, 2.5)  | average #tokens: 505&#xA;gpt-4-0613                     | score: 37.9  | 95% CI: (-2.1, 2.2)  | average #tokens: 354&#xA;mistral-large-2402             | score: 37.7  | 95% CI: (-2.9, 2.8)  | average #tokens: 400&#xA;Qwen1.5-72B-Chat               | score: 36.1  | 95% CI: (-2.1, 2.4)  | average #tokens: 474&#xA;command-r-plus                 | score: 33.1  | 95% CI: (-2.0, 1.9)  | average #tokens: 541&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Running &lt;code&gt;show_results.py&lt;/code&gt; will save generated battles into &lt;code&gt;data/arena_hard_battles.jsonl&lt;/code&gt; and bootstrapping statistics into &lt;code&gt;data/bootstrapping_results.jsonl&lt;/code&gt;. If you don&#39;t want to regenerate battles or bootstrapping statistics, simply toggle argument &lt;code&gt;--load-battles&lt;/code&gt; or &lt;code&gt;--load-bootstrap&lt;/code&gt;, respectively.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluate a new model on Arena-hard-v0.1:&lt;/h2&gt; &#xA;&lt;h3&gt;Step 1. Set up the endpoint config to your model&lt;/h3&gt; &#xA;&lt;p&gt;Fill in your API endpoint in &lt;code&gt;config/api_config.yaml&lt;/code&gt;. We support OpenAI compatible API server. You can specify &lt;code&gt;parallel&lt;/code&gt; to indicate the number of concurrent API requests (default: 1).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;# example&#xA;gpt-3.5-turbo-0125:&#xA;    model_name: gpt-3.5-turbo-0125&#xA;    endpoints: null&#xA;    api_type: openai&#xA;    parallel: 8&#xA;&#xA;[YOUR-MODEL-NAME]:&#xA;    model_name: [YOUR-MODEL-NAME]&#xA;    endpoints:&#xA;        - api_base: [YOUR-ENDPOINT-URL]&#xA;          api_key: [YOUR-API-KEY]&#xA;    api_type: openai&#xA;    parallel: 8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You may use inference engine such as &lt;a href=&#34;https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html&#34;&gt;vLLM&lt;/a&gt; or &lt;a href=&#34;https://github.com/sgl-project/sglang?tab=readme-ov-file#using-local-models&#34;&gt;SGLang&lt;/a&gt; to host your model with an OpenAI compatible API server.&lt;/p&gt; &#xA;&lt;h3&gt;Step 2. Generate Model Answers&lt;/h3&gt; &#xA;&lt;p&gt;In &lt;code&gt;config/gen_answer_config.yaml&lt;/code&gt;, add your model name in &lt;code&gt;model_list&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;bench_name: arena-hard-v0.1&#xA;temperature: 0.0&#xA;max_tokens: 4096&#xA;num_choices: 1&#xA;&#xA;model_list:&#xA;  - [YOUR-MODEL-NAME]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run the command to generate answers:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;python gen_answer.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Caching feature is implemented. The code will skip generating an answer when there is already an existing answer/judgment to the same prompt.&lt;/p&gt; &#xA;&lt;h3&gt;Step 3. Generate Judgments&lt;/h3&gt; &#xA;&lt;p&gt;In &lt;code&gt;config/judge_config.yaml&lt;/code&gt;, add your model name in &lt;code&gt;model_list&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;...&#xA;# Add your model below for evaluation&#xA;model_list:&#xA;  - gpt-3.5-turbo-0125&#xA;  - [YOUR-MODEL-NAME]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run the command to generate judgments:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;python gen_judgment.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Judgment caching is also implemented. It will skip generating judgments that has already been generated or lacks one of the model answers.&lt;/p&gt; &#xA;&lt;h3&gt;Step 4. Show result&lt;/h3&gt; &#xA;&lt;p&gt;Output model win rates. Optionally, use &lt;code&gt;--full-stats&lt;/code&gt; for detailed results.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;&amp;gt; python show_result.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step 5. Arena Hard UI&lt;/h3&gt; &#xA;&lt;p&gt;You can review individual judgment results using our UI code.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;&amp;gt; python qa_broswer.py --share&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Community Contribution&lt;/h2&gt; &#xA;&lt;p&gt;Coming soon...&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{arenahard2024,&#xA;    title = {From Live Data to High-Quality Benchmarks: The Arena-Hard Pipeline},&#xA;    url = {https://lmsys.org/blog/2024-04-19-arena-hard/},&#xA;    author = {Tianle Li*, Wei-Lin Chiang*, Evan Frick, Lisa Dunlap, Banghua Zhu, Joseph E. Gonzalez, Ion Stoica},&#xA;    month = {April},&#xA;    year = {2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>