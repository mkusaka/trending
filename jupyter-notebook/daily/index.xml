<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-04T01:30:20Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>bornofdata/British-Broadcasting-Corporation</title>
    <updated>2024-03-04T01:30:20Z</updated>
    <id>tag:github.com,2024-03-04:/bornofdata/British-Broadcasting-Corporation</id>
    <link href="https://github.com/bornofdata/British-Broadcasting-Corporation" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;British-Broadcasting-Corporation&lt;/h1&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;The objective of this project is to employ Natural Language Processing (NLP) techniques to analyze a collection of articles from the BBC across five distinct categories. The project aims to uncover insights by addressing two primary questions:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Identifying Common Topics by Category: For each of the five specified categories, the project seeks to determine the most prevalent topics among the articles. Utilizing NLP algorithms, the analysis will focus on extracting key themes, subjects, and trends within each category. This exploration aims to provide a comprehensive understanding of the predominant discussions within the different sections of the BBC.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Exploring G20 Country Mentions Across Categories: The second aspect of the project involves examining how many articles across all categories specifically mention each of the G20 countries. By leveraging NLP algorithms, the analysis will identify and quantify the frequency of references to individual G20 nations within the entire dataset. This broader perspective aims to unveil patterns in the global coverage provided by the BBC, shedding light on the prominence of G20 countries in various news topics.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Overall, this NLP-driven project seeks to offer valuable insights into the content of BBC articles, providing a nuanced understanding of prevalent topics within specific categories and the coverage of G20 countries across diverse news segments. The application of NLP techniques enables a data-driven approach to uncover patterns, trends, and relationships within the extensive BBC article dataset.&lt;/p&gt; &#xA;&lt;h2&gt;BBC Datasets&lt;/h2&gt; &#xA;&lt;p&gt;Two news article datasets, originating from &lt;a href=&#34;http://mlg.ucd.ie/datasets/bbc.html&#34;&gt;BBC News&lt;/a&gt;, provided for use as benchmarks for machine learning research. These datasets are made available for non-commercial and research purposes only. If you make use of these datasets please consider citing the publication:&lt;/p&gt; &#xA;&lt;p&gt;D. Greene and P. Cunningham. &#34;Practical Solutions to the Problem of Diagonal Dominance in Kernel Document Clustering&#34;, Proc. ICML 2006. &lt;a href=&#34;http://mlg.ucd.ie/files/publications/greene06icml.pdf&#34;&gt;PDF&lt;/a&gt; &lt;a href=&#34;http://mlg.ucd.ie/files/bib/greene06icml.bib&#34;&gt;BibTeX&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Dataset: BBC&lt;/h3&gt; &#xA;&lt;p&gt;All rights, including copyright, in the content of the original articles are owned by the BBC.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Consists of 2225 documents from the BBC news website corresponding to stories in five topical areas from 2004-2005.&lt;/li&gt; &#xA; &lt;li&gt;Class Labels: 5 (business, entertainment, politics, sport, tech)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;http://mlg.ucd.ie/files/datasets/bbc.zip&#34;&gt;Download pre-processed dataset&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip&#34;&gt;Download raw text files&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Dataset: BBCSport&lt;/h3&gt; &#xA;&lt;p&gt;All rights, including copyright, in the content of the original articles are owned by the BBC.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Consists of 737 documents from the BBC Sport website corresponding to sports news articles in five topical areas from 2004-2005.&lt;/li&gt; &#xA; &lt;li&gt;Class Labels: 5 (athletics, cricket, football, rugby, tennis)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;http://mlg.ucd.ie/files/datasets/bbcsport.zip&#34;&gt;Download pre-processed dataset&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;http://mlg.ucd.ie/files/datasets/bbcsport-fulltext.zip&#34;&gt;Download raw text files&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;File formats&lt;/h3&gt; &#xA;&lt;p&gt;The datasets have been pre-processed as follows: stemming (Porter algorithm), stop-word removal (&lt;a href=&#34;http://mlg.ucd.ie/files/datasets/stopwords.txt&#34;&gt;stop word list&lt;/a&gt;) and low term frequency filtering (count &amp;lt; 3) have already been applied to the data. The files contained in the archives given above have the following formats:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;*.mtx: Original term frequencies stored in a sparse data matrix in &lt;a href=&#34;http://math.nist.gov/MatrixMarket/index.html&#34;&gt;Matrix Market format&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;*.terms: List of content-bearing terms in the corpus, with each line corresponding to a row of the sparse data matrix.&lt;/li&gt; &#xA; &lt;li&gt;*.docs: List of document identifiers, with each line corresponding to a column of the sparse data matrix.&lt;/li&gt; &#xA; &lt;li&gt;*.classes: Assignment of documents to natural classes, with each line corresponding to a document.&lt;/li&gt; &#xA; &lt;li&gt;*.urls: Links to original articles, where appropriate.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Questions:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;For each of the five categories: what are the most common topics among articles?&lt;/li&gt; &#xA; &lt;li&gt;Across all categories: how many articles talk about each of the G20 countries?&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/code/fagbamigbekehinde/bbc-category&#34;&gt;Link to analysis on Kaggle&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>PKU-YuanGroup/Open-Sora-Plan</title>
    <updated>2024-03-04T01:30:20Z</updated>
    <id>tag:github.com,2024-03-04:/PKU-YuanGroup/Open-Sora-Plan</id>
    <link href="https://github.com/PKU-YuanGroup/Open-Sora-Plan" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This project aim to reproducing Sora (Open AI T2V model), but we only have limited resource. We deeply wish the all open source community can contribute to this project.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Open-Sora Plan&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pku-yuangroup.github.io/Open-Sora-Plan/&#34;&gt;[Project Page]&lt;/a&gt; &lt;a href=&#34;https://pku-yuangroup.github.io/Open-Sora-Plan/blog_cn.html&#34;&gt;[中文主页]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This project aims to create a simple and scalable repo, to reproduce &lt;a href=&#34;https://openai.com/sora&#34;&gt;Sora&lt;/a&gt; (OpenAI, but we prefer to call it &#34;CloseAI&#34; ) and build knowledge about Video-VQVAE (VideoGPT) + DiT at scale. However, we have limited resources, we deeply wish all open-source community can contribute to this project. Pull request are welcome!!!&lt;/p&gt; &#xA;&lt;p&gt;本项目希望通过开源社区的力量复现Sora，由北大-兔展AIGC联合实验室共同发起，当前我们资源有限仅搭建了基础架构，无法进行完整训练，希望通过开源社区逐步增加模块并筹集资源进行训练，当前版本离目标差距巨大，仍需持续完善和快速迭代，欢迎Pull request！！！&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/Open-Sora-Plan/main/assets/we_want_you.jpg&#34; width=&#34;27%&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/Open-Sora-Plan/main/assets/framework.jpg&#34; alt=&#34;The architecture of Open-Sora-Plan&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;[2024.03.03]&lt;/strong&gt; We open some &lt;a href=&#34;https://github.com/PKU-YuanGroup/Open-Sora-Plan/discussions&#34;&gt;discussions&lt;/a&gt; and clarify several issues.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[2024.03.01]&lt;/strong&gt; Training codes are available now! Learn more in our &lt;a href=&#34;https://pku-yuangroup.github.io/Open-Sora-Plan/&#34;&gt;project page&lt;/a&gt;. Please feel free to watch 👀 this repository for the latest updates.&lt;/p&gt; &#xA;&lt;h2&gt;Todo&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;support variable aspect ratios, resolutions, durations training on DiT&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;dynamic mask input&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;add class-conditioning on embeddings&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;sampling script&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;add positional interpolation&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;fine-tune Video-VQVAE on higher resolution&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;incorporating SiT&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;incorporating more conditions&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;training with more data and more GPU&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirements and Installation&lt;/h2&gt; &#xA;&lt;p&gt;The recommended requirements are as follows.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python &amp;gt;= 3.8&lt;/li&gt; &#xA; &lt;li&gt;Pytorch &amp;gt;= 1.13.1&lt;/li&gt; &#xA; &lt;li&gt;CUDA Version &amp;gt;= 11.7&lt;/li&gt; &#xA; &lt;li&gt;Install required packages:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/PKU-YuanGroup/Open-Sora-Plan&#xA;cd Open-Sora-Plan&#xA;conda create -n opensora python=3.8 -y&#xA;conda activate opensora&#xA;pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117&#xA;pip install -r requirements.txt&#xA;cd VideoGPT&#xA;pip install -e .&#xA;cd ..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Datasets&lt;/h3&gt; &#xA;&lt;p&gt;We test the code with UCF-101 dataset. In order to download UCF-101 dataset, you can download the necessary files in &lt;a href=&#34;https://www.crcv.ucf.edu/data/UCF101.php&#34;&gt;here&lt;/a&gt;. The code assumes a &lt;code&gt;ucf101&lt;/code&gt; directory with the following structure&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;UCF-101/&#xA;    ApplyEyeMakeup/&#xA;        v1.avi&#xA;        ...&#xA;    ...&#xA;    YoYo/&#xA;        v1.avi&#xA;        ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Video-VQVAE (VideoGPT)&lt;/h3&gt; &#xA;&lt;h4&gt;Training&lt;/h4&gt; &#xA;&lt;p&gt;Refer to origin &lt;a href=&#34;https://github.com/wilson1yan/VideoGPT?tab=readme-ov-file#training-vq-vae&#34;&gt;repo&lt;/a&gt;. Use the &lt;code&gt;scripts/train_vqvae.py&lt;/code&gt; script to train a Video-VQVAE. Execute &lt;code&gt;python scripts/train_vqvae.py -h&lt;/code&gt; for information on all available training settings. A subset of more relevant settings are listed below, along with default values.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd VideoGPT&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;VQ-VAE Specific Settings&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--embedding_dim&lt;/code&gt;: number of dimensions for codebooks embeddings&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--n_codes 2048&lt;/code&gt;: number of codes in the codebook&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--n_hiddens 240&lt;/code&gt;: number of hidden features in the residual blocks&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--n_res_layers 4&lt;/code&gt;: number of residual blocks&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--downsample 4 4 4&lt;/code&gt;: T H W downsampling stride of the encoder&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h5&gt;Training Settings&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--gpus 2&lt;/code&gt;: number of gpus for distributed training&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--sync_batchnorm&lt;/code&gt;: uses &lt;code&gt;SyncBatchNorm&lt;/code&gt; instead of &lt;code&gt;BatchNorm3d&lt;/code&gt; when using &amp;gt; 1 gpu&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--gradient_clip_val 1&lt;/code&gt;: gradient clipping threshold for training&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--batch_size 16&lt;/code&gt;: batch size per gpu&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--num_workers 8&lt;/code&gt;: number of workers for each DataLoader&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h5&gt;Dataset Settings&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--data_path &amp;lt;path&amp;gt;&lt;/code&gt;: path to an &lt;code&gt;hdf5&lt;/code&gt; file or a folder containing &lt;code&gt;train&lt;/code&gt; and &lt;code&gt;test&lt;/code&gt; folders with subdirectories of videos&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--resolution 128&lt;/code&gt;: spatial resolution to train on&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--sequence_length 16&lt;/code&gt;: temporal resolution, or video clip length&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Reconstructing&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;python VideoGPT/rec_video.py --video-path &#34;assets/origin_video_0.mp4&#34; --rec-path &#34;rec_video_0.mp4&#34; --num-frames 500 --sample-rate 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;python VideoGPT/rec_video.py --video-path &#34;assets/origin_video_1.mp4&#34; --rec-path &#34;rec_video_1.mp4&#34; --resolution 196 --num-frames 600 --sample-rate 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We present four reconstructed videos in this demonstration, arranged from left to right as follows:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;3s 596x336&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;10s 256x256&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;18s 196x196&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;24s 168x96&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/Open-Sora-Plan/main/assets/rec_video_2.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/Open-Sora-Plan/main/assets/rec_video_0.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/Open-Sora-Plan/main/assets/rec_video_1.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/Open-Sora-Plan/main/assets/rec_video_3.gif&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;VideoDiT (DiT)&lt;/h3&gt; &#xA;&lt;h4&gt;Training&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd DiT&#xA;torchrun  --nproc_per_node=8 train.py \&#xA;  --model DiT-XL/122 --pt-ckpt DiT-XL-2-256x256.pt \&#xA;  --vae ucf101_stride4x4x4 \&#xA;  --data-path /remote-home/yeyang/UCF-101 --num-classes 101 \&#xA;  --sample-rate 2 --num-frames 8 --max-image-size 128 --clip-grad-norm 1 \&#xA;  --epochs 14000 --global-batch-size 256 --lr 1e-4 \&#xA;  --ckpt-every 1000 --log-every 1000 &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/Open-Sora-Plan/main/assets/loss.jpg&#34; width=&#34;60%&#34;&gt; &lt;/p&gt; &#xA;&lt;h4&gt;Sampling&lt;/h4&gt; &#xA;&lt;p&gt;Coming soon.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/DiT/tree/main&#34;&gt;DiT&lt;/a&gt;: Scalable Diffusion Models with Transformers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/wilson1yan/VideoGPT&#34;&gt;VideoGPT&lt;/a&gt;: Video Generation using VQ-VAE and Transformers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/whlzy/FiT&#34;&gt;FiT&lt;/a&gt;: Flexible Vision Transformer for Diffusion Model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2306.15595&#34;&gt;Positional Interpolation&lt;/a&gt;: Extending Context Window of Large Language Models via Positional Interpolation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The service is a research preview intended for non-commercial use only. See &lt;a href=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/Open-Sora-Plan/main/LICENSE.txt&#34;&gt;LICENSE.txt&lt;/a&gt; for details.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;a href=&#34;https://github.com/PKU-YuanGroup/Open-Sora-Plan/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=PKU-YuanGroup/Open-Sora-Plan&#34;&gt; &lt;/a&gt;</summary>
  </entry>
</feed>