<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-10T01:37:05Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>PacktPublishing/Machine-Learning-for-Imbalanced-Data</title>
    <updated>2023-12-10T01:37:05Z</updated>
    <id>tag:github.com,2023-12-10:/PacktPublishing/Machine-Learning-for-Imbalanced-Data</id>
    <link href="https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Machine Learning for Imbalanced Data, published by Packt&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Machine Learning for Imbalanced Data&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.amazon.com/dp/1801070830&#34;&gt;&lt;img src=&#34;https://m.media-amazon.com/images/W/MEDIAX_792452-T2/images/I/41ldy7Kud6L._SX342_SY445_.jpg&#34; alt=&#34;Machine Learning for Imbalanced Data&#34; height=&#34;256px&#34; align=&#34;right&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is the code repository for &lt;a href=&#34;https://www.amazon.com/dp/1801070830&#34;&gt;Machine Learning for Imbalanced Data&lt;/a&gt;, published by Packt.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tackle imbalanced datasets using machine learning and deep learning techniques&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;What is this book about?&lt;/h2&gt; &#xA;&lt;p&gt;As machine learning practitioners, we often encounter imbalanced datasets in which one class has considerably fewer instances than the other. Many machine learning algorithms assume an equilibrium between majority and minority classes, leading to suboptimal performance on imbalanced data. This comprehensive guide helps you address this class imbalance to significantly improve model performance.&lt;/p&gt; &#xA;&lt;p&gt;This book covers the following exciting features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Use imbalanced data in your machine learning models effectively&lt;/li&gt; &#xA; &lt;li&gt;Explore the metrics used when classes are imbalanced&lt;/li&gt; &#xA; &lt;li&gt;Understand how and when to apply various sampling methods such as over-sampling and under-sampling&lt;/li&gt; &#xA; &lt;li&gt;Apply data-based, algorithm-based, and hybrid approaches to deal with class imbalance&lt;/li&gt; &#xA; &lt;li&gt;Combine and choose from various options for data balancing while avoiding common pitfalls&lt;/li&gt; &#xA; &lt;li&gt;Understand the concepts of model calibration and threshold adjustment in the context of dealing with imbalanced datasets&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you feel this book is for you, get your copy today!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.amazon.com/Machine-Learning-Imbalanced-Data-imbalanced/dp/1801070830&#34;&gt;Amazon.com link&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.amazon.com/Machine-Learning-Imbalanced-Data-imbalanced/dp/1801070830&#34;&gt;Amazon.in link&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.packtpub.com/product/machine-learning-for-imbalanced-data/9781801070836&#34;&gt;Packt link&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.packtpub.com/product/machine-learning-for-imbalanced-data/9781801070836&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/PacktPublishing/GitHub/master/GitHub.png&#34; alt=&#34;https://www.packtpub.com/&#34; border=&#34;5&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Instructions and Navigations&lt;/h2&gt; &#xA;&lt;p&gt;All of the code is organized into folders.&lt;/p&gt; &#xA;&lt;p&gt;The code will look like the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;from collections import Counter&#xA;X, y = make_data(sep=2)&#xA;print(y.value_counts())&#xA;sns.scatterplot(data=X, x=&#34;feature_1&#34;, y=&#34;feature_2&#34;)&#xA;plt.title(&#39;Separation: {}&#39;.format(separation))&#xA;plt.show()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Following is what you need for this book:&lt;/strong&gt; This book is for machine learning practitioners who want to effectively address the challenges of imbalanced datasets in their projects. Data scientists, machine learning engineers/scientists, research scientists/engineers, and data scientists/engineers will find this book helpful. Though complete beginners are welcome to read this book, some familiarity with core machine learning concepts will help readers maximize the benefits and insights gained from this comprehensive resource.&lt;/p&gt; &#xA;&lt;p&gt;With the following software and hardware list you can run all code files present in the book (Chapter 1-10).&lt;/p&gt; &#xA;&lt;h3&gt;Software and Hardware List&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Chapter&lt;/th&gt; &#xA;   &lt;th&gt;Software required&lt;/th&gt; &#xA;   &lt;th&gt;OS required&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1-10&lt;/td&gt; &#xA;   &lt;td&gt;Google Colab&lt;/td&gt; &#xA;   &lt;td&gt;Any OS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Related products &#xA; &lt;other books you may enjoy&gt;&lt;/other&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Machine Learning with PyTorch and Scikit-Learn &lt;a href=&#34;https://www.packtpub.com/product/machine-learning-with-pytorch-and-scikit-learn/9781801819312&#34;&gt;[Packt]&lt;/a&gt; &lt;a href=&#34;https://www.amazon.in/Machine-Learning-PyTorch-Scikit-Learn-learning/dp/1801819319/ref=sr_1_3?keywords=Machine+Learning+with+PyTorch+and+Scikit-Learn&amp;amp;sr=8-3&#34;&gt;[Amazon]&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Graph Machine Learning &lt;a href=&#34;https://www.packtpub.com/product/graph-machine-learning/9781800204492&#34;&gt;[Packt]&lt;/a&gt; &lt;a href=&#34;https://www.amazon.in/Graph-Machine-Learning-techniques-algorithms/dp/1800204493/ref=sr_1_3?keywords=Graph+Machine+Learning&amp;amp;sr=8-3&#34;&gt;[Amazon]&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Get to Know the Authors&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Kumar Abhishek&lt;/strong&gt; is a seasoned Senior Machine Learning Engineer at Expedia Group, US, specializing in risk analysis and fraud detection for Expedia brands. With over a decade of experience at companies such as Microsoft, Amazon, and a Bay Area startup, Kumar holds an MS in Computer Science from the University of Florida.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dr. Mounir Abdelaziz&lt;/strong&gt; is a deep learning researcher specializing in computer vision applications. He holds a Ph.D. in computer science and technology from Central South University, China. During his Ph.D. journey, he developed innovative algorithms to address practical computer vision challenges. He has also authored numerous research articles in the field of few-shot learning for image classification.&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents and Code Notebooks&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Introduction to Data Imbalance in Machine Learning [&lt;a href=&#34;https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/main/chapter01&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Oversampling Methods [&lt;a href=&#34;https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/main/chapter02&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Undersampling Methods [&lt;a href=&#34;https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/main/chapter03&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Ensemble Methods [&lt;a href=&#34;https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/main/chapter04&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Cost-Sensitive Learning [&lt;a href=&#34;https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/main/chapter05&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Data Imbalance in Deep Learning [&lt;a href=&#34;https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/main/chapter06&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Data-Level Deep Learning Methods [&lt;a href=&#34;https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/main/chapter07&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Algorithm-Level Deep Learning Techniques [&lt;a href=&#34;https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/main/chapter08&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Hybrid Deep Learning Methods [&lt;a href=&#34;https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/main/chapter09&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Model Calibration [&lt;a href=&#34;https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/main/chapter10&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/main/Table%20of%20Content.pdf&#34;&gt;Detailed Table of content&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;List of notebooks&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Notebook ID&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Link&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Notebook 1.1&lt;/td&gt; &#xA;   &lt;td&gt;Imbalanced-learn demo&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/main/chapter01/imblearn_demo.ipynb&#34;&gt;ipynb/colab&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Notebook 2.1&lt;/td&gt; &#xA;   &lt;td&gt;Oversampling techniques&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/raw/main/chapter02/chapter02.ipynb&#34;&gt;ipynb/colab&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Notebook 2.2&lt;/td&gt; &#xA;   &lt;td&gt;Oversampling performance&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/raw/main/chapter02/model-perf-comparison.ipynb&#34;&gt;ipynb/colab&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Notebook 2.3&lt;/td&gt; &#xA;   &lt;td&gt;SMOTE problems&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/raw/main/chapter02/smote-plots.ipynb&#34;&gt;ipynb/colab&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Notebook 3.1&lt;/td&gt; &#xA;   &lt;td&gt;Various undersampling techniques&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/raw/main/chapter03/chapter03.ipynb&#34;&gt;ipynb/colab&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Notebook 3.2&lt;/td&gt; &#xA;   &lt;td&gt;Undersampling performance&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/raw/main/chapter03/model_perf_comparison_undersampling.ipynb&#34;&gt;ipynb/colab&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Notebook 4.1&lt;/td&gt; &#xA;   &lt;td&gt;Ensemble techniques overview&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/raw/main/chapter04/chapter04.ipynb&#34;&gt;ipynb/colab&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Notebook 4.2&lt;/td&gt; &#xA;   &lt;td&gt;Ensembling methods performance&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/raw/main/chapter04/model_perf_comparison_ensembling.ipynb&#34;&gt;ipynb/colab&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Notebook 5.1&lt;/td&gt; &#xA;   &lt;td&gt;Class weight with Sklearn/XGBoost&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/raw/main/chapter05/chapter05.ipynb&#34;&gt;ipynb/colab&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Notebook 5.2&lt;/td&gt; &#xA;   &lt;td&gt;Threshold tuning techniques&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/raw/main/chapter05/threshold_computation.ipynb&#34;&gt;ipynb/colab&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Notebook 6.1&lt;/td&gt; &#xA;   &lt;td&gt;Simple neural network&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/raw/main/chapter06/chapter06.ipynb&#34;&gt;ipynb/colab&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Notebook 6.2&lt;/td&gt; &#xA;   &lt;td&gt;Multi-class classification&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/raw/main/chapter06/multiclass_classification_PR_curve.ipynb&#34;&gt;ipynb/colab&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Notebook 7.1&lt;/td&gt; &#xA;   &lt;td&gt;Augmix on FashionMNIST&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/raw/main/chapter07/Augmix_FashionMNIST.ipynb&#34;&gt;ipynb/colab&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Notebook 7.2&lt;/td&gt; &#xA;   &lt;td&gt;Cutmix, Mixup, Remix on FashionMNIST&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/raw/main/chapter07/Cutmix_mixup_remix_FashionMNIST.ipynb&#34;&gt;ipynb/colab&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Notebook 7.3&lt;/td&gt; &#xA;   &lt;td&gt;NLP data-level techniques&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/raw/main/chapter07/Data_level_techniques_NLP.ipynb&#34;&gt;ipynb/colab&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Notebook 7.4&lt;/td&gt; &#xA;   &lt;td&gt;Dynamic sampling&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/raw/main/chapter07/Dynamic_sampler.ipynb&#34;&gt;ipynb/colab&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Notebook 7.5&lt;/td&gt; &#xA;   &lt;td&gt;VAE with MNIST&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/raw/main/chapter07/VAE_MNIST.ipynb&#34;&gt;ipynb/colab&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Notebook 7.6&lt;/td&gt; &#xA;   &lt;td&gt;Cutmix technique&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/raw/main/chapter07/cutmix.ipynb&#34;&gt;ipynb/colab&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Notebook 7.7&lt;/td&gt; &#xA;   &lt;td&gt;Cutout technique&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/raw/main/chapter07/cutout.ipynb&#34;&gt;ipynb/colab&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Notebook 7.8&lt;/td&gt; &#xA;   &lt;td&gt;Mixup technique&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/raw/main/chapter07/mixup.ipynb&#34;&gt;ipynb/colab&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Notebook 7.9&lt;/td&gt; &#xA;   &lt;td&gt;Data transformation plotting&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/raw/main/chapter07/plot_transforms.ipynb&#34;&gt;ipynb/colab&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Notebook 8.1&lt;/td&gt; &#xA;   &lt;td&gt;CIFAR10 focal loss&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/raw/main/chapter08/CIFAR10_LT_Focal_Loss.ipynb&#34;&gt;ipynb/colab&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Notebook 8.2&lt;/td&gt; &#xA;   &lt;td&gt;CDT loss implementation&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/raw/main/chapter08/Class_Dependent_Temperature_(CDT)_Loss.ipynb&#34;&gt;ipynb/colab&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Notebook 8.3&lt;/td&gt; &#xA;   &lt;td&gt;Class balanced loss&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/raw/main/chapter08/Class_balanced_loss.ipynb&#34;&gt;ipynb/colab&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Notebook 8.4&lt;/td&gt; &#xA;   &lt;td&gt;Class-wise difficulty balanced loss&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/raw/main/chapter08/Class_wise_difficulty_balanced_loss.ipynb&#34;&gt;ipynb/colab&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Notebook 8.5&lt;/td&gt; &#xA;   &lt;td&gt;DRW technique&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/raw/main/chapter08/Deferred_reweighting_DRW.ipynb&#34;&gt;ipynb/colab&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Notebook 8.6&lt;/td&gt; &#xA;   &lt;td&gt;Tweet emotion detection&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/raw/main/chapter08/Tweet_emotion_detection_using_class_weights_Huggingface.ipynb&#34;&gt;ipynb/colab&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Notebook 8.7&lt;/td&gt; &#xA;   &lt;td&gt;PyTorch class weighting&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/raw/main/chapter08/class_weighting_pytorch_imbalanced_dataset.ipynb&#34;&gt;ipynb/colab&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Notebook 9.1&lt;/td&gt; &#xA;   &lt;td&gt;GNN demo&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/raw/main/chapter09/GNNs.ipynb&#34;&gt;ipynb/colab&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Notebook 9.2&lt;/td&gt; &#xA;   &lt;td&gt;OHEM technique&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/raw/main/chapter09/Online_hard_example_mining-OHEM.ipynb&#34;&gt;ipynb/colab&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Notebook 9.3&lt;/td&gt; &#xA;   &lt;td&gt;Class rectification loss&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/raw/main/chapter09/Class_Rectification_Loss.ipynb&#34;&gt;ipynb/colab&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Notebook 10.1&lt;/td&gt; &#xA;   &lt;td&gt;Calibration techniques&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/raw/main/chapter10/calibration_MNIST.ipynb&#34;&gt;ipynb/colab&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Notebook 10.2&lt;/td&gt; &#xA;   &lt;td&gt;Sampling/weighting impact on calibration&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/raw/main/chapter10/calibration_to_account_for_sampling_or_weighting.ipynb&#34;&gt;ipynb/colab&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Notebook 10.3&lt;/td&gt; &#xA;   &lt;td&gt;Imbalance handling impact on calibration&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/raw/main/chapter10/model-calibration.ipynb&#34;&gt;ipynb/colab&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Notebook 10.4&lt;/td&gt; &#xA;   &lt;td&gt;Kaggle HR data calibration&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/raw/main/chapter10/model_calibration_on_Kaggle_HR_data.ipynb&#34;&gt;ipynb/colab&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Notebook 10.5&lt;/td&gt; &#xA;   &lt;td&gt;Plat&#39;s scaling and isotonic regression&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/raw/main/chapter10/platts_scaling_and_isotonic_regression.ipynb&#34;&gt;ipynb/colab&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;hr&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;Kumar Abhishek, Dr. Mounir Abdelaziz, &lt;em&gt;Machine Learning for Imbalanced Data&lt;/em&gt;. Packt Publishing, 2023.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    @book{mlimbdata2023,&#xA;    title = {Machine Learning for Imbalanced Data},&#xA;    author = {Kumar Abhishek and Mounir Abdelaziz},&#xA;    year = {2023},&#xA;    publisher = {Packt},&#xA;    isbn = {9781801070836}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>state-spaces/s4</title>
    <updated>2023-12-10T01:37:05Z</updated>
    <id>tag:github.com,2023-12-10:/state-spaces/s4</id>
    <link href="https://github.com/state-spaces/s4" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Structured state space sequence models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Structured State Spaces for Sequence Modeling&lt;/h1&gt; &#xA;&lt;p&gt;This repository provides the official implementations and experiments for models related to &lt;a href=&#34;https://arxiv.org/abs/2111.00396&#34;&gt;S4&lt;/a&gt;, including &lt;a href=&#34;https://arxiv.org/abs/2008.07669&#34;&gt;HiPPO&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2110.13985&#34;&gt;LSSL&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2202.09729&#34;&gt;SaShiMi&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2203.14343&#34;&gt;DSS&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2206.12037&#34;&gt;HTTYH&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2206.11893&#34;&gt;S4D&lt;/a&gt;, and &lt;a href=&#34;https://arxiv.org/abs/2210.06583&#34;&gt;S4ND&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Project-specific information for each of these models, including overview of the source code and specific experiment reproductions, can be found under &lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/models/&#34;&gt;models/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;p&gt;Setting up the environment and porting S4 to external codebases:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/#setup&#34;&gt;Setup&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/#getting-started-with-s4&#34;&gt;Getting Started with S4&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Using this repository for training models:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/#training&#34;&gt;Training&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/#generation&#34;&gt;Generation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/#overall-repository-structure&#34;&gt;Repository Structure&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Changelog&lt;/h3&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/CHANGELOG.md&#34;&gt;CHANGELOG.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Roadmap&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;More documentation for training from scratch using this repository&lt;/li&gt; &#xA; &lt;li&gt;Compilation of S4 resources and implementations&lt;/li&gt; &#xA; &lt;li&gt;pip package&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;p&gt;This repository requires Python 3.9+ and Pytorch 1.10+. It has been tested up to Pytorch 1.13.1. Other packages are listed in &lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/requirements.txt&#34;&gt;requirements.txt&lt;/a&gt;. Some care may be needed to make some of the library versions compatible, particularly torch/torchvision/torchaudio/torchtext.&lt;/p&gt; &#xA;&lt;p&gt;Example installation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda install pytorch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 pytorch-cuda=11.6 -c pytorch -c nvidia&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Structured Kernels&lt;/h3&gt; &#xA;&lt;p&gt;A core operation of S4 are the Cauchy and Vandermonde kernels described in the &lt;a href=&#34;https://arxiv.org/abs/2111.00396&#34;&gt;paper&lt;/a&gt;. These are very simple matrix multiplications; a naive implementation of these operation can be found in the &lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/models/s4/s4.py&#34;&gt;standalone&lt;/a&gt; in the function &lt;code&gt;cauchy_naive&lt;/code&gt; and &lt;code&gt;log_vandermonde_naive&lt;/code&gt;. However, as the paper describes, this has suboptimal memory usage that currently requires a custom kernel to overcome in PyTorch.&lt;/p&gt; &#xA;&lt;p&gt;Two more efficient methods are supported. The code will automatically detect if either of these is installed and call the appropriate kernel.&lt;/p&gt; &#xA;&lt;h4&gt;Custom CUDA Kernel&lt;/h4&gt; &#xA;&lt;p&gt;This version is faster but requires manual compilation for each machine environment. Run &lt;code&gt;python setup.py install&lt;/code&gt; from the directory &lt;code&gt;extensions/kernels/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Pykeops&lt;/h4&gt; &#xA;&lt;p&gt;This version is provided by the &lt;a href=&#34;https://www.kernel-operations.io/keops/python/installation.html&#34;&gt;pykeops library&lt;/a&gt;. Installation usually works out of the box with &lt;code&gt;pip install pykeops cmake&lt;/code&gt; which are also listed in the requirements file.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started with S4&lt;/h2&gt; &#xA;&lt;h3&gt;S4 Module&lt;/h3&gt; &#xA;&lt;p&gt;Self-contained files for the S4 layer and variants can be found in &lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/models/s4/&#34;&gt;models/s4/&lt;/a&gt;, which includes instructions for calling the module.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/notebooks/&#34;&gt;notebooks/&lt;/a&gt; for visualizations explaining some concepts behind HiPPO and S4.&lt;/p&gt; &#xA;&lt;h3&gt;Example Train Script (External Usage)&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/example.py&#34;&gt;example.py&lt;/a&gt; is a self-contained training script for MNIST and CIFAR that imports the standalone S4 file. The default settings &lt;code&gt;python example.py&lt;/code&gt; reaches 88% accuracy on sequential CIFAR with a very simple S4D model of 200k parameters. This script can be used as an example for using S4 variants in external repositories.&lt;/p&gt; &#xA;&lt;h3&gt;Training with this Repository (Internal Usage)&lt;/h3&gt; &#xA;&lt;p&gt;This repository aims to provide a very flexible framework for training sequence models. Many models and datasets are supported.&lt;/p&gt; &#xA;&lt;p&gt;The basic entrypoint is &lt;code&gt;python -m train&lt;/code&gt;, or equivalently&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m train pipeline=mnist model=s4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;which trains an S4 model on the Permuted MNIST dataset. This should get to around 90% after 1 epoch which takes 1-3 minutes depending on GPU.&lt;/p&gt; &#xA;&lt;p&gt;More examples of using this repository are documented throughout. See &lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/#training&#34;&gt;Training&lt;/a&gt; for an overview.&lt;/p&gt; &#xA;&lt;h3&gt;Optimizer Hyperparameters&lt;/h3&gt; &#xA;&lt;p&gt;One important feature of this codebase is supporting parameters that require different optimizer hyperparameters. In particular, the SSM kernel is particularly sensitive to the $(A, B)$ (and sometimes $\Delta$ parameters), so the learning rate on these parameters is sometimes lowered and the weight decay is always set to $0$.&lt;/p&gt; &#xA;&lt;p&gt;See the method &lt;code&gt;register&lt;/code&gt; in the model (e.g. &lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/py&#34;&gt;s4d.py&lt;/a&gt;) and the function &lt;code&gt;setup_optimizer&lt;/code&gt; in the training script (e.g. &lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/example.py&#34;&gt;example.py&lt;/a&gt;) for an examples of how to implement this in external repos.&lt;/p&gt; &#xA;&lt;!--&#xA;Our logic for setting these parameters can be found in the `OptimModule` class under `src/models/sequence/ss/kernel.py` and the corresponding optimizer hook in `SequenceLightningModule.configure_optimizers` under `train.py`&#xA;--&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;The core training infrastructure of this repository is based on &lt;a href=&#34;https://pytorch-lightning.readthedocs.io/en/latest/&#34;&gt;Pytorch-Lightning&lt;/a&gt; with a configuration scheme based on &lt;a href=&#34;https://hydra.cc/docs/intro/&#34;&gt;Hydra&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The main entrypoint is &lt;code&gt;train.py&lt;/code&gt; and configs are found in &lt;code&gt;configs/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Data&lt;/h3&gt; &#xA;&lt;p&gt;Basic datasets are auto-downloaded, including MNIST, CIFAR, and Speech Commands. All logic for creating and loading datasets is in &lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/src/dataloaders/&#34;&gt;src/dataloaders&lt;/a&gt; directory. The README inside this subdirectory documents how to download and organize other datasets.&lt;/p&gt; &#xA;&lt;h3&gt;Models&lt;/h3&gt; &#xA;&lt;p&gt;Models are defined in &lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/src/models&#34;&gt;src/models&lt;/a&gt;. See the README in this subdirectory for an overview.&lt;/p&gt; &#xA;&lt;h3&gt;Configs and Hyperparameters&lt;/h3&gt; &#xA;&lt;p&gt;Pre-defined configs reproducing end-to-end experiments from the papers are provided, found under project-specific information in &lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/models/&#34;&gt;models/&lt;/a&gt;, such as for the &lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/models/s4/experiments.md&#34;&gt;original S4 paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Configs can also be easily modified through the command line. An example experiment is&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m train pipeline=mnist dataset.permute=True model=s4 model.n_layers=3 model.d_model=128 model.norm=batch model.prenorm=True wandb=null&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This uses the Permuted MNIST task with an S4 model with a specified number of layers, backbone dimension, and normalization type.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/configs/&#34;&gt;configs/README.md&lt;/a&gt; for more detailed documentation about the configs.&lt;/p&gt; &#xA;&lt;h4&gt;Hydra&lt;/h4&gt; &#xA;&lt;p&gt;It is recommended to read the &lt;a href=&#34;https://hydra.cc/docs/intro/&#34;&gt;Hydra documentation&lt;/a&gt; to fully understand the configuration framework. For help launching specific experiments, please file an issue.&lt;/p&gt; &#xA;&lt;!--&#xA;#### Registries&#xA;&#xA;This codebase uses a modification of the hydra `instantiate` utility that provides shorthand names of different classes, for convenience in configuration and logging.&#xA;The mapping from shorthand to full path can be found in `src/utils/registry.py`.&#xA;--&gt; &#xA;&lt;h3&gt;Resuming&lt;/h3&gt; &#xA;&lt;p&gt;Each experiment will be logged to its own directory (generated by Hydra) of the form &lt;code&gt;./outputs/&amp;lt;date&amp;gt;/&amp;lt;time&amp;gt;/&lt;/code&gt;. Checkpoints will be saved here inside this folder and printed to console whenever a new checkpoint is created. To resume training, simply point to the desired &lt;code&gt;.ckpt&lt;/code&gt; file (a PyTorch Lightning checkpoint, e.g. &lt;code&gt;./outputs/&amp;lt;date&amp;gt;/&amp;lt;time&amp;gt;/checkpoints/val/loss.ckpt&lt;/code&gt;) and append the flag &lt;code&gt;train.ckpt=&amp;lt;path&amp;gt;/&amp;lt;to&amp;gt;/&amp;lt;checkpoint&amp;gt;.ckpt&lt;/code&gt; to the original training command.&lt;/p&gt; &#xA;&lt;h3&gt;PyTorch Lightning Trainer&lt;/h3&gt; &#xA;&lt;p&gt;The PTL &lt;a href=&#34;https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html&#34;&gt;Trainer&lt;/a&gt; class controls the overall training loop and also provides many useful pre-defined flags. Some useful examples are explained below. The full list of allowable flags can be found in the PTL documentation, as well as our &lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/configs/trainer/&#34;&gt;trainer configs&lt;/a&gt;. See the default trainer config &lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/configs/trainer/default.yaml&#34;&gt;configs/trainer/default.yaml&lt;/a&gt; for the most useful options.&lt;/p&gt; &#xA;&lt;h4&gt;Multi-GPU training&lt;/h4&gt; &#xA;&lt;p&gt;Simply pass in &lt;code&gt;trainer.gpus=2&lt;/code&gt; to train with 2 GPUs.&lt;/p&gt; &#xA;&lt;h4&gt;Inspect model layers&lt;/h4&gt; &#xA;&lt;p&gt;&lt;code&gt;trainer.weights_summary=full&lt;/code&gt; prints out every layer of the model with their parameter counts. Useful for debugging internals of models.&lt;/p&gt; &#xA;&lt;h4&gt;Data subsampling&lt;/h4&gt; &#xA;&lt;p&gt;&lt;code&gt;trainer.limit_{train,val}_batches={10,0.1}&lt;/code&gt; trains (validates) on only 10 batches (0.1 fraction of all batches). Useful for testing the train loop without going through all the data.&lt;/p&gt; &#xA;&lt;h3&gt;WandB&lt;/h3&gt; &#xA;&lt;p&gt;Logging with &lt;a href=&#34;https://wandb.ai/site&#34;&gt;WandB&lt;/a&gt; is built into this repository. In order to use this, simply set your &lt;code&gt;WANDB_API_KEY&lt;/code&gt; environment variable, and change the &lt;code&gt;wandb.project&lt;/code&gt; attribute of &lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/configs/config.yaml&#34;&gt;configs/config.yaml&lt;/a&gt; (or pass it on the command line e.g. &lt;code&gt;python -m train .... wandb.project=s4&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;p&gt;Set &lt;code&gt;wandb=null&lt;/code&gt; to turn off WandB logging.&lt;/p&gt; &#xA;&lt;h2&gt;Generation&lt;/h2&gt; &#xA;&lt;p&gt;Autoregressive generation can be performed with the &lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/generate.py&#34;&gt;generate.py&lt;/a&gt; script. This script can be used in two ways after training a model using this codebase.&lt;/p&gt; &#xA;&lt;h3&gt;Option 1: Checkpoint Path&lt;/h3&gt; &#xA;&lt;p&gt;The more flexible option requires the checkpoint path of the trained PyTorch Lightning model. The generation script accepts the same config options as the train script, with a few additional flags that are documented in &lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/configs/generate.yaml&#34;&gt;configs/generate.yaml&lt;/a&gt;. After training with &lt;code&gt;python -m train &amp;lt;train flags&amp;gt;&lt;/code&gt;, generate with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m generate &amp;lt;train flags&amp;gt; checkpoint_path=&amp;lt;path/to/model.ckpt&amp;gt; &amp;lt;generation flags&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Any of the flags found in the config can be overridden.&lt;/p&gt; &#xA;&lt;p&gt;Note: This option can be used with either &lt;code&gt;.ckpt&lt;/code&gt; checkpoints (PyTorch Lightning, which includes information for the Trainer) or &lt;code&gt;.pt&lt;/code&gt; checkpoints (PyTorch, which is just a model state dict).&lt;/p&gt; &#xA;&lt;h3&gt;Option 2: Experiment Path&lt;/h3&gt; &#xA;&lt;p&gt;The second option for generation does not require passing in training flags again, and instead reads the config from the Hydra experiment folder, along with a PyTorch Lightning checkpoint within the experiment folder.&lt;/p&gt; &#xA;&lt;h3&gt;Example 1 (Language)&lt;/h3&gt; &#xA;&lt;p&gt;Download the &lt;a href=&#34;https://huggingface.co/krandiash/sashimi-release/tree/main/checkpoints&#34;&gt;WikiText-103 model checkpoint&lt;/a&gt;, for example to &lt;code&gt;./checkpoints/s4-wt103.pt&lt;/code&gt;. This model was trained with the command &lt;code&gt;python -m train experiment=lm/s4-wt103&lt;/code&gt;. Note that from the config we can see that the model was trained with a receptive field of length 8192.&lt;/p&gt; &#xA;&lt;p&gt;To generate, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m generate experiment=lm/s4-wt103 checkpoint_path=checkpoints/s4-wt103.pt n_samples=1 l_sample=16384 l_prefix=8192 decode=text&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This generates a sample of length 16384 conditioned on a prefix of length 8192.&lt;/p&gt; &#xA;&lt;h3&gt;Example 2 (Audio)&lt;/h3&gt; &#xA;&lt;p&gt;Let&#39;s train a small SaShiMi model on the SC09 dataset. We can also reduce the number of training and validation batches to get a checkpoint faster:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m train experiment=audio/sashimi-sc09 model.n_layers=2 trainer.limit_train_batches=0.1 trainer.limit_val_batches=0.1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After the first epoch completes, a message is printed indicating where the checkpoint is saved.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Epoch 0, global step 96: val/loss reached 3.71754 (best 3.71754), saving model to &#34;&amp;lt;repository&amp;gt;/outputs/&amp;lt;date&amp;gt;/&amp;lt;time&amp;gt;/checkpoints/val/loss.ckpt&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Option 1:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m generate experiment=audio/sashimi-sc09 model.n_layers=2 checkpoint_path=&amp;lt;repository&amp;gt;/outputs/&amp;lt;date&amp;gt;/&amp;lt;time&amp;gt;/checkpoints/val/loss.ckpt n_samples=4 l_sample=16000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This option redefines the full config so that the model and dataset can be constructed.&lt;/p&gt; &#xA;&lt;p&gt;Option 2:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m generate experiment_path=&amp;lt;repository&amp;gt;/outputs/&amp;lt;date&amp;gt;/&amp;lt;time&amp;gt; checkpoint_path=checkpoints/val/loss.ckpt n_samples=4 l_sample=16000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This option only needs the path to the Hydra experiment folder and the desired checkpoint within.&lt;/p&gt; &#xA;&lt;h2&gt;Overall Repository Structure&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;configs/         Config files for model, data pipeline, training loop, etc.&#xA;data/            Default location of raw data&#xA;extensions/      CUDA extensions (Cauchy and Vandermonde kernels)&#xA;src/             Main source code for models, datasets, etc.&#xA;  callbacks/     Training loop utilities (e.g. checkpointing)&#xA;  dataloaders/   Dataset and dataloader definitions&#xA;  models/        Model definitions&#xA;  tasks/         Encoder/decoder modules to interface between data and model backbone&#xA;  utils/&#xA;models/          Model-specific information (code, experiments, additional resources)&#xA;example.py       Example training script for using S4 externally&#xA;train.py         Training entrypoint for this repo&#xA;generate.py      Autoregressive generation script&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use this codebase, or otherwise found our work valuable, please cite S4 and &lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/models/README.md#citations&#34;&gt;other relevant papers&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{gu2022efficiently,&#xA;  title={Efficiently Modeling Long Sequences with Structured State Spaces},&#xA;  author={Gu, Albert and Goel, Karan and R\&#39;e, Christopher},&#xA;  booktitle={The International Conference on Learning Representations ({ICLR})},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>