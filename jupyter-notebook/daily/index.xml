<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-01-21T01:38:20Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>vincentblot28/conformalized_gp</title>
    <updated>2024-01-21T01:38:20Z</updated>
    <id>tag:github.com,2024-01-21:/vincentblot28/conformalized_gp</id>
    <link href="https://github.com/vincentblot28/conformalized_gp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Conformal Approach To Gaussian Process Surrogate Evaluation With Coverage Guarantees&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;What is done in this repo&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h1&gt;üîó Requirements&lt;/h1&gt; &#xA;&lt;p&gt;Python 3.7+&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://openturns.github.io/www/index.html&#34;&gt;OpenTURNS&lt;/a&gt; is a C++ library made, hence one can need to install gcc to be able to run the library&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Ubuntu&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ sudo apt update&#xA;$ sudo apt install build-essential&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;OSX&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ brew install gcc&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Windows&lt;/strong&gt;: Install MinGW (a Windows distribution of gcc) or Microsoft‚Äôs Visual C&lt;/p&gt; &#xA;&lt;p&gt;Install the required packages:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Via &lt;code&gt;pip&lt;/code&gt;:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Via conda:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ conda install -f environment.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;üõ† Installation&lt;/h1&gt; &#xA;&lt;p&gt;Clone the repo and run the following command in the conformalized_gp directory to install the code&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;‚ö°Ô∏è Quickstart&lt;/h1&gt; &#xA;&lt;p&gt;Here is a @quickstart to use the Jackknife+GP method on any regression dataset. Here, the goal is the compare visually the results given by the standard Jackknife+ method, the Credibility Intervals and our methodology. The notebook from which this quickstart is inspired can be found &lt;a href=&#34;https://github.com/vincentblot28/conformalized_gp/raw/main/notebook/conformalized_gp_quickstart.ipynb&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;We first start to import the necessary packages&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import matplotlib.pyplot as plt&#xA;import numpy as np&#xA;import scipy&#xA;from sklearn.gaussian_process import GaussianProcessRegressor&#xA;from sklearn.model_selection import train_test_split&#xA;&#xA;from mapie.conformity_scores.residual_conformity_scores import GPConformityScore&#xA;from mapie.regression import MapieRegressor&#xA;&#xA;BLUE = np.array([[26, 54, 105]]) / 255&#xA;ORANGE = np.array([[223, 84, 49]]) / 255&#xA;YELLOW = np.array([[242, 188, 64]]) / 255&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;In this example, we are going to work on an analytical function of our imagination which have some good visual behavior :&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;$$g(x) = 3x\sin(x) - 2x\cos(x) + \frac{x^3}{40} - \frac{x^2}{2} - 10x$$&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def g(x):&#xA;    return (3 * x * np.sin(x) - 2 * x * np.cos(x) + ( x ** 3) / 40 - .5 * x ** 2 - 10 * x)&#xA;&#xA;x_mesh = np.linspace(-40, 60, 5000)&#xA;plt.plot(x_mesh, g(x_mesh))&#xA;plt.xlabel(&#34;$x$&#34;)&#xA;plt.ylabel(&#34;$g(x)$&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/vincentblot28/conformalized_gp/raw/main/plots/toy_function.png&#34; alt=&#34;toy function&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Then we split our data into train and test and train au sickit-learn &lt;code&gt;GaussianProcessRegressor&lt;/code&gt; with a &lt;code&gt;RBF&lt;/code&gt; kernel.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_train, X_test, y_train, y_test = train_test_split(x_mesh, g(x_mesh), test_size=.98, random_state=42)&#xA;X_train = X_train.reshape(-1, 1)&#xA;X_test = X_test.reshape(-1, 1)&#xA;gp = GaussianProcessRegressor(normalize_y=True)&#xA;gp.fit(X_train, y_train)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We then define and train the two conformal methods (J+ and J+GP):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mapie_j_plus_gp = MapieRegressor(&#xA;    estimator=gp,&#xA;    cv=-1,&#xA;    method=&#34;plus&#34;,&#xA;    conformity_score=GPConformityScore(),&#xA;    model_has_std=True,&#xA;    random_state=42&#xA;)&#xA;&#xA;mapie_j_plus = MapieRegressor(&#xA;    estimator=gp,&#xA;    cv=-1,&#xA;    method=&#34;plus&#34;,&#xA;    conformity_score=None,&#xA;    model_has_std=False,&#xA;    random_state=42&#xA;)&#xA;&#xA;&#xA;mapie_j_plus_gp.fit(X_train, y_train)&#xA;mapie_j_plus.fit(X_train, y_train)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Finally, we predict and compute prediction intervals with a confidence level of 90% on the test set and plot the prediction intervals of the three methods&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ALPHA = .1&#xA;&#xA;_, y_pss_j_plus_gp = mapie_j_plus_gp.predict(x_mesh.reshape(-1, 1), alpha=ALPHA)&#xA;_, y_pss_j_plus = mapie_j_plus.predict(x_mesh.reshape(-1, 1), alpha=ALPHA)&#xA;&#xA;y_mean, y_std = gp.predict(x_mesh.reshape(-1, 1), return_std=True)&#xA;&#xA;q_alpha_min = scipy.stats.norm.ppf(ALPHA / 2)&#xA;q_alpha_max = scipy.stats.norm.ppf(1 - ALPHA / 2)&#xA;&#xA;f, ax = plt.subplots(1, 1, figsize=(20, 10))&#xA;ax.scatter(X_train, y_train, c=BLUE)&#xA;&#xA;&#xA;ax.plot(x_mesh, g(x_mesh), c=BLUE)&#xA;ax.plot(x_mesh, y_mean, c=YELLOW)&#xA;ax.fill_between(&#xA;        x_mesh,&#xA;        y_mean + y_std * q_alpha_min,&#xA;        y_mean + y_std * q_alpha_max,&#xA;        alpha=0.3,&#xA;        color=YELLOW,&#xA;        label=r&#34;$\pm$ 1 std. dev.&#34;,&#xA;    )&#xA;&#xA;&#xA;ax.fill_between(&#xA;        x_mesh,&#xA;        y_pss_j_plus_gp[:, 0, 0],&#xA;        y_pss_j_plus_gp[:, 1, 0],&#xA;        alpha=.6,&#xA;        color=ORANGE,&#xA;        label=r&#34;$\pm$ 1 std. dev.&#34;,&#xA;    )&#xA;&#xA;ax.fill_between(&#xA;        x_mesh,&#xA;        y_pss_j_plus[:, 0, 0],&#xA;        y_pss_j_plus[:, 1, 0],&#xA;        alpha=.3,&#xA;        color=&#34;g&#34;,&#xA;        label=r&#34;$\pm$ 1 std. dev.&#34;,&#xA;    )&#xA;ax.legend(&#xA;    [&#xA;        &#34;Training Points&#34;,&#xA;        &#34;True function&#34;, &#34;Mean of posterior GP&#34;,&#xA;        &#34;Posterior GP Credibility Interval&#34;,&#xA;        &#34;Prediction Interval J+GP&#34;,&#xA;         &#34;Prediction Interval J+&#34;, &#xA;    ]&#xA;)&#xA;ax.set_xlabel(&#34;$x$&#34;)&#xA;ax.set_ylabel(&#34;$g(x)$&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/vincentblot28/conformalized_gp/raw/main/plots/intervals_toy_function.png&#34; alt=&#34;toy function intervals&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;üîå Plug OpenTURNS GP into MAPIE&lt;/h1&gt; &#xA;&lt;p&gt;If you wish to use our code with an OpenTURNS model, we have implemented a simple wrapper around the model so that it can be used very easily:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from wrappers import GpOTtoSklearnStd&#xA;&#xA;nu = 5/2  # Hyperparameter of the Mat√©rn Kernel&#xA;noise = None  # Standard deviation of the nugget effect. If None, no nugget effect is applied.&#xA;gp_estimator = GpOTtoSklearnStd(scale=1, amplitude=1, nu=nu, noise=None)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This estimator is now fully compatible with MAPIE as it comes with it &lt;code&gt;.fit&lt;/code&gt; and &lt;code&gt;.predict&lt;/code&gt; methods.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>collabora/WhisperSpeech</title>
    <updated>2024-01-21T01:38:20Z</updated>
    <id>tag:github.com,2024-01-21:/collabora/WhisperSpeech</id>
    <link href="https://github.com/collabora/WhisperSpeech" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An Open Source text-to-speech system built by inverting Whisper.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;WhisperSpeech&lt;/h1&gt; &#xA;&lt;!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! --&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1xxGlTbwBmaY6GKA24strRixTXGBOlyiw&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Test it out yourself in Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/FANw4rHD5E&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/FANw4rHD5E&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;em&gt;If you have questions or you want to help you can find us in the #audio-generation channel on the LAION Discord server.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;An Open Source text-to-speech system built by inverting Whisper. Previously known as &lt;strong&gt;spear-tts-pytorch&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We want this model to be like Stable Diffusion but for speech ‚Äì both powerful and easily customizable.&lt;/p&gt; &#xA;&lt;p&gt;We are working only with properly licensed speech recordings and all the code is Open Source so the model will be always safe to use for commercial applications.&lt;/p&gt; &#xA;&lt;p&gt;Currently the models are trained on the English LibreLight dataset. In the next release we want to target multiple languages (Whisper and EnCodec are both multilanguage).&lt;/p&gt; &#xA;&lt;p&gt;Sample of the synthesized voice:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/collabora/WhisperSpeech/assets/107984/aa5a1e7e-dc94-481f-8863-b022c7fd7434&#34;&gt;https://github.com/collabora/WhisperSpeech/assets/107984/aa5a1e7e-dc94-481f-8863-b022c7fd7434&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Progress update [2024-01-18]&lt;/h2&gt; &#xA;&lt;p&gt;We spend the last week optimizing inference performance. We integrated &lt;code&gt;torch.compile&lt;/code&gt;, added kv-caching and tuned some of the layers ‚Äì we are now working over 12x faster than real-time on a consumer 4090!&lt;/p&gt; &#xA;&lt;p&gt;We can mix languages in a single sentence (here the highlighted English project names are seamlessly mixed into Polish speech):&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;To jest pierwszy test wielojƒôzycznego &lt;code&gt;Whisper Speech&lt;/code&gt; modelu zamieniajƒÖcego tekst na mowƒô, kt√≥ry &lt;code&gt;Collabora&lt;/code&gt; i &lt;code&gt;Laion&lt;/code&gt; nauczyli na superkomputerze &lt;code&gt;Jewels&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/collabora/WhisperSpeech/assets/107984/d7092ef1-9df7-40e3-a07e-fdc7a090ae9e&#34;&gt;https://github.com/collabora/WhisperSpeech/assets/107984/d7092ef1-9df7-40e3-a07e-fdc7a090ae9e&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;We also added an easy way to test voice-cloning. Here is a sample voice cloned from &lt;a href=&#34;https://en.wikipedia.org/wiki/File:Winston_Churchill_-_Be_Ye_Men_of_Valour.ogg&#34;&gt;a famous speech by Winston Churchill&lt;/a&gt; (the radio static is a feature, not a bug ;) ‚Äì it is part of the reference recording):&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/collabora/WhisperSpeech/assets/107984/bd28110b-31fb-4d61-83f6-c997f560bc26&#34;&gt;https://github.com/collabora/WhisperSpeech/assets/107984/bd28110b-31fb-4d61-83f6-c997f560bc26&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can &lt;a href=&#34;https://colab.research.google.com/drive/1xxGlTbwBmaY6GKA24strRixTXGBOlyiw&#34;&gt;test all of these on Collab&lt;/a&gt; (we optimized the dependencies so now it takes less than 30 seconds to install). A Huggingface Space is coming soon.&lt;/p&gt; &#xA;&lt;h2&gt;Progress update [2024-01-10]&lt;/h2&gt; &#xA;&lt;p&gt;We‚Äôve pushed a new SD S2A model that is a lot faster while still generating high-quality speech. We‚Äôve also added an example of voice cloning based on a reference audio file.&lt;/p&gt; &#xA;&lt;p&gt;As always, you can &lt;a href=&#34;https://colab.research.google.com/drive/1xxGlTbwBmaY6GKA24strRixTXGBOlyiw&#34;&gt;check out our Colab&lt;/a&gt; to try it yourself!&lt;/p&gt; &#xA;&lt;h2&gt;Progress update [2023-12-10]&lt;/h2&gt; &#xA;&lt;p&gt;Another trio of models, this time they support multiple languages (English and Polish). Here are two new samples for a sneak peek. You can &lt;a href=&#34;https://colab.research.google.com/drive/1xxGlTbwBmaY6GKA24strRixTXGBOlyiw&#34;&gt;check out our Colab&lt;/a&gt; to try it yourself!&lt;/p&gt; &#xA;&lt;p&gt;English speech, female voice (transferred from a Polish language dataset):&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/collabora/WhisperSpeech/assets/107984/aa5a1e7e-dc94-481f-8863-b022c7fd7434&#34;&gt;https://github.com/collabora/WhisperSpeech/assets/107984/aa5a1e7e-dc94-481f-8863-b022c7fd7434&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A Polish sample, male voice:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/collabora/WhisperSpeech/assets/107984/4da14b03-33f9-4e2d-be42-f0fcf1d4a6ec&#34;&gt;https://github.com/collabora/WhisperSpeech/assets/107984/4da14b03-33f9-4e2d-be42-f0fcf1d4a6ec&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/collabora/WhisperSpeech/issues/23&#34;&gt;Older progress updates are archived here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Downloads&lt;/h2&gt; &#xA;&lt;p&gt;We encourage you to start with the Google Colab link above or run the provided notebook locally. If you want to download manually or train the models from scratch then both &lt;a href=&#34;https://huggingface.co/collabora/whisperspeech&#34;&gt;the WhisperSpeech pre-trained models&lt;/a&gt; as well as &lt;a href=&#34;https://huggingface.co/datasets/collabora/whisperspeech&#34;&gt;the converted datasets&lt;/a&gt; are available on HuggingFace.&lt;/p&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;a href=&#34;https://github.com/collabora/spear-tts-pytorch/issues/11&#34;&gt;Gather a bigger emotive speech dataset&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Figure out a way to condition the generation on emotions and prosody&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Create a community effort to gather freely licensed speech in multiple languages&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;a href=&#34;https://github.com/collabora/spear-tts-pytorch/issues/12&#34;&gt;Train final multi-language models&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Architecture&lt;/h2&gt; &#xA;&lt;p&gt;The general architecture is similar to &lt;a href=&#34;https://google-research.github.io/seanet/audiolm/examples/&#34;&gt;AudioLM&lt;/a&gt;, &lt;a href=&#34;https://google-research.github.io/seanet/speartts/examples/&#34;&gt;SPEAR TTS&lt;/a&gt; from Google and &lt;a href=&#34;https://ai.honu.io/papers/musicgen/&#34;&gt;MusicGen&lt;/a&gt; from Meta. We avoided the NIH syndrome and built it on top of powerful Open Source models: &lt;a href=&#34;https://github.com/openai/whisper&#34;&gt;Whisper&lt;/a&gt; from OpenAI to generate semantic tokens and perform transcription, &lt;a href=&#34;https://github.com/facebookresearch/encodec&#34;&gt;EnCodec&lt;/a&gt; from Meta for acoustic modeling and &lt;a href=&#34;https://github.com/charactr-platform/vocos&#34;&gt;Vocos&lt;/a&gt; from Charactr Inc as the high-quality vocoder.&lt;/p&gt; &#xA;&lt;p&gt;We gave two presentation diving deeper into WhisperSpeech. The first one talks about the challenges of large scale training:&lt;/p&gt; &#xA;&lt;div&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=6Fr-rq-yjXo&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/6Fr-rq-yjXo/0.jpg&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;Tricks Learned from Scaling WhisperSpeech Models to 80k+ Hours of Speech - video recording by Jakub C≈Çapa, Collabora&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;The other one goes a bit more into the architectural choices we made:&lt;/p&gt; &#xA;&lt;div&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=1OBvf33S77Y&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/1OBvf33S77Y/0.jpg&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;Open Source Text-To-Speech Projects: WhisperSpeech - In Depth Discussion&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Whisper for modeling semantic tokens&lt;/h3&gt; &#xA;&lt;p&gt;We utilize the OpenAI Whisper encoder block to generate embeddings which we then quantize to get semantic tokens.&lt;/p&gt; &#xA;&lt;p&gt;If the language is already supported by Whisper then this process requires only audio files (without ground truth transcriptions).&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/collabora/WhisperSpeech/main/whisper-block.png&#34; alt=&#34;Using Whisper for semantic token extraction diagram&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;EnCodec for modeling acoustic tokens&lt;/h2&gt; &#xA;&lt;p&gt;We use EnCodec to model the audio waveform. Out of the box it delivers reasonable quality at 1.5kbps and we can bring this to high-quality by using Vocos ‚Äì a vocoder pretrained on EnCodec tokens.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/facebookresearch/encodec/raw/main/architecture.png&#34; alt=&#34;EnCodec block diagram&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Appreciation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.collabora.com&#34;&gt;&lt;img height=&#34;80&#34; src=&#34;https://user-images.githubusercontent.com/107984/229537027-a6d7462b-0c9c-4fd4-b69e-58e98c3ee63f.png&#34; alt=&#34;Collabora logo&#34;&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href=&#34;https://laion.ai&#34;&gt;&lt;img height=&#34;80&#34; src=&#34;https://user-images.githubusercontent.com/107984/229535036-c741d775-4a9b-4193-89a0-9ddb89ecd011.png&#34; alt=&#34;LAION logo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This work would not be possible without the generous sponsorships from:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.collabora.com&#34;&gt;Collabora&lt;/a&gt; ‚Äì code development and model training&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://laion.ai&#34;&gt;LAION&lt;/a&gt; ‚Äì community building and datasets (special thanks to&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.fz-juelich.de/en&#34;&gt;J√ºlich Supercomputing Centre&lt;/a&gt; - JUWELS Booster supercomputer&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We gratefully acknowledge the Gauss Centre for Supercomputing e.V. (&lt;a href=&#34;http://www.gauss-centre.eu&#34;&gt;www.gauss-centre.eu&lt;/a&gt;) for funding part of this work by providing computing time through the John von Neumann Institute for Computing (NIC) on the GCS Supercomputer JUWELS Booster at J√ºlich Supercomputing Centre (JSC), with access to compute provided via LAION cooperation on foundation models research.&lt;/p&gt; &#xA;&lt;p&gt;We‚Äôd like to also thank individual contributors for their great help in building this model:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/inevitable-2031&#34;&gt;inevitable-2031&lt;/a&gt; (&lt;code&gt;qwerty_qwer&lt;/code&gt; on Discord) for dataset curation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Consulting&lt;/h2&gt; &#xA;&lt;p&gt;We are available to help you with both Open Source and proprietary AI projects. You can reach us via the Collabora website or on Discord (&lt;a href=&#34;https://discordapp.com/users/270267134960074762&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/shield/270267134960074762?style=flat&#34; alt=&#34;&#34;&gt;&lt;/a&gt; and &lt;a href=&#34;https://discordapp.com/users/1088938086400016475&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/shield/1088938086400016475?style=flat&#34; alt=&#34;&#34;&gt;&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;Citations&lt;/h2&gt; &#xA;&lt;p&gt;We rely on many amazing Open Source projects and research papers:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{SpearTTS,&#xA;  title = {Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision},&#xA;  url = {https://arxiv.org/abs/2302.03540},&#xA;  author = {Kharitonov, Eugene and Vincent, Damien and Borsos, Zal√°n and Marinier, Rapha√´l and Girgin, Sertan and Pietquin, Olivier and Sharifi, Matt and Tagliasacchi, Marco and Zeghidour, Neil},&#xA;  publisher = {arXiv},&#xA;  year = {2023},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{MusicGen,&#xA;  title={Simple and Controllable Music Generation}, &#xA;  url = {https://arxiv.org/abs/2306.05284},&#xA;  author={Jade Copet and Felix Kreuk and Itai Gat and Tal Remez and David Kant and Gabriel Synnaeve and Yossi Adi and Alexandre D√©fossez},&#xA;  publisher={arXiv},&#xA;  year={2023},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{Whisper&#xA;  title = {Robust Speech Recognition via Large-Scale Weak Supervision},&#xA;  url = {https://arxiv.org/abs/2212.04356},&#xA;  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},&#xA;  publisher = {arXiv},&#xA;  year = {2022},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{EnCodec&#xA;  title = {High Fidelity Neural Audio Compression},&#xA;  url = {https://arxiv.org/abs/2210.13438},&#xA;  author = {D√©fossez, Alexandre and Copet, Jade and Synnaeve, Gabriel and Adi, Yossi},&#xA;  publisher = {arXiv},&#xA;  year = {2022},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{Vocos&#xA;  title={Vocos: Closing the gap between time-domain and Fourier-based neural vocoders for high-quality audio synthesis}, &#xA;  url = {https://arxiv.org/abs/2306.00814},&#xA;  author={Hubert Siuzdak},&#xA;  publisher={arXiv},&#xA;  year={2023},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>