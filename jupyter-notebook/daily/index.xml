<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-15T01:38:04Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>TradeMaster-NTU/TradeMaster</title>
    <updated>2023-03-15T01:38:04Z</updated>
    <id>tag:github.com,2023-03-15:/TradeMaster-NTU/TradeMaster</id>
    <link href="https://github.com/TradeMaster-NTU/TradeMaster" rel="alternate"></link>
    <summary type="html">&lt;p&gt;TradeMaster is an open-source platform for quantitative trading empowered by reinforcement learning üî• ‚ö° üåà&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TradeMaster: An RL Platform for Trading&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img align=&#34;center&#34; src=&#34;https://github.com/TradeMaster-NTU/TradeMaster/raw/main/figure/Logo.png&#34; width=&#34;25%&#34;&gt; &#xA; &lt;div&gt;&#xA;  &amp;nbsp;&#xA; &lt;/div&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://www.python.org/downloads/release/python-3916/&#34;&gt;&lt;img src=&#34;https://shields.io/badge/python-3.9-blue.svg?sanitize=true&#34; alt=&#34;Python 3.9&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/TradeMaster-NTU/TradeMaster/1.0.0/Platform&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/platform-linux%20%7C%20windows%20%7C%20macos-lightgrey&#34; alt=&#34;Platform&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/TradeMaster-NTU/TradeMaster/1.0.0/License&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/TradeMaster-NTU/TradeMaster&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://trademaster.readthedocs.io/en/latest/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/docs-latest-red&#34; alt=&#34;Document&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/TradeMaster-NTU/TradeMaster/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/TradeMaster-NTU/TradeMaster?color=orange&#34; alt=&#34;GitHub stars&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;TradeMaster is a first-of-its kind, best-in-class &lt;strong&gt;open-source platform&lt;/strong&gt; for &lt;strong&gt;quantitative trading (QT)&lt;/strong&gt; empowered by &lt;strong&gt;reinforcement learning (RL)&lt;/strong&gt;, which covers the &lt;strong&gt;full pipeline&lt;/strong&gt; for the design, implementation, evaluation and deployment of RL-based algorithms.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;‚≠ê&lt;/span&gt; &lt;strong&gt;What&#39;s NEW!&lt;/strong&gt; &lt;span&gt;‚è∞&lt;/span&gt;&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Update&lt;/th&gt; &#xA;   &lt;th&gt;Status&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Release TradeMaster 1.0.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img alt=&#34;octocat&#34; src=&#34;https://github.githubassets.com/images/icons/emoji/octocat.png?v8&#34;&gt;) &lt;a href=&#34;https://github.com/TradeMaster-NTU/TradeMaster/releases/tag/v1.0.0&#34;&gt;Released v1.0.0&lt;/a&gt; on 5 March 2023&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Outline&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TradeMaster-NTU/TradeMaster/1.0.0/#trademaster-an-rl-platform-for-trading&#34;&gt;TradeMaster: An RL Platform for Trading&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TradeMaster-NTU/TradeMaster/1.0.0/#outline&#34;&gt;Outline&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TradeMaster-NTU/TradeMaster/1.0.0/#overview&#34;&gt;Overview&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TradeMaster-NTU/TradeMaster/1.0.0/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TradeMaster-NTU/TradeMaster/1.0.0/#tutorial&#34;&gt;Tutorial&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TradeMaster-NTU/TradeMaster/1.0.0/#useful-script&#34;&gt;Useful Script&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TradeMaster-NTU/TradeMaster/1.0.0/#dataset&#34;&gt;Dataset&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TradeMaster-NTU/TradeMaster/1.0.0/#model-zoo&#34;&gt;Model Zoo&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TradeMaster-NTU/TradeMaster/1.0.0/#visualization-toolkit&#34;&gt;Visualization Toolkit&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TradeMaster-NTU/TradeMaster/1.0.0/#file-structure&#34;&gt;File Structure&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TradeMaster-NTU/TradeMaster/1.0.0/#publications&#34;&gt;Publications&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TradeMaster-NTU/TradeMaster/1.0.0/#news&#34;&gt;News&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TradeMaster-NTU/TradeMaster/1.0.0/#team&#34;&gt;Team&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TradeMaster-NTU/TradeMaster/1.0.0/#competition&#34;&gt;Competition&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TradeMaster-NTU/TradeMaster/1.0.0/#contact-us&#34;&gt;Contact Us&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img align=&#34;center&#34; src=&#34;https://github.com/TradeMaster-NTU/TradeMaster/raw/1.0.0/figure/architecture.jpg&#34; width=&#34;97%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;strong&gt;TradeMaster&lt;/strong&gt; is composed of 6 key modules: 1) multi-modality market data of different financial assets at multiple granularity; 2) whole data preprocessing pipeline; 3) a series of high-fidelity data-driven market simulators for mainstream QT tasks; 4) efficient implementations of over 13 novel RL-based trading algorithms; 5) systematic evaluation toolkits with 6 axes and 17 measures; 6) different interfaces for interdisciplinary users.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Here are the installation tutorials for different operating systems and docker:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/TradeMaster-NTU/TradeMaster/tree/1.0.0/installation/requirements.md&#34;&gt;Installation on Linux/Windows/MacOS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/TradeMaster-NTU/TradeMaster/tree/1.0.0/installation/docker.md&#34;&gt;Installation with Docker&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Tutorial&lt;/h2&gt; &#xA;&lt;p&gt;We provide tutorials covering core features of TradeMaster for users to get start with.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Algorithm&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Dataset&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Market&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Task&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Code Link&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;EIIE&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;DJ30&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;US Stock&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Portfolio Management&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/TradeMaster-NTU/TradeMaster/raw/1.0.0/tutorial/Tutorial1_EIIE.ipynb&#34;&gt;tutorial&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;DeepScalper&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;BTC&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Crypto&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Intraday Trading&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/TradeMaster-NTU/TradeMaster/raw/1.0.0/tutorial/Tutorial2_DeepScalper.ipynb&#34;&gt;tutorial&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SARL&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;DJ30&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;US Stock&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Portfolio Management&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/TradeMaster-NTU/TradeMaster/raw/1.0.0/tutorial/Tutorial3_SARL.ipynb&#34;&gt;tutorial&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;PPO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SSE 50&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;China Stock&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Portfolio Management&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/TradeMaster-NTU/TradeMaster/raw/1.0.0/tutorial/Tutorial4_PPO.ipynb&#34;&gt;tutorial&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ETTO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Bitcoin&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Crypto&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Order Execution&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/TradeMaster-NTU/TradeMaster/raw/1.0.0/tutorial/Tutorial5_ETEO.ipynb&#34;&gt;tutorial&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Double DQN&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Bitcoin&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Crypto&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;High Frequency Trading&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/TradeMaster-NTU/TradeMaster/raw/1.0.0/tutorial/Tutorial6_DDQN.ipynb&#34;&gt;tutorial&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Useful Script&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/TradeMaster-NTU/TradeMaster/raw/1.0.0/tutorial/Tutorial7_auto_tuning.ipynb&#34;&gt;Automatic hyperparameter tuning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/TradeMaster-NTU/TradeMaster/raw/1.0.0/trademaster/evaluation/market_dynamics_labeling/example.ipynb&#34;&gt;Market dynamic labeling&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/TradeMaster-NTU/TradeMaster/raw/1.0.0/tools/missing_value_imputation/run.py&#34;&gt;Financial data imputation with diffusion models&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Dataset&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Dataset&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Data Source&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Type&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Range and Frequency&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Raw Data&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Datasheet&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;S&amp;amp;P500&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pypi.org/project/yfinance/&#34;&gt;Yahoo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;US Stock&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2000/01/01-2022/01/01, 1day&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;OHLCV&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;SP500&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;DJ30&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pypi.org/project/yfinance/&#34;&gt;Yahoo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;US Stock&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2012/01/01-2021/12/31, 1day&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;OHLCV&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/TradeMaster-NTU/TradeMaster/raw/main/data/data/dj30/DJ30.pdf&#34;&gt;DJ30&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;BTC&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pypi.org/project/yfinance/&#34;&gt;Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Foreign Exchange&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2000/01/01-2019/12/31, 1day&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;OHLCV&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/TradeMaster-NTU/TradeMaster/raw/main/data/data/exchange/FX.pdf&#34;&gt;FX&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Crypto&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pypi.org/project/yfinance/&#34;&gt;Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Crypto&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2013/04/29-2021/07/06, 1day&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;OHLCV&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/TradeMaster-NTU/TradeMaster/raw/main/data/data/BTC/Crypto.pdf&#34;&gt;Crypto&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SSE50&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pypi.org/project/yfinance/&#34;&gt;Yahoo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;China Stock&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2009/01/02-2021/01/01, 1day&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;OHLCV&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/TradeMaster-NTU/TradeMaster/raw/main/data/data/sz50/SZ50.pdf&#34;&gt;SSE50&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Bitcoin&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.binance.com/&#34;&gt;Binance&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Crypto&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2021/04/07-2021/04/19, 1min&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;LOB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/TradeMaster-NTU/TradeMaster/raw/main/data/data/OE_BTC/limit_order_book.pdf&#34;&gt;Binance&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Dates are in YY/MM/DD format.&lt;/p&gt; &#xA;&lt;p&gt;OHLCV: open, high, low, and close prices; volume: corresponding trading volume; LOB: Limit order book.&lt;/p&gt; &#xA;&lt;p&gt;Users can download data of the above datasets from &lt;a href=&#34;https://drive.google.com/drive/folders/19Tk5ifPz1y8i_pJVwZFxaSueTLjz6qo3?usp=sharing&#34;&gt;Google Drive&lt;/a&gt; or &lt;a href=&#34;https://pan.baidu.com/s/1njghvez53hD5v3WpLgCg0w&#34;&gt;Baidu Cloud&lt;/a&gt; (extraction code:x24b)&lt;/p&gt; &#xA;&lt;h2&gt;Model Zoo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2201.09058&#34;&gt;DeepScalper based on Pytorch (Shuo Sun et al, CIKM 22)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/view/16083&#34;&gt;OPD based on Pytorch (Fang et al, AAAI 21)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/view/16144&#34;&gt;DeepTrader based on Pytorch (Wang et al, AAAI 21)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2002.05780&#34;&gt;SARL based on Pytorch (Yunan Ye et al, AAAI 20)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.ijcai.org/Proceedings/2020/627?msclkid=a2b6ad5db7ca11ecb537627a9ca1d4f6&#34;&gt;ETTO based on Pytorch (Lin et al, 20)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.kdd.org/kdd2018/accepted-papers/view/investor-imitator-a-framework-for-trading-knowledge-extraction&#34;&gt;Investor-Imitator based on Pytorch (Yi Ding et al, KDD 18)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1706.10059&#34;&gt;EIIE based on Pytorch (Jiang et al, 17)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Classic RL based on Pytorch and Ray: &lt;a href=&#34;https://docs.ray.io/en/latest/rllib/rllib-algorithms.html#ppo&#34;&gt;PPO&lt;/a&gt; &lt;a href=&#34;https://docs.ray.io/en/latest/rllib/rllib-algorithms.html#a3c&#34;&gt;A2C&lt;/a&gt; &lt;a href=&#34;https://docs.ray.io/en/releases-1.13.0/rllib/rllib-algorithms.html#dqn&#34;&gt;Rainbow&lt;/a&gt; &lt;a href=&#34;https://docs.ray.io/en/latest/rllib/rllib-algorithms.html#sac&#34;&gt;SAC&lt;/a&gt; &lt;a href=&#34;https://docs.ray.io/en/latest/rllib/rllib-algorithms.html#ddpg&#34;&gt;DDPG&lt;/a&gt; &lt;a href=&#34;https://docs.ray.io/en/latest/rllib/rllib-algorithms.html#dqn&#34;&gt;DQN&lt;/a&gt; &lt;a href=&#34;https://docs.ray.io/en/latest/rllib/rllib-algorithms.html#pg&#34;&gt;PG&lt;/a&gt; &lt;a href=&#34;https://docs.ray.io/en/latest/rllib/rllib-algorithms.html#ddpg&#34;&gt;TD3&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Visualization Toolkit&lt;/h2&gt; &#xA;&lt;p&gt;TradeMaster provides many visualization toolkits for a systematic evaluation of RL-based quantitative trading methods. Please check this &lt;a href=&#34;https://openreview.net/forum?id=JjbsIYOuNi&#34;&gt;paper&lt;/a&gt; and &lt;a href=&#34;https://github.com/TradeMaster-NTU/PRUDEX-Compass&#34;&gt;repository&lt;/a&gt; for details. Some examples are as follows:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PRIDE-Star&lt;/strong&gt; is a star plot containing normalized score of 8 key financial measures such total return (TR) and Sharpe ratio (SR) to evaluate profitability,risk-control and diversity:&lt;/p&gt; &#xA;&lt;table align=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&#xA;    &lt;center&gt;&#xA;     &lt;img src=&#34;https://github.com/TradeMaster-NTU/TradeMaster/raw/1.0.0/figure/visualization/A2C.jpg&#34; width=&#34;95%&#34;&gt; &#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&#xA;    &lt;center&gt;&#xA;     &lt;img src=&#34;https://github.com/TradeMaster-NTU/TradeMaster/raw/1.0.0/figure/visualization/DeepTrader.jpg&#34; width=&#34;95%&#34;&gt; &#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&#xA;    &lt;center&gt;&#xA;     &lt;img src=&#34;https://github.com/TradeMaster-NTU/TradeMaster/raw/1.0.0/figure/visualization/PPO.jpg&#34; width=&#34;95%&#34;&gt; &#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&#xA;    &lt;center&gt;&#xA;     &lt;img src=&#34;https://github.com/TradeMaster-NTU/TradeMaster/raw/1.0.0/figure/visualization/EIIE.jpg&#34; width=&#34;95%&#34;&gt; &#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;div align=&#34;left&#34;&gt; &#xA; &lt;img align=&#34;center&#34; src=&#34;https://github.com/TradeMaster-NTU/TradeMaster/raw/1.0.0/figure/visualization/plot1.jpg&#34; width=&#34;100%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;left&#34;&gt; &#xA; &lt;img align=&#34;center&#34; src=&#34;https://github.com/TradeMaster-NTU/TradeMaster/raw/1.0.0/figure/visualization/plot2.jpg&#34; width=&#34;100%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;File Structure&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;| TradeMaster&#xA;| ‚îú‚îÄ‚îÄ configs&#xA;| ‚îú‚îÄ‚îÄ data&#xA;| ‚îÇ   ‚îú‚îÄ‚îÄ algorithmic_trading &#xA;| ‚îÇ   ‚îú‚îÄ‚îÄ high_frequency_trading  &#xA;| ‚îÇ   ‚îú‚îÄ‚îÄ order_excution          &#xA;| ‚îÇ   ‚îî‚îÄ‚îÄ porfolio_management&#xA;| ‚îú‚îÄ‚îÄ deploy&#xA;| ‚îÇ   ‚îú‚îÄ‚îÄ backend_client.py&#xA;| ‚îÇ   ‚îú‚îÄ‚îÄ backend_client_test.py &#xA;| ‚îÇ   ‚îî‚îÄ‚îÄ backend_service.py        &#xA;| ‚îÇ   ‚îú‚îÄ‚îÄ backend_service_test.py  &#xA;| ‚îú‚îÄ‚îÄ docs&#xA;| ‚îú‚îÄ‚îÄ figure&#xA;| ‚îú‚îÄ‚îÄ installation&#xA;| ‚îÇ   ‚îú‚îÄ‚îÄ docker.md&#xA;| ‚îÇ   ‚îú‚îÄ‚îÄ requirements.md&#xA;| ‚îú‚îÄ‚îÄ tools&#xA;| ‚îÇ   ‚îú‚îÄ‚îÄ algorithmic_trading          &#xA;| ‚îÇ   ‚îú‚îÄ‚îÄ data_preprocessor&#xA;| ‚îÇ   ‚îú‚îÄ‚îÄ high_frequency_trading&#xA;| ‚îÇ   ‚îú‚îÄ‚îÄ market_dynamics_labeling&#xA;| ‚îÇ   ‚îú‚îÄ‚îÄ missing_value_imputation  &#xA;| ‚îÇ   ‚îú‚îÄ‚îÄ order_excution  &#xA;| ‚îÇ   ‚îú‚îÄ‚îÄ porfolio_management  &#xA;| ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py      &#xA;| ‚îú‚îÄ‚îÄ tradmaster       &#xA;| ‚îÇ   ‚îú‚îÄ‚îÄ agents   &#xA;| ‚îÇ   ‚îú‚îÄ‚îÄ datasets &#xA;| ‚îÇ   ‚îú‚îÄ‚îÄ enviornments &#xA;| ‚îÇ   ‚îú‚îÄ‚îÄ evaluation &#xA;| ‚îÇ   ‚îú‚îÄ‚îÄ imputation &#xA;| ‚îÇ   ‚îú‚îÄ‚îÄ losses&#xA;| ‚îÇ   ‚îú‚îÄ‚îÄ nets&#xA;| ‚îÇ   ‚îú‚îÄ‚îÄ preprocessor&#xA;| ‚îÇ   ‚îú‚îÄ‚îÄ optimizers&#xA;| ‚îÇ   ‚îú‚îÄ‚îÄ pretrained&#xA;| ‚îÇ   ‚îú‚îÄ‚îÄ trainers&#xA;| ‚îÇ   ‚îú‚îÄ‚îÄ transition&#xA;| ‚îÇ   ‚îú‚îÄ‚îÄ utils&#xA;| ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py     &#xA;| ‚îú‚îÄ‚îÄ unit_testing&#xA;| ‚îú‚îÄ‚îÄ Dockerfile&#xA;| ‚îú‚îÄ‚îÄ LICENSE&#xA;| ‚îú‚îÄ‚îÄ README.md&#xA;| ‚îú‚îÄ‚îÄ pyproject.toml&#xA;| ‚îî‚îÄ‚îÄ requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Publications&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://openreview.net/forum?id=JjbsIYOuNi&#34;&gt;PRUDEX-Compass: Towards Systematic Evaluation of Reinforcement Learning in Financial Markets&lt;/a&gt; &lt;em&gt;(2023)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://dl.acm.org/doi/10.1145/3582560&#34;&gt;Reinforcement Learning for Quantitative Trading (Survey)&lt;/a&gt; &lt;em&gt;(ACM Transactions on Intelligent Systems and Technology 2023)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/9779600&#34;&gt;Deep Reinforcement Learning for Quantitative Trading: Challenges and Opportunities&lt;/a&gt; &lt;em&gt;(IEEE Intelligent Systems 2022)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2201.09058&#34;&gt;DeepScalper: A Risk-Aware Reinforcement Learning Framework to Capture Fleeting Intraday Trading Opportunities&lt;/a&gt; &lt;em&gt;(CIKM 2022)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/view/16142&#34;&gt;Commission Fee is not Enough: A Hierarchical Reinforced Framework for Portfolio Management&lt;/a&gt; &lt;em&gt;(AAAI 21)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;p&gt;[Êú∫Âô®‰πãÂøÉ]&lt;a href=&#34;https://mp.weixin.qq.com/s/MTUOksGGgaWX6GkXZT6wwA&#34;&gt;ÂçóÊ¥ãÁêÜÂ∑•ÂèëÂ∏ÉÈáèÂåñ‰∫§ÊòìÂ§ßÂ∏àTradeMasterÔºåÊ∂µÁõñ15ÁßçÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ï&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Team&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This repository is developed and maintained by &lt;a href=&#34;https://personal.ntu.edu.sg/boan/&#34;&gt;AMI&lt;/a&gt; group lead by &lt;a href=&#34;https://raw.githubusercontent.com/TradeMaster-NTU/TradeMaster/1.0.0/boan@ntu.edu.sg&#34;&gt;Prof Bo An&lt;/a&gt; at &lt;a href=&#34;https://www.ntu.edu.sg/&#34;&gt;Nanyang Technological University&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;We have positions for software engineer, research associate and postdoc. If you are interested in working at the intersection of RL and quantitative trading, feel free to send us an email with your CV.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Competition&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://codalab.lisn.upsaclay.fr/competitions/8440?secret_key=51d5952f-d68d-47d9-baef-6032445dea01&#34;&gt;TradeMaster Cup 2022&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contact Us&lt;/h2&gt; &#xA;&lt;p&gt;If you have any further questions of this project, please contact &lt;a href=&#34;https://raw.githubusercontent.com/TradeMaster-NTU/TradeMaster/1.0.0/TradeMaster.NTU@gmail.com&#34;&gt;TradeMaster.NTU@gmail.com&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>huawei-noah/HEBO</title>
    <updated>2023-03-15T01:38:04Z</updated>
    <id>tag:github.com,2023-03-15:/huawei-noah/HEBO</id>
    <link href="https://github.com/huawei-noah/HEBO" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Bayesian optimisation library developped by Huawei Noah&#39;s Ark Lab&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Bayesian Optimisation &amp;amp; Reinforcement Learning Research&lt;/h1&gt; &#xA;&lt;p&gt;This directory contains official implementations for Bayesian optimisation &amp;amp; Reinforcement Learning works developped by Huawei, Noah&#39;s Ark Lab.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bayesian Optimisation Research &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huawei-noah/HEBO/master/HEBO&#34;&gt;HEBO: Heteroscedastic Evolutionary Bayesian Optimisation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huawei-noah/HEBO/master/T-LBO&#34;&gt;T-LBO&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huawei-noah/HEBO/master/BOiLS&#34;&gt;BOiLS: Bayesian Optimisation for Logic Synthesis&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huawei-noah/HEBO/master/CompBO&#34;&gt;Bayesian Optimisation with Compositional Optimisers&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huawei-noah/HEBO/master/AntBO&#34;&gt;AntBO: Antibody Design with Combinatorial Bayesian Optimisation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Reinforcement Learning Research &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huawei-noah/HEBO/master/SIMMER&#34;&gt;Saut√© RL and Simmer RL: Safe Reinforcement Learning Using Safety State Augmentation &lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huawei-noah/HEBO/master/PMDB&#34;&gt;Model-Based Offline Reinforcement Learning with Pessimism-Modulated Dynamics Belief&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Further instructions are provided in the README files associated to each project.&lt;/p&gt; &#xA;&lt;h1&gt;Bayesian Optimisation Research&lt;/h1&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huawei-noah/HEBO/master/HEBO&#34;&gt;HEBO&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huawei-noah/HEBO/master/HEBO/hebo.png&#34; alt=&#34;drawing&#34; width=&#34;400&#34;&gt; &#xA;&lt;p&gt;Bayesian optimsation library developped by Huawei Noahs Ark Decision Making and Reasoning (DMnR) lab. The &lt;strong&gt; winning submission &lt;/strong&gt; to the &lt;a href=&#34;https://bbochallenge.com/leaderboard&#34;&gt;NeurIPS 2020 Black-Box Optimisation Challenge&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huawei-noah/HEBO/master/T-LBO&#34;&gt;T-LBO&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p float=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/huawei-noah/HEBO/master/T-LBO/figures/LSBO.png&#34; width=&#34;400&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/huawei-noah/HEBO/master/T-LBO/figures/magnets.png&#34; width=&#34;400&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Codebase associated to: &lt;a href=&#34;https://arxiv.org/abs/2106.03609&#34;&gt;High-Dimensional Bayesian Optimisation with Variational Autoencoders and Deep Metric Learning&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h5&gt;Abstract&lt;/h5&gt; &#xA;&lt;p&gt;We introduce a method based on deep metric learning to perform Bayesian optimisation over high-dimensional, structured input spaces using variational autoencoders (VAEs). By extending ideas from supervised deep metric learning, we address a longstanding problem in high-dimensional VAE Bayesian optimisation, namely how to enforce a discriminative latent space as an inductive bias. Importantly, we achieve such an inductive bias using just 1% of the available labelled data relative to previous work, highlighting the sample efficiency of our approach. As a theoretical contribution, we present a proof of vanishing regret for our method. As an empirical contribution, we present state-of-the-art results on real-world high-dimensional black-box optimisation problems including property-guided molecule generation. It is the hope that the results presented in this paper can act as a guiding principle for realising effective high-dimensional Bayesian optimisation.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huawei-noah/HEBO/master/BOiLS&#34;&gt;BOiLS: Bayesian Optimisation for Logic Synthesis&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/huawei-noah/HEBO/master/BOiLS/results/sample-eff-1.png&#34; alt=&#34;drawing&#34; width=&#34;500&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Codebase associated to: &lt;a href=&#34;https://arxiv.org/abs/2111.06178&#34;&gt;BOiLS: Bayesian Optimisation for Logic Synthesis&lt;/a&gt; accepted at &lt;strong&gt;DATE22&lt;/strong&gt; conference.&lt;/p&gt; &#xA;&lt;h5&gt;Abstract&lt;/h5&gt; &#xA;&lt;p&gt;Optimising the quality-of-results (QoR) of circuits during logic synthesis is a formidable challenge necessitating the exploration of exponentially sized search spaces. While expert-designed operations aid in uncovering effective sequences, the increase in complexity of logic circuits favours automated procedures. Inspired by the successes of machine learning, researchers adapted deep learning and reinforcement learning to logic synthesis applications. However successful, those techniques suffer from high sample complexities preventing widespread adoption. To enable efficient and scalable solutions, we propose BOiLS, the first algorithm adapting modern Bayesian optimisation to navigate the space of synthesis operations. BOiLS requires no human intervention and effectively trades-off exploration versus exploitation through novel Gaussian process kernels and trust-region constrained acquisitions. In a set of experiments on EPFL benchmarks, we demonstrate BOiLS&#39;s superior performance compared to state-of-the-art in terms of both sample efficiency and QoR values.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huawei-noah/HEBO/master/CompBO&#34;&gt;Bayesian Optimisation with Compositional Optimisers&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;div style=&#34;text-align:center&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/huawei-noah/HEBO/master/CompBO/image/summary-Best-performance-on-Synthetic-tasks-matern-52-3.png&#34; alt=&#34;drawing&#34; width=&#34;600&#34;&gt; &#xA; &lt;div style=&#34;text-align:left&#34;&gt; &#xA;  &lt;p&gt;Codebase associated to: &lt;a href=&#34;https://www.jmlr.org/papers/v22/20-1422.html&#34;&gt;Are we Forgetting about Compositional Optimisers in Bayesian Optimisation?&lt;/a&gt; accepted at &lt;strong&gt;JMLR&lt;/strong&gt;.&lt;/p&gt; &#xA;  &lt;h5&gt;Abstract&lt;/h5&gt; &#xA;  &lt;p&gt;Bayesian optimisation presents a sample-efficient methodology for global optimisation. Within this framework, a crucial performance-determining subroutine is the maximisation of the acquisition function, a task complicated by the fact that acquisition functions tend to be non-convex and thus nontrivial to optimise. In this paper, we undertake a comprehensive empirical study of approaches to maximise the acquisition function. Additionally, by deriving novel, yet mathematically equivalent, compositional forms for popular acquisition functions, we recast the maximisation task as a compositional optimisation problem, allowing us to benefit from the extensive literature in this field. We highlight the empirical advantages of the compositional approach to acquisition function maximisation across 3958 individual experiments comprising synthetic optimisation tasks as well as tasks from Bayesmark. Given the generality of the acquisition function maximisation subroutine, we posit that the adoption of compositional optimisers has the potential to yield performance improvements across all domains in which Bayesian optimisation is currently being applied.&lt;/p&gt; &#xA;  &lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huawei-noah/HEBO/master/AntBO&#34;&gt;AntBO: Antibody Design with Combinatorial Bayesian Optimisation&lt;/a&gt;&lt;/h2&gt; &#xA;  &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/huawei-noah/HEBO/master/AntBO/figures/AntBO_illustrationPNG.PNG?raw=true&#34; alt=&#34;AntBO overview&#34;&gt;&lt;/p&gt; &#xA;  &lt;p&gt;Codebase associated to: &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S2667237522002764&#34;&gt;AntBO: Towards Real-World Automated Antibody Design with Combinatorial Bayesian Optimisation&lt;/a&gt;.&lt;/p&gt; &#xA;  &lt;h5&gt;Abstract&lt;/h5&gt; &#xA;  &lt;p&gt;Antibodies are canonically Y-shaped multimeric proteins capable of highly specific molecular recognition. The CDRH3 region located at the tip of variable chains of an antibody dominates antigen-binding specificity. Therefore, it is a priority to design optimal antigen-specific CDRH3 regions to develop therapeutic antibodies to combat harmful pathogens. However, the combinatorial nature of CDRH3 sequence space makes it impossible to search for an optimal binding sequence exhaustively and efficiently, especially not experimentally. Here, we present AntBO: a Combinatorial Bayesian Optimisation framework enabling efficient in silico design of the CDRH3 region. Ideally, antibodies should bind to their target antigen and be free from any harmful outcomes. Therefore, we introduce the CDRH3 trust region that restricts the search to sequences with feasible developability scores. To benchmark AntBO, we use the Absolut! software suite as a black-box oracle because it can score the target specificity and affinity of designed antibodies in silico in an unconstrained fashion. The results across 188 antigens demonstrate the benefit of AntBO in designing CDRH3 regions with diverse biophysical properties. In under 200 protein designs, AntBO can suggest antibody sequences that outperform the best binding sequence drawn from 6.9 million experimentally obtained CDRH3s and a commonly used genetic algorithm baseline. Additionally, AntBO finds very-high affinity CDRH3 sequences in only 38 protein designs whilst requiring no domain knowledge. We conclude AntBO brings automated antibody design methods closer to what is practically viable for in vitro experimentation.&lt;/p&gt; &#xA;  &lt;h1&gt;Reinforcement Learning Research&lt;/h1&gt; &#xA;  &lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huawei-noah/HEBO/master/SIMMER&#34;&gt;Saut√© RL and Simmer RL: Safe Reinforcement Learning Using Safety State Augmentation&lt;/a&gt;&lt;/h2&gt; &#xA;  &lt;p&gt;Codebase associated to: &lt;a href=&#34;https://arxiv.org/pdf/2202.06558.pdf&#34;&gt;Saut√© RL: Almost Surely Safe RL Using State Augmentation&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/pdf/2206.02675.pdf&#34;&gt;Enhancing Safe Exploration Using Safety State Augmentation&lt;/a&gt;.&lt;/p&gt; &#xA;  &lt;h5&gt;Abstract for Saut√© RL: Almost Surely Safe RL Using State Augmentation (ICML 2022)&lt;/h5&gt; &#xA;  &lt;p&gt;Satisfying safety constraints almost surely (or with probability one) can be critical for deployment of Reinforcement Learning (RL) in real-life applications. For example, plane landing and take-off should ideally occur with probability one. We address the problem by introducing Safety Augmented (Saute) Markov Decision Processes (MDPs), where the safety constraints are eliminated by augmenting them into the state-space and reshaping the objective. We show that Saute MDP satisfies the Bellman equation and moves us closer to solving Safe RL with constraints satisfied almost surely. We argue that Saute MDP allows to view Safe RL problem from a different perspective enabling new features. For instance, our approach has a plug-and-play nature, i.e., any RL algorithm can be &#34;sauteed&#34;. Additionally, state augmentation allows for policy generalization across safety constraints. We finally show that Saute RL algorithms can outperform their state-of-the-art counterparts when constraint satisfaction is of high importance.&lt;/p&gt; &#xA;  &lt;h5&gt;Abstract for Effects of Safety State Augmentation on Safe Exploration (NeurIPS 2022)&lt;/h5&gt; &#xA;  &lt;p&gt;Safe exploration is a challenging and important problem in model-free reinforcement learning (RL). Often the safety cost is sparse and unknown, which unavoidably leads to constraint violations -- a phenomenon ideally to be avoided in safety-critical applications. We tackle this problem by augmenting the state-space with a safety state, which is nonnegative if and only if the constraint is satisfied. The value of this state also serves as a distance toward constraint violation, while its initial value indicates the available safety budget. This idea allows us to derive policies for scheduling the safety budget during training. We call our approach Simmer (Safe policy IMproveMEnt for RL) to reflect the careful nature of these schedules. We apply this idea to two safe RL problems: RL with constraints imposed on an average cost, and RL with constraints imposed on a cost with probability one. Our experiments suggest that simmering a safe algorithm can improve safety during training for both settings. We further show that Simmer can stabilize training and improve the performance of safe RL with average constraints.&lt;/p&gt; &#xA;  &lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huawei-noah/HEBO/master/PMDB&#34;&gt;Model-Based Offline Reinforcement Learning with Pessimism-Modulated Dynamics Belief&lt;/a&gt;&lt;/h2&gt; &#xA;  &lt;p&gt;Code associdated to: &lt;a href=&#34;https://nips.cc/Conferences/2022/Schedule?showEvent=54842&#34;&gt;Model-Based Offline Reinforcement Learning with Pessimism-Modulated Dynamics Belief&lt;/a&gt; accepted at &lt;strong&gt;NeurIPS22&lt;/strong&gt; conference.&lt;/p&gt; &#xA;  &lt;h4&gt;Abstract&lt;/h4&gt; &#xA;  &lt;p&gt;Model-based offline reinforcement learning (RL) aims to find highly rewarding policy, by leveraging a previously collected static dataset and a dynamics model. While learned through reuse of static dataset, the dynamics model&#39;s generalization ability hopefully promotes policy learning if properly utilized. To that end, several works propose to quantify the uncertainty of predicted dynamics, and explicitly apply it to penalize reward. However, as the dynamics and the reward are intrinsically different factors in context of MDP, characterizing the impact of dynamics uncertainty through reward penalty may incur unexpected tradeoff between model utilization and risk avoidance. In this work, we instead maintain a belief distribution over dynamics, and evaluate/optimize policy through biased sampling from the belief. The sampling procedure, biased towards pessimism, is derived based on an alternating Markov game formulation of offline RL. We formally show that the biased sampling naturally induces an updated dynamics belief with policy-dependent reweighting factor, termed &lt;em&gt;Pessimism-Modulated Dynamics Belief&lt;/em&gt;. To improve policy, we devise an iterative regularized policy optimization algorithm for the game, with guarantee of monotonous improvement under certain condition. To make practical, we further devise an offline RL algorithm to approximately find the solution. Empirical results show that the proposed approach achieves state-of-the-art performance on a wide range of benchmark tasks.&lt;/p&gt; &#xA;  &lt;hr&gt; &#xA;  &lt;h2&gt;Codebase Contributors&lt;/h2&gt; &#xA;  &lt;p&gt;&lt;strong&gt; Current contributors: &lt;/strong&gt; Antoine Grosnit, Alexandre Max Maravel, Taher Jafferjee, Wenlong Lyu, Kaiyang Guo.&lt;/p&gt; &#xA;  &lt;p&gt;&lt;strong&gt; Alumni contributors: &lt;/strong&gt; Alexander I. Cowen-Rivers, Aivar Sootla, Ryan Rhys Griffiths, Zhi Wang.&lt;/p&gt; &#xA; &lt;/div&gt;&#xA;&lt;/div&gt;</summary>
  </entry>
  <entry>
    <title>huggingface/setfit</title>
    <updated>2023-03-15T01:38:04Z</updated>
    <id>tag:github.com,2023-03-15:/huggingface/setfit</id>
    <link href="https://github.com/huggingface/setfit" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Efficient few-shot learning with Sentence Transformers&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://raw.githubusercontent.com/huggingface/setfit/main/assets/setfit.png&#34;&gt; &#xA;&lt;p align=&#34;center&#34;&gt; ü§ó &lt;a href=&#34;https://huggingface.co/setfit&#34; target=&#34;_blank&#34;&gt;Models &amp;amp; Datasets&lt;/a&gt; | üìñ &lt;a href=&#34;https://huggingface.co/blog/setfit&#34; target=&#34;_blank&#34;&gt;Blog&lt;/a&gt; | üìÉ &lt;a href=&#34;https://arxiv.org/abs/2209.11055&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h1&gt;SetFit - Efficient Few-shot Learning with Sentence Transformers&lt;/h1&gt; &#xA;&lt;p&gt;SetFit is an efficient and prompt-free framework for few-shot fine-tuning of &lt;a href=&#34;https://sbert.net/&#34;&gt;Sentence Transformers&lt;/a&gt;. It achieves high accuracy with little labeled data - for instance, with only 8 labeled examples per class on the Customer Reviews sentiment dataset, SetFit is competitive with fine-tuning RoBERTa Large on the full training set of 3k examples ü§Ø!&lt;/p&gt; &#xA;&lt;p&gt;Compared to other few-shot learning methods, SetFit has several unique features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üó£ &lt;strong&gt;No prompts or verbalisers:&lt;/strong&gt; Current techniques for few-shot fine-tuning require handcrafted prompts or verbalisers to convert examples into a format that&#39;s suitable for the underlying language model. SetFit dispenses with prompts altogether by generating rich embeddings directly from text examples.&lt;/li&gt; &#xA; &lt;li&gt;üèé &lt;strong&gt;Fast to train:&lt;/strong&gt; SetFit doesn&#39;t require large-scale models like T0 or GPT-3 to achieve high accuracy. As a result, it is typically an order of magnitude (or more) faster to train and run inference with.&lt;/li&gt; &#xA; &lt;li&gt;üåé &lt;strong&gt;Multilingual support&lt;/strong&gt;: SetFit can be used with any &lt;a href=&#34;https://huggingface.co/models?library=sentence-transformers&amp;amp;sort=downloads&#34;&gt;Sentence Transformer&lt;/a&gt; on the Hub, which means you can classify text in multiple languages by simply fine-tuning a multilingual checkpoint.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Download and install &lt;code&gt;setfit&lt;/code&gt; by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m pip install setfit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want the bleeding-edge version, install from source by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m pip install git+https://github.com/huggingface/setfit.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;The examples below provide a quick overview on the various features supported in &lt;code&gt;setfit&lt;/code&gt;. For more examples, check out the &lt;a href=&#34;https://github.com/huggingface/setfit/tree/main/notebooks&#34;&gt;&lt;code&gt;notebooks&lt;/code&gt;&lt;/a&gt; folder.&lt;/p&gt; &#xA;&lt;h3&gt;Training a SetFit model&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;setfit&lt;/code&gt; is integrated with the &lt;a href=&#34;https://huggingface.co/&#34;&gt;Hugging Face Hub&lt;/a&gt; and provides two main classes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;SetFitModel&lt;/code&gt;: a wrapper that combines a pretrained body from &lt;code&gt;sentence_transformers&lt;/code&gt; and a classification head from either &lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html&#34;&gt;&lt;code&gt;scikit-learn&lt;/code&gt;&lt;/a&gt; or &lt;a href=&#34;https://github.com/huggingface/setfit/raw/main/src/setfit/modeling.py&#34;&gt;&lt;code&gt;SetFitHead&lt;/code&gt;&lt;/a&gt; (a differentiable head built upon &lt;code&gt;PyTorch&lt;/code&gt; with similar APIs to &lt;code&gt;sentence_transformers&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;SetFitTrainer&lt;/code&gt;: a helper class that wraps the fine-tuning process of SetFit.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Here is an end-to-end example using a classification head from &lt;code&gt;scikit-learn&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from datasets import load_dataset&#xA;from sentence_transformers.losses import CosineSimilarityLoss&#xA;&#xA;from setfit import SetFitModel, SetFitTrainer, sample_dataset&#xA;&#xA;&#xA;# Load a dataset from the Hugging Face Hub&#xA;dataset = load_dataset(&#34;sst2&#34;)&#xA;&#xA;# Simulate the few-shot regime by sampling 8 examples per class&#xA;train_dataset = sample_dataset(dataset[&#34;train&#34;], label_column=&#34;label&#34;, num_samples=8)&#xA;eval_dataset = dataset[&#34;validation&#34;]&#xA;&#xA;# Load a SetFit model from Hub&#xA;model = SetFitModel.from_pretrained(&#34;sentence-transformers/paraphrase-mpnet-base-v2&#34;)&#xA;&#xA;# Create trainer&#xA;trainer = SetFitTrainer(&#xA;    model=model,&#xA;    train_dataset=train_dataset,&#xA;    eval_dataset=eval_dataset,&#xA;    loss_class=CosineSimilarityLoss,&#xA;    metric=&#34;accuracy&#34;,&#xA;    batch_size=16,&#xA;    num_iterations=20, # The number of text pairs to generate for contrastive learning&#xA;    num_epochs=1, # The number of epochs to use for contrastive learning&#xA;    column_mapping={&#34;sentence&#34;: &#34;text&#34;, &#34;label&#34;: &#34;label&#34;} # Map dataset columns to text/label expected by trainer&#xA;)&#xA;&#xA;# Train and evaluate&#xA;trainer.train()&#xA;metrics = trainer.evaluate()&#xA;&#xA;# Push model to the Hub&#xA;trainer.push_to_hub(&#34;my-awesome-setfit-model&#34;)&#xA;&#xA;# Download from Hub and run inference&#xA;model = SetFitModel.from_pretrained(&#34;lewtun/my-awesome-setfit-model&#34;)&#xA;# Run inference&#xA;preds = model([&#34;i loved the spiderman movie!&#34;, &#34;pineapple on pizza is the worst ü§Æ&#34;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here is an end-to-end example using &lt;code&gt;SetFitHead&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from datasets import load_dataset&#xA;from sentence_transformers.losses import CosineSimilarityLoss&#xA;&#xA;from setfit import SetFitModel, SetFitTrainer, sample_dataset&#xA;&#xA;&#xA;# Load a dataset from the Hugging Face Hub&#xA;dataset = load_dataset(&#34;sst2&#34;)&#xA;&#xA;# Simulate the few-shot regime by sampling 8 examples per class&#xA;train_dataset = sample_dataset(dataset[&#34;train&#34;], label_column=&#34;label&#34;, num_samples=8)&#xA;eval_dataset = dataset[&#34;validation&#34;]&#xA;&#xA;# Load a SetFit model from Hub&#xA;model = SetFitModel.from_pretrained(&#xA;    &#34;sentence-transformers/paraphrase-mpnet-base-v2&#34;,&#xA;    use_differentiable_head=True,&#xA;    head_params={&#34;out_features&#34;: num_classes},&#xA;)&#xA;&#xA;# Create trainer&#xA;trainer = SetFitTrainer(&#xA;    model=model,&#xA;    train_dataset=train_dataset,&#xA;    eval_dataset=eval_dataset,&#xA;    loss_class=CosineSimilarityLoss,&#xA;    metric=&#34;accuracy&#34;,&#xA;    batch_size=16,&#xA;    num_iterations=20, # The number of text pairs to generate for contrastive learning&#xA;    num_epochs=1, # The number of epochs to use for contrastive learning&#xA;    column_mapping={&#34;sentence&#34;: &#34;text&#34;, &#34;label&#34;: &#34;label&#34;} # Map dataset columns to text/label expected by trainer&#xA;)&#xA;&#xA;# Train and evaluate&#xA;trainer.freeze() # Freeze the head&#xA;trainer.train() # Train only the body&#xA;&#xA;# Unfreeze the head and freeze the body -&amp;gt; head-only training&#xA;trainer.unfreeze(keep_body_frozen=True)&#xA;# or&#xA;# Unfreeze the head and unfreeze the body -&amp;gt; end-to-end training&#xA;trainer.unfreeze(keep_body_frozen=False)&#xA;&#xA;trainer.train(&#xA;    num_epochs=25, # The number of epochs to train the head or the whole model (body and head)&#xA;    batch_size=16,&#xA;    body_learning_rate=1e-5, # The body&#39;s learning rate&#xA;    learning_rate=1e-2, # The head&#39;s learning rate&#xA;    l2_weight=0.0, # Weight decay on **both** the body and head. If `None`, will use 0.01.&#xA;)&#xA;metrics = trainer.evaluate()&#xA;&#xA;# Push model to the Hub&#xA;trainer.push_to_hub(&#34;my-awesome-setfit-model&#34;)&#xA;&#xA;# Download from Hub and run inference&#xA;model = SetFitModel.from_pretrained(&#34;lewtun/my-awesome-setfit-model&#34;)&#xA;# Run inference&#xA;preds = model([&#34;i loved the spiderman movie!&#34;, &#34;pineapple on pizza is the worst ü§Æ&#34;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Based on our experiments, &lt;code&gt;SetFitHead&lt;/code&gt; can achieve similar performance as using a &lt;code&gt;scikit-learn&lt;/code&gt; head. We use &lt;code&gt;AdamW&lt;/code&gt; as the optimizer and scale down learning rates by 0.5 every 5 epochs. For more details about the experiments, please check out &lt;a href=&#34;https://github.com/huggingface/setfit/pull/112#issuecomment-1295773537&#34;&gt;here&lt;/a&gt;. We recommend using a large learning rate (e.g. &lt;code&gt;1e-2&lt;/code&gt;) for &lt;code&gt;SetFitHead&lt;/code&gt; and a small learning rate (e.g. &lt;code&gt;1e-5&lt;/code&gt;) for the body in your first attempt.&lt;/p&gt; &#xA;&lt;h3&gt;Training on multilabel datasets&lt;/h3&gt; &#xA;&lt;p&gt;To train SetFit models on multilabel datasets, specify the &lt;code&gt;multi_target_strategy&lt;/code&gt; argument when loading the pretrained model:&lt;/p&gt; &#xA;&lt;h4&gt;Example using a classification head from &lt;code&gt;scikit-learn&lt;/code&gt;:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from setfit import SetFitModel&#xA;&#xA;model = SetFitModel.from_pretrained(&#xA;    model_id,&#xA;    multi_target_strategy=&#34;one-vs-rest&#34;,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will initialise a multilabel classification head from &lt;code&gt;sklearn&lt;/code&gt; - the following options are available for &lt;code&gt;multi_target_strategy&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;one-vs-rest&lt;/code&gt;: uses a &lt;code&gt;OneVsRestClassifier&lt;/code&gt; head.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;multi-output&lt;/code&gt;: uses a &lt;code&gt;MultiOutputClassifier&lt;/code&gt; head.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;classifier-chain&lt;/code&gt;: uses a &lt;code&gt;ClassifierChain&lt;/code&gt; head.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;From here, you can instantiate a &lt;code&gt;SetFitTrainer&lt;/code&gt; using the same example above, and train it as usual.&lt;/p&gt; &#xA;&lt;h4&gt;Example using the differentiable &lt;code&gt;SetFitHead&lt;/code&gt;:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from setfit import SetFitModel&#xA;&#xA;model = SetFitModel.from_pretrained(&#xA;    model_id,&#xA;    multi_target_strategy=&#34;one-vs-rest&#34;&#xA;    use_differentiable_head=True,&#xA;    head_params={&#34;out_features&#34;: num_classes},&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you use the differentiable &lt;code&gt;SetFitHead&lt;/code&gt; classifier head, it will automatically use &lt;code&gt;BCEWithLogitsLoss&lt;/code&gt; for training. The prediction involves a &lt;code&gt;sigmoid&lt;/code&gt; after which probabilities are rounded to 1 or 0. Furthermore, the &lt;code&gt;&#34;one-vs-rest&#34;&lt;/code&gt; and &lt;code&gt;&#34;multi-output&#34;&lt;/code&gt; multi-target strategies are equivalent for the differentiable &lt;code&gt;SetFitHead&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Zero-shot text classification&lt;/h3&gt; &#xA;&lt;p&gt;SetFit can also be applied to scenarios where no labels are available. To do so, create a synthetic dataset of training examples:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from datasets import Dataset&#xA;from setfit import get_templated_dataset&#xA;&#xA;candidate_labels = [&#34;negative&#34;, &#34;positive&#34;]&#xA;train_dataset = get_templated_dataset(candidate_labels=candidate_labels, sample_size=8)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will create examples of the form &lt;code&gt;&#34;This sentence is {}&#34;&lt;/code&gt;, where the &lt;code&gt;{}&lt;/code&gt; is filled in with one of the candidate labels. From here you can train a SetFit model as usual:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from setfit import SetFitModel, SetFitTrainer&#xA;&#xA;model = SetFitModel.from_pretrained(&#34;sentence-transformers/paraphrase-mpnet-base-v2&#34;)&#xA;trainer = SetFitTrainer(&#xA;    model=model,&#xA;    train_dataset=train_dataset&#xA;)&#xA;trainer.train()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We find this approach typically outperforms the &lt;a href=&#34;https://huggingface.co/docs/transformers/v4.24.0/en/main_classes/pipelines#transformers.ZeroShotClassificationPipeline&#34;&gt;zero-shot pipeline&lt;/a&gt; in ü§ó Transformers (based on MNLI with Bart), while being 5x faster to generate predictions with.&lt;/p&gt; &#xA;&lt;h3&gt;Running hyperparameter search&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;SetFitTrainer&lt;/code&gt; provides a &lt;code&gt;hyperparameter_search()&lt;/code&gt; method that you can use to find good hyperparameters for your data. To use this feature, first install the &lt;code&gt;optuna&lt;/code&gt; backend:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m pip install setfit[optuna]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use this method, you need to define two functions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;model_init()&lt;/code&gt;: A function that instantiates the model to be used. If provided, each call to &lt;code&gt;train()&lt;/code&gt; will start from a new instance of the model as given by this function.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;hp_space()&lt;/code&gt;: A function that defines the hyperparameter search space.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Here is an example of a &lt;code&gt;model_init()&lt;/code&gt; function that we&#39;ll use to scan over the hyperparameters associated with the classification head in &lt;code&gt;SetFitModel&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from setfit import SetFitModel&#xA;&#xA;def model_init(params):&#xA;    params = params or {}&#xA;    max_iter = params.get(&#34;max_iter&#34;, 100)&#xA;    solver = params.get(&#34;solver&#34;, &#34;liblinear&#34;)&#xA;    params = {&#xA;        &#34;head_params&#34;: {&#xA;            &#34;max_iter&#34;: max_iter,&#xA;            &#34;solver&#34;: solver,&#xA;        }&#xA;    }&#xA;    return SetFitModel.from_pretrained(&#34;sentence-transformers/paraphrase-albert-small-v2&#34;, **params)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Similarly, to scan over hyperparameters associated with the SetFit training process, we can define a &lt;code&gt;hp_space()&lt;/code&gt; function as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def hp_space(trial):  # Training parameters&#xA;    return {&#xA;        &#34;learning_rate&#34;: trial.suggest_float(&#34;learning_rate&#34;, 1e-6, 1e-4, log=True),&#xA;        &#34;num_epochs&#34;: trial.suggest_int(&#34;num_epochs&#34;, 1, 5),&#xA;        &#34;batch_size&#34;: trial.suggest_categorical(&#34;batch_size&#34;, [4, 8, 16, 32, 64]),&#xA;        &#34;seed&#34;: trial.suggest_int(&#34;seed&#34;, 1, 40),&#xA;        &#34;num_iterations&#34;: trial.suggest_categorical(&#34;num_iterations&#34;, [5, 10, 20]),&#xA;        &#34;max_iter&#34;: trial.suggest_int(&#34;max_iter&#34;, 50, 300),&#xA;        &#34;solver&#34;: trial.suggest_categorical(&#34;solver&#34;, [&#34;newton-cg&#34;, &#34;lbfgs&#34;, &#34;liblinear&#34;]),&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; In practice, we found &lt;code&gt;num_iterations&lt;/code&gt; to be the most important hyperparameter for the contrastive learning process.&lt;/p&gt; &#xA;&lt;p&gt;The next step is to instantiate a &lt;code&gt;SetFitTrainer&lt;/code&gt; and call &lt;code&gt;hyperparameter_search()&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from datasets import Dataset&#xA;from setfit import SetFitTrainer&#xA;&#xA;dataset = Dataset.from_dict(&#xA;            {&#34;text_new&#34;: [&#34;a&#34;, &#34;b&#34;, &#34;c&#34;], &#34;label_new&#34;: [0, 1, 2], &#34;extra_column&#34;: [&#34;d&#34;, &#34;e&#34;, &#34;f&#34;]}&#xA;        )&#xA;&#xA;trainer = SetFitTrainer(&#xA;    train_dataset=dataset,&#xA;    eval_dataset=dataset,&#xA;    model_init=model_init,&#xA;    column_mapping={&#34;text_new&#34;: &#34;text&#34;, &#34;label_new&#34;: &#34;label&#34;},&#xA;)&#xA;best_run = trainer.hyperparameter_search(direction=&#34;maximize&#34;, hp_space=hp_space, n_trials=20)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, you can apply the hyperparameters you found to the trainer, and lock in the optimal model, before training for a final time.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;trainer.apply_hyperparameters(best_run.hyperparameters, final_model=True)&#xA;trainer.train()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Compressing a SetFit model with knowledge distillation&lt;/h2&gt; &#xA;&lt;p&gt;If you have access to unlabeled data, you can use knowledge distillation to compress a trained SetFit model into a smaller version. The result is a model that can run inference much faster, with little to no drop in accuracy. Here&#39;s an end-to-end example (see our paper for more details):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from datasets import load_dataset&#xA;from sentence_transformers.losses import CosineSimilarityLoss&#xA;&#xA;from setfit import SetFitModel, SetFitTrainer, DistillationSetFitTrainer, sample_dataset&#xA;&#xA;# Load a dataset from the Hugging Face Hub&#xA;dataset = load_dataset(&#34;ag_news&#34;)&#xA;&#xA;# Create a sample few-shot dataset to train the teacher model&#xA;train_dataset_teacher = sample_dataset(dataset[&#34;train&#34;], label_column=&#34;label&#34;, num_samples=16)&#xA;# Create a dataset of unlabeled examples to train the student&#xA;train_dataset_student = dataset[&#34;train&#34;].shuffle(seed=0).select(range(500))&#xA;# Dataset for evaluation&#xA;eval_dataset = dataset[&#34;test&#34;]&#xA;&#xA;# Load teacher model&#xA;teacher_model = SetFitModel.from_pretrained(&#xA;    &#34;sentence-transformers/paraphrase-mpnet-base-v2&#34;&#xA;)&#xA;&#xA;# Create trainer for teacher model&#xA;teacher_trainer = SetFitTrainer(&#xA;    model=teacher_model,&#xA;    train_dataset=train_dataset_teacher,&#xA;    eval_dataset=eval_dataset,&#xA;    loss_class=CosineSimilarityLoss,&#xA;)&#xA;&#xA;# Train teacher model&#xA;teacher_trainer.train()&#xA;&#xA;# Load small student model&#xA;student_model = SetFitModel.from_pretrained(&#34;paraphrase-MiniLM-L3-v2&#34;)&#xA;&#xA;# Create trainer for knowledge distillation&#xA;student_trainer = DistillationSetFitTrainer(&#xA;    teacher_model=teacher_model,&#xA;    train_dataset=train_dataset_student,&#xA;    student_model=student_model,&#xA;    eval_dataset=eval_dataset,&#xA;    loss_class=CosineSimilarityLoss,&#xA;    metric=&#34;accuracy&#34;,&#xA;    batch_size=16,&#xA;    num_iterations=20,&#xA;    num_epochs=1,&#xA;)&#xA;&#xA;# Train student with knowledge distillation&#xA;student_trainer.train()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Reproducing the results from the paper&lt;/h2&gt; &#xA;&lt;p&gt;We provide scripts to reproduce the results for SetFit and various baselines presented in Table 2 of our paper. Check out the setup and training instructions in the &lt;code&gt;scripts/&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;h2&gt;Developer installation&lt;/h2&gt; &#xA;&lt;p&gt;To run the code in this project, first create a Python virtual environment using e.g. Conda:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n setfit python=3.9 &amp;amp;&amp;amp; conda activate setfit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then install the base requirements with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m pip install -e &#39;.[dev]&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will install &lt;code&gt;datasets&lt;/code&gt; and packages like &lt;code&gt;black&lt;/code&gt; and &lt;code&gt;isort&lt;/code&gt; that we use to ensure consistent code formatting.&lt;/p&gt; &#xA;&lt;h3&gt;Formatting your code&lt;/h3&gt; &#xA;&lt;p&gt;We use &lt;code&gt;black&lt;/code&gt; and &lt;code&gt;isort&lt;/code&gt; to ensure consistent code formatting. After following the installation steps, you can check your code locally by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make style &amp;amp;&amp;amp; make quality&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Project structure&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;‚îú‚îÄ‚îÄ LICENSE&#xA;‚îú‚îÄ‚îÄ Makefile        &amp;lt;- Makefile with commands like `make style` or `make tests`&#xA;‚îú‚îÄ‚îÄ README.md       &amp;lt;- The top-level README for developers using this project.&#xA;‚îú‚îÄ‚îÄ notebooks       &amp;lt;- Jupyter notebooks.&#xA;‚îú‚îÄ‚îÄ final_results   &amp;lt;- Model predictions from the paper&#xA;‚îú‚îÄ‚îÄ scripts         &amp;lt;- Scripts for training and inference&#xA;‚îú‚îÄ‚îÄ setup.cfg       &amp;lt;- Configuration file to define package metadata&#xA;‚îú‚îÄ‚îÄ setup.py        &amp;lt;- Make this project pip installable with `pip install -e`&#xA;‚îú‚îÄ‚îÄ src             &amp;lt;- Source code for SetFit&#xA;‚îî‚îÄ‚îÄ tests           &amp;lt;- Unit tests&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Related work&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jxpress/setfit-pytorch-lightning&#34;&gt;jxpress/setfit-pytorch-lightning&lt;/a&gt; - A PyTorch Lightning implementation of SetFit.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-@misc{https://doi.org/10.48550/arxiv.2209.11055,&#34;&gt;  doi = {10.48550/ARXIV.2209.11055},&#xA;  url = {https://arxiv.org/abs/2209.11055},&#xA;  author = {Tunstall, Lewis and Reimers, Nils and Jo, Unso Eun Seo and Bates, Luke and Korat, Daniel and Wasserblat, Moshe and Pereg, Oren},&#xA;  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},&#xA;  title = {Efficient Few-Shot Learning Without Prompts},&#xA;  publisher = {arXiv},&#xA;  year = {2022},&#xA;  copyright = {Creative Commons Attribution 4.0 International}}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>