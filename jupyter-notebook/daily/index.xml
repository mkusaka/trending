<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-12-25T01:32:01Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>microsoft/OmniParser</title>
    <updated>2024-12-25T01:32:01Z</updated>
    <id>tag:github.com,2024-12-25:/microsoft/OmniParser</id>
    <link href="https://github.com/microsoft/OmniParser" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A simple screen parsing tool towards pure vision based GUI agent&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OmniParser: Screen Parsing tool for Pure Vision Based GUI Agent&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/OmniParser/master/imgs/logo.png&#34; alt=&#34;Logo&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2408.00203&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Paper-green&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;ðŸ“¢ [&lt;a href=&#34;https://microsoft.github.io/OmniParser/&#34;&gt;Project Page&lt;/a&gt;] [&lt;a href=&#34;https://www.microsoft.com/en-us/research/articles/omniparser-for-pure-vision-based-gui-agent/&#34;&gt;Blog Post&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/microsoft/OmniParser&#34;&gt;Models&lt;/a&gt;] &lt;a href=&#34;https://huggingface.co/spaces/microsoft/OmniParser&#34;&gt;huggingface space&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;OmniParser&lt;/strong&gt; is a comprehensive method for parsing user interface screenshots into structured and easy-to-understand elements, which significantly enhances the ability of GPT-4V to generate actions that can be accurately grounded in the corresponding regions of the interface.&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2024/11/26] We release an updated version, OmniParser V1.5 which features 1) more fine grained/small icon detection, 2) prediction of whether each screen element is interactable or not. Examples in the demo.ipynb.&lt;/li&gt; &#xA; &lt;li&gt;[2024/10] OmniParser was the #1 trending model on huggingface model hub (starting 10/29/2024).&lt;/li&gt; &#xA; &lt;li&gt;[2024/10] Feel free to checkout our demo on &lt;a href=&#34;https://huggingface.co/spaces/microsoft/OmniParser&#34;&gt;huggingface space&lt;/a&gt;! (stay tuned for OmniParser + Claude Computer Use)&lt;/li&gt; &#xA; &lt;li&gt;[2024/10] Both Interactive Region Detection Model and Icon functional description model are released! &lt;a href=&#34;https://huggingface.co/microsoft/OmniParser&#34;&gt;Hugginface models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[2024/09] OmniParser achieves the best performance on &lt;a href=&#34;https://microsoft.github.io/WindowsAgentArena/&#34;&gt;Windows Agent Arena&lt;/a&gt;!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;Install environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;conda create -n &#34;omni&#34; python==3.12&#xA;conda activate omni&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then download the model ckpts files in: &lt;a href=&#34;https://huggingface.co/microsoft/OmniParser&#34;&gt;https://huggingface.co/microsoft/OmniParser&lt;/a&gt;, and put them under weights/, default folder structure is: weights/icon_detect, weights/icon_caption_florence, weights/icon_caption_blip2.&lt;/p&gt; &#xA;&lt;p&gt;For v1: convert the safetensor to .pt file.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python weights/convert_safetensor_to_pt.py&#xA;&#xA;For v1.5: &#xA;download &#39;model_v1_5.pt&#39; from https://huggingface.co/microsoft/OmniParser/tree/main/icon_detect_v1_5, make a new dir: weights/icon_detect_v1_5, and put it inside the folder. No weight conversion is needed. &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Examples:&lt;/h2&gt; &#xA;&lt;p&gt;We put together a few simple examples in the demo.ipynb.&lt;/p&gt; &#xA;&lt;h2&gt;Gradio Demo&lt;/h2&gt; &#xA;&lt;p&gt;To run gradio demo, simply run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# For v1&#xA;python gradio_demo.py --icon_detect_model weights/icon_detect/best.pt --icon_caption_model florence2&#xA;# For v1.5&#xA;python gradio_demo.py --icon_detect_model weights/icon_detect_v1_5/model_v1_5.pt --icon_caption_model florence2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Model Weights License&lt;/h2&gt; &#xA;&lt;p&gt;For the model checkpoints on huggingface model hub, please note that icon_detect model is under AGPL license since it is a license inherited from the original yolo model. And icon_caption_blip2 &amp;amp; icon_caption_florence is under MIT license. Please refer to the LICENSE file in the folder of each model: &lt;a href=&#34;https://huggingface.co/microsoft/OmniParser&#34;&gt;https://huggingface.co/microsoft/OmniParser&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;ðŸ“š Citation&lt;/h2&gt; &#xA;&lt;p&gt;Our technical report can be found &lt;a href=&#34;https://arxiv.org/abs/2408.00203&#34;&gt;here&lt;/a&gt;. If you find our work useful, please consider citing our work:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{lu2024omniparserpurevisionbased,&#xA;      title={OmniParser for Pure Vision Based GUI Agent}, &#xA;      author={Yadong Lu and Jianwei Yang and Yelong Shen and Ahmed Awadallah},&#xA;      year={2024},&#xA;      eprint={2408.00203},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV},&#xA;      url={https://arxiv.org/abs/2408.00203}, &#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>