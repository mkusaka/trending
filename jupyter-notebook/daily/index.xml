<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-11T01:37:08Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Devtown-India/Python-and-Machine-Learning</title>
    <updated>2023-05-11T01:37:08Z</updated>
    <id>tag:github.com,2023-05-11:/Devtown-India/Python-and-Machine-Learning</id>
    <link href="https://github.com/Devtown-India/Python-and-Machine-Learning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;6th Feb 2021&lt;/p&gt;&lt;hr&gt;</summary>
  </entry>
  <entry>
    <title>s4afa451dgf415f/colab_stable_diffusion</title>
    <updated>2023-05-11T01:37:08Z</updated>
    <id>tag:github.com,2023-05-11:/s4afa451dgf415f/colab_stable_diffusion</id>
    <link href="https://github.com/s4afa451dgf415f/colab_stable_diffusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;strong&gt;colab_stable_diffusion_webui&lt;/strong&gt;&lt;/h1&gt; &#xA;&lt;p&gt;-&lt;a href=&#34;https://colab.research.google.com/github/s4afa451dgf415f/colab_stable_diffusion/blob/main/%E4%BA%91stable_diffusion(%E4%BF%AE%E5%A4%8D%E6%8C%96%E7%9F%BF%E5%AB%8C%E7%96%91).ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?message=Open%20in%20Colab&amp;amp;logo=googlecolab&amp;amp;labelColor=5c5c5c&amp;amp;color=0f80c1&amp;amp;label=%20&amp;amp;style=flat&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h3&gt;关于谷歌colab&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;请确认有可以魔法上网的工具，工具用于谷歌colab的机器学习，属于合法范畴。&lt;/li&gt; &#xA; &lt;li&gt;优势：不需要显卡，手机也能用，且云端运行速度快。&lt;/li&gt; &#xA; &lt;li&gt;报错或无法解决的问题请加群710895662&lt;/li&gt; &#xA; &lt;li&gt;一每个号一天的使用时间不等，多弄几个号在云盘里主号的json给其他号添加编辑权限就实现了所有号共用&lt;/li&gt; &#xA; &lt;li&gt;一天内第一次启动过程约7分钟左右下载插件模型依赖，请耐心等待。之后如果再启动就非常快。&lt;/li&gt; &#xA; &lt;li&gt;你也可以电脑魔法上网colab，手机直接打开代码运行后给出的域名就行。&lt;/li&gt; &#xA; &lt;li&gt;谷歌运行环境为12G 内存15G显存，大显存小内存所以推荐使用半精度进行计算&lt;/li&gt; &#xA; &lt;li&gt;教程可以进群下载或点击链接:&lt;a href=&#34;https://www.bilibili.com/video/BV17h4y1J79g/?spm_id_from=333.788.top_right_bar_window_history.content.click&#34;&gt;https://www.bilibili.com/video/BV17h4y1J79g/?spm_id_from=333.788.top_right_bar_window_history.content.click&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;关于sd&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;mod管理单元中lora和checkpoint只需填下载地址即可，你也可以添加一些信息方便管理。&lt;/li&gt; &#xA; &lt;li&gt;采样方法推荐 DDIM 与DPM++ 2M 高清放大推荐 潜变量(bicubic)与潜变量最邻近&lt;/li&gt; &#xA; &lt;li&gt;图片可以选择输出在云盘的outputs文件夹里&lt;/li&gt; &#xA; &lt;li&gt;生成图的速度与执行代码的速度与网速无关，你就算断网几分钟他也在执行&lt;/li&gt; &#xA; &lt;li&gt;如果要使用手机进行局部绘图请使用Edge浏览器，谷歌浏览器不兼容&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Update?&lt;/h2&gt; &#xA;&lt;h3&gt;v.2.0.4 (23/05/09)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;增加了不挂载云盘运行的选项&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;v.2.0.3 (23/05/03)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;根据大版本更新修复了脚本&lt;/li&gt; &#xA; &lt;li&gt;默认UI改回&lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui&#34;&gt;Automatic111&lt;/a&gt;，anapnoe为可选择&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;v.2.0.2 (23/04/29)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;内存泄露问题已解决&lt;/li&gt; &#xA; &lt;li&gt;优化了mod管理单元的交互，支持切换文档了&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;v.2.0.1 (23/04/27)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;改用了更好更适配的ui系统，来源&lt;a href=&#34;https://github.com/anapnoe/stable-diffusion-webui-ux&#34;&gt;anapnoe&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;用指令一定程度解决了mod切换内存不足的问题&lt;/li&gt; &#xA; &lt;li&gt;解决了图生图某些特定尺寸无法生成的问题&lt;/li&gt; &#xA; &lt;li&gt;配置了ngrok加速&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;v.2.0.0 (23/04/27)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;增加了mod增删改查系统&lt;/li&gt; &#xA; &lt;li&gt;重构mod下载模块，不需要再用云盘保存mod&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;v.1.1.4 (23/04/24)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;更新了controlnet 1.1&lt;/li&gt; &#xA; &lt;li&gt;增加部分了自定义设置&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;v.1.1.3 (23/04/21)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;修复了出现怀疑挖矿提示框的问题&lt;/li&gt; &#xA; &lt;li&gt;部分单元格采用异步下载提高执行效率&lt;/li&gt; &#xA; &lt;li&gt;默认大模型换成了&lt;a href=&#34;https://civitai.com/api/download/models/33482&#34;&gt;Dark Sushi Mix&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;controlNet控制模型默认数量变为3&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;v.1.1.2 (23/04/17)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;修复了手机局部绘图图片width过大的问题&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;v.1.1.1 (23/04/12)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;更新了&lt;a href=&#34;https://www.bilibili.com/video/BV1tg4y137mt/?spm_id_from=333.880.my_history.page.click&amp;amp;vd_source=931a87555c05909a4816745522b3ce74&#34;&gt;CN_tag_trans&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;增加了从png_info图片秒读秒填充的功能&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>pablomarin/GPT-Azure-Search-Engine</title>
    <updated>2023-05-11T01:37:08Z</updated>
    <id>tag:github.com,2023-05-11:/pablomarin/GPT-Azure-Search-Engine</id>
    <link href="https://github.com/pablomarin/GPT-Azure-Search-Engine" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Azure Cognitive Search + Azure OpenAI Accelerator&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/113465005/226238596-cc76039e-67c2-46b6-b0bb-35d037ae66e1.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Accelerator powered by Azure Cognitive Search + Azure OpenAI&lt;/h1&gt; &#xA;&lt;p&gt;Your organization requires a chatbot and search engine capable of comprehending diverse types of data scattered across various locations. Additionally, the chatbot/search engine should be able to provide answers to inquiries, along with the source and an explanation of how and where the answer was obtained. In other words, you want &lt;strong&gt;private and secured ChatGPT for your organization that can interpret, comprehend, and answer questions about your business data&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The goal of the MVP workshop is to show/prove the value of a GPT Smart Search Engine built with the Azure Services, with your own data in your own environment. The repo is made to teach customers step-by-step on how to build a Smart Search Engine Chat Bot. Each Notebook builds on top of each other and ends in building a web application.&lt;/p&gt; &#xA;&lt;p&gt;For more information on the 3 day workshop, click the powerpoint presentation below:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/pablomarin/GPT-Azure-Search-Engine/raw/main/Intro%20Deck%20-%20AOAI%20GPT%20Search%20Engine%20Accelerator.pdf&#34;&gt;Accelerator Pitch Deck&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prerequisites Client 3-Day Workshop&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Azure subscription&lt;/li&gt; &#xA; &lt;li&gt;Accepted Application to Azure Open AI&lt;/li&gt; &#xA; &lt;li&gt;Microsoft members need to be added as Guests in clients Azure AD&lt;/li&gt; &#xA; &lt;li&gt;A Resource Group (RG) needs to be set for this Workshop POC, in the customer Azure tenant&lt;/li&gt; &#xA; &lt;li&gt;The customer team and the Microsoft team must have Contributor permissions to this resource group so they can set everything up 2 weeks prior to the workshop&lt;/li&gt; &#xA; &lt;li&gt;A storage account must be set in place in the RG. &lt;b&gt;Disable firewalls and enable public network access from all networks&lt;/b&gt;&lt;/li&gt; &#xA; &lt;li&gt;Data/Documents must be uploaded to the blob storage account, at least two weeks prior to the workshop date&lt;/li&gt; &#xA; &lt;li&gt;For IDE collaboration during workshop, Azure Machine Learning Workspace must be deployed in the RG &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Note: Please ensure you have enough core compute quota in your Azure Machine Learning workspace&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Architecture&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pablomarin/GPT-Azure-Search-Engine/main/images/GPT-Smart-Search-Architecture.jpg&#34; alt=&#34;Architecture&#34; title=&#34;Architecture&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Flow&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;The user asks a question.&lt;/li&gt; &#xA; &lt;li&gt;In the app, an OpenAI LLM uses a clever prompt to determine which source contains the answer to the question.&lt;/li&gt; &#xA; &lt;li&gt;Four types of sources are available: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;3a. Azure SQL Database - contains COVID-related statistics in the US.&lt;/li&gt; &#xA;   &lt;li&gt;3b. Azure Bing Search API - provides online web search for current information.&lt;/li&gt; &#xA;   &lt;li&gt;3c. Azure Cognitive Search - contains AI-enriched documents from Blob Storage (10k PDFs and 52k articles). &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;3c.1. Uses an LLM (OpenAI or Local Bert model) to vectorize the top K document chunks from 3c.&lt;/li&gt; &#xA;     &lt;li&gt;3c.2. Uses in-memory cosine similarity to get the top N chunks.&lt;/li&gt; &#xA;     &lt;li&gt;3c.3. Uses an OpenAI GPT model to craft the response from the Cog Search Engine (3c) by combining the question and the top N chunks.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;3d. CSV Tabular File - contains COVID-related statistics in the US.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;The app retrieves the result from the source and crafts the answer.&lt;/li&gt; &#xA; &lt;li&gt;The tuple (Question and Answer) is saved to CosmosDB to keep a record of the interaction.&lt;/li&gt; &#xA; &lt;li&gt;The answer is delivered to the user.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pablomarin-gpt-azure-search-engine-apphome-oq98mn.streamlit.app&#34;&gt;https://pablomarin-gpt-azure-search-engine-apphome-oq98mn.streamlit.app&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;🔧&lt;strong&gt;Features&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Shows how you can use &lt;a href=&#34;https://azure.microsoft.com/en-us/products/cognitive-services/openai-service/&#34;&gt;Azure OpenAI&lt;/a&gt; + &lt;a href=&#34;https://azure.microsoft.com/en-us/products/search&#34;&gt;Azure Cognitive Search&lt;/a&gt; to have a GPT powered Smart Search engine that not only provides links of the search results, but also answers the question by reading and understanding those search results.&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;strong&gt;Solve 80% of the use cases where companies want to use OpenAI to provide answers from their knowledge base to customers or employees, without the need of retraining/fine tuning and hosting the models.&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;All Azure services and configuration are deployed via python code.&lt;/li&gt; &#xA; &lt;li&gt;Uses &lt;a href=&#34;https://azure.microsoft.com/en-us/products/cognitive-services/&#34;&gt;Azure Cognitive Services&lt;/a&gt; to enrich documents: Detect Language, OCR images, Key-phrases extraction, entity recognition (persons, emails, addresses, organizations, urls).&lt;/li&gt; &#xA; &lt;li&gt;Uses &lt;a href=&#34;https://langchain.readthedocs.io/en/latest/&#34;&gt;LangChain&lt;/a&gt; as a wrapper for interacting with Azure OpenAI , vector stores, constructing prompts and creating agents.&lt;/li&gt; &#xA; &lt;li&gt;Uses &lt;a href=&#34;https://streamlit.io/&#34;&gt;Streamlit&lt;/a&gt; to build the web application in python.&lt;/li&gt; &#xA; &lt;li&gt;Multi-Lingual (ingests, indexes and understand any language)&lt;/li&gt; &#xA; &lt;li&gt;Multi-Index -&amp;gt; multiple search indexes&lt;/li&gt; &#xA; &lt;li&gt;Parses CSVs -&amp;gt; one-to-many documents (one row is an indexed document)&lt;/li&gt; &#xA; &lt;li&gt;Tabular Data Q&amp;amp;A in CSV files and SQL Databases using GPT-4&lt;/li&gt; &#xA; &lt;li&gt;Uses Bing Search API service for web search&lt;/li&gt; &#xA; &lt;li&gt;ChatBot Interface&lt;/li&gt; &#xA; &lt;li&gt;(Coming soon) Recommends new searches based on users&#39; history.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;&lt;strong&gt;Steps to Run the Accelerator&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Note: (Pre-requisite) You need to have an Azure OpenAI service already created&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Fork this repo to your Github account.&lt;/li&gt; &#xA; &lt;li&gt;In Azure OpenAI studio, deploy these two models: &lt;strong&gt;Make sure that the deployment name is the same as the model name.&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&#34;gpt-35-turbo&#34; for the model &#34;gpt-35-turbo (0301)&#34;. If you have &#34;gpt-4&#34;, use it (it is definitely better)&lt;/li&gt; &#xA;   &lt;li&gt;&#34;text-embedding-ada-002&#34;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Create a Resource Group where all the assets of this accelerator are going to be. Azure OpenAI can be in different RG or a different Subscription.&lt;/li&gt; &#xA; &lt;li&gt;ClICK BELOW to create an Azure Cognitive Search Service and Cognitive Services Account:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fpablomarin%2FGPT-Azure-Search-Engine%2Fmain%2Fazuredeploy.json&#34;&gt;&lt;img src=&#34;https://aka.ms/deploytoazurebutton&#34; alt=&#34;Deploy To Azure&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Note: If you have never created a cognitive multi-service account before, please create one manually in the azure portal to read and accept the Responsible AI terms. Once this is deployed, delete this and then use the above deployment button.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;Enable Semantic Search on your Azure Cognitive Search Service: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;On the left-nav pane, select Semantic Search (Preview).&lt;/li&gt; &#xA;   &lt;li&gt;Select either the Free plan or the Standard plan. You can switch between the free plan and the standard plan at any time.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Make sure you run the notebooks on a &lt;strong&gt;Python 3.10 conda enviroment&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Install the dependencies on your machine (make sure you do the below comand on the same conda environment that you are going to run the notebooks:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r ./requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;8&#34;&gt; &#xA; &lt;li&gt;Run 01-Load-Data-ACogSearch.ipynb:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Loads 9.8k PDFs into your Search Engine and create the first index with AI skills&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;9&#34;&gt; &#xA; &lt;li&gt;Run 02-LoadCSVOneToMany-ACogSearch.ipynb and:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ingests 52k documents into your Search Engine coming from 1 CSV file&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;10&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Run the remaining Notebooks in order&lt;/strong&gt;. They build up on top of each other.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;After you ran all the notebooks&lt;/strong&gt;, Go to the app/ folder and click the Deploy to Azure function to deploy the Web Application in Azure Web App Service. It takes about 15-20 minutes.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The deployment automatically comes with CI/CD, so any change that you commit/push to your github forked repo will automatically trigger a deployment in the web application.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;&lt;strong&gt;FAQs&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Why the vector similarity is done in memory using FAISS versus having a separate vector database like RedisSearch or Pinecone?&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;A: True, doing the embeddings of the documents pages everytime that there is a query is not efficient. The ideal scenario is to vectorize the docs chunks once (first time they are needed) and then retrieve them from a database the next time they are needed. For this a special vector database is necessary. The ideal scenario though, is Azure Search to savea and retreive the vectors as part of the search results, along with the document chunks. Azure Search will soon allow this in a few months, let&#39;s wait for it. As of right now the embedding process doesn&#39;t take that much time or money, so it is worth the wait versus using another database just for vectors.&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Why use the MAP_REDUCE type in LangChaing sometimes versus STUFF type everytime?&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;A: Because using STUFF type with all the content of the pages as context, in many ocoasions, uses too many tokens. So the best way to deal with large documents is to find the answer by going trough all of the search results and doing many calls to the LLM looking for summarized answer, then combine this summaries and put them all in the call as context. For more information of the difference between STUFF and MAP_REDUCE, see &lt;a href=&#34;https://github.com/hwchase17/langchain/tree/master/langchain/chains/question_answering&#34;&gt;HERE&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Why use Azure Cognitive Search engine to provide the context for the LLM and not fine tune the LLM instead?&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;A: Quoting the &lt;a href=&#34;https://platform.openai.com/docs/guides/fine-tuning&#34;&gt;OpenAI documentation&lt;/a&gt;: &#34;GPT-3 has been pre-trained on a vast amount of text from the open internet. When given a prompt with just a few examples, it can often intuit what task you are trying to perform and generate a plausible completion. This is often called &#34;few-shot learning. Fine-tuning improves on few-shot learning by training on many more examples than can fit in the prompt, letting you achieve better results on a wide number of tasks. Once a model has been fine-tuned, you won&#39;t need to provide examples in the prompt anymore. This &lt;strong&gt;saves costs and enables lower-latency requests&lt;/strong&gt;&#34;&lt;/p&gt; &#xA;&lt;p&gt;However, fine-tuning the model requires providing hundreds or thousands of Prompt and Completion tuples, which are essentially query-response samples. The purpose of fine-tuning is not to give the LLM knowledge of the company&#39;s data but to provide it with examples so it can perform tasks really well without requiring examples on every prompt.&lt;/p&gt; &#xA;&lt;p&gt;There are cases where fine-tuning is necessary, such as when the examples contain proprietary data that should not be exposed in prompts or when the language used is highly specialized, as in healthcare, pharmacy, or other industries or use cases where the language used is not commonly found on the internet.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;&lt;strong&gt;Known Issues&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Error when sending question: &#34;This model&#39;s maximum context length is 2047 tokens, however you requested xxxx tokens (xxxxx in your prompt; 0 for the completion). Please reduce your prompt; or completion length&#34;&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;This error happens if your embedding model &lt;em&gt;text-embedding-ada-002&lt;/em&gt; has a limit of 2047 max tokens. Older versions of this model in Azure OpenAI has this reduced limit. However the newer versions have the 8192 limit. Make sure you request the newer version, or if not possible, reduce the size of the TextSplit in Azure Search indexing from 5000 (default) to 3500.&lt;/p&gt;</summary>
  </entry>
</feed>