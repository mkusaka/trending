<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-01T01:29:04Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Amirrezahmi/SelfTalker</title>
    <updated>2023-08-01T01:29:04Z</updated>
    <id>tag:github.com,2023-08-01:/Amirrezahmi/SelfTalker</id>
    <link href="https://github.com/Amirrezahmi/SelfTalker" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Engage in conversation with your virtual self using AI techniques like NLP, voice cloning, and computer vision. Get accurate answers with generated keywords, phrases, images, videos. Summarization included. Synthesize voice, create talking-head video. Seek advice, career guidance, satisfy curiosity. Endless possibilities.&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34; style=&#34;color: red;&#34;&gt;${\color{red}\text{Self Talker AI}}$&lt;/h1&gt; &#xA;&lt;h2&gt;${\color{cyan}\text{Project Overview}}$&lt;/h2&gt; &#xA;&lt;p&gt;Imagine being able to converse with a version of yourself that can provide answers to any question you may have. This project aimed to achieve just that!&lt;/p&gt; &#xA;&lt;p&gt;You can now engage in a conversation with yourself and seek answers to questions that may have previously eluded you.&lt;/p&gt; &#xA;&lt;p&gt;Using cutting-edge machine learning techniques such as NLP, voice cloning, computer vision, etc., we have created a virtual version of you that can seamlessy communicate with you. This AI-powered version of yourself is capable of understanding the nuances of human language and can provide insightful and accurate responses to your queries.&lt;/p&gt; &#xA;&lt;p&gt;Upon receiving the input prompt, the program initiates the answer generation process. Subsequently, it proceeds to extract relevant keywords and phrases from the generated response. Also the program summarize the generated answer to improve the quality of the final text. After obtaining the final answer, the Styleformer feature is applied, allowing you to switch between formal and casual styles seamlessly.&lt;/p&gt; &#xA;&lt;p&gt;To enhance the understanding of the content, the program creates two images: the first image is generated using the extracted keywords, while the second image utilizes the extracted phrases from the generated text. Additionally, leveraging the aforementioned keywords and phrases, the program generates two videos. Next, employing voice cloning techniques and utilizing a dataset of your recorded voice in &lt;code&gt;.wav&lt;/code&gt; format, the program synthesizes the text into speech, employing your unique vocal characteristics. Furthermore, by utilizing your $256 \times 256$ input image in &lt;code&gt;.jpg&lt;/code&gt; format, the program automatically generates a talking-head video, employing your input image as a base, and synchronized with the cloned voice.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/Amirrezahmi/SelfTalker/assets/89692207/282e2d9c-de0f-44eb-bb7f-326489d77a8b&#34; width=&#34;700&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;So, whether you&#39;re seeking advice on a personal matter, looking for guidance in your career, or simply curious about the world around you, this AI-powered version of yourself is always at your disposal.&lt;/p&gt; &#xA;&lt;p&gt;With this project, the possibilities are endless. Your AI-powered virtual self is ready for conversation at any time.&lt;/p&gt; &#xA;&lt;h1&gt;${\color{cyan}\text{Features}}$&lt;/h1&gt; &#xA;&lt;p&gt;Welcome to the Features section!&lt;/p&gt; &#xA;&lt;p&gt;We&#39;ve numbered the features below to highlight their specific order and importance. This helps you understand the logical progression of our product&#39;s capabilities, making it easier to grasp the underlying flow and prioritize accordingly. Enjoy exploring the exciting features we have in store for you!&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Text generation powered by fine-tuned GPT-2&lt;/li&gt; &#xA; &lt;li&gt;Keyword Extraction with KeyBERT&lt;/li&gt; &#xA; &lt;li&gt;Text summarization&lt;/li&gt; &#xA; &lt;li&gt;Styleformer Integration: Seamless Style Switching in Conversations&lt;/li&gt; &#xA; &lt;li&gt;AI Text-to-Image with minimal DALL-E Mini&lt;/li&gt; &#xA; &lt;li&gt;Text-to-video with Diffusers&lt;/li&gt; &#xA; &lt;li&gt;Voice Cloning&lt;/li&gt; &#xA; &lt;li&gt;MakeItTalk: Speaker-Aware Talking-Head Animation&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;${\color{cyan}\text{Prerequisites}}$&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.x&lt;/li&gt; &#xA; &lt;li&gt;Google Colab or Jupyter&lt;/li&gt; &#xA; &lt;li&gt;As this is a Jupyter Notebook file, It&#39;s not OS-specific and can be run on any operating system that supports Python, including Windows, macOS, and Linux.&lt;/li&gt; &#xA; &lt;li&gt;If you want to convert the Jupyter notebook (&lt;code&gt;.ipynb&lt;/code&gt;) into a standalone web application with a user interface (UI), you can use &lt;code&gt;Anvil&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;${\color{cyan}\text{Getting Started}}$&lt;/h1&gt; &#xA;&lt;h2&gt;${\color{pink}\text{Clone the repository}}$&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the repository:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/Amirrezahmi/SelfTalker.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;${\color{pink}\text{Usage}}$&lt;/h2&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;However I&#39;ve told everything in &lt;code&gt;Self_Talker.ipynb&lt;/code&gt;, but let me summarize it here one more time! First of all download my fine-tuned GPT-2 model from &lt;a href=&#34;https://drive.google.com/drive/folders/11W-KNm_lFtqAu0Kw7ANw-PcefkpD0aCP&#34;&gt; here&lt;/a&gt; or if you have your own dataset and you want to fine-tune your own GPT-2 model, I&#39;ve provided my both jupyter notebok and dataset in &lt;code&gt;fine-tune GPT2&lt;/code&gt; directory. This folder contains my code which used to fine-tune GPT-2 on my own dataset.&lt;/li&gt; &#xA; &lt;li&gt;The sixth step of &lt;code&gt;Self_Talker.ipynb&lt;/code&gt; is related to voice cloning, so a dataset of your voice is required. Please note that the files should be formated in &lt;code&gt;.wav&lt;/code&gt;. Ten files of 10-15 seconds are enough for your dataset.&lt;/li&gt; &#xA; &lt;li&gt;The last step of &lt;code&gt;Self_Talker.ipynb&lt;/code&gt; asks you for a $256 \times 256$ picture of yourself which is formated in &lt;code&gt;.jpg&lt;/code&gt; file.&lt;/li&gt; &#xA; &lt;li&gt;After going through the previous steps, it&#39;s time to run cell&#39;s from the &lt;code&gt;Self_Talker.ipynb&lt;/code&gt; jupyter note book. All you need to run this project is a colab or jupyter account. By running &#34;requirements&#34; section from this notebook, you will simply install all requirements. Please note that you should change your run time type to GPU and import the fine-tuned GPT-2 model or even the voice dataset of yours if you&#39;re using google colab.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;${\color{cyan}\text{Example}}$&lt;/h1&gt; &#xA;&lt;p&gt;Here&#39;s a sample example:&lt;/p&gt; &#xA;&lt;p&gt;${\color{red}\text{Prompt:}}$&lt;/p&gt; &#xA;&lt;p&gt;${\color{orange}\text{\quad \quad \quad How to become a rich guy?}}$&lt;/p&gt; &#xA;&lt;p&gt;${\color{red}\text{Generated Response:}}$&lt;/p&gt; &#xA;&lt;p&gt;If you want to be rich, you need to have a lot of money. If you don&#39;t have enough money, it will be difficult for you to achieve your goals, because you will not be able to buy the things that you really want. The only way to make money is by investing. Investing in stocks, bonds, real estate, and other assets is one of the best ways to get rich. There are many methods of investing, but the most important is to invest your money in the stock market. For example, if you invest $ 10,000 in a mutual fund, the fund will give you a return of 10% per year for the next 10 years. This is called a compound annual growth rate (CAGR). The more money you put into a fund the greater the return.&lt;/p&gt; &#xA;&lt;p&gt;${\color{red}\text{The top 3 keywords:}}$&lt;/p&gt; &#xA;&lt;p&gt;${\color{orange}\text{\quad \quad \quad &#39;rich&#39;, &#39;investing&#39; and &#39;invest&#39;}}$&lt;/p&gt; &#xA;&lt;p&gt;${\color{red}\text{The top 3 phrases:}}$&lt;/p&gt; &#xA;&lt;p&gt;${\color{orange}\text{\quad \quad \quad &#39;want rich&#39;, &#39;ways rich&#39; and &#39;invest money&#39;}}$&lt;/p&gt; &#xA;&lt;p&gt;${\color{red}\text{Text summarization:}}$&lt;/p&gt; &#xA;&lt;p&gt;If you want to be rich, you need to have a lot of money. The only way to make money is by investing. Investing in stocks, bonds, real estate, and other assets is one of the best ways to get rich. For example, if you invest $10,000 in a mutual fund, the fund will give you a return of 10% per year for the next 10 years.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;&lt;em&gt;NOTE:&lt;/em&gt;&lt;/strong&gt; The final text comprises the generated response combined with our summarized text.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;${\color{red}\text{Switching between formal and casual language styles:}}$&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;&lt;em&gt;NOTE:&lt;/em&gt;&lt;/strong&gt; Using Styleformer we convert our generated response and summarized text from formal to casual language style. Please note that the genereated text from our fine-tuned GPT-2 model is primarily formal in nature. Visit &lt;code&gt;Self_Talker.ipynb&lt;/code&gt; for more details.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;em&gt;Changing generated response language style to casual:&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;if you want to be rich, you have to have money. You will have a hard time achieving your goals if you&#39;re not rich enough to buy what you really want. Investing is the only way to make money. investing in stocks, bonds, real estate and other assets is one of the best ways to get rich. there are many ways to invest, but the most important is to invest your money in the stock market. like if you invest $ 10,000 in a mutual fund, that mutual will return you 10% each year for the next 10 years. it&#39;s called a compound annual growth rate. the more money you put into a fund the better the return.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;em&gt;Changing summarized text language style to casual:&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;if you want to be rich, you have to have money. You can only make money by investing. Investing stocks, bonds, real estate is one of the best ways to get rich. like if you invest $10,000 in a mutual fund, the fund will give you a return of 10% per year for the next 10 years.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;&lt;em&gt;NOTE:&lt;/em&gt;&lt;/strong&gt; In this example, I continued with the formal text but in your case it&#39;s all up to you.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;${\color{red}\text{Generating our first Text-to-Image. The prompt is our keywords variable:}}$&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/Amirrezahmi/SelfTalker/assets/89692207/39e0594a-2338-4e33-ad68-e09df7ee2e21&#34; alt=&#34;20230617_123525_0000&#34; width=&#34;300&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;${\color{red}\text{Generating our second Text-to-Image. The prompt is our phrases variable:}}$&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/Amirrezahmi/SelfTalker/assets/89692207/d7e202ca-7be0-42cb-9b0d-64f0986f4758&#34; alt=&#34;20230617_123525_0000&#34; width=&#34;300&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;${\color{red}\text{Generating our first video. The prompt is our keywords variable:}}$&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Amirrezahmi/SelfTalker/assets/89692207/f2e3e59f-9dd3-4bf9-815a-801b9a89f1c4&#34;&gt;https://github.com/Amirrezahmi/SelfTalker/assets/89692207/f2e3e59f-9dd3-4bf9-815a-801b9a89f1c4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;${\color{red}\text{Generating our second video. The prompt is our phrases variable:}}$&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Amirrezahmi/SelfTalker/assets/89692207/7b76ce01-531b-4db8-bdf2-11ec1fe47ca2&#34;&gt;https://github.com/Amirrezahmi/SelfTalker/assets/89692207/7b76ce01-531b-4db8-bdf2-11ec1fe47ca2&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;${\color{red}\text{Voice Cloning:}}$&lt;/p&gt; &#xA;&lt;p&gt;In this remarkable process, we have successfully cloned Elon Musk&#39;s voice using a dataset of his own recordings. The result of our text-to-speech synthesis can be found in the following &lt;code&gt;.mov&lt;/code&gt; file:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Amirrezahmi/SelfTalker/assets/89692207/ddb84548-5014-428e-b8f8-b46375bac940&#34;&gt;https://github.com/Amirrezahmi/SelfTalker/assets/89692207/ddb84548-5014-428e-b8f8-b46375bac940&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Our text here is the abridged text of the generated response.&lt;/p&gt; &#xA;&lt;p&gt;${\color{red}256 \times 256}$ ${\color{red} \text{ Input Image:}}$&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/Amirrezahmi/SelfTalker/assets/89692207/51224db3-4aa6-458b-8644-aeb978d6b719&#34; alt=&#34;20230617_123525_0000&#34; width=&#34;200&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;${\color{red} \text{Speaker-Aware Talking-Head Animation:}}$&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Amirrezahmi/SelfTalker/assets/89692207/3a076a6d-6fe4-4e30-b9b1-f2cb9a9d11f7&#34;&gt;https://github.com/Amirrezahmi/SelfTalker/assets/89692207/3a076a6d-6fe4-4e30-b9b1-f2cb9a9d11f7&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;${\color{red} \text{Final video:}}$&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Amirrezahmi/SelfTalker/assets/89692207/7506c5ca-f3d4-4c0a-a31d-73b0dad0a3e6&#34;&gt;https://github.com/Amirrezahmi/SelfTalker/assets/89692207/7506c5ca-f3d4-4c0a-a31d-73b0dad0a3e6&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;${\color{cyan}\text{Contributing}}$&lt;/h1&gt; &#xA;&lt;p&gt;Contributions are welcome! If you&#39;d like to contribute to this project, please follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Fork the repository.&lt;/li&gt; &#xA; &lt;li&gt;Create a new branch: git checkout -b my-new-branch.&lt;/li&gt; &#xA; &lt;li&gt;Make your changes and commit them: git commit -m &#39;Add some feature&#39;.&lt;/li&gt; &#xA; &lt;li&gt;Push to the branch: git push origin my-new-branch.&lt;/li&gt; &#xA; &lt;li&gt;Submit a pull request.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;${\color{cyan}\text{License}}$&lt;/h1&gt; &#xA;&lt;p&gt;This project is licensed under the &lt;a href=&#34;https://opensource.org/license/mit/&#34;&gt;MIT License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;${\color{cyan}\text{Credits}}$&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/gpt-2&#34;&gt;GPT-2&lt;/a&gt;- For Text generation step.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/MaartenGr/KeyBERT&#34;&gt;KeyBERT&lt;/a&gt;- For Keyword Extraction step.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PrithivirajDamodaran/Styleformer&#34;&gt;Styleformer&lt;/a&gt;- For Style Switching in Conversations.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/borisdayma/dalle-mini&#34;&gt;DALLÂ·E Mini&lt;/a&gt;- For AI Text-to-Image step.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/damo-vilab/text-to-video-ms-1.7b&#34;&gt;Diffusers&lt;/a&gt;- For Text-to-Video step&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jnordberg/tortoise-tts&#34;&gt;TorToiSe&lt;/a&gt;- For voice cloning step.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yzhou359/MakeItTalk&#34;&gt;MakeItTalk&lt;/a&gt;- For Speaker-Aware Talking-Head Animation step.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;${\color{cyan}\text{Contact}}$&lt;/h1&gt; &#xA;&lt;p&gt;For any questions or inquiries, please contact &lt;a href=&#34;mailto:amirrezahmi2002@gmail.com&#34;&gt;amirrezahmi2002@gmail.com&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>neonbjb/tortoise-tts</title>
    <updated>2023-08-01T01:29:04Z</updated>
    <id>tag:github.com,2023-08-01:/neonbjb/tortoise-tts</id>
    <link href="https://github.com/neonbjb/tortoise-tts" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A multi-voice TTS system trained with an emphasis on quality&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TorToiSe&lt;/h1&gt; &#xA;&lt;p&gt;Tortoise is a text-to-speech program built with the following priorities:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Strong multi-voice capabilities.&lt;/li&gt; &#xA; &lt;li&gt;Highly realistic prosody and intonation.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;This repo contains all the code needed to run Tortoise TTS in inference mode.&lt;/p&gt; &#xA;&lt;p&gt;Manuscript: &lt;a href=&#34;https://arxiv.org/abs/2305.07243&#34;&gt;https://arxiv.org/abs/2305.07243&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Version history&lt;/h3&gt; &#xA;&lt;h4&gt;v2.6; 2023/7/26&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bug fixes&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v2.5; 2023/7/09&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Added kv_cache support 5x faster&lt;/li&gt; &#xA; &lt;li&gt;Added deepspeed support 10x faster&lt;/li&gt; &#xA; &lt;li&gt;Added half precision support&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v2.4; 2022/5/17&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Removed CVVP model. Found that it does not, in fact, make an appreciable difference in the output.&lt;/li&gt; &#xA; &lt;li&gt;Add better debugging support; existing tools now spit out debug files which can be used to reproduce bad runs.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v2.3; 2022/5/12&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;New CLVP-large model for further improved decoding guidance.&lt;/li&gt; &#xA; &lt;li&gt;Improvements to read.py and do_tts.py (new options)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v2.2; 2022/5/5&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Added several new voices from the training set.&lt;/li&gt; &#xA; &lt;li&gt;Automated redaction. Wrap the text you want to use to prompt the model but not be spoken in brackets.&lt;/li&gt; &#xA; &lt;li&gt;Bug fixes&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v2.1; 2022/5/2&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Added ability to produce totally random voices.&lt;/li&gt; &#xA; &lt;li&gt;Added ability to download voice conditioning latent via a script, and then use a user-provided conditioning latent.&lt;/li&gt; &#xA; &lt;li&gt;Added ability to use your own pretrained models.&lt;/li&gt; &#xA; &lt;li&gt;Refactored directory structures.&lt;/li&gt; &#xA; &lt;li&gt;Performance improvements &amp;amp; bug fixes.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What&#39;s in a name?&lt;/h2&gt; &#xA;&lt;p&gt;I&#39;m naming my speech-related repos after Mojave desert flora and fauna. Tortoise is a bit tongue in cheek: this model is insanely slow. It leverages both an autoregressive decoder &lt;strong&gt;and&lt;/strong&gt; a diffusion decoder; both known for their low sampling rates. On a K80, expect to generate a medium sized sentence every 2 minutes.&lt;/p&gt; &#xA;&lt;h2&gt;Demos&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;http://nonint.com/static/tortoise_v2_examples.html&#34;&gt;this page&lt;/a&gt; for a large list of example outputs.&lt;/p&gt; &#xA;&lt;p&gt;Cool application of Tortoise+GPT-3 (not by me): &lt;a href=&#34;https://twitter.com/lexman_ai&#34;&gt;https://twitter.com/lexman_ai&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage guide&lt;/h2&gt; &#xA;&lt;h3&gt;Local Installation&lt;/h3&gt; &#xA;&lt;p&gt;If you want to use this on your own computer, you must have an NVIDIA GPU.&lt;/p&gt; &#xA;&lt;p&gt;On Windows, I &lt;strong&gt;highly&lt;/strong&gt; recommend using the Conda installation path. I have been told that if you do not do this, you will spend a lot of time chasing dependency problems.&lt;/p&gt; &#xA;&lt;p&gt;First, install miniconda: &lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;https://docs.conda.io/en/latest/miniconda.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Then run the following commands, using anaconda prompt as the terminal (or any other terminal configured to work with conda)&lt;/p&gt; &#xA;&lt;p&gt;This will:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;create conda environment with minimal dependencies specified&lt;/li&gt; &#xA; &lt;li&gt;activate the environment&lt;/li&gt; &#xA; &lt;li&gt;install pytorch with the command provided here: &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;https://pytorch.org/get-started/locally/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;clone tortoise-tts&lt;/li&gt; &#xA; &lt;li&gt;change the current directory to tortoise-tts&lt;/li&gt; &#xA; &lt;li&gt;run tortoise python setup install script&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda create --name tortoise python=3.9 numba inflect&#xA;conda activate tortoise&#xA;conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia&#xA;conda install transformers=4.29.2&#xA;git clone https://github.com/neonbjb/tortoise-tts.git&#xA;cd tortoise-tts&#xA;python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Optionally, pytorch can be installed in the base environment, so that other conda environments can use it too. To do this, simply send the &lt;code&gt;conda install pytorch...&lt;/code&gt; line before activating the tortoise environment.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; When you want to use tortoise-tts, you will always have to ensure the &lt;code&gt;tortoise&lt;/code&gt; conda environment is activated.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;If you are on windows, you may also need to install pysoundfile: &lt;code&gt;conda install -c conda-forge pysoundfile&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;do_tts.py&lt;/h3&gt; &#xA;&lt;p&gt;This script allows you to speak a single phrase with one or more voices.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tortoise/do_tts.py --text &#34;I&#39;m going to speak this&#34; --voice random --preset fast&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;read.py&lt;/h3&gt; &#xA;&lt;p&gt;This script provides tools for reading large amounts of text.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tortoise/read.py --textfile &amp;lt;your text to be read&amp;gt; --voice random&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will break up the textfile into sentences, and then convert them to speech one at a time. It will output a series of spoken clips as they are generated. Once all the clips are generated, it will combine them into a single file and output that as well.&lt;/p&gt; &#xA;&lt;p&gt;Sometimes Tortoise screws up an output. You can re-generate any bad clips by re-running &lt;code&gt;read.py&lt;/code&gt; with the --regenerate argument.&lt;/p&gt; &#xA;&lt;h3&gt;API&lt;/h3&gt; &#xA;&lt;p&gt;Tortoise can be used programmatically, like so:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;reference_clips = [utils.audio.load_audio(p, 22050) for p in clips_paths]&#xA;tts = api.TextToSpeech()&#xA;pcm_audio = tts.tts_with_preset(&#34;your text here&#34;, voice_samples=reference_clips, preset=&#39;fast&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use deepspeed:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;reference_clips = [utils.audio.load_audio(p, 22050) for p in clips_paths]&#xA;tts = api.TextToSpeech(use_deepspeed=True)&#xA;pcm_audio = tts.tts_with_preset(&#34;your text here&#34;, voice_samples=reference_clips, preset=&#39;fast&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use kv cache:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;reference_clips = [utils.audio.load_audio(p, 22050) for p in clips_paths]&#xA;tts = api.TextToSpeech(kv_cache=True)&#xA;pcm_audio = tts.tts_with_preset(&#34;your text here&#34;, voice_samples=reference_clips, preset=&#39;fast&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run model in float16:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;reference_clips = [utils.audio.load_audio(p, 22050) for p in clips_paths]&#xA;tts = api.TextToSpeech(half=True)&#xA;pcm_audio = tts.tts_with_preset(&#34;your text here&#34;, voice_samples=reference_clips, preset=&#39;fast&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;for Faster runs use all three:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;reference_clips = [utils.audio.load_audio(p, 22050) for p in clips_paths]&#xA;tts = api.TextToSpeech(use_deepspeed=True, kv_cache=True, half=True)&#xA;pcm_audio = tts.tts_with_preset(&#34;your text here&#34;, voice_samples=reference_clips, preset=&#39;fast&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Voice customization guide&lt;/h2&gt; &#xA;&lt;p&gt;Tortoise was specifically trained to be a multi-speaker model. It accomplishes this by consulting reference clips.&lt;/p&gt; &#xA;&lt;p&gt;These reference clips are recordings of a speaker that you provide to guide speech generation. These clips are used to determine many properties of the output, such as the pitch and tone of the voice, speaking speed, and even speaking defects like a lisp or stuttering. The reference clip is also used to determine non-voice related aspects of the audio output like volume, background noise, recording quality and reverb.&lt;/p&gt; &#xA;&lt;h3&gt;Provided voices&lt;/h3&gt; &#xA;&lt;p&gt;This repo comes with several pre-packaged voices. Voices prepended with &#34;train_&#34; came from the training set and perform far better than the others. If your goal is high quality speech, I recommend you pick one of them. If you want to see what Tortoise can do for zero-shot mimicking, take a look at the others.&lt;/p&gt; &#xA;&lt;h3&gt;Adding a new voice&lt;/h3&gt; &#xA;&lt;p&gt;To add new voices to Tortoise, you will need to do the following:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Gather audio clips of your speaker(s). Good sources are YouTube interviews (you can use youtube-dl to fetch the audio), audiobooks or podcasts. Guidelines for good clips are in the next section.&lt;/li&gt; &#xA; &lt;li&gt;Cut your clips into ~10 second segments. You want at least 3 clips. More is better, but I only experimented with up to 5 in my testing.&lt;/li&gt; &#xA; &lt;li&gt;Save the clips as a WAV file with floating point format and a 22,050 sample rate.&lt;/li&gt; &#xA; &lt;li&gt;Create a subdirectory in voices/&lt;/li&gt; &#xA; &lt;li&gt;Put your clips in that subdirectory.&lt;/li&gt; &#xA; &lt;li&gt;Run tortoise utilities with --voice=&amp;lt;your_subdirectory_name&amp;gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Picking good reference clips&lt;/h3&gt; &#xA;&lt;p&gt;As mentioned above, your reference clips have a profound impact on the output of Tortoise. Following are some tips for picking good clips:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Avoid clips with background music, noise or reverb. These clips were removed from the training dataset. Tortoise is unlikely to do well with them.&lt;/li&gt; &#xA; &lt;li&gt;Avoid speeches. These generally have distortion caused by the amplification system.&lt;/li&gt; &#xA; &lt;li&gt;Avoid clips from phone calls.&lt;/li&gt; &#xA; &lt;li&gt;Avoid clips that have excessive stuttering, stammering or words like &#34;uh&#34; or &#34;like&#34; in them.&lt;/li&gt; &#xA; &lt;li&gt;Try to find clips that are spoken in such a way as you wish your output to sound like. For example, if you want to hear your target voice read an audiobook, try to find clips of them reading a book.&lt;/li&gt; &#xA; &lt;li&gt;The text being spoken in the clips does not matter, but diverse text does seem to perform better.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Advanced Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Generation settings&lt;/h3&gt; &#xA;&lt;p&gt;Tortoise is primarily an autoregressive decoder model combined with a diffusion model. Both of these have a lot of knobs that can be turned that I&#39;ve abstracted away for the sake of ease of use. I did this by generating thousands of clips using various permutations of the settings and using a metric for voice realism and intelligibility to measure their effects. I&#39;ve set the defaults to the best overall settings I was able to find. For specific use-cases, it might be effective to play with these settings (and it&#39;s very likely that I missed something!)&lt;/p&gt; &#xA;&lt;p&gt;These settings are not available in the normal scripts packaged with Tortoise. They are available, however, in the API. See &lt;code&gt;api.tts&lt;/code&gt; for a full list.&lt;/p&gt; &#xA;&lt;h3&gt;Prompt engineering&lt;/h3&gt; &#xA;&lt;p&gt;Some people have discovered that it is possible to do prompt engineering with Tortoise! For example, you can evoke emotion by including things like &#34;I am really sad,&#34; before your text. I&#39;ve built an automated redaction system that you can use to take advantage of this. It works by attempting to redact any text in the prompt surrounded by brackets. For example, the prompt &#34;[I am really sad,] Please feed me.&#34; will only speak the words &#34;Please feed me&#34; (with a sad tonality).&lt;/p&gt; &#xA;&lt;h3&gt;Playing with the voice latent&lt;/h3&gt; &#xA;&lt;p&gt;Tortoise ingests reference clips by feeding them through individually through a small submodel that produces a point latent, then taking the mean of all of the produced latents. The experimentation I have done has indicated that these point latents are quite expressive, affecting everything from tone to speaking rate to speech abnormalities.&lt;/p&gt; &#xA;&lt;p&gt;This lends itself to some neat tricks. For example, you can combine feed two different voices to tortoise and it will output what it thinks the &#34;average&#34; of those two voices sounds like.&lt;/p&gt; &#xA;&lt;h4&gt;Generating conditioning latents from voices&lt;/h4&gt; &#xA;&lt;p&gt;Use the script &lt;code&gt;get_conditioning_latents.py&lt;/code&gt; to extract conditioning latents for a voice you have installed. This script will dump the latents to a .pth pickle file. The file will contain a single tuple, (autoregressive_latent, diffusion_latent).&lt;/p&gt; &#xA;&lt;p&gt;Alternatively, use the api.TextToSpeech.get_conditioning_latents() to fetch the latents.&lt;/p&gt; &#xA;&lt;h4&gt;Using raw conditioning latents to generate speech&lt;/h4&gt; &#xA;&lt;p&gt;After you&#39;ve played with them, you can use them to generate speech by creating a subdirectory in voices/ with a single &#34;.pth&#34; file containing the pickled conditioning latents as a tuple (autoregressive_latent, diffusion_latent).&lt;/p&gt; &#xA;&lt;h2&gt;Tortoise-detect&lt;/h2&gt; &#xA;&lt;p&gt;Out of concerns that this model might be misused, I&#39;ve built a classifier that tells the likelihood that an audio clip came from Tortoise.&lt;/p&gt; &#xA;&lt;p&gt;This classifier can be run on any computer, usage is as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;python tortoise/is_this_from_tortoise.py --clip=&amp;lt;path_to_suspicious_audio_file&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This model has 100% accuracy on the contents of the results/ and voices/ folders in this repo. Still, treat this classifier as a &#34;strong signal&#34;. Classifiers can be fooled and it is likewise not impossible for this classifier to exhibit false positives.&lt;/p&gt; &#xA;&lt;h2&gt;Model architecture&lt;/h2&gt; &#xA;&lt;p&gt;Tortoise TTS is inspired by OpenAI&#39;s DALLE, applied to speech data and using a better decoder. It is made up of 5 separate models that work together. I&#39;ve assembled a write-up of the system architecture here: &lt;a href=&#34;https://nonint.com/2022/04/25/tortoise-architectural-design-doc/&#34;&gt;https://nonint.com/2022/04/25/tortoise-architectural-design-doc/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;These models were trained on my &#34;homelab&#34; server with 8 RTX 3090s over the course of several months. They were trained on a dataset consisting of ~50k hours of speech data, most of which was transcribed by &lt;a href=&#34;http://www.github.com/neonbjb/ocotillo&#34;&gt;ocotillo&lt;/a&gt;. Training was done on my own &lt;a href=&#34;https://github.com/neonbjb/DL-Art-School&#34;&gt;DLAS&lt;/a&gt; trainer.&lt;/p&gt; &#xA;&lt;p&gt;I currently do not have plans to release the training configurations or methodology. See the next section..&lt;/p&gt; &#xA;&lt;h2&gt;Ethical Considerations&lt;/h2&gt; &#xA;&lt;p&gt;Tortoise v2 works considerably better than I had planned. When I began hearing some of the outputs of the last few versions, I began wondering whether or not I had an ethically unsound project on my hands. The ways in which a voice-cloning text-to-speech system could be misused are many. It doesn&#39;t take much creativity to think up how.&lt;/p&gt; &#xA;&lt;p&gt;After some thought, I have decided to go forward with releasing this. Following are the reasons for this choice:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;It is primarily good at reading books and speaking poetry. Other forms of speech do not work well.&lt;/li&gt; &#xA; &lt;li&gt;It was trained on a dataset which does not have the voices of public figures. While it will attempt to mimic these voices if they are provided as references, it does not do so in such a way that most humans would be fooled.&lt;/li&gt; &#xA; &lt;li&gt;The above points could likely be resolved by scaling up the model and the dataset. For this reason, I am currently withholding details on how I trained the model, pending community feedback.&lt;/li&gt; &#xA; &lt;li&gt;I am releasing a separate classifier model which will tell you whether a given audio clip was generated by Tortoise or not. See &lt;code&gt;tortoise-detect&lt;/code&gt; above.&lt;/li&gt; &#xA; &lt;li&gt;If I, a tinkerer with a BS in computer science with a ~$15k computer can build this, then any motivated corporation or state can as well. I would prefer that it be in the open and everyone know the kinds of things ML can do.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Diversity&lt;/h3&gt; &#xA;&lt;p&gt;The diversity expressed by ML models is strongly tied to the datasets they were trained on.&lt;/p&gt; &#xA;&lt;p&gt;Tortoise was trained primarily on a dataset consisting of audiobooks. I made no effort to balance diversity in this dataset. For this reason, Tortoise will be particularly poor at generating the voices of minorities or of people who speak with strong accents.&lt;/p&gt; &#xA;&lt;h2&gt;Looking forward&lt;/h2&gt; &#xA;&lt;p&gt;Tortoise v2 is about as good as I think I can do in the TTS world with the resources I have access to. A phenomenon that happens when training very large models is that as parameter count increases, the communication bandwidth needed to support distributed training of the model increases multiplicatively. On enterprise-grade hardware, this is not an issue: GPUs are attached together with exceptionally wide buses that can accommodate this bandwidth. I cannot afford enterprise hardware, though, so I am stuck.&lt;/p&gt; &#xA;&lt;p&gt;I want to mention here that I think Tortoise could be a &lt;strong&gt;lot&lt;/strong&gt; better. The three major components of Tortoise are either vanilla Transformer Encoder stacks or Decoder stacks. Both of these types of models have a rich experimental history with scaling in the NLP realm. I see no reason to believe that the same is not true of TTS.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;This project has garnered more praise than I expected. I am standing on the shoulders of giants, though, and I want to credit a few of the amazing folks in the community that have helped make this happen:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Hugging Face, who wrote the GPT model and the generate API used by Tortoise, and who hosts the model weights.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2102.12092.pdf&#34;&gt;Ramesh et al&lt;/a&gt; who authored the DALLE paper, which is the inspiration behind Tortoise.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2102.09672.pdf&#34;&gt;Nichol and Dhariwal&lt;/a&gt; who authored the (revision of) the code that drives the diffusion model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2106.07889.pdf&#34;&gt;Jang et al&lt;/a&gt; who developed and open-sourced univnet, the vocoder this repo uses.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mindslab-ai/univnet&#34;&gt;Kim and Jung&lt;/a&gt; who implemented univnet pytorch model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lucidrains&#34;&gt;lucidrains&lt;/a&gt; who writes awesome open source pytorch models, many of which are used here.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/patrickvonplaten&#34;&gt;Patrick von Platen&lt;/a&gt; whose guides on setting up wav2vec were invaluable to building my dataset.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Notice&lt;/h2&gt; &#xA;&lt;p&gt;Tortoise was built entirely by me using my own hardware. My employer was not involved in any facet of Tortoise&#39;s development.&lt;/p&gt; &#xA;&lt;p&gt;If you use this repo or the ideas therein for your research, please cite it! A bibtex entree can be found in the right pane on GitHub.&lt;/p&gt;</summary>
  </entry>
</feed>