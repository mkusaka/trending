<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-06T01:29:42Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>DCDmllm/WorldGPT</title>
    <updated>2024-05-06T01:29:42Z</updated>
    <id>tag:github.com,2024-05-06:/DCDmllm/WorldGPT</id>
    <link href="https://github.com/DCDmllm/WorldGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;WorldGPT: Empowering LLM as Multimodal World Model&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; WorldGPT: Empowering LLM as Multimodal World Model &lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA;  Zhiqi Ge&#xA; &lt;sup&gt;1&lt;/sup&gt;*, Hongzhe Huang&#xA; &lt;sup&gt;1&lt;/sup&gt;*, Mingze Zhou&#xA; &lt;sup&gt;1&lt;/sup&gt;*, &#xA; &lt;p&gt;Juncheng Li&lt;sup&gt;1,2&lt;/sup&gt;, Guoming Wang&lt;sup&gt;1&lt;/sup&gt;, Siliang Tang&lt;sup&gt;1&lt;/sup&gt;, Yueting Zhuang&lt;sup&gt;1&lt;/sup&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;Zhejiang University, &lt;sup&gt;2&lt;/sup&gt;National University of Singapore&lt;/p&gt; &#xA; &lt;p&gt;*Equal Contribution&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2404.18202&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Paper-Arxiv-red&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Highlights&lt;/h2&gt; &#xA;&lt;h3&gt;Development of a Generalist World Model&lt;/h3&gt; &#xA;&lt;p&gt;We propose &lt;strong&gt;WorldGPT&lt;/strong&gt;, a generalist world model trained on millions of videos through a &lt;strong&gt;progressive state transition training&lt;/strong&gt; process, which naturally supports input and output across any combination of modalities.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/DCDmllm/WorldGPT/main/figs/3_train.png&#34; alt=&#34;images&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Novel Cognitive Architecture&lt;/h3&gt; &#xA;&lt;p&gt;We introduce a novel &lt;strong&gt;cognitive architecture&lt;/strong&gt; tailored for world models, encompassing memory offloading, knowledge retrieval, and &lt;strong&gt;ContextReflector&lt;/strong&gt;. We construct high-quality sequential samples and retrieval-augmented samples to teach WorldGPT to utilize information from retrieved context through the &lt;strong&gt;cognitive-augmented tuning&lt;/strong&gt; process.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/DCDmllm/WorldGPT/main/figs/2_model.png&#34; alt=&#34;images&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Construction of World State Transitions Dataset&lt;/h3&gt; &#xA;&lt;p&gt;We present &lt;strong&gt;WorldNet&lt;/strong&gt;, a comprehensive dataset for world state transitions, ideal for training and evaluating world models.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/DCDmllm/WorldGPT/main/figs/4_dataset.png&#34; alt=&#34;images&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Novel Learning Paradigm for Multimodal Agents&lt;/h3&gt; &#xA;&lt;p&gt;We explore a new learning paradigm wherein multimodal agents can efficiently acquire knowledge from WorldGPT through &lt;strong&gt;dream tuning&lt;/strong&gt; on synthesized data.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/DCDmllm/WorldGPT/main/figs/5_instruct.png&#34; alt=&#34;images&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;h4&gt;1. Prepare Environment&lt;/h4&gt; &#xA;&lt;p&gt;First clone our repository, and create a python environment via the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone&#xA;cd WorldGPT&#xA;conda env create -f environment.yaml&#xA;conda activate worldgpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;2. Prepare Pretrained Weights&lt;/h4&gt; &#xA;&lt;p&gt;WorldGPT is based on following existing models. Please download the corresponding weights following the instructions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Vicuna&lt;/code&gt;: WorldGPT employs &lt;code&gt;Vicuna V0 7B&lt;/code&gt; as the language decoder. Prepare the full pretrained weights following the &lt;a href=&#34;https://huggingface.co/lmsys/vicuna-7b-delta-v0&#34;&gt;official instructions&lt;/a&gt;. Then, set the variable &lt;em&gt;vicuna_path&lt;/em&gt; in the &lt;a href=&#34;https://raw.githubusercontent.com/DCDmllm/WorldGPT/main/config/base.yaml#L8&#34;&gt;base config&lt;/a&gt; at Line 8.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;LanguageBind&lt;/code&gt; is the unified image/video/audio encoder. The model version for each modality is listed as follows:&lt;/p&gt; &#xA;  &lt;table&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th&gt;image&lt;/th&gt; &#xA;     &lt;th&gt;video&lt;/th&gt; &#xA;     &lt;th&gt;audio&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;&lt;a href=&#34;https://huggingface.co/LanguageBind/LanguageBind_Image&#34;&gt;LanguageBind_Image&lt;/a&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;a href=&#34;https://huggingface.co/LanguageBind/LanguageBind_Video_V1.5_FT&#34;&gt;LanguageBind_Video_V1.5_FT&lt;/a&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;a href=&#34;https://huggingface.co/LanguageBind/LanguageBind_Audio_FT&#34;&gt;LanguageBind_Audio_FT&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &lt;p&gt;The weights are automatically downloaded by default. For manual downloaded weights or customed model versions, set the variable &lt;em&gt;languagebind_path&lt;/em&gt; in the &lt;a href=&#34;https://raw.githubusercontent.com/DCDmllm/WorldGPT/main/config/base.yaml#L13&#34;&gt;base config&lt;/a&gt; at Line 13 - 16.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Diffusion Models&lt;/code&gt; are used to generate image/video/audio outputs (if generation is enabled). The model for each modality is listed as follows:&lt;/p&gt; &#xA;  &lt;table&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th&gt;image&lt;/th&gt; &#xA;     &lt;th&gt;video&lt;/th&gt; &#xA;     &lt;th&gt;audio&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;&lt;a href=&#34;https://huggingface.co/runwayml/stable-diffusion-v1-5&#34;&gt;stable-diffusion-v1-5&lt;/a&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;a href=&#34;https://huggingface.co/cerspense/zeroscope_v2_576w&#34;&gt;zeroscope_v2_576w&lt;/a&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;a href=&#34;https://huggingface.co/cvssp/audioldm-l-full&#34;&gt;audioldm-l-full&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &lt;p&gt;The weights are automatically downloaded by default. For manual downloaded weights or customed model versions, set the variable &lt;em&gt;decoder_path&lt;/em&gt; in the &lt;a href=&#34;https://raw.githubusercontent.com/DCDmllm/WorldGPT/main/config/base.yaml#L9&#34;&gt;base config&lt;/a&gt; at Line 9 - 12.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;3. Prepare WorldGPT Checkpoints&lt;/h4&gt; &#xA;&lt;p&gt;Choose a pretrained checkpoint from the versions below:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;version&lt;/th&gt; &#xA;   &lt;th&gt;link&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;worldgpt-languagebind-image&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/1evhqNfhndxXRN0vx5xcbHkoesRptRLO-?usp=drive_link&#34;&gt;download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;worldgpt-languagebind-multimodal&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/1T-Wp_BIwUjhqahVS1e0FCitsOnudLLTc?usp=drive_link&#34;&gt;download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;worldgpt-decode-image&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/13C8NZzW0FZmcguHZzAAw94k_3BRmzIdq?usp=drive_link&#34;&gt;download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;worldgpt-decode-multimodal&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/16Q5j4b7Ssj1t_H9B4T-XeURSYaECujgJ?usp=drive_link&#34;&gt;download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Note that WorldGPT uses worldgpt-languagebind checkpoints by default, which output LanguageBind embeddings. Though we provide worldgpt-decode checkpoints for visualization, they do not accuately reflect the true model capabilities.&lt;/p&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;h4&gt;1. Prepare Datasets&lt;/h4&gt; &#xA;&lt;h5&gt;1.1. WorldNet&lt;/h5&gt; &#xA;&lt;p&gt;We collect state transition datasets from various source and construct a comprehensive dataset named &lt;strong&gt;WorldNet&lt;/strong&gt;. WorldNet consists of two subsets: WorldNet-Wild and WorldNet-Crafted. Each dataset is further split into subsets by the data source. The available modalities of each subset are listed as follows. The WorldNet-Crafted subset can be downloaded &lt;a href=&#34;https://drive.google.com/drive/folders/1hntZ8Q4GQg5Esq2q5EBStvAEs_tqVxW_?usp=drive_link&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Subset&lt;/th&gt; &#xA;   &lt;th&gt;Source&lt;/th&gt; &#xA;   &lt;th&gt;Modality&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;WorldNet-Wild&lt;/td&gt; &#xA;   &lt;td&gt;YT-Temporal-180M&lt;/td&gt; &#xA;   &lt;td&gt;image, video,audio&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HowTo100M&lt;/td&gt; &#xA;   &lt;td&gt;image, video, audio&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;5&#34;&gt;WorldNet-Crafted&lt;/td&gt; &#xA;   &lt;td&gt;Charades&lt;/td&gt; &#xA;   &lt;td&gt;image, video, audio&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AVQA&lt;/td&gt; &#xA;   &lt;td&gt;image, video, audio&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ego4D&lt;/td&gt; &#xA;   &lt;td&gt;image, video, audio&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Something-Something V2&lt;/td&gt; &#xA;   &lt;td&gt;image&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YouCook2&lt;/td&gt; &#xA;   &lt;td&gt;image, video&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;The downloaded dataset contains only data of original modalities. For faster training, please precompute the LanguageBind embeddings via the command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python preprocess.py --data_root path/to/subset/modality/Train --modality image/video/audio --languagebind_path path/to/languagebind/weights&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;With the data prepared, organize each subset as the structure below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;└── ag/avqa/ego4d/s2s/youcook&#xA;    └── image/video/audio&#xA;        ├── Train&#xA;        └── Train_pt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In the directory of each modaltity, &lt;code&gt;Train&lt;/code&gt; contains raw data, and &lt;code&gt;Train_pt&lt;/code&gt; contains corresponding precomputed LanguageBind embeddings.&lt;/p&gt; &#xA;&lt;p&gt;Finally, specify the dataset path in the &lt;a href=&#34;&#34;&gt;training config&lt;/a&gt; via &lt;em&gt;dataset_list&lt;/em&gt; variable. Here is an example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;dataset_list:&#xA;  -&#xA;    root: worldnet/ag                                    # root of subset&#xA;    annotaion_path: worldnet/ag/state/action_train.json  # path to annotations&#xA;    modality: [&#39;image&#39;, &#39;video&#39;, &#39;audio&#39;]                # available modalities of subset&#xA;    weight: 0.2                                          # possibility to be chosen in training&#xA;  ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Additionally, set variable &lt;em&gt;precomputed_languagebind&lt;/em&gt; to &lt;code&gt;True&lt;/code&gt; if precomputed LanguageBind embeddings are available.&lt;/p&gt; &#xA;&lt;h5&gt;1.2. Custom Dataset&lt;/h5&gt; &#xA;&lt;p&gt;For training on custom dataset, first convert the annotas into the WorldNet format. Here is an example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;[&#xA;  {&#xA;    &#34;state0&#34;: {&#xA;      &#34;video&#34;: &#34;00001_state0.mp4&#34;,&#xA;      &#34;audio&#34;: &#34;00001_state0.wav&#34;,&#xA;      &#34;image&#34;: &#34;00001_state0.jpg&#34;,&#xA;      &#34;text&#34;: &#34;&#34;&#xA;    },&#xA;    &#34;state1&#34;: {&#xA;      &#34;video&#34;: &#34;00001_state1.mp4&#34;,&#xA;      &#34;audio&#34;: &#34;00001_state1.wav&#34;,&#xA;      &#34;image&#34;: &#34;00001_state1.jpg&#34;,&#xA;      &#34;text&#34;: &#34;&#34;&#xA;    },&#xA;    &#34;action&#34;: {&#xA;      &#34;text&#34;: &#34;adjust the mold&#34;&#xA;    },&#xA;  },&#xA;  ...&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Only &lt;code&gt;.jpg&lt;/code&gt; for image, &lt;code&gt;.mp4&lt;/code&gt; for video, &lt;code&gt;.wav&lt;/code&gt; for audio are valid data formats. Use an empty string to mark a missing modality. Then, you can follow the same steps as WorldNet to run on your custom dataset.&lt;/p&gt; &#xA;&lt;h4&gt;2. Start Training&lt;/h4&gt; &#xA;&lt;p&gt;Before starting training, first check the &lt;a href=&#34;https://raw.githubusercontent.com/DCDmllm/WorldGPT/main/config/base.yaml&#34;&gt;base config&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/DCDmllm/WorldGPT/main/config/train.yaml&#34;&gt;training config&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/DCDmllm/WorldGPT/main/config/ds_base.json&#34;&gt;deepspeed config&lt;/a&gt; for detailed settings.&lt;/p&gt; &#xA;&lt;p&gt;For quick starting, run the script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash scripts/train.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Specifying the command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;deepspeed --master_addr 127.0.0.1 --master_port 28459 train.py \&#xA;    --cfg_path config/train.yaml \&#xA;    --save_path ckpt/worldgpt \&#xA;    --load_path /path/to/worldgpt-languagebind-ckpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Inference&lt;/h3&gt; &#xA;&lt;h4&gt;1. LanguageBind Inference&lt;/h4&gt; &#xA;&lt;p&gt;Before starting inference, check the &lt;a href=&#34;https://raw.githubusercontent.com/DCDmllm/WorldGPT/main/config/test.yaml&#34;&gt;config&lt;/a&gt; for detailed settings, and set &lt;em&gt;load_path&lt;/em&gt; to the worldgpt-languagebind checkpoint.&lt;/p&gt; &#xA;&lt;p&gt;For inference with LanguageBind outputs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;import torch.nn.functional as F&#xA;&#xA;from model.worldgpt import load_worldgpt_model&#xA;from model.preprocessor import PreProcessor&#xA;from dataset.utils import to&#xA;from config import load_config&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    _args = {&#39;mode&#39;: &#39;test&#39;, &#39;dtype&#39;: torch.float16, &#39;preprocess_modality&#39;: [&#39;video&#39;, &#39;audio&#39;], &#39;cfg_path&#39;: &#39;config/test.yaml&#39;}&#xA;    args = load_config(_args)&#xA;    args.update(_args)&#xA;&#xA;    model = load_worldgpt_model(**args)&#xA;    model = model.eval().to(&#39;cuda&#39;, dtype=args[&#39;dtype&#39;])&#xA;&#xA;    preprocessor = PreProcessor(args)&#xA;    preprocessor.to_(&#39;cuda&#39;, dtype=args[&#39;dtype&#39;])&#xA;&#xA;    video = [&#39;demo/languagebind/skiing/video/state0.mp4&#39;, &#39;demo/languagebind/aircraft/video/state0.mp4&#39;]&#xA;    audio = [&#39;demo/languagebind/skiing/audio/state0.wav&#39;, &#39;demo/languagebind/aircraft/audio/state0.wav&#39;]&#xA;    action = [&#39;In the video, the person is skiing.&#39;, &#39;The aircraft starts to spray water over the forest.&#39;]&#xA;    inputs, _ = preprocessor({&#39;video&#39;: video, &#39;audio&#39;: audio, &#39;text&#39;: action})&#xA;    target_modality = [&#39;video&#39;, &#39;audio&#39;]&#xA;&#xA;    with torch.no_grad():&#xA;        pred = model.generate(inputs, target_modality,&#xA;                              max_tgt_length=args[&#39;max_tgt_length&#39;], top_p=args[&#39;top_p&#39;],&#xA;                              temperature=args[&#39;temperature&#39;])&#xA;        to(pred, &#39;cpu&#39;, dtype=torch.float)&#xA;&#xA;    video_targets = [&#39;demo/languagebind/skiing/video/state1.mp4&#39;, &#39;demo/languagebind/aircraft/video/state1.mp4&#39;]&#xA;    audio_targets = [&#39;demo/languagebind/skiing/audio/state1.wav&#39;, &#39;demo/languagebind/aircraft/audio/state1.wav&#39;]&#xA;    targets, _ = preprocessor({&#39;video&#39;: video_targets, &#39;audio&#39;: audio_targets})&#xA;    for m in pred:&#xA;        targets[m] = targets[m].to(&#39;cpu&#39;, dtype=torch.float)&#xA;        print(m, F.cosine_similarity(pred[m], targets[m], dim=1))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, use precomputed LanguageBind embeddings inputs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;import torch.nn.functional as F&#xA;&#xA;from model.worldgpt import load_worldgpt_model&#xA;from dataset.utils import to&#xA;from config import load_config&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    _args = {&#39;mode&#39;: &#39;test&#39;, &#39;dtype&#39;: torch.float16, &#39;cfg_path&#39;: &#39;config/test.yaml&#39;}&#xA;    args = load_config(_args)&#xA;    args.update(_args)&#xA;&#xA;    model = load_worldgpt_model(**args)&#xA;    model = model.eval().to(&#39;cuda&#39;, dtype=args[&#39;dtype&#39;])&#xA;&#xA;    video = [&#39;demo/languagebind/skiing/video/state0.pt&#39;, &#39;demo/languagebind/aircraft/video/state0.pt&#39;]&#xA;    audio = [&#39;demo/languagebind/skiing/audio/state0.pt&#39;, &#39;demo/languagebind/aircraft/audio/state0.pt&#39;]&#xA;    action = [&#39;In the video, the person is skiing.&#39;, &#39;The aircraft starts to spray water over the forest.&#39;]&#xA;    target_modality = [&#39;video&#39;, &#39;audio&#39;]&#xA;&#xA;    video = torch.stack([torch.load(p) for p in video]).to(&#39;cuda&#39;, dtype=args[&#39;dtype&#39;])&#xA;    audio = torch.stack([torch.load(p) for p in audio]).to(&#39;cuda&#39;, dtype=args[&#39;dtype&#39;])&#xA;    inputs = {&#39;video&#39;: video, &#39;audio&#39;: audio, &#39;text&#39;: action}&#xA;&#xA;    with torch.no_grad():&#xA;        pred = model.generate(inputs, target_modality,&#xA;                              max_tgt_length=args[&#39;max_tgt_length&#39;], top_p=args[&#39;top_p&#39;],&#xA;                              temperature=args[&#39;temperature&#39;])&#xA;        to(pred, &#39;cpu&#39;, dtype=torch.float)&#xA;&#xA;    video_targets = [&#39;demo/languagebind/skiing/video/state1.pt&#39;, &#39;demo/languagebind/aircraft/video/state1.pt&#39;]&#xA;    audio_targets = [&#39;demo/languagebind/skiing/audio/state1.pt&#39;, &#39;demo/languagebind/aircraft/audio/state1.pt&#39;]&#xA;    video_targets = torch.stack([torch.load(p) for p in video_targets])&#xA;    audio_targets = torch.stack([torch.load(p) for p in audio_targets])&#xA;    targets = {&#39;video&#39;: video_targets, &#39;audio&#39;: audio_targets}&#xA;    for m in pred:&#xA;        print(m, F.cosine_similarity(pred[m], targets[m], dim=1))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We also provide a script for batch inference. First prepare your input data following instructions in &lt;a href=&#34;https://raw.githubusercontent.com/DCDmllm/WorldGPT/main/#1-prepare-datasets&#34;&gt;Prepare Datasets&lt;/a&gt;, and specify the dataset and input/target modalities in the &lt;a href=&#34;https://raw.githubusercontent.com/DCDmllm/WorldGPT/main/config/batch_inference.yaml&#34;&gt;config&lt;/a&gt;. Then run the command to start inference:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python batch_inference.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;2. Visualized Inference&lt;/h4&gt; &#xA;&lt;p&gt;We provide the worldgpt-decode checkpoints for visualized inference. See &lt;a href=&#34;https://raw.githubusercontent.com/DCDmllm/WorldGPT/main/visualized_inference.ipynb&#34;&gt;visualized_inference.ipynb&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you found this work useful, please consider giving this repository a star and citing our paper as followed:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{ge2024worldgpt,&#xA;      title={WorldGPT: Empowering LLM as Multimodal World Model}, &#xA;      author={Zhiqi Ge and Hongzhe Huang and Mingze Zhou and Juncheng Li and Guoming Wang and Siliang Tang and Yueting Zhuang},&#xA;      year={2024},&#xA;      eprint={2404.18202},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.AI}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgment&lt;/h2&gt; &#xA;&lt;p&gt;Our project is developed based on the following repositories:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/NExT-GPT/NExT-GPT&#34;&gt;NExT-GPT&lt;/a&gt;: Any-to-Any Multimodal Large Language Model&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/PKU-YuanGroup/LanguageBind&#34;&gt;LanguageBind&lt;/a&gt;: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This repository is under &lt;a href=&#34;https://github.com/DCDmllm/WorldGPT/raw/main/license.md&#34;&gt;BSD 3-Clause License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any question about our work, please feel free to contact &lt;a href=&#34;mailto:22321202@zju.edu.cn&#34;&gt;Hongzhe Huang&lt;/a&gt; and &lt;a href=&#34;mailto:zhiqige2000@gmail.com&#34;&gt;Zhiqi Ge&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>jonnor/embeddedml</title>
    <updated>2024-05-06T01:29:42Z</updated>
    <id>tag:github.com,2024-05-06:/jonnor/embeddedml</id>
    <link href="https://github.com/jonnor/embeddedml" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Notes on Machine Learning on edge for embedded/sensor/IoT uses&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Machine learning on embedded devices&lt;/h1&gt; &#xA;&lt;p&gt;Focused primarily on running inference/prediction/feed-forward part on a microcontroller (or small embedded device). Training phase can run on a standard computer/server, using existing tools as much as possible.&lt;/p&gt; &#xA;&lt;h1&gt;Background&lt;/h1&gt; &#xA;&lt;h2&gt;What and when to use machine learning&lt;/h2&gt; &#xA;&lt;p&gt;The defaults right now are to do conventional signal processing (no learning) in sensor, and stream raw data to the cloud for storage and processing. Machine learning happens in the cloud. If gateways are used, they mostly forward communication (no data processing).&lt;/p&gt; &#xA;&lt;p&gt;On-edge processing valueable when&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Local response needed. Autonomy&lt;/li&gt; &#xA; &lt;li&gt;Adaptable response needed. Over time, in context.&lt;/li&gt; &#xA; &lt;li&gt;Low/predictable latency needed&lt;/li&gt; &#xA; &lt;li&gt;Sending raw sensor data has privacy implications. Audio, video.&lt;/li&gt; &#xA; &lt;li&gt;Unreliable connection&lt;/li&gt; &#xA; &lt;li&gt;High bandwidth sensor input. Audio, video, accelerometer/IMU, current sensor, radiowaves.&lt;/li&gt; &#xA; &lt;li&gt;Low bandwidth algorithm output&lt;/li&gt; &#xA; &lt;li&gt;Events of interest are rare&lt;/li&gt; &#xA; &lt;li&gt;Low energy usage needed&lt;/li&gt; &#xA; &lt;li&gt;Full/raw sensor data is not valuable to store&lt;/li&gt; &#xA; &lt;li&gt;Sensor system should be low cost&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Example usecases&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Predictive maintenance, using audio/vibration data&lt;/li&gt; &#xA; &lt;li&gt;Activitity detection for people, using audio/accelerometer data. Assistive tech, medical&lt;/li&gt; &#xA; &lt;li&gt;Appliance disaggregation, using aggregated power consumption data. &#34;Non-Intrusive Load Monitoring&#34; (NILM)&lt;/li&gt; &#xA; &lt;li&gt;Anomaly/change detection for predictive maintenance, using audio/vibration data, or electrical data&lt;/li&gt; &#xA; &lt;li&gt;Gesture recognition as human input device, using accelerometer/gyro data.&lt;/li&gt; &#xA; &lt;li&gt;Speech/command recognition as human input device, using microphone. Keyword/Wake-word detection&lt;/li&gt; &#xA; &lt;li&gt;Battery saving in wireless sensors. Normally sending day/week aggregates, on event/anomaly detection send data immediately&lt;/li&gt; &#xA; &lt;li&gt;Health status of animals via activity detected using accelerometer&lt;/li&gt; &#xA; &lt;li&gt;Monitoring eating activity using accelerometer &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0010482515000086&#34;&gt;1&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Environmental monitoring, using microphone to detect unwanted activity like cutting down trees&lt;/li&gt; &#xA; &lt;li&gt;Adaptive signalling and routing for wireless transmission in Wireless Sensor networks&lt;/li&gt; &#xA; &lt;li&gt;Electronic nose using arrays of MEMS detectors&lt;/li&gt; &#xA; &lt;li&gt;Material identification using reflecive spectrometer &lt;a href=&#34;https://hackaday.io/project/143014-compact-25-spectrometer/&#34;&gt;1&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;More notes on &lt;a href=&#34;https://raw.githubusercontent.com/jonnor/embeddedml/master/applications&#34;&gt;Applications&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Motivation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://petewarden.com/2018/06/11/why-the-future-of-machine-learning-is-tiny&#34;&gt;Why the Future of Machine Learning is Tiny (devices)&lt;/a&gt; Tiny Computers are Already Cheap and Everywhere. Energy is the Limiting Factor.We Capture Much More Sensor Data Than We Use.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.embedded.com/electronics-blogs/say-what-/4460873/Bringing-machine-learning-to-the-edge--A-Q-A-with-Neurala-s-Anatoli-Gorshechnikov-&#34;&gt;embedded.com: Bringing machine learning to the edge&lt;/a&gt; Predictions are much lower bandwidth than the raw sensor data (e.g. video) It allows for local adaptation in the AI logic (L-DNN) It achieves lower latency between observed event and action resulting from AI logic &#34;the most important question is what is the least amount accuracy and computation complexity we can do while still delivering the business value?&#34; Top mistake: &#34;Continuing with the top down approach ‘let’s make it perform the task first and then squeeze it on device` instead of switching to bottom up ‘let’s make it run on device and fulfill all hardware constraints first, and then tune it for the task at hand’.&#34;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.dlology.com/blog/how-to-run-deep-learning-model-on-microcontroller-with-cmsis-nn/&#34;&gt;How to run deep learning model on microcontroller with CMSIS-NN&lt;/a&gt;. Why run deep learning model on a microcontroller? Sensitive data gets to the cloud, photos, and audio recordings. The company who sells this may charge a service fee to use its service and even worse sell your private data. It won&#39;t work without the network connection to the server. Data traveling back and forth between the device and server introduces lag. Require network and wireless hardware components on the circuit design which increase the cost. It might waste bandwidth sending useless data.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;State of the Art in 2019&lt;/h2&gt; &#xA;&lt;p&gt;Of ML inference on general-purpose microcontrollers.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;TODO: update for 2023&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Deep models have efficient implementations for ARM Cortex-M. Ex: CNN and RNN in CMSIS-NN, FC in uTensor&lt;/li&gt; &#xA; &lt;li&gt;Some implementations available for non-neural models that &lt;em&gt;can&lt;/em&gt; be used. Ex: SVM,RF,AdaBoost in sklearn-porter&lt;/li&gt; &#xA; &lt;li&gt;A few special-designed ML algorithms made. Ex: ProtoNN, Bonsai&lt;/li&gt; &#xA; &lt;li&gt;Basic tools available for converting Tensorflow models&lt;/li&gt; &#xA; &lt;li&gt;Keyword-spotting/wake-word on audio well established. Used in commercial products (Alexa etc)&lt;/li&gt; &#xA; &lt;li&gt;Human activity detecton on accelerometers.&lt;/li&gt; &#xA; &lt;li&gt;Computer vision is actively developed&lt;/li&gt; &#xA; &lt;li&gt;Lots of research and many announcements of low-power co-processors, but little on market yet&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Limitations&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Neural models lacking for non-ARM micros. ESP8266/ESP32&lt;/li&gt; &#xA; &lt;li&gt;Non-neural models missing inference engines designed for microcontrollers&lt;/li&gt; &#xA; &lt;li&gt;&#34;Small DNN&#34; work mostly on computer vision for mobile phones (model size 1000x of uC)&lt;/li&gt; &#xA; &lt;li&gt;Few/no pretrained models available. Transfer learning little explored?&lt;/li&gt; &#xA; &lt;li&gt;Very little documentation of entire development process. From planning, data aquisition, model design&lt;/li&gt; &#xA; &lt;li&gt;Best practices underdocumented (or underdeveloped?)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Ways of advancing, make contributions&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Faster inference. Power saving, or bigger problems.&lt;/li&gt; &#xA; &lt;li&gt;Smaller models. Cheaper MCU, or bigger problems.&lt;/li&gt; &#xA; &lt;li&gt;Better accuracy on a problem. Better user experience, new usecases&lt;/li&gt; &#xA; &lt;li&gt;Solve a concrete usecase. Easier to deploy similar usecases&lt;/li&gt; &#xA; &lt;li&gt;Comparison between approaches. Microcontroller, ML model&lt;/li&gt; &#xA; &lt;li&gt;Libraries or tools. Lower time to market, enable more developers&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Learning material&lt;/h1&gt; &#xA;&lt;p&gt;Books&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mitpress.mit.edu/books/learning-embedded-systems&#34;&gt;Learning in Embedded Systems&lt;/a&gt;, May 1993.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tinymlbook.com/&#34;&gt;TinyML: Machine Learning with TensorFlow on Arduino, and Ultra-Low Power Micro-Controllers&lt;/a&gt;. January, 2020.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.packtpub.com/product/tinyml-cookbook/9781801814973&#34;&gt;TinyML Cookbook&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Articles&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.embedded.com/applying-machine-learning-in-embedded-systems&#34;&gt;Embedded.com: Applying machine learning in embedded systems&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.embedded.com/transfer-learning-for-the-iot/&#34;&gt;Embedded.com: Transfer learning for the IoT&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Tools&lt;/h1&gt; &#xA;&lt;p&gt;Open-source&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://github.com/emlearn/emlearn&#34;&gt;emlearn&lt;/a&gt;. Inference engine for microcontrollers. Supports converting scikit-learn models to plain C code. No dynamic allocations. No runtime needed.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org/lite/microcontrollers&#34;&gt;TensorFlow Lite for Microcontrollers&lt;/a&gt;. Supports neural network models made with TensorFlow (including Keras). Can run on wide range of platforms. Since November 2018. Supports ARM Cortex M, RISC-V, ESP32/Xtensa and Linux/MacOS host. Requires a runtime.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/majianjia/nnom&#34;&gt;nnom&lt;/a&gt; - Fixed-point neural network compiler for microcontrollers. Supports wide range of networks. Outputs plain C code. Can use CMSIS-NN on ARM Cortex M.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/eloquentarduino/micromlgen&#34;&gt;micromlgen&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Microsoft/ELL&#34;&gt;Embedded Learning Library&lt;/a&gt; by Microsoft. Set of C++ libraries for machine learning on embedded platforms. Includes code for kNN, RandomForest etc. Also has some node-based dataflow system in place it seems. JavaScript and Python bindings.&lt;/li&gt; &#xA; &lt;li&gt;ONNC project has a &lt;a href=&#34;https://github.com/ONNC/onnc-tutorial/raw/master/lab_2_Digit_Recognition_with_ARM_CortexM/lab_2.md&#34;&gt;backend for ARM Cortex M&lt;/a&gt; (using CMSIS-NN) and a &lt;a href=&#34;https://github.com/ONNC/onnc/raw/74e59908b2881844329c3d330eea7a7c306e1e22/docs/ONNC-C-Backend-Guide.md&#34;&gt;C backend&lt;/a&gt;. Allows to convert an ONNX models to run on devices.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/correlllab/nn4mc_cpp&#34;&gt;nn4mc_cpp&lt;/a&gt;. Neural Networks for Microcontrollers. Supports Keras natively. Provides instructions for PyTorch et.c via ONNX. Documentation for using from Python is lacking, as well as the type of networks supported. Does not seem maintained since 2020.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/uTensor/uTensor&#34;&gt;uTensor&lt;/a&gt;. Export Tensorflow model to mbed/ARM microcontrollers. Not supported on ESP32 or RISC-V or similar.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ma2th/ecst&#34;&gt;Embedded Classification Software Toolbox&lt;/a&gt;. Unmaintained since 2018.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/nok/sklearn-porter&#34;&gt;sklearn-porter&lt;/a&gt;. Can compile DecisionTreeClassifier and SVC models to C. Uses dynamic memory. Not optimized for use on embedded devices.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tvm.apache.org/docs/topic/microtvm/index.html&#34;&gt;microTVM&lt;/a&gt;. Depends only on the C standard library, and runs bare metal such as STM32 and NRF52 microcontrollers. Is under development. Which models are supported on microcontrollers not specified.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Proprietary&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;X-CUBE-AI for STM32&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Models&lt;/h1&gt; &#xA;&lt;p&gt;A range of Machine Learning models are useful in an embedded devices setting. Classical methods are used when the amount of data is quite small, and neural networks for large datasets and complex inputs.&lt;/p&gt; &#xA;&lt;p&gt;Below are notes on the various models in the context of embedded Machine Learning, including model size and compute-time optimization.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jonnor/embeddedml/master/models/tree-based.md&#34;&gt;Tree-based methods&lt;/a&gt;. Random Forest, Extratrees, Decision Trees, et.c.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jonnor/embeddedml/master/models/neural-networks.md&#34;&gt;Neural Networks&lt;/a&gt;. Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), Autoencoders&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jonnor/embeddedml/master/models/support-vector-machine.md&#34;&gt;Support Vector Machines&lt;/a&gt; (SVM).&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jonnor/embeddedml/master/models/mixtures.md&#34;&gt;Mixture models&lt;/a&gt;. Gaussian Mixture Models (GMM).&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jonnor/embeddedml/master/models/k-nearest-neighbours.md&#34;&gt;Nearest Neighbours&lt;/a&gt;. kNN et.c.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;More topics&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jonnor/embeddedml/master/topics/privacy.md&#34;&gt;Privacy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jonnor/embeddedml/master/topics/energy-usage.md&#34;&gt;Energy usage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jonnor/embeddedml/master/topics/model-size.md&#34;&gt;Model size&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jonnor/embeddedml/master/on-device-learning&#34;&gt;On-device learning&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>HVision-NKU/StoryDiffusion</title>
    <updated>2024-05-06T01:29:42Z</updated>
    <id>tag:github.com,2024-05-06:/HVision-NKU/StoryDiffusion</id>
    <link href="https://github.com/HVision-NKU/StoryDiffusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Create Magic Story!&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/HVision-NKU/StoryDiffusion/assets/49511209/f79da6b7-0b3b-4dd7-8dd0-ba0b15306fe6&#34; height=&#34;100&#34;&gt; &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h2&gt;StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video Generation &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://huggingface.co/datasets/huggingface/badges/resolve/main/paper-page-md-dark.svg?sanitize=true&#34; alt=&#34;Paper page&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA; &lt;p&gt;[&lt;a href=&#34;https://arxiv.org/abs/2405.01434&#34;&gt;Paper&lt;/a&gt;]   [&lt;a href=&#34;https://storydiffusion.github.io/&#34;&gt;Project Page&lt;/a&gt;]   [&lt;a href=&#34;https://huggingface.co/spaces/YupengZhou/StoryDiffusion&#34;&gt;🤗 Comic Generation Demo &lt;/a&gt;] &lt;a href=&#34;https://replicate.com/cjwbw/StoryDiffusion&#34;&gt;&lt;img src=&#34;https://replicate.com/cjwbw/StoryDiffusion/badge&#34; alt=&#34;Replicate&#34;&gt;&lt;/a&gt; &lt;br&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Official implementation of &lt;strong&gt;&lt;a href=&#34;&#34;&gt;StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video Generation&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;strong&gt;Demo Video&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/HVision-NKU/StoryDiffusion/assets/49511209/d5b80f8f-09b0-48cd-8b10-daff46d422af&#34;&gt;https://github.com/HVision-NKU/StoryDiffusion/assets/49511209/d5b80f8f-09b0-48cd-8b10-daff46d422af&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;🌠 &lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;StoryDiffusion can create a magic story by generating consistent images and videos. Our work mainly has two parts:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Consistent self-attention for character-consistent image generation over long-range sequences. It is hot-pluggable and compatible with all SD1.5 and SDXL-based image diffusion models. For the current implementation, the user needs to provide at least 3 text prompts for the consistent self-attention module. We recommend at least 5 - 6 text prompts for better layout arrangement.&lt;/li&gt; &#xA; &lt;li&gt;Motion predictor for long-range video generation, which predicts motion between Condition Images in a compressed image semantic space, achieving larger motion prediction.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;🔥 &lt;strong&gt;Examples&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;h3&gt;Comics generation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/HVision-NKU/StoryDiffusion/assets/49511209/b3771cbc-b6ca-4e26-bdc5-d944daf9f266&#34; alt=&#34;1&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Image-to-Video generation （Results are HIGHLY compressed for speed）&lt;/h3&gt; &#xA;&lt;p&gt;Leveraging the images produced through our Consistent Self-Attention mechanism, we can extend the process to create videos by seamlessly transitioning between these images. This can be considered as a two-stage long video generation approach.&lt;/p&gt; &#xA;&lt;p&gt;Note: results are &lt;strong&gt;highly compressed&lt;/strong&gt; for speed, you can visit &lt;a href=&#34;https://storydiffusion.github.io/&#34;&gt;our website&lt;/a&gt; for the high-quality version.&lt;/p&gt; &#xA;&lt;h4&gt;Two-stage Long Videos Generation (New Update)&lt;/h4&gt; &#xA;&lt;p&gt;Combining the two parts, we can generate very long and high-quality AIGC videos.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Video1&lt;/th&gt; &#xA;   &lt;th&gt;Video2&lt;/th&gt; &#xA;   &lt;th&gt;Video3&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/HVision-NKU/StoryDiffusion/assets/49511209/4e7e0f24-5f90-419b-9a1e-cdf36d361b26&#34; width=&#34;224&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/HVision-NKU/StoryDiffusion/assets/49511209/f509343d-d691-4e2a-b615-7d96381ef7c1&#34; width=&#34;224&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/HVision-NKU/StoryDiffusion/assets/49511209/4f0f7abb-4ae4-47a6-b692-5bdd8d9c8006&#34; width=&#34;224&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;Long Video Results using Condition Images&lt;/h4&gt; &#xA;&lt;p&gt;Our Image-to-Video model can generate a video by providing a sequence of user-input condition images.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Video1&lt;/th&gt; &#xA;   &lt;th&gt;Video2&lt;/th&gt; &#xA;   &lt;th&gt;Video3&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/HVision-NKU/StoryDiffusion/assets/49511209/af6f5c50-c773-4ef2-a757-6d7a46393f39&#34; width=&#34;224&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/HVision-NKU/StoryDiffusion/assets/49511209/d58e4037-d8df-4f90-8c81-ce4b6d2d868e&#34; width=&#34;224&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/HVision-NKU/StoryDiffusion/assets/49511209/40da15ba-f5c1-48d8-84d6-8d327207d696&#34; width=&#34;224&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Video4&lt;/th&gt; &#xA;   &lt;th&gt;Video5&lt;/th&gt; &#xA;   &lt;th&gt;Video6&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/HVision-NKU/StoryDiffusion/assets/49511209/8f04c9fc-3031-49e3-9de8-83d582b80a1f&#34; width=&#34;224&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/HVision-NKU/StoryDiffusion/assets/49511209/604107fb-8afe-4052-bda4-362c646a756e&#34; width=&#34;224&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/HVision-NKU/StoryDiffusion/assets/49511209/b05fa6a0-12e6-4111-abf8-18b8cd84f3ff&#34; width=&#34;224&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;Short Videos&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Video1&lt;/th&gt; &#xA;   &lt;th&gt;Video2&lt;/th&gt; &#xA;   &lt;th&gt;Video3&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/HVision-NKU/StoryDiffusion/assets/49511209/5e7f717f-daad-46f6-b3ba-c087bd843158&#34; width=&#34;224&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/HVision-NKU/StoryDiffusion/assets/49511209/79aa52b2-bf37-4c9c-8555-c7050aec0cdf&#34; width=&#34;224&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/HVision-NKU/StoryDiffusion/assets/49511209/9fdfd091-10e6-434e-9ce7-6d6e6d8f4b22&#34; width=&#34;224&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Video4&lt;/th&gt; &#xA;   &lt;th&gt;Video5&lt;/th&gt; &#xA;   &lt;th&gt;Video6&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/HVision-NKU/StoryDiffusion/assets/49511209/0b219b60-a998-4820-9657-6abe1747cb6b&#34; width=&#34;224&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/HVision-NKU/StoryDiffusion/assets/49511209/d387aef0-ffc8-41b0-914f-4b0392d9f8c5&#34; width=&#34;224&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/HVision-NKU/StoryDiffusion/assets/49511209/3c64958a-1079-4ca0-a9cf-e0486adbc57f&#34; width=&#34;224&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;🚩 &lt;strong&gt;TODO/Updates&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Commic Results of StoryDiffusion.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Video Results of StoryDiffusion.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Source code of Comic Generation&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Source code of gradio demo&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Source code of Video Generation Model&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Pretrained weight of Video Generation Model&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;🔧 Dependencies and Installation&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python &amp;gt;= 3.8 (Recommend to use &lt;a href=&#34;https://www.anaconda.com/download/#linux&#34;&gt;Anaconda&lt;/a&gt; or &lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;Miniconda&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch &amp;gt;= 2.0.0&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create --name storydiffusion python=3.10&#xA;conda activate storydiffusion&#xA;pip install -U pip&#xA;&#xA;# Install requirements&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;How to use&lt;/h1&gt; &#xA;&lt;p&gt;Currently, we provide two ways for you to generate comics.&lt;/p&gt; &#xA;&lt;h2&gt;Use the jupyter notebook&lt;/h2&gt; &#xA;&lt;p&gt;You can open the &lt;code&gt;Comic_Generation.ipynb&lt;/code&gt; and run the code.&lt;/p&gt; &#xA;&lt;h2&gt;Start a local gradio demo&lt;/h2&gt; &#xA;&lt;p&gt;Run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python gradio_app_sdxl_specific_id.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions, you are very welcome to email &lt;a href=&#34;mailto:ypzhousdu@gmail.com&#34;&gt;ypzhousdu@gmail.com&lt;/a&gt; and &lt;a href=&#34;mailto:zhoudaquan21@gmail.com&#34;&gt;zhoudaquan21@gmail.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Disclaimer&lt;/h1&gt; &#xA;&lt;p&gt;This project strives to impact the domain of AI-driven image and video generation positively. Users are granted the freedom to create images and videos using this tool, but they are expected to comply with local laws and utilize it responsibly. The developers do not assume any responsibility for potential misuse by users.&lt;/p&gt; &#xA;&lt;h1&gt;BibTeX&lt;/h1&gt; &#xA;&lt;p&gt;If you find StoryDiffusion useful for your research and applications, please cite using this BibTeX:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-BibTeX&#34;&gt;@article{Zhou2024storydiffusion,&#xA;  title={StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video Generation},&#xA;  author={Zhou, Yupeng and Zhou, Daquan and Cheng, Ming-Ming and Feng, Jiashi and Hou, Qibin},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>