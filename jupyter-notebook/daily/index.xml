<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-11-17T01:37:35Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>jeffheaton/app_deep_learning</title>
    <updated>2023-11-17T01:37:35Z</updated>
    <id>tag:github.com,2023-11-17:/jeffheaton/app_deep_learning</id>
    <link href="https://github.com/jeffheaton/app_deep_learning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;T81-558: PyTorch - Applications of Deep Neural Networks @washington University in St. Louis&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;T81 558:Applications of Deep Neural Networks&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://www.wustl.edu&#34;&gt;Washington University in St. Louis&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Instructor: &lt;a href=&#34;https://sites.wustl.edu/jeffheaton/&#34;&gt;Jeff Heaton&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Section 1. Fall 2023, Monday, 2:30 PM, Location: Louderman/461&lt;/li&gt; &#xA; &lt;li&gt;Section 2. Fall 2023, Online&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Course Description&lt;/h1&gt; &#xA;&lt;p&gt;Deep learning is a group of exciting new technologies for neural networks. Through a combination of advanced training techniques and neural network architectural components, it is now possible to create neural networks that can handle tabular data, images, text, and audio as both input and output. Deep learning allows a neural network to learn hierarchies of information in a way that is like the function of the human brain. This course will introduce the student to classic neural network structures, Convolution Neural Networks (CNN), Long Short-Term Memory (LSTM), Gated Recurrent Neural Networks (GRU), General Adversarial Networks (GAN) and reinforcement learning. Application of these architectures to computer vision, time series, security, natural language processing (NLP), and data generation will be covered. High Performance Computing (HPC) aspects will demonstrate how deep learning can be leveraged both on graphical processing units (GPUs), as well as grids. Focus is primarily upon the application of deep learning to problems, with some introduction to mathematical foundations. Students will use the Python programming language to implement deep learning using PyTorch. It is not necessary to know Python prior to this course; however, familiarity of at least one programming language is assumed. This course will be delivered in a hybrid format that includes both classroom and online instruction.&lt;/p&gt; &#xA;&lt;h1&gt;Objectives&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Explain how neural networks (deep and otherwise) compare to other machine learning models.&lt;/li&gt; &#xA; &lt;li&gt;Determine when a deep neural network would be a good choice for a particular problem.&lt;/li&gt; &#xA; &lt;li&gt;Demonstrate your understanding of the material through a final project uploaded to GitHub.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Syllabus&lt;/h1&gt; &#xA;&lt;p&gt;This syllabus presents the expected class schedule, due dates, and reading assignments. &lt;a href=&#34;https://data.heatonresearch.com/wustl/jheaton-t81-558-fall-2023-syllabus.pdf&#34;&gt;Download current syllabus.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Module&lt;/th&gt; &#xA;   &lt;th&gt;Content&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/app_deep_learning/main/t81_558_class_01_1_overview.ipynb&#34;&gt;Module 1&lt;/a&gt;&lt;br&gt;&lt;strong&gt;Meet on 08/28/2023&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 1: Python Preliminaries&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 1.1: Course Overview&lt;/li&gt;&#xA;     &lt;li&gt;Part 1.2: Introduction to Python&lt;/li&gt;&#xA;     &lt;li&gt;Part 1.3: Python Lists, Dictionaries, Sets &amp;amp; JSON&lt;/li&gt;&#xA;     &lt;li&gt;Part 1.4: File Handling&lt;/li&gt;&#xA;     &lt;li&gt;Part 1.5: Functions, Lambdas, and Map/ReducePython Preliminaries&lt;/li&gt;&#xA;     &lt;li&gt;&lt;strong&gt;We will meet on campus this week! (first meeting)&lt;/strong&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/app_deep_learning/main/t81_558_class_02_1_python_pandas.ipynb&#34;&gt;Module 2&lt;/a&gt;&lt;br&gt;Week of 09/11/2023&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 2: Python for Machine Learning&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt; Part 2.1: Introduction to Pandas for Deep Learning&lt;/li&gt;&#xA;     &lt;li&gt;Part 2.2: Encoding Categorical Values in Pandas&lt;/li&gt;&#xA;     &lt;li&gt;Part 2.3: Grouping, Sorting, and Shuffling&lt;/li&gt;&#xA;     &lt;li&gt;Part 2.4: Using Apply and Map in Pandas&lt;/li&gt;&#xA;     &lt;li&gt;Part 2.5: Feature Engineering in Padas&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/app_deep_learning/main/assignments/assignment_yourname_class1.ipynb&#34;&gt;Module 1 Program&lt;/a&gt; due: 09/12/2023&lt;/li&gt;&#xA;     &lt;li&gt; Icebreaker due: 09/12/2023&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/app_deep_learning/main/t81_558_class_03_1_neural_net.ipynb&#34;&gt;Module 3&lt;/a&gt;&lt;br&gt;Week of 09/18/2023&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 3: PyTorch for Neural Networks&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 3.1: Deep Learning and Neural Network Introduction&lt;/li&gt;&#xA;     &lt;li&gt; Part 3.2: Introduction to PyTorch&lt;/li&gt;&#xA;     &lt;li&gt;Part 3.3: Encoding a Feature Vector for PyTorch Deep Learning&lt;/li&gt;&#xA;     &lt;li&gt;Part 3.4: Early Stopping and Network Persistence&lt;/li&gt;&#xA;     &lt;li&gt;Part 3.5: Sequences vs Classes in PyTorch&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/app_deep_learning/main/assignments/assignment_yourname_class2.ipynb&#34;&gt;Module 2: Program&lt;/a&gt; due: 09/19/2023&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/app_deep_learning/main/t81_558_class_04_1_kfold.ipynb&#34;&gt;Module 4&lt;/a&gt;&lt;br&gt;Week of 09/25/2023&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 4: Training for Tabular Data&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 4.1: Using K-Fold Cross-validation with PyTorch&lt;/li&gt;&#xA;     &lt;li&gt;Part 4.2: Training Schedules for PyTorch&lt;/li&gt;&#xA;     &lt;li&gt;Part 4.3: Dropout Regularization&lt;/li&gt;&#xA;     &lt;li&gt;Part 4.4: Batch Normalization&lt;/li&gt;&#xA;     &lt;li&gt;Part 4.5: RAPIDS for Tabular Data&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/app_deep_learning/main/assignments/assignment_yourname_class3.ipynb&#34;&gt;Module 3 Program&lt;/a&gt; due: 09/26/2023&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/app_deep_learning/main/t81_558_class_05_1_python_images.ipynb&#34;&gt;Module 5&lt;/a&gt;&lt;br&gt;&lt;strong&gt;Meet on 10/02/2023&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 5: CNN and Computer Vision&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;5.1 Image Processing in Python&lt;/li&gt;&#xA;     &lt;li&gt;5.2 Using Convolutional Neural Networks&lt;/li&gt;&#xA;     &lt;li&gt;5.3 Using Pretrained Neural Networks&lt;/li&gt;&#xA;     &lt;li&gt;5.4 Looking at Generators and Image Augmentation&lt;/li&gt;&#xA;     &lt;li&gt;5.5 Recognizing Multiple Images with YOLO&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/app_deep_learning/main/assignments/assignment_yourname_class4.ipynb&#34;&gt;Module 4 Program&lt;/a&gt; due: 10/03/2023&lt;/li&gt;&#xA;     &lt;li&gt;&lt;strong&gt;We will meet on campus this week! (second meeting)&lt;/strong&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/app_deep_learning/main/t81_558_class_06_1_transformers.ipynb&#34;&gt;Module 6&lt;/a&gt;&lt;br&gt;Week of 10/16/2023&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 6: ChatGPT and Large Language Models&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;6.1: Introduction to Transformers&lt;/li&gt;&#xA;     &lt;li&gt;6.2: Accessing the ChatGPT API&lt;/li&gt;&#xA;     &lt;li&gt;6.3: Llama, Alpaca, and LORA&lt;/li&gt;&#xA;     &lt;li&gt;6.4: Introduction to Embeddings&lt;/li&gt;&#xA;     &lt;li&gt;6.5: Prompt Engineering&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/app_deep_learning/main/assignments/assignment_yourname_class5.ipynb&#34;&gt;Module 5 Program&lt;/a&gt; due: 10/17/2023&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/app_deep_learning/main/t81_558_class_07_1_img_generative.ipynb&#34;&gt;Module 7&lt;/a&gt;&lt;br&gt;Week of 10/23/2023&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 7: Image Generative Models&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;7.1: Introduction to Generative AI&lt;/li&gt;&#xA;     &lt;li&gt;7.2: Generating Faces with StyleGAN3&lt;/li&gt;&#xA;     &lt;li&gt;7.3: GANS to Enhance Old Photographs Deoldify&lt;/li&gt;&#xA;     &lt;li&gt;7.4: Text to Images with StableDiffusion&lt;/li&gt;&#xA;     &lt;li&gt;7.5: Finetuning with Dreambooth&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/app_deep_learning/main/assignments/assignment_yourname_class6.ipynb&#34;&gt;Module 6 Assignment&lt;/a&gt; due: 10/24/2023&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/app_deep_learning/main/t81_558_class_08_1_kaggle_intro.ipynb&#34;&gt;Module 8&lt;/a&gt;&lt;br&gt;&lt;strong&gt;Meet on 10/30/2023&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 8: Kaggle&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;8.1 Introduction to Kaggle&lt;/li&gt;&#xA;     &lt;li&gt;8.2 Building Ensembles with Scikit-Learn and PyTorch&lt;/li&gt;&#xA;     &lt;li&gt;8.3 How Should you Architect Your PyTorch Neural Network: Hyperparameters&lt;/li&gt;&#xA;     &lt;li&gt;8.4 Bayesian Hyperparameter Optimization for PyTorch&lt;/li&gt;&#xA;     &lt;li&gt;8.5 Current Semester&#39;s Kaggle&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/app_deep_learning/main/assignments/assignment_yourname_class7.ipynb&#34;&gt;Module 7 Assignment&lt;/a&gt; due: 10/31/2023&lt;/li&gt;&#xA;     &lt;li&gt;&lt;strong&gt;We will meet on campus this week! (third meeting)&lt;/strong&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/app_deep_learning/main/t81_558_class_09_1_faces.ipynb&#34;&gt;Module 9&lt;/a&gt;&lt;br&gt;Week of 11/06/2023&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 9: Facial Recognition&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;9.1 Detecting Faces in an Image&lt;/li&gt;&#xA;     &lt;li&gt;9.2 Detecting Facial Features&lt;/li&gt;&#xA;     &lt;li&gt;9.3 Image Augmentation&lt;/li&gt;&#xA;     &lt;li&gt;9.4 Application: Emotion Detection&lt;/li&gt;&#xA;     &lt;li&gt;9.5 Application: Blink Efficiency&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/app_deep_learning/main/assignments/assignment_yourname_class8.ipynb&#34;&gt;Module 8 Assignment&lt;/a&gt; due: 11/07/2023&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/app_deep_learning/main/t81_558_class_10_1_timeseries.ipynb&#34;&gt;Module 10&lt;/a&gt;&lt;br&gt;Week of 11/13/2023&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 10: Time Series in PyTorch&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Time Series Data Encoding for Deep Learning, PyTorch&lt;/li&gt;&#xA;     &lt;li&gt;Seasonality and Trend&lt;/li&gt;&#xA;     &lt;li&gt;LSTM-Based Time Series with PyTorch&lt;/li&gt;&#xA;     &lt;li&gt;CNN-Based Time Series with PyTorch&lt;/li&gt;&#xA;     &lt;li&gt;Predicting with Meta Prophet&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/app_deep_learning/main/assignments/assignment_yourname_class9.ipynb&#34;&gt;Module 9 Assignment&lt;/a&gt; due: 11/14/2023&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/app_deep_learning/main/t81_558_class_11_1_hf.ipynb&#34;&gt;Module 11&lt;/a&gt;&lt;br&gt;Week of 11/20/2023&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 11: Natural Language Processing&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;11.1 Introduction to Natural Language Processing&lt;/li&gt;&#xA;     &lt;li&gt;11.2 Hugging Face Introduction&lt;/li&gt;&#xA;     &lt;li&gt;11.3 Hugging Face Tokenizers&lt;/li&gt;&#xA;     &lt;li&gt;11.4 Hugging Face Data Sets&lt;/li&gt;&#xA;     &lt;li&gt;11.5 Training a Model in Hugging Face&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/app_deep_learning/main/assignments/assignment_yourname_class10.ipynb&#34;&gt;Module 10 Assignment&lt;/a&gt; due: 11/21/2023&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/app_deep_learning/main/t81_558_class_12_1_ai_gym.ipynb&#34;&gt;Module 12&lt;/a&gt;&lt;br&gt;Week of 11/27/2023&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 12: Reinforcement Learning&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Kaggle Assignment due: 11/27/2023 (approx 4-6PM, due to Kaggle GMT timezone)&lt;/li&gt;&#xA;     &lt;li&gt;Introduction to Gymnasium&lt;/li&gt;&#xA;     &lt;li&gt;Introduction to Q-Learning&lt;/li&gt;&#xA;     &lt;li&gt;Stable Baselines Q-Learning&lt;/li&gt;&#xA;     &lt;li&gt;Atari Games with Stable Baselines Neural Networks&lt;/li&gt;&#xA;     &lt;li&gt;Future of Reinforcement Learning&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/app_deep_learning/main/t81_558_class_13_1_auto_encode.ipynb&#34;&gt;Module 13&lt;/a&gt;&lt;br&gt;&lt;strong&gt;Meet on 12/04/2023&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 13: Deployment and Monitoring&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 13.1: Using Denoising AutoEncoders &lt;/li&gt;&#xA;     &lt;li&gt;Part 13.2: Anomaly Detection&lt;/li&gt;&#xA;     &lt;li&gt;Part 13.3: Model Drift and Retraining&lt;/li&gt;&#xA;     &lt;li&gt;Part 13.4: Tensor Processing Units (TPUs)&lt;/li&gt;&#xA;     &lt;li&gt;Part 13.5: Future Directions in Artificial Intelligence&lt;/li&gt;&#xA;     &lt;li&gt;&lt;strong&gt;We will meet on campus this week! (fourth meeting)&lt;/strong&gt;&lt;/li&gt;&#xA;     &lt;li&gt;Final project due: 12/05/2023&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Datasets&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://data.heatonresearch.com/data/t81-558/index.html&#34;&gt;Datasets can be downloaded here&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>DrugowitschLab/ML-from-scratch-seminar</title>
    <updated>2023-11-17T01:37:35Z</updated>
    <id>tag:github.com,2023-11-17:/DrugowitschLab/ML-from-scratch-seminar</id>
    <link href="https://github.com/DrugowitschLab/ML-from-scratch-seminar" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This repository is part of a &#34;Machine Learning from Scratch&#34; seminar at Harvard Medical School.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ML-from-scratch-seminar&lt;/h1&gt; &#xA;&lt;p&gt;This repository is part of the &lt;em&gt;Machine Learning from Scratch&lt;/em&gt; seminar in the Department of Neurobiology at Harvard Medical School. In this seminar, a group of interested graduate students and postdocs develop minimal Python implementations of popular ML models. The primary goal is to exemplify the learning dynamics, strengths and limitations of the algorithms while keeping the involved computations &#34;tractable&#34;.&lt;/p&gt; &#xA;&lt;h2&gt;Outline of the format&lt;/h2&gt; &#xA;&lt;p&gt;Each session is chaired by 1 or 2 people. Per session we discuss one machine learning model. Each session consists of two sub-sessions on two late afternoons / evenings, e.g., Wed and Thu at 5 p.m. We convene roughly every two months for a session. The ideal group size is ~10 people. Usually, we will meet in GB 422 (10 person capacity) or WAB 236 (larger room).&lt;/p&gt; &#xA;&lt;p&gt;On day one, a brief (20-30min) introduction on the topic is given by the session chairs, using white boards or slides. At the end of the introduction, readings on the theory are shared which are studied by the class together over dinner. (Post-Covid note: For the time being, dinner can only be eaten outside on the Quad. (Post-post-Covid note: we are allowed to have dinner indoors, again!)) The collective reading session is a ``service&#39;&#39;, not an obligation: Feel free to study the theory at home. Readings could be provided as a PDF or simply as one or two URLs. Some presenters have decided to use the reading session as a training opportunity for giving a full lecture on the white board. You are, of course, very welcome to do so, but please be reminded that this is not expected. In any case: Please keep the provided material concise and manageable for a 90 min reading session! The day-one sub-session should not exceed 2 1/2 hours in total (incl. dinner).&lt;/p&gt; &#xA;&lt;p&gt;On day two, we implement a minimal version of the model using Python 3 and mainly standard libraries. We aim for a conceptually clear realization of one particularly relevant model incarnation. ``&lt;em&gt;ML from scratch&lt;/em&gt;&#39;&#39; means that we want to understand every computation. Thus, we do not aim for an efficient implementation nor for generic / flexible solutions. The implementation should just be good enough for application to one or two toy tasks. These tasks (and the involved data sets) should be chosen such that distinguishing properties of the model become apparent — ideally, highlighting also limitations of the model or algorithm. The model implementation and its application to example tasks should be doable within ~3 hours.&lt;/p&gt; &#xA;&lt;p&gt;Ideally, everyone brings her/his own laptop. Packages for symbolic differentiation and prepared functions for data loading and data visualization can be used. Training time should not exceed 5 minutes on a standard desktop computer (w/o using GPUs).&lt;/p&gt; &#xA;&lt;p&gt;People are invited to participate without committing to hosting a session. Regular participants, however, may want to consider chairing at some point.&lt;/p&gt; &#xA;&lt;h2&gt;Role of the session chair(s)&lt;/h2&gt; &#xA;&lt;p&gt;Each session is prepared by one or two session chairs. The chairs have the following responsibilities:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Give an short introduction on the first day,&lt;/li&gt; &#xA; &lt;li&gt;provide the readings,&lt;/li&gt; &#xA; &lt;li&gt;help others on theory and implementation questions,&lt;/li&gt; &#xA; &lt;li&gt;have an at least somewhat working reference implementation (can be copy-and-paste from a web blog),&lt;/li&gt; &#xA; &lt;li&gt;announce any non-standard software requirements beforehand, so people can install them at home,&lt;/li&gt; &#xA; &lt;li&gt;share data sets and auxiliary functions on our github repository: &lt;a href=&#34;https://github.com/DrugowitschLab/ML-from-scratch-seminar&#34;&gt;https://github.com/DrugowitschLab/ML-from-scratch-seminar&lt;/a&gt;,&lt;/li&gt; &#xA; &lt;li&gt;identify instructive example tasks (= data sets + what to try out with them) for the coding session.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The last point will require the most preparation time as these examples are at the heart of the seminar. Please, plan ahead and dedicate enough time to this point.&lt;/p&gt; &#xA;&lt;p&gt;For the code, auto-diff packages and other little helpers are okay: the session chairs decide on what is the right balance between &#34;from-scratch&#34; and &#34;black-box&#34;. Btw, a seemingly unlimited source of examples and code snippets can be found in the &lt;a href=&#34;https://github.com/eriklindernoren/ML-From-Scratch&#34;&gt;``ML-From-Scratch&#39;&#39; repository of Erik Linder-Norén&lt;/a&gt;. (Ironically, I found this repository when googling for &#39;ml from scratch&#39; after writing this document.)&lt;/p&gt; &#xA;&lt;p&gt;When preparing the theory part on day one, in contrast, you can save time by relying heavily on existing web blogs, lecture slides, and other sources. The primary goal of day one is &lt;em&gt;not&lt;/em&gt; a comprehensive understanding of the theory. This would be too ambitious for a 2 hour meeting. Rather we want to arrive at the specific equations which constitute the algorithm and need to be implemented. Clearly highlighting these equations will facilitate everyone&#39;s implementation on day two.&lt;/p&gt; &#xA;&lt;p&gt;After the session, please share the material (notes, readings, reference implementations,...) in a new sub-directory on github. Our github repository has public read access and grants full write permissions to every participant. To get write permissions, send your github username to Johannes (johannes _ bill AT hms DOT harvard DOT edu).&lt;/p&gt; &#xA;&lt;h2&gt;Role of the organizer&lt;/h2&gt; &#xA;&lt;p&gt;The organizer (Johannes) is also a participant, i.e., serves as a session chair from time to time. Apart from that, the organizer helps scheduling the sessions, offers advice to the session chairs on the theory and when planning coding tasks, maintains the platform for document and code sharing, and takes care of ordering dinner.&lt;/p&gt; &#xA;&lt;h2&gt;List of past sessions&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Dates&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Time&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Chairs&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Topic&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Jan 30, 2019&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;5-7 p.m.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Johannes&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Kick-off meeting&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Feb 12-13 (Tue/Wed)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;5-8 p.m.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Luke &amp;amp; Johannes&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Variational auto-encoders&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Apr 23+25 (Tue/Thu)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;5-8 p.m.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Chong &amp;amp; Alex&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Hidden Markov Models&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Jun 26-27 (Wed/Thu)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;5-8 p.m.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Selmaan&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Gaussian processes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Sep 25-26 (Wed/Thu)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;5-8 p.m.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Shih-Yi &amp;amp; Johannes&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Generative Adversarial Networks&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Nov 20-21 (Wed/Thu)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;5-8 p.m.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Seul Ah &amp;amp; Win&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Intro to Reinforcement Learning&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Feb 12+19, 2020 (Wed/Wed)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;5-8 p.m.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Anna K.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Kalman &amp;amp; particle filters&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Apr 29+30, 2020 (Wed/Thu)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;4-7 p.m.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Emma &amp;amp; Jeff&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Deep Reinforcement Learning&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;COVID BREAK&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;(⚈̥̥̥̥̥́⌢⚈̥̥̥̥̥̀)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Dec 15+16, 2021 (Wed/Thu)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;5-8 p.m.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Alex &amp;amp; Johannes&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Bayesian Neural Nets &amp;amp; BBVI&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Mar 7+10, 2022 (Mon/Thu)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;5-8 p.m.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;John &amp;amp; Zach&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Actor Critic Methods for RL&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Jun 21+22, 2022 (Tue/Wed)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;5-8 p.m.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Binxu &amp;amp; John&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Diffusion Generative Models&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Some suggestions for future topics:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Switching Linear Dynamical Systems&lt;/li&gt; &#xA; &lt;li&gt;LSTM/GRU&lt;/li&gt; &#xA; &lt;li&gt;ODE nets&lt;/li&gt; &#xA; &lt;li&gt;Hierarchical Dirichlet Processes&lt;/li&gt; &#xA; &lt;li&gt;Natural Gradient Descent&lt;/li&gt; &#xA; &lt;li&gt;Graph Neural Networks&lt;/li&gt; &#xA; &lt;li&gt;Meta-learning (A neat small paper: &lt;a href=&#34;https://paperswithcode.com/method/maml&#34;&gt;https://paperswithcode.com/method/maml&lt;/a&gt; )&lt;/li&gt; &#xA; &lt;li&gt;Vector-symbolic architectures and their link to cognitive neuroscience (Some sources: &lt;a href=&#34;https://arxiv.org/pdf/cs/0412059.pdf&#34;&gt;https://arxiv.org/pdf/cs/0412059.pdf&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/2001.11797.pdf&#34;&gt;https://arxiv.org/pdf/2001.11797.pdf&lt;/a&gt;, &lt;a href=&#34;https://www.hd-computing.com/&#34;&gt;https://www.hd-computing.com/&lt;/a&gt;, &lt;a href=&#34;https://bioengineeringcommunity.nature.com/posts/the-best-of-both-worlds-deep-learning-meets-vector-symbolic-ai&#34;&gt;https://bioengineeringcommunity.nature.com/posts/the-best-of-both-worlds-deep-learning-meets-vector-symbolic-ai&lt;/a&gt; )&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>