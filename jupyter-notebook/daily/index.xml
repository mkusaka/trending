<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-21T01:29:53Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>topoteretes/cognee</title>
    <updated>2024-03-21T01:29:53Z</updated>
    <id>tag:github.com,2024-03-21:/topoteretes/cognee</id>
    <link href="https://github.com/topoteretes/cognee" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Memory management for the AI Applications and AI Agents&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;cognee&lt;/h1&gt; &#xA;&lt;p&gt;Make data processing for LLMs easy&lt;/p&gt; &#xA;&lt;p&gt; &lt;a href=&#34;https://cognee.ai&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/topoteretes/cognee/main/assets/cognee-logo.png&#34; width=&#34;160px&#34; alt=&#34;Cognee logo&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt; &lt;i&gt;Open-source framework for creating knowledge graphs and data models for LLMs.&lt;/i&gt; &lt;/p&gt; &#xA;&lt;p&gt; &lt;a href=&#34;https://github.com/topoteretes/cognee/fork&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/forks/topoteretes/cognee?style=for-the-badge&#34; alt=&#34;cognee forks&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/topoteretes/cognee/stargazers&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/stars/topoteretes/cognee?style=for-the-badge&#34; alt=&#34;cognee stars&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/topoteretes/cognee/pulls&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/issues-pr/topoteretes/cognee?style=for-the-badge&#34; alt=&#34;cognee pull-requests&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/topoteretes/cognee/releases&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/release/topoteretes/cognee?&amp;amp;label=Latest&amp;amp;style=for-the-badge&#34; alt=&#34;cognee releases&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;ğŸš€ It&#39;s alive&lt;/h2&gt; &#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p&gt;Try it yourself on Whatsapp with one of our &lt;a href=&#34;https://keepi.ai&#34;&gt;partners&lt;/a&gt; by typing &lt;code&gt;/save {content you want to save}&lt;/code&gt; followed by &lt;code&gt;/query {knowledge you saved previously}&lt;/code&gt; For more info here are the &lt;a href=&#34;https://topoteretes.github.io/cognee/&#34;&gt;docs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ“¦ Installation&lt;/h2&gt; &#xA;&lt;p&gt;With pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install cognee&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;With poetry:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry add cognee&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ğŸ’» Usage&lt;/h2&gt; &#xA;&lt;p&gt;Check out our demo notebook &lt;a href=&#34;https://raw.githubusercontent.com/topoteretes/cognee/main/cognee%20-%20Get%20Started.ipynb&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Set OpenAI API Key as an environment variable&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;import os&#xA;&#xA;# Setting an environment variable&#xA;os.environ[&#39;OPENAI_API_KEY&#39;] = &#39;&#39;&#xA;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add a new piece of information to storage&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;import cognee&#xA;cognee.add(absolute_data_path, dataset_name)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Use LLMs and cognee to create graphs&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;cognee.cognify(dataset_name)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Render the graph after adding your Graphistry credentials to .env&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;graph_url = await render_graph(graph, graph_type = &#34;networkx&#34;)&#xA;print(graph_url)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Query the graph for a piece of information&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;query_params = {&#xA;    SearchType.SIMILARITY: {&#39;query&#39;: &#39;your search query here&#39;}&#xA;}&#xA;cognee.search(graph, query_params) &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/-ARUfIzhzC4&#34; title=&#34;Learn about cognee: 55&#34;&gt;&lt;img src=&#34;https://i3.ytimg.com/vi/-ARUfIzhzC4/maxresdefault.jpg&#34; width=&#34;100%&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Architecture&lt;/h2&gt; &#xA;&lt;h3&gt;How Cognee Enhances Your Contextual Memory&lt;/h3&gt; &#xA;&lt;p&gt;Our framework for the OpenAI, Graph (Neo4j) and Vector (Weaviate) databases introduces three key enhancements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Query Classifiers: Navigate information graph using Pydantic OpenAI classifiers.&lt;/li&gt; &#xA; &lt;li&gt;Document Topology: Structure and store documents in public and private domains.&lt;/li&gt; &#xA; &lt;li&gt;Personalized Context: Provide a context object to the LLM for a better response.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/topoteretes/cognee/main/assets/architecture.png&#34; alt=&#34;Image&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>datawhalechina/llm-cookbook</title>
    <updated>2024-03-21T01:29:53Z</updated>
    <id>tag:github.com,2024-03-21:/datawhalechina/llm-cookbook</id>
    <link href="https://github.com/datawhalechina/llm-cookbook" rel="alternate"></link>
    <summary type="html">&lt;p&gt;é¢å‘å¼€å‘è€…çš„ LLM å…¥é—¨æ•™ç¨‹ï¼Œå´æ©è¾¾å¤§æ¨¡å‹ç³»åˆ—è¯¾ç¨‹ä¸­æ–‡ç‰ˆ&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/datawhalechina/llm-cookbook/main/figures/readme.jpg&#34; alt=&#34;figures/readme.jpg&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;é¢å‘å¼€å‘è€…çš„å¤§æ¨¡å‹æ‰‹å†Œ - LLM Cookbook&lt;/h1&gt; &#xA;&lt;h2&gt;é¡¹ç›®ç®€ä»‹&lt;/h2&gt; &#xA;&lt;p&gt;æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªé¢å‘å¼€å‘è€…çš„å¤§æ¨¡å‹æ‰‹å†Œï¼Œé’ˆå¯¹å›½å†…å¼€å‘è€…çš„å®é™…éœ€æ±‚ï¼Œä¸»æ‰“ LLM å…¨æ–¹ä½å…¥é—¨å®è·µã€‚æœ¬é¡¹ç›®åŸºäºå´æ©è¾¾è€å¸ˆå¤§æ¨¡å‹ç³»åˆ—è¯¾ç¨‹å†…å®¹ï¼Œå¯¹åŸè¯¾ç¨‹å†…å®¹è¿›è¡Œç­›é€‰ã€ç¿»è¯‘ã€å¤ç°å’Œè°ƒä¼˜ï¼Œè¦†ç›–ä» Prompt Engineering åˆ° RAG å¼€å‘ã€æ¨¡å‹å¾®è°ƒçš„å…¨éƒ¨æµç¨‹ï¼Œç”¨æœ€é€‚åˆå›½å†…å­¦ä¹ è€…çš„æ–¹å¼ï¼ŒæŒ‡å¯¼å›½å†…å¼€å‘è€…å¦‚ä½•å­¦ä¹ ã€å…¥é—¨ LLM ç›¸å…³é¡¹ç›®ã€‚&lt;/p&gt; &#xA;&lt;p&gt;é’ˆå¯¹ä¸åŒå†…å®¹çš„ç‰¹ç‚¹ï¼Œæˆ‘ä»¬å¯¹å…±è®¡ 11 é—¨å´æ©è¾¾è€å¸ˆçš„å¤§æ¨¡å‹è¯¾ç¨‹è¿›è¡Œäº†ç¿»è¯‘å¤ç°ï¼Œå¹¶ç»“åˆå›½å†…å­¦ä¹ è€…çš„å®é™…æƒ…å†µï¼Œå¯¹ä¸åŒè¯¾ç¨‹è¿›è¡Œäº†åˆ†çº§å’Œæ’åºï¼Œåˆå­¦è€…å¯ä»¥å…ˆç³»ç»Ÿå­¦ä¹ æˆ‘ä»¬çš„å¿…ä¿®ç±»è¯¾ç¨‹ï¼ŒæŒæ¡å…¥é—¨ LLM æ‰€æœ‰æ–¹å‘éƒ½éœ€è¦æŒæ¡çš„åŸºç¡€æŠ€èƒ½å’Œæ¦‚å¿µï¼Œå†é€‰æ‹©æ€§åœ°å­¦ä¹ æˆ‘ä»¬çš„é€‰ä¿®ç±»è¯¾ç¨‹ï¼Œåœ¨è‡ªå·±æ„Ÿå…´è¶£çš„æ–¹å‘ä¸Šä¸æ–­æ¢ç´¢å’Œå­¦ä¹ ã€‚&lt;/p&gt; &#xA;&lt;p&gt;å¦‚æœæœ‰ä½ éå¸¸å–œæ¬¢ä½†æˆ‘ä»¬è¿˜æ²¡æœ‰è¿›è¡Œå¤ç°çš„å´æ©è¾¾è€å¸ˆå¤§æ¨¡å‹è¯¾ç¨‹ï¼Œæˆ‘ä»¬æ¬¢è¿æ¯ä¸€ä½å¼€å‘è€…å‚è€ƒæˆ‘ä»¬å·²æœ‰è¯¾ç¨‹çš„æ ¼å¼å’Œå†™æ³•æ¥å¯¹è¯¾ç¨‹è¿›è¡Œå¤ç°å¹¶æäº¤ PRï¼Œåœ¨ PR å®¡æ ¸é€šè¿‡åï¼Œæˆ‘ä»¬ä¼šæ ¹æ®è¯¾ç¨‹å†…å®¹å°†è¯¾ç¨‹è¿›è¡Œåˆ†çº§åˆå¹¶ã€‚æ¬¢è¿æ¯ä¸€ä½å¼€å‘è€…çš„è´¡çŒ®ï¼&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;åœ¨çº¿é˜…è¯»åœ°å€ï¼š&lt;a href=&#34;https://datawhalechina.github.io/llm-cookbook/&#34;&gt;é¢å‘å¼€å‘è€…çš„ LLM å…¥é—¨è¯¾ç¨‹-åœ¨çº¿é˜…è¯»&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PDFä¸‹è½½åœ°å€ï¼š&lt;a href=&#34;https://datawhalechina.github.io/llm-cookbook/releases&#34;&gt;é¢å‘å¼€å‘è€…çš„ LLM å…¥é—¨æ•™ç¨‹-PDF&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;è‹±æ–‡åŸç‰ˆåœ°å€ï¼š&lt;a href=&#34;https://learn.deeplearning.ai&#34;&gt;å´æ©è¾¾å…³äºå¤§æ¨¡å‹çš„ç³»åˆ—è¯¾ç¨‹&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;é¡¹ç›®æ„ä¹‰&lt;/h2&gt; &#xA;&lt;p&gt;LLM æ­£åœ¨é€æ­¥æ”¹å˜äººä»¬çš„ç”Ÿæ´»ï¼Œè€Œå¯¹äºå¼€å‘è€…ï¼Œå¦‚ä½•åŸºäº LLM æä¾›çš„ API å¿«é€Ÿã€ä¾¿æ·åœ°å¼€å‘ä¸€äº›å…·å¤‡æ›´å¼ºèƒ½åŠ›ã€é›†æˆLLM çš„åº”ç”¨ï¼Œæ¥ä¾¿æ·åœ°å®ç°ä¸€äº›æ›´æ–°é¢–ã€æ›´å®ç”¨çš„èƒ½åŠ›ï¼Œæ˜¯ä¸€ä¸ªæ€¥éœ€å­¦ä¹ çš„é‡è¦èƒ½åŠ›ã€‚&lt;/p&gt; &#xA;&lt;p&gt;ç”±å´æ©è¾¾è€å¸ˆä¸ OpenAI åˆä½œæ¨å‡ºçš„å¤§æ¨¡å‹ç³»åˆ—æ•™ç¨‹ï¼Œä»å¤§æ¨¡å‹æ—¶ä»£å¼€å‘è€…çš„åŸºç¡€æŠ€èƒ½å‡ºå‘ï¼Œæ·±å…¥æµ…å‡ºåœ°ä»‹ç»äº†å¦‚ä½•åŸºäºå¤§æ¨¡å‹ APIã€LangChain æ¶æ„å¿«é€Ÿå¼€å‘ç»“åˆå¤§æ¨¡å‹å¼ºå¤§èƒ½åŠ›çš„åº”ç”¨ã€‚å…¶ä¸­ï¼Œã€ŠPrompt Engineering for Developersã€‹æ•™ç¨‹é¢å‘å…¥é—¨ LLM çš„å¼€å‘è€…ï¼Œæ·±å…¥æµ…å‡ºåœ°ä»‹ç»äº†å¯¹äºå¼€å‘è€…ï¼Œå¦‚ä½•æ„é€  Prompt å¹¶åŸºäº OpenAI æä¾›çš„ API å®ç°åŒ…æ‹¬æ€»ç»“ã€æ¨æ–­ã€è½¬æ¢ç­‰å¤šç§å¸¸ç”¨åŠŸèƒ½ï¼Œæ˜¯å…¥é—¨ LLM å¼€å‘çš„ç»å…¸æ•™ç¨‹ï¼›ã€ŠBuilding Systems with the ChatGPT APIã€‹æ•™ç¨‹é¢å‘æƒ³è¦åŸºäº LLM å¼€å‘åº”ç”¨ç¨‹åºçš„å¼€å‘è€…ï¼Œç®€æ´æœ‰æ•ˆè€Œåˆç³»ç»Ÿå…¨é¢åœ°ä»‹ç»äº†å¦‚ä½•åŸºäº ChatGPT API æ‰“é€ å®Œæ•´çš„å¯¹è¯ç³»ç»Ÿï¼›ã€ŠLangChain for LLM Application Developmentã€‹æ•™ç¨‹ç»“åˆç»å…¸å¤§æ¨¡å‹å¼€æºæ¡†æ¶ LangChainï¼Œä»‹ç»äº†å¦‚ä½•åŸºäº LangChain æ¡†æ¶å¼€å‘å…·å¤‡å®ç”¨åŠŸèƒ½ã€èƒ½åŠ›å…¨é¢çš„åº”ç”¨ç¨‹åºï¼Œã€ŠLangChain Chat With Your Dataã€‹æ•™ç¨‹åˆ™åœ¨æ­¤åŸºç¡€ä¸Šè¿›ä¸€æ­¥ä»‹ç»äº†å¦‚ä½•ä½¿ç”¨ LangChain æ¶æ„ç»“åˆä¸ªäººç§æœ‰æ•°æ®å¼€å‘ä¸ªæ€§åŒ–å¤§æ¨¡å‹åº”ç”¨ï¼›ã€ŠBuilding Generative AI Applications with Gradioã€‹ã€ã€ŠEvaluating and Debugging Generative AIã€‹æ•™ç¨‹åˆ†åˆ«ä»‹ç»äº†ä¸¤ä¸ªå®ç”¨å·¥å…· Gradio ä¸ W&amp;amp;Bï¼ŒæŒ‡å¯¼å¼€å‘è€…å¦‚ä½•ç»“åˆè¿™ä¸¤ä¸ªå·¥å…·æ¥æ‰“é€ ã€è¯„ä¼°ç”Ÿæˆå¼ AI åº”ç”¨ã€‚&lt;/p&gt; &#xA;&lt;p&gt;ä¸Šè¿°æ•™ç¨‹éå¸¸é€‚ç”¨äºå¼€å‘è€…å­¦ä¹ ä»¥å¼€å¯åŸºäº LLM å®é™…æ­å»ºåº”ç”¨ç¨‹åºä¹‹è·¯ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†è¯¥ç³»åˆ—è¯¾ç¨‹ç¿»è¯‘ä¸ºä¸­æ–‡ï¼Œå¹¶å¤ç°å…¶èŒƒä¾‹ä»£ç ï¼Œä¹Ÿä¸ºå…¶ä¸­ä¸€ä¸ªè§†é¢‘å¢åŠ äº†ä¸­æ–‡å­—å¹•ï¼Œæ”¯æŒå›½å†…ä¸­æ–‡å­¦ä¹ è€…ç›´æ¥ä½¿ç”¨ï¼Œä»¥å¸®åŠ©ä¸­æ–‡å­¦ä¹ è€…æ›´å¥½åœ°å­¦ä¹  LLM å¼€å‘ï¼›æˆ‘ä»¬ä¹ŸåŒæ—¶å®ç°äº†æ•ˆæœå¤§è‡´ç›¸å½“çš„ä¸­æ–‡ Promptï¼Œæ”¯æŒå­¦ä¹ è€…æ„Ÿå—ä¸­æ–‡è¯­å¢ƒä¸‹ LLM çš„å­¦ä¹ ä½¿ç”¨ï¼Œå¯¹æ¯”æŒæ¡å¤šè¯­è¨€è¯­å¢ƒä¸‹çš„ Prompt è®¾è®¡ä¸ LLM å¼€å‘ã€‚æœªæ¥ï¼Œæˆ‘ä»¬ä¹Ÿå°†åŠ å…¥æ›´å¤š Prompt é«˜çº§æŠ€å·§ï¼Œä»¥ä¸°å¯Œæœ¬è¯¾ç¨‹å†…å®¹ï¼Œå¸®åŠ©å¼€å‘è€…æŒæ¡æ›´å¤šã€æ›´å·§å¦™çš„ Prompt æŠ€èƒ½ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;é¡¹ç›®å—ä¼—&lt;/h2&gt; &#xA;&lt;p&gt;æ‰€æœ‰å…·å¤‡åŸºç¡€ Python èƒ½åŠ›ï¼Œæƒ³è¦å…¥é—¨ LLM çš„å¼€å‘è€…ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;é¡¹ç›®äº®ç‚¹&lt;/h2&gt; &#xA;&lt;p&gt;ã€ŠChatGPT Prompt Engineering for Developersã€‹ã€ã€ŠBuilding Systems with the ChatGPT APIã€‹ç­‰æ•™ç¨‹ä½œä¸ºç”±å´æ©è¾¾è€å¸ˆä¸ OpenAI è”åˆæ¨å‡ºçš„å®˜æ–¹æ•™ç¨‹ï¼Œåœ¨å¯é¢„è§çš„æœªæ¥ä¼šæˆä¸º LLM çš„é‡è¦å…¥é—¨æ•™ç¨‹ï¼Œä½†æ˜¯ç›®å‰è¿˜åªæ”¯æŒè‹±æ–‡ç‰ˆä¸”å›½å†…è®¿é—®å—é™ï¼Œæ‰“é€ ä¸­æ–‡ç‰ˆä¸”å›½å†…æµç•…è®¿é—®çš„æ•™ç¨‹å…·æœ‰é‡è¦æ„ä¹‰ï¼›åŒæ—¶ï¼ŒGPT å¯¹ä¸­æ–‡ã€è‹±æ–‡å…·æœ‰ä¸åŒçš„ç†è§£èƒ½åŠ›ï¼Œæœ¬æ•™ç¨‹åœ¨å¤šæ¬¡å¯¹æ¯”ã€å®éªŒä¹‹åç¡®å®šäº†æ•ˆæœå¤§è‡´ç›¸å½“çš„ä¸­æ–‡ Promptï¼Œæ”¯æŒå­¦ä¹ è€…ç ”ç©¶å¦‚ä½•æå‡ ChatGPT åœ¨ä¸­æ–‡è¯­å¢ƒä¸‹çš„ç†è§£ä¸ç”Ÿæˆèƒ½åŠ›ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;å­¦ä¹ æŒ‡å—&lt;/h2&gt; &#xA;&lt;p&gt;æœ¬æ•™ç¨‹é€‚ç”¨äºæ‰€æœ‰å…·å¤‡åŸºç¡€ Python èƒ½åŠ›ï¼Œæƒ³è¦å…¥é—¨ LLM çš„å¼€å‘è€…ã€‚&lt;/p&gt; &#xA;&lt;p&gt;å¦‚æœä½ æƒ³è¦å¼€å§‹å­¦ä¹ æœ¬æ•™ç¨‹ï¼Œä½ éœ€è¦æå‰å…·å¤‡ï¼š&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;è‡³å°‘ä¸€ä¸ª LLM APIï¼ˆæœ€å¥½æ˜¯ OpenAIï¼Œå¦‚æœæ˜¯å…¶ä»– APIï¼Œä½ å¯èƒ½éœ€è¦å‚è€ƒ&lt;a href=&#34;https://github.com/datawhalechina/llm-universe&#34;&gt;å…¶ä»–æ•™ç¨‹&lt;/a&gt;å¯¹ API è°ƒç”¨ä»£ç è¿›è¡Œä¿®æ”¹ï¼‰&lt;/li&gt; &#xA; &lt;li&gt;èƒ½å¤Ÿä½¿ç”¨ Python Jupyter Notebook&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;æœ¬æ•™ç¨‹å…±åŒ…æ‹¬ 11 é—¨è¯¾ç¨‹ï¼Œåˆ†ä¸ºå¿…ä¿®ç±»ã€é€‰ä¿®ç±»ä¸¤ä¸ªç±»åˆ«ã€‚å¿…ä¿®ç±»è¯¾ç¨‹æ˜¯æˆ‘ä»¬è®¤ä¸ºæœ€é€‚åˆåˆå­¦è€…å­¦ä¹ ä»¥å…¥é—¨ LLM çš„è¯¾ç¨‹ï¼ŒåŒ…æ‹¬äº†å…¥é—¨ LLM æ‰€æœ‰æ–¹å‘éƒ½éœ€è¦æŒæ¡çš„åŸºç¡€æŠ€èƒ½å’Œæ¦‚å¿µï¼Œæˆ‘ä»¬ä¹Ÿé’ˆå¯¹å¿…ä¿®ç±»è¯¾ç¨‹åˆ¶ä½œäº†é€‚åˆé˜…è¯»çš„åœ¨çº¿é˜…è¯»å’Œ PDF ç‰ˆæœ¬ï¼Œåœ¨å­¦ä¹ å¿…ä¿®ç±»è¯¾ç¨‹æ—¶ï¼Œæˆ‘ä»¬å»ºè®®å­¦ä¹ è€…æŒ‰ç…§æˆ‘ä»¬åˆ—å‡ºçš„é¡ºåºè¿›è¡Œå­¦ä¹ ï¼›é€‰ä¿®ç±»è¯¾ç¨‹æ˜¯åœ¨å¿…ä¿®ç±»è¯¾ç¨‹ä¸Šçš„æ‹“å±•å»¶ä¼¸ï¼ŒåŒ…æ‹¬äº† RAG å¼€å‘ã€æ¨¡å‹å¾®è°ƒã€æ¨¡å‹è¯„ä¼°ç­‰å¤šä¸ªæ–¹é¢ï¼Œé€‚åˆå­¦ä¹ è€…åœ¨æŒæ¡äº†å¿…ä¿®ç±»è¯¾ç¨‹ä¹‹åé€‰æ‹©è‡ªå·±æ„Ÿå…´è¶£çš„æ–¹å‘å’Œè¯¾ç¨‹è¿›è¡Œå­¦ä¹ ã€‚&lt;/p&gt; &#xA;&lt;p&gt;å¿…ä¿®ç±»è¯¾ç¨‹åŒ…æ‹¬ï¼š&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;é¢å‘å¼€å‘è€…çš„ Prompt Engineeringã€‚åŸºäºå´æ©è¾¾è€å¸ˆã€ŠChatGPT Prompt Engineering for Developersã€‹è¯¾ç¨‹æ‰“é€ ï¼Œé¢å‘å…¥é—¨ LLM çš„å¼€å‘è€…ï¼Œæ·±å…¥æµ…å‡ºåœ°ä»‹ç»äº†å¯¹äºå¼€å‘è€…ï¼Œå¦‚ä½•æ„é€  Prompt å¹¶åŸºäº OpenAI æä¾›çš„ API å®ç°åŒ…æ‹¬æ€»ç»“ã€æ¨æ–­ã€è½¬æ¢ç­‰å¤šç§å¸¸ç”¨åŠŸèƒ½ï¼Œæ˜¯å…¥é—¨ LLM å¼€å‘çš„ç¬¬ä¸€æ­¥ã€‚&lt;/li&gt; &#xA; &lt;li&gt;æ­å»ºåŸºäº ChatGPT çš„é—®ç­”ç³»ç»Ÿã€‚åŸºäºå´æ©è¾¾è€å¸ˆã€ŠBuilding Systems with the ChatGPT APIã€‹è¯¾ç¨‹æ‰“é€ ï¼ŒæŒ‡å¯¼å¼€å‘è€…å¦‚ä½•åŸºäº ChatGPT æä¾›çš„ API å¼€å‘ä¸€ä¸ªå®Œæ•´çš„ã€å…¨é¢çš„æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€‚é€šè¿‡ä»£ç å®è·µï¼Œå®ç°äº†åŸºäº ChatGPT å¼€å‘é—®ç­”ç³»ç»Ÿçš„å…¨æµç¨‹ï¼Œä»‹ç»äº†åŸºäºå¤§æ¨¡å‹å¼€å‘çš„æ–°èŒƒå¼ï¼Œæ˜¯å¤§æ¨¡å‹å¼€å‘çš„å®è·µåŸºç¡€ã€‚&lt;/li&gt; &#xA; &lt;li&gt;ä½¿ç”¨ LangChain å¼€å‘åº”ç”¨ç¨‹åºã€‚åŸºäºå´æ©è¾¾è€å¸ˆã€ŠLangChain for LLM Application Developmentã€‹è¯¾ç¨‹æ‰“é€ ï¼Œå¯¹ LangChain å±•å¼€æ·±å…¥ä»‹ç»ï¼Œå¸®åŠ©å­¦ä¹ è€…äº†è§£å¦‚ä½•ä½¿ç”¨ LangChainï¼Œå¹¶åŸºäº LangChain å¼€å‘å®Œæ•´çš„ã€å…·å¤‡å¼ºå¤§èƒ½åŠ›çš„åº”ç”¨ç¨‹åºã€‚&lt;/li&gt; &#xA; &lt;li&gt;ä½¿ç”¨ LangChain è®¿é—®ä¸ªäººæ•°æ®ã€‚åŸºäºå´æ©è¾¾è€å¸ˆã€ŠLangChain Chat with Your Dataã€‹è¯¾ç¨‹æ‰“é€ ï¼Œæ·±å…¥æ‹“å±• LangChain æä¾›çš„ä¸ªäººæ•°æ®è®¿é—®èƒ½åŠ›ï¼ŒæŒ‡å¯¼å¼€å‘è€…å¦‚ä½•ä½¿ç”¨ LangChain å¼€å‘èƒ½å¤Ÿè®¿é—®ç”¨æˆ·ä¸ªäººæ•°æ®ã€æä¾›ä¸ªæ€§åŒ–æœåŠ¡çš„å¤§æ¨¡å‹åº”ç”¨ã€‚&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;é€‰ä¿®ç±»è¯¾ç¨‹åŒ…æ‹¬ï¼š&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;ä½¿ç”¨ Gradio æ­å»ºç”Ÿæˆå¼ AI åº”ç”¨ã€‚åŸºäºå´æ©è¾¾è€å¸ˆã€ŠBuilding Generative AI Applications with Gradioã€‹è¯¾ç¨‹æ‰“é€ ï¼ŒæŒ‡å¯¼å¼€å‘è€…å¦‚ä½•ä½¿ç”¨ Gradio é€šè¿‡ Python æ¥å£ç¨‹åºå¿«é€Ÿã€é«˜æ•ˆåœ°ä¸ºç”Ÿæˆå¼ AI æ„å»ºç”¨æˆ·ç•Œé¢ã€‚&lt;/li&gt; &#xA; &lt;li&gt;è¯„ä¼°æ”¹è¿›ç”Ÿæˆå¼ AIã€‚åŸºäºå´æ©è¾¾è€å¸ˆã€ŠEvaluating and Debugging Generative AIã€‹è¯¾ç¨‹æ‰“é€ ï¼Œç»“åˆ wandbï¼Œæä¾›ä¸€å¥—ç³»ç»ŸåŒ–çš„æ–¹æ³•å’Œå·¥å…·ï¼Œå¸®åŠ©å¼€å‘è€…æœ‰æ•ˆåœ°è·Ÿè¸ªå’Œè°ƒè¯•ç”Ÿæˆå¼ AI æ¨¡å‹ã€‚&lt;/li&gt; &#xA; &lt;li&gt;å¾®è°ƒå¤§è¯­è¨€æ¨¡å‹ã€‚åŸºäºå´æ©è¾¾è€å¸ˆã€ŠFinetuning Large Language Modelã€‹è¯¾ç¨‹æ‰“é€ ï¼Œç»“åˆ lamini æ¡†æ¶ï¼Œè®²è¿°å¦‚ä½•ä¾¿æ·é«˜æ•ˆåœ°åœ¨æœ¬åœ°åŸºäºä¸ªäººæ•°æ®å¾®è°ƒå¼€æºå¤§è¯­è¨€æ¨¡å‹ã€‚&lt;/li&gt; &#xA; &lt;li&gt;å¤§æ¨¡å‹ä¸è¯­ä¹‰æ£€ç´¢ã€‚åŸºäºå´æ©è¾¾è€å¸ˆã€ŠLarge Language Models with Semantic Searchã€‹è¯¾ç¨‹æ‰“é€ ï¼Œé’ˆå¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆï¼Œè®²è¿°äº†å¤šç§é«˜çº§æ£€ç´¢æŠ€å·§ä»¥å®ç°æ›´å‡†ç¡®ã€é«˜æ•ˆçš„æ£€ç´¢å¢å¼º LLM ç”Ÿæˆæ•ˆæœã€‚&lt;/li&gt; &#xA; &lt;li&gt;åŸºäº Chroma çš„é«˜çº§æ£€ç´¢ã€‚åŸºäºå´æ©è¾¾è€å¸ˆã€ŠAdvanced Retrieval for AI with Chromaã€‹è¯¾ç¨‹æ‰“é€ ï¼Œæ—¨åœ¨ä»‹ç»åŸºäº Chroma çš„é«˜çº§æ£€ç´¢æŠ€æœ¯ï¼Œæå‡æ£€ç´¢ç»“æœçš„å‡†ç¡®æ€§ã€‚&lt;/li&gt; &#xA; &lt;li&gt;æ­å»ºå’Œè¯„ä¼°é«˜çº§ RAG åº”ç”¨ã€‚åŸºäºå´æ©è¾¾è€å¸ˆã€ŠBuilding and Evaluating Advanced RAG Applicationsã€‹è¯¾ç¨‹æ‰“é€ ï¼Œä»‹ç»æ„å»ºå’Œå®ç°é«˜è´¨é‡RAGç³»ç»Ÿæ‰€éœ€çš„å…³é”®æŠ€æœ¯å’Œè¯„ä¼°æ¡†æ¶ã€‚&lt;/li&gt; &#xA; &lt;li&gt;LangChain çš„ Functionsã€Tools å’Œ Agentsã€‚åŸºäºå´æ©è¾¾è€å¸ˆã€ŠFunctions, Tools and Agents with LangChainã€‹è¯¾ç¨‹æ‰“é€ ï¼Œä»‹ç»å¦‚ä½•åŸºäº LangChain çš„æ–°è¯­æ³•æ„å»º Agentã€‚&lt;/li&gt; &#xA; &lt;li&gt;Prompt é«˜çº§æŠ€å·§ã€‚åŸåˆ›å†…å®¹ï¼Œåˆ›ä½œä¸­ã€‚&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;å…¶ä»–èµ„æ–™åŒ…æ‹¬ï¼š&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;åŒè¯­å­—å¹•è§†é¢‘åœ°å€ï¼š&lt;a href=&#34;https://www.bilibili.com/video/BV1Bo4y1A7FU/?share_source=copy_web&#34;&gt;å´æ©è¾¾ x OpenAIçš„Prompt Engineeringè¯¾ç¨‹ä¸“ä¸šç¿»è¯‘ç‰ˆ&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ä¸­è‹±åŒè¯­å­—å¹•ä¸‹è½½ï¼š&lt;a href=&#34;https://github.com/GitHubDaily/ChatGPT-Prompt-Engineering-for-Developers-in-Chinese&#34;&gt;ã€ŠChatGPTæç¤ºå·¥ç¨‹ã€‹éå®˜æ–¹ç‰ˆä¸­è‹±åŒè¯­å­—å¹•&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;è§†é¢‘è®²è§£ï¼š&lt;a href=&#34;https://www.bilibili.com/video/BV1PN4y1k7y2/?spm_id_from=333.999.0.0&#34;&gt;é¢å‘å¼€å‘è€…çš„ Prompt Engineering è®²è§£ï¼ˆæ•°å­—æ¸¸æ°‘å¤§ä¼šï¼‰&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;ç›®å½•ç»“æ„è¯´æ˜ï¼š&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;contentï¼šåŸºäºåŸè¯¾ç¨‹å¤ç°çš„åŒè¯­ç‰ˆä»£ç ï¼Œå¯è¿è¡Œçš„ Notebookï¼Œæ›´æ–°é¢‘ç‡æœ€é«˜ï¼Œæ›´æ–°é€Ÿåº¦æœ€å¿«ã€‚&#xA;&#xA;docsï¼šå¿…ä¿®ç±»è¯¾ç¨‹æ–‡å­—æ•™ç¨‹ç‰ˆåœ¨çº¿é˜…è¯»æºç ï¼Œé€‚åˆé˜…è¯»çš„ mdã€‚&#xA;&#xA;figuresï¼šå›¾ç‰‡æ–‡ä»¶ã€‚&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;è‡´è°¢&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;æ ¸å¿ƒè´¡çŒ®è€…&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/logan-zou&#34;&gt;é‚¹é›¨è¡¡-é¡¹ç›®è´Ÿè´£äºº&lt;/a&gt;ï¼ˆDatawhaleæˆå‘˜-å¯¹å¤–ç»æµè´¸æ˜“å¤§å­¦ç ”ç©¶ç”Ÿï¼‰&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://yam.gift/&#34;&gt;é•¿ç´-é¡¹ç›®å‘èµ·äºº&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæˆå‘˜-AIç®—æ³•å·¥ç¨‹å¸ˆï¼‰&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Sophia-Huang&#34;&gt;ç‰ç³-é¡¹ç›®å‘èµ·äºº&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæˆå‘˜ï¼‰&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/xuhu0115&#34;&gt;å¾è™-æ•™ç¨‹ç¼–æ’°è€…&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæˆå‘˜ï¼‰&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Weihong-Liu&#34;&gt;åˆ˜ä¼Ÿé¸¿-æ•™ç¨‹ç¼–æ’°è€…&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-æ±Ÿå—å¤§å­¦éå…¨ç ”ç©¶ç”Ÿï¼‰&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://Joyenjoye.com&#34;&gt;Joye-æ•™ç¨‹ç¼–æ’°è€…&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-æ•°æ®ç§‘å­¦å®¶ï¼‰&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/0-yy-0&#34;&gt;é«˜ç«‹ä¸š&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-DataWhaleæˆå‘˜-ç®—æ³•å·¥ç¨‹å¸ˆï¼‰&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/GKDGKD&#34;&gt;é‚“å®‡æ–‡&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæˆå‘˜ï¼‰&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/wisdom-pan&#34;&gt;é­‚å…®&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-å‰ç«¯å·¥ç¨‹å¸ˆï¼‰&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KMnO4-zx&#34;&gt;å®‹å¿—å­¦&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæˆå‘˜ï¼‰&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/YikunHan42&#34;&gt;éŸ©é¢å ƒ&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæˆå‘˜ï¼‰&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/6forwater29&#34;&gt;é™ˆé€¸æ¶µ&lt;/a&gt; (å†…å®¹åˆ›ä½œè€…-Datawhaleæ„å‘æˆå‘˜-AIçˆ±å¥½è€…)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ztgg0228&#34;&gt;ä»²æ³°&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæˆå‘˜ï¼‰&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/leason-wan&#34;&gt;ä¸‡ç¤¼è¡Œ&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-è§†é¢‘ç¿»è¯‘è€…ï¼‰&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Bald0Wang&#34;&gt;ç‹ç† æ˜&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæˆå‘˜ï¼‰&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://yetingyun.blog.csdn.net&#34;&gt;æ›¾æµ©é¾™&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhale æ„å‘æˆå‘˜-JLU AI ç ”ç©¶ç”Ÿï¼‰&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/xinqi-fan&#34;&gt;å°é¥­åŒå­¦&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…ï¼‰&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sunhanyu714%5D&#34;&gt;å­™éŸ©ç‰&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-ç®—æ³•é‡åŒ–éƒ¨ç½²å·¥ç¨‹å¸ˆï¼‰&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/YinHan-Zhang&#34;&gt;å¼ é“¶æ™—&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæˆå‘˜ï¼‰&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/LinChentang&#34;&gt;å·¦æ˜¥ç”Ÿ&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæˆå‘˜ï¼‰&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Jin-Zhang-Yaoguang&#34;&gt;å¼ æ™‹&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæˆå‘˜ï¼‰&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Aphasia0515&#34;&gt;æå¨‡å¨‡&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæˆå‘˜ï¼‰&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Kedreamix&#34;&gt;é‚“æºä¿Š&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæˆå‘˜ï¼‰&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Zhiyuan-Fan&#34;&gt;èŒƒè‡´è¿œ&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæˆå‘˜ï¼‰&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Beyondzjl&#34;&gt;å‘¨æ™¯æ—&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæˆå‘˜ï¼‰&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/very-very-very&#34;&gt;è¯¸ä¸–çºª&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-ç®—æ³•å·¥ç¨‹å¸ˆï¼‰&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/YixinZ-NUS&#34;&gt;Zhang Yixin&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-ITçˆ±å¥½è€…ï¼‰&lt;/li&gt; &#xA; &lt;li&gt;Saraiï¼ˆå†…å®¹åˆ›ä½œè€…-AIåº”ç”¨çˆ±å¥½è€…ï¼‰&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;å…¶ä»–&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;ç‰¹åˆ«æ„Ÿè°¢ &lt;a href=&#34;https://github.com/Sm1les&#34;&gt;@Sm1les&lt;/a&gt;ã€&lt;a href=&#34;https://github.com/LSGOMYP&#34;&gt;@LSGOMYP&lt;/a&gt; å¯¹æœ¬é¡¹ç›®çš„å¸®åŠ©ä¸æ”¯æŒï¼›&lt;/li&gt; &#xA; &lt;li&gt;æ„Ÿè°¢ &lt;a href=&#34;https://github.com/GitHubDaily&#34;&gt;GithubDaily&lt;/a&gt; æä¾›çš„åŒè¯­å­—å¹•ï¼›&lt;/li&gt; &#xA; &lt;li&gt;å¦‚æœæœ‰ä»»ä½•æƒ³æ³•å¯ä»¥è”ç³»æˆ‘ä»¬ DataWhale ä¹Ÿæ¬¢è¿å¤§å®¶å¤šå¤šæå‡º issueï¼›&lt;/li&gt; &#xA; &lt;li&gt;ç‰¹åˆ«æ„Ÿè°¢ä»¥ä¸‹ä¸ºæ•™ç¨‹åšå‡ºè´¡çŒ®çš„åŒå­¦ï¼&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;a href=&#34;https://datawhalechina.github.io/llm-cookbook/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=datawhalechina/llm-cookbook&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;Made with &lt;a href=&#34;https://contrib.rocks&#34;&gt;contrib.rocks&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#datawhalechina/llm-cookbook&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=datawhalechina/llm-cookbook&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;å…³æ³¨æˆ‘ä»¬&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;æ‰«æä¸‹æ–¹äºŒç»´ç å…³æ³¨å…¬ä¼—å·ï¼šDatawhale&lt;/p&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/datawhalechina/llm-cookbook/main/figures/qrcode.jpeg&#34; width=&#34;180&#34; height=&#34;180&#34;&gt; &#xA;&lt;/div&gt; Datawhale æ˜¯ä¸€ä¸ªä¸“æ³¨äºæ•°æ®ç§‘å­¦ä¸ AI é¢†åŸŸçš„å¼€æºç»„ç»‡ï¼Œæ±‡é›†äº†ä¼—å¤šé¢†åŸŸé™¢æ ¡å’ŒçŸ¥åä¼ä¸šçš„ä¼˜ç§€å­¦ä¹ è€…ï¼Œèšåˆäº†ä¸€ç¾¤æœ‰å¼€æºç²¾ç¥å’Œæ¢ç´¢ç²¾ç¥çš„å›¢é˜Ÿæˆå‘˜ã€‚å¾®ä¿¡æœç´¢å…¬ä¼—å·Datawhaleå¯ä»¥åŠ å…¥æˆ‘ä»¬ã€‚ &#xA;&lt;h2&gt;LICENSE&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a rel=&#34;license&#34; href=&#34;http://creativecommons.org/licenses/by-nc-sa/4.0/&#34;&gt;&lt;img alt=&#34;çŸ¥è¯†å…±äº«è®¸å¯åè®®&#34; style=&#34;border-width:0&#34; src=&#34;https://img.shields.io/badge/license-CC%20BY--NC--SA%204.0-lightgrey&#34;&gt;&lt;/a&gt;&lt;br&gt;æœ¬ä½œå“é‡‡ç”¨&lt;a rel=&#34;license&#34; href=&#34;http://creativecommons.org/licenses/by-nc-sa/4.0/&#34;&gt;çŸ¥è¯†å…±äº«ç½²å-éå•†ä¸šæ€§ä½¿ç”¨-ç›¸åŒæ–¹å¼å…±äº« 4.0 å›½é™…è®¸å¯åè®®&lt;/a&gt;è¿›è¡Œè®¸å¯ã€‚&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mrdbourke/simple-local-rag</title>
    <updated>2024-03-21T01:29:53Z</updated>
    <id>tag:github.com,2024-03-21:/mrdbourke/simple-local-rag</id>
    <link href="https://github.com/mrdbourke/simple-local-rag" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Build a RAG (Retrieval Augmented Generation) pipeline from scratch and have it all run locally.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Simple Local RAG Tutorial&lt;/h1&gt; &#xA;&lt;p&gt;Local RAG pipeline we&#39;re going to build:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mrdbourke/simple-local-rag/main/images/simple-local-rag-workflow-flowchart.png&#34; alt=&#34;&amp;quot;This is a flowchart describing a simple local retrieval-augmented generation (RAG) workflow for document processing and embedding creation, followed by search and answer functionality. The process begins with a collection of documents, such as PDFs or a 1200-page nutrition textbook, which are preprocessed into smaller chunks, for example, groups of 10 sentences each. These chunks are used as context for the Large Language Model (LLM). A cool person (potentially the user) asks a query such as &amp;quot;What are the macronutrients? And what do they do?&amp;quot; This query is then transformed by an embedding model into a numerical representation using sentence transformers or other options from Hugging Face, which are stored in a torch.tensor format for efficiency, especially with large numbers of embeddings (around 100k+). For extremely large datasets, a vector database/index may be used. The numerical query and relevant document passages are processed on a local GPU, specifically an RTX 4090. The LLM generates output based on the context related to the query, which can be interacted with through an optional chat web app interface. All of this processing happens on a local GPU. The flowchart includes icons for documents, processing steps, and hardware, with arrows indicating the flow from document collection to user interaction with the generated text and resources.&amp;quot;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;All designed to run locally on a NVIDIA GPU.&lt;/p&gt; &#xA;&lt;p&gt;All the way from PDF ingestion to &#34;chat with PDF&#34; style features.&lt;/p&gt; &#xA;&lt;p&gt;All using open-source tools.&lt;/p&gt; &#xA;&lt;p&gt;In our specific example, we&#39;ll build NutriChat, a RAG workflow that allows a person to query a 1200 page PDF version of a Nutrition Textbook and have an LLM generate responses back to the query based on passages of text from the textbook.&lt;/p&gt; &#xA;&lt;p&gt;PDF source: &lt;a href=&#34;https://pressbooks.oer.hawaii.edu/humannutrition2/&#34;&gt;https://pressbooks.oer.hawaii.edu/humannutrition2/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can also run notebook &lt;code&gt;00-simple-local-rag.ipynb&lt;/code&gt; directly in &lt;a href=&#34;https://colab.research.google.com/github/mrdbourke/simple-local-rag/blob/main/00-simple-local-rag.ipynb&#34;&gt;Google Colab&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;TODO:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Finish setup instructions&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Make header image of workflow&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add intro to RAG info in README?&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add extensions to README&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Record video of code writing/walkthrough - DONE, follow along with each line of code on YouTube: &lt;a href=&#34;https://youtu.be/qN_2fnOPY-M&#34;&gt;https://youtu.be/qN_2fnOPY-M&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Two main options:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;If you have a local NVIDIA GPU with 5GB+ VRAM, follow the steps below to have this pipeline run locally on your machine.&lt;/li&gt; &#xA; &lt;li&gt;If you donâ€™t have a local NVIDIA GPU, you can follow along in Google Colab and have it run on a NVIDIA GPU there.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Comfortable writing Python code.&lt;/li&gt; &#xA; &lt;li&gt;1-2 beginner machine learning/deep learning courses.&lt;/li&gt; &#xA; &lt;li&gt;Familiarity with PyTorch, see my &lt;a href=&#34;https://youtu.be/Z_ikDlimN6A?si=NIkrslkvHaNdlYgx&#34;&gt;beginner PyTorch video&lt;/a&gt; for more.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;Note: Tested in Python 3.11, running on Windows 11 with a NVIDIA RTX 4090 with CUDA 12.1.&lt;/p&gt; &#xA;&lt;h3&gt;Clone repo&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/mrdbourke/simple-local-rag.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd simple-local-rag&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Create environment&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m venv venv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Activate environment&lt;/h3&gt; &#xA;&lt;p&gt;Linux/macOS:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;source venv/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Windows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;.\venv\Scripts\activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Install requirements&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; I found I had to install &lt;code&gt;torch&lt;/code&gt; manually (&lt;code&gt;torch&lt;/code&gt; 2.1.1+ is required for newer versions of attention for faster inference) with CUDA, see: &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;https://pytorch.org/get-started/locally/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;On Windows I used:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip3 install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Launch notebook&lt;/h3&gt; &#xA;&lt;p&gt;VS Code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;code .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Jupyter Notebook&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;jupyter notebook&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Setup notes:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you run into any install/setup troubles, please leave an issue.&lt;/li&gt; &#xA; &lt;li&gt;To get access to the Gemma LLM models, you will have to &lt;a href=&#34;https://huggingface.co/google/gemma-7b-it&#34;&gt;agree to the terms &amp;amp; conditions&lt;/a&gt; on the Gemma model page on Hugging Face. You will then have to authorize your local machine via the &lt;a href=&#34;https://huggingface.co/docs/huggingface_hub/en/quick-start#authentication&#34;&gt;Hugging Face CLI/Hugging Face Hub &lt;code&gt;login()&lt;/code&gt; function&lt;/a&gt;. Once you&#39;ve done this, you&#39;ll be able to download the models. If you&#39;re using Google Colab, you can add a &lt;a href=&#34;https://huggingface.co/docs/hub/en/security-tokens&#34;&gt;Hugging Face token&lt;/a&gt; to the &#34;Secrets&#34; tab.&lt;/li&gt; &#xA; &lt;li&gt;For speedups, installing and compiling Flash Attention 2 (faster attention implementation) can take ~5 minutes to 3 hours depending on your system setup. See the &lt;a href=&#34;https://github.com/Dao-AILab/flash-attention/tree/main&#34;&gt;Flash Attention 2 GitHub&lt;/a&gt; for more. In particular, if you&#39;re running on Windows, see this &lt;a href=&#34;https://github.com/Dao-AILab/flash-attention/issues/595&#34;&gt;GitHub issue thread&lt;/a&gt;. I&#39;ve commented out &lt;code&gt;flash-attn&lt;/code&gt; in the requirements.txt due to compile time, feel free to uncomment if you&#39;d like use it or run &lt;code&gt;pip install flash-attn&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What is RAG?&lt;/h2&gt; &#xA;&lt;p&gt;RAG stands for Retrieval Augmented Generation.&lt;/p&gt; &#xA;&lt;p&gt;It was introduced in the paper &lt;a href=&#34;https://arxiv.org/abs/2005.11401&#34;&gt;&lt;em&gt;Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Each step can be roughly broken down to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Retrieval&lt;/strong&gt; - Seeking relevant information from a source given a query. For example, getting relevant passages of Wikipedia text from a database given a question.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Augmented&lt;/strong&gt; - Using the relevant retrieved information to modify an input to a generative model (e.g. an LLM).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Generation&lt;/strong&gt; - Generating an output given an input. For example, in the case of an LLM, generating a passage of text given an input prompt.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Why RAG?&lt;/h2&gt; &#xA;&lt;p&gt;The main goal of RAG is to improve the generation outptus of LLMs.&lt;/p&gt; &#xA;&lt;p&gt;Two primary improvements can be seen as:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Preventing hallucinations&lt;/strong&gt; - LLMs are incredible but they are prone to potential hallucination, as in, generating something that &lt;em&gt;looks&lt;/em&gt; correct but isn&#39;t. RAG pipelines can help LLMs generate more factual outputs by providing them with factual (retrieved) inputs. And even if the generated answer from a RAG pipeline doesn&#39;t seem correct, because of retrieval, you also have access to the sources where it came from.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Work with custom data&lt;/strong&gt; - Many base LLMs are trained with internet-scale text data. This means they have a great ability to model language, however, they often lack specific knowledge. RAG systems can provide LLMs with domain-specific data such as medical information or company documentation and thus customized their outputs to suit specific use cases.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The authors of the original RAG paper mentioned above outlined these two points in their discussion.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;This work offers several positive societal benefits over previous work: the fact that it is more strongly grounded in real factual knowledge (in this case Wikipedia) makes it â€œhallucinateâ€ less with generations that are more factual, and offers more control and interpretability. RAG could be employed in a wide variety of scenarios with direct benefit to society, for example by endowing it with a medical index and asking it open-domain questions on that topic, or by helping people be more effective at their jobs.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;RAG can also be a much quicker solution to implement than fine-tuning an LLM on specific data.&lt;/p&gt; &#xA;&lt;h2&gt;What kind of problems can RAG be used for?&lt;/h2&gt; &#xA;&lt;p&gt;RAG can help anywhere there is a specific set of information that an LLM may not have in its training data (e.g. anything not publicly accessible on the internet).&lt;/p&gt; &#xA;&lt;p&gt;For example you could use RAG for:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Customer support Q&amp;amp;A chat&lt;/strong&gt; - By treating your existing customer support documentation as a resource, when a customer asks a question, you could have a system retrieve relevant documentation snippets and then have an LLM craft those snippets into an answer. Think of this as a &#34;chatbot for your documentation&#34;. Klarna, a large financial company, &lt;a href=&#34;https://www.klarna.com/international/press/klarna-ai-assistant-handles-two-thirds-of-customer-service-chats-in-its-first-month/&#34;&gt;uses a system like this&lt;/a&gt; to save $40M per year on customer support costs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Email chain analysis&lt;/strong&gt; - Let&#39;s say you&#39;re an insurance company with long threads of emails between customers and insurance agents. Instead of searching through each individual email, you could retrieve relevant passages and have an LLM create strucutred outputs of insurance claims.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Company internal documentation chat&lt;/strong&gt; - If you&#39;ve worked at a large company, you know how hard it can be to get an answer sometimes. Why not let a RAG system index your company information and have an LLM answer questions you may have? The benefit of RAG is that you will have references to resources to learn more if the LLM answer doesn&#39;t suffice.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Textbook Q&amp;amp;A&lt;/strong&gt; - Let&#39;s say you&#39;re studying for your exams and constantly flicking through a large textbook looking for answers to your quesitons. RAG can help provide answers as well as references to learn more.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;All of these have the common theme of retrieving relevant resources and then presenting them in an understandable way using an LLM.&lt;/p&gt; &#xA;&lt;p&gt;From this angle, you can consider an LLM a calculator for words.&lt;/p&gt; &#xA;&lt;h2&gt;Why local?&lt;/h2&gt; &#xA;&lt;p&gt;Privacy, speed, cost.&lt;/p&gt; &#xA;&lt;p&gt;Running locally means you use your own hardware.&lt;/p&gt; &#xA;&lt;p&gt;From a privacy standpoint, this means you don&#39;t have send potentially sensitive data to an API.&lt;/p&gt; &#xA;&lt;p&gt;From a speed standpoint, it means you won&#39;t necessarily have to wait for an API queue or downtime, if your hardware is running, the pipeline can run.&lt;/p&gt; &#xA;&lt;p&gt;And from a cost standpoint, running on your own hardware often has a heavier starting cost but little to no costs after that.&lt;/p&gt; &#xA;&lt;p&gt;Performance wise, LLM APIs may still perform better than an open-source model running locally on general tasks but there are more and more examples appearing of smaller, focused models outperforming larger models.&lt;/p&gt; &#xA;&lt;h2&gt;Key terms&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Term&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Token&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A sub-word piece of text. For example, &#34;hello, world!&#34; could be split into [&#34;hello&#34;, &#34;,&#34;, &#34;world&#34;, &#34;!&#34;]. A token can be a whole word,&lt;br&gt; part of a word or group of punctuation characters. 1 token ~= 4 characters in English, 100 tokens ~= 75 words.&lt;br&gt; Text gets broken into tokens before being passed to an LLM.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Embedding&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A learned numerical representation of a piece of data. For example, a sentence of text could be represented by a vector with&lt;br&gt; 768 values. Similar pieces of text (in meaning) will ideally have similar values.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Embedding model&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A model designed to accept input data and output a numerical representation. For example, a text embedding model may take in 384 &lt;br&gt;tokens of text and turn it into a vector of size 768. An embedding model can and often is different to an LLM model.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Similarity search/vector search&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Similarity search/vector search aims to find two vectors which are close together in high-demensional space. For example, &lt;br&gt;two pieces of similar text passed through an embedding model should have a high similarity score, whereas two pieces of text about&lt;br&gt; different topics will have a lower similarity score. Common similarity score measures are dot product and cosine similarity.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Large Language Model (LLM)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A model which has been trained to numerically represent the patterns in text. A generative LLM will continue a sequence when given a sequence. &lt;br&gt;For example, given a sequence of the text &#34;hello, world!&#34;, a genertive LLM may produce &#34;we&#39;re going to build a RAG pipeline today!&#34;.&lt;br&gt; This generation will be highly dependant on the training data and prompt.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;LLM context window&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The number of tokens a LLM can accept as input. For example, as of March 2024, GPT-4 has a default context window of 32k tokens&lt;br&gt; (about 96 pages of text) but can go up to 128k if needed. A recent open-source LLM from Google, Gemma (March 2024) has a context&lt;br&gt; window of 8,192 tokens (about 24 pages of text). A higher context window means an LLM can accept more relevant information&lt;br&gt; to assist with a query. For example, in a RAG pipeline, if a model has a larger context window, it can accept more reference items&lt;br&gt; from the retrieval system to aid with its generation.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Prompt&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A common term for describing the input to a generative LLM. The idea of &#34;&lt;a href=&#34;https://en.wikipedia.org/wiki/Prompt_engineering&#34;&gt;prompt engineering&lt;/a&gt;&#34; is to structure a text-based&lt;br&gt; (or potentially image-based as well) input to a generative LLM in a specific way so that the generated output is ideal. This technique is&lt;br&gt; possible because of a LLMs capacity for in-context learning, as in, it is able to use its representation of language to breakdown &lt;br&gt;the prompt and recognize what a suitable output may be (note: the output of LLMs is probable, so terms like &#34;may output&#34; are used).&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;TK - Extensions&lt;/h1&gt; &#xA;&lt;p&gt;Coming soon.&lt;/p&gt;</summary>
  </entry>
</feed>