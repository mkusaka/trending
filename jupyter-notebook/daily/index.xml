<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-03T01:36:55Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>bofenghuang/vigogne</title>
    <updated>2023-04-03T01:36:55Z</updated>
    <id>tag:github.com,2023-04-03:/bofenghuang/vigogne</id>
    <link href="https://github.com/bofenghuang/vigogne" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Fine-tune French instruction-following models&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/bofenghuang/vigogne/main/assets/vigogne_logo.png&#34; alt=&#34;Vigogne&#34; style=&#34;width: 40%; min-width: 300px; display: block; margin: auto;&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Vigogne ü¶ô: French Instruction-following Models&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/bofenghuang/vigogne/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg?sanitize=true&#34; alt=&#34;Code License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/bofenghuang/vigogne/raw/main/DATA_LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Data%20License-CC%20By%20NC%204.0-red.svg?sanitize=true&#34; alt=&#34;Data License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;The vigogne (French name for vicu√±a) is a South American camelid native to the Andes Mountains. It is closely related to the llama, alpaca, and guanaco.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository contains code for reproducing the &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca&lt;/a&gt; in French üá´üá∑ using &lt;a href=&#34;https://arxiv.org/abs/2106.09685&#34;&gt;low-rank adaptation (LoRA)&lt;/a&gt; provided by ü§ó Hugging Face&#39;s &lt;a href=&#34;https://github.com/huggingface/peft&#34;&gt;PEFT&lt;/a&gt; library. In addition to the LoRA technique, we also use &lt;a href=&#34;https://arxiv.org/abs/2208.07339&#34;&gt;LLM.int8()&lt;/a&gt; provided by &lt;a href=&#34;https://github.com/TimDettmers/bitsandbytes&#34;&gt;bitsandbytes&lt;/a&gt; to quantize pretrained language models (PLMs) to int8. Combining these two techniques allows us to fine-tune PLMs on a single consumer GPU such as RTX 4090.&lt;/p&gt; &#xA;&lt;p&gt;This project is based on &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;LLaMA&lt;/a&gt;, &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca&lt;/a&gt;, &lt;a href=&#34;https://github.com/tloen/alpaca-lora&#34;&gt;&lt;strong&gt;Alpaca-Lora&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/22-hours/cabrita&#34;&gt;Cabrita&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/docs/transformers/main_classes/trainer&#34;&gt;Hugging Face&lt;/a&gt;. In addition, we adapted the &lt;a href=&#34;https://github.com/bofenghuang/vigogne/raw/main/finetune.py&#34;&gt;training script&lt;/a&gt; to fine-tune on more models such as &lt;a href=&#34;https://huggingface.co/bigscience/bloom-7b1&#34;&gt;BLOOM&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/google/mt5-xxl&#34;&gt;mT5&lt;/a&gt;. We also share the &lt;a href=&#34;https://github.com/bofenghuang/vigogne/raw/main/data/vigogne_data_cleaned.json&#34;&gt;translated dataset&lt;/a&gt; and the trained &lt;a href=&#34;https://huggingface.co/bofenghuang/vigogne-lora-7b&#34;&gt;vigogne-lora-7b&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/bofenghuang/vigogne-lora-bloom-7b1&#34;&gt;vigogne-lora-bloom-7b1&lt;/a&gt; weights.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Usage and License Notices&lt;/strong&gt;: Same as &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca&lt;/a&gt;, Vigogne is intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes.&lt;/p&gt; &#xA;&lt;p&gt;üí° &lt;em&gt;The screencast below shows the current ü¶ô Vigogne-LoRA-7B model running on Apple M1 Pro using 4GB of weights (no sped up).&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bofenghuang/vigogne/main/assets/screencast.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bofenghuang/vigogne/main/#setup&#34;&gt;Setup&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bofenghuang/vigogne/main/#play-with--vigogne-models&#34;&gt;Play with ü¶ô Vigogne models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bofenghuang/vigogne/main/#try-it-out-on-your-own-pc&#34;&gt;Try it out on your own PC&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bofenghuang/vigogne/main/#data&#34;&gt;Data&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bofenghuang/vigogne/main/#training&#34;&gt;Training&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bofenghuang/vigogne/main/#example-outputs&#34;&gt;Example outputs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bofenghuang/vigogne/main/#bias-risks-and-limitations&#34;&gt;Bias, Risks, and Limitations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bofenghuang/vigogne/main/#next-steps&#34;&gt;Next steps&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;Install dependencies&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Play with ü¶ô Vigogne models&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;User Notice&lt;/strong&gt;: Facebook has not made the official LLaMA model weights open source, although various third-party download links are available online, such as &lt;code&gt;decapoda-research/llama-7b-hf&lt;/code&gt; in the HuggingFace model library. It should be noted that the use of these links may not comply with Facebook&#39;s policies. Due to the reasons mentioned above, the project cannot release the complete weights of fine-tuned models. However, only the LoRA weights can be provided, which can be considered as a &#34;patch&#34; for the original LLaMA model.&lt;/p&gt; &#xA;&lt;p&gt;The fine-tuned instruction-following vigogne models are available on ü§ó Hugging Face:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fine-tuned LLaMA-7B model: &lt;a href=&#34;https://huggingface.co/bofenghuang/vigogne-lora-7b&#34;&gt;bofenghuang/vigogne-lora-7b&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Fine-tuned LLaMA-13B model: &lt;a href=&#34;https://huggingface.co/bofenghuang/vigogne-lora-13b&#34;&gt;bofenghuang/vigogne-lora-13b&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Fine-tuned LLaMA-30B model: &lt;a href=&#34;https://huggingface.co/bofenghuang/vigogne-lora-30b&#34;&gt;bofenghuang/vigogne-lora-30b&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Fine-tuned BLOOM-7B1 model: &lt;a href=&#34;https://huggingface.co/bofenghuang/vigogne-lora-bloom-7b1&#34;&gt;bofenghuang/vigogne-lora-bloom-7b1&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Fine-tuned OPT-6.7B model: &lt;a href=&#34;https://huggingface.co/bofenghuang/vigogne-lora-opt-6.7b&#34;&gt;bofenghuang/vigogne-lora-opt-6.7b&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can infer these models by using the following Google Colab Notebook.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/bofenghuang/vigogne/blob/main/infer.ipynb&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can also run a Gradio demo using the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./demo.py \&#xA;    --base_model_name_or_path &amp;lt;name/or/path/to/hf/llama/7b/model&amp;gt; \&#xA;    --lora_model_name_or_path bofenghuang/vigogne-lora-7b&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Try it out on your own PC&lt;/h2&gt; &#xA;&lt;p&gt;The Vigogne models can now be easily deployed on PCs, thanks to the excellent tools created by the community. The following steps provide detailed instructions on how to combine Vigogne-LoRA weights with the original LLaMA model, quantize the resulting model to 4-bit, and finally deploy it on your own PC using &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note: the models will be quantized into 4-bit, so the performance might be worse than the non-quantized version. The responses are random due to the generation hyperparameters.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Please ensure that the following requirements are met prior to running:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;As the models are currently fully loaded into memory, you will need adequate disk space to save them and sufficient RAM to load them. You will need at least 13GB of RAM to quantize the 7B model. For more information, refer to this &lt;a href=&#34;https://github.com/ggerganov/llama.cpp#memorydisk-requirements&#34;&gt;link&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;It&#39;s best to use Python 3.9 or Python 3.10, as sentencepiece has not yet published a wheel for Python 3.11.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;1. Clone and build llama.cpp repo&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/ggerganov/llama.cpp&#xA;cd llama.cpp&#xA;make&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. Combine Vigogne-LoRA weights with the corresponding original LLaMA model&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# combine&#xA;python ../scripts/export_state_dict_checkpoint.py \&#xA;    --base_model_name_or_path &amp;lt;name/or/path/to/hf/llama/7b/model&amp;gt; \&#xA;    --lora_model_name_or_path &#34;bofenghuang/vigogne-lora-7b&#34; \&#xA;    --output_dir ./models/7B&#xA;&#xA;# download the tokenizer.model file&#xA;wget -P ./models https://huggingface.co/bofenghuang/vigogne-lora-7b/resolve/main/tokenizer.model&#xA;&#xA;# check the files&#xA;tree models&#xA;# models&#xA;# ‚îú‚îÄ‚îÄ 7B&#xA;# ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ consolidated.00.pth&#xA;# ‚îÇ&amp;nbsp;&amp;nbsp; ‚îî‚îÄ‚îÄ params.json&#xA;# ‚îî‚îÄ‚îÄ tokenizer.model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;3. Quantize the combined model&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# convert the 7B model to ggml FP16 format&#xA;python convert-pth-to-ggml.py ./models/7B/ 1&#xA;&#xA;# further quantize the model to 4-bit&#xA;python quantize.py 7B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;4. Run the inference&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# ./main -h for more information&#xA;./main -m ./models/7B/ggml-model-q4_0.bin --color -ins -c 2048 --temp 0.1 -n 256&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Data&lt;/h2&gt; &#xA;&lt;p&gt;We translated the original &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca/raw/main/alpaca_data.json&#34;&gt;alpaca_data.json&lt;/a&gt; to French using &lt;code&gt;gpt-3.5-turbo&lt;/code&gt; by the chat completion API.&lt;/p&gt; &#xA;&lt;p&gt;You can also translate it to other languages using the &lt;a href=&#34;https://github.com/bofenghuang/vigogne/raw/main/scripts/translate_data.py&#34;&gt;translation script&lt;/a&gt;. Don&#39;t forget to modify your &lt;a href=&#34;https://github.com/bofenghuang/vigogne/raw/e6ae25fc0569ca85c25529a6d06122b35426aa2d/scripts/translate_data.py#L47-L57&#34;&gt;translation prompt&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The translation may have compromised the accuracy of certain tasks, such as generating rhyming words or correcting grammar (discussed &lt;a href=&#34;https://github.com/tloen/alpaca-lora/pull/127&#34;&gt;here&lt;/a&gt;). We warmly welcome PRs to help clean up this dataset!&lt;/p&gt; &#xA;&lt;p&gt;The following command shows how to estimate the price for translating the full dataset.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./scripts/translate_data.py estimate_price \&#xA;    --input_json_file data/alpaca_data_cleaned.json \&#xA;    --ratio_output_input 1.0 \&#xA;    --model gpt-3.5-turbo-0301 \&#xA;    --price_per_thousand_tokens 0.002&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can translate the dataset using the following command.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Specify your OpenAI API key&#xA;export OPENAI_API_KEY=xx&#xA;&#xA;./scripts/translate_data.py process_data \&#xA;    --input_json_file data/alpaca_data_cleaned.json \&#xA;    --output_json_file data/vigogne_data_cleaned.json \&#xA;    --model gpt-3.5-turbo \&#xA;    --max_parallel_requests 32&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;h3&gt;Fine-tuning LLaMA-7B model&lt;/h3&gt; &#xA;&lt;p&gt;The following command shows how to fine-tune LLaMA-7B model using a single GPU.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python finetune.py \&#xA;    --model_name_or_path &amp;lt;name/or/path/to/hf/llama/7b/model&amp;gt; \&#xA;    --train_file &#34;data/vigogne_data_cleaned.json&#34; \&#xA;    --output_dir &#34;outputs/llama-7b-ft-vigogne-lora&#34; \&#xA;    --run_name &#34;llama-7b-ft-vigogne-lora&#34; \&#xA;    --overwrite_output_dir \&#xA;    --model_max_length_percentile 95 \&#xA;    --preprocessing_num_workers 4 \&#xA;    --dataloader_num_workers 1 \&#xA;    --lora_r 8 \&#xA;    --lora_alpha 16 \&#xA;    --lora_dropout 0.05 \&#xA;    --target_modules &#34;q_proj&#34; &#34;v_proj&#34; \&#xA;    --per_device_train_batch_size 16 \&#xA;    --per_device_eval_batch_size 8 \&#xA;    --gradient_accumulation_steps 8 \&#xA;    --num_train_epochs 3 \&#xA;    --learning_rate 3e-4 \&#xA;    --warmup_steps 100 \&#xA;    --logging_steps 25 \&#xA;    --save_strategy &#34;steps&#34; \&#xA;    --save_steps 200 \&#xA;    --save_total_limit 3 \&#xA;    --report_to &#34;tensorboard&#34; &#34;wandb&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Fine-tuning LLaMA-30B model&lt;/h3&gt; &#xA;&lt;p&gt;The following command shows how to fine-tune LLaMA-30B model using multi GPUs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;WORLD_SIZE=2 torchrun --nproc_per_node=2 --master_port=29001 finetune.py \&#xA;    --model_name_or_path &amp;lt;name/or/path/to/hf/llama/30b/model&amp;gt; \&#xA;    --train_file &#34;data/vigogne_data_cleaned.json&#34; \&#xA;    --output_dir &#34;outputs/llama-30b-ft-vigogne-lora&#34; \&#xA;    --run_name &#34;llama-30b-ft-vigogne-lora&#34; \&#xA;    --overwrite_output_dir \&#xA;    --model_max_length_percentile 95 \&#xA;    --preprocessing_num_workers 4 \&#xA;    --dataloader_num_workers 1 \&#xA;    --lora_r 8 \&#xA;    --lora_alpha 16 \&#xA;    --lora_dropout 0.05 \&#xA;    --target_modules &#34;q_proj&#34; &#34;v_proj&#34; \&#xA;    --per_device_train_batch_size 4 \&#xA;    --per_device_eval_batch_size 2 \&#xA;    --gradient_accumulation_steps 16 \&#xA;    --num_train_epochs 3 \&#xA;    --learning_rate 3e-4 \&#xA;    --warmup_steps 100 \&#xA;    --logging_steps 25 \&#xA;    --save_strategy &#34;steps&#34; \&#xA;    --save_steps 200 \&#xA;    --save_total_limit 3 \&#xA;    --report_to &#34;tensorboard&#34; &#34;wandb&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Fine-tuning BLOOM-7B1 model&lt;/h3&gt; &#xA;&lt;p&gt;The following command shows how to fine-tune &lt;a href=&#34;https://huggingface.co/bigscience/bloom-7b1&#34;&gt;bigscience/bloom-7b1&lt;/a&gt; model using a single GPU.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python finetune.py \&#xA;    --model_name_or_path &#34;bigscience/bloom-7b1&#34; \&#xA;    --train_file &#34;data/vigogne_data_cleaned.json&#34; \&#xA;    --output_dir &#34;outputs/bloom-7b1-ft-vigogne&#34; \&#xA;    --run_name &#34;bloom-7b1-ft-vigogne&#34; \&#xA;    --overwrite_output_dir \&#xA;    --model_max_length_percentile 95 \&#xA;    --preprocessing_num_workers 4 \&#xA;    --dataloader_num_workers 1 \&#xA;    --lora_r 16 \&#xA;    --lora_alpha 32 \&#xA;    --lora_dropout 0.05 \&#xA;    --target_modules &#34;query_key_value&#34; \&#xA;    --per_device_train_batch_size 16 \&#xA;    --per_device_eval_batch_size 8 \&#xA;    --gradient_accumulation_steps 8 \&#xA;    --num_train_epochs 3 \&#xA;    --learning_rate 3e-4 \&#xA;    --warmup_steps 100 \&#xA;    --logging_steps 25 \&#xA;    --save_strategy &#34;steps&#34; \&#xA;    --save_steps 200 \&#xA;    --save_total_limit 3 \&#xA;    --report_to &#34;tensorboard&#34; &#34;wandb&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Fine-tuning OPT-6.7B model&lt;/h3&gt; &#xA;&lt;p&gt;The following command shows how to fine-tune &lt;a href=&#34;https://huggingface.co/facebook/opt-6.7b&#34;&gt;facebook/opt-6.7b&lt;/a&gt; model using a single GPU.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python finetune.py \&#xA;    --model_name_or_path &#34;facebook/opt-6.7b&#34; \&#xA;    --train_file &#34;data/vigogne_data_cleaned.json&#34; \&#xA;    --output_dir &#34;outputs/opt-6b7-ft-vigogne&#34; \&#xA;    --run_name &#34;opt-6b7-ft-vigogne&#34; \&#xA;    --overwrite_output_dir \&#xA;    --model_max_length_percentile 95 \&#xA;    --preprocessing_num_workers 4 \&#xA;    --dataloader_num_workers 1 \&#xA;    --lora_r 16 \&#xA;    --lora_alpha 32 \&#xA;    --lora_dropout 0.05 \&#xA;    --target_modules &#34;q_proj&#34; &#34;v_proj&#34; \&#xA;    --per_device_train_batch_size 16 \&#xA;    --per_device_eval_batch_size 8 \&#xA;    --gradient_accumulation_steps 8 \&#xA;    --num_train_epochs 3 \&#xA;    --learning_rate 3e-4 \&#xA;    --warmup_steps 100 \&#xA;    --logging_steps 25 \&#xA;    --save_strategy &#34;steps&#34; \&#xA;    --save_steps 200 \&#xA;    --save_total_limit 3 \&#xA;    --report_to &#34;tensorboard&#34; &#34;wandb&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Example outputs&lt;/h2&gt; &#xA;&lt;p&gt;R√©pondez √† la question suivante : Les pratiques artistiques transforment-elles le monde ?&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Vigogne-LoRA-7B&lt;/strong&gt;: Les pratiques d&#39;art peuvent transformer le monde en cr√©ant des ≈ìuvres qui √©voquent des sentiments, des id√©es et des √©motions. Elles peuvent √©galement √™tre utilis√©es pour cr√©er des ≈íuvres d&#39;Art Vivant qui sont des ≈ìuvre vivantes qui peuvent √™tre utilis√©s pour cr√©√© des exp√©riences et des conversations. Les praticiens d&#39;arts peuvent √©galement utiliser leurs pratiques pour cr√©erer des ≈ìufs d&#39;√¢me, des √™tres vivants cr√©√©s √† partir de mat√©riaux naturels, qui peuvent avoir un impact sur la fa√ßon dont les gens pensent et ressentent le monde.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Other models&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Alpaca-LoRA&lt;/strong&gt;: Yes, artistic practices can transform the world. Art can be used to express ideas, explore new perspectives, and create dialogue. Art can also be used to inspire and motivate people to take action, and to create positive change in their communities.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;text-davinci-003&lt;/strong&gt;: Oui, les pratiques artistiques peuvent transformer le monde. Les artistes peuvent utiliser leur art pour exprimer leurs opinions et leurs id√©es, et ces id√©es peuvent avoir un impact sur la soci√©t√© et le monde. Les artistes peuvent √©galement utiliser leur art pour sensibiliser le public √† des sujets importants et pour encourager le changement. Les pratiques artistiques peuvent donc √™tre un puissant outil pour transformer le monde.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Expliquez la diff√©rence entre DoS et phishing en fran√ßais.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Vigogne-LoRA-7B&lt;/strong&gt;: Le DoS (Denial of Service) est une attaque informatique qui consiste √† bloquer ou √† interrompre l&#39;acc√®s √† un service ou √† un syst√®me informatique. Le phishing est une technique de fraude qui consisterait √† obtenir des informations sensibles d&#39;un utilisateur en lui demandant de fournir des renseignements, tels que des mots de passe ou des num√©ros de carte de cr√©dit.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Other models&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Alpaca-LoRA&lt;/strong&gt;: In French, the difference between a DDoS attack and phishing is that a DDoS attack is an attempt to overwhelm a system with a large amount of traffic, while phishing is an attempt to steal personal information by posing as a trusted source.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;text-davinci-003&lt;/strong&gt;:&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;DoS (Denial of Service) est une attaque qui vise √† rendre indisponible des services web ou r√©seau en saturant un serveur ou un r√©seau avec des requ√™tes. Elle peut aussi √™tre utilis√©e pour perturber un r√©seau en envoyant des paquets mal form√©s ou en le surchargant.&lt;/p&gt; &#xA; &lt;p&gt;Phishing est une forme d&#39;escroquerie en ligne visant √† voler des informations personnelles telles que des mots de passe et des num√©ros de carte de cr√©dit en incitant les victimes √† fournir leurs informations sur des faux sites Web ou des emails frauduleux.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;R√©sumez le texte suivant en une seule phrase.&lt;/p&gt; &#xA;&lt;p&gt;La science des donn√©es est l&#39;√©tude des donn√©es afin d&#39;en extraire des informations significatives pour les entreprises. Il s&#39;agit d&#39;une approche pluridisciplinaire qui combine des principes et des pratiques issus des domaines des math√©matiques, des statistiques, de l&#39;intelligence artificielle et du g√©nie informatique, en vue d&#39;analyser de grands volumes de donn√©es. Cette analyse aide les scientifiques des donn√©es √† poser des questions et √† y r√©pondre, comme Que s&#39;est-il pass√©, Pourquoi cela s&#39;est-il pass√©, Que va-t-il se passer et Que peut-on faire avec des r√©sultats.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Vigogne-LoRA-7B&lt;/strong&gt;: Le scientifique des donn√©es utilise des donn√©es pour analyser et extraire de l‚Äôinformation pour aider les entrepreneurs √† posent des questions, r√©pondre √† des questions existantes et prendre des d√©cisions.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Other models&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Alpaca-LoRA&lt;/strong&gt;: La science des donn√©es est l&#39;√©tude des donn√©es afin d&#39;en extraire des informations significatives pour les entreprises.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;text-davinci-003&lt;/strong&gt;: La science des donn√©es combine des principes et des pratiques issus des domaines des math√©matiques, des statistiques, de l&#39;intelligence artificielle et du g√©nie informatique pour analyser de grands volumes de donn√©es afin d&#39;en extraire des informations significatives pour les entreprises.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;√âcrivez une fonction qui prend une liste de cha√Ænes de caract√®res et renvoie une liste sans doublons.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Vigogne-LoRA-7B&lt;/strong&gt;:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def remove_duplicates(lst):&#xA;    result = []&#xA;    for item in lst:&#xA;        if item not in result:&#xA;            result.append(item)&#xA;    return result&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Other models&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Alpaca-LoRA&lt;/strong&gt;:&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def remove_duplicates(list):&#xA;    seen = set()&#xA;    return [x for x in list if x not in seen]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;text-davinci-003&lt;/strong&gt;:&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def remove_duplicates(liste):&#xA;  new_list = []&#xA;  for item in liste:&#xA;    if item not in new_list:&#xA;      new_list.append(item)&#xA;  return new_list&#xA;&#xA;liste = [&#34;a&#34;, &#34;b&#34;, &#34;c&#34;, &#34;a&#34;, &#34;d&#34;, &#34;e&#34;, &#34;f&#34;, &#34;c&#34;]&#xA;print(remove_duplicates(liste)) # [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;f&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Bias, Risks, and Limitations&lt;/h2&gt; &#xA;&lt;p&gt;Vigogne is still under development, and there are many limitations that have to be addressed. Please note that it is possible that the model generates harmful or biased content, incorrect information or generally unhelpful answers.&lt;/p&gt; &#xA;&lt;h2&gt;Next steps&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Collect more and cleaner French instruction-following data&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>Visualize-ML/Book1_Python-For-Beginners</title>
    <updated>2023-04-03T01:36:55Z</updated>
    <id>tag:github.com,2023-04-03:/Visualize-ML/Book1_Python-For-Beginners</id>
    <link href="https://github.com/Visualize-ML/Book1_Python-For-Beginners" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Book_1_„ÄäÁºñÁ®ã‰∏çÈöæ„Äã | È∏¢Â∞æËä±‰π¶Ôºö‰ªéÂä†Âáè‰πòÈô§Âà∞Êú∫Âô®Â≠¶‰π†ÔºõÂºÄÂßã‰∏ä‰º†PDFËçâÁ®ø„ÄÅJupyterÁ¨îËÆ∞„ÄÇÊñá‰ª∂Ëøò‰ºöÁªèËøáËá≥Â∞ë‰∏§ËΩÆ‰øÆÊîπÔºåÊîπÂä®‰ºöÂæàÂ§ßÔºåÂ§ßÂÆ∂Ê≥®ÊÑè‰∏ãËΩΩÊúÄÊñ∞ÁâàÊú¨„ÄÇËØ∑Â§öÊèêÊÑèËßÅÔºåË∞¢Ë∞¢&lt;/p&gt;&lt;hr&gt;</summary>
  </entry>
  <entry>
    <title>sophiamyang/tutorials-LangChain</title>
    <updated>2023-04-03T01:36:55Z</updated>
    <id>tag:github.com,2023-04-03:/sophiamyang/tutorials-LangChain</id>
    <link href="https://github.com/sophiamyang/tutorials-LangChain" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;ü§© Is LangChain the easiest way to work with LLMs? It&#39;s an open-source tool and recently added ChatGPT Plugins. It provides so many capabilities that I find useful. Check out my video to learn more:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=kmbS6FDQh7c&#34;&gt;LangChain Overview video&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>