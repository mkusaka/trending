<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-06-22T01:44:14Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>mrdbourke/zero-to-mastery-ml</title>
    <updated>2022-06-22T01:44:14Z</updated>
    <id>tag:github.com,2022-06-22:/mrdbourke/zero-to-mastery-ml</id>
    <link href="https://github.com/mrdbourke/zero-to-mastery-ml" rel="alternate"></link>
    <summary type="html">&lt;p&gt;All course materials for the Zero to Mastery Machine Learning and Data Science course.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Zero to Mastery Machine Learning&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://mybinder.org/v2/gh/mrdbourke/zero-to-mastery-ml/master&#34;&gt;&lt;img src=&#34;https://mybinder.org/badge_logo.svg?sanitize=true&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.deepnote.com/launch?template=data-science&amp;amp;url=https://github.com/mrdbourke/zero-to-mastery-ml/raw/master/section-2-data-science-and-ml-tools/introduction-to-pandas.ipynb&#34;&gt;&lt;img src=&#34;https://deepnote.com/buttons/launch-in-deepnote.svg?sanitize=true&#34; alt=&#34;Deepnote&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/mrdbourke/zero-to-mastery-ml/blob/master&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Welcome! This repository contains all of the code, notebooks, images and other materials related to the &lt;a href=&#34;https://dbourke.link/mlcourse&#34;&gt;Zero to Mastery Machine Learning Course on Udemy&lt;/a&gt; and &lt;a href=&#34;https://dbourke.link/ZTMmlcourse&#34;&gt;zerotomastery.io&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;d like to see anything in particular, please send me an email: &lt;a href=&#34;mailto:daniel@mrdbourke.com&#34;&gt;daniel@mrdbourke.com&lt;/a&gt; or leave an issue.&lt;/p&gt; &#xA;&lt;h2&gt;What this course focuses on&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create a framework for working through problems (&lt;a href=&#34;https://github.com/mrdbourke/zero-to-mastery-ml/raw/master/section-1-getting-ready-for-machine-learning/a-6-step-framework-for-approaching-machine-learning-projects.md&#34;&gt;6 step machine learning modelling framework&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Find tools to fit the framework&lt;/li&gt; &#xA; &lt;li&gt;Targeted practice = use tools and framework steps to work on end-to-end machine learning modelling projects&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;How this course is structured&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Section 1 - Getting your mind and computer ready for machine learning (concepts, computer setup)&lt;/li&gt; &#xA; &lt;li&gt;Section 2 - Tools for machine learning and data science (pandas, NumPy, Matplotlib, Scikit-Learn)&lt;/li&gt; &#xA; &lt;li&gt;Section 3 - End-to-end structured data projects (classification and regression)&lt;/li&gt; &#xA; &lt;li&gt;Section 4 - Neural networks, deep learning and transfer learning with TensorFlow 2.0&lt;/li&gt; &#xA; &lt;li&gt;Section 5 - Communicating and sharing your work&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Student notes&lt;/h2&gt; &#xA;&lt;p&gt;Some students have taken and shared extensive notes on this course, see them below.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;d like to submit yours, leave a pull request.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Chester&#39;s notes - &lt;a href=&#34;https://github.com/chesterheng/machinelearning-datascience&#34;&gt;https://github.com/chesterheng/machinelearning-datascience&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Sophia&#39;s notes - &lt;a href=&#34;https://www.rockyourcode.com/tags/udemy-complete-machine-learning-and-data-science-zero-to-mastery/&#34;&gt;https://www.rockyourcode.com/tags/udemy-complete-machine-learning-and-data-science-zero-to-mastery/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>karpathy/minGPT</title>
    <updated>2022-06-22T01:44:14Z</updated>
    <id>tag:github.com,2022-06-22:/karpathy/minGPT</id>
    <link href="https://github.com/karpathy/minGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A minimal PyTorch re-implementation of the OpenAI GPT (Generative Pretrained Transformer) training&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;minGPT&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/karpathy/minGPT/master/mingpt.jpg&#34; alt=&#34;mingpt&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;A PyTorch re-implementation of &lt;a href=&#34;https://github.com/openai/gpt-3&#34;&gt;GPT&lt;/a&gt; training. minGPT tries to be small, clean, interpretable and educational, as most of the currently available ones are a bit sprawling. GPT is not a complicated model and this implementation is appropriately about 300 lines of code, including boilerplate and a totally unnecessary custom causal self-attention module. Anyway, all that&#39;s going on is that a sequence of indices goes into a sequence of transformer blocks, and a probability distribution of the next index comes out. The rest of the complexity is just being clever with batching (both across examples and over sequence length) so that training is efficient.&lt;/p&gt; &#xA;&lt;p&gt;The core minGPT &#34;library&#34; (hah) is two files: &lt;code&gt;mingpt/model.py&lt;/code&gt; contains the actual Transformer model definition and &lt;code&gt;mingpt/trainer.py&lt;/code&gt; is (GPT-independent) PyTorch boilerplate that trains the model. The attached Jupyter notebooks then show how the &#34;library&#34; (hah) can be used to train sequence models:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;play_math.ipynb&lt;/code&gt; trains a GPT focused on addition (inspired by the addition section in the GPT-3 paper)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;play_char.ipynb&lt;/code&gt; trains a GPT to be a character-level language model on arbitrary text, similar to my older char-rnn but with a transformer instead of an RNN&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;play_image.ipynb&lt;/code&gt; trains a GPT on (small) images (CIFAR-10), showing that we can model images just as text, as both can be reduced to just a sequence of integers&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;play_words.ipynb&lt;/code&gt; a BPE version that does not yet exist&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;With a bpe encoder, distributed training and maybe fp16 this implementation may be able to reproduce GPT-1/GPT-2 results, though I haven&#39;t tried $$$. GPT-3 is likely out of reach as my understanding is that it does not fit into GPU memory and requires a more careful model-parallel treatment.&lt;/p&gt; &#xA;&lt;h3&gt;Example usage&lt;/h3&gt; &#xA;&lt;p&gt;This code is simple enough to just hack inline, not &#34;used&#34;, but current API looks something like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;# you&#39;re on your own to define a class that returns individual examples as PyTorch LongTensors&#xA;from torch.utils.data import Dataset&#xA;train_dataset = MyDataset(...)&#xA;test_dataset = MyDataset(...)&#xA;&#xA;# construct a GPT model&#xA;from mingpt.model import GPT, GPTConfig&#xA;mconf = GPTConfig(vocab_size, block_size, n_layer=12, n_head=12, n_embd=768) # a GPT-1&#xA;model = GPT(mconf)&#xA;&#xA;# construct a trainer&#xA;from mingpt.trainer import Trainer, TrainerConfig&#xA;tconf = TrainerConfig(max_epochs=10, batch_size=256)&#xA;trainer = Trainer(model, train_dataset, test_dataset, tconf)&#xA;trainer.train()&#xA;# (... enjoy the show for a while... )&#xA;&#xA;# sample from the model (the [None, ...] and [0] are to push/pop a needed dummy batch dimension)&#xA;from mingpt.utils import sample&#xA;x = torch.tensor([1, 2, 3], dtype=torch.long)[None, ...] # context conditioning&#xA;y = sample(model, x, steps=30, temperature=1.0, sample=True, top_k=5)[0]&#xA;print(y) # our model filled in the integer sequence with 30 additional likely integers&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;References&lt;/h3&gt; &#xA;&lt;p&gt;Code:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/gpt-2&#34;&gt;openai/gpt-2&lt;/a&gt; has the model but not the training code, and in TensorFlow&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/image-gpt&#34;&gt;openai/image-gpt&lt;/a&gt; has some more modern gpt-3 like modification in its code, good reference as well&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;huggingface/transformers&lt;/a&gt; has a &lt;a href=&#34;https://github.com/huggingface/transformers/tree/master/examples/pytorch/language-modeling&#34;&gt;language-modeling example&lt;/a&gt;. It is full-featured but as a result also somewhat challenging to trace. E.g. some large functions have as much as 90% unused code behind various branching statements that is unused in the default setting of simple language modeling.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Papers + some implementation notes:&lt;/p&gt; &#xA;&lt;h4&gt;Improving Language Understanding by Generative Pre-Training (GPT-1)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Our model largely follows the original transformer work&lt;/li&gt; &#xA; &lt;li&gt;We trained a 12-layer decoder-only transformer with masked self-attention heads (768 dimensional states and 12 attention heads). For the position-wise feed-forward networks, we used 3072 dimensional inner states.&lt;/li&gt; &#xA; &lt;li&gt;Adam max learning rate of 2.5e-4. (later GPT-3 for this model size uses 6e-4)&lt;/li&gt; &#xA; &lt;li&gt;LR decay: increased linearly from zero over the first 2000 updates and annealed to 0 using a cosine schedule&lt;/li&gt; &#xA; &lt;li&gt;We train for 100 epochs on minibatches of 64 randomly sampled, contiguous sequences of 512 tokens.&lt;/li&gt; &#xA; &lt;li&gt;Since layernorm is used extensively throughout the model, a simple weight initialization of N(0, 0.02) was sufficient&lt;/li&gt; &#xA; &lt;li&gt;bytepair encoding (BPE) vocabulary with 40,000 merges&lt;/li&gt; &#xA; &lt;li&gt;residual, embedding, and attention dropouts with a rate of 0.1 for regularization.&lt;/li&gt; &#xA; &lt;li&gt;modified version of L2 regularization proposed in (37), with w = 0.01 on all non bias or gain weights&lt;/li&gt; &#xA; &lt;li&gt;For the activation function, we used the Gaussian Error Linear Unit (GELU).&lt;/li&gt; &#xA; &lt;li&gt;We used learned position embeddings instead of the sinusoidal version proposed in the original work&lt;/li&gt; &#xA; &lt;li&gt;For finetuning: We add dropout to the classifier with a rate of 0.1. learning rate of 6.25e-5 and a batchsize of 32. 3 epochs. We use a linear learning rate decay schedule with warmup over 0.2% of training. λ was set to 0.5.&lt;/li&gt; &#xA; &lt;li&gt;GPT-1 model is 12 layers and d_model 768, ~117M params&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Language Models are Unsupervised Multitask Learners (GPT-2)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;LayerNorm was moved to the input of each sub-block, similar to a pre-activation residual network&lt;/li&gt; &#xA; &lt;li&gt;an additional layer normalization was added after the final self-attention block.&lt;/li&gt; &#xA; &lt;li&gt;modified initialization which accounts for the accumulation on the residual path with model depth is used. We scale the weights of residual layers at initialization by a factor of 1/√N where N is the number of residual layers. (weird because in their released code i can only find a simple use of the old 0.02... in their release of image-gpt I found it used for c_proj, and even then only for attn, not for mlp. huh. &lt;a href=&#34;https://github.com/openai/image-gpt/raw/master/src/model.py&#34;&gt;https://github.com/openai/image-gpt/blob/master/src/model.py&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;the vocabulary is expanded to 50,257&lt;/li&gt; &#xA; &lt;li&gt;increase the context size from 512 to 1024 tokens&lt;/li&gt; &#xA; &lt;li&gt;larger batchsize of 512 is used&lt;/li&gt; &#xA; &lt;li&gt;GPT-2 used 48 layers and d_model 1600 (vs. original 12 layers and d_model 768). ~1.542B params&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Language Models are Few-Shot Learners (GPT-3)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GPT-3: 96 layers, 96 heads, with d_model of 12,288 (175B parameters).&lt;/li&gt; &#xA; &lt;li&gt;GPT-1-like: 12 layers, 12 heads, d_model 768 (125M)&lt;/li&gt; &#xA; &lt;li&gt;We use the same model and architecture as GPT-2, including the modified initialization, pre-normalization, and reversible tokenization described therein&lt;/li&gt; &#xA; &lt;li&gt;we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer&lt;/li&gt; &#xA; &lt;li&gt;we always have the feedforward layer four times the size of the bottleneck layer, dff = 4 ∗ dmodel&lt;/li&gt; &#xA; &lt;li&gt;all models use a context window of nctx = 2048 tokens.&lt;/li&gt; &#xA; &lt;li&gt;Adam with β1 = 0.9, β2 = 0.95, and eps = 10−8&lt;/li&gt; &#xA; &lt;li&gt;All models use weight decay of 0.1 to provide a small amount of regularization. (NOTE: GPT-1 used 0.01 I believe, see above)&lt;/li&gt; &#xA; &lt;li&gt;clip the global norm of the gradient at 1.0&lt;/li&gt; &#xA; &lt;li&gt;Linear LR warmup over the first 375 million tokens. Then use cosine decay for learning rate down to 10% of its value, over 260 billion tokens.&lt;/li&gt; &#xA; &lt;li&gt;gradually increase the batch size linearly from a small value (32k tokens) to the full value over the first 4-12 billion tokens of training, depending on the model size.&lt;/li&gt; &#xA; &lt;li&gt;full 2048-sized time context window is always used, with a special END OF DOCUMENT token delimiter&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Generative Pretraining from Pixels (Image GPT)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;When working with images, we pick the identity permutation πi = i for 1 ≤ i ≤ n, also known as raster order.&lt;/li&gt; &#xA; &lt;li&gt;we create our own 9-bit color palette by clustering (R, G, B) pixel values using k-means with k = 512.&lt;/li&gt; &#xA; &lt;li&gt;Our largest model, iGPT-XL, contains L = 60 layers and uses an embedding size of d = 3072 for a total of 6.8B parameters.&lt;/li&gt; &#xA; &lt;li&gt;Our next largest model, iGPT-L, is essentially identical to GPT-2 with L = 48 layers, but contains a slightly smaller embedding size of d = 1536 (vs 1600) for a total of 1.4M parameters.&lt;/li&gt; &#xA; &lt;li&gt;We use the same model code as GPT-2, except that we initialize weights in the layerdependent fashion as in Sparse Transformer (Child et al., 2019) and zero-initialize all projections producing logits.&lt;/li&gt; &#xA; &lt;li&gt;We also train iGPT-M, a 455M parameter model with L = 36 and d = 1024&lt;/li&gt; &#xA; &lt;li&gt;iGPT-S, a 76M parameter model with L = 24 and d = 512 (okay, and how many heads? looks like the Github code claims 8)&lt;/li&gt; &#xA; &lt;li&gt;When pre-training iGPT-XL, we use a batch size of 64 and train for 2M iterations, and for all other models we use a batch size of 128 and train for 1M iterations.&lt;/li&gt; &#xA; &lt;li&gt;Adam with β1 = 0.9 and β2 = 0.95&lt;/li&gt; &#xA; &lt;li&gt;The learning rate is warmed up for one epoch, and then decays to 0&lt;/li&gt; &#xA; &lt;li&gt;We did not use weight decay because applying a small weight decay of 0.01 did not change representation quality.&lt;/li&gt; &#xA; &lt;li&gt;iGPT-S lr 0.003&lt;/li&gt; &#xA; &lt;li&gt;No dropout is used.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;License&lt;/h3&gt; &#xA;&lt;p&gt;MIT&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>aialgorithm/Blog</title>
    <updated>2022-06-22T01:44:14Z</updated>
    <id>tag:github.com,2022-06-22:/aialgorithm/Blog</id>
    <link href="https://github.com/aialgorithm/Blog" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Python机器学习算法技术博客，有原创干货！有code实践！&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;&#34;算法进阶&#34;公众号精选文章及代码 【欢迎点亮Star收藏~】&lt;/h2&gt; &#xA;&lt;p&gt;分享Python机器学习及深度学习算法等原创文章、代码及学习资源。原创不易，感谢阅读及关注~&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3 id=&#34;2&#34;&gt;从机器学习到深度学习序列文章&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;300/&#34;&gt;文章&lt;img width=&#34;300/&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;代码&amp;amp;资料&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/19&#34;&gt;《一篇白话机器学习概念》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/46&#34;&gt;《一文浅谈深度学习泛化能力》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/tree/master/projects/%E4%B8%80%E6%96%87%E6%B5%85%E8%B0%88%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B&#34;&gt;资料&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/20&#34;&gt;《一文速览机器学习的类别（Python代码）》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/tree/master/projects/%E4%B8%80%E6%96%87%E9%80%9F%E8%A7%88%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%B1%BB%E5%88%AB%EF%BC%88Python%E4%BB%A3%E7%A0%81%EF%BC%89&#34;&gt;代码&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/21&#34;&gt;《一文全览机器学习建模流程（Python代码）》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/tree/master/projects/%E4%B8%80%E6%96%87%E5%85%A8%E8%A7%88%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%BB%BA%E6%A8%A1%E6%B5%81%E7%A8%8B%EF%BC%88Python%E4%BB%A3%E7%A0%81%EF%BC%89&#34;&gt;代码&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/23&#34;&gt;《神经元谈到深度神经网络》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/24&#34;&gt;《一文讲透神经网络的激活函数》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/25&#34;&gt;《神经网络拟合能力的提升之路（Pyhton）》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/tree/master/projects/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%8B%9F%E5%90%88%E8%83%BD%E5%8A%9B%E7%9A%84%E6%8F%90%E5%8D%87%E4%B9%8B%E8%B7%AF%EF%BC%88Pyhton%EF%BC%89&#34;&gt;代码&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/26&#34;&gt;《一文概览神经网络模型》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/27&#34;&gt;《神经网络学习到的是什么？（Python）》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/tree/master/projects/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E5%88%B0%E7%9A%84%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%EF%BC%88Python%EF%BC%89&#34;&gt;代码&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/29&#34;&gt;《一文深层解决过拟合》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/30&#34;&gt;《一文概览神经网络优化算法》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/31&#34;&gt;《一文搞定深度学习建模预测全流程（Python）》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/tree/master/projects/%E4%B8%80%E6%96%87%E6%90%9E%E5%AE%9A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BB%BA%E6%A8%A1%E9%A2%84%E6%B5%8B%E5%85%A8%E6%B5%81%E7%A8%8B(Python)&#34;&gt;代码&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/35&#34;&gt;《一文详解RNN及股票预测实战(Python)！》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/tree/master/projects/%E4%B8%80%E6%96%87%E8%AF%A6%E8%A7%A3RNN%E5%8F%8A%E5%AE%9E%E6%88%98(Python)&#34;&gt;代码&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/37&#34;&gt;《一文弄懂CNN及图像识别(Python)》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/tree/master/projects/%E4%B8%80%E6%96%87%E5%BC%84%E6%87%82CNN%E5%8F%8A%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB(Python)&#34;&gt;代码&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/41&#34;&gt;《LSTM原理及生成藏头诗（Python）》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/tree/master/projects/LSTM&#34;&gt;代码&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/52&#34;&gt;《一文概览NLP算法(Python)》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/tree/master/projects/%E4%B8%80%E6%96%87%E6%A6%82%E8%A7%88NLP%E7%AE%97%E6%B3%95%EF%BC%88Python%EF%BC%89&#34;&gt;代码&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3 id=&#34;1&#34;&gt;机器学习&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;300/&#34;&gt;文章&lt;img width=&#34;300/&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;代码&amp;amp;资料&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/16&#34;&gt;《几经沉浮，人工智能(AI)前路何方？》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/22&#34;&gt;《Python人工智能学习路线(长篇干货) 》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/AiPy&#34;&gt;资源&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/2&#34;&gt;《Python机器学习入门指南（全）》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/tree/master/projects/Python%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97demo&#34;&gt;代码&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/14&#34;&gt;《Python数据分析指南(全)》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/tree/master/projects/Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%8C%87%E5%8D%97(%E5%85%A8)&#34;&gt;代码&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/3&#34;&gt;《程序员说模型过拟合的时候，说的是什么?》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/11&#34;&gt;《一文归纳Python特征生成方法(全)》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/tree/master/projects/%E4%B8%80%E6%96%87%E5%BD%92%E7%BA%B3Python%E7%89%B9%E5%BE%81%E7%94%9F%E6%88%90%E6%96%B9%E6%B3%95&#34;&gt;代码&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/10&#34;&gt;《Python特征选择(全)》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/tree/master/projects/Python%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9&#34;&gt;代码&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/13&#34;&gt;《一文归纳Ai数据增强之法》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/tree/master/projects/%E4%B8%80%E6%96%87%E5%BD%92%E7%BA%B3Ai%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E4%B9%8B%E6%B3%95&#34;&gt;代码&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/12&#34;&gt;《一文归纳Ai调参炼丹之法》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/tree/master/projects/%E4%B8%80%E6%96%87%E5%BD%92%E7%BA%B3Ai%E8%B0%83%E5%8F%82%E7%82%BC%E4%B8%B9%E4%B9%8B%E6%B3%95&#34;&gt;代码&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/18&#34;&gt;《异常检测算法速览(Python)》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/tree/master/projects/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E9%80%9F%E8%A7%88(Python%E6%BA%90%E7%A0%81)&#34;&gt;代码&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/7&#34;&gt;《一文囊括序列预测方法(Python)》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/tree/master/projects/%E4%B8%80%E6%96%87%E5%9B%8A%E6%8B%AC%E5%BA%8F%E5%88%97%E9%A2%84%E6%B5%8B%E6%96%B9%E6%B3%95(Python)&#34;&gt;代码&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/15&#34;&gt;《Python半监督算法概览》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/tree/master/projects/Python%E5%8D%8A%E7%9B%91%E7%9D%A3%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88&#34;&gt;代码&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/4&#34;&gt;《一文道尽XGBOOST的前世今生》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/datamining&#34;&gt;《数据挖掘概要(Python)》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/tree/master/projects/Python%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%A6%82%E8%A6%81&#34;&gt;代码&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/17&#34;&gt;《分布式机器学习原理及实战(Pyspark)》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/tree/master/projects/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E6%88%98(Pyspark)&#34;&gt;代码&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/32&#34;&gt;《一文深度解读模型评估方法》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/tree/master/projects/%E4%B8%80%E6%96%87%E6%B7%B1%E5%BA%A6%E8%A7%A3%E8%AF%BB%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95&#34;&gt;代码&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/33&#34;&gt;《全面解析并实现逻辑回归(Python)》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/tree/master/projects/%E5%85%A8%E9%9D%A2%E8%A7%A3%E6%9E%90%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%AE%9E%E7%8E%B0(Python)&#34;&gt;代码&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/34&#34;&gt;《逻辑回归优化技巧总结(全)》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/tree/master/projects/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A7%E6%80%BB%E7%BB%93(%E5%85%A8)&#34;&gt;代码&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/36&#34;&gt;《全面归纳距离和相似度方法(7种)》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/38&#34;&gt;《深入理解KNN扩展到ANN)》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/tree/master/projects/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3KNN%E6%89%A9%E5%B1%95%E5%88%B0ANN&#34;&gt;代码&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/38&#34;&gt;《从深度学习到深度森林方法（Python）》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/tree/master/projects/%E6%B7%B1%E5%BA%A6%E6%A3%AE%E6%9E%97%E9%A2%84%E6%B5%8B&#34;&gt;代码&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/40&#34;&gt;《一文全面解决样本不均衡》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/tree/master/projects/%E4%B8%80%E6%96%87%E8%A7%A3%E5%86%B3%E6%A0%B7%E6%9C%AC%E4%B8%8D%E5%9D%87%E8%A1%A1(%E5%85%A8)&#34;&gt;代码&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/42&#34;&gt;《全面解析Kmeans聚类(Python)》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aialgorithm/Blog/master/projects/kmeans++&#34;&gt;代码&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/47&#34;&gt;《一文通俗讲透树模型》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/48&#34;&gt;《Pandas、Numpy性能优化秘籍（全）》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/49&#34;&gt;《树模型遇上类别型特征(Python)》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/50&#34;&gt;《TensorFlow决策森林构建GBDT（Python）》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/tree/master/projects/TensorFlow%E5%86%B3%E7%AD%96%E6%A3%AE%E6%9E%97%E5%AE%9E%E8%B7%B5&#34;&gt;代码&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/51&#34;&gt;《深入机器学习的梯度优化》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3 id=&#34;3&#34;&gt;金融科技&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;300/&#34;&gt;文章&lt;img width=&#34;300/&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;代码&amp;amp;资料&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/44&#34;&gt;《一文梳理金融风控建模全流程(Python)》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/tree/master/projects/%E4%B8%80%E6%96%87%E6%A2%B3%E7%90%86%E9%A3%8E%E6%8E%A7%E5%BB%BA%E6%A8%A1%E5%85%A8%E6%B5%81%E7%A8%8B&#34;&gt;代码&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/6&#34;&gt;《基于知识图谱的营销反欺诈全流程》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/9&#34;&gt;《一文贯穿信贷反欺诈全流程》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/8&#34;&gt;《客户流失预测及营销(Python)》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/tree/master/projects/%E5%AE%A2%E6%88%B7%E6%B5%81%E5%A4%B1%E9%A2%84%E6%B5%8B%E5%8F%8A%E8%90%A5%E9%94%80&#34;&gt;代码&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/43&#34;&gt;《一窥推荐系统的原理》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/45&#34;&gt;《电影推荐项目实战（双塔模型）》&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/tree/master/projects/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E5%AE%9E%E6%88%98&#34;&gt;代码&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;其他&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/aialgorithm/AiPy&#34;&gt;【Python、机器学习算法学习资源汇总】&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/28&#34;&gt;《程序员面试完全指南》&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/aialgorithm/Blog/issues/1&#34;&gt;《TCP/IP--图解从URL到网页通信原理》&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/33707637/119756102-9f9b5200-bed5-11eb-85f9-5dbc264d447d.png&#34; alt=&#34;关注公众号：算法进阶&#34;&gt;&lt;/p&gt;</summary>
  </entry>
</feed>