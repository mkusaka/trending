<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-17T01:30:03Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>rasbt/pycon2024</title>
    <updated>2024-05-17T01:30:03Z</updated>
    <id>tag:github.com,2024-05-17:/rasbt/pycon2024</id>
    <link href="https://github.com/rasbt/pycon2024" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Tutorial Materials for &#34;The Fundamentals of Modern Deep Learning with PyTorch&#34; workshop at PyCon 2024&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;PyCon US 2024: The Fundamentals of Modern Deep Learning with PyTorch&lt;/h1&gt; &#xA;&lt;p&gt;Tutorial materials for &lt;strong&gt;The Fundamentals of Modern Deep Learning with PyTorch&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;a target=&#34;_blank&#34; href=&#34;https://lightning.ai/lightning-ai/studios/dl-pycon2024&#34;&gt; &lt;img src=&#34;https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/studio-badge.svg?sanitize=true&#34; alt=&#34;Open In Studio&#34;&gt; &lt;/a&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;At &lt;a href=&#34;https://us.pycon.org/2024/&#34;&gt;PyCon US 2024&lt;/a&gt; in Pittsburgh, Pennsylvania&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Wed 15 May 2024, 9:00 am to 12:30 pm (EDT), Room 321&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;img src=&#34;https://sebastianraschka.com/images/talks/2024-pycon-cover.webp&#34; width=&#34;600&#34;&gt; &#xA;&lt;p&gt;I’ll be giving a 3.5 hour deep learning workshop at PyCon 2024 in May. &lt;strong&gt;It’s my first PyCon, and I’m very excited!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h3&gt;Target audience&lt;/h3&gt; &#xA;&lt;p&gt;This tutorial is aimed at Python programmers new to PyTorch and deep learning. However, even more experienced deep learning practitioners and PyTorch users may be exposed to new concepts and ideas when exploring other open source libraries to extend PyTorch.&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h3&gt;Abstract&lt;/h3&gt; &#xA;&lt;p&gt;We will kick off this tutorial with an introduction to deep learning and highlight its primary strengths and use cases compared to traditional machine learning. In recent years, PyTorch has emerged as the most widely used deep learning library for research. However, a lot has changed regarding how we train neural networks these days. After getting a firm grasp of the PyTorch API, you will learn how to train deep neural networks using various multi-GPU training paradigms. We will also fine-tune large language models (transformers)!&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h3&gt;Preparation&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] A reproducible cloud environment will be shared with participants on the day of the workshop, so no setup steps are required. However, this document provides suggestions for those who wish to install the dependencies locally on their own machines.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;(Optional) You may find the &lt;strong&gt;Python Setup Guide&lt;/strong&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/pycon2024/main/00-1_python-setup-guide&#34;&gt;./00-1_python-setup-guide&lt;/a&gt;) helpful, which mainly describes how I set up Python on my computer(s).&lt;/li&gt; &#xA; &lt;li&gt;Please go through &lt;strong&gt;Python Library Installation&lt;/strong&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/pycon2024/main/00-2_python-libraries-for-workshop&#34;&gt;./00-2_python-libraries-for-workshop&lt;/a&gt;) guide to ensure you have all the required libraries installed prior to the workshop.&lt;/li&gt; &#xA; &lt;li&gt;I recommend downloading this repository before the event so you can access the materials offline in case of a slow internet connection during the workshop.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Looking forward to seeing you there!&lt;/p&gt; &#xA;&lt;p&gt;PS: If you have any questions, please feel free to reach out via the &lt;a href=&#34;https://github.com/rasbt/pycon2024/discussions&#34;&gt;Discussion page&lt;/a&gt; here on GitHub.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;Schedule and Slides&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Introduction to Deep Learning &amp;amp; Setup (9:00 - 9:30 am) &lt;a href=&#34;https://sebastianraschka.com/pdf/pycon2024/01_intro-to-deeplearning_compressed.pdf&#34;&gt;🔗 Slides&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Understanding the PyTorch API (9:30 - 10:00 am) &lt;a href=&#34;https://sebastianraschka.com/pdf/pycon2024/02_pytorch-api_compressed.pdf&#34;&gt;🔗 Slides&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;10:30 - 11:00 am: PyCon coffee and snack break&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Training Deep Neural Networks (11:00 - 11:30 am) &lt;a href=&#34;https://sebastianraschka.com/pdf/pycon2024/03_training-dnns_compressed.pdf&#34;&gt;🔗 Slides&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Accelerating PyTorch Model Training (11:30 am - 12:00 pm) &lt;a href=&#34;https://sebastianraschka.com/pdf/pycon2024/04-accelerating-pytorch_compressed.pdf&#34;&gt;🔗 Slides&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Finetuning Large Language Models (12:00 - 12:30 pm) &lt;a href=&#34;https://sebastianraschka.com/pdf/pycon2024/05_finetuning-llms_compressed.pdf&#34;&gt;🔗 Slides&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>AntonioTepsich/Convolutional-KANs</title>
    <updated>2024-05-17T01:30:03Z</updated>
    <id>tag:github.com,2024-05-17:/AntonioTepsich/Convolutional-KANs</id>
    <link href="https://github.com/AntonioTepsich/Convolutional-KANs" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This project extends the idea of the innovative architecture of Kolmogorov-Arnold Networks (KAN) to the Convolutional Layers, changing the classic linear transformation of the convolution to learnable non linear activations in each pixel.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Convolutional Kolmogorov-Arnold Network (CKAN)&lt;/h1&gt; &#xA;&lt;h3&gt;Introducing Convolutional KAN Networks!&lt;/h3&gt; &#xA;&lt;p&gt;This project extends the idea of the innovative architecture of Kolmogorov-Arnold Networks (KAN) to the Convolutional Layers, changing the classic linear transformation of the convolution to learnable non linear activations in each pixel.&lt;/p&gt; &#xA;&lt;h3&gt;Authors&lt;/h3&gt; &#xA;&lt;p&gt;This repository was made by:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Alexander Bodner | &lt;a href=&#34;mailto:abodner@udesa.edu.ar&#34;&gt;abodner@udesa.edu.ar&lt;/a&gt; | &lt;a href=&#34;https://twitter.com/AlexBodner_&#34;&gt;Twitter&lt;/a&gt; | &lt;a href=&#34;https://www.linkedin.com/in/alexanderbodner/&#34;&gt;LinkedIn&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Antonio Tepsich | &lt;a href=&#34;mailto:atepsich@udesa.edu.ar&#34;&gt;atepsich@udesa.edu.ar&lt;/a&gt; | &lt;a href=&#34;https://twitter.com/antotepsich&#34;&gt;Twitter&lt;/a&gt; | &lt;a href=&#34;https://www.linkedin.com/in/antonio-tepsich/&#34;&gt;LinkedIn&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Jack Spolski | &lt;a href=&#34;mailto:jspolski@udesa.edu.ar&#34;&gt;jspolski@udesa.edu.ar&lt;/a&gt; | &lt;a href=&#34;https://www.linkedin.com/in/jack-spolski-9882a3196/&#34;&gt;LinkedIn&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Santiago Pourteau | &lt;a href=&#34;mailto:spourteau@udesa.edu.ar&#34;&gt;spourteau@udesa.edu.ar&lt;/a&gt; | &lt;a href=&#34;https://twitter.com/SantiPourteau&#34;&gt;Twitter&lt;/a&gt; | &lt;a href=&#34;https://www.linkedin.com/in/santiago-pourteau-1bba8619a/&#34;&gt;LinkedIn&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Credits&lt;/h3&gt; &#xA;&lt;p&gt;This repository uses an efficient implementation of KAN which is available &lt;a href=&#34;https://github.com/Blealtan/efficient-kan&#34;&gt;here&lt;/a&gt;. The original implementation of KAN is available &lt;a href=&#34;https://github.com/KindXiaoming/pykan&#34;&gt;here&lt;/a&gt;. The original paper of the KAN is available &lt;a href=&#34;https://arxiv.org/pdf/2404.19756&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;What is a KAN?&lt;/h3&gt; &#xA;&lt;p&gt;KANs are promising alternatives of Multi-Layer Perceptrons (MLPs). KANs have strong mathematical foundations just like MLPs: MLPs are based on the universal approximation theorem, while KANs are based on Kolmogorov-Arnold representation theorem. KANs and MLPs are dual: KANs have activation functions on edges, while MLPs have activation functions on nodes. KAN seems to be more parameter efficient than MLPs, but each KAN Layer has more parameters than a MLP layer.&lt;/p&gt; &#xA;&lt;img width=&#34;1163&#34; alt=&#34;mlp_kan_compare&#34; src=&#34;https://github.com/KindXiaoming/pykan/assets/23551623/695adc2d-0d0b-4e4b-bcff-db2c8070f841&#34;&gt; &#xA;&lt;p&gt;For more information about this novel architecture please visit:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The official Pytorch implementation of the architecture: &lt;a href=&#34;https://github.com/KindXiaoming/pykan&#34;&gt;https://github.com/KindXiaoming/pykan&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;The research paper: &lt;a href=&#34;https://arxiv.org/abs/2404.19756&#34;&gt;https://arxiv.org/abs/2404.19756&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;What is a KAN Convolution?&lt;/h3&gt; &#xA;&lt;p&gt;KAN Convolutions are very similar to convolutions, but instead of applying the dot product between the kernel and the corresponding pixels in the image, we apply a &lt;strong&gt;Learnable Non Linear activation function&lt;/strong&gt; to each element, and then add them up. The kernel of the KAN Convolution is equivalent to a KAN Linear Layer of 4 inputs and 1 output neuron. For each input i, we apply a ϕ_i learnable function, and the resulting pixel of that convolution step is the sum of ϕ_i(x_i). This can be visualized in the following figure.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AntonioTepsich/Convolutional-KANs/master/images/Convs.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Parameters in a KAN Convolution&lt;/h3&gt; &#xA;&lt;p&gt;Suppose that we have a KxK kernel. In this case, for each element of this matrix we have a ϕ, which its parameter count is: gridsize + 1. For implementation issues, efficient kan defines: &lt;img src=&#34;https://raw.githubusercontent.com/AntonioTepsich/Convolutional-KANs/master/images/splines.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This gives more expressability to the activation function b. So the parameter count for a linear layer is gridsize + 2. So in total we have K²(gridsize + 2) parameters for KAN Convolution, vs only K² for a common convolution. Consider that gridsize is typically (in our experiments) between k and k², but k tends to be a small value, between 2 and 16.&lt;/p&gt; &#xA;&lt;h2&gt;Preliminary Evaluations&lt;/h2&gt; &#xA;&lt;p&gt;The different architectures we have tested are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;KAN Convolutional Layers connected to Kan Linear Layers (KKAN)&lt;/li&gt; &#xA; &lt;li&gt;Kan Convolutional Layers connected to a MLP (CKAN)&lt;/li&gt; &#xA; &lt;li&gt;CKAN with Batch Normalization between convolutions (CKAN_BN)&lt;/li&gt; &#xA; &lt;li&gt;ConvNet (Classic Convolutions connected to a MLP) (ConvNet)&lt;/li&gt; &#xA; &lt;li&gt;Simple MLPs&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AntonioTepsich/Convolutional-KANs/master/images/experiment_28x28.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Have a look at &lt;code&gt;experiment_28x28.ipynb&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Discussion&lt;/h3&gt; &#xA;&lt;p&gt;The implementation of KAN Convolutions is a promising idea, although it is still in its early stages. We have conducted some preliminary experiments to evaluate the performance of KAN Convolutions. The reason we say preliminary is because we wanted to publish this idea as soon as possible, so that the community can start working on it.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Here we have some results:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Test Accuracy&lt;/th&gt; &#xA;   &lt;th&gt;Test Precision&lt;/th&gt; &#xA;   &lt;th&gt;Test Recall&lt;/th&gt; &#xA;   &lt;th&gt;Test F1 Score&lt;/th&gt; &#xA;   &lt;th&gt;Number of Parameters&lt;/th&gt; &#xA;   &lt;th&gt;Convolutional Layers&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1 Layer MLP&lt;/td&gt; &#xA;   &lt;td&gt;0.922&lt;/td&gt; &#xA;   &lt;td&gt;0.922&lt;/td&gt; &#xA;   &lt;td&gt;0.921&lt;/td&gt; &#xA;   &lt;td&gt;0.921&lt;/td&gt; &#xA;   &lt;td&gt;7850&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ConvNet (Small)&lt;/td&gt; &#xA;   &lt;td&gt;0.976&lt;/td&gt; &#xA;   &lt;td&gt;0.976&lt;/td&gt; &#xA;   &lt;td&gt;0.976&lt;/td&gt; &#xA;   &lt;td&gt;0.976&lt;/td&gt; &#xA;   &lt;td&gt;2740&lt;/td&gt; &#xA;   &lt;td&gt;[5,1] k=[3,3]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ConvNet (Medium)&lt;/td&gt; &#xA;   &lt;td&gt;0.991&lt;/td&gt; &#xA;   &lt;td&gt;0.991&lt;/td&gt; &#xA;   &lt;td&gt;0.991&lt;/td&gt; &#xA;   &lt;td&gt;0.991&lt;/td&gt; &#xA;   &lt;td&gt;157 030&lt;/td&gt; &#xA;   &lt;td&gt;[5,5] k=[3,3]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ConvNet (Big)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.995&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.995&lt;/td&gt; &#xA;   &lt;td&gt;0.995&lt;/td&gt; &#xA;   &lt;td&gt;0.995&lt;/td&gt; &#xA;   &lt;td&gt;887 530&lt;/td&gt; &#xA;   &lt;td&gt;[32,1,2,1] k=[5,5,3,3]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;KANConv &amp;amp; MLP&lt;/td&gt; &#xA;   &lt;td&gt;0.985&lt;/td&gt; &#xA;   &lt;td&gt;0.985&lt;/td&gt; &#xA;   &lt;td&gt;0.984&lt;/td&gt; &#xA;   &lt;td&gt;0.984&lt;/td&gt; &#xA;   &lt;td&gt;163 726&lt;/td&gt; &#xA;   &lt;td&gt;KanConvs[5,5] k =[3,3]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Simple Conv &amp;amp; KAN&lt;/td&gt; &#xA;   &lt;td&gt;0.980&lt;/td&gt; &#xA;   &lt;td&gt;0.980&lt;/td&gt; &#xA;   &lt;td&gt;0.980&lt;/td&gt; &#xA;   &lt;td&gt;0.980&lt;/td&gt; &#xA;   &lt;td&gt;37 030&lt;/td&gt; &#xA;   &lt;td&gt;[5,1] k=[3,3]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;KKAN&lt;/td&gt; &#xA;   &lt;td&gt;0.987&lt;/td&gt; &#xA;   &lt;td&gt;0.987&lt;/td&gt; &#xA;   &lt;td&gt;0.987&lt;/td&gt; &#xA;   &lt;td&gt;0.987&lt;/td&gt; &#xA;   &lt;td&gt;94 650&lt;/td&gt; &#xA;   &lt;td&gt;KanConvs[5,5] k =[3,3]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;em&gt;The lists in Convolutional Layers contain in each element the number of convolutions and then the corresponding kernel size.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Based on a 28x28 MNIST dataset, we can observe that the KANConv &amp;amp; MLP model achieves acceptable accuracy compared to the ConvNet (Big). However, the difference is that the number of parameters required by the KANConv &amp;amp; MLP is seven times less than those needed by the standard ConvNet. Also the KKAN achieved 0.04 less Accuracy than ConvNet Medium, with almst half the parameter count (94k vs 157k), which shows the potential of the architecture. Experiments on more datasets need to be conducted to take certain conclussions on this&lt;/p&gt; &#xA;&lt;p&gt;We are aware that there are many hyperparameters to tune, and many experiments to conduct. In the coming days and weeks we will be thoroughly tuning the hyperparameters of our model and the models we use to compare. We have tried some variations in the hyperparameters and architectures, but it was heuristically and not done with any precise method. We also recognize that we have not used large or more complex datasets because of computational power and time reasons and we are working on that.&lt;br&gt; We will be conducting experiments on more complex datasets in the future, this implies that the amount parameters of the KANS will increase since we will need to implement more Kan Convolutional layers.&lt;/p&gt; &#xA;&lt;h3&gt;Conclusion&lt;/h3&gt; &#xA;&lt;p&gt;At the moment we aren&#39;t seeing a significant improvement in the performance of the KAN Convolutional Networks compared to the traditional Convolutional Networks. We believe that this is due to the fact that we are using simple datasets and small models since the strength of our architecture lies in its requirement for significantly fewer parameters compared to the best architecture we have tried (ConvNet Big, which is an unfair comparison because of its size). The comparison between 2 equal convolutional and KAN convolutional layers with the same MLP connected at the end showed a small win to the classic approach, getting 0.06 better accuracy, while the KAN convolutions and a KAN Linear Layer with almost half the parameter count got 0.04 less Accuracy. We are confident that as we increase the complexity of the models and the datasets we will see a significant improvement in the performance of the KAN Convolutional Networks. But also the parameter count of our models will grow faster with higher dimentional inputs.&lt;/p&gt; &#xA;&lt;h3&gt;Work in progress&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Experiments on more complex datasets.&lt;/li&gt; &#xA; &lt;li&gt;Hyperparameter tuning with Random Search.&lt;/li&gt; &#xA; &lt;li&gt;Experiments with more architectures.&lt;/li&gt; &#xA; &lt;li&gt;Dinamically updating grid ranges.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone git@github.com/AntonioTepsich/ckan.git&#xA;cd ckan&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;p&gt;Just copy the file &lt;code&gt;kan_convolutional&lt;/code&gt; to your project and import it.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from kan_convolutional.KANConv import KAN_Convolutional_Layer&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Example&lt;/h1&gt; &#xA;&lt;p&gt;Construct a KANConv for MNIST&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from torch import nn&#xA;import torch.nn.functional as F&#xA;&#xA;from kan_convolutional.KANConv import KAN_Convolutional_Layer&#xA;&#xA;class KANC_MLP(nn.Module):&#xA;    def __init__(self,device: str = &#39;cpu&#39;):&#xA;        super().__init__()&#xA;        self.conv1 = KAN_Convolutional_Layer(&#xA;            n_convs = 5,&#xA;            kernel_size= (3,3),&#xA;            device = device&#xA;        )&#xA;&#xA;        self.conv2 = KAN_Convolutional_Layer(&#xA;            n_convs = 5,&#xA;            kernel_size = (3,3),&#xA;            device = device&#xA;        )&#xA;&#xA;        self.pool1 = nn.MaxPool2d(&#xA;            kernel_size=(2, 2)&#xA;        )&#xA;        &#xA;        self.flat = nn.Flatten() &#xA;        &#xA;        self.linear1 = nn.Linear(625, 256)&#xA;        self.linear2 = nn.Linear(256, 10)&#xA;&#xA;&#xA;    def forward(self, x):&#xA;        x = self.conv1(x)&#xA;&#xA;        x = self.pool1(x)&#xA;&#xA;        x = self.conv2(x)&#xA;        x = self.pool1(x)&#xA;        x = self.flat(x)&#xA;        x = self.linear1(x)&#xA;        x = self.linear2(x)&#xA;        x = F.log_softmax(x, dim=1)&#xA;        return x&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We invite the community to join us in advancing this project. There are numerous ways to contribute. You are welcome to contribute by submitting pull requests or opening issues to share ideas and suggest enhancements. Together, we can unlock the full possibilities of KAN and push the boundaries of Computer Vision ❤️.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>severian42/Vodalus-Expert-LLM-Forge</title>
    <updated>2024-05-17T01:30:03Z</updated>
    <id>tag:github.com,2024-05-17:/severian42/Vodalus-Expert-LLM-Forge</id>
    <link href="https://github.com/severian42/Vodalus-Expert-LLM-Forge" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Dataset Crafting and Efficient Fine-Tuning Using Only Free Open-Source Tools&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Vodalus Expert LLM Forge&lt;/h1&gt; &#xA;&lt;h3&gt;Dataset Crafting and Efficient Fine-Tuning Using Only Free Open-Source Tools&lt;/h3&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/severian42/Vodalus-Expert-LLM-Forge/assets/133655553/1b926eff-41ed-4516-a128-c9e3edce2770&#34; alt=&#34;vodalus-readme&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Stack Components Overview&lt;/h2&gt; &#xA;&lt;p&gt;The Vodalus Expert LLM Forge includes several key components and functionalities:&lt;/p&gt; &#xA;&lt;h3&gt;&lt;em&gt;Datasets:&lt;/em&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Data Generation&lt;/strong&gt;: Utilizes local language models (LLMs) to generate synthetic data based on Wikipedia content. See &lt;code&gt;main.py&lt;/code&gt; for implementation details.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;LLM Interaction&lt;/strong&gt;: Manages interactions with LLMs through the &lt;code&gt;llm_handler.py&lt;/code&gt;, which configures and handles messaging with the LLM.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;RAG and Wikipedia Content Processing&lt;/strong&gt;: Uses RAG as well as processes and searches Wikipedia content to find relevant details to use as ground truth.&lt;/p&gt; &lt;p&gt;&lt;em&gt;While I&#39;m releasing this tool for free, I&#39;ve also completed an extensive tutorial/course with lots of videos and instructions that guide you through each step of maximizing the potential of this stack and also teach you about the theory and concepts behind each part of the process. This course is available for purchase at &lt;a href=&#34;https://ko-fi.com/s/076479f834&#34;&gt;Vodalus LLM Course&lt;/a&gt; and is designed to enhance your experience and results with the Vodalus Expert LLM Forge.&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/severian42/Vodalus-Expert-LLM-Forge/assets/133655553/418ddde4-8073-4b6b-9a01-d63863d41782&#34; alt=&#34;Vodalus - Starting the Crafting&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;&lt;em&gt;Fine-Tuning and Quantization:&lt;/em&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Model Training and Fine-Tuning&lt;/strong&gt;: Supports training and fine-tuning of MLX models with custom datasets, as detailed in the MLX_Fine-Tuning guide.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Quantizing Models&lt;/strong&gt;: Guides on quantizing models to GGUF format for efficient local execution, as described in the Quantize_GGUF guide.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Interactive Notebooks&lt;/strong&gt;: Provides Jupyter notebooks for training and fine-tuning models, such as &lt;code&gt;mlx-fine-tuning.ipynb&lt;/code&gt; and &lt;code&gt;convert_to_gguf.ipynb&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;&lt;em&gt;Designed for All Levels of Users:&lt;/em&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Comprehensive Documentation&lt;/strong&gt;: Each component is accompanied by detailed guides and instructions to assist users in setup, usage, and customization.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For more detailed information on each component, refer to the respective guides and source files included in the repository.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Getting Started With Vodalus Dataset Generator&lt;/h2&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ensure Python is installed on your system.&lt;/li&gt; &#xA; &lt;li&gt;Familiarity with basic command line operations is helpful.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the repository to your local machine.&lt;/li&gt; &#xA; &lt;li&gt;Navigate to the project directory in your command line interface.&lt;/li&gt; &#xA; &lt;li&gt;Run the following commands to set up the environment:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;Create env: conda create -n vodalus -y&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;conda activate vodalus&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Running the Application&lt;/h3&gt; &#xA;&lt;p&gt;Execute the main script to start data generation:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;python main.py&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Key Components&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;code&gt;main.py&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Imports and Setup&lt;/strong&gt;: Imports libraries and modules, sets the provider for the LLM.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Data Generation (&lt;code&gt;generate_data&lt;/code&gt; function)&lt;/strong&gt;: Fetches Wikipedia content, constructs prompts, and generates data using the LLM.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Execution (&lt;code&gt;main&lt;/code&gt; function)&lt;/strong&gt;: Manages the data generation process using multiple workers for efficiency.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;&lt;code&gt;llm_handler.py&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;OpenAI Client Configuration&lt;/strong&gt;: Sets up the client for interacting with the LLM.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Message Handling Functions&lt;/strong&gt;: Includes functions to send messages to the LLM and handle the responses.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;&lt;code&gt;wiki.py&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Model Loading&lt;/strong&gt;: Loads necessary models for understanding and processing Wikipedia content.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Search Function&lt;/strong&gt;: Implements semantic search to find relevant Wikipedia articles based on a query.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage Instructions&lt;/h2&gt; &#xA;&lt;h3&gt;Modifying Topics and System Messages&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To change the topics, edit &lt;code&gt;topics.py&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;To modify system messages, adjust &lt;code&gt;system_messages.py&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Configuration&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Adjust the number of workers and other parameters in &lt;code&gt;params.py&lt;/code&gt; to optimize performance based on your system&#39;s capabilities.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Support This Project&lt;/h1&gt; &#xA;&lt;p&gt;If this project aids your work, please consider supporting it through a donation at my &lt;a href=&#34;https://ko-fi.com/severian42&#34;&gt;www.ko-fi.com/severian42&lt;/a&gt;. Your support helps sustain my further LLM developments and experiments, always with a focus on using those efforts to give back to the LLM community&lt;/p&gt; &#xA;&lt;p&gt;Also, if you love this concept and approach but don&#39;t want to do it yourself, you can hire me and we will work together to accomplish your ideal Expert LLM! I also offer 1-on-1 sessions to help with your LLM needs.&lt;/p&gt; &#xA;&lt;p&gt;Feel free to reach out! You can find the details on my Ko-Fi: &lt;a href=&#34;https://ko-fi.com/severian42&#34;&gt;www.ko-fi.com/severian42&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ko-fi.com/N4N4XZ2TZ&#34;&gt;&lt;img src=&#34;https://ko-fi.com/img/githubbutton_sm.svg?sanitize=true&#34; alt=&#34;ko-fi&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>