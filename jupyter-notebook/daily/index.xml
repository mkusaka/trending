<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-12-03T01:33:18Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>breezedeus/Pix2Text</title>
    <updated>2024-12-03T01:33:18Z</updated>
    <id>tag:github.com,2024-12-03:/breezedeus/Pix2Text</id>
    <link href="https://github.com/breezedeus/Pix2Text" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An Open-Source Python3 tool with SMALL models for recognizing layouts, tables, math formulas (LaTeX), and text in images, converting them into Markdown format. A free alternative to Mathpix, empowering seamless conversion of visual content into text-based representations. 80+ languages are supported.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/breezedeus/Pix2Text/main/docs/figs/p2t-logo.png&#34; width=&#34;250px&#34;&gt; &#xA; &lt;div&gt;&#xA;  &amp;nbsp;&#xA; &lt;/div&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://discord.gg/GgD87WM8Tf&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1200765964434821260?label=Discord&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/pix2text&#34;&gt;&lt;img src=&#34;https://static.pepy.tech/personalized-badge/pix2text?period=total&amp;amp;units=international_system&amp;amp;left_color=grey&amp;amp;right_color=orange&amp;amp;left_text=Downloads&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://visitorbadge.io/status?path=https%3A%2F%2Fgithub.com%2Fbreezedeus%2FPix2Text&#34;&gt;&lt;img src=&#34;https://api.visitorbadge.io/api/visitors?path=https%3A%2F%2Fgithub.com%2Fbreezedeus%2FPix2Text&amp;amp;label=Visitors&amp;amp;countColor=%23ff8a65&amp;amp;style=flat&amp;amp;labelStyle=none&#34; alt=&#34;Visitors&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/breezedeus/Pix2Text/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/breezedeus/pix2text&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/py/pix2text&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/pix2text.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/breezedeus/pix2text&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/forks/breezedeus/pix2text&#34; alt=&#34;forks&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/breezedeus/pix2text&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/breezedeus/pix2text&#34; alt=&#34;stars&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/release-date/breezedeus/pix2text&#34; alt=&#34;last-release&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/last-commit/breezedeus/pix2text&#34; alt=&#34;last-commit&#34;&gt; &lt;a href=&#34;https://twitter.com/breezedeus&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/url?url=https%3A%2F%2Ftwitter.com%2Fbreezedeus&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://pix2text.readthedocs.io&#34;&gt;üìñ Doc&lt;/a&gt; | &lt;a href=&#34;https://p2t.breezedeus.com&#34;&gt;üë©üèª‚Äçüíª Online Service&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/spaces/breezedeus/Pix2Text-Demo&#34;&gt;üë®üèª‚Äçüíª Demo&lt;/a&gt; | &lt;a href=&#34;https://www.breezedeus.com/article/join-group&#34;&gt;üí¨ Contact&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/breezedeus/Pix2Text/main/README_cn.md&#34;&gt;‰∏≠Êñá&lt;/a&gt; | English&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;Pix2Text&lt;/h1&gt; &#xA;&lt;h2&gt;Update 2024.11.17: &lt;strong&gt;V1.1.2&lt;/strong&gt; Released&lt;/h2&gt; &#xA;&lt;p&gt;Major Changes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A new layout analysis model &lt;a href=&#34;https://github.com/opendatalab/DocLayout-YOLO&#34;&gt;DocLayout-YOLO&lt;/a&gt; has been integrated, improving the accuracy of layout analysis.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Update 2024.06.18Ôºö&lt;strong&gt;V1.1.1&lt;/strong&gt; Released&lt;/h2&gt; &#xA;&lt;p&gt;Major changes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support the new mathematical formula detection models (MFD): &lt;a href=&#34;https://huggingface.co/breezedeus/pix2text-mfd&#34;&gt;breezedeus/pix2text-mfd&lt;/a&gt; (&lt;a href=&#34;https://hf-mirror.com/breezedeus/pix2text-mfd&#34;&gt;Mirror&lt;/a&gt;), which significantly improves the accuracy of formula detection.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See details: &lt;a href=&#34;https://www.breezedeus.com/article/p2t-mfd-v1.1.1&#34;&gt;Pix2Text V1.1.1 Released, Bringing Better Mathematical Formula Detection Models | Breezedeus.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Update 2024.04.28: &lt;strong&gt;V1.1&lt;/strong&gt; Released&lt;/h2&gt; &#xA;&lt;p&gt;Major changes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Added layout analysis and table recognition models, supporting the conversion of images with complex layouts into Markdown format. See examples: &lt;a href=&#34;https://pix2text.readthedocs.io/zh-cn/stable/examples_en/&#34;&gt;Pix2Text Online Documentation / Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Added support for converting entire PDF files to Markdown format. See examples: &lt;a href=&#34;https://pix2text.readthedocs.io/zh-cn/stable/examples_en/&#34;&gt;Pix2Text Online Documentation / Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Enhanced the interface with more features, including adjustments to existing interface parameters.&lt;/li&gt; &#xA; &lt;li&gt;Launched the &lt;a href=&#34;https://pix2text.readthedocs.io&#34;&gt;Pix2Text Online Documentation&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Update 2024.02.26: &lt;strong&gt;V1.0&lt;/strong&gt; Released&lt;/h2&gt; &#xA;&lt;p&gt;Main Changes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The Mathematical Formula Recognition (MFR) model employs a new architecture and has been trained on a new dataset, achieving state-of-the-art (SOTA) accuracy. For detailed information, please see: &lt;a href=&#34;https://www.breezedeus.com/article/p2t-v1.0&#34;&gt;Pix2Text V1.0 New Release: The Best Open-Source Formula Recognition Model | Breezedeus.com&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See more at: &lt;a href=&#34;https://raw.githubusercontent.com/breezedeus/Pix2Text/main/docs/RELEASE.md&#34;&gt;RELEASE.md&lt;/a&gt; .&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pix2Text (P2T)&lt;/strong&gt; aims to be a &lt;strong&gt;free and open-source Python&lt;/strong&gt; alternative to &lt;strong&gt;&lt;a href=&#34;https://mathpix.com/&#34;&gt;Mathpix&lt;/a&gt;&lt;/strong&gt;, and it can already accomplish &lt;strong&gt;Mathpix&lt;/strong&gt;&#39;s core functionality. &lt;strong&gt;Pix2Text (P2T) can recognize layouts, tables, images, text, mathematical formulas, and integrate all of these contents into Markdown format. P2T can also convert an entire PDF file (which can contain scanned images or any other format) into Markdown format.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pix2Text (P2T)&lt;/strong&gt; integrates the following models:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Layout Analysis Model&lt;/strong&gt;: &lt;a href=&#34;https://huggingface.co/breezedeus/pix2text-layout&#34;&gt;breezedeus/pix2text-layout&lt;/a&gt; (&lt;a href=&#34;https://hf-mirror.com/breezedeus/pix2text-layout&#34;&gt;Mirror&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Table Recognition Model&lt;/strong&gt;: &lt;a href=&#34;https://huggingface.co/breezedeus/pix2text-table-rec&#34;&gt;breezedeus/pix2text-table-rec&lt;/a&gt; (&lt;a href=&#34;https://hf-mirror.com/breezedeus/pix2text-table-rec&#34;&gt;Mirror&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Text Recognition Engine&lt;/strong&gt;: Supports &lt;strong&gt;80+ languages&lt;/strong&gt; such as &lt;strong&gt;English, Simplified Chinese, Traditional Chinese, Vietnamese&lt;/strong&gt;, etc. For English and Simplified Chinese recognition, it uses the open-source OCR tool &lt;a href=&#34;https://github.com/breezedeus/cnocr&#34;&gt;CnOCR&lt;/a&gt;, while for other languages, it uses the open-source OCR tool &lt;a href=&#34;https://github.com/JaidedAI/EasyOCR&#34;&gt;EasyOCR&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Mathematical Formula Detection Model (MFD)&lt;/strong&gt;: &lt;a href=&#34;https://huggingface.co/breezedeus/pix2text-mfd&#34;&gt;breezedeus/pix2text-mfd&lt;/a&gt; (&lt;a href=&#34;https://hf-mirror.com/breezedeus/pix2text-mfd&#34;&gt;Mirror&lt;/a&gt;). Implemented based on &lt;a href=&#34;https://github.com/breezedeus/cnstd&#34;&gt;CnSTD&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Mathematical Formula Recognition Model (MFR)&lt;/strong&gt;: &lt;a href=&#34;https://huggingface.co/breezedeus/pix2text-mfr&#34;&gt;breezedeus/pix2text-mfr&lt;/a&gt; (&lt;a href=&#34;https://hf-mirror.com/breezedeus/pix2text-mfr&#34;&gt;Mirror&lt;/a&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Several models are contributed by other open-source authors, and their contributions are highly appreciated.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/breezedeus/Pix2Text/main/docs/figs/arch-flow.jpg&#34; alt=&#34;Pix2Text Arch Flow&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;For detailed explanations, please refer to the &lt;a href=&#34;https://pix2text.readthedocs.io/zh-cn/stable/models/&#34;&gt;Pix2Text Online Documentation/Models&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;As a Python3 toolkit, P2T may not be very user-friendly for those who are not familiar with Python. Therefore, we also provide a &lt;strong&gt;&lt;a href=&#34;https://p2t.breezedeus.com&#34;&gt;free-to-use P2T Online Web&lt;/a&gt;&lt;/strong&gt;, where you can directly upload images and get P2T parsing results. The web version uses the latest models, resulting in better performance compared to the open-source models.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;re interested, feel free to add the assistant as a friend by scanning the QR code and mentioning &lt;code&gt;p2t&lt;/code&gt;. The assistant will regularly invite everyone to join the group where the latest developments related to P2T tools will be announced:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://pix2text.readthedocs.io/zh-cn/stable/figs/wx-qr-code.JPG&#34; alt=&#34;Wechat-QRCode&#34; width=&#34;300px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;The author also maintains a &lt;strong&gt;Knowledge Planet&lt;/strong&gt; &lt;a href=&#34;https://t.zsxq.com/FEYZRJQ&#34;&gt;&lt;strong&gt;P2T/CnOCR/CnSTD Private Group&lt;/strong&gt;&lt;/a&gt;, where questions are answered promptly. You&#39;re welcome to join. The &lt;strong&gt;knowledge planet private group&lt;/strong&gt; will also gradually release some private materials related to P2T/CnOCR/CnSTD, including &lt;strong&gt;some unreleased models&lt;/strong&gt;, &lt;strong&gt;discounts on purchasing premium models&lt;/strong&gt;, &lt;strong&gt;code snippets for different application scenarios&lt;/strong&gt;, and answers to difficult problems encountered during use. The planet will also publish the latest research materials related to P2T/OCR/STD.&lt;/p&gt; &#xA;&lt;p&gt;For more contact method, please refer to &lt;a href=&#34;https://pix2text.readthedocs.io/zh-cn/stable/contact/&#34;&gt;Contact&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;List of Supported Languages&lt;/h2&gt; &#xA;&lt;p&gt;The text recognition engine of Pix2Text supports &lt;strong&gt;&lt;code&gt;80+&lt;/code&gt; languages&lt;/strong&gt;, including &lt;strong&gt;English, Simplified Chinese, Traditional Chinese, Vietnamese&lt;/strong&gt;, etc. Among these, &lt;strong&gt;English&lt;/strong&gt; and &lt;strong&gt;Simplified Chinese&lt;/strong&gt; recognition utilize the open-source OCR tool &lt;strong&gt;&lt;a href=&#34;https://github.com/breezedeus/cnocr&#34;&gt;CnOCR&lt;/a&gt;&lt;/strong&gt;, while recognition for other languages employs the open-source OCR tool &lt;strong&gt;&lt;a href=&#34;https://github.com/JaidedAI/EasyOCR&#34;&gt;EasyOCR&lt;/a&gt;&lt;/strong&gt;. Special thanks to the respective authors.&lt;/p&gt; &#xA;&lt;p&gt;List of &lt;strong&gt;Supported Languages&lt;/strong&gt; and &lt;strong&gt;Language Codes&lt;/strong&gt; are shown below:&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;‚Üì‚Üì‚Üì Click to show details ‚Üì‚Üì‚Üì&lt;/summary&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Language&lt;/th&gt; &#xA;    &lt;th&gt;Code Name&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Abaza&lt;/td&gt; &#xA;    &lt;td&gt;abq&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Adyghe&lt;/td&gt; &#xA;    &lt;td&gt;ady&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Afrikaans&lt;/td&gt; &#xA;    &lt;td&gt;af&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Angika&lt;/td&gt; &#xA;    &lt;td&gt;ang&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Arabic&lt;/td&gt; &#xA;    &lt;td&gt;ar&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Assamese&lt;/td&gt; &#xA;    &lt;td&gt;as&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Avar&lt;/td&gt; &#xA;    &lt;td&gt;ava&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Azerbaijani&lt;/td&gt; &#xA;    &lt;td&gt;az&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Belarusian&lt;/td&gt; &#xA;    &lt;td&gt;be&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Bulgarian&lt;/td&gt; &#xA;    &lt;td&gt;bg&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Bihari&lt;/td&gt; &#xA;    &lt;td&gt;bh&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Bhojpuri&lt;/td&gt; &#xA;    &lt;td&gt;bho&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Bengali&lt;/td&gt; &#xA;    &lt;td&gt;bn&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Bosnian&lt;/td&gt; &#xA;    &lt;td&gt;bs&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Simplified Chinese&lt;/td&gt; &#xA;    &lt;td&gt;ch_sim&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Traditional Chinese&lt;/td&gt; &#xA;    &lt;td&gt;ch_tra&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Chechen&lt;/td&gt; &#xA;    &lt;td&gt;che&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Czech&lt;/td&gt; &#xA;    &lt;td&gt;cs&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Welsh&lt;/td&gt; &#xA;    &lt;td&gt;cy&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Danish&lt;/td&gt; &#xA;    &lt;td&gt;da&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Dargwa&lt;/td&gt; &#xA;    &lt;td&gt;dar&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;German&lt;/td&gt; &#xA;    &lt;td&gt;de&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;English&lt;/td&gt; &#xA;    &lt;td&gt;en&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Spanish&lt;/td&gt; &#xA;    &lt;td&gt;es&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Estonian&lt;/td&gt; &#xA;    &lt;td&gt;et&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Persian (Farsi)&lt;/td&gt; &#xA;    &lt;td&gt;fa&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;French&lt;/td&gt; &#xA;    &lt;td&gt;fr&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Irish&lt;/td&gt; &#xA;    &lt;td&gt;ga&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Goan Konkani&lt;/td&gt; &#xA;    &lt;td&gt;gom&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Hindi&lt;/td&gt; &#xA;    &lt;td&gt;hi&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Croatian&lt;/td&gt; &#xA;    &lt;td&gt;hr&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Hungarian&lt;/td&gt; &#xA;    &lt;td&gt;hu&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Indonesian&lt;/td&gt; &#xA;    &lt;td&gt;id&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Ingush&lt;/td&gt; &#xA;    &lt;td&gt;inh&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Icelandic&lt;/td&gt; &#xA;    &lt;td&gt;is&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Italian&lt;/td&gt; &#xA;    &lt;td&gt;it&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Japanese&lt;/td&gt; &#xA;    &lt;td&gt;ja&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kabardian&lt;/td&gt; &#xA;    &lt;td&gt;kbd&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kannada&lt;/td&gt; &#xA;    &lt;td&gt;kn&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Korean&lt;/td&gt; &#xA;    &lt;td&gt;ko&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kurdish&lt;/td&gt; &#xA;    &lt;td&gt;ku&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Latin&lt;/td&gt; &#xA;    &lt;td&gt;la&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Lak&lt;/td&gt; &#xA;    &lt;td&gt;lbe&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Lezghian&lt;/td&gt; &#xA;    &lt;td&gt;lez&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Lithuanian&lt;/td&gt; &#xA;    &lt;td&gt;lt&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Latvian&lt;/td&gt; &#xA;    &lt;td&gt;lv&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Magahi&lt;/td&gt; &#xA;    &lt;td&gt;mah&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Maithili&lt;/td&gt; &#xA;    &lt;td&gt;mai&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Maori&lt;/td&gt; &#xA;    &lt;td&gt;mi&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Mongolian&lt;/td&gt; &#xA;    &lt;td&gt;mn&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Marathi&lt;/td&gt; &#xA;    &lt;td&gt;mr&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Malay&lt;/td&gt; &#xA;    &lt;td&gt;ms&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Maltese&lt;/td&gt; &#xA;    &lt;td&gt;mt&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Nepali&lt;/td&gt; &#xA;    &lt;td&gt;ne&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Newari&lt;/td&gt; &#xA;    &lt;td&gt;new&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Dutch&lt;/td&gt; &#xA;    &lt;td&gt;nl&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Norwegian&lt;/td&gt; &#xA;    &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Occitan&lt;/td&gt; &#xA;    &lt;td&gt;oc&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Pali&lt;/td&gt; &#xA;    &lt;td&gt;pi&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Polish&lt;/td&gt; &#xA;    &lt;td&gt;pl&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Portuguese&lt;/td&gt; &#xA;    &lt;td&gt;pt&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Romanian&lt;/td&gt; &#xA;    &lt;td&gt;ro&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Russian&lt;/td&gt; &#xA;    &lt;td&gt;ru&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Serbian (cyrillic)&lt;/td&gt; &#xA;    &lt;td&gt;rs_cyrillic&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Serbian (latin)&lt;/td&gt; &#xA;    &lt;td&gt;rs_latin&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Nagpuri&lt;/td&gt; &#xA;    &lt;td&gt;sck&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Slovak&lt;/td&gt; &#xA;    &lt;td&gt;sk&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Slovenian&lt;/td&gt; &#xA;    &lt;td&gt;sl&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Albanian&lt;/td&gt; &#xA;    &lt;td&gt;sq&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Swedish&lt;/td&gt; &#xA;    &lt;td&gt;sv&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Swahili&lt;/td&gt; &#xA;    &lt;td&gt;sw&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Tamil&lt;/td&gt; &#xA;    &lt;td&gt;ta&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Tabassaran&lt;/td&gt; &#xA;    &lt;td&gt;tab&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Telugu&lt;/td&gt; &#xA;    &lt;td&gt;te&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Thai&lt;/td&gt; &#xA;    &lt;td&gt;th&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Tajik&lt;/td&gt; &#xA;    &lt;td&gt;tjk&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Tagalog&lt;/td&gt; &#xA;    &lt;td&gt;tl&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Turkish&lt;/td&gt; &#xA;    &lt;td&gt;tr&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Uyghur&lt;/td&gt; &#xA;    &lt;td&gt;ug&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Ukranian&lt;/td&gt; &#xA;    &lt;td&gt;uk&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Urdu&lt;/td&gt; &#xA;    &lt;td&gt;ur&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Uzbek&lt;/td&gt; &#xA;    &lt;td&gt;uz&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Vietnamese&lt;/td&gt; &#xA;    &lt;td&gt;vi&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;Ref: &lt;a href=&#34;https://www.jaided.ai/easyocr/&#34;&gt;Supported Languages&lt;/a&gt; .&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Online Service&lt;/h2&gt; &#xA;&lt;p&gt;Everyone can use the &lt;strong&gt;&lt;a href=&#34;https://p2t.breezedeus.com&#34;&gt;P2T Online Service&lt;/a&gt;&lt;/strong&gt; for free, with a daily limit of 10,000 characters per account, which should be sufficient for normal use. &lt;em&gt;Please refrain from bulk API calls, as machine resources are limited, and this could prevent others from accessing the service.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Due to hardware constraints, the Online Service currently only supports &lt;strong&gt;Simplified Chinese&lt;/strong&gt; and &lt;strong&gt;English&lt;/strong&gt; languages. To try the models in other languages, please use the following &lt;strong&gt;Online Demo&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Online Demo ü§ó&lt;/h2&gt; &#xA;&lt;p&gt;You can also try the &lt;strong&gt;&lt;a href=&#34;https://huggingface.co/spaces/breezedeus/Pix2Text-Demo&#34;&gt;Online Demo&lt;/a&gt;&lt;/strong&gt; to see the performance of &lt;strong&gt;P2T&lt;/strong&gt; in various languages. However, the online demo operates on lower hardware specifications and may be slower. For Simplified Chinese or English images, it is recommended to use the &lt;strong&gt;&lt;a href=&#34;https://p2t.breezedeus.com&#34;&gt;P2T Online Service&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;See: &lt;a href=&#34;https://pix2text.readthedocs.io/zh-cn/stable/examples_en/&#34;&gt;Pix2Text Online Documentation/Examples&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;See: &lt;a href=&#34;https://pix2text.readthedocs.io/zh-cn/stable/usage/&#34;&gt;Pix2Text Online Documentation/Usage&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Models&lt;/h2&gt; &#xA;&lt;p&gt;See: &lt;a href=&#34;https://pix2text.readthedocs.io/zh-cn/stable/models/&#34;&gt;Pix2Text Online Documentation/Models&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;Well, one line of command is enough if it goes well.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install pix2text&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you need to recognize languages other than &lt;strong&gt;English&lt;/strong&gt; and &lt;strong&gt;Simplified Chinese&lt;/strong&gt;, please use the following command to install additional packages:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install pix2text[multilingual]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If the installation is slow, you can specify an installation source, such as using the Aliyun source:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install pix2text -i https://mirrors.aliyun.com/pypi/simple&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more information, please refer to: &lt;a href=&#34;https://pix2text.readthedocs.io/zh-cn/stable/install/&#34;&gt;Pix2Text Online Documentation/Install&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Command Line Tool&lt;/h2&gt; &#xA;&lt;p&gt;See: &lt;a href=&#34;https://pix2text.readthedocs.io/zh-cn/stable/command/&#34;&gt;Pix2Text Online Documentation/Command Tool&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;HTTP Service&lt;/h2&gt; &#xA;&lt;p&gt;See: &lt;a href=&#34;https://pix2text.readthedocs.io/zh-cn/stable/command/&#34;&gt;Pix2Text Online Documentation/Command Tool/Start Service&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;MacOS Desktop Application&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://github.com/breezedeus/Pix2Text-Mac&#34;&gt;Pix2Text-Mac&lt;/a&gt; for installing the Pix2Text Desktop App for MacOS.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/breezedeus/Pix2Text-Mac/raw/main/assets/on_menu_bar.jpg&#34; alt=&#34;Pix2Text Mac App&#34; width=&#34;400px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;A cup of coffee for the author&lt;/h2&gt; &#xA;&lt;p&gt;It is not easy to maintain and evolve the project, so if it is helpful to you, please consider &lt;a href=&#34;https://www.breezedeus.com/article/buy-me-coffee&#34;&gt;offering the author a cup of coffee ü•§&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Official code base: &lt;a href=&#34;https://github.com/breezedeus/pix2text&#34;&gt;https://github.com/breezedeus/pix2text&lt;/a&gt;. Please cite it properly.&lt;/p&gt; &#xA;&lt;p&gt;For more information on Pix2Text (P2T), visit: &lt;a href=&#34;https://www.breezedeus.com/article/pix2text&#34;&gt;https://www.breezedeus.com/article/pix2text&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/BiomedParse</title>
    <updated>2024-12-03T01:33:18Z</updated>
    <id>tag:github.com,2024-12-03:/microsoft/BiomedParse</id>
    <link href="https://github.com/microsoft/BiomedParse" rel="alternate"></link>
    <summary type="html">&lt;p&gt;BiomedParse: A Foundation Model for Joint Segmentation, Detection, and Recognition of Biomedical Objects Across Nine Modalities&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;strong&gt;BiomedParse&lt;/strong&gt;&lt;/h1&gt; &#xA;&lt;p&gt;This repository hosts the code and resources for the paper &lt;strong&gt;&#34;A Foundation Model for Joint Segmentation, Detection, and Recognition of Biomedical Objects Across Nine Modalities&#34;&lt;/strong&gt; (published in &lt;a href=&#34;https://aka.ms/biomedparse-paper&#34;&gt;&lt;em&gt;Nature Methods&lt;/em&gt;&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://aka.ms/biomedparse-paper&#34;&gt;&lt;code&gt;Paper&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://microsoft.github.io/BiomedParse/&#34;&gt;&lt;code&gt;Demo&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/microsoft/BiomedParse&#34;&gt;&lt;code&gt;Model&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/datasets/microsoft/BiomedParseData&#34;&gt;&lt;code&gt;Data&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/BiomedParse/main/#Citation&#34;&gt;&lt;code&gt;BibTeX&lt;/code&gt;&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;BiomedParse&lt;/strong&gt; is designed for comprehensive biomedical image analysis. It offers a unified approach to perform &lt;strong&gt;segmentation&lt;/strong&gt;, &lt;strong&gt;detection&lt;/strong&gt;, and &lt;strong&gt;recognition&lt;/strong&gt; across diverse biomedical imaging modalities. By consolidating these tasks, BiomedParse provides an efficient and flexible tool tailored for researchers and practitioners, facilitating the interpretation and analysis of complex biomedical data.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/BiomedParse/main/assets/readmes/biomedparse_prediction_examples.png&#34; alt=&#34;Example Predictions&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Nov. 22, 2024: We added negative prediction p-value example in inference_example_DICOM.ipynb&lt;/li&gt; &#xA; &lt;li&gt;Nov. 18, 2024: BiomedParse is officially online in &lt;a href=&#34;https://aka.ms/biomedparse-paper&#34;&gt;&lt;em&gt;Nature Methods&lt;/em&gt;&lt;/a&gt;!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/microsoft/BiomedParse.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Conda Environment Setup&lt;/h3&gt; &#xA;&lt;h4&gt;Option 1: Directly build the conda environment&lt;/h4&gt; &#xA;&lt;p&gt;Under the project directory, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;conda env create -f environment.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Option 2: Create a new conda environment from scratch&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;conda create -n biomedparse python=3.9.19&#xA;conda activate biomedparse&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install Pytorch&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;conda install pytorch torchvision torchaudio pytorch-cuda=12.4 -c pytorch -c nvidia&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In case there is issue with detectron2 installation, make sure your pytorch version is compatible with CUDA version on your machine at &lt;a href=&#34;https://pytorch.org/&#34;&gt;https://pytorch.org/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Install dependencies&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install -r assets/requirements/requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Dataset&lt;/h2&gt; &#xA;&lt;p&gt;BiomedParseData was created from preprocessing publicly available biomedical image segmentation datasets. Check a subset of our processed datasets on HuggingFace: &lt;a href=&#34;https://huggingface.co/datasets/microsoft/BiomedParseData&#34;&gt;https://huggingface.co/datasets/microsoft/BiomedParseData&lt;/a&gt;. For the source datasets, please check the details here: &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/BiomedParse/main/assets/readmes/DATASET.md&#34;&gt;BiomedParseData&lt;/a&gt;. As a quick start, we&#39;ve samples a tiny demo dataset at biomedparse_datasets/BiomedParseData-Demo&lt;/p&gt; &#xA;&lt;h2&gt;Model Checkpoints&lt;/h2&gt; &#xA;&lt;p&gt;We host our model checkpoints on HuggingFace here: &lt;a href=&#34;https://huggingface.co/microsoft/BiomedParse&#34;&gt;https://huggingface.co/microsoft/BiomedParse&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Step 1. Create pretrained model folder&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mkdir pretrained&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Step 2. Download model checkpoint and put the model in the pretrained folder when running the code. Change file name to biomed_parse.pt&lt;/p&gt; &#xA;&lt;p&gt;Expect future updates of the model as we are making it more robust and powerful based on feedbacks from the community. We recomment using the latest version of the model.&lt;/p&gt; &#xA;&lt;h2&gt;Running Inference with BiomedParse&lt;/h2&gt; &#xA;&lt;p&gt;We‚Äôve streamlined the process for running inference using BiomedParse. Below are details and resources to help you get started.&lt;/p&gt; &#xA;&lt;h3&gt;How to Run Inference&lt;/h3&gt; &#xA;&lt;p&gt;To perform inference with BiomedParse, use the provided example code and resources:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Inference Code&lt;/strong&gt;: Use the example inference script in &lt;code&gt;example_prediction.py&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Sample Images&lt;/strong&gt;: Load and test with the provided example images located in the &lt;code&gt;examples&lt;/code&gt; directory.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Model Configuration&lt;/strong&gt;: The model settings are defined in &lt;code&gt;configs/biomedparse_inference.yaml&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Example Notebooks&lt;/h3&gt; &#xA;&lt;p&gt;We‚Äôve included sample notebooks to guide you through running inference with BiomedParse:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;DICOM Inference Example&lt;/strong&gt;: Check out the &lt;code&gt;inference_examples_DICOM.ipynb&lt;/code&gt; notebook for example using DICOM images.&lt;/li&gt; &#xA; &lt;li&gt;You can also try a quick online demo: &lt;a href=&#34;https://colab.research.google.com/github/microsoft/BiomedParse/blob/main/inference_colab_demo.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Model Setup&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;from PIL import Image&#xA;import torch&#xA;from modeling.BaseModel import BaseModel&#xA;from modeling import build_model&#xA;from utilities.distributed import init_distributed&#xA;from utilities.arguments import load_opt_from_config_files&#xA;from utilities.constants import BIOMED_CLASSES&#xA;from inference_utils.inference import interactive_infer_image&#xA;import numpy as np&#xA;&#xA;# Build model config&#xA;opt = load_opt_from_config_files([&#34;configs/biomedparse_inference.yaml&#34;])&#xA;opt = init_distributed(opt)&#xA;&#xA;# Load model from pretrained weights&#xA;#pretrained_pth = &#39;pretrained/biomed_parse.pt&#39;&#xA;pretrained_pth = &#39;hf_hub:microsoft/BiomedParse&#39;&#xA;&#xA;model = BaseModel(opt, build_model(opt)).from_pretrained(pretrained_pth).eval().cuda()&#xA;with torch.no_grad():&#xA;    model.model.sem_seg_head.predictor.lang_encoder.get_text_embeddings(BIOMED_CLASSES + [&#34;background&#34;], is_eval=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Segmentation On Example Images&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# RGB image input of shape (H, W, 3). Currently only batch size 1 is supported.&#xA;image = Image.open(&#39;examples/Part_1_516_pathology_breast.png&#39;, formats=[&#39;png&#39;])&#xA;image = image.convert(&#39;RGB&#39;)&#xA;# text prompts querying objects in the image. Multiple ones can be provided.&#xA;prompts = [&#39;neoplastic cells&#39;, &#39;inflammatory cells&#39;]&#xA;&#xA;# load ground truth mask&#xA;gt_masks = []&#xA;for prompt in prompts:&#xA;    gt_mask = Image.open(f&#34;examples/Part_1_516_pathology_breast_{prompt.replace(&#39; &#39;, &#39;+&#39;)}.png&#34;, formats=[&#39;png&#39;])&#xA;    gt_mask = 1*(np.array(gt_mask.convert(&#39;RGB&#39;))[:,:,0] &amp;gt; 0)&#xA;    gt_masks.append(gt_mask)&#xA;&#xA;pred_mask = interactive_infer_image(model, image, prompts)&#xA;&#xA;# prediction with ground truth mask&#xA;for i, pred in enumerate(pred_mask):&#xA;    gt = gt_masks[i]&#xA;    dice = (1*(pred&amp;gt;0.5) &amp;amp; gt).sum() * 2.0 / (1*(pred&amp;gt;0.5).sum() + gt.sum())&#xA;    print(f&#39;Dice score for {prompts[i]}: {dice:.4f}&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Detection and recognition inference code are provided in &lt;code&gt;inference_utils/output_processing.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;check_mask_stats()&lt;/code&gt;: Outputs p-value for model-predicted mask for detection.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;combine_masks()&lt;/code&gt;: Combines predictions for non-overlapping masks.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Finetune on Your Own Data&lt;/h2&gt; &#xA;&lt;p&gt;While BiomedParse can take in arbitrary image and text prompt, it can only reasonably segment the targets that it has learned during pretraining! If you have a specific segmentation task that the latest checkpint doesn&#39;t do well, here is the instruction on how to finetune it on your own data.&lt;/p&gt; &#xA;&lt;h3&gt;Raw Image and Annotation&lt;/h3&gt; &#xA;&lt;p&gt;BiomedParse expects images and ground truth masks in 1024x1024 PNG format. For each dataset, put the raw image and mask files in the following format&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;‚îú‚îÄ‚îÄ biomedparse_datasets&#xA;    ‚îú‚îÄ‚îÄ YOUR_DATASET_NAME&#xA;     &amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ train&#xA;     &amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ train_mask&#xA;     &amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ test&#xA;     &amp;nbsp;&amp;nbsp; ‚îî‚îÄ‚îÄ test_mask&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Each folder should contain .png files. The mask files should be binary images where pixels != 0 indicates the foreground region.&lt;/p&gt; &#xA;&lt;h3&gt;File Name Convention&lt;/h3&gt; &#xA;&lt;p&gt;Each file name follows certain convention as&lt;/p&gt; &#xA;&lt;p&gt;[IMAGE-NAME]_[MODALITY]_[SITE].png&lt;/p&gt; &#xA;&lt;p&gt;[IMAGE-NAME] is any string that is unique for one image. The format can be anything. [MODALITY] is a string for the modality, such as &#34;X-Ray&#34; [SITE] is the anatomic site for the image, such as &#34;chest&#34;&lt;/p&gt; &#xA;&lt;p&gt;One image can be associated with multiple masks corresponding to multiple targets in the image. The mask file name convention is&lt;/p&gt; &#xA;&lt;p&gt;[IMAGE-NAME]_[MODALITY]_[SITE]_[TARGET].png&lt;/p&gt; &#xA;&lt;p&gt;[IMAGE-NAME], [MODALITY], and [SITE] are the same with the image file name. [TARGET] is the name of the target with spaces replaced by &#39;+&#39;. E.g. &#34;tube&#34; or &#34;chest+tube&#34;. Make sure &#34;_&#34; doesn&#39;t appear in [TARGET].&lt;/p&gt; &#xA;&lt;h3&gt;Get Final Data File with Text Prompts&lt;/h3&gt; &#xA;&lt;p&gt;In biomedparse_datasets/create-customer-datasets.py, specify YOUR_DATASET_NAME. Run the script with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd biomedparse_datasets&#xA;python create-customer-datasets.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After that, the dataset folder should be of the following format&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;‚îú‚îÄ‚îÄ dataset_name&#xA;     &amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ train&#xA;     &amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ train_mask&#xA;        ‚îú‚îÄ‚îÄ train.json&#xA;     &amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ test&#xA;        ‚îú‚îÄ‚îÄ test_mask&#xA;     &amp;nbsp;&amp;nbsp; ‚îî‚îÄ‚îÄ test.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Register Your Dataset for Training and Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;In datasets/registration/register_biomed_datasets.py, simply add YOUR_DATASET_NAME to the datasets list. Registered datasets are ready to be added to the training and evaluation config file configs/biomed_seg_lang_v1.yaml. Your training dataset is registered as biomed_YOUR_DATASET_NAME_train, and your test dataset is biomed_YOUR_DATASET_NAME_test.&lt;/p&gt; &#xA;&lt;h2&gt;Train BiomedParse&lt;/h2&gt; &#xA;&lt;p&gt;To train the BiomedParse model, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;bash assets/scripts/train.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will continue train the model using the training datasets you specified in configs/biomed_seg_lang_v1.yaml&lt;/p&gt; &#xA;&lt;h2&gt;Evaluate BiomedParse&lt;/h2&gt; &#xA;&lt;p&gt;To evaluate the model, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;bash assets/scripts/eval.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will continue evaluate the model on the test datasets you specified in configs/biomed_seg_lang_v1.yaml. We put BiomedParseData-Demo as the default. You can add any other datasets in the list.&lt;/p&gt; &#xA;&lt;!-- ### Install Docker&#xA;&#xA;In order to make sure the environment is set up correctly, we use run BiomedParse on a Docker image. Follow these commands to install Docker on Ubuntu:&#xA;&#xA;```sh&#xA;sudo apt update&#xA;sudo apt install apt-transport-https ca-certificates curl software-properties-common&#xA;curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -&#xA;sudo add-apt-repository &#34;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable&#34;&#xA;sudo apt update&#xA;apt-cache policy docker-ce&#xA;sudo apt install docker-ce&#xA;```&#xA;&#xA;## Prepare Docker Environment&#xA;&#xA;Specify the project directories in `docker/README.md`.&#xA;&#xA;Run the following commands to set up the Docker environment:&#xA;&#xA;```sh&#xA;bash docker/docker_build.sh&#xA;bash docker/docker_run.sh&#xA;bash docker/setup_inside_docker.sh&#xA;source docker/data_env.sh&#xA;``` --&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Please cite our paper if you use the code, model, or data.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{zhao2024biomedparse,&#xA;  title = {A foundation model for joint segmentation, detection, and recognition of biomedical objects across nine modalities},&#xA;  author = {Zhao, Theodore and Gu, Yu and Yang, Jianwei and Usuyama, Naoto and Lee, Ho Hin and Kiblawi, Sid and Naumann, Tristan and Gao, Jianfeng and Crabtree, Angela and Abel, Jacob and Moung-Wen, Christine and Piening, Brian and Bifulco, Carlo and Wei, Mu and Poon, Hoifung and Wang, Sheng},&#xA;  journal = {Nature Methods},&#xA;  year = {2024},&#xA;  publisher = {Nature Publishing Group UK London},&#xA;  url = {https://www.nature.com/articles/s41592-024-02499-w},&#xA;  doi = {10.1038/s41592-024-02499-w}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage and License Notices&lt;/h2&gt; &#xA;&lt;p&gt;The model described in this repository is provided for research and development use only. The model is not intended for use in clinical decision-making or for any other clinical use, and the performance of the model for clinical use has not been established. You bear sole responsibility for any use of this model, including incorporation into any product intended for clinical use.&lt;/p&gt;</summary>
  </entry>
</feed>