<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-09-02T01:31:13Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>microsoft/BatteryML</title>
    <updated>2023-09-02T01:31:13Z</updated>
    <id>tag:github.com,2023-09-02:/microsoft/BatteryML</id>
    <link href="https://github.com/microsoft/BatteryML" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/BatteryML/main/image/Logo_RGB.png&#34; width=&#34;300&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;BatteryML: An Open-Source Tool for Machine Learning on Battery Degradation&lt;/h1&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;The performance degradation of lithium batteries is a complex electrochemical process, involving factors such as the growth of solid electrolyte interface, lithium precipitation, loss of active materials, etc. Furthermore, this inevitable performance degradation can have a significant impact on critical commercial scenarios, such as causing &#39;range anxiety&#39; for electric vehicle users and affecting the power stability of energy storage systems. Therefore, effectively analyzing and predicting the performance degradation of lithium batteries to provide guidance for early prevention and intervention has become a crucial research topic.&lt;/p&gt; &#xA;&lt;p&gt;To this end, we open source the BatteryML tool to facilitate the research and development of machine learning on battery degradation. We hope BatteryML can empower both battery researchers and data scientists to gain deeper insights from battery degradation data and build more powerful models for accurate predictions and early interventions.&lt;/p&gt; &#xA;&lt;h2&gt;Framework&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/BatteryML/main/image/framework.png&#34; width=&#34;800&#34;&gt; &#xA;&lt;h2&gt;Highlights:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Open-source and Community-driven:&lt;/strong&gt; BatteryML is an open-source project for battery degradation modeling, encouraging contributions and collaboration from the communities of both computer science and battery research to push the frontiers of this crucial field.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Comprehensive Dataset Collection:&lt;/strong&gt; BatteryML includes a comprehensive dataset collection, allowing easy accesses to most publicly available battery data.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Preprocessing and Feature Engineering:&lt;/strong&gt; Our tool offers built-in data preprocessing and feature engineering capabilities, making it easier for researchers and developers to prepare ready-to-use battery datasets for machine learning.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Wide Range of Models:&lt;/strong&gt; BatteryML already includes most classic models in the literature, enabling developers to quickly compare and benchmark different approaches.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Extensible and Customizable:&lt;/strong&gt; BatteryML provides flexible interfaces to support further extensions and customizations, making it a versatile tool for potential applications in battery research.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;h3&gt;Install the dependencies&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Download Raw Data and Run Preprocessing Scripts&lt;/h3&gt; &#xA;&lt;p&gt;To begin, download the raw data and execute the preprocessing scripts as per the provided &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/BatteryML/main/dataprepare.md&#34;&gt;instruction&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Run Pipeline&lt;/h3&gt; &#xA;&lt;p&gt;To get started, simply configure the data, features, models, etc. in the config file. Once you&#39;ve set everything up, run the following code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scripts.pipeline import Pipeline&#xA;&#xA;pipeline = Pipeline(config_path=`path/to/your/config`,&#xA;                    device=&#39;cuda&#39;,&#xA;                    metric=&#39;RMSE&#39;,&#xA;                    workspace=&#39;workspaces&#39;&#xA;&#xA;train_loss , test_loss = pipeline.train()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: Replace &lt;code&gt;path/to/your/config&lt;/code&gt; with the actual config_path.&lt;/p&gt; &#xA;&lt;p&gt;Besides, we have prepared an example &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/BatteryML/main/baseline.ipynb&#34;&gt;baseline&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;By leveraging BatteryML, researchers can gain valuable insights into the latest advancements in battery prediction and materials science, enabling them to conduct experiments efficiently and effectively. We invite you to join us in our journey to accelerate battery research and innovation by contributing to and utilizing BatteryML for your research endeavors.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ktynski/Marketing_Automations_Notebooks_With_GPT</title>
    <updated>2023-09-02T01:31:13Z</updated>
    <id>tag:github.com,2023-09-02:/ktynski/Marketing_Automations_Notebooks_With_GPT</id>
    <link href="https://github.com/ktynski/Marketing_Automations_Notebooks_With_GPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A collection of automations and experiments exploring the applications of generative AI in Marketing, SEO, and Public Relations&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;20+ Free Scripts for Automating SEO, Content Marketing, PR, and Social Media with Artificial Intelligence&lt;/h1&gt; &#xA;&lt;h2&gt;This repo contains 20+ Google Colab experiments that leverage AI to partially or fully automate many different discrete Content Marketing, PR, Social Media, and SEO tasks written by Kristin Tynski (&lt;a href=&#34;mailto:Kristin@Frac.tl&#34;&gt;Kristin@Frac.tl&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;h1&gt;üé• Leveraging SOTA MultiModal AI for Video Understanding - An Iterative Approach to Replicating Viral Success on TikTok üêç&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ktynski/Marketing_Automations_Notebooks_With_GPT/raw/main/Automated_TikTok_Video_Understanding_for_Social_Media_Strategy_(public).ipynb&#34;&gt;Google Colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This script utilizes Apify API for TikTok data collection, OpenAI Whisper for automated transcription, and Vicuna multi-modal AI for video understanding to provide automated and customized analysis of TikTok videos. It identifies key factors for viral success on TikTok and generates a structured report.&lt;/p&gt; &#xA;&lt;p&gt;üîë Key features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Automated data collection&lt;/li&gt; &#xA; &lt;li&gt;Audio separation and transcription&lt;/li&gt; &#xA; &lt;li&gt;Customized video understanding with state-of-the-art AI&lt;/li&gt; &#xA; &lt;li&gt;Aggregated analysis to identify viral factors&lt;/li&gt; &#xA; &lt;li&gt;Structured report generation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìú Requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Apify API key&lt;/li&gt; &#xA; &lt;li&gt;OpenAI API key&lt;/li&gt; &#xA; &lt;li&gt;Google Colab+&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;üì∞ Comprehensive News Media Monitoring &amp;amp; Analysis Using Clustering üìä&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ktynski/Marketing_Automations_Notebooks_With_GPT/raw/main/Automatic_Brand_or_Entity_News_Media_Monitoring_and_Analysis_by_Kristin_frac_tl_(public).ipynb&#34;&gt;Google Colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This script scrapes Google News for relevant articles, clusters them using sentence embeddings, and analyzes each cluster to identify key narratives, perspectives, biases, etc. using GPT-3.&lt;/p&gt; &#xA;&lt;p&gt;üîë Key features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Automated news data collection&lt;/li&gt; &#xA; &lt;li&gt;Clustering articles using sentence embeddings&lt;/li&gt; &#xA; &lt;li&gt;GPT-3 powered analysis of clusters&lt;/li&gt; &#xA; &lt;li&gt;Structured analysis focusing on themes, biases, emotions, etc.&lt;/li&gt; &#xA; &lt;li&gt;Final report summarizing key insights&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìú Requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;SerpAPI Key&lt;/li&gt; &#xA; &lt;li&gt;OpenAI API Key&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;üìà Automated Keyword Clustering for Content Gap Analysis üïµÔ∏è&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ktynski/Marketing_Automations_Notebooks_With_GPT/raw/main/Automatic_Content_Gap_Report_and_Analysis_With_Clustering_and_Cluster_Descriptions_(Public).ipynb&#34;&gt;Google Colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This script performs automated keyword research and clustering to identify content gaps. It integrates with SEMrush API, generates embeddings using SentenceTransformers, clusters keywords using K-Means, and generates cluster descriptions using GPT-3.&lt;/p&gt; &#xA;&lt;p&gt;üîë Key features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;SEMrush API integration for keyword data&lt;/li&gt; &#xA; &lt;li&gt;Automated embedding generation&lt;/li&gt; &#xA; &lt;li&gt;K-Means clustering of keywords&lt;/li&gt; &#xA; &lt;li&gt;GPT-3 powered cluster descriptions&lt;/li&gt; &#xA; &lt;li&gt;Content gap identification&lt;/li&gt; &#xA; &lt;li&gt;Cluster analysis report&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìú Requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OpenAI API Key&lt;/li&gt; &#xA; &lt;li&gt;SEMrush API Key&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;üì∞ Automated Long-form Article Generation with Semantic SEO Optimization üìù&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ktynski/Marketing_Automations_Notebooks_With_GPT/raw/main/Automatic_Long_Form_Article_Generation_With_Semantic_SEO_Driven_Outlines_and_Iteration_(Public).ipynb&#34;&gt;Google Colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This script generates high-quality long-form articles leveraging semantic SEO data. It scrapes Google results, performs NLP analysis, generates outlines optimized for semantic SEO metrics, breaks outlines into sections, and iteratively improves content using GPT-3.&lt;/p&gt; &#xA;&lt;p&gt;üîë Key features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Automated data collection from Google&lt;/li&gt; &#xA; &lt;li&gt;NLP analysis of top results&lt;/li&gt; &#xA; &lt;li&gt;Semantic SEO optimized outline generation&lt;/li&gt; &#xA; &lt;li&gt;Iterative long-form content generation&lt;/li&gt; &#xA; &lt;li&gt;GPT-3 powered content improvement&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìú Requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OpenAI API Key&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;üí° Automatic Newsjacking Content Ideation using Clustering üí≠&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ktynski/Marketing_Automations_Notebooks_With_GPT/raw/main/Automatic_Newsjacking_Ideation_and_Trend_Analysis_(Public).ipynb&#34;&gt;Google Colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This script collects news articles on a topic, clusters them using sentence embeddings, summarizes clusters using GPT-3, and generates newsjacking content ideas for each cluster.&lt;/p&gt; &#xA;&lt;p&gt;üîë Key features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Automated news data collection&lt;/li&gt; &#xA; &lt;li&gt;Clustering articles using sentence embeddings&lt;/li&gt; &#xA; &lt;li&gt;GPT-3 powered cluster summarization&lt;/li&gt; &#xA; &lt;li&gt;Structured newsjacking ideation based on clusters&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìú Requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;SerpAPI Key&lt;/li&gt; &#xA; &lt;li&gt;OpenAI API Key&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;‚è∞Ô∏è Automatic TikTok Video Understanding for Social Media Strategy&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ktynski/Marketing_Automations_Notebooks_With_GPT/main/Automatic_TikTok_Video_Understanding_for_Social_Media_Strategy_(public).ipynb&#34;&gt;Google Colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This script enables automated analysis of TikTok video content using state-of-the-art AI models. It separates audio, transcribes using Whisper, encodes video understanding with Vicuna, and generates insights tailored to marketing goals. The key features are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Automated TikTok data collection&lt;/li&gt; &#xA; &lt;li&gt;Audio separation and transcription&lt;/li&gt; &#xA; &lt;li&gt;Video understanding with Vicuna multi-modal AI&lt;/li&gt; &#xA; &lt;li&gt;Customized insights for marketing strategy&lt;/li&gt; &#xA; &lt;li&gt;Detailed analysis focusing on virality factors, audience segmentation, brand presence, etc.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìú Requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Apify API Key&lt;/li&gt; &#xA; &lt;li&gt;OpenAI API Key&lt;/li&gt; &#xA; &lt;li&gt;Google Colab + A100 GPU&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;üì£ Automatic Newsjacking Ideation and Trend_Analysis&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ktynski/Marketing_Automations_Notebooks_With_GPT/main/Automatic_Newsjacking_Ideation_and_Trend_Analysis_(Public).ipynb&#34;&gt;Google Colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This script enables automated newsjacking ideation by collecting Google News articles on a topic, clustering them using sentence embeddings, summarizing clusters with GPT-3, and generating tailored content ideas. Key features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;News data collection from Google&lt;/li&gt; &#xA; &lt;li&gt;Clustering articles using sentence embeddings&lt;/li&gt; &#xA; &lt;li&gt;Summarizing clusters with GPT-3&lt;/li&gt; &#xA; &lt;li&gt;Structured newsjacking ideation based on clusters&lt;/li&gt; &#xA; &lt;li&gt;Automated tailored content creation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìú Requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;SerpAPI Key&lt;/li&gt; &#xA; &lt;li&gt;OpenAI API Key&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;‚õìÔ∏è Automated Onsite SEO Link Optimizations&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ktynski/Marketing_Automations_Notebooks_With_GPT/main/Automated_Onsite_SEO_Link_Optimizations_(Public).ipynb&#34;&gt;Google Colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This script automates analysis of on-page links to provide SEO optimization recommendations. It scrapes specified URLs, analyzes linked page content with Newspaper3k, generates anchor text recommendations with GPT-3, and outputs results to a CSV.&lt;/p&gt; &#xA;&lt;p&gt;üîë Key features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Automated on-page link analysis&lt;/li&gt; &#xA; &lt;li&gt;Content scraping with Newspaper3k&lt;/li&gt; &#xA; &lt;li&gt;GPT-3 powered anchor text recommendations&lt;/li&gt; &#xA; &lt;li&gt;SEO optimization suggestions&lt;/li&gt; &#xA; &lt;li&gt;CSV output of results&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìú Requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OpenAI API Key&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;üí≠ Automated Subreddit and Post Title Recommendations Based on Any Article&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ktynski/Marketing_Automations_Notebooks_With_GPT/main/Automated_Subreddit_and_Post_Title_Recommendations_Based_on_Any_Article_(Public).ipynb&#34;&gt;Google Colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This script analyzes an article and generates subreddit and post title recommendations using GPT-3. It summarizes the article, recommends target subreddits, and creates tailored titles for each subreddit.&lt;/p&gt; &#xA;&lt;p&gt;üîë Key features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Article summarization&lt;/li&gt; &#xA; &lt;li&gt;GPT-3 based subreddit recommendations&lt;/li&gt; &#xA; &lt;li&gt;Custom post titles optimized for each subreddit&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìú Requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OpenAI API Key&lt;/li&gt; &#xA; &lt;li&gt;Reddit API Credentials&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;üí° Automatic Article Outline Generation by Analyzing The Article Text of Top Ranking Pages for a Given Keyword&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ktynski/Marketing_Automations_Notebooks_With_GPT/main/Automatic_Article_Outline_Generation_by_Analyzing_The_Article_Text_of_Top_Ranking_Pages_for_a_Given_Keyword_(Public).ipynb&#34;&gt;Google Colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This script scrapes and analyzes top-ranking pages for a keyword to generate automated article outlines optimized for SEO. Key features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Scrapes and analyzes Google results&lt;/li&gt; &#xA; &lt;li&gt;NLP analysis of top pages&lt;/li&gt; &#xA; &lt;li&gt;GPT-3 outline generation optimized for SEO metrics&lt;/li&gt; &#xA; &lt;li&gt;Automated end-to-end outline creation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìú Requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OpenAI API Key&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;üé• Automated Video Translation with LipSync&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ktynski/Marketing_Automations_Notebooks_With_GPT/main/Automated_Video_Translation_with_LipSync_Public_Kristin_frac_tl.ipynb&#34;&gt;Google Colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This script enables automated video translation with lip sync using OpenAI Whisper for speech-to-text and Wav2Lip for lip generation. Key features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Video download from YouTube link&lt;/li&gt; &#xA; &lt;li&gt;Speech extraction and translation by Whisper&lt;/li&gt; &#xA; &lt;li&gt;Generating new translated speech audio&lt;/li&gt; &#xA; &lt;li&gt;Lip sync using Wav2Lip machine learning model&lt;/li&gt; &#xA; &lt;li&gt;Automated end-to-end video translation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìú Requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OpenAI API Key&lt;/li&gt; &#xA; &lt;li&gt;Wav2Lip, FFmpeg&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;üìë Automatically Generate a Summary, Article Outline, Long form Article, and Tweet Thread from a Youtube URL&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ktynski/Marketing_Automations_Notebooks_With_GPT/main/Automatically_Generate_a_Summary,_Article_Outline,_Long_form_Article,_and_Tweet_Thread_from_a_Youtube_URL_(Public).ipynb&#34;&gt;Google Colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This script automatically generates a summary, outline, article, and tweet thread from a YouTube video transcript using GPT-3. Key features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Transcript extraction using youtube-dl&lt;/li&gt; &#xA; &lt;li&gt;Text truncation to fit GPT-3 limits&lt;/li&gt; &#xA; &lt;li&gt;Summary generation with T5 or GPT-3&lt;/li&gt; &#xA; &lt;li&gt;Tweet thread creation with GPT-3&lt;/li&gt; &#xA; &lt;li&gt;Outline creation for a long-form article&lt;/li&gt; &#xA; &lt;li&gt;Automated article section writing with GPT-3&lt;/li&gt; &#xA; &lt;li&gt;Structured output for all generated text&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìú Requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OpenAI API Key&lt;/li&gt; &#xA; &lt;li&gt;youtube-dl&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;ü§ñ Exploring Multi Agent AI Collaboration for Iterative Invention, Critique, and Synthesis&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ktynski/Marketing_Automations_Notebooks_With_GPT/main/Exploring_Multi_Agent_AI_Collaboration_for_Iterative_Invention,_Critique,_and_Synthesis_Public_By_Kristin_Frac_tl.ipynb&#34;&gt;Google Colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This explores using multiple AI agents to iteratively generate, critique, refine, and evaluate invention ideas. Key features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Idea generation with GPT-4&lt;/li&gt; &#xA; &lt;li&gt;Critiquing ideas with GPT-3.5&lt;/li&gt; &#xA; &lt;li&gt;Idea refinement and synthesis with GPT-4&lt;/li&gt; &#xA; &lt;li&gt;Scoring ideas on multiple metrics with GPT-3.5&lt;/li&gt; &#xA; &lt;li&gt;Image prompting and rendering with Stability AI&lt;/li&gt; &#xA; &lt;li&gt;Iterative looping until criteria met&lt;/li&gt; &#xA; &lt;li&gt;Markdown rendering of the final synthesized idea&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìú Requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OpenAI API Key&lt;/li&gt; &#xA; &lt;li&gt;Stability AI API Key&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;üßî Automatic Intent, Persona ,and Buyer Inference&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ktynski/Marketing_Automations_Notebooks_With_GPT/main/Intents,_Personas,_and_Buyer_Inference_(Public).ipynb&#34;&gt;Google Colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This script predicts search intents from keywords, generates corresponding personas, and has GPT-3 infer buyer needs and marketing tactics per persona. Key features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Intent prediction using GPT-3&lt;/li&gt; &#xA; &lt;li&gt;Persona profiling with GPT-3&lt;/li&gt; &#xA; &lt;li&gt;Concurrent questions to GPT-3&lt;/li&gt; &#xA; &lt;li&gt;Buyer needs, objections, and marketing tactics&lt;/li&gt; &#xA; &lt;li&gt;Structured DataFrame output&lt;/li&gt; &#xA; &lt;li&gt;Ideal for SEO and marketing strategy&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìú Requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OpenAI API Key&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;üîé Large Language Model Search Optimization&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ktynski/Marketing_Automations_Notebooks_With_GPT/main/Large_Language_Model_Search_Optimization_(Public).ipynb&#34;&gt;Google Colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This script optimizes search analysis using large language models like GPT-3. It generates queries, analyzes brand mentions, and creates natural language reports. Key features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Query generation with GPT-3&lt;/li&gt; &#xA; &lt;li&gt;Concurrent requests for speed&lt;/li&gt; &#xA; &lt;li&gt;Brand extraction and analysis&lt;/li&gt; &#xA; &lt;li&gt;Query response summarization&lt;/li&gt; &#xA; &lt;li&gt;Natural language report generation&lt;/li&gt; &#xA; &lt;li&gt;Structured DataFrame output&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìú Requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OpenAI API Key&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;‚ö°Ô∏è Prompt Chaining Instant Content Plan&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ktynski/Marketing_Automations_Notebooks_With_GPT/main/Prompt_Chaining_Instant_Content_Plan_(Public).ipynb&#34;&gt;Google Colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This script instantly generates a structured SEO content plan by chaining prompts to GPT-3. Key features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Automated subtopic generation&lt;/li&gt; &#xA; &lt;li&gt;Automated subsubtopic generation&lt;/li&gt; &#xA; &lt;li&gt;Title generation for subsubtopics&lt;/li&gt; &#xA; &lt;li&gt;Concurrency for speed&lt;/li&gt; &#xA; &lt;li&gt;Structured output as DataFrame&lt;/li&gt; &#xA; &lt;li&gt;Rapid automated content planning&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìú Requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OpenAI API Key&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;üì∞ Prompt Chaining For Press Earning Data Journalism Stories&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ktynski/Marketing_Automations_Notebooks_With_GPT/main/Prompt_Chaining_Press_Earning_Data_Journalism_Stories_(Public).ipynb&#34;&gt;Google Colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This script generates viral data journalism story ideas by chaining GPT-3 prompts for subtopics, data sources, and narratives. Key features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Automated subtopic generation&lt;/li&gt; &#xA; &lt;li&gt;Data source identification&lt;/li&gt; &#xA; &lt;li&gt;Story narrative generation&lt;/li&gt; &#xA; &lt;li&gt;Structured DataFrame output&lt;/li&gt; &#xA; &lt;li&gt;Optimized for press pitching&lt;/li&gt; &#xA; &lt;li&gt;Requires only a seed keyword&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìú Requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OpenAI API Key&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;‚úçÔ∏è The Ultimate AI Researcher&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ktynski/Marketing_Automations_Notebooks_With_GPT/raw/main/The_Ultimate_AI_Researcher_(Public).ipynb&#34;&gt;Google Colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Automates academic literature research using ArXiv, semantic search, summaries, and natural language generation.&lt;/li&gt; &#xA; &lt;li&gt;ArXiv paper querying and download&lt;/li&gt; &#xA; &lt;li&gt;GPT-3 relevance scoring&lt;/li&gt; &#xA; &lt;li&gt;Text summarization with POE&lt;/li&gt; &#xA; &lt;li&gt;Concurrency for speed&lt;/li&gt; &#xA; &lt;li&gt;Natural language report generation&lt;/li&gt; &#xA; &lt;li&gt;Automates the literature review process&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìú Requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OpenAI API Key&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;üë• Automated Content and Keyword Clustering Descriptions with HuggingFace Embeddings, Agglomerative Clustering, and GPT-3&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ktynski/Marketing_Automations_Notebooks_With_GPT/raw/main/Automatic_Content_and_Keyword_Clustering_Descriptions_with_HuggingFace_Embeddings,_Agglomerative_Clustering_and_GPT_3_(Public).ipynb&#34;&gt;Google Colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Simplifies content and keyword clustering using HuggingFace embeddings for keywords, agglomerative clustering for content, and GPT-3 for enhanced cluster descriptions.&lt;/li&gt; &#xA; &lt;li&gt;Provides more insightful and organized cluster analysis.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìú Requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OpenAI API Key&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;‚è∞Ô∏è Automatic Deep TikTok Insights with GPT and Whisper&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ktynski/Marketing_Automations_Notebooks_With_GPT/raw/main/Automatic_Deep_TikTok_Insights_with_GPT_and_Whisper_Public.ipynb&#34;&gt;Google Colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Feed it a seed keyword, it will leverage the #Apify #TikTokScraper to extract associated search keywords in a recursive manner as per your specifications, and then download all related videos.&lt;/li&gt; &#xA; &lt;li&gt;Transforms videos to audio, transcribes using Whisper, and translates non-English videos to English. üåé&lt;/li&gt; &#xA; &lt;li&gt;Cleans and preps transcripts.&lt;/li&gt; &#xA; &lt;li&gt;Prompts to GPT-4 for in-depth analysis of transcripts in batches.&lt;/li&gt; &#xA; &lt;li&gt;GPT-4 unearths elements propelling engagement. üìä&lt;/li&gt; &#xA; &lt;li&gt;Collates the most recurrent and poignant insights from all batches&lt;/li&gt; &#xA; &lt;li&gt;Synthesizes a report outlining key recommendations. üìù&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìú Requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OpenAI API Key&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;üïµ Automatic Persona and Motivation Research&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ktynski/Marketing_Automations_Notebooks_With_GPT/raw/main/Automatic_Persona_and_Motivation_Research_(public).ipynb&#34;&gt;Google Colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Takes a list of keywords&lt;/li&gt; &#xA; &lt;li&gt;Runs each keyword through 28 different evaluations predicting answers about the person searching and their goals when searching with a given keyword&lt;/li&gt; &#xA; &lt;li&gt;Generating a csv that contains the answers to all the evaluations for each keyword.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìú Requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OpenAI API Key&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;‚ùì Automatic Question Expander ala AnswerThePublic&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ktynski/Marketing_Automations_Notebooks_With_GPT/raw/main/Automatic_Question_Expander_ala_AnswerThePublic_(Public).ipynb&#34;&gt;Google Colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;-Takes a given topic -Generates sub topics related to the topic -Generates sub-sub topics related to the subtopic and primary topic -Generates 30 relevant questions for each sub-sub topic. -Collates everything into a well organized dataframe for export to CSV.&lt;/p&gt; &#xA;&lt;p&gt;üìú Requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OpenAI API Key&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;üìà Automatic Reddit Trend Research with GPT3&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ktynski/Marketing_Automations_Notebooks_With_GPT/raw/main/Automatic_Reddit_Trend_Research_with_GPT3_(Public).ipynb&#34;&gt;Google Colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Takes a given keyword and scrapes all Reddit posts that match that keyword for a given number of posts and a date range.&lt;/li&gt; &#xA; &lt;li&gt;Runs the set of Reddit posts scraped in the last step through 11 different prompts that analyze the corpus of post titles and give a readout of what they are generally about, main points, and other important takeaways.&lt;/li&gt; &#xA; &lt;li&gt;Generates two CSVs, one that is the output of the Reddit scrape, and one that is the analysis/readout of those posts.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìú Requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OpenAI API Key&lt;/li&gt; &#xA; &lt;li&gt;Reddit api credentials&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;üì∞ Automatic Schema Improvements with GPT4&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ktynski/Marketing_Automations_Notebooks_With_GPT/raw/main/Automatic_Schema_Improvements_with_GPT4_(Public).ipynb&#34;&gt;Google Colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Extracts relevant information from a given URL using the newspaper3k library.&lt;/li&gt; &#xA; &lt;li&gt;Analyzes the content and determines the appropriate schema type using #GPT4.&lt;/li&gt; &#xA; &lt;li&gt;Extracts relevant data points based on the determined schema type.&lt;/li&gt; &#xA; &lt;li&gt;Generates JSON-LD schema markup using the extracted data points.&lt;/li&gt; &#xA; &lt;li&gt;Outputs the enhanced HTML with schema markup applied.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìú Requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OpenAI API Key&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/dinov2</title>
    <updated>2023-09-02T01:31:13Z</updated>
    <id>tag:github.com,2023-09-02:/facebookresearch/dinov2</id>
    <link href="https://github.com/facebookresearch/dinov2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;PyTorch code and models for the DINOv2 self-supervised learning method.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DINOv2: Learning Robust Visual Features without Supervision&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://ai.facebook.com/research/&#34;&gt;Meta AI Research, FAIR&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Maxime Oquab, Timoth√©e Darcet, Th√©o Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Patrick Labatut, Armand Joulin, Piotr Bojanowski&lt;/p&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://arxiv.org/abs/2304.07193&#34;&gt;&lt;code&gt;Paper&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://ai.facebook.com/blog/dino-v2-computer-vision-self-supervised-learning/&#34;&gt;&lt;code&gt;Blog&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://dinov2.metademolab.com&#34;&gt;&lt;code&gt;Demo&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/dinov2/main/#citing-dinov2&#34;&gt;&lt;code&gt;BibTeX&lt;/code&gt;&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;PyTorch implementation and pretrained models for DINOv2. For details, see the paper: &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.07193&#34;&gt;DINOv2: Learning Robust Visual Features without Supervision&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;DINOv2 models produce high-performance visual features that can be directly employed with classifiers as simple as linear layers on a variety of computer vision tasks; these visual features are robust and perform well across domains without any requirement for fine-tuning. The models were pretrained on a dataset of 142 M images without using any labels or annotations.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/facebookresearch/dinov2/assets/60359573/f168823e-7922-415a-b429-578badf5c356&#34;&gt;https://github.com/facebookresearch/dinov2/assets/60359573/f168823e-7922-415a-b429-578badf5c356&lt;/a&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA;  Visualization of the three first principal components of the patch features of all frames, mapped to RGB values. &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Pretrained models&lt;/h2&gt; &#xA;&lt;table style=&#34;margin: auto&#34;&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;model&lt;/th&gt; &#xA;   &lt;th&gt;# of&lt;br&gt;params&lt;/th&gt; &#xA;   &lt;th&gt;ImageNet&lt;br&gt;k-NN&lt;/th&gt; &#xA;   &lt;th&gt;ImageNet&lt;br&gt;linear&lt;/th&gt; &#xA;   &lt;th&gt;download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-S/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;21 M&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;79.0%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;81.1%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_pretrain.pth&#34;&gt;backbone only&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-B/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;86 M&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;82.1%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;84.5%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_pretrain.pth&#34;&gt;backbone only&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-L/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;300 M&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;83.5%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;86.3%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_pretrain.pth&#34;&gt;backbone only&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-g/14&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1,100 M&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;83.5%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;86.5%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_pretrain.pth&#34;&gt;backbone only&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Pretrained backbones (via PyTorch Hub)&lt;/h3&gt; &#xA;&lt;p&gt;Please follow the instructions &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;here&lt;/a&gt; to install PyTorch (the only required dependency for loading the model). Installing PyTorch with CUDA support is strongly recommended.&lt;/p&gt; &#xA;&lt;p&gt;A corresponding &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/dinov2/main/MODEL_CARD.md&#34;&gt;model card&lt;/a&gt; is included in the repository.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;&#xA;dinov2_vits14 = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vits14&#39;)&#xA;dinov2_vitb14 = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitb14&#39;)&#xA;dinov2_vitl14 = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitl14&#39;)&#xA;dinov2_vitg14 = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitg14&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Pretrained heads - Image classification&lt;/h3&gt; &#xA;&lt;table style=&#34;margin: auto&#34;&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th rowspan=&#34;2&#34;&gt;backbone&lt;/th&gt; &#xA;   &lt;th&gt;download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;ImageNet&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-S/14 distilled&lt;/td&gt; &#xA;   &lt;td&gt; linear head (&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_linear_head.pth&#34;&gt;1 layer&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_linear4_head.pth&#34;&gt;4 layers&lt;/a&gt;) &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-B/14 distilled&lt;/td&gt; &#xA;   &lt;td&gt; linear head (&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_linear_head.pth&#34;&gt;1 layer&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_linear4_head.pth&#34;&gt;4 layers&lt;/a&gt;) &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-L/14 distilled&lt;/td&gt; &#xA;   &lt;td&gt; linear head (&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_linear_head.pth&#34;&gt;1 layer&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_linear4_head.pth&#34;&gt;4 layers&lt;/a&gt;) &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-g/14&lt;/td&gt; &#xA;   &lt;td&gt; linear head (&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_linear_head.pth&#34;&gt;1 layer&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_linear4_head.pth&#34;&gt;4 layers&lt;/a&gt;) &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The (full) classifier models can be loaded via PyTorch Hub:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;&#xA;dinov2_vits14_lc = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vits14_lc&#39;)&#xA;dinov2_vitb14_lc = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitb14_lc&#39;)&#xA;dinov2_vitl14_lc = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitl14_lc&#39;)&#xA;dinov2_vitg14_lc = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitg14_lc&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Pretrained heads - Depth estimation&lt;/h3&gt; &#xA;&lt;table style=&#34;margin: auto&#34;&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th rowspan=&#34;2&#34;&gt;backbone&lt;/th&gt; &#xA;   &lt;th colspan=&#34;2&#34;&gt;download head&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;NYUd&lt;/th&gt; &#xA;   &lt;th&gt;KITTI&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-S/14 distilled&lt;/td&gt; &#xA;   &lt;td&gt; linear (&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_nyu_linear_head.pth&#34;&gt;1 layer&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_nyu_linear4_head.pth&#34;&gt;4 layers&lt;/a&gt;), &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_nyu_dpt_head.pth&#34;&gt;DPT&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; linear (&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_kitti_linear_head.pth&#34;&gt;1 layer&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_kitti_linear4_head.pth&#34;&gt;4 layers&lt;/a&gt;), &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_kitti_dpt_head.pth&#34;&gt;DPT&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-B/14 distilled&lt;/td&gt; &#xA;   &lt;td&gt; linear (&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_linear_head.pth&#34;&gt;1 layer&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_nyu_linear4_head.pth&#34;&gt;4 layers&lt;/a&gt;), &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_nyu_dpt_head.pth&#34;&gt;DPT&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; linear (&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_kitti_linear_head.pth&#34;&gt;1 layer&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_kitti_linear4_head.pth&#34;&gt;4 layers&lt;/a&gt;), &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_kitti_dpt_head.pth&#34;&gt;DPT&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-L/14 distilled&lt;/td&gt; &#xA;   &lt;td&gt; linear (&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_linear_head.pth&#34;&gt;1 layer&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_nyu_linear4_head.pth&#34;&gt;4 layers&lt;/a&gt;), &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_nyu_dpt_head.pth&#34;&gt;DPT&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; linear (&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_kitti_linear_head.pth&#34;&gt;1 layer&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_kitti_linear4_head.pth&#34;&gt;4 layers&lt;/a&gt;), &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_kitti_dpt_head.pth&#34;&gt;DPT&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-g/14&lt;/td&gt; &#xA;   &lt;td&gt; linear (&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_linear_head.pth&#34;&gt;1 layer&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_nyu_linear4_head.pth&#34;&gt;4 layers&lt;/a&gt;), &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_nyu_dpt_head.pth&#34;&gt;DPT&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; linear (&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_kitti_linear_head.pth&#34;&gt;1 layer&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_kitti_linear4_head.pth&#34;&gt;4 layers&lt;/a&gt;), &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_kitti_dpt_head.pth&#34;&gt;DPT&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Pretrained heads - Semantic segmentation&lt;/h3&gt; &#xA;&lt;table style=&#34;margin: auto&#34;&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th rowspan=&#34;2&#34;&gt;backbone&lt;/th&gt; &#xA;   &lt;th&gt;download model&lt;/th&gt; &#xA;   &lt;th colspan=&#34;2&#34;&gt;download head&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;ADE20K&lt;/th&gt; &#xA;   &lt;th&gt;ADE20K&lt;/th&gt; &#xA;   &lt;th&gt;VOC2012&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-S/14 distilled&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_ade20k_linear_head.pth&#34;&gt;linear&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_ade20k_ms_head.pth&#34;&gt;multi-scale&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_voc2012_linear_head.pth&#34;&gt;linear&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_voc2012_ms_head.pth&#34;&gt;multi-scale&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-B/14 distilled&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_ade20k_linear_head.pth&#34;&gt;linear&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_ade20k_ms_head.pth&#34;&gt;multi-scale&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_voc2012_linear_head.pth&#34;&gt;linear&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_voc2012_ms_head.pth&#34;&gt;multi-scale&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-L/14 distilled&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_ade20k_linear_head.pth&#34;&gt;linear&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_ade20k_ms_head.pth&#34;&gt;multi-scale&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_voc2012_linear_head.pth&#34;&gt;linear&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_voc2012_ms_head.pth&#34;&gt;multi-scale&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-g/14&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_ade20k_m2f.pth&#34;&gt;Mask2Former&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_ade20k_linear_head.pth&#34;&gt;linear&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_ade20k_ms_head.pth&#34;&gt;multi-scale&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_voc2012_linear_head.pth&#34;&gt;linear&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_voc2012_ms_head.pth&#34;&gt;multi-scale&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;The training and evaluation code requires PyTorch 2.0 and &lt;a href=&#34;https://github.com/facebookresearch/xformers&#34;&gt;xFormers&lt;/a&gt; 0.0.18 as well as a number of other 3rd party packages. Note that the code has only been tested with the specified versions and also expects a Linux environment. To setup all the required dependencies for training and evaluation, please follow the instructions below:&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://docs.conda.io/projects/conda/en/latest/user-guide/getting-started.html&#34;&gt;conda&lt;/a&gt;&lt;/em&gt; &lt;strong&gt;(Recommended)&lt;/strong&gt; - Clone the repository and then create and activate a &lt;code&gt;dinov2&lt;/code&gt; conda environment using the provided environment definition:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda env create -f conda.yaml&#xA;conda activate dinov2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://pip.pypa.io/en/stable/getting-started/&#34;&gt;pip&lt;/a&gt;&lt;/em&gt; - Clone the repository and then use the provided &lt;code&gt;requirements.txt&lt;/code&gt; to install the dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For dense tasks (depth estimation and semantic segmentation), there are additional dependencies (specific versions of &lt;code&gt;mmcv&lt;/code&gt; and &lt;code&gt;mmsegmentation&lt;/code&gt;) which are captured in the &lt;code&gt;extras&lt;/code&gt; dependency specifications:&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://docs.conda.io/projects/conda/en/latest/user-guide/getting-started.html&#34;&gt;conda&lt;/a&gt;&lt;/em&gt; &lt;strong&gt;(Recommended)&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda env create -f conda-extras.yaml&#xA;conda activate dinov2-extras&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://pip.pypa.io/en/stable/getting-started/&#34;&gt;pip&lt;/a&gt;&lt;/em&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r requirements.txt -r requirements-extras.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Data preparation&lt;/h2&gt; &#xA;&lt;h3&gt;ImageNet-1k&lt;/h3&gt; &#xA;&lt;p&gt;The root directory of the dataset should hold the following contents:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/test/ILSVRC2012_test_00000001.JPEG&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/test/[..]&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/test/ILSVRC2012_test_00100000.JPEG&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/train/n01440764/n01440764_10026.JPEG&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/train/[...]&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/train/n15075141/n15075141_9993.JPEG&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/val/n01440764/ILSVRC2012_val_00000293.JPEG&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/val/[...]&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/val/n15075141/ILSVRC2012_val_00049174.JPEG&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/labels.txt&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The provided dataset implementation expects a few additional metadata files to be present under the extra directory:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;EXTRA&amp;gt;/class-ids-TRAIN.npy&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;EXTRA&amp;gt;/class-ids-VAL.npy&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;EXTRA&amp;gt;/class-names-TRAIN.npy&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;EXTRA&amp;gt;/class-names-VAL.npy&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;EXTRA&amp;gt;/entries-TEST.npy&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;EXTRA&amp;gt;/entries-TRAIN.npy&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;EXTRA&amp;gt;/entries-VAL.npy&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;These metadata files can be generated (once) with the following lines of Python code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from dinov2.data.datasets import ImageNet&#xA;&#xA;for split in ImageNet.Split:&#xA;    dataset = ImageNet(split=split, root=&#34;&amp;lt;ROOT&amp;gt;&#34;, extra=&#34;&amp;lt;EXTRA&amp;gt;&#34;)&#xA;    dataset.dump_extra()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that the root and extra directories do not have to be distinct directories.&lt;/p&gt; &#xA;&lt;h3&gt;ImageNet-22k&lt;/h3&gt; &#xA;&lt;p&gt;Please adapt the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/dinov2/main/dinov2/data/datasets/image_net_22k.py&#34;&gt;dataset class&lt;/a&gt; to match your local setup.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;span&gt;‚ö†&lt;/span&gt; To execute the commands provided in the next sections for training and evaluation, the &lt;code&gt;dinov2&lt;/code&gt; package should be included in the Python module search path, i.e. simply prefix the command to run with &lt;code&gt;PYTHONPATH=.&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;h3&gt;Fast setup: training DINOv2 ViT-L/16 on ImageNet-1k&lt;/h3&gt; &#xA;&lt;p&gt;Run DINOv2 training on 4 A100-80GB nodes (32 GPUs) in a SLURM cluster environment with submitit:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python dinov2/run/train/train.py \&#xA;    --nodes 4 \&#xA;    --config-file dinov2/configs/train/vitl16_short.yaml \&#xA;    --output-dir &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt; \&#xA;    train.dataset_path=ImageNet:split=TRAIN:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Training time is approximately 1 day and the resulting checkpoint should reach 81.6% on k-NN eval and 82.9% on linear eval.&lt;/p&gt; &#xA;&lt;p&gt;The training code saves the weights of the teacher in the &lt;code&gt;eval&lt;/code&gt; folder every 12500 iterations for evaluation.&lt;/p&gt; &#xA;&lt;h3&gt;Long setup: training DINOv2 ViT-L/14 on ImageNet-22k&lt;/h3&gt; &#xA;&lt;p&gt;Run DINOv2 training on 12 A100-80GB nodes (96 GPUs) in a SLURM cluster environment with submitit:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python dinov2/run/train/train.py \&#xA;    --nodes 12 \&#xA;    --config-file dinov2/configs/train/vitl14.yaml \&#xA;    --output-dir &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt; \&#xA;    train.dataset_path=ImageNet22k:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Training time is approximately 3.3 days and the resulting checkpoint should reach 82.0% on k-NN eval and 84.5% on linear eval.&lt;/p&gt; &#xA;&lt;p&gt;The training code saves the weights of the teacher in the &lt;code&gt;eval&lt;/code&gt; folder every 12500 iterations for evaluation.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;The training code regularly saves the teacher weights. In order to evaluate the model, run the following evaluation on a single node:&lt;/p&gt; &#xA;&lt;h3&gt;k-NN classification on ImageNet-1k&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python dinov2/run/eval/knn.py \&#xA;    --config-file &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/config.yaml \&#xA;    --pretrained-weights &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/eval/training_24999/teacher_checkpoint.pth \&#xA;    --output-dir &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/eval/training_24999/knn \&#xA;    --train-dataset ImageNet:split=TRAIN:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt; \&#xA;    --val-dataset ImageNet:split=VAL:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Logistic regression classification on ImageNet-1k&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python dinov2/run/eval/log_regression.py \&#xA;    --config-file &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/config.yaml \&#xA;    --pretrained-weights &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/eval/training_24999/teacher_checkpoint.pth \&#xA;    --output-dir &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/eval/training_24999/logreg \&#xA;    --train-dataset ImageNet:split=TRAIN:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt; \&#xA;    --val-dataset ImageNet:split=VAL:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Linear classification with data augmentation on ImageNet-1k&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python dinov2/run/eval/linear.py \&#xA;    --config-file &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/config.yaml \&#xA;    --pretrained-weights &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/eval/training_24999/teacher_checkpoint.pth \&#xA;    --output-dir &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/eval/training_24999/linear \&#xA;    --train-dataset ImageNet:split=TRAIN:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt; \&#xA;    --val-dataset ImageNet:split=VAL:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We release the weights from evaluating the different models:&lt;/p&gt; &#xA;&lt;table style=&#34;margin: auto&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;model&lt;/th&gt; &#xA;   &lt;th&gt;ImageNet&lt;br&gt;top-1&lt;/th&gt; &#xA;   &lt;th&gt;linear evaluation&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-S/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;81.1%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_linear_head.pth&#34;&gt;linear head weights&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-B/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;84.5%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_linear_head.pth&#34;&gt;linear head weights&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-L/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;86.3%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_linear_head.pth&#34;&gt;linear head weights&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-g/14&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;86.5%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_linear_head.pth&#34;&gt;linear head weights&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;The performance of the provided pretrained model weights can be evaluated as follows on ImageNet-1k:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python dinov2/run/eval/linear.py \&#xA;    --config-file dinov2/configs/eval/vitg14_pretrain.yaml \&#xA;    --pretrained-weights https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_pretrain.pth \&#xA;    --train-dataset ImageNet:split=TRAIN:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt; \&#xA;    --val-dataset ImageNet:split=VAL:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Notebooks&lt;/h2&gt; &#xA;&lt;p&gt;A few notebooks are provided to help the community leverage the models and code:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/dinov2/raw/main/notebooks/depth_estimation.ipynb&#34;&gt;Depth estimation&lt;/a&gt; - How to load and use the depth heads in combination with a matching backbone via mmcv&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/dinov2/raw/main/notebooks/semantic_segmentation.ipynb&#34;&gt;Semantic segmentation&lt;/a&gt; - How to load and use the segmentation heads in combination with a matching backbone via mmcv, and also how to load and use the Mask2Former-based segmentation model trained on ADE20K&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;DINOv2 code and model weights are released under the Apache License 2.0. See &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/dinov2/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; for additional details.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/dinov2/main/CONTRIBUTING.md&#34;&gt;contributing&lt;/a&gt; and the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/dinov2/main/CODE_OF_CONDUCT.md&#34;&gt;code of conduct&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citing DINOv2&lt;/h2&gt; &#xA;&lt;p&gt;If you find this repository useful, please consider giving a star &lt;span&gt;‚≠ê&lt;/span&gt; and citation &lt;span&gt;ü¶ñ&lt;/span&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{oquab2023dinov2,&#xA;  title={DINOv2: Learning Robust Visual Features without Supervision},&#xA;  author={Oquab, Maxime and Darcet, Timoth√©e and Moutakanni, Theo and Vo, Huy V. and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and Howes, Russell and Huang, Po-Yao and Xu, Hu and Sharma, Vasu and Li, Shang-Wen and Galuba, Wojciech and Rabbat, Mike and Assran, Mido and Ballas, Nicolas and Synnaeve, Gabriel and Misra, Ishan and Jegou, Herve and Mairal, Julien and Labatut, Patrick and Joulin, Armand and Bojanowski, Piotr},&#xA;  journal={arXiv:2304.07193},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>