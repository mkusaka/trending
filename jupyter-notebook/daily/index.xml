<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-10-18T01:32:51Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>panaverse/learn-modern-python</title>
    <updated>2023-10-18T01:32:51Z</updated>
    <id>tag:github.com,2023-10-18:/panaverse/learn-modern-python</id>
    <link href="https://github.com/panaverse/learn-modern-python" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Learn Modern Python for Data Analysis&lt;/h1&gt; &#xA;&lt;p&gt;This course is part of the &lt;a href=&#34;https://docs.google.com/presentation/d/1XVSZhmv4XH14YpyDxJIvYWiUrF1EO9tsUnle17wCLIc/edit?usp=sharing&#34;&gt;GenAI, Web 3, and Metaverse Program&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;After completing this course, move on to the &lt;a href=&#34;https://github.com/panaverse/learn-generative-ai&#34;&gt;Learn Generative AI Repo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Live Zoom Classes: Data Science and Modern Python Crash Course for WMD Q1, Q2, and Q3&lt;/h2&gt; &#xA;&lt;p&gt;Days: Monday, Tuesday, Thursday, Friday, Saturday, and Sunday&lt;/p&gt; &#xA;&lt;p&gt;Time: 08:00 PM to 10:00 PM&lt;/p&gt; &#xA;&lt;p&gt;Join Zoom Meeting&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://us06web.zoom.us/j/82224691502?pwd=6Mq6Wt09uUkVLme859duUYP1Efww4K.1&#34;&gt;https://us06web.zoom.us/j/82224691502?pwd=6Mq6Wt09uUkVLme859duUYP1Efww4K.1&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Meeting ID: 822 2469 1502&lt;/p&gt; &#xA;&lt;p&gt;Passcode: 936542&lt;/p&gt; &#xA;&lt;p&gt;This session will also be live streamed on YouTube:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/@panaverse/streams&#34;&gt;https://www.youtube.com/@panaverse/streams&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;What is Modern Python?&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.easypost.com/blog/2022-09-14-modern-python-new-features-better-code&#34;&gt;Modern Python: New Features, Better Code&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.stuartellis.name/articles/python-modern-practices/&#34;&gt;Modern Good Practices for Python Development&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.anaconda.com/download&#34;&gt;Install Anaconda with Python 3.12&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://code.visualstudio.com/&#34;&gt;Install VS Code&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=ms-python.python&#34;&gt;Install Python Plugin&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://python.plainenglish.io/typed-python-for-typescript-developers-791145e7171c&#34;&gt;Read this Document for Reference&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Commands to Upgrade to Latest Python 3.12 in Anaconda&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create --name myenv3_12 python=3.12&#xA;conda env list&#xA;conda activate myenv3_12&#xA;python --version&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Text Books&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.amazon.com/Python-Crash-Course-Eric-Matthes/dp/1718502702/ref=sr_1_1&#34;&gt;Python Crash Course 3rd Edition&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.amazon.com/Python-Data-Analysis-Wrangling-Jupyter/dp/109810403X/ref=sr_1_1&#34;&gt;Chapter 5 of Python for Data Analysis: Data Wrangling with pandas, NumPy, and Jupyter 3rd Edition&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Learning Material&lt;/h2&gt; &#xA;&lt;p&gt;We will cover Python version 3.12+ with Latest Features&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.amazon.com/Python-Crash-Course-Eric-Matthes/dp/1718502702/ref=sr_1_1&#34;&gt;Chapters 1-10 of Python Crash Course 3rd Edition&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ehmatthes.github.io/pcc_3e/&#34;&gt;Update the Book Code with Type Hints Resources for Python Crash Course&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.google/&#34;&gt;Google Colaboratory&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://itslinuxfoss.com/determine-type-object-python/&#34;&gt;How to Determine the Type of an Object in Python?&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.geeksforgeeks.org/gfact-50-python-end-parameter-in-print/&#34;&gt;Python end parameter in print()&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Typing&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.python.org/3/library/typing.html&#34;&gt;https://docs.python.org/3/library/typing.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.javatpoint.com/type-hint-concepts-to-improve-python-code&#34;&gt;https://www.javatpoint.com/type-hint-concepts-to-improve-python-code&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Note: The last article is not current because it does not use collections.abc&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://mypy.readthedocs.io/en/stable/cheat_sheet_py3.html&#34;&gt;Typing Cheat Sheet&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;Abstract Base Classes (collections.abc)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=oUt1feRoyvI&#34;&gt;https://www.youtube.com/watch?v=oUt1feRoyvI&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.python.org/3/library/collections.abc.html&#34;&gt;https://docs.python.org/3/library/collections.abc.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ol start=&#34;6&#34;&gt; &#xA; &lt;li&gt;Yield in Python&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.simplilearn.com/tutorials/python-tutorial/yield-in-python&#34;&gt;https://www.simplilearn.com/tutorials/python-tutorial/yield-in-python&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://stackoverflow.com/questions/74774919/proper-typing-for-a-interesting-yield-function&#34;&gt;https://stackoverflow.com/questions/74774919/proper-typing-for-a-interesting-yield-function&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Example of type hint for a function returning yield i.e. generator function:&lt;/p&gt; &#xA;&lt;p&gt;First Import:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;from collections.abc import Iterator&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then define the function that returns yield:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;def my_generator()-&amp;gt; Iterator[int]:&#xA;    function statements&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;7&#34;&gt; &#xA; &lt;li&gt;Advanced Classes Topics in Python&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Class Attributes and Methods vs Instance Attributes and Methods in Python&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.tutorialsteacher.com/articles/class-attributes-vs-instance-attributes-in-python&#34;&gt;https://www.tutorialsteacher.com/articles/class-attributes-vs-instance-attributes-in-python&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.digitalocean.com/community/tutorials/python-static-method&#34;&gt;https://www.digitalocean.com/community/tutorials/python-static-method&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Public, Protected, Private Members&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.tutorialsteacher.com/python/public-private-protected-modifiers&#34;&gt;https://www.tutorialsteacher.com/python/public-private-protected-modifiers&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.linkedin.com/pulse/pythons-access-specifiers-public-protected-private-swarooprani-manoor/&#34;&gt;https://www.linkedin.com/pulse/pythons-access-specifiers-public-protected-private-swarooprani-manoor/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;How To Use the &lt;strong&gt;str&lt;/strong&gt;() and &lt;strong&gt;repr&lt;/strong&gt;() Methods in Python:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.digitalocean.com/community/tutorials/python-str-repr-functions&#34;&gt;https://www.digitalocean.com/community/tutorials/python-str-repr-functions&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Duck Typing:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ioflood.com/blog/duck-typing/&#34;&gt;https://ioflood.com/blog/duck-typing/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Type Statements:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.python.org/3.12/reference/simple_stmts.html#the-type-statement&#34;&gt;https://docs.python.org/3.12/reference/simple_stmts.html#the-type-statement&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ol start=&#34;8&#34;&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.amazon.com/Python-Data-Analysis-Wrangling-Jupyter/dp/109810403X/ref=sr_1_1&#34;&gt;Chapter 5 of Python for Data Analysis: Data Wrangling with pandas, NumPy, and Jupyter 3rd Edition&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Additional Readings:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/python-type-hinting-duck-type-compatibility-and-consistent-with-72e8b348d8ac&#34;&gt;https://towardsdatascience.com/python-type-hinting-duck-type-compatibility-and-consistent-with-72e8b348d8ac&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Projects&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/code/prashant111/comprehensive-data-analysis-with-pandas&#34;&gt;Comprehensive Data Analysis with Pandas&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://realpython.com/pandas-project-gradebook/&#34;&gt;Pandas Project: Make a Gradebook With Python &amp;amp; Pandas&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Fundamentals of Modern Python for Data Analysis Quiz&lt;/h1&gt; &#xA;&lt;p&gt;Total Questions: 40&lt;/p&gt; &#xA;&lt;p&gt;Duration: 60 minutes&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/llama-recipes</title>
    <updated>2023-10-18T01:32:51Z</updated>
    <id>tag:github.com,2023-10-18:/facebookresearch/llama-recipes</id>
    <link href="https://github.com/facebookresearch/llama-recipes" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Examples and recipes for Llama 2 model&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Llama 2 Fine-tuning / Inference Recipes and Examples&lt;/h1&gt; &#xA;&lt;p&gt;The &#39;llama-recipes&#39; repository is a companion to the &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;Llama 2 model&lt;/a&gt;. The goal of this repository is to provide examples to quickly get started with fine-tuning for domain adaptation and how to run inference for the fine-tuned models. For ease of use, the examples use Hugging Face converted versions of the models. See steps for conversion of the model &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/#model-conversion-to-hugging-face&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In addition, we also provide a number of demo apps, to showcase the Llama2 usage along with other ecosystem solutions to run Llama2 locally on your mac and on cloud.&lt;/p&gt; &#xA;&lt;p&gt;Llama 2 is a new technology that carries potential risks with use. Testing conducted to date has not — and could not — cover all scenarios. In order to help developers address these risks, we have created the &lt;a href=&#34;https://github.com/facebookresearch/llama/raw/main/Responsible-Use-Guide.pdf&#34;&gt;Responsible Use Guide&lt;/a&gt;. More details can be found in our research paper as well. For downloading the models, follow the instructions on &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;Llama 2 repo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Table of Contents&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/#quick-start&#34;&gt;Quick start&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/#model-conversion-to-hugging-face&#34;&gt;Model Conversion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/#fine-tuning&#34;&gt;Fine-tuning&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/#single-gpu&#34;&gt;Single GPU&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/#multiple-gpus-one-node&#34;&gt;Multi GPU One Node&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/#multi-gpu-multi-node&#34;&gt;Multi GPU Multi Node&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/docs/inference.md&#34;&gt;Inference&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/#demo-apps&#34;&gt;Demo Apps&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/#repository-organization&#34;&gt;Repository Organization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/#license&#34;&gt;License and Acceptable Use Policy&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Quick Start&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/examples/quickstart.ipynb&#34;&gt;Llama 2 Jupyter Notebook&lt;/a&gt;: This jupyter notebook steps you through how to finetune a Llama 2 model on the text summarization task using the &lt;a href=&#34;https://huggingface.co/datasets/samsum&#34;&gt;samsum&lt;/a&gt;. The notebook uses parameter efficient finetuning (PEFT) and int8 quantization to finetune a 7B on a single GPU like an A10 with 24GB gpu memory.&lt;/p&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;p&gt;Llama-recipes provides a pip distribution for easy install and usage in other projects. Alternatively, it can be installed from source.&lt;/p&gt; &#xA;&lt;h2&gt;Install with pip&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install --extra-index-url https://download.pytorch.org/whl/test/cu118 llama-recipes&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Install from source&lt;/h2&gt; &#xA;&lt;p&gt;To install from source e.g. for development use this command. We&#39;re using hatchling as our build backend which requires an up-to-date pip as well as setuptools package.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -U pip setuptools&#xA;pip install --extra-index-url https://download.pytorch.org/whl/test/cu118 -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For development and contributing to llama-recipes please install all optional dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -U pip setuptools&#xA;pip install --extra-index-url https://download.pytorch.org/whl/test/cu118 -e .[tests,auditnlg,vllm]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Install with optional dependencies&lt;/h2&gt; &#xA;&lt;p&gt;Llama-recipes offers the installation of optional packages. There are three optional dependency groups. To run the unit tests we can install the required dependencies with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install --extra-index-url https://download.pytorch.org/whl/test/cu118 llama-recipes[tests]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For the vLLM example we need additional requirements that can be installed with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install --extra-index-url https://download.pytorch.org/whl/test/cu118 llama-recipes[vllm]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use the sensitive topics safety checker install with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install --extra-index-url https://download.pytorch.org/whl/test/cu118 llama-recipes[auditnlg]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Optional dependencies can also be combines with [option1,option2].&lt;/p&gt; &#xA;&lt;p&gt;⚠️ &lt;strong&gt;Note&lt;/strong&gt; ⚠️ Some features (especially fine-tuning with FSDP + PEFT) currently require PyTorch nightlies to be installed. Please make sure to install the nightlies if you&#39;re using these features following &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;this guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; All the setting defined in &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/src/llama_recipes/configs/&#34;&gt;config files&lt;/a&gt; can be passed as args through CLI when running the script, there is no need to change from config files directly.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; In case need to run PEFT model with FSDP, please make sure to use the PyTorch Nightlies.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;For more in depth information checkout the following:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/docs/single_gpu.md&#34;&gt;Single GPU Fine-tuning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/docs/multi_gpu.md&#34;&gt;Multi-GPU Fine-tuning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/docs/LLM_finetuning.md&#34;&gt;LLM Fine-tuning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/docs/Dataset.md&#34;&gt;Adding custom datasets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/docs/inference.md&#34;&gt;Inference&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/docs/FAQ.md&#34;&gt;FAQs&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Where to find the models?&lt;/h1&gt; &#xA;&lt;p&gt;You can find llama v2 models on HuggingFace hub &lt;a href=&#34;https://huggingface.co/meta-llama&#34;&gt;here&lt;/a&gt;, where models with &lt;code&gt;hf&lt;/code&gt; in the name are already converted to HuggingFace checkpoints so no further conversion is needed. The conversion step below is only for original model weights from Meta that are hosted on HuggingFace model hub as well.&lt;/p&gt; &#xA;&lt;h1&gt;Model conversion to Hugging Face&lt;/h1&gt; &#xA;&lt;p&gt;The recipes and notebooks in this folder are using the Llama 2 model definition provided by Hugging Face&#39;s transformers library.&lt;/p&gt; &#xA;&lt;p&gt;Given that the original checkpoint resides under models/7B you can install all requirements and convert the checkpoint with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;## Install HuggingFace Transformers from source&#xA;pip freeze | grep transformers ## verify it is version 4.31.0 or higher&#xA;&#xA;git clone git@github.com:huggingface/transformers.git&#xA;cd transformers&#xA;pip install protobuf&#xA;python src/transformers/models/llama/convert_llama_weights_to_hf.py \&#xA;   --input_dir /path/to/downloaded/llama/weights --model_size 7B --output_dir /output/path&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Fine-tuning&lt;/h1&gt; &#xA;&lt;p&gt;For fine-tuning Llama 2 models for your domain-specific use cases recipes for PEFT, FSDP, PEFT+FSDP have been included along with a few test datasets. For details see &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/docs/LLM_finetuning.md&#34;&gt;LLM Fine-tuning&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Single and Multi GPU Finetune&lt;/h2&gt; &#xA;&lt;p&gt;If you want to dive right into single or multi GPU fine-tuning, run the examples below on a single GPU like A10, T4, V100, A100 etc. All the parameters in the examples and recipes below need to be further tuned to have desired results based on the model, method, data and task at hand.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;To change the dataset in the commands below pass the &lt;code&gt;dataset&lt;/code&gt; arg. Current options for integrated dataset are &lt;code&gt;grammar_dataset&lt;/code&gt;, &lt;code&gt;alpaca_dataset&lt;/code&gt;and &lt;code&gt;samsum_dataset&lt;/code&gt;. Additionally, we integrate the OpenAssistant/oasst1 dataset as an &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/examples/custom_dataset.py&#34;&gt;example for a custom dataset&lt;/a&gt;. A description of how to use your own dataset and how to add custom datasets can be found in &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/docs/Dataset.md#using-custom-datasets&#34;&gt;Dataset.md&lt;/a&gt;. For &lt;code&gt;grammar_dataset&lt;/code&gt;, &lt;code&gt;alpaca_dataset&lt;/code&gt; please make sure you use the suggested instructions from &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/docs/single_gpu.md#how-to-run-with-different-datasets&#34;&gt;here&lt;/a&gt; to set them up.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Default dataset and other LORA config has been set to &lt;code&gt;samsum_dataset&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Make sure to set the right path to the model in the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/src/llama_recipes/configs/training.py&#34;&gt;training config&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Single GPU:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#if running on multi-gpu machine&#xA;export CUDA_VISIBLE_DEVICES=0&#xA;&#xA;python -m llama_recipes.finetuning  --use_peft --peft_method lora --quantization --model_name /patht_of_model_folder/7B --output_dir Path/to/save/PEFT/model&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here we make use of Parameter Efficient Methods (PEFT) as described in the next section. To run the command above make sure to pass the &lt;code&gt;peft_method&lt;/code&gt; arg which can be set to &lt;code&gt;lora&lt;/code&gt;, &lt;code&gt;llama_adapter&lt;/code&gt; or &lt;code&gt;prefix&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; if you are running on a machine with multiple GPUs please make sure to only make one of them visible using &lt;code&gt;export CUDA_VISIBLE_DEVICES=GPU:id&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Make sure you set &lt;code&gt;save_model&lt;/code&gt; parameter to save the model. Be sure to check the other training parameter in &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/src/llama_recipes/configs/training.py&#34;&gt;train config&lt;/a&gt; as well as others in the config folder as needed. All parameter can be passed as args to the training script. No need to alter the config files.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Multiple GPUs One Node:&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt; please make sure to use PyTorch Nightlies for using PEFT+FSDP. Also, note that int8 quantization from bit&amp;amp;bytes currently is not supported in FSDP.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&#xA;torchrun --nnodes 1 --nproc_per_node 4  examples/finetuning.py --enable_fsdp --use_peft --peft_method lora --model_name /patht_of_model_folder/7B --pure_bf16 --output_dir Path/to/save/PEFT/model&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here we use FSDP as discussed in the next section which can be used along with PEFT methods. To make use of PEFT methods with FSDP make sure to pass &lt;code&gt;use_peft&lt;/code&gt; and &lt;code&gt;peft_method&lt;/code&gt; args along with &lt;code&gt;enable_fsdp&lt;/code&gt;. Here we are using &lt;code&gt;BF16&lt;/code&gt; for training.&lt;/p&gt; &#xA;&lt;h2&gt;Flash Attention and Xformer Memory Efficient Kernels&lt;/h2&gt; &#xA;&lt;p&gt;Setting &lt;code&gt;use_fast_kernels&lt;/code&gt; will enable using of Flash Attention or Xformer memory-efficient kernels based on the hardware being used. This would speed up the fine-tuning job. This has been enabled in &lt;code&gt;optimum&lt;/code&gt; library from HuggingFace as a one-liner API, please read more &lt;a href=&#34;https://pytorch.org/blog/out-of-the-box-acceleration/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;torchrun --nnodes 1 --nproc_per_node 4  examples/finetuning.py --enable_fsdp --use_peft --peft_method lora --model_name /patht_of_model_folder/7B --pure_bf16 --output_dir Path/to/save/PEFT/model --use_fast_kernels&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Fine-tuning using FSDP Only&lt;/h3&gt; &#xA;&lt;p&gt;If you are interested in running full parameter fine-tuning without making use of PEFT methods, please use the following command. Make sure to change the &lt;code&gt;nproc_per_node&lt;/code&gt; to your available GPUs. This has been tested with &lt;code&gt;BF16&lt;/code&gt; on 8xA100, 40GB GPUs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&#xA;torchrun --nnodes 1 --nproc_per_node 8  examples/finetuning.py --enable_fsdp --model_name /patht_of_model_folder/7B --dist_checkpoint_root_folder model_checkpoints --dist_checkpoint_folder fine-tuned --use_fast_kernels&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Fine-tuning using FSDP on 70B Model&lt;/h3&gt; &#xA;&lt;p&gt;If you are interested in running full parameter fine-tuning on the 70B model, you can enable &lt;code&gt;low_cpu_fsdp&lt;/code&gt; mode as the following command. This option will load model on rank0 only before moving model to devices to construct FSDP. This can dramatically save cpu memory when loading large models like 70B (on a 8-gpu node, this reduces cpu memory from 2+T to 280G for 70B model). This has been tested with &lt;code&gt;BF16&lt;/code&gt; on 16xA100, 80GB GPUs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&#xA;torchrun --nnodes 1 --nproc_per_node 8 examples/finetuning.py --enable_fsdp --low_cpu_fsdp --pure_bf16 --model_name /patht_of_model_folder/70B --batch_size_training 1 --dist_checkpoint_root_folder model_checkpoints --dist_checkpoint_folder fine-tuned&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Multi GPU Multi Node:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&#xA;sbatch multi_node.slurm&#xA;# Change the num nodes and GPU per nodes in the script before running.&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can read more about our fine-tuning strategies &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/docs/LLM_finetuning.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Demo Apps&lt;/h1&gt; &#xA;&lt;p&gt;This folder contains a series of Llama2-powered apps:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Quickstart Llama deployments and basic interactions with Llama&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Llama on your Mac and ask Llama general questions&lt;/li&gt; &#xA; &lt;li&gt;Llama on Google Colab&lt;/li&gt; &#xA; &lt;li&gt;Llama on Cloud and ask Llama questions about unstructured data in a PDF&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Specialized Llama use cases:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Ask Llama to summarize a video content&lt;/li&gt; &#xA; &lt;li&gt;Ask Llama questions about structured data in a DB&lt;/li&gt; &#xA; &lt;li&gt;Ask Llama questions about live data on the web&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Repository Organization&lt;/h1&gt; &#xA;&lt;p&gt;This repository is organized in the following way:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/src/llama_recipes/configs/&#34;&gt;configs&lt;/a&gt;: Contains the configuration files for PEFT methods, FSDP, Datasets.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/docs/&#34;&gt;docs&lt;/a&gt;: Example recipes for single and multi-gpu fine-tuning recipes.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/src/llama_recipes/datasets/&#34;&gt;datasets&lt;/a&gt;: Contains individual scripts for each dataset to download and process. Note: Use of any of the datasets should be in compliance with the dataset&#39;s underlying licenses (including but not limited to non-commercial uses)&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/demo_apps&#34;&gt;demo_apps&lt;/a&gt; contains a series of Llama2-powered apps, from quickstart deployments to how to ask Llama questions about unstructured data, structured data, live data, and video summary.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/examples/&#34;&gt;examples&lt;/a&gt;: Contains examples script for finetuning and inference of the Llama 2 model as well as how to use them safely.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/src/llama_recipes/inference/&#34;&gt;inference&lt;/a&gt;: Includes modules for inference for the fine-tuned models.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/src/llama_recipes/model_checkpointing/&#34;&gt;model_checkpointing&lt;/a&gt;: Contains FSDP checkpoint handlers.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/src/llama_recipes/policies/&#34;&gt;policies&lt;/a&gt;: Contains FSDP scripts to provide different policies, such as mixed precision, transformer wrapping policy and activation checkpointing along with any precision optimizer (used for running FSDP with pure bf16 mode).&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/src/llama_recipes/utils/&#34;&gt;utils&lt;/a&gt;: Utility files for:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;train_utils.py&lt;/code&gt; provides training/eval loop and more train utils.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;dataset_utils.py&lt;/code&gt; to get preprocessed datasets.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;config_utils.py&lt;/code&gt; to override the configs received from CLI.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;fsdp_utils.py&lt;/code&gt; provides FSDP wrapping policy for PEFT methods.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;memory_utils.py&lt;/code&gt; context manager to track different memory stats in train loop.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;See the License file &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/LICENSE&#34;&gt;here&lt;/a&gt; and Acceptable Use Policy &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/USE_POLICY.md&#34;&gt;here&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ngruver/llmtime</title>
    <updated>2023-10-18T01:32:51Z</updated>
    <id>tag:github.com,2023-10-18:/ngruver/llmtime</id>
    <link href="https://github.com/ngruver/llmtime" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Large Language Models Are Zero Shot Time Series Forecasters&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains the code for the paper &lt;a href=&#34;https://arxiv.org/abs/2310.07820&#34;&gt;&lt;em&gt;Large Language Models Are Zero Shot Time Series Forecasters&lt;/em&gt;&lt;/a&gt; by Nate Gruver, Marc Finzi, Shikai Qiu and Andrew Gordon Wilson (NeurIPS 2023).&lt;/p&gt; &#xA;&lt;figure&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/ngruver/llmtime/main/assets/llmtime_top_fig.png&#34; alt=&#34;Image&#34;&gt; &#xA; &lt;figcaption&gt;&#xA;   We propose &#xA;  &lt;em&gt;LLMTime&lt;/em&gt;, a method for &#xA;  &lt;em&gt;zero-shot&lt;/em&gt; time series forecasting with large language models (LLMs) by encoding numbers as text and sampling possible extrapolations as text completions. LLMTime can outperform many popular timeseries methods without any training on the target dataset (i.e. zero shot). The performance of LLMTime also scales with the power of the underlying base model. However, models that undergo alignment (e.g. RLHF) do not follow the scaling trend. For example, GPT-4 demonstrates inferior performance to GPT-3. &#xA; &lt;/figcaption&gt; &#xA;&lt;/figure&gt; &#xA;&lt;h2&gt;🛠 Installation&lt;/h2&gt; &#xA;&lt;p&gt;Run the following command to install all dependencies in a conda environment named &lt;code&gt;llmtime&lt;/code&gt;. Change the cuda version for torch if you don&#39;t have cuda 11.8.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sh install.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After installation, activate the environment with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda activate llmtime&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you prefer not using conda, you can also install the dependencies listed in &lt;code&gt;install.sh&lt;/code&gt; manually.&lt;/p&gt; &#xA;&lt;p&gt;Finally, add your openai api key to &lt;code&gt;~/.bashrc&lt;/code&gt; with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;echo &#34;export OPENAI_API_KEY=&amp;lt;your key&amp;gt;&#34; &amp;gt;&amp;gt; ~/.bashrc&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🚀 Trying out LLMTime&lt;/h2&gt; &#xA;&lt;p&gt;Want a quick taste of the power of LLMTime? Run the quick demo in the &lt;code&gt;demo.ipynb&lt;/code&gt; notebook. No GPUs required!&lt;/p&gt; &#xA;&lt;h2&gt;🤖 Plugging in other LLMs&lt;/h2&gt; &#xA;&lt;p&gt;We currently support GPT-3, GPT-3.5, GPT-4, and LLaMA 2. It&#39;s easy to plug in other LLMs by simply specifying how to generate text completions from them in &lt;code&gt;models/llms.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;💡 Tips&lt;/h2&gt; &#xA;&lt;p&gt;Here are some tips for using LLMTime:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Performance is not too sensitive to the data scaling hyperparameters &lt;code&gt;alpha, beta, basic&lt;/code&gt;. A good default is &lt;code&gt;alpha=0.95, beta=0.3, basic=False&lt;/code&gt;. For data exhibiting symmetry around 0 (e.g. a sine wave), we recommend setting &lt;code&gt;basic=True&lt;/code&gt; to avoid shifting the data.&lt;/li&gt; &#xA; &lt;li&gt;The recently released &lt;code&gt;gpt-3.5-turbo-instruct&lt;/code&gt; seems to require a lower temperature (e.g. 0.3) than other models, and tends to not outperform &lt;code&gt;text-davinci-003&lt;/code&gt; from our limited experiments.&lt;/li&gt; &#xA; &lt;li&gt;Tuning hyperparameters based on validation likelihoods, as done by &lt;code&gt;get_autotuned_predictions_data&lt;/code&gt;, will often yield better test likelihoods, but won&#39;t necessarily yield better samples.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;📊 Replicating experiments in paper&lt;/h2&gt; &#xA;&lt;p&gt;Run the following commands to replicate the experiments in the paper. The outputs will be saved in &lt;code&gt;./outputs/&lt;/code&gt;. You can use &lt;code&gt;visualize.ipynb&lt;/code&gt; to visualize the results. We also provide precomputed outputs used in the paper in &lt;code&gt;./precomputed_outputs/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Darts (Section 4)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m experiments.run_darts&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Monash (Section 4)&lt;/h3&gt; &#xA;&lt;p&gt;First download and process the dataset&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m data.monash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then run the experiment&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m experiments.run_monash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Synthetic (Section 5)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m experiments.run_synthetic&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Missing values (Section 6)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m experiments.run_missing&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Memorization (Appendix B)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m experiments.run_memorization&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Please cite our work as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{gruver2023llmtime,&#xA;    title={{Large Language Models Are Zero Shot Time Series Forecasters}},&#xA;    author={Nate Gruver, Marc Finzi, Shikai Qiu and Andrew Gordon Wilson},&#xA;    booktitle={Advances in Neural Information Processing Systems},&#xA;    year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>