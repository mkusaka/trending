<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-10-18T01:35:44Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>7eu7d7/APT-stable-diffusion-auto-prompt</title>
    <updated>2022-10-18T01:35:44Z</updated>
    <id>tag:github.com,2022-10-18:/7eu7d7/APT-stable-diffusion-auto-prompt</id>
    <link href="https://github.com/7eu7d7/APT-stable-diffusion-auto-prompt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;stable diffusion webui with advance prompt tuning&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Stable Diffusion web UI (Advance Prompt Tuning)&lt;/h1&gt; &#xA;&lt;p&gt;基于&lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui&#34;&gt;Stable Diffusion web UI&lt;/a&gt;魔改。&lt;/p&gt; &#xA;&lt;p&gt;APT-prompt, 每一张都是壁纸！告别繁琐的咒文咏唱，一个词足矣！ &lt;img src=&#34;https://raw.githubusercontent.com/7eu7d7/APT-stable-diffusion-auto-prompt/master/txt2img_Screenshot.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;优化prompt tuning，添加如下算法：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;添加负面词条的prompt embedding学习，大幅提高生成图像质量。可以仅从单张图像学习(one-shot learning)&lt;/li&gt; &#xA; &lt;li&gt;添加重建损失，提高生成图像的细节质量和丰富程度。&lt;/li&gt; &#xA; &lt;li&gt;增加基于模型的prompt tuning。基于我之前开发的&lt;a href=&#34;https://github.com/7eu7d7/pixiv_AI_crawler&#34;&gt;AI p站爬虫&lt;/a&gt;中 训练的convnext模型，用另一个AI评估生成图像质量，作为判别器辅助模型训练。可以让prompt学会高质量这一概念，或是学会你的xp。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;预览&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/7eu7d7/APT-stable-diffusion-auto-prompt/master/imgs/p1.png&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/7eu7d7/APT-stable-diffusion-auto-prompt/master/imgs/p2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;使用方法&lt;/h1&gt; &#xA;&lt;p&gt;在创建embedding时，需要分别创建正面和负面的&lt;code&gt;{name}&lt;/code&gt;和&lt;code&gt;{name}-uc&lt;/code&gt;，建议大小分别为3和10。 训练prompt tuning时，选择正面prompt &lt;code&gt;{name}&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;其他使用方法与原版一致，prompt tuning训练过程可以选择是否加载判别器模型。 训练判别器或使用现有判别器见&lt;a href=&#34;https://github.com/7eu7d7/pixiv_AI_crawler&#34;&gt;AI p站爬虫&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/7eu7d7/APT-stable-diffusion-auto-prompt/master/PT.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;效果对比&lt;/h1&gt; &#xA;&lt;p&gt;复杂咒文prompt: (prompt: masterpiece, best quality, loli; negtive: lowres, bad anatomy, bad hands, text, error, missing fingers, extra fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry) &lt;img src=&#34;https://raw.githubusercontent.com/7eu7d7/APT-stable-diffusion-auto-prompt/master/imgs/artificial.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;APT-prompt tuning: (prompt: ptun-cnx, loli; negtive: ptun-cnx-uc) &lt;img src=&#34;https://raw.githubusercontent.com/7eu7d7/APT-stable-diffusion-auto-prompt/master/imgs/cnx.png&#34; alt=&#34;&#34;&gt; 加入细节描述可以进一步提升&lt;/p&gt; &#xA;&lt;p&gt;(prompt: ptun-d, loli, light blue hair, outside, blue butterfly, with flowers in hand, cat ears and tail, buildings in the background;&lt;br&gt; negtive: ptun-d-uc, lowres, bad anatomy, bad hands, text, error, missing fingers, extra fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry) &lt;img src=&#34;https://raw.githubusercontent.com/7eu7d7/APT-stable-diffusion-auto-prompt/master/imgs/cnx2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;webui prompt tuning: (prompt: ptun-webui, loli; negtive: None) &lt;img src=&#34;https://raw.githubusercontent.com/7eu7d7/APT-stable-diffusion-auto-prompt/master/imgs/pwebui.png&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/7eu7d7/APT-stable-diffusion-auto-prompt/master/imgs/webui.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;no prompt tuning: (prompt: loli; negtive: None) &lt;img src=&#34;https://raw.githubusercontent.com/7eu7d7/APT-stable-diffusion-auto-prompt/master/imgs/raw.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>JoePenna/Dreambooth-Stable-Diffusion</title>
    <updated>2022-10-18T01:35:44Z</updated>
    <id>tag:github.com,2022-10-18:/JoePenna/Dreambooth-Stable-Diffusion</id>
    <link href="https://github.com/JoePenna/Dreambooth-Stable-Diffusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Implementation of Dreambooth (https://arxiv.org/abs/2208.12242) by way of Textual Inversion (https://arxiv.org/abs/2208.01618) for Stable Diffusion (https://arxiv.org/abs/2112.10752). Tweaks focused on training faces, objects, and styles.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Index&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JoePenna/Dreambooth-Stable-Diffusion/main/#notes-by-joe-penna&#34;&gt;Notes by Joe Penna&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JoePenna/Dreambooth-Stable-Diffusion/main/#setup&#34;&gt;Setup&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JoePenna/Dreambooth-Stable-Diffusion/main/#easy-runpod-instructions&#34;&gt;Easy RunPod Instructions&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JoePenna/Dreambooth-Stable-Diffusion/main/#vast-ai-setup&#34;&gt;Vast.AI Setup&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JoePenna/Dreambooth-Stable-Diffusion/main/#text-vs-dreamb&#34;&gt;Textual Inversion vs. Dreambooth&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JoePenna/Dreambooth-Stable-Diffusion/main/#using-the-generated-model&#34;&gt;Using the Generated Model&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JoePenna/Dreambooth-Stable-Diffusion/main/#debugging-your-results&#34;&gt;Debugging Your Results&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JoePenna/Dreambooth-Stable-Diffusion/main/#they-dont-look-like-you&#34;&gt;They don&#39;t look like you at all!&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JoePenna/Dreambooth-Stable-Diffusion/main/#they-sorta-look-like-you-but-exactly-like-your-training-images&#34;&gt;They sorta look like you, but exactly like your training images&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JoePenna/Dreambooth-Stable-Diffusion/main/#they-look-like-you-but-not-when-you-try-different-styles&#34;&gt;They look like you, but not when you try different styles&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JoePenna/Dreambooth-Stable-Diffusion/main/#hugging-face-diffusers&#34;&gt;Hugging Face Diffusers&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;The Repo Formerly Known As &#34;Dreambooth&#34;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/100188076/192390551-cb89364f-af57-4aed-8f3d-f9eb9b61cf95.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;notes-by-joe-penna&#34;&gt;&lt;/a&gt; Notes by Joe Penna&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;strong&gt;INTRODUCTIONS!&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Hi! My name is Joe Penna.&lt;/p&gt; &#xA;&lt;p&gt;You might have seen a few YouTube videos of mine under &lt;em&gt;MysteryGuitarMan&lt;/em&gt;. I&#39;m now a feature film director. You might have seen &lt;a href=&#34;https://www.youtube.com/watch?v=N5aD9ppoQIo&amp;amp;t=6s&#34;&gt;ARCTIC&lt;/a&gt; or &lt;a href=&#34;https://www.youtube.com/watch?v=A_apvQkWsVY&#34;&gt;STOWAWAY&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For my movies, I need to be able to train specific actors, props, locations, etc. So, I did a bunch of changes to @XavierXiao&#39;s repo in order to train people&#39;s faces.&lt;/p&gt; &#xA;&lt;p&gt;I can&#39;t release all the tests for the movie I&#39;m working on, but when I test with my own face, I release those on my Twitter page - &lt;a href=&#34;https://twitter.com/MysteryGuitarM&#34;&gt;@MysteryGuitarM&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Lots of these tests were done with a buddy of mine -- Niko from CorridorDigital. It might be how you found this repo!&lt;/p&gt; &#xA;&lt;p&gt;I&#39;m not really a coder. I&#39;m just stubborn, and I&#39;m not afraid of googling. So, eventually, some really smart folks joined in and have been contributing. In this repo, specifically: &lt;a href=&#34;https://github.com/djbielejeski&#34;&gt;@djbielejeski&lt;/a&gt; @gammagec @MrSaad –– but so many others in our Discord!&lt;/p&gt; &#xA;&lt;p&gt;This is no longer my repo. This is the people-who-wanna-see-Dreambooth-on-SD-working-well&#39;s repo!&lt;/p&gt; &#xA;&lt;p&gt;Now, if you wanna try to do this... please read the warnings below first:&lt;/p&gt; &#xA;&lt;h3&gt;&lt;strong&gt;WARNING!&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;This is bleeding edge stuff&lt;/strong&gt;... there is currently no easy way to run this. This repo is based on a repo based on another repo.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;At the moment, it takes a LOT of effort to create something that&#39;s basically duct tape and bubble gum -- but eventually works SUPER well.&lt;/li&gt; &#xA;   &lt;li&gt;Step in, please! Don&#39;t let that scare ya -- but please know that you&#39;re wading through the jungle at night, with no torch...&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Unfreezing the model takes a lot of juice.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;del&gt;You&#39;re gonna need an A6000 / A40 / A100 (or similar top-of-the-line thousands-of-dollars GPU).&lt;/del&gt;&lt;/li&gt; &#xA;   &lt;li&gt;You can now run this on a GPU with 24GB of VRAM (e.g. 3090). Training will be slower, and you&#39;ll need to be sure this is the &lt;em&gt;only&lt;/em&gt; program running.&lt;/li&gt; &#xA;   &lt;li&gt;If, like myself, you don&#39;t happen to own one of those, I&#39;m including a Jupyter notebook here to help you run it on a rented cloud computing platform.&lt;/li&gt; &#xA;   &lt;li&gt;It&#39;s currently tailored to &lt;a href=&#34;https://runpod.io?ref=n8yfwyum&#34;&gt;runpod.io&lt;/a&gt;, but can work on &lt;a href=&#34;https://raw.githubusercontent.com/JoePenna/Dreambooth-Stable-Diffusion/main/#vast-ai-setup&#34;&gt;vast.ai&lt;/a&gt; / etc.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;This implementation does not fully implement Google&#39;s ideas on how to preserve the latent space.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Most images that are similar to what you&#39;re training will be shifted towards that.&lt;/li&gt; &#xA;   &lt;li&gt;e.g. If you&#39;re training a person, all people will look like you. If you&#39;re training an object, anything in that class will look like your object.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;There doesn&#39;t seem to be an easy way to train two subjects consecutively. You will end up with an &lt;code&gt;11-12GB&lt;/code&gt; file before pruning.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The provided notebook has a pruner that crunches it down to &lt;code&gt;~2gb&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Best practice is to change the &lt;strong&gt;token&lt;/strong&gt; to a celebrity name (&lt;em&gt;note: token, not class&lt;/em&gt; -- so your prompt would be something like: &lt;code&gt;Chris Evans person&lt;/code&gt;). Here&#39;s &lt;a href=&#34;https://raw.githubusercontent.com/JoePenna/Dreambooth-Stable-Diffusion/main/#using-the-generated-model&#34;&gt;my wife trained with the exact same settings, except for the token&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;&lt;a name=&#34;setup&#34;&gt;&lt;/a&gt; Setup&lt;/h1&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;easy-runpod-instructions&#34;&gt;&lt;/a&gt; Easy RunPod Instructions&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Sign up for RunPod. Feel free to use my &lt;a href=&#34;https://runpod.io?ref=n8yfwyum&#34;&gt;referral link here&lt;/a&gt;, so that I don&#39;t have to pay for it (but you do).&lt;/li&gt; &#xA; &lt;li&gt;Click &lt;strong&gt;Deploy&lt;/strong&gt; on either &lt;code&gt;SECURE CLOUD&lt;/code&gt; or &lt;code&gt;COMMUNITY CLOUD&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Follow these video instructions here:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=7m__xadX0z0#t=5m33.1s&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/7m__xadX0z0/0.jpg&#34; alt=&#34;VIDEO INSTRUCTIONS&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;vast-ai-setup&#34;&gt;&lt;/a&gt; Vast.AI Instructions&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Sign up for &lt;a href=&#34;https://vast.ai/&#34;&gt;Vast.AI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Add some funds (I typically add them in $10 increments)&lt;/li&gt; &#xA; &lt;li&gt;Navigate to the &lt;a href=&#34;https://vast.ai/console/create/&#34;&gt;Client - Create page&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Select pytorch/pytorch as your docker image, and the buttons &#34;Use Jupyter Lab Interface&#34; and &#34;Jupyter direct HTTPS&#34;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://raw.githubusercontent.com/JoePenna/Dreambooth-Stable-Diffusion/main/readme-images/vast-ai-step1-select-docker-image.png&#34; alt=&#34;img.png&#34;&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;You will want to increase your disk space, and filter on GPU RAM (12gb checkpoint files + 4gb model file + regularization images + other stuff adds up fast) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;I typically allocate 150GB&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://raw.githubusercontent.com/JoePenna/Dreambooth-Stable-Diffusion/main/readme-images/vast-ai-step2-instance-filters.png&#34; alt=&#34;img.png&#34;&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Also good to check the Upload/Download speed for enough bandwidth so you don&#39;t spend all your money waiting for things to download.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Select the instance you want, and click &lt;code&gt;Rent&lt;/code&gt;, then head over to your &lt;a href=&#34;https://vast.ai/console/instances/&#34;&gt;Instances&lt;/a&gt; page and click &lt;code&gt;Open&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://raw.githubusercontent.com/JoePenna/Dreambooth-Stable-Diffusion/main/readme-images/vast-ai-step3-instances.png&#34; alt=&#34;img.png&#34;&gt;&lt;/li&gt; &#xA;   &lt;li&gt;You will get an unsafe certificate warning. Click past the warning or install the &lt;a href=&#34;https://vast.ai/static/jvastai_root.cer&#34;&gt;Vast cert&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Click &lt;code&gt;Notebook -&amp;gt; Python 3&lt;/code&gt; (You can do this next step a number of ways, but I typically do this) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://raw.githubusercontent.com/JoePenna/Dreambooth-Stable-Diffusion/main/readme-images/vast-ai-step4-get-repo.png&#34; alt=&#34;img.png&#34;&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Clone Joe&#39;s repo with this command &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;!git clone https://github.com/JoePenna/Dreambooth-Stable-Diffusion.git&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Click &lt;code&gt;run&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://raw.githubusercontent.com/JoePenna/Dreambooth-Stable-Diffusion/main/readme-images/vast-ai-step5-clone-repo.png&#34; alt=&#34;img.png&#34;&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Navigate into the new &lt;code&gt;Dreambooth-Stable-Diffusion&lt;/code&gt; directory on the left and open the &lt;code&gt;dreambooth_runpod_joepenna.ipynb&lt;/code&gt; file &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://raw.githubusercontent.com/JoePenna/Dreambooth-Stable-Diffusion/main/readme-images/vast-ai-step6-open-notebook.png&#34; alt=&#34;img.png&#34;&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Follow the instructions in the workbook and start training&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;&lt;a name=&#34;text-vs-dreamb&#34;&gt;&lt;/a&gt; Textual Inversion vs. Dreambooth&lt;/h1&gt; &#xA;&lt;p&gt;The majority of the code in this repo was written by Rinon Gal et. al, the authors of the Textual Inversion research paper. Though a few ideas about regularization images and prior loss preservation (ideas from &#34;Dreambooth&#34;) were added in, out of respect to both the MIT team and the Google researchers, I&#39;m renaming this fork to: &lt;em&gt;&#34;The Repo Formerly Known As &#34;Dreambooth&#34;&#34;&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For an alternate implementation , please see &lt;a href=&#34;https://raw.githubusercontent.com/JoePenna/Dreambooth-Stable-Diffusion/main/#hugging-face-diffusers&#34;&gt;&#34;Alternate Option&#34;&lt;/a&gt; below.&lt;/p&gt; &#xA;&lt;h1&gt;&lt;a name=&#34;using-the-generated-model&#34;&gt;&lt;/a&gt; Using the generated model&lt;/h1&gt; &#xA;&lt;p&gt;The &lt;code&gt;ground truth&lt;/code&gt; (real picture, caution: very beautiful woman) &lt;br&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/100188076/192403948-8d1d0e50-3e9f-495f-b8ba-1bcb6b536fc8.png&#34; width=&#34;200&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Same prompt for all of these images below:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;code&gt;sks person&lt;/code&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;code&gt;woman person&lt;/code&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;code&gt;Natalie Portman person&lt;/code&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;code&gt;Kate Mara person&lt;/code&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/100188076/192403506-ab96c652-f7d0-47b0-98fa-267defa1e511.png&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/100188076/192403491-cb258777-5091-4492-a6cc-82305fa729f4.png&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/100188076/192403437-f9a93720-d41c-4334-8901-fa2d2a10fe36.png&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/100188076/192403461-1f6972d9-64d0-46b0-b2ed-737e47aae31e.png&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;&lt;a name=&#34;debugging-your-results&#34;&gt;&lt;/a&gt; Debugging your results&lt;/h1&gt; &#xA;&lt;h3&gt;❗❗ THE NUMBER ONE MISTAKE PEOPLE MAKE ❗❗&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompting with just your token. ie &#34;joepenna&#34; instead of &#34;joepenna person&#34;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you trained with &lt;code&gt;joepenna&lt;/code&gt; under the class &lt;code&gt;person&lt;/code&gt;, the model should only know your face as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;joepenna person&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Example Prompts:&lt;/p&gt; &#xA;&lt;p&gt;🚫 Incorrect (missing &lt;code&gt;person&lt;/code&gt; following &lt;code&gt;joepenna&lt;/code&gt;)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;portrait photograph of joepenna 35mm film vintage glass&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;✅ This is right (&lt;code&gt;person&lt;/code&gt; is included after &lt;code&gt;joepenna&lt;/code&gt;)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;portrait photograph of joepenna person 35mm film vintage glass&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You might sometimes get someone who kinda looks like you with joepenna (especially if you trained for too many steps), but that&#39;s only because this current iteration of Dreambooth overtrains that token so much that it bleeds into that token.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;☢ Be careful with the types of images you train&lt;/h4&gt; &#xA;&lt;p&gt;While training, Stable doesn&#39;t know that you&#39;re a person. It&#39;s just going to mimic what it sees.&lt;/p&gt; &#xA;&lt;p&gt;So, if these are your training images look like this:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/JoePenna/Dreambooth-Stable-Diffusion/main/readme-images/caution-training.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;You&#39;re only going to get generations of you outside next to a spiky tree, wearing a white-and-gray shirt, in the style of... well, selfie photograph.&lt;/p&gt; &#xA;&lt;p&gt;Instead, this training set is much better:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/JoePenna/Dreambooth-Stable-Diffusion/main/readme-images/better-training-images.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The only thing that is consistent between images is the subject. So, Stable will look through the images and learn only your face, which will make &#34;editing&#34; it into other styles possible.&lt;/p&gt; &#xA;&lt;h2&gt;Oh no! You&#39;re not getting good generations!&lt;/h2&gt; &#xA;&lt;h4&gt;&lt;a name=&#34;they-dont-look-like-you&#34;&gt;&lt;/a&gt; OPTION 1: They&#39;re not looking like you at all! (Train longer, or get better training images)&lt;/h4&gt; &#xA;&lt;p&gt;Are you sure you&#39;re prompting it right?&lt;/p&gt; &#xA;&lt;p&gt;It should be &lt;code&gt;&amp;lt;token&amp;gt; &amp;lt;class&amp;gt;&lt;/code&gt;, not just &lt;code&gt;&amp;lt;token&amp;gt;&lt;/code&gt;. For example:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;JoePenna person, portrait photograph, 85mm medium format photo&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;If it still doesn&#39;t look like you, you didn&#39;t train long enough.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;&lt;a name=&#34;they-sorta-look-like-you-but-exactly-like-your-training-images&#34;&gt;&lt;/a&gt; OPTION 2: They&#39;re looking like you, but are all looking like your training images. (Train for less steps, get better training images, fix with prompting)&lt;/h4&gt; &#xA;&lt;p&gt;Okay, a few reasons why: you might have trained too long... or your images were too similar... or you didn&#39;t train with enough images.&lt;/p&gt; &#xA;&lt;p&gt;No problem. We can fix that with the prompt. Stable Diffusion puts a LOT of merit to whatever you type first. So save it for later:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;an exquisite portrait photograph, 85mm medium format photo of JoePenna person with a classic haircut&lt;/code&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;&lt;a name=&#34;they-look-like-you-but-not-when-you-try-different-styles&#34;&gt;&lt;/a&gt; OPTION 3: They&#39;re looking like you, but not when you try different styles. (Train longer, get better training images)&lt;/h4&gt; &#xA;&lt;p&gt;You didn&#39;t train long enough...&lt;/p&gt; &#xA;&lt;p&gt;No problem. We can fix that with the prompt:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;JoePenna person in a portrait photograph, JoePenna person in a 85mm medium format photo of JoePenna person&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;More tips and help here: &lt;a href=&#34;https://discord.com/invite/qbMuXBXyHA&#34;&gt;Stable Diffusion Dreambooth Discord&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h1&gt;&lt;a name=&#34;hugging-face-diffusers&#34;&gt;&lt;/a&gt; Hugging Face Diffusers - Alternate Option&lt;/h1&gt; &#xA;&lt;p&gt;Dreambooth is now supported in HuggingFace Diffusers for training with Stable Diffusion.&lt;/p&gt; &#xA;&lt;p&gt;Try it out here:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ShapeAI/Python-for-Data-Science-</title>
    <updated>2022-10-18T01:35:44Z</updated>
    <id>tag:github.com,2022-10-18:/ShapeAI/Python-for-Data-Science-</id>
    <link href="https://github.com/ShapeAI/Python-for-Data-Science-" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;</summary>
  </entry>
</feed>