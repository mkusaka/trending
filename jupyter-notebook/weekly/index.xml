<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-10-30T01:48:02Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>kwea123/nerf_pl</title>
    <updated>2022-10-30T01:48:02Z</updated>
    <id>tag:github.com,2022-10-30:/kwea123/nerf_pl</id>
    <link href="https://github.com/kwea123/nerf_pl" rel="alternate"></link>
    <summary type="html">&lt;p&gt;NeRF (Neural Radiance Fields) and NeRF in the Wild using pytorch-lightning&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;nerf_pl&lt;/h1&gt; &#xA;&lt;h3&gt;Update: NVIDIA open-sourced a lightning-fast version of NeRF: &lt;a href=&#34;https://github.com/NVlabs/instant-ngp&#34;&gt;NGP&lt;/a&gt;. I re-implemented in pytorch &lt;a href=&#34;https://github.com/kwea123/ngp_pl&#34;&gt;here&lt;/a&gt;. This version is ~100x faster than this repo with also better quality!&lt;/h3&gt; &#xA;&lt;h3&gt;Update: an improved &lt;a href=&#34;https://www.cs.cornell.edu/~zl548/NSFF/&#34;&gt;NSFF&lt;/a&gt; implementation to handle dynamic scene is &lt;a href=&#34;https://github.com/kwea123/nsff_pl&#34;&gt;open&lt;/a&gt;!&lt;/h3&gt; &#xA;&lt;h3&gt;Update: &lt;a href=&#34;https://nerf-w.github.io/&#34;&gt;NeRF-W&lt;/a&gt; (NeRF in the Wild) implementation is added to &lt;a href=&#34;https://github.com/kwea123/nerf_pl/tree/nerfw&#34;&gt;nerfw&lt;/a&gt; branch!&lt;/h3&gt; &#xA;&lt;h3&gt;Update: The lastest code (using the latest libraries) will be updated to &lt;a href=&#34;https://github.com/kwea123/nerf_pl/tree/dev&#34;&gt;dev&lt;/a&gt; branch. The master branch remains to support the colab files. If you don&#39;t use colab, it is recommended to switch to dev branch. Only issues of the dev and nerfw branch will be considered currently.&lt;/h3&gt; &#xA;&lt;h3&gt;&lt;span&gt;💎&lt;/span&gt; &lt;a href=&#34;https://kwea123.github.io/nerf_pl/&#34;&gt;&lt;strong&gt;Project page&lt;/strong&gt;&lt;/a&gt; (live demo!)&lt;/h3&gt; &#xA;&lt;p&gt;Unofficial implementation of &lt;a href=&#34;https://arxiv.org/pdf/2003.08934.pdf&#34;&gt;NeRF&lt;/a&gt; (Neural Radiance Fields) using pytorch (&lt;a href=&#34;https://github.com/PyTorchLightning/pytorch-lightning&#34;&gt;pytorch-lightning&lt;/a&gt;). This repo doesn&#39;t aim at reproducibility, but aim at providing a simpler and faster training procedure (also simpler code with detailed comments to help to understand the work). Moreover, I try to extend much more opportunities by integrating this algorithm into game engine like Unity.&lt;/p&gt; &#xA;&lt;p&gt;Official implementation: &lt;a href=&#34;https://github.com/bmild/nerf&#34;&gt;nerf&lt;/a&gt; .. Reference pytorch implementation: &lt;a href=&#34;https://github.com/yenchenlin/nerf-pytorch&#34;&gt;nerf-pytorch&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Recommend to read: A detailed NeRF extension list: &lt;a href=&#34;https://github.com/yenchenlin/awesome-NeRF&#34;&gt;awesome-NeRF&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h2&gt;&lt;span&gt;🌌&lt;/span&gt; Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Multi-gpu training: Training on 8 GPUs finishes within 1 hour for the synthetic dataset!&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/#mortar_board-colab&#34;&gt;Colab&lt;/a&gt; notebooks to allow easy usage!&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/#ribbon-mesh&#34;&gt;Reconstruct&lt;/a&gt; &lt;strong&gt;colored&lt;/strong&gt; mesh!&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtu.be/S5phWFTs2iM&#34;&gt;Mixed Reality&lt;/a&gt; in Unity!&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtu.be/w9qTbVzCdWk&#34;&gt;REAL TIME volume rendering&lt;/a&gt; in Unity!&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/#portable-scenes&#34;&gt;Portable Scenes&lt;/a&gt; to let you play with other people&#39;s scenes!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;You can find the Unity project including mesh, mixed reality and volume rendering &lt;a href=&#34;https://github.com/kwea123/nerf_Unity&#34;&gt;here&lt;/a&gt;! See &lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/README_Unity.md&#34;&gt;README_Unity&lt;/a&gt; for generating your own data for Unity rendering!&lt;/h3&gt; &#xA;&lt;h2&gt;&lt;span&gt;🔰&lt;/span&gt; Tutorial&lt;/h2&gt; &#xA;&lt;h3&gt;What can NeRF do?&lt;/h3&gt; &#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/11364490/82124460-1ccbbb80-97da-11ea-88ad-25e22868a5c1.png&#34; style=&#34;max-width:100%&#34;&gt; &#xA;&lt;h3&gt;Tutorial videos&lt;/h3&gt; &#xA;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLDV2CyUo4q-K02pNEyDr7DYpTQuka3mbV&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/11364490/80913471-d5781080-8d7f-11ea-9f72-9d68402b8271.png&#34;&gt; &lt;/a&gt; &#xA;&lt;h1&gt;&lt;span&gt;💻&lt;/span&gt; Installation&lt;/h1&gt; &#xA;&lt;h2&gt;Hardware&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OS: Ubuntu 18.04&lt;/li&gt; &#xA; &lt;li&gt;NVIDIA GPU with &lt;strong&gt;CUDA&amp;gt;=10.1&lt;/strong&gt; (tested with 1 RTX2080Ti)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Software&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Clone this repo by &lt;code&gt;git clone --recursive https://github.com/kwea123/nerf_pl&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Python&amp;gt;=3.6 (installation via &lt;a href=&#34;https://www.anaconda.com/distribution/&#34;&gt;anaconda&lt;/a&gt; is recommended, use &lt;code&gt;conda create -n nerf_pl python=3.6&lt;/code&gt; to create a conda environment and activate it by &lt;code&gt;conda activate nerf_pl&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Python libraries &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Install core requirements by &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Install &lt;code&gt;torchsearchsorted&lt;/code&gt; by &lt;code&gt;cd torchsearchsorted&lt;/code&gt; then &lt;code&gt;pip install .&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;&lt;span&gt;🔑&lt;/span&gt; Training&lt;/h1&gt; &#xA;&lt;p&gt;Please see each subsection for training on different datasets. Available training datasets:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/#blender&#34;&gt;Blender&lt;/a&gt; (Realistic Synthetic 360)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/#llff&#34;&gt;LLFF&lt;/a&gt; (Real Forward-Facing)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/#your-own-data&#34;&gt;Your own data&lt;/a&gt; (Forward-Facing/360 inward-facing)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Blender&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Steps&lt;/summary&gt; &#xA; &lt;h3&gt;Data download&lt;/h3&gt; &#xA; &lt;p&gt;Download &lt;code&gt;nerf_synthetic.zip&lt;/code&gt; from &lt;a href=&#34;https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;h3&gt;Training model&lt;/h3&gt; &#xA; &lt;p&gt;Run (example)&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;python train.py \&#xA;   --dataset_name blender \&#xA;   --root_dir $BLENDER_DIR \&#xA;   --N_importance 64 --img_wh 400 400 --noise_std 0 \&#xA;   --num_epochs 16 --batch_size 1024 \&#xA;   --optimizer adam --lr 5e-4 \&#xA;   --lr_scheduler steplr --decay_step 2 4 8 --decay_gamma 0.5 \&#xA;   --exp_name exp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;These parameters are chosen to best mimic the training settings in the original repo. See &lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/opt.py&#34;&gt;opt.py&lt;/a&gt; for all configurations.&lt;/p&gt; &#xA; &lt;p&gt;NOTE: the above configuration doesn&#39;t work for some scenes like &lt;code&gt;drums&lt;/code&gt;, &lt;code&gt;ship&lt;/code&gt;. In that case, consider increasing the &lt;code&gt;batch_size&lt;/code&gt; or change the &lt;code&gt;optimizer&lt;/code&gt; to &lt;code&gt;radam&lt;/code&gt;. I managed to train on all scenes with these modifications.&lt;/p&gt; &#xA; &lt;p&gt;You can monitor the training process by &lt;code&gt;tensorboard --logdir logs/&lt;/code&gt; and go to &lt;code&gt;localhost:6006&lt;/code&gt; in your browser.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;LLFF&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Steps&lt;/summary&gt; &#xA; &lt;h3&gt;Data download&lt;/h3&gt; &#xA; &lt;p&gt;Download &lt;code&gt;nerf_llff_data.zip&lt;/code&gt; from &lt;a href=&#34;https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;h3&gt;Training model&lt;/h3&gt; &#xA; &lt;p&gt;Run (example)&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;python train.py \&#xA;   --dataset_name llff \&#xA;   --root_dir $LLFF_DIR \&#xA;   --N_importance 64 --img_wh 504 378 \&#xA;   --num_epochs 30 --batch_size 1024 \&#xA;   --optimizer adam --lr 5e-4 \&#xA;   --lr_scheduler steplr --decay_step 10 20 --decay_gamma 0.5 \&#xA;   --exp_name exp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;These parameters are chosen to best mimic the training settings in the original repo. See &lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/opt.py&#34;&gt;opt.py&lt;/a&gt; for all configurations.&lt;/p&gt; &#xA; &lt;p&gt;You can monitor the training process by &lt;code&gt;tensorboard --logdir logs/&lt;/code&gt; and go to &lt;code&gt;localhost:6006&lt;/code&gt; in your browser.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Your own data&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Steps&lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Install &lt;a href=&#34;https://github.com/colmap/colmap&#34;&gt;COLMAP&lt;/a&gt; following &lt;a href=&#34;https://colmap.github.io/install.html&#34;&gt;installation guide&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Prepare your images in a folder (around 20 to 30 for forward facing, and 40 to 50 for 360 inward-facing)&lt;/li&gt; &#xA;  &lt;li&gt;Clone &lt;a href=&#34;https://github.com/Fyusion/LLFF&#34;&gt;LLFF&lt;/a&gt; and run &lt;code&gt;python img2poses.py $your-images-folder&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Train the model using the same command as in &lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/#llff&#34;&gt;LLFF&lt;/a&gt;. If the scene is captured in a 360 inward-facing manner, add &lt;code&gt;--spheric&lt;/code&gt; argument.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;For more details of training a good model, please see the video &lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/#colab&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Pretrained models and logs&lt;/h2&gt; &#xA;&lt;p&gt;Download the pretrained models and training logs in &lt;a href=&#34;https://github.com/kwea123/nerf_pl/releases&#34;&gt;release&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Comparison with other repos&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;training GPU memory in GB&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Speed (1 step)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/bmild/nerf&#34;&gt;Original&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.177s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/yenchenlin/nerf-pytorch&#34;&gt;Ref pytorch&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.147s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;This repo&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.12s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The speed is measured on 1 RTX2080Ti. Detailed profile can be found in &lt;a href=&#34;https://github.com/kwea123/nerf_pl/releases&#34;&gt;release&lt;/a&gt;. Training memory is largely reduced, since the original repo loads the whole data to GPU at the beginning, while we only pass batches to GPU every step.&lt;/p&gt; &#xA;&lt;h1&gt;&lt;span&gt;🔎&lt;/span&gt; Testing&lt;/h1&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/test.ipynb&#34;&gt;test.ipynb&lt;/a&gt; for a simple view synthesis and depth prediction on 1 image.&lt;/p&gt; &#xA;&lt;p&gt;Use &lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/eval.py&#34;&gt;eval.py&lt;/a&gt; to create the whole sequence of moving views. E.g.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python eval.py \&#xA;   --root_dir $BLENDER \&#xA;   --dataset_name blender --scene_name lego \&#xA;   --img_wh 400 400 --N_importance 64 --ckpt_path $CKPT_PATH&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;IMPORTANT&lt;/strong&gt; : Don&#39;t forget to add &lt;code&gt;--spheric_poses&lt;/code&gt; if the model is trained under &lt;code&gt;--spheric&lt;/code&gt; setting!&lt;/p&gt; &#xA;&lt;p&gt;It will create folder &lt;code&gt;results/{dataset_name}/{scene_name}&lt;/code&gt; and run inference on all test data, finally create a gif out of them.&lt;/p&gt; &#xA;&lt;p&gt;Example of lego scene using pretrained model and the reconstructed &lt;strong&gt;colored&lt;/strong&gt; mesh: (PSNR=31.39, paper=32.54)&lt;/p&gt; &#xA;&lt;p&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/11364490/79932648-f8a1e680-8488-11ea-98fe-c11ec22fc8a1.gif&#34; width=&#34;200&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/11364490/80813179-822d8300-8c04-11ea-84e6-142f04714c58.png&#34; width=&#34;200&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Example of fern scene using pretrained model:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/11364490/79932650-f9d31380-8488-11ea-8dad-b70a6a3daa6e.gif&#34; alt=&#34;fern&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Example of own scene (&lt;a href=&#34;https://www.youtube.com/watch?v=hVQIvEq_Av0&#34;&gt;Silica GGO figure&lt;/a&gt;) and the reconstructed &lt;strong&gt;colored&lt;/strong&gt; mesh. Click to link to youtube video.&lt;/p&gt; &#xA;&lt;p&gt; &lt;a href=&#34;https://youtu.be/yH1ZBcdNsUY&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/11364490/80279695-324d4880-873a-11ea-961a-d6350e149ece.gif&#34; height=&#34;252&#34;&gt; &lt;/a&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/11364490/80813184-83f74680-8c04-11ea-8606-40580f753355.png&#34; height=&#34;252&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Portable scenes&lt;/h2&gt; &#xA;&lt;p&gt;The concept of NeRF is that the whole scene is compressed into a NeRF model, then we can render from any pose we want. To render from plausible poses, we can leverage the training poses; therefore, you can generate video with &lt;strong&gt;only&lt;/strong&gt; the trained model and the poses (hence the name of portable scenes). I provided my silica model in &lt;a href=&#34;https://github.com/kwea123/nerf_pl/releases&#34;&gt;release&lt;/a&gt;, feel free to play around with it!&lt;/p&gt; &#xA;&lt;p&gt;If you trained some interesting scenes, you are also welcomed to share the model (and the &lt;code&gt;poses_bounds.npy&lt;/code&gt;) by sending me an email, or post in issues! After all, a model is just around &lt;strong&gt;5MB&lt;/strong&gt;! Please run &lt;code&gt;python utils/save_weights_only.py --ckpt_path $YOUR_MODEL_PATH&lt;/code&gt; to extract the final model.&lt;/p&gt; &#xA;&lt;h1&gt;&lt;span&gt;🎀&lt;/span&gt; Mesh&lt;/h1&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/README_mesh.md&#34;&gt;README_mesh&lt;/a&gt; for reconstruction of &lt;strong&gt;colored&lt;/strong&gt; mesh. Only supported for blender dataset and 360 inward-facing data!&lt;/p&gt; &#xA;&lt;h1&gt;&lt;span&gt;⚠&lt;/span&gt; Notes on differences with the original repo&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The learning rate decay in the original repo is &lt;strong&gt;by step&lt;/strong&gt;, which means it decreases every step, here I use learning rate decay &lt;strong&gt;by epoch&lt;/strong&gt;, which means it changes only at the end of 1 epoch.&lt;/li&gt; &#xA; &lt;li&gt;The validation image for LLFF dataset is chosen as the most centered image here, whereas the original repo chooses every 8th image.&lt;/li&gt; &#xA; &lt;li&gt;The rendering spiral path is slightly different from the original repo (I use approximate values to simplify the code).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;&lt;span&gt;🎓&lt;/span&gt; COLAB&lt;/h1&gt; &#xA;&lt;p&gt;I also prepared colab notebooks that allow you to run the algorithm on any machine without GPU requirement.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gist.github.com/kwea123/f0e8f38ff2aa94495dbfe7ae9219f75c&#34;&gt;colmap&lt;/a&gt; to prepare camera poses for your own training data&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gist.github.com/kwea123/a3c541a325e895ef79ecbc0d2e6d7221&#34;&gt;nerf&lt;/a&gt; to train on your data&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gist.github.com/kwea123/77ed1640f9bc9550136dc13a6a419e88&#34;&gt;extract_mesh&lt;/a&gt; to extract colored mesh&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://www.youtube.com/playlist?list=PLDV2CyUo4q-K02pNEyDr7DYpTQuka3mbV&#34;&gt;this playlist&lt;/a&gt; for the detailed tutorials.&lt;/p&gt; &#xA;&lt;h1&gt;&lt;span&gt;🎃&lt;/span&gt; SHOWOFF&lt;/h1&gt; &#xA;&lt;p&gt;We can incorporate &lt;em&gt;ray tracing&lt;/em&gt; techniques into the volume rendering pipeline, and realize realistic scene editing (following is the &lt;code&gt;materials&lt;/code&gt; scene with an object removed, and a mesh is inserted and rendered with ray tracing). The code &lt;strong&gt;will not&lt;/strong&gt; be released.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/11364490/90312710-92face00-df41-11ea-9eea-10f24849b407.gif&#34; alt=&#34;add&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/11364490/90360796-92744b80-e097-11ea-859d-159aa2519375.gif&#34; alt=&#34;add2&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;With my integration in Unity, I can realize realistic mixed reality photos (note my character casts shadow on the scene, &lt;strong&gt;zero&lt;/strong&gt; post- image editing required): &lt;img src=&#34;https://user-images.githubusercontent.com/11364490/140264589-295acebe-8ace-4d61-b871-26eb8ae10ab0.png&#34; alt=&#34;defer&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/11364490/140264596-59daebe5-b88d-48e7-82bd-5ccaaff2283f.png&#34; alt=&#34;defer2&#34;&gt; BTW, I would like to visit the museum one day...&lt;/p&gt; &#xA;&lt;h1&gt;&lt;span&gt;📖&lt;/span&gt; Citation&lt;/h1&gt; &#xA;&lt;p&gt;If you use (part of) my code or find my work helpful, please consider citing&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{queianchen_nerf,&#xA;  author={Quei-An, Chen},&#xA;  title={Nerf_pl: a pytorch-lightning implementation of NeRF},&#xA;  url={https://github.com/kwea123/nerf_pl/},&#xA;  year={2020},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>cocodataset/cocoapi</title>
    <updated>2022-10-30T01:48:02Z</updated>
    <id>tag:github.com,2022-10-30:/cocodataset/cocoapi</id>
    <link href="https://github.com/cocodataset/cocoapi" rel="alternate"></link>
    <summary type="html">&lt;p&gt;COCO API - Dataset @ http://cocodataset.org/&lt;/p&gt;&lt;hr&gt;&lt;p&gt;COCO API - &lt;a href=&#34;http://cocodataset.org/&#34;&gt;http://cocodataset.org/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;COCO is a large image dataset designed for object detection, segmentation, person keypoints detection, stuff segmentation, and caption generation. This package provides Matlab, Python, and Lua APIs that assists in loading, parsing, and visualizing the annotations in COCO. Please visit &lt;a href=&#34;http://cocodataset.org/&#34;&gt;http://cocodataset.org/&lt;/a&gt; for more information on COCO, including for the data, paper, and tutorials. The exact format of the annotations is also described on the COCO website. The Matlab and Python APIs are complete, the Lua API provides only basic functionality.&lt;/p&gt; &#xA;&lt;p&gt;In addition to this API, please download both the COCO images and annotations in order to run the demos and use the API. Both are available on the project website. -Please download, unzip, and place the images in: coco/images/ -Please download and place the annotations in: coco/annotations/ For substantially more details on the API please see &lt;a href=&#34;http://cocodataset.org/#download&#34;&gt;http://cocodataset.org/#download&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;After downloading the images and annotations, run the Matlab, Python, or Lua demos for example usage.&lt;/p&gt; &#xA;&lt;p&gt;To install: -For Matlab, add coco/MatlabApi to the Matlab path (OSX/Linux binaries provided) -For Python, run &#34;make&#34; under coco/PythonAPI -For Lua, run “luarocks make LuaAPI/rocks/coco-scm-1.rockspec” under coco/&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>pharmapsychotic/clip-interrogator</title>
    <updated>2022-10-30T01:48:02Z</updated>
    <id>tag:github.com,2022-10-30:/pharmapsychotic/clip-interrogator</id>
    <link href="https://github.com/pharmapsychotic/clip-interrogator" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;clip-interrogator&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/pharmapsychotic/clip-interrogator/blob/main/clip_interrogator.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; Version 2&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/pharmapsychotic/clip-interrogator/blob/v1/clip_interrogator.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; Version 1&lt;/p&gt; &#xA;&lt;p&gt;The CLIP Interrogator uses the OpenAI CLIP models to test a given image against a variety of artists, mediums, and styles to study how the different models see the content of the image. It also combines the results with BLIP caption to suggest a text prompt to create more images similar to what was given.&lt;/p&gt;</summary>
  </entry>
</feed>