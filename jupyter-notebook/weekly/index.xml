<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-07-03T02:13:46Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>GokuMohandas/Made-With-ML</title>
    <updated>2022-07-03T02:13:46Z</updated>
    <id>tag:github.com,2022-07-03:/GokuMohandas/Made-With-ML</id>
    <link href="https://github.com/GokuMohandas/Made-With-ML" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Learn how to responsibly deliver value with ML.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;&lt;img width=&#34;30&#34; src=&#34;https://madewithml.com/static/images/rounded_logo.png&#34;&gt;&amp;nbsp;&lt;a href=&#34;https://madewithml.com/&#34;&gt;Made With ML&lt;/a&gt;&lt;/h1&gt; Applied ML ¬∑ MLOps ¬∑ Production &#xA; &lt;br&gt; Join 30K+ developers in learning how to responsibly &#xA; &lt;a href=&#34;https://madewithml.com/about/&#34;&gt;deliver value&lt;/a&gt; with ML. &#xA; &lt;br&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a target=&#34;_blank&#34; href=&#34;https://newsletter.madewithml.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Subscribe-30K-brightgreen&#34;&gt;&lt;/a&gt;&amp;nbsp; &#xA; &lt;a target=&#34;_blank&#34; href=&#34;https://github.com/GokuMohandas/Made-With-ML&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/GokuMohandas/Made-With-ML.svg?style=social&amp;amp;label=Star&#34;&gt;&lt;/a&gt;&amp;nbsp; &#xA; &lt;a target=&#34;_blank&#34; href=&#34;https://www.linkedin.com/in/goku&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/style--5eba00.svg?label=LinkedIn&amp;amp;logo=linkedin&amp;amp;style=social&#34;&gt;&lt;/a&gt;&amp;nbsp; &#xA; &lt;a target=&#34;_blank&#34; href=&#34;https://twitter.com/GokuMohandas&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/GokuMohandas.svg?label=Follow&amp;amp;style=social&#34;&gt;&lt;/a&gt; &#xA; &lt;br&gt; üî•&amp;nbsp; Among the &#xA; &lt;a href=&#34;https://github.com/GokuMohandas/Made-With-ML&#34; target=&#34;_blank&#34;&gt;top MLOps&lt;/a&gt; repositories on GitHub &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Foundations&lt;/h2&gt; &#xA;&lt;p&gt;Learn the foundations of ML through intuitive explanations, clean code and visuals.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Lessons: &lt;a href=&#34;https://madewithml.com/#foundations&#34;&gt;https://madewithml.com/#foundations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Code: &lt;a href=&#34;https://github.com/GokuMohandas/Made-With-ML/tree/master/notebooks&#34;&gt;GokuMohandas/Made-With-ML/tree/master/notebooks&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table class=&#34;table table-striped table-bordered table-vcenter&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;b&gt;üõ†&amp;nbsp; Toolkit&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;b&gt;üî•&amp;nbsp; Machine Learning&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;b&gt;ü§ñ&amp;nbsp; Deep Learning&lt;/b&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://madewithml.com/courses/foundations/notebooks/&#34;&gt;Notebooks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://madewithml.com/courses/foundations/linear-regression/&#34;&gt;Linear Regression&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://madewithml.com/courses/foundations/convolutional-neural-networks/&#34;&gt;CNNs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://madewithml.com/courses/foundations/python/&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://madewithml.com/courses/foundations/logistic-regression/&#34;&gt;Logistic Regression&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://madewithml.com/courses/foundations/embeddings/&#34;&gt;Embeddings&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://madewithml.com/courses/foundations/numpy/&#34;&gt;NumPy&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://madewithml.com/courses/foundations/neural-networks/&#34;&gt;Neural Network&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://madewithml.com/courses/foundations/recurrent-neural-networks/&#34;&gt;RNNs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://madewithml.com/courses/foundations/pandas/&#34;&gt;Pandas&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://madewithml.com/courses/foundations/data-quality/&#34;&gt;Data Quality&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://madewithml.com/courses/foundations/attention/&#34;&gt;Attention&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://madewithml.com/courses/foundations/pytorch/&#34;&gt;PyTorch&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://madewithml.com/courses/foundations/utilities/&#34;&gt;Utilities&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://madewithml.com/courses/foundations/transformers/&#34;&gt;Transformers&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;üìÜ&amp;nbsp; More content coming soon!&lt;br&gt; &lt;a href=&#34;https://newsletter.madewithml.com&#34; target=&#34;_blank&#34;&gt;Subscribe&lt;/a&gt; for our monthly updates on new content.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;MLOps course&lt;/h2&gt; &#xA;&lt;p&gt;Learn how to apply ML to build a production grade product to deliver value.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Lessons: &lt;a href=&#34;https://madewithml.com/#mlops&#34;&gt;https://madewithml.com/#mlops&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Code: &lt;a href=&#34;https://github.com/GokuMohandas/mlops-course&#34;&gt;GokuMohandas/mlops-course&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;üì¶&amp;nbsp; Purpose&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;üíª&amp;nbsp; Developing&lt;/strong&gt;&amp;nbsp;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;‚ôªÔ∏è&amp;nbsp; Reproducibility&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://madewithml.com/courses/mlops/purpose/&#34;&gt;Product&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://madewithml.com/courses/mlops/packaging/&#34;&gt;Packaging&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://madewithml.com/courses/mlops/git/&#34;&gt;Git&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://madewithml.com/courses/mlops/product/#system-design&#34;&gt;System design&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://madewithml.com/courses/mlops/organization/&#34;&gt;Organization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://madewithml.com/courses/mlops/pre-commit/&#34;&gt;Pre-commit&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://madewithml.com/courses/mlops/purpose#project-management&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://madewithml.com/courses/mlops/logging/&#34;&gt;Logging&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://madewithml.com/courses/mlops/versioning/&#34;&gt;Versioning&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;üî¢&amp;nbsp; Data&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://madewithml.com/courses/mlops/documentation/&#34;&gt;Documentation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://madewithml.com/courses/mlops/docker/&#34;&gt;Docker&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr style=&#34;height: 23.5px;&#34;&gt; &#xA;   &lt;td style=&#34;height: 23.5px;&#34;&gt;&lt;a href=&#34;https://madewithml.com/courses/mlops/exploratory-data-analysis/&#34;&gt;Exploration&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td style=&#34;height: 23.5px;&#34;&gt;&lt;a href=&#34;https://madewithml.com/courses/mlops/styling/&#34;&gt;Styling&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td style=&#34;height: 23.5px;&#34;&gt;&lt;strong&gt;üöÄ&amp;nbsp; Production&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://madewithml.com/courses/mlops/labeling/&#34;&gt;Labeling&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://madewithml.com/courses/mlops/makefile/&#34;&gt;Makefile&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://madewithml.com/courses/mlops/dashboard/&#34;&gt;Dashboard&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://madewithml.com/courses/mlops/preprocessing/&#34;&gt;Preprocessing&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;üì¶&amp;nbsp; Serving&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://madewithml.com/courses/mlops/cicd/&#34;&gt;CI/CD workflows&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://madewithml.com/courses/mlops/splitting/&#34;&gt;Splitting&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://madewithml.com/courses/mlops/cli/&#34;&gt;Command-line&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://madewithml.com/courses/mlops/infrastructure/&#34;&gt;Infrastructure&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://madewithml.com/courses/mlops/augmentation/&#34;&gt;Augmentation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://madewithml.com/courses/mlops/api/&#34;&gt;RESTful API&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://madewithml.com/courses/mlops/monitoring/&#34;&gt;Monitoring&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;üìà&amp;nbsp; Modeling&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;‚úÖ&amp;nbsp; Testing&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://madewithml.com/courses/mlops/feature-store/&#34;&gt;Feature store&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&amp;nbsp;&lt;a href=&#34;https://madewithml.com/courses/mlops/baselines/&#34;&gt;Baselines&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://madewithml.com/courses/mlops/testing/&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a&gt;Data stack&lt;/a&gt;&amp;nbsp;&lt;small&gt;(Aug 2022)&lt;/small&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://madewithml.com/courses/mlops/evaluation/&#34;&gt;Evaluation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://madewithml.com/courses/mlops/testing/#data&#34;&gt;Data&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a&gt;Orchestration&lt;/a&gt;&amp;nbsp;&lt;small&gt;(Aug 2022)&lt;/small&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://madewithml.com/courses/mlops/experiment-tracking/&#34;&gt;Experiment tracking&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://madewithml.com/courses/mlops/testing/#models&#34;&gt;Models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&amp;nbsp;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://madewithml.com/courses/mlops/optimization/&#34;&gt;Optimization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&amp;nbsp;&lt;/td&gt; &#xA;   &lt;td&gt;&amp;nbsp;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;üìÜ&amp;nbsp; More content coming soon!&lt;br&gt; &lt;a href=&#34;https://newsletter.madewithml.com&#34; target=&#34;_blank&#34;&gt;Subscribe&lt;/a&gt; for our monthly updates on new content.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;h3&gt;Who is this content for?&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;Software engineers&lt;/code&gt; looking to learn ML and become even better software engineers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Data scientists&lt;/code&gt; who want to learn how to responsibly deliver value with ML.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;College graduates&lt;/code&gt; looking to learn the practical skills they&#39;ll need for the industry.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Product Managers&lt;/code&gt; who want to develop a technical foundation for ML applications.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;What is the structure?&lt;/h3&gt; &#xA;&lt;p&gt;Lessons will be released weekly and each one will include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;intuition&lt;/code&gt;: high level overview of the concepts that will be covered and how it all fits together.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;code&lt;/code&gt;: simple code examples to illustrate the concept.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;application&lt;/code&gt;: applying the concept to our specific task.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;extensions&lt;/code&gt;: brief look at other tools and techniques that will be useful for difference situations.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;What makes this content unique?&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;hands-on&lt;/code&gt;: If you search production ML or MLOps online, you&#39;ll find great blog posts and tweets. But in order to really understand these concepts, you need to implement them. Unfortunately, you don‚Äôt see a lot of the inner workings of running production ML because of scale, proprietary content &amp;amp; expensive tools. However, Made With ML is free, open and live which makes it a perfect learning opportunity for the community.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;intuition-first&lt;/code&gt;: We will never jump straight to code. In every lesson, we will develop intuition for the concepts and think about it from a product perspective.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;software engineering&lt;/code&gt;: This course isn&#39;t just about ML. In fact, it&#39;s mostly about clean software engineering! We&#39;ll cover important concepts like versioning, testing, logging, etc. that really makes something production-grade product.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;focused yet holistic&lt;/code&gt;: For every concept, we&#39;ll not only cover what&#39;s most important for our specific task (this is the case study aspect) but we&#39;ll also cover related methods (this is the guide aspect) which may prove to be useful in other situations.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Who is the author?&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;I&#39;ve deployed large scale ML systems at Apple as well as smaller systems with constraints at startups and want to share the common principles I&#39;ve learned.&lt;/li&gt; &#xA; &lt;li&gt;Connect with me on &lt;a href=&#34;https://twitter.com/GokuMohandas&#34; target=&#34;_blank&#34;&gt;&lt;i class=&#34;fab fa-twitter ai-color-info mr-1&#34;&gt;&lt;/i&gt;Twitter&lt;/a&gt; and &lt;a href=&#34;https://www.linkedin.com/in/goku&#34; target=&#34;_blank&#34;&gt;&lt;i class=&#34;fab fa-linkedin ai-color-primary mr-1&#34;&gt;&lt;/i&gt;LinkedIn&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Why is this free?&lt;/h3&gt; &#xA;&lt;p&gt;While this content is for everyone, it&#39;s especially targeted towards people who don&#39;t have as much opportunity to learn. I believe that creativity and intelligence are randomly distributed while opportunities are siloed. I want to enable more people to create and contribute to innovation.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;!-- Citation --&gt; To cite this content, please use: &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{madewithml,&#xA;    author       = {Goku Mohandas},&#xA;    title        = {Made With ML},&#xA;    howpublished = {\url{https://madewithml.com/}},&#xA;    year         = {2021}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>NVIDIA-AI-IOT/deepstream_python_apps</title>
    <updated>2022-07-03T02:13:46Z</updated>
    <id>tag:github.com,2022-07-03:/NVIDIA-AI-IOT/deepstream_python_apps</id>
    <link href="https://github.com/NVIDIA-AI-IOT/deepstream_python_apps" rel="alternate"></link>
    <summary type="html">&lt;p&gt;DeepStream SDK Python bindings and sample applications&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DeepStream Python Apps&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains Python bindings and sample applications for the &lt;a href=&#34;https://developer.nvidia.com/deepstream-sdk&#34;&gt;DeepStream SDK&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;SDK version supported: 6.1&lt;/p&gt; &#xA;&lt;p&gt;&lt;b&gt;The bindings sources along with build instructions are now available under &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/bindings&#34;&gt;bindings&lt;/a&gt;! &lt;/b&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;b&gt;This release comes with Operating System upgrades (from Ubuntu 18.04 to Ubuntu 20.04) for DeepStreamSDK 6.1 support. This translates to upgrade in Python version to 3.8 and &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/3rdparty/gst-python/&#34;&gt;gst-python&lt;/a&gt; version has also been upgraded to 1.16.2 !&lt;/b&gt;&lt;/p&gt; &#xA;&lt;p&gt;Download the latest release package complete with bindings and sample applications from the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/releases&#34;&gt;release section&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Please report any issues or bugs on the &lt;a href=&#34;https://devtalk.nvidia.com/default/board/209&#34;&gt;DeepStream SDK Forums&lt;/a&gt;. This enables the DeepStream community to find help at a central location.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/#deepstream-python-apps&#34;&gt;DeepStream Python Apps&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/#python-bindings&#34;&gt;Python Bindings&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/#sample-applications&#34;&gt;Sample Applications&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a name=&#34;metadata_bindings&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Python Bindings&lt;/h2&gt; &#xA;&lt;p&gt;DeepStream pipelines can be constructed using Gst Python, the GStreamer framework&#39;s Python bindings. For accessing DeepStream MetaData, Python &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/bindings&#34;&gt;bindings&lt;/a&gt; are provided as part of this repository. This module is generated using &lt;a href=&#34;https://github.com/pybind/pybind11&#34;&gt;Pybind11&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/.python-app-pipeline.png&#34; alt=&#34;bindings pipeline&#34; height=&#34;600px&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;These bindings support a Python interface to the MetaData structures and functions. Usage of this interface is documented in the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/HOWTO.md&#34;&gt;HOW-TO Guide&lt;/a&gt; and demonstrated in the sample applications.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;sample_applications&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Sample Applications&lt;/h2&gt; &#xA;&lt;p&gt;Sample applications provided here demonstrate how to work with DeepStream pipelines using Python.&lt;br&gt; The sample applications require &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/#metadata_bindings&#34;&gt;MetaData Bindings&lt;/a&gt; to work.&lt;/p&gt; &#xA;&lt;p&gt;To run the sample applications or write your own, please consult the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/HOWTO.md&#34;&gt;HOW-TO Guide&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/.test3-app.png&#34; alt=&#34;deepstream python app screenshot&#34; height=&#34;400px&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;We currently provide the following sample applications:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps/deepstream-test1&#34;&gt;deepstream-test1&lt;/a&gt; -- 4-class object detection pipeline&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps/deepstream-test2&#34;&gt;deepstream-test2&lt;/a&gt; -- 4-class object detection, tracking and attribute classification pipeline&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;UPDATE&lt;/b&gt; &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps/deepstream-test3&#34;&gt;deepstream-test3&lt;/a&gt; -- multi-stream pipeline performing 4-class object detection - now also supports triton inference server, no-display mode, file-loop and silent mode&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps/deepstream-test4&#34;&gt;deepstream-test4&lt;/a&gt; -- msgbroker for sending analytics results to the cloud&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps/deepstream-imagedata-multistream&#34;&gt;deepstream-imagedata-multistream&lt;/a&gt; -- multi-stream pipeline with access to image buffers&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps/deepstream-ssd-parser&#34;&gt;deepstream-ssd-parser&lt;/a&gt; -- SSD model inference via Triton server with output parsing in Python&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps/deepstream-test1-usbcam&#34;&gt;deepstream-test1-usbcam&lt;/a&gt; -- deepstream-test1 pipelien with USB camera input&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps/deepstream-test1-rtsp-out&#34;&gt;deepstream-test1-rtsp-out&lt;/a&gt; -- deepstream-test1 pipeline with RTSP output&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps/deepstream-opticalflow&#34;&gt;deepstream-opticalflow&lt;/a&gt; -- optical flow and visualization pipeline with flow vectors returned in NumPy array&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps/deepstream-segmentation&#34;&gt;deepstream-segmentation&lt;/a&gt; -- segmentation and visualization pipeline with segmentation mask returned in NumPy array&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps/deepstream-nvdsanalytics&#34;&gt;deepstream-nvdsanalytics&lt;/a&gt; -- multistream pipeline with analytics plugin&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps/runtime_source_add_delete&#34;&gt;runtime_source_add_delete&lt;/a&gt; -- add/delete source streams at runtime&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps/deepstream-imagedata-multistream-redaction&#34;&gt;deepstream-imagedata-multistream-redaction&lt;/a&gt; -- multi-stream pipeline with face detection and redaction&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps/deepstream-rtsp-in-rtsp-out&#34;&gt;deepstream-rtsp-in-rtsp-out&lt;/a&gt; -- multi-stream pipeline with RTSP input/output&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;NEW&lt;/b&gt; &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps/deepstream-preprocess-test&#34;&gt;deepstream-preprocess-test&lt;/a&gt; -- multi-stream pipeline using nvdspreprocess plugin with custom ROIs&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Detailed application information is provided in each application&#39;s subdirectory under &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps&#34;&gt;apps&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>bmild/nerf</title>
    <updated>2022-07-03T02:13:46Z</updated>
    <id>tag:github.com,2022-07-03:/bmild/nerf</id>
    <link href="https://github.com/bmild/nerf" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code release for NeRF (Neural Radiance Fields)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;NeRF: Neural Radiance Fields&lt;/h1&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;http://tancik.com/nerf&#34;&gt;Project Page&lt;/a&gt; | &lt;a href=&#34;https://youtu.be/JuH79E8rdKc&#34;&gt;Video&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2003.08934&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1&#34;&gt;Data&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/bmild/nerf/blob/master/tiny_nerf.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open Tiny-NeRF in Colab&#34;&gt;&lt;/a&gt;&lt;br&gt; Tensorflow implementation of optimizing a neural representation for a single scene and rendering new views.&lt;br&gt;&lt;br&gt; &lt;a href=&#34;http://tancik.com/nerf&#34;&gt;NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://people.eecs.berkeley.edu/~bmild/&#34;&gt;Ben Mildenhall&lt;/a&gt;*&lt;sup&gt;1&lt;/sup&gt;, &lt;a href=&#34;https://people.eecs.berkeley.edu/~pratul/&#34;&gt;Pratul P. Srinivasan&lt;/a&gt;*&lt;sup&gt;1&lt;/sup&gt;, &lt;a href=&#34;http://tancik.com/&#34;&gt;Matthew Tancik&lt;/a&gt;*&lt;sup&gt;1&lt;/sup&gt;, &lt;a href=&#34;http://jonbarron.info/&#34;&gt;Jonathan T. Barron&lt;/a&gt;&lt;sup&gt;2&lt;/sup&gt;, &lt;a href=&#34;http://cseweb.ucsd.edu/~ravir/&#34;&gt;Ravi Ramamoorthi&lt;/a&gt;&lt;sup&gt;3&lt;/sup&gt;, &lt;a href=&#34;https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html&#34;&gt;Ren Ng&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt; &lt;br&gt; &lt;sup&gt;1&lt;/sup&gt;UC Berkeley, &lt;sup&gt;2&lt;/sup&gt;Google Research, &lt;sup&gt;3&lt;/sup&gt;UC San Diego&lt;br&gt; *denotes equal contribution&lt;br&gt; in ECCV 2020 (Oral Presentation, Best Paper Honorable Mention)&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/bmild/nerf/master/imgs/pipeline.jpg&#34;&gt; &#xA;&lt;h2&gt;TL;DR quickstart&lt;/h2&gt; &#xA;&lt;p&gt;To setup a conda environment, download example training data, begin the training process, and launch Tensorboard:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env create -f environment.yml&#xA;conda activate nerf&#xA;bash download_example_data.sh&#xA;python run_nerf.py --config config_fern.txt&#xA;tensorboard --logdir=logs/summaries --port=6006&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If everything works without errors, you can now go to &lt;code&gt;localhost:6006&lt;/code&gt; in your browser and watch the &#34;Fern&#34; scene train.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;Python 3 dependencies:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Tensorflow 1.15&lt;/li&gt; &#xA; &lt;li&gt;matplotlib&lt;/li&gt; &#xA; &lt;li&gt;numpy&lt;/li&gt; &#xA; &lt;li&gt;imageio&lt;/li&gt; &#xA; &lt;li&gt;configargparse&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The LLFF data loader requires ImageMagick.&lt;/p&gt; &#xA;&lt;p&gt;We provide a conda environment setup file including all of the above dependencies. Create the conda environment &lt;code&gt;nerf&lt;/code&gt; by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env create -f environment.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You will also need the &lt;a href=&#34;http://github.com/fyusion/llff&#34;&gt;LLFF code&lt;/a&gt; (and COLMAP) set up to compute poses if you want to run on your own real data.&lt;/p&gt; &#xA;&lt;h2&gt;What is a NeRF?&lt;/h2&gt; &#xA;&lt;p&gt;A neural radiance field is a simple fully connected network (weights are ~5MB) trained to reproduce input views of a single scene using a rendering loss. The network directly maps from spatial location and viewing direction (5D input) to color and opacity (4D output), acting as the &#34;volume&#34; so we can use volume rendering to differentiably render new views.&lt;/p&gt; &#xA;&lt;p&gt;Optimizing a NeRF takes between a few hours and a day or two (depending on resolution) and only requires a single GPU. Rendering an image from an optimized NeRF takes somewhere between less than a second and ~30 seconds, again depending on resolution.&lt;/p&gt; &#xA;&lt;h2&gt;Running code&lt;/h2&gt; &#xA;&lt;p&gt;Here we show how to run our code on two example scenes. You can download the rest of the synthetic and real data used in the paper &lt;a href=&#34;https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Optimizing a NeRF&lt;/h3&gt; &#xA;&lt;p&gt;Run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash download_example_data.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to get the our synthetic Lego dataset and the LLFF Fern dataset.&lt;/p&gt; &#xA;&lt;p&gt;To optimize a low-res Fern NeRF:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run_nerf.py --config config_fern.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After 200k iterations (about 15 hours), you should get a video like this at &lt;code&gt;logs/fern_test/fern_test_spiral_200000_rgb.mp4&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://people.eecs.berkeley.edu/~bmild/nerf/fern_200k_256w.gif&#34; alt=&#34;ferngif&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;To optimize a low-res Lego NeRF:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run_nerf.py --config config_lego.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After 200k iterations, you should get a video like this:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://people.eecs.berkeley.edu/~bmild/nerf/lego_200k_256w.gif&#34; alt=&#34;legogif&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Rendering a NeRF&lt;/h3&gt; &#xA;&lt;p&gt;Run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash download_example_weights.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to get a pretrained high-res NeRF for the Fern dataset. Now you can use &lt;a href=&#34;https://github.com/bmild/nerf/raw/master/render_demo.ipynb&#34;&gt;&lt;code&gt;render_demo.ipynb&lt;/code&gt;&lt;/a&gt; to render new views.&lt;/p&gt; &#xA;&lt;h3&gt;Replicating the paper results&lt;/h3&gt; &#xA;&lt;p&gt;The example config files run at lower resolutions than the quantitative/qualitative results in the paper and video. To replicate the results from the paper, start with the config files in &lt;a href=&#34;https://github.com/bmild/nerf/tree/master/paper_configs&#34;&gt;&lt;code&gt;paper_configs/&lt;/code&gt;&lt;/a&gt;. Our synthetic Blender data and LLFF scenes are hosted &lt;a href=&#34;https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1&#34;&gt;here&lt;/a&gt; and the DeepVoxels data is hosted by Vincent Sitzmann &lt;a href=&#34;https://drive.google.com/open?id=1lUvJWB6oFtT8EQ_NzBrXnmi25BufxRfl&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Extracting geometry from a NeRF&lt;/h3&gt; &#xA;&lt;p&gt;Check out &lt;a href=&#34;https://github.com/bmild/nerf/raw/master/extract_mesh.ipynb&#34;&gt;&lt;code&gt;extract_mesh.ipynb&lt;/code&gt;&lt;/a&gt; for an example of running marching cubes to extract a triangle mesh from a trained NeRF network. You&#39;ll need the install the &lt;a href=&#34;https://github.com/pmneila/PyMCubes&#34;&gt;PyMCubes&lt;/a&gt; package for marching cubes plus the &lt;a href=&#34;https://github.com/mikedh/trimesh&#34;&gt;trimesh&lt;/a&gt; and &lt;a href=&#34;https://github.com/mmatl/pyrender&#34;&gt;pyrender&lt;/a&gt; packages if you want to render the mesh inside the notebook:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install trimesh pyrender PyMCubes&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Generating poses for your own scenes&lt;/h2&gt; &#xA;&lt;h3&gt;Don&#39;t have poses?&lt;/h3&gt; &#xA;&lt;p&gt;We recommend using the &lt;code&gt;imgs2poses.py&lt;/code&gt; script from the &lt;a href=&#34;https://github.com/fyusion/llff&#34;&gt;LLFF code&lt;/a&gt;. Then you can pass the base scene directory into our code using &lt;code&gt;--datadir &amp;lt;myscene&amp;gt;&lt;/code&gt; along with &lt;code&gt;-dataset_type llff&lt;/code&gt;. You can take a look at the &lt;code&gt;config_fern.txt&lt;/code&gt; config file for example settings to use for a forward facing scene. For a spherically captured 360 scene, we recomment adding the &lt;code&gt;--no_ndc --spherify --lindisp&lt;/code&gt; flags.&lt;/p&gt; &#xA;&lt;h3&gt;Already have poses!&lt;/h3&gt; &#xA;&lt;p&gt;In &lt;code&gt;run_nerf.py&lt;/code&gt; and all other code, we use the same pose coordinate system as in OpenGL: the local camera coordinate system of an image is defined in a way that the X axis points to the right, the Y axis upwards, and the Z axis backwards as seen from the image.&lt;/p&gt; &#xA;&lt;p&gt;Poses are stored as 3x4 numpy arrays that represent camera-to-world transformation matrices. The other data you will need is simple pinhole camera intrinsics (&lt;code&gt;hwf = [height, width, focal length]&lt;/code&gt;) and near/far scene bounds. Take a look at &lt;a href=&#34;https://github.com/bmild/nerf/raw/master/run_nerf.py#L406&#34;&gt;our data loading code&lt;/a&gt; to see more.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{mildenhall2020nerf,&#xA;  title={NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis},&#xA;  author={Ben Mildenhall and Pratul P. Srinivasan and Matthew Tancik and Jonathan T. Barron and Ravi Ramamoorthi and Ren Ng},&#xA;  year={2020},&#xA;  booktitle={ECCV},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>