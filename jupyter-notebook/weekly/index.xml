<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-07-24T02:13:01Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>alexeygrigorev/mlbookcamp-code</title>
    <updated>2022-07-24T02:13:01Z</updated>
    <id>tag:github.com,2022-07-24:/alexeygrigorev/mlbookcamp-code</id>
    <link href="https://github.com/alexeygrigorev/mlbookcamp-code" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The code from the Machine Learning Bookcamp book and a free course based on the book&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Machine Learning Bookcamp&lt;/h1&gt; &#xA;&lt;p&gt;The code from the Machine Learning Bookcamp book&lt;/p&gt; &#xA;&lt;p&gt;Useful links:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mlbookcamp.com&#34;&gt;https://mlbookcamp.com&lt;/a&gt;: supplimentary materials&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://datatalks.club&#34;&gt;https://datatalks.club&lt;/a&gt;: the place to talk about data (and the book: join the &lt;code&gt;#ml-bookcamp&lt;/code&gt; channel to ask questions about the book and report any problems)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Machine Learning Zoomcamp&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/course-zoomcamp&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/images/zoomcamp.jpg&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Machine Learning Zoomcamp is a course based on the book&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;It&#39;s online and free&lt;/li&gt; &#xA; &lt;li&gt;You can join at any moment&lt;/li&gt; &#xA; &lt;li&gt;More information in the &lt;a href=&#34;https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/course-zoomcamp&#34;&gt;course-zoomcamp&lt;/a&gt; folder&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Reading Plan&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/images/plan.png&#34;&gt; &#xA;&lt;h1&gt;Chapters&lt;/h1&gt; &#xA;&lt;h2&gt;Chapter 1: Introduction to Machine Learning&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Understanding machine learning and the problems it can solve&lt;/li&gt; &#xA; &lt;li&gt;CRISP-DM: Organizing a successful machine learning project&lt;/li&gt; &#xA; &lt;li&gt;Training and selecting machine learning models&lt;/li&gt; &#xA; &lt;li&gt;Performing model validation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;No code&lt;/p&gt; &#xA;&lt;h2&gt;Chapter 2: Machine Learning for Regression&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Creating a car-price prediction project with a linear regression model&lt;/li&gt; &#xA; &lt;li&gt;Doing an initial exploratory data analysis with Jupyter notebooks&lt;/li&gt; &#xA; &lt;li&gt;Setting up a validation framework&lt;/li&gt; &#xA; &lt;li&gt;Implementing the linear regression model from scratch&lt;/li&gt; &#xA; &lt;li&gt;Performing simple feature engineering for the model&lt;/li&gt; &#xA; &lt;li&gt;Keeping the model under control with regularization&lt;/li&gt; &#xA; &lt;li&gt;Using the model to predict car prices&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Code: &lt;a href=&#34;https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/chapter-02-car-price/02-carprice.ipynb&#34;&gt;chapter-02-car-price/02-carprice.ipynb&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Chapter 3: Machine Learning for Classification&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Predicting customers who will churn with logistic regression&lt;/li&gt; &#xA; &lt;li&gt;Doing exploratory data analysis for identifying important features&lt;/li&gt; &#xA; &lt;li&gt;Encoding categorical variables to use them in machine learning models&lt;/li&gt; &#xA; &lt;li&gt;Using logistic regression for classification&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Code: &lt;a href=&#34;https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/chapter-03-churn-prediction/03-churn.ipynb&#34;&gt;chapter-03-churn-prediction/03-churn.ipynb&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Chapter 4: Evaluation Metrics for Classification&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Accuracy as a way of evaluating binary classification models and its limitations&lt;/li&gt; &#xA; &lt;li&gt;Determining where our model makes mistakes using a confusion table&lt;/li&gt; &#xA; &lt;li&gt;Deriving other metrics like precision and recall from the confusion table&lt;/li&gt; &#xA; &lt;li&gt;Using ROC and AUC to further understand the performance of a binary classification model&lt;/li&gt; &#xA; &lt;li&gt;Cross-validating a model to make sure it behaves optimally&lt;/li&gt; &#xA; &lt;li&gt;Tuning the parameters of a model to achieve the best predictive performance&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Code: &lt;a href=&#34;https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/chapter-03-churn-prediction/04-metrics.ipynb&#34;&gt;chapter-03-churn-prediction/04-metrics.ipynb&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Chapter 5: Deploying Machine Learning Models&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Saving models with Pickle&lt;/li&gt; &#xA; &lt;li&gt;Serving models with Flask&lt;/li&gt; &#xA; &lt;li&gt;Managing dependencies with Pipenv&lt;/li&gt; &#xA; &lt;li&gt;Making the service self-contained with Docker&lt;/li&gt; &#xA; &lt;li&gt;Deploying it to the cloud using AWS Elastic Beanstalk&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Code: &lt;a href=&#34;https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/chapter-05-deployment&#34;&gt;chapter-05-deployment&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Chapter 6: Decision Trees and Ensemble Learning&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Predicting the risk of default with tree-based models&lt;/li&gt; &#xA; &lt;li&gt;Decision trees and the decision tree learning algorithm&lt;/li&gt; &#xA; &lt;li&gt;Random forest: putting multiple trees together into one model&lt;/li&gt; &#xA; &lt;li&gt;Gradient boosting as an alternative way of combining decision trees&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Code: &lt;a href=&#34;https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/chapter-06-trees/06-trees.ipynb&#34;&gt;chapter-06-trees/06-trees.ipynb&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Chapter 7: Neural Networks and Deep Learning&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Convolutional neural networks for image classification&lt;/li&gt; &#xA; &lt;li&gt;TensorFlow and Keras — frameworks for building neural networks&lt;/li&gt; &#xA; &lt;li&gt;Using pre-trained neural networks&lt;/li&gt; &#xA; &lt;li&gt;Internals of a convolutional neural network&lt;/li&gt; &#xA; &lt;li&gt;Training a model with transfer learning&lt;/li&gt; &#xA; &lt;li&gt;Data augmentations — the process of generating more training data&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Code: &lt;a href=&#34;https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/chapter-07-neural-nets/07-neural-nets-train.ipynb&#34;&gt;chapter-07-neural-nets/07-neural-nets-train.ipynb&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Chapter 8: Serverless Deep Learning&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Serving models with TensorFlow-Lite — a light-weight environment for applying TensorFlow models&lt;/li&gt; &#xA; &lt;li&gt;Deploying deep learning models with AWS Lambda&lt;/li&gt; &#xA; &lt;li&gt;Exposing the Lambda function as a web service via API Gateway&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Code: &lt;a href=&#34;https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/chapter-08-serverless&#34;&gt;chapter-08-serverless&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Chapter 9: Kubernetes and Kubeflow&lt;/h2&gt; &#xA;&lt;p&gt;Kubernetes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Understanding different methods of deploying and serving models in the cloud.&lt;/li&gt; &#xA; &lt;li&gt;Serving Keras and TensorFlow models with TensorFlow-Serving&lt;/li&gt; &#xA; &lt;li&gt;Deploying TensorFlow-Serving to Kubernetes&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Code: &lt;a href=&#34;https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/chapter-09-kubernetes&#34;&gt;chapter-09-kubernetes&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Kubeflow:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Using Kubeflow and KFServing for simplifying the deployment process&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Code: &lt;a href=&#34;https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/chapter-09-kubeflow&#34;&gt;chapter-09-kubeflow&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Articles from &lt;a href=&#34;https://mlbookcamp.com&#34;&gt;mlbookcamp.com&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mlbookcamp.com/article/eks&#34;&gt;Creating an EKS Cluster&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mlbookcamp.com/article/kfserving-eks-install&#34;&gt;Creating a KFServing Cluster on EKS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mlbookcamp.com/article/kfserving-transformers&#34;&gt;KFServing Transformers&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Appendices&lt;/h1&gt; &#xA;&lt;h2&gt;Appendix A: Setting up the Environment&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Installing Anaconda, a Python distribution that includes most of the scientific libraries we need&lt;/li&gt; &#xA; &lt;li&gt;Running a Jupyter Notebook service from a remote machine&lt;/li&gt; &#xA; &lt;li&gt;Installing and configuring the Kaggle command line interface tool for accessing datasets from Kaggle&lt;/li&gt; &#xA; &lt;li&gt;Creating an EC2 machine on AWS using the web interface and the command-line interface&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Code: no code&lt;/p&gt; &#xA;&lt;p&gt;Articles from &lt;a href=&#34;https://mlbookcamp.com&#34;&gt;mlbookcamp.com&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mlbookcamp.com/article/aws&#34;&gt;Creating an AWS account&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mlbookcamp.com/article/aws-ec2&#34;&gt;Renting an EC2 instance&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Appendix B: Introduction to Python&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Basic python syntax: variables and control-flow structures&lt;/li&gt; &#xA; &lt;li&gt;Collections: lists, tuples, sets, and dictionaries&lt;/li&gt; &#xA; &lt;li&gt;List comprehensions: a concise way of operating on collections&lt;/li&gt; &#xA; &lt;li&gt;Reusability: functions, classes and importing code&lt;/li&gt; &#xA; &lt;li&gt;Package management: using pip for installing libraries&lt;/li&gt; &#xA; &lt;li&gt;Running python scripts&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Code: &lt;a href=&#34;https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/appendix-b-python.ipynb&#34;&gt;appendix-b-python.ipynb&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Articles from &lt;a href=&#34;https://mlbookcamp.com&#34;&gt;mlbookcamp.com&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mlbookcamp.com/article/python&#34;&gt;Introduction to Python&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Appendix C: Introduction to NumPy and Linear Algebra&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;One-dimensional and two-dimensional NumPy arrays&lt;/li&gt; &#xA; &lt;li&gt;Generating NumPy arrays randomly&lt;/li&gt; &#xA; &lt;li&gt;Operations with NumPy arrays: element-wise operations, summarizing operations, sorting and filtering&lt;/li&gt; &#xA; &lt;li&gt;Multiplication in linear algebra: vector-vector, matrix-vector and matrix-matrix multiplications&lt;/li&gt; &#xA; &lt;li&gt;Finding the inverse of a matrix and solving the normal equation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Code: &lt;a href=&#34;https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/appendix-c-numpy.ipynb&#34;&gt;appendix-c-numpy.ipynb&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Articles from &lt;a href=&#34;https://mlbookcamp.com&#34;&gt;mlbookcamp.com&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mlbookcamp.com/article/numpy&#34;&gt;Introduction to NumPy&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Appendix C: Introduction to Pandas&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The main data structures in Pandas: DataFrame and Series&lt;/li&gt; &#xA; &lt;li&gt;Accessing rows and columns of a DataFrame&lt;/li&gt; &#xA; &lt;li&gt;Element-wise and summarizing operations&lt;/li&gt; &#xA; &lt;li&gt;Working with missing values&lt;/li&gt; &#xA; &lt;li&gt;Sorting and grouping&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Code: &lt;a href=&#34;https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/appendix-d-pandas.ipynb&#34;&gt;appendix-d-pandas.ipynb&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Appendix D: AWS SageMaker&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Increasing the GPU quota limits&lt;/li&gt; &#xA; &lt;li&gt;Renting a Jupyter notebook with GPU in AWS SageMaker&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>srush/GPU-Puzzles</title>
    <updated>2022-07-24T02:13:01Z</updated>
    <id>tag:github.com,2022-07-24:/srush/GPU-Puzzles</id>
    <link href="https://github.com/srush/GPU-Puzzles" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Solve puzzles. Learn CUDA.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GPU Puzzles&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;by &lt;a href=&#34;http://rush-nlp.com&#34;&gt;Sasha Rush&lt;/a&gt; - &lt;a href=&#34;https://twitter.com/srush_nlp&#34;&gt;srush_nlp&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/srush/GPU-Puzzles/raw/main/cuda.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;GPU architectures are critical to machine learning, and seem to be becoming even more important every day. However you can be an expert in machine learning without ever touching GPU code. It is a bit weird to be work always through abstraction.&lt;/p&gt; &#xA;&lt;p&gt;This notebook is an attempt to teach beginner GPU programming in a completely interactive fashion. Instead of providing text with concepts, it throws you right into coding and building GPU kernels. The exercises use NUMBA which directly maps Python code to CUDA kernels. It looks like Python but is basically identical to writing low-level CUDA code. In a few hours, I think you can go from basics to understanding the real algorithms that power 99% of deep learning today. If you do want to read the manual, it is here:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://numba.readthedocs.io/en/stable/cuda/index.html&#34;&gt;NUMBA CUDA Guide&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;I recommend doing these in Colab, as it is easy to get started. Be sure to make your own copy, turn on GPU mode in the settings (&lt;code&gt;Runtime / Change runtime type&lt;/code&gt;, then set &lt;code&gt;Hardware accelerator&lt;/code&gt; to &lt;code&gt;GPU&lt;/code&gt;), and then get to coding.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/srush/GPU-Puzzles/blob/main/GPU_puzzlers.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;(If you are into this style of puzzle, also check out my &lt;a href=&#34;https://github.com/srush/Tensor-Puzzles&#34;&gt;Tensor Puzzles&lt;/a&gt; for PyTorch.)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;!pip install -qqq git+https://github.com/danoneata/chalk@srush-patch-1&#xA;!wget -q https://github.com/srush/GPU-Puzzles/raw/main/robot.png https://github.com/srush/GPU-Puzzles/raw/main/lib.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numba&#xA;import numpy as np&#xA;import warnings&#xA;from lib import CudaProblem, Coord&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;warnings.filterwarnings(&#xA;    action=&#34;ignore&#34;, category=numba.NumbaPerformanceWarning, module=&#34;numba&#34;&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Puzzle 1: Map&lt;/h2&gt; &#xA;&lt;p&gt;Implement a &#34;kernel&#34; (GPU function) that adds 10 to each position of vector &lt;code&gt;a&lt;/code&gt; and stores it in vector &lt;code&gt;out&lt;/code&gt;. You have 1 thread per position.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt; This code looks like Python but it is really CUDA! You cannot use standard python tools like list comprehensions or ask for Numpy properties like shape or size (if you need the size, it is given as an argument). The puzzles only require doing simple operations, basically +, *, simple array indexing, for loops, and if statements. You are allowed to use local variables. If you get an error it is probably because you did something fancy :).&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Tip: Think of the function &lt;code&gt;call&lt;/code&gt; as being run 1 time for each thread. The only difference is that &lt;code&gt;cuda.threadIdx.x&lt;/code&gt; changes each time.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def map_spec(a):&#xA;    return a + 10&#xA;&#xA;&#xA;def map_test(cuda):&#xA;    def call(out, a) -&amp;gt; None:&#xA;        local_i = cuda.threadIdx.x&#xA;        # FILL ME IN (roughly 1 lines)&#xA;&#xA;    return call&#xA;&#xA;&#xA;SIZE = 4&#xA;out = np.zeros((SIZE,))&#xA;a = np.arange(SIZE)&#xA;problem = CudaProblem(&#xA;    &#34;Map&#34;, map_test, [a], out, threadsperblock=Coord(SIZE, 1), spec=map_spec&#xA;)&#xA;problem.show()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Map&#xA; &#xA;   Score (Max Per Thread):&#xA;   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |&#xA;   |             0 |             0 |             0 |             0 | &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/srush/GPU-Puzzles/main/GPU_puzzlers_files/GPU_puzzlers_14_1.svg?sanitize=true&#34; alt=&#34;svg&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;problem.check()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;Failed Tests.&#xA;Yours: [0. 0. 0. 0.]&#xA;Spec : [10 11 12 13]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Puzzle 2 - Zip&lt;/h2&gt; &#xA;&lt;p&gt;Implement a kernel that adds together each position of &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; and stores it in &lt;code&gt;out&lt;/code&gt;. You have 1 thread per position.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def zip_spec(a, b):&#xA;    return a + b&#xA;&#xA;&#xA;def zip_test(cuda):&#xA;    def call(out, a, b) -&amp;gt; None:&#xA;        local_i = cuda.threadIdx.x&#xA;        # FILL ME IN (roughly 1 lines)&#xA;&#xA;    return call&#xA;&#xA;&#xA;SIZE = 4&#xA;out = np.zeros((SIZE,))&#xA;a = np.arange(SIZE)&#xA;b = np.arange(SIZE)&#xA;problem = CudaProblem(&#xA;    &#34;Zip&#34;, zip_test, [a, b], out, threadsperblock=Coord(SIZE, 1), spec=zip_spec&#xA;)&#xA;problem.show()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Zip&#xA; &#xA;   Score (Max Per Thread):&#xA;   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |&#xA;   |             0 |             0 |             0 |             0 | &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/srush/GPU-Puzzles/main/GPU_puzzlers_files/GPU_puzzlers_17_1.svg?sanitize=true&#34; alt=&#34;svg&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;problem.check()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;Failed Tests.&#xA;Yours: [0. 0. 0. 0.]&#xA;Spec : [0 2 4 6]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Puzzle 3 - Guards&lt;/h2&gt; &#xA;&lt;p&gt;Implement a kernel that adds 10 to each position of &lt;code&gt;a&lt;/code&gt; and stores it in &lt;code&gt;out&lt;/code&gt;. You have more threads than positions.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def map_guard_test(cuda):&#xA;    def call(out, a, size) -&amp;gt; None:&#xA;        local_i = cuda.threadIdx.x&#xA;        # FILL ME IN (roughly 2 lines)&#xA;&#xA;    return call&#xA;&#xA;&#xA;SIZE = 4&#xA;out = np.zeros((SIZE,))&#xA;a = np.arange(SIZE)&#xA;problem = CudaProblem(&#xA;    &#34;Guard&#34;,&#xA;    map_guard_test,&#xA;    [a],&#xA;    out,&#xA;    [SIZE],&#xA;    threadsperblock=Coord(8, 1),&#xA;    spec=map_spec,&#xA;)&#xA;problem.show()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Guard&#xA; &#xA;   Score (Max Per Thread):&#xA;   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |&#xA;   |             0 |             0 |             0 |             0 | &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/srush/GPU-Puzzles/main/GPU_puzzlers_files/GPU_puzzlers_21_1.svg?sanitize=true&#34; alt=&#34;svg&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;problem.check()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;Failed Tests.&#xA;Yours: [0. 0. 0. 0.]&#xA;Spec : [10 11 12 13]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Puzzle 4 - Map 2D&lt;/h2&gt; &#xA;&lt;p&gt;Implement a kernel that adds 10 to each position of &lt;code&gt;a&lt;/code&gt; and stores it in &lt;code&gt;out&lt;/code&gt;. Input &lt;code&gt;a&lt;/code&gt; is 2D and square. You have more threads than positions.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def map_2D_test(cuda):&#xA;    def call(out, a, size) -&amp;gt; None:&#xA;        local_i = cuda.threadIdx.x&#xA;        local_j = cuda.threadIdx.y&#xA;        # FILL ME IN (roughly 2 lines)&#xA;&#xA;    return call&#xA;&#xA;&#xA;SIZE = 2&#xA;out = np.zeros((SIZE, SIZE))&#xA;a = np.arange(SIZE * SIZE).reshape((SIZE, SIZE))&#xA;problem = CudaProblem(&#xA;    &#34;Map 2D&#34;, map_2D_test, [a], out, [SIZE], threadsperblock=Coord(3, 3), spec=map_spec&#xA;)&#xA;problem.show()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Map 2D&#xA; &#xA;   Score (Max Per Thread):&#xA;   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |&#xA;   |             0 |             0 |             0 |             0 | &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/srush/GPU-Puzzles/main/GPU_puzzlers_files/GPU_puzzlers_24_1.svg?sanitize=true&#34; alt=&#34;svg&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;problem.check()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;Failed Tests.&#xA;Yours: [[0. 0.]&#xA; [0. 0.]]&#xA;Spec : [[10 11]&#xA; [12 13]]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Puzzle 5 - Broadcast&lt;/h2&gt; &#xA;&lt;p&gt;Implement a kernel that adds &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; and stores it in &lt;code&gt;out&lt;/code&gt;. Inputs &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; are vectors. You have more threads than positions.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def broadcast_test(cuda):&#xA;    def call(out, a, b, size) -&amp;gt; None:&#xA;        local_i = cuda.threadIdx.x&#xA;        local_j = cuda.threadIdx.y&#xA;        # FILL ME IN (roughly 2 lines)&#xA;&#xA;    return call&#xA;&#xA;&#xA;SIZE = 2&#xA;out = np.zeros((SIZE, SIZE))&#xA;a = np.arange(SIZE).reshape(SIZE, 1)&#xA;b = np.arange(SIZE).reshape(1, SIZE)&#xA;problem = CudaProblem(&#xA;    &#34;Broadcast&#34;,&#xA;    broadcast_test,&#xA;    [a, b],&#xA;    out,&#xA;    [SIZE],&#xA;    threadsperblock=Coord(3, 3),&#xA;    spec=zip_spec,&#xA;)&#xA;problem.show()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Broadcast&#xA; &#xA;   Score (Max Per Thread):&#xA;   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |&#xA;   |             0 |             0 |             0 |             0 | &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/srush/GPU-Puzzles/main/GPU_puzzlers_files/GPU_puzzlers_27_1.svg?sanitize=true&#34; alt=&#34;svg&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;problem.check()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;Failed Tests.&#xA;Yours: [[0. 0.]&#xA; [0. 0.]]&#xA;Spec : [[0 1]&#xA; [1 2]]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Puzzle 6 - Blocks&lt;/h2&gt; &#xA;&lt;p&gt;Implement a kernel that adds 10 to each position of &lt;code&gt;a&lt;/code&gt; and stores it in &lt;code&gt;out&lt;/code&gt;. You have fewer threads per block than the size of &lt;code&gt;a&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Tip: A block is a group of threads. The number of threads per block is limited, but we can have many different blocks. Variable &lt;code&gt;cuda.blockIdx&lt;/code&gt; tells us what block we are in.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def map_block_test(cuda):&#xA;    def call(out, a, size) -&amp;gt; None:&#xA;        i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x&#xA;        # FILL ME IN (roughly 2 lines)&#xA;&#xA;    return call&#xA;&#xA;&#xA;SIZE = 9&#xA;out = np.zeros((SIZE,))&#xA;a = np.arange(SIZE)&#xA;problem = CudaProblem(&#xA;    &#34;Blocks&#34;,&#xA;    map_block_test,&#xA;    [a],&#xA;    out,&#xA;    [SIZE],&#xA;    threadsperblock=Coord(4, 1),&#xA;    blockspergrid=Coord(3, 1),&#xA;    spec=map_spec,&#xA;)&#xA;problem.show()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Blocks&#xA; &#xA;   Score (Max Per Thread):&#xA;   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |&#xA;   |             0 |             0 |             0 |             0 | &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/srush/GPU-Puzzles/main/GPU_puzzlers_files/GPU_puzzlers_31_1.svg?sanitize=true&#34; alt=&#34;svg&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;problem.check()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;Failed Tests.&#xA;Yours: [0. 0. 0. 0. 0. 0. 0. 0. 0.]&#xA;Spec : [10 11 12 13 14 15 16 17 18]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Puzzle 7 - Blocks 2D&lt;/h2&gt; &#xA;&lt;p&gt;Implement the same kernel in 2D. You have fewer threads per block than the size of &lt;code&gt;a&lt;/code&gt; in both directions.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def map_block2D_test(cuda):&#xA;    def call(out, a, size) -&amp;gt; None:&#xA;        i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x&#xA;        # FILL ME IN (roughly 4 lines)&#xA;&#xA;    return call&#xA;&#xA;&#xA;SIZE = 5&#xA;out = np.zeros((SIZE, SIZE))&#xA;a = np.ones((SIZE, SIZE))&#xA;&#xA;problem = CudaProblem(&#xA;    &#34;Blocks 2D&#34;,&#xA;    map_block2D_test,&#xA;    [a],&#xA;    out,&#xA;    [SIZE],&#xA;    threadsperblock=Coord(3, 3),&#xA;    blockspergrid=Coord(2, 2),&#xA;    spec=map_spec,&#xA;)&#xA;problem.show()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Blocks 2D&#xA; &#xA;   Score (Max Per Thread):&#xA;   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |&#xA;   |             0 |             0 |             0 |             0 | &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/srush/GPU-Puzzles/main/GPU_puzzlers_files/GPU_puzzlers_34_1.svg?sanitize=true&#34; alt=&#34;svg&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;problem.check()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;Failed Tests.&#xA;Yours: [[0. 0. 0. 0. 0.]&#xA; [0. 0. 0. 0. 0.]&#xA; [0. 0. 0. 0. 0.]&#xA; [0. 0. 0. 0. 0.]&#xA; [0. 0. 0. 0. 0.]]&#xA;Spec : [[11. 11. 11. 11. 11.]&#xA; [11. 11. 11. 11. 11.]&#xA; [11. 11. 11. 11. 11.]&#xA; [11. 11. 11. 11. 11.]&#xA; [11. 11. 11. 11. 11.]]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Puzzle 8 - Shared&lt;/h2&gt; &#xA;&lt;p&gt;Implement a kernel that adds 10 to each position of &lt;code&gt;a&lt;/code&gt; and stores it in &lt;code&gt;out&lt;/code&gt;. You have fewer threads per block than the size of &lt;code&gt;a&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;: Each block can only have a &lt;em&gt;constant&lt;/em&gt; amount of shared memory that threads in that block can read and write to. This needs to be a literal python constant not a variable. After writing to shared memory you need to call &lt;code&gt;cuda.syncthreads&lt;/code&gt; to ensure that threads do not cross.&lt;/p&gt; &#xA;&lt;p&gt;(This example does not really need shared memory or syncthreads, but it is a demo.)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;TPB = 4&#xA;def shared_test(cuda):&#xA;    def call(out, a, size) -&amp;gt; None:&#xA;        shared = cuda.shared.array(TPB, numba.float32)&#xA;        i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x&#xA;        local_i = cuda.threadIdx.x&#xA;&#xA;        if i &amp;lt; size:&#xA;            shared[local_i] = a[i]&#xA;            cuda.syncthreads()&#xA;&#xA;        # FILL ME IN (roughly 2 lines)&#xA;&#xA;    return call&#xA;&#xA;&#xA;SIZE = 8&#xA;out = np.zeros(SIZE)&#xA;a = np.ones(SIZE)&#xA;problem = CudaProblem(&#xA;    &#34;Shared&#34;,&#xA;    shared_test,&#xA;    [a],&#xA;    out,&#xA;    [SIZE],&#xA;    threadsperblock=Coord(TPB, 1),&#xA;    blockspergrid=Coord(2, 1),&#xA;    spec=map_spec,&#xA;)&#xA;problem.show()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Shared&#xA; &#xA;   Score (Max Per Thread):&#xA;   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |&#xA;   |             1 |             0 |             0 |             1 | &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/srush/GPU-Puzzles/main/GPU_puzzlers_files/GPU_puzzlers_39_1.svg?sanitize=true&#34; alt=&#34;svg&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;problem.check()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;Failed Tests.&#xA;Yours: [0. 0. 0. 0. 0. 0. 0. 0.]&#xA;Spec : [11. 11. 11. 11. 11. 11. 11. 11.]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Puzzle 9 - Pooling&lt;/h2&gt; &#xA;&lt;p&gt;Implement a kernel that sums together the last 3 position of &lt;code&gt;a&lt;/code&gt; and stores it in &lt;code&gt;out&lt;/code&gt;. You have 1 thread per position. You only need 1 global read and 1 global write per thread.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Tip: Remember to be careful about syncing.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def pool_spec(a):&#xA;    out = np.zeros(*a.shape)&#xA;    for i in range(a.shape[0]):&#xA;        out[i] = a[max(i - 2, 0) : i + 1].sum()&#xA;    return out&#xA;&#xA;&#xA;TPB = 8&#xA;def pool_test(cuda):&#xA;    def call(out, a, size) -&amp;gt; None:&#xA;        shared = cuda.shared.array(TPB, numba.float32)&#xA;        i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x&#xA;        local_i = cuda.threadIdx.x&#xA;        # FILL ME IN (roughly 8 lines)&#xA;&#xA;    return call&#xA;&#xA;&#xA;SIZE = 8&#xA;out = np.zeros(SIZE)&#xA;a = np.arange(SIZE)&#xA;problem = CudaProblem(&#xA;    &#34;Pooling&#34;,&#xA;    pool_test,&#xA;    [a],&#xA;    out,&#xA;    [SIZE],&#xA;    threadsperblock=Coord(TPB, 1),&#xA;    blockspergrid=Coord(1, 1),&#xA;    spec=pool_spec,&#xA;)&#xA;problem.show()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Pooling&#xA; &#xA;   Score (Max Per Thread):&#xA;   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |&#xA;   |             0 |             0 |             0 |             0 | &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/srush/GPU-Puzzles/main/GPU_puzzlers_files/GPU_puzzlers_43_1.svg?sanitize=true&#34; alt=&#34;svg&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;problem.check()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;Failed Tests.&#xA;Yours: [0. 0. 0. 0. 0. 0. 0. 0.]&#xA;Spec : [ 0.  1.  3.  6.  9. 12. 15. 18.]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Puzzle 10 - Dot Product&lt;/h2&gt; &#xA;&lt;p&gt;Implement a kernel that computes the dot-product of &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; and stores it in &lt;code&gt;out&lt;/code&gt;. You have 1 thread per position. You only need 2 global reads and 1 global write per thread.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Note: For this problem you don&#39;t need to worry about number of shared reads. We will handle that challenge later.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def dot_spec(a, b):&#xA;    return a @ b&#xA;&#xA;TPB = 8&#xA;def dot_test(cuda):&#xA;    def call(out, a, b, size) -&amp;gt; None:&#xA;        shared = cuda.shared.array(TPB, numba.float32)&#xA;&#xA;        i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x&#xA;        local_i = cuda.threadIdx.x&#xA;        # FILL ME IN (roughly 9 lines)&#xA;    return call&#xA;&#xA;&#xA;SIZE = 8&#xA;out = np.zeros(1)&#xA;a = np.arange(SIZE)&#xA;b = np.arange(SIZE)&#xA;problem = CudaProblem(&#xA;    &#34;Dot&#34;,&#xA;    dot_test,&#xA;    [a, b],&#xA;    out,&#xA;    [SIZE],&#xA;    threadsperblock=Coord(SIZE, 1),&#xA;    blockspergrid=Coord(1, 1),&#xA;    spec=dot_spec,&#xA;)&#xA;problem.show()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Dot&#xA; &#xA;   Score (Max Per Thread):&#xA;   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |&#xA;   |             0 |             0 |             0 |             0 | &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/srush/GPU-Puzzles/main/GPU_puzzlers_files/GPU_puzzlers_47_1.svg?sanitize=true&#34; alt=&#34;svg&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;problem.check()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;Failed Tests.&#xA;Yours: [0.]&#xA;Spec : 140&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Puzzle 11 - 1D Convolution&lt;/h2&gt; &#xA;&lt;p&gt;Implement a kernel that computes a 1D convolution between &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; and stores it in &lt;code&gt;out&lt;/code&gt;. You need to handle the general case. You only need 2 global reads and 1 global write per thread.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def conv_spec(a, b):&#xA;    out = np.zeros(*a.shape)&#xA;    len = b.shape[0]&#xA;    for i in range(a.shape[0]):&#xA;        out[i] = sum([a[i + j] * b[j] for j in range(len) if i + j &amp;lt; a.shape[0]])&#xA;    return out&#xA;&#xA;&#xA;MAX_CONV = 4&#xA;TPB = 8&#xA;TPB_MAX_CONV = TPB + MAX_CONV&#xA;def conv_test(cuda):&#xA;    def call(out, a, b, a_size, b_size) -&amp;gt; None:&#xA;        i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x&#xA;        local_i = cuda.threadIdx.x&#xA;&#xA;        # FILL ME IN (roughly 17 lines)&#xA;&#xA;    return call&#xA;&#xA;&#xA;# Test 1&#xA;&#xA;SIZE = 6&#xA;CONV = 3&#xA;out = np.zeros(SIZE)&#xA;a = np.arange(SIZE)&#xA;b = np.arange(CONV)&#xA;problem = CudaProblem(&#xA;    &#34;1D Conv (Simple)&#34;,&#xA;    conv_test,&#xA;    [a, b],&#xA;    out,&#xA;    [SIZE, CONV],&#xA;    Coord(1, 1),&#xA;    Coord(TPB, 1),&#xA;    spec=conv_spec,&#xA;)&#xA;problem.show()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;# 1D Conv (Simple)&#xA; &#xA;   Score (Max Per Thread):&#xA;   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |&#xA;   |             0 |             0 |             0 |             0 | &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/srush/GPU-Puzzles/main/GPU_puzzlers_files/GPU_puzzlers_50_1.svg?sanitize=true&#34; alt=&#34;svg&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;problem.check()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;Failed Tests.&#xA;Yours: [0. 0. 0. 0. 0. 0.]&#xA;Spec : [ 5.  8. 11. 14.  5.  0.]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Test 2&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;out = np.zeros(15)&#xA;a = np.arange(15)&#xA;b = np.arange(4)&#xA;problem = CudaProblem(&#xA;    &#34;1D Conv (Full)&#34;,&#xA;    conv_test,&#xA;    [a, b],&#xA;    out,&#xA;    [15, 4],&#xA;    Coord(2, 1),&#xA;    Coord(TPB, 1),&#xA;    spec=conv_spec,&#xA;)&#xA;problem.show()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;# 1D Conv (Full)&#xA; &#xA;   Score (Max Per Thread):&#xA;   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |&#xA;   |             0 |             0 |             0 |             0 | &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/srush/GPU-Puzzles/main/GPU_puzzlers_files/GPU_puzzlers_53_1.svg?sanitize=true&#34; alt=&#34;svg&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;problem.check()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;Failed Tests.&#xA;Yours: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]&#xA;Spec : [14. 20. 26. 32. 38. 44. 50. 56. 62. 68. 74. 80. 41. 14.  0.]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Puzzle 12 - Prefix Sum&lt;/h2&gt; &#xA;&lt;p&gt;Implement a kernel that computes a sum over &lt;code&gt;a&lt;/code&gt; and stores it in &lt;code&gt;out&lt;/code&gt;. If the size of &lt;code&gt;a&lt;/code&gt; is greater than the block size, only store the sum of each block.&lt;/p&gt; &#xA;&lt;p&gt;We will do this using the &lt;a href=&#34;https://en.wikipedia.org/wiki/Prefix_sum&#34;&gt;parallel prefix sum&lt;/a&gt; algorithm in shared memory. That is, each step of the algorithm should sum together half the remaining numbers. Follow this diagram:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/35882/178757889-1c269623-93af-4a2e-a7e9-22cd55a42e38.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;TPB = 8&#xA;def sum_spec(a):&#xA;    out = np.zeros((a.shape[0] + TPB - 1) // TPB)&#xA;    for j, i in enumerate(range(0, a.shape[-1], TPB)):&#xA;        out[j] = a[i : i + TPB].sum()&#xA;    return out&#xA;&#xA;&#xA;def sum_test(cuda):&#xA;    def call(out, a, size: int) -&amp;gt; None:&#xA;        cache = cuda.shared.array(TPB, numba.float32)&#xA;        i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x&#xA;        local_i = cuda.threadIdx.x&#xA;        # FILL ME IN (roughly 12 lines)&#xA;&#xA;    return call&#xA;&#xA;&#xA;# Test 1&#xA;&#xA;SIZE = 8&#xA;out = np.zeros(1)&#xA;inp = np.arange(SIZE)&#xA;problem = CudaProblem(&#xA;    &#34;Sum (Simple)&#34;,&#xA;    sum_test,&#xA;    [inp],&#xA;    out,&#xA;    [SIZE],&#xA;    Coord(1, 1),&#xA;    Coord(TPB, 1),&#xA;    spec=sum_spec,&#xA;)&#xA;problem.show()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Sum (Simple)&#xA; &#xA;   Score (Max Per Thread):&#xA;   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |&#xA;   |             0 |             0 |             0 |             0 | &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/srush/GPU-Puzzles/main/GPU_puzzlers_files/GPU_puzzlers_58_1.svg?sanitize=true&#34; alt=&#34;svg&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;problem.check()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;Failed Tests.&#xA;Yours: [0.]&#xA;Spec : [28.]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Test 2&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;SIZE = 15&#xA;out = np.zeros(2)&#xA;inp = np.arange(SIZE)&#xA;problem = CudaProblem(&#xA;    &#34;Sum (Full)&#34;,&#xA;    sum_test,&#xA;    [inp],&#xA;    out,&#xA;    [SIZE],&#xA;    Coord(2, 1),&#xA;    Coord(TPB, 1),&#xA;    spec=sum_spec,&#xA;)&#xA;problem.show()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Sum (Full)&#xA; &#xA;   Score (Max Per Thread):&#xA;   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |&#xA;   |             0 |             0 |             0 |             0 | &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/srush/GPU-Puzzles/main/GPU_puzzlers_files/GPU_puzzlers_61_1.svg?sanitize=true&#34; alt=&#34;svg&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;problem.check()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;Failed Tests.&#xA;Yours: [0. 0.]&#xA;Spec : [28. 77.]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Puzzle 13 - Axis Sum&lt;/h2&gt; &#xA;&lt;p&gt;Implement a kernel that computes a sum over each row of &lt;code&gt;a&lt;/code&gt; and stores it in &lt;code&gt;out&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;TPB = 8&#xA;def sum_spec(a):&#xA;    out = np.zeros((a.shape[0], (a.shape[1] + TPB - 1) // TPB))&#xA;    for j, i in enumerate(range(0, a.shape[-1], TPB)):&#xA;        out[..., j] = a[..., i : i + TPB].sum(-1)&#xA;    return out&#xA;&#xA;&#xA;def axis_sum_test(cuda):&#xA;    def call(out, a, size: int) -&amp;gt; None:&#xA;        cache = cuda.shared.array(TPB, numba.float32)&#xA;        i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x&#xA;        local_i = cuda.threadIdx.x&#xA;        batch = cuda.blockIdx.y&#xA;        # FILL ME IN (roughly 12 lines)&#xA;&#xA;    return call&#xA;&#xA;&#xA;BATCH = 4&#xA;SIZE = 6&#xA;out = np.zeros((BATCH, 1))&#xA;inp = np.arange(BATCH * SIZE).reshape((BATCH, SIZE))&#xA;problem = CudaProblem(&#xA;    &#34;Axis Sum&#34;,&#xA;    axis_sum_test,&#xA;    [inp],&#xA;    out,&#xA;    [SIZE],&#xA;    Coord(1, BATCH),&#xA;    Coord(TPB, 1),&#xA;    spec=sum_spec,&#xA;)&#xA;problem.show()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Axis Sum&#xA; &#xA;   Score (Max Per Thread):&#xA;   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |&#xA;   |             0 |             0 |             0 |             0 | &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/srush/GPU-Puzzles/main/GPU_puzzlers_files/GPU_puzzlers_64_1.svg?sanitize=true&#34; alt=&#34;svg&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;problem.check()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;Failed Tests.&#xA;Yours: [[0.]&#xA; [0.]&#xA; [0.]&#xA; [0.]]&#xA;Spec : [[ 15.]&#xA; [ 51.]&#xA; [ 87.]&#xA; [123.]]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Puzzle 14 - Matrix Multiply!&lt;/h2&gt; &#xA;&lt;p&gt;Implement a kernel that multiplies square matrices &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; and stores it in &lt;code&gt;out&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Tip: The most efficient algorithm here will copy a block into shared memory before computing each of the individual row-column dot products. This is easy to do if the matrix fits in shared memory. Do that case first. Then update your code to compute a partial dot-product and then iteratively move the part that you copied into shared memory.&lt;/em&gt; You should be able to do the hard case in 6 global reads.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def matmul_spec(a, b):&#xA;    return a @ b&#xA;&#xA;&#xA;TPB = 3&#xA;def mm_oneblock_test(cuda):&#xA;    def call(out, a, b, size: int) -&amp;gt; None:&#xA;        a_shared = cuda.shared.array((TPB, TPB), numba.float32)&#xA;        b_shared = cuda.shared.array((TPB, TPB), numba.float32)&#xA;&#xA;        i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x&#xA;        j = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y&#xA;        local_i = cuda.threadIdx.x&#xA;        local_j = cuda.threadIdx.y&#xA;        # FILL ME IN (roughly 14 lines)&#xA;&#xA;    return call&#xA;&#xA;# Test 1&#xA;&#xA;SIZE = 2&#xA;out = np.zeros((SIZE, SIZE))&#xA;inp1 = np.arange(SIZE * SIZE).reshape((SIZE, SIZE))&#xA;inp2 = np.arange(SIZE * SIZE).reshape((SIZE, SIZE)).T&#xA;&#xA;problem = CudaProblem(&#xA;    &#34;Matmul (Simple)&#34;,&#xA;    mm_oneblock_test,&#xA;    [inp1, inp2],&#xA;    out,&#xA;    [SIZE],&#xA;    Coord(1, 1),&#xA;    Coord(TPB, TPB),&#xA;    spec=matmul_spec,&#xA;)&#xA;problem.show(sparse=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Matmul (Simple)&#xA; &#xA;   Score (Max Per Thread):&#xA;   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |&#xA;   |             0 |             0 |             0 |             0 | &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/srush/GPU-Puzzles/main/GPU_puzzlers_files/GPU_puzzlers_67_1.svg?sanitize=true&#34; alt=&#34;svg&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;problem.check()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;Failed Tests.&#xA;Yours: [[0. 0.]&#xA; [0. 0.]]&#xA;Spec : [[ 1  3]&#xA; [ 3 13]]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Test 2&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;SIZE = 8&#xA;out = np.zeros((SIZE, SIZE))&#xA;inp1 = np.arange(SIZE * SIZE).reshape((SIZE, SIZE))&#xA;inp2 = np.arange(SIZE * SIZE).reshape((SIZE, SIZE)).T&#xA;&#xA;problem = CudaProblem(&#xA;    &#34;Matmul (Full)&#34;,&#xA;    mm_oneblock_test,&#xA;    [inp1, inp2],&#xA;    out,&#xA;    [SIZE],&#xA;    Coord(3, 3),&#xA;    Coord(TPB, TPB),&#xA;    spec=matmul_spec,&#xA;)&#xA;problem.show(sparse=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Matmul (Full)&#xA; &#xA;   Score (Max Per Thread):&#xA;   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |&#xA;   |             0 |             0 |             0 |             0 | &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/srush/GPU-Puzzles/main/GPU_puzzlers_files/GPU_puzzlers_70_1.svg?sanitize=true&#34; alt=&#34;svg&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;problem.check()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;Failed Tests.&#xA;Yours: [[0. 0. 0. 0. 0. 0. 0. 0.]&#xA; [0. 0. 0. 0. 0. 0. 0. 0.]&#xA; [0. 0. 0. 0. 0. 0. 0. 0.]&#xA; [0. 0. 0. 0. 0. 0. 0. 0.]&#xA; [0. 0. 0. 0. 0. 0. 0. 0.]&#xA; [0. 0. 0. 0. 0. 0. 0. 0.]&#xA; [0. 0. 0. 0. 0. 0. 0. 0.]&#xA; [0. 0. 0. 0. 0. 0. 0. 0.]]&#xA;Spec : [[  140   364   588   812  1036  1260  1484  1708]&#xA; [  364  1100  1836  2572  3308  4044  4780  5516]&#xA; [  588  1836  3084  4332  5580  6828  8076  9324]&#xA; [  812  2572  4332  6092  7852  9612 11372 13132]&#xA; [ 1036  3308  5580  7852 10124 12396 14668 16940]&#xA; [ 1260  4044  6828  9612 12396 15180 17964 20748]&#xA; [ 1484  4780  8076 11372 14668 17964 21260 24556]&#xA; [ 1708  5516  9324 13132 16940 20748 24556 28364]]&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>ageron/handson-ml2</title>
    <updated>2022-07-24T02:13:01Z</updated>
    <id>tag:github.com,2022-07-24:/ageron/handson-ml2</id>
    <link href="https://github.com/ageron/handson-ml2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A series of Jupyter notebooks that walk you through the fundamentals of Machine Learning and Deep Learning in Python using Scikit-Learn, Keras and TensorFlow 2.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Machine Learning Notebooks&lt;/h1&gt; &#xA;&lt;p&gt;This project aims at teaching you the fundamentals of Machine Learning in python. It contains the example code and solutions to the exercises in the second edition of my O&#39;Reilly book &lt;a href=&#34;https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/&#34;&gt;Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;img src=&#34;https://images-na.ssl-images-amazon.com/images/I/51aqYc1QyrL._SX379_BO1,204,203,200_.jpg&#34; title=&#34;book&#34; width=&#34;150&#34;&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you are looking for the first edition notebooks, check out &lt;a href=&#34;https://github.com/ageron/handson-ml&#34;&gt;ageron/handson-ml&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;h3&gt;Want to play with these notebooks online without having to install anything?&lt;/h3&gt; &#xA;&lt;p&gt;Use any of the following services (I recommended Colab or Kaggle, since they offer free GPUs and TPUs).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;WARNING&lt;/strong&gt;: &lt;em&gt;Please be aware that these services provide temporary environments: anything you do will be deleted after a while, so make sure you download any data you care about.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/ageron/handson-ml2/blob/master/&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://homl.info/kaggle/&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Open in Kaggle&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://mybinder.org/v2/gh/ageron/handson-ml2/HEAD?filepath=%2Findex.ipynb&#34;&gt;&lt;img src=&#34;https://mybinder.org/badge_logo.svg?sanitize=true&#34; alt=&#34;Launch binder&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://homl.info/deepnote/&#34;&gt;&lt;img src=&#34;https://deepnote.com/buttons/launch-in-deepnote-small.svg?sanitize=true&#34; alt=&#34;Launch in Deepnote&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Just want to quickly look at some notebooks, without executing any code?&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/ageron/handson-ml2/blob/master/index.ipynb&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg?sanitize=true&#34; alt=&#34;Render nbviewer&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/ageron/handson-ml2/raw/master/index.ipynb&#34;&gt;github.com&#39;s notebook viewer&lt;/a&gt; also works but it&#39;s not ideal: it&#39;s slower, the math equations are not always displayed correctly, and large notebooks often fail to open.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Want to run this project using a Docker image?&lt;/h3&gt; &#xA;&lt;p&gt;Read the &lt;a href=&#34;https://github.com/ageron/handson-ml2/tree/master/docker&#34;&gt;Docker instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Want to install this project on your own machine?&lt;/h3&gt; &#xA;&lt;p&gt;Start by installing &lt;a href=&#34;https://www.anaconda.com/distribution/&#34;&gt;Anaconda&lt;/a&gt; (or &lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;Miniconda&lt;/a&gt;), &lt;a href=&#34;https://git-scm.com/downloads&#34;&gt;git&lt;/a&gt;, and if you have a TensorFlow-compatible GPU, install the &lt;a href=&#34;https://www.nvidia.com/Download/index.aspx&#34;&gt;GPU driver&lt;/a&gt;, as well as the appropriate version of CUDA and cuDNN (see TensorFlow&#39;s documentation for more details).&lt;/p&gt; &#xA;&lt;p&gt;Next, clone this project by opening a terminal and typing the following commands (do not type the first &lt;code&gt;$&lt;/code&gt; signs on each line, they just indicate that these are terminal commands):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/ageron/handson-ml2.git&#xA;$ cd handson-ml2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, run the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ conda env create -f environment.yml&#xA;$ conda activate tf2&#xA;$ python -m ipykernel install --user --name=python3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, start Jupyter:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ jupyter notebook&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you need further instructions, read the &lt;a href=&#34;https://raw.githubusercontent.com/ageron/handson-ml2/master/INSTALL.md&#34;&gt;detailed installation instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;FAQ&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Which Python version should I use?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;I recommend Python 3.8. If you follow the installation instructions above, that&#39;s the version you will get. Most code will work with other versions of Python 3, but some libraries do not support Python 3.9 or 3.10 yet, which is why I recommend Python 3.8.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;I&#39;m getting an error when I call &lt;code&gt;load_housing_data()&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Make sure you call &lt;code&gt;fetch_housing_data()&lt;/code&gt; &lt;em&gt;before&lt;/em&gt; you call &lt;code&gt;load_housing_data()&lt;/code&gt;. If you&#39;re getting an HTTP error, make sure you&#39;re running the exact same code as in the notebook (copy/paste it if needed). If the problem persists, please check your network configuration.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;I&#39;m getting an SSL error on MacOSX&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You probably need to install the SSL certificates (see this &lt;a href=&#34;https://stackoverflow.com/questions/27835619/urllib-and-ssl-certificate-verify-failed-error&#34;&gt;StackOverflow question&lt;/a&gt;). If you downloaded Python from the official website, then run &lt;code&gt;/Applications/Python\ 3.8/Install\ Certificates.command&lt;/code&gt; in a terminal (change &lt;code&gt;3.8&lt;/code&gt; to whatever version you installed). If you installed Python using MacPorts, run &lt;code&gt;sudo port install curl-ca-bundle&lt;/code&gt; in a terminal.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;I&#39;ve installed this project locally. How do I update it to the latest version?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/ageron/handson-ml2/master/INSTALL.md&#34;&gt;INSTALL.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;How do I update my Python libraries to the latest versions, when using Anaconda?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/ageron/handson-ml2/master/INSTALL.md&#34;&gt;INSTALL.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;p&gt;I would like to thank everyone &lt;a href=&#34;https://github.com/ageron/handson-ml2/graphs/contributors&#34;&gt;who contributed to this project&lt;/a&gt;, either by providing useful feedback, filing issues or submitting Pull Requests. Special thanks go to Haesun Park and Ian Beauregard who reviewed every notebook and submitted many PRs, including help on some of the exercise solutions. Thanks as well to Steven Bunkley and Ziembla who created the &lt;code&gt;docker&lt;/code&gt; directory, and to github user SuperYorio who helped on some exercise solutions.&lt;/p&gt;</summary>
  </entry>
</feed>