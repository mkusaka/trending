<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-02T01:58:44Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>mshumer/gpt-author</title>
    <updated>2023-07-02T01:58:44Z</updated>
    <id>tag:github.com,2023-07-02:/mshumer/gpt-author</id>
    <link href="https://github.com/mshumer/gpt-author" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;gpt-author&lt;/h1&gt; &#xA;&lt;p&gt;This project utilizes a chain of GPT-4 and Stable Diffusion API calls to generate an original fantasy novel. Users can provide an initial prompt and enter how many chapters they&#39;d like it to be, and the AI then generates an entire novel, outputting an EPUB file compatible with e-book readers.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A 15-chapter novel can cost as little as $4 to produce, and is written in just a few minutes.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;A few output novel examples are provided in this repo. To read one, you can download its file and view it on &lt;a href=&#34;https://www.fviewer.com/view-epub&#34;&gt;https://www.fviewer.com/view-epub&lt;/a&gt;, or install it on your Kindle, etc.&lt;/p&gt; &#xA;&lt;h2&gt;How It Works&lt;/h2&gt; &#xA;&lt;p&gt;The AI is asked to generate a list of potential plots based on a given prompt. It then selects the most engaging plot, improves upon it, and extracts a title. After that, it generates a detailed storyline with a specified number of chapters, and then tries to improve upon that storyline. Each chapter is then individually written by the AI, following the plot and taking into account the content of previous chapters. Finally, a prompt to design the cover art is generated, and the cover is created. Finally, it&#39;s all pulled together, and the novel is compiled into an EPUB file.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;You can &lt;a href=&#34;https://colab.research.google.com/drive/1er_3U7lr6m4GJ-aHE6Pgeq9KXploxp4d?usp=sharing&#34;&gt;run this project in Google Colab&lt;/a&gt; or in a local Jupyter notebook.&lt;/p&gt; &#xA;&lt;p&gt;In Google Colab, simply open the notebook, add your API keys, and run the cells in order.&lt;/p&gt; &#xA;&lt;p&gt;If you are using a local Jupyter notebook, you will need to install the necessary dependencies. You can do this by running the following command in your terminal:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install openai ebooklib requests&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In the last cell of the notebook, you can customize the prompt and the number of chapters for your novel. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;prompt = &#34;Similar to Percy Jackson or Harry Potter in terms of vibes, but a different plot entirely. Set in modern day. Add some element of technology to it.&#34;&#xA;num_chapters = 20&#xA;writing_style = &#34;Clear and easily understandable, similar to a young adult novel. Highly descriptive and sometimes long-winded.&#34;&#xA;novel, title, chapters, chapter_titles = write_fantasy_novel(prompt, num_chapters, writing_style)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will generate a novel based on the given prompt with 20 chapters. Note -- prompts with less than 7 chapters tend to cause issues.&lt;/p&gt; &#xA;&lt;h2&gt;Contributions&lt;/h2&gt; &#xA;&lt;p&gt;Contributions, issues, and feature requests are welcome!&lt;/p&gt; &#xA;&lt;p&gt;Some initial ideas:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;modify it to work solely with GPT-3.5-Turbo and GPT-3.5-Turbo-16k (it will likely require some level of compression/summariztion of early chapters so we don&#39;t run out of tokens when generating later chapters).&lt;/li&gt; &#xA; &lt;li&gt;improve the system for generating the first chapter -- the better the first chapter comes out, the better the rest of the novel is&lt;/li&gt; &#xA; &lt;li&gt;improve the prompts, as they were written very quickly&lt;/li&gt; &#xA; &lt;li&gt;improve each step in the process, adding more checks, improvement generations, etc.&lt;/li&gt; &#xA; &lt;li&gt;before generating improvements, have a model call identify potential improvements to add to the prompt, which will likely improve performance significantly&lt;/li&gt; &#xA; &lt;li&gt;modify it to go beyond just fantasy, allowing users to generate other genres as well&lt;/li&gt; &#xA; &lt;li&gt;fix the issue that causes some chapters to cut off early&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is &lt;a href=&#34;https://github.com/your_username/your_repository/raw/master/LICENSE&#34;&gt;MIT&lt;/a&gt; licensed.&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;Matt Shumer - &lt;a href=&#34;https://twitter.com/mattshumer_&#34;&gt;@mattshumer_&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Project Link: &lt;a href=&#34;https://github.com/mshumer/gpt-author/&#34;&gt;https://github.com/mshumer/gpt-author/&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Fafa-DL/Lhy_Machine_Learning</title>
    <updated>2023-07-02T01:58:44Z</updated>
    <id>tag:github.com,2023-07-02:/Fafa-DL/Lhy_Machine_Learning</id>
    <link href="https://github.com/Fafa-DL/Lhy_Machine_Learning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;李宏毅2021/2022/2023春季机器学习课程课件及作业&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;李宏毅春季机器学习课程&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://space.bilibili.com/46880349&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Fafa-DL/readme-data/main/Bilibili.png&#34; alt=&#34;BILIBILI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Bilibili-%E5%95%A5%E9%83%BD%E4%BC%9A%E4%B8%80%E7%82%B9%E7%9A%84%E7%A0%94%E7%A9%B6%E7%94%9F-green&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/%E5%85%AC%E4%BC%97%E5%8F%B7-%E5%95%A5%E9%83%BD%E4%BC%9A%E4%B8%80%E7%82%B9%E7%9A%84%E7%A0%94%E7%A9%B6%E7%94%9F-brightgreen&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/Fafa-DL/Lhy_Machine_Learning&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/Fafa-DL/Lhy_Machine_Learning&#34; alt=&#34;GitHub stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Fafa-DL/Lhy_Machine_Learning&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/forks/Fafa-DL/Lhy_Machine_Learning&#34; alt=&#34;GitHub forks&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://visitor-badge.glitch.me/badge?page_id=Fafa-DL.Lhy_Machine_Learning&amp;amp;right_color=yellow&#34; alt=&#34;Visitors&#34;&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/Fafa-DL/Lhy_Machine_Learning/tree/main/2021%20ML&#34;&gt;2021课程及资料链接&lt;/a&gt; &lt;a href=&#34;https://github.com/Fafa-DL/Lhy_Machine_Learning/tree/main/2022%20ML&#34;&gt;2022课程及资料链接&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h2&gt;目录&lt;/h2&gt; &#xA;&lt;p&gt;  &lt;a href=&#34;https://raw.githubusercontent.com/Fafa-DL/Lhy_Machine_Learning/main/#%E9%87%8D%E7%A3%85%E9%A1%BB%E7%9F%A5&#34;&gt;重磅须知&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;  &lt;a href=&#34;https://raw.githubusercontent.com/Fafa-DL/Lhy_Machine_Learning/main/#%E6%88%91%E7%BB%B4%E6%8A%A4%E7%9A%84%E5%85%B6%E4%BB%96%E9%A1%B9%E7%9B%AE&#34;&gt;我维护的其他项目&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;  &lt;a href=&#34;https://raw.githubusercontent.com/Fafa-DL/Lhy_Machine_Learning/main/#%E6%9B%B4%E6%96%B0%E6%97%A5%E5%BF%97&#34;&gt;更新日志&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;  &lt;a href=&#34;https://raw.githubusercontent.com/Fafa-DL/Lhy_Machine_Learning/main/#%E8%AF%BE%E7%A8%8B%E5%9C%B0%E5%9D%80&#34;&gt;课程地址&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;  &lt;a href=&#34;https://raw.githubusercontent.com/Fafa-DL/Lhy_Machine_Learning/main/#%E8%AF%BE%E7%A8%8B%E8%B5%84%E6%96%99%E7%9B%B4%E9%93%BE&#34;&gt;课程资料直链&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;  &lt;a href=&#34;https://raw.githubusercontent.com/Fafa-DL/Lhy_Machine_Learning/main/#%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A%E7%9B%B4%E9%93%BE&#34;&gt;课程作业直链&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;  &lt;a href=&#34;https://raw.githubusercontent.com/Fafa-DL/Lhy_Machine_Learning/main/#%E5%85%B6%E4%BB%96%E4%BC%98%E8%B4%A8%E8%AF%BE%E7%A8%8B&#34;&gt;其他优质课程&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;重磅须知&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;为方便所有网课资料与优质电子书籍的实时更新维护，&lt;strong&gt;创建一个在线实时网盘文件夹&lt;/strong&gt;；&lt;/p&gt; &#xA; &lt;p&gt;网盘获取方式：&lt;strong&gt;公众号【啥都会一点的研究生】-&amp;gt;回复【05】-&amp;gt;阅读原文&lt;/strong&gt;；&lt;/p&gt; &#xA; &lt;p&gt;UP将&lt;strong&gt;2021&amp;amp;2022&amp;amp;2023&lt;/strong&gt;所有资料整理打包，&lt;strong&gt;在线网盘能满足该课程所需资料的全部需求&lt;/strong&gt;；&lt;/p&gt; &#xA; &lt;p&gt;链接挂掉与任何通知会及时更新，祝大家学习顺利，&lt;strong&gt;再次感谢李宏毅老师授权转载&lt;/strong&gt;；&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;我维护的其他项目&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Fafa-DL/Image-Augmentation&#34;&gt;&lt;strong&gt;图片数据不够？我做了一款图像增强软件&lt;/strong&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Fafa-DL/Image-Augmentation&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/Fafa-DL/Image-Augmentation&#34; alt=&#34;GitHub stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Fafa-DL/Image-Augmentation&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/forks/Fafa-DL/Image-Augmentation&#34; alt=&#34;GitHub forks&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Fafa-DL/Awesome-Backbones&#34;&gt;&lt;strong&gt;开箱即用，涵盖主流模型的图像分类|主干网络学习/对比/魔改项目&lt;/strong&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Fafa-DL/Awesome-Backbones&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/Fafa-DL/Awesome-Backbones&#34; alt=&#34;GitHub stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Fafa-DL/Awesome-Backbones&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/forks/Fafa-DL/Awesome-Backbones&#34; alt=&#34;GitHub forks&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Fafa-DL/LabelConvert&#34;&gt;&lt;strong&gt;一键转换与编辑图像标注文件软件，极大提高效率&lt;/strong&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Fafa-DL/LabelConvert&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/Fafa-DL/LabelConvert&#34; alt=&#34;GitHub stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Fafa-DL/LabelConvert&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/forks/Fafa-DL/LabelConvert&#34; alt=&#34;GitHub forks&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;更新日志&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;日期&lt;/th&gt; &#xA;   &lt;th&gt;项目&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2023/02/25&lt;/td&gt; &#xA;   &lt;td&gt;更新Topic【正确认识ChatGPT】及作业一&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2023/03/05&lt;/td&gt; &#xA;   &lt;td&gt;更新Topic【机器学习基本概念介绍】及作业二&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2023/03/12&lt;/td&gt; &#xA;   &lt;td&gt;更新Topic【机器如何生成文句】及作业三&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2023/03/20&lt;/td&gt; &#xA;   &lt;td&gt;更新Topic【生成式学习的各种议题：多样性、评量方式、其他挑战】及作业四，删除作业选修，自行查看2021&amp;amp;2022相应章节&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2023/03/26&lt;/td&gt; &#xA;   &lt;td&gt;更新Topic【机器如何生成图像】及作业五&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2023/04/02&lt;/td&gt; &#xA;   &lt;td&gt;更新作业六&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2023/04/16&lt;/td&gt; &#xA;   &lt;td&gt;更新Topic【Diffusion Model 原理剖析】&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2023/04/26&lt;/td&gt; &#xA;   &lt;td&gt;更新Topic【基石模型的各种变形】及作业7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2023/05/03&lt;/td&gt; &#xA;   &lt;td&gt;更新Topic【大型语言模型的应用】及作业8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2023/05/14&lt;/td&gt; &#xA;   &lt;td&gt;更新HW9、HW10，正式课程无法获取&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2023/05/21&lt;/td&gt; &#xA;   &lt;td&gt;更新Topic【语音基石模型】、【大型语言模型】及HW11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2023/05/29&lt;/td&gt; &#xA;   &lt;td&gt;更新HW12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2023/06/03&lt;/td&gt; &#xA;   &lt;td&gt;更新HW13、HW14、HW15&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;课程地址&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;项目&lt;/th&gt; &#xA;   &lt;th&gt;内容&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;视频合集&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=1&#34;&gt;【授权】李宏毅2023春机器学习课程&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1Wv411h7kN&#34;&gt;(强推)李宏毅2021/2022春机器学习课程&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;课程主页&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://speech.ee.ntu.edu.tw/~hylee/ml/2023-spring.php&#34;&gt;李宏毅2023春季机器学习&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://speech.ee.ntu.edu.tw/~hylee/ml/2022-spring.php&#34;&gt;李宏毅2022春季机器学习&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://speech.ee.ntu.edu.tw/~hylee/ml/2021-spring.html&#34;&gt;李宏毅2021春季机器学习&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;B站主页/公众号&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://space.bilibili.com/46880349&#34;&gt;啥都会一点的研究生&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;人工智能技术探讨群&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://jq.qq.com/?_wv=1027&amp;amp;k=lY5KVICA&#34;&gt;QQ群1：78174903&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://jq.qq.com/?_wv=1027&amp;amp;k=bakez5Yz&#34;&gt;QQ群3：584723646&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;课程资料直链&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Topic&lt;/th&gt; &#xA;   &lt;th&gt;正课内容&lt;/th&gt; &#xA;   &lt;th&gt;延申内容&lt;/th&gt; &#xA;   &lt;th&gt;选修内容&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;正确认识ChatGPT&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=1&#34;&gt;机器学习2023规则说明&lt;/a&gt; &lt;a href=&#34;https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/rule.pdf&#34;&gt;slides&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=2&#34;&gt;ChatGPT 原理剖析 (1/3) — 对 ChatGPT 的常见误解&lt;/a&gt; &lt;a href=&#34;https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/ChatGPT_basic_(v5).pdf&#34;&gt;slides&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=3&#34;&gt;ChatGPT 原理剖析 (2/3) — 预训练 (Pre-train)&lt;/a&gt; &lt;a href=&#34;https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/ChatGPT_SSL_(v4).pdf&#34;&gt;slides&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=4&#34;&gt;ChatGPT 原理剖析 (3/3) — ChatGPT 所带来的研究问题&lt;/a&gt;&lt;a href=&#34;https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/ChatGPT_Question_(v2).pdf&#34;&gt;slides&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=5&#34;&gt;用 ChatGPT 和 Midjourney 来玩文字冒险游戏&lt;/a&gt; &lt;a href=&#34;https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/TextGame_(v2).pdf&#34;&gt;slides&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=6&#34;&gt;ChatGPT (可能)是怎么炼成的&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=7&#34;&gt;Predicting Pokémon CP&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=8&#34;&gt;Pokemon classification&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=9&#34;&gt;Logistic Regression&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;机器学习基本概念介绍&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=13&#34;&gt;【生成式AI】快速了解机器学习基本原理( 1_2)&lt;/a&gt; &lt;a href=&#34;https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/ML%20basic%20(v8).pdf&#34;&gt;slides&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=14&#34;&gt;【生成式AI】快速了解机器学习基本原理( 2_2)&lt;/a&gt; &lt;a href=&#34;https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/ML%20basic%20(v8).pdf&#34;&gt;slides&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=15&#34;&gt;【生成式AI】生成式学习的两种策略：要各个击破，还是要一次到位&lt;/a&gt; &lt;a href=&#34;https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/ARandNAR%20(v2).pdf&#34;&gt;slides&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=16&#34;&gt;【生成式AI】能够使用工具的AI：New Bing, WebGPT, Toolformer&lt;/a&gt;&lt;a href=&#34;https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/NewBing%20(v4).pdf&#34;&gt;slides&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=17&#34;&gt;Brief Introduction of Deep Learning&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=18&#34;&gt;Gradient Descent&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=19&#34;&gt;Backpropagation&lt;/a&gt;&lt;br&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=20&#34;&gt;卷积神经网络&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=21&#34;&gt;自注意力机制&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;机器如何生成文句&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/Prompt%20(v3).pdf&#34;&gt;slides&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=23&#34;&gt;【生成式AI】Finetuning vs. Prompting：对于大型语言模型的不同期待所衍生的两类使用方式 (1_3)&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=24&#34;&gt;【生成式AI】Finetuning vs. Prompting：对于大型语言模型的不同期待所衍生的两类使用方式 (2_3)&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=25&#34;&gt;【生成式AI】Finetuning vs. Prompting：对于大型语言模型的不同期待所衍生的两类使用方式 (3_3)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=26&#34;&gt;自督导式学习 (Self-supervised Learning) (二) – BERT简介&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=27&#34;&gt;自督导式学习 (Self-supervised Learning) (四) – GPT的野望&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://d223302.github.io/AACL2022-Pretrain-Language-Model-Tutorial&#34;&gt;AACL 2022 Tutorial: Recent Advances in Pre-trained Language Models:Why Do They Work and How to Use Them&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;生成式学习的各种议题：多样性、评量方式、其他挑战&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/LargeLM-v7.pdf&#34;&gt;slides&lt;/a&gt; &lt;a href=&#34;https://drive.google.com/file/d/1sDHgtlZLnmXiSNRhB6lA14gYpFFoCTbw/view&#34;&gt;slides&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=29&#34;&gt;【大模型 + 大资料 = 神奇结果？ (1_3)：大模型的顿悟时刻&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=30&#34;&gt;大模型 + 大资料 = 神奇结果？ (2_3)：到底要多少资料才够&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=31&#34;&gt;大模型 + 大资料 = 神奇结果？ (3_3)：另辟蹊径 — KNNLM&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;&#34;&gt;GPT-4 来了! GPT-4 这次有什么神奇的能力呢？&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=32&#34;&gt;自督导式学习 (Self-supervised Learning) (二) – BERT简介&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=33&#34;&gt;自督导式学习 (Self-supervised Learning) (四) – GPT的野望&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://d223302.github.io/AACL2022-Pretrain-Language-Model-Tutorial&#34;&gt;AACL 2022 Tutorial: Recent Advances in Pre-trained Language Models:Why Do They Work and How to Use Them&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;机器如何生成图像&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=34&#34;&gt;【生成式AI】速览图像生成常见模型&lt;/a&gt; &lt;a href=&#34;https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/ImageGeneration%20(v3).pdf&#34;&gt;slides&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=35&#34;&gt;浅谈图像生成模型 Diffusion Model 原理&lt;/a&gt; &lt;a href=&#34;https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/DiffusionModel%20(v2).pdf&#34;&gt;slides&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=36&#34;&gt;Stable Diffusion、DALL-E、Imagen 背后共同的套路&lt;/a&gt; &lt;a href=&#34;https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/StableDiffusion%20(v2).pdf&#34;&gt;slides&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=37&#34;&gt;Variational Auto-encoder (VAE)&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=38&#34;&gt;Flow-based Generative Model&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=39&#34;&gt;Generative Adversarial Network (GAN)&lt;/a&gt;&lt;br&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;量子机器学习&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Diffusion Model 原理剖析&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/DDPM%20(v7).pdf&#34;&gt;slide&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=42&#34;&gt;Diffusion Model 原理剖析 (1_4) (optional)&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=43&#34;&gt;Diffusion Model 原理剖析 (2_4) (optional)&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=44&#34;&gt;Diffusion Model 原理剖析 (3_4) (optional)&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=45&#34;&gt;Diffusion Model 原理剖析 (4_4) (optional)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2006.11239.pdf&#34;&gt;Denoising Diffusion Probabilistic Models&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://arxiv.org/pdf/2208.11970.pdf&#34;&gt;Understanding Diffusion Models: A Unified Perspective&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1904.09751.pdf&#34;&gt;The Curious Case of Neural Text Degeneration&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://arxiv.org/pdf/1712.05884.pdf&#34;&gt;Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://arxiv.org/pdf/2205.14217.pdf&#34;&gt;Diffusion-LM Improves Controllable Text Generation&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://aclanthology.org/D19-1633.pdf&#34;&gt;Mask-Predict: Parallel Decoding of Conditional Masked Language Models&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;基石模型的各种变形&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=46&#34;&gt;【生成式AI】穷人如何低资源复刻自己的 ChatGPT&lt;/a&gt; &lt;a href=&#34;https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/PoorChatGPT-v2.pdf&#34;&gt;slide&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=47&#34;&gt;让 AI 村民组成虚拟村庄会发生什么事？&lt;/a&gt; &lt;a href=&#34;https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/ThinkMore-v2.pdf&#34;&gt;slide&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;&#34;&gt;ChatGPT 可以自我反省!（B站审核不过）&lt;/a&gt; &lt;a href=&#34;https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/AI-interaction-v3.pdf&#34;&gt;slide&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;基石模型的各种变形&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=48&#34;&gt;【生成式AI】大型语言模型的应用&lt;/a&gt; &lt;a href=&#34;https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/LLM.pdf&#34;&gt;slide&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;基石模型的各种变形&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=46&#34;&gt;【生成式AI】穷人如何低资源复刻自己的 ChatGPT&lt;/a&gt; &lt;a href=&#34;https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/PoorChatGPT-v2.pdf&#34;&gt;slide&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=47&#34;&gt;让 AI 村民组成虚拟村庄会发生什么事？&lt;/a&gt; &lt;a href=&#34;https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/ThinkMore-v2.pdf&#34;&gt;slide&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;&#34;&gt;ChatGPT 可以自我反省!（B站审核不过）&lt;/a&gt; &lt;a href=&#34;https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/AI-interaction-v3.pdf&#34;&gt;slide&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;大型语言模型的应用&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;隐私保护机器学习&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;语音基石模型&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/%E5%BC%B5%E5%87%B1%E7%88%B2-x-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-x-%E8%AA%9E%E9%9F%B3%E5%9F%BA%E7%9F%B3%E6%A8%A1%E5%9E%8B.pdf&#34;&gt;slide&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=52&#34;&gt;语音基石模型 (1_2)&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=52&#34;&gt;语音基石模型 (2_2)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;大型语言模型&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/FrugalGPT-v2.pdf&#34;&gt;slides1&lt;/a&gt; &lt;a href=&#34;https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/AIexpAI-v5.pdf&#34;&gt;slides2&lt;/a&gt; &lt;a href=&#34;https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/LLM-plan-v5.pdf&#34;&gt;slides3&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=55&#34;&gt;用语言模型來解释语言模型 (上)&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=56&#34;&gt;用语言模型來解释语言模型 (下)&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=57&#34;&gt;让 AI 做计划然后自己运行自己&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=58&#34;&gt;)FrugalGPT- 来看看穷人怎么用省钱的方式来使用 ChatGPT(上)&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=59&#34;&gt;)FrugalGPT- 来看看穷人怎么用省钱的方式来使用 ChatGPT(下)&lt;/a&gt;&lt;br&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;课程作业直链&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Topic&lt;/th&gt; &#xA;   &lt;th&gt;讲解视频&lt;/th&gt; &#xA;   &lt;th&gt;讲义&lt;/th&gt; &#xA;   &lt;th&gt;代码&lt;/th&gt; &#xA;   &lt;th&gt;平台&lt;/th&gt; &#xA;   &lt;th&gt;预备知识(见2021&amp;amp;2022相应章节)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;x&lt;/td&gt; &#xA;   &lt;td&gt;Colab Tutorial&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=10&#34;&gt;Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/Colab_Tutorial.pdf&#34;&gt;slide&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1Qi4-BRqZ3qI3x_Jtr5ci_oRvHDMQpdiW?usp=sharing&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;x&lt;/td&gt; &#xA;   &lt;td&gt;PyTorch Tutorial&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=11&#34;&gt;Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/Pytorch_Tutorial_1.pdf&#34;&gt;slide 1&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/Pytorch_Tutorial_2.pdf&#34;&gt;slide 2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;x&lt;/td&gt; &#xA;   &lt;td&gt;x&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HW1&lt;/td&gt; &#xA;   &lt;td&gt;Regression&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=12&#34;&gt;Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/HW01.pdf&#34;&gt;slide&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1BESEu-l3qrGRULoATuXnWasUNuUlVF1Z?fbclid=IwAR1FrjUsp4rTy5PPFV-aWq6IG_Z44mFT4VH5e1lIhlekFl7fAvxGRCTCyR0&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/t/a339b77fa5214978bfb8dde62d3151fe&#34;&gt;Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HW2&lt;/td&gt; &#xA;   &lt;td&gt;Classification&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=22&#34;&gt;Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/HW02+%E8%81%BD%E6%B8%AC.pdf&#34;&gt;slide&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1wzeiVy2g7HpSjlidUr0Gi50NnHBWTkvN#scrollTo=KVUGfWTo7_Oj&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/t/03ac116596a247219b5a8d7a8e2b800e&#34;&gt;Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HW3&lt;/td&gt; &#xA;   &lt;td&gt;CNN&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=28&#34;&gt;Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/HW03.pdf&#34;&gt;slide&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/15A_8ilH-6-T3HOmSFrKbjDinBJl-s-16&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/t/86ca241732c04da99aca6490080bae73&#34;&gt;Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HW4&lt;/td&gt; &#xA;   &lt;td&gt;Self-attention&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=33&#34;&gt;Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/HW04.pdf&#34;&gt;slide&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1u-610KA-urqfJjDH5O0pecwfP--V9DQs?usp=sharing&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/t/49ea0c385a974db5919ec67299ba2e6b&#34;&gt;Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HW5&lt;/td&gt; &#xA;   &lt;td&gt;Transformer&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=40&#34;&gt;Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/HW05.pdf&#34;&gt;slide&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1qj_J9ld5KR3TTNbU5PnCAKMkDH3vN27p#scrollTo=59neB_Sxp5Ub&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ml.ee.ntu.edu.tw/home&#34;&gt;JudgeBoi&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HW6&lt;/td&gt; &#xA;   &lt;td&gt;Generative Model&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=41&#34;&gt;Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/1x8c38zgEbN2zN4EboWhquZ5b3LhVCN8ElhaJCO2vnzY/edit#slide=id.g11dca28fc13_0_140&#34;&gt;slide&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/code/b07202024/hw6-diffusion-model&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ml.ee.ntu.edu.tw/home&#34;&gt;JudgeBoi&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HW7&lt;/td&gt; &#xA;   &lt;td&gt;BERT&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=47&#34;&gt;Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/HW07.pdf&#34;&gt;slide&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1m0fQjJfkK9vAovxPj9Nd3-hQuxezB2w1&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/t/e001cad568dc4d77b6a5e762172f44d6&#34;&gt;Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HW8&lt;/td&gt; &#xA;   &lt;td&gt;Auto-encoder&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=49&#34;&gt;Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/HW08.pdf&#34;&gt;slide&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1iqvGMVMkmTynKI8UQWaYeXxdQXeO7CKM#scrollTo=YiVfKn-6tXz8&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/t/c76950cc460140eba30a576ca7668d28&#34;&gt;Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HW9&lt;/td&gt; &#xA;   &lt;td&gt;Explainable AI&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=51&#34;&gt;Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/HW09.pdf&#34;&gt;slide&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1w7p96mLz8uPQSCCXYPm1HxDVtLPHPZcS?usp=sharing&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.gradescope.com/courses/515619&#34;&gt;Gradescope&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HW10&lt;/td&gt; &#xA;   &lt;td&gt;Attack&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=54&#34;&gt;Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/HW10.pdf&#34;&gt;slide&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1GmZI-58gQXxu6VTYHGpf0Ob1iZ8PB4GV?usp=share_link&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ml.ee.ntu.edu.tw/home&#34;&gt;JudgeBoi&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HW11&lt;/td&gt; &#xA;   &lt;td&gt;Adaptation&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=60&#34;&gt;Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/HW11.pdf&#34;&gt;slide&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1mzs--uJmW3fE2Dy-FIU_EKe2MIPBzz8I?usp=sharing&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/t/77117edf751b445b86baaaf745a7f89c&#34;&gt;Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HW12&lt;/td&gt; &#xA;   &lt;td&gt;Reinforcement Learning&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=61&#34;&gt;Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/1yY3Yel6-W4Lvx6YYHD_-z574xcKr0xEoKHXB2exSeBg/edit#slide=id.p&#34;&gt;slide&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1XIw2FLg4FVYpAblLlTf14zP9grVoDmaU&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ml.ee.ntu.edu.tw/home&#34;&gt;JudgeBoi&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HW13&lt;/td&gt; &#xA;   &lt;td&gt;Network Compression&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=62&#34;&gt;Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/HW13.pdf&#34;&gt;slide&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1rW8tgC9QpNmvOo-e7MXI2LH1ojFXjsmu?usp=sharing#scrollTo=-hQgM5oQSz5g&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/t/899315e53b164aaeacdc1f5125a32f95&#34;&gt;Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HW14&lt;/td&gt; &#xA;   &lt;td&gt;Life-long Learning&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=63&#34;&gt;Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/HW14.pdf&#34;&gt;slide&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1O1pAeKmSMwrNXWxeE9SApgdxK8CKC5dk?usp=sharing&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.gradescope.com/courses/515619&#34;&gt;JudgeBoi&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HW15&lt;/td&gt; &#xA;   &lt;td&gt;Meta Learning&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1TD4y137mP/?p=64&#34;&gt;Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/HW15.pdf&#34;&gt;slide&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1tnsxsSJ0Ltr-BRGYE8rA4a4NzG7ZgnNF?usp=sharing&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.kaggle.com/competitions/ml2023spring-hw15&#34;&gt;Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;其他优质课程&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;名称&lt;/th&gt; &#xA;   &lt;th&gt;链接&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;(高清重制)麻省理工学院 MIT 18.06 线性代数&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1iU4y1K7oZ?spm_id_from=333.337.search-card.all.click&#34;&gt;视频地址&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;(强推)Python面向对象编程五步曲-从零到就业&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1v4411B7Zv?spm_id_from=333.337.search-card.all.click&#34;&gt;视频地址&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;【吴恩达亲授】适用于所有人的人工智能课程（中字）&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1gE411M7Eg?spm_id_from=333.337.search-card.all.click&#34;&gt;视频地址&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;(强推 双字)网易版吴恩达机器学习课程&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1LE411h7P4?spm_id_from=333.337.search-card.all.click&#34;&gt;视频地址&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;(强推 双字)2022吴恩达机器学习Deeplearning.ai课程&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1Pa411X76s?spm_id_from=333.337.search-card.all.click&#34;&gt;视频地址&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;(强推 双字)2018秋季CS229机器学习-官方高清版&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1Et4y1U7WB?spm_id_from=333.337.search-card.all.click&#34;&gt;视频地址&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;(强推 双字)2021版吴恩达深度学习课程Deeplearning.ai&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV12E411a7Xn?spm_id_from=333.337.search-card.all.click&#34;&gt;视频地址&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;(强推)浙江大学-机器学习&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1qf4y1x7kB?spm_id_from=333.337.search-card.all.click&#34;&gt;视频地址&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;吃透《统计学习方法》&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1tZ4y1U7ot?spm_id_from=333.337.search-card.all.click&#34;&gt;视频地址&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;北理-Python数据分析与展示-Numpy、Matplotlib、Pandas&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1L64y1X7om?spm_id_from=333.337.search-card.all.click&#34;&gt;视频地址&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;适用于初学者的Pytorch神经网络编程教学&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV15K411N7CF?spm_id_from=333.337.search-card.all.click&#34;&gt;视频地址&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;(强推)Pytorch深度学习实战教学&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1Zv4y1o7uG?spm_id_from=333.337.search-card.all.click&#34;&gt;视频地址&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;(强推)TensorFlow官方入门实操课程&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1rz4y117p1?spm_id_from=333.337.search-card.all.click&#34;&gt;视频地址&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;【北交】图像处理与机器学习&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1Kh411X7Qv?spm_id_from=333.337.search-card.all.click&#34;&gt;视频地址&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;零基础OpenCV4-C++极简入门&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1YY41177NU?spm_id_from=333.337.search-card.all.click&#34;&gt;视频地址&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;（全）基于python的Opencv项目实战&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1oJ411D71z?spm_id_from=333.337.search-card.all.click&#34;&gt;视频地址&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;(强推)最新斯坦福CS231n计算机视觉课程&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1hi4y1t7kF?spm_id_from=333.337.search-card.all.click&#34;&gt;视频地址&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://space.bilibili.com/46880349&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Fafa-DL/readme-data/main/gzh.jpg&#34; alt=&#34;BILIBILI&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>IDEA-Research/Grounded-Segment-Anything</title>
    <updated>2023-07-02T01:58:44Z</updated>
    <id>tag:github.com,2023-07-02:/IDEA-Research/Grounded-Segment-Anything</id>
    <link href="https://github.com/IDEA-Research/Grounded-Segment-Anything" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Grounded-SAM: Marrying Grounding DINO with Segment Anything &amp; Stable Diffusion &amp; Recognize Anything - Automatically Detect , Segment and Generate Anything&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/assets/Grounded-SAM_logo.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Grounded-Segment-Anything&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/oEQYStnF2l8&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/youtube.svg?sanitize=true&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/automated-dataset-annotation-and-evaluation-with-grounding-dino-and-sam.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/camenduru/grounded-segment-anything-colab&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open in Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/IDEA-Research/Grounded-SAM&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97-HuggingFace%20Space-cyan.svg?sanitize=true&#34; alt=&#34;HuggingFace Space&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://replicate.com/cjwbw/grounded-recognize-anything&#34;&gt;&lt;img src=&#34;https://replicate.com/cjwbw/grounded-recognize-anything/badge&#34; alt=&#34;Replicate&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://modelscope.cn/studios/tuofeilunhifi/Grounded-Segment-Anything/summary&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ModelScope-Official%20Demo-important&#34; alt=&#34;ModelScope Official Demo&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/yizhangliu/Grounded-Segment-Anything&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Huggingface-Demo%20by%20Community-red&#34; alt=&#34;Huggingface Demo by Community&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/continue-revolution/sd-webui-segment-anything&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Stable--Diffusion-WebUI%20by%20Community-critical&#34; alt=&#34;Stable-Diffusion WebUI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/grounded_sam.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-Jupyter%20Notebook-informational&#34; alt=&#34;Jupyter Notebook Demo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;We plan to create a very interesting demo by combining &lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO&#34;&gt;Grounding DINO&lt;/a&gt; and &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;Segment Anything&lt;/a&gt; which aims to detect and segment Anything with text inputs! And we will continue to improve it and create more interesting demos based on this foundation.&lt;/p&gt; &#xA;&lt;p&gt;We are very willing to &lt;strong&gt;help everyone share and promote new projects&lt;/strong&gt; based on Segment-Anything, Please check out here for more amazing demos and works in the community: &lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/#highlighted-projects&#34;&gt;Highlight Extension Projects&lt;/a&gt;. You can submit a new issue (with &lt;code&gt;project&lt;/code&gt; tag) or a new pull request to add new project&#39;s links.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/assets/grounded_sam_new_demo_image.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/assets/ram_grounded_sam_new.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;🍄 Why Building this Project?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The &lt;strong&gt;core idea&lt;/strong&gt; behind this project is to &lt;strong&gt;combine the strengths of different models in order to build a very powerful pipeline for solving complex problems&lt;/strong&gt;. And it&#39;s worth mentioning that this is a workflow for combining strong expert models, where &lt;strong&gt;all parts can be used separately or in combination, and can be replaced with any similar but different models (like replacing Grounding DINO with GLIP or other detectors / replacing Stable-Diffusion with ControlNet or GLIGEN/ Combining with ChatGPT)&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;🍇 Updates&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/06/28&lt;/code&gt;&lt;/strong&gt; Combining Grounding-DINO with Efficient SAM variants including &lt;a href=&#34;https://github.com/CASIA-IVA-Lab/FastSAM&#34;&gt;FastSAM&lt;/a&gt; and &lt;a href=&#34;https://github.com/ChaoningZhang/MobileSAM&#34;&gt;MobileSAM&lt;/a&gt; in &lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/EfficientSAM/&#34;&gt;EfficientSAM&lt;/a&gt; for faster annotating, thanks a lot for their great work!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/06/20&lt;/code&gt;&lt;/strong&gt; By combining &lt;strong&gt;Grounding-DINO-L&lt;/strong&gt; with &lt;strong&gt;SAM-ViT-H&lt;/strong&gt;, Grounded-SAM has won &lt;strong&gt;the first place&lt;/strong&gt; in &lt;a href=&#34;https://eval.ai/web/challenges/challenge-page/1931/overview&#34;&gt;Segmentation in the Wild&lt;/a&gt; competition zero-shot track on &lt;a href=&#34;https://computer-vision-in-the-wild.github.io/cvpr-2023/&#34;&gt;CVPR 2023 workshop&lt;/a&gt;, surpassing the second place about &lt;strong&gt;4 mAP&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/06/16&lt;/code&gt;&lt;/strong&gt; Release &lt;a href=&#34;https://replicate.com/cjwbw/ram-grounded-sam&#34;&gt;RAM-Grounded-SAM Replicate Online Demo&lt;/a&gt;. Thanks a lot to &lt;a href=&#34;https://chenxwh.github.io/&#34;&gt;Chenxi&lt;/a&gt; for providing this nice demo 🌹.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/06/14&lt;/code&gt;&lt;/strong&gt; Support &lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/automatic_label_ram_demo.py&#34;&gt;RAM-Grounded-SAM &amp;amp; SAM-HQ&lt;/a&gt; and update &lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/automatic_label_ram_demo.py&#34;&gt;Simple Automatic Label Demo&lt;/a&gt; to support &lt;a href=&#34;https://github.com/xinyu1205/Recognize_Anything-Tag2Text&#34;&gt;RAM&lt;/a&gt;, setting up a strong automatic annotation pipeline.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/06/13&lt;/code&gt;&lt;/strong&gt; Checkout the &lt;a href=&#34;https://youtu.be/gKTYMfwPo4M&#34;&gt;Autodistill: Train YOLOv8 with ZERO Annotations&lt;/a&gt; tutorial to learn how to use Grounded-SAM + &lt;a href=&#34;https://github.com/autodistill/autodistill&#34;&gt;Autodistill&lt;/a&gt; for automated data labeling and real-time model training.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/06/13&lt;/code&gt;&lt;/strong&gt; Support &lt;a href=&#34;https://github.com/SysCV/sam-hq&#34;&gt;SAM-HQ&lt;/a&gt; in &lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/#running_man-grounded-sam-detect-and-segment-everything-with-text-prompt&#34;&gt;Grounded-SAM Demo&lt;/a&gt; for higher quality prediction.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/06/12&lt;/code&gt;&lt;/strong&gt; Support &lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/#label-grounded-sam-with-ram-or-tag2text-for-automatic-labeling&#34;&gt;RAM-Grounded-SAM&lt;/a&gt; for strong automatic labeling pipeline! Thanks for &lt;a href=&#34;https://github.com/xinyu1205/Recognize_Anything-Tag2Text&#34;&gt;Recognize-Anything&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/06/01&lt;/code&gt;&lt;/strong&gt; Our Grounded-SAM has been accepted to present a &lt;strong&gt;demo&lt;/strong&gt; at &lt;a href=&#34;https://iccv2023.thecvf.com/&#34;&gt;ICCV 2023&lt;/a&gt;! See you in Paris!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/05/23&lt;/code&gt;&lt;/strong&gt;: Support &lt;code&gt;Image-Referring-Segment&lt;/code&gt;, &lt;code&gt;Audio-Referring-Segment&lt;/code&gt; and &lt;code&gt;Text-Referring-Segment&lt;/code&gt; in &lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/playground/ImageBind_SAM/&#34;&gt;ImageBind-SAM&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/05/03&lt;/code&gt;&lt;/strong&gt;: Checkout the &lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/automated-dataset-annotation-and-evaluation-with-grounding-dino-and-sam.ipynb&#34;&gt;Automated Dataset Annotation and Evaluation with GroundingDINO and SAM&lt;/a&gt; which is an amazing tutorial on automatic labeling! Thanks a lot for &lt;a href=&#34;https://github.com/SkalskiP&#34;&gt;Piotr Skalski&lt;/a&gt; and &lt;a href=&#34;https://github.com/roboflow/notebooks&#34;&gt;Roboflow&lt;/a&gt;!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/#grounded-segment-anything&#34;&gt;Grounded-Segment-Anything&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/#preliminary-works&#34;&gt;Preliminary Works&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/#highlighted-projects&#34;&gt;Highlighted Projects&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/#installation&#34;&gt;Installation&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/#install-with-docker&#34;&gt;Install with Docker&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/#install-without-docker&#34;&gt;Install locally&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/#grounded-sam-playground&#34;&gt;Grounded-SAM Playground&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/#open_book-step-by-step-notebook-demo&#34;&gt;Step-by-Step Notebook Demo&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/#running_man-groundingdino-detect-everything-with-text-prompt&#34;&gt;GroundingDINO: Detect Everything with Text Prompt&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/#running_man-grounded-sam-detect-and-segment-everything-with-text-prompt&#34;&gt;Grounded-SAM: Detect and Segment Everything with Text Prompt&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/#skier-grounded-sam-with-inpainting-detect-segment-and-generate-everything-with-text-prompt&#34;&gt;Grounded-SAM with Inpainting: Detect, Segment and Generate Everything with Text Prompt&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/#golfing-grounded-sam-and-inpaint-gradio-app&#34;&gt;Grounded-SAM and Inpaint Gradio APP&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/#label-grounded-sam-with-ram-or-tag2text-for-automatic-labeling&#34;&gt;Grounded-SAM with RAM or Tag2Text for Automatic Labeling&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/#robot-grounded-sam-with-blip-for-automatic-labeling&#34;&gt;Grounded-SAM with BLIP &amp;amp; ChatGPT for Automatic Labeling&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/#open_mouth-grounded-sam-with-whisper-detect-and-segment-anything-with-audio&#34;&gt;Grounded-SAM with Whisper: Detect and Segment Anything with Audio&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/#speech_balloon-grounded-sam-chatbot-demo&#34;&gt;Grounded-SAM ChatBot with Visual ChatGPT&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/#man_dancing-run-grounded-segment-anything--osx-demo&#34;&gt;Grounded-SAM with OSX for 3D Whole-Body Mesh Recovery&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/#man_dancing-run-grounded-segment-anything--visam-demo&#34;&gt;Grounded-SAM with VISAM for Tracking and Segment Anything&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/#dancers-interactive-editing&#34;&gt;Interactive Fashion-Edit Playground: Click for Segmentation And Editing&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/#dancers-interactive-editing&#34;&gt;Interactive Human-face Editing Playground: Click And Editing Human Face&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/#camera-3d-box-via-segment-anything&#34;&gt;3D Box Via Segment Anything&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/playground/&#34;&gt;Playground: More Interesting and Imaginative Demos with Grounded-SAM&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/playground/DeepFloyd/&#34;&gt;DeepFloyd: Image Generation with Text Prompt&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/playground/PaintByExample/&#34;&gt;PaintByExample: Exemplar-based Image Editing with Diffusion Models&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/playground/LaMa/&#34;&gt;LaMa: Resolution-robust Large Mask Inpainting with Fourier Convolutions&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/playground/RePaint/&#34;&gt;RePaint: Inpainting using Denoising Diffusion Probabilistic Models&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/playground/ImageBind_SAM/&#34;&gt;ImageBind with SAM: Segment with Different Modalities&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/EfficientSAM/&#34;&gt;Efficient SAM Series for faster labelling&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything/tree/main/EfficientSAM#run-grounded-fastsam-demo&#34;&gt;Grounded-FastSAM Demo&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything/tree/main/EfficientSAM#run-grounded-mobilesam-demo&#34;&gt;Grounded-MobileSAM Demo&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Preliminary Works&lt;/h2&gt; &#xA;&lt;p&gt;Here we provide some background knowledge that you may need to know before trying the demos.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Title&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Intro&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Description&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Links&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.02643&#34;&gt;Segment-Anything&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/facebookresearch/segment-anything/raw/main/assets/model_diagram.png?raw=true&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;A strong foundation model aims to segment everything in an image, which needs prompts (as boxes/points/text) to generate masks&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;[&lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; [&lt;a href=&#34;https://segment-anything.com/&#34;&gt;Page&lt;/a&gt;] &lt;br&gt; [&lt;a href=&#34;https://segment-anything.com/demo&#34;&gt;Demo&lt;/a&gt;]&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.05499&#34;&gt;Grounding DINO&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/IDEA-Research/GroundingDINO/raw/main/.asset/hero_figure.png?raw=True&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;A strong zero-shot detector which is capable of to generate high quality boxes and labels with free-form text.&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;[&lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; [&lt;a href=&#34;https://huggingface.co/spaces/ShilongLiu/Grounding_DINO_demo&#34;&gt;Demo&lt;/a&gt;]&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://arxiv.org/abs/2303.16160&#34;&gt;OSX&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/IDEA-Research/OSX/raw/main/assets/demo_video.gif?raw=True&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;A strong and efficient one-stage motion capture method to generate high quality 3D human mesh from monucular image. OSX also releases a large-scale upper-body dataset UBody for a more accurate reconstrution in the upper-body scene.&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;[&lt;a href=&#34;https://github.com/IDEA-Research/OSX&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; [&lt;a href=&#34;https://osx-ubody.github.io/&#34;&gt;Page&lt;/a&gt;] &lt;br&gt; [&lt;a href=&#34;https://osx-ubody.github.io/&#34;&gt;Video&lt;/a&gt;] &lt;br&gt; [&lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLSehgBP7wdn_XznGAM2AiJPiPLTqXXHw5uX9l7qeQ1Dh9HoO_A/viewform&#34;&gt;Data&lt;/a&gt;]&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.10752&#34;&gt;Stable-Diffusion&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/CompVis/stable-diffusion/raw/main/assets/stable-samples/txt2img/merged-0006.png?raw=True&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;A super powerful open-source latent text-to-image diffusion model&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;[&lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; [&lt;a href=&#34;https://ommer-lab.com/research/latent-diffusion-models/&#34;&gt;Page&lt;/a&gt;]&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://recognize-anything.github.io/&#34;&gt;RAM&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/xinyu1205/Tag2Text/raw/main/images/localization_and_recognition.jpg&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;RAM is an image tagging model, which can recognize any common category with high accuracy.&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;[&lt;a href=&#34;https://github.com/xinyu1205/Recognize_Anything-Tag2Text&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; [&lt;a href=&#34;https://huggingface.co/spaces/xinyu1205/Recognize_Anything-Tag2Text&#34;&gt;Demo&lt;/a&gt;]&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2201.12086&#34;&gt;BLIP&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/salesforce/LAVIS/raw/main/docs/_static/logo_final.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;A wonderful language-vision model for image understanding.&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;[&lt;a href=&#34;https://github.com/salesforce/LAVIS&#34;&gt;GitHub&lt;/a&gt;]&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.04671&#34;&gt;Visual ChatGPT&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/microsoft/TaskMatrix/raw/main/assets/figure.jpg&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;A wonderful tool that connects ChatGPT and a series of Visual Foundation Models to enable sending and receiving images during chatting.&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;[&lt;a href=&#34;https://github.com/microsoft/TaskMatrix&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; [&lt;a href=&#34;https://huggingface.co/spaces/microsoft/visual_chatgpt&#34;&gt;Demo&lt;/a&gt;]&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://tag2text.github.io/&#34;&gt;Tag2Text&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/xinyu1205/Tag2Text/raw/main/images/tag2text_framework.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;An efficient and controllable vision-language model which can simultaneously output superior image captioning and image tagging.&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;[&lt;a href=&#34;https://github.com/xinyu1205/Tag2Text&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; [&lt;a href=&#34;https://huggingface.co/spaces/xinyu1205/Tag2Text&#34;&gt;Demo&lt;/a&gt;]&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.11301&#34;&gt;VoxelNeXt&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/dvlab-research/VoxelNeXt/raw/master/docs/sequence-v2.gif&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;A clean, simple, and fully-sparse 3D object detector, which predicts objects directly upon sparse voxel features.&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;[&lt;a href=&#34;https://github.com/dvlab-research/VoxelNeXt&#34;&gt;Github&lt;/a&gt;]&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Highlighted Projects&lt;/h2&gt; &#xA;&lt;p&gt;Here we provide some impressive works you may find interesting:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Title&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Description&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Links&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2304.06718.pdf&#34;&gt;SEEM: Segment Everything Everywhere All at Once&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;A powerful promptable segmentation model supports segmenting with various types of prompts (text, point, scribble, referring image, etc.) and any combination of prompts.&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;[&lt;a href=&#34;https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; [&lt;a href=&#34;https://huggingface.co/spaces/xdecoder/SEEM&#34;&gt;Demo&lt;/a&gt;]&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2303.08131.pdf&#34;&gt;OpenSeeD&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;A simple framework for open-vocabulary segmentation and detection which supports interactive segmentation with box input to generate mask&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;[&lt;a href=&#34;https://github.com/IDEA-Research/OpenSeeD&#34;&gt;Github&lt;/a&gt;]&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.08485&#34;&gt;LLaVA&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;Visual instruction tuning with GPT-4&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;[&lt;a href=&#34;https://github.com/haotian-liu/LLaVA&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; [&lt;a href=&#34;https://llava-vl.github.io/&#34;&gt;Page&lt;/a&gt;] &lt;br&gt; [&lt;a href=&#34;https://llava.hliu.cc/&#34;&gt;Demo&lt;/a&gt;] &lt;br&gt; [&lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K&#34;&gt;Data&lt;/a&gt;] &lt;br&gt; [&lt;a href=&#34;https://huggingface.co/liuhaotian/LLaVA-13b-delta-v0&#34;&gt;Model&lt;/a&gt;]&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;We also list some awesome segment-anything extension projects here you may find interesting:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Computer-Vision-in-the-Wild/CVinW_Readings&#34;&gt;Computer Vision in the Wild (CVinW) Readings&lt;/a&gt; for those who are interested in open-set tasks in computer vision.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/caoyunkang/GroundedSAM-zero-shot-anomaly-detection&#34;&gt;Zero-Shot Anomaly Detection&lt;/a&gt; by Yunkang Cao&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sail-sg/EditAnything&#34;&gt;EditAnything: ControlNet + StableDiffusion based on the SAM segmentation mask&lt;/a&gt; by Shanghua Gao and Pan Zhou&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/feizc/IEA&#34;&gt;IEA: Image Editing Anything&lt;/a&gt; by Zhengcong Fei&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Li-Qingyun/sam-mmrotate&#34;&gt;SAM-MMRorate: Combining Rotated Object Detector and SAM&lt;/a&gt; by Qingyun Li and Xue Yang&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/VainF/Awesome-Anything&#34;&gt;Awesome-Anything&lt;/a&gt; by Gongfan Fang&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/RockeyCoss/Prompt-Segment-Anything&#34;&gt;Prompt-Segment-Anything&lt;/a&gt; by Rockey&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/continue-revolution/sd-webui-segment-anything&#34;&gt;WebUI for Segment-Anything and Grounded-SAM&lt;/a&gt; by Chengsong Zhang&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/geekyutao/Inpaint-Anything&#34;&gt;Inpainting Anything: Inpaint Anything with SAM + Inpainting models&lt;/a&gt; by Tao Yu&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Cheems-Seminar/segment-anything-and-name-it&#34;&gt;Grounded Segment Anything From Objects to Parts: Combining Segment-Anything with VLPart &amp;amp; GLIP &amp;amp; Visual ChatGPT&lt;/a&gt; by Peize Sun and Shoufa Chen&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/MIC-DKFZ/napari-sam&#34;&gt;Narapi-SAM: Integration of Segment Anything into Narapi (A nice viewer for SAM)&lt;/a&gt; by MIC-DKFZ&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/camenduru/grounded-segment-anything-colab&#34;&gt;Grounded Segment Anything Colab&lt;/a&gt; by camenduru&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yeungchenwa/OCR-SAM&#34;&gt;Optical Character Recognition with Segment Anything&lt;/a&gt; by Zhenhua Yang&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/showlab/Image2Paragraph&#34;&gt;Transform Image into Unique Paragraph with ChatGPT, BLIP2, OFA, GRIT, Segment Anything, ControlNet&lt;/a&gt; by showlab&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/luca-medeiros/lang-segment-anything&#34;&gt;Lang-Segment-Anything: Another awesome demo for combining GroundingDINO with Segment-Anything&lt;/a&gt; by Luca Medeiros&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/playground&#34;&gt;🥳 🚀 &lt;strong&gt;Playground: Integrate SAM and OpenMMLab!&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dvlab-research/3D-Box-Segment-Anything&#34;&gt;3D-object via Segment Anything&lt;/a&gt; by Yukang Chen&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/showlab/Image2Paragraph&#34;&gt;Image2Paragraph: Transform Image Into Unique Paragraph&lt;/a&gt; by Show Lab&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/showlab/Image2Paragraph&#34;&gt;Zero-shot Scene Graph Generate with Grounded-SAM&lt;/a&gt; by JackWhite-rwx&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/xmed-lab/CLIP_Surgery&#34;&gt;CLIP Surgery for Better Explainability with Enhancement in Open-Vocabulary Tasks&lt;/a&gt; by Eli-YiLi&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/segments-ai/panoptic-segment-anything&#34;&gt;Panoptic-Segment-Anything: Zero-shot panoptic segmentation using SAM&lt;/a&gt; by segments-ai&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ttengwang/Caption-Anything&#34;&gt;Caption-Anything: Generates Descriptive Captions for Any Object within an Image&lt;/a&gt; by Teng Wang&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Pointcept/SegmentAnything3D&#34;&gt;Segment-Anything-3D: Transferring Segmentation Information of 2D Images to 3D Space&lt;/a&gt; by Yunhan Yang&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Expedit-LargeScale-Vision-Transformer/Expedit-SAM&#34;&gt;Expediting SAM without Fine-tuning&lt;/a&gt; by Weicong Liang and Yuhui Yuan&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/fudan-zvg/Semantic-Segment-Anything&#34;&gt;Semantic Segment Anything: Providing Rich Semantic Category Annotations for SAM&lt;/a&gt; by Jiaqi Chen and Zeyu Yang and Li Zhang&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lixinustc/Enhance-Anything&#34;&gt;Enhance Everything: Combining SAM with Image Restoration and Enhancement Tasks&lt;/a&gt; by Xin Li&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Zeqiang-Lai/DragGAN&#34;&gt;DragGAN&lt;/a&gt; by Shanghai AI Lab.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;The code requires &lt;code&gt;python&amp;gt;=3.8&lt;/code&gt;, as well as &lt;code&gt;pytorch&amp;gt;=1.7&lt;/code&gt; and &lt;code&gt;torchvision&amp;gt;=0.8&lt;/code&gt;. Please follow the instructions &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;here&lt;/a&gt; to install both PyTorch and TorchVision dependencies. Installing both PyTorch and TorchVision with CUDA support is strongly recommended.&lt;/p&gt; &#xA;&lt;h3&gt;Install with Docker&lt;/h3&gt; &#xA;&lt;p&gt;Open one terminal:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make build-image&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;make run&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;That&#39;s it.&lt;/p&gt; &#xA;&lt;p&gt;If you would like to allow visualization across docker container, open another terminal and type:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;xhost +&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Install without Docker&lt;/h3&gt; &#xA;&lt;p&gt;You should set the environment variable manually as follows if you want to build a local GPU environment for Grounded-SAM:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export AM_I_DOCKER=False&#xA;export BUILD_WITH_CUDA=True&#xA;export CUDA_HOME=/path/to/cuda-11.3/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install Segment Anything:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m pip install -e segment_anything&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install Grounding DINO:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m pip install -e GroundingDINO&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install diffusers:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install --upgrade diffusers[torch]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install osx:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git submodule update --init --recursive&#xA;cd grounded-sam-osx &amp;amp;&amp;amp; bash install.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install RAM &amp;amp; Tag2Text:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git submodule update --init --recursive&#xA;cd Tag2Text &amp;amp;&amp;amp; pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The following optional dependencies are necessary for mask post-processing, saving masks in COCO format, the example notebooks, and exporting the model in ONNX format. &lt;code&gt;jupyter&lt;/code&gt; is also required to run the example notebooks.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install opencv-python pycocotools matplotlib onnxruntime onnx ipykernel&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;More details can be found in &lt;a href=&#34;https://github.com/facebookresearch/segment-anything#installation&#34;&gt;install segment anything&lt;/a&gt; and &lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO#install&#34;&gt;install GroundingDINO&lt;/a&gt; and &lt;a href=&#34;https://github.com/IDEA-Research/OSX&#34;&gt;install OSX&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Grounded-SAM Playground&lt;/h2&gt; &#xA;&lt;p&gt;Let&#39;s start exploring our Grounding-SAM Playground and we will release more interesting demos in the future, stay tuned!&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;📖&lt;/span&gt; Step-by-Step Notebook Demo&lt;/h2&gt; &#xA;&lt;p&gt;Here we list some notebook demo provided in this project:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/grounded_sam.ipynb&#34;&gt;grounded_sam.ipynb&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/grounded_sam_colab_demo.ipynb&#34;&gt;grounded_sam_colab_demo.ipynb&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/grounded_sam_3d_box&#34;&gt;grounded_sam_3d_box.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;&lt;span&gt;🏃♂&lt;/span&gt; GroundingDINO: Detect Everything with Text Prompt&lt;/h3&gt; &#xA;&lt;p&gt;&lt;span&gt;🍇&lt;/span&gt; [&lt;a href=&#34;https://arxiv.org/abs/2303.05499&#34;&gt;arXiv Paper&lt;/a&gt;] &amp;nbsp; &lt;span&gt;🌹&lt;/span&gt;[&lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-grounding-dino.ipynb&#34;&gt;Try the Colab Demo&lt;/a&gt;] &amp;nbsp; &lt;span&gt;🌻&lt;/span&gt; [&lt;a href=&#34;https://huggingface.co/spaces/ShilongLiu/Grounding_DINO_demo&#34;&gt;Try Huggingface Demo&lt;/a&gt;] &amp;nbsp; &lt;span&gt;🍄&lt;/span&gt; [&lt;a href=&#34;https://youtu.be/C4NqaRBz_Kw&#34;&gt;Automated Dataset Annotation and Evaluation&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;Here&#39;s the step-by-step tutorial on running &lt;code&gt;GroundingDINO&lt;/code&gt; demo:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step 1: Download the pretrained weights&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd Grounded-Segment-Anything&#xA;&#xA;# download the pretrained groundingdino-swin-tiny model&#xA;wget https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step 2: Running the demo&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python grounding_dino_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; &lt;b&gt; Running with Python (same as demo but you can run it anywhere after installing GroundingDINO) &lt;/b&gt; &lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from groundingdino.util.inference import load_model, load_image, predict, annotate&#xA;import cv2&#xA;&#xA;model = load_model(&#34;GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py&#34;, &#34;./groundingdino_swint_ogc.pth&#34;)&#xA;IMAGE_PATH = &#34;assets/demo1.jpg&#34;&#xA;TEXT_PROMPT = &#34;bear.&#34;&#xA;BOX_THRESHOLD = 0.35&#xA;TEXT_THRESHOLD = 0.25&#xA;&#xA;image_source, image = load_image(IMAGE_PATH)&#xA;&#xA;boxes, logits, phrases = predict(&#xA;    model=model,&#xA;    image=image,&#xA;    caption=TEXT_PROMPT,&#xA;    box_threshold=BOX_THRESHOLD,&#xA;    text_threshold=TEXT_THRESHOLD&#xA;)&#xA;&#xA;annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)&#xA;cv2.imwrite(&#34;annotated_image.jpg&#34;, annotated_frame)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tips&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you want to detect multiple objects in one sentence with &lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO&#34;&gt;Grounding DINO&lt;/a&gt;, we suggest separating each name with &lt;code&gt;.&lt;/code&gt; . An example: &lt;code&gt;cat . dog . chair .&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step 3: Check the annotated image&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The annotated image will be saved as &lt;code&gt;./annotated_image.jpg&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Text Prompt&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Demo Image&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Annotated Image&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;code&gt;Bear.&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/assets/demo1.jpg&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/assets/annotated_image.jpg&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;code&gt;Horse. Clouds. Grasses. Sky. Hill&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/assets/demo7.jpg&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/IDEA-Research/detrex-storage/raw/main/assets/grounded_sam/grounding_dino/groundingdino_demo7.jpg?raw=true&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;&lt;span&gt;🏃♂&lt;/span&gt; Grounded-SAM: Detect and Segment Everything with Text Prompt&lt;/h3&gt; &#xA;&lt;p&gt;Here&#39;s the step-by-step tutorial on running &lt;code&gt;Grounded-SAM&lt;/code&gt; demo:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step 1: Download the pretrained weights&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd Grounded-Segment-Anything&#xA;&#xA;wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth&#xA;wget https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We provide two versions of Grounded-SAM demo here:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/grounded_sam_demo.py&#34;&gt;grounded_sam_demo.py&lt;/a&gt;: our original implementation for Grounded-SAM.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/grounded_sam_simple_demo.py&#34;&gt;grounded_sam_simple_demo.py&lt;/a&gt; our updated more elegant version for Grounded-SAM.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step 2: Running original grounded-sam demo&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;export CUDA_VISIBLE_DEVICES=0&#xA;python grounded_sam_demo.py \&#xA;  --config GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py \&#xA;  --grounded_checkpoint groundingdino_swint_ogc.pth \&#xA;  --sam_checkpoint sam_vit_h_4b8939.pth \&#xA;  --input_image assets/demo1.jpg \&#xA;  --output_dir &#34;outputs&#34; \&#xA;  --box_threshold 0.3 \&#xA;  --text_threshold 0.25 \&#xA;  --text_prompt &#34;bear&#34; \&#xA;  --device &#34;cuda&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The annotated results will be saved in &lt;code&gt;./outputs&lt;/code&gt; as follows&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Input Image&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Annotated Image&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Generated Mask&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/assets/demo1.jpg&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/IDEA-Research/detrex-storage/raw/main/assets/grounded_sam/grounded_sam/original_grounded_sam_demo1.jpg?raw=true&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/IDEA-Research/detrex-storage/raw/main/assets/grounded_sam/grounded_sam/mask.jpg?raw=true&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step 3: Running grounded-sam demo with sam-hq&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download the demo image&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget https://github.com/IDEA-Research/detrex-storage/releases/download/grounded-sam-storage/sam_hq_demo_image.png&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Download SAM-HQ checkpoint &lt;a href=&#34;https://github.com/SysCV/sam-hq#model-checkpoints&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Running grounded-sam-hq demo as follows:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;export CUDA_VISIBLE_DEVICES=0&#xA;python grounded_sam_demo.py \&#xA;  --config GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py \&#xA;  --grounded_checkpoint groundingdino_swint_ogc.pth \&#xA;  --sam_hq_checkpoint ./sam_hq_vit_h.pth \  # path to sam-hq checkpoint&#xA;  --use_sam_hq \  # set to use sam-hq model&#xA;  --input_image sam_hq_demo_image.png \&#xA;  --output_dir &#34;outputs&#34; \&#xA;  --box_threshold 0.3 \&#xA;  --text_threshold 0.25 \&#xA;  --text_prompt &#34;chair.&#34; \&#xA;  --device &#34;cuda&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The annotated results will be saved in &lt;code&gt;./outputs&lt;/code&gt; as follows&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Input Image&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;SAM Output&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;SAM-HQ Output&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/IDEA-Research/detrex-storage/raw/main/assets/grounded_sam/sam_hq/sam_hq_demo.png?raw=true&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/IDEA-Research/detrex-storage/raw/main/assets/grounded_sam/sam_hq/sam_output.jpg?raw=true&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/IDEA-Research/detrex-storage/raw/main/assets/grounded_sam/sam_hq/sam_hq_output.jpg?raw=true&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step 4: Running the updated grounded-sam demo (optional)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Note that this demo is almost same as the original demo, but &lt;strong&gt;with more elegant code&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python grounded_sam_simple_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The annotated results will be saved as &lt;code&gt;./groundingdino_annotated_image.jpg&lt;/code&gt; and &lt;code&gt;./grounded_sam_annotated_image.jpg&lt;/code&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Text Prompt&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Input Image&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;GroundingDINO Annotated Image&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Grounded-SAM Annotated Image&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;code&gt;The running dog&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/assets/demo2.jpg&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/IDEA-Research/detrex-storage/raw/main/assets/grounded_sam/grounded_sam/groundingdino_annotated_image_demo2.jpg?raw=true&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/IDEA-Research/detrex-storage/raw/main/assets/grounded_sam/grounded_sam/grounded_sam_annotated_image_demo2.jpg?raw=true&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;code&gt;Horse. Clouds. Grasses. Sky. Hill&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/assets/demo7.jpg&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/assets/groundingdino_annotated_image.jpg&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/assets/grounded_sam_annotated_image.jpg&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;&lt;span&gt;⛷&lt;/span&gt; Grounded-SAM with Inpainting: Detect, Segment and Generate Everything with Text Prompt&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step 1: Download the pretrained weights&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd Grounded-Segment-Anything&#xA;&#xA;wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth&#xA;wget https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step 2: Running grounded-sam inpainting demo&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_VISIBLE_DEVICES=0&#xA;python grounded_sam_inpainting_demo.py \&#xA;  --config GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py \&#xA;  --grounded_checkpoint groundingdino_swint_ogc.pth \&#xA;  --sam_checkpoint sam_vit_h_4b8939.pth \&#xA;  --input_image assets/inpaint_demo.jpg \&#xA;  --output_dir &#34;outputs&#34; \&#xA;  --box_threshold 0.3 \&#xA;  --text_threshold 0.25 \&#xA;  --det_prompt &#34;bench&#34; \&#xA;  --inpaint_prompt &#34;A sofa, high quality, detailed&#34; \&#xA;  --device &#34;cuda&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The annotated and inpaint image will be saved in &lt;code&gt;./outputs&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step 3: Check the results&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Input Image&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Det Prompt&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Annotated Image&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Inpaint Prompt&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Inpaint Image&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/assets/inpaint_demo.jpg&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;code&gt;Bench&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/IDEA-Research/detrex-storage/raw/main/assets/grounded_sam/grounded_sam_inpaint/grounded_sam_output.jpg?raw=true&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;code&gt;A sofa, high quality, detailed&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/IDEA-Research/detrex-storage/raw/main/assets/grounded_sam/grounded_sam_inpaint/grounded_sam_inpainting_output.jpg?raw=true&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;&lt;span&gt;🏌&lt;/span&gt; Grounded-SAM and Inpaint Gradio APP&lt;/h3&gt; &#xA;&lt;p&gt;We support 6 tasks in the local Gradio APP：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;scribble&lt;/strong&gt;: Segmentation is achieved through Segment Anything and mouse click interaction (you need to click on the object with the mouse, no need to specify the prompt).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;automask&lt;/strong&gt;: Segment the entire image at once through Segment Anything (no need to specify a prompt).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;det&lt;/strong&gt;: Realize detection through Grounding DINO and text interaction (text prompt needs to be specified).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;seg&lt;/strong&gt;: Realize text interaction by combining Grounding DINO and Segment Anything to realize detection + segmentation (need to specify text prompt).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;inpainting&lt;/strong&gt;: By combining Grounding DINO + Segment Anything + Stable Diffusion to achieve text exchange and replace the target object (need to specify text prompt and inpaint prompt) .&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;automatic&lt;/strong&gt;: By combining BLIP + Grounding DINO + Segment Anything to achieve non-interactive detection + segmentation (no need to specify prompt).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python gradio_app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The gradio_app visualization as follows:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/assets/gradio_demo.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;span&gt;🏷&lt;/span&gt; Grounded-SAM with RAM or Tag2Text for Automatic Labeling&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/xinyu1205/Recognize_Anything-Tag2Text&#34;&gt;&lt;strong&gt;The Recognize Anything Model (RAM) and Tag2Text&lt;/strong&gt;&lt;/a&gt; exhibits &lt;strong&gt;exceptional recognition abilities&lt;/strong&gt;, in terms of &lt;strong&gt;both accuracy and scope&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;It is seamlessly linked to generate pseudo labels automatically as follows:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Use RAM/Tag2Text to generate tags.&lt;/li&gt; &#xA; &lt;li&gt;Use Grounded-Segment-Anything to generate the boxes and masks.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step 1: Init submodule and download the pretrained checkpoint&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Init submodule:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd Grounded-Segment-Anything&#xA;git submodule init&#xA;git submodule update&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download pretrained weights for &lt;code&gt;GroundingDINO&lt;/code&gt;, &lt;code&gt;SAM&lt;/code&gt; and &lt;code&gt;RAM/Tag2Text&lt;/code&gt;:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth&#xA;wget https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth&#xA;&#xA;cd Tag2Text&#xA;wget https://huggingface.co/spaces/xinyu1205/Tag2Text/resolve/main/ram_swin_large_14m.pth&#xA;wget https://huggingface.co/spaces/xinyu1205/Tag2Text/resolve/main/tag2text_swin_14m.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step 2: Running the demo with RAM&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CUDA_VISIBLE_DEVICES=0&#xA;python automatic_label_ram_demo.py \&#xA;  --config GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py \&#xA;  --ram_checkpoint ./Tag2Text/ram_swin_large_14m.pth \&#xA;  --grounded_checkpoint groundingdino_swint_ogc.pth \&#xA;  --sam_checkpoint sam_vit_h_4b8939.pth \&#xA;  --input_image assets/demo9.jpg \&#xA;  --output_dir &#34;outputs&#34; \&#xA;  --box_threshold 0.25 \&#xA;  --text_threshold 0.2 \&#xA;  --iou_threshold 0.5 \&#xA;  --device &#34;cuda&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step 2: Or Running the demo with Tag2Text&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CUDA_VISIBLE_DEVICES=0&#xA;python automatic_label_tag2text_demo.py \&#xA;  --config GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py \&#xA;  --tag2text_checkpoint ./Tag2Text/tag2text_swin_14m.pth \&#xA;  --grounded_checkpoint groundingdino_swint_ogc.pth \&#xA;  --sam_checkpoint sam_vit_h_4b8939.pth \&#xA;  --input_image assets/demo9.jpg \&#xA;  --output_dir &#34;outputs&#34; \&#xA;  --box_threshold 0.25 \&#xA;  --text_threshold 0.2 \&#xA;  --iou_threshold 0.5 \&#xA;  --device &#34;cuda&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Tag2Text also provides powerful captioning capabilities, and the process with captions can refer to &lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/#robot-run-grounded-segment-anything--blip-demo&#34;&gt;BLIP&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The pseudo labels and model prediction visualization will be saved in &lt;code&gt;output_dir&lt;/code&gt; as follows (right figure):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/assets/automatic_label_output/demo9_tag2text_ram.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;span&gt;🤖&lt;/span&gt; Grounded-SAM with BLIP for Automatic Labeling&lt;/h3&gt; &#xA;&lt;p&gt;It is easy to generate pseudo labels automatically as follows:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Use BLIP (or other caption models) to generate a caption.&lt;/li&gt; &#xA; &lt;li&gt;Extract tags from the caption. We use ChatGPT to handle the potential complicated sentences.&lt;/li&gt; &#xA; &lt;li&gt;Use Grounded-Segment-Anything to generate the boxes and masks.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Run Demo&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export OPENAI_API_KEY=your_openai_key&#xA;export OPENAI_API_BASE=https://closeai.deno.dev/v1&#xA;export CUDA_VISIBLE_DEVICES=0&#xA;python automatic_label_demo.py \&#xA;  --config GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py \&#xA;  --grounded_checkpoint groundingdino_swint_ogc.pth \&#xA;  --sam_checkpoint sam_vit_h_4b8939.pth \&#xA;  --input_image assets/demo3.jpg \&#xA;  --output_dir &#34;outputs&#34; \&#xA;  --openai_key $OPENAI_API_KEY \&#xA;  --box_threshold 0.25 \&#xA;  --text_threshold 0.2 \&#xA;  --iou_threshold 0.5 \&#xA;  --device &#34;cuda&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;When you don&#39;t have a paid Account for ChatGPT is also possible to use NLTK instead. Just don&#39;t include the &lt;code&gt;openai_key&lt;/code&gt; Parameter when starting the Demo. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The Script will automatically download the necessary NLTK Data.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;The pseudo labels and model prediction visualization will be saved in &lt;code&gt;output_dir&lt;/code&gt; as follows:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/assets/automatic_label_output_demo3.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;span&gt;😮&lt;/span&gt; Grounded-SAM with Whisper: Detect and Segment Anything with Audio&lt;/h3&gt; &#xA;&lt;p&gt;Detect and segment anything with speech!&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/assets/acoustics/gsam_whisper_inpainting_demo.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Install Whisper&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -U openai-whisper&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://github.com/openai/whisper#setup&#34;&gt;whisper official page&lt;/a&gt; if you have other questions for the installation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Run Voice-to-Label Demo&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Optional: Download the demo audio file&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget https://huggingface.co/ShilongLiu/GroundingDINO/resolve/main/demo_audio.mp3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CUDA_VISIBLE_DEVICES=0&#xA;python grounded_sam_whisper_demo.py \&#xA;  --config GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py \&#xA;  --grounded_checkpoint groundingdino_swint_ogc.pth \&#xA;  --sam_checkpoint sam_vit_h_4b8939.pth \&#xA;  --input_image assets/demo4.jpg \&#xA;  --output_dir &#34;outputs&#34; \&#xA;  --box_threshold 0.3 \&#xA;  --text_threshold 0.25 \&#xA;  --speech_file &#34;demo_audio.mp3&#34; \&#xA;  --device &#34;cuda&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/assets/grounded_sam_whisper_output.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Run Voice-to-inpaint Demo&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can enable chatgpt to help you automatically detect the object and inpainting order with &lt;code&gt;--enable_chatgpt&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Or you can specify the object you want to inpaint [stored in &lt;code&gt;args.det_speech_file&lt;/code&gt;] and the text you want to inpaint with [stored in &lt;code&gt;args.inpaint_speech_file&lt;/code&gt;].&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export OPENAI_API_KEY=your_openai_key&#xA;export OPENAI_API_BASE=https://closeai.deno.dev/v1&#xA;# Example: enable chatgpt&#xA;export CUDA_VISIBLE_DEVICES=0&#xA;python grounded_sam_whisper_inpainting_demo.py \&#xA;  --config GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py \&#xA;  --grounded_checkpoint groundingdino_swint_ogc.pth \&#xA;  --sam_checkpoint sam_vit_h_4b8939.pth \&#xA;  --input_image assets/inpaint_demo.jpg \&#xA;  --output_dir &#34;outputs&#34; \&#xA;  --box_threshold 0.3 \&#xA;  --text_threshold 0.25 \&#xA;  --prompt_speech_file assets/acoustics/prompt_speech_file.mp3 \&#xA;  --enable_chatgpt \&#xA;  --openai_key $OPENAI_API_KEY\&#xA;  --device &#34;cuda&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Example: without chatgpt&#xA;export CUDA_VISIBLE_DEVICES=0&#xA;python grounded_sam_whisper_inpainting_demo.py \&#xA;  --config GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py \&#xA;  --grounded_checkpoint groundingdino_swint_ogc.pth \&#xA;  --sam_checkpoint sam_vit_h_4b8939.pth \&#xA;  --input_image assets/inpaint_demo.jpg \&#xA;  --output_dir &#34;outputs&#34; \&#xA;  --box_threshold 0.3 \&#xA;  --text_threshold 0.25 \&#xA;  --det_speech_file &#34;assets/acoustics/det_voice.mp3&#34; \&#xA;  --inpaint_speech_file &#34;assets/acoustics/inpaint_voice.mp3&#34; \&#xA;  --device &#34;cuda&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/assets/acoustics/gsam_whisper_inpainting_pipeline.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;span&gt;💬&lt;/span&gt; Grounded-SAM ChatBot Demo&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/24236723/231955561-2ae4ec1a-c75f-4cc5-9b7b-517aa1432123.mp4&#34;&gt;https://user-images.githubusercontent.com/24236723/231955561-2ae4ec1a-c75f-4cc5-9b7b-517aa1432123.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Following &lt;a href=&#34;https://github.com/microsoft/visual-chatgpt&#34;&gt;Visual ChatGPT&lt;/a&gt;, we add a ChatBot for our project. Currently, it supports:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&#34;Describe the image.&#34;&lt;/li&gt; &#xA; &lt;li&gt;&#34;Detect the dog (and the cat) in the image.&#34;&lt;/li&gt; &#xA; &lt;li&gt;&#34;Segment anything in the image.&#34;&lt;/li&gt; &#xA; &lt;li&gt;&#34;Segment the dog (and the cat) in the image.&#34;&lt;/li&gt; &#xA; &lt;li&gt;&#34;Help me label the image.&#34;&lt;/li&gt; &#xA; &lt;li&gt;&#34;Replace the dog with a cat in the image.&#34;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;To use the ChatBot:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install whisper if you want to use audio as input.&lt;/li&gt; &#xA; &lt;li&gt;Set the default model setting in the tool &lt;code&gt;Grounded_dino_sam_inpainting&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Run Demo&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export OPENAI_API_KEY=your_openai_key&#xA;export OPENAI_API_BASE=https://closeai.deno.dev/v1&#xA;export CUDA_VISIBLE_DEVICES=0&#xA;python chatbot.py &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;span&gt;🕺&lt;/span&gt; Run Grounded-Segment-Anything + OSX Demo&lt;/h3&gt; &#xA;&lt;p align=&#34;middle&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/assets/osx/grouned_sam_osx_demo.gif&#34;&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Download the checkpoint &lt;code&gt;osx_l_wo_decoder.pth.tar&lt;/code&gt; from &lt;a href=&#34;https://drive.google.com/drive/folders/1x7MZbB6eAlrq5PKC9MaeIm4GqkBpokow?usp=share_link&#34;&gt;here&lt;/a&gt; for OSX:&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Download the human model files and place it into &lt;code&gt;grounded-sam-osx/utils/human_model_files&lt;/code&gt; following the instruction of &lt;a href=&#34;https://github.com/IDEA-Research/OSX&#34;&gt;OSX&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run Demo&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export CUDA_VISIBLE_DEVICES=0&#xA;python grounded_sam_osx_demo.py \&#xA;  --config GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py \&#xA;  --grounded_checkpoint groundingdino_swint_ogc.pth \&#xA;  --sam_checkpoint sam_vit_h_4b8939.pth \&#xA;  --osx_checkpoint osx_l_wo_decoder.pth.tar \&#xA;  --input_image assets/osx/grounded_sam_osx_demo.png \&#xA;  --output_dir &#34;outputs&#34; \&#xA;  --box_threshold 0.3 \&#xA;  --text_threshold 0.25 \&#xA;  --text_prompt &#34;humans, chairs&#34; \&#xA;  --device &#34;cuda&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The model prediction visualization will be saved in &lt;code&gt;output_dir&lt;/code&gt; as follows:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/assets/osx/grounded_sam_osx_output.jpg&#34; style=&#34;zoom: 49%;&#34;&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We also support promptable 3D whole-body mesh recovery. For example, you can track someone with a text prompt and estimate his 3D pose and shape :&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/assets/osx/grounded_sam_osx_output1.jpg&#34; alt=&#34;space-1.jpg&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;A person with pink clothes&lt;/em&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/assets/osx/grounded_sam_osx_output2.jpg&#34; alt=&#34;space-1.jpg&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;A man with a sunglasses&lt;/em&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;&lt;span&gt;🕺&lt;/span&gt; Run Grounded-Segment-Anything + VISAM Demo&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Download the checkpoint &lt;code&gt;motrv2_dancetrack.pth&lt;/code&gt; from &lt;a href=&#34;https://drive.google.com/file/d/1EA4lndu2yQcVgBKR09KfMe5efbf631Th/view?usp=share_link&#34;&gt;here&lt;/a&gt; for MOTRv2:&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;See the more thing if you have other questions for the installation.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run Demo&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export CUDA_VISIBLE_DEVICES=0&#xA;python grounded_sam_visam.py \&#xA;  --meta_arch motr \&#xA;  --dataset_file e2e_dance \&#xA;  --with_box_refine \&#xA;  --query_interaction_layer QIMv2 \&#xA;  --num_queries 10 \&#xA;  --det_db det_db_motrv2.json \&#xA;  --use_checkpoint \&#xA;  --mot_path your_data_path \&#xA;  --resume motrv2_dancetrack.pth \&#xA;  --sam_checkpoint sam_vit_h_4b8939.pth \&#xA;  --video_path DanceTrack/test/dancetrack0003 &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;|&lt;img src=&#34;https://raw.githubusercontent.com/BingfengYan/MOTSAM/main/visam.gif&#34; alt=&#34;&#34;&gt;|&lt;/p&gt; &#xA;&lt;h3&gt;&lt;span&gt;👯&lt;/span&gt; Interactive Editing&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Release the interactive fashion-edit playground in &lt;a href=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything/tree/humanFace&#34;&gt;here&lt;/a&gt;. Run in the notebook, just click for annotating points for further segmentation. Enjoy it!&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything/raw/humanFace/assets/interactive-fashion-edit.png&#34; width=&#34;500&#34; height=&#34;260&#34;&gt;&lt;img src=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything/raw/humanFace/assets/interactive-mark.gif&#34; width=&#34;250&#34; height=&#34;250&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Release human-face-edit branch &lt;a href=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything/tree/humanFace&#34;&gt;here&lt;/a&gt;. We&#39;ll keep updating this branch with more interesting features. Here are some examples:&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything/raw/humanFace/assets/231-hair-edit.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;span&gt;📷&lt;/span&gt; 3D-Box via Segment Anything&lt;/h2&gt; &#xA;&lt;p&gt;We extend the scope to 3D world by combining Segment Anything and &lt;a href=&#34;https://github.com/dvlab-research/VoxelNeXt&#34;&gt;VoxelNeXt&lt;/a&gt;. When we provide a prompt (e.g., a point / box), the result is not only 2D segmentation mask, but also 3D boxes. Please check &lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-Segment-Anything/main/voxelnext_3d_box/&#34;&gt;voxelnext_3d_box&lt;/a&gt; for more details. &lt;img src=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything/raw/main/voxelnext_3d_box/images/sam-voxelnext.png&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything/raw/main/voxelnext_3d_box/images/image_boxes2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;💘&lt;/span&gt; Acknowledgements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;Segment Anything&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO&#34;&gt;Grounding DINO&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;p&gt;Our project wouldn&#39;t be possible without the contributions of these amazing people! Thank you all for making this project better.&lt;/p&gt; &#xA;&lt;a href=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=IDEA-Research/Grounded-Segment-Anything&#34;&gt; &lt;/a&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this project helpful for your research, please consider citing the following BibTeX entry.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-BibTex&#34;&gt;@article{kirillov2023segany,&#xA;  title={Segment Anything}, &#xA;  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Doll{\&#39;a}r, Piotr and Girshick, Ross},&#xA;  journal={arXiv:2304.02643},&#xA;  year={2023}&#xA;}&#xA;&#xA;@article{liu2023grounding,&#xA;  title={Grounding dino: Marrying dino with grounded pre-training for open-set object detection},&#xA;  author={Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Li, Chunyuan and Yang, Jianwei and Su, Hang and Zhu, Jun and others},&#xA;  journal={arXiv preprint arXiv:2303.05499},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>