<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-10-02T01:44:14Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>XavierXiao/Dreambooth-Stable-Diffusion</title>
    <updated>2022-10-02T01:44:14Z</updated>
    <id>tag:github.com,2022-10-02:/XavierXiao/Dreambooth-Stable-Diffusion</id>
    <link href="https://github.com/XavierXiao/Dreambooth-Stable-Diffusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Implementation of Dreambooth (https://arxiv.org/abs/2208.12242) with Stable Diffusion&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Dreambooth on Stable Diffusion&lt;/h1&gt; &#xA;&lt;p&gt;This is an implementtaion of Google&#39;s &lt;a href=&#34;https://arxiv.org/abs/2208.12242&#34;&gt;Dreambooth&lt;/a&gt; with &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;Stable Diffusion&lt;/a&gt;. The original Dreambooth is based on &lt;a href=&#34;https://imagen.research.google/&#34;&gt;Imagen&lt;/a&gt; text-to-image model. However, neither the model nor the pre-trained weights of Imagen is available. To enable people to fine-tune a text-to-image model with a few examples, I implemented the idea of Dreambooth on Stable diffusion.&lt;/p&gt; &#xA;&lt;p&gt;This code repository is based on that of &lt;a href=&#34;https://github.com/rinongal/textual_inversion&#34;&gt;Textual Inversion&lt;/a&gt;. Note that Textual Inversion only optimizes word ebedding, while dreambooth fine-tunes the whole diffusion model.&lt;/p&gt; &#xA;&lt;p&gt;The implementation makes minimum changes over the official codebase of Textual Inversion. In fact, due to lazyness, some components in Textual Inversion, such as the embedding manager, are not deleted, although they will never be used here.&lt;/p&gt; &#xA;&lt;h2&gt;Update&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;9/20/2022&lt;/strong&gt;: I just found a way to reduce the GPU memory a bit. Remember that this code is based on Textual Inversion, and TI&#39;s code base has &lt;a href=&#34;https://github.com/rinongal/textual_inversion/raw/main/ldm/modules/diffusionmodules/util.py#L112&#34;&gt;this line&lt;/a&gt;, which disable gradient checkpointing in a hard-code way. This is because in TI, the Unet is not optimized. However, in Dreambooth we optimize the Unet, so we can turn on the gradient checkpoint pointing trick, as in the original SD repo &lt;a href=&#34;https://github.com/CompVis/stable-diffusion/raw/main/ldm/modules/diffusionmodules/util.py#L112&#34;&gt;here&lt;/a&gt;. The gradient checkpoint is default to be True in &lt;a href=&#34;https://github.com/XavierXiao/Dreambooth-Stable-Diffusion/raw/main/configs/stable-diffusion/v1-finetune_unfrozen.yaml#L47&#34;&gt;config&lt;/a&gt;. I have updated the codes.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Preparation&lt;/h3&gt; &#xA;&lt;p&gt;First set-up the &lt;code&gt;ldm&lt;/code&gt; enviroment following the instruction from textual inversion repo, or the original Stable Diffusion repo.&lt;/p&gt; &#xA;&lt;p&gt;To fine-tune a stable diffusion model, you need to obtain the pre-trained stable diffusion models following their &lt;a href=&#34;https://github.com/CompVis/stable-diffusion#stable-diffusion-v1&#34;&gt;instructions&lt;/a&gt;. Weights can be downloaded on &lt;a href=&#34;https://huggingface.co/CompVis&#34;&gt;HuggingFace&lt;/a&gt;. You can decide which version of checkpoint to use, but I use &lt;code&gt;sd-v1-4-full-ema.ckpt&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We also need to create a set of images for regularization, as the fine-tuning algorithm of Dreambooth requires that. Details of the algorithm can be found in the paper. Note that in the original paper, the regularization images seem to be generated on-the-fly. However, here I generated a set of regularization images before the training. The text prompt for generating regularization images can be &lt;code&gt;photo of a &amp;lt;class&amp;gt;&lt;/code&gt;, where &lt;code&gt;&amp;lt;class&amp;gt;&lt;/code&gt; is a word that describes the class of your object, such as &lt;code&gt;dog&lt;/code&gt;. The command is&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/stable_txt2img.py --ddim_eta 0.0 --n_samples 8 --n_iter 1 --scale 10.0 --ddim_steps 50  --ckpt /path/to/original/stable-diffusion/sd-v1-4-full-ema.ckpt --prompt &#34;a photo of a &amp;lt;class&amp;gt;&#34; &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;I generate 8 images for regularization, but more regularization images may lead to stronger regularization and better editability. After that, save the generated images (separately, one image per &lt;code&gt;.png&lt;/code&gt; file) at &lt;code&gt;/root/to/regularization/images&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Updates on 9/9&lt;/strong&gt; We should definitely use more images for regularization. Please try 100 or 200, to better align with the original paper. To acomodate this, I shorten the &#34;repeat&#34; of reg dataset in the &lt;a href=&#34;https://github.com/XavierXiao/Dreambooth-Stable-Diffusion/raw/main/configs/stable-diffusion/v1-finetune_unfrozen.yaml#L96&#34;&gt;config file&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For some cases, if the generated regularization images are highly unrealistic (happens when you want to generate &#34;man&#34; or &#34;woman&#34;), you can find a diverse set of images (of man/woman) online, and use them as regularization images.&lt;/p&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;p&gt;Training can be done by running the following command&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python main.py --base configs/stable-diffusion/v1-finetune_unfrozen.yaml &#xA;                -t &#xA;                --actual_resume /path/to/original/stable-diffusion/sd-v1-4-full-ema.ckpt  &#xA;                -n &amp;lt;job name&amp;gt; &#xA;                --gpus 0, &#xA;                --data_root /root/to/training/images &#xA;                --reg_data_root /root/to/regularization/images &#xA;                --class_word &amp;lt;xxx&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Detailed configuration can be found in &lt;code&gt;configs/stable-diffusion/v1-finetune_unfrozen.yaml&lt;/code&gt;. In particular, the default learning rate is &lt;code&gt;1.0e-6&lt;/code&gt; as I found the &lt;code&gt;1.0e-5&lt;/code&gt; in the Dreambooth paper leads to poor editability. The parameter &lt;code&gt;reg_weight&lt;/code&gt; corresponds to the weight of regularization in the Dreambooth paper, and the default is set to &lt;code&gt;1.0&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Dreambooth requires a placeholder word &lt;code&gt;[V]&lt;/code&gt;, called identifier, as in the paper. This identifier needs to be a relatively rare tokens in the vocabulary. The original paper approaches this by using a rare word in T5-XXL tokenizer. For simplicity, here I just use a random word &lt;code&gt;sks&lt;/code&gt; and hard coded it.. If you want to change that, simply make a change in &lt;a href=&#34;https://github.com/XavierXiao/Dreambooth-Stable-Diffusion/raw/main/ldm/data/personalized.py#L10&#34;&gt;this file&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Training will be run for 800 steps, and two checkpoints will be saved at &lt;code&gt;./logs/&amp;lt;job_name&amp;gt;/checkpoints&lt;/code&gt;, one at 500 steps and one at final step. Typically the one at 500 steps works well enough. I train the model use two A6000 GPUs and it takes ~15 mins.&lt;/p&gt; &#xA;&lt;h3&gt;Generation&lt;/h3&gt; &#xA;&lt;p&gt;After training, personalized samples can be obtained by running the command&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/stable_txt2img.py --ddim_eta 0.0 &#xA;                                 --n_samples 8 &#xA;                                 --n_iter 1 &#xA;                                 --scale 10.0 &#xA;                                 --ddim_steps 100  &#xA;                                 --ckpt /path/to/saved/checkpoint/from/training&#xA;                                 --prompt &#34;photo of a sks &amp;lt;class&amp;gt;&#34; &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In particular, &lt;code&gt;sks&lt;/code&gt; is the identifier, which should be replaced by your choice if you happen to change the identifier, and &lt;code&gt;&amp;lt;class&amp;gt;&lt;/code&gt; is the class word &lt;code&gt;--class_word&lt;/code&gt; for training.&lt;/p&gt; &#xA;&lt;h2&gt;Results&lt;/h2&gt; &#xA;&lt;p&gt;Here I show some qualitative results. The training images are obtained from the &lt;a href=&#34;https://github.com/rinongal/textual_inversion/issues/8&#34;&gt;issue&lt;/a&gt; in the Textual Inversion repository, and they are 3 images of a large trash container. Regularization images are generated by prompt &lt;code&gt;photo of a container&lt;/code&gt;. Regularization images are shown here:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/XavierXiao/Dreambooth-Stable-Diffusion/main/assets/a-container-0038.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;After training, generated images with prompt &lt;code&gt;photo of a sks container&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/XavierXiao/Dreambooth-Stable-Diffusion/main/assets/photo-of-a-sks-container-0018.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Generated images with prompt &lt;code&gt;photo of a sks container on the beach&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/XavierXiao/Dreambooth-Stable-Diffusion/main/assets/photo-of-a-sks-container-on-the-beach-0017.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Generated images with prompt &lt;code&gt;photo of a sks container on the moon&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/XavierXiao/Dreambooth-Stable-Diffusion/main/assets/photo-of-a-sks-container-on-the-moon-0016.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Some not-so-perfect but still interesting results:&lt;/p&gt; &#xA;&lt;p&gt;Generated images with prompt &lt;code&gt;photo of a red sks container&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/XavierXiao/Dreambooth-Stable-Diffusion/main/assets/a-red-sks-container-0021.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Generated images with prompt &lt;code&gt;a dog on top of sks container&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/XavierXiao/Dreambooth-Stable-Diffusion/main/assets/a-dog-on-top-of-sks-container-0023.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>NielsRogge/Transformers-Tutorials</title>
    <updated>2022-10-02T01:44:14Z</updated>
    <id>tag:github.com,2022-10-02:/NielsRogge/Transformers-Tutorials</id>
    <link href="https://github.com/NielsRogge/Transformers-Tutorials" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This repository contains demos I made with the Transformers library by HuggingFace.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Transformers-Tutorials&lt;/h1&gt; &#xA;&lt;p&gt;Hi there!&lt;/p&gt; &#xA;&lt;p&gt;This repository contains demos I made with the &lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;Transformers library&lt;/a&gt; by ðŸ¤— HuggingFace. Currently, all of them are implemented in PyTorch.&lt;/p&gt; &#xA;&lt;p&gt;NOTE: if you are not familiar with HuggingFace and/or Transformers, I highly recommend to check out our &lt;a href=&#34;https://huggingface.co/course/chapter1&#34;&gt;free course&lt;/a&gt;, which introduces you to several Transformer architectures (such as BERT, GPT-2, T5, BART, etc.), as well as an overview of the HuggingFace libraries, including &lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;Transformers&lt;/a&gt;, &lt;a href=&#34;https://github.com/huggingface/tokenizers&#34;&gt;Tokenizers&lt;/a&gt;, &lt;a href=&#34;https://github.com/huggingface/datasets&#34;&gt;Datasets&lt;/a&gt;, &lt;a href=&#34;https://github.com/huggingface/accelerate&#34;&gt;Accelerate&lt;/a&gt; and the &lt;a href=&#34;https://huggingface.co/&#34;&gt;hub&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For an overview of the ecosystem of HuggingFace for computer vision (June 2022), refer to &lt;a href=&#34;https://github.com/NielsRogge/Transformers-Tutorials/raw/master/HuggingFace_vision_ecosystem_overview_(June_2022).ipynb&#34;&gt;this notebook&lt;/a&gt; with corresponding &lt;a href=&#34;https://www.youtube.com/watch?v=oL-xmufhZM8&amp;amp;t=2884s&#34;&gt;video&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Currently, it contains the following demos:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;BERT (&lt;a href=&#34;https://arxiv.org/abs/1810.04805&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;BertForTokenClassification&lt;/code&gt; on a named entity recognition (NER) dataset. &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Custom_Named_Entity_Recognition_with_BERT_only_first_wordpiece.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;BertForSequenceClassification&lt;/code&gt; for multi-label text classification. &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;BEiT (&lt;a href=&#34;https://raw.githubusercontent.com/NielsRogge/Transformers-Tutorials/master/%5Bhttps://arxiv.org/abs/2103.06874%5D(https://arxiv.org/abs/2106.08254)&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;understanding &lt;code&gt;BeitForMaskedImageModeling&lt;/code&gt; &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BEiT/Understanding_BeitForMaskedImageModeling.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;CANINE (&lt;a href=&#34;https://arxiv.org/abs/2103.06874&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;CanineForSequenceClassification&lt;/code&gt; on IMDb &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/CANINE/Fine_tune_CANINE_on_IMDb_(movie_review_binary_classification).ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;ConvNeXT (&lt;a href=&#34;https://arxiv.org/abs/2201.03545&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;fine-tuning (and performing inference with) &lt;code&gt;ConvNextForImageClassification&lt;/code&gt; &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/ConvNeXT/Fine_tune_ConvNeXT_for_image_classification.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;DINO (&lt;a href=&#34;https://arxiv.org/abs/2104.14294&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;visualize self-attention of Vision Transformers trained using the DINO method &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DINO/Visualize_self_attention_of_DINO.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;DETR (&lt;a href=&#34;https://arxiv.org/abs/2005.12872&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;performing inference with &lt;code&gt;DetrForObjectDetection&lt;/code&gt; &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DETR/DETR_minimal_example_(with_DetrFeatureExtractor).ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;DetrForObjectDetection&lt;/code&gt; on a custom object detection dataset &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DETR/Fine_tuning_DetrForObjectDetection_on_custom_dataset_(balloon).ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;evaluating &lt;code&gt;DetrForObjectDetection&lt;/code&gt; on the COCO detection 2017 validation set &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DETR/Evaluating_DETR_on_COCO_validation_2017.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;performing inference with &lt;code&gt;DetrForSegmentation&lt;/code&gt; &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DETR/DETR_panoptic_segmentation_minimal_example_(with_DetrFeatureExtractor).ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;DetrForSegmentation&lt;/code&gt; on COCO panoptic 2017 &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DETR/Fine_tuning_DetrForSegmentation_on_custom_dataset_end_to_end_approach.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;DPT (&lt;a href=&#34;https://arxiv.org/abs/2103.13413&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;performing inference with DPT for monocular depth estimation &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DPT/DPT_inference_notebook_(depth_estimation).ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;performing inference with DPT for semantic segmentation &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DPT/DPT_inference_notebook_(semantic_segmentation).ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;DiT (&lt;a href=&#34;https://arxiv.org/abs/2203.02378&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;performing inference with DiT for document image classification &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DiT/Inference_with_DiT_(Document_Image_Transformer)_for_document_image_classification.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Donut (&lt;a href=&#34;https://arxiv.org/abs/2111.15664&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;performing inference with Donut for document image classification &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/Donut/RVL-CDIP/Quick_inference_with_DONUT_for_Document_Image_Classification.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning Donut for document image classification &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/Donut/RVL-CDIP/Fine_tune_Donut_on_toy_RVL_CDIP_(document_image_classification).ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;performing inference with Donut for document visual question answering (DocVQA) &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/Donut/DocVQA/Quick_inference_with_DONUT_for_DocVQA.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;performing inference with Donut for document parsing &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/Donut/CORD/Quick_inference_with_DONUT_for_Document_Parsing.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning Donut for document parsing with PyTorch Lightning &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/Donut/CORD/Fine_tune_Donut_on_a_custom_dataset_(CORD)_with_PyTorch_Lightning.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;GLPN (&lt;a href=&#34;https://arxiv.org/abs/2201.07436&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;performing inference with &lt;code&gt;GLPNForDepthEstimation&lt;/code&gt; to illustrate monocular depth estimation &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/GLPN/GLPN_inference_(depth_estimation).ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;GPT-J-6B (&lt;a href=&#34;https://github.com/kingoflolz/mesh-transformer-jax&#34;&gt;repository&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;performing inference with &lt;code&gt;GPTJForCausalLM&lt;/code&gt; to illustrate few-shot learning and code generation &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/GPT-J-6B/Inference_with_GPT_J_6B.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;GroupViT (&lt;a href=&#34;https://github.com/NVlabs/GroupViT&#34;&gt;repository&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;performing inference with &lt;code&gt;GroupViTModel&lt;/code&gt; to illustrate zero-shot semantic segmentation &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/GroupViT/Inference_with_GroupViT_for_zero_shot_semantic_segmentation.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;ImageGPT (&lt;a href=&#34;https://openai.com/blog/image-gpt/&#34;&gt;blog post&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;(un)conditional image generation with &lt;code&gt;ImageGPTForCausalLM&lt;/code&gt; &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/ImageGPT/(Un)conditional_image_generation_with_ImageGPT.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;linear probing with ImageGPT &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/ImageGPT/Linear_probing_with_ImageGPT.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;LUKE (&lt;a href=&#34;https://arxiv.org/abs/2010.01057&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;LukeForEntityPairClassification&lt;/code&gt; on a custom relation extraction dataset using PyTorch Lightning &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LUKE/Supervised_relation_extraction_with_LukeForEntityPairClassification.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;LayoutLM (&lt;a href=&#34;https://arxiv.org/abs/1912.13318&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;LayoutLMForTokenClassification&lt;/code&gt; on the &lt;a href=&#34;https://guillaumejaume.github.io/FUNSD/&#34;&gt;FUNSD&lt;/a&gt; dataset &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForTokenClassification_on_FUNSD.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;LayoutLMForSequenceClassification&lt;/code&gt; on the &lt;a href=&#34;https://www.cs.cmu.edu/~aharley/rvl-cdip/&#34;&gt;RVL-CDIP&lt;/a&gt; dataset &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForSequenceClassification_on_RVL_CDIP.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;adding image embeddings to LayoutLM during fine-tuning on the &lt;a href=&#34;https://guillaumejaume.github.io/FUNSD/&#34;&gt;FUNSD&lt;/a&gt; dataset &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Add_image_embeddings_to_LayoutLM.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;LayoutLMv2 (&lt;a href=&#34;https://arxiv.org/abs/2012.14740&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;LayoutLMv2ForSequenceClassification&lt;/code&gt; on RVL-CDIP &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/RVL-CDIP/Fine_tuning_LayoutLMv2ForSequenceClassification_on_RVL_CDIP.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;LayoutLMv2ForTokenClassification&lt;/code&gt; on FUNSD &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/FUNSD/Fine_tuning_LayoutLMv2ForTokenClassification_on_FUNSD.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;LayoutLMv2ForTokenClassification&lt;/code&gt; on FUNSD using the ðŸ¤— Trainer &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/FUNSD/Fine_tuning_LayoutLMv2ForTokenClassification_on_FUNSD_using_HuggingFace_Trainer.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;performing inference with &lt;code&gt;LayoutLMv2ForTokenClassification&lt;/code&gt; on FUNSD &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/FUNSD/Inference_with_LayoutLMv2ForTokenClassification.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;true inference with &lt;code&gt;LayoutLMv2ForTokenClassification&lt;/code&gt; (when no labels are available) + Gradio demo &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/FUNSD/True_inference_with_LayoutLMv2ForTokenClassification_%2B_Gradio_demo.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;LayoutLMv2ForTokenClassification&lt;/code&gt; on CORD &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/CORD/Fine_tuning_LayoutLMv2ForTokenClassification_on_CORD.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;LayoutLMv2ForQuestionAnswering&lt;/code&gt; on DOCVQA &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/DocVQA/Fine_tuning_LayoutLMv2ForQuestionAnswering_on_DocVQA.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;LayoutLMv3 (&lt;a href=&#34;https://arxiv.org/abs/2204.08387&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;LayoutLMv3ForTokenClassification&lt;/code&gt; on the &lt;a href=&#34;https://guillaumejaume.github.io/FUNSD/&#34;&gt;FUNSD&lt;/a&gt; dataset &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv3/Fine_tune_LayoutLMv3_on_FUNSD_(HuggingFace_Trainer).ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;MarkupLM (&lt;a href=&#34;https://arxiv.org/abs/2110.08518&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;inference with MarkupLM to perform question answering on web pages &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/MarkupLM/Inference_with_MarkupLM_for_question_answering_on_web_pages.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;MarkupLMForTokenClassification&lt;/code&gt; on a toy dataset for NER on web pages &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/MarkupLM/Fine_tune_MarkupLMForTokenClassification_on_a_custom_dataset.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;MaskFormer (&lt;a href=&#34;https://arxiv.org/abs/2107.06278&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;performing inference with &lt;code&gt;MaskFormer&lt;/code&gt; (both semantic and panoptic segmentation): &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/MaskFormer/maskformer_minimal_example(with_MaskFormerFeatureExtractor).ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;MaskFormer&lt;/code&gt; on a custom dataset for semantic segmentation &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/MaskFormer/Fine_tune_MaskFormer_on_custom_dataset.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Perceiver IO (&lt;a href=&#34;https://arxiv.org/abs/2107.14795&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;showcasing masked language modeling and image classification with the Perceiver &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/Perceiver/Perceiver_for_masked_language_modeling_and_image_classification.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning the Perceiver for image classification &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/Perceiver/Fine_tune_the_Perceiver_for_image_classification.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning the Perceiver for text classification &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/Perceiver/Fine_tune_Perceiver_for_text_classification.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;predicting optical flow between a pair of images with &lt;code&gt;PerceiverForOpticalFlow&lt;/code&gt;&lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/Perceiver/Perceiver_for_Optical_Flow.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;auto-encoding a video (images, audio, labels) with &lt;code&gt;PerceiverForMultimodalAutoencoding&lt;/code&gt; &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/Perceiver/Perceiver_for_Multimodal_Autoencoding.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;SegFormer (&lt;a href=&#34;https://arxiv.org/abs/2105.15203&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;performing inference with &lt;code&gt;SegformerForSemanticSegmentation&lt;/code&gt; &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/SegFormer/Segformer_inference_notebook.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;SegformerForSemanticSegmentation&lt;/code&gt; on custom data using native PyTorch &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/SegFormer/Fine_tune_SegFormer_on_custom_dataset.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;T5 (&lt;a href=&#34;https://arxiv.org/abs/1910.10683&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;T5ForConditionalGeneration&lt;/code&gt; on a Dutch summarization dataset on TPU using HuggingFace Accelerate &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/tree/master/T5&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;T5ForConditionalGeneration&lt;/code&gt; (CodeT5) for Ruby code summarization using PyTorch Lightning &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/T5/Fine_tune_CodeT5_for_generating_docstrings_from_Ruby_code.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;TAPAS (&lt;a href=&#34;https://arxiv.org/abs/2004.02349&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;TapasForQuestionAnswering&lt;/code&gt; on the Microsoft &lt;a href=&#34;https://www.microsoft.com/en-us/download/details.aspx?id=54253&#34;&gt;Sequential Question Answering (SQA)&lt;/a&gt; dataset &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;evaluating &lt;code&gt;TapasForSequenceClassification&lt;/code&gt; on the &lt;a href=&#34;https://tabfact.github.io/&#34;&gt;Table Fact Checking (TabFact)&lt;/a&gt; dataset &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Evaluating_TAPAS_on_the_Tabfact_test_set.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;TrOCR (&lt;a href=&#34;https://arxiv.org/abs/2109.10282&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;performing inference with &lt;code&gt;TrOCR&lt;/code&gt; to illustrate optical character recognition with Transformers, as well as making a Gradio demo &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TrOCR/Inference_with_TrOCR_%2B_Gradio_demo.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;TrOCR&lt;/code&gt; on the IAM dataset using the Seq2SeqTrainer &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TrOCR/Fine_tune_TrOCR_on_IAM_Handwriting_Database_using_Seq2SeqTrainer.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;TrOCR&lt;/code&gt; on the IAM dataset using native PyTorch &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TrOCR/Fine_tune_TrOCR_on_IAM_Handwriting_Database_using_native_PyTorch.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;evaluating &lt;code&gt;TrOCR&lt;/code&gt; on the IAM test set &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TrOCR/Evaluating_TrOCR_base_handwritten_on_the_IAM_test_set.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;VideoMAE (&lt;a href=&#34;https://arxiv.org/abs/2203.12602&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;performing inference with &lt;code&gt;VideoMAEForVideoClassification&lt;/code&gt; &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VideoMAE/Quick_inference_with_VideoMAE.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;ViLT (&lt;a href=&#34;https://arxiv.org/abs/2102.03334&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;ViLT&lt;/code&gt; for visual question answering (VQA) &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/ViLT/Fine_tuning_ViLT_for_VQA.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;performing inference with &lt;code&gt;ViLT&lt;/code&gt; to illustrate visual question answering (VQA) &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/ViLT/Inference_with_ViLT_(visual_question_answering).ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;masked language modeling (MLM) with a pre-trained &lt;code&gt;ViLT&lt;/code&gt; model &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/ViLT/Masked_language_modeling_with_ViLT.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;performing inference with &lt;code&gt;ViLT&lt;/code&gt; for image-text retrieval &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/ViLT/Using_ViLT_for_image_text_retrieval.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;performing inference with &lt;code&gt;ViLT&lt;/code&gt; to illustrate natural language for visual reasoning (NLVR) &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/ViLT/ViLT_for_natural_language_visual_reasoning.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;ViTMAE (&lt;a href=&#34;https://arxiv.org/abs/2111.06377&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;reconstructing pixel values with &lt;code&gt;ViTMAEForPreTraining&lt;/code&gt; &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/ViTMAE/ViT_MAE_visualization_demo.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Vision Transformer (&lt;a href=&#34;https://arxiv.org/abs/2010.11929&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;performing inference with &lt;code&gt;ViTForImageClassification&lt;/code&gt; &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Quick_demo_of_HuggingFace_version_of_Vision_Transformer_inference.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;ViTForImageClassification&lt;/code&gt; on &lt;a href=&#34;https://www.cs.toronto.edu/~kriz/cifar.html&#34;&gt;CIFAR-10&lt;/a&gt; using PyTorch Lightning &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_PyTorch_Lightning.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;ViTForImageClassification&lt;/code&gt; on &lt;a href=&#34;https://www.cs.toronto.edu/~kriz/cifar.html&#34;&gt;CIFAR-10&lt;/a&gt; using the ðŸ¤— Trainer &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_the_%F0%9F%A4%97_Trainer.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;YOLOS (&lt;a href=&#34;https://arxiv.org/abs/2106.00666&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;YolosForObjectDetection&lt;/code&gt; on a custom dataset &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/YOLOS/Fine_tuning_YOLOS_for_object_detection_on_custom_dataset_(balloon).ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;inference with &lt;code&gt;YolosForObjectDetection&lt;/code&gt; &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/YOLOS/YOLOS_minimal_inference_example.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;X-CLIP (&lt;a href=&#34;https://arxiv.org/abs/2208.02816&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;performing zero-shot video classification with X-CLIP &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/X-CLIP/Video_text_matching_with_X_CLIP.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;zero-shot classifying a YouTube video with X-CLIP &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/X-CLIP/Zero_shot_classify_a_YouTube_video_with_X_CLIP.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;... more to come! ðŸ¤—&lt;/p&gt; &#xA;&lt;p&gt;If you have any questions regarding these demos, feel free to open an issue on this repository.&lt;/p&gt; &#xA;&lt;p&gt;Btw, I was also the main contributor to add the following algorithms to the library:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;TAbular PArSing (TAPAS) by Google AI&lt;/li&gt; &#xA; &lt;li&gt;Vision Transformer (ViT) by Google AI&lt;/li&gt; &#xA; &lt;li&gt;DINO by Facebook AI&lt;/li&gt; &#xA; &lt;li&gt;Data-efficient Image Transformers (DeiT) by Facebook AI&lt;/li&gt; &#xA; &lt;li&gt;LUKE by Studio Ousia&lt;/li&gt; &#xA; &lt;li&gt;DEtection TRansformers (DETR) by Facebook AI&lt;/li&gt; &#xA; &lt;li&gt;CANINE by Google AI&lt;/li&gt; &#xA; &lt;li&gt;BEiT by Microsoft Research&lt;/li&gt; &#xA; &lt;li&gt;LayoutLMv2 (and LayoutXLM) by Microsoft Research&lt;/li&gt; &#xA; &lt;li&gt;TrOCR by Microsoft Research&lt;/li&gt; &#xA; &lt;li&gt;SegFormer by NVIDIA&lt;/li&gt; &#xA; &lt;li&gt;ImageGPT by OpenAI&lt;/li&gt; &#xA; &lt;li&gt;Perceiver by Deepmind&lt;/li&gt; &#xA; &lt;li&gt;MAE by Facebook AI&lt;/li&gt; &#xA; &lt;li&gt;ViLT by NAVER AI Lab&lt;/li&gt; &#xA; &lt;li&gt;ConvNeXT by Facebook AI&lt;/li&gt; &#xA; &lt;li&gt;DiT By Microsoft Research&lt;/li&gt; &#xA; &lt;li&gt;GLPN by KAIST&lt;/li&gt; &#xA; &lt;li&gt;DPT by Intel Labs&lt;/li&gt; &#xA; &lt;li&gt;YOLOS by School of EIC, Huazhong University of Science &amp;amp; Technology&lt;/li&gt; &#xA; &lt;li&gt;TAPEX by Microsoft Research&lt;/li&gt; &#xA; &lt;li&gt;LayoutLMv3 by Microsoft Research&lt;/li&gt; &#xA; &lt;li&gt;VideoMAE by Multimedia Computing Group, Nanjing University&lt;/li&gt; &#xA; &lt;li&gt;X-CLIP by Microsoft Research&lt;/li&gt; &#xA; &lt;li&gt;MarkupLM by Microsoft Research&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;All of them were an incredible learning experience. I can recommend anyone to contribute an AI algorithm to the library!&lt;/p&gt; &#xA;&lt;h2&gt;Data preprocessing&lt;/h2&gt; &#xA;&lt;p&gt;Regarding preparing your data for a PyTorch model, there are a few options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;a native PyTorch dataset + dataloader. This is the standard way to prepare data for a PyTorch model, namely by subclassing &lt;code&gt;torch.utils.data.Dataset&lt;/code&gt;, and then creating a corresponding &lt;code&gt;DataLoader&lt;/code&gt; (which is a Python generator that allows to loop over the items of a dataset). When subclassing the &lt;code&gt;Dataset&lt;/code&gt; class, one needs to implement 3 methods: &lt;code&gt;__init__&lt;/code&gt;, &lt;code&gt;__len__&lt;/code&gt; (which returns the number of examples of the dataset) and &lt;code&gt;__getitem__&lt;/code&gt; (which returns an example of the dataset, given an integer index). Here&#39;s an example of creating a basic text classification dataset (assuming one has a CSV that contains 2 columns, namely &#34;text&#34; and &#34;label&#34;):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from torch.utils.data import Dataset&#xA;&#xA;class CustomTrainDataset(Dataset):&#xA;    def __init__(self, df, tokenizer):&#xA;        self.df = df&#xA;        self.tokenizer = tokenizer&#xA;&#xA;    def __len__(self):&#xA;        return len(self.df)&#xA;&#xA;    def __getitem__(self, idx):&#xA;        # get item&#xA;        item = df.iloc[idx]&#xA;        text = item[&#39;text&#39;]&#xA;        label = item[&#39;label&#39;]&#xA;        # encode text&#xA;        encoding = self.tokenizer(text, padding=&#34;max_length&#34;, max_length=128, truncation=True, return_tensors=&#34;pt&#34;)&#xA;        # remove batch dimension which the tokenizer automatically adds&#xA;        encoding = {k:v.squeeze() for k,v in encoding.items()}&#xA;        # add label&#xA;        encoding[&#34;label&#34;] = torch.tensor(label)&#xA;        &#xA;        return encoding&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Instantiating the dataset then happens as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import BertTokenizer&#xA;import pandas as pd&#xA;&#xA;tokenizer = BertTokenizer.from_pretrained(&#34;bert-base-uncased&#34;)&#xA;df = pd.read_csv(&#34;path_to_your_csv&#34;)&#xA;&#xA;train_dataset = CustomTrainDataset(df=df, tokenizer=tokenizer)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Accessing the first example of the dataset can then be done as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;encoding = train_dataset[0]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In practice, one creates a corresponding &lt;code&gt;DataLoader&lt;/code&gt;, that allows to get batches from the dataset:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from torch.utils.data import DataLoader&#xA;&#xA;train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;I often check whether the data is created correctly by fetching the first batch from the data loader, and then printing out the shapes of the tensors, decoding the input_ids back to text, etc.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;batch = next(iter(train_dataloader))&#xA;for k,v in batch.items():&#xA;    print(k, v.shape)&#xA;# decode the input_ids of the first example of the batch&#xA;print(tokenizer.decode(batch[&#39;input_ids&#39;][0].tolist())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/datasets/&#34;&gt;HuggingFace Datasets&lt;/a&gt;. Datasets is a library by HuggingFace that allows to easily load and process data in a very fast and memory-efficient way. It is backed by &lt;a href=&#34;https://arrow.apache.org/&#34;&gt;Apache Arrow&lt;/a&gt;, and has cool features such as memory-mapping, which allow you to only load data into RAM when it is required. It only has deep interoperability with the &lt;a href=&#34;https://huggingface.co/datasets&#34;&gt;HuggingFace hub&lt;/a&gt;, allowing to easily load well-known datasets as well as share your own with the community.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Loading a custom dataset as a Dataset object can be done as follows (you can install datasets using &lt;code&gt;pip install datasets&lt;/code&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from datasets import load_dataset&#xA;&#xA;dataset = load_dataset(&#39;csv&#39;, data_files={&#39;train&#39;: [&#39;my_train_file_1.csv&#39;, &#39;my_train_file_2.csv&#39;] &#39;test&#39;: &#39;my_test_file.csv&#39;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here I&#39;m loading local CSV files, but there are other formats supported (including JSON, Parquet, txt) as well as loading data from a local Pandas dataframe or dictionary for instance. You can check out the &lt;a href=&#34;https://huggingface.co/docs/datasets/loading.html#local-and-remote-files&#34;&gt;docs&lt;/a&gt; for all details.&lt;/p&gt; &#xA;&lt;h2&gt;Training frameworks&lt;/h2&gt; &#xA;&lt;p&gt;Regarding fine-tuning Transformer models (or more generally, PyTorch models), there are a few options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;using native PyTorch. This is the most basic way to train a model, and requires the user to manually write the training loop. The advantage is that this is very easy to debug. The disadvantage is that one needs to implement training him/herself, such as setting the model in the appropriate mode (&lt;code&gt;model.train()&lt;/code&gt;/&lt;code&gt;model.eval()&lt;/code&gt;), handle device placement (&lt;code&gt;model.to(device)&lt;/code&gt;), etc. A typical training loop in PyTorch looks as follows (inspired by &lt;a href=&#34;&#34;&gt;this great PyTorch intro tutorial&lt;/a&gt;):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from transformers import BertForSequenceClassification&#xA;&#xA;# Instantiate pre-trained BERT model with randomly initialized classification head&#xA;model = BertForSequenceClassification.from_pretrained(&#34;bert-base-uncased&#34;)&#xA;&#xA;# I almost always use a learning rate of 5e-5 when fine-tuning Transformer based models&#xA;optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)&#xA;&#xA;# put model on GPU, if available&#xA;device = torch.device(&#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34;)&#xA;model.to(device)&#xA;&#xA;for epoch in range(epochs):&#xA;    model.train()&#xA;    train_loss = 0.0&#xA;    for batch in train_dataloader:&#xA;        # put batch on device&#xA;        batch = {k:v.to(device) for k,v in batch.items()}&#xA;        &#xA;        # forward pass&#xA;        outputs = model(**batch)&#xA;        loss = outputs.loss&#xA;        &#xA;        train_loss += loss.item()&#xA;        &#xA;        loss.backward()&#xA;        optimizer.step()&#xA;        optimizer.zero_grad()&#xA;&#xA;    print(&#34;Loss after epoch {epoch}:&#34;, train_loss/len(train_dataloader))&#xA;    &#xA;    model.eval()&#xA;    val_loss = 0.0&#xA;    with torch.no_grad():&#xA;        for batch in eval_dataloader:&#xA;            # put batch on device&#xA;            batch = {k:v.to(device) for k,v in batch.items()}&#xA;            &#xA;            # forward pass&#xA;            outputs = model(**batch)&#xA;            loss = outputs.logits&#xA;            &#xA;            val_loss += loss.item()&#xA;                  &#xA;    print(&#34;Validation loss after epoch {epoch}:&#34;, val_loss/len(eval_dataloader))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.pytorchlightning.ai/&#34;&gt;PyTorch Lightning (PL)&lt;/a&gt;. PyTorch Lightning is a framework that automates the training loop written above, by abstracting it away in a Trainer object. Users don&#39;t need to write the training loop themselves anymore, instead they can just do &lt;code&gt;trainer = Trainer()&lt;/code&gt; and then &lt;code&gt;trainer.fit(model)&lt;/code&gt;. The advantage is that you can start training models very quickly (hence the name lightning), as all training-related code is handled by the &lt;code&gt;Trainer&lt;/code&gt; object. The disadvantage is that it may be more difficult to debug your model, as the training and evaluation is now abstracted away.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/transformers/main_classes/trainer.html&#34;&gt;HuggingFace Trainer&lt;/a&gt;. The HuggingFace Trainer API can be seen as a framework similar to PyTorch Lightning in the sense that it also abstracts the training away using a Trainer object. However, contrary to PyTorch Lightning, it is not meant not be a general framework. Rather, it is made especially for fine-tuning Transformer-based models available in the HuggingFace Transformers library. The Trainer also has an extension called &lt;code&gt;Seq2SeqTrainer&lt;/code&gt; for encoder-decoder models, such as BART, T5 and the &lt;code&gt;EncoderDecoderModel&lt;/code&gt; classes. Note that all &lt;a href=&#34;https://github.com/huggingface/transformers/tree/master/examples/pytorch&#34;&gt;PyTorch example scripts&lt;/a&gt; of the Transformers library make use of the Trainer.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/accelerate&#34;&gt;HuggingFace Accelerate&lt;/a&gt;: Accelerate is a new project, that is made for people who still want to write their own training loop (as shown above), but would like to make it work automatically irregardless of the hardware (i.e. multiple GPUs, TPU pods, mixed precision, etc.).&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>https-deeplearning-ai/machine-learning-engineering-for-production-public</title>
    <updated>2022-10-02T01:44:14Z</updated>
    <id>tag:github.com,2022-10-02:/https-deeplearning-ai/machine-learning-engineering-for-production-public</id>
    <link href="https://github.com/https-deeplearning-ai/machine-learning-engineering-for-production-public" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Public repo for DeepLearning.AI MLEP Specialization&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Machine Learning Engineering for Production&lt;/h2&gt; &#xA;&lt;p&gt;Welcome to the public repo for &lt;a href=&#34;https://www.deeplearning.ai/&#34;&gt;deeplearning.ai&lt;/a&gt;&#39;s Machine Learning Engineering for Production Specialization.&lt;/p&gt; &#xA;&lt;p&gt;Here you will find public resources for the courses of this specialization.&lt;/p&gt;</summary>
  </entry>
</feed>