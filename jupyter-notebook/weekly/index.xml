<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-11-24T01:40:00Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Zefan-Cai/KVCache-Factory</title>
    <updated>2024-11-24T01:40:00Z</updated>
    <id>tag:github.com,2024-11-24:/Zefan-Cai/KVCache-Factory</id>
    <link href="https://github.com/Zefan-Cai/KVCache-Factory" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Unified KV Cache Compression Methods for Auto-Regressive Models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Pyramid KV&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Zefan-Cai/KVCache-Factory/main/figs/PyramidKV.png&#34; width=&#34;100%&#34;&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;[2024-06-25] Support multi-GPUs inference with big LLMs now! Try out PyramidKV on LlaMa-3-70B-Instruct!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2024-06-10] Support PyramidKV, SnapKV, H2O and StreamingLLM at Flash Attention v2, Sdpa Attention now! If your devices (i.e., V100, 3090) does not support Flash Attention v2, you can set attn_implementation=sdpa to try PyramidKV at Sdpa Attention!&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TODO:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;Support implementation of Streaming LLM, H2O and SnapKV&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;Support Mistral model&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;Support implementation of Needle&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;Support KV cache compression without Flash Attention v2 (i.e. Sdpa Attention) for V100&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;Support multi-GPU inference for 70B LlaMa-3&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;Introduce new functions to support kv cache budget allocation (i.e., supports for percentage.)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;Support Mixtral&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;Support Batch Inference&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;Support KV cache compression at decoding stage&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Performence&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Zefan-Cai/KVCache-Factory/main/figs/Result.png&#34; width=&#34;100%&#34;&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Zefan-Cai/KVCache-Factory/main/figs/Needle.png&#34; width=&#34;80%&#34;&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Visualization: Inefficient Attention&lt;/h2&gt; &#xA;&lt;p&gt;The Llama model attention map with 3 documents is represented as follows:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Zefan-Cai/KVCache-Factory/main/figs/attention_pattern.png&#34; width=&#34;100%&#34;&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;p&gt;we provide a notebook &lt;code&gt;visualization.ipynb&lt;/code&gt; to reproduce the visualization result of each Llama-2-7b-hf model layer for a given 3 document.&lt;/p&gt; &#xA;&lt;p&gt;Model attention maps for different layers would be stored at &lt;code&gt;./attention&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;transformers &amp;gt;= 4.41&#xA;flash-attn &amp;gt;= 2.4.0.post1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;git clone https://github.com/Zefan-Cai/PyramidKV.git&#xA;cd PyramidKV&#xA;pip install -r requirements.txt .&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;We support inference code on &lt;code&gt;LongBench&lt;/code&gt; to repuduce our result.&lt;/p&gt; &#xA;&lt;p&gt;Please refer to &lt;code&gt;scripts/scripts_longBench/eval.sh&lt;/code&gt; to modify the parameters according to your requirements.&lt;/p&gt; &#xA;&lt;p&gt;Our codebase support Flash Attention v2, Sdpa Attention, etc. The results presented in our paper in based on Flash Attention v2.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CUDA_VISIBLE_DEVICES=$1&#xA;&#xA;method=$2 # Support PyramidKV, SnapKV, H2O, StreamingLLM&#xA;max_capacity_prompts=64 # 128,2048 in paper&#xA;attn_implementation=$3 # Support &#34;flash_attention_2&#34;, &#34;sdpa&#34;, &#34;eager&#34;.&#xA;source_path=$4&#xA;model_path=$5&#xA;save_dir=${source_path}&#34;results_long_bench&#34; # path to result save_dir&#xA;&#xA;python3 run_longbench.py \&#xA;    --method ${method} \&#xA;    --model_path ${model_path} \&#xA;    --max_capacity_prompts ${max_capacity_prompts} \&#xA;    --attn_implementation ${attn_implementation} \&#xA;    --save_dir ${save_dir} \&#xA;    --use_cache True&#xA;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CUDA_VISIBLE_DEVICES: For multi-GPU inference for big LLMs, just need to specify CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7. For single GPU inference, just need to specify CUDA_VISIBLE_DEVICES=0.&lt;/li&gt; &#xA; &lt;li&gt;model_path: Path to your model. Support &#34;Llama-3-8B-Instruct&#34; for now.&lt;/li&gt; &#xA; &lt;li&gt;method: Support &lt;code&gt;PyramidKV&lt;/code&gt;, &lt;code&gt;SnapKV&lt;/code&gt;, &lt;code&gt;StreamingLLM&lt;/code&gt;, &lt;code&gt;H2O&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;max_capacity_prompts: Selected KV Size in each layer. Ôºàe.g. 128, 2048 in paperÔºâ. When method is &#34;PyramidKV&#34;, given that the total number of KV remains unchanged, the specific KV length for each layer will be modified accordingly&lt;/li&gt; &#xA; &lt;li&gt;save_dir: Path to your dir to save LongBench result.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;After modifying parameters, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&#xA;sh scripts/scripts_longBench/eval.sh&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Needle in haystack&lt;/h2&gt; &#xA;&lt;p&gt;We support inference code on &lt;code&gt;Needle in haystack&lt;/code&gt; to repuduce our result.&lt;/p&gt; &#xA;&lt;p&gt;Please refer to &lt;code&gt;scripts/scripts_needle/eval.sh&lt;/code&gt; to modify the parameters according to your requirements.&lt;/p&gt; &#xA;&lt;p&gt;Our codebase support Flash Attention v2, Sdpa Attention, etc. The results presented in our paper in based on Flash Attention v2.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#xA;METHOD=&#39;pyramidkv&#39;       # [&#39;full&#39;, &#39;pyramidkv&#39;, &#39;snapkv&#39;, &#39;streamingllm&#39;, &#39;h2o&#39;]&#xA;MAX_CAPACITY_PROMPT=96  # [64, 96, 128, 256, 512, 1024, 2048, ...]&#xA;attn_implementation=&#34;flash_attention_2&#34; # Support &#34;flash_attention_2&#34;, &#34;sdpa&#34;, &#34;&#34;.&#xA;TAG=test&#xA;&#xA;&#xA;# For Llama3-8b&#xA;&#xA;(&#xA;python -u run_needle_in_haystack.py --s_len 1000 --e_len 8001\&#xA;    --model_provider LLaMA3 \&#xA;    --model_name /mnt/workspace/zhiyuanhu/yuliang/models/llama3-8b_raw \&#xA;    --attn_implementation ${attn_implementation} \&#xA;    --step 100 \&#xA;    --method $METHOD \&#xA;    --max_capacity_prompt $MAX_CAPACITY_PROMPT \&#xA;    --model_version LlaMA3_${METHOD}_${MAX_CAPACITY_PROMPT}_${TAG}&#xA;) 2&amp;gt;&amp;amp;1  | tee results_needle/logs/LlaMA3_${METHOD}_${MAX_CAPACITY_PROMPT}_${TAG}.log&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Both LLaMA3 and Mistral2 inference support on single GPU.&lt;/li&gt; &#xA; &lt;li&gt;model_provider: LLaMA3 or Mistral2&lt;/li&gt; &#xA; &lt;li&gt;model_name: Path to your model. Support &#34;Llama-3-8B-Instruct&#34; &#34;Mistral-7B-Instruct-v0.2&#34; and for now.&lt;/li&gt; &#xA; &lt;li&gt;step: The increase of context length.&lt;/li&gt; &#xA; &lt;li&gt;method: Support &lt;code&gt;PyramidKV&lt;/code&gt;, &lt;code&gt;SnapKV&lt;/code&gt;, &lt;code&gt;StreamingLLM&lt;/code&gt;, &lt;code&gt;H2O&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;max_capacity_prompt: Selected KV Size in each layer. Ôºàe.g. 128, 2048 in paperÔºâ. When method is &#34;PyramidKV&#34;, given that the total number of KV remains unchanged, the specific KV length for each layer will be modified accordingly&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To reproduce our results, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash scripts/scripts_needle/eval.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After inference, run&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;python scripts/scripts_needle/visualize.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;to draw the img, you should change &lt;code&gt;FOLDER_PATH&lt;/code&gt; in &lt;code&gt;visualize.py&lt;/code&gt; to your output path (the argument of &lt;code&gt;--model_version&lt;/code&gt; in &lt;code&gt;eval.sh&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find &lt;strong&gt;PyramidKV&lt;/strong&gt; useful for your research and applications, please kindly cite using this BibTeX:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-latex&#34;&gt;@article{cai2024pyramidkv,&#xA;  title={PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information Funneling},&#xA;  author={Cai, Zefan and Zhang, Yichi and Gao, Bofei and Liu, Yuliang and Liu, Tianyu and Lu, Keming and Xiong, Wayne and Dong, Yue and Chang, Baobao and Hu, Junjie and others},&#xA;  journal={CoRR},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;Thanks &lt;strong&gt;[SnapKV]&lt;/strong&gt; &lt;a href=&#34;https://github.com/FasterDecoding/SnapKV&#34;&gt;SnapKV: LLM Knows What You are Looking for Before Generation&lt;/a&gt; for providing open-source code to support the expansion of this project.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>modelscope/facechain</title>
    <updated>2024-11-24T01:40:00Z</updated>
    <id>tag:github.com,2024-11-24:/modelscope/facechain</id>
    <link href="https://github.com/modelscope/facechain" rel="alternate"></link>
    <summary type="html">&lt;p&gt;FaceChain is a deep-learning toolchain for generating your Digital-Twin.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;br&gt; &lt;img src=&#34;https://modelscope.oss-cn-beijing.aliyuncs.com/modelscope.gif&#34; width=&#34;400&#34;&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;h1&gt;FaceChain&lt;/h1&gt; &#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://trendshift.io/repositories/1185&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://trendshift.io/api/badge/repositories/1185&#34; alt=&#34;modelscope%2Ffacechain | Trendshift&#34; style=&#34;width: 250px; height: 55px;&#34; width=&#34;250&#34; height=&#34;55&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h1&gt;News&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;More Technology Details of FaceChain-FACT train-free portrait generation can be seen in &lt;a href=&#34;https://arxiv.org/abs/2410.12312&#34;&gt;Paper&lt;/a&gt;. (October 17th, 2024 UTC)&lt;/li&gt; &#xA; &lt;li&gt;Our work &lt;a href=&#34;https://arxiv.org/abs/2410.10587&#34;&gt;TopoFR&lt;/a&gt; got accepted to NeurIPS 2024 ! (September 26th, 2024 UTC)&lt;/li&gt; &#xA; &lt;li&gt;We provide training scripts for new styles, offering an automatic training for new style LoRas as well as the corresponding style prompts, along with the one click call in Infinite Style Portrait generation tab! (July 3rd, 2024 UTC)&lt;/li&gt; &#xA; &lt;li&gt;üöÄüöÄüöÄ We are launching [FACT] into the main branch, offering a 10-second impressive speed and seamless integration with standard ready-to-use LoRas and ControlNets, along with improved instruction-following capabilities ! The original train-based FaceChain is moved to (&lt;a href=&#34;https://github.com/modelscope/facechain/tree/v3.0.0&#34;&gt;https://github.com/modelscope/facechain/tree/v3.0.0&lt;/a&gt; ). (May 28th, 2024 UTC)&lt;/li&gt; &#xA; &lt;li&gt;Our work &lt;a href=&#34;https://arxiv.org/abs/2403.01901&#34;&gt;FaceChain-ImagineID&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2403.06775&#34;&gt;FaceChain-SuDe&lt;/a&gt; got accepted to CVPR 2024 ! (February 27th, 2024 UTC)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Introduction&lt;/h1&gt; &#xA;&lt;p&gt;Â¶ÇÊûúÊÇ®ÁÜüÊÇâ‰∏≠ÊñáÔºåÂèØ‰ª•ÈòÖËØª&lt;a href=&#34;https://raw.githubusercontent.com/modelscope/facechain/main/README_ZH.md&#34;&gt;‰∏≠ÊñáÁâàÊú¨ÁöÑREADME&lt;/a&gt;„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;FaceChain is a novel framework for generating identity-preserved human portraits. In the newest FaceChain FACT (Face Adapter with deCoupled Training) version, with only 1 photo and 10 seconds, you can generate personal portraits in different settings (multiple styles now supported!). FaceChain has both high controllability and authenticity in portrait generation, including text-to-image and inpainting based pipelines, and is seamlessly compatible with ControlNet and LoRAs. You may generate portraits via FaceChain&#39;s Python scripts, or via the familiar Gradio interface, or via sd webui. FaceChain is powered by &lt;a href=&#34;https://github.com/modelscope/modelscope&#34;&gt;ModelScope&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; ModelScope Studio &lt;a href=&#34;https://modelscope.cn/studios/CVstudio/FaceChain-FACT&#34;&gt;ü§ñ&lt;/a&gt;&lt;a&gt;&lt;/a&gt;&amp;nbsp; ÔΩúAPI &lt;a href=&#34;https://help.aliyun.com/zh/dashscope/developer-reference/facechain-quick-start&#34;&gt;üî•&lt;/a&gt;&lt;a&gt;&lt;/a&gt;&amp;nbsp; | SD WebUI | HuggingFace Space &lt;a href=&#34;https://huggingface.co/spaces/modelscope/FaceChain-FACT&#34;&gt;ü§ó&lt;/a&gt;&amp;nbsp; &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://facechain-fact.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Page-Green&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://youtu.be/DHqEl0qwi-M?si=y6VpInXdhIX0HpbI&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/youtube.svg?sanitize=true&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/modelscope/facechain/main/resources/git_cover.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;News&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;More Technology Details of FaceChain-FACT train-free portrait generation can be seen in &lt;a href=&#34;https://arxiv.org/abs/2410.12312&#34;&gt;Paper&lt;/a&gt;. (October 17th, 2024 UTC)&lt;/li&gt; &#xA; &lt;li&gt;Our work &lt;a href=&#34;https://arxiv.org/abs/2410.10587&#34;&gt;TopoFR&lt;/a&gt; got accepted to NeurIPS 2024 ! (September 26th, 2024 UTC)&lt;/li&gt; &#xA; &lt;li&gt;We provide training scripts for new styles, offering an automatic training for new style LoRas as well as the corresponding style prompts, along with the one click call in Infinite Style Portrait generation tab! (July 3rd, 2024 UTC)&lt;/li&gt; &#xA; &lt;li&gt;üöÄüöÄüöÄ We are launching [FACT], offering a 10-second impressive speed and seamless integration with standard ready-to-use LoRas and ControlNets, along with improved instruction-following capabilities ! (May 28th, 2024 UTC)&lt;/li&gt; &#xA; &lt;li&gt;Our work &lt;a href=&#34;https://arxiv.org/abs/2403.01901&#34;&gt;FaceChain-ImagineID&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2403.06775&#34;&gt;FaceChain-SuDe&lt;/a&gt; got accepted to CVPR 2024 ! (February 27th, 2024 UTC)&lt;/li&gt; &#xA; &lt;li&gt;üèÜüèÜüèÜAlibaba Annual Outstanding Open Source Project, Alibaba Annual Open Source Pioneer (Yang Liu, Baigui Sun). (January 20th, 2024 UTC)&lt;/li&gt; &#xA; &lt;li&gt;Our work &lt;a href=&#34;https://github.com/henryqin1997/InfoBatch&#34;&gt;InfoBatch&lt;/a&gt; co-authored with NUS team got accepted to ICLR 2024(Oral)! (January 16th, 2024 UTC)&lt;/li&gt; &#xA; &lt;li&gt;üèÜOpenAtom&#39;s 2023 Rapidly Growing Open Source Projects Award. (December 20th, 2023 UTC)&lt;/li&gt; &#xA; &lt;li&gt;Add SDXL pipelineüî•üî•üî•, image detail is improved obviously. (November 22th, 2023 UTC)&lt;/li&gt; &#xA; &lt;li&gt;Support super resolutionüî•üî•üî•, provide multiple resolution choice (512&lt;em&gt;512, 768&lt;/em&gt;768, 1024&lt;em&gt;1024, 2048&lt;/em&gt;2048). (November 13th, 2023 UTC)&lt;/li&gt; &#xA; &lt;li&gt;üèÜFaceChain has been selected in the &lt;a href=&#34;https://www.benchcouncil.org/evaluation/opencs/annual.html#Institutions&#34;&gt;BenchCouncil Open100 (2022-2023)&lt;/a&gt; annual ranking. (November 8th, 2023 UTC)&lt;/li&gt; &#xA; &lt;li&gt;Add virtual try-on module. (October 27th, 2023 UTC)&lt;/li&gt; &#xA; &lt;li&gt;Add wanx version &lt;a href=&#34;https://tongyi.aliyun.com/wanxiang/app/portrait-gallery&#34;&gt;online free app&lt;/a&gt;. (October 26th, 2023 UTC)&lt;/li&gt; &#xA; &lt;li&gt;üèÜ1024 Programmer&#39;s Day AIGC Application Tool Most Valuable Business Award. (2023-10-24, 2023 UTC)&lt;/li&gt; &#xA; &lt;li&gt;Support FaceChain in stable-diffusion-webuiüî•üî•üî•. (October 13th, 2023 UTC)&lt;/li&gt; &#xA; &lt;li&gt;High performance inpainting for single &amp;amp; double person, Simplify User Interface. (September 09th, 2023 UTC)&lt;/li&gt; &#xA; &lt;li&gt;More Technology Details can be seen in &lt;a href=&#34;https://arxiv.org/abs/2308.14256&#34;&gt;Paper&lt;/a&gt;. (August 30th, 2023 UTC)&lt;/li&gt; &#xA; &lt;li&gt;Add validate &amp;amp; ensemble for Lora training, and InpaintTab(hide in gradio for now). (August 28th, 2023 UTC)&lt;/li&gt; &#xA; &lt;li&gt;Add pose control module. (August 27th, 2023 UTC)&lt;/li&gt; &#xA; &lt;li&gt;Add robust face lora training module, enhance the performance of one pic training &amp;amp; style-lora blending. (August 27th, 2023 UTC)&lt;/li&gt; &#xA; &lt;li&gt;HuggingFace Space is available now! You can experience FaceChain directly with &lt;a href=&#34;https://huggingface.co/spaces/modelscope/FaceChain&#34;&gt;ü§ó&lt;/a&gt; (August 25th, 2023 UTC)&lt;/li&gt; &#xA; &lt;li&gt;Add awesome prompts! Refer to: &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/facechain/main/resources/awesome-prompts-facechain.txt&#34;&gt;awesome-prompts-facechain&lt;/a&gt; (August 18th, 2023 UTC)&lt;/li&gt; &#xA; &lt;li&gt;Support a series of new style models in a plug-and-play fashion. (August 16th, 2023 UTC)&lt;/li&gt; &#xA; &lt;li&gt;Support customizable prompts. (August 16th, 2023 UTC)&lt;/li&gt; &#xA; &lt;li&gt;Colab notebook is available now! You can experience FaceChain directly with &lt;a href=&#34;https://colab.research.google.com/github/modelscope/facechain/blob/main/facechain_demo.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;. (August 15th, 2023 UTC)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;To-Do List&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Develop RLHF methods, make its quality more higher.&lt;/li&gt; &#xA; &lt;li&gt;Support more beauty-retouch effects.&lt;/li&gt; &#xA; &lt;li&gt;Provide more funny apps.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Citation&lt;/h1&gt; &#xA;&lt;p&gt;Please cite FaceChain and FaceChain-FACT in your publications if it helps your research&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{liu2023facechain,&#xA;  title={FaceChain: A Playground for Identity-Preserving Portrait Generation},&#xA;  author={Liu, Yang and Yu, Cheng and Shang, Lei and Wu, Ziheng and &#xA;          Wang, Xingjun and Zhao, Yuze and Zhu, Lin and Cheng, Chen and &#xA;          Chen, Weitao and Xu, Chao and Xie, Haoyu and Yao, Yuan and &#xA;          Zhou,  Wenmeng and Chen Yingda and Xie, Xuansong and Sun, Baigui},&#xA;  journal={arXiv preprint arXiv:2308.14256},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{yu2024facechain,&#xA;  title={FaceChain-FACT: Face Adapter with Decoupled Training for Identity-preserved Personalization},&#xA;  author={Yu, Cheng and Xie, Haoyu and Shang, Lei and Liu, Yang and Dan, Jun and Sun, Baigui and Bo, Liefeng},&#xA;  journal={arXiv preprint arXiv:2410.12312},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;h2&gt;Compatibility Verification&lt;/h2&gt; &#xA;&lt;p&gt;We have verified e2e execution on the following environment:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;python: py3.8, py3.10&lt;/li&gt; &#xA; &lt;li&gt;pytorch: torch2.0.0, torch2.0.1&lt;/li&gt; &#xA; &lt;li&gt;CUDA: 11.7&lt;/li&gt; &#xA; &lt;li&gt;CUDNN: 8+&lt;/li&gt; &#xA; &lt;li&gt;OS: Ubuntu 20.04, CentOS 7.9&lt;/li&gt; &#xA; &lt;li&gt;GPU: Nvidia-A10 24G&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Memory Optimization&lt;/h2&gt; &#xA;&lt;p&gt;Jemalloc are recommanded to install for optimizing the memory from above 30G to below 20G. Here is an example for installing Jemalloc in Modelscope notebook.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;apt-get install -y libjemalloc-dev&#xA;export LD_PRELOAD=/lib/x86_64-linux-gnu/libjemalloc.so&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Installation Guide&lt;/h2&gt; &#xA;&lt;p&gt;The following installation methods are supported:&lt;/p&gt; &#xA;&lt;h3&gt;1. ModelScope notebook„Äêrecommended„Äë&lt;/h3&gt; &#xA;&lt;p&gt;The ModelScope Notebook offers a free-tier that allows ModelScope user to run the FaceChain application with minimum setup, refer to &lt;a href=&#34;https://modelscope.cn/my/mynotebook/preset&#34;&gt;ModelScope Notebook&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Step1: ÊàëÁöÑnotebook -&amp;gt; PAI-DSW -&amp;gt; GPUÁéØÂ¢É&#xA;# Note: Please use: ubuntu20.04-py38-torch2.0.1-tf1.15.5-modelscope1.8.1&#xA;&#xA;# Step2: Entry the Notebook cellÔºåclone FaceChain from github:&#xA;!GIT_LFS_SKIP_SMUDGE=1 git clone https://github.com/modelscope/facechain.git --depth 1&#xA;&#xA;# Step3: Change the working directory to facechain, and install the dependencies:&#xA;import os&#xA;os.chdir(&#39;/mnt/workspace/facechain&#39;)    # You may change to your own path&#xA;print(os.getcwd())&#xA;&#xA;!pip3 install gradio==3.47.1&#xA;!pip3 install controlnet_aux==0.0.6&#xA;!pip3 install python-slugify&#xA;!pip3 install diffusers==0.29.0&#xA;!pip3 install peft==0.11.1&#xA;&#xA;# Step4: Start the app service, click &#34;public URL&#34; or &#34;local URL&#34;, upload your images to &#xA;# train your own model and then generate your digital twin.&#xA;!python3 app.py&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, you may also purchase a &lt;a href=&#34;https://www.aliyun.com/activity/bigdata/pai/dsw&#34;&gt;PAI-DSW&lt;/a&gt; instance (using A10 resource), with the option of ModelScope image to run FaceChain following similar steps.&lt;/p&gt; &#xA;&lt;h3&gt;2. Docker&lt;/h3&gt; &#xA;&lt;p&gt;If you are familiar with using docker, we recommend to use this way:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Step1: Prepare the environment with GPU on local or cloud, we recommend to use Alibaba Cloud ECS, refer to: https://www.aliyun.com/product/ecs&#xA;&#xA;# Step2: Download the docker image (for installing docker engine, refer to https://docs.docker.com/engine/install/Ôºâ&#xA;# For China Mainland users:&#xA;docker pull registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.7.1-py38-torch2.0.1-tf1.15.5-1.8.1&#xA;# For users outside China Mainland:&#xA;docker pull registry.us-west-1.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.7.1-py38-torch2.0.1-tf1.15.5-1.8.1&#xA;&#xA;# Step3: run the docker container&#xA;docker run -it --name facechain -p 7860:7860 --gpus all registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.7.1-py38-torch2.0.1-tf1.15.5-1.8.1 /bin/bash&#xA;# Note: you may need to install the nvidia-container-runtime, follow the instructions:&#xA;# 1. Install nvidia-container-runtimeÔºöhttps://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html&#xA;# 2. sudo systemctl restart docker&#xA;&#xA;# Step4: Install the gradio in the docker container:&#xA;pip3 install gradio==3.47.1&#xA;pip3 install controlnet_aux==0.0.6&#xA;pip3 install python-slugify&#xA;pip3 install diffusers==0.29.0&#xA;pip3 install peft==0.11.1&#xA;&#xA;# Step5 clone facechain from github&#xA;GIT_LFS_SKIP_SMUDGE=1 git clone https://github.com/modelscope/facechain.git --depth 1&#xA;cd facechain&#xA;python3 app.py&#xA;# Note: FaceChain currently assume single-GPU, if your environment has multiple GPU, please use the following instead:&#xA;# CUDA_VISIBLE_DEVICES=0 python3 app.py&#xA;&#xA;# Step6&#xA;Run the app server: click &#34;public URL&#34; --&amp;gt; in the form of: https://xxx.gradio.live&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;3. stable-diffusion-webui&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Select the &lt;code&gt;Extensions Tab&lt;/code&gt;, then choose &lt;code&gt;Install From URL&lt;/code&gt; (official plugin integration is integrated, please install from URL currently). &lt;img src=&#34;https://raw.githubusercontent.com/modelscope/facechain/main/resources/sdwebui_install.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Switch to &lt;code&gt;Installed&lt;/code&gt;, check the FaceChain plugin, then click &lt;code&gt;Apply and restart UI&lt;/code&gt;. It may take a while for installing the dependencies and downloading the models. Make sure that the &#34;CUDA Toolkit&#34; is installed correctly, otherwise the &#34;mmcv&#34; package cannot be successfully installed. &lt;img src=&#34;https://raw.githubusercontent.com/modelscope/facechain/main/resources/sdwebui_restart.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;After the page refreshes, the appearance of the &lt;code&gt;FaceChain&lt;/code&gt; Tab indicates a successful installation. &lt;img src=&#34;https://raw.githubusercontent.com/modelscope/facechain/main/resources/sdwebui_success.jpg&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Script Execution&lt;/h1&gt; &#xA;&lt;p&gt;FaceChain supports direct inference in the python environment. When inferring for Infinite Style Portrait generation, please edit the code in run_inference.py:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Use pose control, default False&#xA;use_pose_model = False&#xA;# The path of the input image containing ID information for portrait generation&#xA;input_img_path = &#39;poses/man/pose2.png&#39;&#xA;# The path of the image for pose control, only effective when using pose control&#xA;pose_image = &#39;poses/man/pose1.png&#39;&#xA;# The number of images to generate in inference&#xA;num_generate = 5&#xA;# The weight for the style model, see styles for detail&#xA;multiplier_style = 0.25&#xA;# Specify a folder to save the generated images, this parameter can be modified as needed&#xA;output_dir = &#39;./generated&#39;&#xA;# The index of the chosen base model, see facechain/constants.py for detail&#xA;base_model_idx = 0&#xA;# The index of the style model, see styles for detail&#xA;style_idx = 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then execute:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python run_inference.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can find the generated personal digital image photos in the &lt;code&gt;output_dir&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When inferring for Fixed Templates Portrait generation, please edit the code in run_inference_inpaint.py.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Number of faces for the template image&#xA;num_faces = 1&#xA;# Index of face for inpainting, counting from left to right&#xA;selected_face = 1&#xA;# The strength for inpainting, you do not need to change the parameter&#xA;strength = 0.6&#xA;# The path of the template image&#xA;inpaint_img = &#39;poses/man/pose1.png&#39;&#xA;# The path of the input image containing ID information for portrait generation&#xA;input_img_path = &#39;poses/man/pose2.png&#39;&#xA;# The number of images to generate in inference&#xA;num_generate = 1&#xA;# Specify a folder to save the generated images, this parameter can be modified as needed&#xA;output_dir = &#39;./generated_inpaint&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then execute:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python run_inference_inpaint.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can find the generated personal digital image photos in the &lt;code&gt;output_dir&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Algorithm Introduction&lt;/h1&gt; &#xA;&lt;p&gt;The capability of AI portraits generation comes from the large generative models like Stable Diffusion and its fine-tuning techniques. Due to the strong generalization capability of large models, it is possible to perform downstream tasks by fine-tuning on specific types of data and tasks, while preserving the model&#39;s overall ability of text following and image generation. The technical foundation of train-based and train-free AI portraits generation comes from applying different fine-tuning tasks to generative models. Currently, most existing AI portraits tools adopt a two-stage ‚Äútrain then generate‚Äù pipeline, where the fine-tuning task is ‚Äúto generate portrait photos of a fixed character ID‚Äù, and the corresponding training data are multiple images of the fixed character ID. The effectiveness of such train-based pipeline depends on the scale of the training data, thus requiring certain image data support and training time, which also increases the cost for users.&lt;/p&gt; &#xA;&lt;p&gt;Different from train-based pipeline, train-free pipeline adjusts the fine-tuning task to ‚Äúgenerate portrait photos of a specified character ID‚Äù, meaning that the character ID image (face photo) is used as an additional input, and the output is a portrait photo preserving the input ID. Such a pipeline completely separates offline training from online inference, allowing users to generate portraits directly based on the fine-tuned model with only one photo in just 10 seconds, avoiding the cost for extensive data and training time. The fine-tuning task of train-free AI portraits generation is based on the adapter module. Face photos are processed through an image encoder with fixed weights and a parameter-efficient feature projection layer to obtain aligned features, and are then fed into the U-Net model of Stable Diffusion through attention mechanism similar as text conditions. At this point, face information as an independent branch condition is fed into the model alongside text information for inference, thereby enabling the generated images to maintain ID fidelity.&lt;/p&gt; &#xA;&lt;p&gt;The basic algorithm based on face adapter is capable of achieving train-free AI portraits, but still requires certain adjustments to further improve its effectiveness. Existing train-free portrait tools generally suffer from the following issues: poor image quality of portraits, inadequate text following and style retention abilities in portraits, poor controllability and richness of portrait faces, and poor compatibility with extensions like ControlNet and style Lora. To address these issues, FaceChain attribute them to the fact that the fine-tuning tasks for existing train-free AI portrait tools have coupled with too much information beyond character IDs, and propose FaceChain Face Adapter with Decoupled Training (FaceChain FACT) to solve these problems. By fine-tuning the Stable Diffusion model on millions of portrait data, FaceChain FACT can achieve high-quality portrait image generation for specified character IDs. The entire framework of FaceChain FACT is shown in the figure below.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/modelscope/facechain/main/resources/framework.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The decoupled training of FaceChain FACT consists of two parts: decoupling face from image, and decoupling ID from face. Existing methods often treat denoising portrait images as the fine-tuning task, which makes the model hard to accurately focus on the face area, thereby affecting the text-to-image ability of the base Stable Diffusion model. FaceChain FACT draws on the sequential processing and regional control advantages of face-swapping algorithms and implements the fine-tuning method for decoupling faces from images from both structural and training strategy aspects. Structurally, unlike existing methods that use a parallel cross-attention mechanism to process face and text information, FaceChain FACT adopts a sequential processing approach as an independent adapter layer inserted into the original Stable Diffusion&#39;s blocks. This way, face adaptation acts as an independent step similar to face-swapping during the denoising process, avoiding interference between face and text conditions. In terms of training strategy, besides the original MSE loss function, FaceChain FACT introduces the Face Adapting Incremental Regularization (FAIR) loss function, which controls the feature increment of the face adaptation step in the adapter layer to focus on the face region. During inference, users can flexibly adjust the generated effects by modifying the weight of the face adapter, balancing fidelity and generalization of the face while maintaining the text-to-image ability of Stable Diffusion. The FAIR loss function is formulated as follows:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/modelscope/facechain/main/resources/FAIR.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Furthermore, addressing the issue of poor controllability and richness of generated faces, FaceChain FACT proposes a training method for decoupling ID from face, so that the portrait process only preserves the character ID rather than the entire face. Firstly, to better extract the ID information from the face while maintaining certain key facial details, and to better adapt to the structure of Stable Diffusion, FaceChain FACT employs a face feature extractor named &lt;a href=&#34;https://github.com/DanJun6737/TransFace&#34;&gt;TransFace&lt;/a&gt; based on the Transformer architecture, which is pre-trained on a large-scale face dataset. All tokens from the penultimate layer are subsequently fed into a simple attention query model for feature projection, thereby ensuring that the extracted ID features meet the aforementioned requirements. Additionally, during the training process, FaceChain FACT uses the Classifier Free Guidance (CFG) method to perform random shuffle and drop for different portrait images of the same ID, thus ensuring that the input face images and the target images used for denoising may have different faces with the same ID, thus further preventing the model from overfitting to non-ID information of the face. As such, FaceChain FACT possesses high compatibility with the massive exquisite styles of FaceChain, which is shown as follows.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/modelscope/facechain/main/resources/generated_examples.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Model List&lt;/h2&gt; &#xA;&lt;p&gt;The models used in FaceChain:&lt;/p&gt; &#xA;&lt;p&gt;[1] Face recognition model TransFaceÔºö&lt;a href=&#34;https://www.modelscope.cn/models/iic/cv_vit_face-recognition&#34;&gt;https://www.modelscope.cn/models/iic/cv_vit_face-recognition&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[2] Face detection model DamoFDÔºö&lt;a href=&#34;https://modelscope.cn/models/damo/cv_ddsar_face-detection_iclr23-damofd&#34;&gt;https://modelscope.cn/models/damo/cv_ddsar_face-detection_iclr23-damofd&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[3] Human parsing model M2FPÔºö&lt;a href=&#34;https://modelscope.cn/models/damo/cv_resnet101_image-multiple-human-parsing&#34;&gt;https://modelscope.cn/models/damo/cv_resnet101_image-multiple-human-parsing&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[4] Skin retouching model ABPNÔºö&lt;a href=&#34;https://www.modelscope.cn/models/damo/cv_unet_skin_retouching_torch&#34;&gt;https://www.modelscope.cn/models/damo/cv_unet_skin_retouching_torch&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[5] Face fusion modelÔºö&lt;a href=&#34;https://www.modelscope.cn/models/damo/cv_unet_face_fusion_torch&#34;&gt;https://www.modelscope.cn/models/damo/cv_unet_face_fusion_torch&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[6] FaceChain FACT model: &lt;a href=&#34;https://www.modelscope.cn/models/yucheng1996/FaceChain-FACT&#34;&gt;https://www.modelscope.cn/models/yucheng1996/FaceChain-FACT&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[7] Face attribute recognition model FairFace: &lt;a href=&#34;https://modelscope.cn/models/damo/cv_resnet34_face-attribute-recognition_fairface&#34;&gt;https://modelscope.cn/models/damo/cv_resnet34_face-attribute-recognition_fairface&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;More Information&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/modelscope/modelscope/&#34;&gt;ModelScope library&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;‚Äã ModelScope Library provides the foundation for building the model-ecosystem of ModelScope, including the interface and implementation to integrate various models into ModelScope.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://modelscope.cn/docs/ModelScope%E6%A8%A1%E5%9E%8B%E6%8E%A5%E5%85%A5%E6%B5%81%E7%A8%8B%E6%A6%82%E8%A7%88&#34;&gt;Contribute models to ModelScope&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;This project is licensed under the &lt;a href=&#34;https://github.com/modelscope/modelscope/raw/master/LICENSE&#34;&gt;Apache License (Version 2.0)&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>IDEA-Research/Grounded-SAM-2</title>
    <updated>2024-11-24T01:40:00Z</updated>
    <id>tag:github.com,2024-11-24:/IDEA-Research/Grounded-SAM-2</id>
    <link href="https://github.com/IDEA-Research/Grounded-SAM-2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Grounded SAM 2: Ground and Track Anything in Videos with Grounding DINO, Florence-2 and SAM 2&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Grounded SAM 2: Ground and Track Anything in Videos&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://ai.meta.com/research/&#34;&gt;IDEA-Research&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://rentainhe.github.io/&#34;&gt;Tianhe Ren&lt;/a&gt;, &lt;a href=&#34;https://github.com/ShuoShenDe&#34;&gt;Shuo Shen&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://arxiv.org/abs/2408.00714&#34;&gt;&lt;code&gt;SAM2 Paper&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/2303.05499&#34;&gt;&lt;code&gt;Grounding DINO Paper&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/2405.10300&#34;&gt;&lt;code&gt;Grounding DINO 1.5 Paper&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/#citation&#34;&gt;&lt;code&gt;BibTeX&lt;/code&gt;&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/f0fb0022-779a-49fb-8f46-3a18a8b4e893&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/assets/grounded_sam_2_intro.jpg&#34; alt=&#34;Video Name&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;üî• Project Highlight&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Grounded SAM 2 is a foundation model pipeline towards grounding and track anything in Videos with &lt;a href=&#34;https://arxiv.org/abs/2303.05499&#34;&gt;Grounding DINO&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2405.10300&#34;&gt;Grounding DINO 1.5&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2311.06242&#34;&gt;Florence-2&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2408.00714&#34;&gt;SAM 2&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In this repo, we&#39;ve supported the following demo with &lt;strong&gt;simple implementations&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Ground and Segment Anything&lt;/strong&gt; with Grounding DINO, Grounding DINO 1.5 &amp;amp; 1.6 and SAM 2&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Ground and Track Anything&lt;/strong&gt; with Grounding DINO, Grounding DINO 1.5 &amp;amp; 1.6 and SAM 2&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Detect, Segment and Track Visualization&lt;/strong&gt; based on the powerful &lt;a href=&#34;https://github.com/roboflow/supervision&#34;&gt;supervision&lt;/a&gt; library.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Grounded SAM 2 does not introduce significant methodological changes compared to &lt;a href=&#34;https://arxiv.org/abs/2401.14159&#34;&gt;Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks&lt;/a&gt;. Both approaches leverage the capabilities of open-world models to address complex visual tasks. Consequently, we try to &lt;strong&gt;simplify the code implementation&lt;/strong&gt; in this repository, aiming to enhance user convenience.&lt;/p&gt; &#xA;&lt;h2&gt;Latest updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;2024/10/24&lt;/code&gt;: Support &lt;a href=&#34;https://docs.ultralytics.com/guides/sahi-tiled-inference/&#34;&gt;SAHI (Slicing Aided Hyper Inference)&lt;/a&gt; on Grounded SAM 2 (with Grounding DINO 1.5) which may be helpful for inferencing high resolution image with dense small objects (e.g. &lt;strong&gt;4K&lt;/strong&gt; images).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2024/10/10&lt;/code&gt;: Support &lt;code&gt;SAM-2.1&lt;/code&gt; models, if you want to use &lt;code&gt;SAM 2.1&lt;/code&gt; model, you need to update to the latest code and reinstall SAM 2 follow &lt;a href=&#34;https://github.com/facebookresearch/sam2?tab=readme-ov-file#latest-updates&#34;&gt;SAM 2.1 Installation&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2024/08/31&lt;/code&gt;: Support &lt;code&gt;dump json results&lt;/code&gt; in Grounded SAM 2 Image Demos (with Grounding DINO).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2024/08/20&lt;/code&gt;: Support &lt;strong&gt;Florence-2 SAM 2 Image Demo&lt;/strong&gt; which includes &lt;code&gt;dense region caption&lt;/code&gt;, &lt;code&gt;object detection&lt;/code&gt;, &lt;code&gt;phrase grounding&lt;/code&gt;, and cascaded auto-label pipeline &lt;code&gt;caption + phrase grounding&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2024/08/09&lt;/code&gt;: Support &lt;strong&gt;Ground and Track New Object&lt;/strong&gt; throughout the whole videos. This feature is still under development now. Credits to &lt;a href=&#34;https://github.com/ShuoShenDe&#34;&gt;Shuo Shen&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2024/08/07&lt;/code&gt;: Support &lt;strong&gt;Custom Video Inputs&lt;/strong&gt;, users need only submit their video file (e.g. &lt;code&gt;.mp4&lt;/code&gt; file) with specific text prompts to get an impressive demo videos.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/#grounded-sam-2-demos&#34;&gt;Grounded SAM 2 Demos&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/#grounded-sam-2-image-demo-with-grounding-dino&#34;&gt;Grounded SAM 2 Image Demo&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/#grounded-sam-2-image-demo-with-grounding-dino-15--16&#34;&gt;Grounded SAM 2 Image Demo (with Grounding DINO 1.5 &amp;amp; 1.6)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/#sahi-slicing-aided-hyper-inference-with-grounding-dino-15-and-sam-2&#34;&gt;Grounded SAM 2 with SAHI for High Resolution Image Inference&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/#automatically-saving-grounding-results-image-demo&#34;&gt;Automatically Saving Grounding and Segmentation Results&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/#grounded-sam-2-video-object-tracking-demo&#34;&gt;Grounded SAM 2 Video Object Tracking Demo&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/#grounded-sam-2-video-object-tracking-demo-with-grounding-dino-15--16&#34;&gt;Grounded SAM 2 Video Object Tracking Demo (with Grounding DINO 1.5 &amp;amp; 1.6)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/#grounded-sam-2-video-object-tracking-demo-with-custom-video-input-with-grounding-dino&#34;&gt;Grounded SAM 2 Video Object Tracking with Custom Video Input (using Grounding DINO)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/#grounded-sam-2-video-object-tracking-demo-with-custom-video-input-with-grounding-dino-15--16&#34;&gt;Grounded SAM 2 Video Object Tracking with Custom Video Input (using Grounding DINO 1.5 &amp;amp; 1.6)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/#grounded-sam-2-video-object-tracking-with-continuous-id-with-grounding-dino&#34;&gt;Grounded SAM 2 Video Object Tracking with Continues ID (using Grounding DINO)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/#grounded-sam-2-florence-2-demos&#34;&gt;Grounded SAM 2 Florence-2 Demos&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/#grounded-sam-2-florence-2-image-demo&#34;&gt;Grounded SAM 2 Florence-2 Image Demo&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/#grounded-sam-2-florence-2-image-auto-labeling-demo&#34;&gt;Grounded SAM 2 Florence-2 Image Auto-Labeling Demo&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Download the pretrained &lt;code&gt;SAM 2&lt;/code&gt; checkpoints:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd checkpoints&#xA;bash download_ckpts.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download the pretrained &lt;code&gt;Grounding DINO&lt;/code&gt; checkpoints:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd gdino_checkpoints&#xA;bash download_ckpts.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Installation without docker&lt;/h3&gt; &#xA;&lt;p&gt;Install PyTorch environment first. We use &lt;code&gt;python=3.10&lt;/code&gt;, as well as &lt;code&gt;torch &amp;gt;= 2.3.1&lt;/code&gt;, &lt;code&gt;torchvision&amp;gt;=0.18.1&lt;/code&gt; and &lt;code&gt;cuda-12.1&lt;/code&gt; in our environment to run this demo. Please follow the instructions &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;here&lt;/a&gt; to install both PyTorch and TorchVision dependencies. Installing both PyTorch and TorchVision with CUDA support is strongly recommended. You can easily install the latest version of PyTorch as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip3 install torch torchvision torchaudio&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Since we need the CUDA compilation environment to compile the &lt;code&gt;Deformable Attention&lt;/code&gt; operator used in Grounding DINO, we need to check whether the CUDA environment variables have been set correctly (which you can refer to &lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO?tab=readme-ov-file#hammer_and_wrench-install&#34;&gt;Grounding DINO Installation&lt;/a&gt; for more details). You can set the environment variable manually as follows if you want to build a local GPU environment for Grounding DINO to run Grounded SAM 2:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CUDA_HOME=/path/to/cuda-12.1/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install &lt;code&gt;Segment Anything 2&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install &lt;code&gt;Grounding DINO&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install --no-build-isolation -e grounding_dino&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Installation with docker&lt;/h3&gt; &#xA;&lt;p&gt;Build the Docker image and Run the Docker container:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd Grounded-SAM-2&#xA;make build-image&#xA;make run&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After executing these commands, you will be inside the Docker environment. The working directory within the container is set to: &lt;code&gt;/home/appuser/Grounded-SAM-2&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Once inside the Docker environment, you can start the demo by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python grounded_sam2_tracking_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Grounded SAM 2 Demos&lt;/h2&gt; &#xA;&lt;h3&gt;Grounded SAM 2 Image Demo (with Grounding DINO)&lt;/h3&gt; &#xA;&lt;p&gt;Note that &lt;code&gt;Grounding DINO&lt;/code&gt; has already been supported in &lt;a href=&#34;https://huggingface.co/IDEA-Research/grounding-dino-tiny&#34;&gt;Huggingface&lt;/a&gt;, so we provide two choices for running &lt;code&gt;Grounded SAM 2&lt;/code&gt; model:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Use huggingface API to inference Grounding DINO (which is simple and clear)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python grounded_sam2_hf_model_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] üö® If you encounter network issues while using the &lt;code&gt;HuggingFace&lt;/code&gt; model, you can resolve them by setting the appropriate mirror source as &lt;code&gt;export HF_ENDPOINT=https://hf-mirror.com&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Load local pretrained Grounding DINO checkpoint and inference with Grounding DINO original API (make sure you&#39;ve already downloaded the pretrained checkpoint)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python grounded_sam2_local_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Grounded SAM 2 Image Demo (with Grounding DINO 1.5 &amp;amp; 1.6)&lt;/h3&gt; &#xA;&lt;p&gt;We&#39;ve already released our most capable open-set detection model &lt;a href=&#34;https://github.com/IDEA-Research/Grounding-DINO-1.5-API&#34;&gt;Grounding DINO 1.5 &amp;amp; 1.6&lt;/a&gt;, which can be combined with SAM 2 for stronger open-set detection and segmentation capability. You can apply the API token first and run Grounded SAM 2 with Grounding DINO 1.5 as follows:&lt;/p&gt; &#xA;&lt;p&gt;Install the latest DDS cloudapi:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install dds-cloudapi-sdk --upgrade&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Apply your API token from our official website here: &lt;a href=&#34;https://deepdataspace.com/request_api&#34;&gt;request API token&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python grounded_sam2_gd1.5_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;SAHI (Slicing Aided Hyper Inference) with Grounding DINO 1.5 and SAM 2&lt;/h3&gt; &#xA;&lt;p&gt;If your images are high resolution with dense objects, directly using Grounding DINO 1.5 for inference on the original image may not be the best choice. We support &lt;a href=&#34;https://docs.ultralytics.com/guides/sahi-tiled-inference/&#34;&gt;SAHI (Slicing Aided Hyper Inference)&lt;/a&gt;, which works by first dividing the original image into smaller overlapping patches. Inference is then performed separately on each patch, and the final detection results are merged. This method is highly effective and accuracy for dense and small objects detection in high resolution images.&lt;/p&gt; &#xA;&lt;p&gt;You can run SAHI inference by setting the following param in &lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/grounded_sam2_gd1.5_demo.py&#34;&gt;grounded_sam2_gd1.5_demo.py&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;WITH_SLICE_INFERENCE = True&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The visualization is shown as follows:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Text Prompt&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Input Image&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Grounded SAM 2&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Grounded SAM 2 with SAHI&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;Person&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/IDEA-Research/detrex-storage/raw/main/assets/grounded_sam_2/demo_images/dense%20people.png?raw=true&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/IDEA-Research/detrex-storage/raw/main/assets/grounded_sam_2/grounding_dino_1.5_slice_inference/grounded_sam2_annotated_image_with_mask.jpg?raw=true&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/IDEA-Research/detrex-storage/raw/main/assets/grounded_sam_2/grounding_dino_1.5_slice_inference/grounded_sam2_annotated_image_with_mask_with_slice_inference.jpg?raw=true&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Notes:&lt;/strong&gt; We only support SAHI on Grounding DINO 1.5 because it works better with stronger grounding model which may produce less hallucination results.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Automatically Saving Grounding Results (Image Demo)&lt;/h3&gt; &#xA;&lt;p&gt;After setting &lt;code&gt;DUMP_JSON_RESULTS=True&lt;/code&gt; in the following Grounded SAM 2 Image Demos:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/grounded_sam2_local_demo.py&#34;&gt;grounded_sam2_local_demo.py&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/grounded_sam2_hf_model_demo.py&#34;&gt;grounded_sam2_hf_model_demo.py&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/grounded_sam2_gd1.5_demo.py&#34;&gt;grounded_sam2_gd1.5_demo.py&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The &lt;code&gt;grounding&lt;/code&gt; and &lt;code&gt;segmentation&lt;/code&gt; results will be automatically saved in the &lt;code&gt;outputs&lt;/code&gt; dir with the following format:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;{&#xA;    &#34;image_path&#34;: &#34;path/to/image.jpg&#34;,&#xA;    &#34;annotations&#34;: [&#xA;        {&#xA;            &#34;class_name&#34;: &#34;class_name&#34;,&#xA;            &#34;bbox&#34;: [x1, y1, x2, y2],&#xA;            &#34;segmentation&#34;: {&#xA;                &#34;size&#34;: [h, w],&#xA;                &#34;counts&#34;: &#34;rle_encoded_mask&#34;&#xA;            },&#xA;            &#34;score&#34;: confidence score&#xA;        }&#xA;    ],&#xA;    &#34;box_format&#34;: &#34;xyxy&#34;,&#xA;    &#34;img_width&#34;: w,&#xA;    &#34;img_height&#34;: h&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Grounded SAM 2 Video Object Tracking Demo&lt;/h3&gt; &#xA;&lt;p&gt;Based on the strong tracking capability of SAM 2, we can combined it with Grounding DINO for open-set object segmentation and tracking. You can run the following scripts to get the tracking results with Grounded SAM 2:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python grounded_sam2_tracking_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The tracking results of each frame will be saved in &lt;code&gt;./tracking_results&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;The video will be save as &lt;code&gt;children_tracking_demo_video.mp4&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;You can refine this file with different text prompt and video clips yourself to get more tracking results.&lt;/li&gt; &#xA; &lt;li&gt;We only prompt the first video frame with Grounding DINO here for simple usage.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Support Various Prompt Type for Tracking&lt;/h4&gt; &#xA;&lt;p&gt;We&#39;ve supported different types of prompt for Grounded SAM 2 tracking demo:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Point Prompt&lt;/strong&gt;: In order to &lt;strong&gt;get a stable segmentation results&lt;/strong&gt;, we re-use the SAM 2 image predictor to get the prediction mask from each object based on Grounding DINO box outputs, then we &lt;strong&gt;uniformly sample points from the prediction mask&lt;/strong&gt; as point prompts for SAM 2 video predictor&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Box Prompt&lt;/strong&gt;: We directly use the box outputs from Grounding DINO as box prompts for SAM 2 video predictor&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Mask Prompt&lt;/strong&gt;: We use the SAM 2 mask prediction results based on Grounding DINO box outputs as mask prompt for SAM 2 video predictor.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/assets/g_sam2_tracking_pipeline_vis_new.png&#34; alt=&#34;Grounded SAM 2 Tracking Pipeline&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Grounded SAM 2 Video Object Tracking Demo (with Grounding DINO 1.5 &amp;amp; 1.6)&lt;/h3&gt; &#xA;&lt;p&gt;We&#39;ve also support video object tracking demo based on our stronger &lt;code&gt;Grounding DINO 1.5&lt;/code&gt; model and &lt;code&gt;SAM 2&lt;/code&gt;, you can try the following demo after applying the API keys for running &lt;code&gt;Grounding DINO 1.5&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python grounded_sam2_tracking_demo_with_gd1.5.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Grounded SAM 2 Video Object Tracking Demo with Custom Video Input (with Grounding DINO)&lt;/h3&gt; &#xA;&lt;p&gt;Users can upload their own video file (e.g. &lt;code&gt;assets/hippopotamus.mp4&lt;/code&gt;) and specify their custom text prompts for grounding and tracking with Grounding DINO and SAM 2 by using the following scripts:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python grounded_sam2_tracking_demo_custom_video_input_gd1.0_hf_model.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are not convenient to use huggingface demo, you can also run tracking demo with local grounding dino model with the following scripts:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python grounded_sam2_tracking_demo_custom_video_input_gd1.0_local_model.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Grounded SAM 2 Video Object Tracking Demo with Custom Video Input (with Grounding DINO 1.5 &amp;amp; 1.6)&lt;/h3&gt; &#xA;&lt;p&gt;Users can upload their own video file (e.g. &lt;code&gt;assets/hippopotamus.mp4&lt;/code&gt;) and specify their custom text prompts for grounding and tracking with Grounding DINO 1.5 and SAM 2 by using the following scripts:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python grounded_sam2_tracking_demo_custom_video_input_gd1.5.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can specify the params in this file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;VIDEO_PATH = &#34;./assets/hippopotamus.mp4&#34;&#xA;TEXT_PROMPT = &#34;hippopotamus.&#34;&#xA;OUTPUT_VIDEO_PATH = &#34;./hippopotamus_tracking_demo.mp4&#34;&#xA;API_TOKEN_FOR_GD1_5 = &#34;Your API token&#34; # api token for G-DINO 1.5&#xA;PROMPT_TYPE_FOR_VIDEO = &#34;mask&#34; # using SAM 2 mask prediction as prompt for video predictor&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After running our demo code, you can get the tracking results as follows:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/1fbdc6f4-3e50-4221-9600-98c397beecdf&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/assets/hippopotamus_seg.jpg&#34; alt=&#34;Video Name&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;And we will automatically save the tracking visualization results in &lt;code&gt;OUTPUT_VIDEO_PATH&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!WARNING] We initialize the box prompts on the first frame of the input video. If you want to start from different frame, you can refine &lt;code&gt;ann_frame_idx&lt;/code&gt; by yourself in our code.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Grounded-SAM-2 Video Object Tracking with Continuous ID (with Grounding DINO)&lt;/h3&gt; &#xA;&lt;p&gt;In above demos, we only prompt Grounded SAM 2 in specific frame, which may not be friendly to find new object during the whole video. In this demo, we try to &lt;strong&gt;find new objects&lt;/strong&gt; and assign them with new ID across the whole video, this function is &lt;strong&gt;still under develop&lt;/strong&gt;. it&#39;s not that stable now.&lt;/p&gt; &#xA;&lt;p&gt;Users can upload their own video files and specify custom text prompts for grounding and tracking using the Grounding DINO and SAM 2 frameworks. To do this, execute the script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python grounded_sam2_tracking_demo_with_continuous_id.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can customize various parameters including:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;text&lt;/code&gt;: The grounding text prompt.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;video_dir&lt;/code&gt;: Directory containing the video files.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;output_dir&lt;/code&gt;: Directory to save the processed output.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;output_video_path&lt;/code&gt;: Path for the output video.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;step&lt;/code&gt;: Frame stepping for processing.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;box_threshold&lt;/code&gt;: box threshold for groundingdino model&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;text_threshold&lt;/code&gt;: text threshold for groundingdino model Note: This method supports only the mask type of text prompt.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;After running our demo code, you can get the tracking results as follows:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/d3f91ad0-3d32-43c4-a0dc-0bed661415f4&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/assets/tracking_car_mask_1.jpg&#34; alt=&#34;Video Name&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to try &lt;code&gt;Grounding DINO 1.5&lt;/code&gt; model, you can run the following scripts after setting your API token:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python grounded_sam2_tracking_demo_with_continuous_id_gd1.5.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Grounded-SAM-2 Video Object Tracking with Continuous ID plus Reverse Tracking(with Grounding DINO)&lt;/h3&gt; &#xA;&lt;p&gt;This method could simply cover the whole lifetime of the object&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python grounded_sam2_tracking_demo_with_continuous_id_plus.py&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Grounded SAM 2 Florence-2 Demos&lt;/h2&gt; &#xA;&lt;h3&gt;Grounded SAM 2 Florence-2 Image Demo&lt;/h3&gt; &#xA;&lt;p&gt;In this section, we will explore how to integrate the feature-rich and robust open-source models &lt;a href=&#34;https://arxiv.org/abs/2311.06242&#34;&gt;Florence-2&lt;/a&gt; and SAM 2 to develop practical applications.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2311.06242&#34;&gt;Florence-2&lt;/a&gt; is a powerful vision foundation model by Microsoft which supports a series of vision tasks by prompting with special &lt;code&gt;task_prompt&lt;/code&gt; includes but not limited to:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Task&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Task Prompt&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Text Input&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Task Introduction&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Object Detection&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;&amp;lt;OD&amp;gt;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úò&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Detect main objects with single category name&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Dense Region Caption&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;&amp;lt;DENSE_REGION_CAPTION&amp;gt;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úò&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Detect main objects with short description&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Region Proposal&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;&amp;lt;REGION_PROPOSAL&amp;gt;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úò&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Generate proposals without category name&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Phrase Grounding&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;&amp;lt;CAPTION_TO_PHRASE_GROUNDING&amp;gt;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úî&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Ground main objects in image mentioned in caption&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Referring Expression Segmentation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;&amp;lt;REFERRING_EXPRESSION_SEGMENTATION&amp;gt;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úî&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Ground the object which is most related to the text input&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Open Vocabulary Detection and Segmentation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;&amp;lt;OPEN_VOCABULARY_DETECTION&amp;gt;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úî&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Ground any object with text input&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Integrate &lt;code&gt;Florence-2&lt;/code&gt; with &lt;code&gt;SAM-2&lt;/code&gt;, we can build a strong vision pipeline to solve complex vision tasks, you can try the following scripts to run the demo:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] üö® If you encounter network issues while using the &lt;code&gt;HuggingFace&lt;/code&gt; model, you can resolve them by setting the appropriate mirror source as &lt;code&gt;export HF_ENDPOINT=https://hf-mirror.com&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;Object Detection and Segmentation&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python grounded_sam2_florence2_image_demo.py \&#xA;    --pipeline object_detection_segmentation \&#xA;    --image_path ./notebooks/images/cars.jpg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dense Region Caption and Segmentation&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python grounded_sam2_florence2_image_demo.py \&#xA;    --pipeline dense_region_caption_segmentation \&#xA;    --image_path ./notebooks/images/cars.jpg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Region Proposal and Segmentation&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python grounded_sam2_florence2_image_demo.py \&#xA;    --pipeline region_proposal_segmentation \&#xA;    --image_path ./notebooks/images/cars.jpg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Phrase Grounding and Segmentation&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python grounded_sam2_florence2_image_demo.py \&#xA;    --pipeline phrase_grounding_segmentation \&#xA;    --image_path ./notebooks/images/cars.jpg \&#xA;    --text_input &#34;The image shows two vintage Chevrolet cars parked side by side, with one being a red convertible and the other a pink sedan, \&#xA;            set against the backdrop of an urban area with a multi-story building and trees. \&#xA;            The cars have Cuban license plates, indicating a location likely in Cuba.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Referring Expression Segmentation&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python grounded_sam2_florence2_image_demo.py \&#xA;    --pipeline referring_expression_segmentation \&#xA;    --image_path ./notebooks/images/cars.jpg \&#xA;    --text_input &#34;The left red car.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Open-Vocabulary Detection and Segmentation&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python grounded_sam2_florence2_image_demo.py \&#xA;    --pipeline open_vocabulary_detection_segmentation \&#xA;    --image_path ./notebooks/images/cars.jpg \&#xA;    --text_input &#34;car &amp;lt;and&amp;gt; building&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Note that if you want to &lt;strong&gt;detect multiple classes&lt;/strong&gt; you should split them with &lt;code&gt;&amp;lt;and&amp;gt;&lt;/code&gt; in your input text.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Grounded SAM 2 Florence-2 Image Auto-Labeling Demo&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;Florence-2&lt;/code&gt; can be used as a auto image annotator by cascading its caption capability with its grounding capability.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Task&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Task Prompt&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Text Input&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Caption + Phrase Grounding&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;&amp;lt;CAPTION&amp;gt;&lt;/code&gt; + &lt;code&gt;&amp;lt;CAPTION_TO_PHRASE_GROUNDING&amp;gt;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úò&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Detailed Caption + Phrase Grounding&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;&amp;lt;DETAILED_CAPTION&amp;gt;&lt;/code&gt; + &lt;code&gt;&amp;lt;CAPTION_TO_PHRASE_GROUNDING&amp;gt;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úò&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;More Detailed Caption + Phrase Grounding&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;&amp;lt;MORE_DETAILED_CAPTION&amp;gt;&lt;/code&gt; + &lt;code&gt;&amp;lt;CAPTION_TO_PHRASE_GROUNDING&amp;gt;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úò&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;You can try the following scripts to run these demo:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Caption to Phrase Grounding&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python grounded_sam2_florence2_autolabel_pipeline.py \&#xA;    --image_path ./notebooks/images/groceries.jpg \&#xA;    --pipeline caption_to_phrase_grounding \&#xA;    --caption_type caption&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can specify &lt;code&gt;caption_type&lt;/code&gt; to control the granularity of the caption, if you want a more detailed caption, you can try &lt;code&gt;--caption_type detailed_caption&lt;/code&gt; or &lt;code&gt;--caption_type more_detailed_caption&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Citation&lt;/h3&gt; &#xA;&lt;p&gt;If you find this project helpful for your research, please consider citing the following BibTeX entry.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-BibTex&#34;&gt;@misc{ravi2024sam2segmentimages,&#xA;      title={SAM 2: Segment Anything in Images and Videos}, &#xA;      author={Nikhila Ravi and Valentin Gabeur and Yuan-Ting Hu and Ronghang Hu and Chaitanya Ryali and Tengyu Ma and Haitham Khedr and Roman R√§dle and Chloe Rolland and Laura Gustafson and Eric Mintun and Junting Pan and Kalyan Vasudev Alwala and Nicolas Carion and Chao-Yuan Wu and Ross Girshick and Piotr Doll√°r and Christoph Feichtenhofer},&#xA;      year={2024},&#xA;      eprint={2408.00714},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV},&#xA;      url={https://arxiv.org/abs/2408.00714}, &#xA;}&#xA;&#xA;@article{liu2023grounding,&#xA;  title={Grounding dino: Marrying dino with grounded pre-training for open-set object detection},&#xA;  author={Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Li, Chunyuan and Yang, Jianwei and Su, Hang and Zhu, Jun and others},&#xA;  journal={arXiv preprint arXiv:2303.05499},&#xA;  year={2023}&#xA;}&#xA;&#xA;@misc{ren2024grounding,&#xA;      title={Grounding DINO 1.5: Advance the &#34;Edge&#34; of Open-Set Object Detection}, &#xA;      author={Tianhe Ren and Qing Jiang and Shilong Liu and Zhaoyang Zeng and Wenlong Liu and Han Gao and Hongjie Huang and Zhengyu Ma and Xiaoke Jiang and Yihao Chen and Yuda Xiong and Hao Zhang and Feng Li and Peijun Tang and Kent Yu and Lei Zhang},&#xA;      year={2024},&#xA;      eprint={2405.10300},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&#xA;@misc{ren2024grounded,&#xA;      title={Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks}, &#xA;      author={Tianhe Ren and Shilong Liu and Ailing Zeng and Jing Lin and Kunchang Li and He Cao and Jiayu Chen and Xinyu Huang and Yukang Chen and Feng Yan and Zhaoyang Zeng and Hao Zhang and Feng Li and Jie Yang and Hongyang Li and Qing Jiang and Lei Zhang},&#xA;      year={2024},&#xA;      eprint={2401.14159},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&#xA;@article{kirillov2023segany,&#xA;  title={Segment Anything}, &#xA;  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Doll{\&#39;a}r, Piotr and Girshick, Ross},&#xA;  journal={arXiv:2304.02643},&#xA;  year={2023}&#xA;}&#xA;&#xA;@misc{jiang2024trex2,&#xA;      title={T-Rex2: Towards Generic Object Detection via Text-Visual Prompt Synergy}, &#xA;      author={Qing Jiang and Feng Li and Zhaoyang Zeng and Tianhe Ren and Shilong Liu and Lei Zhang},&#xA;      year={2024},&#xA;      eprint={2403.14610},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>