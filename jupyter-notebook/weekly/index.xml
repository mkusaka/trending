<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-02-04T01:53:20Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>krishnaik06/Complete-Langchain-Tutorials</title>
    <updated>2024-02-04T01:53:20Z</updated>
    <id>tag:github.com,2024-02-04:/krishnaik06/Complete-Langchain-Tutorials</id>
    <link href="https://github.com/krishnaik06/Complete-Langchain-Tutorials" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Complete-Langchain-Tutorials&lt;/h1&gt;</summary>
  </entry>
  <entry>
    <title>bowang-lab/MedSAM</title>
    <updated>2024-02-04T01:53:20Z</updated>
    <id>tag:github.com,2024-02-04:/bowang-lab/MedSAM</id>
    <link href="https://github.com/bowang-lab/MedSAM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Segment Anything in Medical Images&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MedSAM&lt;/h1&gt; &#xA;&lt;p&gt;This is the official repository for MedSAM: Segment Anything in Medical Images.&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2024.01.15: Welcome to join &lt;a href=&#34;https://www.codabench.org/competitions/1847/&#34;&gt;CVPR 2024 Challenge: MedSAM on Laptop&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;2024.01.15: Release &lt;a href=&#34;https://github.com/bowang-lab/MedSAM/raw/LiteMedSAM/README.md&#34;&gt;LiteMedSAM&lt;/a&gt; and &lt;a href=&#34;https://github.com/bowang-lab/MedSAMSlicer&#34;&gt;3D Slicer Plugin&lt;/a&gt;, 10x faster than MedSAM!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create a virtual environment &lt;code&gt;conda create -n medsam python=3.10 -y&lt;/code&gt; and activate it &lt;code&gt;conda activate medsam&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;Pytorch 2.0&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;git clone https://github.com/bowang-lab/MedSAM&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Enter the MedSAM folder &lt;code&gt;cd MedSAM&lt;/code&gt; and run &lt;code&gt;pip install -e .&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;p&gt;Download the &lt;a href=&#34;https://drive.google.com/drive/folders/1ETWmi4AiniJeWOt6HAsYgTjYv_fkgzoN?usp=drive_link&#34;&gt;model checkpoint&lt;/a&gt; and place it at e.g., &lt;code&gt;work_dir/MedSAM/medsam_vit_b&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;We provide three ways to quickly test the model on your images&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Command line&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python MedSAM_Inference.py # segment the demo image&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Segment other images with the following flags&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;-i input_img&#xA;-o output path&#xA;--box bounding box of the segmentation target&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Jupyter-notebook&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;We provide a step-by-step tutorial on &lt;a href=&#34;https://colab.research.google.com/drive/19WNtRMbpsxeqimBlmJwtd1dzpaIvK2FZ?usp=sharing&#34;&gt;CoLab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can also run it locally with &lt;code&gt;tutorial_quickstart.ipynb&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;GUI&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Install &lt;code&gt;PyQt5&lt;/code&gt; with &lt;a href=&#34;https://pypi.org/project/PyQt5/&#34;&gt;pip&lt;/a&gt;: &lt;code&gt;pip install PyQt5 &lt;/code&gt; or &lt;a href=&#34;https://anaconda.org/anaconda/pyqt&#34;&gt;conda&lt;/a&gt;: &lt;code&gt;conda install -c anaconda pyqt&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python gui.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Load the image to the GUI and specify segmentation targets by drawing bounding boxes.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/bowang-lab/MedSAM/assets/19947331/a8d94b4d-0221-4d09-a43a-1251842487ee&#34;&gt;https://github.com/bowang-lab/MedSAM/assets/19947331/a8d94b4d-0221-4d09-a43a-1251842487ee&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Model Training&lt;/h2&gt; &#xA;&lt;h3&gt;Data preprocessing&lt;/h3&gt; &#xA;&lt;p&gt;Download &lt;a href=&#34;https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth&#34;&gt;SAM checkpoint&lt;/a&gt; and place it at &lt;code&gt;work_dir/SAM/sam_vit_b_01ec64.pth&lt;/code&gt; .&lt;/p&gt; &#xA;&lt;p&gt;Download the demo &lt;a href=&#34;https://zenodo.org/record/7860267&#34;&gt;dataset&lt;/a&gt; and unzip it to &lt;code&gt;data/FLARE22Train/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This dataset contains 50 abdomen CT scans and each scan contains an annotation mask with 13 organs. The names of the organ label are available at &lt;a href=&#34;https://flare22.grand-challenge.org/&#34;&gt;MICCAI FLARE2022&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Run pre-processing&lt;/p&gt; &#xA;&lt;p&gt;Install &lt;code&gt;cc3d&lt;/code&gt;: &lt;code&gt;pip install connected-components-3d&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python pre_CT_MR.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;split dataset: 80% for training and 20% for testing&lt;/li&gt; &#xA; &lt;li&gt;adjust CT scans to &lt;a href=&#34;https://radiopaedia.org/articles/windowing-ct&#34;&gt;soft tissue&lt;/a&gt; window level (40) and width (400)&lt;/li&gt; &#xA; &lt;li&gt;max-min normalization&lt;/li&gt; &#xA; &lt;li&gt;resample image size to &lt;code&gt;1024x2014&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;save the pre-processed images and labels as &lt;code&gt;npy&lt;/code&gt; files&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Training on multiple GPUs (Recommend)&lt;/h3&gt; &#xA;&lt;p&gt;The model was trained on five A100 nodes and each node has four GPUs (80G) (20 A100 GPUs in total). Please use the slurm script to start the training process.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbatch train_multi_gpus.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When the training process is done, please convert the checkpoint to SAM&#39;s format for convenient inference.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python utils/ckpt_convert.py # Please set the corresponding checkpoint path first&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Training on one GPU&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python train_one_gpu.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you only want to train the mask decoder, please check the tutorial on the &lt;a href=&#34;https://github.com/bowang-lab/MedSAM/tree/0.1&#34;&gt;0.1 branch&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We highly appreciate all the challenge organizers and dataset owners for providing the public dataset to the community.&lt;/li&gt; &#xA; &lt;li&gt;We thank Meta AI for making the source code of &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;segment anything&lt;/a&gt; publicly available.&lt;/li&gt; &#xA; &lt;li&gt;We also thank Alexandre Bonnet for sharing this great &lt;a href=&#34;https://encord.com/blog/learn-how-to-fine-tune-the-segment-anything-model-sam/&#34;&gt;blog&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Reference&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{MedSAM,&#xA;  title={Segment Anything in Medical Images},&#xA;  author={Ma, Jun and He, Yuting and Li, Feifei and Han, Lin and You, Chenyu and Wang, Bo},&#xA;  journal={Nature Communications},&#xA;  volume={15},&#xA;  pages={1--9},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>fivethirtyeight/data</title>
    <updated>2024-02-04T01:53:20Z</updated>
    <id>tag:github.com,2024-02-04:/fivethirtyeight/data</id>
    <link href="https://github.com/fivethirtyeight/data" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Data and code behind the articles and graphics at FiveThirtyEight&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/repo-size/fivethirtyeight/data&#34; alt=&#34;GitHub repo size&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://github.com/fivethirtyeight/data/raw/master/index.csv&#34;&gt;index&lt;/a&gt; for a list of the data and code we&#39;ve published and their accompanying stories.&lt;/p&gt; &#xA;&lt;p&gt;As of June 13, 2023, sports predictions and forecasts are &lt;a href=&#34;https://awfulannouncing.com/disney/fivethirtyeight-no-more-sports-forecasts.html&#34;&gt;no longer being updated&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Unless otherwise noted, our data sets are available under the &lt;a href=&#34;https://creativecommons.org/licenses/by/4.0/&#34;&gt;Creative Commons Attribution 4.0 International License&lt;/a&gt;, and the code is available under the &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;MIT License&lt;/a&gt;. If you find this information useful, please &lt;a href=&#34;mailto:contact@fivethirtyeight.com&#34;&gt;let us know&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>