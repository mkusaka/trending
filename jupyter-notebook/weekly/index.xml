<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-10-16T01:44:24Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ossamamehmood/Hacktoberfest2022</title>
    <updated>2022-10-16T01:44:24Z</updated>
    <id>tag:github.com,2022-10-16:/ossamamehmood/Hacktoberfest2022</id>
    <link href="https://github.com/ossamamehmood/Hacktoberfest2022" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Hacktoberfest 2022 OPEN FIRST Pull Request - FREE T-SHIRTðŸŽ‰&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Hacktoberfest 2022 &lt;code&gt;OPEN FIRST&lt;/code&gt; Pull Request - &lt;code&gt;FREE T-SHIRT&lt;/code&gt;ðŸŽ‰&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/ossamamehmood/Hacktoberfest2022/raw/main/.github/logo.png&#34; alt=&#34;Hacktoberfest 2021&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributors of &lt;code&gt;Hacktoberfest 2022&lt;/code&gt;&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/ossamamehmood/Hacktoberfest2022/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=ossamamehmood/Hacktoberfest2022&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;This project is perfect for your first pull request&lt;/h2&gt; &#xA;&lt;p&gt;ðŸ—£ &lt;strong&gt;Hacktoberfest encourages participation in the open source community, which grows bigger every year. Complete the challenge and earn a limited edition T-shirt.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;ðŸ“¢ &lt;strong&gt;Register &lt;a href=&#34;https://hacktoberfest.digitalocean.com&#34;&gt;here&lt;/a&gt; for Hacktoberfest and make four pull requests (PRs) between October 1st-31st to grab free SWAGS ðŸ”¥.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://img.shields.io/badge/hacktoberfest-2022-blueviolet&#34; alt=&#34;Hacktober Badge&#34;&gt; &#xA; &lt;img src=&#34;https://img.shields.io/static/v1?label=%F0%9F%8C%9F&amp;amp;message=If%20Useful&amp;amp;style=style=flat&amp;amp;color=BC4E99&#34; alt=&#34;Star Badge&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/ossamamehmood&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Contributions-welcome-violet.svg?style=flat&amp;amp;logo=git&#34; alt=&#34;Contributions&#34;&gt;&lt;/a&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/ossamamehmood/hacktoberfest2022/pulls&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-pr/ossamamehmood/hacktoberfest2022&#34; alt=&#34;Pull Requests Badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ossamamehmood/hacktoberfest2022/graphs/contributors&#34;&gt;&lt;img alt=&#34;GitHub contributors&#34; src=&#34;https://img.shields.io/github/contributors/ossamamehmood/hacktoberfest2022?color=2b9348&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ossamamehmood/hacktoberfest2022/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/ossamamehmood/hacktoberfest2022?color=2b9348&#34; alt=&#34;License Badge&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Instructions-&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fork this Repository using the button at the top on right corner.&lt;/li&gt; &#xA; &lt;li&gt;Clone your forked repository to your pc ( git clone &#34;url from clone option.)&lt;/li&gt; &#xA; &lt;li&gt;Create a new branch for your modifications (ie. &lt;code&gt;git branch new-user&lt;/code&gt; and check it out &lt;code&gt;git checkout new-user&lt;/code&gt; or simply do &lt;code&gt;git checkout -b new-user&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Add your profile image in &lt;code&gt;static/images/&lt;/code&gt; ( use drag and drop option or upload by commands.)&lt;/li&gt; &#xA; &lt;li&gt;Add your profile data in &lt;code&gt;content/participant/&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Add your files (&lt;code&gt;git add -A&lt;/code&gt;), commit (&lt;code&gt;git commit -m &#34;added myself&#34;&lt;/code&gt;) and push (&lt;code&gt;git push origin new-user&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Create a pull request&lt;/li&gt; &#xA; &lt;li&gt;Star this repository&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;How to make your first Pull Request&lt;/h1&gt; &#xA;&lt;p&gt;Replace &lt;code&gt;&amp;lt;YOUR-USERNAME&amp;gt;&lt;/code&gt; with your GitHub username in this guide.&lt;/p&gt; &#xA;&lt;h2&gt;1. Add your profile picture to the folder&lt;/h2&gt; &#xA;&lt;p&gt;Add a picture picture of your choice in &lt;code&gt;static/images/&lt;/code&gt;. Accepted files are &lt;strong&gt;png&lt;/strong&gt; and &lt;strong&gt;jpg&lt;/strong&gt;, should be squared and minimum size 544x544 pixels. Ex.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;static/images/&amp;lt;YOUR-USERNAME&amp;gt;.png&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;2. Add your profile information&lt;/h2&gt; &#xA;&lt;p&gt;Create a markdown file in your folder following the convention &lt;code&gt;content/participant/&amp;lt;YOUR-USERNAME&amp;gt;.md&lt;/code&gt;. Ex.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;content/participant/&amp;lt;YOUR-USERNAME&amp;gt;.md&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Copy the next template into your file, delete the boilerplate data and fill the information with yours.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;---&#xA;name: YOURNAME&#xA;institution/company: INSTITUTION_NAME&#xA;github:USERNAME&#xA;---&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h2&gt; OR &lt;/h2&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;3. Create / Upload Your Code / Algorithms&lt;/h2&gt; &#xA;&lt;p&gt;Create/Upload your code in folder following the convention &lt;code&gt;Add Code Here&lt;/code&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Choose an extact lanaguage folder &lt;code&gt;drop your code&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Below is &lt;code&gt;an example&lt;/code&gt; to add file properly&lt;/li&gt; &#xA; &lt;li&gt;You can follow up &lt;code&gt;any languages&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;Add Code Here/PYTHON/&amp;lt;YOUR-FILERNAME&amp;gt;.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;Add Code Here/C++/&amp;lt;YOUR-FILERNAME&amp;gt;.cpp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can follow any pathway a &lt;code&gt;code&lt;/code&gt; or &lt;code&gt;profile information&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;4. Wait for Pull Request to merge&lt;/h2&gt; &#xA;&lt;h2&gt;5. Celebrate - you&#39;ve done your first pull request!!&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;&#39;&#39;&#39;&#xA;Always make more then 4 pull requests.&#xA;Lets say you have made only 4 pull request to different projects,&#xA;but one project is excluded from hackoctoberfest event then your pull request will not be counted and &#xA;then you have remaining 3 valid pull requests if these projects is not excluded.&#xA;If you fail to make 4 pull requests then you can&#39;t get swags or t-shirts.&#xA;I will recommend you to make pull request to your own repo which is very very safest side for you..&#xA;keep in mind that repo has hacktoberfest topic..&#xA;&#39;&#39;&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;+ Follow Me : } Quick Approval of Pull Request&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;&#39;&#39;&#39;&#xA;To get approval of the pull request much quicker and faster (`Follow Me`)ðŸš€&#xA;&#39;&#39;&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;a href=&#34;https://github.com/ossamamehmood&#34;&gt;&lt;kbd&gt;&lt;img src=&#34;https://avatars3.githubusercontent.com/ossamamehmood?size=100&#34; width=&#34;100px;&#34; alt=&#34;&#34;&gt;&lt;/kbd&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Ossama Mehmood&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&#xA;&lt;br&gt;</summary>
  </entry>
  <entry>
    <title>bloc97/CrossAttentionControl</title>
    <updated>2022-10-16T01:44:24Z</updated>
    <id>tag:github.com,2022-10-16:/bloc97/CrossAttentionControl</id>
    <link href="https://github.com/bloc97/CrossAttentionControl" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Unofficial implementation of &#34;Prompt-to-Prompt Image Editing with Cross Attention Control&#34; with Stable Diffusion&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Cross Attention Control with Stable Diffusion&lt;/h1&gt; &#xA;&lt;p&gt;Unofficial implementation of &#34;Prompt-to-Prompt Image Editing with Cross Attention Control&#34; with Stable Diffusion, some modifications were made to the methods described in the paper in order to make them work with Stable Diffusion.&lt;/p&gt; &#xA;&lt;p&gt;Paper: &lt;a href=&#34;https://arxiv.org/abs/2208.01626&#34;&gt;https://arxiv.org/abs/2208.01626&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;What is Cross Attention Control?&lt;/h2&gt; &#xA;&lt;p&gt;Large-scale language-image models (eg. Stable Diffusion) are usually hard to control just with editing the prompts alone and can be very unpredictable and unintuitive for users. Most existing methods require the user to input a mask which is cumbersome and might not yield good results if the mask has an inadequate shape. Cross Attention Control allows much finer control of the prompt by modifying the internal attention maps of the diffusion model during inference without the need for the user to input a mask and does so with minimal performance penalities (compared to clip guidance) and no additional training or fine-tuning of the diffusion model.&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;This notebook uses the following libraries: &lt;code&gt;torch transformers diffusers numpy PIL tqdm difflib&lt;/code&gt;&lt;br&gt; The last known working version of &lt;code&gt;diffusers&lt;/code&gt; for the notebook is &lt;code&gt;diffusers==0.4.1&lt;/code&gt;. A different version of diffusers might cause errors as this notebook injects code into the model and any code change from the &lt;code&gt;diffusers&lt;/code&gt; library is likely to break compatibility. Simply install the required libraries using &lt;code&gt;pip&lt;/code&gt; and run the jupyter notebook, some examples are given inside.&lt;br&gt; A description of the parameters are given at the end of the readme.&lt;/p&gt; &#xA;&lt;p&gt;Alternatively there is this easy-to-follow colab demo by &lt;a href=&#34;https://github.com/Lewington-pitsos&#34;&gt;Lewington-pitsos&lt;/a&gt;: &lt;a href=&#34;https://colab.research.google.com/drive/1PsWKXtqAAoDz-KGB45VeCXdTsqW-Mumo&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Results/Demonstrations&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;All images shown below are generated using the same seed. The initial and target images must be generated with the same seed for cross attention control to work.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;New: Image inversion&lt;/h2&gt; &#xA;&lt;p&gt;This method takes an existing image and finds its corresponding gaussian latent vector using a modified inverse DDIM process that keeps compatibility with other ODE schedulers such as K-LMS, then edits using prompt to prompt editing with cross attention control. A finite difference gradient descent method that corrects for high CFG values is also provided. It allows inversion with higher CFG values (eg. 3.0-5.0), while without it only CFG values below 3.0 are usable.&lt;/p&gt; &#xA;&lt;p&gt;Middle: Original image&lt;br&gt; Top left: Reconstructed image using the prompt &lt;code&gt;a photo of a woman with blonde hair&lt;/code&gt;&lt;br&gt; Clockwise: See &lt;a href=&#34;https://github.com/bloc97/CrossAttentionControl/raw/main/InverseCrossAttention_Release.ipynb&#34;&gt;InverseCrossAttention_Release.ipynb&lt;/a&gt; for the prompts in order.&lt;br&gt; Note that some fine tuning on the prompts have been done to make these images consistent. For example, when changing the hair color, sometimes the person starts smiling, which can be removed by adding a &lt;code&gt;smile&lt;/code&gt; token in the prompt and adjust its weight downwards using cross attention control. &lt;img src=&#34;https://github.com/bloc97/CrossAttentionControl/raw/main/images/faces_test.png?raw=true&#34; alt=&#34;Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Target replacement&lt;/h2&gt; &#xA;&lt;p&gt;Top left prompt: &lt;code&gt;[a cat] sitting on a car&lt;/code&gt;&lt;br&gt; Clockwise: &lt;code&gt;a smiling dog...&lt;/code&gt;, &lt;code&gt;a hamster...&lt;/code&gt;, &lt;code&gt;a tiger...&lt;/code&gt;&lt;br&gt; Note: different strength values for &lt;code&gt;prompt_edit_spatial_start&lt;/code&gt; were used, clockwise: &lt;code&gt;0.7&lt;/code&gt;, &lt;code&gt;0.5&lt;/code&gt;, &lt;code&gt;1.0&lt;/code&gt; &lt;img src=&#34;https://github.com/bloc97/CrossAttentionControl/raw/main/images/fouranimals.png?raw=true&#34; alt=&#34;Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Style injection&lt;/h2&gt; &#xA;&lt;p&gt;Top left prompt: &lt;code&gt;a fantasy landscape with a maple forest&lt;/code&gt;&lt;br&gt; Clockwise: &lt;code&gt;a watercolor painting of...&lt;/code&gt;, &lt;code&gt;a van gogh painting of...&lt;/code&gt;, &lt;code&gt;a charcoal pencil sketch of...&lt;/code&gt;&lt;br&gt; &lt;img src=&#34;https://github.com/bloc97/CrossAttentionControl/raw/main/images/fourstyles.png?raw=true&#34; alt=&#34;Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Global editing&lt;/h2&gt; &#xA;&lt;p&gt;Top left prompt: &lt;code&gt;a fantasy landscape with a pine forest&lt;/code&gt;&lt;br&gt; Clockwise: &lt;code&gt;..., autumn&lt;/code&gt;, &lt;code&gt;..., winter&lt;/code&gt;, &lt;code&gt;..., spring, green&lt;/code&gt;&lt;br&gt; &lt;img src=&#34;https://github.com/bloc97/CrossAttentionControl/raw/main/images/fourseasons.png?raw=true&#34; alt=&#34;Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Reducing unpredictability when modifying prompts&lt;/h2&gt; &#xA;&lt;p&gt;Left image prompt: &lt;code&gt;a fantasy landscape with a pine forest&lt;/code&gt;&lt;br&gt; Right image prompt: &lt;code&gt;a winter fantasy landscape with a pine forest&lt;/code&gt;&lt;br&gt; Middle image: Cross attention enabled prompt editing (left image -&amp;gt; right image)&lt;br&gt; &lt;img src=&#34;https://github.com/bloc97/CrossAttentionControl/raw/main/images/a%20fantasy%20landscape%20with%20a%20pine%20forest%20-%20a%20winter%20fantasy%20landscape%20with%20a%20pine%20forest.png?raw=true&#34; alt=&#34;Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Left image prompt: &lt;code&gt;a fantasy landscape with a pine forest&lt;/code&gt;&lt;br&gt; Right image prompt: &lt;code&gt;a watercolor painting of a landscape with a pine forest&lt;/code&gt;&lt;br&gt; Middle image: Cross attention enabled prompt editing (left image -&amp;gt; right image)&lt;br&gt; &lt;img src=&#34;https://github.com/bloc97/CrossAttentionControl/raw/main/images/a%20fantasy%20landscape%20with%20a%20pine%20forest%20-%20a%20watercolor%20painting%20of%20a%20landscape%20with%20a%20pine%20forest.png?raw=true&#34; alt=&#34;Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Left image prompt: &lt;code&gt;a fantasy landscape with a pine forest&lt;/code&gt;&lt;br&gt; Right image prompt: &lt;code&gt;a fantasy landscape with a pine forest and a river&lt;/code&gt;&lt;br&gt; Middle image: Cross attention enabled prompt editing (left image -&amp;gt; right image)&lt;br&gt; &lt;img src=&#34;https://github.com/bloc97/CrossAttentionControl/raw/main/images/a%20fantasy%20landscape%20with%20a%20pine%20forest%20-%20A%20fantasy%20landscape%20with%20a%20pine%20forest%20and%20a%20river.png?raw=true&#34; alt=&#34;Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Direct token attention control&lt;/h2&gt; &#xA;&lt;p&gt;Left image prompt: &lt;code&gt;a fantasy landscape with a pine forest&lt;/code&gt;&lt;br&gt; Towards the right: &lt;code&gt;-fantasy&lt;/code&gt; &lt;img src=&#34;https://github.com/bloc97/CrossAttentionControl/raw/main/images/a%20fantasy%20landscape%20with%20a%20pine%20forest%20-%20decrease%20fantasy.png?raw=true&#34; alt=&#34;Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Left image prompt: &lt;code&gt;a fantasy landscape with a pine forest&lt;/code&gt;&lt;br&gt; Towards the right: &lt;code&gt;+fantasy&lt;/code&gt; and &lt;code&gt;+forest&lt;/code&gt; &lt;img src=&#34;https://github.com/bloc97/CrossAttentionControl/raw/main/images/a%20fantasy%20landscape%20with%20a%20pine%20forest%20-%20increase%20fantasy%20and%20forest.png?raw=true&#34; alt=&#34;Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Left image prompt: &lt;code&gt;a fantasy landscape with a pine forest&lt;/code&gt;&lt;br&gt; Towards the right: &lt;code&gt;-fog&lt;/code&gt; &lt;img src=&#34;https://github.com/bloc97/CrossAttentionControl/raw/main/images/a%20fantasy%20landscape%20with%20a%20pine%20forest%20-%20decrease%20fog.png?raw=true&#34; alt=&#34;Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Left image: from previous example&lt;br&gt; Towards the right: &lt;code&gt;-rocks&lt;/code&gt; &lt;img src=&#34;https://github.com/bloc97/CrossAttentionControl/raw/main/images/a%20fantasy%20landscape%20with%20a%20pine%20forest%20-%20decrease%20rocks.png?raw=true&#34; alt=&#34;Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Comparison to standard prompt editing&lt;/h2&gt; &#xA;&lt;p&gt;Let&#39;s compare our results above where we removed fog and rocks from our fantasy landscape using cross attention maps against what people usually do, by editing the prompt alone.&lt;br&gt; We can first try adding &#34;without fog and without rocks&#34; to our prompt.&lt;/p&gt; &#xA;&lt;p&gt;Image prompt: &lt;code&gt;A fantasy landscape with a pine forest without fog and without rocks&lt;/code&gt;&lt;br&gt; However, we still see fog and rocks.&lt;br&gt; &lt;img src=&#34;https://github.com/bloc97/CrossAttentionControl/raw/main/images/A%20fantasy%20landscape%20with%20a%20pine%20forest%20without%20fog%20and%20without%20rocks.png?raw=true&#34; alt=&#34;Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;We can try adding words like dry, sunny and grass.&lt;br&gt; Image prompt: &lt;code&gt;A fantasy landscape with a pine forest without fog and rocks, dry sunny day, grass&lt;/code&gt;&lt;br&gt; There are less rocks and fog, but the image&#39;s composition and style is completely different from before and we still haven&#39;t obtained our desired fog and rock-free image...&lt;br&gt; &lt;img src=&#34;https://github.com/bloc97/CrossAttentionControl/raw/main/images/A%20fantasy%20landscape%20with%20a%20pine%20forest%20without%20fog%20and%20rocks%2C%20dry%20sunny%20day%2C%20grass.png?raw=true&#34; alt=&#34;Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Two functions are included, &lt;code&gt;stablediffusion(...)&lt;/code&gt; which generates images and &lt;code&gt;prompt_token(...)&lt;/code&gt; that is used to help the user find the token index for words in the prompt, which is used to tweak token weights in &lt;code&gt;prompt_edit_token_weights&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Parameters of &lt;code&gt;stabledifusion(...)&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name = Default Value&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Example&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;prompt=&#34;&#34;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;the prompt as a string&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;&#34;a cat riding a bicycle&#34;&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;prompt_edit=None&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;the second prompt as a string, used to edit the first prompt using cross attention, set &lt;code&gt;None&lt;/code&gt; to disable&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;&#34;a dog riding a bicycle&#34;&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;prompt_edit_token_weights=[]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;values to scale the importance of the tokens in cross attention layers, as a list of tuples representing &lt;code&gt;(token id, strength)&lt;/code&gt;, this is used to increase or decrease the importance of a word in the prompt, it is applied to &lt;code&gt;prompt_edit&lt;/code&gt; when possible (if &lt;code&gt;prompt_edit&lt;/code&gt; is &lt;code&gt;None&lt;/code&gt;, weights are applied to &lt;code&gt;prompt&lt;/code&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;[(2, 2.5), (6, -5.0)]&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;prompt_edit_tokens_start=0.0&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;how strict is the generation with respect to the initial prompt, increasing this will let the network be more creative for smaller details/textures, should be smaller than &lt;code&gt;prompt_edit_tokens_end&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;0.0&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;prompt_edit_tokens_end=1.0&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;how strict is the generation with respect to the initial prompt, decreasing this will let the network be more creative for larger features/general scene composition, should be bigger than &lt;code&gt;prompt_edit_tokens_start&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;1.0&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;prompt_edit_spatial_start=0.0&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;how strict is the generation with respect to the initial image &lt;em&gt;(generated from the first prompt, not from img2img)&lt;/em&gt;, increasing this will let the network be more creative for smaller details/textures, should be smaller than &lt;code&gt;prompt_edit_spatial_end&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;0.0&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;prompt_edit_spatial_end=1.0&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;how strict is the generation with respect to the initial image &lt;em&gt;(generated from the first prompt, not from img2img)&lt;/em&gt;, decreasing this will let the network be more creative for larger features/general scene composition, should be bigger than &lt;code&gt;prompt_edit_spatial_start&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;1.0&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;guidance_scale=7.5&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;standard classifier-free guidance strength for stable diffusion&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;7.5&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;steps=50&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;number of diffusion steps as an integer, higher usually produces better images but is slower&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;50&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;seed=None&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;random seed as an integer, set &lt;code&gt;None&lt;/code&gt; to use a random seed&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;126794873&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;width=512&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;image width&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;512&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;height=512&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;image height&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;512&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;init_image=None&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;init image for image to image generation, as a PIL image, it will be resized to &lt;code&gt;width x height&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;PIL.Image()&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;init_image_strength=0.5&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;strength of the noise added for image to image generation, higher will make the generation care less about the initial image&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;0.5&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Parameters of &lt;code&gt;inversestabledifusion(...)&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name = Default Value&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Example&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;init_image&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;the image to invert&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;PIL.Image(&#34;portrait.png&#34;)&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;prompt=&#34;&#34;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;the prompt as a string used for inversion&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;&#34;portrait of a person&#34;&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;guidance_scale=3.0&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;standard classifier-free guidance strength for stable diffusion&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;3.0&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;steps=50&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;number of diffusion steps used for inversion, as an integer&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;50&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;refine_iterations=3&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;inversion refinement iterations for high CFG values, set to 0 to disable refinement when using lower CFG values, for higher CFG values, consider increasing it. Higher values slow down the algorithm significantly.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;3&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;refine_strength=0.9&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;initial strength value for the refinement steps, the internal strength is adaptive&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;0.9&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;refine_skip=0.7&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;how many diffusion steps of refinement are skipped (value between &lt;code&gt;0.0&lt;/code&gt; and &lt;code&gt;1.0&lt;/code&gt;), there is usually no need to refine earlier diffusion steps as CFG is not very important in lower time steps, higher values will skip even more steps&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;0.7&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt;</summary>
  </entry>
  <entry>
    <title>rasbt/python-machine-learning-book-3rd-edition</title>
    <updated>2022-10-16T01:44:24Z</updated>
    <id>tag:github.com,2022-10-16:/rasbt/python-machine-learning-book-3rd-edition</id>
    <link href="https://github.com/rasbt/python-machine-learning-book-3rd-edition" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The &#34;Python Machine Learning (3rd edition)&#34; book code repository&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Python Machine Learning (3rd Ed.) Code Repository&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/#&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Python-3.7-blue.svg?sanitize=true&#34; alt=&#34;Python 3.6&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/LICENSE.txt&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code%20License-MIT-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Code repositories for the 1st and 2nd edition are available at&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rasbt/python-machine-learning-book&#34;&gt;https://github.com/rasbt/python-machine-learning-book&lt;/a&gt; and&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rasbt/python-machine-learning-book-2nd-edition&#34;&gt;https://github.com/rasbt/python-machine-learning-book-2nd-edition&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Python Machine Learning, 3rd Ed.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;to be published December 12th, 2019&lt;/p&gt; &#xA;&lt;p&gt;Paperback: 770 pages&lt;br&gt; Publisher: Packt Publishing&lt;br&gt; Language: English&lt;/p&gt; &#xA;&lt;p&gt;ISBN-10: 1789955750&lt;br&gt; ISBN-13: 978-1789955750&lt;br&gt; Kindle ASIN: B07VBLX2W7&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.amazon.com/Python-Machine-Learning-scikit-learn-TensorFlow/dp/1789955750/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/.other/cover_1.jpg&#34; width=&#34;248&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Links&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.amazon.com/Python-Machine-Learning-scikit-learn-TensorFlow/dp/1789955750/&#34;&gt;Amazon Page&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.packtpub.com/data/python-machine-learning-third-edition&#34;&gt;Packt Page&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Table of Contents and Code Notebooks&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Helpful installation and setup instructions can be found in the &lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch01/README.md&#34;&gt;README.md file of Chapter 1&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Please note that these are just the code examples accompanying the book, which we uploaded for your convenience; be aware that these notebooks may not be useful without the formulae and descriptive text.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Machine Learning - Giving Computers the Ability to Learn from Data [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch01&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Training Machine Learning Algorithms for Classification [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch02&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;A Tour of Machine Learning Classifiers Using Scikit-Learn [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch03&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Building Good Training Sets â€“ Data Pre-Processing [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch04&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Compressing Data via Dimensionality Reduction [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch05&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Learning Best Practices for Model Evaluation and Hyperparameter Optimization [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch06&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Combining Different Models for Ensemble Learning [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch07&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Applying Machine Learning to Sentiment Analysis [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch08&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Embedding a Machine Learning Model into a Web Application [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch09&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Predicting Continuous Target Variables with Regression Analysis [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch10&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Working with Unlabeled Data â€“ Clustering Analysis [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch11&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Implementing a Multi-layer Artificial Neural Network from Scratch [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch12&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Parallelizing Neural Network Training with TensorFlow [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch13&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Going Deeper: The Mechanics of TensorFlow [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch14&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Classifying Images with Deep Convolutional Neural Networks [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch15&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Modeling Sequential Data Using Recurrent Neural Networks [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch16&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Generative Adversarial Networks for Synthesizing New Data [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch17&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Reinforcement Learning for Decision Making in Complex Environments [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch18&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;Raschka, Sebastian, and Vahid Mirjalili. &lt;em&gt;Python Machine Learning, 3rd Ed&lt;/em&gt;. Packt Publishing, 2019.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@book{RaschkaMirjalili2019,  &#xA;address = {Birmingham, UK},  &#xA;author = {Raschka, Sebastian and Mirjalili, Vahid},  &#xA;edition = {3},  &#xA;isbn = {978-1789955750},   &#xA;publisher = {Packt Publishing},  &#xA;title = {{Python Machine Learning, 3rd Ed.}},  &#xA;year = {2019}  &#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>