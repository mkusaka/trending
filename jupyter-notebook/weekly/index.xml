<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-07-28T01:39:22Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Nixtla/nixtla</title>
    <updated>2024-07-28T01:39:22Z</updated>
    <id>tag:github.com,2024-07-28:/Nixtla/nixtla</id>
    <link href="https://github.com/Nixtla/nixtla" rel="alternate"></link>
    <summary type="html">&lt;p&gt;TimeGPT-1: production ready pre-trained Time Series Foundation Model for forecasting and anomaly detection. Generative pretrained transformer for time series trained on over 100B data points. It&#39;s capable of accurately predicting various domains such as retail, electricity, finance, and IoT with just a few lines of code üöÄ.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Nixtla &amp;nbsp; &lt;a href=&#34;https://twitter.com/intent/tweet?text=Statistical%20Forecasting%20Algorithms%20by%20Nixtla%20&amp;amp;url=https://github.com/Nixtla/neuralforecast&amp;amp;via=nixtlainc&amp;amp;hashtags=StatisticalModels,TimeSeries,Forecasting&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/url/http/shields.io.svg?style=social&#34; alt=&#34;Tweet&#34;&gt;&lt;/a&gt; &amp;nbsp;&lt;a href=&#34;https://join.slack.com/t/nixtlacommunity/shared_invite/zt-1pmhan9j5-F54XR20edHk0UtYAPcW4KQ&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Slack-4A154B?&amp;amp;logo=slack&amp;amp;logoColor=white&#34; alt=&#34;Slack&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/Nixtla/neuralforecast/main/nbs/imgs_indx/logo_new.png&#34;&gt; &#xA; &lt;h1 align=&#34;center&#34;&gt;TimeGPT-1 &lt;/h1&gt; &#xA; &lt;h3 align=&#34;center&#34;&gt;The first foundation model for forecasting and anomaly detection&lt;/h3&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/Nixtla/nixtla/actions/workflows/ci.yaml&#34;&gt;&lt;img src=&#34;https://github.com/Nixtla/nixtla/actions/workflows/ci.yaml/badge.svg?branch=main&#34; alt=&#34;CI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/nixtla/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/nixtla?color=blue&#34; alt=&#34;PyPi&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Nixtla/nixtla/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache_2.0-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://docs.nixtla.io&#34;&gt;&lt;img src=&#34;https://img.shields.io/website-up-down-green-red/http/docs.nixtla.io/.svg?label=docs&#34; alt=&#34;docs&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/nixtla&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/nixtla&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/nixtla&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/nixtla/month&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/nixtla&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/nixtla/week&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://buildwithfern.com/?utm_source=nixtla/nixtla/readme&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%8C%BF-SDK%20generated%20by%20Fern-brightgreen&#34; alt=&#34;fern shield&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;TimeGPT&lt;/strong&gt; is a production ready, generative pretrained transformer for time series. It&#39;s capable of accurately predicting various domains such as retail, electricity, finance, and IoT with just a few lines of code üöÄ. &lt;/p&gt;&#xA;&lt;/div&gt;&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üìñ Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Nixtla/nixtla/main/#-quick-start&#34;&gt;Quick Start&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Nixtla/nixtla/main/#install-nixtlas-sdk&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Nixtla/nixtla/main/#forecast-using-timegpt-in-3-easy-steps&#34;&gt;Forecasting with TimeGPT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Nixtla/nixtla/main/#anomaly-detection-using-timegpt-in-3-easy-steps&#34;&gt;Anomaly Detection&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Nixtla/nixtla/main/#%EF%B8%8F-zero-shot-results&#34;&gt;Zero-shot Results&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Nixtla/nixtla/main/#-how-to-cite&#34;&gt;How to Cite&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Nixtla/nixtla/main/#-features-and-mentions&#34;&gt;Features and Mentions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Nixtla/nixtla/main/#-license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Nixtla/nixtla/main/#-get-in-touch&#34;&gt;Get in Touch&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Nixtla/nixtla/assets/4086186/163ad9e6-7a16-44e1-b2e9-dab8a0b7b6b6&#34;&gt;https://github.com/Nixtla/nixtla/assets/4086186/163ad9e6-7a16-44e1-b2e9-dab8a0b7b6b6&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Install nixtla&#39;s SDK&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pip install nixtla&amp;gt;=0.5.1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Import libraries and load data&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd&#xA;from nixtla import NixtlaClient&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Forecast using TimeGPT in 3 easy steps&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get your API Key at dashboard.nixtla.io&#xA;&#xA;# 1. Instantiate the NixtlaClient&#xA;nixtla_client = NixtlaClient(api_key = &#39;YOUR API KEY HERE&#39;)&#xA;&#xA;# 2. Read historic electricity demand data &#xA;df = pd.read_csv(&#39;https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/electricity-short.csv&#39;)&#xA;&#xA;# 3. Forecast the next 24 hours&#xA;fcst_df = nixtla_client.forecast(df, h=24, level=[80, 90])&#xA;&#xA;# 4. Plot your results (optional)&#xA;nixtla_client.plot(df, timegpt_fcst_df, time_col=&#39;timestamp&#39;, target_col=&#39;value&#39;, level=[80, 90])&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Nixtla/nixtla/main/nbs/img/forecast_readme.png&#34; alt=&#34;Forecast Results&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Anomaly detection using TimeGPT in 3 easy steps&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get your API Key at dashboard.nixtla.io&#xA;&#xA;# 1. Instantiate the NixtlaClient&#xA;nixtla_client = NixtlaClient(api_key = &#39;YOUR API KEY HERE&#39;)&#xA;&#xA;# 2. Read Data # Wikipedia visits of NFL Star (&#xA;df = pd.read_csv(&#39;https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/peyton_manning.csv&#39;)&#xA;&#xA;&#xA;# 3. Detect Anomalies &#xA;anomalies_df = nixtla_client.detect_anomalies(df, time_col=&#39;timestamp&#39;, target_col=&#39;value&#39;, freq=&#39;D&#39;)&#xA;&#xA;# 4. Plot your results (optional)&#xA;nixtla_client.plot(df, anomalies_df,time_col=&#39;timestamp&#39;, target_col=&#39;value&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Nixtla/nixtla/main/nbs/img/anomaly.png&#34; alt=&#34;AnomalyDetection&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ü§ì API support for other languages&lt;/h2&gt; &#xA;&lt;p&gt;Explore our &lt;a href=&#34;https://docs.nixtla.io&#34;&gt;API Reference&lt;/a&gt; to discover how to leverage TimeGPT across various programming languages including JavaScript, Go, and more.&lt;/p&gt; &#xA;&lt;h2&gt;üî• Features and Capabilities&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Zero-shot Inference&lt;/strong&gt;: TimeGPT can generate forecasts and detect anomalies straight out of the box, requiring no prior training data. This allows for immediate deployment and quick insights from any time series data.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Fine-tuning&lt;/strong&gt;: Enhance TimeGPT&#39;s capabilities by fine-tuning the model on your specific datasets, enabling the model to adapt to the nuances of your unique time series data and improving performance on tailored tasks.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;API Access&lt;/strong&gt;: Integrate TimeGPT seamlessly into your applications via our robust API. Upcoming support for Azure Studio will provide even more flexible integration options. Alternatively, deploy TimeGPT on your own infrastructure to maintain full control over your data and workflows.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Add Exogenous Variables&lt;/strong&gt;: Incorporate additional variables that might influence your predictions to enhance forecast accuracy. (E.g. Special Dates, events or prices)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multiple Series Forecasting&lt;/strong&gt;: Simultaneously forecast multiple time series data, optimizing workflows and resources.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Custom Loss Function&lt;/strong&gt;: Tailor the fine-tuning process with a custom loss function to meet specific performance metrics.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Cross Validation&lt;/strong&gt;: Implement out of the box cross-validation techniques to ensure model robustness and generalizability.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Prediction Intervals&lt;/strong&gt;: Provide intervals in your predictions to quantify uncertainty effectively.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Irregular Timestamps&lt;/strong&gt;: Handle data with irregular timestamps, accommodating non-uniform interval series without preprocessing.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üìö Documentation with examples and use cases&lt;/h2&gt; &#xA;&lt;p&gt;Dive into our &lt;a href=&#34;https://docs.nixtla.io/docs/getting-started-timegpt_quickstart&#34;&gt;comprehensive documentation&lt;/a&gt; to discover examples and practical use cases for TimeGPT. Our documentation covers a wide range of topics, including:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Getting Started&lt;/strong&gt;: Begin with our user-friendly &lt;a href=&#34;https://docs.nixtla.io/docs/getting-started-timegpt_quickstart&#34;&gt;Quickstart Guide&lt;/a&gt; and learn how to &lt;a href=&#34;https://docs.nixtla.io/docs/getting-started-setting_up_your_api_key&#34;&gt;set up your API key&lt;/a&gt; effortlessly.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Advanced Techniques&lt;/strong&gt;: Master advanced forecasting methods and learn how to enhance model accuracy with our tutorials on &lt;a href=&#34;https://docs.nixtla.io/docs/tutorials-anomaly_detection&#34;&gt;anomaly detection&lt;/a&gt;, fine-tuning models using specific loss functions, and scaling computations across distributed frameworks such as &lt;a href=&#34;https://docs.nixtla.io/docs/tutorials-computing_at_scale&#34;&gt;Spark, Dask, and Ray&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Specialized Topics&lt;/strong&gt;: Explore specialized topics like &lt;a href=&#34;https://docs.nixtla.io/docs/tutorials-holidays_and_special_dates&#34;&gt;handling exogenous variables&lt;/a&gt;, model validation through &lt;a href=&#34;https://docs.nixtla.io/docs/tutorials-cross_validation&#34;&gt;cross-validation&lt;/a&gt;, and strategies for &lt;a href=&#34;https://docs.nixtla.io/docs/tutorials-uncertainty_quantification&#34;&gt;forecasting under uncertainty&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Real-World Applications&lt;/strong&gt;: Uncover how TimeGPT is applied in real-world scenarios through case studies on &lt;a href=&#34;https://docs.nixtla.io/docs/use-cases-forecasting_web_traffic&#34;&gt;forecasting web traffic&lt;/a&gt; and &lt;a href=&#34;https://docs.nixtla.io/docs/use-cases-bitcoin_price_prediction&#34;&gt;predicting Bitcoin prices&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üóûÔ∏è TimeGPT-1: Revolutionizing Forecasting and Anomaly Detection&lt;/h2&gt; &#xA;&lt;p&gt;Time series data is pivotal across various sectors, including finance, healthcare, meteorology, and social sciences. Whether it&#39;s monitoring ocean tides or tracking the Dow Jones&#39;s daily closing values, time series data is crucial for forecasting and decision-making.&lt;/p&gt; &#xA;&lt;p&gt;Traditional analysis methods such as ARIMA, ETS, MSTL, Theta, CES, machine learning models like XGBoost and LightGBM, and deep learning approaches have been standard tools for analysts. However, TimeGPT introduces a paradigm shift with its standout performance, efficiency, and simplicity. Thanks to its zero-shot inference capability, TimeGPT streamlines the analytical process, making it accessible even to users with minimal coding experience.&lt;/p&gt; &#xA;&lt;p&gt;TimeGPT is user-friendly and low-code, enabling users to upload their time series data and either generate forecasts or detect anomalies with just a single line of code. As the only foundation model for time series analysis out of the box, TimeGPT can be integrated via our public APIs, through Azure Studio (coming soon), or deployed on your own infrastructure.&lt;/p&gt; &#xA;&lt;h2&gt;‚öôÔ∏è TimeGPT&#39;s Architecture&lt;/h2&gt; &#xA;&lt;p&gt;Self-attention, the revolutionary concept introduced by the paper ‚ÄúAttention is all you need‚Äú, is the basis of the this foundational model. The TimeGPT model is not based on any existing large language model(LLMs). It is independently trained on vast timeseries dataset as a large transformer model and is designed so as to minimize the forecasting error.&lt;/p&gt; &#xA;&lt;p&gt;The architecture consists of an encoder-decoder structure with multiple layers, each with residual connections and layer normalization. Finally, a linear layer maps the decoder‚Äôs output to the forecasting window dimension. The general intuition is that attentionbased mechanisms are able to capture the diversity of past events and correctly extrapolate potential future distributions.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Nixtla/nixtla/main/nbs/img/forecast.png&#34; alt=&#34;Arquitecture&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;TimeGPT was trained on, to our knowledge, the largest collection of publicly available time series, collectively encompassing over 100 billion data points. This training set incorporates time series from a broad array of domains, including finance, economics, demographics, healthcare, weather, IoT sensor data, energy, web traffic, sales, transport, and banking. Due to this diverse set of domains, the training dataset contains time series with a wide range of characteristics&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;‚ö°Ô∏è Zero-shot Results&lt;/h2&gt; &#xA;&lt;h3&gt;Accuracy:&lt;/h3&gt; &#xA;&lt;p&gt;TimeGPT has been tested for its zero-shot inference capabilities on more than 300K unique series, which involve using the model without additional fine-tuning on the test dataset. TimeGPT outperforms a comprehensive range of well-established statistical and cutting-edge deep learning models, consistently ranking among the top three performers across various frequencies.&lt;/p&gt; &#xA;&lt;h3&gt;Ease of use:&lt;/h3&gt; &#xA;&lt;p&gt;TimeGPT also excels by offering simple and rapid predictions using a pre-trained model. This stands in stark contrast to other models that typically require an extensive training and prediction pipeline.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Nixtla/nixtla/main/nbs/img/results.jpg&#34; alt=&#34;Results&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Efficiency and Speed:&lt;/h3&gt; &#xA;&lt;p&gt;For zero-shot inference, our internal tests recorded an average GPU inference speed of 0.6 milliseconds per series for TimeGPT, which nearly mirrors that of the simple Seasonal Naive.&lt;/p&gt; &#xA;&lt;h2&gt;üìù How to cite?&lt;/h2&gt; &#xA;&lt;p&gt;If you find TimeGPT useful for your research, please consider citing the associated &lt;a href=&#34;https://arxiv.org/abs/2310.03589&#34;&gt;paper&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{garza2023timegpt1,&#xA;      title={TimeGPT-1}, &#xA;      author={Azul Garza and Max Mergenthaler-Canseco},&#xA;      year={2023},&#xA;      eprint={2310.03589},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.LG}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üéâ Features and Mentions&lt;/h2&gt; &#xA;&lt;p&gt;TimeGPT has been featured in many publications and has been recognized for its innovative approach to time series forecasting. Here are some of the features and mentions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.analyticsvidhya.com/blog/2024/02/timegpt-revolutionizing-time-series-forecasting/&#34;&gt;TimeGPT Revolutionizing Time Series Forecasting&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/timegpt-the-first-foundation-model-for-time-series-forecasting-bf0a75e63b3a&#34;&gt;TimeGPT: The First Foundation Model for Time Series Forecasting&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/@22meera99/timegpt-revolutionising-time-series-forecasting-with-generative-models-86be6c09fa51&#34;&gt;TimeGPT: Revolutionising Time Series Forecasting with Generative Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.turingpost.com/p/timegpt&#34;&gt;TimeGPT on Turing Post&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=5pYkT0rTCfE&amp;amp;ab_channel=AWSEvents&#34;&gt;TimeGPT Presentation at AWS Events&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://podcasts.apple.com/bg/podcast/timegpt-machine-learning-for-time-series-made-accessible/id1487704458?i=1000638551991&#34;&gt;TimeGPT: Machine Learning for Time Series Made Accessible - Podcast&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://thedataexchange.media/timegpt/&#34;&gt;TimeGPT on The Data Exchange&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hackernoon.com/how-timegpt-transforms-predictive-analytics-with-ai&#34;&gt;How TimeGPT Transforms Predictive Analytics with AI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aihorizonforecast.substack.com/p/timegpt-the-first-foundation-model&#34;&gt;TimeGPT: The First Foundation Model - AI Horizon Forecast&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üîñ License&lt;/h2&gt; &#xA;&lt;p&gt;TimeGPT is closed source. However, this SDK is open source and available under the Apache 2.0 License. Feel free to contribute.&lt;/p&gt; &#xA;&lt;h2&gt;üìû Get in touch&lt;/h2&gt; &#xA;&lt;p&gt;For any questions or feedback, please feel free to reach out to us at ops [at] nixtla.io.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>langchain-ai/langchain</title>
    <updated>2024-07-28T01:39:22Z</updated>
    <id>tag:github.com,2024-07-28:/langchain-ai/langchain</id>
    <link href="https://github.com/langchain-ai/langchain" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ü¶úüîó Build context-aware reasoning applications&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ü¶úÔ∏èüîó LangChain&lt;/h1&gt; &#xA;&lt;p&gt;‚ö° Build context-aware reasoning applications ‚ö°&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/langchain-ai/langchain/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/release/langchain-ai/langchain?style=flat-square&#34; alt=&#34;Release Notes&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/langchain-ai/langchain/actions/workflows/check_diffs.yml&#34;&gt;&lt;img src=&#34;https://github.com/langchain-ai/langchain/actions/workflows/check_diffs.yml/badge.svg?sanitize=true&#34; alt=&#34;CI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/l/langchain-core?style=flat-square&#34; alt=&#34;PyPI - License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypistats.org/packages/langchain-core&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/dm/langchain-core?style=flat-square&#34; alt=&#34;PyPI - Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://star-history.com/#langchain-ai/langchain&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/langchain-ai/langchain?style=flat-square&#34; alt=&#34;GitHub star chart&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://libraries.io/github/langchain-ai/langchain&#34;&gt;&lt;img src=&#34;https://img.shields.io/librariesio/github/langchain-ai/langchain?style=flat-square&#34; alt=&#34;Dependency Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/langchain-ai/langchain/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-raw/langchain-ai/langchain?style=flat-square&#34; alt=&#34;Open Issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/langchain-ai/langchain&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Dev%20Containers&amp;amp;message=Open&amp;amp;color=blue&amp;amp;logo=visualstudiocode&amp;amp;style=flat-square&#34; alt=&#34;Open in Dev Containers&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codespaces.new/langchain-ai/langchain&#34;&gt;&lt;img src=&#34;https://github.com/codespaces/badge.svg?sanitize=true&#34; alt=&#34;Open in GitHub Codespaces&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/langchainai&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&amp;amp;label=Follow%20%40LangChainAI&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Looking for the JS/TS library? Check out &lt;a href=&#34;https://github.com/langchain-ai/langchainjs&#34;&gt;LangChain.js&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To help you ship LangChain apps to production faster, check out &lt;a href=&#34;https://smith.langchain.com&#34;&gt;LangSmith&lt;/a&gt;. &lt;a href=&#34;https://smith.langchain.com&#34;&gt;LangSmith&lt;/a&gt; is a unified developer platform for building, testing, and monitoring LLM applications. Fill out &lt;a href=&#34;https://www.langchain.com/contact-sales&#34;&gt;this form&lt;/a&gt; to speak with our sales team.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Install&lt;/h2&gt; &#xA;&lt;p&gt;With pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install langchain&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;With conda:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda install langchain -c conda-forge&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ü§î What is LangChain?&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;LangChain&lt;/strong&gt; is a framework for developing applications powered by large language models (LLMs).&lt;/p&gt; &#xA;&lt;p&gt;For these applications, LangChain simplifies the entire application lifecycle:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Open-source libraries&lt;/strong&gt;: Build your applications using LangChain&#39;s open-source &lt;a href=&#34;https://python.langchain.com/v0.2/docs/concepts#langchain-expression-language-lcel&#34;&gt;building blocks&lt;/a&gt;, &lt;a href=&#34;https://python.langchain.com/v0.2/docs/concepts&#34;&gt;components&lt;/a&gt;, and &lt;a href=&#34;https://python.langchain.com/v0.2/docs/integrations/platforms/&#34;&gt;third-party integrations&lt;/a&gt;. Use &lt;a href=&#34;https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/concepts/#langgraph&#34;&gt;LangGraph&lt;/a&gt; to build stateful agents with first-class streaming and human-in-the-loop support.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Productionization&lt;/strong&gt;: Inspect, monitor, and evaluate your apps with &lt;a href=&#34;https://docs.smith.langchain.com/&#34;&gt;LangSmith&lt;/a&gt; so that you can constantly optimize and deploy with confidence.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Deployment&lt;/strong&gt;: Turn your LangGraph applications into production-ready APIs and Assistants with &lt;a href=&#34;https://langchain-ai.github.io/langgraph/cloud/&#34;&gt;LangGraph Cloud&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Open-source libraries&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;langchain-core&lt;/code&gt;&lt;/strong&gt;: Base abstractions and LangChain Expression Language.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;langchain-community&lt;/code&gt;&lt;/strong&gt;: Third party integrations. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Some integrations have been further split into &lt;strong&gt;partner packages&lt;/strong&gt; that only rely on &lt;strong&gt;&lt;code&gt;langchain-core&lt;/code&gt;&lt;/strong&gt;. Examples include &lt;strong&gt;&lt;code&gt;langchain_openai&lt;/code&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;code&gt;langchain_anthropic&lt;/code&gt;&lt;/strong&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;langchain&lt;/code&gt;&lt;/strong&gt;: Chains, agents, and retrieval strategies that make up an application&#39;s cognitive architecture.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://langchain-ai.github.io/langgraph/&#34;&gt;&lt;code&gt;LangGraph&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;: A library for building robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph. Integrates smoothly with LangChain, but can be used without it.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Productionization:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://docs.smith.langchain.com/&#34;&gt;LangSmith&lt;/a&gt;&lt;/strong&gt;: A developer platform that lets you debug, test, evaluate, and monitor chains built on any LLM framework and seamlessly integrates with LangChain.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Deployment:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://langchain-ai.github.io/langgraph/cloud/&#34;&gt;LangGraph Cloud&lt;/a&gt;&lt;/strong&gt;: Turn your LangGraph applications into production-ready APIs and Assistants.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/static/svg/langchain_stack_062024.svg?sanitize=true&#34; alt=&#34;Diagram outlining the hierarchical organization of the LangChain framework, displaying the interconnected parts across multiple layers.&#34; title=&#34;LangChain Architecture Overview&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üß± What can you build with LangChain?&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;‚ùì Question answering with RAG&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://python.langchain.com/v0.2/docs/tutorials/rag/&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;End-to-end Example: &lt;a href=&#34;https://chat.langchain.com&#34;&gt;Chat LangChain&lt;/a&gt; and &lt;a href=&#34;https://github.com/langchain-ai/chat-langchain&#34;&gt;repo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;üß± Extracting structured output&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://python.langchain.com/v0.2/docs/tutorials/extraction/&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;End-to-end Example: &lt;a href=&#34;https://github.com/langchain-ai/langchain-extract/&#34;&gt;SQL Llama2 Template&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;ü§ñ Chatbots&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://python.langchain.com/v0.2/docs/tutorials/chatbot/&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;End-to-end Example: &lt;a href=&#34;https://weblangchain.vercel.app&#34;&gt;Web LangChain (web researcher chatbot)&lt;/a&gt; and &lt;a href=&#34;https://github.com/langchain-ai/weblangchain&#34;&gt;repo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;And much more! Head to the &lt;a href=&#34;https://python.langchain.com/v0.2/docs/tutorials/&#34;&gt;Tutorials&lt;/a&gt; section of the docs for more.&lt;/p&gt; &#xA;&lt;h2&gt;üöÄ How does LangChain help?&lt;/h2&gt; &#xA;&lt;p&gt;The main value props of the LangChain libraries are:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Components&lt;/strong&gt;: composable building blocks, tools and integrations for working with language models. Components are modular and easy-to-use, whether you are using the rest of the LangChain framework or not&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Off-the-shelf chains&lt;/strong&gt;: built-in assemblages of components for accomplishing higher-level tasks&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Off-the-shelf chains make it easy to get started. Components make it easy to customize existing chains and build new ones.&lt;/p&gt; &#xA;&lt;h2&gt;LangChain Expression Language (LCEL)&lt;/h2&gt; &#xA;&lt;p&gt;LCEL is the foundation of many of LangChain&#39;s components, and is a declarative way to compose chains. LCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest ‚Äúprompt + LLM‚Äù chain to the most complex chains.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://python.langchain.com/v0.2/docs/concepts/#langchain-expression-language-lcel&#34;&gt;Overview&lt;/a&gt;&lt;/strong&gt;: LCEL and its benefits&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://python.langchain.com/v0.2/docs/concepts/#runnable-interface&#34;&gt;Interface&lt;/a&gt;&lt;/strong&gt;: The standard Runnable interface for LCEL objects&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://python.langchain.com/v0.2/docs/how_to/#langchain-expression-language-lcel&#34;&gt;Primitives&lt;/a&gt;&lt;/strong&gt;: More on the primitives LCEL includes&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://python.langchain.com/v0.2/docs/how_to/lcel_cheatsheet/&#34;&gt;Cheatsheet&lt;/a&gt;&lt;/strong&gt;: Quick overview of the most common usage patterns&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Components&lt;/h2&gt; &#xA;&lt;p&gt;Components fall into the following &lt;strong&gt;modules&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;üìÉ Model I/O&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;This includes &lt;a href=&#34;https://python.langchain.com/v0.2/docs/concepts/#prompt-templates&#34;&gt;prompt management&lt;/a&gt;, &lt;a href=&#34;https://python.langchain.com/v0.2/docs/concepts/#example-selectors&#34;&gt;prompt optimization&lt;/a&gt;, a generic interface for &lt;a href=&#34;https://python.langchain.com/v0.2/docs/concepts/#chat-models&#34;&gt;chat models&lt;/a&gt; and &lt;a href=&#34;https://python.langchain.com/v0.2/docs/concepts/#llms&#34;&gt;LLMs&lt;/a&gt;, and common utilities for working with &lt;a href=&#34;https://python.langchain.com/v0.2/docs/concepts/#output-parsers&#34;&gt;model outputs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;üìö Retrieval&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Retrieval Augmented Generation involves &lt;a href=&#34;https://python.langchain.com/v0.2/docs/concepts/#document-loaders&#34;&gt;loading data&lt;/a&gt; from a variety of sources, &lt;a href=&#34;https://python.langchain.com/v0.2/docs/concepts/#text-splitters&#34;&gt;preparing it&lt;/a&gt;, then &lt;a href=&#34;https://python.langchain.com/v0.2/docs/concepts/#retrievers&#34;&gt;searching over (a.k.a. retrieving from)&lt;/a&gt; it for use in the generation step.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ü§ñ Agents&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Agents allow an LLM autonomy over how a task is accomplished. Agents make decisions about which Actions to take, then take that Action, observe the result, and repeat until the task is complete. LangChain provides a &lt;a href=&#34;https://python.langchain.com/v0.2/docs/concepts/#agents&#34;&gt;standard interface for agents&lt;/a&gt;, along with &lt;a href=&#34;https://github.com/langchain-ai/langgraph&#34;&gt;LangGraph&lt;/a&gt; for building custom agents.&lt;/p&gt; &#xA;&lt;h2&gt;üìñ Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://python.langchain.com&#34;&gt;here&lt;/a&gt; for full documentation, which includes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://python.langchain.com/v0.2/docs/introduction/&#34;&gt;Introduction&lt;/a&gt;: Overview of the framework and the structure of the docs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://python.langchain.com/docs/use_cases/&#34;&gt;Tutorials&lt;/a&gt;: If you&#39;re looking to build something specific or are more of a hands-on learner, check out our tutorials. This is the best place to get started.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://python.langchain.com/v0.2/docs/how_to/&#34;&gt;How-to guides&lt;/a&gt;: Answers to ‚ÄúHow do I‚Ä¶.?‚Äù type questions. These guides are goal-oriented and concrete; they&#39;re meant to help you complete a specific task.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://python.langchain.com/v0.2/docs/concepts/&#34;&gt;Conceptual guide&lt;/a&gt;: Conceptual explanations of the key parts of the framework.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://api.python.langchain.com&#34;&gt;API Reference&lt;/a&gt;: Thorough documentation of every class and method.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üåê Ecosystem&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.smith.langchain.com/&#34;&gt;ü¶úüõ†Ô∏è LangSmith&lt;/a&gt;: Trace and evaluate your language model applications and intelligent agents to help you move from prototype to production.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://langchain-ai.github.io/langgraph/&#34;&gt;ü¶úüï∏Ô∏è LangGraph&lt;/a&gt;: Create stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://python.langchain.com/docs/langserve&#34;&gt;ü¶úüèì LangServe&lt;/a&gt;: Deploy LangChain runnables and chains as REST APIs.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üíÅ Contributing&lt;/h2&gt; &#xA;&lt;p&gt;As an open-source project in a rapidly developing field, we are extremely open to contributions, whether it be in the form of a new feature, improved infrastructure, or better documentation.&lt;/p&gt; &#xA;&lt;p&gt;For detailed information on how to contribute, see &lt;a href=&#34;https://python.langchain.com/v0.2/docs/contributing/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üåü Contributors&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/langchain-ai/langchain/graphs/contributors&#34;&gt;&lt;img src=&#34;https://contrib.rocks/image?repo=langchain-ai/langchain&amp;amp;max=2000&#34; alt=&#34;langchain contributors&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>TencentARC/PhotoMaker</title>
    <updated>2024-07-28T01:39:22Z</updated>
    <id>tag:github.com,2024-07-28:/TencentARC/PhotoMaker</id>
    <link href="https://github.com/TencentARC/PhotoMaker" rel="alternate"></link>
    <summary type="html">&lt;p&gt;PhotoMaker [CVPR 2024]&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://photo-maker.github.io/assets/logo.png&#34; height=&#34;100&#34;&gt; &lt;/p&gt; &#xA;&lt;!-- ## &lt;div align=&#34;center&#34;&gt;&lt;b&gt;PhotoMaker&lt;/b&gt;&lt;/div&gt; --&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h2&gt;PhotoMaker: Customizing Realistic Human Photos via Stacked ID Embedding &lt;a href=&#34;https://huggingface.co/papers/2312.04461&#34;&gt;&lt;img src=&#34;https://huggingface.co/datasets/huggingface/badges/resolve/main/paper-page-md-dark.svg?sanitize=true&#34; alt=&#34;Paper page&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA; &lt;p&gt;[&lt;a href=&#34;https://huggingface.co/papers/2312.04461&#34;&gt;Paper&lt;/a&gt;] ‚ÄÉ [&lt;a href=&#34;https://photo-maker.github.io&#34;&gt;Project Page&lt;/a&gt;] ‚ÄÉ [&lt;a href=&#34;https://huggingface.co/TencentARC/PhotoMaker&#34;&gt;Model Card&lt;/a&gt;] &lt;br&gt;&lt;/p&gt; &#xA; &lt;p&gt;[&lt;a href=&#34;https://huggingface.co/spaces/TencentARC/PhotoMaker-V2&#34;&gt;üí•New ü§ó &lt;strong&gt;Demo (PhotoMaker V2)&lt;/strong&gt;&lt;/a&gt;] ‚ÄÉ [&lt;a href=&#34;https://huggingface.co/spaces/TencentARC/PhotoMaker&#34;&gt;ü§ó Demo (Realistic)&lt;/a&gt;] ‚ÄÉ [&lt;a href=&#34;https://huggingface.co/spaces/TencentARC/PhotoMaker-Style&#34;&gt;ü§ó Demo (Stylization)&lt;/a&gt;] &lt;br&gt;&lt;/p&gt; &#xA; &lt;p&gt;[&lt;a href=&#34;https://replicate.com/jd7h/photomaker&#34;&gt;Replicate Demo (Realistic)&lt;/a&gt;] ‚ÄÉ [&lt;a href=&#34;https://replicate.com/yorickvp/photomaker-style&#34;&gt;Replicate Demo (Stylization)&lt;/a&gt;] &#xA;  &lt;be&gt;&lt;/be&gt;&lt;/p&gt; &#xA; &lt;p&gt;If the ID fidelity is not enough for you, please try our &lt;a href=&#34;https://huggingface.co/spaces/TencentARC/PhotoMaker-V2&#34;&gt;PhotoMaker V2&lt;/a&gt; or &lt;a href=&#34;https://huggingface.co/spaces/TencentARC/PhotoMaker-Style&#34;&gt;stylization application&lt;/a&gt;, you may be pleasantly surprised.&lt;/p&gt; &#xA; &lt;p&gt;ü•≥ We release &lt;strong&gt;PhotoMaker V2&lt;/strong&gt;. Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/PhotoMaker/main/README_pmv2.md&#34;&gt;comparisons&lt;/a&gt; between PhotoMaker V1, PhotoMaker V2, IP-Adapter-FaceID-plus-V2, and InstantID. Please watch &lt;a href=&#34;https://photo-maker.github.io/assets/demo_pm_v2_full.mp4&#34;&gt;this video&lt;/a&gt; for how to use our demo.&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;üå† &lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Rapid customization &lt;strong&gt;within seconds&lt;/strong&gt;, with no additional LoRA training.&lt;/li&gt; &#xA; &lt;li&gt;Ensures impressive ID fidelity, offering diversity, promising text controllability, and high-quality generation.&lt;/li&gt; &#xA; &lt;li&gt;Can serve as an &lt;strong&gt;Adapter&lt;/strong&gt; to collaborate with other Base Models alongside LoRA modules in community.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://trendshift.io/repositories/7008&#34; target=&#34;_blank&#34; align=&#34;center&#34;&gt;&lt;img src=&#34;https://trendshift.io/api/badge/repositories/7008&#34; alt=&#34;TencentARC%2FPhotoMaker | Trendshift&#34; style=&#34;width: 250px; height: 55px;&#34; width=&#34;250&#34; height=&#34;55&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;‚ùó‚ùó Note: If there are any PhotoMaker based resources and applications, please leave them in the &lt;a href=&#34;https://github.com/TencentARC/PhotoMaker/discussions/36&#34;&gt;discussion&lt;/a&gt; and we will list them in the &lt;a href=&#34;https://github.com/TencentARC/PhotoMaker?tab=readme-ov-file#related-resources&#34;&gt;Related Resources&lt;/a&gt; section in README file. Now we know the implementation of &lt;strong&gt;Replicate&lt;/strong&gt;, &lt;strong&gt;Windows&lt;/strong&gt;, &lt;strong&gt;ComfyUI&lt;/strong&gt;, and &lt;strong&gt;WebUI&lt;/strong&gt;. Thank you all!&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://github.com/TencentARC/PhotoMaker/assets/21050959/e72cbf4d-938f-417d-b308-55e76a4bc5c8&#34; alt=&#34;photomaker_demo_fast&#34;&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;üö© &lt;strong&gt;New Features/Updates&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;‚úÖ July 22, 2024. üí• We release PhotoMaker V2 with &lt;strong&gt;improved ID fidelity&lt;/strong&gt;. At the same time, it still maintains the generation quality, editability, and compatibility with any plugins that PhotoMaker V1 offers. We have also provided scripts for integration with &lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/PhotoMaker/main/inference_scripts/inference_pmv2_contronet.py&#34;&gt;ControlNet&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/PhotoMaker/main/inference_scripts/inference_pmv2_t2i_adapter.py&#34;&gt;T2I-Adapter&lt;/a&gt;, and &lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/PhotoMaker/main/inference_scripts/inference_pmv2_ip_adapter.py&#34;&gt;IP-Adapter&lt;/a&gt; to offer excellent control capabilities. Users can further customize scripts for upgrades, such as combining with LCM for acceleration or integrating with IP-Adapter-FaceID or InstantID to further improve ID fidelity. We will release technical report of PhotoMaker V2 soon. Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/PhotoMaker/main/README_pmv2.md&#34;&gt;this doc&lt;/a&gt; for a quick preview.&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ January 20, 2024. An &lt;strong&gt;important&lt;/strong&gt; note: For those GPUs that do not support bfloat16, please change &lt;a href=&#34;https://github.com/TencentARC/PhotoMaker/raw/6ec44fc13909d64a65c635b9e3b6f238eb1de9fe/gradio_demo/app.py#L39&#34;&gt;this line&lt;/a&gt; to &lt;code&gt;torch_dtype = torch.float16&lt;/code&gt;, the speed will be &lt;strong&gt;greatly improved&lt;/strong&gt; (1min/img (before) vs. 14s/img (after) on V100). The minimum GPU memory requirement for PhotoMaker is &lt;strong&gt;11G&lt;/strong&gt; (Please refer to &lt;a href=&#34;https://github.com/TencentARC/PhotoMaker/discussions/114&#34;&gt;this link&lt;/a&gt; for saving GPU memory).&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ January 15, 2024. We release PhotoMaker.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;üî• &lt;strong&gt;Examples&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;h3&gt;Realistic generation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/TencentARC/PhotoMaker&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Demo&amp;amp;message=Huggingface%20Gradio&amp;amp;color=orange&#34; alt=&#34;Huggingface PhotoMaker&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/PhotoMaker/main/photomaker_demo.ipynb&#34;&gt;&lt;strong&gt;PhotoMaker notebook demo&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://cdn-uploads.huggingface.co/production/uploads/6285a9133ab6642179158944/BYBZNyfmN4jBKBxxt4uxz.jpeg&#34; height=&#34;450&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://cdn-uploads.huggingface.co/production/uploads/6285a9133ab6642179158944/9KYqoDxfbNVLzVKZzSzwo.jpeg&#34; height=&#34;450&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Stylization generation&lt;/h3&gt; &#xA;&lt;p&gt;Note: only change the base model and add the LoRA modules for better stylization&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/TencentARC/PhotoMaker-Style&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Demo&amp;amp;message=Huggingface%20Gradio&amp;amp;color=orange&#34; alt=&#34;Huggingface PhotoMaker-Style&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/PhotoMaker/main/photomaker_style_demo.ipynb&#34;&gt;&lt;strong&gt;PhotoMaker-Style notebook demo&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://cdn-uploads.huggingface.co/production/uploads/6285a9133ab6642179158944/du884lcjpqqjnJIxpATM2.jpeg&#34; height=&#34;450&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://cdn-uploads.huggingface.co/production/uploads/6285a9133ab6642179158944/-AC7Hr5YL4yW1zXGe_Izl.jpeg&#34; height=&#34;450&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;üîß Dependencies and Installation&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python &amp;gt;= 3.8 (Recommend to use &lt;a href=&#34;https://www.anaconda.com/download/#linux&#34;&gt;Anaconda&lt;/a&gt; or &lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;Miniconda&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch &amp;gt;= 2.0.0&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create --name photomaker python=3.10&#xA;conda activate photomaker&#xA;pip install -U pip&#xA;&#xA;# Install requirements&#xA;pip install -r requirements.txt&#xA;&#xA;# Install photomaker&#xA;pip install git+https://github.com/TencentARC/PhotoMaker.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you can run the following command to use it&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from photomaker import PhotoMakerStableDiffusionXLPipeline&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;‚è¨ Download Models&lt;/h1&gt; &#xA;&lt;p&gt;The model will be automatically downloaded through the following two lines:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from huggingface_hub import hf_hub_download&#xA;photomaker_path = hf_hub_download(repo_id=&#34;TencentARC/PhotoMaker&#34;, filename=&#34;photomaker-v1.bin&#34;, repo_type=&#34;model&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also choose to download manually from this &lt;a href=&#34;https://huggingface.co/TencentARC/PhotoMaker&#34;&gt;url&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;üíª How to Test&lt;/h1&gt; &#xA;&lt;h2&gt;Use like &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;diffusers&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Dependency&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;import torch&#xA;import os&#xA;from diffusers.utils import load_image&#xA;from diffusers import EulerDiscreteScheduler&#xA;from photomaker import PhotoMakerStableDiffusionXLPipeline&#xA;&#xA;### Load base model&#xA;pipe = PhotoMakerStableDiffusionXLPipeline.from_pretrained(&#xA;    base_model_path,  # can change to any base model based on SDXL&#xA;    torch_dtype=torch.bfloat16, &#xA;    use_safetensors=True, &#xA;    variant=&#34;fp16&#34;&#xA;).to(device)&#xA;&#xA;### Load PhotoMaker checkpoint&#xA;pipe.load_photomaker_adapter(&#xA;    os.path.dirname(photomaker_path),&#xA;    subfolder=&#34;&#34;,&#xA;    weight_name=os.path.basename(photomaker_path),&#xA;    trigger_word=&#34;img&#34;  # define the trigger word&#xA;)     &#xA;&#xA;pipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config)&#xA;&#xA;### Also can cooperate with other LoRA modules&#xA;# pipe.load_lora_weights(os.path.dirname(lora_path), weight_name=lora_model_name, adapter_name=&#34;xl_more_art-full&#34;)&#xA;# pipe.set_adapters([&#34;photomaker&#34;, &#34;xl_more_art-full&#34;], adapter_weights=[1.0, 0.5])&#xA;&#xA;pipe.fuse_lora()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Input ID Images&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;### define the input ID images&#xA;input_folder_name = &#39;./examples/newton_man&#39;&#xA;image_basename_list = os.listdir(input_folder_name)&#xA;image_path_list = sorted([os.path.join(input_folder_name, basename) for basename in image_basename_list])&#xA;&#xA;input_id_images = []&#xA;for image_path in image_path_list:&#xA;    input_id_images.append(load_image(image_path))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/TencentARC/PhotoMaker/assets/21050959/01d53dfa-7528-4f09-a1a5-96b349ae7800&#34; align=&#34;center&#34;&gt;&lt;img style=&#34;margin:0;padding:0;&#34; src=&#34;https://github.com/TencentARC/PhotoMaker/assets/21050959/01d53dfa-7528-4f09-a1a5-96b349ae7800&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Generation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;# Note that the trigger word `img` must follow the class word for personalization&#xA;prompt = &#34;a half-body portrait of a man img wearing the sunglasses in Iron man suit, best quality&#34;&#xA;negative_prompt = &#34;(asymmetry, worst quality, low quality, illustration, 3d, 2d, painting, cartoons, sketch), open mouth, grayscale&#34;&#xA;generator = torch.Generator(device=device).manual_seed(42)&#xA;images = pipe(&#xA;    prompt=prompt,&#xA;    input_id_images=input_id_images,&#xA;    negative_prompt=negative_prompt,&#xA;    num_images_per_prompt=1,&#xA;    num_inference_steps=num_steps,&#xA;    start_merge_step=10,&#xA;    generator=generator,&#xA;).images[0]&#xA;gen_images.save(&#39;out_photomaker.png&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/TencentARC/PhotoMaker/assets/21050959/703c00e1-5e50-4c19-899e-25ee682d2c06&#34; align=&#34;center&#34;&gt;&lt;img width=&#34;400&#34; style=&#34;margin:0;padding:0;&#34; src=&#34;https://github.com/TencentARC/PhotoMaker/assets/21050959/703c00e1-5e50-4c19-899e-25ee682d2c06&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Start a local gradio demo&lt;/h2&gt; &#xA;&lt;p&gt;Run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python gradio_demo/app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You could customize this script in &lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/PhotoMaker/main/gradio_demo/app.py&#34;&gt;this file&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you want to run it on MAC, you should follow &lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/PhotoMaker/main/MacGPUEnv.md&#34;&gt;this Instruction&lt;/a&gt; and then run the app.py.&lt;/p&gt; &#xA;&lt;h2&gt;Usage Tips:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Upload more photos of the person to be customized to improve ID fidelity. If the input is Asian face(s), maybe consider adding &#39;Asian&#39; before the class word, e.g., &lt;code&gt;Asian woman img&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;When stylizing, does the generated face look too realistic? Adjust the Style strength to 30-50, the larger the number, the less ID fidelity, but the stylization ability will be better. You could also try out other base models or LoRAs with good stylization effects.&lt;/li&gt; &#xA; &lt;li&gt;Reduce the number of generated images and sampling steps for faster speed. However, please keep in mind that reducing the sampling steps may compromise the ID fidelity.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Related Resources&lt;/h1&gt; &#xA;&lt;h3&gt;Replicate demo of PhotoMaker:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://replicate.com/jd7h/photomaker&#34;&gt;Demo link&lt;/a&gt;, run PhotoMaker on replicate, provided by &lt;a href=&#34;https://github.com/yorickvP&#34;&gt;@yorickvP&lt;/a&gt; and &lt;a href=&#34;https://github.com/jd7h&#34;&gt;@jd7h&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://replicate.com/yorickvp/photomaker-style&#34;&gt;Demo link (style version)&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;WebUI version of PhotoMaker:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;stable-diffusion-webui-forge&lt;/strong&gt;: &lt;a href=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge&#34;&gt;https://github.com/lllyasviel/stable-diffusion-webui-forge&lt;/a&gt; provided by &lt;a href=&#34;https://github.com/lllyasviel&#34;&gt;@Lvmin Zhang&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fooocus App&lt;/strong&gt;: &lt;a href=&#34;https://github.com/machineminded/Fooocus-inswapper&#34;&gt;Fooocus-inswapper&lt;/a&gt; provided by &lt;a href=&#34;https://github.com/machineminded&#34;&gt;@machineminded&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Windows version of PhotoMaker:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/bmaltais/PhotoMaker/tree/v1.0.1&#34;&gt;bmaltais/PhotoMaker&lt;/a&gt; by &lt;a href=&#34;https://github.com/bmaltais&#34;&gt;@bmaltais&lt;/a&gt;, easy to deploy PhotoMaker on Windows. The description can be found in &lt;a href=&#34;https://github.com/TencentARC/PhotoMaker/discussions/36#discussioncomment-8156199&#34;&gt;this link&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sdbds/PhotoMaker-for-windows/tree/windows&#34;&gt;sdbds/PhotoMaker-for-windows&lt;/a&gt; by &lt;a href=&#34;https://github.com/sdbds&#34;&gt;@sdbds&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;ComfyUI:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;üî• &lt;strong&gt;Official Implementation by &lt;a href=&#34;https://github.com/comfyanonymous/ComfyUI&#34;&gt;ComfyUI&lt;/a&gt;&lt;/strong&gt;: &lt;a href=&#34;https://github.com/comfyanonymous/ComfyUI/commit/d1533d9c0f1dde192f738ef1b745b15f49f41e02&#34;&gt;https://github.com/comfyanonymous/ComfyUI/commit/d1533d9c0f1dde192f738ef1b745b15f49f41e02&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ZHO-ZHO-ZHO/ComfyUI-PhotoMaker&#34;&gt;https://github.com/ZHO-ZHO-ZHO/ComfyUI-PhotoMaker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/StartHua/Comfyui-Mine-PhotoMaker&#34;&gt;https://github.com/StartHua/Comfyui-Mine-PhotoMaker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/shiimizu/ComfyUI-PhotoMaker&#34;&gt;https://github.com/shiimizu/ComfyUI-PhotoMaker&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Purely C/C++/CUDA version of PhotoMaker:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/leejet/stable-diffusion.cpp/pull/179&#34;&gt;stable-diffusion.cpp&lt;/a&gt; by &lt;a href=&#34;https://github.com/bssrdf&#34;&gt;@bssrdf&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Other Applications / Web Demos&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Wisemodel ÂßãÊô∫ (Easy to use in China)&lt;/strong&gt; &lt;a href=&#34;https://wisemodel.cn/space/gradio/photomaker&#34;&gt;https://wisemodel.cn/space/gradio/photomaker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;OpenXLab (Easy to use in China)&lt;/strong&gt;: &lt;a href=&#34;https://openxlab.org.cn/apps/detail/camenduru/PhotoMaker&#34;&gt;https://openxlab.org.cn/apps/detail/camenduru/PhotoMaker&lt;/a&gt; &lt;a href=&#34;https://openxlab.org.cn/apps/detail/camenduru/PhotoMaker&#34;&gt;&lt;img src=&#34;https://cdn-static.openxlab.org.cn/app-center/openxlab_app.svg?sanitize=true&#34; alt=&#34;Open in OpenXLab&#34;&gt;&lt;/a&gt; by &lt;a href=&#34;https://github.com/camenduru&#34;&gt;@camenduru&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Colab&lt;/strong&gt;: &lt;a href=&#34;https://github.com/camenduru/PhotoMaker-colab&#34;&gt;https://github.com/camenduru/PhotoMaker-colab&lt;/a&gt; by &lt;a href=&#34;https://github.com/camenduru&#34;&gt;@camenduru&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Monster API&lt;/strong&gt;: &lt;a href=&#34;https://monsterapi.ai/playground?model=photo-maker&#34;&gt;https://monsterapi.ai/playground?model=photo-maker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Pinokio&lt;/strong&gt;: &lt;a href=&#34;https://pinokio.computer/item?uri=https://github.com/cocktailpeanutlabs/photomaker&#34;&gt;https://pinokio.computer/item?uri=https://github.com/cocktailpeanutlabs/photomaker&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Graido demo in 45 lines&lt;/h3&gt; &#xA;&lt;p&gt;Provided by &lt;a href=&#34;https://twitter.com/Gradio/status/1747683500495691942&#34;&gt;@Gradio&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;ü§ó Acknowledgements&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;PhotoMaker is co-hosted by Tencent ARC Lab and Nankai University &lt;a href=&#34;https://mmcheng.net/cmm/&#34;&gt;MCG-NKU&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Inspired from many excellent demos and repos, including &lt;a href=&#34;https://github.com/tencent-ailab/IP-Adapter&#34;&gt;IP-Adapter&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/spaces/multimodalart/Ip-Adapter-FaceID&#34;&gt;multimodalart/Ip-Adapter-FaceID&lt;/a&gt;, &lt;a href=&#34;https://github.com/mit-han-lab/fastcomposer&#34;&gt;FastComposer&lt;/a&gt;, and &lt;a href=&#34;https://github.com/TencentARC/T2I-Adapter&#34;&gt;T2I-Adapter&lt;/a&gt;. Thanks for their great work!&lt;/li&gt; &#xA; &lt;li&gt;Thanks to the &lt;a href=&#34;https://github.com/Tencent/HunyuanDiT&#34;&gt;HunyuanDiT&lt;/a&gt; team for their generous support and suggestions!&lt;/li&gt; &#xA; &lt;li&gt;Thanks to the Venus team in Tencent PCG for their feedback and suggestions.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to the HuggingFace team for their generous support!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Disclaimer&lt;/h1&gt; &#xA;&lt;p&gt;This project strives to impact the domain of AI-driven image generation positively. Users are granted the freedom to create images using this tool, but they are expected to comply with local laws and utilize it responsibly. The developers do not assume any responsibility for potential misuse by users.&lt;/p&gt; &#xA;&lt;h1&gt;BibTeX&lt;/h1&gt; &#xA;&lt;p&gt;If you find PhotoMaker useful for your research and applications, please cite using this BibTeX:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-BibTeX&#34;&gt;@inproceedings{li2023photomaker,&#xA;  title={PhotoMaker: Customizing Realistic Human Photos via Stacked ID Embedding},&#xA;  author={Li, Zhen and Cao, Mingdeng and Wang, Xintao and Qi, Zhongang and Cheng, Ming-Ming and Shan, Ying},&#xA;  booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>