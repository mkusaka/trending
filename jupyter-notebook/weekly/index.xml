<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-01-01T01:44:34Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>labmlai/annotated_deep_learning_paper_implementations</title>
    <updated>2023-01-01T01:44:34Z</updated>
    <id>tag:github.com,2023-01-01:/labmlai/annotated_deep_learning_paper_implementations</id>
    <link href="https://github.com/labmlai/annotated_deep_learning_paper_implementations" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üßë‚Äçüè´ 59 Implementations/tutorials of deep learning papers with side-by-side notes üìù; including transformers (original, xl, switch, feedback, vit, ...), optimizers (adam, adabelief, ...), gans(cyclegan, stylegan2, ...), üéÆ reinforcement learning (ppo, dqn), capsnet, distillation, ... üß†&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://twitter.com/labmlai&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/labmlai?style=social&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/sponsors/labmlai&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Sponsor&amp;amp;message=%E2%9D%A4&amp;amp;logo=GitHub&amp;amp;color=%23fe8e86&#34; alt=&#34;Sponsor&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;&lt;a href=&#34;https://nn.labml.ai/index.html&#34;&gt;labml.ai Deep Learning Paper Implementations&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;This is a collection of simple PyTorch implementations of neural networks and related algorithms. These implementations are documented with explanations,&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://nn.labml.ai/index.html&#34;&gt;The website&lt;/a&gt; renders these as side-by-side formatted notes. We believe these would help you understand these algorithms better.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://nn.labml.ai/dqn-light.png&#34; alt=&#34;Screenshot&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;We are actively maintaining this repo and adding new implementations almost weekly. &lt;a href=&#34;https://twitter.com/labmlai&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/labmlai?style=social&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt; for updates.&lt;/p&gt; &#xA;&lt;h2&gt;Paper Implementations&lt;/h2&gt; &#xA;&lt;h4&gt;‚ú® &lt;a href=&#34;https://nn.labml.ai/transformers/index.html&#34;&gt;Transformers&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/mha.html&#34;&gt;Multi-headed attention&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/models.html&#34;&gt;Transformer building blocks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/xl/index.html&#34;&gt;Transformer XL&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/xl/relative_mha.html&#34;&gt;Relative multi-headed attention&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/rope/index.html&#34;&gt;Rotary Positional Embeddings&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/alibi/index.html&#34;&gt;Attention with Linear Biases (ALiBi)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/retro/index.html&#34;&gt;RETRO&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/compressive/index.html&#34;&gt;Compressive Transformer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/gpt/index.html&#34;&gt;GPT Architecture&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/glu_variants/simple.html&#34;&gt;GLU Variants&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/knn&#34;&gt;kNN-LM: Generalization through Memorization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/feedback/index.html&#34;&gt;Feedback Transformer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/switch/index.html&#34;&gt;Switch Transformer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/fast_weights/index.html&#34;&gt;Fast Weights Transformer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/fnet/index.html&#34;&gt;FNet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/aft/index.html&#34;&gt;Attention Free Transformer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/mlm/index.html&#34;&gt;Masked Language Model&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/mlp_mixer/index.html&#34;&gt;MLP-Mixer: An all-MLP Architecture for Vision&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/gmlp/index.html&#34;&gt;Pay Attention to MLPs (gMLP)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/vit/index.html&#34;&gt;Vision Transformer (ViT)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/primer_ez/index.html&#34;&gt;Primer EZ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/hour_glass/index.html&#34;&gt;Hourglass&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;‚ú® &lt;a href=&#34;https://nn.labml.ai/neox/index.html&#34;&gt;Eleuther GPT-NeoX&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/neox/samples/generate.html&#34;&gt;Generate on a 48GB GPU&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/neox/samples/finetune.html&#34;&gt;Finetune on two 48GB GPUs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/neox/utils/llm_int8.html&#34;&gt;LLM.int8()&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;‚ú® &lt;a href=&#34;https://nn.labml.ai/diffusion/index.html&#34;&gt;Diffusion models&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/diffusion/ddpm/index.html&#34;&gt;Denoising Diffusion Probabilistic Models (DDPM)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/diffusion/stable_diffusion/sampler/ddim.html&#34;&gt;Denoising Diffusion Implicit Models (DDIM)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/diffusion/stable_diffusion/latent_diffusion.html&#34;&gt;Latent Diffusion Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/diffusion/stable_diffusion/index.html&#34;&gt;Stable Diffusion&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;‚ú® &lt;a href=&#34;https://nn.labml.ai/gan/index.html&#34;&gt;Generative Adversarial Networks&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/gan/original/index.html&#34;&gt;Original GAN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/gan/dcgan/index.html&#34;&gt;GAN with deep convolutional network&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/gan/cycle_gan/index.html&#34;&gt;Cycle GAN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/gan/wasserstein/index.html&#34;&gt;Wasserstein GAN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/gan/wasserstein/gradient_penalty/index.html&#34;&gt;Wasserstein GAN with Gradient Penalty&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/gan/stylegan/index.html&#34;&gt;StyleGAN 2&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;‚ú® &lt;a href=&#34;https://nn.labml.ai/recurrent_highway_networks/index.html&#34;&gt;Recurrent Highway Networks&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;‚ú® &lt;a href=&#34;https://nn.labml.ai/lstm/index.html&#34;&gt;LSTM&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;‚ú® &lt;a href=&#34;https://nn.labml.ai/hypernetworks/hyper_lstm.html&#34;&gt;HyperNetworks - HyperLSTM&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;‚ú® &lt;a href=&#34;https://nn.labml.ai/resnet/index.html&#34;&gt;ResNet&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;‚ú® &lt;a href=&#34;https://nn.labml.ai/conv_mixer/index.html&#34;&gt;ConvMixer&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;‚ú® &lt;a href=&#34;https://nn.labml.ai/capsule_networks/index.html&#34;&gt;Capsule Networks&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;‚ú® &lt;a href=&#34;https://nn.labml.ai/unet/index.html&#34;&gt;U-Net&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;‚ú® &lt;a href=&#34;https://nn.labml.ai/sketch_rnn/index.html&#34;&gt;Sketch RNN&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;‚ú® Graph Neural Networks&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/graphs/gat/index.html&#34;&gt;Graph Attention Networks (GAT)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/graphs/gatv2/index.html&#34;&gt;Graph Attention Networks v2 (GATv2)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;‚ú® &lt;a href=&#34;https://nn.labml.ai/cfr/index.html&#34;&gt;Counterfactual Regret Minimization (CFR)&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Solving games with incomplete information such as poker with CFR.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/cfr/kuhn/index.html&#34;&gt;Kuhn Poker&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;‚ú® &lt;a href=&#34;https://nn.labml.ai/rl/index.html&#34;&gt;Reinforcement Learning&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/rl/ppo/index.html&#34;&gt;Proximal Policy Optimization&lt;/a&gt; with &lt;a href=&#34;https://nn.labml.ai/rl/ppo/gae.html&#34;&gt;Generalized Advantage Estimation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/rl/dqn/index.html&#34;&gt;Deep Q Networks&lt;/a&gt; with with &lt;a href=&#34;https://nn.labml.ai/rl/dqn/model.html&#34;&gt;Dueling Network&lt;/a&gt;, &lt;a href=&#34;https://nn.labml.ai/rl/dqn/replay_buffer.html&#34;&gt;Prioritized Replay&lt;/a&gt; and Double Q Network.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;‚ú® &lt;a href=&#34;https://nn.labml.ai/optimizers/index.html&#34;&gt;Optimizers&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/optimizers/adam.html&#34;&gt;Adam&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/optimizers/amsgrad.html&#34;&gt;AMSGrad&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/optimizers/adam_warmup.html&#34;&gt;Adam Optimizer with warmup&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/optimizers/noam.html&#34;&gt;Noam Optimizer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/optimizers/radam.html&#34;&gt;Rectified Adam Optimizer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/optimizers/ada_belief.html&#34;&gt;AdaBelief Optimizer&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;‚ú® &lt;a href=&#34;https://nn.labml.ai/normalization/index.html&#34;&gt;Normalization Layers&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/normalization/batch_norm/index.html&#34;&gt;Batch Normalization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/normalization/layer_norm/index.html&#34;&gt;Layer Normalization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/normalization/instance_norm/index.html&#34;&gt;Instance Normalization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/normalization/group_norm/index.html&#34;&gt;Group Normalization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/normalization/weight_standardization/index.html&#34;&gt;Weight Standardization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/normalization/batch_channel_norm/index.html&#34;&gt;Batch-Channel Normalization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/normalization/deep_norm/index.html&#34;&gt;DeepNorm&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;‚ú® &lt;a href=&#34;https://nn.labml.ai/distillation/index.html&#34;&gt;Distillation&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;‚ú® &lt;a href=&#34;https://nn.labml.ai/adaptive_computation/index.html&#34;&gt;Adaptive Computation&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/adaptive_computation/ponder_net/index.html&#34;&gt;PonderNet&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;‚ú® &lt;a href=&#34;https://nn.labml.ai/uncertainty/index.html&#34;&gt;Uncertainty&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/uncertainty/evidence/index.html&#34;&gt;Evidential Deep Learning to Quantify Classification Uncertainty&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;‚ú® &lt;a href=&#34;https://nn.labml.ai/activations/index.html&#34;&gt;Activations&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/activations/fta/index.html&#34;&gt;Fuzzy Tiling Activations&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;‚ú® &lt;a href=&#34;https://nn.labml.ai/sampling/index.html&#34;&gt;Langauge Model Sampling Techniques&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/sampling/greedy.html&#34;&gt;Greedy Sampling&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/sampling/temperature.html&#34;&gt;Temperature Sampling&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/sampling/top_k.html&#34;&gt;Top-k Sampling&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/sampling/nucleus.html&#34;&gt;Nucleus Sampling&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;‚ú® &lt;a href=&#34;https://nn.labml.ai/scaling/index.html&#34;&gt;Scalable Training/Inference&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/scaling/zero3/index.html&#34;&gt;Zero3 memory optimizations&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Highlighted Research Paper PDFs&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/2205.14135.pdf&#34;&gt;FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/2204.10628.pdf&#34;&gt;Autoregressive Search Engines: Generating Substrings as Document Identifiers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/2203.15556.pdf&#34;&gt;Training Compute-Optimal Large Language Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/1910.02054.pdf&#34;&gt;ZeRO: Memory Optimizations Toward Training Trillion Parameter Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/2204.02311.pdf&#34;&gt;PaLM: Scaling Language Modeling with Pathways&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/dall-e-2.pdf&#34;&gt;Hierarchical Text-Conditional Image Generation with CLIP Latents&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/2203.14465.pdf&#34;&gt;STaR: Self-Taught Reasoner Bootstrapping Reasoning With Reasoning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/2112.04426.pdf&#34;&gt;Improving language models by retrieving from trillions of tokens&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/2003.08934.pdf&#34;&gt;NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/1706.03762.pdf&#34;&gt;Attention Is All You Need&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/2006.11239.pdf&#34;&gt;Denoising Diffusion Probabilistic Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/2109.08668.pdf&#34;&gt;Primer: Searching for Efficient Transformers for Language Modeling&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/1803.02999.pdf&#34;&gt;On First-Order Meta-Learning Algorithms&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/2103.00020.pdf&#34;&gt;Learning Transferable Visual Models From Natural Language Supervision&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/2109.02869.pdf&#34;&gt;The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/1805.09801.pdf&#34;&gt;Meta-Gradient Reinforcement Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/google_maps_eta.pdf&#34;&gt;ETA Prediction with Graph Neural Networks in Google Maps&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/ponder_net.pdf&#34;&gt;PonderNet: Learning to Ponder&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/muzero.pdf&#34;&gt;Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/gans_n_roses.pdf&#34;&gt;GANs N‚Äô Roses: Stable, Controllable, Diverse Image to Image Translation (works for videos too!)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/vit.pdf&#34;&gt;An Image is Worth 16X16 Word: Transformers for Image Recognition at Scale&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/resnet.pdf&#34;&gt;Deep Residual Learning for Image Recognition&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/distillation.pdf&#34;&gt;Distilling the Knowledge in a Neural Network&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install labml-nn&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Citing&lt;/h3&gt; &#xA;&lt;p&gt;If you use this for academic research, please cite it using the following BibTeX entry.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{labml,&#xA; author = {Varuna Jayasiri, Nipun Wijerathne},&#xA; title = {labml.ai Annotated Paper Implementations},&#xA; year = {2020},&#xA; url = {https://nn.labml.ai/},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Other Projects&lt;/h3&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://papers.labml.ai/&#34;&gt;üöÄ Trending Research Papers&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This shows the most popular research papers on social media. It also aggregates links to useful resources like paper explanations videos and discussions.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://github.com/labmlai/labml&#34;&gt;üß™ labml.ai/labml&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This is a library that let&#39;s you monitor deep learning model training and hardware usage from your mobile phone. It also comes with a bunch of other tools to help write deep learning code efficiently.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>amanchadha/coursera-deep-learning-specialization</title>
    <updated>2023-01-01T01:44:34Z</updated>
    <id>tag:github.com,2023-01-01:/amanchadha/coursera-deep-learning-specialization</id>
    <link href="https://github.com/amanchadha/coursera-deep-learning-specialization" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Notes, programming assignments and quizzes from all courses within the Coursera Deep Learning specialization offered by deeplearning.ai: (i) Neural Networks and Deep Learning; (ii) Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization; (iii) Structuring Machine Learning Projects; (iv) Convolutional Neural Network‚Ä¶&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Deep Learning Specialization on Coursera (offered by deeplearning.ai)&lt;/h1&gt; &#xA;&lt;p&gt;Programming assignments and quizzes from all courses in the Coursera &lt;a href=&#34;https://www.coursera.org/specializations/deep-learning&#34;&gt;Deep Learning specialization&lt;/a&gt; offered by &lt;code&gt;deeplearning.ai&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Instructor: &lt;a href=&#34;http://www.andrewng.org/&#34;&gt;Andrew Ng&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Notes&lt;/h2&gt; &#xA;&lt;h3&gt;For detailed interview-ready notes on all courses in the Coursera Deep Learning specialization, refer &lt;a href=&#34;https://aman.ai/&#34;&gt;www.aman.ai&lt;/a&gt;.&lt;/h3&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;Run &lt;code&gt;setup.sh&lt;/code&gt; to (i) download a pre-trained VGG-19 dataset and (ii) extract the zip&#39;d pre-trained models and datasets that are needed for all the assignments.&lt;/p&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;p&gt;This repo contains my work for this specialization. The code base, quiz questions and diagrams are taken from the &lt;a href=&#34;https://www.coursera.org/specializations/deep-learning&#34;&gt;Deep Learning Specialization on Coursera&lt;/a&gt;, unless specified otherwise.&lt;/p&gt; &#xA;&lt;h2&gt;2021 Version&lt;/h2&gt; &#xA;&lt;p&gt;This specialization was updated in April 2021 to include developments in deep learning and programming frameworks, with the biggest change being shifting from TensorFlow 1 to TensorFlow 2. This repo has been updated accordingly as well.&lt;/p&gt; &#xA;&lt;h2&gt;Programming Assignments&lt;/h2&gt; &#xA;&lt;h3&gt;Course 1: Neural Networks and Deep Learning&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%202/Python%20Basics%20with%20Numpy/Python_Basics_With_Numpy_v3a.ipynb&#34;&gt;Week 2 - PA 1 - Python Basics with Numpy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%202/Logistic%20Regression%20as%20a%20Neural%20Network/Logistic_Regression_with_a_Neural_Network_mindset_v6a.ipynb&#34;&gt;Week 2 - PA 2 - Logistic Regression with a Neural Network mindset&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%203/Planar%20data%20classification%20with%20one%20hidden%20layer/Planar_data_classification_with_onehidden_layer_v6c.ipynb&#34;&gt;Week 3 - PA 3 - Planar data classification with one hidden layer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%204/Building%20your%20Deep%20Neural%20Network%20-%20Step%20by%20Step/Building_your_Deep_Neural_Network_Step_by_Step_v8a.ipynb&#34;&gt;Week 4 - PA 4 - Building your Deep Neural Network: Step by Step&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%204/Deep%20Neural%20Network%20Application_%20Image%20Classification/Deep%20Neural%20Network%20-%20Application%20v8.ipynb&#34;&gt;Week 4 - PA 5 - Deep Neural Network for Image Classification: Application&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Course 2: Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%201/Initialization/Initialization.ipynb&#34;&gt;Week 1 - PA 1 - Initialization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%201/Regularization/Regularization_v2a.ipynb&#34;&gt;Week 1 - PA 2 - Regularization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%201/Gradient%20Checking/Gradient%20Checking%20v1.ipynb&#34;&gt;Week 1 - PA 3 - Gradient Checking&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%202/Optimization_methods_v1b.ipynb&#34;&gt;Week 2 - PA 4 - Optimization Methods&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%203/Tensorflow_introduction.ipynb&#34;&gt;Week 3 - PA 5 - TensorFlow Tutorial&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Course 3: Structuring Machine Learning Projects&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;There are no programming assignments for this course. But this course comes with very interesting case study quizzes (below).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Course 4: Convolutional Neural Networks&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%201/Convolution_model_Step_by_Step_v1.ipynb&#34;&gt;Week 1 - PA 1 - Convolutional Model: step by step&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%201/Convolution_model_Application.ipynb&#34;&gt;Week 1 - PA 2 - Convolutional Neural Networks: Application&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%202/KerasTutorial/Keras%20-%20Tutorial%20-%20Happy%20House%20v2.ipynb&#34;&gt;Week 2 - PA 1 - Keras - Tutorial - Happy House&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%202/ResNets/Residual_Networks.ipynb&#34;&gt;Week 2 - PA 2 - Residual Networks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%202/Transfer%20Learning%20with%20MobileNet/Transfer_learning_with_MobileNet_v1.ipynb&#34;&gt;Week 2 - PA 2 - Transfer Learning with MobileNet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%203/Car%20detection%20for%20Autonomous%20Driving/Autonomous_driving_application_Car_detection.ipynb&#34;&gt;Week 3 - PA 1 - Car detection with YOLO for Autonomous Driving&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%203/Image%20Segmentation%20Unet/Image_segmentation_Unet_v2.ipynb&#34;&gt;Week 3 - PA 2 - Image Segmentation Unet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%204/Neural%20Style%20Transfer/Art_Generation_with_Neural_Style_Transfer.ipynb&#34;&gt;Week 4 - PA 1 - Art Generation with Neural Style Transfer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%204/Face%20Recognition/Face_Recognition.ipynb&#34;&gt;Week 4 - PA 2 - Face Recognition&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Course 5: Sequence Models&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%201/Building%20a%20Recurrent%20Neural%20Network%20-%20Step%20by%20Step/Building_a_Recurrent_Neural_Network_Step_by_Step.ipynb&#34;&gt;Week 1 - PA 1 - Building a Recurrent Neural Network - Step by Step&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%201/Dinosaur%20Island%20--%20Character-level%20language%20model/Dinosaurus_Island_Character_level_language_model.ipynb&#34;&gt;Week 1 - PA 2 - Dinosaur Land -- Character-level Language Modeling&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%201/Jazz%20improvisation%20with%20LSTM/Improvise_a_Jazz_Solo_with_an_LSTM_Network_v4_Solution.ipynb&#34;&gt;Week 1 - PA 3 - Jazz improvisation with LSTM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%202/Word%20Vector%20Representation/Operations_on_word_vectors_v2a.ipynb&#34;&gt;Week 2 - PA 1 - Word Vector Representation and Debiasing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%202/Emojify/Emoji_v3a.ipynb&#34;&gt;Week 2 - PA 2 - Emojify!&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%203/Machine%20Translation/Neural_machine_translation_with_attention_v4a.ipynb&#34;&gt;Week 3 - PA 1 - Neural Machine Translation with Attention&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%203/Trigger%20word%20detection/Trigger_word_detection_v2a.ipynb&#34;&gt;Week 3 - PA 2 - Trigger Word Detection&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%204/Transformer%20Subclass/C5_W4_A1_Transformer_Subclass_v1.ipynb&#34;&gt;Week 4 - PA 1 - Transformer Network&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%203/Named%20Entity%20Recognition/Transformer_application_Named_Entity_Recognition.ipynb&#34;&gt;Week 3 - PA 2 - Transformer Network Application: Named-Entity Recognition&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%203/Question%20Answering/QA_transformer.ipynb&#34;&gt;Week 3 - PA 2 - Transformer Network Application: Question Answering&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quiz Solutions&lt;/h2&gt; &#xA;&lt;h3&gt;Course 1: Neural Networks and Deep Learning&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Week 1 Quiz - Introduction to deep learning: &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%201/Week%201%20Quiz%20-%20Introduction%20to%20deep%20learning.md&#34;&gt;Text&lt;/a&gt; | &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%201/Week%201%20Quiz%20-%20Introduction%20to%20deep%20learning.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Week 2 Quiz - Neural Network Basics: &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%202/Week%202%20Quiz%20-%20Neural%20Network%20Basics.md&#34;&gt;Text&lt;/a&gt; | &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%202/Week%202%20Quiz%20-%20Neural%20Network%20Basics.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Week 3 Quiz - Shallow Neural Networks: &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%203/Week%203%20Quiz%20-%20Shallow%20Neural%20Networks.md&#34;&gt;Text&lt;/a&gt; | &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%203/Week%203%20Quiz%20-%20Shallow%20Neural%20Networks.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Week 4 Quiz - Key concepts on Deep Neural Networks: &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%204/Week%204%20Quiz%20-%20Key%20concepts%20on%20Deep%20Neural%20Networks.md&#34;&gt;Text&lt;/a&gt; | &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%204/Week%204%20Quiz%20-%20Key%20concepts%20on%20Deep%20Neural%20Networks.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Course 2: Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Week 1 Quiz - Practical aspects of deep learning: &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%201/Week%201%20Quiz%20-%20Practical%20aspects%20of%20deep%20learning.md&#34;&gt;Text&lt;/a&gt; | &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%201/Week%201%20Quiz%20-%20Practical%20aspects%20of%20deep%20learning.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Week 2 Quiz - Optimization algorithms: &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%202/Week%202%20Quiz%20-%20Optimization%20algorithms.md&#34;&gt;Text&lt;/a&gt; | &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%202/Week%202%20Quiz%20-%20Optimization%20algorithms.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Week 3 Quiz - Hyperparameter tuning, Batch Normalization, Programming Frameworks: &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%203/Week%203%20Quiz%20-%20Hyperparameter%20tuning%2C%20Batch%20Normalization%2C%20Programming%20Frameworks.md&#34;&gt;Text&lt;/a&gt; | &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%203/Week%203%20Quiz%20-%20Hyperparameter%20tuning%2C%20Batch%20Normalization%2C%20Programming%20Frameworks.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Course 3: Structuring Machine Learning Projects&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Week 1 Quiz - Bird recognition in the city of Peacetopia (case study): &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C3%20-%20Structuring%20Machine%20Learning%20Projects/Week%201%20Quiz%20-%20Bird%20recognition%20in%20the%20city%20of%20Peacetopia%20(case%20study).md&#34;&gt;Text&lt;/a&gt; | &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C3%20-%20Structuring%20Machine%20Learning%20Projects/Week%201%20Quiz%20-%20Bird%20recognition%20in%20the%20city%20of%20Peacetopia%20(case%20study).pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Week 2 Quiz - Autonomous driving (case study): &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C3%20-%20Structuring%20Machine%20Learning%20Projects/Week%202%20Quiz%20-%20Autonomous%20driving%20(case%20study).md&#34;&gt;Text&lt;/a&gt; | &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C3%20-%20Structuring%20Machine%20Learning%20Projects/Week%202%20Quiz%20-%20Autonomous%20driving%20(case%20study).pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Course 4: Convolutional Neural Networks&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Week 1 Quiz - The basics of ConvNets: &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%201/Week%201%20Quiz%20-%20The%20basics%20of%20ConvNets.md&#34;&gt;Text&lt;/a&gt; | &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%201/Week%201%20Quiz%20-%20The%20basics%20of%20ConvNets.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Week 2 Quiz - Deep convolutional models: &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%202/Week%202%20Quiz%20-%20Deep%20convolutional%20models.md&#34;&gt;Text&lt;/a&gt; | &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%202/Week%202%20Quiz%20-%20Deep%20convolutional%20models.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Week 3 Quiz - Detection algorithms: &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%203/Week%203%20Quiz%20-%20Detection%20algorithms.md&#34;&gt;Text&lt;/a&gt; | &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%203/Week%203%20Quiz%20-%20Detection%20algorithms.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Week 4 Quiz - Special applications: Face recognition &amp;amp; Neural style transfer: &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%204/Week%204%20Quiz%20-%20Special%20applications%20Face%20Recognition%20and%20Neural%20Style%20Transfer.md&#34;&gt;Text&lt;/a&gt; | &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%204/Week%204%20Quiz%20-%20Special%20applications%20Face%20Recognition%20and%20Neural%20Style%20Transfer.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Course 5: Sequence Models&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Week 1 Quiz - Recurrent Neural Networks: &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%201/Week%201%20Quiz%20-%20Recurrent%20Neural%20Networks.md&#34;&gt;Text&lt;/a&gt; | &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%201/Week%201%20Quiz%20-%20Recurrent%20Neural%20Networks.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Week 2 Quiz - Natural Language Processing &amp;amp; Word Embeddings: &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%202/Week%202%20Quiz%20-%20Natural%20Language%20Processing%20%26%20Word%20Embeddings.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Week 3 Quiz - Sequence models &amp;amp; Attention mechanism: &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%203/Week%203%20Quiz%20-%20Sequence%20models%20%26%20Attention%20mechanisms.md&#34;&gt;Text&lt;/a&gt; | &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%203/Week%203%20Quiz%20-%20Sequence%20models%20%26%20Attention%20mechanisms.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;I recognize the time people spend on building intuition, understanding new concepts and debugging assignments. The solutions uploaded here are &lt;strong&gt;only for reference&lt;/strong&gt;. They are meant to unblock you if you get stuck somewhere. Please do not copy any part of the code as-is (the programming assignments are fairly easy if you read the instructions carefully). Similarly, try out the quizzes yourself before you refer to the quiz solutions. This course is the most straight-forward deep learning course I have ever taken, with fabulous course content and structure. It&#39;s a treasure by the deeplearning.ai team.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mrdbourke/zero-to-mastery-ml</title>
    <updated>2023-01-01T01:44:34Z</updated>
    <id>tag:github.com,2023-01-01:/mrdbourke/zero-to-mastery-ml</id>
    <link href="https://github.com/mrdbourke/zero-to-mastery-ml" rel="alternate"></link>
    <summary type="html">&lt;p&gt;All course materials for the Zero to Mastery Machine Learning and Data Science course.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Zero to Mastery Machine Learning&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://mybinder.org/v2/gh/mrdbourke/zero-to-mastery-ml/master&#34;&gt;&lt;img src=&#34;https://mybinder.org/badge_logo.svg?sanitize=true&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.deepnote.com/launch?template=data-science&amp;amp;url=https://github.com/mrdbourke/zero-to-mastery-ml/raw/master/section-2-data-science-and-ml-tools/introduction-to-pandas.ipynb&#34;&gt;&lt;img src=&#34;https://deepnote.com/buttons/launch-in-deepnote.svg?sanitize=true&#34; alt=&#34;Deepnote&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/mrdbourke/zero-to-mastery-ml/blob/master&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Welcome! This repository contains all of the code, notebooks, images and other materials related to the &lt;a href=&#34;https://dbourke.link/mlcourse&#34;&gt;Zero to Mastery Machine Learning Course on Udemy&lt;/a&gt; and &lt;a href=&#34;https://dbourke.link/ZTMmlcourse&#34;&gt;zerotomastery.io&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;d like to see anything in particular, please send me an email: &lt;a href=&#34;mailto:daniel@mrdbourke.com&#34;&gt;daniel@mrdbourke.com&lt;/a&gt; or leave an issue.&lt;/p&gt; &#xA;&lt;h2&gt;What this course focuses on&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create a framework for working through problems (&lt;a href=&#34;https://github.com/mrdbourke/zero-to-mastery-ml/raw/master/section-1-getting-ready-for-machine-learning/a-6-step-framework-for-approaching-machine-learning-projects.md&#34;&gt;6 step machine learning modelling framework&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Find tools to fit the framework&lt;/li&gt; &#xA; &lt;li&gt;Targeted practice = use tools and framework steps to work on end-to-end machine learning modelling projects&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;How this course is structured&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Section 1 - Getting your mind and computer ready for machine learning (concepts, computer setup)&lt;/li&gt; &#xA; &lt;li&gt;Section 2 - Tools for machine learning and data science (pandas, NumPy, Matplotlib, Scikit-Learn)&lt;/li&gt; &#xA; &lt;li&gt;Section 3 - End-to-end structured data projects (classification and regression)&lt;/li&gt; &#xA; &lt;li&gt;Section 4 - Neural networks, deep learning and transfer learning with TensorFlow 2.0&lt;/li&gt; &#xA; &lt;li&gt;Section 5 - Communicating and sharing your work&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Student notes&lt;/h2&gt; &#xA;&lt;p&gt;Some students have taken and shared extensive notes on this course, see them below.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;d like to submit yours, leave a pull request.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Chester&#39;s notes - &lt;a href=&#34;https://github.com/chesterheng/machinelearning-datascience&#34;&gt;https://github.com/chesterheng/machinelearning-datascience&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Sophia&#39;s notes - &lt;a href=&#34;https://www.rockyourcode.com/tags/udemy-complete-machine-learning-and-data-science-zero-to-mastery/&#34;&gt;https://www.rockyourcode.com/tags/udemy-complete-machine-learning-and-data-science-zero-to-mastery/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
</feed>