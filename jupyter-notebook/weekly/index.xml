<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-17T01:47:37Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>anthropics/anthropic-cookbook</title>
    <updated>2024-03-17T01:47:37Z</updated>
    <id>tag:github.com,2024-03-17:/anthropics/anthropic-cookbook</id>
    <link href="https://github.com/anthropics/anthropic-cookbook" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A collection of notebooks/recipes showcasing some fun and effective ways of using Claude.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Anthropic Cookbook&lt;/h1&gt; &#xA;&lt;p&gt;The Anthropic Cookbook provides code and guides designed to help developers build with Claude, providing copy-able code snippets that you can easily integrate into your own projects.&lt;/p&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;p&gt;To make the most of the examples in this cookbook, you&#39;ll need an Anthropic API key (sign up for free &lt;a href=&#34;https://www.anthropic.com&#34;&gt;here&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;While the code examples are primarily written in Python, the concepts can be adapted to any programming language that supports interaction with the Anthropic API.&lt;/p&gt; &#xA;&lt;h2&gt;Explore Further&lt;/h2&gt; &#xA;&lt;p&gt;Looking for more resources to enhance your experience with Claude and AI assistants? Check out these helpful links:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.anthropic.com/claude/docs/guide-to-anthropics-prompt-engineering-resources&#34;&gt;Anthropic developer documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://support.anthropic.com&#34;&gt;Anthropic support docs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.anthropic.com/discord&#34;&gt;Anthropic Discord community&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;The Anthropic Cookbook thrives on the contributions of the developer community. We value your input, whether it&#39;s submitting an idea, fixing a typo, adding a new guide, or improving an existing one. By contributing, you help make this resource even more valuable for everyone.&lt;/p&gt; &#xA;&lt;p&gt;To avoid duplication of efforts, please review the existing issues and pull requests before contributing.&lt;/p&gt; &#xA;&lt;p&gt;If you have ideas for new examples or guides, share them on the &lt;a href=&#34;https://github.com/anthropics/anthropic-cookbook/issues&#34;&gt;issues page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Table of recipes&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/anthropics/anthropic-cookbook/raw/main/function_calling/function_calling.ipynb&#34;&gt;Tool use &amp;amp; function calling&lt;/a&gt;: Learn how to integrate Claude with external tools and functions to extend its capabilities, including how to prompt Claude to &lt;a href=&#34;https://github.com/anthropics/anthropic-cookbook/raw/main/misc/how_to_make_sql_queries.ipynb&#34;&gt;make SQL queries&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/anthropics/anthropic-cookbook/tree/main/third_party&#34;&gt;Retrieval augmented generation&lt;/a&gt;: Learn how to supplement Claude&#39;s knowledge and improve the accuracy and relevance of Claude&#39;s responses with data retrieved from &lt;a href=&#34;https://github.com/anthropics/anthropic-cookbook/raw/main/third_party/Pinecone/rag_using_pinecone.ipynb&#34;&gt;vector databases&lt;/a&gt;, &lt;a href=&#34;https://github.com/anthropics/anthropic-cookbook/raw/main/third_party/Wikipedia/wikipedia-search-cookbook.ipynb/&#34;&gt;Wikipedia&lt;/a&gt;, and the &lt;a href=&#34;https://github.com/anthropics/anthropic-cookbook/raw/main/third_party/Brave/web_search_using_brave.ipynb&#34;&gt;internet&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/anthropics/anthropic-cookbook/raw/main/third_party/VoyageAI/how_to_create_embeddings.md&#34;&gt;Embeddings&lt;/a&gt;: Learn how to use Voyage AI to handle embeddings.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/anthropics/anthropic-cookbook/tree/main/multimodal&#34;&gt;Use images with Claude&lt;/a&gt;: This cookbook has tips on &lt;a href=&#34;https://github.com/anthropics/anthropic-cookbook/raw/main/multimodal/getting_started_with_vision.ipynb&#34;&gt;getting started with images&lt;/a&gt; and &lt;a href=&#34;https://github.com/anthropics/anthropic-cookbook/raw/main/multimodal/best_practices_for_vision.ipynb&#34;&gt;best practice techniques&lt;/a&gt; to ensure the highest quality performance with images. See how you can effectively prompt Claude with images to carry out tasks such as &lt;a href=&#34;https://github.com/anthropics/anthropic-cookbook/raw/main/multimodal/reading_charts_graphs_powerpoints.ipynb&#34;&gt;interpreting and analyzing charts&lt;/a&gt; or &lt;a href=&#34;https://github.com/anthropics/anthropic-cookbook/raw/main/multimodal/how_to_transcribe_text.ipynb&#34;&gt;extracting content from forms&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/anthropics/anthropic-cookbook/raw/main/misc/how_to_generate_imgaes.ipynb&#34;&gt;Generate images with Claude&lt;/a&gt;: This cookbook will show you how to have Claude generate images via Stable Diffusion.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/anthropics/anthropic-cookbook/raw/main/misc/pdf_upload_summarization.ipynb&#34;&gt;Upload PDFs to Claude&lt;/a&gt;: Discover how to parse and pass PDFs as text to Claude&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/anthropics/anthropic-cookbook/raw/main/misc/building_evals.ipynb&#34;&gt;Automated evaluations&lt;/a&gt;: Discover how to use Claude to automate the prompt evaluation process.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/anthropics/anthropic-cookbook/raw/main/misc/how_to_enable_json_mode.ipynb&#34;&gt;Enable JSON mode&lt;/a&gt;: Ensure you always get JSON with this quick and easy cookbook.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/anthropics/anthropic-cookbook/raw/main/misc/building_moderation_filter.ipynb&#34;&gt;Create a moderation filter with Claude&lt;/a&gt;: Find out how to use Claude to create a content moderation filter for your application.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>xiaolai/everyone-can-use-english</title>
    <updated>2024-03-17T01:47:37Z</updated>
    <id>tag:github.com,2024-03-17:/xiaolai/everyone-can-use-english</id>
    <link href="https://github.com/xiaolai/everyone-can-use-english" rel="alternate"></link>
    <summary type="html">&lt;p&gt;人人都能用英语&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;一千小时&lt;/h1&gt; &#xA;&lt;h2&gt;在线阅读&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://1000h.org&#34;&gt;1000h.org&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;本地阅读&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;yarn install&#xA;yarn docs:dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Enjoy App&lt;/h1&gt; &#xA;&lt;h2&gt;使用说明&lt;/h2&gt; &#xA;&lt;p&gt;请参阅 &lt;a href=&#34;https://1000h.org/enjoy-app/&#34;&gt;https://1000h.org/enjoy-app/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;本地启动&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;yarn install&#xA;yarn enjoy:start&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;人人都能用英语&lt;/h1&gt; &#xA;&lt;h2&gt;目录&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xiaolai/everyone-can-use-english/main/book/README.md&#34;&gt;简介&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xiaolai/everyone-can-use-english/main/book/chapter1.md&#34;&gt;第一章：起点&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xiaolai/everyone-can-use-english/main/book/chapter2.md&#34;&gt;第二章：口语&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xiaolai/everyone-can-use-english/main/book/chapter3.md&#34;&gt;第三章：语音&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xiaolai/everyone-can-use-english/main/book/chapter4.md&#34;&gt;第四章：朗读&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xiaolai/everyone-can-use-english/main/book/chapter5.md&#34;&gt;第五章：词典&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xiaolai/everyone-can-use-english/main/book/chapter6.md&#34;&gt;第六章：语法&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xiaolai/everyone-can-use-english/main/book/chapter7.md&#34;&gt;第七章：精读&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xiaolai/everyone-can-use-english/main/book/chapter8.md&#34;&gt;第八章：叮嘱&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xiaolai/everyone-can-use-english/main/book/end.md&#34;&gt;后记&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>NoviScl/Design2Code</title>
    <updated>2024-03-17T01:47:37Z</updated>
    <id>tag:github.com,2024-03-17:/NoviScl/Design2Code</id>
    <link href="https://github.com/NoviScl/Design2Code" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Design2Code: How Far Are We From Automating Front-End Engineering?&lt;/h1&gt; &#xA;&lt;p&gt;Quick Links: &lt;a href=&#34;https://huggingface.co/datasets/SALT-NLP/Design2Code-hf&#34;&gt;[Dataset]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/SALT-NLP/Design2Code-18B-v0&#34;&gt;[Model Checkpoint]&lt;/a&gt; &lt;a href=&#34;https://salt-nlp.github.io/Design2Code/&#34;&gt;[Project Page]&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2403.03163&#34;&gt;[Paper]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;This is the official repo for our Design2Code project, maintained by the SALT lab from Stanford NLP. In this repo, we provide:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;The Design2Code benchmark dataset for the task of converting visual design (screenshot) into code implementation, which consists of 484 real-world webpages from C4 (examples shown below).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Code for running all automatic evaluation.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Code for running multimodal prompting experiments on GPT-4V and Gemini Pro Vision.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Code for finetuning and running inference on our open-source Design2Code-18B model.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/NoviScl/Design2Code/main/example.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Set Up&lt;/h2&gt; &#xA;&lt;p&gt;All code is tested on Python 3.11. We recommend using a virtual environment to manage the dependencies.&lt;/p&gt; &#xA;&lt;p&gt;Clone this repo and install the necessary libraries:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Taking screenshots and running evaluations also need to install browsers&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;playwright install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If the above doesn&#39;t work, try:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 -m playwright install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Data and Predictions&lt;/h2&gt; &#xA;&lt;h3&gt;Testset&lt;/h3&gt; &#xA;&lt;p&gt;You can download the full testset from this &lt;a href=&#34;https://drive.google.com/file/d/12uRO5EC7hkg6qAOyfJhb4YsrQ_qpL5bt/view?usp=sharing&#34;&gt;Google Drive link&lt;/a&gt; or access it from the Huggingface dataset &lt;a href=&#34;https://huggingface.co/datasets/SALT-NLP/Design2Code&#34;&gt;page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;After you unzip it into &lt;code&gt;testset_final/&lt;/code&gt;, the folder should include 484 pairs of screenshots (&lt;code&gt;xx.png&lt;/code&gt;) and corresponding HTML code (&lt;code&gt;xx.html&lt;/code&gt;). We also include the placeholder image file &lt;code&gt;rick.jpg&lt;/code&gt; which is used in the HTML codes.&lt;/p&gt; &#xA;&lt;h3&gt;Taking Screenshots&lt;/h3&gt; &#xA;&lt;p&gt;In case you want to take screenshots of webpages by yourself, you can do so by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd Design2Code&#xA;python3 data_utils/screenshot.py &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Remember to replace the file name or directory in the script with your own.&lt;/p&gt; &#xA;&lt;h3&gt;Model Predictions&lt;/h3&gt; &#xA;&lt;p&gt;To facilitate more analysis, we also release all model predictions on our benchmark:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://drive.google.com/file/d/1SgWL4E5uKVo-8D3Bj-VWvysJs_2OguA1/view?usp=sharing&#34;&gt;GPT-4V&lt;/a&gt; (including Direct Prompting, Text-Augmented Prompting, and Self-Revision Prompting)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://drive.google.com/file/d/18cpGdL1Yhv9UU7odcqkncDItGo0Guuy_/view?usp=sharing&#34;&gt;Gemini Pro Vision&lt;/a&gt; (including Direct Prompting, Text-Augmented Prompting, and Self-Revision Prompting)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://drive.google.com/file/d/1lFqLyJSDwZAEhZ4mhRqrK_-d5hhcrrEM/view?usp=sharing&#34;&gt;WebSight VLM-8B&lt;/a&gt; (Huggingface)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://drive.google.com/file/d/1XxZMeVpAGu3fGvtKetHvk2bk3vyBcj2e/view?usp=sharing&#34;&gt;Design2Code-18B&lt;/a&gt; (Ours)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://drive.google.com/file/d/1qahQCmGqEXPXKmn2RzNwHsOI-CAQSP6P/view?usp=sharing&#34;&gt;Automatic Evaluation Results&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/SALT-NLP/Design2Code_human_eval_pairwise&#34;&gt;Human Eval - Pairwise Model Comparison&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/SALT-NLP/Design2Code_human_eval_reference_vs_gpt4v&#34;&gt;Human Eval - Direct Assessment&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Running Prompting Experiments&lt;/h2&gt; &#xA;&lt;p&gt;To run prompting experiments, first put your OpenAI / Google Gemini API keys in a file called &lt;code&gt;api_keys.json&lt;/code&gt; in the root directory. It should look like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;    &#34;organization_id&#34;: &#34;&#34;,&#xA;    &#34;openai_key&#34;: &#34;&#34;,&#xA;    &#34;openai_endpoint&#34;: &#34;&#34;,&#xA;    &#34;gemini_api_key&#34;: &#34;&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, to run GPT-4V experiments, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash prompting/gpt4v.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run Gemini Pro Vision experiments, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash prompting/gemini.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The bash scripts include scripts for running Direct Prompting, Text-Augmented Prompting, and Self-Revision Prompting. All prompts are written in &lt;code&gt;prompting/gpt4v.py&lt;/code&gt; and &lt;code&gt;prompting/gemini.py&lt;/code&gt;, you can modify it to run your own prompts or develop smarter prompting strategies. We welcome any contributions to this part of the project!&lt;/p&gt; &#xA;&lt;p&gt;Also note that we are accessing the OpenAI API from Azure, and you might need some slight modification for directly calling the OpenAI API.&lt;/p&gt; &#xA;&lt;h3&gt;Running Inference on CogAgent-18B&lt;/h3&gt; &#xA;&lt;p&gt;We also provide code to run inference on the base model CogAgent-18B:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 prompting/cogagent.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Be aware that the model is not finetuned on Design2Code, so the performance is very bad, often times not even producing valid HTML code.&lt;/p&gt; &#xA;&lt;h2&gt;Running Inference on Design2Code-18B&lt;/h2&gt; &#xA;&lt;p&gt;The finetuned model is based on &lt;a href=&#34;https://raw.githubusercontent.com/NoviScl/Design2Code/main/CogVLM/CogAgent_README.md&#34;&gt;CogAgent&lt;/a&gt;, please install necessary libraries following the instructions.&lt;/p&gt; &#xA;&lt;p&gt;You can run inference by:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 CogVLM/finetune_demo/inference_design2code.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Finetuning Design2Code-18B&lt;/h2&gt; &#xA;&lt;p&gt;The finetuning script is &lt;a href=&#34;https://raw.githubusercontent.com/NoviScl/Design2Code/main/CogVLM/finetune_demo/finetune_cogagent_lora_design2code.sh&#34;&gt;finetune_cogagent_lora_design2code.sh&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Running Automatic Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;You can use the following command to run automatic evaluation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 metrics/multi_processing_eval.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that you need to specify the directories where you store the model predictions in &lt;code&gt;metrics/multi_processing_eval.py&lt;/code&gt; (starting at line 54), like the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;test_dirs = {&#xA;        &#34;gpt4v_direct_prompting&#34;: &#34;../predictions_final/gpt4v_direct_prompting&#34;,&#xA;        &#34;gemini_direct_prompting&#34;: &#34;../predictions_final/gemini_direct_prompting&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where we assume each directory in the dict contains the predictions of the corresponding model/method (i.e., each directory should contain 484 predicted HTML files for the full test set, or for some subset that you sampled for yourself). The script will compute scores for all automatic metrics for all examples in each directory and store the results in a dictionary, with the following format:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;{&#xA;    &#34;gpt4v_direct_prompting&#34;: {&#xA;        &#34;2.html&#34;: [0.1, 0.2, ...],&#xA;        &#34;6.html&#34;: [0.3, 0.4, ...],&#xA;        ...&#xA;    },&#xA;    &#34;gemini_direct_prompting&#34;: {&#xA;        &#34;2.html&#34;: [0.5, 0.6, ...],&#xA;        &#34;6.html&#34;: [0.7, 0.8, ...],&#xA;        ...&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where each list contains the fine-grained breakdown metrics. The script will also print the average scores for each model/method in the end, with the following format:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;gpt4v_direct_prompting&#xA;&#xA;Block-Match:  0.6240771561959276&#xA;Text:  0.9769471025300969&#xA;Position:  0.7787072741618328&#xA;Color:  0.7068853534416764&#xA;CLIP:  0.8924754858016968&#xA;--------------------------------&#xA;&#xA;gemini_direct_prompting&#xA;&#xA;Block-Match:  0.6697374012874602&#xA;Text:  0.9731735845969769&#xA;Position:  0.6502285758036523&#xA;Color:  0.8531304981602478&#xA;CLIP:  0.8571878373622894&#xA;--------------------------------&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;These metrics are also what we reported in the paper. By default, we support multiprocessing to speed up evaluation, you can also manually turn it off by setting &lt;code&gt;multiprocessing = False&lt;/code&gt; on line 40. For your reference, it can take up to 1 hour to run the the evaluation on the full testset (for each model/method).&lt;/p&gt; &#xA;&lt;h2&gt;Other Functions&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;data_utils&lt;/code&gt; contains various filtering and processing scripts that we used to construct the test data from C4.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The data, code and model checkpoint are intended and licensed for research use only. Please do not use them for any malicious purposes.&lt;/p&gt; &#xA;&lt;p&gt;The benchmark is built on top of the C4 dataset, under the ODC Attribution License (ODC-By).&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;Our testset is filtered from &lt;a href=&#34;https://huggingface.co/datasets/c4&#34;&gt;C4&lt;/a&gt;, training examples are sampled from &lt;a href=&#34;https://huggingface.co/datasets/HuggingFaceM4/WebSight&#34;&gt;Websight&lt;/a&gt;. Our model is finetuned based on &lt;a href=&#34;https://github.com/THUDM/CogVLM&#34;&gt;CogAgent&lt;/a&gt;. Thanks for their awsome work!&lt;/p&gt; &#xA;&lt;p&gt;If you find our work helpful, please consider citing our paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{si2024design2code,&#xA;    title={Design2Code: How Far Are We From Automating Front-End Engineering?},&#xA;    author={Chenglei Si and Yanzhe Zhang and Zhengyuan Yang and Ruibo Liu and Diyi Yang},&#xA;    year={2024},&#xA;    eprint={2403.03163},&#xA;    archivePrefix={arXiv},&#xA;    primaryClass={cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We welcome all types of contributions to this project (PRs are welcome!). If you have any questions, please feel free to leave issues or email us.&lt;/p&gt;</summary>
  </entry>
</feed>