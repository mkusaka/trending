<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-12-18T01:46:47Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>cloneofsimo/lora</title>
    <updated>2022-12-18T01:46:47Z</updated>
    <id>tag:github.com,2022-12-18:/cloneofsimo/lora</id>
    <link href="https://github.com/cloneofsimo/lora" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Using Low-rank adaptation to quickly fine-tune diffusion models.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Low-rank Adaptation for Fast Text-to-Image Diffusion Fine-tuning&lt;/h1&gt; &#xA;&lt;!-- #region --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/cloneofsimo/lora/master/contents/alpha_scale.gif&#34;&gt; &lt;/p&gt; &#xA;&lt;!-- #endregion --&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Using LoRA to fine tune on illustration dataset : $W = W_0 + \alpha \Delta W$, where $\alpha$ is the merging ratio. Above gif is scaling alpha from 0 to 1. Setting alpha to 0 is same as using the original model, and setting alpha to 1 is same as using the fully fine-tuned model.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;!-- #region --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/cloneofsimo/lora/master/contents/lora_with_clip.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;!-- #endregion --&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&#34;game character bnha, wearing a red shirt, riding a donkey&#34;, with Overwatch-fine-tuned LoRA model, for both CLIP and Unet.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;!-- #region --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/cloneofsimo/lora/master/contents/disney_lora.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;!-- #endregion --&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&#34;style of sks, baby lion&#34;, with disney-style LoRA model.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;!-- #region --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/cloneofsimo/lora/master/contents/pop_art.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;!-- #endregion --&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&#34;style of sks, superman&#34;, with pop-art style LoRA model.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Main Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fine-tune Stable diffusion models twice as faster than dreambooth method, by Low-rank Adaptation&lt;/li&gt; &#xA; &lt;li&gt;Get insanely small end result (3MB for just unet, 6MB for both unet + clip), easy to share and download.&lt;/li&gt; &#xA; &lt;li&gt;Easy to use, compatible with &lt;code&gt;diffusers&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Sometimes &lt;em&gt;even better performance&lt;/em&gt; than full fine-tuning (but left as future work for extensive comparisons)&lt;/li&gt; &#xA; &lt;li&gt;Merge checkpoints + Build recipes by merging LoRAs together&lt;/li&gt; &#xA; &lt;li&gt;Fine-tune both CLIP &amp;amp; Unet to gain better results.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Web Demo&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Integrated into &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces &lt;/a&gt; using &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;. Try out the Web Demo &lt;a href=&#34;https://huggingface.co/spaces/ysharma/Low-rank-Adaptation&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Easy &lt;a href=&#34;https://colab.research.google.com/drive/1iSFDpRBKEWr2HLlz243rbym3J2X95kcy?usp=sharing&#34;&gt;colab running example&lt;/a&gt; of Dreambooth by @pedrogengo&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;UPDATES &amp;amp; Notes&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;You can now fine-tune text_encoder as well! Enabled with simple &lt;code&gt;--train_text_encoder&lt;/code&gt;&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Converting to CKPT format for A1111&#39;s repo consumption!&lt;/strong&gt; (Thanks to &lt;a href=&#34;https://github.com/jachiam&#34;&gt;jachiam&lt;/a&gt;&#39;s conversion script)&lt;/li&gt; &#xA; &lt;li&gt;Img2Img Examples added.&lt;/li&gt; &#xA; &lt;li&gt;Please use large learning rate! Around 1e-4 worked well for me, but certainly not around 1e-6 which will not be able to learn anything.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Lengthy Introduction&lt;/h1&gt; &#xA;&lt;p&gt;Thanks to the generous work of Stability AI and Huggingface, so many people have enjoyed fine-tuning stable diffusion models to fit their needs and generate higher fidelity images. &lt;strong&gt;However, the fine-tuning process is very slow, and it is not easy to find a good balance between the number of steps and the quality of the results.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Also, the final results (fully fined-tuned model) is very large. Some people instead works with textual-inversion as an alternative for this. But clearly this is suboptimal: textual inversion only creates a small word-embedding, and the final image is not as good as a fully fine-tuned model.&lt;/p&gt; &#xA;&lt;p&gt;Well, what&#39;s the alternative? In the domain of LLM, researchers have developed Efficient fine-tuning methods. LoRA, especially, tackles the very problem the community currently has: end users with Open-sourced stable-diffusion model want to try various other fine-tuned model that is created by the community, but the model is too large to download and use. LoRA instead attempts to fine-tune the &#34;residual&#34; of the model instead of the entire model: i.e., train the $\Delta W$ instead of $W$.&lt;/p&gt; &#xA;&lt;p&gt;$$ W&#39; = W + \Delta W $$&lt;/p&gt; &#xA;&lt;p&gt;Where we can further decompose $\Delta W$ into low-rank matrices : $\Delta W = A B^T $, where $A, \in \mathbb{R}^{n \times d}, B \in \mathbb{R}^{m \times d}, d &amp;lt;&amp;lt; n$. This is the key idea of LoRA. We can then fine-tune $A$ and $B$ instead of $W$. In the end, you get an insanely small model as $A$ and $B$ are much smaller than $W$.&lt;/p&gt; &#xA;&lt;p&gt;Also, not all of the parameters need tuning: they found that often, $Q, K, V, O$ (i.e., attention layer) of the transformer model is enough to tune. (This is also the reason why the end result is so small). This repo will follow the same idea.&lt;/p&gt; &#xA;&lt;p&gt;Enough of the lengthy introduction, let&#39;s get to the code.&lt;/p&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install git+https://github.com/cloneofsimo/lora.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Getting Started&lt;/h1&gt; &#xA;&lt;h2&gt;Fine-tuning Stable diffusion with LoRA.&lt;/h2&gt; &#xA;&lt;p&gt;Basic usage is as follows: prepare sets of $A, B$ matrices in an unet model, and fine-tune them.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from lora_diffusion import inject_trainable_lora, extract_lora_up_downs&#xA;&#xA;...&#xA;&#xA;unet = UNet2DConditionModel.from_pretrained(&#xA;    pretrained_model_name_or_path,&#xA;    subfolder=&#34;unet&#34;,&#xA;)&#xA;unet.requires_grad_(False)&#xA;unet_lora_params, train_names = inject_trainable_lora(unet)  # This will&#xA;# turn off all of the gradients of unet, except for the trainable LoRA params.&#xA;optimizer = optim.Adam(&#xA;    itertools.chain(*unet_lora_params, text_encoder.parameters()), lr=1e-4&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;An example of this can be found in &lt;code&gt;train_lora_dreambooth.py&lt;/code&gt;. Run this example with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;run_lora_db.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Another dreambooth example, with text_encoder training on can be run with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;run_lora_db_w_text.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Loading, merging, and interpolating trained LORAs with CLIs.&lt;/h2&gt; &#xA;&lt;p&gt;We&#39;ve seen that people have been merging different checkpoints with different ratios, and this seems to be very useful to the community. LoRA is extremely easy to merge.&lt;/p&gt; &#xA;&lt;p&gt;By the nature of LoRA, one can interpolate between different fine-tuned models by adding different $A, B$ matrices.&lt;/p&gt; &#xA;&lt;p&gt;Currently, LoRA cli has three options : merge full model with LoRA, merge LoRA with LoRA, or merge full model with LoRA and changes to &lt;code&gt;ckpt&lt;/code&gt; format (original format)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;SYNOPSIS&#xA;    lora_add PATH_1 PATH_2 OUTPUT_PATH &amp;lt;flags&amp;gt;&#xA;&#xA;POSITIONAL ARGUMENTS&#xA;    PATH_1&#xA;        Type: str&#xA;    PATH_2&#xA;        Type: str&#xA;    OUTPUT_PATH&#xA;        Type: str&#xA;&#xA;FLAGS&#xA;    --alpha&#xA;        Type: float&#xA;        Default: 0.5&#xA;    --mode&#xA;        Type: Literal[&#39;upl&#39;, &#39;lpl&#39;, &#39;upl&#39;, &#39;upl-ckpt-v2&#39;]&#xA;        Default: &#39;lpl&#39;&#xA;    --with_text_lora&#xA;        Type: bool&#xA;        Default: False&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Merging full model with LoRA&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ lora_add --path_1 PATH_TO_DIFFUSER_FORMAT_MODEL --path_2 PATH_TO_LORA.PT --mode upl --alpha 1.0 --output_path OUTPUT_PATH&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;path_1&lt;/code&gt; can be both local path or huggingface model name. When adding LoRA to unet, alpha is the constant as below:&lt;/p&gt; &#xA;&lt;p&gt;$$ W&#39; = W + \alpha \Delta W $$&lt;/p&gt; &#xA;&lt;p&gt;So, set alpha to 1.0 to fully add LoRA. If the LoRA seems to have too much effect (i.e., overfitted), set alpha to lower value. If the LoRA seems to have too little effect, set alpha to higher than 1.0. You can tune these values to your needs. This value can be even slightly greater than 1.0!&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ lora_add --path_1 stabilityai/stable-diffusion-2-base --path_2 lora_illust.pt --mode upl --alpha 1.0 --output_path merged_model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Mergigng Full model with LoRA and changing to original CKPT format&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;TESTED WITH V2, V2.1 ONLY!&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Everything same as above, but with mode &lt;code&gt;upl-ckpt-v2&lt;/code&gt; instead of &lt;code&gt;upl&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ lora_add --path_1 stabilityai/stable-diffusion-2-base --path_2 lora_illust.pt --mode upl-ckpt-v2 --alpha 1.2 --output_path merged_model.ckpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Merging LoRA with LoRA&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ lora_add --path_1 PATH_TO_LORA.PT --path_2 PATH_TO_LORA.PT --mode lpl --alpha 0.5 --output_path OUTPUT_PATH.PT&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;alpha is the ratio of the first model to the second model. i.e.,&lt;/p&gt; &#xA;&lt;p&gt;$$ \Delta W = (\alpha A_1 + (1 - \alpha) A_2) (\alpha B_1 + (1 - \alpha) B_2)^T $$&lt;/p&gt; &#xA;&lt;p&gt;Set alpha to 0.5 to get the average of the two models. Set alpha close to 1.0 to get more effect of the first model, and set alpha close to 0.0 to get more effect of the second model.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ lora_add --path_1 lora_illust.pt --path_2 lora_pop.pt --alpha 0.3 --output_path lora_merged.pt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;More bash examples with Text Encoder Lora:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ lora_add --path_1 stabilityai/stable-diffusion-2-base --path_2 lora_kiriko.pt --mode upl-ckpt-v2 --alpha 1.2 --with_text_lora --output_path merged_model.ckpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;: This will build a &lt;code&gt;merged_model.ckpt&lt;/code&gt; with LoRA merged with $\alpha=1.2$ and text encoder LoRA.&lt;/p&gt; &#xA;&lt;h3&gt;Making Text2Img Inference with trained LoRA&lt;/h3&gt; &#xA;&lt;p&gt;Checkout &lt;code&gt;scripts/run_inference.ipynb&lt;/code&gt; for an example of how to make inference with LoRA.&lt;/p&gt; &#xA;&lt;h3&gt;Making Img2Img Inference with LoRA&lt;/h3&gt; &#xA;&lt;p&gt;Checkout &lt;code&gt;scripts/run_img2img.ipynb&lt;/code&gt; for an example of how to make inference with LoRA.&lt;/p&gt; &#xA;&lt;h3&gt;Merging Lora with Lora, and making inference dynamically using &lt;code&gt;monkeypatch_add_lora&lt;/code&gt;.&lt;/h3&gt; &#xA;&lt;p&gt;Checkout &lt;code&gt;scripts/merge_lora_with_lora.ipynb&lt;/code&gt; for an example of how to merge Lora with Lora, and make inference dynamically using &lt;code&gt;monkeypatch_add_lora&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;!-- #region --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/cloneofsimo/lora/master/contents/lora_with_clip_and_illust.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;!-- #endregion --&gt; &#xA;&lt;p&gt;Above results are from merging &lt;code&gt;lora_illust.pt&lt;/code&gt; with &lt;code&gt;lora_kiriko.pt&lt;/code&gt; with both 1.0 as weights and 0.5 as $\alpha$.&lt;/p&gt; &#xA;&lt;p&gt;$$ W_{unet} \leftarrow W_{unet} + 0.5 (A_{kiriko} + A_{illust})(B_{kiriko} + B_{illust})^T $$&lt;/p&gt; &#xA;&lt;p&gt;and&lt;/p&gt; &#xA;&lt;p&gt;$$ W_{clip} \leftarrow W_{clip} + 0.5 A_{kiriko}B_{kiriko}^T $$&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Tips and Discussions&lt;/h1&gt; &#xA;&lt;h2&gt;&lt;strong&gt;Training tips in general&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;p&gt;I&#39;m curating a list of tips and discussions here. Feel free to add your own tips and discussions with a PR!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Discussion by @nitrosocke, can be found &lt;a href=&#34;https://github.com/cloneofsimo/lora/issues/19#issuecomment-1347149627&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Configurations by @xsteenbrugge, Using Clip-interrogator to get a decent prompt seems to work well for him, &lt;a href=&#34;https://twitter.com/xsteenbrugge/status/1602799180698763264&#34;&gt;https://twitter.com/xsteenbrugge/status/1602799180698763264&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Super easy &lt;a href=&#34;https://colab.research.google.com/drive/1iSFDpRBKEWr2HLlz243rbym3J2X95kcy?usp=sharing&#34;&gt;colab running example&lt;/a&gt; of Dreambooth by @pedrogengo&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/cloneofsimo/lora/discussions/37&#34;&gt;Amazing in-depth analysis&lt;/a&gt; on the effect of rank, $\alpha_{unet}$, $\alpha_{clip}$, and training configurations from brian6091!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;&lt;strong&gt;How long should you train?&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Effect of fine tuning (both Unet + CLIP) can be seen in the following image, where each image is another 500 steps. Trained with 9 images, with lr of &lt;code&gt;1e-4&lt;/code&gt; for unet, and &lt;code&gt;5e-5&lt;/code&gt; for CLIP. (You can adjust this with &lt;code&gt;--learning_rate=1e-4&lt;/code&gt; and &lt;code&gt;--learning_rate_text=5e-5&lt;/code&gt;)&lt;/p&gt; &#xA;&lt;!-- #region --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/cloneofsimo/lora/master/contents/lora_with_clip_4x4_training_progress.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;!-- #endregion --&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&#34;female game character bnha, in a steampunk city, 4K render, trending on artstation, masterpiece&#34;. Visualization notebook can be found at scripts/lora_training_process_visualized.ipynb&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;You can see that with 2500 steps, you already get somewhat good results.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;strong&gt;What is a good learning rate for LoRA?&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;People using dreambooth are used to using lr around &lt;code&gt;1e-6&lt;/code&gt;, but this is way too small for training LoRAs. &lt;strong&gt;I&#39;ve tried using 1e-4, and it is OK&lt;/strong&gt;. I think these values should be more explored statistically.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;strong&gt;What happens to Text Encoder LoRA and Unet LoRA?&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Let&#39;s see: the following is only using Unet LoRA:&lt;/p&gt; &#xA;&lt;!-- #region --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/cloneofsimo/lora/master/contents/lora_just_unet.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;!-- #endregion --&gt; &#xA;&lt;p&gt;And the following is only using Text Encoder LoRA:&lt;/p&gt; &#xA;&lt;!-- #region --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/cloneofsimo/lora/master/contents/lora_just_text_encoder.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;!-- #endregion --&gt; &#xA;&lt;p&gt;So they learnt different aspect of the dataset, but they are not mutually exclusive. You can use both of them to get better results, and tune them seperately to get even better results.&lt;/p&gt; &#xA;&lt;p&gt;With LoRA Text Encoder, Unet, all the schedulers, guidance scale, negative prompt etc. etc., you have so much to play around with to get the best result you want. For example, with $\alpha_{unet} = 0.6$, $\alpha_{text} = 0.9$, you get a better result compared to $\alpha_{unet} = 1.0$, $\alpha_{text} = 1.0$ (default). Checkout below:&lt;/p&gt; &#xA;&lt;!-- #region --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/cloneofsimo/lora/master/contents/lora_some_tweaks.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;!-- #endregion --&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Left with tuned $\alpha_{unet} = 0.6$, $\alpha_{text} = 0.9$, right with $\alpha_{unet} = 1.0$, $\alpha_{text} = 1.0$.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Here is an extensive visualization on the effect of $\alpha_{unet}$, $\alpha_{text}$, by @brian6091 from &lt;a href=&#34;https://github.com/cloneofsimo/lora/discussions/37&#34;&gt;his analysis &lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- #region --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/cloneofsimo/lora/master/contents/comp_scale_clip_unet.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;!-- #endregion --&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&#34;a photo of (S*)&#34;, trained with 21 images, with rank 16 LoRA. More details can be found &lt;a href=&#34;https://github.com/cloneofsimo/lora/discussions/37&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;TODOS&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Make this more user friendly for non-programmers&lt;/li&gt; &#xA; &lt;li&gt;Make a better CLI&lt;/li&gt; &#xA; &lt;li&gt;Make a better documentation&lt;/li&gt; &#xA; &lt;li&gt;Kronecker product, like LoRA [https://arxiv.org/abs/2106.04647]&lt;/li&gt; &#xA; &lt;li&gt;Adaptor-guidance&lt;/li&gt; &#xA; &lt;li&gt;Time-aware fine-tuning.&lt;/li&gt; &#xA; &lt;li&gt;Test alpha scheduling. I think it will be meaningful.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>keras-team/keras-io</title>
    <updated>2022-12-18T01:46:47Z</updated>
    <id>tag:github.com,2022-12-18:/keras-team/keras-io</id>
    <link href="https://github.com/keras-team/keras-io" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Keras documentation, hosted live at keras.io&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Keras.io documentation generator&lt;/h1&gt; &#xA;&lt;p&gt;This repository hosts the code used to generate the &lt;a href=&#34;https://keras.io&#34;&gt;keras.io&lt;/a&gt; website.&lt;/p&gt; &#xA;&lt;h2&gt;Generating a local copy of the website&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;cd scripts&#xA;python autogen.py make&#xA;python autogen.py serve&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you have Docker (you don&#39;t need the gpu version of Docker), you can run instead:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker build -t keras-io . &amp;amp;&amp;amp; docker run --rm -p 8000:8000 keras-io&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It will take a while the first time because it&#39;s going to pull the image and the dependencies, but on the next times it&#39;ll be much faster.&lt;/p&gt; &#xA;&lt;p&gt;Another way of testing using Docker is via our Makefile:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make container-test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This command will build a Docker image with a documentation server and run it.&lt;/p&gt; &#xA;&lt;h2&gt;Call for examples&lt;/h2&gt; &#xA;&lt;p&gt;Are you interested in submitting new examples for publication on keras.io? We welcome your contributions! Please read the information below about adding new code examples.&lt;/p&gt; &#xA;&lt;p&gt;We are currently interested in &lt;a href=&#34;https://github.com/keras-team/keras-io/raw/master/call_for_contributions.md&#34;&gt;the following examples&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Adding a new code example&lt;/h2&gt; &#xA;&lt;p&gt;Keras code examples are implemented as &lt;strong&gt;tutobooks&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;A tutobook is a script available simultaneously as a notebook, as a Python file, and as a nicely-rendered webpage.&lt;/p&gt; &#xA;&lt;p&gt;Its source-of-truth (for manual edition and version control) is its Python script form, but you can also create one by starting from a notebook and converting it with the command &lt;code&gt;nb2py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Text cells are stored in markdown-formatted comment blocks. the first line (starting with &lt;code&gt;&#34;&#34;&#34;&lt;/code&gt;) may optionally contain a special annotation, one of:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;shell&lt;/code&gt;: execute this block while prefixing each line with &lt;code&gt;!&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;invisible&lt;/code&gt;: do not render this block.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The script form should start with a header with the following fields:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Title: (title)&#xA;Author: (could be `Authors`: as well, and may contain markdown links)&#xA;Date created: (date in yyyy/mm/dd format)&#xA;Last modified: (date in yyyy/mm/dd format)&#xA;Description: (one-line text description)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To see examples of tutobooks, you can check out any &lt;code&gt;.py&lt;/code&gt; file in &lt;code&gt;examples/&lt;/code&gt; or &lt;code&gt;guides/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Creating a new example starting from a &lt;code&gt;ipynb&lt;/code&gt; file&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Save the &lt;code&gt;ipynb&lt;/code&gt; file to local disk.&lt;/li&gt; &#xA; &lt;li&gt;Convert the file to a tutobook by running: (assuming you are in the &lt;code&gt;scripts/&lt;/code&gt; directory)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python tutobooks.py nb2py path_to_your_nb.ipynb ../examples/vision/script_name.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will create the file &lt;code&gt;examples/vision/script_name.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Open it, fill in the headers, and generally edit it so that it looks nice.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;NOTE THAT THE CONVERSION SCRIPT MAY MAKE MISTAKES IN ITS ATTEMPTS TO SHORTEN LINES. MAKE SURE TO PROOFREAD THE GENERATED .py IN FULL. Or alternatively, make sure to keep your lines reasonably-sized (&amp;lt;90 char) to start with, so that the script won&#39;t have to shorten them.&lt;/p&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Run &lt;code&gt;python autogen.py add_example vision/script_name&lt;/code&gt;. This will generate an ipynb and markdown rendering of your example, creating files in &lt;code&gt;examples/vision/ipynb&lt;/code&gt;, &lt;code&gt;examples/vision/md&lt;/code&gt;, and &lt;code&gt;examples/vision/img&lt;/code&gt;. Do not modify any of these files by hand; only the original Python script should ever be edited manually.&lt;/li&gt; &#xA; &lt;li&gt;Submit a PR adding &lt;code&gt;examples/vision/script_name.py&lt;/code&gt; (only the &lt;code&gt;.py&lt;/code&gt;, not the generated files). Get a review and approval.&lt;/li&gt; &#xA; &lt;li&gt;Once the PR is approved, add to the PR the files created by the &lt;code&gt;add_example&lt;/code&gt; command. Then we will merge the PR.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Creating a new example starting from a Python script&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Format the script with &lt;code&gt;black&lt;/code&gt;: &lt;code&gt;black script_name.py&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Add tutobook header&lt;/li&gt; &#xA; &lt;li&gt;Put the script in the relevant subfolder of &lt;code&gt;examples/&lt;/code&gt; (e.g. &lt;code&gt;examples/vision/script_name&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;python autogen.py add_example vision/script_name&lt;/code&gt;. This will generate an ipynb and markdown rendering of your example, creating files in &lt;code&gt;examples/vision/ipynb&lt;/code&gt;, &lt;code&gt;examples/vision/md&lt;/code&gt;, and &lt;code&gt;examples/vision/img&lt;/code&gt;. Do not modify any of these files by hand; only the original Python script should ever be edited manually.&lt;/li&gt; &#xA; &lt;li&gt;Submit a PR adding &lt;code&gt;examples/vision/script_name.py&lt;/code&gt; (only the &lt;code&gt;.py&lt;/code&gt;, not the generated files). Get a review and approval.&lt;/li&gt; &#xA; &lt;li&gt;Once the PR is approved, add to the PR the files created by the &lt;code&gt;add_example&lt;/code&gt; command. Then we will merge the PR.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Previewing a new example&lt;/h3&gt; &#xA;&lt;p&gt;You can locally preview what the example looks like by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd scripts&#xA;python autogen.py add_example vision/script_name&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(Assuming the tutobook file is &lt;code&gt;examples/vision/script_name.py&lt;/code&gt;.)&lt;/p&gt; &#xA;&lt;p&gt;NOTE THAT THIS COMMAND WILL ERROR OUT IF ANY CELLS TAKES TOO LONG TO EXECUTE. In that case, make your code lighter/faster. Remember that examples are meant to demonstrate workflows, not train state-of-the-art models. They should stay very lightweight.&lt;/p&gt; &#xA;&lt;p&gt;Then serving the website:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python autogen.py make&#xA;python autogen.py serve&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And navigating to &lt;code&gt;0.0.0.0:8000/examples&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Read-only autogenerated files&lt;/h2&gt; &#xA;&lt;p&gt;The contents of the following folders should &lt;strong&gt;not&lt;/strong&gt; be modified by hand:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;site/*&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;sources/*&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;templates/examples/*&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;templates/guides/*&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;examples/*/md/*&lt;/code&gt;, &lt;code&gt;examples/*/ipynb/*&lt;/code&gt;, &lt;code&gt;examples/*/img/*&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;guides/md/*&lt;/code&gt;, &lt;code&gt;guides/ipynb/*&lt;/code&gt;, &lt;code&gt;guides/img/*&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Modifiable files&lt;/h2&gt; &#xA;&lt;p&gt;These are the only files that should be edited by hand:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;templates/*.md&lt;/code&gt;, with the exception of &lt;code&gt;templates/examples/*&lt;/code&gt; and &lt;code&gt;templates/guides/*&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;examples/*/*.py&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;guides/*.py&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;theme/*&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;scripts/*.py&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>mustafamuratcoskun/Sifirdan-Ileri-Seviyeye-Python-Programlama</title>
    <updated>2022-12-18T01:46:47Z</updated>
    <id>tag:github.com,2022-12-18:/mustafamuratcoskun/Sifirdan-Ileri-Seviyeye-Python-Programlama</id>
    <link href="https://github.com/mustafamuratcoskun/Sifirdan-Ileri-Seviyeye-Python-Programlama" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Udemy 羹zerindeki Python kurslar覺nda kullan覺lan materyaller&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;S覺f覺rdan 襤leri Seviyeye Python Programlama&lt;/h1&gt; &#xA;&lt;p&gt;Udemy 羹zerindeki &lt;a href=&#34;https://www.udemy.com/course/sifirdan-ileri-seviyeye-python/?couponCode=PYTHON6&#34;&gt;S覺f覺rdan 襤leri Seviyeye Python Programlama&lt;/a&gt; kursundaki al覺ma Notebooklar覺 ve Kullan覺lan Kodlar&lt;/p&gt;</summary>
  </entry>
</feed>