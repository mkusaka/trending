<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-05T01:38:37Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>vikhyat/moondream</title>
    <updated>2024-05-05T01:38:37Z</updated>
    <id>tag:github.com,2024-05-05:/vikhyat/moondream</id>
    <link href="https://github.com/vikhyat/moondream" rel="alternate"></link>
    <summary type="html">&lt;p&gt;tiny vision language model&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;🌔 moondream&lt;/h1&gt; &#xA;&lt;p&gt;a tiny vision language model that kicks ass and runs anywhere&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://moondream.ai/&#34;&gt;Website&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/vikhyatk/moondream2&#34;&gt;Hugging Face&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/spaces/vikhyatk/moondream2&#34;&gt;Demo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Benchmarks&lt;/h2&gt; &#xA;&lt;p&gt;moondream2 is a 1.86B parameter model initialized with weights from &lt;a href=&#34;https://huggingface.co/timm/ViT-SO400M-14-SigLIP-384&#34;&gt;SigLIP&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/microsoft/phi-1_5&#34;&gt;Phi 1.5&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;VQAv2&lt;/th&gt; &#xA;   &lt;th&gt;GQA&lt;/th&gt; &#xA;   &lt;th&gt;TextVQA&lt;/th&gt; &#xA;   &lt;th&gt;TallyQA (simple)&lt;/th&gt; &#xA;   &lt;th&gt;TallyQA (full)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;moondream1&lt;/td&gt; &#xA;   &lt;td&gt;74.7&lt;/td&gt; &#xA;   &lt;td&gt;57.9&lt;/td&gt; &#xA;   &lt;td&gt;35.6&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;moondream2&lt;/strong&gt; (latest)&lt;/td&gt; &#xA;   &lt;td&gt;77.7&lt;/td&gt; &#xA;   &lt;td&gt;61.7&lt;/td&gt; &#xA;   &lt;td&gt;49.7&lt;/td&gt; &#xA;   &lt;td&gt;80.1&lt;/td&gt; &#xA;   &lt;td&gt;74.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Image&lt;/th&gt; &#xA;   &lt;th&gt;Example&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/vikhyat/moondream/main/assets/demo-1.jpg&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;What is the girl doing?&lt;/strong&gt;&lt;br&gt;The girl is sitting at a table and eating a large hamburger.&lt;br&gt;&lt;br&gt;&lt;strong&gt;What color is the girl&#39;s hair?&lt;/strong&gt;&lt;br&gt;The girl&#39;s hair is white.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/vikhyat/moondream/main/assets/demo-2.jpg&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;What is this?&lt;/strong&gt;&lt;br&gt;The image features a computer server rack, which is a large metal structure designed to hold and organize multiple computer components, such as motherboards, cooling systems, and other peripherals. The rack is filled with various computer parts, including multiple computer chips, wires, and other electronic components. The rack is placed on a carpeted floor, and there is a couch in the background, suggesting that the setup is likely in a living or working space.&lt;br&gt;&lt;br&gt;&lt;strong&gt;What is behind the stand?&lt;/strong&gt;&lt;br&gt;There is a brick wall behind the stand.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Using transformers&lt;/strong&gt; (recommended)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install transformers einops&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoModelForCausalLM, AutoTokenizer&#xA;from PIL import Image&#xA;&#xA;model_id = &#34;vikhyatk/moondream2&#34;&#xA;revision = &#34;2024-04-02&#34;&#xA;model = AutoModelForCausalLM.from_pretrained(&#xA;    model_id, trust_remote_code=True, revision=revision&#xA;)&#xA;tokenizer = AutoTokenizer.from_pretrained(model_id, revision=revision)&#xA;&#xA;image = Image.open(&#39;&amp;lt;IMAGE_PATH&amp;gt;&#39;)&#xA;enc_image = model.encode_image(image)&#xA;print(model.answer_question(enc_image, &#34;Describe this image.&#34;, tokenizer))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The model is updated regularly, so we recommend pinning the model version to a specific release as shown above.&lt;/p&gt; &#xA;&lt;p&gt;To enable Flash Attention on the text model, pass in &lt;code&gt;attn_implementation=&#34;flash_attention_2&#34;&lt;/code&gt; when instantiating the model.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = AutoModelForCausalLM.from_pretrained(&#xA;    model_id, trust_remote_code=True, revision=revision,&#xA;    torch_dtype=torch.float16, attn_implementation=&#34;flash_attention_2&#34;&#xA;).to(&#34;cuda&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Batch inference is also supported.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;answers = moondream.batch_answer(&#xA;    images=[Image.open(&#39;&amp;lt;IMAGE_PATH_1&amp;gt;&#39;), Image.open(&#39;&amp;lt;IMAGE_PATH_2&amp;gt;&#39;)],&#xA;    prompts=[&#34;Describe this image.&#34;, &#34;Are there people in this image?&#34;],&#xA;    tokenizer=tokenizer,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Using this repository&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Clone this repository and install dependencies.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;sample.py&lt;/code&gt; provides a CLI interface for running the model. When the &lt;code&gt;--prompt&lt;/code&gt; argument is not provided, the script will allow you to ask questions interactively.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python sample.py --image [IMAGE_PATH] --prompt [PROMPT]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Use &lt;code&gt;gradio_demo.py&lt;/code&gt; script to start a Gradio interface for the model.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python gradio_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;webcam_gradio_demo.py&lt;/code&gt; provides a Gradio interface for the model that uses your webcam as input and performs inference in real-time.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python webcam_gradio_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Limitations&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The model may generate inaccurate statements, and struggle to understand intricate or nuanced instructions.&lt;/li&gt; &#xA; &lt;li&gt;The model may not be free from societal biases. Users should be aware of this and exercise caution and critical thinking when using the model.&lt;/li&gt; &#xA; &lt;li&gt;The model may generate offensive, inappropriate, or hurtful content if it is prompted to do so.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>aws-samples/amazon-bedrock-samples</title>
    <updated>2024-05-05T01:38:37Z</updated>
    <id>tag:github.com,2024-05-05:/aws-samples/amazon-bedrock-samples</id>
    <link href="https://github.com/aws-samples/amazon-bedrock-samples" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This repository contains examples for customers to get started using the Amazon Bedrock Service. This contains examples for all available foundational models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Amazon Bedrock Samples&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains pre-built examples to help customers get started with the Amazon Bedrock service.&lt;/p&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/amazon-bedrock-samples/main/introduction-to-bedrock&#34;&gt;Introduction to Bedrock&lt;/a&gt; - Learn the basics of the Bedrock service&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/amazon-bedrock-samples/main/prompt-engineering&#34;&gt;Prompt Engineering &lt;/a&gt; - Tips for crafting effective prompts&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/amazon-bedrock-samples/main/bedrock-fine-tuning&#34;&gt;Bedrock Fine-tuning&lt;/a&gt; - Fine-tune Bedrock models for your specific use case&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/amazon-bedrock-samples/main/generative-ai-solutions&#34;&gt;Generative AI Solutions&lt;/a&gt; - Example use cases for generative AI&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/amazon-bedrock-samples/main/knowledge-bases&#34;&gt;Knowledge Bases&lt;/a&gt; - Build knowledge bases with Bedrock&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/amazon-bedrock-samples/main/rag-solutions&#34;&gt;Retrival Augmented Generation (RAG)&lt;/a&gt; - Implementing RAG with Amazon Bedrock&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/amazon-bedrock-samples/main/agents&#34;&gt;Agents&lt;/a&gt; - Generative AI agents with Bedrock&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/amazon-bedrock-samples/main/security-and-governance&#34;&gt;Security and Governance&lt;/a&gt; - Secure your Bedrock applications&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/amazon-bedrock-samples/main/responsible-ai&#34;&gt;Responsible AI&lt;/a&gt; - Use Bedrock responsibly and ethically&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/amazon-bedrock-samples/main/ops-tooling&#34;&gt;Operational Tooling&lt;/a&gt; - Helpful samples to help operationalize your useage of Amazon Bedrock&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/amazon-bedrock-samples/main/multimodal&#34;&gt;Multimodal&lt;/a&gt; - Working with multimodal data using Amazon Bedrock&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;To get started with the code examples, ensure you have access to &lt;a href=&#34;https://aws.amazon.com/bedrock/&#34;&gt;Amazon Bedrock&lt;/a&gt;. Then clone this repo and navigate to one of the folders above. Detailed instructions are provided in each folder&#39;s README.&lt;/p&gt; &#xA;&lt;h3&gt;Enable AWS IAM permissions for Bedrock&lt;/h3&gt; &#xA;&lt;p&gt;The AWS identity you assume from your environment (which is the &lt;a href=&#34;https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html&#34;&gt;&lt;em&gt;Studio/notebook Execution Role&lt;/em&gt;&lt;/a&gt; from SageMaker, or could be a role or IAM User for self-managed notebooks or other use-cases), must have sufficient &lt;a href=&#34;https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html&#34;&gt;AWS IAM permissions&lt;/a&gt; to call the Amazon Bedrock service.&lt;/p&gt; &#xA;&lt;p&gt;To grant Bedrock access to your identity, you can:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Open the &lt;a href=&#34;https://us-east-1.console.aws.amazon.com/iam/home?#&#34;&gt;AWS IAM Console&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Find your &lt;a href=&#34;https://us-east-1.console.aws.amazon.com/iamv2/home?#/roles&#34;&gt;Role&lt;/a&gt; (if using SageMaker or otherwise assuming an IAM Role), or else &lt;a href=&#34;https://us-east-1.console.aws.amazon.com/iamv2/home?#/users&#34;&gt;User&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Select &lt;em&gt;Add Permissions &amp;gt; Create Inline Policy&lt;/em&gt; to attach new inline permissions, open the &lt;em&gt;JSON&lt;/em&gt; editor and paste in the below example policy:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;{&#xA;    &#34;Version&#34;: &#34;2012-10-17&#34;,&#xA;    &#34;Statement&#34;: [&#xA;        {&#xA;            &#34;Sid&#34;: &#34;BedrockFullAccess&#34;,&#xA;            &#34;Effect&#34;: &#34;Allow&#34;,&#xA;            &#34;Action&#34;: [&#34;bedrock:*&#34;],&#xA;            &#34;Resource&#34;: &#34;*&#34;&#xA;        }&#xA;    ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;⚠️ &lt;strong&gt;Note:&lt;/strong&gt; With Amazon SageMaker, your notebook execution role will typically be &lt;em&gt;separate&lt;/em&gt; from the user or role that you log in to the AWS Console with. If you&#39;d like to explore the AWS Console for Amazon Bedrock, you&#39;ll need to grant permissions to your Console user/role too.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;For more information on the fine-grained action and resource permissions in Bedrock, check out the Bedrock Developer Guide.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome community contributions! Please see &lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/amazon-bedrock-samples/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for guidelines.&lt;/p&gt; &#xA;&lt;h2&gt;Security&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/amazon-bedrock-samples/main/CONTRIBUTING.md#security-issue-notifications&#34;&gt;CONTRIBUTING&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This library is licensed under the MIT-0 License. See the &lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/amazon-bedrock-samples/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>krishnaik06/Updated-Langchain</title>
    <updated>2024-05-05T01:38:37Z</updated>
    <id>tag:github.com,2024-05-05:/krishnaik06/Updated-Langchain</id>
    <link href="https://github.com/krishnaik06/Updated-Langchain" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Updated-Langchain&lt;/h1&gt;</summary>
  </entry>
</feed>