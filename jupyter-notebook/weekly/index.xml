<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-12-31T01:51:48Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>opendilab/LMDrive</title>
    <updated>2024-12-31T01:51:48Z</updated>
    <id>tag:github.com,2024-12-31:/opendilab/LMDrive</id>
    <link href="https://github.com/opendilab/LMDrive" rel="alternate"></link>
    <summary type="html">&lt;p&gt;LMDrive: Closed-Loop End-to-End Driving with Large Language Models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LMDrive: Closed-Loop End-to-End Driving with Large Language Models&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;An end-to-end, closed-loop, language-based autonomous driving framework, which interacts with the dynamic environment via multi-modal multi-view sensor data and natural language instructions.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://hao-shao.com/projects/lmdrive.html&#34;&gt;Project Page&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/2312.07488&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/datasets/OpenDILabCommunity/LMDrive&#34;&gt;Dataset&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/opendilab/LMDrive/main/#lmdrive-weigths&#34;&gt;Model Zoo&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://hits.seeyoufarm.com&#34;&gt;&lt;img src=&#34;https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2Fopendilab%2FLMDrive&amp;amp;count_bg=%2379C83D&amp;amp;title_bg=%23555555&amp;amp;icon=&amp;amp;icon_color=%23E7E7E7&amp;amp;title=hits&amp;amp;edge_flat=false&#34; alt=&#34;Hits&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg?sanitize=true&#34; alt=&#34;Code License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca/raw/main/DATA_LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Data%20License-CC%20By%20NC%204.0-red.svg?sanitize=true&#34; alt=&#34;Data License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[12/21] We released our project website &lt;a href=&#34;https://hao-shao.com/projects/lmdrive.html&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img width=&#34;800&#34; src=&#34;https://raw.githubusercontent.com/opendilab/LMDrive/main/assets/pipeline.png&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;http://hao-shao.com/&#34;&gt;Hao Shao&lt;/a&gt;, Yuxuan Hu, &lt;a href=&#34;https://letianwang0.wixsite.com/myhome&#34;&gt;Letian Wang&lt;/a&gt;, &lt;a href=&#34;https://www.trailab.utias.utoronto.ca/stevenwaslander&#34;&gt;Steven L. Waslander&lt;/a&gt;, &lt;a href=&#34;https://liuyu.us/&#34;&gt;Yu Liu&lt;/a&gt;, &lt;a href=&#34;http://www.ee.cuhk.edu.hk/~hsli/&#34;&gt;Hongsheng Li&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;This repository contains code for the paper &lt;a href=&#34;https://arxiv.org/abs/2312.07488&#34;&gt;LMDrive: Closed-Loop End-to-End Driving with Large Language Models&lt;/a&gt;. This work proposes a novel language-guided, end-to-end, closed-loop autonomous driving framework.&lt;/p&gt; &#xA;&lt;h2&gt;Demo Video&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;video width=&#34;800&#34; src=&#34;https://github.com/opendilab/LMDrive/assets/17512647/65b2785d-e8bc-4ec1-ac86-e077299a465d&#34;&gt;&lt;/video&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/opendilab/LMDrive/main/#setup&#34;&gt;Setup&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/opendilab/LMDrive/main/#lmdrive-Weights&#34;&gt;Model Weights&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/opendilab/LMDrive/main/#dataset&#34;&gt;Dataset&lt;/a&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/opendilab/LMDrive/main/#overview&#34;&gt;Overview&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/opendilab/LMDrive/main/#data-generation&#34;&gt;Data Generation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/opendilab/LMDrive/main/#data-pre-procession&#34;&gt;Data Pre-procession&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/opendilab/LMDrive/main/#data-parsing&#34;&gt;Data Parsing&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/opendilab/LMDrive/main/#training&#34;&gt;Training&lt;/a&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/opendilab/LMDrive/main/#vision-encoder-pre-training&#34;&gt;Vision encoder pre-training&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/opendilab/LMDrive/main/#instruction-finetuning&#34;&gt;Instruction finetuning&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/opendilab/LMDrive/main/#evaluation&#34;&gt;Evaluation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/opendilab/LMDrive/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/opendilab/LMDrive/main/#acknowledgements&#34;&gt;Acknowledgements&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;Our project is built on three parts: (1) vision encoder (corresponding repo: timm); (2) vision LLM (corresponding repo: LAVIS); (3) data collection, agent controller (corresponding repo: InterFuser, Leaderboard, ScenarioRunner).&lt;/p&gt; &#xA;&lt;p&gt;Install anaconda&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;wget https://repo.anaconda.com/archive/Anaconda3-2020.11-Linux-x86_64.sh&#xA;bash Anaconda3-2020.11-Linux-x86_64.sh&#xA;source ~/.bashrc&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Clone the repo and build the environment&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;git clone https://github.com/opendilab/LMDrive.git&#xA;cd LMDrive&#xA;conda create -n lmdrive python=3.8&#xA;conda activate lmdrive&#xA;cd vision_encoder&#xA;python setup.py develop # if you have installed timm before, please uninstall it&#xA;cd ../LAVIS&#xA;python setup.py develop # if you have installed LAVIS before, please uninstall it&#xA;pip3 install -r requirements.txt&#xA;&#xA;pip install flash-attn --no-build-isolation # optional&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download and setup CARLA 0.9.10.1&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;chmod +x setup_carla.sh&#xA;./setup_carla.sh&#xA;pip install carla&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;LMDrive Weights&lt;/h2&gt; &#xA;&lt;p&gt;If you are interested in including any other details in Model Zoo, please open an issue :)&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Version&lt;/th&gt; &#xA;   &lt;th&gt;Size&lt;/th&gt; &#xA;   &lt;th&gt;Checkpoint&lt;/th&gt; &#xA;   &lt;th&gt;VisionEncoder&lt;/th&gt; &#xA;   &lt;th&gt;LLM-base&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;DS (LangAuto)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;DS (LangAuto-short)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LMDrive-1.0 (LLaVA-v1.5-7B)&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/OpenDILabCommunity/LMDrive-llava-v1.5-7b-v1.0&#34;&gt;LMDrive-llava-v1.5-7b-v1.0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/OpenDILabCommunity/LMDrive-vision-encoder-r50-v1.0&#34;&gt;R50&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/liuhaotian/llava-v1.5-7b&#34;&gt;LLaVA-v1.5-7B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LMDrive-1.0 (Vicuna-v1.5-7B)&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/OpenDILabCommunity/LMDrive-vicuna-v1.5-7b-v1.0&#34;&gt;LMDrive-vicuna-v1.5-7b-v1.0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/OpenDILabCommunity/LMDrive-vision-encoder-r50-v1.0&#34;&gt;R50&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/lmsys/vicuna-7b-v1.5-16k&#34;&gt;Vicuna-v1.5-7B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;45.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LMDrive-1.0 (LLaMA-7B)&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/OpenDILabCommunity/LMDrive-llama-7b-v1.0&#34;&gt;LMDrive-llama-7b-v1.0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/OpenDILabCommunity/LMDrive-vision-encoder-r50-v1.0&#34;&gt;R50&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/huggyllama/llama-7b&#34;&gt;LLaMA-7B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;em&gt;DS denotes the driving score&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Dataset&lt;/h2&gt; &#xA;&lt;p&gt;We aim to develop an intelligent driving agent that can generate driving actions based on three sources of input: 1) sensor data (multi-view camera and LiDAR), so that the agent can generate actions that are aware of and compliant with the current scene; 2) navigation instructions (e.g. lane changing, turning), so that the agent can drive to meet the requirement in natural language (instruction from humans or navigation software); and 3) human notice instruction, so that the agent can interact with humans and adapt to human&#39;s suggestions and preferences (e.g. pay attention to adversarial events, deal with long-tail events, etc).&lt;/p&gt; &#xA;&lt;p&gt;We provide a dataset with about 64K data clips, where each clip includes one navigation instruction, several notice instructions, a sequence of multi-modal multi-view sensor data, and control signals. The duration of the clip spans from 2 to 20 seconds. The dataset used in our paper can be downloaded &lt;a href=&#34;https://huggingface.co/datasets/OpenDILabCommunity/LMDrive&#34;&gt;here&lt;/a&gt;. If you want to create your own dataset, please follow the steps we&#39;ve outlined below.&lt;/p&gt; &#xA;&lt;h3&gt;Overview&lt;/h3&gt; &#xA;&lt;p&gt;The data is generated with &lt;code&gt;leaderboard/team_code/auto_pilot.py&lt;/code&gt; in 8 CARLA towns using the routes and scenarios files provided at &lt;code&gt;leaderboard/data&lt;/code&gt; on CARLA 0.9.10.1 . The dataset is collected at a high frequency (~10Hz).&lt;/p&gt; &#xA;&lt;p&gt;Once you have downloaded our dataset or collected your own dataset, it&#39;s necessary to systematically organize the data as follows. DATASET_ROOT is the root directory where your dataset is stored.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;‚îú‚îÄ‚îÄ $DATASET_ROOT&#xA;‚îÇ   ‚îî‚îÄ‚îÄ dataset_index.txt  # for vision encoder pretraining&#xA;‚îÇ   ‚îî‚îÄ‚îÄ navigation_instruction_list.txt  # for instruction finetuning&#xA;‚îÇ   ‚îî‚îÄ‚îÄ notice_instruction_list.json  # for instruction finetuning&#xA;‚îÇ   ‚îî‚îÄ‚îÄ routes_town06_long_w7_11_28_18_28_35  #  data folder&#xA;‚îÇ   ‚îî‚îÄ‚îÄ routes_town01_short_w2_11_16_08_27_10&#xA;‚îÇ   ‚îî‚îÄ‚îÄ routes_town02_short_w2_11_16_22_55_25&#xA;‚îÇ   ‚îî‚îÄ‚îÄ routes_town01_short_w2_11_16_11_44_08 &#xA;      ‚îú‚îÄ‚îÄ rgb_full&#xA;      ‚îú‚îÄ‚îÄ lidar&#xA;      ‚îî‚îÄ‚îÄ ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;navigation_instruction_list.txt&lt;/code&gt; and &lt;code&gt;notice_instruction_list.txt&lt;/code&gt; can be generated with our scripts by the data parsing &lt;a href=&#34;https://raw.githubusercontent.com/opendilab/LMDrive/main/#data-parsing&#34;&gt;scripts&lt;/a&gt;. Each subfolder in the dataset you&#39;ve collected should be structured as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;- routes_town(town_id)_{tiny,short,long}_w(weather_id)_timestamp: corresponding to different towns and routes files&#xA;    - routes_X: contains data for an individual route&#xA;        - rgb_full: a big multi-view camera image at 400x1200 resolution, which can be split into four images (left, center, right, rear)&#xA;        - lidar: 3d point cloud in .npy format. It only includes the LiDAR points captured in 1/20 second, covering 180 degrees of horizontal view. So if you want to utilize 360 degrees of view, you need to merge it with the data from lidar_odd.&#xA;        - lidar_odd: 3d point cloud in .npy format.&#xA;        - birdview: topdown segmentation images, LAV and LBC used this type of data for training&#xA;        - topdown: similar to birdview but it&#39;s captured by the down-facing camera&#xA;        - 3d_bbs: 3d bounding boxes for different agents&#xA;        - affordances: different types of affordances&#xA;        - actors_data: contains the positions, velocities and other metadata of surrounding vehicles and the traffic lights&#xA;        - measurements: contains ego agent&#39;s position, velocity, future waypoints, and other metadata&#xA;        - measurements_full: merges measurement and actors_data&#xA;        - measurements_all.json: merges the files in measurement_full into a single file&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;$DATASET_ROOT&lt;/code&gt; directory must contain a file named &lt;code&gt;dataset_index.txt&lt;/code&gt;, which can be generated by our data pre-processing &lt;a href=&#34;https://raw.githubusercontent.com/opendilab/LMDrive/main/#data-pre-procession&#34;&gt;script&lt;/a&gt;. It should list the training and evaluation data in the following format:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;relative_route_path_dir&amp;gt; &amp;lt;num_data_frames_in_this_dir&amp;gt;&#xA;routes_town06_long_w7_11_28_18_28_35/ 1062&#xA;routes_town01_short_w2_11_16_08_27_10/ 1785&#xA;routes_town01_short_w2_11_16_09_55_05/ 918&#xA;routes_town02_short_w2_11_16_22_55_25/ 134&#xA;routes_town01_short_w2_11_16_11_44_08/ 569&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here, &lt;code&gt;&amp;lt;relative_route_path_dir&amp;gt;&lt;/code&gt; should be a relative path to the &lt;code&gt;$DATASET_ROOT&lt;/code&gt;. The training code will concatenate the &lt;code&gt;$DATASET_ROOT&lt;/code&gt; and &lt;code&gt;&amp;lt;relative_route_path_dir&amp;gt;&lt;/code&gt; to create the full path for loading the data. In this format, 1062 represents the number of frames in the routes_town06_long_w7_11_28_18_28_35/rgb_full directory or routes_town06_long_w7_11_28_18_28_35/lidar, etc.&lt;/p&gt; &#xA;&lt;h3&gt;Data Generation&lt;/h3&gt; &#xA;&lt;h4&gt;Data Generation with multiple CARLA Servers&lt;/h4&gt; &#xA;&lt;p&gt;In addition to the dataset, we have also provided all the scripts used for generating data and these can be modified as required for different CARLA versions. The dataset is collected by a rule-based expert agent in different weathers and towns.&lt;/p&gt; &#xA;&lt;h5&gt;Running CARLA Servers&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Start 4 carla servers: ip [localhost], port [2000, 2002, 2004, 2006]. You can adjust the number of CARLA servers according to your situation and more servers can collect more data. If you use N servers to collect data, it means you have collected data N times on each route, except that the weather and traffic scenarios are random each time.&#xA;&#xA;cd carla&#xA;CUDA_VISIBLE_DEVICES=0 ./CarlaUE4.sh --world-port=2000 -opengl &amp;amp;&#xA;CUDA_VISIBLE_DEVICES=1 ./CarlaUE4.sh --world-port=2002 -opengl &amp;amp;&#xA;CUDA_VISIBLE_DEVICES=2 ./CarlaUE4.sh --world-port=2004 -opengl &amp;amp;&#xA;CUDA_VISIBLE_DEVICES=3 ./CarlaUE4.sh --world-port=2006 -opengl &amp;amp;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Instructions for setting up docker are available &lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker&#34;&gt;here&lt;/a&gt;. Pull the docker image of CARLA 0.9.10.1 &lt;code&gt;docker pull carlasim/carla:0.9.10.1&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Docker 18:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker run -it --rm -p 2000-2002:2000-2002 --runtime=nvidia -e NVIDIA_VISIBLE_DEVICES=0 carlasim/carla:0.9.10.1 ./CarlaUE4.sh --world-port=2000 -opengl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Docker 19:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;docker run -it --rm --net=host --gpus &#39;&#34;device=0&#34;&#39; carlasim/carla:0.9.10.1 ./CarlaUE4.sh --world-port=2000 -opengl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If the docker container doesn&#39;t start properly then add another environment variable &lt;code&gt;-e SDL_AUDIODRIVER=dsp&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h5&gt;Run the Autopilot&lt;/h5&gt; &#xA;&lt;p&gt;Generate scripts for collecting data in batches.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd dataset&#xA;python init_dir.py&#xA;cd ..&#xA;cd data_collection&#xA;python generate_yamls.py # You can modify FPS, waypoints distribution strength ...&#xA;&#xA;# If you do not use 4 servers, the following Python scripts are needed to modify&#xA;python generate_bashs.py&#xA;python generate_batch_collect.py &#xA;cd ..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run batch-run scripts of the town and route type that you need to collect.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash data_collection/batch_run/run_route_routes_town01_long.sh&#xA;bash data_collection/batch_run/run_route_routes_town01_short.sh&#xA;...&#xA;bash data_collection/batch_run/run_route_routes_town07_tiny.sh&#xA;...&#xA;bash data_collection/batch_run/run_route_routes_town10_tiny.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Our scripts will use a random weather condition for data collection&lt;/p&gt; &#xA;&lt;h5&gt;Data Generation with a single CARLA Server&lt;/h5&gt; &#xA;&lt;p&gt;With a single CARLA server, roll out the autopilot to start data generation.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;carla/CarlaUE4.sh --world-port=2000 -opengl&#xA;./leaderboard/scripts/run_evaluation.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The expert agent used for data generation is defined in &lt;code&gt;leaderboard/team_code/auto_pilot.py&lt;/code&gt;. Different variables which need to be set are specified in &lt;code&gt;leaderboard/scripts/run_evaluation.sh&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Data Pre-procession&lt;/h3&gt; &#xA;&lt;p&gt;We provide some Python scripts for pre-processing the collected data in &lt;code&gt;tools/data_preprocessing&lt;/code&gt;, some of them are optional. Please execute them &lt;strong&gt;in the order&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;python get_list_file.py $DATASET_ROOT&lt;/code&gt;: obtain the dataset_list.txt.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;python batch_merge_data.py $DATASET_ROOT&lt;/code&gt;: merge several scattered data files into one file to reduce IO time when training. &lt;strong&gt;[Optional]&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;python batch_rm_rgb_data.py $DATASET_ROOT&lt;/code&gt;: delete redundant files after we have merged them into new files. &lt;strong&gt;[Optional]&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;python batch_stat_blocked_data.py $DATASET_ROOT&lt;/code&gt;: find the frames that the ego-vehicle is blocked for a long time. By removing them, we can enhance data distribution and decrease the overall data size.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;python batch_rm_blocked_data.py $DATASET_ROOT&lt;/code&gt;: delete the blocked frames.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;python batch_recollect_data.py $DATASET_ROOT&lt;/code&gt;: since we have removed some frames, we need to reorganize them to ensure that the frame ids are continuous.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;python batch_merge_measurements.py $DATASET_ROOT&lt;/code&gt;: merge the measurement files from all frames in one route folder to reduce IO time&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Data Parsing&lt;/h3&gt; &#xA;&lt;p&gt;After collecting and pre-processing the data, we need to parse the navigation instructions and notice instructions data with some Python scripts in &lt;code&gt;tools/data_parsing&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The script for parsing navigation instructions:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 parse_instruction.py $DATSET_ROOT &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The parsed navigation clips will be saved in &lt;code&gt;$DATSET_ROOT/navigation_instruction_list.txt&lt;/code&gt;, under the root directory of the dataset.&lt;/p&gt; &#xA;&lt;p&gt;The script for parsing notice instructions:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 parse_notice.py $DATSET_ROOT &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The parsed notice clips will be saved in &lt;code&gt;$DATSET_ROOT/notice_instruction_list.txt&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The script for parsing misleading instructions:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 parse_misleading.py $DATSET_ROOT &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The parsed misleading clips will be saved in &lt;code&gt;$DATSET_ROOT/misleading_data.txt&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;LMDrive&#39;s training consists of two stages: 1) the vision encoder pre-training stage, to generate visual tokens from sensor inputs; and 2) the instruction-finetuning stage, to align the instruction/vision and control signal.&lt;/p&gt; &#xA;&lt;p&gt;LMDrive is trained on 8 A100 GPUs with 80GB memory (the first stage can be trained on GPUS with 32G memory). To train on fewer GPUs, you can reduce the &lt;code&gt;batch-size&lt;/code&gt; and the &lt;code&gt;learning-rate&lt;/code&gt; while maintaining their proportion. Please download the multi-modal dataset with instructions collected in the CARLA simulator we use in the paper &lt;a href=&#34;https://huggingface.co/datasets/OpenDILabCommunity/LMDrive&#34;&gt;here&lt;/a&gt;, if you do not collect the dataset by yourself.&lt;/p&gt; &#xA;&lt;h3&gt;Vision encoder pre-training&lt;/h3&gt; &#xA;&lt;p&gt;Pretrain takes around 2~3 days for the visual encoder on 8x A100 (80G). Once the training is completed, you can locate the checkpoint of the vision encoder in the &lt;code&gt;output/&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd vision_encoder&#xA;bash scripts/train.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Some options to note:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;GPU_NUM&lt;/code&gt;: the number of GPUs you want to use. By default, it is set to 8.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;DATASET_ROOT&lt;/code&gt;: the root directory for storing the dataset.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--model&lt;/code&gt;: the structure of visual model. You can choose memfuser_baseline_e1d3_r26 which replaces ResNet50 with ResNet26. It&#39;s also possible to create new model variants in &lt;code&gt;visual_encoder/timm/models/memfuser.py&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--train-towns/train-weathers&lt;/code&gt;: the data filter for the training dataset. Similarly, there are corresponding options, &lt;code&gt;val-towns/val-weathers&lt;/code&gt; to filter the validation dataset accordingly.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Instruction finetuning&lt;/h3&gt; &#xA;&lt;p&gt;Instruction finetuning takes around 2~3 days for the visual encoder on 8x A100 (80G). Once the training is completed, you can locate the checkpoint of the adapter and qformer in the &lt;code&gt;lavis/output/&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd LAVIS&#xA;bash run.sh 8 lavis/projects/lmdrive/notice_llava15_visual_encoder_r50_seq40.yaml # 8 is the GPU number&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Some options in the config.yaml to note:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;preception_model&lt;/code&gt;: the model architecture of the vision encoder.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;preception_model_ckpt&lt;/code&gt;: the checkpoint path of the vision encoder.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;llm_model&lt;/code&gt;: the checkpoint path of the llm (Vicuna/LLaVA).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;use_notice_prompt&lt;/code&gt;: whether to use notice instruction data when training.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;split_section_num_for_visual_encoder&lt;/code&gt;: the number of sections the frames are divided into during the forward encoding of visual features. Higher values can save more memory, and it needs to be a factor of &lt;code&gt;token_max_length&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;datasets:&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;storage&lt;/code&gt;: the root directory for storing the dataset.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;towns/weathers&lt;/code&gt;: the data filter for training/evaluating.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;token_max_length&lt;/code&gt;: the maximum number of frames, if the number of frames exceeds this value, they will be truncated.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;sample_interval&lt;/code&gt;: the interval at which frames are sampled.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;Start a CARLA server (described above) and run the required agent. The adequate routes and scenarios files are provided in &lt;code&gt;leaderboard/data&lt;/code&gt; and the required variables need to be set in &lt;code&gt;leaderboard/scripts/run_evaluation.sh&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Some options need to be updated in the &lt;code&gt;leaderboard/team_code/lmdrive_config.py&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;preception_model&lt;/code&gt;: the model architecture of the vision encoder.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;preception_model_ckpt&lt;/code&gt;: the checkpoint path of the vision encoder (obtained in the vision encoder pretraining stage).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;llm_model&lt;/code&gt;: the checkpoint path of the llm (LLaMA/Vicuna/LLaVA).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;lmdrive_ckpt&lt;/code&gt;: the checkpoint path of the lmdrive (obtained in the instruction finetuing stage).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Update &lt;code&gt;leaderboard/scripts/run_evaluation.sh&lt;/code&gt; to include the following code for evaluating the model on Town05 Long Benchmark.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export CARLA_ROOT=/path/to/carla/root&#xA;export TEAM_AGENT=leaderboard/team_code/lmdrive_agent.py&#xA;export TEAM_CONFIG=leaderboard/team_code/lmdrive_config.py&#xA;export CHECKPOINT_ENDPOINT=results/lmdrive_result.json&#xA;export SCENARIOS=leaderboard/data/official/all_towns_traffic_scenarios_public.json&#xA;export ROUTES=leaderboard/data/LangAuto/long.xml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=0 ./leaderboard/scripts/run_evaluation.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here, the &lt;code&gt;long.json&lt;/code&gt; and &lt;code&gt;long.xml&lt;/code&gt; files are replaced with &lt;code&gt;short.json&lt;/code&gt; and &lt;code&gt;short.xml&lt;/code&gt; for the evaluation of the agent in the LangAuto-Short benchmark.&lt;/p&gt; &#xA;&lt;p&gt;For LangAuto-Tiny benchmark evaluation, replace the &lt;code&gt;long.json&lt;/code&gt; and &lt;code&gt;long.xml&lt;/code&gt; files with &lt;code&gt;tiny.json&lt;/code&gt; and &lt;code&gt;tiny.xml&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export SCENARIOS=leaderboard/data/LangAuto/tiny.json&#xA;export ROUTES=leaderboard/data/LangAuto/tiny.xml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;LangAuto-Notice&lt;/h3&gt; &#xA;&lt;p&gt;Set the &lt;code&gt;agent_use_notice&lt;/code&gt; as True in the lmdriver_config.py.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our repo, dataset or paper useful, please cite us as&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{shao2023lmdrive,&#xA;      title={LMDrive: Closed-Loop End-to-End Driving with Large Language Models}, &#xA;      author={Hao Shao and Yuxuan Hu and Letian Wang and Steven L. Waslander and Yu Liu and Hongsheng Li},&#xA;      year={2023},&#xA;      eprint={2312.07488},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;This implementation is based on code from several repositories.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/opendilab/InterFuser&#34;&gt;InterFuser&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/autonomousvision/transfuser&#34;&gt;Transfuser&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/bradyz/2020_CARLA_challenge&#34;&gt;2020_CARLA_challenge&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/carla-simulator/leaderboard&#34;&gt;CARLA Leaderboard&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/carla-simulator/scenario_runner&#34;&gt;Scenario Runner&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/salesforce/LAVIS&#34;&gt;LAVIS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/pytorch-image-models&#34;&gt;pytorch-image-models&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;All code within this repository is under &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;Apache License 2.0&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mrdbourke/tensorflow-deep-learning</title>
    <updated>2024-12-31T01:51:48Z</updated>
    <id>tag:github.com,2024-12-31:/mrdbourke/tensorflow-deep-learning</id>
    <link href="https://github.com/mrdbourke/tensorflow-deep-learning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;All course materials for the Zero to Mastery Deep Learning with TensorFlow course.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Zero to Mastery Deep Learning with TensorFlow&lt;/h1&gt; &#xA;&lt;p&gt;All of the course materials for the &lt;a href=&#34;https://dbourke.link/ZTMTFcourse&#34;&gt;Zero to Mastery Deep Learning with TensorFlow course&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This course will teach you the foundations of deep learning and TensorFlow as well as prepare you to pass the TensorFlow Developer Certification exam (optional).&lt;/p&gt; &#xA;&lt;h2&gt;Important links&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üé• Watch the &lt;a href=&#34;https://dbourke.link/tfpart1part2&#34;&gt;first 14-hours of the course on YouTube&lt;/a&gt; (notebooks 00, 01, 02)&lt;/li&gt; &#xA; &lt;li&gt;üìñ Read the &lt;a href=&#34;https://dev.mrdbourke.com/tensorflow-deep-learning/&#34;&gt;beautiful online book version of the course&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üíª &lt;a href=&#34;https://dbourke.link/ZTMTFcourse&#34;&gt;Sign up&lt;/a&gt; to the full course on the Zero to Mastery Academy (videos for notebooks 03-10)&lt;/li&gt; &#xA; &lt;li&gt;ü§î Got questions about the course? Check out the &lt;a href=&#34;https://youtu.be/rqAqcFcfeK8&#34;&gt;livestream Q&amp;amp;A for the course launch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üìù Get a quick overview of TensorFlow with the &lt;a href=&#34;https://zerotomastery.io/cheatsheets/tensorflow-cheat-sheet/&#34;&gt;TensorFlow Cheatsheet&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contents of this page&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning#fixes-and-updates&#34;&gt;Fixes and updates&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning#course-materials&#34;&gt;Course materials&lt;/a&gt; (everything you&#39;ll need for completing the course)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning#course-structure&#34;&gt;Course structure&lt;/a&gt; (how this course is taught)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning#should-you-do-this-course&#34;&gt;Should you do this course?&lt;/a&gt; (decide by answering a couple simple questions)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning#prerequisites&#34;&gt;Prerequisites&lt;/a&gt; (what skills you&#39;ll need to do this course)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning#-exercises---extra-curriculum&#34;&gt;Exercises &amp;amp; Extra-curriculum&lt;/a&gt; (challenges to practice what you&#39;ve learned and resources to learn more)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning#ask-questions&#34;&gt;Ask a question&lt;/a&gt; (like to know more? go here)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning#status&#34;&gt;Status&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning#log&#34;&gt;Log&lt;/a&gt; (updates, changes and progress)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Fixes and updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;18 Aug 2023 - Update &lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/05_transfer_learning_in_tensorflow_part_2_fine_tuning.ipynb&#34;&gt;Notebook 05&lt;/a&gt; to fix &lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/issues/544&#34;&gt;#544&lt;/a&gt; and &lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/issues/553&#34;&gt;#553&lt;/a&gt;, see &lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/discussions/575&#34;&gt;https://github.com/mrdbourke/tensorflow-deep-learning/discussions/575&lt;/a&gt; for full notes &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;In short, if you&#39;re using &lt;code&gt;tf.keras.applications.EfficientNetB0&lt;/code&gt; and facing errors, swap to &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/keras/applications/efficientnet_v2/EfficientNetV2B0&#34;&gt;&lt;code&gt;tf.keras.applications.efficientnet_v2.EfficientNetV2B0&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;26 May 2023 - Update &lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/08_introduction_to_nlp_in_tensorflow.ipynb&#34;&gt;Notebook 08&lt;/a&gt; for new version of TensorFlow + update &lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/09_SkimLit_nlp_milestone_project_2.ipynb&#34;&gt;Notebook 09&lt;/a&gt; for new version of TensorFlow &amp;amp; spaCy, see update notes for 09: &lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/discussions/557&#34;&gt;https://github.com/mrdbourke/tensorflow-deep-learning/discussions/557&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;19 May 2023 - Update &lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/07_food_vision_milestone_project_1.ipynb&#34;&gt;Notebook 07&lt;/a&gt; for new version of TensorFlow + fix model loading errors (TensorFlow 2.13+ required), see: &lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/discussions/550&#34;&gt;https://github.com/mrdbourke/tensorflow-deep-learning/discussions/550&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;18 May 2023 - Update Notebook 06 for new TensorFlow namespaces (no major functionality change, just different imports), see: &lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/discussions/549&#34;&gt;https://github.com/mrdbourke/tensorflow-deep-learning/discussions/549&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;12 May 2023 - Notebook 05 new namespaces added for &lt;code&gt;tf.keras.layers&lt;/code&gt;, see &lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/discussions/547&#34;&gt;https://github.com/mrdbourke/tensorflow-deep-learning/discussions/547&lt;/a&gt;, also add fix for issue with &lt;code&gt;model.load_weights()&lt;/code&gt; in Notebook 05, see &lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/issues/544&#34;&gt;https://github.com/mrdbourke/tensorflow-deep-learning/issues/544&lt;/a&gt;, if you&#39;re having trouble saving/loading the model weights, also see &lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/issues/553&#34;&gt;https://github.com/mrdbourke/tensorflow-deep-learning/issues/553&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;12 May 2023 - Newer versions of TensorFlow (2.10+) use &lt;code&gt;learning_rate&lt;/code&gt; instead of &lt;code&gt;lr&lt;/code&gt; in &lt;code&gt;tf.keras.optimizers&lt;/code&gt; (e.g. &lt;code&gt;tf.keras.optimizers.Adam(learning_rate=0.001)&lt;/code&gt;, old &lt;code&gt;lr&lt;/code&gt; still works but is deprecated&lt;/li&gt; &#xA; &lt;li&gt;02 Dec 2021 - Added fix for TensorFlow 2.7.0+ for notebook 02, &lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/discussions/278&#34;&gt;see discussion for more&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;11 Nov 2021 - Added fix for TensorFlow 2.7.0+ for notebook 01, &lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/discussions/256&#34;&gt;see discussion for more&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Course materials&lt;/h2&gt; &#xA;&lt;p&gt;This table is the ground truth for course materials. All the links you need for everything will be here.&lt;/p&gt; &#xA;&lt;p&gt;Key:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Number:&lt;/strong&gt; The number of the target notebook (this may not match the video section of the course but it ties together all of the materials in the table)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Notebook:&lt;/strong&gt; The notebook for a particular module with lots of code and text annotations (notebooks from the videos are based on these)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Data/model:&lt;/strong&gt; Links to datasets/pre-trained models for the associated notebook&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Exercises &amp;amp; Extra-curriculum:&lt;/strong&gt; Each module comes with a set of exercises and extra-curriculum to help practice your skills and learn more, I suggest going through these &lt;strong&gt;before&lt;/strong&gt; you move onto the next module&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Slides:&lt;/strong&gt; Although we focus on writing TensorFlow code, we sometimes use pretty slides to describe different concepts, you&#39;ll find them here&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; You can get all of the notebook code created during the videos in the &lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/tree/main/video_notebooks&#34;&gt;&lt;code&gt;video_notebooks&lt;/code&gt;&lt;/a&gt; directory.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Number&lt;/th&gt; &#xA;   &lt;th&gt;Notebook&lt;/th&gt; &#xA;   &lt;th&gt;Data/Model&lt;/th&gt; &#xA;   &lt;th&gt;Exercises &amp;amp; Extra-curriculum&lt;/th&gt; &#xA;   &lt;th&gt;Slides&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;00&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/00_tensorflow_fundamentals.ipynb&#34;&gt;TensorFlow Fundamentals&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning#-00-tensorflow-fundamentals-exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/slides/00_introduction_to_tensorflow_and_deep_learning.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;01&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/01_neural_network_regression_in_tensorflow.ipynb&#34;&gt;TensorFlow Regression&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning#-01-neural-network-regression-with-tensorflow-exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/slides/01_neural_network_regression_with_tensorflow.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;02&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/02_neural_network_classification_in_tensorflow.ipynb&#34;&gt;TensorFlow Classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning#-02-neural-network-classification-with-tensorflow-exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/slides/02_neural_network_classification_with_tensorflow.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;03&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/03_convolutional_neural_networks_in_tensorflow.ipynb&#34;&gt;TensorFlow Computer Vision&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/ztm_tf_course/food_vision/pizza_steak.zip&#34;&gt;&lt;code&gt;pizza_steak&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_all_data.zip&#34;&gt;&lt;code&gt;10_food_classes_all_data&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning#-03-computer-vision--convolutional-neural-networks-in-tensorflow-exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/slides/03_convolution_neural_networks_and_computer_vision_with_tensorflow.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;04&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/04_transfer_learning_in_tensorflow_part_1_feature_extraction.ipynb&#34;&gt;Transfer Learning Part 1: Feature extraction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip&#34;&gt;&lt;code&gt;10_food_classes_10_percent&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning#-04-transfer-learning-in-tensorflow-part-1-feature-extraction-exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/slides/04_transfer_learning_with_tensorflow_part_1_feature_extraction.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;05&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/05_transfer_learning_in_tensorflow_part_2_fine_tuning.ipynb&#34;&gt;Transfer Learning Part 2: Fine-tuning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip&#34;&gt;&lt;code&gt;10_food_classes_10_percent&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_1_percent.zip&#34;&gt;&lt;code&gt;10_food_classes_1_percent&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_all_data.zip&#34;&gt;&lt;code&gt;10_food_classes_all_data&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning#-05-transfer-learning-in-tensorflow-part-2-fine-tuning-exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/slides/05_transfer_learning_with_tensorflow_part_2_fine_tuning.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;06&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/06_transfer_learning_in_tensorflow_part_3_scaling_up.ipynb&#34;&gt;Transfer Learning Part 3: Scaling up&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/ztm_tf_course/food_vision/101_food_classes_10_percent.zip&#34;&gt;&lt;code&gt;101_food_classes_10_percent&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://storage.googleapis.com/ztm_tf_course/food_vision/custom_food_images.zip&#34;&gt;&lt;code&gt;custom_food_images&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://storage.googleapis.com/ztm_tf_course/food_vision/06_101_food_class_10_percent_saved_big_dog_model.zip&#34;&gt;&lt;code&gt;fine_tuned_efficientnet_model&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning#-06-transfer-learning-in-tensorflow-part-3-scaling-up-exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/slides/06_transfer_learning_with_tensorflow_part_3_scaling_up.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;07&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/07_food_vision_milestone_project_1.ipynb&#34;&gt;Milestone Project 1: Food Vision üçîüëÅ&lt;/a&gt;, &lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/extras/TEMPLATE_07_food_vision_milestone_project_1.ipynb&#34;&gt;Template (your challenge)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/ztm_tf_course/food_vision/07_efficientnetb0_feature_extract_model_mixed_precision.zip&#34;&gt;&lt;code&gt;feature_extraction_mixed_precision_efficientnet_model&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://storage.googleapis.com/ztm_tf_course/food_vision/07_efficientnetb0_fine_tuned_101_classes_mixed_precision.zip&#34;&gt;&lt;code&gt;fine_tuned_mixed_precision_efficientnet_model&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning#-07-milestone-project-1--food-vision-big-exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/slides/07_milestone_project_1_food_vision.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;08&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/08_introduction_to_nlp_in_tensorflow.ipynb&#34;&gt;TensorFlow NLP Fundamentals&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip&#34;&gt;&lt;code&gt;diaster_or_no_diaster_tweets&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://storage.googleapis.com/ztm_tf_course/08_model_6_USE_feature_extractor.zip&#34;&gt;&lt;code&gt;USE_feature_extractor_model&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning#-08-introduction-to-nlp-natural-language-processing-in-tensorflow-exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/slides/08_natural_language_processing_in_tensorflow.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;09&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/09_SkimLit_nlp_milestone_project_2.ipynb&#34;&gt;Milestone Project 2: SkimLit üìÑüî•&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Franck-Dernoncourt/pubmed-rct.git&#34;&gt;&lt;code&gt;pubmed_RCT_200k_dataset&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://storage.googleapis.com/ztm_tf_course/skimlit/skimlit_tribrid_model.zip&#34;&gt;&lt;code&gt;skimlit_tribrid_model&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning#-09-milestone-project-2-skimlit--exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/slides/09_milestone_project_2_skimlit.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;10&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/10_time_series_forecasting_in_tensorflow.ipynb&#34;&gt;TensorFlow Time Series Fundamentals &amp;amp; Milestone Project 3: BitPredict üí∞üìà&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv&#34;&gt;&lt;code&gt;bitcoin_price_data_USD_2013-10-01_2021-05-18.csv&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning#-10-time-series-fundamentals-and-milestone-project-3-bitpredict--exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/slides/10_time_series_fundamentals_and_milestone_project_3_bitpredict.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;11&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/11_passing_the_tensorflow_developer_certification_exam.md&#34;&gt;Preparing to Pass the TensorFlow Developer Certification Exam&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning#-11-passing-the-tensorflow-developer-certification-exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/slides/11_passing_the_tensorflow_developer_certification_exam.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Course structure&lt;/h2&gt; &#xA;&lt;p&gt;This course is code first. The goal is to get you writing deep learning code as soon as possible.&lt;/p&gt; &#xA;&lt;p&gt;It is taught with the following mantra:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Code -&amp;gt; Concept -&amp;gt; Code -&amp;gt; Concept -&amp;gt; Code -&amp;gt; Concept&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This means we write code first then step through the concepts behind it.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;ve got 6-months experience writing Python code and a willingness to learn (most important), you&#39;ll be able to do the course.&lt;/p&gt; &#xA;&lt;h2&gt;Should you do this course?&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Do you have 1+ years experience with deep learning and writing TensorFlow code?&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;If yes, no you shouldn&#39;t, use your skills to build something.&lt;/p&gt; &#xA;&lt;p&gt;If no, move onto the next question.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Have you done at least one beginner machine learning course and would like to learn about deep learning/pass the TensorFlow Developer Certification?&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;If yes, this course is for you.&lt;/p&gt; &#xA;&lt;p&gt;If no, go and do a beginner machine learning course and if you decide you want to learn TensorFlow, this page will still be here.&lt;/p&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;What do I need to know to go through this course?&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;6+ months writing Python code.&lt;/strong&gt; Can you write a Python function which accepts and uses parameters? That‚Äôs good enough. If you don‚Äôt know what that means, spend another month or two writing Python code and then come back here.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;At least one beginner machine learning course.&lt;/strong&gt; Are you familiar with the idea of training, validation and test sets? Do you know what supervised learning is? Have you used pandas, NumPy or Matplotlib before? If no to any of these, I‚Äôd going through at least one machine learning course which teaches these first and then coming back.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Comfortable using Google Colab/Jupyter Notebooks.&lt;/strong&gt; This course uses Google Colab throughout. If you have never used Google Colab before, it works very similar to Jupyter Notebooks with a few extra features. If you‚Äôre not familiar with Google Colab notebooks, I‚Äôd suggest going through the Introduction to Google Colab notebook.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Plug:&lt;/strong&gt; The &lt;a href=&#34;https://dbourke.link/ZTMMLcourse&#34;&gt;Zero to Mastery beginner-friendly machine learning course&lt;/a&gt; (I also teach this) teaches all of the above (and this course is designed as a follow on).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üõ† Exercises &amp;amp; üìñ Extra-curriculum&lt;/h2&gt; &#xA;&lt;p&gt;To prevent the course from being 100+ hours (deep learning is a broad field), various external resources for different sections are recommended to puruse under your own discretion.&lt;/p&gt; &#xA;&lt;p&gt;You can find solutions to the exercises in &lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/tree/main/extras/solutions&#34;&gt;&lt;code&gt;extras/solutions/&lt;/code&gt;&lt;/a&gt;, there&#39;s a notebook per set of exercises (one for 00, 01, 02... etc). Thank you to &lt;a href=&#34;https://github.com/ashikshafi08&#34;&gt;Ashik Shafi&lt;/a&gt; for all of the efforts creating these.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;üõ† 00. TensorFlow Fundamentals Exercises&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create a vector, scalar, matrix and tensor with values of your choosing using &lt;code&gt;tf.constant()&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Find the shape, rank and size of the tensors you created in 1.&lt;/li&gt; &#xA; &lt;li&gt;Create two tensors containing random values between 0 and 1 with shape &lt;code&gt;[5, 300]&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Multiply the two tensors you created in 3 using matrix multiplication.&lt;/li&gt; &#xA; &lt;li&gt;Multiply the two tensors you created in 3 using dot product.&lt;/li&gt; &#xA; &lt;li&gt;Create a tensor with random values between 0 and 1 with shape &lt;code&gt;[224, 224, 3]&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Find the min and max values of the tensor you created in 6 along the first axis.&lt;/li&gt; &#xA; &lt;li&gt;Created a tensor with random values of shape &lt;code&gt;[1, 224, 224, 3]&lt;/code&gt; then squeeze it to change the shape to &lt;code&gt;[224, 224, 3]&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Create a tensor with shape &lt;code&gt;[10]&lt;/code&gt; using your own choice of values, then find the index which has the maximum value.&lt;/li&gt; &#xA; &lt;li&gt;One-hot encode the tensor you created in 9.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;üìñ 00. TensorFlow Fundamentals Extra-curriculum&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Read through the &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/&#34;&gt;list of TensorFlow Python APIs&lt;/a&gt;, pick one we haven&#39;t gone through in this notebook, reverse engineer it (write out the documentation code for yourself) and figure out what it does.&lt;/li&gt; &#xA; &lt;li&gt;Try to create a series of tensor functions to calculate your most recent grocery bill (it&#39;s okay if you don&#39;t use the names of the items, just the price in numerical form). &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;How would you calculate your grocery bill for the month and for the year using tensors?&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Go through the &lt;a href=&#34;https://www.tensorflow.org/tutorials/quickstart/beginner&#34;&gt;TensorFlow 2.x quick start for beginners&lt;/a&gt; tutorial (be sure to type out all of the code yourself, even if you don&#39;t understand it). &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Are there any functions we used in here that match what&#39;s used in there? Which are the same? Which haven&#39;t you seen before?&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Watch the video &lt;a href=&#34;https://www.youtube.com/watch?v=f5liqUk0ZTw&#34;&gt;&#34;What&#39;s a tensor?&#34;&lt;/a&gt; - a great visual introduction to many of the concepts we&#39;ve covered in this notebook.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;üõ† 01. Neural network regression with TensorFlow Exercises&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create your own regression dataset (or make the one we created in &#34;Create data to view and fit&#34; bigger) and build fit a model to it.&lt;/li&gt; &#xA; &lt;li&gt;Try building a neural network with 4 Dense layers and fitting it to your own regression dataset, how does it perform?&lt;/li&gt; &#xA; &lt;li&gt;Try and improve the results we got on the insurance dataset, some things you might want to try include:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Building a larger model (how does one with 4 dense layers go?).&lt;/li&gt; &#xA; &lt;li&gt;Increasing the number of units in each layer.&lt;/li&gt; &#xA; &lt;li&gt;Lookup the documentation of &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam&#34;&gt;Adam&lt;/a&gt; and find out what the first parameter is, what happens if you increase it by 10x?&lt;/li&gt; &#xA; &lt;li&gt;What happens if you train for longer (say 300 epochs instead of 200)?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Import the &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/keras/datasets/boston_housing/load_data&#34;&gt;Boston pricing dataset&lt;/a&gt; from TensorFlow &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/keras/datasets&#34;&gt;&lt;code&gt;tf.keras.datasets&lt;/code&gt;&lt;/a&gt; and model it.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;üìñ 01. Neural network regression with TensorFlow Extra-curriculum&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=7sB052Pz0sQ&amp;amp;ab_channel=AlexanderAmini&#34;&gt;MIT introduction deep learning lecture 1&lt;/a&gt; - gives a great overview of what&#39;s happening behind all of the code we&#39;re running.&lt;/li&gt; &#xA; &lt;li&gt;Reading: 1-hour of &lt;a href=&#34;http://neuralnetworksanddeeplearning.com/chap1.html&#34;&gt;Chapter 1 of Neural Networks and Deep Learning&lt;/a&gt; by Michael Nielson - a great in-depth and hands-on example of the intuition behind neural networks.&lt;/li&gt; &#xA; &lt;li&gt;To practice your regression modelling with TensorFlow, I&#39;d also encourage you to look through &lt;a href=&#34;https://www.kaggle.com/data&#34;&gt;Kaggle&#39;s datasets&lt;/a&gt;, find a regression dataset which sparks your interest and try to model.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;üõ† 02. Neural network classification with TensorFlow Exercises&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Play with neural networks in the &lt;a href=&#34;https://playground.tensorflow.org/&#34;&gt;TensorFlow Playground&lt;/a&gt; for 10-minutes. Especially try different values of the learning, what happens when you decrease it? What happens when you increase it?&lt;/li&gt; &#xA; &lt;li&gt;Replicate the model pictured in the &lt;a href=&#34;https://playground.tensorflow.org/#activation=relu&amp;amp;batchSize=10&amp;amp;dataset=circle&amp;amp;regDataset=reg-plane&amp;amp;learningRate=0.001&amp;amp;regularizationRate=0&amp;amp;noise=0&amp;amp;networkShape=6,6,6,6,6&amp;amp;seed=0.51287&amp;amp;showTestData=false&amp;amp;discretize=false&amp;amp;percTrainData=50&amp;amp;x=true&amp;amp;y=true&amp;amp;xTimesY=false&amp;amp;xSquared=false&amp;amp;ySquared=false&amp;amp;cosX=false&amp;amp;sinX=false&amp;amp;cosY=false&amp;amp;sinY=false&amp;amp;collectStats=false&amp;amp;problem=classification&amp;amp;initZero=false&amp;amp;hideText=false&amp;amp;regularization_hide=true&amp;amp;discretize_hide=true&amp;amp;regularizationRate_hide=true&amp;amp;percTrainData_hide=true&amp;amp;dataset_hide=true&amp;amp;problem_hide=true&amp;amp;noise_hide=true&amp;amp;batchSize_hide=true&#34;&gt;TensorFlow Playground diagram&lt;/a&gt; below using TensorFlow code. Compile it using the Adam optimizer, binary crossentropy loss and accuracy metric. Once it&#39;s compiled check a summary of the model. &lt;img src=&#34;https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/02-tensorflow-playground-replication-exercise.png&#34; alt=&#34;tensorflow playground example neural network&#34;&gt; &lt;em&gt;Try this network out for yourself on the &lt;a href=&#34;https://playground.tensorflow.org/#activation=relu&amp;amp;batchSize=10&amp;amp;dataset=circle&amp;amp;regDataset=reg-plane&amp;amp;learningRate=0.001&amp;amp;regularizationRate=0&amp;amp;noise=0&amp;amp;networkShape=6,6,6,6,6&amp;amp;seed=0.51287&amp;amp;showTestData=false&amp;amp;discretize=false&amp;amp;percTrainData=50&amp;amp;x=true&amp;amp;y=true&amp;amp;xTimesY=false&amp;amp;xSquared=false&amp;amp;ySquared=false&amp;amp;cosX=false&amp;amp;sinX=false&amp;amp;cosY=false&amp;amp;sinY=false&amp;amp;collectStats=false&amp;amp;problem=classification&amp;amp;initZero=false&amp;amp;hideText=false&amp;amp;regularization_hide=true&amp;amp;discretize_hide=true&amp;amp;regularizationRate_hide=true&amp;amp;percTrainData_hide=true&amp;amp;dataset_hide=true&amp;amp;problem_hide=true&amp;amp;noise_hide=true&amp;amp;batchSize_hide=true&#34;&gt;TensorFlow Playground website&lt;/a&gt;. Hint: there are 5 hidden layers but the output layer isn&#39;t pictured, you&#39;ll have to decide what the output layer should be based on the input data.&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;Create a classification dataset using Scikit-Learn&#39;s &lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html&#34;&gt;&lt;code&gt;make_moons()&lt;/code&gt;&lt;/a&gt; function, visualize it and then build a model to fit it at over 85% accuracy.&lt;/li&gt; &#xA; &lt;li&gt;Train a model to get 88%+ accuracy on the fashion MNIST test set. Plot a confusion matrix to see the results after.&lt;/li&gt; &#xA; &lt;li&gt;Recreate &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/keras/activations/softmax&#34;&gt;TensorFlow&#39;s&lt;/a&gt; &lt;a href=&#34;https://en.wikipedia.org/wiki/Softmax_function&#34;&gt;softmax activation function&lt;/a&gt; in your own code. Make sure it can accept a tensor and return that tensor after having the softmax function applied to it.&lt;/li&gt; &#xA; &lt;li&gt;Create a function (or write code) to visualize multiple image predictions for the fashion MNIST at the same time. Plot at least three different images and their prediction labels at the same time. Hint: see the &lt;a href=&#34;https://www.tensorflow.org/tutorials/keras/classification&#34;&gt;classification tutorial in the TensorFlow documentation&lt;/a&gt; for ideas.&lt;/li&gt; &#xA; &lt;li&gt;Make a function to show an image of a certain class of the fashion MNIST dataset and make a prediction on it. For example, plot 3 images of the &lt;code&gt;T-shirt&lt;/code&gt; class with their predictions.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;üìñ 02. Neural network classification with TensorFlow Extra-curriculum&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Watch 3Blue1Brown&#39;s neural networks video 2: &lt;a href=&#34;https://www.youtube.com/watch?v=IHZwWFHWa-w&#34;&gt;&lt;em&gt;Gradient descent, how neural networks learn&lt;/em&gt;&lt;/a&gt;. After you&#39;re done, write 100 words about what you&#39;ve learned. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;If you haven&#39;t already, watch video 1: &lt;a href=&#34;https://www.youtube.com/watch?v=aircAruvnKk&#34;&gt;&lt;em&gt;But what is a Neural Network?&lt;/em&gt;&lt;/a&gt;. Note the activation function they talk about at the end.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Watch &lt;a href=&#34;https://youtu.be/njKP3FqW3Sk&#34;&gt;MIT&#39;s introduction to deep learning lecture 1&lt;/a&gt; (if you haven&#39;t already) to get an idea of the concepts behind using linear and non-linear functions.&lt;/li&gt; &#xA; &lt;li&gt;Spend 1-hour reading &lt;a href=&#34;http://neuralnetworksanddeeplearning.com/index.html&#34;&gt;Michael Nielsen&#39;s Neural Networks and Deep Learning book&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Read the &lt;a href=&#34;https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html&#34;&gt;ML-Glossary documentation on activation functions&lt;/a&gt;. Which one is your favourite? &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;After you&#39;ve read the ML-Glossary, see which activation functions are available in TensorFlow by searching &#34;tensorflow activation functions&#34;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;üõ† 03. Computer vision &amp;amp; convolutional neural networks in TensorFlow Exercises&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Spend 20-minutes reading and interacting with the &lt;a href=&#34;https://poloclub.github.io/cnn-explainer/&#34;&gt;CNN explainer website&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;What are the key terms? e.g. explain convolution in your own words, pooling in your own words&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Play around with the &#34;understanding hyperparameters&#34; section in the &lt;a href=&#34;https://poloclub.github.io/cnn-explainer/&#34;&gt;CNN explainer&lt;/a&gt; website for 10-minutes.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;What is the kernel size?&lt;/li&gt; &#xA; &lt;li&gt;What is the stride?&lt;/li&gt; &#xA; &lt;li&gt;How could you adjust each of these in TensorFlow code?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Take 10 photos of two different things and build your own CNN image classifier using the techniques we&#39;ve built here.&lt;/li&gt; &#xA; &lt;li&gt;Find an ideal learning rate for a simple convolutional neural network model on your the 10 class dataset.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;üìñ 03. Computer vision &amp;amp; convolutional neural networks in TensorFlow Extra-curriculum&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Watch:&lt;/strong&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=uapdILWYTzE&amp;amp;list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&amp;amp;index=3&amp;amp;ab_channel=AlexanderAmini&#34;&gt;MIT&#39;s Introduction to Deep Computer Vision&lt;/a&gt; lecture. This will give you a great intuition behind convolutional neural networks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Watch:&lt;/strong&gt; Deep dive on &lt;a href=&#34;https://youtu.be/-_4Zi8fCZO4&#34;&gt;mini-batch gradient descent&lt;/a&gt; by deeplearning.ai. If you&#39;re still curious about why we use &lt;strong&gt;batches&lt;/strong&gt; to train models, this technical overview covers many of the reasons why.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Read:&lt;/strong&gt; &lt;a href=&#34;https://cs231n.github.io/convolutional-networks/&#34;&gt;CS231n Convolutional Neural Networks for Visual Recognition&lt;/a&gt; class notes. This will give a very deep understanding of what&#39;s going on behind the scenes of the convolutional neural network architectures we&#39;re writing.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Read:&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/pdf/1603.07285.pdf&#34;&gt;&#34;A guide to convolution arithmetic for deep learning&#34;&lt;/a&gt;. This paper goes through all of the mathematics running behind the scenes of our convolutional layers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Code practice:&lt;/strong&gt; &lt;a href=&#34;https://www.tensorflow.org/tutorials/images/data_augmentation&#34;&gt;TensorFlow Data Augmentation Tutorial&lt;/a&gt;. For a more in-depth introduction on data augmentation with TensorFlow, spend an hour or two reading through this tutorial.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;üõ† 04. Transfer Learning in TensorFlow Part 1: Feature Extraction Exercises&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Build and fit a model using the same data we have here but with the MobileNetV2 architecture feature extraction (&lt;a href=&#34;https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4&#34;&gt;&lt;code&gt;mobilenet_v2_100_224/feature_vector&lt;/code&gt;&lt;/a&gt;) from TensorFlow Hub, how does it perform compared to our other models?&lt;/li&gt; &#xA; &lt;li&gt;Name 3 different image classification models on TensorFlow Hub that we haven&#39;t used.&lt;/li&gt; &#xA; &lt;li&gt;Build a model to classify images of two different things you&#39;ve taken photos of.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can use any feature extraction layer from TensorFlow Hub you like for this.&lt;/li&gt; &#xA; &lt;li&gt;You should aim to have at least 10 images of each class, for example to build a fridge versus oven classifier, you&#39;ll want 10 images of fridges and 10 images of ovens.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;What is the current best performing model on ImageNet?&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Hint: you might want to check &lt;a href=&#34;https://www.sotabench.com&#34;&gt;sotabench.com&lt;/a&gt; for this.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;üìñ 04. Transfer Learning in TensorFlow Part 1: Feature Extraction Extra-curriculum&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Read through the &lt;a href=&#34;https://www.tensorflow.org/tutorials/images/transfer_learning&#34;&gt;TensorFlow Transfer Learning Guide&lt;/a&gt; and define the main two types of transfer learning in your own words.&lt;/li&gt; &#xA; &lt;li&gt;Go through the &lt;a href=&#34;https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub&#34;&gt;Transfer Learning with TensorFlow Hub tutorial&lt;/a&gt; on the TensorFlow website and rewrite all of the code yourself into a new Google Colab notebook making comments about what each step does along the way.&lt;/li&gt; &#xA; &lt;li&gt;We haven&#39;t covered fine-tuning with TensorFlow Hub in this notebook, but if you&#39;d like to know more, go through the &lt;a href=&#34;https://www.tensorflow.org/hub/tf2_saved_model#fine-tuning&#34;&gt;fine-tuning a TensorFlow Hub model tutorial&lt;/a&gt; on the TensorFlow homepage.How to fine-tune a tensorflow hub model:&lt;/li&gt; &#xA; &lt;li&gt;Look into &lt;a href=&#34;https://www.wandb.com/experiment-tracking&#34;&gt;experiment tracking with Weights &amp;amp; Biases&lt;/a&gt;, how could you integrate it with our existing TensorBoard logs?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;üõ† 05. Transfer Learning in TensorFlow Part 2: Fine-tuning Exercises&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Use feature-extraction to train a transfer learning model on 10% of the Food Vision data for 10 epochs using &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/keras/applications/EfficientNetB0&#34;&gt;&lt;code&gt;tf.keras.applications.EfficientNetB0&lt;/code&gt;&lt;/a&gt; as the base model. Use the &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint&#34;&gt;&lt;code&gt;ModelCheckpoint&lt;/code&gt;&lt;/a&gt; callback to save the weights to file.&lt;/li&gt; &#xA; &lt;li&gt;Fine-tune the last 20 layers of the base model you trained in 2 for another 10 epochs. How did it go?&lt;/li&gt; &#xA; &lt;li&gt;Fine-tune the last 30 layers of the base model you trained in 2 for another 10 epochs. How did it go?&lt;/li&gt; &#xA; &lt;li&gt;Write a function to visualize an image from any dataset (train or test file) and any class (e.g. &#34;steak&#34;, &#34;pizza&#34;... etc), visualize it and make a prediction on it using a trained model.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;üìñ 05. Transfer Learning in TensorFlow Part 2: Fine-tuning Extra-curriculum&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Read the &lt;a href=&#34;https://www.tensorflow.org/tutorials/images/data_augmentation&#34;&gt;documentation on data augmentation&lt;/a&gt; in TensorFlow.&lt;/li&gt; &#xA; &lt;li&gt;Read the &lt;a href=&#34;https://arxiv.org/abs/1801.06146&#34;&gt;ULMFit paper&lt;/a&gt; (technical) for an introduction to the concept of freezing and unfreezing different layers.&lt;/li&gt; &#xA; &lt;li&gt;Read up on learning rate scheduling (there&#39;s a &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler&#34;&gt;TensorFlow callback&lt;/a&gt; for this), how could this influence our model training? &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;If you&#39;re training for longer, you probably want to reduce the learning rate as you go... the closer you get to the bottom of the hill, the smaller steps you want to take. Imagine it like finding a coin at the bottom of your couch. In the beginning your arm movements are going to be large and the closer you get, the smaller your movements become.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;üõ† 06. Transfer Learning in TensorFlow Part 3: Scaling-up Exercises&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Take 3 of your own photos of food and use the trained model to make predictions on them, share your predictions with the other students in Discord and show off your Food Vision model üçîüëÅ.&lt;/li&gt; &#xA; &lt;li&gt;Train a feature-extraction transfer learning model for 10 epochs on the same data and compare its performance versus a model which used feature extraction for 5 epochs and fine-tuning for 5 epochs (like we&#39;ve used in this notebook). Which method is better?&lt;/li&gt; &#xA; &lt;li&gt;Recreate the first model (the feature extraction model) with &lt;a href=&#34;https://www.tensorflow.org/guide/mixed_precision&#34;&gt;&lt;code&gt;mixed_precision&lt;/code&gt;&lt;/a&gt; turned on.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Does it make the model train faster?&lt;/li&gt; &#xA; &lt;li&gt;Does it effect the accuracy or performance of our model?&lt;/li&gt; &#xA; &lt;li&gt;What&#39;s the advantages of using &lt;code&gt;mixed_precision&lt;/code&gt; training?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;üìñ 06. Transfer Learning in TensorFlow Part 3: Scaling-up Extra-curriculum&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Spend 15-minutes reading up on the &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping&#34;&gt;EarlyStopping callback&lt;/a&gt;. What does it do? How could we use it in our model training?&lt;/li&gt; &#xA; &lt;li&gt;Spend an hour reading about &lt;a href=&#34;https://www.streamlit.io/&#34;&gt;Streamlit&lt;/a&gt;. What does it do? How might you integrate some of the things we&#39;ve done in this notebook in a Streamlit app?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;üõ† 07. Milestone Project 1: üçîüëÅ Food Vision Big‚Ñ¢ Exercises&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The chief exercise for Milestone Project 1 is to finish the &#34;TODO&#34; sections in the &lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/extras/TEMPLATE_07_food_vision_milestone_project_1.ipynb&#34;&gt;Milestone Project 1 Template notebook&lt;/a&gt;. After doing so, move onto the following.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Use the same evaluation techniques on the large-scale Food Vision model as you did in the previous notebook (&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/06_transfer_learning_in_tensorflow_part_3_scaling_up.ipynb&#34;&gt;Transfer Learning Part 3: Scaling up&lt;/a&gt;). More specifically, it would be good to see:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A confusion matrix between all of the model&#39;s predictions and true labels.&lt;/li&gt; &#xA; &lt;li&gt;A graph showing the f1-scores of each class.&lt;/li&gt; &#xA; &lt;li&gt;A visualization of the model making predictions on various images and comparing the predictions to the ground truth. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;For example, plot a sample image from the test dataset and have the title of the plot show the prediction, the prediction probability and the ground truth label.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Note:&lt;/strong&gt; To compare predicted labels to test labels, it might be a good idea when loading the test data to set &lt;code&gt;shuffle=False&lt;/code&gt; (so the ordering of test data is preserved alongside the order of predicted labels).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Take 3 of your own photos of food and use the Food Vision model to make predictions on them. How does it go? Share your images/predictions with the other students.&lt;/li&gt; &#xA; &lt;li&gt;Retrain the model (feature extraction and fine-tuning) we trained in this notebook, except this time use &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/keras/applications/EfficientNetB4&#34;&gt;&lt;code&gt;EfficientNetB4&lt;/code&gt;&lt;/a&gt; as the base model instead of &lt;code&gt;EfficientNetB0&lt;/code&gt;. Do you notice an improvement in performance? Does it take longer to train? Are there any tradeoffs to consider?&lt;/li&gt; &#xA; &lt;li&gt;Name one important benefit of mixed precision training, how does this benefit take place?&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;üìñ 07. Milestone Project 1: üçîüëÅ Food Vision Big‚Ñ¢ Extra-curriculum&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Read up on learning rate scheduling and the &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler&#34;&gt;learning rate scheduler callback&lt;/a&gt;. What is it? And how might it be helpful to this project?&lt;/li&gt; &#xA; &lt;li&gt;Read up on TensorFlow data loaders (&lt;a href=&#34;https://www.tensorflow.org/guide/data_performance&#34;&gt;improving TensorFlow data loading performance&lt;/a&gt;). Is there anything we&#39;ve missed? What methods you keep in mind whenever loading data in TensorFlow? Hint: check the summary at the bottom of the page for a great round up of ideas.&lt;/li&gt; &#xA; &lt;li&gt;Read up on the documentation for &lt;a href=&#34;https://www.tensorflow.org/guide/mixed_precision&#34;&gt;TensorFlow mixed precision training&lt;/a&gt;. What are the important things to keep in mind when using mixed precision training?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;üõ† 08. Introduction to NLP (Natural Language Processing) in TensorFlow Exercises&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Rebuild, compile and train &lt;code&gt;model_1&lt;/code&gt;, &lt;code&gt;model_2&lt;/code&gt; and &lt;code&gt;model_5&lt;/code&gt; using the &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/keras/Sequential&#34;&gt;Keras Sequential API&lt;/a&gt; instead of the Functional API.&lt;/li&gt; &#xA; &lt;li&gt;Retrain the baseline model with 10% of the training data. How does perform compared to the Universal Sentence Encoder model with 10% of the training data?&lt;/li&gt; &#xA; &lt;li&gt;Try fine-tuning the TF Hub Universal Sentence Encoder model by setting &lt;code&gt;training=True&lt;/code&gt; when instantiating it as a Keras layer.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;# We can use this encoding layer in place of our text_vectorizer and embedding layer&#xA;sentence_encoder_layer = hub.KerasLayer(&#34;https://tfhub.dev/google/universal-sentence-encoder/4&#34;,&#xA;                                        input_shape=[],&#xA;                                        dtype=tf.string,&#xA;                                        trainable=True) # turn training on to fine-tune the TensorFlow Hub model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Retrain the best model you&#39;ve got so far on the whole training set (no validation split). Then use this trained model to make predictions on the test dataset and format the predictions into the same format as the &lt;code&gt;sample_submission.csv&lt;/code&gt; file from Kaggle (see the Files tab in Colab for what the &lt;code&gt;sample_submission.csv&lt;/code&gt; file looks like). Once you&#39;ve done this, &lt;a href=&#34;https://www.kaggle.com/c/nlp-getting-started/data&#34;&gt;make a submission to the Kaggle competition&lt;/a&gt;, how did your model perform?&lt;/li&gt; &#xA; &lt;li&gt;Combine the ensemble predictions using the majority vote (mode), how does this perform compare to averaging the prediction probabilities of each model?&lt;/li&gt; &#xA; &lt;li&gt;Make a confusion matrix with the best performing model&#39;s predictions on the validation set and the validation ground truth labels.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;üìñ 08. Introduction to NLP (Natural Language Processing) in TensorFlow Extra-curriculum&lt;/h3&gt; &#xA;&lt;p&gt;To practice what you&#39;ve learned, a good idea would be to spend an hour on 3 of the following (3-hours total, you could through them all if you want) and then write a blog post about what you&#39;ve learned.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For an overview of the different problems within NLP and how to solve them read through: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://becominghuman.ai/a-simple-introduction-to-natural-language-processing-ea66a1747b32&#34;&gt;A Simple Introduction to Natural Language Processing&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e&#34;&gt;How to solve 90% of NLP problems: a step-by-step guide&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Go through &lt;a href=&#34;https://youtu.be/SEnXr6v2ifU&#34;&gt;MIT&#39;s Recurrent Neural Networks lecture&lt;/a&gt;. This will be one of the greatest additions to what&#39;s happening behind the RNN model&#39;s you&#39;ve been building.&lt;/li&gt; &#xA; &lt;li&gt;Read through the &lt;a href=&#34;https://www.tensorflow.org/tutorials/text/word_embeddings&#34;&gt;word embeddings page on the TensorFlow website&lt;/a&gt;. Embeddings are such a large part of NLP. We&#39;ve covered them throughout this notebook but extra practice would be well worth it. A good exercise would be to write out all the code in the guide in a new notebook.&lt;/li&gt; &#xA; &lt;li&gt;For more on RNN&#39;s in TensorFlow, read and reproduce &lt;a href=&#34;https://www.tensorflow.org/guide/keras/rnn&#34;&gt;the TensorFlow RNN guide&lt;/a&gt;. We&#39;ve covered many of the concepts in this guide, but it&#39;s worth writing the code again for yourself.&lt;/li&gt; &#xA; &lt;li&gt;Text data doesn&#39;t always come in a nice package like the data we&#39;ve downloaded. So if you&#39;re after more on preparing different text sources for being with your TensorFlow deep learning models, it&#39;s worth checking out the following: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org/tutorials/load_data/text&#34;&gt;TensorFlow text loading tutorial&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://realpython.com/read-write-files-python/&#34;&gt;Reading text files with Python&lt;/a&gt; by Real Python.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;This notebook has focused on writing NLP code. For a mathematically rich overview of how NLP with Deep Learning happens, read &lt;a href=&#34;https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf&#34;&gt;Stanford&#39;s Natural Language Processing with Deep Learning lecture notes Part 1&lt;/a&gt;. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;For an even deeper dive, you could even do the whole &lt;a href=&#34;http://web.stanford.edu/class/cs224n/&#34;&gt;CS224n&lt;/a&gt; (Natural Language Processing with Deep Learning) course.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Great blog posts to read: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Andrei Karpathy&#39;s &lt;a href=&#34;https://karpathy.github.io/2015/05/21/rnn-effectiveness/&#34;&gt;The Unreasonable Effectiveness of RNNs&lt;/a&gt; dives into generating Shakespeare text with RNNs.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794&#34;&gt;Text Classification with NLP: Tf-Idf vs Word2Vec vs BERT&lt;/a&gt; by Mauro Di Pietro. An overview of different techniques for turning text into numbers and then classifying it.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://machinelearningmastery.com/what-are-word-embeddings/&#34;&gt;What are word embeddings?&lt;/a&gt; by Machine Learning Mastery.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Other topics worth looking into: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/&#34;&gt;Attention mechanisms&lt;/a&gt;. These are a foundational component of the transformer architecture and also often add improvements to deep NLP models.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;http://jalammar.github.io/illustrated-transformer/&#34;&gt;Transformer architectures&lt;/a&gt;. This model architecture has recently taken the NLP world by storm, achieving state of the art on many benchmarks. However, it does take a little more processing to get off the ground, the &lt;a href=&#34;https://huggingface.co/models/&#34;&gt;HuggingFace Models (formerly HuggingFace Transformers) library&lt;/a&gt; is probably your best quick start.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;üõ† 09. Milestone Project 2: SkimLit üìÑüî• Exercises&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Train &lt;code&gt;model_5&lt;/code&gt; on all of the data in the training dataset for as many epochs until it stops improving. Since this might take a while, you might want to use:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint&#34;&gt;&lt;code&gt;tf.keras.callbacks.ModelCheckpoint&lt;/code&gt;&lt;/a&gt; to save the model&#39;s best weights only.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping&#34;&gt;&lt;code&gt;tf.keras.callbacks.EarlyStopping&lt;/code&gt;&lt;/a&gt; to stop the model from training once the validation loss has stopped improving for ~3 epochs.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Checkout the &lt;a href=&#34;https://keras.io/examples/nlp/pretrained_word_embeddings/&#34;&gt;Keras guide on using pretrained GloVe embeddings&lt;/a&gt;. Can you get this working with one of our models?&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Hint: You&#39;ll want to incorporate it with a custom token &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding&#34;&gt;Embedding&lt;/a&gt; layer.&lt;/li&gt; &#xA; &lt;li&gt;It&#39;s up to you whether or not you fine-tune the GloVe embeddings or leave them frozen.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Try replacing the TensorFlow Hub Universal Sentence Encoder pretrained embedding for the &lt;a href=&#34;https://tfhub.dev/google/experts/bert/pubmed/2&#34;&gt;TensorFlow Hub BERT PubMed expert&lt;/a&gt; (a language model pretrained on PubMed texts) pretrained embedding. Does this effect results?&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Note: Using the BERT PubMed expert pretrained embedding requires an extra preprocessing step for sequences (as detailed in the &lt;a href=&#34;https://tfhub.dev/google/experts/bert/pubmed/2&#34;&gt;TensorFlow Hub guide&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Does the BERT model beat the results mentioned in this paper? &lt;a href=&#34;https://arxiv.org/pdf/1710.06071.pdf&#34;&gt;https://arxiv.org/pdf/1710.06071.pdf&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;What happens if you were to merge our &lt;code&gt;line_number&lt;/code&gt; and &lt;code&gt;total_lines&lt;/code&gt; features for each sequence? For example, created a &lt;code&gt;X_of_Y&lt;/code&gt; feature instead? Does this effect model performance?&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Another example: &lt;code&gt;line_number=1&lt;/code&gt; and &lt;code&gt;total_lines=11&lt;/code&gt; turns into &lt;code&gt;line_of_X=1_of_11&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;Write a function (or series of functions) to take a sample abstract string, preprocess it (in the same way our model has been trained), make a prediction on each sequence in the abstract and return the abstract in the format:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;PREDICTED_LABEL&lt;/code&gt;: &lt;code&gt;SEQUENCE&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;PREDICTED_LABEL&lt;/code&gt;: &lt;code&gt;SEQUENCE&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;PREDICTED_LABEL&lt;/code&gt;: &lt;code&gt;SEQUENCE&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;PREDICTED_LABEL&lt;/code&gt;: &lt;code&gt;SEQUENCE&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;... &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;You can find your own unstructured RCT abstract from PubMed or try this one from: &lt;a href=&#34;https://pubmed.ncbi.nlm.nih.gov/22244707/&#34;&gt;&lt;em&gt;Baclofen promotes alcohol abstinence in alcohol dependent cirrhotic patients with hepatitis C virus (HCV) infection&lt;/em&gt;&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;üìñ 09. Milestone Project 2: SkimLit üìÑüî• Extra-curriculum&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For more on working with text/spaCy, see &lt;a href=&#34;https://course.spacy.io/en/&#34;&gt;spaCy&#39;s advanced NLP course&lt;/a&gt;. If you&#39;re going to be working on production-level NLP problems, you&#39;ll probably end up using spaCy.&lt;/li&gt; &#xA; &lt;li&gt;For another look at how to approach a text classification problem like the one we&#39;ve just gone through, I&#39;d suggest going through &lt;a href=&#34;https://developers.google.com/machine-learning/guides/text-classification&#34;&gt;Google&#39;s Machine Learning Course for text classification&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Since our dataset has imbalanced classes (as with many real-world datasets), so it might be worth looking into the &lt;a href=&#34;https://www.tensorflow.org/tutorials/structured_data/imbalanced_data&#34;&gt;TensorFlow guide for different methods to training a model with imbalanced classes&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;üõ† 10. Time series fundamentals and Milestone Project 3: BitPredict üí∞üìà Exercises&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Does scaling the data help for univariate/multivariate data? (e.g. getting all of the values between 0 &amp;amp; 1)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Try doing this for a univariate model (e.g. &lt;code&gt;model_1&lt;/code&gt;) and a multivariate model (e.g. &lt;code&gt;model_6&lt;/code&gt;) and see if it effects model training or evaluation results.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Get the most up to date data on Bitcoin, train a model &amp;amp; see how it goes (our data goes up to May 18 2021).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can download the Bitcoin historical data for free from &lt;a href=&#34;https://www.coindesk.com/price/bitcoin&#34;&gt;coindesk.com/price/bitcoin&lt;/a&gt; and clicking &#34;Export Data&#34; -&amp;gt; &#34;CSV&#34;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;For most of our models we used &lt;code&gt;WINDOW_SIZE=7&lt;/code&gt;, but is there a better window size?&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Setup a series of experiments to find whether or not there&#39;s a better window size.&lt;/li&gt; &#xA; &lt;li&gt;For example, you might train 10 different models with &lt;code&gt;HORIZON=1&lt;/code&gt; but with window sizes ranging from 2-12.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Create a windowed dataset just like the ones we used for &lt;code&gt;model_1&lt;/code&gt; using &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/timeseries_dataset_from_array&#34;&gt;&lt;code&gt;tf.keras.preprocessing.timeseries_dataset_from_array()&lt;/code&gt;&lt;/a&gt; and retrain &lt;code&gt;model_1&lt;/code&gt; using the recreated dataset.&lt;/li&gt; &#xA; &lt;li&gt;For our multivariate modelling experiment, we added the Bitcoin block reward size as an extra feature to make our time series multivariate.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Are there any other features you think you could add?&lt;/li&gt; &#xA; &lt;li&gt;If so, try it out, how do these affect the model?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;6&#34;&gt; &#xA; &lt;li&gt;Make prediction intervals for future forecasts. To do so, one way would be to train an ensemble model on all of the data, make future forecasts with it and calculate the prediction intervals of the ensemble just like we did for &lt;code&gt;model_8&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For future predictions, try to make a prediction, retrain a model on the predictions, make a prediction, retrain a model, make a prediction, retrain a model, make a prediction (retrain a model each time a new prediction is made). Plot the results, how do they look compared to the future predictions where a model wasn&#39;t retrained for every forecast (&lt;code&gt;model_9&lt;/code&gt;)?&lt;/li&gt; &#xA; &lt;li&gt;Throughout this notebook, we&#39;ve only tried algorithms we&#39;ve handcrafted ourselves. But it&#39;s worth seeing how a purpose built forecasting algorithm goes.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Try out one of the extra algorithms listed in the modelling experiments part such as: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/Kats&#34;&gt;Facebook&#39;s Kats library&lt;/a&gt; - there are many models in here, remember the machine learning practioner&#39;s motto: experiment, experiment, experiment.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/linkedin/greykite&#34;&gt;LinkedIn&#39;s Greykite library&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;üìñ 10. Time series fundamentals and Milestone Project 3: BitPredict üí∞üìà Extra-curriculum&lt;/h3&gt; &#xA;&lt;p&gt;We&#39;ve only really scratched the surface with time series forecasting and time series modelling in general. But the good news is, you&#39;ve got plenty of hands-on coding experience with it already.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;d like to dig deeper in to the world of time series, I&#39;d recommend the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://otexts.com/fpp3/&#34;&gt;Forecasting: Principles and Practice&lt;/a&gt; is an outstanding online textbook which discusses at length many of the most important concepts in time series forecasting. I&#39;d especially recommend reading at least Chapter 1 in full. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;I&#39;d definitely recommend at least checking out chapter 1 as well as the chapter on forecasting accuracy measures.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;üé• &lt;a href=&#34;https://youtu.be/wqQKFu41FIw&#34;&gt;Introduction to machine learning and time series&lt;/a&gt; by Markus Loning goes through different time series problems and how to approach them. It focuses on using the &lt;code&gt;sktime&lt;/code&gt; library (Scikit-Learn for time series), though the principles are applicable elsewhere.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/why-you-should-care-about-the-nate-silver-vs-nassim-taleb-twitter-war-a581dce1f5fc&#34;&gt;&lt;em&gt;Why you should care about the Nate Silver vs. Nassim Taleb Twitter war&lt;/em&gt;&lt;/a&gt; by Isaac Faber is an outstanding discussion insight into the role of uncertainty in the example of election prediction.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org/tutorials/structured_data/time_series&#34;&gt;TensorFlow time series tutorial&lt;/a&gt; - A tutorial on using TensorFlow to forecast weather time series data with TensorFlow.&lt;/li&gt; &#xA; &lt;li&gt;üìï &lt;a href=&#34;https://en.wikipedia.org/wiki/The_Black_Swan:_The_Impact_of_the_Highly_Improbable&#34;&gt;&lt;em&gt;The Black Swan&lt;/em&gt;&lt;/a&gt; by Nassim Nicholas Taleb - Nassim Taleb was a pit trader (a trader who trades on their own behalf) for 25 years, this book compiles many of the lessons he learned from first-hand experience. It changed my whole perspective on our ability to predict.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/3-facts-about-time-series-forecasting-that-surprise-experienced-machine-learning-practitioners-69c18ee89387&#34;&gt;&lt;em&gt;3 facts about time series forecasting that surprise experienced machine learning practitioners&lt;/em&gt;&lt;/a&gt; by Skander Hannachi, Ph.D - time series data is different to other kinds of data, if you&#39;ve worked on other kinds of machine learning problems before, getting into time series might require some adjustments, Hannachi outlines 3 of the most common.&lt;/li&gt; &#xA; &lt;li&gt;üé• World-class lectures by Jordan Kern, watching these will take you from 0 to 1 with time series problems: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://youtu.be/Prpu_U5tKkE&#34;&gt;Time Series Analysis&lt;/a&gt; - how to analyse time series data.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=s3XH7fTHMb4&#34;&gt;Time Series Modelling&lt;/a&gt; - different techniques for modelling time series data (many of which aren&#39;t deep learning).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;üõ† 11. Passing the TensorFlow Developer Certification Exercises&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Preparing your brain&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Read through the &lt;a href=&#34;https://www.tensorflow.org/extras/cert/TF_Certificate_Candidate_Handbook.pdf&#34;&gt;TensorFlow Developer Certificate Candidate Handbook&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Go through the Skills checklist section of the TensorFlow Developer Certification Candidate Handbook and create a notebook which covers all of the skills required, write code for each of these (this notebook can be used as a point of reference during the exam).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/11-map-the-skills-checklist-to-a-notebook.png&#34; alt=&#34;mapping the TensorFlow Developer handbook to code in a notebook&#34;&gt; &lt;em&gt;Example of mapping the Skills checklist section of the TensorFlow Developer Certification Candidate handbook to a notebook.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prearing your computer&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Go through the &lt;a href=&#34;https://www.jetbrains.com/pycharm/learning-center/&#34;&gt;PyCharm quick start&lt;/a&gt; tutorials to make sure you&#39;re familiar with PyCharm (the exam uses PyCharm, you can download the free version).&lt;/li&gt; &#xA; &lt;li&gt;Read through and follow the suggested steps in the &lt;a href=&#34;https://www.tensorflow.org/extras/cert/Setting_Up_TF_Developer_Certificate_Exam.pdf&#34;&gt;setting up for the TensorFlow Developer Certificate Exam guide&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;After going through (2), go into PyCharm and make sure you can train a model in TensorFlow. The model and dataset in the example &lt;code&gt;image_classification_test.py&lt;/code&gt; &lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/extras/image_classification_test.py&#34;&gt;script on GitHub&lt;/a&gt; should be enough. If you can train and save the model in under 5-10 minutes, your computer will be powerful enough to train the models in the exam. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Make sure you&#39;ve got experience running models locally in PyCharm before taking the exam. Google Colab (what we used through the course) is a little different to PyCharm.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/11-getting-example-script-to-run-in-pycharm.png&#34; alt=&#34;before taking the TensorFlow Developer certification exam, make sure you can run TensorFlow code in PyCharm on your local machine&#34;&gt; &lt;em&gt;Before taking the exam make sure you can run TensorFlow code on your local machine in PyCharm. If the &lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/extras/image_classification_test.py&#34;&gt;example &lt;code&gt;image_class_test.py&lt;/code&gt; script&lt;/a&gt; can run completely in under 5-10 minutes on your local machine, your local machine can handle the exam (if not, you can use Google Colab to train, save and download models to submit for the exam).&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;üìñ 11. Passing the TensorFlow Developer Certification Extra-curriculum&lt;/h3&gt; &#xA;&lt;p&gt;If you&#39;d like some extra materials to go through to further your skills with TensorFlow and deep learning in general or to prepare more for the exam, I&#39;d highly recommend the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üìÑ &lt;strong&gt;Read:&lt;/strong&gt; &lt;a href=&#34;https://www.mrdbourke.com/how-i-got-tensorflow-developer-certified/&#34;&gt;How I got TensorFlow Developer Certified (and how you can too)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üé• &lt;strong&gt;Watch:&lt;/strong&gt; &lt;a href=&#34;https://youtu.be/ya5NwvKafDk&#34;&gt;How I passed the TensorFlow Developer Certification exam (and how you can too)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Go through the &lt;a href=&#34;https://dbourke.link/tfinpractice&#34;&gt;TensorFlow in Practice Specialization on Coursera&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Read through the second half of &lt;a href=&#34;https://amzn.to/3aYexF2&#34;&gt;Hands-On Machine Learning with Scikit-Learn, Keras &amp;amp; TensorFlow 2nd Edition&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What this course is missing&lt;/h2&gt; &#xA;&lt;p&gt;Deep learning is a broad topic. So this course doesn&#39;t cover it all.&lt;/p&gt; &#xA;&lt;p&gt;Here are some of the main topics you might want to look into next:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Transformers (the neural network architecture taking the NLP world by storm)&lt;/li&gt; &#xA; &lt;li&gt;Multi-modal models (models which use more than one data source such as text &amp;amp; images)&lt;/li&gt; &#xA; &lt;li&gt;Reinforcement learning&lt;/li&gt; &#xA; &lt;li&gt;Unsupervised learning&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Extensions (possible places to go after the course)&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://neuralnetworksanddeeplearning.com/&#34;&gt;Neural Networks and Deep Learning Book&lt;/a&gt; by Michael Nielsen - If the Zero to Mastery TensorFlow for Deep Learning course is top down, this book is bottom up. A fantastic resource to sandwich your knowledge.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.deeplearning.ai&#34;&gt;Deeplearning.AI specializations&lt;/a&gt; - The ZTM TensorFLow course focuses on code-first, the deeplearning.ai specializations will teach you what&#39;s going on behind the code.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/&#34;&gt;Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow Book&lt;/a&gt; (especially the 2nd half) - Many of the materials in this course were inspired by and guided by the pages of this beautiful text book.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://fullstackdeeplearning.com&#34;&gt;Full Stack Deep Learning&lt;/a&gt; - Learn how to turn your models into machine learning-powered applications.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://madewithml.com/#mlops&#34;&gt;Made with ML MLOps materials&lt;/a&gt; - Similar to Full Stack Deep Learning but comprised into many small lessons around all the pieces of the puzzle (data collection, labelling, deployment and more) required to build a full-stack machine learning-powered application.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.fast.ai&#34;&gt;fast.ai Curriculum&lt;/a&gt; - One of the best (and free) AI/deep learning courses online. Enough said.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.mrdbourke.com/how-can-a-beginner-data-scientist-like-me-gain-experience/&#34;&gt;&#34;How does a beginner data scientist like me gain experience?&#34;&lt;/a&gt; by Daniel Bourke - Read this on how to get experience for a job after studying online/at unveristy (start the job before you have it).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Ask questions&lt;/h2&gt; &#xA;&lt;p&gt;Contact &lt;a href=&#34;mailto:daniel@mrdbourke.com&#34;&gt;Daniel Bourke&lt;/a&gt; or &lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/discussions&#34;&gt;add a discussion&lt;/a&gt; (preferred).&lt;/p&gt; &#xA;&lt;h2&gt;Status&lt;/h2&gt; &#xA;&lt;p&gt;As of: 12 May 2023&lt;/p&gt; &#xA;&lt;p&gt;Course completed! üï∫&lt;/p&gt; &#xA;&lt;p&gt;Any further updates/changes will be added below.&lt;/p&gt; &#xA;&lt;h2&gt;Log&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;12 May 2023 - update several course notebooks for latest version of TensorFlow, several API updates for Notebook 05 here: &lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/discussions/547&#34;&gt;https://github.com/mrdbourke/tensorflow-deep-learning/discussions/547&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;02 Dec 2021 - add fix for TensorFlow 2.7 to notebook 02&lt;/li&gt; &#xA; &lt;li&gt;11 Nov 2021 - add fix for TensorFlow 2.7 to notebook 01&lt;/li&gt; &#xA; &lt;li&gt;14 Aug 2021 - added a discussion with TensorFlow 2.6 updates and EfficientNetV2 notes: &lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/discussions/166&#34;&gt;https://github.com/mrdbourke/tensorflow-deep-learning/discussions/166&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;16 Jul 2021 - added 35 videos to ZTM Academy + Udemy versions of the course for time series and how to pass TensorFlow Developer Certification&lt;/li&gt; &#xA; &lt;li&gt;10 Jul 2021 - added 29 edited time series videos to ZTM Academy + Udemy versions of the course, more to come soon&lt;/li&gt; &#xA; &lt;li&gt;07 Jul 2021 - recorded 5 videos for passing TensorFlow Developer Certification exam section - ALL VIDEOS FOR COURSE DONE!!! time to edit/upload! üéâ&lt;/li&gt; &#xA; &lt;li&gt;06 Jul 2021 - added guide to TensorFlow Certification Exam: &lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/11_passing_the_tensorflow_developer_certification_exam.md&#34;&gt;https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/11_passing_the_tensorflow_developer_certification_exam.md&lt;/a&gt; - going to record videos for it tomorrow&lt;/li&gt; &#xA; &lt;li&gt;05 Jul 2021 - making materials for TF certification exam (what/why/how)&lt;/li&gt; &#xA; &lt;li&gt;02 Jul 2021 - FINISHED RECORDING VIDEOS FOR TIME SERIES SECTION!!!!! time to upload&lt;/li&gt; &#xA; &lt;li&gt;30 Jun 2021 - recorded 12 videos for time series section, total heading past 60 (the biggest section yet), nearly done!!!&lt;/li&gt; &#xA; &lt;li&gt;29 Jun 2021 - recorded 10 videos for time series section, total heading towards 60&lt;/li&gt; &#xA; &lt;li&gt;28 Jun 2021 - recorded 10 videos for time series section, the line below says 40 videos total, actually more like 50&lt;/li&gt; &#xA; &lt;li&gt;26 Jun 2021 - recorded 4 videos for time series section, looks like it&#39;ll be about 40 videos total&lt;/li&gt; &#xA; &lt;li&gt;25 Jun 2021 - recorded 8 videos for time series section + fixed a bunch of typos in time series notebook&lt;/li&gt; &#xA; &lt;li&gt;24 Jun 2021 - recorded 14 videos for time series section, more to come tomorrow&lt;/li&gt; &#xA; &lt;li&gt;23 Jun 2021 - finished adding images to time series notebook, now to start video recording&lt;/li&gt; &#xA; &lt;li&gt;22 Jun 2021 - added a bunch of images to the time series notebook/started making slides&lt;/li&gt; &#xA; &lt;li&gt;21 Jun 2021 - code for time series notebook is done, now creating slides/images to prepare for recording&lt;/li&gt; &#xA; &lt;li&gt;19 Jun 2021 - turned curriculum into an online book, you can read it here: &lt;a href=&#34;https://dev.mrdbourke.com/tensorflow-deep-learning/&#34;&gt;https://dev.mrdbourke.com/tensorflow-deep-learning/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;18 Jun 2021 - add exercises/extra-curriculum/outline to time series notebook&lt;/li&gt; &#xA; &lt;li&gt;17 Jun 2021 - add annotations for turkey problem and model comparison in time series notebook, next is outline/images&lt;/li&gt; &#xA; &lt;li&gt;16 Jun 2021 - add annotations for uncertainty and future predictions in time series notebook, next is turkey problem&lt;/li&gt; &#xA; &lt;li&gt;14 Jun 2021 - add annotations for ensembling, begin on prediction intervals&lt;/li&gt; &#xA; &lt;li&gt;10 Jun 2021 - finished annotations for N-BEATS algorithm, now onto ensembling/prediction intervals&lt;/li&gt; &#xA; &lt;li&gt;9 Jun 2021 - add annotations for N-BEATS algorithm implementation for time series notebook&lt;/li&gt; &#xA; &lt;li&gt;8 Jun 2021 - add annotations to time series notebook, all will be finished by end of week (failed)&lt;/li&gt; &#xA; &lt;li&gt;4 Jun 2021 - more annotation updates to time series notebook, brick by brick!&lt;/li&gt; &#xA; &lt;li&gt;3 Jun 2021 - added a bunch of annotations/explanations to time series notebook, momentum building, plenty more to come!&lt;/li&gt; &#xA; &lt;li&gt;2 Jun 2021 - started adding annotations explaining the code + resources to learn more, will continue for next few days&lt;/li&gt; &#xA; &lt;li&gt;1 Jun 2021 - added turkey problem to time series notebook, cleaned up a bunch of code, draft code is ready, now to write annotations/explanations&lt;/li&gt; &#xA; &lt;li&gt;28 May 2021 - added future forecasts, added ensemble model, added prediction intervals to time series notebook&lt;/li&gt; &#xA; &lt;li&gt;25 May 2021 - added multivariate time series to time series notebook, fix LSTM model, next we add TensorFlow windowing/experimenting with window sizes&lt;/li&gt; &#xA; &lt;li&gt;24 May 2021 - fixed broken preprocessing function in time series notebook, LSTM model is broken, more material to come&lt;/li&gt; &#xA; &lt;li&gt;20 May 2021 - more time series material creation&lt;/li&gt; &#xA; &lt;li&gt;19 May 2021 - more time series material creation, streaming much of it live on Twitch - &lt;a href=&#34;https://twitch.tv/mrdbourke&#34;&gt;https://twitch.tv/mrdbourke&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;18 May 2021 - added time series forecasting notebook outline (&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/10_time_series_forecasting_in_tensorflow.ipynb&#34;&gt;notebook 10&lt;/a&gt;), going to really start ramping up the materials here&lt;/li&gt; &#xA; &lt;li&gt;12 May 2021 - all videos for 09 have now been released on Udemy &amp;amp; ZTM!!! enjoy build SkimLit üìÑüî•&lt;/li&gt; &#xA; &lt;li&gt;11 May 2021 - 40+ section 08 &amp;amp; 09 videos released on Udemy &amp;amp; ZTM!!!&lt;/li&gt; &#xA; &lt;li&gt;10 May 2021 - time series materials research + preparation&lt;/li&gt; &#xA; &lt;li&gt;08 May 2021 - time series materials research + preparation&lt;/li&gt; &#xA; &lt;li&gt;05 May 2021 - ~20+ videos edited for 08, ~10+ videos edited for 09, time series materials in 1st draft mode&lt;/li&gt; &#xA; &lt;li&gt;04 May 2021 - fixed the remaining videos for 08 (audio missing), now onto making time series materials!&lt;/li&gt; &#xA; &lt;li&gt;03 May 2021 - rerecorded 10 videos for 08 fixing the sound isse, these are going straight to editing and should be uploaded by end of week&lt;/li&gt; &#xA; &lt;li&gt;02 May 2021 - found an issue with videos 09-20 of section 08 (no audio), going to rerecord them&lt;/li&gt; &#xA; &lt;li&gt;29 Apr 2021 - üöÄüöÄüöÄ launched on Udemy!!! üöÄüöÄüöÄ&lt;/li&gt; &#xA; &lt;li&gt;22 Apr 2021 - finished recording videos for 09! added slides and video notebook 09&lt;/li&gt; &#xA; &lt;li&gt;21 Apr 2021 - recorded 14 videos for 09! biggggg day of recording! getting closer to finishing 09&lt;/li&gt; &#xA; &lt;li&gt;20 Apr 2021 - recorded 10 videos for 09&lt;/li&gt; &#xA; &lt;li&gt;19 Apr 2021 - recorded 9 videos for 09&lt;/li&gt; &#xA; &lt;li&gt;16 Apr 2021 - slides done for 09, ready to start recording!&lt;/li&gt; &#xA; &lt;li&gt;15 Apr 2021 - added slides, extra-curriculum, exercises and video notebook for 08, started making slides for 09, will finish tomorrow&lt;/li&gt; &#xA; &lt;li&gt;14 Apr 2021 - recorded 12 videos for notebook 08, finished the section! time to make slides for 09 and get into it&lt;/li&gt; &#xA; &lt;li&gt;10 Apr 2021 - recorded 4 videos for notebook 08&lt;/li&gt; &#xA; &lt;li&gt;9 Apr 2021 - recorded 6 videos for notebook 08&lt;/li&gt; &#xA; &lt;li&gt;8 Apr 2021 - recorded 10 videos for notebook 08! more coming tomorrow! home stretch baby!!!&lt;/li&gt; &#xA; &lt;li&gt;7 Apr 2021 - added a whole bunch of images to notebook 08, getting ready for recording tomorrow!&lt;/li&gt; &#xA; &lt;li&gt;1 Apr 2021 - added &lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/09_SkimLit_nlp_milestone_project_2.ipynb&#34;&gt;notebook 09: SkimLit&lt;/a&gt;, almost finished, a little cleaning and we&#39;ll be ready for slide making!&lt;/li&gt; &#xA; &lt;li&gt;31 Mar 2021 - added notebook 08, going to finish tomorrow, then onto 09!&lt;/li&gt; &#xA; &lt;li&gt;24 Mar 2021 - Recorded 8 videos for 07, finished! onto materials (slides/notebooks) for 08, 09&lt;/li&gt; &#xA; &lt;li&gt;23 Mar 2021 - Recorded 6 videos for 07 (finally), going to finish tomorrow&lt;/li&gt; &#xA; &lt;li&gt;22 Mar 2021 - Polished notebook 07 ready for recording, made slides for 07, added template for 07 (for a student to go through and practice), ready to record!&lt;/li&gt; &#xA; &lt;li&gt;17 Mar 2021 - 99% finished notebook 07, added links to first 14 hours of the course on YouTube (&lt;a href=&#34;https://youtu.be/tpCFfeUEGs8&#34;&gt;10 hours in part 1&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/ZUKz4125WNI&#34;&gt;4 hours in part 2&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;11 Mar 2021 - added even more text annotations to notebook 07, finishing tomorrow, then slides&lt;/li&gt; &#xA; &lt;li&gt;10 Mar 2021 - Typed a whole bunch of explanations into notebook 07, continuing tomorrow&lt;/li&gt; &#xA; &lt;li&gt;09 Mar 2021 - fixed plenty of code in notebook 07, should run end to end very cleanly (though loading times are still a thing)&lt;/li&gt; &#xA; &lt;li&gt;05 Mar 2021 - added draft notebook 07 (heaps of data loading and model training improvements in this one!), gonna fix up over next few days&lt;/li&gt; &#xA; &lt;li&gt;01 Mar 2021 - Added slides for 06 (&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/slides/06_transfer_learning_with_tensorflow_part_3_scaling_up.pdf&#34;&gt;see them here&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;26 Feb 2021 - üöÄ LAUNCHED!!!!! also finished recording videos for 06, onto 07, 08, 09 for next release&lt;/li&gt; &#xA; &lt;li&gt;24 Feb 2021 - recorded 9 videos for section 06, launch inbound!!!&lt;/li&gt; &#xA; &lt;li&gt;23 Feb 2021 - rearranged GitHub in preparation for launch üöÄ&lt;/li&gt; &#xA; &lt;li&gt;18 Feb 2021 - recorded 8 videos for 05 and... it&#39;s done! onto polishing the GitHub&lt;/li&gt; &#xA; &lt;li&gt;17 Feb 2021 - recorded 10 videos for 05! going to finish tomorrow üöÄ&lt;/li&gt; &#xA; &lt;li&gt;16 Feb 2021 - polished slides for 05 and started recording videos, got 7 videos done for 05&lt;/li&gt; &#xA; &lt;li&gt;15 Feb 2021 - finished videos for 04, now preparing to record for 05!&lt;/li&gt; &#xA; &lt;li&gt;12 Feb 2021 - recored 7 videos for section 04... wanted 10 but we&#39;ll take 7 (ü§î this seems to have happened before)&lt;/li&gt; &#xA; &lt;li&gt;11 Feb 2021 - NO PROGRESS - gave a Machine Learning deployment tutorial for &lt;a href=&#34;https://stanford-cs329s.github.io/syllabus.html&#34;&gt;Stanford&#39;s CS329s&lt;/a&gt; (using the model code from this course!!!) - &lt;a href=&#34;https://github.com/mrdbourke/cs329s-ml-deployment-tutorial&#34;&gt;see the full tutorial materials&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;08 Feb 2021 - recorded 10 videos for section 03... and section 03 is done! üöÄ onto section 04&lt;/li&gt; &#xA; &lt;li&gt;30 Jan 2021 - 07 Feb 2021: NO PROGRESS (working on a ML deployment lecture for &lt;a href=&#34;https://stanford-cs329s.github.io/syllabus.html&#34;&gt;Stanford&#39;s CS329s&lt;/a&gt;... more on this later)&lt;/li&gt; &#xA; &lt;li&gt;29 Jan 2021 - recorded 9 videos for section 03... closer to 10 than yesterday but still not there&lt;/li&gt; &#xA; &lt;li&gt;28 Jan 2021 - recorded 7 videos for section 03... wanted 10 but we&#39;ll take 7&lt;/li&gt; &#xA; &lt;li&gt;27 Jan 2021 - recorded 10 videos for section 03&lt;/li&gt; &#xA; &lt;li&gt;26 Jan 2021 - polished GitHub README (what you&#39;re looking at) with a &lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning#course-materials&#34;&gt;nice table&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;23 Jan 2021 - finished slides of 06&lt;/li&gt; &#xA; &lt;li&gt;22 Jan 2021 - finished review of notebook 06 &amp;amp; started slides of 06&lt;/li&gt; &#xA; &lt;li&gt;21 Jan 2021 - finished slides for 05 &amp;amp; started review of 06&lt;/li&gt; &#xA; &lt;li&gt;20 Jan 2021 - finished notebook 05 &amp;amp; 95% slides for 05&lt;/li&gt; &#xA; &lt;li&gt;19 Jan 2021 - found a storage idea for data during course (use Google Storage in same region as Colab Notebooks, cheapest/fastest)&lt;/li&gt; &#xA; &lt;li&gt;18 Jan 2021 - reviewed notebook 05 &amp;amp; slides for 05&lt;/li&gt; &#xA; &lt;li&gt;17 Jan 2021 - finished notebook 04 &amp;amp; slides for 04&lt;/li&gt; &#xA; &lt;li&gt;16 Jan 2021 - review notebook 04 &amp;amp; made slides for transfer learning&lt;/li&gt; &#xA; &lt;li&gt;13 Jan 2021 - review notebook 03 again &amp;amp; finished slides for 03, BIGGGGG updates to the README, notebook 03 99% done, just need to figure out optimum way to transfer data (e.g. when a student downloads it, where&#39;s best to store it in the meantime? Dropbox? S3? &lt;del&gt;GS&lt;/del&gt; (too expensive)&lt;/li&gt; &#xA; &lt;li&gt;11 Jan 2021 - reviewed notebook 03, 95% ready for recording, onto slides for 03&lt;/li&gt; &#xA; &lt;li&gt;9 Jan 2021 - I&#39;m back baby! Finished all videos for 02, now onto slides/materials for 03, 04, 05 (then I&#39;ll get back in the lab)&lt;/li&gt; &#xA; &lt;li&gt;19 Dec 2020 - ON HOLD (family holiday until Jan 02 2021)&lt;/li&gt; &#xA; &lt;li&gt;18 Dec 2020 - recorded 75% of videos for 02&lt;/li&gt; &#xA; &lt;li&gt;17 Dec 2020 - recorded 50% of videos for 02&lt;/li&gt; &#xA; &lt;li&gt;16 Dec 2020 - recorded 100% of videos for 01&lt;/li&gt; &#xA; &lt;li&gt;15 Dec 2020 - recorded 90% of videos for 01&lt;/li&gt; &#xA; &lt;li&gt;09 Dec 2020 - finished recording videos for 00&lt;/li&gt; &#xA; &lt;li&gt;08 Dec 2020 - recorded 90% of videos for 00&lt;/li&gt; &#xA; &lt;li&gt;05 Dec 2020 - trialled recording studio for ~6 videos with notebook 00 material&lt;/li&gt; &#xA; &lt;li&gt;04 Dec 2020 - setup &lt;a href=&#34;https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/misc-studio-setup.jpeg&#34;&gt;recording studio in closet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;03 Dec 2020 - finished notebook 02, finished slides for 02, time to setup recording studio&lt;/li&gt; &#xA; &lt;li&gt;02 Dec 2020 - notebook 02 95% done, slides for 02 90% done&lt;/li&gt; &#xA; &lt;li&gt;01 Dec 2020 - added notebook 02 (90% polished), start preparing slides for 02&lt;/li&gt; &#xA; &lt;li&gt;27 Nov 2020 - polished notebook 01, made slides for notebook 01&lt;/li&gt; &#xA; &lt;li&gt;26 Nov 2020 - polished notebook 00, made slides for notebook 00&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>PWhiddy/PokemonRedExperiments</title>
    <updated>2024-12-31T01:51:48Z</updated>
    <id>tag:github.com,2024-12-31:/PWhiddy/PokemonRedExperiments</id>
    <link href="https://github.com/PWhiddy/PokemonRedExperiments" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Playing Pokemon Red with Reinforcement Learning&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Train RL agents to play Pokemon Red&lt;/h1&gt; &#xA;&lt;h2&gt;Watch the Video on Youtube!&lt;/h2&gt; &#xA;&lt;p float=&#34;left&#34;&gt; &lt;a href=&#34;https://youtu.be/DcYLT37ImBY&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/PWhiddy/PokemonRedExperiments/master/assets/youtube.jpg?raw=true&#34; height=&#34;192&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://youtu.be/DcYLT37ImBY&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/PWhiddy/PokemonRedExperiments/master/assets/poke_map.gif?raw=true&#34; height=&#34;192&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Join the discord server&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://discord.gg/RvadteZk4G&#34;&gt;&lt;img src=&#34;https://invidget.switchblade.xyz/RvadteZk4G&#34; alt=&#34;Join the Discord server!&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Running the Pretrained Model Interactively üéÆ&lt;/h2&gt; &#xA;&lt;p&gt;üêç Python 3.10 is recommended. Other versions may work but have not been tested.&lt;br&gt; You also need to install ffmpeg and have it available in the command line.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Copy your legally obtained Pokemon Red ROM into the base directory. You can find this using google, it should be 1MB. Rename it to &lt;code&gt;PokemonRed.gb&lt;/code&gt; if it is not already. The sha1 sum should be &lt;code&gt;ea9bcae617fdf159b045185467ae58b2e4a48b9a&lt;/code&gt;, which you can verify by running &lt;code&gt;shasum PokemonRed.gb&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Move into the &lt;code&gt;baselines/&lt;/code&gt; directory:&lt;br&gt; &lt;code&gt;cd baselines&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Install dependencies:&lt;br&gt; &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;br&gt; It may be necessary in some cases to separately install the SDL libraries.&lt;/li&gt; &#xA; &lt;li&gt;Run:&lt;br&gt; &lt;code&gt;python run_pretrained_interactive.py&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Interact with the emulator using the arrow keys and the &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;s&lt;/code&gt; keys (A and B buttons).&lt;br&gt; You can pause the AI&#39;s input during the game by editing &lt;code&gt;agent_enabled.txt&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Note: the Pokemon.gb file MUST be in the main directory and your current directory MUST be the &lt;code&gt;baselines/&lt;/code&gt; directory in order for this to work.&lt;/p&gt; &#xA;&lt;h2&gt;Training the Model üèãÔ∏è&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/PWhiddy/PokemonRedExperiments/master/assets/grid.png?raw=true&#34; height=&#34;156&#34;&gt; &#xA;&lt;h3&gt;10-21-23: Updated Version!&lt;/h3&gt; &#xA;&lt;p&gt;This version still needs some tuning, but it can clear the first gym in a small fraction of the time and compute resources. It can work with as few as 16 cores and ~20G of RAM. This is the place for active development and updates!&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Previous steps 1-3&lt;/li&gt; &#xA; &lt;li&gt;Run:&lt;br&gt; &lt;code&gt;python run_baseline_parallel_fast.py&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Tracking Training Progress üìà&lt;/h2&gt; &#xA;&lt;p&gt;The current state of each game is rendered to images in the session directory.&lt;br&gt; You can track the progress in tensorboard by moving into the session directory and running:&lt;br&gt; &lt;code&gt;tensorboard --logdir .&lt;/code&gt;&lt;br&gt; You can then navigate to &lt;code&gt;localhost:6006&lt;/code&gt; in your browser to view metrics.&lt;br&gt; To enable wandb integration, change &lt;code&gt;use_wandb_logging&lt;/code&gt; in the training script to &lt;code&gt;True&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Extra üêú&lt;/h2&gt; &#xA;&lt;p&gt;Map visualization code can be found in &lt;code&gt;visualization/&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;h2&gt;Supporting Libraries&lt;/h2&gt; &#xA;&lt;p&gt;Check out these awesome projects!&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/Baekalfen/PyBoy&#34;&gt;PyBoy&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;a href=&#34;https://github.com/Baekalfen/PyBoy&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/PWhiddy/PokemonRedExperiments/master/assets/pyboy.svg?sanitize=true&#34; height=&#34;64&#34;&gt; &lt;/a&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/DLR-RM/stable-baselines3&#34;&gt;Stable Baselines 3&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;a href=&#34;https://github.com/DLR-RM/stable-baselines3&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/PWhiddy/PokemonRedExperiments/master/assets/sblogo.png&#34; height=&#34;64&#34;&gt; &lt;/a&gt;</summary>
  </entry>
</feed>