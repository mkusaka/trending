<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-30T01:59:10Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>blue-yonder/tsfresh</title>
    <updated>2023-07-30T01:59:10Z</updated>
    <id>tag:github.com,2023-07-30:/blue-yonder/tsfresh</id>
    <link href="https://github.com/blue-yonder/tsfresh" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Automatic extraction of relevant features from time series:&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img width=&#34;70%&#34; src=&#34;https://raw.githubusercontent.com/blue-yonder/tsfresh/main/docs/images/tsfresh_logo.svg?sanitize=true&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;tsfresh&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://tsfresh.readthedocs.io/en/latest/?badge=latest&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/tsfresh/badge/?version=latest&#34; alt=&#34;Documentation Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/blue-yonder/tsfresh/actions&#34;&gt;&lt;img src=&#34;https://github.com/blue-yonder/tsfresh/workflows/Test%20Default%20Branch/badge.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/blue-yonder/tsfresh&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/blue-yonder/tsfresh/branch/main/graph/badge.svg?sanitize=true&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/blue-yonder/tsfresh/raw/main/LICENSE.txt&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/mashape/apistatus.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://mybinder.org/v2/gh/blue-yonder/tsfresh/main?filepath=notebooks&#34;&gt;&lt;img src=&#34;https://mybinder.org/badge.svg?sanitize=true&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/tsfresh&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/tsfresh&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository contains the &lt;em&gt;TSFRESH&lt;/em&gt; python package. The abbreviation stands for&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&#34;Time Series Feature extraction based on scalable hypothesis tests&#34;&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The package provides systematic time-series feature extraction by combining established algorithms from statistics, time-series analysis, signal processing, and nonlinear dynamics with a robust feature selection algorithm. In this context, the term &lt;em&gt;time-series&lt;/em&gt; is interpreted in the broadest possible sense, such that any types of sampled data or even event sequences can be characterised.&lt;/p&gt; &#xA;&lt;h2&gt;Spend less time on feature engineering&lt;/h2&gt; &#xA;&lt;p&gt;Data Scientists often spend most of their time either cleaning data or building features. While we cannot change the first thing, the second can be automated. &lt;em&gt;TSFRESH&lt;/em&gt; frees your time spent on building features by extracting them automatically. Hence, you have more time to study the newest deep learning paper, read hacker news or build better models.&lt;/p&gt; &#xA;&lt;h2&gt;Automatic extraction of 100s of features&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;TSFRESH&lt;/em&gt; automatically extracts 100s of features from time series. Those features describe basic characteristics of the time series such as the number of peaks, the average or maximal value or more complex features such as the time reversal symmetry statistic.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/blue-yonder/tsfresh/main/docs/images/introduction_ts_exa_features.png&#34; alt=&#34;The features extracted from a exemplary time series&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The set of features can then be used to construct statistical or machine learning models on the time series to be used for example in regression or classification tasks.&lt;/p&gt; &#xA;&lt;h2&gt;Forget irrelevant features&lt;/h2&gt; &#xA;&lt;p&gt;Time series often contain noise, redundancies or irrelevant information. As a result most of the extracted features will not be useful for the machine learning task at hand.&lt;/p&gt; &#xA;&lt;p&gt;To avoid extracting irrelevant features, the &lt;em&gt;TSFRESH&lt;/em&gt; package has a built-in filtering procedure. This filtering procedure evaluates the explaining power and importance of each characteristic for the regression or classification tasks at hand.&lt;/p&gt; &#xA;&lt;p&gt;It is based on the well developed theory of hypothesis testing and uses a multiple test procedure. As a result the filtering process mathematically controls the percentage of irrelevant extracted features.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;em&gt;TSFRESH&lt;/em&gt; package is described in the following open access paper:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Christ, M., Braun, N., Neuffer, J., and Kempa-Liehr A.W. (2018). &lt;em&gt;Time Series FeatuRe Extraction on basis of Scalable Hypothesis tests (tsfresh -- A Python package).&lt;/em&gt; Neurocomputing 307, p. 72-77, &lt;a href=&#34;https://doi.org/10.1016/j.neucom.2018.03.067&#34;&gt;doi: 10.1016/j.neucom.2018.03.067&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The FRESH algorithm is described in the following whitepaper:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Christ, M., Kempa-Liehr, A.W., and Feindt, M. (2017). &lt;em&gt;Distributed and parallel time series feature extraction for industrial big data applications.&lt;/em&gt; ArXiv e-print 1610.07717, &lt;a href=&#34;https://arxiv.org/abs/1610.07717&#34;&gt;https://arxiv.org/abs/1610.07717&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Systematic time-series feature extraction even works for unsupervised problems:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Teh, H.Y., Wang, K.I-K., Kempa-Liehr, A.W. (2021). &lt;em&gt;Expect the Unexpected: Unsupervised feature selection for automated sensor anomaly detection.&lt;/em&gt; IEEE Sensors Journal 15.16, p. 18033-18046, &lt;a href=&#34;https://doi.org/10.1109/JSEN.2021.3084970&#34;&gt;doi: 10.1109/JSEN.2021.3084970&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Due to the fact that tsfresh basically provides time-series feature extraction for free, you can now concentrate on engineering new time-series, like e.g. differences of signals from synchronous measurements, which provide even better time-series features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Kempa-Liehr, A.W., Oram, J., Wong, A., Finch, M., Besier, T. (2020). &lt;em&gt;Feature engineering workflow for activity recognition from synchronized inertial measurement units.&lt;/em&gt; In: Pattern Recognition. ACPR 2019. Ed. by M. Cree et al. Vol. 1180. Communications in Computer and Information Science (CCIS). Singapore: Springer, p. 223–231. &lt;a href=&#34;https://doi.org/10.1007/978-981-15-3651-9_20&#34;&gt;doi: 10.1007/978-981-15-3651-9_20&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Simmons, S., Jarvis, L., Dempsey, D., Kempa-Liehr, A.W. (2021). &lt;em&gt;Data Mining on Extremely Long Time-Series.&lt;/em&gt; In: 2021 International Conference on Data Mining Workshops (ICDMW). Ed. by B. Xue et al. Los Alamitos: IEEE, p. 1057-1066. &lt;a href=&#34;https://doi.org/10.1109/ICDMW53433.2021.00137&#34;&gt;doi: 10.1109/ICDMW53433.2021.00137&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Systematic time-series features engineering allows to work with time-series samples of different lengths, because every time-series is projected into a well-defined feature space. This approach allows the design of robust machine learning algorithms in applications with missing data.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Kennedy, A., Gemma, N., Rattenbury, N., Kempa-Liehr, A.W. (2021). &lt;em&gt;Modelling the projected separation of microlensing events using systematic time-series feature engineering.&lt;/em&gt; Astronomy and Computing 35.100460, p. 1–14, &lt;a href=&#34;https://doi.org/10.1016/j.ascom.2021.100460&#34;&gt;doi: 10.1016/j.ascom.2021.100460&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Is your time-series classification problem imbalanced? There is a good chance that undersampling of time-series feature matrices might solve your problem:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Dempsey, D.E., Cronin, S.J., Mei, S., Kempa-Liehr, A.W. (2020). &lt;em&gt;Automatic precursor recognition and real-time forecasting of sudden explosive volcanic eruptions at Whakaari, New Zealand&lt;/em&gt;. Nature Communications 11.3562, p. 1-8, &lt;a href=&#34;https://doi.org/10.1038/s41467-020-17375-2&#34;&gt;doi: 10.1038/s41467-020-17375-2&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Natural language processing of written texts is an example of applying systematic time-series feature engineering to event sequences, which is described in the following open access paper:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Tang, Y., Blincoe, K., Kempa-Liehr, A.W. (2020). &lt;em&gt;Enriching Feature Engineering for Short Text Samples by Language Time Series Analysis.&lt;/em&gt; EPJ Data Science 9.26, p. 1–59. &lt;a href=&#34;https://doi.org/10.1140/epjds/s13688-020-00244-9&#34;&gt;doi: 10.1140/epjds/s13688-020-00244-9&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Advantages of tsfresh&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;TSFRESH&lt;/em&gt; has several selling points, for example&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;it is field tested&lt;/li&gt; &#xA; &lt;li&gt;it is unit tested&lt;/li&gt; &#xA; &lt;li&gt;the filtering process is statistically/mathematically correct&lt;/li&gt; &#xA; &lt;li&gt;it has a comprehensive documentation&lt;/li&gt; &#xA; &lt;li&gt;it is compatible with sklearn, pandas and numpy&lt;/li&gt; &#xA; &lt;li&gt;it allows anyone to easily add their favorite features&lt;/li&gt; &#xA; &lt;li&gt;it both runs on your local machine or even on a cluster&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Next steps&lt;/h2&gt; &#xA;&lt;p&gt;If you are interested in the technical workings, go to see our comprehensive Read-The-Docs documentation at &lt;a href=&#34;http://tsfresh.readthedocs.io&#34;&gt;http://tsfresh.readthedocs.io&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The algorithm, especially the filtering part are also described in the paper mentioned above.&lt;/p&gt; &#xA;&lt;p&gt;We appreciate any contributions, if you are interested in helping us to make &lt;em&gt;TSFRESH&lt;/em&gt; the biggest archive of feature extraction methods in python, just head over to our &lt;a href=&#34;http://tsfresh.readthedocs.io/en/latest/text/how_to_contribute.html&#34;&gt;How-To-Contribute&lt;/a&gt; instructions.&lt;/p&gt; &#xA;&lt;p&gt;If you want to try out &lt;code&gt;tsfresh&lt;/code&gt; quickly or if you want to integrate it into your workflow, we also have a docker image available:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker pull nbraun/tsfresh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Backwards compatibility&lt;/h2&gt; &#xA;&lt;p&gt;If you need to reproduce or update time-series features, which were computed with the &lt;code&gt;matrixprofile&lt;/code&gt; feature calculators, you need to create a Python 3.8 environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create --name tsfresh__py_3.8 python=3.8&#xA;conda activate tsfresh__py_3.8&#xA;pip install tsfresh[matrixprofile]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;The research and development of &lt;em&gt;TSFRESH&lt;/em&gt; was funded in part by the German Federal Ministry of Education and Research under grant number 01IS14004 (project iPRODICT).&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>jupyter/notebook</title>
    <updated>2023-07-30T01:59:10Z</updated>
    <id>tag:github.com,2023-07-30:/jupyter/notebook</id>
    <link href="https://github.com/jupyter/notebook" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Jupyter Interactive Notebook&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Jupyter Notebook&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/jupyter/notebook/workflows/Build/badge.svg?sanitize=true&#34; alt=&#34;Github Actions Status&#34;&gt; &lt;a href=&#34;https://jupyter-notebook.readthedocs.io/en/latest/?badge=latest&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/jupyter-notebook/badge/?version=latest&#34; alt=&#34;Documentation Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://mybinder.org/v2/gh/jupyter/notebook/main?urlpath=tree&#34;&gt;&lt;img src=&#34;https://mybinder.org/badge_logo.svg?sanitize=true&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitpod.io/#https://github.com/jupyter/notebook&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/gitpod_editor-open-blue.svg?sanitize=true&#34; alt=&#34;Gitpod&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The Jupyter notebook is a web-based notebook environment for interactive computing.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jupyter/notebook/main/docs/resources/running_code_med.png&#34; alt=&#34;Jupyter notebook example&#34; title=&#34;Jupyter notebook example&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Maintained versions&lt;/h2&gt; &#xA;&lt;p&gt;We maintain the &lt;strong&gt;two most recently released major versions of Jupyter Notebook&lt;/strong&gt;, Notebook v5 and Classic Notebook v6. After Notebook v7.0 is released, we will no longer maintain Notebook v5. All Notebook v5 users are strongly advised to upgrade to Classic Notebook v6 as soon as possible.&lt;/p&gt; &#xA;&lt;p&gt;The Jupyter Notebook project is currently undertaking a transition to a more modern code base built from the ground-up using JupyterLab components and extensions.&lt;/p&gt; &#xA;&lt;p&gt;There is new stream of work which was submitted and then accepted as a Jupyter Enhancement Proposal (JEP) as part of the next version (v7): &lt;a href=&#34;https://jupyter.org/enhancement-proposals/79-notebook-v7/notebook-v7.html&#34;&gt;https://jupyter.org/enhancement-proposals/79-notebook-v7/notebook-v7.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;There is also a plan to continue maintaining Notebook v6 with bug and security fixes only, to ease the transition to Notebook v7: &lt;a href=&#34;https://github.com/jupyter/notebook-team-compass/issues/5#issuecomment-1085254000&#34;&gt;https://github.com/jupyter/notebook-team-compass/issues/5#issuecomment-1085254000&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Notebook v7&lt;/h3&gt; &#xA;&lt;p&gt;The newest major version of Notebook is based on:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;JupyterLab components for the frontend&lt;/li&gt; &#xA; &lt;li&gt;Jupyter Server for the Python server&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This represents a significant change to the &lt;code&gt;jupyter/notebook&lt;/code&gt; code base.&lt;/p&gt; &#xA;&lt;p&gt;To learn more about Notebook v7: &lt;a href=&#34;https://jupyter.org/enhancement-proposals/79-notebook-v7/notebook-v7.html&#34;&gt;https://jupyter.org/enhancement-proposals/79-notebook-v7/notebook-v7.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Classic Notebook v6&lt;/h3&gt; &#xA;&lt;p&gt;Maintainance and security-related issues are now being addressed in the &lt;a href=&#34;https://github.com/jupyter/notebook/tree/6.4.x&#34;&gt;&lt;code&gt;6.4.x&lt;/code&gt;&lt;/a&gt; branch.&lt;/p&gt; &#xA;&lt;p&gt;A &lt;code&gt;6.5.x&lt;/code&gt; branch will be soon created and will depend on &lt;a href=&#34;https://github.com/jupyter/nbclassic&#34;&gt;&lt;code&gt;nbclassic&lt;/code&gt;&lt;/a&gt; for the HTML/JavaScript/CSS assets.&lt;/p&gt; &#xA;&lt;p&gt;New features and continuous improvement is now focused on Notebook v7 (see section above).&lt;/p&gt; &#xA;&lt;p&gt;If you have an open pull request with a new feature or if you were planning to open one, we encourage switching over to the Jupyter Server and JupyterLab architecture, and distribute it as a server extension and / or JupyterLab prebuilt extension. That way your new feature will also be compatible with the new Notebook v7.&lt;/p&gt; &#xA;&lt;h2&gt;Jupyter notebook, the language-agnostic evolution of IPython notebook&lt;/h2&gt; &#xA;&lt;p&gt;Jupyter notebook is a language-agnostic HTML notebook application for Project Jupyter. In 2015, Jupyter notebook was released as a part of The Big Split™ of the IPython codebase. IPython 3 was the last major monolithic release containing both language-agnostic code, such as the &lt;em&gt;IPython notebook&lt;/em&gt;, and language specific code, such as the &lt;em&gt;IPython kernel for Python&lt;/em&gt;. As computing spans across many languages, Project Jupyter will continue to develop the language-agnostic &lt;strong&gt;Jupyter notebook&lt;/strong&gt; in this repo and with the help of the community develop language specific kernels which are found in their own discrete repos.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.jupyter.org/the-big-split-9d7b88a031a7&#34;&gt;The Big Split™ announcement&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.jupyter.org/jupyter-ascending-1bf5b362d97e&#34;&gt;Jupyter Ascending blog post&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;You can find the installation documentation for the &lt;a href=&#34;https://jupyter.readthedocs.io/en/latest/install.html&#34;&gt;Jupyter platform, on ReadTheDocs&lt;/a&gt;. The documentation for advanced usage of Jupyter notebook can be found &lt;a href=&#34;https://jupyter-notebook.readthedocs.io/en/latest/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For a local installation, make sure you have &lt;a href=&#34;https://pip.readthedocs.io/en/stable/installing/&#34;&gt;pip installed&lt;/a&gt; and run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install notebook&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage - Running Jupyter notebook&lt;/h2&gt; &#xA;&lt;h3&gt;Running in a local installation&lt;/h3&gt; &#xA;&lt;p&gt;Launch with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;jupyter notebook&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running in a remote installation&lt;/h3&gt; &#xA;&lt;p&gt;You need some configuration before starting Jupyter notebook remotely. See &lt;a href=&#34;https://jupyter-server.readthedocs.io/en/latest/operators/public-server.html&#34;&gt;Running a notebook server&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Development Installation&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/jupyter/notebook/main/CONTRIBUTING.md&#34;&gt;&lt;code&gt;CONTRIBUTING.md&lt;/code&gt;&lt;/a&gt; for how to set up a local development installation.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;If you are interested in contributing to the project, see &lt;a href=&#34;https://raw.githubusercontent.com/jupyter/notebook/main/CONTRIBUTING.md&#34;&gt;&lt;code&gt;CONTRIBUTING.md&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Community Guidelines and Code of Conduct&lt;/h2&gt; &#xA;&lt;p&gt;This repository is a Jupyter project and follows the Jupyter &lt;a href=&#34;https://jupyter.readthedocs.io/en/latest/community/content-community.html&#34;&gt;Community Guides and Code of Conduct&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jupyter.org&#34;&gt;Project Jupyter website&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jupyter.org/try&#34;&gt;Online Demo at jupyter.org/try&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jupyter-notebook.readthedocs.io/en/latest/&#34;&gt;Documentation for Jupyter notebook&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ChungJooHo/Jupyter_Kor_doc/&#34;&gt;Korean Version of Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jupyter.readthedocs.io/en/latest/index.html&#34;&gt;Documentation for Project Jupyter&lt;/a&gt; [&lt;a href=&#34;https://media.readthedocs.org/pdf/jupyter/latest/jupyter.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jupyter/notebook/issues&#34;&gt;Issues&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discourse.jupyter.org/&#34;&gt;Technical support - Jupyter Google Group&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;About the Jupyter Development Team&lt;/h2&gt; &#xA;&lt;p&gt;The Jupyter Development Team is the set of all contributors to the Jupyter project. This includes all of the Jupyter subprojects.&lt;/p&gt; &#xA;&lt;p&gt;The core team that coordinates development on GitHub can be found here: &lt;a href=&#34;https://github.com/jupyter/&#34;&gt;https://github.com/jupyter/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Our Copyright Policy&lt;/h2&gt; &#xA;&lt;p&gt;Jupyter uses a shared copyright model. Each contributor maintains copyright over their contributions to Jupyter. But, it is important to note that these contributions are typically only changes to the repositories. Thus, the Jupyter source code, in its entirety is not the copyright of any single person or institution. Instead, it is the collective copyright of the entire Jupyter Development Team. If individual contributors want to maintain a record of what changes/contributions they have specific copyright on, they should indicate their copyright in the commit message of the change, when they commit the change to one of the Jupyter repositories.&lt;/p&gt; &#xA;&lt;p&gt;With this in mind, the following banner should be used in any source code file to indicate the copyright and license terms:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Copyright (c) Jupyter Development Team.&#xA;# Distributed under the terms of the Modified BSD License.&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>artidoro/qlora</title>
    <updated>2023-07-30T01:59:10Z</updated>
    <id>tag:github.com,2023-07-30:/artidoro/qlora</id>
    <link href="https://github.com/artidoro/qlora" rel="alternate"></link>
    <summary type="html">&lt;p&gt;QLoRA: Efficient Finetuning of Quantized LLMs&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;QLoRA: Efficient Finetuning of Quantized LLMs&lt;/h1&gt; &#xA;&lt;p&gt;| &lt;a href=&#34;https://arxiv.org/abs/2305.14314&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/timdettmers&#34;&gt;Adapter Weights&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/spaces/uwnlp/guanaco-playground-tgi&#34;&gt;Demo&lt;/a&gt; |&lt;/p&gt; &#xA;&lt;p&gt;This repo supports the paper &#34;QLoRA: Efficient Finetuning of Quantized LLMs&#34;, an effort to democratize access to LLM research.&lt;/p&gt; &#xA;&lt;p&gt;QLoRA uses &lt;a href=&#34;https://github.com/TimDettmers/bitsandbytes&#34;&gt;bitsandbytes&lt;/a&gt; for quantization and is integrated with Hugging Face&#39;s &lt;a href=&#34;https://github.com/huggingface/peft&#34;&gt;PEFT&lt;/a&gt; and &lt;a href=&#34;https://github.com/huggingface/transformers/&#34;&gt;transformers&lt;/a&gt; libraries. QLoRA was developed by members of the &lt;a href=&#34;https://twitter.com/uwnlp?s=20&#34;&gt;University of Washington&#39;s UW NLP group&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;7/19/2023 - Added LLaMA 2 example script and updated version requirements&lt;/li&gt; &#xA; &lt;li&gt;7/18/2023 - Fixed non-frozen embeddings when adding new tokens&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) Double Quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) Paged Optimizers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. We release all of our models and code, including CUDA kernels for 4-bit training.&lt;/p&gt; &#xA;&lt;h2&gt;License and Intended Use&lt;/h2&gt; &#xA;&lt;p&gt;We release the resources associated with QLoRA finetuning in this repository under MIT license. In addition, we release the Guanaco model family for base LLaMA model sizes of 7B, 13B, 33B, and 65B. These models are intended for purposes in line with the LLaMA license and require access to the LLaMA models.&lt;/p&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;Guanaco is a system purely intended for research purposes and could produce problematic outputs.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Access the &lt;a href=&#34;https://huggingface.co/spaces/uwnlp/guanaco-playground-tgi&#34;&gt;live demo here&lt;/a&gt;. Note this is the 33B model, the 65B model demo will come later.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Or host your own Guanaco gradio demo directly in Colab with &lt;a href=&#34;https://colab.research.google.com/drive/17XEqL1JcmVWjHkT-WczdYkJlNINacwG7?usp=sharing&#34;&gt;this notebook&lt;/a&gt;. Works with free GPUs for 7B and 13B models.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Alternatively, can you distinguish ChatGPT from Guanaco? Give it a try! You can access &lt;a href=&#34;https://colab.research.google.com/drive/1kK6xasHiav9nhiRUJjPMZb4fAED4qRHb?usp=sharing&#34;&gt;the model response Colab here&lt;/a&gt; comparing ChatGPT and Guanaco 65B on Vicuna prompts.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;To load models in 4bits with transformers and bitsandbytes, you have to install accelerate and transformers from source and make sure you have the latest version of the bitsandbytes library. After installing PyTorch (follow instructions &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;here&lt;/a&gt;), you can achieve the above with the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -U -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;qlora.py&lt;/code&gt; code is a starting point for finetuning and inference on various datasets. Basic command for finetuning a baseline model on the Alpaca dataset:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python qlora.py --model_name_or_path &amp;lt;path_or_name&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For models larger than 13B, we recommend adjusting the learning rate:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python qlora.py –learning_rate 0.0001 --model_name_or_path &amp;lt;path_or_name&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To replicate our Guanaco models see below.&lt;/p&gt; &#xA;&lt;h3&gt;Tutorials and Demonstrations&lt;/h3&gt; &#xA;&lt;p&gt;Here is &lt;a href=&#34;https://huggingface.co/blog/4bit-transformers-bitsandbytes&#34;&gt;a blog&lt;/a&gt; discussing 4-bit quantization, QLoRA, and how they are integrated in transformers.&lt;/p&gt; &#xA;&lt;p&gt;You can host your own gradio Guanaco demo directly in Colab following &lt;a href=&#34;https://colab.research.google.com/drive/17XEqL1JcmVWjHkT-WczdYkJlNINacwG7?usp=sharing&#34;&gt;this notebook&lt;/a&gt;. In addition, here are Colab notebooks with examples for inference and finetuning using QLoRA:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1ge2F1QSK8Q7h0hn3YKuBCOAS0bK8E0wf?usp=sharing&#34;&gt;Inference notebook&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1VoYNfYDKcKRQRor98Zbf2-9VQTtGJ24k?usp=sharing&#34;&gt;Finetuning notebook&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Other examples are found under the &lt;code&gt;examples/&lt;/code&gt; folder. We include a generation getting started example with guanaco at &lt;code&gt;examples/guanaco_generate.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Quantization&lt;/h3&gt; &#xA;&lt;p&gt;Quantization parameters are controlled from the &lt;code&gt;BitsandbytesConfig&lt;/code&gt; (&lt;a href=&#34;https://huggingface.co/docs/transformers/main_classes/quantization#transformers.BitsAndBytesConfig&#34;&gt;see HF documenation&lt;/a&gt;) as follows:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Loading in 4 bits is activated through &lt;code&gt;load_in_4bit&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;The datatype used for the linear layer computations with &lt;code&gt;bnb_4bit_compute_dtype&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Nested quantization is activated through &lt;code&gt;bnb_4bit_use_double_quant&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;The datatype used for qunatization is specified with &lt;code&gt;bnb_4bit_quant_type&lt;/code&gt;. Note that there are two supported quantization datatypes &lt;code&gt;fp4&lt;/code&gt; (four bit float) and &lt;code&gt;nf4&lt;/code&gt; (normal four bit float). The latter is theoretically optimal for normally distributed weights and we recommend using &lt;code&gt;nf4&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;    model = AutoModelForCausalLM.from_pretrained(&#xA;        model_name_or_path=&#39;/name/or/path/to/your/model&#39;,&#xA;        load_in_4bit=True,&#xA;        device_map=&#39;auto&#39;,&#xA;        max_memory=max_memory,&#xA;        torch_dtype=torch.bfloat16,&#xA;        quantization_config=BitsAndBytesConfig(&#xA;            load_in_4bit=True,&#xA;            bnb_4bit_compute_dtype=torch.bfloat16,&#xA;            bnb_4bit_use_double_quant=True,&#xA;            bnb_4bit_quant_type=&#39;nf4&#39;&#xA;        ),&#xA;    )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Paged Optimizer&lt;/h3&gt; &#xA;&lt;p&gt;You can access the paged optimizer with the argument &lt;code&gt;--optim paged_adamw_32bit&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Guanaco Finetuning&lt;/h3&gt; &#xA;&lt;p&gt;You can select &lt;code&gt;--dataset oasst1&lt;/code&gt; to load the OpenAssistant dataset that was used to train Guanaco. You can also find it on HF at &lt;a href=&#34;https://huggingface.co/datasets/timdettmers/openassistant-guanaco&#34;&gt;timdettmers/openassistant-guanaco&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We include scripts to reproduce the hyperparameters of Guanaco model training for various sizes at &lt;code&gt;./scripts/finetune_guanaco*.sh&lt;/code&gt;. Make sure to adjust &lt;code&gt;per_device_train_batch_size&lt;/code&gt; and &lt;code&gt;gradient_accumulation_steps&lt;/code&gt; so that their product is 16 and training fits on your GPUs.&lt;/p&gt; &#xA;&lt;h3&gt;Using Local Datasets&lt;/h3&gt; &#xA;&lt;p&gt;You can specify the path to your dataset using the &lt;code&gt;--dataset&lt;/code&gt; argument. If the &lt;code&gt;--dataset_format&lt;/code&gt; argument is not set, it will default to the Alpaca format. Here are a few examples:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Training with an &lt;em&gt;alpaca&lt;/em&gt; format dataset: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python qlora.py --dataset=&#34;path/to/your/dataset&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Training with a &lt;em&gt;self-instruct&lt;/em&gt; format dataset: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python qlora.py --dataset=&#34;path/to/your/dataset&#34; --dataset_format=&#34;self-instruct&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Multi GPU&lt;/h3&gt; &#xA;&lt;p&gt;Multi GPU training and inference work out-of-the-box with Hugging Face&#39;s Accelerate. Note that the &lt;code&gt;per_device_train_batch_size&lt;/code&gt; and &lt;code&gt;per_device_eval_batch_size&lt;/code&gt; arguments are global batch sizes unlike what their name suggest.&lt;/p&gt; &#xA;&lt;p&gt;When loading a model for training or inference on multiple GPUs you should pass something like the following to &lt;code&gt;AutoModelForCausalLM.from_pretrained()&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;device_map = &#34;auto&#34;&#xA;max_memory = {i: &#39;46000MB&#39; for i in range(torch.cuda.device_count())}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Sample Outputs&lt;/h2&gt; &#xA;&lt;p&gt;We provide generations for the models described in the paper for both OA and Vicuna queries in the &lt;code&gt;eval/generations&lt;/code&gt; folder. These are intended to foster further research on model evaluation and analysis.&lt;/p&gt; &#xA;&lt;p&gt;Can you distinguish ChatGPT from Guanaco? Give it a try! You can access &lt;a href=&#34;https://colab.research.google.com/drive/1kK6xasHiav9nhiRUJjPMZb4fAED4qRHb?usp=sharing&#34;&gt;the model response Colab here&lt;/a&gt; comparing ChatGPT and Guanaco 65B on Vicuna prompts.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;We include scripts adapted from the FastChat repo to automatically evaluate model generations using GPT-4. We include script for comparisons relative to ChatGPT with scores out of 10 as well as &#34;pairwise comparisons&#34; with three class labeling (win, loose, or tie). These are found in the &lt;code&gt;eval&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;p&gt;To facilitate the replication of our evaluation and future work in this area, we release GPT-4 and human ratings of our systems. These are found under &lt;code&gt;eval/ratings-human&lt;/code&gt; and &lt;code&gt;eval/ratings-gpt4&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;More details can be found at &lt;code&gt;eval/EVAL_README.md&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Known Issues and Limitations&lt;/h2&gt; &#xA;&lt;p&gt;Here a list of known issues and bugs. If your issue is not reported here, please open a new issue and describe the problem.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;4-bit inference is slow. Currently, our 4-bit inference implementation is not yet integrated with the 4-bit matrix multiplication&lt;/li&gt; &#xA; &lt;li&gt;Resuming a LoRA training run with the Trainer currently not supported by HF.&lt;/li&gt; &#xA; &lt;li&gt;Currently, using &lt;code&gt;bnb_4bit_compute_type=&#39;fp16&#39;&lt;/code&gt; can lead to instabilities. For 7B LLaMA, only 80% of finetuning runs complete without error. We have solutions, but they are not integrated yet into bitsandbytes.&lt;/li&gt; &#xA; &lt;li&gt;Make sure that &lt;code&gt;tokenizer.bos_token_id = 1&lt;/code&gt; to avoid generation issues.&lt;/li&gt; &#xA; &lt;li&gt;If you get an this &lt;a href=&#34;https://github.com/artidoro/qlora/issues/82&#34;&gt;issue&lt;/a&gt; (&#34;illegal memory access&#34;) then you should use a newer HF LLaMA conversion or downgrade your PyTorch version.&lt;/li&gt; &#xA; &lt;li&gt;Problems with adding new tokens outlined in #214. Embeddings need to be updated and stored/reloaded if you are adding new tokens.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{dettmers2023qlora,&#xA;  title={QLoRA: Efficient Finetuning of Quantized LLMs},&#xA;  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},&#xA;  journal={arXiv preprint arXiv:2305.14314},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;We thank the Hugging Face team, in particular Younes Belkada, for their support integrating QLoRA with PEFT and transformers libraries. We also thank Meta for releasing the LLaMA models without which this work would not have been possible.&lt;/p&gt; &#xA;&lt;p&gt;This repo builds on the &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca&lt;/a&gt; and &lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;LMSYS FastChat&lt;/a&gt; repos.&lt;/p&gt;</summary>
  </entry>
</feed>