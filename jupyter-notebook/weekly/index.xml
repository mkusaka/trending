<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-06-23T01:43:48Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>neo4j-labs/llm-graph-builder</title>
    <updated>2024-06-23T01:43:48Z</updated>
    <id>tag:github.com,2024-06-23:/neo4j-labs/llm-graph-builder</id>
    <link href="https://github.com/neo4j-labs/llm-graph-builder" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Neo4j graph construction from unstructured data&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Knowledge Graph Builder App&lt;/h1&gt; &#xA;&lt;p&gt;This application is designed to convert PDF documents into a knowledge graph stored in Neo4j. It utilizes the power of OpenAI&#39;s GPT/Diffbot LLM(Large language model) to extract nodes, relationships and properties from the text content of the PDF and then organizes them into a structured knowledge graph using Langchain framework. Files can be uploaded from local machine or S3 bucket and then LLM model can be chosen to create the knowledge graph.&lt;/p&gt; &#xA;&lt;h3&gt;Getting started&lt;/h3&gt; &#xA;&lt;p&gt;&lt;span&gt;⚠&lt;/span&gt; You will need to have a Neo4j Database V5.15 or later with &lt;a href=&#34;https://neo4j.com/docs/apoc/current/installation/&#34;&gt;APOC installed&lt;/a&gt; to use this Knowledge Graph Builder. You can use any &lt;a href=&#34;https://neo4j.com/aura/&#34;&gt;Neo4j Aura database&lt;/a&gt; (including the free database) If you are using Neo4j Desktop, you will not be able to use the docker-compose but will have to follow the &lt;a href=&#34;https://raw.githubusercontent.com/neo4j-labs/llm-graph-builder/main/#running-backend-and-frontend-separately-dev-environment&#34;&gt;separate deployment of backend and frontend section&lt;/a&gt;. &lt;span&gt;⚠&lt;/span&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Deploy locally&lt;/h3&gt; &#xA;&lt;h4&gt;Running through docker-compose&lt;/h4&gt; &#xA;&lt;p&gt;By default only OpenAI and Diffbot are enabled since Gemini requires extra GCP configurations.&lt;/p&gt; &#xA;&lt;p&gt;In your root folder, create a .env file with your OPENAI and DIFFBOT keys (if you want to use both):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-env&#34;&gt;OPENAI_API_KEY=&#34;your-openai-key&#34;&#xA;DIFFBOT_API_KEY=&#34;your-diffbot-key&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;if you only want OpenAI:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-env&#34;&gt;LLM_MODELS=&#34;OpenAI GPT 3.5,OpenAI GPT 4o&#34;&#xA;OPENAI_API_KEY=&#34;your-openai-key&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;if you only want Diffbot:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-env&#34;&gt;LLM_MODELS=&#34;Diffbot&#34;&#xA;DIFFBOT_API_KEY=&#34;your-diffbot-key&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can then run Docker Compose to build and start all components:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker-compose up --build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Additional configs&lt;/h5&gt; &#xA;&lt;p&gt;By default, the input sources will be: Local files, Youtube, Wikipedia and AWS S3. As this default config is applied:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-env&#34;&gt;REACT_APP_SOURCES=&#34;local,youtube,wiki,s3&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If however you want the Google GCS integration, add &lt;code&gt;gcs&lt;/code&gt; and your Google client ID:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-env&#34;&gt;REACT_APP_SOURCES=&#34;local,youtube,wiki,s3,gcs&#34;&#xA;GOOGLE_CLIENT_ID=&#34;xxxx&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can of course combine all (local, youtube, wikipedia, s3 and gcs) or remove any you don&#39;t want/need.&lt;/p&gt; &#xA;&lt;h4&gt;Running Backend and Frontend separately (dev environment)&lt;/h4&gt; &#xA;&lt;p&gt;Alternatively, you can run the backend and frontend separately:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For the frontend:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create the frontend/.env file by copy/pasting the frontend/example.env.&lt;/li&gt; &#xA; &lt;li&gt;Change values as needed&lt;/li&gt; &#xA; &lt;li&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd frontend&#xA;yarn&#xA;yarn run dev&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For the backend:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create the backend/.env file by copy/pasting the backend/example.env.&lt;/li&gt; &#xA; &lt;li&gt;Change values as needed&lt;/li&gt; &#xA; &lt;li&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd backend&#xA;python -m venv envName&#xA;source envName/bin/activate &#xA;pip install -r requirements.txt&#xA;uvicorn score:app --reload&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;ENV&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Env Variable Name&lt;/th&gt; &#xA;   &lt;th&gt;Mandatory/Optional&lt;/th&gt; &#xA;   &lt;th&gt;Default Value&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OPENAI_API_KEY&lt;/td&gt; &#xA;   &lt;td&gt;Mandatory&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;API key for OpenAI&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DIFFBOT_API_KEY&lt;/td&gt; &#xA;   &lt;td&gt;Mandatory&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;API key for Diffbot&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EMBEDDING_MODEL&lt;/td&gt; &#xA;   &lt;td&gt;Optional&lt;/td&gt; &#xA;   &lt;td&gt;all-MiniLM-L6-v2&lt;/td&gt; &#xA;   &lt;td&gt;Model for generating the text embedding (all-MiniLM-L6-v2 , openai , vertexai)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;IS_EMBEDDING&lt;/td&gt; &#xA;   &lt;td&gt;Optional&lt;/td&gt; &#xA;   &lt;td&gt;true&lt;/td&gt; &#xA;   &lt;td&gt;Flag to enable text embedding&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;KNN_MIN_SCORE&lt;/td&gt; &#xA;   &lt;td&gt;Optional&lt;/td&gt; &#xA;   &lt;td&gt;0.94&lt;/td&gt; &#xA;   &lt;td&gt;Minimum score for KNN algorithm&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GEMINI_ENABLED&lt;/td&gt; &#xA;   &lt;td&gt;Optional&lt;/td&gt; &#xA;   &lt;td&gt;False&lt;/td&gt; &#xA;   &lt;td&gt;Flag to enable Gemini&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GCP_LOG_METRICS_ENABLED&lt;/td&gt; &#xA;   &lt;td&gt;Optional&lt;/td&gt; &#xA;   &lt;td&gt;False&lt;/td&gt; &#xA;   &lt;td&gt;Flag to enable Google Cloud logs&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NUMBER_OF_CHUNKS_TO_COMBINE&lt;/td&gt; &#xA;   &lt;td&gt;Optional&lt;/td&gt; &#xA;   &lt;td&gt;6&lt;/td&gt; &#xA;   &lt;td&gt;Number of chunks to combine when processing embeddings&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;UPDATE_GRAPH_CHUNKS_PROCESSED&lt;/td&gt; &#xA;   &lt;td&gt;Optional&lt;/td&gt; &#xA;   &lt;td&gt;20&lt;/td&gt; &#xA;   &lt;td&gt;Number of chunks processed before updating progress&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NEO4J_URI&lt;/td&gt; &#xA;   &lt;td&gt;Optional&lt;/td&gt; &#xA;   &lt;td&gt;neo4j://database:7687&lt;/td&gt; &#xA;   &lt;td&gt;URI for Neo4j database&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NEO4J_USERNAME&lt;/td&gt; &#xA;   &lt;td&gt;Optional&lt;/td&gt; &#xA;   &lt;td&gt;neo4j&lt;/td&gt; &#xA;   &lt;td&gt;Username for Neo4j database&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NEO4J_PASSWORD&lt;/td&gt; &#xA;   &lt;td&gt;Optional&lt;/td&gt; &#xA;   &lt;td&gt;password&lt;/td&gt; &#xA;   &lt;td&gt;Password for Neo4j database&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LANGCHAIN_API_KEY&lt;/td&gt; &#xA;   &lt;td&gt;Optional&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;API key for Langchain&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LANGCHAIN_PROJECT&lt;/td&gt; &#xA;   &lt;td&gt;Optional&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Project for Langchain&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LANGCHAIN_TRACING_V2&lt;/td&gt; &#xA;   &lt;td&gt;Optional&lt;/td&gt; &#xA;   &lt;td&gt;true&lt;/td&gt; &#xA;   &lt;td&gt;Flag to enable Langchain tracing&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LANGCHAIN_ENDPOINT&lt;/td&gt; &#xA;   &lt;td&gt;Optional&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://api.smith.langchain.com&#34;&gt;https://api.smith.langchain.com&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Endpoint for Langchain API&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BACKEND_API_URL&lt;/td&gt; &#xA;   &lt;td&gt;Optional&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://localhost:8000&#34;&gt;http://localhost:8000&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;URL for backend API&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BLOOM_URL&lt;/td&gt; &#xA;   &lt;td&gt;Optional&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://workspace-preview.neo4j.io/workspace/explore?connectURL=%7BCONNECT_URL%7D&amp;amp;search=Show+me+a+graph&amp;amp;featureGenAISuggestions=true&amp;amp;featureGenAISuggestionsInternal=true&#34;&gt;https://workspace-preview.neo4j.io/workspace/explore?connectURL={CONNECT_URL}&amp;amp;search=Show+me+a+graph&amp;amp;featureGenAISuggestions=true&amp;amp;featureGenAISuggestionsInternal=true&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;URL for Bloom visualization&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;REACT_APP_SOURCES&lt;/td&gt; &#xA;   &lt;td&gt;Optional&lt;/td&gt; &#xA;   &lt;td&gt;local,youtube,wiki,s3&lt;/td&gt; &#xA;   &lt;td&gt;List of input sources that will be available&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLM_MODELS&lt;/td&gt; &#xA;   &lt;td&gt;Optional&lt;/td&gt; &#xA;   &lt;td&gt;Diffbot,OpenAI GPT 3.5,OpenAI GPT 4o&lt;/td&gt; &#xA;   &lt;td&gt;Models available for selection on the frontend, used for entities extraction and Q&amp;amp;A Chatbot&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ENV&lt;/td&gt; &#xA;   &lt;td&gt;Optional&lt;/td&gt; &#xA;   &lt;td&gt;DEV&lt;/td&gt; &#xA;   &lt;td&gt;Environment variable for the app&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TIME_PER_CHUNK&lt;/td&gt; &#xA;   &lt;td&gt;Optional&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;Time per chunk for processing&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CHUNK_SIZE&lt;/td&gt; &#xA;   &lt;td&gt;Optional&lt;/td&gt; &#xA;   &lt;td&gt;5242880&lt;/td&gt; &#xA;   &lt;td&gt;Size of each chunk for processing&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GOOGLE_CLIENT_ID&lt;/td&gt; &#xA;   &lt;td&gt;Optional&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Client ID for Google authentication&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;&lt;/h3&gt; &#xA;&lt;p&gt;To deploy the app and packages on Google Cloud Platform, run the following command on google cloud run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Frontend deploy &#xA;gcloud run deploy &#xA;source location current directory &amp;gt; Frontend&#xA;region : 32 [us-central 1]&#xA;Allow unauthenticated request : Yes&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Backend deploy &#xA;gcloud run deploy --set-env-vars &#34;OPENAI_API_KEY = &#34; --set-env-vars &#34;DIFFBOT_API_KEY = &#34; --set-env-vars &#34;NEO4J_URI = &#34; --set-env-vars &#34;NEO4J_PASSWORD = &#34; --set-env-vars &#34;NEO4J_USERNAME = &#34;&#xA;source location current directory &amp;gt; Backend&#xA;region : 32 [us-central 1]&#xA;Allow unauthenticated request : Yes&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Features&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;PDF Upload&lt;/strong&gt;: Users can upload PDF documents using the Drop Zone.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;S3 Bucket Integration&lt;/strong&gt;: Users can also specify PDF documents stored in an S3 bucket for processing.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Knowledge Graph Generation&lt;/strong&gt;: The application employs OpenAI/Diffbot&#39;s LLM to extract relevant information from the PDFs and construct a knowledge graph.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Neo4j Integration&lt;/strong&gt;: The extracted nodes and relationships are stored in a Neo4j database for easy visualization and querying.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Grid View of source node files with&lt;/strong&gt; : Name,Type,Size,Nodes,Relations,Duration,Status,Source,Model&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Functions/Modules&lt;/h2&gt; &#xA;&lt;h4&gt;extract_graph_from_file(uri, userName, password, file_path, model):&lt;/h4&gt; &#xA;&lt;p&gt;Extracts nodes , relationships and properties from a PDF file leveraging LLM models.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Args:&#xA; uri: URI of the graph to extract&#xA; userName: Username to use for graph creation ( if None will use username from config file )&#xA; password: Password to use for graph creation ( if None will use password from config file )&#xA; file: File object containing the PDF file path to be used&#xA; model: Type of model to use (&#39;Gemini Pro&#39; or &#39;Diffbot&#39;)&#xA;&#xA; Returns: &#xA; Json response to API with fileName, nodeCount, relationshipCount, processingTime, &#xA; status and model as attributes.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img width=&#34;692&#34; alt=&#34;neoooo&#34; src=&#34;https://github.com/neo4j-labs/llm-graph-builder/assets/118245454/01e731df-b565-4f4f-b577-c47e39dd1748&#34;&gt; &#xA;&lt;h4&gt;create_source_node_graph(uri, userName, password, file):&lt;/h4&gt; &#xA;&lt;p&gt;Creates a source node in Neo4jGraph and sets properties.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Args:&#xA; uri: URI of Graph Service to connect to&#xA; userName: Username to connect to Graph Service with ( default : None )&#xA; password: Password to connect to Graph Service with ( default : None )&#xA; file: File object with information about file to be added&#xA;&#xA;Returns: &#xA; Success or Failure message of node creation&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img width=&#34;958&#34; alt=&#34;neo_workspace&#34; src=&#34;https://github.com/neo4j-labs/llm-graph-builder/assets/118245454/f2eb11cd-718c-453e-bec9-11410ec6e45d&#34;&gt; &#xA;&lt;h4&gt;get_source_list_from_graph():&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt; Returns a list of file sources in the database by querying the graph and &#xA; sorting the list by the last updated date. &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img width=&#34;822&#34; alt=&#34;get_source&#34; src=&#34;https://github.com/neo4j-labs/llm-graph-builder/assets/118245454/1d8c7a86-6f10-4916-a4c1-8fdd9f312bcc&#34;&gt; &#xA;&lt;h4&gt;Chunk nodes and embeddings creation in Neo4j&lt;/h4&gt; &#xA;&lt;img width=&#34;926&#34; alt=&#34;chunking&#34; src=&#34;https://github.com/neo4j-labs/llm-graph-builder/assets/118245454/4d61479c-e5e9-415e-954e-3edf6a773e72&#34;&gt; &#xA;&lt;h2&gt;Application Walkthrough&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/neo4j-labs/llm-graph-builder/assets/121786590/b725a503-6ade-46d2-9e70-61d57443c311&#34;&gt;https://github.com/neo4j-labs/llm-graph-builder/assets/121786590/b725a503-6ade-46d2-9e70-61d57443c311&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Links&lt;/h2&gt; &#xA;&lt;p&gt;The Public &lt;a href=&#34;https://devfrontend-dcavk67s4a-uc.a.run.app&#34;&gt; Google cloud Run URL&lt;/a&gt;. &lt;a href=&#34;https://workspace-preview.neo4j.io/workspace&#34;&gt;Workspace URL&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>tatsu-lab/alpaca_eval</title>
    <updated>2024-06-23T01:43:48Z</updated>
    <id>tag:github.com,2024-06-23:/tatsu-lab/alpaca_eval</id>
    <link href="https://github.com/tatsu-lab/alpaca_eval" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An automatic evaluator for instruction-following language models. Human-validated, high-quality, cheap, and fast.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;a href=&#34;https://tatsu-lab.github.io/alpaca_eval/&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/docs/AlpacaFarm_small.png&#34; width=&#34;35&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://tatsu-lab.github.io/alpaca_eval/&#34;&gt;AlpacaEval&lt;/a&gt; : An Automatic Evaluator for Instruction-following Language Models&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/tatsu-lab/alpaca_farm/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg?sanitize=true&#34; alt=&#34;Code License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_farm/raw/main/DATA_LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Data%20License-CC%20By%20NC%204.0-red.svg?sanitize=true&#34; alt=&#34;Data License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.python.org/downloads/release/python-3100/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.10+-blue.svg?sanitize=true&#34; alt=&#34;Python 3.10+&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/GJMxJSVZZM&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/discord-server-blue?logo=discord&amp;amp;logoColor=white&#34; alt=&#34;discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AlpacaEval 2.0 with length-controlled win-rates&lt;/strong&gt; (&lt;a href=&#34;https://arxiv.org/abs/2404.04475&#34;&gt;paper&lt;/a&gt;) has a spearman correlation of &lt;strong&gt;0.98&lt;/strong&gt; with &lt;a href=&#34;https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard&#34;&gt;ChatBot Arena&lt;/a&gt; while costing less than &lt;strong&gt;$10&lt;/strong&gt; of OpenAI credits run and running in less than 3 minutes. Our goal is to have a benchmark for chat LLMs that is: fast (&amp;lt; 5min), cheap (&amp;lt; $10), and highly correlated with humans (0.98). Here&#39;s a comparison with other benchmarks:&lt;/p&gt; &#xA;&lt;p float=&#34;left&#34; align=&#34;middle&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/figures/chat_correlations_no_ae.png&#34; alt=&#34;LC AlpacaEval is the most highly correlated benchmark with Chat Arena.&#34; width=&#34;500&#34;&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Updates:&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;🎉&lt;/span&gt; &lt;strong&gt;Length-controlled Win Rates&lt;/strong&gt; are out and used by default! This increases the correlation with ChatBot Arena from 0.93 to 0.98, while significantly decreasing length gameability. The raw win rates are still shown on the website and the CLI. More details &lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#length-controlled-win-rates&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;🎉&lt;/span&gt; &lt;strong&gt;AlpacaEval 2.0&lt;/strong&gt; is out and used by default! We improved the auto-annotator (better and cheaper) and use GPT-4 preview as baseline. More details &lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#alpacaeval-20&#34;&gt;here&lt;/a&gt;. For the old version, set your environment variable &lt;code&gt;IS_ALPACA_EVAL_2=False&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;&lt;b&gt;Table of Contents&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#overview&#34;&gt;Overview&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#quick-start&#34;&gt;Quick Start&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#leaderboards-and-how-to-interpret-them&#34;&gt;Leaderboards and how to interpret them&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#models&#34;&gt;Models&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#evaluators&#34;&gt;Evaluators&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#use-cases&#34;&gt;Use-cases&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#evaluating-a-model&#34;&gt;Evaluating a model&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#making-a-new-leaderboard&#34;&gt;Making a new leaderboard&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#making-a-new-evaluator&#34;&gt;Making a new evaluator&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#contributing&#34;&gt;Contributing&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#contributing-a-model&#34;&gt;Contributing a model&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#contributing-an-evaluator&#34;&gt;Contributing an evaluator&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#contributing-an-eval-set&#34;&gt;Contributing an eval set&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#contributing-a-completion-function&#34;&gt;Contributing a completion function&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#limitations&#34;&gt;Limitations&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#additional-analysis-and-plots&#34;&gt;Analysis&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#analyzing-an-evaluator&#34;&gt;Analyzing an evaluator&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#analyzing-an-eval-set&#34;&gt;Analyzing an eval set&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#additional-information&#34;&gt;Additional information&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#length-controlled-win-rates&#34;&gt;Length-controlled win rates&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#alpacaeval-20&#34;&gt;AlpacaEval 2.0&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#data-release&#34;&gt;Data Release&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#differences-with-alpacafarm&#34;&gt;Differences with AlpacaFarm&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#related-work&#34;&gt;Related work&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#interpreting-annotations&#34;&gt;Interpreting annotations&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#major-updates&#34;&gt;Major updates&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;h1&gt;Overview&lt;/h1&gt; &#xA;&lt;p&gt;Evaluation of instruction-following models (e.g., ChatGPT) typically requires human interactions. This is time-consuming, expensive, and hard to replicate. AlpacaEval in an LLM-based automatic evaluation that is fast, cheap, replicable, and validated against 20K human annotations. It is particularly useful for model development. Although we improved over prior automatic evaluation pipelines, there are still fundamental &lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#limitations&#34;&gt;limitations&lt;/a&gt; like the preference for longer outputs. AlpacaEval provides the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tatsu-lab.github.io/alpaca_eval/&#34;&gt;&lt;strong&gt;Leaderboard&lt;/strong&gt;&lt;/a&gt;: a leaderboard of common models on the AlpacaEval evaluation set. &lt;strong&gt;Caution&lt;/strong&gt;: Automatic evaluators (e.g. GPT-4) may be biased towards models that generate longer outputs and/or that were fine-tuned on the model underlying the evaluator (e.g. GPT-4).&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#evaluators&#34;&gt;&lt;strong&gt;Automatic evaluator&lt;/strong&gt;&lt;/a&gt;: an automatic evaluator that has high agreement with humans (validated on 20K annotations). We evaluate a model by measuring the fraction of times a powerful LLM (e.g. GPT-4) prefers the outputs from that model over outputs from a reference model. Our evaluators enable caching and output randomization by default.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#analysis&#34;&gt;&lt;strong&gt;Toolkit for building automatic evaluators&lt;/strong&gt;&lt;/a&gt;: a simple interface for building advanced automatic evaluators (e.g. with caching, batching, or multi-annotators) and analyzing them (quality, price, speed, statistical power, bias, variance etc).&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#data-release&#34;&gt;&lt;strong&gt;Human evaluation data&lt;/strong&gt;&lt;/a&gt;: 20K human preferences between a given and reference model on the &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_farm/tree/main&#34;&gt;AlpacaFarm&lt;/a&gt; evaluation set. 2.5K of these are cross-annotations (4 humans annotating the same 650 examples).&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/tatsu-lab/alpaca_eval/blob/main/alpaca_eval.json&#34;&gt;&lt;strong&gt;AlpacaEval dataset&lt;/strong&gt;&lt;/a&gt;: a simplification of &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_farm/tree/main&#34;&gt;AlpacaFarm&#39;s&lt;/a&gt; evaluation set, where &#34;instructions&#34; and &#34;inputs&#34; are merged into one field, and reference outputs are longer. &lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#data-release&#34;&gt;Details here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;When to use and not use AlpacaEval?&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt;&lt;strong&gt;When to use AlpacaEval?&lt;/strong&gt; Our automatic evaluator is a quick and cheap proxy for human evaluation of simple instruction-following tasks. It is useful if you have to run many evaluations quickly, e.g., during model development.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;When not to use AlpacaEval?&lt;/strong&gt; As any other automatic evaluator, AlpacaEval should &lt;strong&gt;not replace human evaluation in high-stake decision-making&lt;/strong&gt;, e.g., to decide on model release. In particular, AlpacaEval is limited by the fact that (1) the instructions in the eval set might not be representative of advanced usage of LLMs; (2) automatic evaluators may have biases such as favoring style over factuality of the answer; and (3) AlpacaEval does not measure the risks that a model could cause. Details in &lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#limitations&#34;&gt;limitations&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h1&gt;Quick Start&lt;/h1&gt; &#xA;&lt;p&gt;To install the stable release, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install alpaca-eval&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To install the nightly version, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install git+https://github.com/tatsu-lab/alpaca_eval&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you can use it as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export OPENAI_API_KEY=&amp;lt;your_api_key&amp;gt; # for more complex configs, e.g. using Azure or switching clients see client_configs/README.md &#xA;alpaca_eval --model_outputs &#39;example/outputs.json&#39; &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will print the leaderboard to the console, and save both the leaderboard and the annotations to the same directory as the &lt;code&gt;model_outputs&lt;/code&gt; file. Important parameters are the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;model_outputs&lt;/strong&gt; : A path to a json file for the outputs of the model to add to the leaderboard. Each dictionary should contain the keys &lt;code&gt;instruction&lt;/code&gt; and &lt;code&gt;output&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;annotators_config&lt;/strong&gt;: This is the annotator to use. We recommend using &lt;code&gt;weighted_alpaca_eval_gpt4_turbo&lt;/code&gt; ( default for AlpacaEval 2.0), which has a high agreement rate with our human annotation data, large context size, and is pretty cheap. For a comparison of all annotators see &lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#evaluators&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;reference_outputs&lt;/strong&gt;: The outputs of the reference model. Same format as &lt;code&gt;model_outputs&lt;/code&gt;. By default, this is &lt;code&gt;gpt4_turbo&lt;/code&gt; for AlpacaEval 2.0.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;output_path&lt;/strong&gt;: Path for saving annotations and leaderboard.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you don&#39;t have the model outputs, you can use &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_eval/tree/main#evaluating-a-model&#34;&gt;&lt;code&gt;evaluate_from_model&lt;/code&gt;&lt;/a&gt; and pass a local path or a name of a HuggingFace model, or a model from a standard API (OpenAI, Anthropic, Cohere, google, ...). Other commands:&lt;/p&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; alpaca_eval -- --help&lt;/code&gt;&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;SYNOPSIS&#xA;    alpaca_eval COMMAND&#xA;&#xA;COMMANDS&#xA;    COMMAND is one of the following:&#xA;&#xA;     evaluate&#xA;       Evaluate a model based on its outputs. This is the default entrypoint if no command is specified.&#xA;&#xA;     evaluate_from_model&#xA;       Evaluate a model from HuggingFace or an API provider. This is a wrapper around `evaluate` which includes generating from a desired model.&#xA;&#xA;     make_leaderboard&#xA;       Precompute and save an entire leaderboard for a given dataset / evaluator / set of models generations.&#xA;&#xA;     analyze_evaluators&#xA;       Analyze an evaluator and populates the evaluators leaderboard (agreement with human, speed, price,...).&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;For more information about each function use &lt;code&gt;alpaca_eval &amp;lt;command&amp;gt; -- --help&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Leaderboards and how to interpret them&lt;/h1&gt; &#xA;&lt;h2&gt;Models&lt;/h2&gt; &#xA;&lt;p&gt;Our leaderboards are computed on the &lt;a href=&#34;https://huggingface.co/datasets/tatsu-lab/alpaca_eval&#34;&gt;AlpacaEval dataset&lt;/a&gt;. We precomputed the leaderboard for important models using different baseline models and autoannotators. Our two main leaderboards (&#34;AlpacaEval 2.0&#34; and &#34;AlpacaEval&#34;) can be found &lt;a href=&#34;https://tatsu-lab.github.io/alpaca_eval/&#34;&gt;on this page&lt;/a&gt;. &#34;AlpacaEval 2.0&#34; uses &lt;code&gt;weighted_alpaca_eval_gpt4_turbo&lt;/code&gt; for the annotator and &lt;code&gt;gpt4_turbo&lt;/code&gt; for the baseline. &#34;AlpacaEval&#34; uses &lt;code&gt;alpaca_eval_gpt4&lt;/code&gt; for the annotator and &lt;code&gt;text_davinci_003&lt;/code&gt; for the baseline. For all precomputed leaderboards see &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_eval/tree/main/src/alpaca_eval/leaderboards&#34;&gt;here&lt;/a&gt;. Later we also show how to &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_eval#evaluating-a-model&#34;&gt;add your model&lt;/a&gt; to the leaderboard and how to make a &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_eval#making-a-new-leaderboard&#34;&gt;new leaderboard for your evaluator/dataset&lt;/a&gt;. See &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_eval/tree/main/src/alpaca_eval/models_configs&#34;&gt;here&lt;/a&gt; for the configs of all models that are available out of the box.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AlpacaEval minimal leaderboard&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Win Rate&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Std Error&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;gpt4&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;95.3&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;claude&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;88.4&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;chatgpt&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;86.1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;guanaco-65b&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;71.8&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;vicuna-13b&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;70.4&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;text_davinci_003&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;50.0&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;alpaca-farm-ppo-human&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;41.2&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;alpaca-7b&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;26.5&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;text_davinci_001&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;15.2&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;How exactly are those metrics computed?&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt;&lt;strong&gt;Win Rate&lt;/strong&gt;: the win rate measures the fraction of time the model&#39;s output is preferred over the reference&#39;s outputs (&lt;code&gt;test-davinci-003&lt;/code&gt; for AlpacaEval and &lt;code&gt;gpt4_turbo&lt;/code&gt; for AlpacaEval 2.0). More specifically, to compute the win rate we collect pairs of outputs of the desired model on every instruction from the ApacaEval dataset. We then pair each output with the output of our reference model (e.g. &lt;code&gt;text-davinci-003&lt;/code&gt;) on the same instruction. We then ask our automatic evaluator which output they prefer. See &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_eval/tree/main/src/alpaca_eval/evaluators_configs/alpaca_eval_gpt4&#34;&gt;AlpacaEval&#39;s&lt;/a&gt; and &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_eval/tree/main/src/alpaca_eval/evaluators_configs/weighted_alpaca_eval_gpt4_turbo&#34;&gt;AlpacaEval 2.0&#39;s&lt;/a&gt; prompts and configs, in particular we randomize the order of outputs to avoid position bias. We then average the preferences over all instructions in the dataset to get the win rate of the model over the baseline. If both outputs are exactly the same we use a half preference for both models.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Standard error&lt;/strong&gt;: this is the standard error (normalized by N-1) of the win rate, i.e., the preferences averaged over the different instructions.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;Details about our auto-annotator: &lt;code&gt;alpaca_eval_gpt4&lt;/code&gt;&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt;Our &lt;code&gt;alpaca_eval_gpt4&lt;/code&gt; ( see &lt;a href=&#34;#https://github.com/tatsu-lab/alpaca_eval/blob/main/src/alpaca_eval/evaluators_configs/alpaca_eval_gpt4/configs.yaml#L5&#34;&gt;configs&lt;/a&gt;) annotator averages over preferences, where preferences are obtained as follows:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;it takes in an instruction and a pair of outputs (from the desired model and the reference model)&lt;/li&gt; &#xA;  &lt;li&gt;if a preference was this triple was already computed, it returns it (i.e. it uses caching)&lt;/li&gt; &#xA;  &lt;li&gt;it randomizes the order of the outputs to avoid position bias&lt;/li&gt; &#xA;  &lt;li&gt;it formats the instruction and outputs into the &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_eval/raw/main/src/alpaca_eval/evaluators_configs/alpaca_eval_gpt4/alpaca_eval.txt&#34;&gt;following zero-shot prompt&lt;/a&gt;, which asks to order the outputs in order of preference&lt;/li&gt; &#xA;  &lt;li&gt;it completes the prompt using GPT4 with &lt;code&gt;temperature=0&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;it parses the preference from the completions and returns it&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;The annotator is a mix between (and was highly influenced by) &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_farm&#34;&gt;AlpacaFarm&lt;/a&gt; and &lt;a href=&#34;https://github.com/ray-project/aviary/tree/master&#34;&gt;Aviary&lt;/a&gt; evaluators. In particular, we use the same code as for AlpacaFarm (caching/randomization/hyperparameters) but use a ranking prompt similar to that of Aviary. We make changes to Aviary&#39;s prompt to decrease the bias for longer outputs. Details in &lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#related-work&#34;&gt;Related work&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;For AlpacaEval 2.0 we use &lt;code&gt;weighted_alpaca_eval_gpt4_turbo&lt;/code&gt;, which uses logprobs to compute continuous preference and uses GPT4_turbo as model ( see &lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#https://github.com/tatsu-lab/alpaca_eval/raw/main/src/alpaca_eval/evaluators_configs/weighted_alpaca_eval_gpt4_turbo/configs.yaml&#34;&gt;configs&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Evaluators&lt;/h2&gt; &#xA;&lt;p&gt;We evaluate different automatic annotators on the AlpacaEval set by comparing to 2.5K &lt;a href=&#34;https://huggingface.co/datasets/tatsu-lab/alpaca_eval/blob/main/alpaca_farm_human_crossannotations.json&#34;&gt;human annotations&lt;/a&gt; we collected (~650 instructions each with 4 human annotations). Below we show metrics for our suggested evaluators (&lt;code&gt;weighted_alpaca_eval_gpt4_turbo&lt;/code&gt;,&lt;code&gt;alpaca_eval_gpt4&lt;/code&gt;), for prior automatic evaluators (&lt;a href=&#34;https://github.com/tatsu-lab/alpaca_farm&#34;&gt;&lt;code&gt;alpaca_farm_greedy_gpt4&lt;/code&gt;&lt;/a&gt;,&lt;a href=&#34;https://aviary.anyscale.com/&#34;&gt;&lt;code&gt;aviary_gpt4&lt;/code&gt;&lt;/a&gt;,&lt;a href=&#34;https://chat.lmsys.org/&#34;&gt;&lt;code&gt;lmsys_gpt4&lt;/code&gt;&lt;/a&gt;), for humans (&lt;code&gt;humans&lt;/code&gt;), and for different base models with essentially the same prompt (&lt;code&gt;gpt4&lt;/code&gt;,&lt;code&gt;claude&lt;/code&gt;,&lt;code&gt;text_davinci_003&lt;/code&gt;,&lt;code&gt;chatgpt_fn&lt;/code&gt;,&lt;code&gt;guanaco_33b&lt;/code&gt;, &lt;code&gt;chatgpt&lt;/code&gt;). See &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_eval/tree/main/src/alpaca_eval/evaluators_configs&#34;&gt;here&lt;/a&gt; for the configs of all evaluators that are available out of the box and their associated metrics.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Human agreement&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Price [$/1000 examples]&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Time [seconds/1000 examples]&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Spearman corr.&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Pearson corr.&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Bias&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Variance&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Proba. prefer longer&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;alpaca_eval_gpt4&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;69.2&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;13.6&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1455&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.97&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.93&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;28.4&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;14.6&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.68&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;alpaca_eval_cot_gpt4_turbo_fn&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;68.6&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;6.3&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1989&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.97&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.90&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;29.3&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;18.4&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.67&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;alpaca_eval_gpt4_turbo_fn&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;68.1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;5.5&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;864&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.93&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.82&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;30.2&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;15.6&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.65&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;alpaca_eval_llama3_70b_fn&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;67.5&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.4&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;209&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.90&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.86&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;32.3&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;8.2&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.79&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;gpt4&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;66.9&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;12.5&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1037&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.88&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.87&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;31.5&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;14.6&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.65&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;alpaca_farm_greedy_gpt4&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;66.4&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;15.3&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;878&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.85&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.75&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;30.2&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;19.3&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.60&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;alpaca_eval_cot_gpt4_turbo_fn&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;65.7&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;4.3&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;228&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.78&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.77&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;33.9&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;23.7&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.61&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;humans&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;65.7&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;300.0&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;36800&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.0&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;34.3&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.64&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;claude&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;65.3&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;3.3&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;173&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.93&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.90&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;32.4&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;18.5&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.66&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;lmsys_gpt4&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;65.3&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;13.9&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;17982&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.98&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.97&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;31.6&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;15.9&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.74&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;text_davinci_003&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;64.1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;8.7&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;121&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.85&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.83&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;33.8&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;22.7&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.70&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;longest&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;62.2&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.0&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.27&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.56&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;37.8&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.0&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;chatgpt&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;57.3&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.8&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;285&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.72&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.71&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;39.4&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;34.1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.59&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;How exactly are those metrics computed?&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt;We now explain in words how we compute the metrics in the table above. &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_eval/raw/f05cbd651b79ac93906b19d01fe443b45828b0f2/src/alpaca_eval/analyze.py#L366&#34;&gt;The code is here&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Human agreement&lt;/strong&gt;: this measures the agreement between the current annotator and the majority preferences of humans on our ~650 annotations from our &lt;a href=&#34;https://huggingface.co/datasets/tatsu-lab/alpaca_eval/blob/main/alpaca_farm_human_crossannotations.json&#34;&gt;cross-annotation set&lt;/a&gt;, which contains 4 human annotations per example. To estimate the agreement between a single human (&lt;code&gt;humans&lt;/code&gt; row in the table above) and the majority of humans, we take one of the 4 annotations and compute the accuracy that it has when predicting the mode of the other 3 annotations. We then average this accuracy over all 4 annotations and over the 650 instructions to get the human agreement, i.e., we compute the expected (over humans and samples) leave-one-out agreement. If the mode is not unique, we take one of the modes at random. We perform exactly the same computation for the automatic annotators, so that the final numbers are comparable.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Price [$/1000 examples]&lt;/strong&gt;: this is the average price of every 1000 annotations. For humans, it is the price that &lt;a href=&#34;https://arxiv.org/abs/2305.14387&#34;&gt;we paid Mechanical Turkers&lt;/a&gt; to collect those annotations ($21/hour). If the price depends on the machine used to compute the annotations (e.g. Guanaco) we leave it empty.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Time [seconds/1000 examples]&lt;/strong&gt;: this is the average time it takes to compute 1000 annotations. For humans, it is the estimated median time that each Mechanical Turker took to annotate 1000 examples. For automatic annotators, it is the average time that it took us when running the annotations. Note that this can depend on API limits that are different for different users and the number of requests that the clusters are processing.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Spearman corr.&lt;/strong&gt;: this measures the Spearman correlation between a leaderboard computed with the auto-annotator&#39;s preference and the leaderboard computed with human preferences. As with &lt;code&gt;Human agreement&lt;/code&gt;, we use the human annotations from AlpacaFarm but we now consider the method-level agreement rather than only the sample-wise agreement with humans. Note that we only use have 9 models and so the correlation is not very reliable.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Pearson corr.&lt;/strong&gt;: same as with &lt;code&gt;Spearman corr.&lt;/code&gt; but with Pearson correlation.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Bias&lt;/strong&gt;: agreement between the most likely human label and the most likely automatic one. For automatic annotators we estimate it by sampling 4 different annotations for each example. The randomness here comes from the order of the outputs in the prompt, sampling from the LLM, and if applicable the order of the instruction in the batch and the choice of annotator in the pool. We then take the mode of the 4 annotations and compute the accuracy of the mode when predicting the mode of the 4 human annotations. Note that this is likely an overestimate on the real bias that we would get if we had an &#34;infinite&#34; number of cross-annotations. A low bias means that the annotator has in expectation the same preferences as humans. For the case of humans, the bias is zero by definition. Note that this is related to but not the standard statistical bias, because we take the mode instead of average over annotations and we consider 0-1 loss instead of squared loss.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Variance&lt;/strong&gt;: expected agreement a single automatic preference and the most likely one. We estimate it the same way as we estimated &#34;human agreement&#34; for humans, i.e., we take the expected leave one out error when predicting the mode of the 3 annotations using the 4th annotation. A low variance means that the annotator is consistent with its preference, i.e., if you sample from it with different seeds it will give the same result. As with the bias, this is not exactly the standard statistical variance, because we take the mode instead of average over annotations and we consider 0-1 loss instead of squared loss.&lt;/p&gt; &#xA; &lt;p&gt;Note that the &#34;human agreement&#34; is tightly related to the bias and variance. In particular, the variance measures the error due to the fact that we only use a single annotation while the bias aims to measure the irreducible error for the current annotator.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Proba. prefer longer&lt;/strong&gt;: this is the probability that the annotator prefers the longer output when one of the two outputs is significantly longer than the other (more than 30 characters difference).&lt;/p&gt; &#xA; &lt;p&gt;In the &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_eval/raw/main/src/alpaca_eval/evaluators_configs/README.md&#34;&gt;full table&lt;/a&gt; we also provide the following metrics:&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Proba. prefer lists&lt;/strong&gt;: this is the probability that the annotator prefers the output that contains a list/bullet points when one output does but not the other.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Proba. prefer 1&lt;/strong&gt;: this is the probability that the annotator prefers the first of the pair of outputs. All our proposed annotators randomize over outputs in the prompt, so this should be 0.5. Prior annotators, such as &lt;code&gt;lmsys&lt;/code&gt; and &lt;code&gt;aviary&lt;/code&gt;, do not.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;# parsed&lt;/strong&gt;: this is the number of examples that the annotator was able to parse.&lt;/p&gt; &#xA; &lt;p&gt;Note that if the variance and bias is empty, it means that we only performed one single annotation for each 648 example due to resource (time and price) constraints. This explains why the #parsed is 648, otherwise it should be 2592.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;Tips for choosing evaluators&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt;Overall we recommend using &lt;code&gt;annotators_config=weighted_alpaca_eval_gpt4_turbo&lt;/code&gt; if you want the high agreement with humans, and &lt;code&gt;annotators_config=chatgpt_fn&lt;/code&gt; if you are on a tight budget.&lt;/p&gt; &#xA; &lt;p&gt;When choosing an annotator we recommend you to consider the following (the first three are obvious):&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;&#34;Human agreement [%]&#34;&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;&#34;Price [$/1000 examples]&#34;&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;&#34;Time [seconds/1000 examples]&#34;&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;&#34;* corr.&#34;&lt;/code&gt; approx. &amp;gt; 0.7. It is important that the correlation is not too low, but we do not recommend using it as the main metric as the correlation is computed on only 9 models.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;&#34;Proba. prefer longer&#34;&lt;/code&gt; approx. &amp;lt; 0.7. Indeed, we found see that the majority of preference of human annotators have strong bias for longer answers (as shown by the high &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_eval/raw/main/src/alpaca_eval/evaluators_configs/README.md&#34;&gt;performance=62.2&lt;/a&gt; of the &lt;code&gt;&#34;longest&#34;&lt;/code&gt; evaluator that always prefers the longest output). This suggests that it might more of a bias with the human annotators. In order to avoid having leaderboards with strong biases for length, we suggest using automatic annotators with less than 0.7 &#34;Proba. prefer longer&#34;.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;&#34;Variance&#34;&lt;/code&gt; approx. &amp;lt; 0.2. We believe that a good evaluator should have as little variance as possible so that results are mostly reproducible. Note that variance can be desirable in the case where we are simulating humans as shown in &lt;a href=&#34;https://arxiv.org/abs/2305.14387&#34;&gt;AlpacaFarm&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;We filtered the annotators that do not satisfy those requirements in the table above (besides humans / ChatGPT / 003 / lmsys for reference purposes). For all results see &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_eval/raw/main/src/alpaca_eval/evaluators_configs/README.md&#34;&gt;here&lt;/a&gt;. In general, we found &lt;code&gt;weighted_alpaca_eval_gpt4_turbo&lt;/code&gt; to be a good trade-off between quality / price / time / variance / length bias.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;The above metrics are computed with respect to annotations from crowd-workers. Although useful, those annotations are not perfect, e.g., crowd-workers often favor style over factuality. We thus recommend users to validate automatic evaluators on their own instructions and human annotations. Details in &lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#limitations&#34;&gt;limitations&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Use-cases&lt;/h1&gt; &#xA;&lt;h2&gt;Evaluating a model&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; alpaca_eval evaluate -- --help&lt;/code&gt;&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;NAME&#xA;    alpaca_eval evaluate - Evaluate a model based on its outputs. This is the default entrypoint if no command is specified.&#xA;&#xA;SYNOPSIS&#xA;    alpaca_eval evaluate &amp;lt;flags&amp;gt;&#xA;&#xA;DESCRIPTION&#xA;    Evaluate a model based on its outputs. This is the default entrypoint if no command is specified.&#xA;&#xA;FLAGS&#xA;    --model_outputs=MODEL_OUTPUTS&#xA;        Type: Optional[Union]&#xA;        Default: None&#xA;        The outputs of the model to add to the leaderboard. Accepts data (list of dictionary, pd.dataframe, datasets.Dataset) or a path to read those (json, csv, tsv) or a function to generate those. Each dictionary (or row of dataframe) should contain the keys that are formatted in the prompts. E.g. by default `instruction` and `output` with optional `input`. If None, we just print the leaderboard.&#xA;    -r, --reference_outputs=REFERENCE_OUTPUTS&#xA;        Type: Union&#xA;        Default: &amp;lt;func...&#xA;        The outputs of the reference model. Same format as `model_outputs`. If None, the reference outputs are a specific set of Davinci 003 outputs on the AlpacaEval set:&#xA;    --annotators_config=ANNOTATORS_CONFIG&#xA;        Type: Union&#xA;        Default: &#39;alpaca_eval_gpt4_turbo_fn&#39;&#xA;        The path the (or list of dict of) the annotator&#39;s config file. For details see the docstring of `PairwiseAnnotator`.&#xA;    -n, --name=NAME&#xA;        Type: Optional[Optional]&#xA;        Default: None&#xA;        The name of the model to add to the leaderboard. If None we check if `generator is in model_outputs` if not we use &#34;Current model&#34;.&#xA;    -o, --output_path=OUTPUT_PATH&#xA;        Type: Union&#xA;        Default: &#39;auto&#39;&#xA;        Path to the directory where the new leaderboard and the annotations should be stored. If None we don&#39;t save. If `auto` we use `model_outputs` if it is a path, and otherwise use the directory from which we call the script.&#xA;    -p, --precomputed_leaderboard=PRECOMPUTED_LEADERBOARD&#xA;        Type: Union&#xA;        Default: &#39;auto&#39;&#xA;        The precomputed leaderboard or a path to it (json, csv, or tsv). The leaderboard should contain at least the column `win_rate`. If `auto` we will try to use the corresponding leaderboard for the reference outputs (only if in CORRESPONDING_OUTPUTS_LEADERBOARDS). If `None` we won&#39;t add other models from the leaderboard.&#xA;    --is_overwrite_leaderboard=IS_OVERWRITE_LEADERBOARD&#xA;        Type: bool&#xA;        Default: False&#xA;        Whether to overwrite the leaderboard if the model is already in it.&#xA;    -l, --leaderboard_mode_to_print=LEADERBOARD_MODE_TO_PRINT&#xA;        Type: Optional&#xA;        Default: &#39;minimal&#39;&#xA;        The mode of the leaderboard to use. Only used if the precomputed leaderboard has a column `mode`, in which case it will filter the leaderboard by this mode. If None keeps all.&#xA;    -c, --current_leaderboard_mode=CURRENT_LEADERBOARD_MODE&#xA;        Type: str&#xA;        Default: &#39;community&#39;&#xA;        The mode of the leaderboard for the current method.&#xA;    --is_return_instead_of_print=IS_RETURN_INSTEAD_OF_PRINT&#xA;        Type: bool&#xA;        Default: False&#xA;        Whether to return the metrics instead of printing the results.&#xA;    -f, --fn_metric=FN_METRIC&#xA;        Type: Union&#xA;        Default: &#39;pairwise_to_winrate&#39;&#xA;        The function or function name in `metrics.py` that will be used to convert preference to metrics. The function should take a sequence of preferences (0 for draw, 1 for base win, 2 when the model to compare wins) and return a dictionary of metrics and the key by which to sort the leaderboard.&#xA;    -s, --sort_by=SORT_BY&#xA;        Type: str&#xA;        Default: &#39;win_rate&#39;&#xA;        The key by which to sort the leaderboard.&#xA;    --is_cache_leaderboard=IS_CACHE_LEADERBOARD&#xA;        Type: Optional[Optional]&#xA;        Default: None&#xA;        Whether to save the result leaderboard to `precomputed_leaderboard`. If None we save only if max_instances not None. A preferred way of adding models to the leaderboard is to set `precomputed_leaderboard` to the previously saved leaderboard at `&amp;lt;output_path&amp;gt;/leaderboard.csv`.&#xA;    --max_instances=MAX_INSTANCES&#xA;        Type: Optional[Optional]&#xA;        Default: None&#xA;        The maximum number of instances to annotate. Useful for testing.&#xA;    --annotation_kwargs=ANNOTATION_KWARGS&#xA;        Type: Optional[Optional]&#xA;        Default: None&#xA;        Additional arguments to pass to `PairwiseAnnotator.annotate_head2head`.&#xA;    -A, --Annotator=ANNOTATOR&#xA;        Default: &amp;lt;class &#39;alpaca_eval.annotators.pairwise_evaluator.PairwiseAn...&#xA;        The annotator class to use.&#xA;    Additional flags are accepted.&#xA;        Additional arguments to pass to `PairwiseAnnotator`.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; alpaca_eval evaluate_from_model -- --help&lt;/code&gt;&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;NAME&#xA;    alpaca_eval evaluate_from_model - Evaluate a model from HuggingFace or an API provider. This is a wrapper around `evaluate` which includes generating from a desired model.&#xA;&#xA;SYNOPSIS&#xA;    alpaca_eval evaluate_from_model MODEL_CONFIGS &amp;lt;flags&amp;gt;&#xA;&#xA;DESCRIPTION&#xA;    Evaluate a model from HuggingFace or an API provider. This is a wrapper around `evaluate` which includes generating from a desired model.&#xA;&#xA;POSITIONAL ARGUMENTS&#xA;    MODEL_CONFIGS&#xA;        Type: Union&#xA;        A dictionary or path (relative to `models_configs`) to a yaml file containing the configuration of the model to decode from. If a directory,we search for &#39;configs.yaml&#39; in it. The keys in the first dictionary should be the generator&#39;s name, and the value should be a dictionary of the generator&#39;s configuration which should have the&#xA;&#xA;FLAGS&#xA;    -r, --reference_model_configs=REFERENCE_MODEL_CONFIGS&#xA;        Type: Optional[Union]&#xA;        Default: None&#xA;        Same as in `model_configs` but for the reference model. If None, we use the default Davinci003 outputs.&#xA;    -e, --evaluation_dataset=EVALUATION_DATASET&#xA;        Type: Union&#xA;        Default: &amp;lt;func...&#xA;        Path to the evaluation dataset or a function that returns a dataframe. If None, we use the default evaluation&#xA;    -a, --annotators_config=ANNOTATORS_CONFIG&#xA;        Type: Union&#xA;        Default: &#39;alpaca_eval_gpt4_turbo_fn&#39;&#xA;        Path to the annotators configuration or a dictionary. If None, we use the default annotators configuration.&#xA;    -o, --output_path=OUTPUT_PATH&#xA;        Type: Union&#xA;        Default: &#39;auto&#39;&#xA;        Path to save the generations, annotations and leaderboard. If auto saves at `results/&amp;lt;model_name&amp;gt;`&#xA;    -m, --max_instances=MAX_INSTANCES&#xA;        Type: Optional[int]&#xA;        Default: None&#xA;        Maximum number of instances to generate and evaluate. If None, we evaluate all instances.&#xA;    --is_strip_output=IS_STRIP_OUTPUT&#xA;        Type: bool&#xA;        Default: True&#xA;        Whether to strip trailing and leading whitespaces from the outputs.&#xA;    --is_load_outputs=IS_LOAD_OUTPUTS&#xA;        Type: bool&#xA;        Default: True&#xA;        Whether to try to load outputs from the output path. If True and outputs exist we only generate outputs for instructions that don&#39;t have outputs yet.&#xA;    -c, --chunksize=CHUNKSIZE&#xA;        Type: int&#xA;        Default: 64&#xA;        Number of instances to generate before saving. If None, we save after all generations.&#xA;    Additional flags are accepted.&#xA;        Other kwargs to `evaluate`&#xA;&#xA;NOTES&#xA;    You can also use flags syntax for POSITIONAL ARGUMENTS&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;To evaluate a model you need to:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Choose an evaluation set and compute outputs specified as &lt;code&gt;model_outputs&lt;/code&gt;. By default, we use the 805 examples from &lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#data-release&#34;&gt;AlpacaEval&lt;/a&gt;. To compute outputs on AlpacaEval use:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import datasets&#xA;&#xA;eval_set = datasets.load_dataset(&#34;tatsu-lab/alpaca_eval&#34;, &#34;alpaca_eval&#34;)[&#34;eval&#34;]&#xA;for example in eval_set:&#xA;    # generate here is a placeholder for your models generations&#xA;    example[&#34;output&#34;] = generate(example[&#34;instruction&#34;])&#xA;    example[&#34;generator&#34;] = &#34;my_model&#34; # name of your model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;if your model is a HuggingFace model or from a standard API provider (OpenAI, Anthropic, Cohere). Then you can directly use &lt;code&gt;alpaca_eval evaluate_from_model&lt;/code&gt; to also take care of generating outputs.&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Compute the reference outputs &lt;code&gt;reference_outputs&lt;/code&gt;. By default, we use precomputed outputs of &lt;a href=&#34;https://huggingface.co/datasets/tatsu-lab/alpaca_eval&#34;&gt;&lt;code&gt;gpt4_turbo&lt;/code&gt; on AlpacaEval&lt;/a&gt;. If you want to use a different model or a different dataset follow the same steps as (1.).&lt;/li&gt; &#xA; &lt;li&gt;Choose an evaluator specified via &lt;code&gt;annotators_config&lt;/code&gt;. We recommend using &lt;code&gt;alpaca_eval_gpt4_turbo_fn&lt;/code&gt;. For other options and comparisons see &lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#evaluators&#34;&gt;this table&lt;/a&gt;. Depending on the evaluator you might need to set the appropriate API_KEY in your environment or int the &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_eval/tree/main/client_configs&#34;&gt;client_configs&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Running all together:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;alpaca_eval --model_outputs &#39;example/outputs.json&#39; \&#xA;  --annotators_config &#39;alpaca_eval_gpt4_turbo_fn&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you don&#39;t have decoded outputs, you can use &lt;code&gt;evaluate_from_model&lt;/code&gt; which takes care of decoding (model and reference) for you. Here&#39;s an example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# need a GPU for local models&#xA;alpaca_eval evaluate_from_model \&#xA;  --model_configs &#39;oasst_pythia_12b&#39; \&#xA;  --annotators_config &#39;alpaca_eval_gpt4_turbo_fn&#39;      &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here the &lt;code&gt;model_configs&lt;/code&gt; and &lt;code&gt;reference_model_configs&lt;/code&gt; (optional) are paths to a directory that specifies the prompt, the model provider (here HuggingFace) and decoding parameters. See &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_eval/tree/main/src/alpaca_eval/models_configs&#34;&gt;this directory&lt;/a&gt; for examples. For all model providers that are available out-of-the-box see &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_eval/tree/main/src/alpaca_eval/decoders&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;Information about annotators&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Caching&lt;/strong&gt;: by default all annotations are cached on disk at &lt;code&gt;caching_path&lt;/code&gt;. Annotations are thus never recomputed, which makes annotations faster, cheaper and allow for reproducibility. This helps even when evaluating different models as many models have the same outputs.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Output randomization&lt;/strong&gt; by default, we randomize over the examples of outputs, as we found that annotators tend to prefer the first examples they see.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Batching&lt;/strong&gt; we provide code and examples to batch annotations, which decreases cost and time for annotations if the prompt is long. See for example &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_eval/tree/main/src/alpaca_eval/evaluators_configs/alpaca_farm_greedy_gpt4&#34;&gt;alpaca_farm_greedy_gpt4&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Pool of annotators&lt;/strong&gt; we provide code and examples to evaluate using a pool of automatic annotators, which is helpful for replicating the variance of &lt;a href=&#34;https://arxiv.org/abs/2305.14387&#34;&gt;human annotations&lt;/a&gt;. See for example &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_eval/tree/main/src/alpaca_eval/evaluators_configs/alpaca_farm&#34;&gt;alpaca_farm&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Seeding based on instructions&lt;/strong&gt; For reproducibility and more fair comparison between models, we seed all randomness (output order, order in batches, examples for each annotator in a pool) based on the instruction.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h2 tabindex=&#34;-1&#34; dir=&#34;auto&#34;&gt;Making a new leaderboard&lt;/h2&gt;&lt;/summary&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; alpaca_eval make_leaderboard -- --help&lt;/code&gt;&lt;/summary&gt; &#xA;  &lt;pre&gt;&lt;code&gt;NAME&#xA;    alpaca_eval make_leaderboard - Precompute and save an entire leaderboard for a given dataset / evaluator / set of models generations.&#xA;&#xA;SYNOPSIS&#xA;    alpaca_eval make_leaderboard &amp;lt;flags&amp;gt;&#xA;&#xA;DESCRIPTION&#xA;    Precompute and save an entire leaderboard for a given dataset / evaluator / set of models generations.&#xA;&#xA;FLAGS&#xA;    --leaderboard_path=LEADERBOARD_PATH&#xA;        Type: Optional[Union]&#xA;        Default: None&#xA;        The path to save the leaderboard to. The leaderboard will be saved as a csv file, if it already exists it will&#xA;    --annotators_config=ANNOTATORS_CONFIG&#xA;        Type: Union&#xA;        Default: &#39;alpaca_eval_gpt4_turbo_fn&#39;&#xA;        The path the (or list of dict of) the annotator&#39;s config file.&#xA;    --all_model_outputs=ALL_MODEL_OUTPUTS&#xA;        Type: Union&#xA;        Default: &amp;lt;fu...&#xA;        The outputs of all models to add to the leaderboard. Accepts data (list of dictionary, pd.dataframe, datasets.Dataset) or a path to read those (json, csv, tsv potentially with globbing) or a function to generate those. If the path contains a globbing pattern, we will read all files matching the pattern and concatenate them. Each dictionary (or row of dataframe) should contain the keys that are formatted in the prompts. E.g. by default `instruction` and `output` with optional `input`. It should also contain a column `generator` with the name of the current model.&#xA;    -r, --reference_outputs=REFERENCE_OUTPUTS&#xA;        Type: Union&#xA;        Default: &amp;lt;func...&#xA;        The outputs of the reference model. Same format as `all_model_outputs` but without needing `generator`. By default, the reference outputs are the 003 outputs on AlpacaEval set.&#xA;    -f, --fn_add_to_leaderboard=FN_ADD_TO_LEADERBOARD&#xA;        Type: Callable&#xA;        Default: &#39;evaluate&#39;&#xA;        The function to use to add a model to the leaderboard. If a string, it should be the name of a function in `main.py`. The function should take the arguments: `model_outputs`, `annotators_config`, `name`, `precomputed_leaderboard`, `is_return_instead_of_print`, `reference_outputs`.&#xA;    --leaderboard_mode=LEADERBOARD_MODE&#xA;        Type: str&#xA;        Default: &#39;verified&#39;&#xA;        The mode of the leaderboard to save all new entries with.&#xA;    -i, --is_return_instead_of_print=IS_RETURN_INSTEAD_OF_PRINT&#xA;        Type: bool&#xA;        Default: False&#xA;        Whether to return the metrics instead of printing the results.&#xA;    Additional flags are accepted.&#xA;        Additional arguments to pass to `fn_add_to_leaderboard`.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;/details&gt; &#xA; &lt;p&gt;If you want to make a new leaderboard using a single command (rather than multiple &lt;code&gt;alpaca_eval&lt;/code&gt; calls), for your desired evaluation set and evaluators, you can use the following:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;alpaca_eval make_leaderboard \&#xA;  --leaderboard_path &amp;lt;path_to_save_leaderboard&amp;gt; \&#xA;  --all_model_outputs &amp;lt;model_outputs_path&amp;gt; \&#xA;  --reference_outputs &amp;lt;reference_outputs_path&amp;gt; \&#xA;  --annotators_config &amp;lt;path_to_config.yaml&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;where:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;leaderboard_path&lt;/code&gt;: path to save the leaderboard to. The leaderboard will be saved as a csv file, if it already exists it will append.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;all_model_outputs&lt;/code&gt; : The json path to the outputs of all models to add to the leaderboard (as a single file or by globbing multiple files). Each dictionary should contain the keys (&lt;code&gt;instruction&lt;/code&gt; and &lt;code&gt;output&lt;/code&gt;) that are formatted in the prompts and a column &lt;code&gt;generator&lt;/code&gt; with the name of the current model. As an example see &lt;a href=&#34;https://huggingface.co/datasets/tatsu-lab/alpaca_eval/blob/main/alpaca_eval_all_outputs.json&#34;&gt;this file&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;reference_outputs&lt;/code&gt; the path to the outputs of the reference model. Each dictionary should contain the keys (&lt;code&gt;instruction&lt;/code&gt; and &lt;code&gt;output&lt;/code&gt;) that are formatted in the prompts. By default, the reference outputs are the 003 outputs on AlpacaEval set.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;annotators_config&lt;/code&gt;: The path to the annotator&#39;s config file. Defaults to &lt;code&gt;alpaca_eval_gpt4&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h2 tabindex=&#34;-1&#34; dir=&#34;auto&#34;&gt;Making a new evaluator&lt;/h2&gt;&lt;/summary&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; alpaca_eval analyze_evaluators -- --help&lt;/code&gt;&lt;/summary&gt; &#xA;  &lt;pre&gt;&lt;code&gt;NAME&#xA;    alpaca_eval analyze_evaluators - Analyze an evaluator and populates the evaluators leaderboard (agreement with human, speed, price,...).&#xA;&#xA;SYNOPSIS&#xA;    alpaca_eval analyze_evaluators &amp;lt;flags&amp;gt;&#xA;&#xA;DESCRIPTION&#xA;    Analyze an evaluator and populates the evaluators leaderboard (agreement with human, speed, price,...).&#xA;&#xA;FLAGS&#xA;    --annotators_config=ANNOTATORS_CONFIG&#xA;        Type: Union&#xA;        Default: &#39;alpaca_eval_gpt4_turbo_fn&#39;&#xA;        The path the (or list of dict of) the annotator&#39;s config file.&#xA;    -A, --Annotator=ANNOTATOR&#xA;        Default: &amp;lt;class &#39;alpaca_eval.annotators.pairwise_evaluator.PairwiseAn...&#xA;        The annotator class to use.&#xA;    --analyzer_kwargs=ANALYZER_KWARGS&#xA;        Type: Optional[Optional]&#xA;        Default: None&#xA;        Additional arguments to pass to the analyzer.&#xA;    -p, --precomputed_leaderboard=PRECOMPUTED_LEADERBOARD&#xA;        Type: Union&#xA;        Default: PosixPath(&#39;/Users/yanndubois/Desktop/GitHub/alpaca_eval/src/...&#xA;        The precomputed (meta)leaderboard of annotators or a path to it (json, csv, or tsv).&#xA;    --is_save_leaderboard=IS_SAVE_LEADERBOARD&#xA;        Type: bool&#xA;        Default: False&#xA;        Whether to save the leaderboard (ie analyzed results).&#xA;    --is_return_instead_of_print=IS_RETURN_INSTEAD_OF_PRINT&#xA;        Type: bool&#xA;        Default: False&#xA;        Whether to return the leaderboard (ie analyzed results). If True, it will not print the results.&#xA;    --is_overwrite_leaderboard=IS_OVERWRITE_LEADERBOARD&#xA;        Type: bool&#xA;        Default: False&#xA;        Whether to overwrite the leaderboard if it already exists.&#xA;    -m, --max_instances=MAX_INSTANCES&#xA;        Type: Optional[Optional]&#xA;        Default: None&#xA;        The maximum number of instances to analyze.&#xA;    --is_single_annotator=IS_SINGLE_ANNOTATOR&#xA;        Type: bool&#xA;        Default: False&#xA;        Whether to analyze a single annotator. If True, will not be able to estimate the annotator&#39;s bias.&#xA;    -l, --leaderboard_mode_to_print=LEADERBOARD_MODE_TO_PRINT&#xA;        Type: str&#xA;        Default: &#39;minimal&#39;&#xA;        The mode of the leaderboard to print.&#xA;    -c, --current_leaderboard_mode=CURRENT_LEADERBOARD_MODE&#xA;        Type: str&#xA;        Default: &#39;minimal&#39;&#xA;        The mode of the leaderboard to save all new entries with.&#xA;    -o, --output_path=OUTPUT_PATH&#xA;        Type: Union&#xA;        Default: &#39;auto&#39;&#xA;        Path to save the leaderboard and annotataions. If None, we don&#39;t save.&#xA;    Additional flags are accepted.&#xA;        Additional arguments to pass to `Annotator`.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;/details&gt; &#xA; &lt;p&gt;AlpacaEval provides a simple way of making new evaluators. All you need is to make a new &lt;code&gt;configs.yaml&lt;/code&gt; configuration file, which you will then pass as &lt;code&gt;--annotators_config &amp;lt;path_to_config.yaml&amp;gt;&lt;/code&gt; to &lt;code&gt;alpaca_eval&lt;/code&gt;. Here are some ways you can make a new evaluator:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Changing the prompt&lt;/strong&gt;: Write a new prompt in a text file and specify the path in &lt;code&gt;prompt_template&lt;/code&gt; of the configuration file. Paths are relative to the configuration file.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Changing decoding parameters&lt;/strong&gt;: Specify the desired parameters in &lt;code&gt;completions_kwargs&lt;/code&gt; in the configuration file. To see all available parameters refer to the docstrings of the corresponding function &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_eval/raw/main/src/alpaca_eval/decoders/__init__.py&#34;&gt;in this file&lt;/a&gt; specified by &lt;code&gt;fn_completions&lt;/code&gt; in the configuration file.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Changing the model&lt;/strong&gt;: Specify the desired model in &lt;code&gt;model_name&lt;/code&gt; and the corresponding prompt in &lt;code&gt;prompt_template&lt;/code&gt;. If the model comes from another provider you will have to change &lt;code&gt;fn_completions&lt;/code&gt; which maps to the corresponding function in &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_eval/raw/main/src/alpaca_eval/decoders/__init__.py&#34;&gt;this file&lt;/a&gt;. We provide &lt;code&gt;fn_completions&lt;/code&gt; functions to use models from OpenAI, Anthropic, Cohere, or HuggingFace. To install packages needed for all providers use &lt;code&gt;pip install alpaca_eval[all]&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt;&lt;b&gt;Other parameters in the configuration file&lt;/b&gt;&lt;/summary&gt; &#xA;  &lt;p&gt;The easiest is to check the docstrings of &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_eval/raw/main/src/alpaca_eval/annotators/pairwise_evaluator.py#L537&#34;&gt;&lt;code&gt;SinglePairwiseAnnotator&lt;/code&gt;&lt;/a&gt;. Here are some important ones:&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code&gt;Parameters&#xA;----------&#xA;prompt_template : path&#xA;    A prompt that will be given to `fn_prompter` or path to the prompts. Path is relative to&#xA;    `evaluators_configs/`&#xA;&#xA;fn_completion_parser : callable or str&#xA;    Function in `completion_parsers.py` to use for parsing the completions into preferences. For each completion,&#xA;    the number of preferences should be equal to the batch_size if not we set all the preferences in that batch to&#xA;    NaN.&#xA;&#xA;completion_parser_kwargs : dict&#xA;    Kwargs for fn_completion_parser.&#xA;&#xA;fn_completions : callable or str&#xA;    Function in `decoders.py` to use for decoding the output.&#xA;&#xA;completions_kwargs : dict&#xA;    kwargs for fn_completions. E.g. model_name, max_tokens, temperature, top_p, top_k, stop_seq.&#xA;&#xA;is_randomize_output_order : bool&#xA;    Whether to randomize output_1, output_2 when formatting.&#xA;&#xA;batch_size : int&#xA;    Number of examples that will be added in a single prompt.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;/details&gt; &#xA; &lt;p&gt;Once you made the evaluator you can also analyze it and add it to the &lt;em&gt;evaluator&#39;s&lt;/em&gt; &lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#evaluators&#34;&gt;leaderboard&lt;/a&gt; using the following command:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;alpaca_eval analyze_evaluators --annotators_config &#39;&amp;lt;path_to_config.yaml&amp;gt;&#39;    &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;To estimate the bias and variance this evaluates every example with 4 seeds, i.e., 2.5K evaluation. If you want a cheaper evaluation you can use a single seed using &lt;code&gt;--is_single_annotator True&lt;/code&gt; which will skip the estimation of bias and variance.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;We are accepting PRs for new models, evaluators, and eval sets, in addition to bug fixes. We will update the &lt;a href=&#34;https://tatsu-lab.github.io/alpaca_eval/&#34;&gt;leaderboard website&lt;/a&gt; regularly with new community contributions. We have also created a &lt;a href=&#34;https://discord.gg/GJMxJSVZZM&#34;&gt;support discord&lt;/a&gt; for AlpacaEval in case you run into any issues and wish to ask help from the community.&lt;/p&gt; &#xA;&lt;p&gt;To get started, please first fork the repo, and install the package from source &lt;code&gt;pip install -e .&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributing a model&lt;/h2&gt; &#xA;&lt;p&gt;First, you&#39;ll need to add a model config definition in the &lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/src/alpaca_eval/models_configs/&#34;&gt;models_configs&lt;/a&gt; folder. As an example, you can look at the &lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/src/alpaca_eval/models_configs/falcon-7b-instruct/configs.yaml&#34;&gt;falcon-7b-instruct yaml&lt;/a&gt;. Please make sure the folder name and key name in the yaml match exactly.&lt;/p&gt; &#xA;&lt;p&gt;Then, please follow the steps in &lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#evaluating-a-model&#34;&gt;Evaluating a model&lt;/a&gt; to run inference on the model to produce outputs on the eval set and score the model according to one of the evaluators. An example command may look like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;alpaca_eval evaluate_from_model \&#xA;  --model_configs &#39;falcon-7b-instruct&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After running this command, you should have generated an outputs json and a new entry in the corresponding &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_eval/tree/main/src/alpaca_eval/leaderboards/data_AlpacaEval&#34;&gt;leaderboard file&lt;/a&gt;. Please make a PR with the config, outputs file, and updated leaderboard.&lt;/p&gt; &#xA;&lt;p&gt;Concretely you should do something like:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Fork the repository in github&lt;/li&gt; &#xA; &lt;li&gt;Clone the forked repository &lt;code&gt;git clone &amp;lt;URL&amp;gt;&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Make a model config at &lt;code&gt;src/alpaca_eval/models_configs/&amp;lt;model_name&amp;gt;&lt;/code&gt; and evaluate it &lt;code&gt;evaluate_from_model --model_configs &#39;&amp;lt;model_name&amp;gt;&#39;&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Add the model configs, output, and leaderboard entry to the forked repository&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git add src/alpaca_eval/models_configs/&amp;lt;model_name&amp;gt; # add the model config&#xA;git add src/alpaca_eval/leaderboards/ # add the actual leaderboard entry&#xA;git add src/alpaca_eval/metrics/weights # add the weights for LC&#xA;git add -f results/&amp;lt;model_name&amp;gt;/model_outputs.json # force add the outputs on the dataset&#xA;git add -f results/&amp;lt;model_name&amp;gt;/*/annotations.json # force add the evaluations from the annotators&#xA;git commit -m &#34;Add &amp;lt;model_name&amp;gt; to AlpacaEval&#34;&#xA;git push&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;Create a &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_eval/pulls&#34;&gt;pull request on AlpacaEval&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Note: if you are generating outputs outside of AlpacaEval you should still add a model config but with &lt;code&gt;fn_completions: null&lt;/code&gt;. See &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_eval/raw/main/src/alpaca_eval/models_configs/dolphin-2.2.1-mistral-7b/configs.yaml&#34;&gt;this config&lt;/a&gt; for an example.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h3 tabindex=&#34;-1&#34; dir=&#34;auto&#34;&gt;Getting your model verified&lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img align=&#34;center&#34; alt=&#34;verified.png&#34; src=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/figures/verified.png&#34; width=&#34;500&#34;&gt; &lt;/p&gt; &#xA; &lt;p&gt;A verified result in AlpacaEval indicates that a core maintainer has decoded the outputs from the model and performed the evaluation. Unfortunately, we, the AlpacaEval maintainers, lack the resources to verify all the models and so we will only do that for models that are in the top-10 of the leaderboard. We apologize for any inconvenience this may cause and appreciate your understanding. To have your model verified, please follow the steps below:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Contact &lt;code&gt;@yann&lt;/code&gt; or &lt;code&gt;@rtaori&lt;/code&gt; on Discord, or email us if you have our email, providing a brief rationale for why your model should be verified.&lt;/li&gt; &#xA;  &lt;li&gt;Await our response and approval before proceeding.&lt;/li&gt; &#xA;  &lt;li&gt;Prepare a script to decode from your model that does not require a GPU, typically the same script used for your model contribution. It should run using &lt;code&gt;alpaca_eval evaluate_from_model --model_configs &#39;&amp;lt;your_model_name&amp;gt;&#39;&lt;/code&gt; without requiring a local GPU.&lt;/li&gt; &#xA;  &lt;li&gt;Generate temporary API keys for running the script and share them with us. Specifically, we need the keys for both decoding your model and for evaluation (e.g., OpenAI or Anthropic key).&lt;/li&gt; &#xA;  &lt;li&gt;We will execute &lt;code&gt;alpaca_eval evaluate_from_model --model_configs &#39;&amp;lt;your_model_name&amp;gt;&#39;&lt;/code&gt;, update the results, and inform you so that you can revoke the temporary keys.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;Note that we will not re-evaluate the same model. Due to sampling variance, the results might slightly differ from your initial ones. We will replace your previous community results with the verified ones.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h2 tabindex=&#34;-1&#34; dir=&#34;auto&#34;&gt;Contributing an evaluator&lt;/h2&gt;&lt;/summary&gt; &#xA; &lt;p&gt;Please first follow the directions in &lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#making-a-new-evaluator&#34;&gt;Making a new evaluator&lt;/a&gt;. Once you&#39;re created the annotator config, we ask that you create a new leaderboard for the annotator by evaluating the minimal set of models. The outputs for these models can be found by downloading &lt;a href=&#34;https://huggingface.co/datasets/tatsu-lab/alpaca_eval/blob/main/alpaca_eval_all_outputs.json&#34;&gt;alpaca_eval_all_outputs.json&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;alpaca_eval make_leaderboard \&#xA;  --leaderboard_path src/alpaca_eval/leaderboards/data_AlpacaEval/&amp;lt;evaluator&amp;gt;_leaderboard.csv \&#xA;  --all_model_outputs alpaca_eval_all_outputs.json \&#xA;  --annotators_config &amp;lt;evaluator_config&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Then, please create a PR with the annotator config and leaderboard csv.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h2 tabindex=&#34;-1&#34; dir=&#34;auto&#34;&gt;Contributing an eval set&lt;/h2&gt;&lt;/summary&gt; &#xA; &lt;p&gt;To contribute a new eval set, you&#39;ll first need to specify a set of textual instructions. Then, you&#39;ll need to specify a set of reference outputs (model win-rates are computed against this reference). For ease of use, you may use the default &lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/src/alpaca_eval/models_configs/text_davinci_003/&#34;&gt;text-davinci-003&lt;/a&gt; reference config.&lt;/p&gt; &#xA; &lt;p&gt;Place these together into a json, where each entry specifies the fields &lt;code&gt;instruction&lt;/code&gt;, &lt;code&gt;output&lt;/code&gt;, and &lt;code&gt;generator&lt;/code&gt;. You can look to &lt;a href=&#34;https://huggingface.co/datasets/tatsu-lab/alpaca_eval/blob/main/alpaca_eval.json&#34;&gt;alpaca_eval.json&lt;/a&gt; as a guide (the &lt;code&gt;dataset&lt;/code&gt; field is not necessary).&lt;/p&gt; &#xA; &lt;p&gt;Finally, we ask that you create a minimal leaderboard on this new evaluation set. You can do this with the following:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;alpaca_eval make_leaderboard \&#xA;  --leaderboard_path &amp;lt;src/alpaca_eval/leaderboards/data_AlpacaEval/your_leaderboard_name.csv&amp;gt; \&#xA;  --all_model_outputs alpaca_eval_all_outputs.json \&#xA;  --reference_outputs &amp;lt;path_to_json_file&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Please submit a PR with the eval set json and corresponding leaderboard csv.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h2 tabindex=&#34;-1&#34; dir=&#34;auto&#34;&gt;Contributing a completion function&lt;/h2&gt;&lt;/summary&gt; &#xA; &lt;p&gt;Currently, we allow different completion functions, e.g., &lt;code&gt;openai&lt;/code&gt;, &lt;code&gt;anthropic&lt;/code&gt;, &lt;code&gt;huggingface_local&lt;/code&gt;, &lt;code&gt;huggingface_hub_api&lt;/code&gt; ... If you want to contribute a new completion function / API with which to perform inference then follow those steps:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;add a file &#xA;   &lt;name&gt;&#xA;    .py with a function &#xA;    &lt;code&gt;&amp;lt;name&amp;gt;_completions(prompts : Sequence[str], model_name :str, ... )&lt;/code&gt; in the &#xA;    &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_eval/tree/main/src/alpaca_eval/decoders&#34;&gt;decoder folder&lt;/a&gt;. This function should take as argument the prompts + kwargs and return the completions. Please look at other completion functions in the directory for templates. E.g. &#xA;    &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_eval/raw/main/src/alpaca_eval/decoders/huggingface_local.py&#34;&gt;huggingface_local_completions&lt;/a&gt; or &#xA;    &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_eval/raw/main/src/alpaca_eval/decoders/anthropic.py&#34;&gt;anthropic&lt;/a&gt;.&#xA;   &lt;/name&gt;&lt;/li&gt; &#xA;  &lt;li&gt;add &lt;code&gt;&amp;lt;name&amp;gt;_completions&lt;/code&gt; and dependencies in &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_eval/raw/main/src/alpaca_eval/decoders/__init__.py&#34;&gt;&lt;strong&gt;init&lt;/strong&gt;&lt;/a&gt; . Again you can follow the example of &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_eval/raw/main/src/alpaca_eval/decoders/__init__.py#L30&#34;&gt;huggingface_local_completions&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;update optional dependencies in &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_eval/raw/main/setup.py&#34;&gt;setup.py&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;add a model you want to evaluate in the &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_eval/tree/main/src/alpaca_eval/models_configs&#34;&gt;models configs&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;evaluate your model using &lt;code&gt;alpaca_eval evaluate_from_model --model_configs &#39;&amp;lt;model_configs&amp;gt;&#39;&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;(optional) push the results from the previous model on AlpacaEval leaderboard following &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_eval/tree/main#contributing-a-model&#34;&gt;those steps&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;Feel free to start a PR early, we&#39;ll be able to provide some help in the process!&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h1&gt;Limitations&lt;/h1&gt; &#xA;&lt;p&gt;The AlpacaEval evaluation pipeline, like other current evaluators have important limitations and should therefore not be used as replacement for human evaluation in important settings, such as to decide whether a model is ready to be deployed. Those can broadly be clustered into 3 categories:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Instructions might not be representative of real-usage&lt;/strong&gt;: the AlpacaEval set contains examples from a variety of datasets (&lt;a href=&#34;https://github.com/yizhongw/self-instruct&#34;&gt;self-instruct&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/datasets/OpenAssistant/oasst1/viewer/OpenAssistant--oasst1/validation&#34;&gt;open-assistant&lt;/a&gt;, &lt;a href=&#34;https://lmsys.org/blog/2023-03-30-vicuna/&#34;&gt;vicuna&lt;/a&gt;, &lt;a href=&#34;https://github.com/arnav-gudibande/koala-test-set&#34;&gt;koala&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/datasets/Anthropic/hh-rlhf/viewer/Anthropic--hh-rlhf/test&#34;&gt;hh-rlhf&lt;/a&gt;) which might not be representative of real-usage and advanced applications of better models like GPT4. This likely makes the best closed models (GPT4 / Claude / ChatGPT / ...) seem more similar to the open models than what they are. Indeed, those closed models seem to be pretrained/finetuned on much more diverse data. See for example &lt;a href=&#34;https://medium.com/@marcotcr/exploring-chatgpt-vs-open-source-models-on-slightly-harder-tasks-aa0395c31610&#34;&gt;this blog&lt;/a&gt; for preliminary results on more complex instructions. Note, however, that in &lt;a href=&#34;https://arxiv.org/abs/2305.14387&#34;&gt;AlpacaFarm&lt;/a&gt; we showed that win-rates on our evaluation set are highly correlated (0.97 R2) with win-rates on instructions from user interactions with the Alpaca Demo. Furthermore, the AlpacaEval leaderboard shows larger gap between the open models and OpenAI models than other leaderboards ( e.g. &lt;a href=&#34;https://lmsys.org/blog/2023-03-30-vicuna/&#34;&gt;lmsys&lt;/a&gt;).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Biases of automatic annotators&lt;/strong&gt;: the raw automatic annotators seem to have implicit biases. In particular, we found that they tend to prefer longer outputs and outputs that contain lists (e.g. 0.68 / 0.69 for &lt;code&gt;alpaca_eval_gpt4&lt;/code&gt; and 0.62 / 0.58 for &lt;code&gt;claude&lt;/code&gt;). Although we found that humans have similar biases (0.64 / 0.61), we believe that this could be more of a limitation of human annotation pipeline we used rather than a true human bias. More generally, through qualitative analysis, we found that automatic annotators give more importance to the style of the output than its content (e.g. factuality). Finally, we found that automatic evaluators tend to prefer outputs from models that are similar (likely trained on the same data) as suggested by the big difference between ChatGPT/GPT4 on &lt;code&gt;claude&lt;/code&gt;&#39;s and &lt;code&gt;alpaca_eval_gpt4&lt;/code&gt;&#39;s leaderboard. Note that the length bias is partially mitigated in our length-controlled win-rates.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Lack of safety evaluation&lt;/strong&gt;: importantly, AlpacaEval only evaluates the instruction-following capabilities of models rather than the harm that they could cause (e.g. toxic behavior or bias). As a result the small gap between current ChatGPT and the best open source models &lt;strong&gt;should not&lt;/strong&gt; be interpreted as if that the latter are ready to be deployed.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Beyond those limitations about the evaluation pipelines, there are also limitations about our validation of the evaluators and our &lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#analyzing-an-eval-set&#34;&gt;proposed approach&lt;/a&gt; to selecting evaluation sets.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;Limitations about our validation pipeline&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt;First, our validation of evaluators based on human cross-annotations suffers from the following limitations: (1) we qualitatively found that our crowd-workers tend to also favor style such as length and presence of lists over factuality; (2) this does not validate whether win-rates against a reference model is a good evaluation strategy in the first place; (3) preferences from 16 crowd-workers are not representative of preferences of all humans.&lt;/p&gt; &#xA; &lt;p&gt;Second, our suggested approach to selecting evaluation sets based on statistical power suffers from the following limitations: (1) statistical power does not ensure the right direction, e.g. you can have an unnatural set of instructions where Alpaca &#34;performs&#34; better than better model; and (2) this can push users to select data to support the hypothesis that they want to validate.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h1&gt;Additional analysis and plots&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Caution&lt;/strong&gt;: all the following results are about AlpacaEval 1.0 and have not been updated since&lt;/p&gt; &#xA;&lt;h2&gt;Length-controlled AlpacaEval (LCAE)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Length-controlled AlpacaEval Visualizations:&lt;/strong&gt; &lt;a href=&#34;https://colab.research.google.com/github/tatsu-lab/alpaca_eval/blob/main/notebooks/figured_length_controlled.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;analyzing an evaluator&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Length-controlled AlpacaEval Development:&lt;/strong&gt; &lt;a href=&#34;https://colab.research.google.com/github/tatsu-lab/alpaca_eval/blob/main/notebooks/length_controlled.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;analyzing an evaluator&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The notebook shows different options that we considered for mitigating the length bias of automatic annotators.&lt;/p&gt; &#xA;&lt;p&gt;Here we briefly summarize the main results. Namely:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;LCAE increases the correlation with Chat Arena to 0.98&lt;/strong&gt; from 0.94 for AlpacaEval 2.0. This makes LCAE the most highly correlated benchmark with Chat Arena as seen in the plot below.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p float=&#34;left&#34; align=&#34;middle&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/figures/chat_correlations.png&#34; alt=&#34;LC AlpacaEval is the most highly correlated benchmark with Chat Arena.&#34; width=&#34;500&#34;&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;LCAE decreases length gameability&lt;/strong&gt; one of the major issues of AlpacaEval is that you can increase your win-rate by increasing the length of your outputs. For example, in AlpacaEval 2.0 the win-rate for the baseline (50%) increases to 64% when prompted to “give as much detail as possible” and decreases to 23% when prompted to “be as concise as possible while still providing all the necessary information to answer the question”. More generally the relative length gameability was ~21% for AlpacaEval and decreases to ~6% for LCAE, so it&#39;s 3x less gameable through prompt length. This is shown in the plot below.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p float=&#34;left&#34; align=&#34;middle&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/figures/length_gameability.png&#34; alt=&#34;LC AlpacaEval decreases length gameability of the benchmark.&#34; width=&#34;500&#34;&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;We can predict performance for different baselines&lt;/strong&gt; One other benefit of using a GLM for controlling for length bias. Is that we now have a model that can predict the win-rate of a model for different baselines. In particular, our GLM has many nice properties, for example &lt;code&gt;win_rate(m,b) = 1 - win_rate(b,m) \in [0,1]&lt;/code&gt; and &lt;code&gt;win_rate(m,m) = 0.5&lt;/code&gt;. This is shown in the plot below.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p float=&#34;left&#34; align=&#34;middle&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/figures/different_baselines.png&#34; alt=&#34;Predicted win rate for different baselines&#34; width=&#34;500&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Finally, note that we are only controlling for length bias. There are other known biases that we are not controlling for, such as the fact that auto-annotators prefer outputs similar to their model. Although we could control for that, in practice we have found that to be less of an issue than length bias. For two reasons (1) this mostly a single model in the leaderboard because fine-tuning on outputs from the auto-annotator doesn&#39;t seem to have doesn&#39;t seem to impact the win-rate as much, and (2) the bias is actually less strong that what one could think. For example we show below a subset of the leaderboards auto-annotated by three different models, and we see that the ranking of models is exactly the same. In particular, &lt;code&gt;claude-3-opus&lt;/code&gt; prefers &lt;code&gt;gpt4_preview&lt;/code&gt;, and &lt;code&gt;mistral-large&lt;/code&gt; prefers the former two.&lt;/p&gt; &#xA;&lt;p float=&#34;left&#34; align=&#34;middle&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/figures/annotator_bias.png&#34; alt=&#34;Leaderboard by different auto-annotators&#34; width=&#34;500&#34;&gt; &lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h2 tabindex=&#34;-1&#34; dir=&#34;auto&#34;&gt;Analyzing an evaluator&lt;/h2&gt;&lt;/summary&gt; &#xA; &lt;p&gt;&lt;strong&gt;Caution&lt;/strong&gt;: all the following results are about AlpacaEval 1.0 and have not been updated since&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Analyzing evaluators:&lt;/strong&gt; &lt;a href=&#34;https://colab.research.google.com/github/tatsu-lab/alpaca_eval/blob/main/notebooks/analyzing_annotators.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;analyzing an evaluator&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;As we saw in &lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#evaluators&#34;&gt;the evaluator&#39;s leaderboard&lt;/a&gt;, there are many metrics to consider when selecting an evaluator, e.g. the quality, price, and speed. To assist with selection of the evaluator we provide a few functions to plot those metrics. The following shows for example the price/time/agreement of the different evaluators.&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/figures%2Fplot_quality_vs_price_and_time.png&#34; alt=&#34;plot_quality_vs_price_and_time.png&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;Here we see that &lt;code&gt;alpaca_eval_gpt4&lt;/code&gt; performs very well and is better than humans on all the considered metrics.&lt;/p&gt; &#xA; &lt;p&gt;Previously we only considered the agreement with human annotators overall. An additional validation that one could do is checking whether making a leaderboard using our automatic annotator gives similar results as a leaderboard from humans. To enable such analysis, we release &lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#data-release&#34;&gt;human annotations&lt;/a&gt; of outputs from 22 methods from &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_farm&#34;&gt;AlpacaFarm&lt;/a&gt; =&amp;gt; 22*805 = ~18K annotations. As a result we can test the correlation between the win-rates of the 22 models as evaluated by the humans and our automatic annotator. Note that this is arguably a better way of selecting an automatic evaluator than using &#34;human agreement [%]&#34; but is expensive given that it requires 18K annotations. The plot below shows such correlation for the &lt;code&gt;alpaca_eval_gpt4&lt;/code&gt; evaluator.&lt;/p&gt; &#xA; &lt;p float=&#34;left&#34; align=&#34;middle&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/figures/plot_winrate_correlations_alpaca_eval.png&#34; alt=&#34;Correlation between humans and alpaca_eval_gpt4&#34; width=&#34;400&#34;&gt; &lt;/p&gt; &#xA; &lt;p&gt;We see that the &lt;code&gt;alpaca_eval_gpt4&lt;/code&gt; leaderboard is highly correlated (0.94 Pearson correlation) to the leaderboard from humans, which further suggests that automatic evaluation is a good proxy for human evaluation. For the code and more analysis, see &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_eval/raw/main/notebooks/analyzing_annotators.ipynb&#34;&gt;this notebook&lt;/a&gt;, or the colab notebook above.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h2 tabindex=&#34;-1&#34; dir=&#34;auto&#34;&gt;Analyzing an eval set&lt;/h2&gt;&lt;/summary&gt; &#xA; &lt;p&gt;&lt;strong&gt;Caution&lt;/strong&gt;: all the following results are about AlpacaEval 1.0 and have not been updated since.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Making evaluation sets:&lt;/strong&gt; &lt;a href=&#34;https://colab.research.google.com/github/tatsu-lab/alpaca_eval/blob/main/notebooks/analyzing_evalset.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;analyzing an evaluator&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;When creating an evaluation set there are two main factors to consider: how much data to use? and what data?&lt;/p&gt; &#xA; &lt;p&gt;One way of answering those question is by considering a leaderboard of models that you believe are of different quality and checking what and how much data is needed to distinguish between them in a statistically significant way. We will do so below using a paired t-test to test if the difference in win-rates between every pair of models is statistically significant.&lt;/p&gt; &#xA; &lt;p&gt;First, let us consider the question of how much data to use. Below we show the number of random samples needed from AlpacaEval for the paired t-test to give a p-value &amp;lt; 0.05 for each pair of models in the minimal &lt;code&gt;alpaca_eval_gpt4&lt;/code&gt; leaderboard. Grey cells correspond to pairs that are not significantly different on the 805 samples. y- and x-axis are ordered by the win-rate of the first and second model respectively.&lt;/p&gt; &#xA; &lt;p float=&#34;left&#34; align=&#34;middle&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/figures/plot_paired_ttest_nsamples.png&#34; alt=&#34;Number of samples needed to distinguish pairs in the Claude leaderboard&#34; width=&#34;500&#34;&gt; &lt;/p&gt; &#xA; &lt;p&gt;We see that most models can already be distinguished with 50 samples, and that 150 samples allows distinguishing the majority of pairs (74 out of 78). This suggests that we can decrease the evaluation set size by a factor of 4 when testing two models that have similar performance gaps as those on the minimal &lt;code&gt;alpaca_eval_gpt4&lt;/code&gt; &lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/#models&#34;&gt;leaderboard&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;The second question is what data to use. Again we can try to answer this question from a statistical power perspective: what data allows to best distinguish between models. Let&#39;s consider this for all the datasets that are part of AlpacaEval, but let us control for the size of the evaluation sets as we only care about the quality of the data. The following plot shows the p-values from the paired t-test of each pairs of models on 80 examples of each subset of AlpacaEval.&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tatsu-lab/alpaca_eval/main/figures%2Fplot_paired_ttests_per_dataset.png&#34; alt=&#34;plot_paired_ttests_per_dataset.png&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;We see for example that the self-instruct dataset yields the least statistical power, which suggests that one could remove this dataset from the evaluation set. The exact reason should be analyzed in future work. For the code and more analysis see &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_eval/raw/main/notebooks/analyzing_evalset.ipynb&#34;&gt;this notebook&lt;/a&gt;, or the colab notebook above.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h1&gt;Citation&lt;/h1&gt; &#xA;&lt;p&gt;Please consider citing the following depending on what you are using and referring to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Code, results, and general benchmark&lt;/strong&gt;: &lt;code&gt;alpaca_eval&lt;/code&gt; (this repo). Specify whether you are using AlpacaEval or AlpacaEval 2.0. For length-controlled win-rates see below.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Length-controlled (LC) win rates&lt;/strong&gt;: &lt;code&gt;alpaca_eval_length&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Human annotations&lt;/strong&gt;: &lt;code&gt;dubois2023alpacafarm&lt;/code&gt; (&lt;a href=&#34;https://arxiv.org/abs/2305.14387&#34;&gt;AlpacaFarm&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;AlpacaEval evaluation set&lt;/strong&gt;: &lt;code&gt;alpaca_eval&lt;/code&gt; and &lt;a href=&#34;https://github.com/yizhongw/self-instruct&#34;&gt;self-instruct&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/datasets/OpenAssistant/oasst1/viewer/OpenAssistant--oasst1/validation&#34;&gt;open-assistant&lt;/a&gt;, &lt;a href=&#34;https://lmsys.org/blog/2023-03-30-vicuna/&#34;&gt;vicuna&lt;/a&gt;, &lt;a href=&#34;https://github.com/arnav-gudibande/koala-test-set&#34;&gt;koala&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/datasets/Anthropic/hh-rlhf/viewer/Anthropic--hh-rlhf/test&#34;&gt;hh-rlhf&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Here are the bibtex entries:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{alpaca_eval,&#xA;  author = {Xuechen Li and Tianyi Zhang and Yann Dubois and Rohan Taori and Ishaan Gulrajani and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },&#xA;  title = {AlpacaEval: An Automatic Evaluator of Instruction-following Models},&#xA;  year = {2023},&#xA;  publisher = {GitHub},&#xA;  journal = {GitHub repository},&#xA;  howpublished = {\url{https://github.com/tatsu-lab/alpaca_eval}}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{dubois2024length,&#xA;  title={Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators},&#xA;  author={Dubois, Yann and Galambosi, Bal{\&#39;a}zs and Liang, Percy and Hashimoto, Tatsunori B},&#xA;  journal={arXiv preprint arXiv:2404.04475},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{dubois2023alpacafarm,&#xA;  title={AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback}, &#xA;  author={Yann Dubois and Xuechen Li and Rohan Taori and Tianyi Zhang and Ishaan Gulrajani and Jimmy Ba and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto},&#xA;  year={2023},&#xA;  eprint={2305.14387},&#xA;  archivePrefix={arXiv},&#xA;  primaryClass={cs.LG}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;More information&lt;/h1&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h2 tabindex=&#34;-1&#34; dir=&#34;auto&#34;&gt;Length-Controlled Win Rates&lt;/h2&gt;&lt;/summary&gt; &#xA; &lt;p&gt;Length controlled (LC) win-rates are a debiased version of the win-rates that control for the length of the outputs.&lt;/p&gt; &#xA; &lt;p&gt;The main idea is that for each model we will fit a logistic regression to predict the preference of the autoannotator given: (1) the instruction, (2) the model, and (3) the difference of length between the baseline and model output. Given such a logistic regression we can then try to predict the counterfactual &#34;what would the preference be if the model&#39;s output had the same length as the baseline&#34; by setting the length difference to 0. By averaging over this length-controlled preference, we then obtain the length-controlled win-rate. The exact form of the logistic regression is taken such that the interpretation of LC win rates is similar to the raw win rates, for example for any model &lt;code&gt;m1&lt;/code&gt; and &lt;code&gt;m2&lt;/code&gt; we have &lt;code&gt;win_rate(m1, m2) = 1 - win_rate(m2, m1) \in [0,100]&lt;/code&gt; and &lt;code&gt;win_rate(m1, m1) = 0.5&lt;/code&gt;. Length controlled win-rates increase the correlation between AlpacaEval&#39;s leaderboard and Chat Arena from &lt;strong&gt;0.93 to 0.98 Spearman correlation, while significantly decreasing the length gameability of the annotator&lt;/strong&gt;. For more information and results about length controlled win-rates see &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_eval/raw/main/notebooks/length_controlled.ipynb&#34;&gt;this notebook&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;This idea of estimating the controlled direct effect, by predicting the outcome while conditioning on the mediator (the length difference), is common in statistical inference.&lt;/p&gt; &#xA; &lt;p&gt;To get LC win rates on previously annotated models, you can use the following command:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -U alpaca_eval&#xA;alpaca_eval --model_outputs … --is_recompute_metrics_only True&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h2 tabindex=&#34;-1&#34; dir=&#34;auto&#34;&gt;AlpacaEval 2.0&lt;/h2&gt;&lt;/summary&gt; &#xA; &lt;p&gt;AlpacaEval 2.0 is a new version of AlpacaEval. Here are the differences:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;reference: &lt;code&gt;gpt4_turbo&lt;/code&gt;&lt;/strong&gt;: we upgraded the baseline from &lt;code&gt;text-davinci-003&lt;/code&gt; to &lt;code&gt;gpt4_turbo&lt;/code&gt; to make the benchmark more challenging and have a metric that better reflects the current state of the art.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;annotator: &lt;code&gt;weighted_alpaca_eval_gpt4_turbo&lt;/code&gt;&lt;/strong&gt;: we improved the annotator in quality and price. First, we use the &lt;code&gt;gpt4_turbo&lt;/code&gt; model for annotating, which is approximately 2x cheaper than &lt;code&gt;gpt4&lt;/code&gt;. Second, we changed the prompt such that the model outputs a single token, which further reduced cost and speed. Finally, instead of using a binary preference, we used the logprobs to compute a continuous preference, which gives the final weighted win-rate. Note that the latter two changes had the surprising effect of decreasing the annotators&#39; length biased.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;By default, AlpacaEval 2.0 will be used from &lt;code&gt;pip install alpaca_eval==0.5&lt;/code&gt;. If you wish to use the old configs by default, you can set &lt;code&gt;IS_ALPACA_EVAL_2=False&lt;/code&gt; in your environment.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h2 tabindex=&#34;-1&#34; dir=&#34;auto&#34;&gt;Data Release&lt;/h2&gt;&lt;/summary&gt; &#xA; &lt;p&gt;As part of AlpacaEval, we release the following data:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Human annotations (17701)&lt;/strong&gt; in order to develop and understand automatic evaluators, we release all the human pairwise evaluation that we collected for AlpacaFarm. This contains comparisons between 22 models with the &lt;code&gt;text-davinci-003&lt;/code&gt; reference on the AlpacaFarm evaluation set. Annotations are from a pool of 16 crowd workers on Amazon Mechanical Turk. The different models are: 6 from OpenAI, 2 SFT models from AlpacaFarm, 13 RLHF methods from AlpacaFarm, and LLaMA 7B.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Human cross-annotations (2596)&lt;/strong&gt; in order to further analyze automatic evaluators we selected (via stratified sampling across models and datasets) 650 examples from the AlpacaFarm evaluation set and collected 4 human annotations per example.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;AlpacaEval set (805)&lt;/strong&gt; we made slight modifications/simplification of the AlpacaFarm evaluation set. In particular, we first merged the instruction and input fields into a single instruction field. This affects 1/4 of the examples in the AlpacaFarm evaluation set, all of which are from the &lt;a href=&#34;https://arxiv.org/abs/2212.10560&#34;&gt;self-instruct evaluation set&lt;/a&gt;. Second we regenerated the text-davinci-003 reference outputs without limiting the length of its outputs.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;For more details about the human annotations refer to the &lt;a href=&#34;https://arxiv.org/abs/2305.14387&#34;&gt;AlpacaFarm paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h2 tabindex=&#34;-1&#34; dir=&#34;auto&#34;&gt;Differences with AlpacaFarm&lt;/h2&gt;&lt;/summary&gt; &#xA; &lt;p&gt;AlpacaEval is an improvement and simplification of the automatic pairwise preference simulator from &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_farm&#34;&gt;AlpacaFarm&lt;/a&gt;. Outside AlpacaFarm, you should be using AlpacaEval. Here are the main differences:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;AlpacaEval merges instructions and inputs&lt;/strong&gt;: The AlpacaEval evaluation is the same as the AlpacaFarm evaluation except that the instruction and input fields are merged as &lt;code&gt;{instruction}\n\n{input}&lt;/code&gt;. This affects 1/4 of the examples in the AlpacaFarm evaluation set (the &lt;a href=&#34;https://arxiv.org/abs/2212.10560&#34;&gt;self-instruct&lt;/a&gt; subset). This simplification provides a more fair comparison for models that were not trained by distinguishing between the two fields.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;AlpacaEval handles longer generations&lt;/strong&gt;: Models in AlpacaFarm were limited to a maximum number of 300 tokens for generations. We change this number to 2000 for AlpacaEval. Note that this also affects the reference generations (&lt;code&gt;text-davinci-003&lt;/code&gt;), so the results on AlpacaEval are not comparable to those on AlpacaFarm even for examples that had no input field.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;AlpacaEval removes intra- and inter-annotator variance&lt;/strong&gt;: The AlpacaFarm simulator replicates human annotation in terms of both mode behavior and diversity. In particular, AlpacaFarm&#39;s simulator uses a pool of models and prompts and adds noise to replicate human intra- and inter-annotator variance. If the goal is to use an automatic annotator for evaluation or simply training better models, then this variance may not be desirable. The default annotators in AlpacaEval thus don&#39;t have this variance. We give the option to add it back by using &lt;code&gt;--anotators_config &#39;alpaca_farm&#39;&lt;/code&gt; and &lt;code&gt;--p_label_flip 0.25&lt;/code&gt; when creating an evaluator.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h2 tabindex=&#34;-1&#34; dir=&#34;auto&#34;&gt;Related work&lt;/h2&gt;&lt;/summary&gt; &#xA; &lt;p&gt;There have been several work that propose new automatic annotators for instruction-following models. Here we list the ones that we are aware of and discuss how they differ from ours. We evaluated all of those in &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_eval#evaluators&#34;&gt;our evaluator&#39;s leaderboard&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Vicuna/lmsys&lt;/strong&gt; The lmsys annotator (&lt;code&gt;lmsys_gpt4&lt;/code&gt;) evaluates the pair by asking the annotator a score from 1-10 for each output, and then selecting the output with the highest score as preferred. They do not randomize over output order and they ask an explanation &lt;em&gt;after&lt;/em&gt; the score. Overall, we found that this annotator has strong bias towards longer outputs (0.74) and relatively low correlation with human annotations (63.2).&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;AlpacaFarm&lt;/strong&gt; The best AlpacaFarm annotator (&lt;code&gt;alpaca_farm_greedy_gpt4&lt;/code&gt;) evaluates the pair by directly asking the annotator which output it prefers. Furthermore, it batches 5 examples together to amortize the length of the prompt and randomizes the order of outputs. Overall, we found that this annotator has much less bias towards longer outputs (0.60) and is faster (878 seconds/1000 examples) than others. It has a slightly higher correlation with the majority of human annotations (66.4) than humans themselves (65.7). However, it is more expensive ($15.3/1000 examples) and doesn&#39;t work with very long outputs given the batching.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Aviary&lt;/strong&gt; The Aviary annotator (&lt;code&gt;aviary_gpt4&lt;/code&gt;) asks the annotator to order the output by its preference, rather than simply selecting the preferred output. It does not randomize the order of outputs and uses high temperature for decoding (0.9). Overall, we found that this annotator has relatively strong bias towards longer outputs (0.70) and very high correlation with human annotations (69.1). By decreasing the temperature and randomizing the order of outputs, we &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_eval/raw/main/src/alpaca_eval/evaluators_configs/README.md&#34;&gt;further improved&lt;/a&gt; the correlation to 69.8 (&lt;code&gt;improved_aviary_gpt4&lt;/code&gt;) but this further increased the length bias to 0.73.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;Our &lt;code&gt;alpaca_eval_gpt4&lt;/code&gt; is a mix between the AlpacaFarm and Aviary annotators. It asks the annotator to order the outputs by preference, but it uses temperature 0, randomizes over outputs, and made some modifications to the prompt to decrease length bias to 0.68.&lt;/p&gt; &#xA; &lt;p&gt;Other related work include recent papers which analyze automatic evaluators. For example:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.14387&#34;&gt;AlpacaFarm Appx C&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2305.17926v1&#34;&gt;Large Language Models are not Fair Evaluators&lt;/a&gt; both found that automatic annotators have a position bias.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.14387&#34;&gt;AlpacaFarm Sec. 5.2.&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2305.15717&#34;&gt;The False Promise of Imitating Proprietary LLMs&lt;/a&gt; both found that automatic annotators favor style (e.g. use of list, tone, word choice, length) over factuality.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h2 tabindex=&#34;-1&#34; dir=&#34;auto&#34;&gt;Interpreting annotations&lt;/h2&gt;&lt;/summary&gt; &#xA; &lt;p&gt;For all models you can find the auto-annotations under &lt;code&gt;results/&amp;lt;model_name&amp;gt;/*/annotations.json&lt;/code&gt;. The annotations have the following columns:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;instruction&lt;/code&gt;: the prompt&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;generator_1&lt;/code&gt;: the baseline model&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;output_1&lt;/code&gt;: the output of the baseline model&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;generator_2&lt;/code&gt;: the model being evaluated&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;output_2&lt;/code&gt;: the output of the model being evaluated&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;annotator&lt;/code&gt;: the auto-annotator&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;preference&lt;/code&gt;: the result of the auto-annotator. This is a float between 1 and 2. Closer to 1 means that the auto-annotator prefers &lt;code&gt;output_1&lt;/code&gt;, closer to 2 means that it prefers &lt;code&gt;output_2&lt;/code&gt;. For AlpacaEval 2.0, &lt;code&gt;preference-1&lt;/code&gt; corresponds to the probability of &lt;code&gt;output_1&lt;/code&gt; being preferred. For AlpacaEval 1.0, &lt;code&gt;preference&lt;/code&gt; is 1 if &lt;code&gt;output_1&lt;/code&gt; is preferred, 2 if &lt;code&gt;output_2&lt;/code&gt; is preferred, and 1.5 if they are the same. The win rate is always&lt;code&gt;(preference -1).mean()&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;raw_completion&lt;/code&gt;: the raw output of the auto-annotator. This is field contains the completions before de-randomization of the order between &lt;code&gt;output_1&lt;/code&gt; and &lt;code&gt;output_2&lt;/code&gt;! It is thus much harder to interpret, see below for more information.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;&lt;strong&gt;Chain of thought&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;For some annotators, e.g. &lt;code&gt;alpaca_eval_cot_gpt4_turbo_fn&lt;/code&gt; we use &lt;strong&gt;chain of thought reasoning&lt;/strong&gt; to make the models preferences more interpretable. Those can then be found under &lt;code&gt;concise_explanation&lt;/code&gt;. To interpret them, you should also look at &lt;code&gt;referenced_models&lt;/code&gt; which translates the temporary model name (in the prompt) to the actual output. Below, we provide more explanation as to what is happening behind the scenes.&lt;/p&gt; &#xA; &lt;p&gt;You can check the &lt;code&gt;raw_annotations[&#34;concise_explanation]&lt;/code&gt; column in &lt;code&gt;annotations.json&lt;/code&gt; (e.g. &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_eval/tree/main/results/gpt4/alpaca_eval_cot_gpt4_turbo_fn/annotations.json&#34;&gt;here&lt;/a&gt;) which contains the chain of thought reasoning of the auto annotator. Note that the raw_annotations is not modified by the randomization of the order of the outputs. In particular, &lt;code&gt;&#34;m&#34;&lt;/code&gt; and &lt;code&gt;&#34;M&#34;&lt;/code&gt; can sometime refer to the first model (the reference) and sometime to the second model (the model being evaluated). To understand which model is being referred to, you should use the column &lt;code&gt;preference&lt;/code&gt; and &lt;code&gt;ordered_models&lt;/code&gt;. To make it easier we add a column &lt;code&gt;&#34;referenced_models&#34;&lt;/code&gt; mapping the model names to the corresponding outputs. For example in the following annotation we see that the preference is 1.0 (i.e. &lt;code&gt;output_1&lt;/code&gt;) and corresponds to model &lt;code&gt;M&lt;/code&gt; in &lt;code&gt;concise_explanation&lt;/code&gt; (see &lt;code&gt;ordered_models&lt;/code&gt;).&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;instruction&#34;: &#34;How did US states get their names?&#34;,&#xA;  &#34;output_1&#34;: &#34;The names of U.S. states are derived from a variety of languages and sources, including indigenous American languages, European languages (primarily English, Spanish, and French), as well as other historical influences. Here is a brief overview of how some states got their names:\n\n1. **Indigenous American Languages**: Many state names come from Native American words or names of tribes, often from the languages of the indigenous peoples who lived in or near those areas.\n   - **Alabama**: Possibly derived from the Choctaw language, meaning \&#34;thicket clearers.\&#34;\n   - **Connecticut**: From a Mohegan-Pequot word meaning \&#34;long tidal river.\&#34;\n   - **Massachusetts**: [...]&#34;,&#xA;  &#34;generator_1&#34;: &#34;gpt4_1106_preview&#34;,&#xA;  &#34;dataset&#34;: &#34;helpful_base&#34;,&#xA;  &#34;output_2&#34;: &#34;The names of the 50 U.S. states come from a variety of sources, including Native American languages, European languages, and historical figures. Here&#39;s a brief overview of how some states got their names:\n\n1. Native American origins: Many states have names derived from Native American languages. For example, Alabama comes from the Choctaw word \&#34;Albah amo,\&#34; meaning \&#34;plant gatherers\&#34; or \&#34;herb gatherers.\&#34; Similarly, the name Mississippi comes from the Ojibwe word \&#34;Misi-ziibi,\&#34; meaning \&#34;great river.\&#34;\n\n2. European languages: [...].&#34;,&#xA;  &#34;generator_2&#34;: &#34;gpt4&#34;,&#xA;  &#34;annotator&#34;: &#34;alpaca_eval_cot_gpt4_turbo_fn&#34;,&#xA;  &#34;preference&#34;: 1.0,&#xA;  &#34;raw_completion&#34;: {&#xA;    &#34;concise_explanation&#34;: &#34;Model M provided a more detailed and structured response, including bold headings for each category and a wider range of examples. It also included additional categories such as &#39;Other European Languages&#39; and &#39;Combination of Languages and Influences&#39;, which added depth to the explanation. Model m&#39;s response was accurate but less comprehensive and lacked the clear structure found in Model M&#39;s output.&#34;,&#xA;    &#34;ordered_models&#34;: [&#xA;      {&#xA;        &#34;model&#34;: &#34;M&#34;,&#xA;        &#34;rank&#34;: 1&#xA;      },&#xA;      {&#xA;        &#34;model&#34;: &#34;m&#34;,&#xA;        &#34;rank&#34;: 2&#xA;      }&#xA;    ]&#xA;  },&#xA;  &#34;referenced_models&#34;: {&#xA;    &#34;M&#34;: &#34;output_1&#34;,&#xA;    &#34;m&#34;: &#34;output_2&#34;&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h2 tabindex=&#34;-1&#34; dir=&#34;auto&#34;&gt;Major updates&lt;/h2&gt;&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;12th March 2024: updated to use length-controlled (LC) win rates. This is a debiased version of the win-rates that control for the length of the outputs.&lt;/li&gt; &#xA;  &lt;li&gt;3rd January 2024: updated to AlpacaEval 2.0, which uses GPT4-turbo as baseline and annotator.&lt;/li&gt; &#xA;  &lt;li&gt;2nd January 2024: added Azure API and more general way of setting client configs. See &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_eval/tree/main/client_configs/README.md&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;19th June 2023: add leaderboard &lt;code&gt;chatgpt_fn&lt;/code&gt; that anyone can use (no waiting lists).&lt;/li&gt; &#xA;  &lt;li&gt;19th June 2023: update to use &lt;a href=&#34;https://openai.com/blog/function-calling-and-other-api-updates&#34;&gt;OpenAI&#39;s function calling&lt;/a&gt;. Example: &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_eval/tree/main/src/alpaca_eval/evaluators_configs/chatgpt_fn&#34;&gt;&lt;code&gt;chatgpt_fn&lt;/code&gt;&lt;/a&gt; or &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_eval/tree/main/src/alpaca_eval/evaluators_configs/alpaca_eval_gpt4_fn&#34;&gt;&lt;code&gt;alpaca_eval_gpt4_fn&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt;</summary>
  </entry>
  <entry>
    <title>GoogleCloudPlatform/python-docs-samples</title>
    <updated>2024-06-23T01:43:48Z</updated>
    <id>tag:github.com,2024-06-23:/GoogleCloudPlatform/python-docs-samples</id>
    <link href="https://github.com/GoogleCloudPlatform/python-docs-samples" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code samples used on cloud.google.com&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Google Cloud Platform Python Samples&lt;/h1&gt; &#xA;&lt;p&gt;Python samples for &lt;a href=&#34;https://cloud.google.com/&#34;&gt;Google Cloud Platform products&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://storage.googleapis.com/cloud-devrel-public/python-docs-samples/badges/py-2.7.html&#34;&gt;&lt;img src=&#34;https://storage.googleapis.com/cloud-devrel-public/python-docs-samples/badges/py-2.7.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://storage.googleapis.com/cloud-devrel-public/python-docs-samples/badges/py-3.8.html&#34;&gt;&lt;img src=&#34;https://storage.googleapis.com/cloud-devrel-public/python-docs-samples/badges/py-3.8.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://storage.googleapis.com/cloud-devrel-public/python-docs-samples/badges/py-3.9.html&#34;&gt;&lt;img src=&#34;https://storage.googleapis.com/cloud-devrel-public/python-docs-samples/badges/py-3.9.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://storage.googleapis.com/cloud-devrel-public/python-docs-samples/badges/py-3.10.html&#34;&gt;&lt;img src=&#34;https://storage.googleapis.com/cloud-devrel-public/python-docs-samples/badges/py-310.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://storage.googleapis.com/cloud-devrel-public/python-docs-samples/badges/py-3.11.html&#34;&gt;&lt;img src=&#34;https://storage.googleapis.com/cloud-devrel-public/python-docs-samples/badges/py-311.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Google Cloud Samples&lt;/h2&gt; &#xA;&lt;p&gt;Check out some of the samples found on this repository on the &lt;a href=&#34;https://cloud.google.com/docs/samples?l=python&#34;&gt;Google Cloud Samples&lt;/a&gt; page.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install &lt;a href=&#34;https://cloud.google.com/python/setup&#34;&gt;&lt;code&gt;pip&lt;/code&gt; and &lt;code&gt;virtualenv&lt;/code&gt;&lt;/a&gt; if you do not already have them.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone this repository:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/GoogleCloudPlatform/python-docs-samples.git&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Obtain authentication credentials.&lt;/p&gt; &lt;p&gt;Create local credentials by running the following command and following the oauth2 flow (read more about the command &lt;a href=&#34;https://cloud.google.com/sdk/gcloud/reference/beta/auth/application-default/login&#34;&gt;here&lt;/a&gt;):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;gcloud auth application-default login&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Read more about &lt;a href=&#34;https://cloud.google.com/docs/authentication#projects_and_resources&#34;&gt;Google Cloud Platform Authentication&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;How to run a sample&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Change directory to one of the sample folders, e.g. &lt;code&gt;logging/cloud-client&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cd logging/cloud-client/&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create a virtualenv. Samples are compatible with Python 3.6+.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python3 -m venv env&#xA;source env/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install the dependencies needed to run the samples.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the sample:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python snippets.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Contributions welcome! See the &lt;a href=&#34;https://raw.githubusercontent.com/GoogleCloudPlatform/python-docs-samples/main/CONTRIBUTING.md&#34;&gt;Contributing Guide&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>