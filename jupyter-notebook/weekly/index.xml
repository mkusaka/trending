<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-08-28T01:47:13Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>CompVis/stable-diffusion</title>
    <updated>2022-08-28T01:47:13Z</updated>
    <id>tag:github.com,2022-08-28:/CompVis/stable-diffusion</id>
    <link href="https://github.com/CompVis/stable-diffusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Stable Diffusion&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;Stable Diffusion was made possible thanks to a collaboration with &lt;a href=&#34;https://stability.ai/&#34;&gt;Stability AI&lt;/a&gt; and &lt;a href=&#34;https://runwayml.com/&#34;&gt;Runway&lt;/a&gt; and builds upon our previous work:&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ommer-lab.com/research/latent-diffusion-models/&#34;&gt;&lt;strong&gt;High-Resolution Image Synthesis with Latent Diffusion Models&lt;/strong&gt;&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/rromb&#34;&gt;Robin Rombach&lt;/a&gt;*, &lt;a href=&#34;https://github.com/ablattmann&#34;&gt;Andreas Blattmann&lt;/a&gt;*, &lt;a href=&#34;https://github.com/qp-qp&#34;&gt;Dominik Lorenz&lt;/a&gt;, &lt;a href=&#34;https://github.com/pesser&#34;&gt;Patrick Esser&lt;/a&gt;, &lt;a href=&#34;https://hci.iwr.uni-heidelberg.de/Staff/bommer&#34;&gt;Björn Ommer&lt;/a&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html&#34;&gt;CVPR &#39;22 Oral&lt;/a&gt; | &lt;a href=&#34;https://github.com/CompVis/latent-diffusion&#34;&gt;GitHub&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2112.10752&#34;&gt;arXiv&lt;/a&gt; | &lt;a href=&#34;https://ommer-lab.com/research/latent-diffusion-models/&#34;&gt;Project page&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/txt2img/merged-0006.png&#34; alt=&#34;txt2img-stable2&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/#stable-diffusion-v1&#34;&gt;Stable Diffusion&lt;/a&gt; is a latent text-to-image diffusion model. Thanks to a generous compute donation from &lt;a href=&#34;https://stability.ai/&#34;&gt;Stability AI&lt;/a&gt; and support from &lt;a href=&#34;https://laion.ai/&#34;&gt;LAION&lt;/a&gt;, we were able to train a Latent Diffusion Model on 512x512 images from a subset of the &lt;a href=&#34;https://laion.ai/blog/laion-5b/&#34;&gt;LAION-5B&lt;/a&gt; database. Similar to Google&#39;s &lt;a href=&#34;https://arxiv.org/abs/2205.11487&#34;&gt;Imagen&lt;/a&gt;, this model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts. With its 860M UNet and 123M text encoder, the model is relatively lightweight and runs on a GPU with at least 10GB VRAM. See &lt;a href=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/#stable-diffusion-v1&#34;&gt;this section&lt;/a&gt; below and the &lt;a href=&#34;https://huggingface.co/CompVis/stable-diffusion&#34;&gt;model card&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;A suitable &lt;a href=&#34;https://conda.io/&#34;&gt;conda&lt;/a&gt; environment named &lt;code&gt;ldm&lt;/code&gt; can be created and activated with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env create -f environment.yaml&#xA;conda activate ldm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also update an existing &lt;a href=&#34;https://github.com/CompVis/latent-diffusion&#34;&gt;latent diffusion&lt;/a&gt; environment by running&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda install pytorch torchvision -c pytorch&#xA;pip install transformers==4.19.2 diffusers invisible-watermark&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Stable Diffusion v1&lt;/h2&gt; &#xA;&lt;p&gt;Stable Diffusion v1 refers to a specific configuration of the model architecture that uses a downsampling-factor 8 autoencoder with an 860M UNet and CLIP ViT-L/14 text encoder for the diffusion model. The model was pretrained on 256x256 images and then finetuned on 512x512 images.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Note: Stable Diffusion v1 is a general text-to-image diffusion model and therefore mirrors biases and (mis-)conceptions that are present in its training data. Details on the training procedure and data, as well as the intended use of the model can be found in the corresponding &lt;a href=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/Stable_Diffusion_v1_Model_Card.md&#34;&gt;model card&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;The weights are available via &lt;a href=&#34;https://huggingface.co/CompVis&#34;&gt;the CompVis organization at Hugging Face&lt;/a&gt; under &lt;a href=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/LICENSE&#34;&gt;a license which contains specific use-based restrictions to prevent misuse and harm as informed by the model card, but otherwise remains permissive&lt;/a&gt;. While commercial use is permitted under the terms of the license, &lt;strong&gt;we do not recommend using the provided weights for services or products without additional safety mechanisms and considerations&lt;/strong&gt;, since there are &lt;a href=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/Stable_Diffusion_v1_Model_Card.md#limitations-and-bias&#34;&gt;known limitations and biases&lt;/a&gt; of the weights, and research on safe and ethical deployment of general text-to-image models is an ongoing effort. &lt;strong&gt;The weights are research artifacts and should be treated as such.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/LICENSE&#34;&gt;The CreativeML OpenRAIL M license&lt;/a&gt; is an &lt;a href=&#34;https://www.licenses.ai/blog/2022/8/18/naming-convention-of-responsible-ai-licenses&#34;&gt;Open RAIL M license&lt;/a&gt;, adapted from the work that &lt;a href=&#34;https://bigscience.huggingface.co/&#34;&gt;BigScience&lt;/a&gt; and &lt;a href=&#34;https://www.licenses.ai/&#34;&gt;the RAIL Initiative&lt;/a&gt; are jointly carrying in the area of responsible AI licensing. See also &lt;a href=&#34;https://bigscience.huggingface.co/blog/the-bigscience-rail-license&#34;&gt;the article about the BLOOM Open RAIL license&lt;/a&gt; on which our license is based.&lt;/p&gt; &#xA;&lt;h3&gt;Weights&lt;/h3&gt; &#xA;&lt;p&gt;We currently provide the following checkpoints:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;sd-v1-1.ckpt&lt;/code&gt;: 237k steps at resolution &lt;code&gt;256x256&lt;/code&gt; on &lt;a href=&#34;https://huggingface.co/datasets/laion/laion2B-en&#34;&gt;laion2B-en&lt;/a&gt;. 194k steps at resolution &lt;code&gt;512x512&lt;/code&gt; on &lt;a href=&#34;https://huggingface.co/datasets/laion/laion-high-resolution&#34;&gt;laion-high-resolution&lt;/a&gt; (170M examples from LAION-5B with resolution &lt;code&gt;&amp;gt;= 1024x1024&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;sd-v1-2.ckpt&lt;/code&gt;: Resumed from &lt;code&gt;sd-v1-1.ckpt&lt;/code&gt;. 515k steps at resolution &lt;code&gt;512x512&lt;/code&gt; on &lt;a href=&#34;https://laion.ai/blog/laion-aesthetics/&#34;&gt;laion-aesthetics v2 5+&lt;/a&gt; (a subset of laion2B-en with estimated aesthetics score &lt;code&gt;&amp;gt; 5.0&lt;/code&gt;, and additionally filtered to images with an original size &lt;code&gt;&amp;gt;= 512x512&lt;/code&gt;, and an estimated watermark probability &lt;code&gt;&amp;lt; 0.5&lt;/code&gt;. The watermark estimate is from the &lt;a href=&#34;https://laion.ai/blog/laion-5b/&#34;&gt;LAION-5B&lt;/a&gt; metadata, the aesthetics score is estimated using the &lt;a href=&#34;https://github.com/christophschuhmann/improved-aesthetic-predictor&#34;&gt;LAION-Aesthetics Predictor V2&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;sd-v1-3.ckpt&lt;/code&gt;: Resumed from &lt;code&gt;sd-v1-2.ckpt&lt;/code&gt;. 195k steps at resolution &lt;code&gt;512x512&lt;/code&gt; on &#34;laion-aesthetics v2 5+&#34; and 10% dropping of the text-conditioning to improve &lt;a href=&#34;https://arxiv.org/abs/2207.12598&#34;&gt;classifier-free guidance sampling&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;sd-v1-4.ckpt&lt;/code&gt;: Resumed from &lt;code&gt;sd-v1-2.ckpt&lt;/code&gt;. 225k steps at resolution &lt;code&gt;512x512&lt;/code&gt; on &#34;laion-aesthetics v2 5+&#34; and 10% dropping of the text-conditioning to improve &lt;a href=&#34;https://arxiv.org/abs/2207.12598&#34;&gt;classifier-free guidance sampling&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Evaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0) and 50 PLMS sampling steps show the relative improvements of the checkpoints: &lt;img src=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/v1-variants-scores.jpg&#34; alt=&#34;sd evaluation results&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Text-to-Image with Stable Diffusion&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/txt2img/merged-0005.png&#34; alt=&#34;txt2img-stable2&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/txt2img/merged-0007.png&#34; alt=&#34;txt2img-stable2&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Stable Diffusion is a latent diffusion model conditioned on the (non-pooled) text embeddings of a CLIP ViT-L/14 text encoder. We provide a &lt;a href=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/#reference-sampling-script&#34;&gt;reference script for sampling&lt;/a&gt;, but there also exists a &lt;a href=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/#diffusers-integration&#34;&gt;diffusers integration&lt;/a&gt;, which we expect to see more active community development.&lt;/p&gt; &#xA;&lt;h4&gt;Reference Sampling Script&lt;/h4&gt; &#xA;&lt;p&gt;We provide a reference sampling script, which incorporates&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;a &lt;a href=&#34;https://github.com/CompVis/stable-diffusion/pull/36&#34;&gt;Safety Checker Module&lt;/a&gt;, to reduce the probability of explicit outputs,&lt;/li&gt; &#xA; &lt;li&gt;an &lt;a href=&#34;https://github.com/ShieldMnt/invisible-watermark&#34;&gt;invisible watermarking&lt;/a&gt; of the outputs, to help viewers &lt;a href=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/scripts/tests/test_watermark.py&#34;&gt;identify the images as machine-generated&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;After &lt;a href=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/#weights&#34;&gt;obtaining the &lt;code&gt;stable-diffusion-v1-*-original&lt;/code&gt; weights&lt;/a&gt;, link them&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mkdir -p models/ldm/stable-diffusion-v1/&#xA;ln -s &amp;lt;path/to/model.ckpt&amp;gt; models/ldm/stable-diffusion-v1/model.ckpt &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and sample with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/txt2img.py --prompt &#34;a photograph of an astronaut riding a horse&#34; --plms &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, this uses a guidance scale of &lt;code&gt;--scale 7.5&lt;/code&gt;, &lt;a href=&#34;https://github.com/CompVis/latent-diffusion/pull/51&#34;&gt;Katherine Crowson&#39;s implementation&lt;/a&gt; of the &lt;a href=&#34;https://arxiv.org/abs/2202.09778&#34;&gt;PLMS&lt;/a&gt; sampler, and renders images of size 512x512 (which it was trained on) in 50 steps. All supported arguments are listed below (type &lt;code&gt;python scripts/txt2img.py --help&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;usage: txt2img.py [-h] [--prompt [PROMPT]] [--outdir [OUTDIR]] [--skip_grid] [--skip_save] [--ddim_steps DDIM_STEPS] [--plms] [--laion400m] [--fixed_code] [--ddim_eta DDIM_ETA]&#xA;                  [--n_iter N_ITER] [--H H] [--W W] [--C C] [--f F] [--n_samples N_SAMPLES] [--n_rows N_ROWS] [--scale SCALE] [--from-file FROM_FILE] [--config CONFIG] [--ckpt CKPT]&#xA;                  [--seed SEED] [--precision {full,autocast}]&#xA;&#xA;optional arguments:&#xA;  -h, --help            show this help message and exit&#xA;  --prompt [PROMPT]     the prompt to render&#xA;  --outdir [OUTDIR]     dir to write results to&#xA;  --skip_grid           do not save a grid, only individual samples. Helpful when evaluating lots of samples&#xA;  --skip_save           do not save individual samples. For speed measurements.&#xA;  --ddim_steps DDIM_STEPS&#xA;                        number of ddim sampling steps&#xA;  --plms                use plms sampling&#xA;  --laion400m           uses the LAION400M model&#xA;  --fixed_code          if enabled, uses the same starting code across samples&#xA;  --ddim_eta DDIM_ETA   ddim eta (eta=0.0 corresponds to deterministic sampling&#xA;  --n_iter N_ITER       sample this often&#xA;  --H H                 image height, in pixel space&#xA;  --W W                 image width, in pixel space&#xA;  --C C                 latent channels&#xA;  --f F                 downsampling factor&#xA;  --n_samples N_SAMPLES&#xA;                        how many samples to produce for each given prompt. A.k.a. batch size&#xA;  --n_rows N_ROWS       rows in the grid (default: n_samples)&#xA;  --scale SCALE         unconditional guidance scale: eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty))&#xA;  --from-file FROM_FILE&#xA;                        if specified, load prompts from this file&#xA;  --config CONFIG       path to config which constructs model&#xA;  --ckpt CKPT           path to checkpoint of model&#xA;  --seed SEED           the seed (for reproducible sampling)&#xA;  --precision {full,autocast}&#xA;                        evaluate at this precision&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: The inference config for all v1 versions is designed to be used with EMA-only checkpoints. For this reason &lt;code&gt;use_ema=False&lt;/code&gt; is set in the configuration, otherwise the code will try to switch from non-EMA to EMA weights. If you want to examine the effect of EMA vs no EMA, we provide &#34;full&#34; checkpoints which contain both types of weights. For these, &lt;code&gt;use_ema=False&lt;/code&gt; will load and use the non-EMA weights.&lt;/p&gt; &#xA;&lt;h4&gt;Diffusers Integration&lt;/h4&gt; &#xA;&lt;p&gt;A simple way to download and sample Stable Diffusion is by using the &lt;a href=&#34;https://github.com/huggingface/diffusers/tree/main#new--stable-diffusion-is-now-fully-compatible-with-diffusers&#34;&gt;diffusers library&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;# make sure you&#39;re logged in with `huggingface-cli login`&#xA;from torch import autocast&#xA;from diffusers import StableDiffusionPipeline&#xA;&#xA;pipe = StableDiffusionPipeline.from_pretrained(&#xA;&#x9;&#34;CompVis/stable-diffusion-v1-4&#34;, &#xA;&#x9;use_auth_token=True&#xA;).to(&#34;cuda&#34;)&#xA;&#xA;prompt = &#34;a photo of an astronaut riding a horse on mars&#34;&#xA;with autocast(&#34;cuda&#34;):&#xA;    image = pipe(prompt)[&#34;sample&#34;][0]  &#xA;    &#xA;image.save(&#34;astronaut_rides_horse.png&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Image Modification with Stable Diffusion&lt;/h3&gt; &#xA;&lt;p&gt;By using a diffusion-denoising mechanism as first proposed by &lt;a href=&#34;https://arxiv.org/abs/2108.01073&#34;&gt;SDEdit&lt;/a&gt;, the model can be used for different tasks such as text-guided image-to-image translation and upscaling. Similar to the txt2img sampling script, we provide a script to perform image modification with Stable Diffusion.&lt;/p&gt; &#xA;&lt;p&gt;The following describes an example where a rough sketch made in &lt;a href=&#34;https://www.pinta-project.com/&#34;&gt;Pinta&lt;/a&gt; is converted into a detailed artwork.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/img2img.py --prompt &#34;A fantasy landscape, trending on artstation&#34; --init-img &amp;lt;path-to-img.jpg&amp;gt; --strength 0.8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here, strength is a value between 0.0 and 1.0, that controls the amount of noise that is added to the input image. Values that approach 1.0 allow for lots of variations but will also produce images that are not semantically consistent with the input. See the following example.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Input&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg&#34; alt=&#34;sketch-in&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Outputs&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/mountains-3.png&#34; alt=&#34;out3&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/mountains-2.png&#34; alt=&#34;out2&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This procedure can, for example, also be used to upscale samples from the base model.&lt;/p&gt; &#xA;&lt;h2&gt;Comments&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Our codebase for the diffusion models builds heavily on &lt;a href=&#34;https://github.com/openai/guided-diffusion&#34;&gt;OpenAI&#39;s ADM codebase&lt;/a&gt; and &lt;a href=&#34;https://github.com/lucidrains/denoising-diffusion-pytorch&#34;&gt;https://github.com/lucidrains/denoising-diffusion-pytorch&lt;/a&gt;. Thanks for open-sourcing!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The implementation of the transformer encoder is from &lt;a href=&#34;https://github.com/lucidrains/x-transformers&#34;&gt;x-transformers&lt;/a&gt; by &lt;a href=&#34;https://github.com/lucidrains?tab=repositories&#34;&gt;lucidrains&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;BibTeX&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{rombach2021highresolution,&#xA;      title={High-Resolution Image Synthesis with Latent Diffusion Models}, &#xA;      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},&#xA;      year={2021},&#xA;      eprint={2112.10752},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>alembics/disco-diffusion</title>
    <updated>2022-08-28T01:47:13Z</updated>
    <id>tag:github.com,2022-08-28:/alembics/disco-diffusion</id>
    <link href="https://github.com/alembics/disco-diffusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Disco Diffusion&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/alembics/disco-diffusion/blob/main/Disco_Diffusion.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open in Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A frankensteinian amalgamation of notebooks, models and techniques for the generation of AI Art and Animations.&lt;/p&gt; &#xA;&lt;p&gt;[to be updated with further info soon]&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This project uses a special conversion tool to convert the python files into notebooks for easier development.&lt;/p&gt; &#xA;&lt;p&gt;What this means is you do not have to touch the notebook directly to make changes to it&lt;/p&gt; &#xA;&lt;p&gt;the tool being used is called &lt;a href=&#34;https://github.com/MSFTserver/colab-convert&#34;&gt;Colab-Convert&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;install using &lt;code&gt;pip install colab-convert&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;convert .py to .ipynb &lt;code&gt;colab-convert /path/to/file.py /path/to/file.ipynb&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;convert .ipynb to .py &lt;code&gt;colab-convert /path/to/file.ipynb /path/to/file.py&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Changelog&lt;/h2&gt; &#xA;&lt;h4&gt;v1 Oct 29th 2021 - Somnai&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Initial QoL improvements added, including user friendly UI, settings+prompt saving and improved google drive folder organization.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v1.1 Nov 13th 2021 - Somnai&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Now includes sizing options, intermediate saves and fixed image prompts and perlin inits. unexposed batch option since it doesn&#39;t work&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v2 Update: Nov 22nd 2021 - Somnai&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Initial addition of Katherine Crowson&#39;s Secondary Model Method (&lt;a href=&#34;https://colab.research.google.com/drive/1mpkrhOjoyzPeSWy2r7T8EYRaU7amYOOi#scrollTo=X5gODNAMEUCR&#34;&gt;https://colab.research.google.com/drive/1mpkrhOjoyzPeSWy2r7T8EYRaU7amYOOi#scrollTo=X5gODNAMEUCR&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Fix for incorrectly named settings files&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v3 Update: Dec 24th 2021 - Somnai&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Implemented Dango&#39;s advanced cutout method&lt;/li&gt; &#xA; &lt;li&gt;Added SLIP models, thanks to NeuralDivergent&lt;/li&gt; &#xA; &lt;li&gt;Fixed issue with NaNs resulting in black images, with massive help and testing from @Softology&lt;/li&gt; &#xA; &lt;li&gt;Perlin now changes properly within batches (not sure where this perlin_regen code came from originally, but thank you)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v4 Update: Jan 2022 - Somnai&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Implemented Diffusion Zooming&lt;/li&gt; &#xA; &lt;li&gt;Added Chigozie keyframing&lt;/li&gt; &#xA; &lt;li&gt;Made a bunch of edits to processes&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v4.1 Update: Jan 14th 2022 - Somnai&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Added video input mode&lt;/li&gt; &#xA; &lt;li&gt;Added license that somehow went missing&lt;/li&gt; &#xA; &lt;li&gt;Added improved prompt keyframing, fixed image_prompts and multiple prompts&lt;/li&gt; &#xA; &lt;li&gt;Improved UI&lt;/li&gt; &#xA; &lt;li&gt;Significant under the hood cleanup and improvement&lt;/li&gt; &#xA; &lt;li&gt;Refined defaults for each mode&lt;/li&gt; &#xA; &lt;li&gt;Removed SLIP models for the time being due to import conflicts&lt;/li&gt; &#xA; &lt;li&gt;Added latent-diffusion SuperRes for sharpening&lt;/li&gt; &#xA; &lt;li&gt;Added resume run mode&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v5 Update: Feb 20th 2022 - gandamu / Adam Letts&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Added 3D animation mode. Uses weighted combination of AdaBins and MiDaS depth estimation models. Uses pytorch3d for 3D transforms on Colab and/or Linux.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v5.1 Update: Mar 30th 2022 - zippy / Chris Allen and gandamu / Adam Letts&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Integrated Turbo+Smooth features from Disco Diffusion Turbo -- just the implementation, without its defaults.&lt;/li&gt; &#xA; &lt;li&gt;Implemented resume of turbo animations in such a way that it&#39;s now possible to resume from different batch folders and batch numbers.&lt;/li&gt; &#xA; &lt;li&gt;3D rotation parameter units are now degrees (rather than radians)&lt;/li&gt; &#xA; &lt;li&gt;Corrected name collision in sampling_mode (now diffusion_sampling_mode for plms/ddim, and sampling_mode for 3D transform sampling)&lt;/li&gt; &#xA; &lt;li&gt;Added video_init_seed_continuity option to make init video animations more continuous&lt;/li&gt; &#xA; &lt;li&gt;Removed pytorch3d from needing to be compiled with a lite version specifically made for Disco Diffusion&lt;/li&gt; &#xA; &lt;li&gt;Remove Super Resolution&lt;/li&gt; &#xA; &lt;li&gt;Remove Slip Models&lt;/li&gt; &#xA; &lt;li&gt;Update for crossplatform support&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v5.1 Update: Apr 4th 2022 - MSFTserver aka HostsServer&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Removed pytorch3d from needing to be compiled with a lite version specifically made for Disco Diffusion&lt;/li&gt; &#xA; &lt;li&gt;Remove Super Resolution&lt;/li&gt; &#xA; &lt;li&gt;Remove Slip Models&lt;/li&gt; &#xA; &lt;li&gt;Update for crossplatform support&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v5.2 Update: Apr 10th 2022 - nin_artificial / Tom Mason&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;VR Mode&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v5.3 Update: Jun 10th 2022 - nshepperd, huemin, cut_pow&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Horizontal and Vertical symmetry&lt;/li&gt; &#xA; &lt;li&gt;Addition of ViT-L/14@336px model (requires high VRAM)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v5.4 Update: Jun 14th 2022 - devdef / Alex Spirin, integrated into DD main by gandamu / Adam Letts&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Warp mode - for smooth/continuous video input results leveraging optical flow estimation and frame blending&lt;/li&gt; &#xA; &lt;li&gt;Custom models support&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v5.5 Update: Jul 11th 2022 - Palmweaver / Chris Scalf, KaliYuga_ai, further integration by gandamu / Adam Letts&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OpenCLIP models integration&lt;/li&gt; &#xA; &lt;li&gt;Pixel Art Diffusion, Watercolor Diffusion, and Pulp SciFi Diffusion models&lt;/li&gt; &#xA; &lt;li&gt;cut_ic_pow scheduling&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v5.6 Update: Jul 13th 2022 - Felipe3DArtist, integration by gandamu / Adam Letts&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Integrated portrait_generator_v001 - 512x512 diffusion model trained on faces - from Felipe3DArtist&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Notebook Provenance&lt;/h2&gt; &#xA;&lt;p&gt;Original notebook by Katherine Crowson (&lt;a href=&#34;https://github.com/crowsonkb&#34;&gt;https://github.com/crowsonkb&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/RiversHaveWings&#34;&gt;https://twitter.com/RiversHaveWings&lt;/a&gt;). It uses either OpenAI&#39;s 256x256 unconditional ImageNet or Katherine Crowson&#39;s fine-tuned 512x512 diffusion model (&lt;a href=&#34;https://github.com/openai/guided-diffusion&#34;&gt;https://github.com/openai/guided-diffusion&lt;/a&gt;), together with CLIP (&lt;a href=&#34;https://github.com/openai/CLIP&#34;&gt;https://github.com/openai/CLIP&lt;/a&gt;) to connect text prompts with images.&lt;/p&gt; &#xA;&lt;p&gt;Modified by Daniel Russell (&lt;a href=&#34;https://github.com/russelldc&#34;&gt;https://github.com/russelldc&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/danielrussruss&#34;&gt;https://twitter.com/danielrussruss&lt;/a&gt;) to include (hopefully) optimal params for quick generations in 15-100 timesteps rather than 1000, as well as more robust augmentations.&lt;/p&gt; &#xA;&lt;p&gt;Further improvements from Dango233 and nshepperd helped improve the quality of diffusion in general, and especially so for shorter runs like this notebook aims to achieve.&lt;/p&gt; &#xA;&lt;p&gt;Vark added code to load in multiple Clip models at once, which all prompts are evaluated against, which may greatly improve accuracy.&lt;/p&gt; &#xA;&lt;p&gt;The latest zoom, pan, rotation, and keyframes features were taken from Chigozie Nri&#39;s VQGAN Zoom Notebook (&lt;a href=&#34;https://github.com/chigozienri&#34;&gt;https://github.com/chigozienri&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/chigozienri&#34;&gt;https://twitter.com/chigozienri&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;Advanced DangoCutn Cutout method is also from Dango223.&lt;/p&gt; &#xA;&lt;p&gt;--&lt;/p&gt; &#xA;&lt;p&gt;Somnai (&lt;a href=&#34;https://twitter.com/Somnai_dreams&#34;&gt;https://twitter.com/Somnai_dreams&lt;/a&gt;) added 2D Diffusion animation techniques, QoL improvements and various implementations of tech and techniques, mostly listed in the changelog below.&lt;/p&gt; &#xA;&lt;p&gt;3D animation implementation added by Adam Letts (&lt;a href=&#34;https://twitter.com/gandamu_ml&#34;&gt;https://twitter.com/gandamu_ml&lt;/a&gt;) in collaboration with Somnai.&lt;/p&gt; &#xA;&lt;p&gt;Turbo feature by Chris Allen (&lt;a href=&#34;https://twitter.com/zippy731&#34;&gt;https://twitter.com/zippy731&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;Improvements to ability to run on local systems, Windows support, and dependency installation by HostsServer (&lt;a href=&#34;https://twitter.com/HostsServer&#34;&gt;https://twitter.com/HostsServer&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;VR Mode by Tom Mason (&lt;a href=&#34;https://twitter.com/nin_artificial&#34;&gt;https://twitter.com/nin_artificial&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;Horizontal and Vertical symmetry functionality by nshepperd. Symmetry transformation_steps by huemin (&lt;a href=&#34;https://twitter.com/huemin_art&#34;&gt;https://twitter.com/huemin_art&lt;/a&gt;). Symmetry integration into Disco Diffusion by Dmitrii Tochilkin (&lt;a href=&#34;https://twitter.com/cut_pow&#34;&gt;https://twitter.com/cut_pow&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;Warp and custom model support by Alex Spirin (&lt;a href=&#34;https://twitter.com/devdef&#34;&gt;https://twitter.com/devdef&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;Pixel Art Diffusion, Watercolor Diffusion, and Pulp SciFi Diffusion models from KaliYuga (&lt;a href=&#34;https://twitter.com/KaliYuga_ai&#34;&gt;https://twitter.com/KaliYuga_ai&lt;/a&gt;). Follow KaliYuga&#39;s Twitter for the latest models and for notebooks with specialized settings.&lt;/p&gt; &#xA;&lt;p&gt;Integration of OpenCLIP models and initiation of integration of KaliYuga models by Palmweaver / Chris Scalf (&lt;a href=&#34;https://twitter.com/ChrisScalf11&#34;&gt;https://twitter.com/ChrisScalf11&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;Integrated portrait_generator_v001 from Felipe3DArtist (&lt;a href=&#34;https://twitter.com/Felipe3DArtist&#34;&gt;https://twitter.com/Felipe3DArtist&lt;/a&gt;)&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Azure/MachineLearningNotebooks</title>
    <updated>2022-08-28T01:47:13Z</updated>
    <id>tag:github.com,2022-08-28:/Azure/MachineLearningNotebooks</id>
    <link href="https://github.com/Azure/MachineLearningNotebooks" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Python notebooks with ML and deep learning examples with Azure Machine Learning Python SDK | Microsoft&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Azure Machine Learning Python SDK notebooks&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;a community-driven repository of examples using mlflow for tracking can be found at &lt;a href=&#34;https://github.com/Azure/azureml-examples&#34;&gt;https://github.com/Azure/azureml-examples&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Welcome to the Azure Machine Learning Python SDK notebooks repository!&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;These notebooks are recommended for use in an Azure Machine Learning &lt;a href=&#34;https://docs.microsoft.com/azure/machine-learning/concept-compute-instance&#34;&gt;Compute Instance&lt;/a&gt;, where you can run them without any additional set up.&lt;/p&gt; &#xA;&lt;p&gt;However, the notebooks can be run in any development environment with the correct &lt;code&gt;azureml&lt;/code&gt; packages installed.&lt;/p&gt; &#xA;&lt;p&gt;Install the &lt;code&gt;azureml.core&lt;/code&gt; Python package:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install azureml-core&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install additional packages as needed:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install azureml-mlflow&#xA;pip install azureml-dataset-runtime&#xA;pip install azureml-automl-runtime&#xA;pip install azureml-pipeline&#xA;pip install azureml-pipeline-steps&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We recommend starting with one of the &lt;a href=&#34;https://raw.githubusercontent.com/Azure/MachineLearningNotebooks/master/tutorials/compute-instance-quickstarts&#34;&gt;quickstarts&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This repository is a push-only mirror. Pull requests are ignored.&lt;/p&gt; &#xA;&lt;h2&gt;Code of Conduct&lt;/h2&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. Please see the &lt;a href=&#34;https://raw.githubusercontent.com/Azure/MachineLearningNotebooks/master/CODE_OF_CONDUCT.md&#34;&gt;code of conduct&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;h2&gt;Reference&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/azure/machine-learning&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>