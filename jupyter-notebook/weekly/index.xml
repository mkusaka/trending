<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-16T01:57:38Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>mshumer/gpt-prompt-engineer</title>
    <updated>2023-07-16T01:57:38Z</updated>
    <id>tag:github.com,2023-07-16:/mshumer/gpt-prompt-engineer</id>
    <link href="https://github.com/mshumer/gpt-prompt-engineer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;gpt-prompt-engineer&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://twitter.com/mattshumer_&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/mattshumer_?style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/mshumer/gpt-prompt-engineer/blob/main/gpt_prompt_engineer.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open Main Version In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/16NLMjqyuUWxcokE_NF6RwHD8grwEeoaJ?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open Classification Version In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;Prompt engineering is kind of like alchemy. There&#39;s no clear way to predict what will work best. It&#39;s all about experimenting until you find the right prompt. &lt;code&gt;gpt-prompt-engineer&lt;/code&gt; is a tool that takes this experimentation to a whole new level.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Simply input a description of your task and some test cases, and the system will generate, test, and rank a multitude of prompts to find the ones that perform the best.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Prompt Generation&lt;/strong&gt;: Using GPT-4 and GPT-3.5-Turbo, &lt;code&gt;gpt-prompt-engineer&lt;/code&gt; can generate a variety of possible prompts based on a provided use-case and test cases.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Prompt Testing&lt;/strong&gt;: The real magic happens after the generation. The system tests each prompt against all the test cases, comparing their performance and ranking them using an ELO rating system.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img width=&#34;1563&#34; alt=&#34;Screen Shot 2023-07-04 at 11 41 54 AM&#34; src=&#34;https://github.com/mshumer/gpt-prompt-engineer/assets/41550495/f8171cff-1703-40ca-b9fd-f0aa24d07110&#34;&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ELO Rating System&lt;/strong&gt;: Each prompt starts with an ELO rating of 1200. As they compete against each other in generating responses to the test cases, their ELO ratings change based on their performance. This way, you can easily see which prompts are the most effective.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Classification Version&lt;/strong&gt;: The &lt;code&gt;gpt-prompt-engineer -- Classification Version&lt;/code&gt; notebook is designed to handle classification tasks. It evaluates the correctness of a test case by matching it to the expected output (&#39;true&#39; or &#39;false&#39;) and provides a table with scores for each prompt.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img width=&#34;1607&#34; alt=&#34;Screen Shot 2023-07-10 at 5 22 24 PM&#34; src=&#34;https://github.com/mshumer/gpt-prompt-engineer/assets/41550495/d5c9f2a8-97fa-445d-9c38-dec744f77854&#34;&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://wandb.ai/site/prompts&#34;&gt;Weights &amp;amp; Biases&lt;/a&gt; Logging&lt;/strong&gt;: Optional logging to &lt;a href=&#34;https://wandb.ai/site&#34;&gt;Weights &amp;amp; Biases&lt;/a&gt; of your configs such as temperature and max tokens, the system and user prompts for each part, the test cases used and the final ranked ELO rating for each candidate prompt. Set &lt;code&gt;use_wandb&lt;/code&gt; to &lt;code&gt;True&lt;/code&gt; to use.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/mshumer/gpt-prompt-engineer/blob/main/gpt_prompt_engineer.ipynb&#34;&gt;Open the notebook in Google Colab&lt;/a&gt; or in a local Jupyter notebook. For classification, use &lt;a href=&#34;https://colab.research.google.com/drive/16NLMjqyuUWxcokE_NF6RwHD8grwEeoaJ?usp=sharing&#34;&gt;this one.&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add your OpenAI API key to the line &lt;code&gt;openai.api_key = &#34;ADD YOUR KEY HERE&#34;&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you have GPT-4 access, you&#39;re ready to move on. If not, change &lt;code&gt;CANDIDATE_MODEL=&#39;gpt-4&#39;&lt;/code&gt; to &lt;code&gt;CANDIDATE_MODEL=&#39;gpt-3.5-turbo&#39;&lt;/code&gt;. If you&#39;re using the classification version, and don&#39;t have GPT-4 access, change &lt;code&gt;model=&#39;gpt-4&#39;&lt;/code&gt; in the second cell to &lt;code&gt;model=&#39;gpt-3.5-turbo&#39;&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;How to Use&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Define your use-case and test cases. The use-case is a description of what you want the AI to do. Test cases are specific prompts that you would like the AI to respond to. For example:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;description = &#34;Given a prompt, generate a landing page headline.&#34; # this style of description tends to work well&#xA;&#xA;test_cases = [&#xA;    {&#xA;        &#39;prompt&#39;: &#39;Promoting an innovative new fitness app, Smartly&#39;,&#xA;    },&#xA;    {&#xA;        &#39;prompt&#39;: &#39;Why a vegan diet is beneficial for your health&#39;,&#xA;    },&#xA;    {&#xA;        &#39;prompt&#39;: &#39;Introducing a new online course on digital marketing&#39;,&#xA;    },&#xA;    {&#xA;        &#39;prompt&#39;: &#39;Launching a new line of eco-friendly clothing&#39;,&#xA;    },&#xA;    {&#xA;        &#39;prompt&#39;: &#39;Promoting a new travel blog focusing on budget travel&#39;,&#xA;    },&#xA;    {&#xA;        &#39;prompt&#39;: &#39;Advertising a new software for efficient project management&#39;,&#xA;    },&#xA;    {&#xA;        &#39;prompt&#39;: &#39;Introducing a new book on mastering Python programming&#39;,&#xA;    },&#xA;    {&#xA;        &#39;prompt&#39;: &#39;Promoting a new online platform for learning languages&#39;,&#xA;    },&#xA;    {&#xA;        &#39;prompt&#39;: &#39;Advertising a new service for personalized meal plans&#39;,&#xA;    },&#xA;    {&#xA;        &#39;prompt&#39;: &#39;Launching a new app for mental health and mindfulness&#39;,&#xA;    }&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For the classification version, your test cases should be in the format:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;test_cases = [&#xA;    {&#xA;        &#39;prompt&#39;: &#39;I had a great day!&#39;,&#xA;        &#39;output&#39;: &#39;true&#39;&#xA;    },&#xA;    {&#xA;        &#39;prompt&#39;: &#39;I am feeling gloomy.&#39;,&#xA;        &#39;output&#39;: &#39;false&#39;&#xA;    },&#xA;    // add more test cases here&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;Choose how many prompts to generate. Keep in mind, this can get expensive if you generate many prompts. 10 is a good starting point.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Call &lt;code&gt;generate_optimal_prompt(description, test_cases, number_of_prompts)&lt;/code&gt; to generate a list of potential prompts, and test and rate their performance. For the classification version, just run the last cell.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The final ELO ratings will be printed in a table, sorted in descending order. The higher the rating, the better the prompt.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img width=&#34;1074&#34; alt=&#34;Screen Shot 2023-07-04 at 11 48 45 AM&#34; src=&#34;https://github.com/mshumer/gpt-prompt-engineer/assets/41550495/324f90b8-c0ee-45fd-b219-6c44d9aa281b&#34;&gt; &#xA;&lt;p&gt;For the classification version, the scores for each prompt will be printed in a table (see the image above).&lt;/p&gt; &#xA;&lt;h2&gt;Contributions are welcome! Some ideas:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;have a number of different system prompt generators that create different styles of prompts, to cover more ground (ex. examples, verbose, short, markdown, etc.)&lt;/li&gt; &#xA; &lt;li&gt;automatically generate the test cases&lt;/li&gt; &#xA; &lt;li&gt;expand the classification version to support more than two classes using tiktoken&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is &lt;a href=&#34;https://github.com/your_username/your_repository/raw/master/LICENSE&#34;&gt;MIT&lt;/a&gt; licensed.&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;Matt Shumer - &lt;a href=&#34;https://twitter.com/mattshumer_&#34;&gt;@mattshumer_&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Project Link: &lt;a href=&#34;https://raw.githubusercontent.com/mshumer/gpt-prompt-engineer/main/url&#34;&gt;https://github.com/mshumer/gpt-prompt-engineer&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Lastly, if you want to try something even cooler than this, sign up for &lt;a href=&#34;https://www.hyperwriteai.com/personal-assistant&#34;&gt;Personal Assistant&lt;/a&gt; (most of my time is spent on this). It&#39;s basically an AI that can operate your web browser to complete tasks for you.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>unlearning-challenge/starting-kit</title>
    <updated>2023-07-16T01:57:38Z</updated>
    <id>tag:github.com,2023-07-16:/unlearning-challenge/starting-kit</id>
    <link href="https://github.com/unlearning-challenge/starting-kit" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Starting kit for the NeurIPS 2023 unlearning challenge&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://github.com/unlearning-challenge/starting-kit/assets/277639/d1fa7889-5d91-4e6d-8082-7d59ef728f9c&#34; style=&#34;width: 100px&#34;&gt; &#xA;&lt;h1&gt;Starting kit for the NeurIPS 2023 Machine Unlearning Challenge&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains the starting kit for the NeurIPS 2023 Machine Unlearning Challenge. The starting kit currently contains the following examples:&lt;/p&gt; &#xA;&lt;h3&gt;Unlearning on CIFAR10&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/unlearning-challenge/starting-kit/assets/277639/acee217a-9ecd-484b-be81-8dcf5992eece&#34; alt=&#34;sample images from the CIFAR10 dataset&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The notebook &lt;a href=&#34;https://nbviewer.org/github/unlearning-challenge/starting-kit/tree/main/unlearning-CIFAR10.ipynb&#34;&gt;&lt;code&gt;unlearning-CIFAR10.ipynb&lt;/code&gt;&lt;/a&gt; provides a foundation for participants to build their unlearning models on the CIFAR-10 dataset. This jupyter notebook can be run locally, &lt;a href=&#34;https://colab.research.google.com/github/unlearning-challenge/starting-kit/blob/main/unlearning-CIFAR10.ipynb&#34;&gt;on Colab&lt;/a&gt;, or &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/unlearning-challenge/starting-kit/main/unlearning-CIFAR10.ipynb&#34;&gt;on Kaggle&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ai-forever/Kandinsky-2</title>
    <updated>2023-07-16T01:57:38Z</updated>
    <id>tag:github.com,2023-07-16:/ai-forever/Kandinsky-2</id>
    <link href="https://github.com/ai-forever/Kandinsky-2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Kandinsky 2 — multilingual text2image latent diffusion model&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Kandinsky 2.2&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1MfN9dfmejT8NjXhR353NeP5RzbruHgo7?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; — Inference example&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1lUWfe4CWhPJhUZYjMAE7g4ciHX4764rN?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; — Fine-tuning with LoRA&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://habr.com/ru/companies/sberbank/articles/747446/&#34;&gt;Habr post&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://fusionbrain.ai/diffusion&#34;&gt;Demo fusionbrain.ai&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://t.me/kandinsky21_bot&#34;&gt;Telegram-bot&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ai-forever/Kandinsky-2/main/content/kand_22.png&#34; width=&#34;60%&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Description:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Kandinsky 2.2 brings substantial improvements upon its predecessor, Kandinsky 2.1, by introducing a new, more powerful image encoder - CLIP-ViT-G and the ControlNet support.&lt;/p&gt; &#xA;&lt;p&gt;The switch to CLIP-ViT-G as the image encoder significantly increases the model&#39;s capability to generate more aesthetic pictures and better understand text, thus enhancing the model&#39;s overall performance.&lt;/p&gt; &#xA;&lt;p&gt;The addition of the ControlNet mechanism allows the model to effectively control the process of generating images. This leads to more accurate and visually appealing outputs and opens new possibilities for text-guided image manipulation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Architecture details:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Text encoder (XLM-Roberta-Large-Vit-L-14) - 560M&lt;/li&gt; &#xA; &lt;li&gt;Diffusion Image Prior — 1B&lt;/li&gt; &#xA; &lt;li&gt;CLIP image encoder (ViT-bigG-14-laion2B-39B-b160k) - 1.8B&lt;/li&gt; &#xA; &lt;li&gt;Latent Diffusion U-Net - 1.22B&lt;/li&gt; &#xA; &lt;li&gt;MoVQ encoder/decoder - 67M&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Сheckpoints:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/kandinsky-community/kandinsky-2-2-prior&#34;&gt;Prior&lt;/a&gt;: A prior diffusion model mapping text embeddings to image embeddings&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/kandinsky-community/kandinsky-2-2-decoder&#34;&gt;Text-to-Image / Image-to-Image&lt;/a&gt;: A decoding diffusion model mapping image embeddings to images&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/kandinsky-community/kandinsky-2-2-decoder-inpaint&#34;&gt;Inpainting&lt;/a&gt;: A decoding diffusion model mapping image embeddings and masked images to images&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/kandinsky-community/kandinsky-2-2-controlnet-depth&#34;&gt;ControlNet-depth&lt;/a&gt;: A decoding diffusion model mapping image embedding and additional depth condition to images&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Inference regimes&lt;/h3&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ai-forever/Kandinsky-2/main/content/kand_22_setting.png&#34; width=&#34;60%&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;How to use:&lt;/h2&gt; &#xA;&lt;p&gt;Check our jupyter notebooks with examples in &lt;code&gt;./notebooks&lt;/code&gt; folder&lt;/p&gt; &#xA;&lt;h3&gt;1. text2image&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from kandinsky2 import get_kandinsky2&#xA;model = get_kandinsky2(&#39;cuda&#39;, task_type=&#39;text2img&#39;, model_version=&#39;2.2&#39;)&#xA;images = model.generate_text2img(&#xA;    &#34;red cat, 4k photo&#34;, &#xA;    decoder_steps=50,&#xA;    batch_size=1, &#xA;    h=1024,&#xA;    w=768,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Kandinsky 2.1&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pytorch.org/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Framework-PyTorch-orange.svg?sanitize=true&#34; alt=&#34;Framework: PyTorch&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/sberbank-ai/Kandinsky_2.1&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97-Huggingface-yello.svg?sanitize=true&#34; alt=&#34;Huggingface space&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1xSbu-b-EwYd6GdaFPRVgvXBX_mciZ41e?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://habr.com/ru/company/sberbank/blog/725282/&#34;&gt;Habr post&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://fusionbrain.ai/diffusion&#34;&gt;Demo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install &#34;git+https://github.com/ai-forever/Kandinsky-2.git&#34;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Model architecture:&lt;/h2&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ai-forever/Kandinsky-2/main/content/kandinsky21.png&#34; width=&#34;80%&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Kandinsky 2.1 inherits best practicies from Dall-E 2 and Latent diffusion, while introducing some new ideas.&lt;/p&gt; &#xA;&lt;p&gt;As text and image encoder it uses CLIP model and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.&lt;/p&gt; &#xA;&lt;p&gt;For diffusion mapping of latent spaces we use transformer with num_layers=20, num_heads=32 and hidden_size=2048.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Other architecture parts:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Text encoder (XLM-Roberta-Large-Vit-L-14) - 560M&lt;/li&gt; &#xA; &lt;li&gt;Diffusion Image Prior — 1B&lt;/li&gt; &#xA; &lt;li&gt;CLIP image encoder (ViT-L/14) - 427M&lt;/li&gt; &#xA; &lt;li&gt;Latent Diffusion U-Net - 1.22B&lt;/li&gt; &#xA; &lt;li&gt;MoVQ encoder/decoder - 67M&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Kandinsky 2.1 was trained on a large-scale image-text dataset LAION HighRes and fine-tuned on our internal datasets.&lt;/p&gt; &#xA;&lt;h2&gt;How to use:&lt;/h2&gt; &#xA;&lt;p&gt;Check our jupyter notebooks with examples in &lt;code&gt;./notebooks&lt;/code&gt; folder&lt;/p&gt; &#xA;&lt;h3&gt;1. text2image&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from kandinsky2 import get_kandinsky2&#xA;model = get_kandinsky2(&#39;cuda&#39;, task_type=&#39;text2img&#39;, model_version=&#39;2.1&#39;, use_flash_attention=False)&#xA;images = model.generate_text2img(&#xA;    &#34;red cat, 4k photo&#34;, &#xA;    num_steps=100,&#xA;    batch_size=1, &#xA;    guidance_scale=4,&#xA;    h=768, w=768,&#xA;    sampler=&#39;p_sampler&#39;, &#xA;    prior_cf_scale=4,&#xA;    prior_steps=&#34;5&#34;&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ai-forever/Kandinsky-2/main/content/einstein.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;prompt: &#34;Einstein in space around the logarithm scheme&#34;&lt;/p&gt; &#xA;&lt;h3&gt;2. image fuse&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from kandinsky2 import get_kandinsky2&#xA;from PIL import Image&#xA;model = get_kandinsky2(&#39;cuda&#39;, task_type=&#39;text2img&#39;, model_version=&#39;2.1&#39;, use_flash_attention=False)&#xA;images_texts = [&#39;red cat&#39;, Image.open(&#39;img1.jpg&#39;), Image.open(&#39;img2.jpg&#39;), &#39;a man&#39;]&#xA;weights = [0.25, 0.25, 0.25, 0.25]&#xA;images = model.mix_images(&#xA;    images_texts, &#xA;    weights, &#xA;    num_steps=150,&#xA;    batch_size=1, &#xA;    guidance_scale=5,&#xA;    h=768, w=768,&#xA;    sampler=&#39;p_sampler&#39;, &#xA;    prior_cf_scale=4,&#xA;    prior_steps=&#34;5&#34;&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ai-forever/Kandinsky-2/main/content/fuse.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;3. inpainting&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from kandinsky2 import get_kandinsky2&#xA;from PIL import Image&#xA;import numpy as np&#xA;&#xA;model = get_kandinsky2(&#39;cuda&#39;, task_type=&#39;inpainting&#39;, model_version=&#39;2.1&#39;, use_flash_attention=False)&#xA;init_image = Image.open(&#39;img.jpg&#39;)&#xA;mask = np.ones((768, 768), dtype=np.float32)&#xA;mask[:,:550] =  0&#xA;images = model.generate_inpainting(&#xA;    &#39;man 4k photo&#39;, &#xA;    init_image, &#xA;    mask, &#xA;    num_steps=150,&#xA;    batch_size=1, &#xA;    guidance_scale=5,&#xA;    h=768, w=768,&#xA;    sampler=&#39;p_sampler&#39;, &#xA;    prior_cf_scale=4,&#xA;    prior_steps=&#34;5&#34;&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Kandinsky 2.0&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pytorch.org/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Framework-PyTorch-orange.svg?sanitize=true&#34; alt=&#34;Framework: PyTorch&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/sberbank-ai/Kandinsky_2.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97-Huggingface-yello.svg?sanitize=true&#34; alt=&#34;Huggingface space&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1uPg9KwGZ2hJBl9taGA_3kyKGw12Rh3ij?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://habr.com/ru/company/sberbank/blog/701162/&#34;&gt;Habr post&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://fusionbrain.ai/diffusion&#34;&gt;Demo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install &#34;git+https://github.com/ai-forever/Kandinsky-2.git&#34;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Model architecture:&lt;/h2&gt; &#xA;&lt;p&gt;It is a latent diffusion model with two multilingual text encoders:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;mCLIP-XLMR 560M parameters&lt;/li&gt; &#xA; &lt;li&gt;mT5-encoder-small 146M parameters&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;These encoders and multilingual training datasets unveil the real multilingual text-to-image generation experience!&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Kandinsky 2.0&lt;/strong&gt; was trained on a large 1B multilingual set, including samples that we used to train Kandinsky.&lt;/p&gt; &#xA;&lt;p&gt;In terms of diffusion architecture Kandinsky 2.0 implements UNet with 1.2B parameters.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Kandinsky 2.0&lt;/strong&gt; architecture overview:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ai-forever/Kandinsky-2/main/content/NatallE.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How to use:&lt;/h2&gt; &#xA;&lt;p&gt;Check our jupyter notebooks with examples in &lt;code&gt;./notebooks&lt;/code&gt; folder&lt;/p&gt; &#xA;&lt;h3&gt;1. text2img&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from kandinsky2 import get_kandinsky2&#xA;&#xA;model = get_kandinsky2(&#39;cuda&#39;, task_type=&#39;text2img&#39;)&#xA;images = model.generate_text2img(&#39;A teddy bear на красной площади&#39;, batch_size=4, h=512, w=512, num_steps=75, denoised_type=&#39;dynamic_threshold&#39;, dynamic_threshold_v=99.5, sampler=&#39;ddim_sampler&#39;, ddim_eta=0.05, guidance_scale=10)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ai-forever/Kandinsky-2/main/content/bear.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;prompt: &#34;A teddy bear на красной площади&#34;&lt;/p&gt; &#xA;&lt;h3&gt;2. inpainting&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from kandinsky2 import get_kandinsky2&#xA;from PIL import Image&#xA;import numpy as np&#xA;&#xA;model = get_kandinsky2(&#39;cuda&#39;, task_type=&#39;inpainting&#39;)&#xA;init_image = Image.open(&#39;image.jpg&#39;)&#xA;mask = np.ones((512, 512), dtype=np.float32)&#xA;mask[100:] =  0&#xA;images = model.generate_inpainting(&#39;Девушка в красном платье&#39;, init_image, mask, num_steps=50, denoised_type=&#39;dynamic_threshold&#39;, dynamic_threshold_v=99.5, sampler=&#39;ddim_sampler&#39;, ddim_eta=0.05, guidance_scale=10)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ai-forever/Kandinsky-2/main/content/inpainting.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;prompt: &#34;Девушка в красном платье&#34;&lt;/p&gt; &#xA;&lt;h3&gt;3. img2img&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from kandinsky2 import get_kandinsky2&#xA;from PIL import Image&#xA;&#xA;model = get_kandinsky2(&#39;cuda&#39;, task_type=&#39;img2img&#39;)&#xA;init_image = Image.open(&#39;image.jpg&#39;)&#xA;images = model.generate_img2img(&#39;кошка&#39;, init_image, strength=0.8, num_steps=50, denoised_type=&#39;dynamic_threshold&#39;, dynamic_threshold_v=99.5, sampler=&#39;ddim_sampler&#39;, ddim_eta=0.05, guidance_scale=10)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Authors&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Arseniy Shakhmatov: &lt;a href=&#34;https://github.com/cene555&#34;&gt;Github&lt;/a&gt;, &lt;a href=&#34;https://t.me/gradientdip&#34;&gt;Blog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Anton Razzhigaev: &lt;a href=&#34;https://github.com/razzant&#34;&gt;Github&lt;/a&gt;, &lt;a href=&#34;https://t.me/abstractDL&#34;&gt;Blog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Aleksandr Nikolich: &lt;a href=&#34;https://github.com/AlexWortega&#34;&gt;Github&lt;/a&gt;, &lt;a href=&#34;https://t.me/lovedeathtransformers&#34;&gt;Blog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Vladimir Arkhipkin: &lt;a href=&#34;https://github.com/oriBetelgeuse&#34;&gt;Github&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Igor Pavlov: &lt;a href=&#34;https://github.com/boomb0om&#34;&gt;Github&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Andrey Kuznetsov: &lt;a href=&#34;https://github.com/kuznetsoffandrey&#34;&gt;Github&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Denis Dimitrov: &lt;a href=&#34;https://github.com/denndimitrov&#34;&gt;Github&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>