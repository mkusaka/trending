<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-01-21T01:58:58Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>katanaml/sparrow</title>
    <updated>2024-01-21T01:58:58Z</updated>
    <id>tag:github.com,2024-01-21:/katanaml/sparrow</id>
    <link href="https://github.com/katanaml/sparrow" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Data extraction with ML and LLM&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Sparrow&lt;/h1&gt; &#xA;&lt;p&gt;Data extraction with ML and LLM&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;300&#34; height=&#34;300&#34; src=&#34;https://github.com/katanaml/sparrow/raw/main/sparrow-ui/donut/assets/sparrow_logo_5.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;The Principle&lt;/h2&gt; &#xA;&lt;p&gt;Sparrow is an innovative open-source solution designed for efficient data extraction and processing from various documents and images. It seamlessly handles forms, invoices, receipts, and other unstructured data sources. Sparrow stands out with its modular architecture, offering independent services such as OCR, Donut fine-tuning/inference, and a data labeling UI, all optimized for robust performance. Our current development efforts are focused on enhancing the LLM pipeline, promising exciting new features and capabilities. Our vision for Sparrow is to become the leading tool in data extraction, catering to diverse business domains. With a strong commitment to local data processing, we aim to empower customers with secure, cutting-edge technology. Join us in this journey to redefine data handling in the enterprise world.&lt;/p&gt; &#xA;&lt;h3&gt;Services&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/katanaml/sparrow/tree/main/sparrow-data/donut&#34;&gt;sparrow-data-donut&lt;/a&gt;&lt;/strong&gt; - This service focuses on data preparation specifically for the Donut ML model, including fine-tuning and OCR integration.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/katanaml/sparrow/tree/main/sparrow-data/ocr&#34;&gt;sparrow-data-ocr&lt;/a&gt;&lt;/strong&gt; - A standalone OCR service, providing robust optical character recognition as part of the Sparrow suite.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/katanaml/sparrow/tree/main/sparrow-ml/donut&#34;&gt;sparrow-ml-donut&lt;/a&gt;&lt;/strong&gt; - Dedicated to the Donut ML model, this service handles both fine-tuning and inference, streamlining the machine learning workflow.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/katanaml/sparrow/tree/main/sparrow-ml/lemming&#34;&gt;sparrow-ml-lemming&lt;/a&gt;&lt;/strong&gt; - A specialized service for the LLM RAG pipeline, enhancing the capabilities of language model processing.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/katanaml/sparrow/tree/main/sparrow-ui/donut&#34;&gt;sparrow-ui-donut&lt;/a&gt;&lt;/strong&gt; - A user-friendly interface for managing Donut ML model data labeling services and a dashboard.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Summary:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;For LLM RAG Enthusiasts&lt;/strong&gt; - Opt for the &lt;code&gt;lemming&lt;/code&gt; service, specifically designed to cater to your needs in LLM RAG applications.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;For Traditional ML Implementations&lt;/strong&gt; - The &lt;code&gt;donut&lt;/code&gt; service is available for those seeking machine learning solutions independent of LLM.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Sparrow offers a diverse range of services, as outlined previously. Our current developmental focus is primarily on enhancing and expanding the capabilities of the &lt;code&gt;lemming&lt;/code&gt; service.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;You have the flexibility to install either the Lemming or the Donut service independently. Each service is designed to operate as a standalone entity, without any dependencies on the other. This modular approach ensures that you can select the service that best meets your specific needs.&lt;/p&gt; &#xA;&lt;h3&gt;Lemming&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install Weaviate local DB with Docker:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Install the requirements:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://ollama.ai&#34;&gt;Ollama&lt;/a&gt; and pull LLM model specified in config.yml&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Donut&lt;/h3&gt; &#xA;&lt;p&gt;Follow the install steps outlined here:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Donut Data &lt;a href=&#34;https://github.com/katanaml/sparrow/tree/main/sparrow-data/donut&#34;&gt;install steps&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Donut ML &lt;a href=&#34;https://github.com/katanaml/sparrow/tree/main/sparrow-ml/donut&#34;&gt;install steps&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Donut UI &lt;a href=&#34;https://github.com/katanaml/sparrow/tree/main/sparrow-ui/donut&#34;&gt;install steps&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;OCR&lt;/h3&gt; &#xA;&lt;p&gt;Follow the install steps outlined here:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Sparrow OCR services &lt;a href=&#34;https://github.com/katanaml/sparrow/tree/main/sparrow-data/ocr&#34;&gt;install steps&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Lemming&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Copy text PDF files to the &lt;code&gt;data&lt;/code&gt; folder or use the sample data provided in the &lt;code&gt;data&lt;/code&gt; folder.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the script, to convert text to vector embeddings and save in Weaviate:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;./sparrow.sh ingest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Run the script, to process data with LLM RAG and return the answer:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;./sparrow.sh &#34;invoice_number, invoice_date, client_name, client_address, client_tax_id, seller_name, seller_address,&#xA;seller_tax_id, iban, names_of_invoice_items, gross_worth_of_invoice_items, total_gross_worth&#34; &#34;int, str, str, str, str,&#xA;str, str, str, str, List[str], List[float], str&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Answer:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;    &#34;invoice_number&#34;: 61356291,&#xA;    &#34;invoice_date&#34;: &#34;09/06/2012&#34;,&#xA;    &#34;client_name&#34;: &#34;Rodriguez-Stevens&#34;,&#xA;    &#34;client_address&#34;: &#34;2280 Angela Plain, Hortonshire, MS 93248&#34;,&#xA;    &#34;client_tax_id&#34;: &#34;939-98-8477&#34;,&#xA;    &#34;seller_name&#34;: &#34;Chapman, Kim and Green&#34;,&#xA;    &#34;seller_address&#34;: &#34;64731 James Branch, Smithmouth, NC 26872&#34;,&#xA;    &#34;seller_tax_id&#34;: &#34;949-84-9105&#34;,&#xA;    &#34;iban&#34;: &#34;GB50ACIE59715038217063&#34;,&#xA;    &#34;names_of_invoice_items&#34;: [&#xA;        &#34;Wine Glasses Goblets Pair Clear Glass&#34;,&#xA;        &#34;With Hooks Stemware Storage Multiple Uses Iron Wine Rack Hanging Glass&#34;,&#xA;        &#34;Replacement Corkscrew Parts Spiral Worm Wine Opener Bottle Houdini&#34;,&#xA;        &#34;HOME ESSENTIALS GRADIENT STEMLESS WINE GLASSES SET OF 4 20 FL OZ (591 ml) NEW&#34;&#xA;    ],&#xA;    &#34;gross_worth_of_invoice_items&#34;: [&#xA;        66.0,&#xA;        123.55,&#xA;        8.25,&#xA;        14.29&#xA;    ],&#xA;    &#34;total_gross_worth&#34;: &#34;$212,09&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;FastAPI Endpoint for Local LLM RAG&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Sparrow enables you to run a local LLM RAG as an API using FastAPI, providing a convenient and efficient way to interact with our services.&lt;/p&gt; &#xA;&lt;p&gt;To set this up:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Start the Endpoint&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Launch the endpoint by executing the following command in your terminal:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python api.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Access the Endpoint Documentation&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;You can view detailed documentation for the API by navigating to:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;http://127.0.0.1:8000/api/v1/sparrow-llm/docs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For visual reference, a screenshot of the FastAPI endpoint&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/katanaml/sparrow/raw/main/sparrow-ui/donut/assets/lemming_2.png&#34; alt=&#34;FastAPI endpoint&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Example of API call through CURL&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -X &#39;POST&#39; \&#xA;  &#39;http://127.0.0.1:8000/api/v1/sparrow-llm/inference&#39; \&#xA;  -H &#39;accept: application/json&#39; \&#xA;  -H &#39;Content-Type: application/json&#39; \&#xA;  -d &#39;{&#xA;  &#34;fields&#34;: &#34;invoice_number&#34;,&#xA;  &#34;types&#34;: &#34;int&#34;&#xA;}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Donut&lt;/h3&gt; &#xA;&lt;p&gt;Follow the steps outlined here:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Donut Data &lt;a href=&#34;https://github.com/katanaml/sparrow/tree/main/sparrow-data/donut&#34;&gt;usage steps&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Donut ML &lt;a href=&#34;https://github.com/katanaml/sparrow/tree/main/sparrow-ml/donut&#34;&gt;usage steps&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Donut UI &lt;a href=&#34;https://github.com/katanaml/sparrow/tree/main/sparrow-ui/donut&#34;&gt;usage steps&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;OCR&lt;/h3&gt; &#xA;&lt;p&gt;Follow the steps outlined here:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Sparrow OCR services &lt;a href=&#34;https://github.com/katanaml/sparrow/tree/main/sparrow-data/ocr&#34;&gt;usage steps&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;h3&gt;Inference with local LLM RAG&lt;/h3&gt; &#xA;&lt;p&gt;Request:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./sparrow.sh &#34;invoice_number, invoice_date, client_name, client_address, client_tax_id, seller_name, seller_address,&#xA;seller_tax_id, iban, names_of_invoice_items, gross_worth_of_invoice_items, total_gross_worth&#34; &#34;int, str, str, str, str,&#xA;str, str, str, str, List[str], List[float], str&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Response:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/katanaml/sparrow/raw/main/sparrow-ui/donut/assets/lemming_1.png&#34; alt=&#34;local RAG&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Inference with Donut ML model&lt;/h3&gt; &#xA;&lt;p&gt;Sparrow UI:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/katanaml/sparrow/raw/main/sparrow-ui/donut/assets/inference_actual.png&#34; alt=&#34;Inference Results&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Commercial usage&lt;/h2&gt; &#xA;&lt;p&gt;Sparrow is available for free commercial use. This offer applies to organizations with a gross revenue below $5 million USD in the past 12 months.&lt;/p&gt; &#xA;&lt;p&gt;For businesses exceeding this revenue limit and seeking to bypass GPL license restrictions for inference, please contact me at &lt;a href=&#34;mailto:abaranovskis@redsamuraiconsulting.com&#34;&gt;abaranovskis@redsamuraiconsulting.com&lt;/a&gt; to discuss dual licensing options. The same applies if you are looking for custom workflows to automate business processes, consulting options, and support/maintenance options.&lt;/p&gt; &#xA;&lt;h2&gt;Author&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://katanaml.io&#34;&gt;Katana ML&lt;/a&gt;, &lt;a href=&#34;https://github.com/abaranovskis-redsamurai&#34;&gt;Andrej Baranovskij&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Licensed under the Apache License, Version 2.0. Copyright 2020-2024 Katana ML, Andrej Baranovskij. &lt;a href=&#34;https://github.com/katanaml/sparrow/raw/main/LICENSE&#34;&gt;Copy of the license&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>neonbjb/tortoise-tts</title>
    <updated>2024-01-21T01:58:58Z</updated>
    <id>tag:github.com,2024-01-21:/neonbjb/tortoise-tts</id>
    <link href="https://github.com/neonbjb/tortoise-tts" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A multi-voice TTS system trained with an emphasis on quality&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TorToiSe&lt;/h1&gt; &#xA;&lt;p&gt;Tortoise is a text-to-speech program built with the following priorities:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Strong multi-voice capabilities.&lt;/li&gt; &#xA; &lt;li&gt;Highly realistic prosody and intonation.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;This repo contains all the code needed to run Tortoise TTS in inference mode.&lt;/p&gt; &#xA;&lt;p&gt;Manuscript: &lt;a href=&#34;https://arxiv.org/abs/2305.07243&#34;&gt;https://arxiv.org/abs/2305.07243&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Hugging Face space&lt;/h2&gt; &#xA;&lt;p&gt;A live demo is hosted on Hugging Face Spaces. If you&#39;d like to avoid a queue, please duplicate the Space and add a GPU. Please note that CPU-only spaces do not work for this demo.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/spaces/Manmay/tortoise-tts&#34;&gt;https://huggingface.co/spaces/Manmay/tortoise-tts&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Install via pip&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install tortoise-tts&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you would like to install the latest development version, you can also install it directly from the git repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install git+https://github.com/neonbjb/tortoise-tts&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;What&#39;s in a name?&lt;/h2&gt; &#xA;&lt;p&gt;I&#39;m naming my speech-related repos after Mojave desert flora and fauna. Tortoise is a bit tongue in cheek: this model is insanely slow. It leverages both an autoregressive decoder &lt;strong&gt;and&lt;/strong&gt; a diffusion decoder; both known for their low sampling rates. On a K80, expect to generate a medium sized sentence every 2 minutes.&lt;/p&gt; &#xA;&lt;p&gt;well..... not so slow anymore now we can get a &lt;strong&gt;0.25-0.3 RTF&lt;/strong&gt; on 4GB vram and with streaming we can get &amp;lt; &lt;strong&gt;500 ms&lt;/strong&gt; latency !!!&lt;/p&gt; &#xA;&lt;h2&gt;Demos&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;http://nonint.com/static/tortoise_v2_examples.html&#34;&gt;this page&lt;/a&gt; for a large list of example outputs.&lt;/p&gt; &#xA;&lt;p&gt;A cool application of Tortoise + GPT-3 (not affiliated with this repository): &lt;a href=&#34;https://twitter.com/lexman_ai&#34;&gt;https://twitter.com/lexman_ai&lt;/a&gt;. Unfortunately, this project seems no longer to be active.&lt;/p&gt; &#xA;&lt;h2&gt;Usage guide&lt;/h2&gt; &#xA;&lt;h3&gt;Local installation&lt;/h3&gt; &#xA;&lt;p&gt;If you want to use this on your own computer, you must have an NVIDIA GPU.&lt;/p&gt; &#xA;&lt;p&gt;On Windows, I &lt;strong&gt;highly&lt;/strong&gt; recommend using the Conda installation path. I have been told that if you do not do this, you will spend a lot of time chasing dependency problems.&lt;/p&gt; &#xA;&lt;p&gt;First, install miniconda: &lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;https://docs.conda.io/en/latest/miniconda.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Then run the following commands, using anaconda prompt as the terminal (or any other terminal configured to work with conda)&lt;/p&gt; &#xA;&lt;p&gt;This will:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;create conda environment with minimal dependencies specified&lt;/li&gt; &#xA; &lt;li&gt;activate the environment&lt;/li&gt; &#xA; &lt;li&gt;install pytorch with the command provided here: &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;https://pytorch.org/get-started/locally/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;clone tortoise-tts&lt;/li&gt; &#xA; &lt;li&gt;change the current directory to tortoise-tts&lt;/li&gt; &#xA; &lt;li&gt;run tortoise python setup install script&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda create --name tortoise python=3.9 numba inflect&#xA;conda activate tortoise&#xA;conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia&#xA;conda install transformers=4.29.2&#xA;git clone https://github.com/neonbjb/tortoise-tts.git&#xA;cd tortoise-tts&#xA;python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Optionally, pytorch can be installed in the base environment, so that other conda environments can use it too. To do this, simply send the &lt;code&gt;conda install pytorch...&lt;/code&gt; line before activating the tortoise environment.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; When you want to use tortoise-tts, you will always have to ensure the &lt;code&gt;tortoise&lt;/code&gt; conda environment is activated.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;If you are on windows, you may also need to install pysoundfile: &lt;code&gt;conda install -c conda-forge pysoundfile&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Docker&lt;/h3&gt; &#xA;&lt;p&gt;An easy way to hit the ground running and a good jumping off point depending on your use case.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/neonbjb/tortoise-tts.git&#xA;cd tortoise-tts&#xA;&#xA;docker build . -t tts&#xA;&#xA;docker run --gpus all \&#xA;    -e TORTOISE_MODELS_DIR=/models \&#xA;    -v /mnt/user/data/tortoise_tts/models:/models \&#xA;    -v /mnt/user/data/tortoise_tts/results:/results \&#xA;    -v /mnt/user/data/.cache/huggingface:/root/.cache/huggingface \&#xA;    -v /root:/work \&#xA;    -it tts&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This gives you an interactive terminal in an environment that&#39;s ready to do some tts. Now you can explore the different interfaces that tortoise exposes for tts.&lt;/p&gt; &#xA;&lt;p&gt;For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd app&#xA;conda activate tortoise&#xA;time python tortoise/do_tts.py \&#xA;    --output_path /results \&#xA;    --preset ultra_fast \&#xA;    --voice geralt \&#xA;    --text &#34;Time flies like an arrow; fruit flies like a bananna.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Apple Silicon&lt;/h2&gt; &#xA;&lt;p&gt;On macOS 13+ with M1/M2 chips you need to install the nighly version of PyTorch, as stated in the official page you can do:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cpu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Be sure to do that after you activate the environment. If you don&#39;t use conda the commands would look like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python3.10 -m venv .venv&#xA;source .venv/bin/activate&#xA;pip install numba inflect psutil&#xA;pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cpu&#xA;pip install transformers&#xA;git clone https://github.com/neonbjb/tortoise-tts.git&#xA;cd tortoise-tts&#xA;pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Be aware that DeepSpeed is disabled on Apple Silicon since it does not work. The flag &lt;code&gt;--use_deepspeed&lt;/code&gt; is ignored. You may need to prepend &lt;code&gt;PYTORCH_ENABLE_MPS_FALLBACK=1&lt;/code&gt; to the commands below to make them work since MPS does not support all the operations in Pytorch.&lt;/p&gt; &#xA;&lt;h3&gt;do_tts.py&lt;/h3&gt; &#xA;&lt;p&gt;This script allows you to speak a single phrase with one or more voices.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tortoise/do_tts.py --text &#34;I&#39;m going to speak this&#34; --voice random --preset fast&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;faster inference read.py&lt;/h3&gt; &#xA;&lt;p&gt;This script provides tools for reading large amounts of text.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tortoise/read_fast.py --textfile &amp;lt;your text to be read&amp;gt; --voice random&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;read.py&lt;/h3&gt; &#xA;&lt;p&gt;This script provides tools for reading large amounts of text.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tortoise/read.py --textfile &amp;lt;your text to be read&amp;gt; --voice random&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will break up the textfile into sentences, and then convert them to speech one at a time. It will output a series of spoken clips as they are generated. Once all the clips are generated, it will combine them into a single file and output that as well.&lt;/p&gt; &#xA;&lt;p&gt;Sometimes Tortoise screws up an output. You can re-generate any bad clips by re-running &lt;code&gt;read.py&lt;/code&gt; with the --regenerate argument.&lt;/p&gt; &#xA;&lt;h3&gt;API&lt;/h3&gt; &#xA;&lt;p&gt;Tortoise can be used programmatically, like so:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;reference_clips = [utils.audio.load_audio(p, 22050) for p in clips_paths]&#xA;tts = api.TextToSpeech()&#xA;pcm_audio = tts.tts_with_preset(&#34;your text here&#34;, voice_samples=reference_clips, preset=&#39;fast&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use deepspeed:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;reference_clips = [utils.audio.load_audio(p, 22050) for p in clips_paths]&#xA;tts = api.TextToSpeech(use_deepspeed=True)&#xA;pcm_audio = tts.tts_with_preset(&#34;your text here&#34;, voice_samples=reference_clips, preset=&#39;fast&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use kv cache:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;reference_clips = [utils.audio.load_audio(p, 22050) for p in clips_paths]&#xA;tts = api.TextToSpeech(kv_cache=True)&#xA;pcm_audio = tts.tts_with_preset(&#34;your text here&#34;, voice_samples=reference_clips, preset=&#39;fast&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run model in float16:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;reference_clips = [utils.audio.load_audio(p, 22050) for p in clips_paths]&#xA;tts = api.TextToSpeech(half=True)&#xA;pcm_audio = tts.tts_with_preset(&#34;your text here&#34;, voice_samples=reference_clips, preset=&#39;fast&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;for Faster runs use all three:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;reference_clips = [utils.audio.load_audio(p, 22050) for p in clips_paths]&#xA;tts = api.TextToSpeech(use_deepspeed=True, kv_cache=True, half=True)&#xA;pcm_audio = tts.tts_with_preset(&#34;your text here&#34;, voice_samples=reference_clips, preset=&#39;fast&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;This project has garnered more praise than I expected. I am standing on the shoulders of giants, though, and I want to credit a few of the amazing folks in the community that have helped make this happen:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Hugging Face, who wrote the GPT model and the generate API used by Tortoise, and who hosts the model weights.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2102.12092.pdf&#34;&gt;Ramesh et al&lt;/a&gt; who authored the DALLE paper, which is the inspiration behind Tortoise.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2102.09672.pdf&#34;&gt;Nichol and Dhariwal&lt;/a&gt; who authored the (revision of) the code that drives the diffusion model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2106.07889.pdf&#34;&gt;Jang et al&lt;/a&gt; who developed and open-sourced univnet, the vocoder this repo uses.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mindslab-ai/univnet&#34;&gt;Kim and Jung&lt;/a&gt; who implemented univnet pytorch model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lucidrains&#34;&gt;lucidrains&lt;/a&gt; who writes awesome open source pytorch models, many of which are used here.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/patrickvonplaten&#34;&gt;Patrick von Platen&lt;/a&gt; whose guides on setting up wav2vec were invaluable to building my dataset.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Notice&lt;/h2&gt; &#xA;&lt;p&gt;Tortoise was built entirely by the author (James Betker) using their own hardware. Their employer was not involved in any facet of Tortoise&#39;s development.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Tortoise TTS is licensed under the Apache 2.0 license.&lt;/p&gt; &#xA;&lt;p&gt;If you use this repo or the ideas therein for your research, please cite it! A bibtex entree can be found in the right pane on GitHub.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/dinov2</title>
    <updated>2024-01-21T01:58:58Z</updated>
    <id>tag:github.com,2024-01-21:/facebookresearch/dinov2</id>
    <link href="https://github.com/facebookresearch/dinov2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;PyTorch code and models for the DINOv2 self-supervised learning method.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;span&gt;üÜï&lt;/span&gt; [2023-10-26] &lt;em&gt;Added DINOv2 backbones with registers, following &lt;a href=&#34;https://arxiv.org/abs/2309.16588&#34;&gt;Vision Transformers Need Registers&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h1&gt;DINOv2: Learning Robust Visual Features without Supervision&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://ai.facebook.com/research/&#34;&gt;Meta AI Research, FAIR&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Maxime Oquab, Timoth√©e Darcet, Th√©o Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Patrick Labatut, Armand Joulin, Piotr Bojanowski&lt;/p&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://arxiv.org/abs/2304.07193&#34;&gt;&lt;code&gt;Paper #1&lt;/code&gt;&lt;/a&gt;] &lt;a href=&#34;https://arxiv.org/abs/2309.16588&#34;&gt;&lt;code&gt;Paper #2&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://ai.facebook.com/blog/dino-v2-computer-vision-self-supervised-learning/&#34;&gt;&lt;code&gt;Blog&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://dinov2.metademolab.com&#34;&gt;&lt;code&gt;Demo&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/dinov2/main/#citing-dinov2&#34;&gt;&lt;code&gt;BibTeX&lt;/code&gt;&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;PyTorch implementation and pretrained models for DINOv2. For details, see the papers: &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.07193&#34;&gt;DINOv2: Learning Robust Visual Features without Supervision&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2309.16588&#34;&gt;Vision Transformers Need Registers&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;DINOv2 models produce high-performance visual features that can be directly employed with classifiers as simple as linear layers on a variety of computer vision tasks; these visual features are robust and perform well across domains without any requirement for fine-tuning. The models were pretrained on a dataset of 142 M images without using any labels or annotations.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/facebookresearch/dinov2/assets/60359573/f168823e-7922-415a-b429-578badf5c356&#34;&gt;https://github.com/facebookresearch/dinov2/assets/60359573/f168823e-7922-415a-b429-578badf5c356&lt;/a&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA;  Visualization of the three first principal components of the patch features of all frames, mapped to RGB values. &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Pretrained models&lt;/h2&gt; &#xA;&lt;table style=&#34;margin: auto&#34;&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;model&lt;/th&gt; &#xA;   &lt;th&gt;# of&lt;br&gt;params&lt;/th&gt; &#xA;   &lt;th&gt;with&lt;br&gt;registers&lt;/th&gt; &#xA;   &lt;th&gt;ImageNet&lt;br&gt;k-NN&lt;/th&gt; &#xA;   &lt;th&gt;ImageNet&lt;br&gt;linear&lt;/th&gt; &#xA;   &lt;th&gt;download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-S/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;21 M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;79.0%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;81.1%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_pretrain.pth&#34;&gt;backbone only&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-S/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;21 M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;79.1%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;80.9%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_reg4_pretrain.pth&#34;&gt;backbone only&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-B/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;86 M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;82.1%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;84.5%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_pretrain.pth&#34;&gt;backbone only&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-B/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;86 M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;82.0%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;84.6%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_reg4_pretrain.pth&#34;&gt;backbone only&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-L/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;300 M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;83.5%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;86.3%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_pretrain.pth&#34;&gt;backbone only&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-L/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;300 M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;83.8%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;86.7%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_reg4_pretrain.pth&#34;&gt;backbone only&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-g/14&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1,100 M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;83.5%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;86.5%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_pretrain.pth&#34;&gt;backbone only&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-g/14&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1,100 M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;83.7%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;87.1%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_reg4_pretrain.pth&#34;&gt;backbone only&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Pretrained backbones (via PyTorch Hub)&lt;/h3&gt; &#xA;&lt;p&gt;Please follow the instructions &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;here&lt;/a&gt; to install PyTorch (the only required dependency for loading the model). Installing PyTorch with CUDA support is strongly recommended.&lt;/p&gt; &#xA;&lt;p&gt;A corresponding &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/dinov2/main/MODEL_CARD.md&#34;&gt;model card&lt;/a&gt; is included in the repository.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;&#xA;# DINOv2&#xA;dinov2_vits14 = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vits14&#39;)&#xA;dinov2_vitb14 = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitb14&#39;)&#xA;dinov2_vitl14 = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitl14&#39;)&#xA;dinov2_vitg14 = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitg14&#39;)&#xA;&#xA;# DINOv2 with registers&#xA;dinov2_vits14_reg = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vits14_reg&#39;)&#xA;dinov2_vitb14_reg = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitb14_reg&#39;)&#xA;dinov2_vitl14_reg = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitl14_reg&#39;)&#xA;dinov2_vitg14_reg = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitg14_reg&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Pretrained heads - Image classification&lt;/h3&gt; &#xA;&lt;table style=&#34;margin: auto&#34;&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th rowspan=&#34;2&#34;&gt;backbone&lt;/th&gt; &#xA;   &lt;th rowspan=&#34;2&#34;&gt;with&lt;br&gt;registers&lt;/th&gt; &#xA;   &lt;th&gt;download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;ImageNet&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-S/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt; linear head (&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_linear_head.pth&#34;&gt;1 layer&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_linear4_head.pth&#34;&gt;4 layers&lt;/a&gt;) &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-S/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt; linear head (&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_reg4_linear_head.pth&#34;&gt;1 layer&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_reg4_linear4_head.pth&#34;&gt;4 layers&lt;/a&gt;) &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-B/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt; linear head (&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_linear_head.pth&#34;&gt;1 layer&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_linear4_head.pth&#34;&gt;4 layers&lt;/a&gt;) &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-B/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt; linear head (&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_reg4_linear_head.pth&#34;&gt;1 layer&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_reg4_linear4_head.pth&#34;&gt;4 layers&lt;/a&gt;) &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-L/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt; linear head (&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_linear_head.pth&#34;&gt;1 layer&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_linear4_head.pth&#34;&gt;4 layers&lt;/a&gt;) &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-L/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt; linear head (&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_reg4_linear_head.pth&#34;&gt;1 layer&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_reg4_linear4_head.pth&#34;&gt;4 layers&lt;/a&gt;) &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-g/14&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt; linear head (&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_linear_head.pth&#34;&gt;1 layer&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_linear4_head.pth&#34;&gt;4 layers&lt;/a&gt;) &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-g/14&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt; linear head (&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_lreg4_inear_head.pth&#34;&gt;1 layer&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_reg4_linear4_head.pth&#34;&gt;4 layers&lt;/a&gt;) &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The (full) classifier models can be loaded via PyTorch Hub:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;&#xA;# DINOv2&#xA;dinov2_vits14_lc = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vits14_lc&#39;)&#xA;dinov2_vitb14_lc = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitb14_lc&#39;)&#xA;dinov2_vitl14_lc = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitl14_lc&#39;)&#xA;dinov2_vitg14_lc = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitg14_lc&#39;)&#xA;&#xA;# DINOv2 with registers&#xA;dinov2_vits14_reg_lc = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vits14_reg_lc&#39;)&#xA;dinov2_vitb14_reg_lc = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitb14_reg_lc&#39;)&#xA;dinov2_vitl14_reg_lc = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitl14_reg_lc&#39;)&#xA;dinov2_vitg14_reg_lc = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitg14_reg_lc&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Pretrained heads - Depth estimation&lt;/h3&gt; &#xA;&lt;table style=&#34;margin: auto&#34;&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th rowspan=&#34;2&#34;&gt;backbone&lt;/th&gt; &#xA;   &lt;th colspan=&#34;2&#34;&gt;download head&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;NYUd&lt;/th&gt; &#xA;   &lt;th&gt;KITTI&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-S/14 distilled&lt;/td&gt; &#xA;   &lt;td&gt; linear (&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_nyu_linear_head.pth&#34;&gt;1 layer&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_nyu_linear4_head.pth&#34;&gt;4 layers&lt;/a&gt;), &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_nyu_dpt_head.pth&#34;&gt;DPT&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; linear (&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_kitti_linear_head.pth&#34;&gt;1 layer&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_kitti_linear4_head.pth&#34;&gt;4 layers&lt;/a&gt;), &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_kitti_dpt_head.pth&#34;&gt;DPT&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-B/14 distilled&lt;/td&gt; &#xA;   &lt;td&gt; linear (&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_linear_head.pth&#34;&gt;1 layer&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_nyu_linear4_head.pth&#34;&gt;4 layers&lt;/a&gt;), &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_nyu_dpt_head.pth&#34;&gt;DPT&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; linear (&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_kitti_linear_head.pth&#34;&gt;1 layer&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_kitti_linear4_head.pth&#34;&gt;4 layers&lt;/a&gt;), &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_kitti_dpt_head.pth&#34;&gt;DPT&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-L/14 distilled&lt;/td&gt; &#xA;   &lt;td&gt; linear (&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_linear_head.pth&#34;&gt;1 layer&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_nyu_linear4_head.pth&#34;&gt;4 layers&lt;/a&gt;), &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_nyu_dpt_head.pth&#34;&gt;DPT&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; linear (&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_kitti_linear_head.pth&#34;&gt;1 layer&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_kitti_linear4_head.pth&#34;&gt;4 layers&lt;/a&gt;), &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_kitti_dpt_head.pth&#34;&gt;DPT&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-g/14&lt;/td&gt; &#xA;   &lt;td&gt; linear (&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_linear_head.pth&#34;&gt;1 layer&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_nyu_linear4_head.pth&#34;&gt;4 layers&lt;/a&gt;), &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_nyu_dpt_head.pth&#34;&gt;DPT&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; linear (&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_kitti_linear_head.pth&#34;&gt;1 layer&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_kitti_linear4_head.pth&#34;&gt;4 layers&lt;/a&gt;), &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_kitti_dpt_head.pth&#34;&gt;DPT&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Pretrained heads - Semantic segmentation&lt;/h3&gt; &#xA;&lt;table style=&#34;margin: auto&#34;&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th rowspan=&#34;2&#34;&gt;backbone&lt;/th&gt; &#xA;   &lt;th&gt;download model&lt;/th&gt; &#xA;   &lt;th colspan=&#34;2&#34;&gt;download head&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;ADE20K&lt;/th&gt; &#xA;   &lt;th&gt;ADE20K&lt;/th&gt; &#xA;   &lt;th&gt;VOC2012&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-S/14 distilled&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_ade20k_linear_head.pth&#34;&gt;linear&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_ade20k_ms_head.pth&#34;&gt;multi-scale&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_voc2012_linear_head.pth&#34;&gt;linear&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_voc2012_ms_head.pth&#34;&gt;multi-scale&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-B/14 distilled&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_ade20k_linear_head.pth&#34;&gt;linear&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_ade20k_ms_head.pth&#34;&gt;multi-scale&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_voc2012_linear_head.pth&#34;&gt;linear&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_voc2012_ms_head.pth&#34;&gt;multi-scale&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-L/14 distilled&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_ade20k_linear_head.pth&#34;&gt;linear&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_ade20k_ms_head.pth&#34;&gt;multi-scale&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_voc2012_linear_head.pth&#34;&gt;linear&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_voc2012_ms_head.pth&#34;&gt;multi-scale&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-g/14&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_ade20k_m2f.pth&#34;&gt;Mask2Former&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_ade20k_linear_head.pth&#34;&gt;linear&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_ade20k_ms_head.pth&#34;&gt;multi-scale&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_voc2012_linear_head.pth&#34;&gt;linear&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_voc2012_ms_head.pth&#34;&gt;multi-scale&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;The training and evaluation code requires PyTorch 2.0 and &lt;a href=&#34;https://github.com/facebookresearch/xformers&#34;&gt;xFormers&lt;/a&gt; 0.0.18 as well as a number of other 3rd party packages. Note that the code has only been tested with the specified versions and also expects a Linux environment. To setup all the required dependencies for training and evaluation, please follow the instructions below:&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://docs.conda.io/projects/conda/en/latest/user-guide/getting-started.html&#34;&gt;conda&lt;/a&gt;&lt;/em&gt; &lt;strong&gt;(Recommended)&lt;/strong&gt; - Clone the repository and then create and activate a &lt;code&gt;dinov2&lt;/code&gt; conda environment using the provided environment definition:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda env create -f conda.yaml&#xA;conda activate dinov2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://pip.pypa.io/en/stable/getting-started/&#34;&gt;pip&lt;/a&gt;&lt;/em&gt; - Clone the repository and then use the provided &lt;code&gt;requirements.txt&lt;/code&gt; to install the dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For dense tasks (depth estimation and semantic segmentation), there are additional dependencies (specific versions of &lt;code&gt;mmcv&lt;/code&gt; and &lt;code&gt;mmsegmentation&lt;/code&gt;) which are captured in the &lt;code&gt;extras&lt;/code&gt; dependency specifications:&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://docs.conda.io/projects/conda/en/latest/user-guide/getting-started.html&#34;&gt;conda&lt;/a&gt;&lt;/em&gt; &lt;strong&gt;(Recommended)&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda env create -f conda-extras.yaml&#xA;conda activate dinov2-extras&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://pip.pypa.io/en/stable/getting-started/&#34;&gt;pip&lt;/a&gt;&lt;/em&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r requirements.txt -r requirements-extras.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Data preparation&lt;/h2&gt; &#xA;&lt;h3&gt;ImageNet-1k&lt;/h3&gt; &#xA;&lt;p&gt;The root directory of the dataset should hold the following contents:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/test/ILSVRC2012_test_00000001.JPEG&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/test/[..]&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/test/ILSVRC2012_test_00100000.JPEG&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/train/n01440764/n01440764_10026.JPEG&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/train/[...]&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/train/n15075141/n15075141_9993.JPEG&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/val/n01440764/ILSVRC2012_val_00000293.JPEG&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/val/[...]&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/val/n15075141/ILSVRC2012_val_00049174.JPEG&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/labels.txt&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The provided dataset implementation expects a few additional metadata files to be present under the extra directory:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;EXTRA&amp;gt;/class-ids-TRAIN.npy&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;EXTRA&amp;gt;/class-ids-VAL.npy&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;EXTRA&amp;gt;/class-names-TRAIN.npy&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;EXTRA&amp;gt;/class-names-VAL.npy&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;EXTRA&amp;gt;/entries-TEST.npy&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;EXTRA&amp;gt;/entries-TRAIN.npy&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;EXTRA&amp;gt;/entries-VAL.npy&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;These metadata files can be generated (once) with the following lines of Python code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from dinov2.data.datasets import ImageNet&#xA;&#xA;for split in ImageNet.Split:&#xA;    dataset = ImageNet(split=split, root=&#34;&amp;lt;ROOT&amp;gt;&#34;, extra=&#34;&amp;lt;EXTRA&amp;gt;&#34;)&#xA;    dataset.dump_extra()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that the root and extra directories do not have to be distinct directories.&lt;/p&gt; &#xA;&lt;h3&gt;ImageNet-22k&lt;/h3&gt; &#xA;&lt;p&gt;Please adapt the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/dinov2/main/dinov2/data/datasets/image_net_22k.py&#34;&gt;dataset class&lt;/a&gt; to match your local setup.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;span&gt;‚ö†&lt;/span&gt; To execute the commands provided in the next sections for training and evaluation, the &lt;code&gt;dinov2&lt;/code&gt; package should be included in the Python module search path, i.e. simply prefix the command to run with &lt;code&gt;PYTHONPATH=.&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;h3&gt;Fast setup: training DINOv2 ViT-L/16 on ImageNet-1k&lt;/h3&gt; &#xA;&lt;p&gt;Run DINOv2 training on 4 A100-80GB nodes (32 GPUs) in a SLURM cluster environment with submitit:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python dinov2/run/train/train.py \&#xA;    --nodes 4 \&#xA;    --config-file dinov2/configs/train/vitl16_short.yaml \&#xA;    --output-dir &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt; \&#xA;    train.dataset_path=ImageNet:split=TRAIN:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Training time is approximately 1 day and the resulting checkpoint should reach 81.6% on k-NN eval and 82.9% on linear eval.&lt;/p&gt; &#xA;&lt;p&gt;The training code saves the weights of the teacher in the &lt;code&gt;eval&lt;/code&gt; folder every 12500 iterations for evaluation.&lt;/p&gt; &#xA;&lt;h3&gt;Long setup: training DINOv2 ViT-L/14 on ImageNet-22k&lt;/h3&gt; &#xA;&lt;p&gt;Run DINOv2 training on 12 A100-80GB nodes (96 GPUs) in a SLURM cluster environment with submitit:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python dinov2/run/train/train.py \&#xA;    --nodes 12 \&#xA;    --config-file dinov2/configs/train/vitl14.yaml \&#xA;    --output-dir &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt; \&#xA;    train.dataset_path=ImageNet22k:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Training time is approximately 3.3 days and the resulting checkpoint should reach 82.0% on k-NN eval and 84.5% on linear eval.&lt;/p&gt; &#xA;&lt;p&gt;The training code saves the weights of the teacher in the &lt;code&gt;eval&lt;/code&gt; folder every 12500 iterations for evaluation.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;The training code regularly saves the teacher weights. In order to evaluate the model, run the following evaluation on a single node:&lt;/p&gt; &#xA;&lt;h3&gt;k-NN classification on ImageNet-1k&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python dinov2/run/eval/knn.py \&#xA;    --config-file &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/config.yaml \&#xA;    --pretrained-weights &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/eval/training_24999/teacher_checkpoint.pth \&#xA;    --output-dir &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/eval/training_24999/knn \&#xA;    --train-dataset ImageNet:split=TRAIN:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt; \&#xA;    --val-dataset ImageNet:split=VAL:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Logistic regression classification on ImageNet-1k&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python dinov2/run/eval/log_regression.py \&#xA;    --config-file &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/config.yaml \&#xA;    --pretrained-weights &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/eval/training_24999/teacher_checkpoint.pth \&#xA;    --output-dir &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/eval/training_24999/logreg \&#xA;    --train-dataset ImageNet:split=TRAIN:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt; \&#xA;    --val-dataset ImageNet:split=VAL:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Linear classification with data augmentation on ImageNet-1k&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python dinov2/run/eval/linear.py \&#xA;    --config-file &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/config.yaml \&#xA;    --pretrained-weights &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/eval/training_24999/teacher_checkpoint.pth \&#xA;    --output-dir &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/eval/training_24999/linear \&#xA;    --train-dataset ImageNet:split=TRAIN:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt; \&#xA;    --val-dataset ImageNet:split=VAL:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We release the weights from evaluating the different models:&lt;/p&gt; &#xA;&lt;table style=&#34;margin: auto&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;model&lt;/th&gt; &#xA;   &lt;th&gt;with&lt;br&gt;registers&lt;/th&gt; &#xA;   &lt;th&gt;ImageNet&lt;br&gt;top-1&lt;/th&gt; &#xA;   &lt;th&gt;linear evaluation&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-S/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;81.1%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_linear_head.pth&#34;&gt;linear head weights&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-S/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;80.8%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_reg4_linear_head.pth&#34;&gt;linear head weights&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-B/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;84.5%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_linear_head.pth&#34;&gt;linear head weights&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-B/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;84.4%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_reg4_linear_head.pth&#34;&gt;linear head weights&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-L/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;86.3%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_linear_head.pth&#34;&gt;linear head weights&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-L/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;86.5%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_reg4_linear_head.pth&#34;&gt;linear head weights&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-g/14&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;86.5%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_linear_head.pth&#34;&gt;linear head weights&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-g/14&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;87.0%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_reg4_linear_head.pth&#34;&gt;linear head weights&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;The performance of the provided pretrained model weights can be evaluated as follows on ImageNet-1k:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python dinov2/run/eval/linear.py \&#xA;    --config-file dinov2/configs/eval/vitg14_pretrain.yaml \&#xA;    --pretrained-weights https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_pretrain.pth \&#xA;    --train-dataset ImageNet:split=TRAIN:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt; \&#xA;    --val-dataset ImageNet:split=VAL:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Notebooks&lt;/h2&gt; &#xA;&lt;p&gt;A few notebooks are provided to help the community leverage the models and code:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/dinov2/raw/main/notebooks/depth_estimation.ipynb&#34;&gt;Depth estimation&lt;/a&gt; - How to load and use the depth heads in combination with a matching backbone via mmcv&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/dinov2/raw/main/notebooks/semantic_segmentation.ipynb&#34;&gt;Semantic segmentation&lt;/a&gt; - How to load and use the segmentation heads in combination with a matching backbone via mmcv, and also how to load and use the Mask2Former-based segmentation model trained on ADE20K&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;DINOv2 code and model weights are released under the Apache License 2.0. See &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/dinov2/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; for additional details.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/dinov2/main/CONTRIBUTING.md&#34;&gt;contributing&lt;/a&gt; and the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/dinov2/main/CODE_OF_CONDUCT.md&#34;&gt;code of conduct&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citing DINOv2&lt;/h2&gt; &#xA;&lt;p&gt;If you find this repository useful, please consider giving a star &lt;span&gt;‚≠ê&lt;/span&gt; and citation &lt;span&gt;ü¶ñ&lt;/span&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{oquab2023dinov2,&#xA;  title={DINOv2: Learning Robust Visual Features without Supervision},&#xA;  author={Oquab, Maxime and Darcet, Timoth√©e and Moutakanni, Theo and Vo, Huy V. and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and Howes, Russell and Huang, Po-Yao and Xu, Hu and Sharma, Vasu and Li, Shang-Wen and Galuba, Wojciech and Rabbat, Mike and Assran, Mido and Ballas, Nicolas and Synnaeve, Gabriel and Misra, Ishan and Jegou, Herve and Mairal, Julien and Labatut, Patrick and Joulin, Armand and Bojanowski, Piotr},&#xA;  journal={arXiv:2304.07193},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{darcet2023vitneedreg,&#xA;  title={Vision Transformers Need Registers},&#xA;  author={Darcet, Timoth√©e and Oquab, Maxime and Mairal, Julien and Bojanowski, Piotr},&#xA;  journal={arXiv:2309.16588},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>