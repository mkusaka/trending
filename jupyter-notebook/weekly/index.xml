<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-09-29T01:44:39Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>graviraja/MLOps-Basics</title>
    <updated>2024-09-29T01:44:39Z</updated>
    <id>tag:github.com,2024-09-29:/graviraja/MLOps-Basics</id>
    <link href="https://github.com/graviraja/MLOps-Basics" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MLOps-Basics&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;There is nothing magic about magic. The magician merely understands something simple which doesn’t appear to be simple or natural to the untrained audience. Once you learn how to hold a card while making your hand look empty, you only need practice before you, too, can “do magic.” – Jeffrey Friedl in the book Mastering Regular Expressions&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note: Please raise an issue for any suggestions, corrections, and feedback.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The goal of the series is to understand the basics of MLOps like model building, monitoring, configurations, testing, packaging, deployment, cicd, etc.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/graviraja/MLOps-Basics/main/images/summary.png&#34; alt=&#34;pl&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Week 0: Project Setup&lt;/h2&gt; &#xA;&lt;img src=&#34;https://img.shields.io/static/v1.svg?style=for-the-badge&amp;amp;label=difficulty&amp;amp;message=easy&amp;amp;color=green&#34;&gt; &#xA;&lt;p&gt;Refer to the &lt;a href=&#34;https://deep-learning-blogs.vercel.app/blog/mlops-project-setup-part1&#34;&gt;Blog Post here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The project I have implemented is a simple classification problem. The scope of this week is to understand the following topics:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;How to get the data?&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;How to process the data?&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;How to define dataloaders?&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;How to declare the model?&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;How to train the model?&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;How to do the inference?&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/graviraja/MLOps-Basics/main/images/pl.jpeg&#34; alt=&#34;pl&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Following tech stack is used:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/datasets&#34;&gt;Huggingface Datasets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;Huggingface Transformers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch-lightning.readthedocs.io/&#34;&gt;Pytorch Lightning&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Week 1: Model monitoring - Weights and Biases&lt;/h2&gt; &#xA;&lt;img src=&#34;https://img.shields.io/static/v1.svg?style=for-the-badge&amp;amp;label=difficulty&amp;amp;message=easy&amp;amp;color=green&#34;&gt; &#xA;&lt;p&gt;Refer to the &lt;a href=&#34;https://deep-learning-blogs.vercel.app/blog/mlops-wandb-integration&#34;&gt;Blog Post here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Tracking all the experiments like tweaking hyper-parameters, trying different models to test their performance and seeing the connection between model and the input data will help in developing a better model.&lt;/p&gt; &#xA;&lt;p&gt;The scope of this week is to understand the following topics:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;How to configure basic logging with W&amp;amp;B?&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;How to compute metrics and log them in W&amp;amp;B?&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;How to add plots in W&amp;amp;B?&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;How to add data samples to W&amp;amp;B?&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/graviraja/MLOps-Basics/main/images/wandb.png&#34; alt=&#34;wannb&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Following tech stack is used:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://wandb.ai/site&#34;&gt;Weights and Biases&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://torchmetrics.readthedocs.io/&#34;&gt;torchmetrics&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;References:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=hUXQm46TAKc&#34;&gt;Tutorial on Pytorch Lightning + Weights &amp;amp; Bias&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://docs.wandb.ai/&#34;&gt;WandB Documentation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Week 2: Configurations - Hydra&lt;/h2&gt; &#xA;&lt;img src=&#34;https://img.shields.io/static/v1.svg?style=for-the-badge&amp;amp;label=difficulty&amp;amp;message=easy&amp;amp;color=green&#34;&gt; &#xA;&lt;p&gt;Refer to the &lt;a href=&#34;https://deep-learning-blogs.vercel.app/blog/mlops-hydra-config&#34;&gt;Blog Post here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Configuration management is a necessary for managing complex software systems. Lack of configuration management can cause serious problems with reliability, uptime, and the ability to scale a system.&lt;/p&gt; &#xA;&lt;p&gt;The scope of this week is to understand the following topics:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;Basics of Hydra&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Overridding configurations&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Splitting configuration across multiple files&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Variable Interpolation&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;How to run model with different parameter combinations?&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/graviraja/MLOps-Basics/main/images/hydra.png&#34; alt=&#34;hydra&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Following tech stack is used:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hydra.cc/&#34;&gt;Hydra&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;References&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://hydra.cc/docs/intro&#34;&gt;Hydra Documentation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.sscardapane.it/tutorials/hydra-tutorial/#executing-multiple-runs&#34;&gt;Simone Tutorial on Hydra&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Week 3: Data Version Control - DVC&lt;/h2&gt; &#xA;&lt;img src=&#34;https://img.shields.io/static/v1.svg?style=for-the-badge&amp;amp;label=difficulty&amp;amp;message=easy&amp;amp;color=green&#34;&gt; &#xA;&lt;p&gt;Refer to the &lt;a href=&#34;https://deep-learning-blogs.vercel.app/blog/mlops-dvc&#34;&gt;Blog Post here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Classical code version control systems are not designed to handle large files, which make cloning and storing the history impractical. Which are very common in Machine Learning.&lt;/p&gt; &#xA;&lt;p&gt;The scope of this week is to understand the following topics:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;Basics of DVC&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Initialising DVC&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Configuring Remote Storage&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Saving Model to the Remote Storage&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Versioning the models&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/graviraja/MLOps-Basics/main/images/dvc.png&#34; alt=&#34;dvc&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Following tech stack is used:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dvc.org/&#34;&gt;DVC&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;References&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://dvc.org/doc&#34;&gt;DVC Documentation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=kLKBcPonMYw&#34;&gt;DVC Tutorial on Versioning data&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Week 4: Model Packaging - ONNX&lt;/h2&gt; &#xA;&lt;img src=&#34;https://img.shields.io/static/v1.svg?style=for-the-badge&amp;amp;label=difficulty&amp;amp;message=medium&amp;amp;color=orange&#34;&gt; &#xA;&lt;p&gt;Refer to the &lt;a href=&#34;https://deep-learning-blogs.vercel.app/blog/mlops-onnx&#34;&gt;Blog Post here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Why do we need model packaging? Models can be built using any machine learning framework available out there (sklearn, tensorflow, pytorch, etc.). We might want to deploy models in different environments like (mobile, web, raspberry pi) or want to run in a different framework (trained in pytorch, inference in tensorflow). A common file format to enable AI developers to use models with a variety of frameworks, tools, runtimes, and compilers will help a lot.&lt;/p&gt; &#xA;&lt;p&gt;This is acheived by a community project &lt;code&gt;ONNX&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The scope of this week is to understand the following topics:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;What is ONNX?&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;How to convert a trained model to ONNX format?&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;What is ONNX Runtime?&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;How to run ONNX converted model in ONNX Runtime?&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Comparisions&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/graviraja/MLOps-Basics/main/images/onnx.jpeg&#34; alt=&#34;ONNX&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Following tech stack is used:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://onnx.ai/&#34;&gt;ONNX&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.onnxruntime.ai/&#34;&gt;ONNXRuntime&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;References&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=7nutT3Aacyw&#34;&gt;Abhishek Thakur tutorial on onnx model conversion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch-lightning.readthedocs.io/en/stable/common/production_inference.html&#34;&gt;Pytorch Lightning documentation on onnx conversion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/microsoftazure/accelerate-your-nlp-pipelines-using-hugging-face-transformers-and-onnx-runtime-2443578f4333&#34;&gt;Huggingface Blog on ONNXRuntime&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tugot17.github.io/data-science-blog/onnx/tutorial/2020/09/21/Exporting-lightning-model-to-onnx.html&#34;&gt;Piotr Blog on onnx conversion&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Week 5: Model Packaging - Docker&lt;/h2&gt; &#xA;&lt;img src=&#34;https://img.shields.io/static/v1.svg?style=for-the-badge&amp;amp;label=difficulty&amp;amp;message=easy&amp;amp;color=green&#34;&gt; &#xA;&lt;p&gt;Refer to the &lt;a href=&#34;https://deep-learning-blogs.vercel.app/blog/mlops-docker&#34;&gt;Blog Post here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Why do we need packaging? We might have to share our application with others, and when they try to run the application most of the time it doesn’t run due to dependencies issues / OS related issues and for that, we say (famous quote across engineers) that &lt;code&gt;It works on my laptop/system&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;So for others to run the applications they have to set up the same environment as it was run on the host side which means a lot of manual configuration and installation of components.&lt;/p&gt; &#xA;&lt;p&gt;The solution to these limitations is a technology called Containers.&lt;/p&gt; &#xA;&lt;p&gt;By containerizing/packaging the application, we can run the application on any cloud platform to get advantages of managed services and autoscaling and reliability, and many more.&lt;/p&gt; &#xA;&lt;p&gt;The most prominent tool to do the packaging of application is Docker 🛳&lt;/p&gt; &#xA;&lt;p&gt;The scope of this week is to understand the following topics:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;FastAPI wrapper&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Basics of Docker&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Building Docker Container&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Docker Compose&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/graviraja/MLOps-Basics/main/images/docker_flow.png&#34; alt=&#34;Docker&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;References&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.analyticsvidhya.com/blog/2021/06/a-hands-on-guide-to-containerized-your-machine-learning-workflow-with-docker/&#34;&gt;Analytics vidhya blog&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Week 6: CI/CD - GitHub Actions&lt;/h2&gt; &#xA;&lt;img src=&#34;https://img.shields.io/static/v1.svg?style=for-the-badge&amp;amp;label=difficulty&amp;amp;message=medium&amp;amp;color=orange&#34;&gt; &#xA;&lt;p&gt;Refer to the &lt;a href=&#34;https://deep-learning-blogs.vercel.app/blog/mlops-github-actions&#34;&gt;Blog Post here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;CI/CD is a coding philosophy and set of practices with which you can continuously build, test, and deploy iterative code changes.&lt;/p&gt; &#xA;&lt;p&gt;This iterative process helps reduce the chance that you develop new code based on a buggy or failed previous versions. With this method, you strive to have less human intervention or even no intervention at all, from the development of new code until its deployment.&lt;/p&gt; &#xA;&lt;p&gt;In this post, I will be going through the following topics:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Basics of GitHub Actions&lt;/li&gt; &#xA; &lt;li&gt;First GitHub Action&lt;/li&gt; &#xA; &lt;li&gt;Creating Google Service Account&lt;/li&gt; &#xA; &lt;li&gt;Giving access to Service account&lt;/li&gt; &#xA; &lt;li&gt;Configuring DVC to use Google Service account&lt;/li&gt; &#xA; &lt;li&gt;Configuring Github Action&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/graviraja/MLOps-Basics/main/images/basic_flow.png&#34; alt=&#34;Docker&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;References&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://dvc.org/doc/user-guide/setup-google-drive-remote&#34;&gt;Configuring service account&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://docs.github.com/en/actions/quickstart&#34;&gt;Github actions&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Week 7: Container Registry - AWS ECR&lt;/h2&gt; &#xA;&lt;img src=&#34;https://img.shields.io/static/v1.svg?style=for-the-badge&amp;amp;label=difficulty&amp;amp;message=medium&amp;amp;color=orange&#34;&gt; &#xA;&lt;p&gt;Refer to the &lt;a href=&#34;https://deep-learning-blogs.vercel.app/blog/mlops-container-registry&#34;&gt;Blog Post here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A container registry is a place to store container images. A container image is a file comprised of multiple layers which can execute applications in a single instance. Hosting all the images in one stored location allows users to commit, identify and pull images when needed.&lt;/p&gt; &#xA;&lt;p&gt;Amazon Simple Storage Service (S3) is a storage for the internet. It is designed for large-capacity, low-cost storage provision across multiple geographical regions.&lt;/p&gt; &#xA;&lt;p&gt;In this week, I will be going through the following topics:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Basics of S3&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Programmatic access to S3&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Configuring AWS S3 as remote storage in DVC&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Basics of ECR&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Configuring GitHub Actions to use S3, ECR&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/graviraja/MLOps-Basics/main/images/ecr_flow.png&#34; alt=&#34;Docker&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Week 8: Serverless Deployment - AWS Lambda&lt;/h2&gt; &#xA;&lt;img src=&#34;https://img.shields.io/static/v1.svg?style=for-the-badge&amp;amp;label=difficulty&amp;amp;message=medium&amp;amp;color=orange&#34;&gt; &#xA;&lt;p&gt;Refer to the &lt;a href=&#34;https://deep-learning-blogs.vercel.app/blog/mlops-serverless&#34;&gt;Blog Post here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A serverless architecture is a way to build and run applications and services without having to manage infrastructure. The application still runs on servers, but all the server management is done by third party service (AWS). We no longer have to provision, scale, and maintain servers to run the applications. By using a serverless architecture, developers can focus on their core product instead of worrying about managing and operating servers or runtimes, either in the cloud or on-premises.&lt;/p&gt; &#xA;&lt;p&gt;In this week, I will be going through the following topics:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Basics of Serverless&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Basics of AWS Lambda&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Triggering Lambda with API Gateway&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Deploying Container using Lambda&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Automating deployment to Lambda using Github Actions&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/graviraja/MLOps-Basics/main/images/lambda_flow.png&#34; alt=&#34;Docker&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Week 9: Prediction Monitoring - Kibana&lt;/h2&gt; &#xA;&lt;img src=&#34;https://img.shields.io/static/v1.svg?style=for-the-badge&amp;amp;label=difficulty&amp;amp;message=medium&amp;amp;color=orange&#34;&gt; &#xA;&lt;p&gt;Refer to the &lt;a href=&#34;https://deep-learning-blogs.vercel.app/blog/mlops-monitoring&#34;&gt;Blog Post here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Monitoring systems can help give us confidence that our systems are running smoothly and, in the event of a system failure, can quickly provide appropriate context when diagnosing the root cause.&lt;/p&gt; &#xA;&lt;p&gt;Things we want to monitor during and training and inference are different. During training we are concered about whether the loss is decreasing or not, whether the model is overfitting, etc.&lt;/p&gt; &#xA;&lt;p&gt;But, during inference, We like to have confidence that our model is making correct predictions.&lt;/p&gt; &#xA;&lt;p&gt;There are many reasons why a model can fail to make useful predictions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;The underlying data distribution has shifted over time and the model has gone stale. i.e inference data characteristics is different from the data characteristics used to train the model.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The inference data stream contains edge cases (not seen during model training). In this scenarios model might perform poorly or can lead to errors.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The model was misconfigured in its production deployment. (Configuration issues are common)&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In all of these scenarios, the model could still make a &lt;code&gt;successful&lt;/code&gt; prediction from a service perspective, but the predictions will likely not be useful. Monitoring machine learning models can help us detect such scenarios and intervene (e.g. trigger a model retraining/deployment pipeline).&lt;/p&gt; &#xA;&lt;p&gt;In this week, I will be going through the following topics:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Basics of Cloudwatch Logs&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Creating Elastic Search Cluster&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Configuring Cloudwatch Logs with Elastic Search&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Creating Index Patterns in Kibana&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Creating Kibana Visualisations&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Creating Kibana Dashboard&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/graviraja/MLOps-Basics/main/images/kibana_flow.png&#34; alt=&#34;Docker&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>huseinzol05/Stock-Prediction-Models</title>
    <updated>2024-09-29T01:44:39Z</updated>
    <id>tag:github.com,2024-09-29:/huseinzol05/Stock-Prediction-Models</id>
    <link href="https://github.com/huseinzol05/Stock-Prediction-Models" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Gathers machine learning and deep learning models for Stock forecasting including trading bots and simulations&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/#readme&#34;&gt; &lt;img alt=&#34;logo&#34; width=&#34;50%&#34; src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output/evolution-strategy.png&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/huseinzol05/Stock-Prediction-Models/raw/master/LICENSE&#34;&gt;&lt;img alt=&#34;MIT License&#34; src=&#34;https://img.shields.io/badge/License-Apache--License--2.0-yellow.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/#&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/deeplearning-30--models-success.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/#&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/agent-23--models-success.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stock-Prediction-Models&lt;/strong&gt;, Gathers machine learning and deep learning models for Stock forecasting, included trading bots and simulations.&lt;/p&gt; &#xA;&lt;h2&gt;Table of contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/#models&#34;&gt;Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/#agents&#34;&gt;Agents&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/realtime-agent&#34;&gt;Realtime Agent&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/#data-explorations&#34;&gt;Data Explorations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/#simulations&#34;&gt;Simulations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/#tensorflow-js&#34;&gt;Tensorflow-js&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/#misc&#34;&gt;Misc&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/#results&#34;&gt;Results&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/#results-agent&#34;&gt;Results Agent&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/#results-signal-prediction&#34;&gt;Results signal prediction&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/#results-analysis&#34;&gt;Results analysis&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/#results-simulation&#34;&gt;Results simulation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;h3&gt;Models&lt;/h3&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/deep-learning&#34;&gt;Deep-learning models&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;LSTM&lt;/li&gt; &#xA; &lt;li&gt;LSTM Bidirectional&lt;/li&gt; &#xA; &lt;li&gt;LSTM 2-Path&lt;/li&gt; &#xA; &lt;li&gt;GRU&lt;/li&gt; &#xA; &lt;li&gt;GRU Bidirectional&lt;/li&gt; &#xA; &lt;li&gt;GRU 2-Path&lt;/li&gt; &#xA; &lt;li&gt;Vanilla&lt;/li&gt; &#xA; &lt;li&gt;Vanilla Bidirectional&lt;/li&gt; &#xA; &lt;li&gt;Vanilla 2-Path&lt;/li&gt; &#xA; &lt;li&gt;LSTM Seq2seq&lt;/li&gt; &#xA; &lt;li&gt;LSTM Bidirectional Seq2seq&lt;/li&gt; &#xA; &lt;li&gt;LSTM Seq2seq VAE&lt;/li&gt; &#xA; &lt;li&gt;GRU Seq2seq&lt;/li&gt; &#xA; &lt;li&gt;GRU Bidirectional Seq2seq&lt;/li&gt; &#xA; &lt;li&gt;GRU Seq2seq VAE&lt;/li&gt; &#xA; &lt;li&gt;Attention-is-all-you-Need&lt;/li&gt; &#xA; &lt;li&gt;CNN-Seq2seq&lt;/li&gt; &#xA; &lt;li&gt;Dilated-CNN-Seq2seq&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;Bonus&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;How to use one of the model to forecast &lt;code&gt;t + N&lt;/code&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/deep-learning/how-to-forecast.ipynb&#34;&gt;how-to-forecast.ipynb&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Consensus, how to use sentiment data to forecast &lt;code&gt;t + N&lt;/code&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/deep-learning/sentiment-consensus.ipynb&#34;&gt;sentiment-consensus.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/stacking&#34;&gt;Stacking models&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Deep Feed-forward Auto-Encoder Neural Network to reduce dimension + Deep Recurrent Neural Network + ARIMA + Extreme Boosting Gradient Regressor&lt;/li&gt; &#xA; &lt;li&gt;Adaboost + Bagging + Extra Trees + Gradient Boosting + Random Forest + XGB&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent&#34;&gt;Agents&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Turtle-trading agent&lt;/li&gt; &#xA; &lt;li&gt;Moving-average agent&lt;/li&gt; &#xA; &lt;li&gt;Signal rolling agent&lt;/li&gt; &#xA; &lt;li&gt;Policy-gradient agent&lt;/li&gt; &#xA; &lt;li&gt;Q-learning agent&lt;/li&gt; &#xA; &lt;li&gt;Evolution-strategy agent&lt;/li&gt; &#xA; &lt;li&gt;Double Q-learning agent&lt;/li&gt; &#xA; &lt;li&gt;Recurrent Q-learning agent&lt;/li&gt; &#xA; &lt;li&gt;Double Recurrent Q-learning agent&lt;/li&gt; &#xA; &lt;li&gt;Duel Q-learning agent&lt;/li&gt; &#xA; &lt;li&gt;Double Duel Q-learning agent&lt;/li&gt; &#xA; &lt;li&gt;Duel Recurrent Q-learning agent&lt;/li&gt; &#xA; &lt;li&gt;Double Duel Recurrent Q-learning agent&lt;/li&gt; &#xA; &lt;li&gt;Actor-critic agent&lt;/li&gt; &#xA; &lt;li&gt;Actor-critic Duel agent&lt;/li&gt; &#xA; &lt;li&gt;Actor-critic Recurrent agent&lt;/li&gt; &#xA; &lt;li&gt;Actor-critic Duel Recurrent agent&lt;/li&gt; &#xA; &lt;li&gt;Curiosity Q-learning agent&lt;/li&gt; &#xA; &lt;li&gt;Recurrent Curiosity Q-learning agent&lt;/li&gt; &#xA; &lt;li&gt;Duel Curiosity Q-learning agent&lt;/li&gt; &#xA; &lt;li&gt;Neuro-evolution agent&lt;/li&gt; &#xA; &lt;li&gt;Neuro-evolution with Novelty search agent&lt;/li&gt; &#xA; &lt;li&gt;ABCD strategy agent&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/misc&#34;&gt;Data Explorations&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;stock market study on TESLA stock, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/misc/tesla-study.ipynb&#34;&gt;tesla-study.ipynb&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Outliers study using K-means, SVM, and Gaussian on TESLA stock, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/misc/outliers.ipynb&#34;&gt;outliers.ipynb&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Overbought-Oversold study on TESLA stock, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/misc/overbought-oversold.ipynb&#34;&gt;overbought-oversold.ipynb&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Which stock you need to buy? &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/misc/which-stock.ipynb&#34;&gt;which-stock.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/simulation&#34;&gt;Simulations&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Simple Monte Carlo, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/simulation/monte-carlo-drift.ipynb&#34;&gt;monte-carlo-drift.ipynb&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Dynamic volatility Monte Carlo, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/simulation/monte-carlo-dynamic-volatility.ipynb&#34;&gt;monte-carlo-dynamic-volatility.ipynb&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Drift Monte Carlo, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/simulation/monte-carlo-drift.ipynb&#34;&gt;monte-carlo-drift.ipynb&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Multivariate Drift Monte Carlo BTC/USDT with Bitcurate sentiment, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/simulation/multivariate-drift-monte-carlo.ipynb&#34;&gt;multivariate-drift-monte-carlo.ipynb&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Portfolio optimization, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/simulation/portfolio-optimization.ipynb&#34;&gt;portfolio-optimization.ipynb&lt;/a&gt;, inspired from &lt;a href=&#34;https://pythonforfinance.net/2017/01/21/investment-portfolio-optimisation-with-python/&#34;&gt;https://pythonforfinance.net/2017/01/21/investment-portfolio-optimisation-with-python/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/stock-forecasting-js&#34;&gt;Tensorflow-js&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;I code &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/deep-learning/1.lstm.ipynb&#34;&gt;LSTM Recurrent Neural Network&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/simple-agent.ipynb&#34;&gt;Simple signal rolling agent&lt;/a&gt; inside Tensorflow JS, you can try it here, &lt;a href=&#34;https://huseinhouse.com/stock-forecasting-js/&#34;&gt;huseinhouse.com/stock-forecasting-js&lt;/a&gt;, you can download any historical CSV and upload dynamically.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/misc&#34;&gt;Misc&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;fashion trending prediction with cross-validation, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/misc/fashion-forecasting.ipynb&#34;&gt;fashion-forecasting.ipynb&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Bitcoin analysis with LSTM prediction, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/misc/bitcoin-analysis-lstm.ipynb&#34;&gt;bitcoin-analysis-lstm.ipynb&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Kijang Emas Bank Negara, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/misc/kijang-emas-bank-negara.ipynb&#34;&gt;kijang-emas-bank-negara.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Results&lt;/h2&gt; &#xA;&lt;h3&gt;Results Agent&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;This agent only able to buy or sell 1 unit per transaction.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Turtle-trading agent, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/1.turtle-agent.ipynb&#34;&gt;turtle-agent.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output-agent/turtle-agent.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Moving-average agent, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/2.moving-average-agent.ipynb&#34;&gt;moving-average-agent.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output-agent/moving-average-agent.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Signal rolling agent, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/3.signal-rolling-agent.ipynb&#34;&gt;signal-rolling-agent.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output-agent/signal-rolling-agent.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Policy-gradient agent, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/4.policy-gradient-agent.ipynb&#34;&gt;policy-gradient-agent.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output-agent/policy-gradient-agent.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;Q-learning agent, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/5.q-learning-agent.ipynb&#34;&gt;q-learning-agent.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output-agent/q-learning-agent.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;6&#34;&gt; &#xA; &lt;li&gt;Evolution-strategy agent, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/6.evolution-strategy-agent.ipynb&#34;&gt;evolution-strategy-agent.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output-agent/evolution-strategy-agent.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;7&#34;&gt; &#xA; &lt;li&gt;Double Q-learning agent, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/7.double-q-learning-agent.ipynb&#34;&gt;double-q-learning-agent.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output-agent/double-q-learning.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;8&#34;&gt; &#xA; &lt;li&gt;Recurrent Q-learning agent, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/8.recurrent-q-learning-agent.ipynb&#34;&gt;recurrent-q-learning-agent.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output-agent/recurrent-q-learning.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;9&#34;&gt; &#xA; &lt;li&gt;Double Recurrent Q-learning agent, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/9.double-recurrent-q-learning-agent.ipynb&#34;&gt;double-recurrent-q-learning-agent.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output-agent/double-recurrent-q-learning.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;10&#34;&gt; &#xA; &lt;li&gt;Duel Q-learning agent, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/10.duel-q-learning-agent.ipynb&#34;&gt;duel-q-learning-agent.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output-agent/double-q-learning.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;11&#34;&gt; &#xA; &lt;li&gt;Double Duel Q-learning agent, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/11.double-duel-q-learning-agent.ipynb&#34;&gt;double-duel-q-learning-agent.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output-agent/double-duel-q-learning.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;12&#34;&gt; &#xA; &lt;li&gt;Duel Recurrent Q-learning agent, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/12.duel-recurrent-q-learning-agent.ipynb&#34;&gt;duel-recurrent-q-learning-agent.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output-agent/duel-recurrent-q-learning.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;13&#34;&gt; &#xA; &lt;li&gt;Double Duel Recurrent Q-learning agent, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/13.double-duel-recurrent-q-learning-agent.ipynb&#34;&gt;double-duel-recurrent-q-learning-agent.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output-agent/double-duel-recurrent-q-learning.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;14&#34;&gt; &#xA; &lt;li&gt;Actor-critic agent, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/14.actor-critic-agent.ipynb&#34;&gt;actor-critic-agent.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output-agent/actor-critic.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;15&#34;&gt; &#xA; &lt;li&gt;Actor-critic Duel agent, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/14.actor-critic-duel-agent.ipynb&#34;&gt;actor-critic-duel-agent.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output-agent/actor-critic-duel.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;16&#34;&gt; &#xA; &lt;li&gt;Actor-critic Recurrent agent, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/16.actor-critic-recurrent-agent.ipynb&#34;&gt;actor-critic-recurrent-agent.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output-agent/actor-critic-recurrent.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;17&#34;&gt; &#xA; &lt;li&gt;Actor-critic Duel Recurrent agent, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/17.actor-critic-duel-recurrent-agent.ipynb&#34;&gt;actor-critic-duel-recurrent-agent.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output-agent/actor-critic-duel-recurrent.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;18&#34;&gt; &#xA; &lt;li&gt;Curiosity Q-learning agent, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/18.curiosity-q-learning-agent.ipynb&#34;&gt;curiosity-q-learning-agent.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output-agent/curiosity-q-learning.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;19&#34;&gt; &#xA; &lt;li&gt;Recurrent Curiosity Q-learning agent, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/19.recurrent-curiosity-q-learning-agent.ipynb&#34;&gt;recurrent-curiosity-q-learning.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output-agent/recurrent-curiosity-q-learning.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;20&#34;&gt; &#xA; &lt;li&gt;Duel Curiosity Q-learning agent, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/20.duel-curiosity-q-learning-agent.ipynb&#34;&gt;duel-curiosity-q-learning-agent.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output-agent/duel-curiosity-q-learning.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;21&#34;&gt; &#xA; &lt;li&gt;Neuro-evolution agent, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/21.neuro-evolution-agent.ipynb&#34;&gt;neuro-evolution.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output-agent/neuro-evolution.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;22&#34;&gt; &#xA; &lt;li&gt;Neuro-evolution with Novelty search agent, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/22.neuro-evolution-novelty-search-agent.ipynb&#34;&gt;neuro-evolution-novelty-search.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output-agent/neuro-evolution-novelty-search.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;23&#34;&gt; &#xA; &lt;li&gt;ABCD strategy agent, &lt;a href=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/agent/23.abcd-strategy-agent.ipynb&#34;&gt;abcd-strategy.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output-agent/abcd-strategy.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;h3&gt;Results signal prediction&lt;/h3&gt; &#xA;&lt;p&gt;I will cut the dataset to train and test datasets,&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Train dataset derived from starting timestamp until last 30 days&lt;/li&gt; &#xA; &lt;li&gt;Test dataset derived from last 30 days until end of the dataset&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;So we will let the model do forecasting based on last 30 days, and we will going to repeat the experiment for 10 times. You can increase it locally if you want, and tuning parameters will help you by a lot.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;LSTM, accuracy 95.693%, time taken for 1 epoch 01:09&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output/lstm.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;LSTM Bidirectional, accuracy 93.8%, time taken for 1 epoch 01:40&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output/bidirectional-lstm.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;LSTM 2-Path, accuracy 94.63%, time taken for 1 epoch 01:39&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output/lstm-2path.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;GRU, accuracy 94.63%, time taken for 1 epoch 02:10&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output/gru.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;GRU Bidirectional, accuracy 92.5673%, time taken for 1 epoch 01:40&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output/bidirectional-gru.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;6&#34;&gt; &#xA; &lt;li&gt;GRU 2-Path, accuracy 93.2117%, time taken for 1 epoch 01:39&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output/gru-2path.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;7&#34;&gt; &#xA; &lt;li&gt;Vanilla, accuracy 91.4686%, time taken for 1 epoch 00:52&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output/vanilla.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;8&#34;&gt; &#xA; &lt;li&gt;Vanilla Bidirectional, accuracy 88.9927%, time taken for 1 epoch 01:06&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output/bidirectional-vanilla.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;9&#34;&gt; &#xA; &lt;li&gt;Vanilla 2-Path, accuracy 91.5406%, time taken for 1 epoch 01:08&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output/vanilla-2path.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;10&#34;&gt; &#xA; &lt;li&gt;LSTM Seq2seq, accuracy 94.9817%, time taken for 1 epoch 01:36&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output/lstm-seq2seq.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;11&#34;&gt; &#xA; &lt;li&gt;LSTM Bidirectional Seq2seq, accuracy 94.517%, time taken for 1 epoch 02:30&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output/bidirectional-lstm-seq2seq.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;12&#34;&gt; &#xA; &lt;li&gt;LSTM Seq2seq VAE, accuracy 95.4190%, time taken for 1 epoch 01:48&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output/lstm-seq2seq-vae.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;13&#34;&gt; &#xA; &lt;li&gt;GRU Seq2seq, accuracy 90.8854%, time taken for 1 epoch 01:34&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output/gru-seq2seq.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;14&#34;&gt; &#xA; &lt;li&gt;GRU Bidirectional Seq2seq, accuracy 67.9915%, time taken for 1 epoch 02:30&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output/bidirectional-gru-seq2seq.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;15&#34;&gt; &#xA; &lt;li&gt;GRU Seq2seq VAE, accuracy 89.1321%, time taken for 1 epoch 01:48&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output/gru-seq2seq-vae.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;16&#34;&gt; &#xA; &lt;li&gt;Attention-is-all-you-Need, accuracy 94.2482%, time taken for 1 epoch 01:41&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output/attention-is-all-you-need.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;17&#34;&gt; &#xA; &lt;li&gt;CNN-Seq2seq, accuracy 90.74%, time taken for 1 epoch 00:43&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output/cnn-seq2seq.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;18&#34;&gt; &#xA; &lt;li&gt;Dilated-CNN-Seq2seq, accuracy 95.86%, time taken for 1 epoch 00:14&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output/dilated-cnn-seq2seq.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;p&gt;&lt;strong&gt;Bonus&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;How to forecast,&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output/how-to-forecast.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Sentiment consensus,&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output/sentiment-consensus.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;h3&gt;Results analysis&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Outliers study using K-means, SVM, and Gaussian on TESLA stock&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/misc/outliers.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Overbought-Oversold study on TESLA stock&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/misc/overbought-oversold.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Which stock you need to buy?&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/misc/which-stock.png&#34; width=&#34;40%&#34; align=&#34;&#34;&gt; &#xA;&lt;h3&gt;Results simulation&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Simple Monte Carlo&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/simulation/monte-carlo-simple.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Dynamic volatity Monte Carlo&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/simulation/monte-carlo-dynamic-volatility.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Drift Monte Carlo&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/simulation/monte-carlo-drift.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Multivariate Drift Monte Carlo BTC/USDT with Bitcurate sentiment&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/simulation/multivariate-drift-monte-carlo.png&#34; width=&#34;70%&#34; align=&#34;&#34;&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;Portfolio optimization&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/simulation/portfolio-optimization.png&#34; width=&#34;40%&#34; align=&#34;&#34;&gt;</summary>
  </entry>
  <entry>
    <title>lyogavin/airllm</title>
    <updated>2024-09-29T01:44:39Z</updated>
    <id>tag:github.com,2024-09-29:/lyogavin/airllm</id>
    <link href="https://github.com/lyogavin/airllm" rel="alternate"></link>
    <summary type="html">&lt;p&gt;AirLLM 70B inference with single 4GB GPU&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://github.com/lyogavin/airllm/raw/main/assets/airllm_logo_sm.png?v=3&amp;amp;raw=true&#34; alt=&#34;airllm_logo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lyogavin/airllm/main/#quickstart&#34;&gt;&lt;strong&gt;Quickstart&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/lyogavin/airllm/main/#configurations&#34;&gt;&lt;strong&gt;Configurations&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/lyogavin/airllm/main/#macos&#34;&gt;&lt;strong&gt;MacOS&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/lyogavin/airllm/main/#example-python-notebook&#34;&gt;&lt;strong&gt;Example notebooks&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/lyogavin/airllm/main/#faq&#34;&gt;&lt;strong&gt;FAQ&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AirLLM&lt;/strong&gt; optimizes inference memory usage, allowing 70B large language models to run inference on a single 4GB GPU card without quantization, distillation and pruning. And you can run &lt;strong&gt;405B Llama3.1&lt;/strong&gt; on &lt;strong&gt;8GB vram&lt;/strong&gt; now.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/lyogavin/airllm/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/lyogavin/airllm?style=social&#34; alt=&#34;GitHub Repo stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/airllm&#34;&gt;&lt;img src=&#34;https://static.pepy.tech/personalized-badge/airllm?period=total&amp;amp;units=international_system&amp;amp;left_color=grey&amp;amp;right_color=blue&amp;amp;left_text=downloads&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/LianjiaTech/BELLE/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg?sanitize=true&#34; alt=&#34;Code License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://static.aicompose.cn/static/wecom_barcode.png?t=1671918938&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/wechat-Anima-brightgreen?logo=wechat&#34; alt=&#34;Generic badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/2xffU5sn&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1175437549783760896?logo=discord&amp;amp;color=7289da&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/airllm/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/format/airllm?logo=pypi&amp;amp;color=3571a3&#34; alt=&#34;PyPI - AirLLM&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://medium.com/@lyo.gavin&#34;&gt;&lt;img src=&#34;https://img.shields.io/website?up_message=blog&amp;amp;url=https%3A%2F%2Fmedium.com%2F%40lyo.gavin&amp;amp;logo=medium&amp;amp;color=black&#34; alt=&#34;Website&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gavinliblog.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Gavin_Li-Blog-blue&#34; alt=&#34;Website&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://patreon.com/gavinli&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https%3A%2F%2Fshieldsio-patreon.vercel.app%2Fapi%3Fusername%3Dgavinli%26type%3Dpatrons&amp;amp;style=flat&#34; alt=&#34;Support me on Patreon&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/sponsors/lyogavin&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/sponsors/lyogavin?logo=GitHub&amp;amp;color=lightgray&#34; alt=&#34;GitHub Sponsors&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Updates&lt;/h2&gt; &#xA;&lt;p&gt;[2024/08/20] v2.11.0: Support Qwen2.5&lt;/p&gt; &#xA;&lt;p&gt;[2024/08/18] v2.10.1 Support CPU inference. Support non sharded models. Thanks @NavodPeiris for the great work!&lt;/p&gt; &#xA;&lt;p&gt;[2024/07/30] Support Llama3.1 &lt;strong&gt;405B&lt;/strong&gt; (&lt;a href=&#34;https://colab.research.google.com/github/lyogavin/airllm/blob/main/air_llm/examples/run_llama3.1_405B.ipynb&#34;&gt;example notebook&lt;/a&gt;). Support &lt;strong&gt;8bit/4bit quantization&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;[2024/04/20] AirLLM supports Llama3 natively already. Run Llama3 70B on 4GB single GPU.&lt;/p&gt; &#xA;&lt;p&gt;[2023/12/25] v2.8.2: Support MacOS running 70B large language models.&lt;/p&gt; &#xA;&lt;p&gt;[2023/12/20] v2.7: Support AirLLMMixtral.&lt;/p&gt; &#xA;&lt;p&gt;[2023/12/20] v2.6: Added AutoModel, automatically detect model type, no need to provide model class to initialize model.&lt;/p&gt; &#xA;&lt;p&gt;[2023/12/18] v2.5: added prefetching to overlap the model loading and compute. 10% speed improvement.&lt;/p&gt; &#xA;&lt;p&gt;[2023/12/03] added support of &lt;strong&gt;ChatGLM&lt;/strong&gt;, &lt;strong&gt;QWen&lt;/strong&gt;, &lt;strong&gt;Baichuan&lt;/strong&gt;, &lt;strong&gt;Mistral&lt;/strong&gt;, &lt;strong&gt;InternLM&lt;/strong&gt;!&lt;/p&gt; &#xA;&lt;p&gt;[2023/12/02] added support for safetensors. Now support all top 10 models in open llm leaderboard.&lt;/p&gt; &#xA;&lt;p&gt;[2023/12/01] airllm 2.0. Support compressions: &lt;strong&gt;3x run time speed up!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;[2023/11/20] airllm Initial verion!&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#lyogavin/airllm&amp;amp;Timeline&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=lyogavin/airllm&amp;amp;type=Timeline&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lyogavin/airllm/main/#quickstart&#34;&gt;Quick start&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lyogavin/airllm/main/#model-compression---3x-inference-speed-up&#34;&gt;Model Compression&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lyogavin/airllm/main/#configurations&#34;&gt;Configurations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lyogavin/airllm/main/#macos&#34;&gt;Run on MacOS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lyogavin/airllm/main/#example-python-notebook&#34;&gt;Example notebooks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lyogavin/airllm/main/#supported-models&#34;&gt;Supported Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lyogavin/airllm/main/#acknowledgement&#34;&gt;Acknowledgement&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lyogavin/airllm/main/#faq&#34;&gt;FAQ&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;h3&gt;1. Install package&lt;/h3&gt; &#xA;&lt;p&gt;First, install the airllm pip package.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install airllm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. Inference&lt;/h3&gt; &#xA;&lt;p&gt;Then, initialize AirLLMLlama2, pass in the huggingface repo ID of the model being used, or the local path, and inference can be performed similar to a regular transformer model.&lt;/p&gt; &#xA;&lt;p&gt;(&lt;em&gt;You can also specify the path to save the splitted layered model through &lt;strong&gt;layer_shards_saving_path&lt;/strong&gt; when init AirLLMLlama2.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from airllm import AutoModel&#xA;&#xA;MAX_LENGTH = 128&#xA;# could use hugging face model repo id:&#xA;model = AutoModel.from_pretrained(&#34;garage-bAInd/Platypus2-70B-instruct&#34;)&#xA;&#xA;# or use model&#39;s local path...&#xA;#model = AutoModel.from_pretrained(&#34;/home/ubuntu/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/b585e74bcaae02e52665d9ac6d23f4d0dbc81a0f&#34;)&#xA;&#xA;input_text = [&#xA;        &#39;What is the capital of United States?&#39;,&#xA;        #&#39;I like&#39;,&#xA;    ]&#xA;&#xA;input_tokens = model.tokenizer(input_text,&#xA;    return_tensors=&#34;pt&#34;, &#xA;    return_attention_mask=False, &#xA;    truncation=True, &#xA;    max_length=MAX_LENGTH, &#xA;    padding=False)&#xA;           &#xA;generation_output = model.generate(&#xA;    input_tokens[&#39;input_ids&#39;].cuda(), &#xA;    max_new_tokens=20,&#xA;    use_cache=True,&#xA;    return_dict_in_generate=True)&#xA;&#xA;output = model.tokenizer.decode(generation_output.sequences[0])&#xA;&#xA;print(output)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: During inference, the original model will first be decomposed and saved layer-wise. Please ensure there is sufficient disk space in the huggingface cache directory.&lt;/p&gt; &#xA;&lt;h2&gt;Model Compression - 3x Inference Speed Up!&lt;/h2&gt; &#xA;&lt;p&gt;We just added model compression based on block-wise quantization-based model compression. Which can further &lt;strong&gt;speed up the inference speed&lt;/strong&gt; for up to &lt;strong&gt;3x&lt;/strong&gt; , with &lt;strong&gt;almost ignorable accuracy loss!&lt;/strong&gt; (see more performance evaluation and why we use block-wise quantization in &lt;a href=&#34;https://arxiv.org/abs/2212.09720&#34;&gt;this paper&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lyogavin/airllm/raw/main/assets/airllm2_time_improvement.png?v=2&amp;amp;raw=true&#34; alt=&#34;speed_improvement&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;How to enable model compression speed up:&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Step 1. make sure you have &lt;a href=&#34;https://github.com/TimDettmers/bitsandbytes&#34;&gt;bitsandbytes&lt;/a&gt; installed by &lt;code&gt;pip install -U bitsandbytes &lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Step 2. make sure airllm verion later than 2.0.0: &lt;code&gt;pip install -U airllm&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Step 3. when initialize the model, passing the argument compression (&#39;4bit&#39; or &#39;8bit&#39;):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = AutoModel.from_pretrained(&#34;garage-bAInd/Platypus2-70B-instruct&#34;,&#xA;                     compression=&#39;4bit&#39; # specify &#39;8bit&#39; for 8-bit block-wise quantization &#xA;                    )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;What are the differences between model compression and quantization?&lt;/h4&gt; &#xA;&lt;p&gt;Quantization normally needs to quantize both weights and activations to really speed things up. Which makes it harder to maintain accuracy and avoid the impact of outliers in all kinds of inputs.&lt;/p&gt; &#xA;&lt;p&gt;While in our case the bottleneck is mainly at the disk loading, we only need to make the model loading size smaller. So, we get to only quantize the weights&#39; part, which is easier to ensure the accuracy.&lt;/p&gt; &#xA;&lt;h2&gt;Configurations&lt;/h2&gt; &#xA;&lt;p&gt;When initialize the model, we support the following configurations:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;compression&lt;/strong&gt;: supported options: 4bit, 8bit for 4-bit or 8-bit block-wise quantization, or by default None for no compression&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;profiling_mode&lt;/strong&gt;: supported options: True to output time consumptions or by default False&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;layer_shards_saving_path&lt;/strong&gt;: optionally another path to save the splitted model&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;hf_token&lt;/strong&gt;: huggingface token can be provided here if downloading gated models like: &lt;em&gt;meta-llama/Llama-2-7b-hf&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;prefetching&lt;/strong&gt;: prefetching to overlap the model loading and compute. By default, turned on. For now, only AirLLMLlama2 supports this.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;delete_original&lt;/strong&gt;: if you don&#39;t have too much disk space, you can set delete_original to true to delete the original downloaded hugging face model, only keep the transformed one to save half of the disk space.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;MacOS&lt;/h2&gt; &#xA;&lt;p&gt;Just install airllm and run the code the same as on linux. See more in &lt;a href=&#34;https://raw.githubusercontent.com/lyogavin/airllm/main/#quickstart&#34;&gt;Quick Start&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;make sure you installed &lt;a href=&#34;https://github.com/ml-explore/mlx?tab=readme-ov-file#installation&#34;&gt;mlx&lt;/a&gt; and torch&lt;/li&gt; &#xA; &lt;li&gt;you probably need to install python native see more &lt;a href=&#34;https://stackoverflow.com/a/65432861/21230266&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;only &lt;a href=&#34;https://support.apple.com/en-us/HT211814&#34;&gt;Apple silicon&lt;/a&gt; is supported&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Example [python notebook] (&lt;a href=&#34;https://github.com/lyogavin/airllm/raw/main/air_llm/examples/run_on_macos.ipynb&#34;&gt;https://github.com/lyogavin/airllm/blob/main/air_llm/examples/run_on_macos.ipynb&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;Example Python Notebook&lt;/h2&gt; &#xA;&lt;p&gt;Example colabs here:&lt;/p&gt; &#xA;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/lyogavin/airllm/blob/main/air_llm/examples/run_all_types_of_models.ipynb&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt; &lt;/a&gt; &#xA;&lt;h4&gt;example of other models (ChatGLM, QWen, Baichuan, Mistral, etc):&lt;/h4&gt; &#xA;&lt;details&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;ChatGLM:&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from airllm import AutoModel&#xA;MAX_LENGTH = 128&#xA;model = AutoModel.from_pretrained(&#34;THUDM/chatglm3-6b-base&#34;)&#xA;input_text = [&#39;What is the capital of China?&#39;,]&#xA;input_tokens = model.tokenizer(input_text,&#xA;    return_tensors=&#34;pt&#34;, &#xA;    return_attention_mask=False, &#xA;    truncation=True, &#xA;    max_length=MAX_LENGTH, &#xA;    padding=True)&#xA;generation_output = model.generate(&#xA;    input_tokens[&#39;input_ids&#39;].cuda(), &#xA;    max_new_tokens=5,&#xA;    use_cache= True,&#xA;    return_dict_in_generate=True)&#xA;model.tokenizer.decode(generation_output.sequences[0])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;QWen:&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from airllm import AutoModel&#xA;MAX_LENGTH = 128&#xA;model = AutoModel.from_pretrained(&#34;Qwen/Qwen-7B&#34;)&#xA;input_text = [&#39;What is the capital of China?&#39;,]&#xA;input_tokens = model.tokenizer(input_text,&#xA;    return_tensors=&#34;pt&#34;, &#xA;    return_attention_mask=False, &#xA;    truncation=True, &#xA;    max_length=MAX_LENGTH)&#xA;generation_output = model.generate(&#xA;    input_tokens[&#39;input_ids&#39;].cuda(), &#xA;    max_new_tokens=5,&#xA;    use_cache=True,&#xA;    return_dict_in_generate=True)&#xA;model.tokenizer.decode(generation_output.sequences[0])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Baichuan, InternLM, Mistral, etc:&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from airllm import AutoModel&#xA;MAX_LENGTH = 128&#xA;model = AutoModel.from_pretrained(&#34;baichuan-inc/Baichuan2-7B-Base&#34;)&#xA;#model = AutoModel.from_pretrained(&#34;internlm/internlm-20b&#34;)&#xA;#model = AutoModel.from_pretrained(&#34;mistralai/Mistral-7B-Instruct-v0.1&#34;)&#xA;input_text = [&#39;What is the capital of China?&#39;,]&#xA;input_tokens = model.tokenizer(input_text,&#xA;    return_tensors=&#34;pt&#34;, &#xA;    return_attention_mask=False, &#xA;    truncation=True, &#xA;    max_length=MAX_LENGTH)&#xA;generation_output = model.generate(&#xA;    input_tokens[&#39;input_ids&#39;].cuda(), &#xA;    max_new_tokens=5,&#xA;    use_cache=True,&#xA;    return_dict_in_generate=True)&#xA;model.tokenizer.decode(generation_output.sequences[0])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;To request other model support: &lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLSe0Io9ANMT964Zi-OQOq1TJmnvP-G3_ZgQDhP7SatN0IEdbOg/viewform?usp=sf_link&#34;&gt;here&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;A lot of the code are based on SimJeg&#39;s great work in the Kaggle exam competition. Big shoutout to SimJeg:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/SimJeg&#34;&gt;GitHub account @SimJeg&lt;/a&gt;, &lt;a href=&#34;https://www.kaggle.com/code/simjeg/platypus2-70b-with-wikipedia-rag&#34;&gt;the code on Kaggle&lt;/a&gt;, &lt;a href=&#34;https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/446414&#34;&gt;the associated discussion&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;h3&gt;1. MetadataIncompleteBuffer&lt;/h3&gt; &#xA;&lt;p&gt;safetensors_rust.SafetensorError: Error while deserializing header: MetadataIncompleteBuffer&lt;/p&gt; &#xA;&lt;p&gt;If you run into this error, most possible cause is you run out of disk space. The process of splitting model is very disk-consuming. See &lt;a href=&#34;https://huggingface.co/TheBloke/guanaco-65B-GPTQ/discussions/12&#34;&gt;this&lt;/a&gt;. You may need to extend your disk space, clear huggingface &lt;a href=&#34;https://huggingface.co/docs/datasets/cache&#34;&gt;.cache&lt;/a&gt; and rerun.&lt;/p&gt; &#xA;&lt;h3&gt;2. ValueError: max() arg is an empty sequence&lt;/h3&gt; &#xA;&lt;p&gt;Most likely you are loading QWen or ChatGLM model with Llama2 class. Try the following:&lt;/p&gt; &#xA;&lt;p&gt;For QWen model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from airllm import AutoModel #&amp;lt;----- instead of AirLLMLlama2&#xA;AutoModel.from_pretrained(...)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For ChatGLM model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from airllm import AutoModel #&amp;lt;----- instead of AirLLMLlama2&#xA;AutoModel.from_pretrained(...)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;3. 401 Client Error....Repo model ... is gated.&lt;/h3&gt; &#xA;&lt;p&gt;Some models are gated models, needs huggingface api token. You can provide hf_token:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = AutoModel.from_pretrained(&#34;meta-llama/Llama-2-7b-hf&#34;, #hf_token=&#39;HF_API_TOKEN&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;4. ValueError: Asking to pad but the tokenizer does not have a padding token.&lt;/h3&gt; &#xA;&lt;p&gt;Some model&#39;s tokenizer doesn&#39;t have padding token, so you can set a padding token or simply turn the padding config off:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;input_tokens = model.tokenizer(input_text,&#xA;   return_tensors=&#34;pt&#34;, &#xA;   return_attention_mask=False, &#xA;   truncation=True, &#xA;   max_length=MAX_LENGTH, &#xA;   padding=False  #&amp;lt;-----------   turn off padding &#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citing AirLLM&lt;/h2&gt; &#xA;&lt;p&gt;If you find AirLLM useful in your research and wish to cite it, please use the following BibTex entry:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@software{airllm2023,&#xA;  author = {Gavin Li},&#xA;  title = {AirLLM: scaling large language models on low-end commodity computers},&#xA;  url = {https://github.com/lyogavin/airllm/},&#xA;  version = {0.0},&#xA;  year = {2023},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contribution&lt;/h2&gt; &#xA;&lt;p&gt;Welcomed contributions, ideas and discussions!&lt;/p&gt; &#xA;&lt;p&gt;If you find it useful, please ⭐ or buy me a coffee! 🙏&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://bmc.link/lyogavinQ&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png&#34; alt=&#34;&amp;quot;Buy Me A Coffee&amp;quot;&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>