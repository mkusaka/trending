<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-06-09T01:41:52Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>AI4Finance-Foundation/FinRobot</title>
    <updated>2024-06-09T01:41:52Z</updated>
    <id>tag:github.com,2024-06-09:/AI4Finance-Foundation/FinRobot</id>
    <link href="https://github.com/AI4Finance-Foundation/FinRobot" rel="alternate"></link>
    <summary type="html">&lt;p&gt;FinRobot: An Open-Source AI Agent Platform for Financial Applications using LLMs ðŸš€ ðŸš€ ðŸš€&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img align=&#34;center&#34; width=&#34;30%&#34; alt=&#34;image&#34; src=&#34;https://github.com/AI4Finance-Foundation/FinGPT/assets/31713746/e0371951-1ce1-488e-aa25-0992dafcc139&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;FinRobot: An Open-Source AI Agent Platform for Financial Applications using Large Language Models&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AI4Finance-Foundation/FinRobot/master/%5Bhttps://pepy.tech/project/finrobot%5D(https://pepy.tech/project/finrobot)&#34;&gt;&lt;img src=&#34;https://static.pepy.tech/badge/finrobot&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/finrobot&#34;&gt;&lt;img src=&#34;https://static.pepy.tech/badge/finrobot/week&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.python.org/downloads/release/python-360/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.6-blue.svg?sanitize=true&#34; alt=&#34;Python 3.8&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/finrobot/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/finrobot.svg?sanitize=true&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/license/AI4Finance-Foundation/finrobot.svg?color=brightgreen&#34; alt=&#34;License&#34;&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img align=&#34;center&#34; src=&#34;https://raw.githubusercontent.com/AI4Finance-Foundation/FinRobot/master/figs/logo_white_background.jpg&#34; width=&#34;40%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;FinRobot&lt;/strong&gt; is an AI Agent Platform that transcends the scope of FinGPT, representing a comprehensive solution meticulously designed for financial applications. It integrates &lt;strong&gt;a diverse array of AI technologies&lt;/strong&gt;, extending beyond mere language models. This expansive vision highlights the platform&#39;s versatility and adaptability, addressing the multifaceted needs of the financial industry.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Concept of AI Agent&lt;/strong&gt;: an AI Agent is an intelligent entity that uses large language models as its brain to perceive its environment, make decisions, and execute actions. Unlike traditional artificial intelligence, AI Agents possess the ability to independently think and utilize tools to progressively achieve given objectives.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2405.14767&#34;&gt;Whitepaper of FinRobot&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/trsr8SXpW5&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/trsr8SXpW5&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;FinRobot Ecosystem&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img align=&#34;center&#34; src=&#34;https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/6b30d9c1-35e5-4d36-a138-7e2769718f62&#34; width=&#34;90%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;The overall framework of FinRobot is organized into four distinct layers, each designed to address specific aspects of financial AI processing and application:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Financial AI Agents Layer&lt;/strong&gt;: The Financial AI Agents Layer now includes Financial Chain-of-Thought (CoT) prompting, enhancing complex analysis and decision-making capacity. Market Forecasting Agents, Document Analysis Agents, and Trading Strategies Agents utilize CoT to dissect financial challenges into logical steps, aligning their advanced algorithms and domain expertise with the evolving dynamics of financial markets for precise, actionable insights.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Financial LLMs Algorithms Layer&lt;/strong&gt;: The Financial LLMs Algorithms Layer configures and utilizes specially tuned models tailored to specific domains and global market analysis.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLMOps and DataOps Layers&lt;/strong&gt;: The LLMOps layer implements a multi-source integration strategy that selects the most suitable LLMs for specific financial tasks, utilizing a range of state-of-the-art models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-source LLM Foundation Models Layer&lt;/strong&gt;: This foundational layer supports the plug-and-play functionality of various general and specialized LLMs.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;FinRobot: Agent Workflow&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img align=&#34;center&#34; src=&#34;https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/ff8033be-2326-424a-ac11-17e2c9c4983d&#34; width=&#34;60%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Perception&lt;/strong&gt;: This module captures and interprets multimodal financial data from market feeds, news, and economic indicators, using sophisticated techniques to structure the data for thorough analysis.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Brain&lt;/strong&gt;: Acting as the core processing unit, this module perceives data from the Perception module with LLMs and utilizes Financial Chain-of-Thought (CoT) processes to generate structured instructions.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Action&lt;/strong&gt;: This module executes instructions from the Brain module, applying tools to translate analytical insights into actionable outcomes. Actions include trading, portfolio adjustments, generating reports, or sending alerts, thereby actively influencing the financial environment.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;FinRobot: Smart Scheduler&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img align=&#34;center&#34; src=&#34;https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/06fa0b78-ac53-48d3-8a6e-98d15386327e&#34; width=&#34;60%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;The Smart Scheduler is central to ensuring model diversity and optimizing the integration and selection of the most appropriate LLM for each task.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Director Agent&lt;/strong&gt;: This component orchestrates the task assignment process, ensuring that tasks are allocated to agents based on their performance metrics and suitability for specific tasks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Agent Registration&lt;/strong&gt;: Manages the registration and tracks the availability of agents within the system, facilitating an efficient task allocation process.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Agent Adaptor&lt;/strong&gt;: Tailor agent functionalities to specific tasks, enhancing their performance and integration within the overall system.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Task Manager&lt;/strong&gt;: Manages and stores different general and fine-tuned LLMs-based agents tailored for various financial tasks, updated periodically to ensure relevance and efficacy.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;File Structure&lt;/h2&gt; &#xA;&lt;p&gt;The main folder &lt;strong&gt;finrobot&lt;/strong&gt; has three subfolders &lt;strong&gt;agents, data_source, functional&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;FinRobot&#xA;â”œâ”€â”€ finrobot (main folder)&#xA;â”‚   â”œâ”€â”€ agents&#xA;â”‚   &#x9;â”œâ”€â”€ agent_library.py&#xA;â”‚   &#x9;â””â”€â”€ workflow.py&#xA;â”‚   â”œâ”€â”€ data_source&#xA;â”‚   &#x9;â”œâ”€â”€ finnhub_utils.py&#xA;â”‚   &#x9;â”œâ”€â”€ finnlp_utils.py&#xA;â”‚   &#x9;â”œâ”€â”€ fmp_utils.py&#xA;â”‚   &#x9;â”œâ”€â”€ sec_utils.py&#xA;â”‚   &#x9;â””â”€â”€ yfinance_utils.py&#xA;â”‚   â”œâ”€â”€ functional&#xA;â”‚   &#x9;â”œâ”€â”€ analyzer.py&#xA;â”‚   &#x9;â”œâ”€â”€ charting.py&#xA;â”‚   &#x9;â”œâ”€â”€ coding.py&#xA;â”‚   &#x9;â”œâ”€â”€ quantitative.py&#xA;â”‚   &#x9;â”œâ”€â”€ reportlab.py&#xA;â”‚   &#x9;â””â”€â”€ text.py&#xA;â”‚   â”œâ”€â”€ toolkits.py&#xA;â”‚   â””â”€â”€ utils.py&#xA;â”‚&#xA;â”œâ”€â”€ configs&#xA;â”œâ”€â”€ experiments&#xA;â”œâ”€â”€ tutorials_beginner (hands-on tutorial)&#xA;â”‚   â”œâ”€â”€ agent_fingpt_forecaster.ipynb&#xA;â”‚   â””â”€â”€ agent_annual_report.ipynb &#xA;â”œâ”€â”€ tutorials_advanced (advanced tutorials for potential finrobot developers)&#xA;â”‚   â”œâ”€â”€ agent_trade_strategist.ipynb&#xA;â”‚   â”œâ”€â”€ agent_fingpt_forecaster.ipynb&#xA;â”‚   â”œâ”€â”€ agent_annual_report.ipynb &#xA;â”‚   â”œâ”€â”€ lmm_agent_mplfinance.ipynb&#xA;â”‚   â””â”€â”€ lmm_agent_opt_smacross.ipynb&#xA;â”œâ”€â”€ setup.py&#xA;â”œâ”€â”€ OAI_CONFIG_LIST_sample&#xA;â”œâ”€â”€ config_api_keys_sample&#xA;â”œâ”€â”€ requirements.txt&#xA;â””â”€â”€ README.md&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Installation:&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;1. (Recommended) Create a new virtual environment&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda create --name finrobot python=3.10&#xA;conda activate finrobot&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;2. download the FinRobot repo use terminal or download it manually&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/AI4Finance-Foundation/FinRobot.git&#xA;cd FinRobot&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;3. install finrobot &amp;amp; dependencies from source or pypi&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;get our latest release from pypi&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -U finrobot&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or install from this repo directly&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;4. modify OAI_CONFIG_LIST_sample file&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;1) rename OAI_CONFIG_LIST_sample to OAI_CONFIG_LIST&#xA;2) remove the four lines of comment within the OAI_CONFIG_LIST file&#xA;3) add your own openai api-key &amp;lt;your OpenAI API key here&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;5. modify config_api_keys_sample file&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;1) rename config_api_keys_sample to config_api_keys&#xA;2) remove the comment within the config_api_keys file&#xA;3) add your own finnhub-api &#34;YOUR_FINNHUB_API_KEY&#34;&#xA;4) add your own financialmodelingprep and sec-api keys &#34;YOUR_FMP_API_KEY&#34; and &#34;YOUR_SEC_API_KEY&#34; (for financial report generation)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;6. start navigating the tutorials or the demos below:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# find these notebooks in tutorials&#xA;1) agent_annual_report.ipynb&#xA;2) agent_fingpt_forecaster.ipynb&#xA;3) agent_trade_strategist.ipynb&#xA;4) lmm_agent_mplfinance.ipynb&#xA;5) lmm_agent_opt_smacross.ipynb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Demos&lt;/h2&gt; &#xA;&lt;h3&gt;1. Market Forecaster Agent (Predict Stock Movements Direction)&lt;/h3&gt; &#xA;&lt;p&gt;Takes a company&#39;s ticker symbol, recent basic financials, and market news as input and predicts its stock movements.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Import&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import autogen&#xA;from finrobot.utils import get_current_date, register_keys_from_json&#xA;from finrobot.agents.workflow import SingleAssistant&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Config&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Read OpenAI API keys from a JSON file&#xA;llm_config = {&#xA;    &#34;config_list&#34;: autogen.config_list_from_json(&#xA;        &#34;../OAI_CONFIG_LIST&#34;,&#xA;        filter_dict={&#34;model&#34;: [&#34;gpt-4-0125-preview&#34;]},&#xA;    ),&#xA;    &#34;timeout&#34;: 120,&#xA;    &#34;temperature&#34;: 0,&#xA;}&#xA;&#xA;# Register FINNHUB API keys&#xA;register_keys_from_json(&#34;../config_api_keys&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Run&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;company = &#34;NVDA&#34;&#xA;&#xA;assitant = SingleAssistant(&#xA;    &#34;Market_Analyst&#34;,&#xA;    llm_config,&#xA;    # set to &#34;ALWAYS&#34; if you want to chat instead of simply receiving the prediciton&#xA;    human_input_mode=&#34;NEVER&#34;,&#xA;)&#xA;assitant.chat(&#xA;    f&#34;Use all the tools provided to retrieve information available for {company} upon {get_current_date()}. Analyze the positive developments and potential concerns of {company} &#34;&#xA;    &#34;with 2-4 most important factors respectively and keep them concise. Most factors should be inferred from company related news. &#34;&#xA;    f&#34;Then make a rough prediction (e.g. up/down by 2-3%) of the {company} stock price movement for next week. Provide a summary analysis to support your prediction.&#34;&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Result&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img align=&#34;center&#34; src=&#34;https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/812ec23a-9cb3-4fad-b716-78533ddcd9dc&#34; width=&#34;40%&#34;&gt; &#xA; &lt;img align=&#34;center&#34; src=&#34;https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/9a2f9f48-b0e1-489c-8679-9a4c530f313c&#34; width=&#34;41%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;2. Financial Analyst Agent for Report Writing (Equity Research Report)&lt;/h3&gt; &#xA;&lt;p&gt;Take a company&#39;s 10-k form, financial data, and market data as input and output an equity research report&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Import&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os&#xA;import autogen&#xA;from textwrap import dedent&#xA;from finrobot.utils import register_keys_from_json&#xA;from finrobot.agents.workflow import SingleAssistantShadow&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Config&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;llm_config = {&#xA;    &#34;config_list&#34;: autogen.config_list_from_json(&#xA;        &#34;../OAI_CONFIG_LIST&#34;,&#xA;        filter_dict={&#xA;            &#34;model&#34;: [&#34;gpt-4-0125-preview&#34;],&#xA;        },&#xA;    ),&#xA;    &#34;timeout&#34;: 120,&#xA;    &#34;temperature&#34;: 0.5,&#xA;}&#xA;register_keys_from_json(&#34;../config_api_keys&#34;)&#xA;&#xA;# Intermediate strategy modules will be saved in this directory&#xA;work_dir = &#34;../report&#34;&#xA;os.makedirs(work_dir, exist_ok=True)&#xA;&#xA;assistant = SingleAssistantShadow(&#xA;    &#34;Expert_Investor&#34;,&#xA;    llm_config,&#xA;    max_consecutive_auto_reply=None,&#xA;    human_input_mode=&#34;TERMINATE&#34;,&#xA;)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Run&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;company = &#34;Microsoft&#34;&#xA;fyear = &#34;2023&#34;&#xA;&#xA;message = dedent(&#xA;    f&#34;&#34;&#34;&#xA;    With the tools you&#39;ve been provided, write an annual report based on {company}&#39;s {fyear} 10-k report, format it into a pdf.&#xA;    Pay attention to the followings:&#xA;    - Explicitly explain your working plan before you kick off.&#xA;    - Use tools one by one for clarity, especially when asking for instructions. &#xA;    - All your file operations should be done in &#34;{work_dir}&#34;. &#xA;    - Display any image in the chat once generated.&#xA;    - All the paragraphs should combine between 400 and 450 words, don&#39;t generate the pdf until this is explicitly fulfilled.&#xA;&#34;&#34;&#34;&#xA;)&#xA;&#xA;assistant.chat(message, use_cache=True, max_turns=50,&#xA;               summary_method=&#34;last_msg&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Result&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img align=&#34;center&#34; src=&#34;https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/d2d999e0-dc0e-4196-aca1-218f5fadcc5b&#34; width=&#34;60%&#34;&gt; &#xA; &lt;img align=&#34;center&#34; src=&#34;https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/3a21873f-9498-4d73-896b-3740bf6d116d&#34; width=&#34;60%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;Financial CoT&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Gather Preliminary Data&lt;/strong&gt;: 10-K report, market data, financial ratios&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Analyze Financial Statements&lt;/strong&gt;: balance sheet, income statement, cash flow&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Company Overview and Performance&lt;/strong&gt;: company description, business highlights, segment analysis&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Risk Assessment&lt;/strong&gt;: assess risks&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Financial Performance Visualization&lt;/strong&gt;: plot PE ratio and EPS&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Synthesize Findings into Paragraphs&lt;/strong&gt;: combine all parts into a coherent summary&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Generate PDF Report&lt;/strong&gt;: use tools to generate PDF automatically&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Quality Assurance&lt;/strong&gt;: check word counts&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;3. Trade Strategist Agent with multimodal capabilities&lt;/h3&gt; &#xA;&lt;h2&gt;AI Agent Papers&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[Stanford University + Microsoft Research] &lt;a href=&#34;https://arxiv.org/abs/2401.03568&#34;&gt;Agent AI: Surveying the Horizons of Multimodal Interaction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Stanford University] &lt;a href=&#34;https://arxiv.org/abs/2304.03442&#34;&gt;Generative Agents: Interactive Simulacra of Human Behavior&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Fudan NLP Group] &lt;a href=&#34;https://arxiv.org/abs/2309.07864&#34;&gt;The Rise and Potential of Large Language Model Based Agents: A Survey&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Fudan NLP Group] &lt;a href=&#34;https://github.com/WooooDyy/LLM-Agent-Paper-List&#34;&gt;LLM-Agent-Paper-List&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Tsinghua University] &lt;a href=&#34;https://arxiv.org/abs/2312.11970&#34;&gt;Large Language Models Empowered Agent-based Modeling and Simulation: A Survey and Perspectives&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Renmin University] &lt;a href=&#34;https://arxiv.org/pdf/2308.11432.pdf&#34;&gt;A Survey on Large Language Model-based Autonomous Agents&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Nanyang Technological University] &lt;a href=&#34;https://arxiv.org/abs/2402.18485&#34;&gt;FinAgent: A Multimodal Foundation Agent for Financial Trading: Tool-Augmented, Diversified, and Generalist&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;AI Agent Blogs and Videos&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[Medium] &lt;a href=&#34;https://medium.com/humansdotai/an-introduction-to-ai-agents-e8c4afd2ee8f&#34;&gt;An Introduction to AI Agents&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Medium] &lt;a href=&#34;https://medium.com/@aitrendorbit/unmasking-the-best-character-ai-chatbots-2024-351de43792f4#the-best-character-ai-chatbots&#34;&gt;Unmasking the Best Character AI Chatbots | 2024&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[big-picture] &lt;a href=&#34;https://blog.big-picture.com/en/chatgpt-next-level-meet-10-autonomous-ai-agents-auto-gpt-babyagi-agentgpt-microsoft-jarvis-chaosgpt-friends/&#34;&gt;ChatGPT, Next Level: Meet 10 Autonomous AI Agents&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[TowardsDataScience] &lt;a href=&#34;https://towardsdatascience.com/navigating-the-world-of-llm-agents-a-beginners-guide-3b8d499db7a9&#34;&gt;Navigating the World of LLM Agents: A Beginnerâ€™s Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[YouTube] &lt;a href=&#34;https://www.youtube.com/watch?v=iVbN95ica_k&#34;&gt;Introducing Devin - The &#34;First&#34; AI Agent Software Engineer&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;AI Agent Open-Source Framework &amp;amp; Tool&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Significant-Gravitas/AutoGPT&#34;&gt;AutoGPT (161k stars)&lt;/a&gt; is a tool for everyone to use, aiming to democratize AI, making it accessible for everyone to use and build upon.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/langchain-ai/langchain&#34;&gt;LangChain (82.7k stars)&lt;/a&gt; is a framework for developing context-aware applications powered by language models, enabling them to connect to sources of context and rely on the model&#39;s reasoning capabilities for responses and actions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/geekan/MetaGPT&#34;&gt;MetaGPT (39.1k stars)&lt;/a&gt; is a multi-agent open-source framework that assigns different roles to GPTs, forming a collaborative software entity to execute complex tasks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/autogen&#34;&gt;AutoGen (24.8k stars)&lt;/a&gt; is a framework for developing LLM applications with conversational agents that collaborate to solve tasks. These agents are customizable, support human interaction, and operate in modes combining LLMs, human inputs, and tools.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/langgenius/dify&#34;&gt;dify (22.7k stars)&lt;/a&gt; is an LLM application development platform. It integrates the concepts of Backend as a Service and LLMOps, covering the core tech stack required for building generative AI-native applications, including a built-in RAG engine&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/OpenBMB/ChatDev&#34;&gt;ChatDev (22.7k stars)&lt;/a&gt; is a framework that focuses on developing conversational AI Agents capable of dialogue and question-answering. It provides a range of pre-trained models and interactive interfaces, facilitating the development of customized chat Agents for users.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yoheinakajima/babyagi&#34;&gt;BabyAGI (19.2k stars)&lt;/a&gt; is an AI-powered task management system, dedicated to building AI Agents with preliminary general intelligence.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/TransformerOptimus/SuperAGI&#34;&gt;SuperAGI (14.4k stars)&lt;/a&gt; is a dev-first open-source autonomous AI agent framework enabling developers to build, manage &amp;amp; run useful autonomous agents.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labring/FastGPT&#34;&gt;FastGPT (12.5k stars)&lt;/a&gt; is a knowledge-based platform built on the LLM, offers out-of-the-box data processing and model invocation capabilities, allows for workflow orchestration through Flow visualization.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/joaomdmoura/crewAI&#34;&gt;CrewAI (12.1k stars)&lt;/a&gt; is a framework for orchestrating role-playing, autonomous AI agents. By fostering collaborative intelligence, CrewAI empowers agents to work together seamlessly, tackling complex tasks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/OpenBMB/XAgent&#34;&gt;XAgent (7.5k stars)&lt;/a&gt; is an open-source experimental Large Language Model (LLM) driven autonomous agent that can automatically solve various tasks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dataelement/bisheng&#34;&gt;Bisheng (5.5k stars)&lt;/a&gt; is a leading open-source platform for developing LLM applications.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/OpenBMB/XAgent&#34;&gt;Voyager (5.1k stars)&lt;/a&gt; An Open-Ended Embodied Agent with Large Language Models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/camel-ai/camel&#34;&gt;CAMEL (4.4k stars)&lt;/a&gt; is a framework that offers a comprehensive set of tools and algorithms for building multimodal AI Agents, enabling them to handle various data forms such as text, images, and speech.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/langfuse/langfuse&#34;&gt;Langfuse (2.9k stars)&lt;/a&gt; is a language fusion framework that can integrate the language abilities of multiple AI Agents, enabling them to simultaneously possess multilingual understanding and generation capabilities.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Disclaimer&lt;/strong&gt;: The codes and documents provided herein are released under the Apache-2.0 license. They should not be construed as financial counsel or recommendations for live trading. It is imperative to exercise caution and consult with qualified financial professionals prior to any trading or investment actions.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mistralai/mistral-inference</title>
    <updated>2024-06-09T01:41:52Z</updated>
    <id>tag:github.com,2024-06-09:/mistralai/mistral-inference</id>
    <link href="https://github.com/mistralai/mistral-inference" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official inference library for Mistral models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Mistral Inference&lt;/h1&gt; &#xA;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/mistralai/mistral-inference/blob/main/tutorials/getting_started.ipynb&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;This repository contains minimal code to run our 7B, 8x7B and 8x22B models.&lt;/p&gt; &#xA;&lt;p&gt;Blog 7B: &lt;a href=&#34;https://mistral.ai/news/announcing-mistral-7b/&#34;&gt;https://mistral.ai/news/announcing-mistral-7b/&lt;/a&gt;&lt;br&gt; Blog 8x7B: &lt;a href=&#34;https://mistral.ai/news/mixtral-of-experts/&#34;&gt;https://mistral.ai/news/mixtral-of-experts/&lt;/a&gt;&lt;br&gt; Blog 8x22B: &lt;a href=&#34;https://mistral.ai/news/mixtral-8x22b/&#34;&gt;https://mistral.ai/news/mixtral-8x22b/&lt;/a&gt; Blog Codestral 22B: &lt;a href=&#34;https://mistral.ai/news/codestral/&#34;&gt;https://mistral.ai/news/codestral&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Discord: &lt;a href=&#34;https://discord.com/invite/mistralai&#34;&gt;https://discord.com/invite/mistralai&lt;/a&gt;&lt;br&gt; Documentation: &lt;a href=&#34;https://docs.mistral.ai/&#34;&gt;https://docs.mistral.ai/&lt;/a&gt;&lt;br&gt; Guardrailing: &lt;a href=&#34;https://docs.mistral.ai/usage/guardrailing&#34;&gt;https://docs.mistral.ai/usage/guardrailing&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Note: You will use a GPU to install &lt;code&gt;mistral-inference&lt;/code&gt;, as it currently requires &lt;code&gt;xformers&lt;/code&gt; to be installed and &lt;code&gt;xformers&lt;/code&gt; itself needs a GPU for installation.&lt;/p&gt; &#xA;&lt;h3&gt;PyPI&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install mistral-inference&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Local&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd $HOME &amp;amp;&amp;amp; git clone https://github.com/mistralai/mistral-inference&#xA;cd $HOME/mistral-inference &amp;amp;&amp;amp; poetry install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Model download&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Download&lt;/th&gt; &#xA;   &lt;th&gt;md5sum&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;7B Instruct&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://models.mistralcdn.com/mistral-7b-v0-3/mistral-7B-Instruct-v0.3.tar&#34;&gt;https://models.mistralcdn.com/mistral-7b-v0-3/mistral-7B-Instruct-v0.3.tar&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;80b71fcb6416085bcb4efad86dfb4d52&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;8x7B Instruct&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://models.mistralcdn.com/mixtral-8x7b-v0-1/Mixtral-8x7B-v0.1-Instruct.tar&#34;&gt;https://models.mistralcdn.com/mixtral-8x7b-v0-1/Mixtral-8x7B-v0.1-Instruct.tar&lt;/a&gt; (&lt;strong&gt;Updated model coming soon!&lt;/strong&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;8e2d3930145dc43d3084396f49d38a3f&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;8x22 Instruct&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://models.mistralcdn.com/mixtral-8x22b-v0-3/mixtral-8x22B-Instruct-v0.3.tar&#34;&gt;https://models.mistralcdn.com/mixtral-8x22b-v0-3/mixtral-8x22B-Instruct-v0.3.tar&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;471a02a6902706a2f1e44a693813855b&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;7B Base&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://models.mistralcdn.com/mistral-7b-v0-3/mistral-7B-v0.3.tar&#34;&gt;https://models.mistralcdn.com/mistral-7b-v0-3/mistral-7B-v0.3.tar&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;0663b293810d7571dad25dae2f2a5806&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;8x7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Updated model coming soon!&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;8x22B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://models.mistralcdn.com/mixtral-8x22b-v0-3/mixtral-8x22B-v0.3.tar&#34;&gt;https://models.mistralcdn.com/mixtral-8x22b-v0-3/mixtral-8x22B-v0.3.tar&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;a2fa75117174f87d1197e3a4eb50371a&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Codestral 22B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://models.mistralcdn.com/codestral-22b-v0-1/codestral-22B-v0.1.tar&#34;&gt;https://models.mistralcdn.com/codestral-22b-v0-1/codestral-22B-v0.1.tar&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;1ea95d474a1d374b1d1b20a8e0159de3&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Note:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Important&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;mixtral-8x22B-Instruct-v0.3.tar&lt;/code&gt; is exactly the same as &lt;a href=&#34;https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1&#34;&gt;Mixtral-8x22B-Instruct-v0.1&lt;/a&gt;, only stored in &lt;code&gt;.safetensors&lt;/code&gt; format&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;mixtral-8x22B-v0.3.tar&lt;/code&gt; is the same as &lt;a href=&#34;https://huggingface.co/mistralai/Mixtral-8x22B-v0.1&#34;&gt;Mixtral-8x22B-v0.1&lt;/a&gt;, but has an extended vocabulary of 32768 tokens.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;codestral-22B-v0.1.tar&lt;/code&gt; has a custom non-commercial license, called &lt;a href=&#34;https://mistral.ai/licenses/MNPL-0.1.md&#34;&gt;Mistral AI Non-Production (MNPL) License&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;All of the listed models above support function calling. For example, Mistral 7B Base/Instruct v3 is a minor update to Mistral 7B Base/Instruct v2, with the addition of function calling capabilities.&lt;/li&gt; &#xA; &lt;li&gt;The &#34;coming soon&#34; models will include function calling as well.&lt;/li&gt; &#xA; &lt;li&gt;You can download the previous versions of our models from our &lt;a href=&#34;https://docs.mistral.ai/getting-started/open_weight_models/#downloading&#34;&gt;docs&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Create a local folder to store models&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;export MISTRAL_MODEL=$HOME/mistral_models&#xA;mkdir -p $MISTRAL_MODEL&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download any of the above links and extract the content, &lt;em&gt;e.g.&lt;/em&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;export M7B_DIR=$MISTRAL_MODEL/7B_instruct&#xA;wget https://models.mistralcdn.com/mistral-7b-v0-3/mistral-7B-Instruct-v0.3.tar&#xA;mkdir -p $M7B_DIR&#xA;tar -xf mistral-7B-Instruct-v0.3.tar -C $M7B_DIR&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;export M8x7B_DIR=$MISTRAL_MODEL/8x7b_instruct&#xA;wget https://models.mistralcdn.com/mixtral-8x7b-v0-1/Mixtral-8x7B-v0.1-Instruct.tar&#xA;mkdir -p $M8x7B_DIR&#xA;tar -xf Mixtral-8x7B-v0.1-Instruct.tar -C $M8x7B_DIR&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;The following sections give an overview of how to run the model from the Command-line interface (CLI) or directly within Python.&lt;/p&gt; &#xA;&lt;h3&gt;CLI&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Demo&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To test that a model works in your setup, you can run the &lt;code&gt;mistral-demo&lt;/code&gt; command. The 7B models can be tested on a single GPU as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;mistral-demo $M7B_DIR&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Large models, such &lt;strong&gt;8x7B&lt;/strong&gt; and &lt;strong&gt;8x22B&lt;/strong&gt; have to be run in a multi-GPU setup. For these models, you can use the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;torchrun --nproc-per-node 2 --no-python mistral-demo $M8x7B_DIR&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: Change &lt;code&gt;--nproc-per-node&lt;/code&gt; to more GPUs if available.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Chat&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To interactively chat with the models, you can make use of the &lt;code&gt;mistral-chat&lt;/code&gt; command.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;mistral-chat $M7B_DIR --instruct&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For large models, you can make use of &lt;code&gt;torchrun&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;torchrun --nproc-per-node 2 --no-python mistral-chat $M8x7B_DIR --instruct&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: Change &lt;code&gt;--nproc-per-node&lt;/code&gt; to more GPUs if necessary (&lt;em&gt;e.g.&lt;/em&gt; for 8x22B).&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Chat as Code Assistant&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To use &lt;a href=&#34;https://mistral.ai/news/codestral/&#34;&gt;Codestral&lt;/a&gt; as a coding assistant you can run the following command using &lt;code&gt;mistral-chat&lt;/code&gt;. Make sure &lt;code&gt;$M22B_CODESTRAL&lt;/code&gt; is set to a valid path to the downloaded codestral folder, e.g. &lt;code&gt;$HOME/mistral_models/Codestral-22B-v0.1&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;mistral-chat $M22B_CODESTRAL --instruct --max_tokens 256&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you prompt it with &lt;em&gt;&#34;Write me a function that computes fibonacci in Rust&#34;&lt;/em&gt;, the model should generate something along the following lines:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;Sure, here&#39;s a simple implementation of a function that computes the Fibonacci sequence in Rust. This function takes an integer `n` as an argument and returns the `n`th Fibonacci number.&#xA;&#xA;fn fibonacci(n: u32) -&amp;gt; u32 {&#xA;    match n {&#xA;        0 =&amp;gt; 0,&#xA;        1 =&amp;gt; 1,&#xA;        _ =&amp;gt; fibonacci(n - 1) + fibonacci(n - 2),&#xA;    }&#xA;}&#xA;&#xA;fn main() {&#xA;    let n = 10;&#xA;    println!(&#34;The {}th Fibonacci number is: {}&#34;, n, fibonacci(n));&#xA;}&#xA;&#xA;This function uses recursion to calculate the Fibonacci number. However, it&#39;s not the most efficient solution because it performs a lot of redundant calculations. A more efficient solution would use a loop to iteratively calculate the Fibonacci numbers.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can continue chatting afterwards, &lt;em&gt;e.g.&lt;/em&gt; with &lt;em&gt;&#34;Translate it to Python&#34;&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Python&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;Instruction Following&lt;/em&gt;:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;from mistral_inference.model import Transformer&#xA;from mistral_inference.generate import generate&#xA;&#xA;from mistral_common.tokens.tokenizers.mistral import MistralTokenizer&#xA;from mistral_common.protocol.instruct.messages import UserMessage&#xA;from mistral_common.protocol.instruct.request import ChatCompletionRequest&#xA;&#xA;&#xA;tokenizer = MistralTokenizer.from_file(&#34;./mistral_7b_instruct/tokenizer.model.v3&#34;)  # change to extracted tokenizer file&#xA;model = Transformer.from_folder(&#34;./mistral_7b_instruct&#34;)  # change to extracted model dir&#xA;&#xA;completion_request = ChatCompletionRequest(messages=[UserMessage(content=&#34;Explain Machine Learning to me in a nutshell.&#34;)])&#xA;&#xA;tokens = tokenizer.encode_chat_completion(completion_request).tokens&#xA;&#xA;out_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)&#xA;result = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])&#xA;&#xA;print(result)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;Function Calling&lt;/em&gt;:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;from mistral_common.protocol.instruct.tool_calls import Function, Tool&#xA;&#xA;completion_request = ChatCompletionRequest(&#xA;    tools=[&#xA;        Tool(&#xA;            function=Function(&#xA;                name=&#34;get_current_weather&#34;,&#xA;                description=&#34;Get the current weather&#34;,&#xA;                parameters={&#xA;                    &#34;type&#34;: &#34;object&#34;,&#xA;                    &#34;properties&#34;: {&#xA;                        &#34;location&#34;: {&#xA;                            &#34;type&#34;: &#34;string&#34;,&#xA;                            &#34;description&#34;: &#34;The city and state, e.g. San Francisco, CA&#34;,&#xA;                        },&#xA;                        &#34;format&#34;: {&#xA;                            &#34;type&#34;: &#34;string&#34;,&#xA;                            &#34;enum&#34;: [&#34;celsius&#34;, &#34;fahrenheit&#34;],&#xA;                            &#34;description&#34;: &#34;The temperature unit to use. Infer this from the users location.&#34;,&#xA;                        },&#xA;                    },&#xA;                    &#34;required&#34;: [&#34;location&#34;, &#34;format&#34;],&#xA;                },&#xA;            )&#xA;        )&#xA;    ],&#xA;    messages=[&#xA;        UserMessage(content=&#34;What&#39;s the weather like today in Paris?&#34;),&#xA;        ],&#xA;)&#xA;&#xA;tokens = tokenizer.encode_chat_completion(completion_request).tokens&#xA;&#xA;out_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)&#xA;result = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])&#xA;&#xA;print(result)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;Fill-in-the-middle (FIM)&lt;/em&gt;:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Make sure to have &lt;code&gt;mistral-common &amp;gt;= 1.2.0&lt;/code&gt; installed:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install --upgrade mistral-common&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can simulate a code completion in-filling as follows.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;from mistral_inference.model import Transformer&#xA;from mistral_inference.generate import generate&#xA;from mistral_common.tokens.tokenizers.mistral import MistralTokenizer&#xA;from mistral_common.tokens.instruct.request import FIMRequest&#xA;&#xA;tokenizer = MistralTokenizer.from_model(&#34;codestral-22b&#34;)&#xA;model = Transformer.from_folder(&#34;./mistral_22b_codestral&#34;)&#xA;&#xA;prefix = &#34;&#34;&#34;def add(&#34;&#34;&#34;&#xA;suffix = &#34;&#34;&#34;    return sum&#34;&#34;&#34;&#xA;&#xA;request = FIMRequest(prompt=prefix, suffix=suffix)&#xA;&#xA;tokens = tokenizer.encode_fim(request).tokens&#xA;&#xA;out_tokens, _ = generate([tokens], model, max_tokens=256, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)&#xA;result = tokenizer.decode(out_tokens[0])&#xA;&#xA;middle = result.split(suffix)[0].strip()&#xA;print(middle)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;One-file-ref&lt;/h3&gt; &#xA;&lt;p&gt;If you want a self-contained implementation, look at &lt;code&gt;one_file_ref.py&lt;/code&gt;, or run it with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m one_file_ref $M7B_DIR&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;which should give something along the following lines:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;This is a test of the emergency broadcast system. This is only a test.&#xA;&#xA;If this were a real emergency, you would be told what to do.&#xA;&#xA;This is a test&#xA;=====================&#xA;This is another test of the new blogging software. Iâ€™m not sure if Iâ€™m going to keep it or not. Iâ€™m not sure if Iâ€™m going to keep&#xA;=====================&#xA;This is a third test, mistral AI is very good at testing. ðŸ™‚&#xA;&#xA;This is a third test, mistral AI is very good at testing. ðŸ™‚&#xA;&#xA;This&#xA;=====================&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: To run self-contained implementations, you need to do a local installation.&lt;/p&gt; &#xA;&lt;h3&gt;Test&lt;/h3&gt; &#xA;&lt;p&gt;To run logits equivalence:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m pytest tests&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Deployment&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;deploy&lt;/code&gt; folder contains code to build a &lt;a href=&#34;https://M7B_DIR.com/vllm-project/vllm&#34;&gt;vLLM&lt;/a&gt; image with the required dependencies to serve the Mistral AI model. In the image, the &lt;a href=&#34;https://github.com/huggingface/transformers/&#34;&gt;transformers&lt;/a&gt; library is used instead of the reference implementation. To build it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build deploy --build-arg MAX_JOBS=8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Instructions to run the image can be found in the &lt;a href=&#34;https://docs.mistral.ai/quickstart&#34;&gt;official documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Model platforms&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Use Mistral models on &lt;a href=&#34;https://console.mistral.ai/&#34;&gt;Mistral AI official API&lt;/a&gt; (La Plateforme)&lt;/li&gt; &#xA; &lt;li&gt;Use Mistral models via &lt;a href=&#34;https://docs.mistral.ai/deployment/cloud/overview/&#34;&gt;cloud providers&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;p&gt;[1]: &lt;a href=&#34;https://arxiv.org/abs/2106.09685&#34;&gt;LoRA&lt;/a&gt;: Low-Rank Adaptation of Large Language Models, Hu et al. 2021&lt;/p&gt;</summary>
  </entry>
</feed>