<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-09-25T01:42:36Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>salesforce/BLIP</title>
    <updated>2022-09-25T01:42:36Z</updated>
    <id>tag:github.com,2022-09-25:/salesforce/BLIP</id>
    <link href="https://github.com/salesforce/BLIP" rel="alternate"></link>
    <summary type="html">&lt;p&gt;PyTorch code for BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation&lt;/h2&gt; &#xA;&lt;h2&gt;Announcement: BLIP is now officially integrated into &lt;a href=&#34;https://github.com/salesforce/LAVIS&#34;&gt;LAVIS&lt;/a&gt; - a one-stop library for language-and-vision research and applications!&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/salesforce/BLIP/main/BLIP.gif&#34; width=&#34;700&#34;&gt; &#xA;&lt;p&gt;This is the PyTorch code of the &lt;a href=&#34;https://arxiv.org/abs/2201.12086&#34;&gt;BLIP paper&lt;/a&gt; [&lt;a href=&#34;https://blog.salesforceairesearch.com/blip-bootstrapping-language-image-pretraining/&#34;&gt;blog&lt;/a&gt;]. The code has been tested on PyTorch 1.10. To install the dependencies, run &lt;/p&gt;&#xA;&lt;pre&gt;&lt;/pre&gt;pip install -r requirements.txt&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p&gt;Catalog:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Inference demo&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Pre-trained and finetuned checkpoints&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Finetuning code for Image-Text Retrieval, Image Captioning, VQA, and NLVR2&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Pre-training code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Zero-shot video-text retrieval&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Download of bootstrapped pre-training datasets&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Inference demo:&lt;/h3&gt; &#xA;&lt;p&gt;Run our interactive demo using &lt;a href=&#34;https://colab.research.google.com/github/salesforce/BLIP/blob/main/demo.ipynb&#34;&gt;Colab notebook&lt;/a&gt; (no GPU needed). The demo includes code for:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Image captioning&lt;/li&gt; &#xA; &lt;li&gt;Open-ended visual question answering&lt;/li&gt; &#xA; &lt;li&gt;Multimodal / unimodal feature extraction&lt;/li&gt; &#xA; &lt;li&gt;Image-text matching&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Try out the &lt;a href=&#34;https://huggingface.co/spaces/Salesforce/BLIP&#34;&gt;Web demo&lt;/a&gt;, integrated into &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces 🤗&lt;/a&gt; using &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Replicate web demo and Docker image is also available at &lt;a href=&#34;https://replicate.com/salesforce/blip&#34;&gt;&lt;img src=&#34;https://replicate.com/salesforce/blip/badge&#34; alt=&#34;Replicate&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Pre-trained checkpoints:&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Num. pre-train images&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;BLIP w/ ViT-B&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;BLIP w/ ViT-B and CapFilt-L&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;BLIP w/ ViT-L&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;14M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_14M.pth&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;129M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base.pth&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large.pth&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Finetuned checkpoints:&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Task&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;BLIP w/ ViT-B&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;BLIP w/ ViT-B and CapFilt-L&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;BLIP w/ ViT-L&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Image-Text Retrieval (COCO)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_coco.pth&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_retrieval_coco.pth&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Image-Text Retrieval (Flickr30k)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_flickr.pth&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_retrieval_flickr.pth&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Image Captioning (COCO)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_caption_capfilt_large.pth&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VQA&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_vqa.pth&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_vqa_capfilt_large.pth&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NLVR2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_nlvr.pth&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Image-Text Retrieval:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download COCO and Flickr30k datasets from the original websites, and set &#39;image_root&#39; in configs/retrieval_{dataset}.yaml accordingly.&lt;/li&gt; &#xA; &lt;li&gt;To evaluate the finetuned BLIP model on COCO, run:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;python -m torch.distributed.run --nproc_per_node=8 train_retrieval.py \&#xA;--config ./configs/retrieval_coco.yaml \&#xA;--output_dir output/retrieval_coco \&#xA;--evaluate&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;To finetune the pre-trained checkpoint using 8 A100 GPUs, first set &#39;pretrained&#39; in configs/retrieval_coco.yaml as &#34;&lt;a href=&#34;https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base.pth&#34;&gt;https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base.pth&lt;/a&gt;&#34;. Then run:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;python -m torch.distributed.run --nproc_per_node=8 train_retrieval.py \&#xA;--config ./configs/retrieval_coco.yaml \&#xA;--output_dir output/retrieval_coco &lt;/pre&gt; &#xA;&lt;h3&gt;Image-Text Captioning:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download COCO and NoCaps datasets from the original websites, and set &#39;image_root&#39; in configs/caption_coco.yaml and configs/nocaps.yaml accordingly.&lt;/li&gt; &#xA; &lt;li&gt;To evaluate the finetuned BLIP model on COCO, run:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;python -m torch.distributed.run --nproc_per_node=8 train_caption.py --evaluate&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;To evaluate the finetuned BLIP model on NoCaps, generate results with: (evaluation needs to be performed on official server)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;python -m torch.distributed.run --nproc_per_node=8 eval_nocaps.py &lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;To finetune the pre-trained checkpoint using 8 A100 GPUs, first set &#39;pretrained&#39; in configs/caption_coco.yaml as &#34;&lt;a href=&#34;https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth&#34;&gt;https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth&lt;/a&gt;&#34;. Then run:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;python -m torch.distributed.run --nproc_per_node=8 train_caption.py &lt;/pre&gt; &#xA;&lt;h3&gt;VQA:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download VQA v2 dataset and Visual Genome dataset from the original websites, and set &#39;vqa_root&#39; and &#39;vg_root&#39; in configs/vqa.yaml.&lt;/li&gt; &#xA; &lt;li&gt;To evaluate the finetuned BLIP model, generate results with: (evaluation needs to be performed on official server)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;python -m torch.distributed.run --nproc_per_node=8 train_vqa.py --evaluate&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;To finetune the pre-trained checkpoint using 16 A100 GPUs, first set &#39;pretrained&#39; in configs/vqa.yaml as &#34;&lt;a href=&#34;https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth&#34;&gt;https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth&lt;/a&gt;&#34;. Then run:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;python -m torch.distributed.run --nproc_per_node=16 train_vqa.py &lt;/pre&gt; &#xA;&lt;h3&gt;NLVR2:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download NLVR2 dataset from the original websites, and set &#39;image_root&#39; in configs/nlvr.yaml.&lt;/li&gt; &#xA; &lt;li&gt;To evaluate the finetuned BLIP model, run&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;python -m torch.distributed.run --nproc_per_node=8 train_nlvr.py --evaluate&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;To finetune the pre-trained checkpoint using 16 A100 GPUs, first set &#39;pretrained&#39; in configs/nlvr.yaml as &#34;&lt;a href=&#34;https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base.pth&#34;&gt;https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base.pth&lt;/a&gt;&#34;. Then run:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;python -m torch.distributed.run --nproc_per_node=16 train_nlvr.py &lt;/pre&gt; &#xA;&lt;h3&gt;Finetune with ViT-L:&lt;/h3&gt; &#xA;&lt;p&gt;In order to finetune a model with ViT-L, simply change the config file to set &#39;vit&#39; as large. Batch size and learning rate may also need to be adjusted accordingly (please see the paper&#39;s appendix for hyper-parameter details). &lt;a href=&#34;https://github.com/facebookresearch/fairscale&#34;&gt;Gradient checkpoint&lt;/a&gt; can also be activated in the config file to reduce GPU memory usage.&lt;/p&gt; &#xA;&lt;h3&gt;Pre-train:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Prepare training json files where each json file contains a list. Each item in the list is a dictonary with two key-value pairs: {&#39;image&#39;: path_of_image, &#39;caption&#39;: text_of_image}.&lt;/li&gt; &#xA; &lt;li&gt;In configs/pretrain.yaml, set &#39;train_file&#39; as the paths for the json files .&lt;/li&gt; &#xA; &lt;li&gt;Pre-train the model using 8 A100 GPUs:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;python -m torch.distributed.run --nproc_per_node=8 pretrain.py --config ./configs/Pretrain.yaml --output_dir output/Pretrain &lt;/pre&gt; &#xA;&lt;h3&gt;Zero-shot video-text retrieval:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download MSRVTT dataset following the instructions from &lt;a href=&#34;https://github.com/salesforce/ALPRO&#34;&gt;https://github.com/salesforce/ALPRO&lt;/a&gt;, and set &#39;video_root&#39; accordingly in configs/retrieval_msrvtt.yaml.&lt;/li&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://github.com/dmlc/decord&#34;&gt;decord&lt;/a&gt; with &lt;pre&gt;pip install decord&lt;/pre&gt;&lt;/li&gt; &#xA; &lt;li&gt;To perform zero-shot evaluation, run&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;python -m torch.distributed.run --nproc_per_node=8 eval_retrieval_video.py&lt;/pre&gt; &#xA;&lt;h3&gt;Pre-training datasets download:&lt;/h3&gt; &#xA;&lt;p&gt;We provide bootstrapped pre-training datasets as json files. Each json file contains a list. Each item in the list is a dictonary with two key-value pairs: {&#39;url&#39;: url_of_image, &#39;caption&#39;: text_of_image}.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Image source&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Filtered web caption&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Filtered synthetic caption by ViT-B&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Filtered synthetic caption by ViT-L&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CC3M+CC12M+SBU&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/sfr-vision-language-research/BLIP/datasets/ccs_filtered.json&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/sfr-vision-language-research/BLIP/datasets/ccs_synthetic_filtered.json&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/sfr-vision-language-research/BLIP/datasets/ccs_synthetic_filtered_large.json&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LAION115M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/sfr-vision-language-research/BLIP/datasets/laion_filtered.json&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/sfr-vision-language-research/BLIP/datasets/laion_synthetic_filtered.json&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/sfr-vision-language-research/BLIP/datasets/laion_synthetic_filtered_large.json&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Citation&lt;/h3&gt; &#xA;&lt;p&gt;If you find this code to be useful for your research, please consider citing.&lt;/p&gt; &#xA;&lt;pre&gt;&#xA;@inproceedings{li2022blip,&#xA;      title={BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation}, &#xA;      author={Junnan Li and Dongxu Li and Caiming Xiong and Steven Hoi},&#xA;      year={2022},&#xA;      booktitle={ICML},&#xA;}&lt;/pre&gt; &#xA;&lt;h3&gt;Acknowledgement&lt;/h3&gt; &#xA;&lt;p&gt;The implementation of BLIP relies on resources from &lt;a href=&#34;https://github.com/salesforce/ALBEF&#34;&gt;ALBEF&lt;/a&gt;, &lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;Huggingface Transformers&lt;/a&gt;, and &lt;a href=&#34;https://github.com/rwightman/pytorch-image-models/tree/master/timm&#34;&gt;timm&lt;/a&gt;. We thank the original authors for their open-sourcing.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>pycaret/pycaret</title>
    <updated>2022-09-25T01:42:36Z</updated>
    <id>tag:github.com,2022-09-25:/pycaret/pycaret</id>
    <link href="https://github.com/pycaret/pycaret" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An open-source, low-code machine learning library in Python&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/pycaret/pycaret/master/docs/images/logo.png&#34; alt=&#34;drawing&#34; width=&#34;200&#34;&gt; &#xA; &lt;p&gt;&lt;strong&gt;An open-source, low-code machine learning library in Python&lt;/strong&gt; &lt;br&gt; &lt;span&gt;🚀&lt;/span&gt; &lt;strong&gt;PyCaret 3.0-rc is now out. &lt;code&gt;pip install --pre pycaret&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.pycaret.org&#34;&gt;Official&lt;/a&gt; • &lt;a href=&#34;https://pycaret.gitbook.io/&#34;&gt;Docs&lt;/a&gt; • &lt;a href=&#34;https://pycaret.gitbook.io/docs/get-started/installation&#34;&gt;Install&lt;/a&gt; • &lt;a href=&#34;https://pycaret.gitbook.io/docs/get-started/tutorials&#34;&gt;Tutorials&lt;/a&gt; • &lt;a href=&#34;https://pycaret.gitbook.io/docs/learn-pycaret/faqs&#34;&gt;FAQs&lt;/a&gt; • &lt;a href=&#34;https://pycaret.gitbook.io/docs/learn-pycaret/cheat-sheet&#34;&gt;Cheat sheet&lt;/a&gt; • &lt;a href=&#34;https://github.com/pycaret/pycaret/discussions&#34;&gt;Discussions&lt;/a&gt; • &lt;a href=&#34;https://pycaret.readthedocs.io/en/latest/contribute.html&#34;&gt;Contribute&lt;/a&gt; • &lt;a href=&#34;https://github.com/pycaret/pycaret/tree/master/resources&#34;&gt;Resources&lt;/a&gt; • &lt;a href=&#34;https://pycaret.gitbook.io/docs/learn-pycaret/official-blog&#34;&gt;Blog&lt;/a&gt; • &lt;a href=&#34;https://www.linkedin.com/company/pycaret/&#34;&gt;LinkedIn&lt;/a&gt; • &lt;a href=&#34;https://www.youtube.com/channel/UCxA1YTYJ9BEeo50lxyI_B3g&#34;&gt;YouTube&lt;/a&gt; • &lt;a href=&#34;https://join.slack.com/t/pycaret/shared_invite/zt-row9phbm-BoJdEVPYnGf7_NxNBP307w&#34;&gt;Slack&lt;/a&gt; &lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://badge.fury.io/py/pycaret&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Python-3.7%20%7C%203.8%20%7C%203.9-blue&#34; alt=&#34;Python&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://github.com/pycaret/pycaret/workflows/pytest%20on%20push/badge.svg?sanitize=true&#34; alt=&#34;pytest on push&#34;&gt; &lt;a href=&#34;http://pip.pypa.io/en/stable/?badge=stable&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/pip/badge/?version=stable&#34; alt=&#34;Documentation Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/py/pycaret&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/pycaret.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://img.shields.io/pypi/l/ansicolortags.svg&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/l/ansicolortags.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;!-- [![Git count](http://hits.dwyl.com/pycaret/pycaret/pycaret.svg)](http://hits.dwyl.com/pycaret/pycaret/pycaret) --&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://join.slack.com/t/pycaret/shared_invite/zt-row9phbm-BoJdEVPYnGf7_NxNBP307w&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/slack-chat-green.svg?logo=slack&#34; alt=&#34;Slack&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pycaret/pycaret/master/docs/images/quick_start.gif&#34; alt=&#34;alt text&#34;&gt;&lt;/p&gt; &#xA; &lt;div align=&#34;left&#34;&gt; &#xA;  &lt;h1&gt;Welcome to PyCaret&lt;/h1&gt; &#xA;  &lt;p&gt;PyCaret is an open-source, low-code machine learning library in Python that automates machine learning workflows. It is an end-to-end machine learning and model management tool that speeds up the experiment cycle exponentially and makes you more productive.&lt;/p&gt; &#xA;  &lt;p&gt;In comparison with the other open-source machine learning libraries, PyCaret is an alternate low-code library that can be used to replace hundreds of lines of code with few lines only. This makes experiments exponentially fast and efficient. PyCaret is essentially a Python wrapper around several machine learning libraries and frameworks such as scikit-learn, XGBoost, LightGBM, CatBoost, spaCy, Optuna, Hyperopt, Ray, and few more.&lt;/p&gt; &#xA;  &lt;p&gt;The design and simplicity of PyCaret are inspired by the emerging role of citizen data scientists, a term first used by Gartner. Citizen Data Scientists are power users who can perform both simple and moderately sophisticated analytical tasks that would previously have required more technical expertise.&lt;/p&gt; &#xA;  &lt;table&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th&gt;Important Links&lt;/th&gt; &#xA;     &lt;th&gt;&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;&lt;span&gt;⭐&lt;/span&gt; &lt;strong&gt;&lt;a href=&#34;https://pycaret.gitbook.io/docs/get-started/tutorials&#34;&gt;Tutorials&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;New to PyCaret? Checkout our official notebooks!&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;&lt;span&gt;📋&lt;/span&gt; &lt;strong&gt;&lt;a href=&#34;https://github.com/pycaret/pycaret/tree/master/examples&#34;&gt;Example Notebooks&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;Example notebooks created by community.&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;&lt;span&gt;📙&lt;/span&gt; &lt;strong&gt;&lt;a href=&#34;https://pycaret.gitbook.io/docs/learn-pycaret/official-blog&#34;&gt;Official Blog&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;Tutorials and articles by contributors.&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;&lt;span&gt;📚&lt;/span&gt; &lt;strong&gt;&lt;a href=&#34;https://pycaret.gitbook.io&#34;&gt;Documentation&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;The detailed API docs of PyCaret&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;&lt;span&gt;📺&lt;/span&gt; &lt;strong&gt;&lt;a href=&#34;https://pycaret.gitbook.io/docs/learn-pycaret/videos&#34;&gt;Video Tutorials&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;Our video tutorial from various events.&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;✈️ &lt;strong&gt;&lt;a href=&#34;https://pycaret.gitbook.io/docs/learn-pycaret/cheat-sheet&#34;&gt;Cheat sheet&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;Cheat sheet for all functions across modules.&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;&lt;span&gt;📢&lt;/span&gt; &lt;strong&gt;&lt;a href=&#34;https://github.com/pycaret/pycaret/discussions&#34;&gt;Discussions&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;Have questions? Engage with community and contributors.&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;&lt;span&gt;🛠&lt;/span&gt; &lt;strong&gt;&lt;a href=&#34;https://pycaret.gitbook.io/docs/get-started/release-notes&#34;&gt;Changelog&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;Changes and version history.&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;&lt;span&gt;🌳&lt;/span&gt; &lt;strong&gt;&lt;a href=&#34;https://github.com/pycaret/pycaret/issues/1756&#34;&gt;Roadmap&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;PyCaret&#39;s software and community development plan.&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &#xA;  &lt;h1&gt;Installation&lt;/h1&gt; &#xA;  &lt;p&gt;PyCaret&#39;s default installation only installs hard dependencies as listed in the &lt;a href=&#34;https://raw.githubusercontent.com/pycaret/pycaret/master/requirements.txt&#34;&gt;requirements.txt&lt;/a&gt; file.&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pip install pycaret&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;p&gt;To install the full version:&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pip install pycaret[full]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;div align=&#34;center&#34;&gt; &#xA;   &lt;h1&gt;Supervised Workflow&lt;/h1&gt; &#xA;   &lt;table&gt; &#xA;    &lt;thead&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;Classification&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;Regression&lt;/th&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/thead&gt; &#xA;    &lt;tbody&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pycaret/pycaret/master/docs/images/pycaret_classification.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pycaret/pycaret/master/docs/images/pycaret_regression.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/tbody&gt; &#xA;   &lt;/table&gt; &#xA;   &lt;h1&gt;Unsupervised Workflow&lt;/h1&gt; &#xA;   &lt;table&gt; &#xA;    &lt;thead&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;Clustering&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;Anomaly Detection&lt;/th&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/thead&gt; &#xA;    &lt;tbody&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pycaret/pycaret/master/docs/images/pycaret_clustering.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pycaret/pycaret/master/docs/images/pycaret_anomaly.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/tbody&gt; &#xA;   &lt;/table&gt; &#xA;   &lt;div align=&#34;left&#34;&gt; &#xA;    &lt;h1&gt;⚡ PyCaret Time Series Module&lt;/h1&gt; &#xA;    &lt;p&gt;PyCaret time series module is now available with the main pycaret installation. Staying true to simplicity of PyCaret, it is consistent with our existing API and fully loaded with functionalities. Statistical testing, model training and selection (30+ algorithms), model analysis, automated hyperparameter tuning, experiment logging, deployment on cloud, and more. All of this with only few lines of code (just like the other modules of pycaret).&lt;/p&gt; &#xA;    &lt;table&gt; &#xA;     &lt;thead&gt; &#xA;      &lt;tr&gt; &#xA;       &lt;th&gt;Important Links&lt;/th&gt; &#xA;       &lt;th&gt;&lt;/th&gt; &#xA;      &lt;/tr&gt; &#xA;     &lt;/thead&gt; &#xA;     &lt;tbody&gt; &#xA;      &lt;tr&gt; &#xA;       &lt;td&gt;&lt;span&gt;⭐&lt;/span&gt; &lt;strong&gt;&lt;a href=&#34;https://pycaret.gitbook.io/docs/get-started/quickstart#time-series&#34;&gt;Time Series Quickstart&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;       &lt;td&gt;Get started with Time Series Analysis&lt;/td&gt; &#xA;      &lt;/tr&gt; &#xA;      &lt;tr&gt; &#xA;       &lt;td&gt;&lt;span&gt;📚&lt;/span&gt; &lt;strong&gt;&lt;a href=&#34;https://pycaret.gitbook.io/docs/get-started/tutorials&#34;&gt;Time Series Notebooks&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;       &lt;td&gt;New to Time Series? Checkout our official (detailed) notebooks!&lt;/td&gt; &#xA;      &lt;/tr&gt; &#xA;      &lt;tr&gt; &#xA;       &lt;td&gt;&lt;span&gt;📺&lt;/span&gt; &lt;strong&gt;&lt;a href=&#34;https://pycaret.gitbook.io/docs/learn-pycaret/videos#pycaret-time-series-module&#34;&gt;Time Series Video Tutorials&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;       &lt;td&gt;Our video tutorial from various events.&lt;/td&gt; &#xA;      &lt;/tr&gt; &#xA;      &lt;tr&gt; &#xA;       &lt;td&gt;&lt;span&gt;❓&lt;/span&gt; &lt;strong&gt;&lt;a href=&#34;https://github.com/pycaret/pycaret/discussions/categories/faqs?discussions_q=category%3AFAQs+label%3Atime_series&#34;&gt;Time Series FAQs&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;       &lt;td&gt;Have questions? Queck out the FAQ&#39;s&lt;/td&gt; &#xA;      &lt;/tr&gt; &#xA;      &lt;tr&gt; &#xA;       &lt;td&gt;&lt;span&gt;🛠&lt;/span&gt; &lt;strong&gt;&lt;a href=&#34;https://pycaret.readthedocs.io/en/latest/api/time_series.html&#34;&gt;Time Series API Interface&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;       &lt;td&gt;The detailed API interface for the Time Series Module&lt;/td&gt; &#xA;      &lt;/tr&gt; &#xA;      &lt;tr&gt; &#xA;       &lt;td&gt;&lt;span&gt;🌳&lt;/span&gt; &lt;strong&gt;&lt;a href=&#34;https://github.com/pycaret/pycaret/issues/1648&#34;&gt;Time Series Features and Roadmap&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;       &lt;td&gt;PyCaret&#39;s software and community development plan.&lt;/td&gt; &#xA;      &lt;/tr&gt; &#xA;     &lt;/tbody&gt; &#xA;    &lt;/table&gt; &#xA;    &lt;h1&gt;Installation&lt;/h1&gt; &#xA;    &lt;pre&gt;&lt;code&gt;pip install --pre pycaret&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;    &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pycaret/pycaret/master/docs/images/pycaret_ts_quickdemo.gif&#34; alt=&#34;alt text&#34;&gt;&lt;/p&gt; &#xA;    &lt;h1&gt;Who should use PyCaret?&lt;/h1&gt; &#xA;    &lt;p&gt;PyCaret is an open source library that anybody can use. In our view the ideal target audience of PyCaret is: &lt;br&gt;&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Experienced Data Scientists who want to increase productivity.&lt;/li&gt; &#xA;     &lt;li&gt;Citizen Data Scientists who prefer a low code machine learning solution.&lt;/li&gt; &#xA;     &lt;li&gt;Data Science Professionals who want to build rapid prototypes.&lt;/li&gt; &#xA;     &lt;li&gt;Data Science and Machine Learning students and enthusiasts.&lt;/li&gt; &#xA;    &lt;/ul&gt; &#xA;    &lt;h1&gt;PyCaret GPU support&lt;/h1&gt; &#xA;    &lt;p&gt;With PyCaret &amp;gt;= 2.2, you can train models on GPU and speed up your workflow by 10x. To train models on GPU simply pass &lt;code&gt;use_gpu = True&lt;/code&gt; in the setup function. There is no change in the use of the API, however, in some cases, additional libraries have to be installed as they are not installed with the default version or the full version. As of the latest release, the following models can be trained on GPU:&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Extreme Gradient Boosting (requires no further installation)&lt;/li&gt; &#xA;     &lt;li&gt;CatBoost (requires no further installation)&lt;/li&gt; &#xA;     &lt;li&gt;Light Gradient Boosting Machine requires &lt;a href=&#34;https://lightgbm.readthedocs.io/en/latest/GPU-Tutorial.html&#34;&gt;GPU installation&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Logistic Regression, Ridge Classifier, Random Forest, K Neighbors Classifier, K Neighbors Regressor, Support Vector Machine, Linear Regression, Ridge Regression, Lasso Regression requires &lt;a href=&#34;https://github.com/rapidsai/cuml&#34;&gt;cuML &amp;gt;= 0.15&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &#xA;    &lt;h1&gt;PyCaret Intel sklearnex support&lt;/h1&gt; &#xA;    &lt;p&gt;You can apply &lt;a href=&#34;https://github.com/intel/scikit-learn-intelex&#34;&gt;Intel optimizations&lt;/a&gt; for machine learning algorithms and speed up your workflow. To train models with Intel optimizations use &lt;code&gt;sklearnex&lt;/code&gt; engine. There is no change in the use of the API, however, installation of Intel sklearnex is required:&lt;/p&gt; &#xA;    &lt;p&gt;&lt;code&gt;pip install scikit-learn-intelex&lt;/code&gt;&lt;/p&gt; &#xA;    &lt;h1&gt;License&lt;/h1&gt; &#xA;    &lt;p&gt;PyCaret is completely free and open-source and licensed under the &lt;a href=&#34;https://github.com/pycaret/pycaret/raw/master/LICENSE&#34;&gt;MIT&lt;/a&gt; license.&lt;/p&gt; &#xA;    &lt;h1&gt;Contributors&lt;/h1&gt; &#xA;    &lt;a href=&#34;https://github.com/pycaret/pycaret/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contributors-img.web.app/image?repo=pycaret/pycaret&#34; width=&#34;500/&#34;&gt; &lt;/a&gt; &#xA;   &lt;/div&gt;&#xA;  &lt;/div&gt;&#xA; &lt;/div&gt;&#xA;&lt;/div&gt;</summary>
  </entry>
  <entry>
    <title>karpathy/nn-zero-to-hero</title>
    <updated>2022-09-25T01:42:36Z</updated>
    <id>tag:github.com,2022-09-25:/karpathy/nn-zero-to-hero</id>
    <link href="https://github.com/karpathy/nn-zero-to-hero" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Neural Networks: Zero to Hero&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Neural Networks: Zero to Hero&lt;/h2&gt; &#xA;&lt;p&gt;A course on neural networks that starts all the way at the basics. The course is a series of YouTube videos where we code and train neural networks together. The Jupyter notebooks we build in the videos are then captured here inside the &lt;a href=&#34;https://raw.githubusercontent.com/karpathy/nn-zero-to-hero/master/lectures/&#34;&gt;lectures&lt;/a&gt; directory. Every lecture also has a set of exercises included in the video description. (This may grow into something more respectable).&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Lecture 1: The spelled-out intro to neural networks and backpropagation: building micrograd&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Backpropagation and training of neural networks. Assumes basic knowledge of Python and a vague recollection of calculus from high school.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=VMj-3S1tku0&#34;&gt;YouTube video lecture&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/karpathy/nn-zero-to-hero/master/lectures/micrograd&#34;&gt;Jupyter notebook files&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/karpathy/micrograd&#34;&gt;micrograd Github repo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Lecture 2: The spelled-out intro to language modeling: building makemore&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We implement a bigram character-level language model, which we will further complexify in followup videos into a modern Transformer language model, like GPT. In this video, the focus is on (1) introducing torch.Tensor and its subtleties and use in efficiently evaluating neural networks and (2) the overall framework of language modeling that includes model training, sampling, and the evaluation of a loss (e.g. the negative log likelihood for classification).&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=PaCmpygFfXo&#34;&gt;YouTube video lecture&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/karpathy/nn-zero-to-hero/master/lectures/makemore/makemore_part1_bigrams.ipynb&#34;&gt;Jupyter notebook files&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/karpathy/makemore&#34;&gt;makemore Github repo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Lecture 3: The spelled-out intro to language modeling: building makemore. PART 2: MLP&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We implement a multilayer perceptron (MLP) character-level language model. In this video we also introduce many basics of machine learning (e.g. model training, learning rate tuning, hyperparameters, evaluation, train/dev/test splits, under/overfitting, etc.).&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtu.be/TCH_1BHY58I&#34;&gt;YouTube video lecture&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/karpathy/nn-zero-to-hero/master/lectures/makemore/makemore_part2_mlp.ipynb&#34;&gt;Jupyter notebook files&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/karpathy/makemore&#34;&gt;makemore Github repo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;ongoing...&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;License&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;MIT&lt;/p&gt;</summary>
  </entry>
</feed>