<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-20T01:50:16Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>justLV/onju-voice</title>
    <updated>2023-08-20T01:50:16Z</updated>
    <id>tag:github.com,2023-08-20:/justLV/onju-voice</id>
    <link href="https://github.com/justLV/onju-voice" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A hackable AI home assistant platform&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Onju Voice üçêüîà&lt;/h1&gt; &#xA;&lt;p&gt;üí´ &lt;a href=&#34;https://twitter.com/justLV&#34;&gt;DEMO&#39;s&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A hackable AI home assistant platform using the Google Nest Mini (2nd gen) form factor, consisting of:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;a custom PCB designed to be a drop-in replacement to the original, using the ESP32-S3 for audio processing&lt;/li&gt; &#xA; &lt;li&gt;a server for handling the transcription, response generation and Text-to-Speech from multiple devices on the same network&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/justLV/onju-voice/master/images/header_white.jpg&#34; width=&#34;960&#34;&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;This repo contains firmware, server code and some example applications, intended to be as accessible as possible for getting up and running i.e.:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/justLV/onju-voice/master/#-firmware&#34;&gt;Firmware&lt;/a&gt; for the custom PCB can be programmed using the Arduino IDE and a USB cable (installation of ESP-IDF not required)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/justLV/onju-voice/master/#%EF%B8%8F-server&#34;&gt;Server code&lt;/a&gt; has minimal requirements besides running Whisper locally, and should be able to run on most devices that you can leave plugged in whether MacOS / Linux / Win etc.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/justLV/onju-voice/master/images/rich.png&#34;&gt; &#xA;&lt;h2&gt;Example applications&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üì© Querying and replying to messages (using a &lt;a href=&#34;https://github.com/justLV/onju-voice-maubot&#34;&gt;custom Maubot plugin&lt;/a&gt; &amp;amp; Beeper)&lt;/li&gt; &#xA; &lt;li&gt;üí° Light control with &lt;a href=&#34;https://raw.githubusercontent.com/justLV/onju-voice/master/#-home-assistant&#34;&gt;Home Assistant&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üìù Adding and retrieving notes/memos for the LLM to craft a response with&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;Not included:&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üë• Multiple voice characters. I‚Äôll leave it to the user to clone voices as they deem fair use. Also from experience LLM‚Äôs &amp;lt; GPT4 don‚Äôt consistently enough follow instructions to reliably respond in different characters AND perform multiple function calling with complicated prompts.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Current features of the device &amp;lt;&amp;gt; server platform&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Auto-discovery of devices using multicast announcements&lt;/li&gt; &#xA; &lt;li&gt;Remembering conversation history and voice settings for each device&lt;/li&gt; &#xA; &lt;li&gt;Sending &amp;amp; receiving audio data from the device, packed as 16-bit, 16kHz (UDP sending, TCP receiving partially buffered into PSRAM)&lt;/li&gt; &#xA; &lt;li&gt;Speaker and microphone visualization with the LED‚Äôs, and custom LED control via the server&lt;/li&gt; &#xA; &lt;li&gt;Mute switch functionality, tap-to-wake for enabling the microphone, and setting mic timeout via the server&lt;/li&gt; &#xA; &lt;li&gt;Device-level logging to individual files and console output using &lt;code&gt;rich&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;[coming soon] SoftAP WiFi provisioning to prevent need for programming WiFi credentials&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Limitations of this release:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The Arduino IDE doesn‚Äôt (yet) support the Espressif‚Äôs Audio SDK‚Äôs, such as &lt;a href=&#34;https://github.com/espressif/esp-adf&#34;&gt;ESP-ADF&lt;/a&gt;, &lt;a href=&#34;https://github.com/espressif/esp-skainet&#34;&gt;ESP-Skainet&lt;/a&gt; etc. For these demo&#39;s it&#39;s not absolutely required, but if you use Espressif‚Äôs ESP-IDF with these SDK&#39;s you&#39;d unlock features such as: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;VAD (Voice Activity Detection) - in this example VAD is offloaded to the server using webrtcvad, and the listening period is extended by either tapping the device or by the server sending mic keep alive timeouts (network traffic is really minimal at 16-bit, 16kHz)&lt;/li&gt; &#xA;   &lt;li&gt;AEC (Acoustic Echo Cancellation) - to allow you to effectively talk over the assistant by removing the speaker output from audio input&lt;/li&gt; &#xA;   &lt;li&gt;BSS (Blind Source Separation) - let‚Äôs you use both mic‚Äôs for isolating speakers based on location, and other noise suppression&lt;/li&gt; &#xA;   &lt;li&gt;Wakewords and other on-device commands - I‚Äôm not a believer in this given how finicky these can be and don‚Äôt think these are and think all command logic should be handled by layers of language models on the server.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;The server currently only does transcription locally and uses: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;OpenAI for generating responses &amp;amp; functions calls, but if you have the hardware you could run a local LLM, using something like ToolLLM for calling API‚Äôs to add almost any capabilities you‚Äôd wish.&lt;/li&gt; &#xA;   &lt;li&gt;Text-to-speech from Elevenlabs - this is fair to say the easiest to get running, fastest and most expressive option out there but FWIR data policy is a little dubious so careful about sending anything too sensitive. I‚Äôd really like to see comparable performing open source options that you can run locally&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Conversation flow is highly serialized, i.e. recording &amp;gt; transcription &amp;gt; LLM &amp;gt; TTS needs to finish each step before moving onto the next. Not included here is feeding incomplete transcriptions to a smaller model, and streaming slower LLM&#39;s like GPT4 to Elevenlabs and sending streaming responses back, it&#39;s currently a little too hacky to include in this release.&lt;/li&gt; &#xA; &lt;li&gt;No wakeword usage, mostly done intentionally as I feel uttering a wake-word before every response is a terrible experience. This currently uses a combo of VAD, mic-timeouts sent from server, tap-to-wake, mute switch usage etc. Not included here is experiments running a smaller, faster LLM for classification with a running transcription before handing off to a larger LLM with specific prompt&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Other areas for improvement&lt;/h2&gt; &#xA;&lt;p&gt;These are things I didn&#39;t get time to implement but I believe would be invaluable and pretty achievable&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaker diarization - know who is saying what, and have the LLM enage in multi-user conversations or infer when it isn&#39;t being spoken to&lt;/li&gt; &#xA; &lt;li&gt;Interruptions - requires AEC for simultaneous listening and playback&lt;/li&gt; &#xA; &lt;li&gt;Smaller local models/LLM&#39;s for running classification, detecting intent and routing to larger LLM&#39;s&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;h2&gt;üñ•Ô∏è Server&lt;/h2&gt; &#xA;&lt;p&gt;Ensure you can install &lt;a href=&#34;https://github.com/openai/whisper&#34;&gt;Whisper&lt;/a&gt; and run at least the base model, following any debugging steps they have if not. If you can get past that, it should be as simple as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd server&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Adjust settings in the &lt;code&gt;config.yaml&lt;/code&gt;, and tweak aspects such as how much silence is needed to start processing to trade-off snappiness vs avoiding cutting off the user.&lt;/p&gt; &#xA;&lt;p&gt;Add your Elevenlabs token to &lt;code&gt;credentials.json&lt;/code&gt; and ensure you have a cloned voice in your account that you set in the &lt;code&gt;config.yaml&lt;/code&gt; under &lt;code&gt;elevenlabs_default_voice&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;You&#39;ll also need a greeting WAV set in &lt;code&gt;config.yaml&lt;/code&gt; under &lt;code&gt;greeting_wav&lt;/code&gt;, that will be sent to devices on connecting to the WiFi. This is up to you to record or procure (&lt;a href=&#34;https://github.com/ytdl-org/youtube-dl&#34;&gt;e.g.&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;A small subset of the config parameters can be set as optional arguments when running the script. For e.g. the following will run the server with note-taking, Home Assistant, Maubot, real sending of messages enabled (a safe guard disabled by default), and a smaller English only Whisper model for transcription.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;python server.py --n --ha --mb --send --whisper base.en&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;üè° Home Assistant&lt;/h3&gt; &#xA;&lt;p&gt;I recommend setting this up on the same server or one that is always plugged in on your network, following the &lt;a href=&#34;https://www.home-assistant.io/installation/linux#docker-compose&#34;&gt;Docker Compose instructions&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Then go through the onboarding, setup a user, name your devices and get a Long Lived token to add to &lt;code&gt;credentials.json&lt;/code&gt; together with the URL e.g. &lt;code&gt;http://my-local-server:8123/&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;ü§ñ Maubot&lt;/h3&gt; &#xA;&lt;p&gt;Follow instructions &lt;a href=&#34;https://github.com/justLV/onju-home-maubot&#34;&gt;here&lt;/a&gt; to setup Maubot with your Beeper account. Ensure the correct URL is setup in &lt;code&gt;config.yaml&lt;/code&gt;, set &lt;code&gt;send_replies&lt;/code&gt; to True if your friends are forgiving of the odd mistakes, and set a &lt;code&gt;footer&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Don‚Äôt have Beeper yet and can‚Äôt wait? &lt;a href=&#34;https://docs.mau.fi/bridges/go/imessage/mac/setup.html&#34;&gt;Try setup a Matrix bridge yourself&lt;/a&gt; and a custom function definition for OpenAI function calling (and share how you did it!)&lt;/p&gt; &#xA;&lt;p&gt;Following this example you can also integrate e-mail.&lt;/p&gt; &#xA;&lt;h2&gt;üìü Firmware&lt;/h2&gt; &#xA;&lt;p&gt;Irrespective of what you use for development, the quickest &amp;amp; least error prone setup for building &amp;amp; flashing firmware is probably installing the Arduino IDE &lt;a href=&#34;https://www.arduino.cc/en/software&#34;&gt;Software&lt;/a&gt;, and then using this IDE or your preference i.e. VSCode for development (Copilot)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add the ESP32 boards as detailed &lt;a href=&#34;https://docs.espressif.com/projects/arduino-esp32/en/latest/installing.html&#34;&gt;here&lt;/a&gt; (TL;DR add &lt;code&gt;https://espressif.github.io/arduino-esp32/package_esp32_index.json&lt;/code&gt; to &lt;code&gt;Preferences &amp;gt; Additional Boards Manager URL‚Äôs&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Under Boards Manager, install ‚Äúesp32‚Äù by Espressif Systems&lt;/li&gt; &#xA; &lt;li&gt;Under Library Manager, install ‚ÄúAdafruit NeoPixel Library‚Äù&lt;/li&gt; &#xA; &lt;li&gt;Clone this repo to &lt;code&gt;Documents/Arduino&lt;/code&gt; for simplicity.&lt;/li&gt; &#xA; &lt;li&gt;Add your WiFi credentials to &lt;code&gt;credentials.h&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;bash setup-git-hash.sh&lt;/code&gt; to add a header with the git-hash (optional). This will then automatically update after commits, and help track the firmware that your devices are running from the server side.&lt;/li&gt; &#xA; &lt;li&gt;Open File &amp;gt; Sketchbook &amp;gt; onju-home &amp;gt; onjuino&lt;/li&gt; &#xA; &lt;li&gt;Select Tools &amp;gt; Board &amp;gt; esp32 &amp;gt; ESP32S3 Dev Module&lt;/li&gt; &#xA; &lt;li&gt;Under Tools ensure: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;USB CDC on Boot set to Enabled&lt;/li&gt; &#xA;   &lt;li&gt;PSRAM set to OPI PSRAM&lt;/li&gt; &#xA;   &lt;li&gt;Board is plugged in and Port is selected (you may need to install USB bridge drivers as detailed by Espressif, don‚Äôt worry if name is incorrect)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Build and upload&lt;/li&gt; &#xA; &lt;li&gt;If not reset, press the reset button. In Serial Monitor you can also send &lt;code&gt;r&lt;/code&gt; to reset the device (assuming it is already booted)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üß© Hardware&lt;/h2&gt; &#xA;&lt;p float=&#34;left&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/justLV/onju-voice/master/images/copper.png&#34; width=&#34;48%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/justLV/onju-voice/master/images/render.png&#34; width=&#34;48%&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://365.altium.com/files/C44B8519-69BA-464B-A221-24D527B89E2C&#34;&gt;Schematics &amp;amp; PCB preview&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;PCB&#39;s will be made available from Crowd Supply (&lt;a href=&#34;https://www.crowdsupply.com/onju/onju-voice&#34;&gt;sign up link&lt;/a&gt;), to leverage their fulfillment expertize and bulk ordering.&lt;/p&gt; &#xA;&lt;p&gt;I will be sharing more detailed instructions for replacement.&lt;/p&gt; &#xA;&lt;p&gt;Replacement gaskets for the microphone &amp;amp; LED&#39;s can be made using &lt;a href=&#34;https://www.amazon.com/gp/product/B07KCJ31J9&#34;&gt;adhesive foam&lt;/a&gt; and a &lt;a href=&#34;https://www.amazon.com/gp/product/B087D2Z43F&#34;&gt;punch set&lt;/a&gt;) for example&lt;/p&gt; &#xA;&lt;h2&gt;‚ùìQuestions&lt;/h2&gt; &#xA;&lt;h3&gt;Does this replace my Google Nest Mini?&lt;/h3&gt; &#xA;&lt;p&gt;While this replicates the interfaces of the Google Nest Mini, don‚Äôt expect this to be a 1:1 replacement, for e.g. it is not intended to be a music playback device (although there is probably no reason it couldn‚Äôt be developed to be used as such). It‚Äôs also worth re-iterating that like the Google Nest Mini, this requires a separate server, although this can be in your home running local models instead of in a Google datacenter. &lt;strong&gt;The original is well tested, maintained, certified and works out the box, while this is essentially a dev board with some neat examples for you to build on top of&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;What if I don‚Äôt have a Google Nest Mini but still want to use this?&lt;/h3&gt; &#xA;&lt;p&gt;Fortunately they‚Äôre still being sold, you may find deals for &amp;lt;$40 which is pretty good for the quality of speaker and form factor. I picked up quite a few from eBay, just make sure you get the 2nd gen.&lt;/p&gt; &#xA;&lt;p&gt;The adventurous can get try replacement shells from &lt;a href=&#34;https://www.aliexpress.us/item/3256803723188315.html&#34;&gt;AliExpress&lt;/a&gt; for e.g., but you‚Äôll still need a base, power input, mute switch, speaker &amp;amp; mount, capacitive touch panels, and replacement gaskets etc. A hero out there could design a custom enclosure that fits an off-the-shelf speaker.&lt;/p&gt; &#xA;&lt;h3&gt;But I‚Äôm really impatient and want to get hacking away! What can I do?&lt;/h3&gt; &#xA;&lt;p&gt;a) if you can commit to making significant contributions to the codebase and/or major contributions to the board design or RF review, we may be able to make early samples available&lt;/p&gt; &#xA;&lt;p&gt;b) if you don‚Äôt need the form factor, don‚Äôt mind rolling up my sleeves, and have some HW experience, you can breadboard it out with readily available components until you can get your hands on an order. Here are the components that should be able to get a demo running (üå∏ Adafruit link for convenience but shop around wherever you‚Äôd like)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ESP32-S3 devboard, ideally w/ PSRAM (e.g. &lt;a href=&#34;https://www.adafruit.com/product/5700&#34;&gt;QT Py S3&lt;/a&gt; or &lt;a href=&#34;https://www.adafruit.com/product/5364&#34;&gt;ESP32-S3&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.adafruit.com/product/3421&#34;&gt;Microphone&lt;/a&gt; (only need 1 for the Arduino implementation, ensure it&#39;s a SPH0645 to limit debugging)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.adafruit.com/product/3006&#34;&gt;Amplifier&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.adafruit.com/product/1313&#34;&gt;Speaker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.adafruit.com/product/1426&#34;&gt;Neopixel LED strip&lt;/a&gt; - just set the firmware to the correct #&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.adafruit.com/product/3314&#34;&gt;Breadboard &amp;amp; wire kit&lt;/a&gt; (you can use protruding pieces of wire for cap touch)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You&#39;ll need to update the &lt;code&gt;custom_boards.h&lt;/code&gt; with your pin mapping&lt;/p&gt; &#xA;&lt;h2&gt;&lt;strong&gt;üçê PR&#39;s, issues, suggestions &amp;amp; general feedback welcome!üè°&lt;/strong&gt;&lt;/h2&gt;</summary>
  </entry>
  <entry>
    <title>BerriAI/litellm</title>
    <updated>2023-08-20T01:50:16Z</updated>
    <id>tag:github.com,2023-08-20:/BerriAI/litellm</id>
    <link href="https://github.com/BerriAI/litellm" rel="alternate"></link>
    <summary type="html">&lt;p&gt;lightweight package to simplify LLM API calls - Azure, OpenAI, Cohere, Anthropic, Replicate. Manages input/output translation&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;em&gt;üöÖ litellm&lt;/em&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/litellm/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/litellm.svg?sanitize=true&#34; alt=&#34;PyPI Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/litellm/0.1.1/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/stable%20version-v0.1.424-blue?color=green&amp;amp;link=https://pypi.org/project/litellm/0.1.1/&#34; alt=&#34;PyPI Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://dl.circleci.com/status-badge/redirect/gh/BerriAI/litellm/tree/main&#34;&gt;&lt;img src=&#34;https://dl.circleci.com/status-badge/img/gh/BerriAI/litellm/tree/main.svg?style=svg&#34; alt=&#34;CircleCI&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/pypi/dm/litellm&#34; alt=&#34;Downloads&#34;&gt; &lt;a href=&#34;https://github.com/BerriAI/litellm&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%20%F0%9F%9A%85%20liteLLM-OpenAI%7CAzure%7CAnthropic%7CPalm%7CCohere%7CReplicate%7CHugging%20Face-blue?color=green&#34; alt=&#34;litellm&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/wuPM9dRgDw&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/wuPM9dRgDw&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;a light package to simplify calling OpenAI, Azure, Cohere, Anthropic, Huggingface API Endpoints. It manages:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;translating inputs to the provider&#39;s completion and embedding endpoints&lt;/li&gt; &#xA; &lt;li&gt;guarantees &lt;a href=&#34;https://litellm.readthedocs.io/en/latest/output/&#34;&gt;consistent output&lt;/a&gt;, text responses will always be available at &lt;code&gt;[&#39;choices&#39;][0][&#39;message&#39;][&#39;content&#39;]&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;exception mapping - common exceptions across providers are mapped to the &lt;a href=&#34;https://help.openai.com/en/articles/6897213-openai-library-error-types-guidance&#34;&gt;OpenAI exception types&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;usage&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/completion/supported&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;None&#34; src=&#34;https://img.shields.io/badge/Supported_LLMs-100000?style=for-the-badge&amp;amp;logo=None&amp;amp;logoColor=000000&amp;amp;labelColor=000000&amp;amp;color=8400EA&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Demo - &lt;a href=&#34;https://litellm.ai/playground&#34;&gt;https://litellm.ai/playground&lt;/a&gt; &lt;br&gt; Read the docs - &lt;a href=&#34;https://docs.litellm.ai/docs/&#34;&gt;https://docs.litellm.ai/docs/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;quick start&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install litellm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from litellm import completion&#xA;&#xA;## set ENV variables&#xA;os.environ[&#34;OPENAI_API_KEY&#34;] = &#34;openai key&#34;&#xA;os.environ[&#34;COHERE_API_KEY&#34;] = &#34;cohere key&#34;&#xA;&#xA;messages = [{ &#34;content&#34;: &#34;Hello, how are you?&#34;,&#34;role&#34;: &#34;user&#34;}]&#xA;&#xA;# openai call&#xA;response = completion(model=&#34;gpt-3.5-turbo&#34;, messages=messages)&#xA;&#xA;# cohere call&#xA;response = completion(model=&#34;command-nightly&#34;, messages)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Code Sample: &lt;a href=&#34;https://colab.research.google.com/drive/1gR3pY-JzDZahzpVdbGBtrNGDBmzUNJaJ?usp=sharing&#34;&gt;Getting Started Notebook&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Stable version&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install litellm==0.1.424&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Streaming Queries&lt;/h2&gt; &#xA;&lt;p&gt;liteLLM supports streaming the model response back, pass &lt;code&gt;stream=True&lt;/code&gt; to get a streaming iterator in response. Streaming is supported for OpenAI, Azure, Anthropic, Huggingface models&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;response = completion(model=&#34;gpt-3.5-turbo&#34;, messages=messages, stream=True)&#xA;for chunk in response:&#xA;    print(chunk[&#39;choices&#39;][0][&#39;delta&#39;])&#xA;&#xA;# claude 2&#xA;result = completion(&#39;claude-2&#39;, messages, stream=True)&#xA;for chunk in result:&#xA;  print(chunk[&#39;choices&#39;][0][&#39;delta&#39;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;support / talk with founders&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://calendly.com/d/4mp-gd3-k5k/berriai-1-1-onboarding-litellm-hosted-version&#34;&gt;Our calendar üëã&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.gg/wuPM9dRgDw&#34;&gt;Community Discord üí≠&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Our numbers üìû +1 (770) 8783-106 / ‚Ä≠+1 (412) 618-6238‚Ä¨&lt;/li&gt; &#xA; &lt;li&gt;Our emails ‚úâÔ∏è &lt;a href=&#34;mailto:ishaan@berri.ai&#34;&gt;ishaan@berri.ai&lt;/a&gt; / &lt;a href=&#34;mailto:krrish@berri.ai&#34;&gt;krrish@berri.ai&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;why did we build this&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Need for simplicity&lt;/strong&gt;: Our code started to get extremely complicated managing &amp;amp; translating calls between Azure, OpenAI, Cohere&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>p0p4k/vits2_pytorch</title>
    <updated>2023-08-20T01:50:16Z</updated>
    <id>tag:github.com,2023-08-20:/p0p4k/vits2_pytorch</id>
    <link href="https://github.com/p0p4k/vits2_pytorch" rel="alternate"></link>
    <summary type="html">&lt;p&gt;unofficial vits2-TTS implementation in pytorch&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;del&gt;[this is a work in progress, feel free to contribute! Model will be ready if this line is removed]&lt;/del&gt; [Most of the code is ready and the model is ready to train. Hopefully, someone from the community can share some results ASAP! Thanks!]&lt;/p&gt; &#xA;&lt;h1&gt;Sample audio&lt;/h1&gt; &#xA;&lt;p&gt;(Training is in progress.)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;(08/20/2023) - Added sample audio @42k steps. &lt;a href=&#34;https://raw.githubusercontent.com/p0p4k/vits2_pytorch/main/resources/test.wav&#34;&gt;ljspeech-nosdp&lt;/a&gt; ; &lt;a href=&#34;https://github.com/p0p4k/vits2_pytorch/discussions/12&#34;&gt;tensorboard&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/p0p4k/vits2_pytorch/pull/10#issuecomment-1682307529&#34;&gt;vietnamese samples&lt;/a&gt; Thanks to &lt;a href=&#34;https://github.com/ductho9799&#34;&gt;@ductho9799&lt;/a&gt; for sharing!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;VITS2: Improving Quality and Efficiency of Single-Stage Text-to-Speech with Adversarial Learning and Architecture Design&lt;/h1&gt; &#xA;&lt;h3&gt;Jungil Kong, Jihoon Park, Beomjeong Kim, Jeongmin Kim, Dohee Kong, Sangjin Kim&lt;/h3&gt; &#xA;&lt;p&gt;Unofficial implementation of the &lt;a href=&#34;https://arxiv.org/abs/2307.16430&#34;&gt;VITS2 paper&lt;/a&gt;, sequel to &lt;a href=&#34;https://arxiv.org/abs/2106.06103&#34;&gt;VITS paper&lt;/a&gt;. (thanks to the authors for their work!)&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/p0p4k/vits2_pytorch/main/resources/image.png&#34; alt=&#34;Alt text&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Single-stage text-to-speech models have been actively studied recently, and their results have outperformed two-stage pipeline systems. Although the previous single-stage model has made great progress, there is room for improvement in terms of its intermittent unnaturalness, computational efficiency, and strong dependence on phoneme conversion. In this work, we introduce VITS2, a single-stage text-to-speech model that efficiently synthesizes a more natural speech by improving several aspects of the previous work. We propose improved structures and training mechanisms and present that the proposed methods are effective in improving naturalness, similarity of speech characteristics in a multi-speaker model, and efficiency of training and inference. Furthermore, we demonstrate that the strong dependence on phoneme conversion in previous works can be significantly reduced with our method, which allows a fully end-toend single-stage approach.&lt;/p&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We will build this repo based on the &lt;a href=&#34;https://github.com/jaywalnut310/vits&#34;&gt;VITS repo&lt;/a&gt;. Currently I am adding vits2 changes in the &#39;notebooks&#39; folder. The goal is to make this model easier to transfer learning from VITS pretrained model!&lt;/li&gt; &#xA; &lt;li&gt;(08-17-2023) - The authors were really kind to guide me through the paper and answer my questions. I am open to discuss any changes or answer questions regarding the implementation. Please feel free to open an issue or contact me directly.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Jupyter Notebook for initial experiments&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; check the &#39;notebooks&#39; folder&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;pre-requisites&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Python &amp;gt;= 3.6&lt;/li&gt; &#xA; &lt;li&gt;&lt;del&gt;Now supports Pytorch version 2.0&lt;/del&gt; (08/17/2023) - Tested on Pytorch version 1.13.1 with Google Colab and LambdaLabs cloud.&lt;/li&gt; &#xA; &lt;li&gt;Clone this repository&lt;/li&gt; &#xA; &lt;li&gt;Install python requirements. Please refer &lt;a href=&#34;https://raw.githubusercontent.com/p0p4k/vits2_pytorch/main/requirements.txt&#34;&gt;requirements.txt&lt;/a&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;You may need to install espeak first: &lt;code&gt;apt-get install espeak&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;Download datasets &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;Download and extract the LJ Speech dataset, then rename or create a link to the dataset folder: &lt;code&gt;ln -s /path/to/LJSpeech-1.1/wavs DUMMY1&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;For mult-speaker setting, download and extract the VCTK dataset, and downsample wav files to 22050 Hz. Then rename or create a link to the dataset folder: &lt;code&gt;ln -s /path/to/VCTK-Corpus/downsampled_wavs DUMMY2&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;Build Monotonic Alignment Search and run preprocessing if you use your own datasets.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Cython-version Monotonoic Alignment Search&#xA;cd monotonic_align&#xA;python setup.py build_ext --inplace&#xA;&#xA;# Preprocessing (g2p) for your own datasets. Preprocessed phonemes for LJ Speech and VCTK have been already provided.&#xA;# python preprocess.py --text_index 1 --filelists filelists/ljs_audio_text_train_filelist.txt filelists/ljs_audio_text_val_filelist.txt filelists/ljs_audio_text_test_filelist.txt &#xA;# python preprocess.py --text_index 2 --filelists filelists/vctk_audio_sid_text_train_filelist.txt filelists/vctk_audio_sid_text_val_filelist.txt filelists/vctk_audio_sid_text_test_filelist.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;How to run (dry-run)&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;model forward pass (dry-run)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from models import SynthesizerTrn&#xA;&#xA;net_g = SynthesizerTrn(&#xA;    n_vocab=256,&#xA;    spec_channels=80, # &amp;lt;--- vits2 parameter (changed from 513 to 80)&#xA;    segment_size=8192,&#xA;    inter_channels=192,&#xA;    hidden_channels=192,&#xA;    filter_channels=768,&#xA;    n_heads=2,&#xA;    n_layers=6,&#xA;    kernel_size=3,&#xA;    p_dropout=0.1,&#xA;    resblock=&#34;1&#34;, &#xA;    resblock_kernel_sizes=[3, 7, 11],&#xA;    resblock_dilation_sizes=[[1, 3, 5], [1, 3, 5], [1, 3, 5]],&#xA;    upsample_rates=[8, 8, 2, 2],&#xA;    upsample_initial_channel=512,&#xA;    upsample_kernel_sizes=[16, 16, 4, 4],&#xA;    n_speakers=0,&#xA;    gin_channels=0,&#xA;    use_sdp=True, &#xA;    use_transformer_flows=True, # &amp;lt;--- vits2 parameter&#xA;    transformer_flow_type=&#34;fft&#34;, # &amp;lt;--- vits2 parameter (choose from &#34;pre_conv&#34;,&#34;fft&#34;,&#34;mono_layer&#34;)&#xA;    use_spk_conditioned_encoder=True, # &amp;lt;--- vits2 parameter&#xA;    use_noise_scaled_mas=True, # &amp;lt;--- vits2 parameter&#xA;)&#xA;&#xA;x = torch.LongTensor([[1, 2, 3],[4, 5, 6]]) # token ids&#xA;x_lengths = torch.LongTensor([3, 2]) # token lengths&#xA;y = torch.randn(2, 80, 100) # mel spectrograms&#xA;y_lengths = torch.Tensor([100, 80]) # mel spectrogram lengths&#xA;&#xA;net_g(&#xA;    x=x,&#xA;    x_lengths=x_lengths,&#xA;    y=y,&#xA;    y_lengths=y_lengths,&#xA;)&#xA;&#xA;# calculate loss and backpropagate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training Example&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# LJ Speech&#xA;# python train.py -c configs/vits2_ljs_base.json -m ljs_base&#xA;python train.py -c configs/vits2_ljs_nosdp.json -m ljs_base # no-sdp; suggested till adversarial duration predictor is added&#xA;&#xA;# VCTK&#xA;python train_ms.py -c configs/vits2_vctk_base.json -m vctk_base&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Updates, TODOs, features and notes&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;note - duration predictor is not adversarial yet. In my earlier experiments with VITS-1, I used deterministic duration predictor (no-sdp) and found that it is quite good. So, I am not sure if adversarial duration predictor is necessary. But, I will add it sooner or later if it is necessary. Also, I want to combine parallel tacotron-2 and naturalspeech-1&#39;s learnable upsampling layer to remove MAS completely for E2E differentiable model.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;(08/20/2023) update 1 - Added sample audio @42k steps.&lt;/li&gt; &#xA; &lt;li&gt;(08/17/2023) update 5 - Does not support pytorch2.0 anymore. Please use pytorch1.13.1 ( I tried on Google Colab and it works fine; will update with a Colab notebook soon!)&lt;/li&gt; &#xA; &lt;li&gt;(08/17/2023) update 4 - Fixed multi-spk DataLoader&lt;/li&gt; &#xA; &lt;li&gt;(08/17/2023) update 3 - QOL changes to generate mel spec from existing lin spec. Updated inference.ipynb.&lt;/li&gt; &#xA; &lt;li&gt;(08/17/2023) update 2 - hotfix for &#34;use_mel_posterior_encoder&#34; flag in config file. Should fix &lt;a href=&#34;https://github.com/p0p4k/vits2_pytorch/issues/8&#34;&gt;#8&lt;/a&gt; and &lt;a href=&#34;https://github.com/p0p4k/vits2_pytorch/issues/9&#34;&gt;#9&lt;/a&gt;. Will do a if-else cleanup later.&lt;/li&gt; &#xA; &lt;li&gt;(08/17/2023) update 1 - After some discussions with the authors, I implemented &#34;mono-layer&#34; transformer flow which seems to be the closest to what they intend. It is a single layer transformer flow used as the first layer before the traditional conv-residual-flows. Need to experiment to check the best transformer flow type of the three. (pre_conv, fft, mono_layer). But, each of the layers serves similar purpose of long range dependency using attention.&lt;/li&gt; &#xA; &lt;li&gt;(08/10/2023) update 1 - updated multi_GPU training, &lt;del&gt;support pytorch2.0&lt;/del&gt; &lt;a href=&#34;https://github.com/p0p4k/vits2_pytorch/pull/5&#34;&gt;#5&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;(08/09/2023) update - Corrected MAS with noise_scale and updated train_ms.py, train.py (thanks to &lt;a href=&#34;https://github.com/KdaiP&#34;&gt;@KdaiP&lt;/a&gt; for testing and pointing out the bug in MAS)&lt;/li&gt; &#xA; &lt;li&gt;(08/08/2023) update 2 - Changed data_utils.py to take in &#34;use_mel_posterior_encoder&#34; flag.&lt;/li&gt; &#xA; &lt;li&gt;(08/08/2023) update 1 - Added &#34;use_noise_scaled_mas&#34; flag in config file. Added sanity checks in notebooks. Everything except adverserial duration predictor is ready to train.&lt;/li&gt; &#xA; &lt;li&gt;(08/072023) update 2 - transformer_flow_type &#34;fft&#34; and &#34;pre_conv&#34; added. &lt;a href=&#34;https://github.com/lexkoro&#34;&gt;@lexkoro&lt;/a&gt; suggested &#34;fft&#34; transformer flow is better than &#34;pre_conv&#34; transformer flow in his intial experiments.&lt;/li&gt; &#xA; &lt;li&gt;(08/07/2023 update 1 - vits2_vctk_base.json and vits2_ljs_base.json are ready to train; multi-speaker and single-speaker models respectively)&lt;/li&gt; &#xA; &lt;li&gt;(08/06/2023 update - dry run is ready; duration predictor will complete within next week)&lt;/li&gt; &#xA; &lt;li&gt;(08/05/2023 update - everything except the duration predictor is ready to train and we can expect some improvement from VITS1)&lt;/li&gt; &#xA; &lt;li&gt;(08/04/2023 update - initial codebaase is ready; paper is being read)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Duration predictor (fig 1a)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Added LSTM discriminator to duration predictor in notebook.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Added adversarial loss to duration predictor (TODO)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Monotonic Alignment Search with Gaussian Noise added in &#39;notebooks&#39; folder; need expert verification (Section 2.2)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Added &#34;use_noise_scaled_mas&#34; flag in config file. Choose from True or False; updates noise while training based on number of steps and never goes below 0.0&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Update models.py/train.py/train_ms.py&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Update config files (vits2_vctk_base.json; vits2_ljs_base.json)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Update losses.py (TODO)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Transformer block in the normalizing flow (fig 1b)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Added transformer block to the normalizing flow in notebook. There are three types of transformer blocks: pre-convolution (my implementation), FFT (from &lt;a href=&#34;https://github.com/svc-develop-team/so-vits-svc/commit/fc8336fffd40c39bdb225c1b041ab4dd15fac4e9&#34;&gt;so-vits-svc&lt;/a&gt; repo) and mono-layer.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Added &#34;transformer_flow_type&#34; flag in config file. Choose from &#34;pre_conv&#34; or &#34;fft&#34; or &#34;mono_layer&#34;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Added layers and blocks in models.py (ResidualCouplingTransformersLayer, ResidualCouplingTransformersBlock, FFTransformerCouplingLayer, MonoTransformerFlowLayer)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add in config file (vits2_ljs_base.json; can be turned on using &#34;use_transformer_flows&#34; flag)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Speaker-conditioned text encoder (fig 1c)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Added speaker embedding to the text encoder in notebook.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Added speaker embedding to the text encoder in models.py (TextEncoder; backward compatible with VITS)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add in config file (vits2_ljs_base.json; can be turned on using &#34;use_spk_conditioned_encoder&#34; flag)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Mel spectrogram posterior encoder (Section 3)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Added mel spectrogram posterior encoder in notebook.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Added mel spectrogram posterior encoder in train.py&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Addded new config file (vits2_ljs_base.json; can be turned on using &#34;use_mel_posterior_encoder&#34; flag)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Updated &#39;data_utils.py&#39; to use the &#34;use_mel_posterior_encoder&#34; flag for vits2&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Training scripts&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Added vits2 flags to train.py (single-speaer model)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Added vits2 flags to train_ms.py (multi-speaker model)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Special mentions&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/erogol&#34;&gt;@erogol&lt;/a&gt; for quick feedback and guidance. (Please check his awesome &lt;a href=&#34;https://github.com/coqui-ai/TTS&#34;&gt;CoquiTTS&lt;/a&gt; repo).&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lexkoro&#34;&gt;@lexkoro&lt;/a&gt; for discussions and help with the prototype training.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/manmay-nakhashi&#34;&gt;@manmay-nakhashi&lt;/a&gt; for discussions and help with the code.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/athenasaurav&#34;&gt;@athenasaurav&lt;/a&gt; for offering GPU support for training.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>