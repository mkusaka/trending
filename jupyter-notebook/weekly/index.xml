<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-09-03T01:53:52Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>tencent-ailab/IP-Adapter</title>
    <updated>2023-09-03T01:53:52Z</updated>
    <id>tag:github.com,2023-09-03:/tencent-ailab/IP-Adapter</id>
    <link href="https://github.com/tencent-ailab/IP-Adapter" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The image prompt adapter is designed to enable a pretrained text-to-image diffusion model to generate images with image prompt.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://ip-adapter.github.io&#34;&gt;&lt;strong&gt;Project Page&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;|&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2308.06721&#34;&gt;&lt;strong&gt;Paper (ArXiv)&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;we present IP-Adapter, an effective and lightweight adapter to achieve image prompt capability for the pre-trained text-to-image diffusion models. An IP-Adapter with only 22M parameters can achieve comparable or even better performance to a fine-tuned image prompt model. IP-Adapter can be generalized not only to other custom models fine-tuned from the same base model, but also to controllable generation using existing controllable tools. Moreover, the image prompt can also work well with the text prompt to accomplish multimodal image generation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/assets/figs/fig1.png&#34; alt=&#34;arch&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Release&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2023/8/30] üî• Add an IP-Adapter with face image as prompt. The demo is &lt;a href=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/ip_adapter-plus-face_demo.ipynb&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2023/8/29] üî• Release the training code.&lt;/li&gt; &#xA; &lt;li&gt;[2023/8/23] üî• Add code and models of IP-Adapter with fine-grained features. The demo is &lt;a href=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/ip_adapter-plus_demo.ipynb&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2023/8/18] üî• Add code and models for &lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0&#34;&gt;SDXL 1.0&lt;/a&gt;. The demo is &lt;a href=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/ip_adapter_sdxl_demo.ipynb&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2023/8/16] üî• We release the code and models.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Dependencies&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;diffusers &amp;gt;= 0.19.3&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Download Models&lt;/h2&gt; &#xA;&lt;p&gt;you can download models from &lt;a href=&#34;https://huggingface.co/h94/IP-Adapter&#34;&gt;here&lt;/a&gt;. To run the demo, you should also download the following models:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/runwayml/stable-diffusion-v1-5&#34;&gt;runwayml/stable-diffusion-v1-5&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/sd-vae-ft-mse&#34;&gt;stabilityai/sd-vae-ft-mse&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/SG161222/Realistic_Vision_V4.0_noVAE&#34;&gt;SG161222/Realistic_Vision_V4.0_noVAE&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/lllyasviel&#34;&gt;ControlNet models&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to Use&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/ip_adapter_demo.ipynb&#34;&gt;&lt;strong&gt;ip_adapter_demo&lt;/strong&gt;&lt;/a&gt;: image variations, image-to-image, and inpainting with image prompt.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/tencent-ailab/IP-Adapter/blob/main/ip_adapter_demo.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;ip_adapter_demo&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/assets/demo/image_variations.jpg&#34; alt=&#34;image variations&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/assets/demo/image-to-image.jpg&#34; alt=&#34;image-to-image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/assets/demo/inpainting.jpg&#34; alt=&#34;inpainting&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/ip_adapter_controlnet_demo.ipynb&#34;&gt;&lt;strong&gt;ip_adapter_controlnet_demo&lt;/strong&gt;&lt;/a&gt;: structural generation with image prompt.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/tencent-ailab/IP-Adapter/blob/main/ip_adapter_controlnet_demo.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;ip_adapter_controlnet_demo&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/assets/demo/structural_cond.jpg&#34; alt=&#34;structural_cond&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/ip_adapter_multimodal_prompts_demo.ipynb&#34;&gt;&lt;strong&gt;ip_adapter_multimodal_prompts_demo&lt;/strong&gt;&lt;/a&gt;: generation with multimodal prompts.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/tencent-ailab/IP-Adapter/blob/main/ip_adapter_multimodal_prompts_demo.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;ip_adapter_multimodal_prompts_demo&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/assets/demo/multi_prompts.jpg&#34; alt=&#34;multi_prompts&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/ip_adapter-plus_demo.ipynb&#34;&gt;&lt;strong&gt;ip_adapter-plus_demo&lt;/strong&gt;&lt;/a&gt;: the demo of IP-Adapter with fine-grained features.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/assets/demo/ip_adpter_plus_image_variations.jpg&#34; alt=&#34;ip_adpter_plus_image_variations&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/assets/demo/ip_adpter_plus_multi.jpg&#34; alt=&#34;ip_adpter_plus_multi&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/ip_adapter-plus-face_demo.ipynb&#34;&gt;&lt;strong&gt;ip_adapter-plus-face_demo&lt;/strong&gt;&lt;/a&gt;: generation with face image as prompt.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/assets/demo/sd15_face.jpg&#34; alt=&#34;ip_adpter_plus_face&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you only use the image prompt, you can set the &lt;code&gt;scale=1.0&lt;/code&gt; and &lt;code&gt;text_prompt=&#34;&#34;&lt;/code&gt;(or some generic text prompts, e.g. &#34;best quality&#34;, you can also use any negative text prompt). If you lower the &lt;code&gt;scale&lt;/code&gt;, more diverse images can be generated, but they may not be as consistent with the image prompt.&lt;/li&gt; &#xA; &lt;li&gt;For multimodal prompts, you can adjust the &lt;code&gt;scale&lt;/code&gt; to get the best results. In most cases, setting &lt;code&gt;scale=0.5&lt;/code&gt; can get good results. For the version of SD 1.5, we recommend using community models to generate good images.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to Train&lt;/h2&gt; &#xA;&lt;p&gt;For training, you should install &lt;a href=&#34;https://github.com/huggingface/accelerate&#34;&gt;accelerate&lt;/a&gt; and make your own dataset into a json file.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;accelerate launch --num_processes 8 --multi_gpu --mixed_precision &#34;fp16&#34; \&#xA;  tutorial_train.py \&#xA;  --pretrained_model_name_or_path=&#34;runwayml/stable-diffusion-v1-5/&#34; \&#xA;  --image_encoder_path=&#34;{image_encoder_path}&#34; \&#xA;  --data_json_file=&#34;{data.json}&#34; \&#xA;  --data_root_path=&#34;{image_path}&#34; \&#xA;  --mixed_precision=&#34;fp16&#34; \&#xA;  --resolution=512 \&#xA;  --train_batch_size=8 \&#xA;  --dataloader_num_workers=4 \&#xA;  --learning_rate=1e-04 \&#xA;  --weight_decay=0.01 \&#xA;  --output_dir=&#34;{output_dir}&#34; \&#xA;  --save_steps=10000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This project strives to positively impact the domain of AI-driven image generation. Users are granted the freedom to create images using this tool, but they are expected to comply with local laws and utilize it in a responsible manner. &lt;strong&gt;The developers do not assume any responsibility for potential misuse by users.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find IP-Adapter useful for your research and applications, please cite using this BibTeX:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{ye2023ip-adapter,&#xA;  title={IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models},&#xA;  author={Ye, Hu and Zhang, Jun and Liu, Sibo and Han, Xiao and Yang, Wei},&#xA;  booktitle={arXiv preprint arxiv:2308.06721},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>KevinWang676/Bark-Voice-Cloning</title>
    <updated>2023-09-03T01:53:52Z</updated>
    <id>tag:github.com,2023-09-03:/KevinWang676/Bark-Voice-Cloning</id>
    <link href="https://github.com/KevinWang676/Bark-Voice-Cloning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Bark Voice Cloning and Voice Cloning for Chinese Speech&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Bark Voice Cloning üê∂ &amp;amp; Voice Cloning for Chinese Speech üé∂&lt;/h1&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/KevinWang676/Bark-Voice-Cloning/raw/main/README_zh.md&#34;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h2&gt;1Ô∏è‚É£ Bark Voice Cloning&lt;/h2&gt; &#xA;&lt;p&gt;Based on &lt;a href=&#34;https://github.com/C0untFloyd/bark-gui&#34;&gt;bark-gui&lt;/a&gt;. Thanks to &lt;a href=&#34;https://github.com/C0untFloyd&#34;&gt;C0untFloyd&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Quick start: &lt;a href=&#34;https://colab.research.google.com/github/KevinWang676/Bark-Voice-Cloning/blob/main/Bark_Voice_Cloning.ipynb&#34;&gt;Colab Notebook&lt;/a&gt; ‚ö°&lt;/p&gt; &#xA;&lt;p&gt;HuggingFace Demo: &lt;a href=&#34;https://huggingface.co/spaces/kevinwang676/Bark-with-Voice-Cloning&#34;&gt;Bark Voice Cloning&lt;/a&gt; ü§ó&lt;/p&gt; &#xA;&lt;p&gt;If you would like to run the code locally, remember to replace the original path &lt;code&gt;/content/Bark-Voice-Cloning/bark/assets/prompts/file.npz&lt;/code&gt; with the path of &lt;code&gt;file.npz&lt;/code&gt; in your own computer.&lt;/p&gt; &#xA;&lt;h3&gt;If you like the quick start, please star this repository. ‚≠ê‚≠ê‚≠ê&lt;/h3&gt; &#xA;&lt;h2&gt;Easy to use:&lt;/h2&gt; &#xA;&lt;p&gt;(1) First upload audio for voice cloning and click &lt;code&gt;Create Voice&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/KevinWang676/Bark-Voice-Cloning/assets/126712357/65e2b695-f529-4fb5-9549-4e86e6a4d8b2&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;(2) Choose the option called &#34;file&#34; in &lt;code&gt;Voice&lt;/code&gt; if you&#39;d like to use voice cloning.&lt;/p&gt; &#xA;&lt;p&gt;(3) Click &lt;code&gt;Generate&lt;/code&gt;. Done!&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/KevinWang676/Bark-Voice-Cloning/assets/126712357/20911e37-768d-47d5-bb86-d12a3ab04c5d&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;2Ô∏è‚É£ Voice Cloning for Chinese Speech&lt;/h2&gt; &#xA;&lt;p&gt;We want to point out that &lt;a href=&#34;https://github.com/suno-ai/bark&#34;&gt;Bark&lt;/a&gt; is very good at generating English speech but relatively poor at generating Chinese speech. So we&#39;d like to adopt another approach, which is called &lt;a href=&#34;https://www.modelscope.cn/models/speech_tts/speech_sambert-hifigan_tts_zh-cn_multisp_pretrain_16k/summary&#34;&gt;SambertHifigan&lt;/a&gt;, to realizing voice cloning for Chinese speech. Please check out our &lt;a href=&#34;https://colab.research.google.com/github/KevinWang676/Bark-Voice-Cloning/blob/main/Voice_Cloning_for_Chinese_Speech.ipynb&#34;&gt;Colab Notebook&lt;/a&gt; for the implementation.&lt;/p&gt; &#xA;&lt;p&gt;Quick start: &lt;a href=&#34;https://colab.research.google.com/github/KevinWang676/Bark-Voice-Cloning/blob/main/Voice_Cloning_for_Chinese_Speech_v2.ipynb&#34;&gt;Colab Notebook&lt;/a&gt; ‚ö°&lt;/p&gt; &#xA;&lt;p&gt;HuggingFace demo: &lt;a href=&#34;https://huggingface.co/spaces/kevinwang676/Personal-TTS&#34;&gt;Voice Cloning for Chinese Speech&lt;/a&gt; ü§ó&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>MuhammadMoinFaisal/LargeLanguageModelsProjects</title>
    <updated>2023-09-03T01:53:52Z</updated>
    <id>tag:github.com,2023-09-03:/MuhammadMoinFaisal/LargeLanguageModelsProjects</id>
    <link href="https://github.com/MuhammadMoinFaisal/LargeLanguageModelsProjects" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Large Language Model Projects&lt;/p&gt;&lt;hr&gt;</summary>
  </entry>
</feed>