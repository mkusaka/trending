<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-16T01:58:14Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>LC1332/Chinese-alpaca-lora</title>
    <updated>2023-04-16T01:58:14Z</updated>
    <id>tag:github.com,2023-04-16:/LC1332/Chinese-alpaca-lora</id>
    <link href="https://github.com/LC1332/Chinese-alpaca-lora" rel="alternate"></link>
    <summary type="html">&lt;p&gt;骆驼:A Chinese finetuned instruction LLaMA. Developed by 陈启源 @ 华中师范大学 &amp; 李鲁鲁 @ 商汤科技 &amp; 冷子昂 @ 商汤科技&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;骆驼(Luotuo): Chinese-alpaca-lora&lt;/h1&gt; &#xA;&lt;p&gt;骆驼(Luotuo) is the Chinese pinyin(pronunciation) of camel&lt;/p&gt; &#xA;&lt;p&gt;Specifically, this repo is for vanilla Luotuo, which a Chinese finetuned instruction LLaMA, belongs the project &lt;a href=&#34;https://github.com/LC1332/Luotuo-Chinese-LLM&#34;&gt;骆驼(Luotuo)&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Project &lt;a href=&#34;https://github.com/LC1332/Luotuo-Chinese-LLM&#34;&gt;骆驼(Luotuo)&lt;/a&gt; was found by 冷子昂 @ 商汤科技, 陈启源 @ 华中师范大学(Junior Undergrad.) and 李鲁鲁 @ 商汤科技&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/LC1332/Luotuo-Chinese-LLM/raw/main/image/icon_luotuo.png&#34; alt=&#34;silk-magic-book&#34; width=&#34;300&#34;&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Now this repo will only contain the information about Vanilla-Luotuo, which Chinese finetuned on LLaMA, for other LLM story, will be gradually move to the Project &lt;a href=&#34;https://github.com/LC1332/Luotuo-Chinese-LLM&#34;&gt;骆驼(Luotuo)&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Please visit our home page repo &lt;a href=&#34;https://github.com/LC1332/Luotuo-Chinese-LLM&#34;&gt;https://github.com/LC1332/Luotuo-Chinese-LLM&lt;/a&gt; to see more information.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;对于更多信息，请访问我们的主页面 &lt;a href=&#34;https://github.com/LC1332/Luotuo-Chinese-LLM&#34;&gt;https://github.com/LC1332/Luotuo-Chinese-LLM&lt;/a&gt; , Chinese-alpaca-lora这个仓库将只用于存储LLaMA有关的内容。如果你希望一个更好的中文语言模型，参考主页中的驼铃项目。&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This is NOT an official product of SenseTime&lt;/p&gt; &#xA;&lt;p&gt;We named project in Luotuo(Camel) because both LLaMA and alpaca are all belongs to Artiodactyla-Camelidae(偶蹄目-骆驼科)&lt;/p&gt; &#xA;&lt;h2&gt;News [ &lt;a href=&#34;https://github.com/LC1332/Chinese-alpaca-lora/raw/main/data/previous_news.md&#34;&gt;...&lt;/a&gt; ]&lt;/h2&gt; &#xA;&lt;p&gt;[2023-4-4] For Luotuo, we are working on 1.0 and 1.3 version. Training with larger data, and fixing the Chinese tokenizer issue. We will try to align the Chinese performance, and conduct a more fair comparison between LLaMA and GLM model.&lt;/p&gt; &#xA;&lt;p&gt;[2023-3-30] We released Chinese Summarization Model, CamelBell-C (驼铃-C), try in this &lt;a href=&#34;https://colab.research.google.com/github/LC1332/Chinese-alpaca-lora/blob/main/notebook/TuoLingC_evaluation_code.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;. More result see in &lt;a href=&#34;https://github.com/LC1332/CamelBell-Chinese-LoRA&#34;&gt;CamelBell-repo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;[2023-3-27] We plan to train a &lt;a href=&#34;https://github.com/LC1332/CamelBell-Chinese-LoRA/raw/main/data/HarryPotter/ShortReport.md&#34;&gt;&lt;strong&gt;ChatHarryPotter&lt;/strong&gt;&lt;/a&gt;, we&#39;ve just finished the prelimiary experiment and have ver. 0.1 model, but it did not meet our expectation, see this &lt;a href=&#34;https://github.com/LC1332/CamelBell-Chinese-LoRA/raw/main/data/HarryPotter/ShortReport.md&#34;&gt;report&lt;/a&gt;, and we are pursuing &lt;strong&gt;a Harry Potter enthusiast Pythoner to join&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;[2023-3-25] Luotuo-1.0 is in training! Thanks for all sponsors!&lt;/p&gt; &#xA;&lt;h2&gt;A Quick Start&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Colab Link&lt;/th&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;detail&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CamelBell quick evaluation&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/LC1332/CamelBell-Chinese-LoRA/blob/main/notebook/CamelBell_evaluation_code.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Tuoling specific Evaluation Code&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;A quick evaluation&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/LC1332/Chinese-alpaca-lora/blob/main/notebook/evaluation_code.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Evaluation code with standard HuggingFace pipeline&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Bot with Interface&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/LC1332/Chinese-alpaca-lora/blob/main/notebook/ChatLuotuo.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Interactive Chatting Bot using Gradio&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Training Code&lt;/td&gt; &#xA;   &lt;td&gt;To be released&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Training code, run on colab&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Data Translation&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/LC1332/Chinese-alpaca-lora/blob/main/notebook/translate_json_data.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Translation alpaca.json into Chinese&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Trained Model&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model Name&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Training Data and Setting&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/qychen/luotuo-lora-7b-0.1/tree/main&#34;&gt;luotuo-lora-7b-0.1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Trained on Translated alpaca 52k data&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/silk-road/luotuo-lora-7b-0.3&#34;&gt;luotuo-lora-7b-0.3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Trained on Translated alpaca 52k data + guanaco, 1 epoch&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;luotuo-lora-7b-0.9&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;(In Plan) cleaned alpaca 52k + full guanaco&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;luotuo-lora-7b-0.3 the performance seems have significant improvement than 0.1, even just after 1 epoch.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/LC1332/Chinese-alpaca-lora/main/image/0.3result_1epoch.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;But 1 epoch cost us 7 hours to train, which cost more than $10. We will keep continue tuning it later.&lt;/p&gt; &#xA;&lt;h2&gt;Qualitative Result(0.3)&lt;/h2&gt; &#xA;&lt;p&gt;Evaluation code are now in notebook/evaluation_code.ipynb&lt;/p&gt; &#xA;&lt;h3&gt;Good Examples&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;Input: 中国的首都在哪里？&#xA;Luotuo-Output: 中国的首都是北京。&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;Input: 麦当劳是哪个国家的？&#xA;Luotuo-Output: 美国。&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;Input: 2017年美国总统是谁&#xA;Luotuo-Output: 2017年美国总统是特朗普&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;Input: 华中师范大学在哪里&#xA;Luotuo-0.1-Output: 华中师范大学位于北京&#xA;Luotuo-0.3-Output: 华中师范大学在武汉市。&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;this example failed in 0.1&lt;/p&gt; &#xA;&lt;h3&gt;Bad Examples&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;Input: 商汤科技是做什么的？&#xA;Luotuo-0.1-Output: 商汤科技是一种技术，它用于创建互联网应用程序和服务。&#xA;Luotuo-0.3-Output: 商汤科技是一种技术，它可以用于创建新的产品和服务，以增加产品和服务的吸引力。它可以用于创建新的产品和服务，以增加产品和服务的吸引力。它可以用于创建新的产品和服务，以增加产品和服务的吸引力。它可以用于创建新的产品和服务，以增加产品和服务的吸引力。它可以用于创建新的产品和服务，以增加产品和服务的吸引力。它可以用于创建新的产品和服务，以增加产品和服务的吸引力。它可以用于创建新的&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;Input: 猫和狗的关系是什么&#xA;Luotuo-Output: 猫和狗之间的关系是友好的。它们通常在一起休息或玩耍。猫和狗之间的关系通常很好，它们通常在一起休息或玩耍。猫和狗之间的关系通常很好，它们通常在一起休息或玩耍。猫和狗之间的关系通常很好，它们通常在一起休息或玩耍。猫和狗之间的关系通常很好，它们通常在一起休息或玩耍。猫和狗之间的关系通常很好，它们通常在一起休息或玩耍。猫和狗之间的关系通常&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;We have tuned a Chinese LLaMA model baed on &lt;a href=&#34;https://ai.facebook.com/blog/large-language-model-llama-meta-ai/&#34;&gt;LLaMA&lt;/a&gt;, &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca&lt;/a&gt;, &lt;a href=&#34;https://github.com/tloen/alpaca-lora&#34;&gt;Alpaca LoRA&lt;/a&gt;, &lt;a href=&#34;https://github.com/22-hours/cabrita&#34;&gt;cabrita&lt;/a&gt;, &lt;a href=&#34;https://github.com/masa3141/japanese-alpaca-lora&#34;&gt;Japanese-Alpaca-LoRA&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The training code in in cleaning, if you are in very hurry, check the Japanese project and simply change the json training data file name.&lt;/p&gt; &#xA;&lt;h2&gt;Data&lt;/h2&gt; &#xA;&lt;p&gt;This is an inbuilding project&lt;/p&gt; &#xA;&lt;p&gt;The training code only made a slightly change on the Japanese-Alpaca-LoRA&lt;/p&gt; &#xA;&lt;p&gt;A. &lt;a href=&#34;https://huggingface.co/qychen/luotuo-lora-7b-0.1/tree/main&#34;&gt;0.1 version model&lt;/a&gt; was trained on translated data, which translate the &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca/raw/main/alpaca_data.json&#34;&gt;alpaca_data.json&lt;/a&gt; to Chinese using ChatGPT API. We paid around US $30-45 to translate the full dataset to chinese. Translated data is available. (&lt;a href=&#34;https://raw.githubusercontent.com/LC1332/Chinese-alpaca-lora/main/data/trans_chinese_alpaca_data.json&#34;&gt;trans_chinese_alpaca_data.json&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;B. We are also plan to consider the data in &lt;a href=&#34;https://guanaco-model.github.io/&#34;&gt;Guanaco&lt;/a&gt; hikariming&#39;s &lt;a href=&#34;https://github.com/hikariming/alpaca_chinese_dataset&#34;&gt;alpaca_chinese_dataset&lt;/a&gt; and carbonz0‘s &lt;a href=&#34;https://github.com/carbonz0/alpaca-chinese-dataset&#34;&gt;alpaca-chinese-dataset&lt;/a&gt;, may updated it into later version.&lt;/p&gt; &#xA;&lt;p&gt;We plan to upload two different models A and B, because the provider of B claim the clean data will bring significant improvement.&lt;/p&gt; &#xA;&lt;h2&gt;Sponsorships(赞助)&lt;/h2&gt; &#xA;&lt;p&gt;Top 3 Sponsors&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Time&lt;/th&gt; &#xA;   &lt;th&gt;Sponsor&lt;/th&gt; &#xA;   &lt;th&gt;Amount&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2023/3/28&lt;/td&gt; &#xA;   &lt;td&gt;张**&lt;/td&gt; &#xA;   &lt;td&gt;2000&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2023/3/25&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mobe1978&#34;&gt;肖**&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;520&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2023/3/24&lt;/td&gt; &#xA;   &lt;td&gt;*潇&lt;/td&gt; &#xA;   &lt;td&gt;518&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;balance = 5792 now. Detailed balance see in &lt;a href=&#34;https://raw.githubusercontent.com/LC1332/Chinese-alpaca-lora/main/data/Sponsorship_and_balance.md&#34;&gt;sponsorship_and_balance.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;这原本是我们的一个作业项目，我们原本计划训练到1.0为止。但是社区的热情超过了我们的想象。如果您愿意赞助我们的项目，可以&lt;/p&gt; &#xA;&lt;p&gt;扫描这个&lt;a href=&#34;https://s1.imagehub.cc/images/2023/03/23/fba44d198f0bb887089b4d8739363c0b.jpeg&#34;&gt;二维码&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;并且加这个&lt;a href=&#34;https://s1.imagehub.cc/images/2023/03/23/b69e4e47759132dd3d4bbafa7bd602aa.jpeg&#34;&gt;支付宝&lt;/a&gt;账号，留下您的姓名&lt;/p&gt; &#xA;&lt;p&gt;项目的资金流向将被公开，所有的资金将被用于数据的标注，训练算力的购买或者后续周边产品的发放。数据和算力的捐献也会一同总结在sponsorship的表格中。备用链接 &lt;a href=&#34;https://raw.githubusercontent.com/LC1332/Chinese-alpaca-lora/main/image/sponser_QR_code.jpeg&#34;&gt;二维码&lt;/a&gt; , &lt;a href=&#34;https://raw.githubusercontent.com/LC1332/Chinese-alpaca-lora/main/image/alipay_friend.jpeg&#34;&gt;支付宝&lt;/a&gt;账号&lt;/p&gt; &#xA;&lt;p&gt;This was originally an exercise project for us, and we originally planned to train until version 1.0. However, the enthusiasm of the community exceeded our expectations. If you are willing to sponsor our project, you can scan this &lt;a href=&#34;https://raw.githubusercontent.com/LC1332/Chinese-alpaca-lora/main/image/sponser_QR_code.jpeg&#34;&gt;QR code&lt;/a&gt; and add &lt;a href=&#34;https://raw.githubusercontent.com/LC1332/Chinese-alpaca-lora/main/image/alipay_friend.jpeg&#34;&gt;this Alipay account&lt;/a&gt;, leaving your name.&lt;/p&gt; &#xA;&lt;p&gt;All funds will be used for data annotation, purchase of training computing power, or distribution of subsequent peripheral products.&lt;/p&gt; &#xA;&lt;h2&gt;TODO and Be a Contributor&lt;/h2&gt; &#xA;&lt;p&gt;It seems that there are many follow-up tasks to be done after the basic version is completed. Many developers in the community have put forward more friendly suggestions, and I have put a longer TODO list in &lt;a href=&#34;https://raw.githubusercontent.com/LC1332/Chinese-alpaca-lora/main/data/TODO_list.md&#34;&gt;TODO_list.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;inbuilding project&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; translate alpaca json data into Chinese&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; finetuning with lora(model 0.1)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; release 0.1 model (model A)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; model to hugging face, GUI demo&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; train lora with more alpaca data(model 0.3)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; (In Processing) train lora with more alpaca data(model 0.9)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; clean training code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; write the second phase plan for Luotuo&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We plan to use this Luotuo project as the git repository for the entire Chinese LLM project. After the completion of the original Luotuo: LLaMA-LoRA, it will be migrated to Luotuo-vanilla. The CamelBell, Loulan, Silk-Road and other derivative Chinese language model projects will gradually be added to the Luotuo project.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Please cite the repo if you use the data or code in this repo.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{alpaca,&#xA;  author={Ziang Leng, Qiyuan Chen and Cheng Li},&#xA;  title = {Luotuo: An Instruction-following Chinese Language model, LoRA tuning on LLaMA},&#xA;  year = {2023},&#xA;  publisher = {GitHub},&#xA;  journal = {GitHub repository},&#xA;  howpublished = {\url{https://github.com/LC1332/Chinese-alpaca-lora}},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>EleutherAI/pythia</title>
    <updated>2023-04-16T01:58:14Z</updated>
    <id>tag:github.com,2023-04-16:/EleutherAI/pythia</id>
    <link href="https://github.com/EleutherAI/pythia" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;a href=&#34;https://arxiv.org/pdf/2304.01373.pdf&#34;&gt;Pythia: Interpreting Autoregressive Transformers Across Time and Scale&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;This repository is for EleutherAI&#39;s work-in-progress project &lt;em&gt;Pythia&lt;/em&gt; which combines interpretability analysis and scaling laws to understand how knowledge develops and evolves during training in autoregressive transformers. For detailed info on the models, their training, and their behavior, please see &lt;a href=&#34;https://arxiv.org/pdf/2304.01373.pdf&#34;&gt;our paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Models&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Params&lt;/th&gt; &#xA;   &lt;th&gt;n_layers&lt;/th&gt; &#xA;   &lt;th&gt;d_model&lt;/th&gt; &#xA;   &lt;th&gt;n_heads&lt;/th&gt; &#xA;   &lt;th&gt;d_head&lt;/th&gt; &#xA;   &lt;th&gt;Batch Size&lt;/th&gt; &#xA;   &lt;th&gt;Learning Rate&lt;/th&gt; &#xA;   &lt;th&gt;Checkpoints&lt;/th&gt; &#xA;   &lt;th&gt;Evaluations&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pythia-70M&lt;/td&gt; &#xA;   &lt;td&gt;6&lt;/td&gt; &#xA;   &lt;td&gt;512&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;64&lt;/td&gt; &#xA;   &lt;td&gt;2M&lt;/td&gt; &#xA;   &lt;td&gt;1e-3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/pythia-70m&#34;&gt;Here&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Ready&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pythia-70M-Deduped&lt;/td&gt; &#xA;   &lt;td&gt;6&lt;/td&gt; &#xA;   &lt;td&gt;512&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;64&lt;/td&gt; &#xA;   &lt;td&gt;2M&lt;/td&gt; &#xA;   &lt;td&gt;1e-3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/pythia-70m-deduped&#34;&gt;Here&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Ready&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pythia-160M&lt;/td&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;   &lt;td&gt;768&lt;/td&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;   &lt;td&gt;64&lt;/td&gt; &#xA;   &lt;td&gt;2M&lt;/td&gt; &#xA;   &lt;td&gt;6e-4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/pythia-160m&#34;&gt;Here&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Ready&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pythia-160M-Deduped&lt;/td&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;   &lt;td&gt;768&lt;/td&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;   &lt;td&gt;64&lt;/td&gt; &#xA;   &lt;td&gt;2M&lt;/td&gt; &#xA;   &lt;td&gt;6e-4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/pythia-160m-deduped&#34;&gt;Here&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Ready&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pythia-410M&lt;/td&gt; &#xA;   &lt;td&gt;24&lt;/td&gt; &#xA;   &lt;td&gt;1024&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;64&lt;/td&gt; &#xA;   &lt;td&gt;2M&lt;/td&gt; &#xA;   &lt;td&gt;3e-4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/pythia-410m&#34;&gt;Here&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Ready&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pythia-410M-Deduped&lt;/td&gt; &#xA;   &lt;td&gt;24&lt;/td&gt; &#xA;   &lt;td&gt;1024&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;64&lt;/td&gt; &#xA;   &lt;td&gt;2M&lt;/td&gt; &#xA;   &lt;td&gt;3e-4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/pythia-410m-deduped&#34;&gt;Here&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Ready&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pythia-1B&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;2048&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;256&lt;/td&gt; &#xA;   &lt;td&gt;2M&lt;/td&gt; &#xA;   &lt;td&gt;3e-4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/pythia-1b&#34;&gt;Here&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Ready&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pythia-1B-Deduped&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;2048&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;256&lt;/td&gt; &#xA;   &lt;td&gt;2M&lt;/td&gt; &#xA;   &lt;td&gt;3e-4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/pythia-1b-deduped&#34;&gt;Here&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Ready&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pythia-1.4B&lt;/td&gt; &#xA;   &lt;td&gt;24&lt;/td&gt; &#xA;   &lt;td&gt;2048&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;128&lt;/td&gt; &#xA;   &lt;td&gt;2M&lt;/td&gt; &#xA;   &lt;td&gt;2e-4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/pythia-1.4b&#34;&gt;Here&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Ready&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pythia-1.4B-Deduped&lt;/td&gt; &#xA;   &lt;td&gt;24&lt;/td&gt; &#xA;   &lt;td&gt;2048&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;128&lt;/td&gt; &#xA;   &lt;td&gt;2M&lt;/td&gt; &#xA;   &lt;td&gt;2e-4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/pythia-1.4b-deduped&#34;&gt;Here&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Ready&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pythia-2.8B&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;2560&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;80&lt;/td&gt; &#xA;   &lt;td&gt;2M&lt;/td&gt; &#xA;   &lt;td&gt;1.6e-4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/pythia-2.8b&#34;&gt;Here&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Ready&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pythia-2.8B-Deduped&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;2560&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;80&lt;/td&gt; &#xA;   &lt;td&gt;2M&lt;/td&gt; &#xA;   &lt;td&gt;1.6e-4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/pythia-2.8b-deduped&#34;&gt;Here&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Ready&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pythia-6.9B&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;4096&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;128&lt;/td&gt; &#xA;   &lt;td&gt;2M&lt;/td&gt; &#xA;   &lt;td&gt;1.2e-4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/pythia-6.9b&#34;&gt;Here&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Ready&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pythia-6.9B-Deduped&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;4096&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;128&lt;/td&gt; &#xA;   &lt;td&gt;2M&lt;/td&gt; &#xA;   &lt;td&gt;1.2e-4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/pythia-6.9b-deduped&#34;&gt;Here&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Ready&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pythia-12B&lt;/td&gt; &#xA;   &lt;td&gt;36&lt;/td&gt; &#xA;   &lt;td&gt;5120&lt;/td&gt; &#xA;   &lt;td&gt;40&lt;/td&gt; &#xA;   &lt;td&gt;128&lt;/td&gt; &#xA;   &lt;td&gt;2M&lt;/td&gt; &#xA;   &lt;td&gt;1.2e-4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/pythia-12b&#34;&gt;Here&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Ready&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pythia-12B-Deduped&lt;/td&gt; &#xA;   &lt;td&gt;36&lt;/td&gt; &#xA;   &lt;td&gt;5120&lt;/td&gt; &#xA;   &lt;td&gt;40&lt;/td&gt; &#xA;   &lt;td&gt;128&lt;/td&gt; &#xA;   &lt;td&gt;2M&lt;/td&gt; &#xA;   &lt;td&gt;1.2e-4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/pythia-12b-deduped&#34;&gt;Here&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Ready&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;We train and release a suite of 8 model sizes on 2 different datasets: &lt;a href=&#34;https://pile.eleuther.ai/&#34;&gt;the Pile&lt;/a&gt;, as well as the Pile with deduplication applied.&lt;/p&gt; &#xA;&lt;p&gt;All 8 model sizes are trained on the exact same data, in the exact same order. Each model saw 299,892,736,000 ~= 299.9B tokens during training, and &lt;em&gt;143 checkpoints&lt;/em&gt; for each model are saved every 2,097,152,000 ~= 2B tokens, evenly spaced throughout training. This corresponds to just under 1 epoch on the Pile for non-&#34;deduped&#34; models, and ~= 1.5 epochs on the deduped Pile (which contains 207B tokens in 1 epoch).&lt;/p&gt; &#xA;&lt;p&gt;Config files used to train these models within the &lt;a href=&#34;https://github.com/EleutherAI/gpt-neox&#34;&gt;GPT-NeoX library&lt;/a&gt; can be found at the &lt;code&gt;models/&lt;/code&gt; directory within this repository.&lt;/p&gt; &#xA;&lt;p&gt;We also upload the pre-tokenized data files and a script to reconstruct the dataloader as seen during training for all models. See &lt;strong&gt;Reproducing Training&lt;/strong&gt; section for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Changelog&lt;/h2&gt; &#xA;&lt;p&gt;[April 3, 2023] We have released a new version of all Pythia models, with the following changes to our training procedure:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;All model sizes are now trained with uniform batch size of 2M tokens. Previously, the models of size 160M, 410M, and 1.4B parameters were trained with batch sizes of 4M tokens.&lt;/li&gt; &#xA; &lt;li&gt;We added checkpoints at initialization (step 0) and steps {1,2,4,8,16,32,64, 128,256,512} in addition to every 1000 training steps.&lt;/li&gt; &#xA; &lt;li&gt;Flash Attention was used in the new retrained suite. Empirically, this seems to have effected the dynamic range of model outputs in some cases, which we are investigating further.&lt;/li&gt; &#xA; &lt;li&gt;We remedied a minor inconsistency that existed in the original suite: all models of size 2.8B parameters or smaller had a learning rate (LR) schedule which decayed to a minimum LR of 10% the starting LR rate, but the 6.9B and 12B models all used an LR schedule which decayed to a minimum LR of 0. In the redone training runs, we rectified this inconsistency: all models now were trained with LR decaying to a minimum of 0.1× their maximum LR.&lt;/li&gt; &#xA; &lt;li&gt;the new &lt;code&gt;EleutherAI/pythia-1b&lt;/code&gt; is trained with bf16, because in fp16 the model corrupted due to loss spikes late in training.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The old models (&#34;V0&#34;) remain available at &lt;a href=&#34;https://huggingface.co/models?other=pythia_v0&#34;&gt;https://huggingface.co/models?other=pythia_v0&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;[January 20, 2023] On January 20, 2023, we chose to rename the \textit{Pythia} model suite to better reflect including both embedding layer and unembedding layer parameters in our total parameter counts, in line with many other model suites and because we believe this convention better reflects the on-device memory usage of these models. See &lt;a href=&#34;https://huggingface.co/EleutherAI/pythia-410m-deduped#naming-convention-and-parameter-count&#34;&gt;https://huggingface.co/EleutherAI/pythia-410m-deduped#naming-convention-and-parameter-count&lt;/a&gt; for more details&lt;/p&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;All Pythia models are hosted on &lt;a href=&#34;https://huggingface.co/EleutherAI&#34;&gt;the Huggingface hub&lt;/a&gt;. They can be loaded and used via the following code (shown for the 3rd &lt;code&gt;pythia-70M-deduped&lt;/code&gt; model checkpoint):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import GPTNeoXForCausalLM, AutoTokenizer&#xA;&#xA;model = GPTNeoXForCausalLM.from_pretrained(&#xA;  &#34;EleutherAI/pythia-70m-deduped&#34;,&#xA;  revision=&#34;step3000&#34;,&#xA;  cache_dir=&#34;./pythia-70m-deduped/step3000&#34;,&#xA;)&#xA;&#xA;tokenizer = AutoTokenizer.from_pretrained(&#xA;  &#34;EleutherAI/pythia-70m-deduped&#34;,&#xA;  revision=&#34;step3000&#34;,&#xA;  cache_dir=&#34;./pythia-70m-deduped/step3000&#34;,&#xA;)&#xA;&#xA;inputs = tokenizer(&#34;Hello, I am&#34;, return_tensors=&#34;pt&#34;)&#xA;tokens = model.generate(**inputs)&#xA;tokenizer.decode(tokens[0])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;All models were trained for the equivalent of 143000 steps at a batch size of 2,097,152 tokens. Revision/branch &lt;code&gt;step143000&lt;/code&gt; (e.g. &lt;a href=&#34;https://huggingface.co/EleutherAI/pythia-19m-deduped/tree/step143000&#34;&gt;https://huggingface.co/EleutherAI/pythia-70m-deduped/tree/step143000&lt;/a&gt;) corresponds exactly to the model checkpoint on the &lt;code&gt;main&lt;/code&gt; branch of each model.&lt;/p&gt; &#xA;&lt;p&gt;We additionally have all model checkpoints in the format accepted by the &lt;a href=&#34;https://github.com/EleutherAI/gpt-neox&#34;&gt;GPT-NeoX library&lt;/a&gt;, but do not serve them at scale due to size of optimizer states and anticipated lower demand. If you would like to perform analysis using the models within the GPT-NeoX codebase, or would like the optimizer states, please email &lt;a href=&#34;mailto:hailey@eleuther.ai&#34;&gt;hailey@eleuther.ai&lt;/a&gt; and &lt;a href=&#34;mailto:stella@eleuther.ai&#34;&gt;stella@eleuther.ai&lt;/a&gt; to arrange access.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;code&gt;pythia-{size}-v0&lt;/code&gt; models on Huggingface of sizes &lt;code&gt;160m, 410m, 1.4b&lt;/code&gt; were trained with a batch size of 4M tokens and were originally trained for 71500 steps instead, and checkpointed every 500 steps. The checkpoints on Huggingface for these v0 models are renamed for consistency with all 2M batch models, so &lt;code&gt;step1000&lt;/code&gt; is the first checkpoint for &lt;code&gt;pythia-1.4b-v0&lt;/code&gt; that was saved (corresponding to step 500 in training), and &lt;code&gt;step1000&lt;/code&gt; is likewise the first pythia-6.9b-v0 checkpoint that was saved (corresponding to 1000 &#34;actual&#34; steps.)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Reproducing Training&lt;/h2&gt; &#xA;&lt;p&gt;We provide the training data for replication of our training runs. The &lt;a href=&#34;https://github.com/EleutherAI/gpt-neox&#34;&gt;GPT-NeoX library&lt;/a&gt; requires the pre-tokenized training data in the form of 2 memory-mapped numpy arrays: a &lt;code&gt;.bin&lt;/code&gt; and &lt;code&gt;.idx&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;p&gt;We provide these files, hosted on the Hugging Face hub.&lt;/p&gt; &#xA;&lt;p&gt;To download and use the deduplicated Pile training data, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git lfs clone https://huggingface.co/datasets/EleutherAI/pythia_deduped_pile_idxmaps&#xA;&#xA;python utils/unshard_memmap.py --input_file ./pythia_deduped_pile_idxmaps/pile_0.87_deduped_text_document-00000-of-00082.bin --num_shards 83 --output_dir ./pythia_pile_idxmaps/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will take over a day to run, though it should not require more than 5 GB of RAM. We recommend downloading this rather than retokenizing the Pile from scratch, in order to preserve the data order seen by the Pythia models.&lt;/p&gt; &#xA;&lt;p&gt;TODO: forthcoming: more information on how to replicate + relaunch the Pythia training runs, once the data is actually downloaded.&lt;/p&gt; &#xA;&lt;h3&gt;Dataset Viewer&lt;/h3&gt; &#xA;&lt;p&gt;We provide a tool to view particular portions of the training dataloader used by all models during training, at &lt;code&gt;utils/batch_viewer.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To run, first substitute the filepath to the downloaded &lt;code&gt;.bin&lt;/code&gt; and &lt;code&gt;.idx&lt;/code&gt; files for either the Pile or deduplicated Pile in &lt;code&gt;utils/dummy_config.yml&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;PYTHONPATH=utils/gpt-neox/ python utils/batch_viewer.py \&#xA;  --start_iteration 0 \&#xA;  --end_iteration 1000 \&#xA;  --mode save \&#xA;  --conf_dir utils/dummy_config.yml &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Passing &lt;code&gt;--mode save&lt;/code&gt; will save a separate file containing each batch as a numpy array.&lt;/p&gt; &#xA;&lt;p&gt;Passing &lt;code&gt;--mode custom&lt;/code&gt; will save a dictionary for each batch to a JSONL file--it can be used to compute arbitrary statistics over each batch seen during training.&lt;/p&gt; &#xA;&lt;h2&gt;Benchmark Scores&lt;/h2&gt; &#xA;&lt;p&gt;We also provide benchmark 0-shot and 5-shot results on a variety of NLP datasets:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Lambada (&lt;code&gt;lambada_openai&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Wikitext (&lt;code&gt;wikitext&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;PiQA (&lt;code&gt;piqa&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;SciQ (&lt;code&gt;sciq&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;WSC (&lt;code&gt;wsc&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Winogrande (&lt;code&gt;winogrande&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;ARC-challenge (&lt;code&gt;arc_challenge&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;ARC-easy (&lt;code&gt;arc_easy&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;LogiQA (&lt;code&gt;logiqa&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;BLiMP (&lt;code&gt;blimp_*&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;MMLU (&lt;code&gt;hendrycksTest*&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Evaluations were performed in GPT-NeoX using the &lt;a href=&#34;https://github.com/EleutherAI/lm-evaluation-harness&#34;&gt;LM Evaluation Harness&lt;/a&gt;, and are viewable by model and step at &lt;code&gt;results/json/*&lt;/code&gt; in this repository.&lt;/p&gt; &#xA;&lt;h3&gt;Plotting Results&lt;/h3&gt; &#xA;&lt;p&gt;We will also provide utilities for creating plots based on the dumped zero and few-shot results. Sample notebook and data format forthcoming.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>hemansnation/God-Level-Data-Science-ML-Full-Stack</title>
    <updated>2023-04-16T01:58:14Z</updated>
    <id>tag:github.com,2023-04-16:/hemansnation/God-Level-Data-Science-ML-Full-Stack</id>
    <link href="https://github.com/hemansnation/God-Level-Data-Science-ML-Full-Stack" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This roadmap contains 16 Chapters that can be completed in 8 months, whether you are a fresher in the field or an experienced professional who wants to transition into Data Science.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;God-Level Data Science ML Full Stack&lt;/h1&gt; &#xA;&lt;img src=&#34;https://github.com/hemansnation/Data-Science-ML-Full-Stack/raw/master/images/components.png&#34;&gt; &#xA;&lt;h2&gt;2023 Cohort&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Start&lt;/strong&gt;: 15th May, Monday, 7 AM IST&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://www.notion.so/god-level-python/Data-Science-ML-Full-Stack-Roadmap-05e6c16389c845d1a438ed4cff2b9952?pvs=4#61754d50a2784b6fa8b0485087f94c68&#34;&gt;Register Here&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;The‌ ‌Roadmap‌ ‌is‌ ‌divided‌ ‌into‌ ‌16 ‌Sections‌&lt;/h3&gt; &#xA;&lt;p&gt;Duration:‌ ‌256‌ ‌Hours‌ of Learning ‌(8 ‌Months)‌ ‌and many more hours for practice and project building. ‌&lt;/p&gt; &#xA;&lt;h2&gt;Month 1 - May&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#1--python-programming-and-logic-building&#34;&gt;Python‌ ‌Programming‌ ‌and‌ ‌Logic‌ ‌Building‌&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#2--data-structure--algorithms&#34;&gt;Data‌ ‌Structure‌ ‌&amp;amp;‌ ‌Algorithms‌&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Month 2 - June&lt;/h2&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#3--pandas-numpy-matplotlib&#34;&gt;Pandas‌ ‌Numpy‌ ‌Matplotlib‌&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#4--statistics&#34;&gt;Statistics‌&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Month 3 - July&lt;/h2&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#5--machine-learning&#34;&gt;Machine‌ ‌Learning‌&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#6--mlops&#34;&gt;ML Operations&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Month 4 - August&lt;/h2&gt; &#xA;&lt;ol start=&#34;7&#34;&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#7--natural-language-processing&#34;&gt;Natural‌ ‌Language‌ ‌Processing‌&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#8--computer-vision&#34;&gt;Computer‌ ‌Vision‌‌&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Month 5 - September&lt;/h2&gt; &#xA;&lt;ol start=&#34;9&#34;&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#9--data-visualization-with-tableau&#34;&gt;Data‌ ‌Visualization‌ ‌with‌ ‌Tableau‌&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#10--structure-query-language-sql&#34;&gt;Structure‌d ‌Query‌ ‌Language‌ ‌(SQL)‌&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Month 6 - October&lt;/h2&gt; &#xA;&lt;ol start=&#34;11&#34;&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#11--data-engineering&#34;&gt;Data Engineering&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#12--data-system-design&#34;&gt;Data System Design&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Month 7 - November&lt;/h2&gt; &#xA;&lt;ol start=&#34;13&#34;&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#13--five-major-projects-and-git&#34;&gt;Five‌ ‌Major‌ Capstone ‌Projects‌&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#14--interview-preperation&#34;&gt;Interview Preparations&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Month 8 - December&lt;/h2&gt; &#xA;&lt;ol start=&#34;15&#34;&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#15--git--github&#34;&gt;Git &amp;amp; GitHub&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#16--personal-profile--portfolio&#34;&gt;Personal Branding and portfolio&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#resources&#34;&gt;Resources&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;&#34;&gt;Dataset Collection&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Technology‌ ‌Stack‌&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python‌&lt;/li&gt; &#xA; &lt;li&gt;Data‌ ‌Structures‌&lt;/li&gt; &#xA; &lt;li&gt;NumPy‌&lt;/li&gt; &#xA; &lt;li&gt;Pandas‌&lt;/li&gt; &#xA; &lt;li&gt;Matplotlib‌&lt;/li&gt; &#xA; &lt;li&gt;Seaborn‌&lt;/li&gt; &#xA; &lt;li&gt;Scikit-Learn‌&lt;/li&gt; &#xA; &lt;li&gt;Statsmodels‌&lt;/li&gt; &#xA; &lt;li&gt;Natural‌ ‌Language‌ ‌Toolkit‌ ‌(‌ ‌NLTK‌ ‌)‌&lt;/li&gt; &#xA; &lt;li&gt;PyTorch‌&lt;/li&gt; &#xA; &lt;li&gt;OpenCV‌&lt;/li&gt; &#xA; &lt;li&gt;Tableau‌&lt;/li&gt; &#xA; &lt;li&gt;Structure‌ ‌Query‌ ‌Language‌ ‌(‌ ‌SQL‌ ‌)‌&lt;/li&gt; &#xA; &lt;li&gt;PySpark‌&lt;/li&gt; &#xA; &lt;li&gt;Azure‌ ‌Fundamentals‌&lt;/li&gt; &#xA; &lt;li&gt;Azure‌ ‌Data‌ ‌Factory‌&lt;/li&gt; &#xA; &lt;li&gt;Databricks‌&lt;/li&gt; &#xA; &lt;li&gt;5‌ ‌Major‌ ‌Projects‌&lt;/li&gt; &#xA; &lt;li&gt;Git‌ ‌and‌ ‌GitHub‌ ‌&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;1 | Python Programming and Logic Building&lt;/h1&gt; &#xA;&lt;p&gt;I will prefer Python Programming Language. Python is the best for starting your programming journey. Here is the roadmap of python for logic building.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python basics, Variables, Operators, Conditional Statements&lt;/li&gt; &#xA; &lt;li&gt;List and Strings&lt;/li&gt; &#xA; &lt;li&gt;While Loop, Nested Loops, Loop Else&lt;/li&gt; &#xA; &lt;li&gt;For Loop, Break, and Continue statements&lt;/li&gt; &#xA; &lt;li&gt;Functions, Return Statement, Recursion&lt;/li&gt; &#xA; &lt;li&gt;Dictionary, Tuple, Set&lt;/li&gt; &#xA; &lt;li&gt;File Handling, Exception Handling&lt;/li&gt; &#xA; &lt;li&gt;Object-Oriented Programming&lt;/li&gt; &#xA; &lt;li&gt;Modules and Packages&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/hemansnation/Python-Roadmap-2022&#34;&gt;In-Depth Roadmap of Python&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;2 | Data Structure &amp;amp; Algorithms&lt;/h1&gt; &#xA;&lt;p&gt;Data Structure is the most important thing to learn not only for data scientists but for all the people working in computer science. With data structure, you get an internal understanding of the working of everything in software.&lt;/p&gt; &#xA;&lt;p&gt;Understand these topics&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Types of Algorithm Analysis&lt;/li&gt; &#xA; &lt;li&gt;Asymptotic Notation, Big-O, Omega, Theta&lt;/li&gt; &#xA; &lt;li&gt;Stacks&lt;/li&gt; &#xA; &lt;li&gt;Queues&lt;/li&gt; &#xA; &lt;li&gt;Linked List&lt;/li&gt; &#xA; &lt;li&gt;Trees&lt;/li&gt; &#xA; &lt;li&gt;Graphs&lt;/li&gt; &#xA; &lt;li&gt;Sorting&lt;/li&gt; &#xA; &lt;li&gt;Searching&lt;/li&gt; &#xA; &lt;li&gt;Hashing&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;3 | Pandas Numpy Matplotlib&lt;/h1&gt; &#xA;&lt;p&gt;Python supports n-dimensional arrays with Numpy. For data in 2-dimensions, Pandas is the best library for analysis. You can use other tools but tools have drag-and-drop features and have limitations. Pandas can be customized as per the need as we can code depending upon the real-life problem.&lt;/p&gt; &#xA;&lt;h3&gt;Numpy&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Vectors, Matrix&lt;/li&gt; &#xA; &lt;li&gt;Operations on Matrix&lt;/li&gt; &#xA; &lt;li&gt;Mean, Variance, and Standard Deviation&lt;/li&gt; &#xA; &lt;li&gt;Reshaping Arrays&lt;/li&gt; &#xA; &lt;li&gt;Transpose and Determinant of Matrix&lt;/li&gt; &#xA; &lt;li&gt;Diagonal Operations, Trace&lt;/li&gt; &#xA; &lt;li&gt;Add, Subtract, Multiply, Dot, and Cross Product.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Pandas&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Series and DataFrames&lt;/li&gt; &#xA; &lt;li&gt;Slicing, Rows, and Columns&lt;/li&gt; &#xA; &lt;li&gt;Operations on DataFrame&lt;/li&gt; &#xA; &lt;li&gt;Different ways to create DataFrame&lt;/li&gt; &#xA; &lt;li&gt;Read, Write Operations with CSV files&lt;/li&gt; &#xA; &lt;li&gt;Handling Missing values, replace values, and Regular Expression&lt;/li&gt; &#xA; &lt;li&gt;GroupBy and Concatenation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Matplotlib&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Graph Basics&lt;/li&gt; &#xA; &lt;li&gt;Format Strings in Plots&lt;/li&gt; &#xA; &lt;li&gt;Label Parameters, Legend&lt;/li&gt; &#xA; &lt;li&gt;Bar Chart, Pie Chart, Histogram, Scatter Plot&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;4 | Statistics&lt;/h1&gt; &#xA;&lt;h3&gt;Descriptive Statistics&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Measure of Frequency and Central Tendency&lt;/li&gt; &#xA; &lt;li&gt;Measure of Dispersion&lt;/li&gt; &#xA; &lt;li&gt;Probability Distribution&lt;/li&gt; &#xA; &lt;li&gt;Gaussian Normal Distribution&lt;/li&gt; &#xA; &lt;li&gt;Skewness and Kurtosis&lt;/li&gt; &#xA; &lt;li&gt;Regression Analysis&lt;/li&gt; &#xA; &lt;li&gt;Continuous and Discrete Functions&lt;/li&gt; &#xA; &lt;li&gt;Goodness of Fit&lt;/li&gt; &#xA; &lt;li&gt;Normality Test&lt;/li&gt; &#xA; &lt;li&gt;ANOVA&lt;/li&gt; &#xA; &lt;li&gt;Homoscedasticity&lt;/li&gt; &#xA; &lt;li&gt;Linear and Non-Linear Relationship with Regression&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Inferential Statistics&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;t-Test&lt;/li&gt; &#xA; &lt;li&gt;z-Test&lt;/li&gt; &#xA; &lt;li&gt;Hypothesis Testing&lt;/li&gt; &#xA; &lt;li&gt;Type I and Type II errors&lt;/li&gt; &#xA; &lt;li&gt;t-Test and its types&lt;/li&gt; &#xA; &lt;li&gt;One way ANOVA&lt;/li&gt; &#xA; &lt;li&gt;Two way ANOVA&lt;/li&gt; &#xA; &lt;li&gt;Chi-Square Test&lt;/li&gt; &#xA; &lt;li&gt;Implementation of continuous and categorical data&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;5 | Machine Learning&lt;/h1&gt; &#xA;&lt;p&gt;The best way to master machine learning algorithms is to work with the Scikit-Learn framework. Scikit-Learn contains predefined algorithms and you can work with them just by generating the object of the class. These are the algorithm you must know including the types of Supervised and Unsupervised Machine Learning:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Linear Regression&lt;/li&gt; &#xA; &lt;li&gt;Logistic Regression&lt;/li&gt; &#xA; &lt;li&gt;Decision Tree&lt;/li&gt; &#xA; &lt;li&gt;Gradient Descent&lt;/li&gt; &#xA; &lt;li&gt;Random Forest&lt;/li&gt; &#xA; &lt;li&gt;Ridge and Lasso Regression&lt;/li&gt; &#xA; &lt;li&gt;Naive Bayes&lt;/li&gt; &#xA; &lt;li&gt;Support Vector Machine&lt;/li&gt; &#xA; &lt;li&gt;KMeans Clustering&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Other Concepts and Topics for ML&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Measuring Accuracy&lt;/li&gt; &#xA; &lt;li&gt;Bias-Variance Trade-off&lt;/li&gt; &#xA; &lt;li&gt;Applying Regularization&lt;/li&gt; &#xA; &lt;li&gt;Elastic Net Regression&lt;/li&gt; &#xA; &lt;li&gt;Predictive Analytics&lt;/li&gt; &#xA; &lt;li&gt;Exploratory Data Analysis&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;6 | MLOps&lt;/h1&gt; &#xA;&lt;p&gt;You can master any one of the cloud services provider from AWS, GCP and Azure. You can switch easily once you understand one of them.&lt;/p&gt; &#xA;&lt;p&gt;We will focus on AWS - Amazon Web Services first&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Deploy ML models using Flask&lt;/li&gt; &#xA; &lt;li&gt;Amazon Lex - Natural Language Understanding&lt;/li&gt; &#xA; &lt;li&gt;AWS Polly - Voice Analysis&lt;/li&gt; &#xA; &lt;li&gt;Amazon Transcribe - Speech to Text&lt;/li&gt; &#xA; &lt;li&gt;Amazon Textract - Extract Text&lt;/li&gt; &#xA; &lt;li&gt;Amazon Rekognition - Image Applications&lt;/li&gt; &#xA; &lt;li&gt;Amazon SageMaker - Building and deploying models&lt;/li&gt; &#xA; &lt;li&gt;Working with Deep Learning on AWS&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;7 | Natural Language Processing&lt;/h1&gt; &#xA;&lt;p&gt;If you are interested in working with Text, you should do some of the work an NLP Engineer do and understand the working of Language models.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Sentiment analysis&lt;/li&gt; &#xA; &lt;li&gt;POS Tagging, Parsing,&lt;/li&gt; &#xA; &lt;li&gt;Text preprocessing&lt;/li&gt; &#xA; &lt;li&gt;Stemming and Lemmatization&lt;/li&gt; &#xA; &lt;li&gt;Sentiment classification using Naive Bayes&lt;/li&gt; &#xA; &lt;li&gt;TF-IDF, N-gram,&lt;/li&gt; &#xA; &lt;li&gt;Machine Translation, BLEU Score&lt;/li&gt; &#xA; &lt;li&gt;Text Generation, Summarization, ROUGE Score&lt;/li&gt; &#xA; &lt;li&gt;Language Modeling, Perplexity&lt;/li&gt; &#xA; &lt;li&gt;Building a text classifier&lt;/li&gt; &#xA; &lt;li&gt;Identifying the gender&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;8 | Computer Vision&lt;/h1&gt; &#xA;&lt;p&gt;To work on image and video analytics we can master computer vision. To work on computer vision we have to understand images.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;PyTorch Tensors&lt;/li&gt; &#xA; &lt;li&gt;Understanding Pretrained models like AlexNet, ImageNet, ResNet.&lt;/li&gt; &#xA; &lt;li&gt;Neural Networks&lt;/li&gt; &#xA; &lt;li&gt;Building a perceptron&lt;/li&gt; &#xA; &lt;li&gt;Building a single layer neural network&lt;/li&gt; &#xA; &lt;li&gt;Building a deep neural network&lt;/li&gt; &#xA; &lt;li&gt;Recurrent neural network for sequential data analysis&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Convolutional Neural Networks&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Understanding the ConvNet topology&lt;/li&gt; &#xA; &lt;li&gt;Convolution layers&lt;/li&gt; &#xA; &lt;li&gt;Pooling layers&lt;/li&gt; &#xA; &lt;li&gt;Image Content Analysis&lt;/li&gt; &#xA; &lt;li&gt;Operating on images using OpenCV-Python&lt;/li&gt; &#xA; &lt;li&gt;Detecting edges&lt;/li&gt; &#xA; &lt;li&gt;Histogram equalization&lt;/li&gt; &#xA; &lt;li&gt;Detecting corners&lt;/li&gt; &#xA; &lt;li&gt;Detecting SIFT feature points&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;9 | Data Visualization with Tableau&lt;/h1&gt; &#xA;&lt;p&gt;How to use it Visual Perception&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;What is it, How it works, Why Tableau&lt;/li&gt; &#xA; &lt;li&gt;Connecting to Data&lt;/li&gt; &#xA; &lt;li&gt;Building charts&lt;/li&gt; &#xA; &lt;li&gt;Calculations&lt;/li&gt; &#xA; &lt;li&gt;Dashboards&lt;/li&gt; &#xA; &lt;li&gt;Sharing our work&lt;/li&gt; &#xA; &lt;li&gt;Advanced Charts, Calculated Fields, Calculated Aggregations&lt;/li&gt; &#xA; &lt;li&gt;Conditional Calculation, Parameterized Calculation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;10 | Structured Query Language (SQL)&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fundamental to SQL syntax and Installation&lt;/li&gt; &#xA; &lt;li&gt;Creating Tables, Modifiers&lt;/li&gt; &#xA; &lt;li&gt;Inserting and Retrieving Data, SELECT INSERT UPDATE DELETE&lt;/li&gt; &#xA; &lt;li&gt;Aggregating Data using Functions, Filtering and RegEX&lt;/li&gt; &#xA; &lt;li&gt;Subqueries, retrieve data based on conditions, grouping of Data.&lt;/li&gt; &#xA; &lt;li&gt;Practice Questions&lt;/li&gt; &#xA; &lt;li&gt;JOINs&lt;/li&gt; &#xA; &lt;li&gt;Advanced SQL concepts such as transactions, views, stored procedures, and functions.&lt;/li&gt; &#xA; &lt;li&gt;Database Design principles, normalization, and ER diagrams.&lt;/li&gt; &#xA; &lt;li&gt;Practice, Practice, Practice: Practice writing SQL queries on real-world datasets, and work on projects to apply your knowledge.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;11 | Data Engineering&lt;/h1&gt; &#xA;&lt;h3&gt;BigData&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;What is BigData?&lt;/li&gt; &#xA; &lt;li&gt;How is BigData applied within Business?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;PySpark&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Resilient Distributed Datasets&lt;/li&gt; &#xA; &lt;li&gt;Schema&lt;/li&gt; &#xA; &lt;li&gt;Lambda Expressions&lt;/li&gt; &#xA; &lt;li&gt;Transformations&lt;/li&gt; &#xA; &lt;li&gt;Actions&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Data Modeling&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Duplicate Data&lt;/li&gt; &#xA; &lt;li&gt;Descriptive Analysis on Data&lt;/li&gt; &#xA; &lt;li&gt;Visualizations&lt;/li&gt; &#xA; &lt;li&gt;ML lib&lt;/li&gt; &#xA; &lt;li&gt;ML Packages&lt;/li&gt; &#xA; &lt;li&gt;Pipelines&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Streaming&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Packaging Spark Applications&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;12 | Data System Design&lt;/h1&gt; &#xA;&lt;h3&gt;What is system design?&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;IP and OSI Model&lt;/li&gt; &#xA; &lt;li&gt;Domain Name System (DNS)&lt;/li&gt; &#xA; &lt;li&gt;Load Balancing&lt;/li&gt; &#xA; &lt;li&gt;Clustering&lt;/li&gt; &#xA; &lt;li&gt;Caching&lt;/li&gt; &#xA; &lt;li&gt;Availability, Scalability, Storage&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Databases and DBMS&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;SQL databases&lt;/li&gt; &#xA; &lt;li&gt;NoSQL databases&lt;/li&gt; &#xA; &lt;li&gt;SQL vs NoSQL databases&lt;/li&gt; &#xA; &lt;li&gt;Database Replication&lt;/li&gt; &#xA; &lt;li&gt;Indexes&lt;/li&gt; &#xA; &lt;li&gt;Normalization and Denormalization&lt;/li&gt; &#xA; &lt;li&gt;CAP theorem&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;System Design Interview&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;URL Shortener&lt;/li&gt; &#xA; &lt;li&gt;Whatsapp, Twitter, Netflix, Uber&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;13 | Five Major Projects and Git&lt;/h1&gt; &#xA;&lt;p&gt;We follow project-based learning and we will work on all the projects in parallel.&lt;/p&gt; &#xA;&lt;h1&gt;14 | Interview Preperation&lt;/h1&gt; &#xA;&lt;h1&gt;15 | Git &amp;amp; GitHub&lt;/h1&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://god-level-python.notion.site/Git-GitHub-Course-Make-Recruiters-reach-You-Build-your-stunning-profile-First-open-source-cont-1d4d70450aa94dd7ad2c062c0fec3cb8&#34;&gt;Git &amp;amp; GitHub Course&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Understanding Git&lt;/li&gt; &#xA; &lt;li&gt;Commands and How to commit your first code?&lt;/li&gt; &#xA; &lt;li&gt;How to use GitHub?&lt;/li&gt; &#xA; &lt;li&gt;How to make your first open-source contribution?&lt;/li&gt; &#xA; &lt;li&gt;How to work with a team? - Part 1&lt;/li&gt; &#xA; &lt;li&gt;How to create your stunning GitHub profile?&lt;/li&gt; &#xA; &lt;li&gt;How to build your own viral repository?&lt;/li&gt; &#xA; &lt;li&gt;Building a personal landing page for your Portfolio for FREE&lt;/li&gt; &#xA; &lt;li&gt;How to grow followers on GitHub?&lt;/li&gt; &#xA; &lt;li&gt;How to work with a team? Part 2 - issues, milestone and projects&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;16 | Personal Profile &amp;amp; Portfolio&lt;/h1&gt; &#xA;&lt;h1&gt;Resources&lt;/h1&gt; &#xA;&lt;h3&gt;Datasets&lt;/h3&gt; &#xA;&lt;p&gt;1️⃣ &lt;a href=&#34;https://github.com/awesomedata/awesome-public-datasets&#34;&gt;Awesome Public Datasets&lt;/a&gt; This list of a topic-centric public data sources in high quality.&lt;/p&gt; &#xA;&lt;p&gt;2️⃣&lt;a href=&#34;https://github.com/niderhoff/nlp-datasets&#34;&gt;NLP Datasets&lt;/a&gt; Alphabetical list of free/public domain datasets with text data for use in NLP.&lt;/p&gt; &#xA;&lt;p&gt;3️⃣&lt;a href=&#34;https://github.com/jsbroks/awesome-dataset-tools&#34;&gt;Awesome Dataset Tools&lt;/a&gt; A curated list of awesome dataset tools.&lt;/p&gt; &#xA;&lt;p&gt;4️⃣&lt;a href=&#34;https://github.com/xephonhq/awesome-time-series-database&#34;&gt;Awesome time series database&lt;/a&gt; A curated list of time series databases.&lt;/p&gt; &#xA;&lt;p&gt;5️⃣&lt;a href=&#34;https://github.com/shramos/Awesome-Cybersecurity-Datasets&#34;&gt;Awesome-Cybersecurity-Datasets&lt;/a&gt; A curated list of amazingly awesome Cybersecurity datasets.&lt;/p&gt; &#xA;&lt;p&gt;6️⃣&lt;a href=&#34;https://github.com/mint-lab/awesome-robotics-datasets&#34;&gt;Awesome Robotics Datasets&lt;/a&gt; Robotics Dataset Collections.&lt;/p&gt; &#xA;&lt;h1&gt;Research Starting Point&lt;/h1&gt; &#xA;&lt;h3&gt;Machine Learning&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ime.unicamp.br/~dias/Intoduction%20to%20Statistical%20Learning.pdf&#34;&gt;Introduction to Statistical Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Deep Learning&lt;/h3&gt; &#xA;&lt;h3&gt;Reinforcement Learning&lt;/h3&gt; &#xA;&lt;h1&gt;Projects&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/tree/master/projects&#34;&gt;Here is the list of project ideas&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Data Science ML Full Stack Live Batch -&amp;gt; &lt;a href=&#34;https://god-level-python.notion.site/Data-Science-ML-Full-Stack-Roadmap-05e6c16389c845d1a438ed4cff2b9952&#34;&gt;Register Here&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;h3&gt;Want to join the Community Group for Live Batch?&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://chat.whatsapp.com/BSUPbYhzzM1BcJplcTTIxb&#34;&gt;https://chat.whatsapp.com/BSUPbYhzzM1BcJplcTTIxb&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Socials&lt;/h1&gt; &#xA;&lt;h3&gt;Join Telegram for Data Science ML AI Resources:&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://t.me/+sREuRiFssMo4YWJl&#34;&gt;https://t.me/+sREuRiFssMo4YWJl&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Connect with me on these platforms:&lt;/h3&gt; &#xA;&lt;p&gt;LinkedIn: &lt;a href=&#34;https://www.linkedin.com/in/hemansnation/&#34;&gt;https://www.linkedin.com/in/hemansnation/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;YouTube: &lt;a href=&#34;https://www.youtube.com/@Himanshu-Ramchandani&#34;&gt;https://www.youtube.com/@Himanshu-Ramchandani&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Twitter: &lt;a href=&#34;https://twitter.com/hemansnation&#34;&gt;https://twitter.com/hemansnation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;GitHub: &lt;a href=&#34;https://github.com/hemansnation&#34;&gt;https://github.com/hemansnation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Instagram: &lt;a href=&#34;https://www.instagram.com/masterdexter.ai/&#34;&gt;https://www.instagram.com/masterdexter.ai/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;AI Jobs LinkedIn Group:&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.linkedin.com/groups/12540639/&#34;&gt;https://www.linkedin.com/groups/12540639/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Medium Blog:&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://medium.com/@hemansnation&#34;&gt;https://medium.com/@hemansnation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Notes on Data, Product, and AI - Newsletter:&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.linkedin.com/build-relation/newsletter-follow?entityUrn=7014799989251956736&#34;&gt;https://www.linkedin.com/build-relation/newsletter-follow?entityUrn=7014799989251956736&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Any Query?&lt;/h3&gt; &#xA;&lt;p&gt;Email Me Here: &lt;a href=&#34;mailto:connect@himanshuramchandani.co&#34;&gt;connect@himanshuramchandani.co&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>