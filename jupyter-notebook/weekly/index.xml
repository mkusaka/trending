<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-08-14T02:14:18Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>CompVis/latent-diffusion</title>
    <updated>2022-08-14T02:14:18Z</updated>
    <id>tag:github.com,2022-08-14:/CompVis/latent-diffusion</id>
    <link href="https://github.com/CompVis/latent-diffusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;High-Resolution Image Synthesis with Latent Diffusion Models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Latent Diffusion Models&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.10752&#34;&gt;arXiv&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/#bibtex&#34;&gt;BibTeX&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/assets/results.gif&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.10752&#34;&gt;&lt;strong&gt;High-Resolution Image Synthesis with Latent Diffusion Models&lt;/strong&gt;&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/rromb&#34;&gt;Robin Rombach&lt;/a&gt;*, &lt;a href=&#34;https://github.com/ablattmann&#34;&gt;Andreas Blattmann&lt;/a&gt;*, &lt;a href=&#34;https://github.com/qp-qp&#34;&gt;Dominik Lorenz&lt;/a&gt;, &lt;a href=&#34;https://github.com/pesser&#34;&gt;Patrick Esser&lt;/a&gt;, &lt;a href=&#34;https://hci.iwr.uni-heidelberg.de/Staff/bommer&#34;&gt;BjÃ¶rn Ommer&lt;/a&gt;&lt;br&gt; * equal contribution&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/assets/modelfigure.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;h3&gt;July 2022&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Inference code and model weights to run our &lt;a href=&#34;https://arxiv.org/abs/2204.11824&#34;&gt;retrieval-augmented diffusion models&lt;/a&gt; are now available. See &lt;a href=&#34;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/#retrieval-augmented-diffusion-models&#34;&gt;this section&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;April 2022&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Thanks to &lt;a href=&#34;https://github.com/crowsonkb&#34;&gt;Katherine Crowson&lt;/a&gt;, classifier-free guidance received a ~2x speedup and the &lt;a href=&#34;https://arxiv.org/abs/2202.09778&#34;&gt;PLMS sampler&lt;/a&gt; is available. See also &lt;a href=&#34;https://github.com/CompVis/latent-diffusion/pull/51&#34;&gt;this PR&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Our 1.45B &lt;a href=&#34;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/#text-to-image&#34;&gt;latent diffusion LAION model&lt;/a&gt; was integrated into &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces ðŸ¤—&lt;/a&gt; using &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;. Try out the Web Demo: &lt;a href=&#34;https://huggingface.co/spaces/multimodalart/latentdiffusion&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;More pre-trained LDMs are available:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;A 1.45B &lt;a href=&#34;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/#text-to-image&#34;&gt;model&lt;/a&gt; trained on the &lt;a href=&#34;https://arxiv.org/abs/2111.02114&#34;&gt;LAION-400M&lt;/a&gt; database.&lt;/li&gt; &#xA;   &lt;li&gt;A class-conditional model on ImageNet, achieving a FID of 3.6 when using &lt;a href=&#34;https://openreview.net/pdf?id=qw8AKxfYbI&#34;&gt;classifier-free guidance&lt;/a&gt; Available via a &lt;a href=&#34;https://colab.research.google.com/github/CompVis/latent-diffusion/blob/main/scripts/latent_imagenet_diffusion.ipynb&#34;&gt;colab notebook&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/CompVis/latent-diffusion/blob/main/scripts/latent_imagenet_diffusion.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;A suitable &lt;a href=&#34;https://conda.io/&#34;&gt;conda&lt;/a&gt; environment named &lt;code&gt;ldm&lt;/code&gt; can be created and activated with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env create -f environment.yaml&#xA;conda activate ldm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Pretrained Models&lt;/h1&gt; &#xA;&lt;p&gt;A general list of all available checkpoints is available in via our &lt;a href=&#34;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/#model-zoo&#34;&gt;model zoo&lt;/a&gt;. If you use any of these models in your work, we are always happy to receive a &lt;a href=&#34;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/#bibtex&#34;&gt;citation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Retrieval Augmented Diffusion Models&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/assets/rdm-preview.jpg&#34; alt=&#34;rdm-figure&#34;&gt; We include inference code to run our retrieval-augmented diffusion models (RDMs) as described in &lt;a href=&#34;https://arxiv.org/abs/2204.11824&#34;&gt;https://arxiv.org/abs/2204.11824&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To get started, install the additionally required python packages into your &lt;code&gt;ldm&lt;/code&gt; environment&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install transformers==4.19.2 scann kornia==0.6.4 torchmetrics==0.6.0&#xA;pip install git+https://github.com/arogozhnikov/einops.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and download the trained weights (preliminary ceckpoints):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p models/rdm/rdm768x768/&#xA;wget -O models/rdm/rdm768x768/model.ckpt https://ommer-lab.com/files/rdm/model.ckpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;As these models are conditioned on a set of CLIP image embeddings, our RDMs support different inference modes, which are described in the following.&lt;/p&gt; &#xA;&lt;h4&gt;RDM with text-prompt only (no explicit retrieval needed)&lt;/h4&gt; &#xA;&lt;p&gt;Since CLIP offers a shared image/text feature space, and RDMs learn to cover a neighborhood of a given example during training, we can directly take a CLIP text embedding of a given prompt and condition on it. Run this mode via&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/knn2img.py  --prompt &#34;a happy bear reading a newspaper, oil on canvas&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;RDM with text-to-image retrieval&lt;/h4&gt; &#xA;&lt;p&gt;To be able to run a RDM conditioned on a text-prompt and additionally images retrieved from this prompt, you will also need to download the corresponding retrieval database. We provide two distinct databases extracted from the &lt;a href=&#34;https://storage.googleapis.com/openimages/web/index.html&#34;&gt;Openimages-&lt;/a&gt; and &lt;a href=&#34;https://github.com/liaopeiyuan/artbench&#34;&gt;ArtBench-&lt;/a&gt; datasets. Interchanging the databases results in different capabilities of the model as visualized below, although the learned weights are the same in both cases.&lt;/p&gt; &#xA;&lt;p&gt;Download the retrieval-databases which contain the retrieval-datasets (&lt;a href=&#34;https://storage.googleapis.com/openimages/web/index.html&#34;&gt;Openimages&lt;/a&gt; (~11GB) and &lt;a href=&#34;https://github.com/liaopeiyuan/artbench&#34;&gt;ArtBench&lt;/a&gt; (~82MB)) compressed into CLIP image embeddings:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p data/rdm/retrieval_databases&#xA;wget -O data/rdm/retrieval_databases/artbench.zip https://ommer-lab.com/files/rdm/artbench_databases.zip&#xA;wget -O data/rdm/retrieval_databases/openimages.zip https://ommer-lab.com/files/rdm/openimages_database.zip&#xA;unzip data/rdm/retrieval_databases/artbench.zip -d data/rdm/retrieval_databases/&#xA;unzip data/rdm/retrieval_databases/openimages.zip -d data/rdm/retrieval_databases/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We also provide trained &lt;a href=&#34;https://github.com/google-research/google-research/tree/master/scann&#34;&gt;ScaNN&lt;/a&gt; search indices for ArtBench. Download and extract via&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p data/rdm/searchers&#xA;wget -O data/rdm/searchers/artbench.zip https://ommer-lab.com/files/rdm/artbench_searchers.zip&#xA;unzip data/rdm/searchers/artbench.zip -d data/rdm/searchers&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Since the index for OpenImages is large (~21 GB), we provide a script to create and save it for usage during sampling. Note however, that sampling with the OpenImages database will not be possible without this index. Run the script via&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/train_searcher.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Retrieval based text-guided sampling with visual nearest neighbors can be started via&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/knn2img.py  --prompt &#34;a happy pineapple&#34; --use_neighbors --knn &amp;lt;number_of_neighbors&amp;gt; &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that the maximum supported number of neighbors is 20. The database can be changed via the cmd parameter &lt;code&gt;--database&lt;/code&gt; which can be &lt;code&gt;[openimages, artbench-art_nouveau, artbench-baroque, artbench-expressionism, artbench-impressionism, artbench-post_impressionism, artbench-realism, artbench-renaissance, artbench-romanticism, artbench-surrealism, artbench-ukiyo_e]&lt;/code&gt;. For using &lt;code&gt;--database openimages&lt;/code&gt;, the above script (&lt;code&gt;scripts/train_searcher.py&lt;/code&gt;) must be executed before. Due to their relatively small size, the artbench datasetbases are best suited for creating more abstract concepts and do not work well for detailed text control.&lt;/p&gt; &#xA;&lt;h4&gt;Coming Soon&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;better models&lt;/li&gt; &#xA; &lt;li&gt;more resolutions&lt;/li&gt; &#xA; &lt;li&gt;image-to-image retrieval&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Text-to-Image&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/assets/txt2img-preview.png&#34; alt=&#34;text2img-figure&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Download the pre-trained weights (5.7GB)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mkdir -p models/ldm/text2img-large/&#xA;wget -O models/ldm/text2img-large/model.ckpt https://ommer-lab.com/files/latent-diffusion/nitro/txt2img-f8-large/model.ckpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and sample with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/txt2img.py --prompt &#34;a virus monster is playing guitar, oil on canvas&#34; --ddim_eta 0.0 --n_samples 4 --n_iter 4 --scale 5.0  --ddim_steps 50&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will save each sample individually as well as a grid of size &lt;code&gt;n_iter&lt;/code&gt; x &lt;code&gt;n_samples&lt;/code&gt; at the specified output location (default: &lt;code&gt;outputs/txt2img-samples&lt;/code&gt;). Quality, sampling speed and diversity are best controlled via the &lt;code&gt;scale&lt;/code&gt;, &lt;code&gt;ddim_steps&lt;/code&gt; and &lt;code&gt;ddim_eta&lt;/code&gt; arguments. As a rule of thumb, higher values of &lt;code&gt;scale&lt;/code&gt; produce better samples at the cost of a reduced output diversity.&lt;br&gt; Furthermore, increasing &lt;code&gt;ddim_steps&lt;/code&gt; generally also gives higher quality samples, but returns are diminishing for values &amp;gt; 250. Fast sampling (i.e. low values of &lt;code&gt;ddim_steps&lt;/code&gt;) while retaining good quality can be achieved by using &lt;code&gt;--ddim_eta 0.0&lt;/code&gt;.&lt;br&gt; Faster sampling (i.e. even lower values of &lt;code&gt;ddim_steps&lt;/code&gt;) while retaining good quality can be achieved by using &lt;code&gt;--ddim_eta 0.0&lt;/code&gt; and &lt;code&gt;--plms&lt;/code&gt; (see &lt;a href=&#34;https://arxiv.org/abs/2202.09778&#34;&gt;Pseudo Numerical Methods for Diffusion Models on Manifolds&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;h4&gt;Beyond 256Â²&lt;/h4&gt; &#xA;&lt;p&gt;For certain inputs, simply running the model in a convolutional fashion on larger features than it was trained on can sometimes result in interesting results. To try it out, tune the &lt;code&gt;H&lt;/code&gt; and &lt;code&gt;W&lt;/code&gt; arguments (which will be integer-divided by 8 in order to calculate the corresponding latent size), e.g. run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/txt2img.py --prompt &#34;a sunset behind a mountain range, vector image&#34; --ddim_eta 1.0 --n_samples 1 --n_iter 1 --H 384 --W 1024 --scale 5.0  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to create a sample of size 384x1024. Note, however, that controllability is reduced compared to the 256x256 setting.&lt;/p&gt; &#xA;&lt;p&gt;The example below was generated using the above command. &lt;img src=&#34;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/assets/txt2img-convsample.png&#34; alt=&#34;text2img-figure-conv&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Inpainting&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/assets/inpainting.png&#34; alt=&#34;inpainting&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Download the pre-trained weights&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;wget -O models/ldm/inpainting_big/last.ckpt https://heibox.uni-heidelberg.de/f/4d9ac7ea40c64582b7c9/?dl=1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and sample with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/inpaint.py --indir data/inpainting_examples/ --outdir outputs/inpainting_results&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;indir&lt;/code&gt; should contain images &lt;code&gt;*.png&lt;/code&gt; and masks &lt;code&gt;&amp;lt;image_fname&amp;gt;_mask.png&lt;/code&gt; like the examples provided in &lt;code&gt;data/inpainting_examples&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Class-Conditional ImageNet&lt;/h2&gt; &#xA;&lt;p&gt;Available via a &lt;a href=&#34;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/scripts/latent_imagenet_diffusion.ipynb&#34;&gt;notebook&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/CompVis/latent-diffusion/blob/main/scripts/latent_imagenet_diffusion.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;. &lt;img src=&#34;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/assets/birdhouse.png&#34; alt=&#34;class-conditional&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Unconditional Models&lt;/h2&gt; &#xA;&lt;p&gt;We also provide a script for sampling from unconditional LDMs (e.g. LSUN, FFHQ, ...). Start it via&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=&amp;lt;GPU_ID&amp;gt; python scripts/sample_diffusion.py -r models/ldm/&amp;lt;model_spec&amp;gt;/model.ckpt -l &amp;lt;logdir&amp;gt; -n &amp;lt;\#samples&amp;gt; --batch_size &amp;lt;batch_size&amp;gt; -c &amp;lt;\#ddim steps&amp;gt; -e &amp;lt;\#eta&amp;gt; &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Train your own LDMs&lt;/h1&gt; &#xA;&lt;h2&gt;Data preparation&lt;/h2&gt; &#xA;&lt;h3&gt;Faces&lt;/h3&gt; &#xA;&lt;p&gt;For downloading the CelebA-HQ and FFHQ datasets, proceed as described in the &lt;a href=&#34;https://github.com/CompVis/taming-transformers#celeba-hq&#34;&gt;taming-transformers&lt;/a&gt; repository.&lt;/p&gt; &#xA;&lt;h3&gt;LSUN&lt;/h3&gt; &#xA;&lt;p&gt;The LSUN datasets can be conveniently downloaded via the script available &lt;a href=&#34;https://github.com/fyu/lsun&#34;&gt;here&lt;/a&gt;. We performed a custom split into training and validation images, and provide the corresponding filenames at &lt;a href=&#34;https://ommer-lab.com/files/lsun.zip&#34;&gt;https://ommer-lab.com/files/lsun.zip&lt;/a&gt;. After downloading, extract them to &lt;code&gt;./data/lsun&lt;/code&gt;. The beds/cats/churches subsets should also be placed/symlinked at &lt;code&gt;./data/lsun/bedrooms&lt;/code&gt;/&lt;code&gt;./data/lsun/cats&lt;/code&gt;/&lt;code&gt;./data/lsun/churches&lt;/code&gt;, respectively.&lt;/p&gt; &#xA;&lt;h3&gt;ImageNet&lt;/h3&gt; &#xA;&lt;p&gt;The code will try to download (through &lt;a href=&#34;http://academictorrents.com/&#34;&gt;Academic Torrents&lt;/a&gt;) and prepare ImageNet the first time it is used. However, since ImageNet is quite large, this requires a lot of disk space and time. If you already have ImageNet on your disk, you can speed things up by putting the data into &lt;code&gt;${XDG_CACHE}/autoencoders/data/ILSVRC2012_{split}/data/&lt;/code&gt; (which defaults to &lt;code&gt;~/.cache/autoencoders/data/ILSVRC2012_{split}/data/&lt;/code&gt;), where &lt;code&gt;{split}&lt;/code&gt; is one of &lt;code&gt;train&lt;/code&gt;/&lt;code&gt;validation&lt;/code&gt;. It should have the following structure:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;${XDG_CACHE}/autoencoders/data/ILSVRC2012_{split}/data/&#xA;â”œâ”€â”€ n01440764&#xA;â”‚   â”œâ”€â”€ n01440764_10026.JPEG&#xA;â”‚   â”œâ”€â”€ n01440764_10027.JPEG&#xA;â”‚   â”œâ”€â”€ ...&#xA;â”œâ”€â”€ n01443537&#xA;â”‚   â”œâ”€â”€ n01443537_10007.JPEG&#xA;â”‚   â”œâ”€â”€ n01443537_10014.JPEG&#xA;â”‚   â”œâ”€â”€ ...&#xA;â”œâ”€â”€ ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you haven&#39;t extracted the data, you can also place &lt;code&gt;ILSVRC2012_img_train.tar&lt;/code&gt;/&lt;code&gt;ILSVRC2012_img_val.tar&lt;/code&gt; (or symlinks to them) into &lt;code&gt;${XDG_CACHE}/autoencoders/data/ILSVRC2012_train/&lt;/code&gt; / &lt;code&gt;${XDG_CACHE}/autoencoders/data/ILSVRC2012_validation/&lt;/code&gt;, which will then be extracted into above structure without downloading it again. Note that this will only happen if neither a folder &lt;code&gt;${XDG_CACHE}/autoencoders/data/ILSVRC2012_{split}/data/&lt;/code&gt; nor a file &lt;code&gt;${XDG_CACHE}/autoencoders/data/ILSVRC2012_{split}/.ready&lt;/code&gt; exist. Remove them if you want to force running the dataset preparation again.&lt;/p&gt; &#xA;&lt;h2&gt;Model Training&lt;/h2&gt; &#xA;&lt;p&gt;Logs and checkpoints for trained models are saved to &lt;code&gt;logs/&amp;lt;START_DATE_AND_TIME&amp;gt;_&amp;lt;config_spec&amp;gt;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Training autoencoder models&lt;/h3&gt; &#xA;&lt;p&gt;Configs for training a KL-regularized autoencoder on ImageNet are provided at &lt;code&gt;configs/autoencoder&lt;/code&gt;. Training can be started by running&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=&amp;lt;GPU_ID&amp;gt; python main.py --base configs/autoencoder/&amp;lt;config_spec&amp;gt;.yaml -t --gpus 0,    &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where &lt;code&gt;config_spec&lt;/code&gt; is one of {&lt;code&gt;autoencoder_kl_8x8x64&lt;/code&gt;(f=32, d=64), &lt;code&gt;autoencoder_kl_16x16x16&lt;/code&gt;(f=16, d=16), &lt;code&gt;autoencoder_kl_32x32x4&lt;/code&gt;(f=8, d=4), &lt;code&gt;autoencoder_kl_64x64x3&lt;/code&gt;(f=4, d=3)}.&lt;/p&gt; &#xA;&lt;p&gt;For training VQ-regularized models, see the &lt;a href=&#34;https://github.com/CompVis/taming-transformers&#34;&gt;taming-transformers&lt;/a&gt; repository.&lt;/p&gt; &#xA;&lt;h3&gt;Training LDMs&lt;/h3&gt; &#xA;&lt;p&gt;In &lt;code&gt;configs/latent-diffusion/&lt;/code&gt; we provide configs for training LDMs on the LSUN-, CelebA-HQ, FFHQ and ImageNet datasets. Training can be started by running&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=&amp;lt;GPU_ID&amp;gt; python main.py --base configs/latent-diffusion/&amp;lt;config_spec&amp;gt;.yaml -t --gpus 0,&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where &lt;code&gt;&amp;lt;config_spec&amp;gt;&lt;/code&gt; is one of {&lt;code&gt;celebahq-ldm-vq-4&lt;/code&gt;(f=4, VQ-reg. autoencoder, spatial size 64x64x3),&lt;code&gt;ffhq-ldm-vq-4&lt;/code&gt;(f=4, VQ-reg. autoencoder, spatial size 64x64x3), &lt;code&gt;lsun_bedrooms-ldm-vq-4&lt;/code&gt;(f=4, VQ-reg. autoencoder, spatial size 64x64x3), &lt;code&gt;lsun_churches-ldm-vq-4&lt;/code&gt;(f=8, KL-reg. autoencoder, spatial size 32x32x4),&lt;code&gt;cin-ldm-vq-8&lt;/code&gt;(f=8, VQ-reg. autoencoder, spatial size 32x32x4)}.&lt;/p&gt; &#xA;&lt;h1&gt;Model Zoo&lt;/h1&gt; &#xA;&lt;h2&gt;Pretrained Autoencoding Models&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/assets/reconstruction2.png&#34; alt=&#34;rec2&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;All models were trained until convergence (no further substantial improvement in rFID).&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;rFID vs val&lt;/th&gt; &#xA;   &lt;th&gt;train steps&lt;/th&gt; &#xA;   &lt;th&gt;PSNR&lt;/th&gt; &#xA;   &lt;th&gt;PSIM&lt;/th&gt; &#xA;   &lt;th&gt;Link&lt;/th&gt; &#xA;   &lt;th&gt;Comments&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;f=4, VQ (Z=8192, d=3)&lt;/td&gt; &#xA;   &lt;td&gt;0.58&lt;/td&gt; &#xA;   &lt;td&gt;533066&lt;/td&gt; &#xA;   &lt;td&gt;27.43 +/- 4.26&lt;/td&gt; &#xA;   &lt;td&gt;0.53 +/- 0.21&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/vq-f4.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/vq-f4.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;f=4, VQ (Z=8192, d=3)&lt;/td&gt; &#xA;   &lt;td&gt;1.06&lt;/td&gt; &#xA;   &lt;td&gt;658131&lt;/td&gt; &#xA;   &lt;td&gt;25.21 +/- 4.17&lt;/td&gt; &#xA;   &lt;td&gt;0.72 +/- 0.26&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://heibox.uni-heidelberg.de/f/9c6681f64bb94338a069/?dl=1&#34;&gt;https://heibox.uni-heidelberg.de/f/9c6681f64bb94338a069/?dl=1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;no attention&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;f=8, VQ (Z=16384, d=4)&lt;/td&gt; &#xA;   &lt;td&gt;1.14&lt;/td&gt; &#xA;   &lt;td&gt;971043&lt;/td&gt; &#xA;   &lt;td&gt;23.07 +/- 3.99&lt;/td&gt; &#xA;   &lt;td&gt;1.17 +/- 0.36&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/vq-f8.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/vq-f8.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;f=8, VQ (Z=256, d=4)&lt;/td&gt; &#xA;   &lt;td&gt;1.49&lt;/td&gt; &#xA;   &lt;td&gt;1608649&lt;/td&gt; &#xA;   &lt;td&gt;22.35 +/- 3.81&lt;/td&gt; &#xA;   &lt;td&gt;1.26 +/- 0.37&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/vq-f8-n256.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/vq-f8-n256.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;f=16, VQ (Z=16384, d=8)&lt;/td&gt; &#xA;   &lt;td&gt;5.15&lt;/td&gt; &#xA;   &lt;td&gt;1101166&lt;/td&gt; &#xA;   &lt;td&gt;20.83 +/- 3.61&lt;/td&gt; &#xA;   &lt;td&gt;1.73 +/- 0.43&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://heibox.uni-heidelberg.de/f/0e42b04e2e904890a9b6/?dl=1&#34;&gt;https://heibox.uni-heidelberg.de/f/0e42b04e2e904890a9b6/?dl=1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;f=4, KL&lt;/td&gt; &#xA;   &lt;td&gt;0.27&lt;/td&gt; &#xA;   &lt;td&gt;176991&lt;/td&gt; &#xA;   &lt;td&gt;27.53 +/- 4.54&lt;/td&gt; &#xA;   &lt;td&gt;0.55 +/- 0.24&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/kl-f4.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/kl-f4.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;f=8, KL&lt;/td&gt; &#xA;   &lt;td&gt;0.90&lt;/td&gt; &#xA;   &lt;td&gt;246803&lt;/td&gt; &#xA;   &lt;td&gt;24.19 +/- 4.19&lt;/td&gt; &#xA;   &lt;td&gt;1.02 +/- 0.35&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/kl-f8.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/kl-f8.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;f=16, KL (d=16)&lt;/td&gt; &#xA;   &lt;td&gt;0.87&lt;/td&gt; &#xA;   &lt;td&gt;442998&lt;/td&gt; &#xA;   &lt;td&gt;24.08 +/- 4.22&lt;/td&gt; &#xA;   &lt;td&gt;1.07 +/- 0.36&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/kl-f16.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/kl-f16.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;f=32, KL (d=64)&lt;/td&gt; &#xA;   &lt;td&gt;2.04&lt;/td&gt; &#xA;   &lt;td&gt;406763&lt;/td&gt; &#xA;   &lt;td&gt;22.27 +/- 3.93&lt;/td&gt; &#xA;   &lt;td&gt;1.41 +/- 0.40&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/kl-f32.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/kl-f32.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Get the models&lt;/h3&gt; &#xA;&lt;p&gt;Running the following script downloads und extracts all available pretrained autoencoding models.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;bash scripts/download_first_stages.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The first stage models can then be found in &lt;code&gt;models/first_stage_models/&amp;lt;model_spec&amp;gt;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Pretrained LDMs&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Datset&lt;/th&gt; &#xA;   &lt;th&gt;Task&lt;/th&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;FID&lt;/th&gt; &#xA;   &lt;th&gt;IS&lt;/th&gt; &#xA;   &lt;th&gt;Prec&lt;/th&gt; &#xA;   &lt;th&gt;Recall&lt;/th&gt; &#xA;   &lt;th&gt;Link&lt;/th&gt; &#xA;   &lt;th&gt;Comments&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CelebA-HQ&lt;/td&gt; &#xA;   &lt;td&gt;Unconditional Image Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;LDM-VQ-4 (200 DDIM steps, eta=0)&lt;/td&gt; &#xA;   &lt;td&gt;5.11 (5.11)&lt;/td&gt; &#xA;   &lt;td&gt;3.29&lt;/td&gt; &#xA;   &lt;td&gt;0.72&lt;/td&gt; &#xA;   &lt;td&gt;0.49&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/celeba.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/celeba.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FFHQ&lt;/td&gt; &#xA;   &lt;td&gt;Unconditional Image Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;LDM-VQ-4 (200 DDIM steps, eta=1)&lt;/td&gt; &#xA;   &lt;td&gt;4.98 (4.98)&lt;/td&gt; &#xA;   &lt;td&gt;4.50 (4.50)&lt;/td&gt; &#xA;   &lt;td&gt;0.73&lt;/td&gt; &#xA;   &lt;td&gt;0.50&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/ffhq.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/ffhq.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LSUN-Churches&lt;/td&gt; &#xA;   &lt;td&gt;Unconditional Image Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;LDM-KL-8 (400 DDIM steps, eta=0)&lt;/td&gt; &#xA;   &lt;td&gt;4.02 (4.02)&lt;/td&gt; &#xA;   &lt;td&gt;2.72&lt;/td&gt; &#xA;   &lt;td&gt;0.64&lt;/td&gt; &#xA;   &lt;td&gt;0.52&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/lsun_churches.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/lsun_churches.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LSUN-Bedrooms&lt;/td&gt; &#xA;   &lt;td&gt;Unconditional Image Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;LDM-VQ-4 (200 DDIM steps, eta=1)&lt;/td&gt; &#xA;   &lt;td&gt;2.95 (3.0)&lt;/td&gt; &#xA;   &lt;td&gt;2.22 (2.23)&lt;/td&gt; &#xA;   &lt;td&gt;0.66&lt;/td&gt; &#xA;   &lt;td&gt;0.48&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/lsun_bedrooms.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/lsun_bedrooms.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ImageNet&lt;/td&gt; &#xA;   &lt;td&gt;Class-conditional Image Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;LDM-VQ-8 (200 DDIM steps, eta=1)&lt;/td&gt; &#xA;   &lt;td&gt;7.77(7.76)* /15.82**&lt;/td&gt; &#xA;   &lt;td&gt;201.56(209.52)* /78.82**&lt;/td&gt; &#xA;   &lt;td&gt;0.84* / 0.65**&lt;/td&gt; &#xA;   &lt;td&gt;0.35* / 0.63**&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/cin.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/cin.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;*: w/ guiding, classifier_scale 10 **: w/o guiding, scores in bracket calculated with script provided by &lt;a href=&#34;https://github.com/openai/guided-diffusion&#34;&gt;ADM&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Conceptual Captions&lt;/td&gt; &#xA;   &lt;td&gt;Text-conditional Image Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;LDM-VQ-f4 (100 DDIM steps, eta=0)&lt;/td&gt; &#xA;   &lt;td&gt;16.79&lt;/td&gt; &#xA;   &lt;td&gt;13.89&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/text2img.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/text2img.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;finetuned from LAION&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenImages&lt;/td&gt; &#xA;   &lt;td&gt;Super-resolution&lt;/td&gt; &#xA;   &lt;td&gt;LDM-VQ-4&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/sr_bsr.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/sr_bsr.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;BSR image degradation&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenImages&lt;/td&gt; &#xA;   &lt;td&gt;Layout-to-Image Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;LDM-VQ-4 (200 DDIM steps, eta=0)&lt;/td&gt; &#xA;   &lt;td&gt;32.02&lt;/td&gt; &#xA;   &lt;td&gt;15.92&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/layout2img_model.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/layout2img_model.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Landscapes&lt;/td&gt; &#xA;   &lt;td&gt;Semantic Image Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;LDM-VQ-4&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/semantic_synthesis256.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/semantic_synthesis256.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Landscapes&lt;/td&gt; &#xA;   &lt;td&gt;Semantic Image Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;LDM-VQ-4&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/semantic_synthesis.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/semantic_synthesis.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;finetuned on resolution 512x512&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Get the models&lt;/h3&gt; &#xA;&lt;p&gt;The LDMs listed above can jointly be downloaded and extracted via&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;bash scripts/download_models.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The models can then be found in &lt;code&gt;models/ldm/&amp;lt;model_spec&amp;gt;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Coming Soon...&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;More inference scripts for conditional LDMs.&lt;/li&gt; &#xA; &lt;li&gt;In the meantime, you can play with our colab notebook &lt;a href=&#34;https://colab.research.google.com/drive/1xqzUi2iXQXDqXBHQGP9Mqt2YrYW6cx-J?usp=sharing&#34;&gt;https://colab.research.google.com/drive/1xqzUi2iXQXDqXBHQGP9Mqt2YrYW6cx-J?usp=sharing&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Comments&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Our codebase for the diffusion models builds heavily on &lt;a href=&#34;https://github.com/openai/guided-diffusion&#34;&gt;OpenAI&#39;s ADM codebase&lt;/a&gt; and &lt;a href=&#34;https://github.com/lucidrains/denoising-diffusion-pytorch&#34;&gt;https://github.com/lucidrains/denoising-diffusion-pytorch&lt;/a&gt;. Thanks for open-sourcing!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The implementation of the transformer encoder is from &lt;a href=&#34;https://github.com/lucidrains/x-transformers&#34;&gt;x-transformers&lt;/a&gt; by &lt;a href=&#34;https://github.com/lucidrains?tab=repositories&#34;&gt;lucidrains&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;BibTeX&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{rombach2021highresolution,&#xA;      title={High-Resolution Image Synthesis with Latent Diffusion Models}, &#xA;      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and BjÃ¶rn Ommer},&#xA;      year={2021},&#xA;      eprint={2112.10752},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&#xA;@misc{https://doi.org/10.48550/arxiv.2204.11824,&#xA;  doi = {10.48550/ARXIV.2204.11824},&#xA;  url = {https://arxiv.org/abs/2204.11824},&#xA;  author = {Blattmann, Andreas and Rombach, Robin and Oktay, Kaan and Ommer, BjÃ¶rn},&#xA;  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},&#xA;  title = {Retrieval-Augmented Diffusion Models},&#xA;  publisher = {arXiv},&#xA;  year = {2022},  &#xA;  copyright = {arXiv.org perpetual, non-exclusive license}&#xA;}&#xA;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>larymak/Python-project-Scripts</title>
    <updated>2022-08-14T02:14:18Z</updated>
    <id>tag:github.com,2022-08-14:/larymak/Python-project-Scripts</id>
    <link href="https://github.com/larymak/Python-project-Scripts" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This repositories contains a list of python scripts projects from beginner level advancing slowly. More code snippets to be added soon. feel free to clone this repo&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Python-project-Scripts.&lt;/h1&gt; &#xA;&lt;a href=&#34;https://join.slack.com/t/ngc-goz8665/shared_invite/zt-r01kumfq-dQUT3c95BxEP_fnk4yJFfQ&#34;&gt; &lt;img alt=&#34;Join us on Slack&#34; src=&#34;https://raw.githubusercontent.com/netlify/netlify-cms/master/website/static/img/slack.png&#34; width=&#34;165&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/larymak/Python-project-Scripts?style=plastic&#34; alt=&#34;Contributors&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/forks/larymak/Python-project-Scripts&#34; alt=&#34;Forks&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/stars/larymak/Python-project-Scripts&#34; alt=&#34;Stars&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/license/larymak/Python-project-Scripts&#34; alt=&#34;Licence&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/issues/larymak/Python-project-Scripts&#34; alt=&#34;Issues&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;This is a list of Python scripts for beginner projects.&lt;/h2&gt; &#xA;&lt;h3&gt;Description&lt;/h3&gt; &#xA;&lt;p&gt;Welcome aboard fellow developer, this is where you will find Python scripts which you are free to contribute to. You can contribute by submitting your own scripts, also written in Python, which you think would be amazing for other people to see.&lt;/p&gt; &#xA;&lt;h3&gt;Contribution Guidelines&lt;/h3&gt; &#xA;&lt;p&gt;The contribution guidelines are as per the guide &lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/raw/main/CONTRIBUTING.md&#34;&gt;HERE&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Instructions&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fork this repository&lt;/li&gt; &#xA; &lt;li&gt;Clone your forked repository&lt;/li&gt; &#xA; &lt;li&gt;Add your scripts&lt;/li&gt; &#xA; &lt;li&gt;Commit and push&lt;/li&gt; &#xA; &lt;li&gt;Create a pull request&lt;/li&gt; &#xA; &lt;li&gt;Star this repository&lt;/li&gt; &#xA; &lt;li&gt;Wait for pull request to merge&lt;/li&gt; &#xA; &lt;li&gt;Celebrate your first step into the open source world and contribute more&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Additional tools to help you get Started with Open-Source Contribution&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.freecodecamp.org/news/how-to-contribute-to-open-source-projects-beginners-guide/&#34;&gt;How to Contribute to Open Source Projects â€“ A Beginner&#39;s Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.freecodecamp.org/news/how-to-write-a-good-readme-file/&#34;&gt;How to Write a Good README File for Your GitHub Project&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;Note: When you add a project, add it to the README for ease of finding it.&lt;/h4&gt; &#xA;&lt;h4&gt;Note: Please do not have the project link reference your local forked repository. Always link it to this repository after it has been merged with main.&lt;/h4&gt; &#xA;&lt;h2&gt;Projects&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;SR No&lt;/th&gt; &#xA;   &lt;th&gt;Project&lt;/th&gt; &#xA;   &lt;th&gt;Author&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/ART%20SCRIPTS/image-ascii&#34;&gt;Ascii Image Converter&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak&#34;&gt;Lary Mak&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/TIME%20SCRIPTS/DigitalClock&#34;&gt;DigitalClock&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ozdyck3&#34;&gt;Logan Ozdyck&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/BOTS/InstaSpamBot&#34;&gt;Insta Spam Bot&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak&#34;&gt;Lary Mak&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/OTHERS/pyjokes&#34;&gt;Pyjokes&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak&#34;&gt;Lary Mak&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/BOTS/whatsapp-spam&#34;&gt;WhatsApp Spambot&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak&#34;&gt;Lary Mak&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;6&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/BOTS/InstagramBot&#34;&gt;Instagram Bot&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak&#34;&gt;Lary Mak&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/IMAGES%20%26%20PHOTO%20SCRIPTS/photo%20editor&#34;&gt;Photo Editor App&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak&#34;&gt;Lary Mak&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/OTHERS/RandomNameGen&#34;&gt;Random Name Generator&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak&#34;&gt;Lary Mak&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;9&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/WEB%20SCRAPING/WebScraping&#34;&gt;Web Scrapping&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak&#34;&gt;Lary Mak&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;10&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/PASSWORD%20RELATED/RandomPassword&#34;&gt;Random Password Generator&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Gagan1111&#34;&gt;Gagan prajapatii&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;11&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/OTHERS/YoutubeDownloader&#34;&gt;YouTube Video Downloader&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/jkinathan&#34;&gt;Kinathany&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/AUDIO%20RELATED%20SCRIPTS/texttoaudio&#34;&gt;Text to Audio&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/azminewasi&#34;&gt;Azmine Toushik&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;13&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/AUTOMATION/Sending-Emails&#34;&gt;Sending Emails&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Dhrumil-Zion&#34;&gt;Dhrumil Gohel&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;14&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/CONVERSION%20SCRIPTS/ShortenLinks&#34;&gt;Shorten Links&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Dhrumil-Zion&#34;&gt;Dhrumil Gohel&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;15&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/WEB%20SCRAPING/PYDICTIONARY&#34;&gt;Python Dictionary&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/jkinathan&#34;&gt;Kinathany&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/IMAGES%20%26%20PHOTO%20SCRIPTS/Image%20Captcha%20Generator&#34;&gt;Image Captcha Generator&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Dhrumil-Zion&#34;&gt;Dhrumil Gohel&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;17&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/AUDIO%20RELATED%20SCRIPTS/Audio%20Captcha%20Generator&#34;&gt;Audio Captcha Generator&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Dhrumil-Zion&#34;&gt;Dhrumil Gohel&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;18&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/IMAGES%20%26%20PHOTO%20SCRIPTS/Compress%20Image&#34;&gt;Compress Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/xolanigumbi&#34;&gt;Xolani&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;19&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/IMAGES%20%26%20PHOTO%20SCRIPTS/Image%20Grayscalling&#34;&gt;Image Grayscalling&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Dhrumil-Zion&#34;&gt;Dhrumil Gohel&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;20&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/WEB%20SCRAPING/Weather%20Forcasting&#34;&gt;Weather Forcasting&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Dhrumil-Zion&#34;&gt;Dhrumil Gohel&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;21&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/BOTS/pywhatkit&#34;&gt;Pywhatkit&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/KanakamSasikalyan&#34;&gt;SasiKalyan&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;22&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/WEB%20SCRAPING/wikipedia&#34;&gt;Wikipedia&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/KanakamSasikalyan&#34;&gt;SasiKalyan&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;23&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/AUTOMATION/AutoMoveFiles&#34;&gt;Auto Move Files&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/HishamKhalil1990&#34;&gt;Hisham Khalil&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;24&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/CSV_files&#34;&gt;Working with CSV Files&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Mahmoud-alzoubi95&#34;&gt;Mahmoud alzoubi&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;25&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/AUTOMATION/Getting%20Files%20and%20Folders%20sizes&#34;&gt;Getting files and folders sizes&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Saeedahmadi7714&#34;&gt;Saeedahmadi&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;26&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/OTHERS/Contact-management&#34;&gt;Contact management system&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Gagan1111&#34;&gt;Gagan prajapati&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;27&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/TIME%20SCRIPTS/Countdown%20Timer&#34;&gt;Countdown Timer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Mannuel25&#34;&gt;Emmanuel Tanimowo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;28&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/FLASK%20PROJECTS/FlaskSimpleCalculator&#34;&gt;Simple Flask Calculator&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/devnamrits&#34;&gt;Devnamrits&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;29&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/CONVERSION%20SCRIPTS/Hex%20to%20Base64%20Converter&#34;&gt;Hex to Base64 Converter&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/devmgardner&#34;&gt;Dev M&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;30&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/OTHERS/Notification&#34;&gt;Notifications Alert&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak&#34;&gt;Lary Mak&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;31&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/OTHERS/QrCodeGen&#34;&gt;Qr Code Generator&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak&#34;&gt;Lary Mak&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/FLASK%20PROJECTS/User%20Hash%20Generator&#34;&gt;User Hash Generator&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/urtuba&#34;&gt;Samed Kahyaoglu&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;33&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/WEB%20SCRAPING/Weather%20Updates&#34;&gt;Weather Updates&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/harshadbhere&#34;&gt;Harshad Bhere&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;34&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/TIME%20SCRIPTS/current_time&#34;&gt;Custom Clock&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Saeedahmadi7714&#34;&gt;Saeedahmadi&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;35&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/ART%20SCRIPTS/image-ascii&#34;&gt;Image to ASCII&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak&#34;&gt;Lary Mak&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;36&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/PASSWORD%20RELATED/password-validator&#34;&gt;Password Validator&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Mannuel25&#34;&gt;Emmanuel Tanimowo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;37&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/GAMES/guess-the-number&#34;&gt;Number Guessing Game&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Mannuel25&#34;&gt;Emmanuel Tanimowo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;38&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/FLASK%20PROJECTS/Web%20Dev%20with%20Flask&#34;&gt;Web Dev with python Flask&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/deepaksaipendyala&#34;&gt;Deepak Sai Pendyala&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;39&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/PASSWORD%20RELATED/passwordbreachchecker&#34;&gt;password breach checker&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/deepaksaipendyala&#34;&gt;Deepak Sai Pendyala&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;40&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/AUDIO%20RELATED%20SCRIPTS/AudioBuk&#34;&gt;Audio Book&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak&#34;&gt;Lary Mak&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;41&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/OTHERS/Geocoding%20Google%20API&#34;&gt;Geocoding Google API&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/elsowiny&#34;&gt;Sherief Elsowiny&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;42&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/WEB%20SCRAPING/News_Article_Scraping&#34;&gt;News Article Scraping&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/KanakamSasikalyan&#34;&gt;SasiKalyan&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;43&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/GAMES/SudokuSolver&#34;&gt;Sudoku Solver&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/RgrMz&#34;&gt;Ruben Grande MuÃ±oz&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;44&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/AUTOMATION/Remove%20Duplicate%20Files%20in%20Folder&#34;&gt;Duplicate File Remover&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mas-designs&#34;&gt;Michael Stadler&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;45&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/IMAGES%20%26%20PHOTO%20SCRIPTS/ImageDivider&#34;&gt;Image Divider&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/GSAUC3&#34;&gt;Rajarshi Banerjee&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;46&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/CONVERSION%20SCRIPTS/Morse-Code-Converter&#34;&gt;Morse Code Converter&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/HarshitRV&#34;&gt;HarshitRV&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;47&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/IMAGES%20%26%20PHOTO%20SCRIPTS/CLI-Photo-Watermark&#34;&gt;CLI Photo Watermark&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/odinmay&#34;&gt;Odin May&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;48&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/HarshitRV/Python-project-Scripts/tree/main/Pomodoro-App&#34;&gt;Pomodoro App&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/HarshitRV&#34;&gt;HarshitRV&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;49&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/GAMES/BullsAndCows&#34;&gt;BullsAndCows&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/jerrychen1990&#34;&gt;JerryChen&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;50&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/nrp114/Minsweeper_AI&#34;&gt;Minesweeper AI&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/nrp114&#34;&gt;Nisarg Patel&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;51&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Sdccoding/Python-project-Scripts/tree/main/PDF_Downloader&#34;&gt;PDF Downloader&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Sdccoding&#34;&gt;Souhardya Das Chowdhury&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;52&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/GAMES/ConsoleSnake&#34;&gt;ConsoleSnake&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/tomimara52&#34;&gt;tomimara52&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;53&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/GAMES/ConsoleMinesweeper&#34;&gt;ConsoleMinesweeper&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/tomimara52&#34;&gt;tomimara52&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;54&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/erastusnzula/Python-project-Scripts/tree/face_recognition/Face_recognition&#34;&gt;Face_recognition&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/erastusnzula&#34;&gt;erastusnzula&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;55&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Puskchan/Python-project-Scripts/tree/main/Rain_Alert&#34;&gt;Rain Alert&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Puskchan&#34;&gt;Puskchan&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;56&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/GAMES/Brick-Breaker_Game&#34;&gt;Brick Breaker Game&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/kaustav202&#34;&gt;Kaustav Ganguly&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;57&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/PYTHON%20APPS/FinanceTracker&#34;&gt;Finance App&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Aditya-Tripuraneni&#34;&gt;Aditya Tripuraneni&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;58&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/GAMES/VirtualHandPainter&#34;&gt;Hand Painter&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Aditya-Tripuraneni&#34;&gt;Aditya Tripuraneni&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;59&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/IMAGES%20%26%20PHOTO%20SCRIPTS/Image-Inverter&#34;&gt;Image-Inverter&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/omar-danasoury&#34;&gt;Omar Eldanasoury&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;60&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/larymak/Python-project-Scripts/tree/main/GAMES/Snake_Game(Using%20Turtle)&#34;&gt;Snake Game&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/accodes21&#34;&gt;Aarya Chopkar&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt;</summary>
  </entry>
  <entry>
    <title>sokrypton/ColabFold</title>
    <updated>2022-08-14T02:14:18Z</updated>
    <id>tag:github.com,2022-08-14:/sokrypton/ColabFold</id>
    <link href="https://github.com/sokrypton/ColabFold" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Making Protein folding accessible to all!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ColabFold&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-diff&#34;&gt;+ 2022/07/13: We have set up a new ColabFold MSA server provided by Korean Bioinformation Center. &#xA;+             It provides accelerated MSA generation, we updated the UniRef30 to 2022_02 and PDB/PDB70 to 220313.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/sokrypton/ColabFold/raw/main/.github/ColabFold_Marv_Logo.png&#34; height=&#34;250&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Making Protein folding accessible to all via Google Colab!&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Notebooks&lt;/th&gt; &#xA;   &lt;th&gt;monomers&lt;/th&gt; &#xA;   &lt;th&gt;complexes&lt;/th&gt; &#xA;   &lt;th&gt;mmseqs2&lt;/th&gt; &#xA;   &lt;th&gt;jackhmmer&lt;/th&gt; &#xA;   &lt;th&gt;templates&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/AlphaFold2.ipynb&#34;&gt;AlphaFold2_mmseqs2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/batch/AlphaFold2_batch.ipynb&#34;&gt;AlphaFold2_batch&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/RoseTTAFold.ipynb&#34;&gt;RoseTTAFold&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/deepmind/alphafold/blob/main/notebooks/AlphaFold.ipynb&#34;&gt;AlphaFold2&lt;/a&gt; (from Deepmind)&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;BETA (in development) notebooks&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/beta/omegafold.ipynb&#34;&gt;OmegaFold&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/beta/AlphaFold2_advanced.ipynb&#34;&gt;AlphaFold2_advanced&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;OLD retired notebooks&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/AlphaFold2_complexes.ipynb&#34;&gt;AlphaFold2_complexes&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/beta/AlphaFold_wJackhmmer.ipynb&#34;&gt;AlphaFold2_jackhmmer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/verbose/alphafold_noTemplates_noMD.ipynb&#34;&gt;AlphaFold2_noTemplates_noMD&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/verbose/alphafold_noTemplates_yesMD.ipynb&#34;&gt;AlphaFold2_noTemplates_yesMD&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;FAQ&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Where can I chat with other ColabFold users? &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;See our &lt;a href=&#34;https://discord.gg/gna8maru7d&#34;&gt;Discord&lt;/a&gt; channel!&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Can I use the models for &lt;strong&gt;Molecular Replacement&lt;/strong&gt;? &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Yes, but be &lt;strong&gt;CAREFUL&lt;/strong&gt;, the bfactor column is populated with pLDDT confidence values (higher = better). Phenix.phaser expects a &#34;real&#34; bfactor, where (lower = better). See &lt;a href=&#34;https://twitter.com/cheshireminima/status/1423929241675120643&#34;&gt;post&lt;/a&gt; from Claudia MillÃ¡n.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;What is the maximum length? &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Limits depends on free GPU provided by Google-Colab &lt;code&gt;fingers-crossed&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;For GPU: &lt;code&gt;Tesla T4&lt;/code&gt; or &lt;code&gt;Tesla P100&lt;/code&gt; with ~16G the max length is ~1400&lt;/li&gt; &#xA;   &lt;li&gt;For GPU: &lt;code&gt;Tesla K80&lt;/code&gt; with ~12G the max length is ~1000&lt;/li&gt; &#xA;   &lt;li&gt;To check what GPU you got, open a new code cell and type &lt;code&gt;!nvidia-smi&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Is it okay to use the MMseqs2 MSA server (&lt;code&gt;cf.run_mmseqs2&lt;/code&gt;) on a local computer? &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;You can access the server from a local computer if you queries are serial from a single IP. Please do not use multiple computers to query the server.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Where can I download the databases used by ColabFold? &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The databases are available at &lt;a href=&#34;https://colabfold.mmseqs.com&#34;&gt;colabfold.mmseqs.com&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;I want to render my own images of the predicted structures, how do I color by pLDDT? &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;In pymol for AlphaFold structures: &lt;code&gt;spectrum b, red_yellow_green_cyan_blue, minimum=50, maximum=90&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;In pymol for RoseTTAFold structures: &lt;code&gt;spectrum b, red_yellow_green_cyan_blue, minimum=0.5, maximum=0.9&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;What is the difference between the AlphaFold2_advanced and AlphaFold2_mmseqs2 (_batch) notebook for complex prediction? &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;We currently have two different ways to predict protein complexes: (1) using the AlphaFold2 model with residue index jump and (2) using the AlphaFold2-multimer model. AlphaFold2_advanced supports (1) and AlphaFold2_mmseqs2 (_batch) (2).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;What is the difference between localcolabfold and the pip installable colabfold_batch? &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;localcolabfold is a command line interface for our advanced notebooks. pip is a command line version of the alphafold_mmseqs2 and alphafold_batch notebook.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Running locally&lt;/h3&gt; &#xA;&lt;p&gt;_Note: Checkout &lt;a href=&#34;https://github.com/YoshitakaMo/localcolabfold&#34;&gt;localcolabfold&lt;/a&gt; too&lt;/p&gt; &#xA;&lt;p&gt;Install ColabFold using the &lt;code&gt;pip&lt;/code&gt; commands below. &lt;code&gt;pip&lt;/code&gt; will resolve and install all required dependencies and ColabFold should be ready within a few minutes to use. Please check the &lt;a href=&#34;https://github.com/google/jax#pip-installation-gpu-cuda&#34;&gt;JAX documentation&lt;/a&gt; for how to get JAX to work on your GPU or TPU.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install &#34;colabfold[alphafold] @ git+https://github.com/sokrypton/ColabFold&#34;&#xA;pip install -q &#34;jax[cuda]&amp;gt;=0.3.8,&amp;lt;0.4&#34; -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html&#xA;# For template-based predictions also install kalign and hhsuite&#xA;conda install -c conda-forge -c bioconda kalign2=2.04 hhsuite=3.3.0&#xA;# For amber also install openmm and pdbfixer&#xA;conda install -c conda-forge openmm=7.5.1 pdbfixer&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;colabfold_batch &amp;lt;directory_with_fasta_files&amp;gt; &amp;lt;result_dir&amp;gt; &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If no GPU or TPU is present, &lt;code&gt;colabfold_batch&lt;/code&gt; can be executed (slowly) using only a CPU with the &lt;code&gt;--cpu&lt;/code&gt; parameter.&lt;/p&gt; &#xA;&lt;h3&gt;Generating MSAs for large scale structure/complex predictions&lt;/h3&gt; &#xA;&lt;p&gt;First create a directory for the databases on a disk with sufficient storage (940GB (!)). Depending on where you are, this will take a couple of hours:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;./setup_databases.sh /path/to/db_folder&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download and unpack mmseqs (Note: The required features aren&#39;t in a release yet, so currently, you need to compile the latest version from source yourself or use a &lt;a href=&#34;https://mmseqs.com/latest/mmseqs-linux-avx2.tar.gz&#34;&gt;static binary&lt;/a&gt;). If mmseqs is not in your &lt;code&gt;PATH&lt;/code&gt;, replace &lt;code&gt;mmseqs&lt;/code&gt; below with the path to your mmseqs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# This needs a lot of CPU&#xA;colabfold_search input_sequences.fasta /path/to/db_folder msas&#xA;# This needs a GPU&#xA;colabfold_batch msas predictions&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will create intermediate folder &lt;code&gt;msas&lt;/code&gt; that contains all input multiple sequence alignments formated as a3m files and a &lt;code&gt;predictions&lt;/code&gt; folder with all predicted pdb,json and png files.&lt;/p&gt; &#xA;&lt;p&gt;Searches against the ColabFoldDB can be done in two different modes:&lt;/p&gt; &#xA;&lt;p&gt;(1) Batch searches with many sequences against the ColabFoldDB quires a machine with approx. 128GB RAM. The search should be performed on the same machine that called &lt;code&gt;setup_databases.sh&lt;/code&gt; since the database index size is adjusted to the main memory size. To search on computers with less main memory delete the index by removing all &lt;code&gt;.idx&lt;/code&gt; files, this will force MMseqs2 to create an index on the fly in memory. MMSeqs2 is optimized for large input sequence sets sizes. For batch searches use the &lt;code&gt;--db-load-mode 0&lt;/code&gt; option.&lt;/p&gt; &#xA;&lt;p&gt;(2) single query searches require the full index (the .idx files) to be kept in memory. This can be done with e.g. by using &lt;a href=&#34;https://github.com/hoytech/vmtouch&#34;&gt;vmtouch&lt;/a&gt;. Thus, this type of search requires a machine with at least 768GB RAM for the ColabfoldDB. If the index is in memory use to &lt;code&gt;--db-load-mode 3&lt;/code&gt; parameter in &lt;code&gt;colabfold_search&lt;/code&gt; to avoid index loading overhead. If they database is already in memory use &lt;code&gt;--db-load-mode 2&lt;/code&gt; option.&lt;/p&gt; &#xA;&lt;h3&gt;Tutorials &amp;amp; Presentations&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ColabFold Tutorial presented at the Boston Protein Design and Modeling Club. &lt;a href=&#34;https://www.youtube.com/watch?v=Rfw7thgGTwI&#34;&gt;[video]&lt;/a&gt; &lt;a href=&#34;https://docs.google.com/presentation/d/1mnffk23ev2QMDzGZ5w1skXEadTe54l8-Uei6ACce8eI&#34;&gt;[slides]&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Projects based on ColabFold or helpers&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/YoshitakaMo/localcolabfold&#34;&gt;Run ColabFold on your local computer&lt;/a&gt; by Yoshitaka Moriwaki&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zephyris/discoba_alphafold&#34;&gt;ColabFold/AlphaFold2 for protein structure predictions for Discoba species&lt;/a&gt; by Richard John Wheeler&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pablo-arantes/Making-it-rain&#34;&gt;Cloud-based molecular simulations for everyone&lt;/a&gt; by Pablo R. Arantes, Marcelo D. PolÃªto, Conrado Pedebos and Rodrigo Ligabue-Braun&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.getmoonbear.com/AlphaFold2&#34;&gt;getmoonbear is a webserver to predict protein structures&lt;/a&gt; by Stephanie Zhang and Neil Deshmukh&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/normandavey/AlphaFold2-IDR-complex-prediction&#34;&gt;ColabFold/AlphaFold2 IDR complex prediction&lt;/a&gt; by Balint Meszaros&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/phenix-project/Colabs/blob/main/alphafold2/AlphaFold2.ipynb&#34;&gt;ColabFold/AlphaFold2 (Phenix version) for macromolecular structure determination&lt;/a&gt; by Tom Terwilliger&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/mattarnoldbio/alphapickle/blob/main/AlphaPickle.ipynb&#34;&gt;AlphaPickle: making AlphaFold2/ColabFold outputs interpretable&lt;/a&gt; by Matt Arnold&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Acknowledgments&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We would like to thank the &lt;a href=&#34;https://github.com/RosettaCommons/RoseTTAFold&#34;&gt;RoseTTAFold&lt;/a&gt; and &lt;a href=&#34;https://github.com/deepmind/alphafold&#34;&gt;AlphaFold&lt;/a&gt; team for doing an excellent job open sourcing the software.&lt;/li&gt; &#xA; &lt;li&gt;Also credit to &lt;a href=&#34;https://github.com/dkoes&#34;&gt;David Koes&lt;/a&gt; for his awesome &lt;a href=&#34;https://3dmol.csb.pitt.edu/&#34;&gt;py3Dmol&lt;/a&gt; plugin, without whom these notebooks would be quite boring!&lt;/li&gt; &#xA; &lt;li&gt;A colab by Sergey Ovchinnikov (@sokrypton), Milot Mirdita (@milot_mirdita) and Martin Steinegger (@thesteinegger).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;How do I reference this work?&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Mirdita M, SchÃ¼tze K, Moriwaki Y, Heo L, Ovchinnikov S and Steinegger M. ColabFold: Making protein folding accessible to all. &lt;br&gt; Nature Methods (2022) doi: &lt;a href=&#34;https://www.nature.com/articles/s41592-022-01488-1&#34;&gt;10.1038/s41592-022-01488-1&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;If youâ€™re using &lt;strong&gt;AlphaFold&lt;/strong&gt;, please also cite: &lt;br&gt; Jumper et al. &#34;Highly accurate protein structure prediction with AlphaFold.&#34; &lt;br&gt; Nature (2021) doi: &lt;a href=&#34;https://doi.org/10.1038/s41586-021-03819-2&#34;&gt;10.1038/s41586-021-03819-2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;If youâ€™re using &lt;strong&gt;AlphaFold-multimer&lt;/strong&gt;, please also cite: &lt;br&gt; Evans et al. &#34;Protein complex prediction with AlphaFold-Multimer.&#34; &lt;br&gt; biorxiv (2021) doi: &lt;a href=&#34;https://www.biorxiv.org/content/10.1101/2021.10.04.463034v1&#34;&gt;10.1101/2021.10.04.463034v1&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;If you are using &lt;strong&gt;RoseTTAFold&lt;/strong&gt;, please also cite: &lt;br&gt; Minkyung et al. &#34;Accurate prediction of protein structures and interactions using a three-track neural network.&#34; &lt;br&gt; Science (2021) doi: &lt;a href=&#34;https://doi.org/10.1126/science.abj8754&#34;&gt;10.1126/science.abj8754&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://doi.org/10.5281/zenodo.5123296&#34;&gt;&lt;img src=&#34;https://zenodo.org/badge/doi/10.5281/zenodo.5123296.svg?sanitize=true&#34; alt=&#34;DOI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;OLD Updates&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-diff&#34;&gt;  11Mar2022: We use in default AlphaFold-multimer-v2 weights for complex modeling. &#xA;             We also offer the old complex modes &#34;AlphaFold-ptm&#34; or &#34;AlphaFold-multimer-v1&#34;&#xA;  04Mar2022: ColabFold now uses a much more powerful server for MSAs and searches through the ColabFoldDB instead of BFD/MGnify. &#xA;             Please let us know if you observe any issues.&#xA;  26Jan2022: AlphaFold2_mmseqs2, AlphaFold2_batch and colabfold_batch&#39;s multimer complexes predictions are &#xA;             now in default reranked by iptmscore*0.8+ptmscore*0.2 instead of ptmscore&#xA;  16Aug2021: WARNING - MMseqs2 API is undergoing upgrade, you may see error messages.&#xA;  17Aug2021: If you see any errors, please report them.&#xA;  17Aug2021: We are still debugging the MSA generation procedure...&#xA;  20Aug2021: WARNING - MMseqs2 API is undergoing upgrade, you may see error messages.&#xA;             To avoid Google Colab from crashing, for large MSA we did -diff 1000 to get &#xA;             1K most diverse sequences. This caused some large MSA to degrade in quality,&#xA;             as sequences close to query were being merged to single representive.&#xA;             We are working on updating the server (today) to fix this, by making sure&#xA;             that both diverse and sequences close to query are included in the final MSA.&#xA;             We&#39;ll post update here when update is complete.&#xA;  21Aug2021  The MSA issues should now be resolved! Please report any errors you see.&#xA;             In short, to reduce MSA size we filter (qsc &amp;gt; 0.8, id &amp;gt; 0.95) and take 3K&#xA;             most diverse sequences at different qid (sequence identity to query) intervals &#xA;             and merge them. More specifically 3K sequences at qid at (0â†’0.2),(0.2â†’0.4),&#xA;             (0.4â†’0.6),(0.6â†’0.8) and (0.8â†’1). If you submitted your sequence between&#xA;             16Aug2021 and 20Aug2021, we recommend submitting again for best results!&#xA;  21Aug2021  The use_templates option in AlphaFold2_mmseqs2 is not properly working. We are&#xA;             working on fixing this. If you are not using templates, this does not affect the&#xA;             the results. Other notebooks that do not use_templates are unaffected.&#xA;  21Aug2021  The templates issue is resolved!&#xA;  11Nov2021  [AlphaFold2_mmseqs2] now uses Alphafold-multimer for complex (homo/hetero-oligomer) modeling.&#xA;             Use [AlphaFold2_advanced] notebook for the old complex prediction logic. &#xA;  11Nov2021  ColabFold can be installed locally using pip!&#xA;  14Nov2021  Template based predictions works again in the Alphafold2_mmseqs2 notebook.&#xA;  14Nov2021  WARNING &#34;Single-sequence&#34; mode in AlphaFold2_mmseqs2 and AlphaFold2_batch was broken &#xA;             starting 11Nov2021. The MMseqs2 MSA was being used regardless of selection.&#xA;  14Nov2021  &#34;Single-sequence&#34; mode is now fixed.&#xA;  20Nov2021  WARNING &#34;AMBER&#34; mode in AlphaFold2_mmseqs2 and AlphaFold2_batch was broken &#xA;             starting 11Nov2021. Unrelaxed proteins were returned instead.&#xA;  20Nov2021  &#34;AMBER&#34; is fixed thanks to Kevin Pan&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt;</summary>
  </entry>
</feed>