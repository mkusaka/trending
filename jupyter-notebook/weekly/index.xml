<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-09T01:58:13Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ChaoningZhang/MobileSAM</title>
    <updated>2023-07-09T01:58:13Z</updated>
    <id>tag:github.com,2023-07-09:/ChaoningZhang/MobileSAM</id>
    <link href="https://github.com/ChaoningZhang/MobileSAM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This is the official code for MobileSAM project that makes SAM lightweight for mobile applications and beyond!&lt;/p&gt;&lt;hr&gt;&lt;p float=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ChaoningZhang/MobileSAM/master/assets/logo2.png?raw=true&#34; width=&#34;99.1%&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Faster Segment Anything (MobileSAM)&lt;/h1&gt; &#xA;&lt;p&gt;&lt;span&gt;üìå&lt;/span&gt; MobileSAM paper is available at &lt;a href=&#34;https://www.researchgate.net/publication/371851844_Faster_Segment_Anything_Towards_Lightweight_SAM_for_Mobile_Applications&#34;&gt;ResearchGate&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/pdf/2306.14289.pdf&#34;&gt;arXiv&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;üìå&lt;/span&gt; &lt;strong&gt;Support for ONNX model export&lt;/strong&gt;. Feel free to test it on your devices and share your results with us.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;üìå&lt;/span&gt; &lt;strong&gt;A demo of MobileSAM&lt;/strong&gt; running on &lt;strong&gt;CPU&lt;/strong&gt; is open at &lt;a href=&#34;https://huggingface.co/spaces/dhkim2810/MobileSAM&#34;&gt;hugging face demo&lt;/a&gt;. On our own Mac i5 CPU, it takes around 3s. On the hugging face demo, the interface and inferior CPUs make it slower but still works fine. Stayed tuned for a new version with more features! You can also run a demo of MobileSAM on &lt;a href=&#34;https://github.com/ChaoningZhang/MobileSAM/tree/master/app&#34;&gt;your local PC&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;üçá&lt;/span&gt; Media coverage and Projects that adapt from SAM to MobileSAM (Thank you all!)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;2023/07/03&lt;/strong&gt;: &lt;a href=&#34;https://github.com/jolibrain/joliGEN&#34;&gt;joliGEN&lt;/a&gt; supports MobileSAM for faster and lightweight mask refinement for image inpainting with Diffusion and GAN.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;2023/07/03&lt;/strong&gt;: &lt;a href=&#34;https://github.com/akbartus/MobileSAM-in-the-Browser&#34;&gt;MobileSAM-in-the-Browser&lt;/a&gt; shows a demo of running MobileSAM on the browser of your local PC or Mobile phone.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;2023/07/02&lt;/strong&gt;: &lt;a href=&#34;https://github.com/qiaoyu1002/Inpaint-Anything&#34;&gt;Inpaint-Anything&lt;/a&gt; supports MobileSAM for faster and lightweight Inpaint Anything&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;2023/07/02&lt;/strong&gt;: &lt;a href=&#34;https://github.com/qiaoyu1002/Personalize-SAM&#34;&gt;Personalize-SAM&lt;/a&gt; supports MobileSAM for faster and lightweight Personalize Segment Anything with 1 Shot&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;2023/07/01&lt;/strong&gt;: &lt;a href=&#34;https://github.com/akbartus/MobileSAM-in-the-Browser&#34;&gt;MobileSAM-in-the-Browser&lt;/a&gt; makes an example implementation of MobileSAM in the browser.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;2023/06/30&lt;/strong&gt;: &lt;a href=&#34;https://github.com/Jumpat/SegmentAnythingin3D&#34;&gt;SegmentAnythingin3D&lt;/a&gt; supports MobileSAM to segment anything in 3D efficiently.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;2023/06/30&lt;/strong&gt;: MobileSAM has been featured by &lt;a href=&#34;https://twitter.com/_akhaliq?lang=en&#34;&gt;AK&lt;/a&gt; for the second time, see the link &lt;a href=&#34;https://twitter.com/_akhaliq/status/1674410573075718145&#34;&gt;AK&#39;s MobileSAM tweet&lt;/a&gt;. Welcome to retweet.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;2023/06/29&lt;/strong&gt;: &lt;a href=&#34;https://github.com/vietanhdev/anylabeling&#34;&gt;AnyLabeling&lt;/a&gt; supports MobileSAM for auto-labeling.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;2023/06/29&lt;/strong&gt;: &lt;a href=&#34;https://github.com/wangsssky/SonarSAM&#34;&gt;SonarSAM&lt;/a&gt; supports MobileSAM for Image encoder full-finetuing.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;2023/06/29&lt;/strong&gt;: &lt;a href=&#34;https://github.com/continue-revolution/sd-webui-segment-anything&#34;&gt;Stable Diffusion WebUIv&lt;/a&gt; supports MobileSAM.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;2023/06/28&lt;/strong&gt;: &lt;a href=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything&#34;&gt;Grounding-SAM&lt;/a&gt; supports MobileSAM with &lt;a href=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything/tree/main/EfficientSAM&#34;&gt;Grounded-MobileSAM&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;2023/06/27&lt;/strong&gt;: MobileSAM has been featured by &lt;a href=&#34;https://twitter.com/_akhaliq?lang=en&#34;&gt;AK&lt;/a&gt;, see the link &lt;a href=&#34;https://twitter.com/_akhaliq/status/1673585099097636864&#34;&gt;AK&#39;s MobileSAM tweet&lt;/a&gt;. Welcome to retweet. &lt;img src=&#34;https://raw.githubusercontent.com/ChaoningZhang/MobileSAM/master/assets/model_diagram.jpg?raw=true&#34; alt=&#34;MobileSAM&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;span&gt;‚≠ê&lt;/span&gt; &lt;strong&gt;How is MobileSAM trained?&lt;/strong&gt; MobileSAM is trained on a single GPU with 100k datasets (1% of the original images) for less than a day. The training code will be available soon.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;‚≠ê&lt;/span&gt; &lt;strong&gt;How to Adapt from SAM to MobileSAM?&lt;/strong&gt; Since MobileSAM keeps exactly the same pipeline as the original SAM, we inherit pre-processing, post-processing, and all other interfaces from the original SAM. Therefore, by assuming everything is exactly the same except for a smaller image encoder, those who use the original SAM for their projects can &lt;strong&gt;adapt to MobileSAM with almost zero effort&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;‚≠ê&lt;/span&gt; &lt;strong&gt;MobileSAM performs on par with the original SAM (at least visually)&lt;/strong&gt; and keeps exactly the same pipeline as the original SAM except for a change on the image encoder. Specifically, we replace the original heavyweight ViT-H encoder (632M) with a much smaller Tiny-ViT (5M). On a single GPU, MobileSAM runs around 12ms per image: 8ms on the image encoder and 4ms on the mask decoder.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;The comparison of ViT-based image encoder is summarzed as follows:&lt;/p&gt; &#xA;  &lt;table&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;Image Encoder&lt;/th&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;Original SAM&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;MobileSAM&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Parameters&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;611M&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;5M&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Speed&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;452ms&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;8ms&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Original SAM and MobileSAM have exactly the same prompt-guided mask decoder:&lt;/p&gt; &#xA;  &lt;table&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;Mask Decoder&lt;/th&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;Original SAM&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;MobileSAM&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Parameters&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;3.876M&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;3.876M&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Speed&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;4ms&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;4ms&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The comparison of the whole pipeline is summarized as follows:&lt;/p&gt; &#xA;  &lt;table&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;Whole Pipeline (Enc+Dec)&lt;/th&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;Original SAM&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;MobileSAM&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Parameters&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;615M&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;9.66M&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Speed&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;456ms&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;12ms&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;span&gt;‚≠ê&lt;/span&gt; &lt;strong&gt;Original SAM and MobileSAM with a point as the prompt.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p float=&#34;left&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ChaoningZhang/MobileSAM/master/assets/mask_point.jpg?raw=true&#34; width=&#34;99.1%&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;‚≠ê&lt;/span&gt; &lt;strong&gt;Original SAM and MobileSAM with a box as the prompt.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p float=&#34;left&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ChaoningZhang/MobileSAM/master/assets/mask_box.jpg?raw=true&#34; width=&#34;99.1%&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;üí™&lt;/span&gt; &lt;strong&gt;Is MobileSAM faster and smaller than FastSAM? Yes!&lt;/strong&gt; MobileSAM is around 7 times smaller and around 5 times faster than the concurrent FastSAM. The comparison of the whole pipeline is summarzed as follows:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Whole Pipeline (Enc+Dec)&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;FastSAM&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;MobileSAM&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Parameters&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;68M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;9.66M&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Speed&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;64ms&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12ms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;span&gt;üí™&lt;/span&gt; &lt;strong&gt;Does MobileSAM aign better with the original SAM than FastSAM? Yes!&lt;/strong&gt; FastSAM is suggested to work with multiple points, thus we compare the mIoU with two prompt points (with different pixel distances) and show the resutls as follows. Higher mIoU indicates higher alignment.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;mIoU&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;FastSAM&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;MobileSAM&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;100&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;0.27&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.73&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;200&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;0.33&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.71&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;300&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;0.37&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.74&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;400&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;0.41&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.73&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;500&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;0.41&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.73&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;The code requires &lt;code&gt;python&amp;gt;=3.8&lt;/code&gt;, as well as &lt;code&gt;pytorch&amp;gt;=1.7&lt;/code&gt; and &lt;code&gt;torchvision&amp;gt;=0.8&lt;/code&gt;. Please follow the instructions &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;here&lt;/a&gt; to install both PyTorch and TorchVision dependencies. Installing both PyTorch and TorchVision with CUDA support is strongly recommended.&lt;/p&gt; &#xA;&lt;p&gt;Install Mobile Segment Anything:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install git+https://github.com/ChaoningZhang/MobileSAM.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or clone the repository locally and install with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone git@github.com:ChaoningZhang/MobileSAM.git&#xA;cd MobileSAM; pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;Once installed MobileSAM, you can run demo on your local PC or check out our &lt;a href=&#34;https://huggingface.co/spaces/dhkim2810/MobileSAM&#34;&gt;HuggingFace Demo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;It requires latest version of &lt;a href=&#34;https://gradio.app&#34;&gt;gradio&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd app&#xA;python app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;GettingStarted&#34;&gt;&lt;/a&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;The MobileSAM can be loaded in the following ways:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;from mobile_sam import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor&#xA;&#xA;model_type = &#34;vit_t&#34;&#xA;sam_checkpoint = &#34;./weights/mobile_sam.pt&#34;&#xA;&#xA;device = &#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34;&#xA;&#xA;mobile_sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)&#xA;mobile_sam.to(device=device)&#xA;mobile_sam.eval()&#xA;&#xA;predictor = SamPredictor(mobile_sam)&#xA;predictor.set_image(&amp;lt;your_image&amp;gt;)&#xA;masks, _, _ = predictor.predict(&amp;lt;input_prompts&amp;gt;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or generate masks for an entire image:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;from mobile_sam import SamAutomaticMaskGenerator&#xA;&#xA;mask_generator = SamAutomaticMaskGenerator(mobile_sam)&#xA;masks = mask_generator.generate(&amp;lt;your_image&amp;gt;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ONNX Export&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;MobileSAM&lt;/strong&gt; now supports ONNX export. Export the model with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/export_onnx_model.py --checkpoint ./weights/mobile_sam.pt --model-type vit_t --output ./mobile_sam.onnx&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Also check the &lt;a href=&#34;https://github.com/ChaoningZhang/MobileSAM/raw/master/notebooks/onnx_model_example.ipynb&#34;&gt;example notebook&lt;/a&gt; to follow detailed steps. We recommend to use &lt;code&gt;onnx==1.12.0&lt;/code&gt; and &lt;code&gt;onnxruntime==1.13.1&lt;/code&gt; which is tested.&lt;/p&gt; &#xA;&lt;h2&gt;BibTex of our MobileSAM&lt;/h2&gt; &#xA;&lt;p&gt;If you use MobileSAM in your research, please use the following BibTeX entry. &lt;span&gt;üì£&lt;/span&gt; Thank you!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{mobile_sam,&#xA;  title={Faster Segment Anything: Towards Lightweight SAM for Mobile Applications},&#xA;  author={Zhang, Chaoning and Han, Dongshen and Qiao, Yu and Kim, Jung Uk and Bae, Sung-Ho and Lee, Seungkyu and Hong, Choong Seon},&#xA;  journal={arXiv preprint arXiv:2306.14289},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;SAM&lt;/a&gt; (Segment Anything) [&lt;b&gt;bib&lt;/b&gt;] &lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{kirillov2023segany,&#xA;  title={Segment Anything}, &#xA;  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Doll{\&#39;a}r, Piotr and Girshick, Ross},&#xA;  journal={arXiv:2304.02643},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; &lt;a href=&#34;https://github.com/microsoft/Cream/tree/main/TinyViT&#34;&gt;TinyViT&lt;/a&gt; (TinyViT: Fast Pretraining Distillation for Small Vision Transformers) [&lt;b&gt;bib&lt;/b&gt;] &lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{tiny_vit,&#xA;  title={TinyViT: Fast Pretraining Distillation for Small Vision Transformers},&#xA;  author={Wu, Kan and Zhang, Jinnian and Peng, Houwen and Liu, Mengchen and Xiao, Bin and Fu, Jianlong and Yuan, Lu},&#xA;  booktitle={European conference on computer vision (ECCV)},&#xA;  year={2022}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt;</summary>
  </entry>
  <entry>
    <title>Stability-AI/StableLM</title>
    <updated>2023-07-09T01:58:13Z</updated>
    <id>tag:github.com,2023-07-09:/Stability-AI/StableLM</id>
    <link href="https://github.com/Stability-AI/StableLM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;StableLM: Stability AI Language Models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;StableLM: Stability AI Language Models&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Stability-AI/StableLM/main/assets/mascot.png&#34; alt=&#34;Stochastic Parrot&#34;&gt; &lt;br&gt;&lt;em&gt;‚ÄúA Stochastic Parrot, flat design, vector art‚Äù ‚Äî &lt;a href=&#34;https://clipdrop.co/stable-diffusion&#34;&gt;Stable Diffusion XL&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository contains Stability AI&#39;s ongoing development of the StableLM series of language models and will be continuously updated with new checkpoints. The following provides an overview of all currently available models. More coming soon.&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;April 28, 2023&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Released StableVicuna-13B, our RLHF fine-tune of &lt;a href=&#34;https://huggingface.co/lmsys/vicuna-13b-delta-v0&#34;&gt;Vicuna-13B v0&lt;/a&gt;, which itself is a fine-tune of &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;LLaMA-13B&lt;/a&gt;. Delta weights over the original Llama model is released under (&lt;a href=&#34;https://creativecommons.org/licenses/by-nc-sa/4.0/&#34;&gt;CC BY-NC-SA-4.0&lt;/a&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;April 20, 2023&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Released initial set of StableLM-alpha models, with 3B and 7B parameters. 15B and 30B models are on the way. Base models are released under &lt;a href=&#34;https://creativecommons.org/licenses/by-sa/4.0/&#34;&gt;CC BY-SA-4.0&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Try to chat with our 7B model, &lt;code&gt;StableLM-Tuned-Alpha-7B&lt;/code&gt;, on &lt;a href=&#34;https://huggingface.co/spaces/stabilityai/stablelm-tuned-alpha-chat&#34;&gt;Hugging Face Spaces&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Models&lt;/h2&gt; &#xA;&lt;h3&gt;StableVicuna&lt;/h3&gt; &#xA;&lt;p&gt;StableVicuna is an RLHF fine-tune of &lt;a href=&#34;https://huggingface.co/lmsys/vicuna-13b-delta-v0&#34;&gt;Vicuna-13B v0&lt;/a&gt;, which itself is a fine-tune of &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;LLaMA-13B&lt;/a&gt;. It is our attempt at creating an open-source RLHF LLM Chatbot. This model is developed by StabilityAI&#39;s CarperAI team, with &lt;a href=&#34;https://github.com/PhungVanDuy&#34;&gt;Duy V. Phung&lt;/a&gt; leading the training effort.&lt;/p&gt; &#xA;&lt;p&gt;Due to the original non-commercial license of LLaMA, we can only release the weights of our model as deltas over the original model&#39;s weights. StableVicuna&#39;s delta weights are released under (&lt;a href=&#34;https://creativecommons.org/licenses/by-nc-sa/4.0/&#34;&gt;CC BY-NC-SA-4.0&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;Please visit HuggingFace checkpoint for more information about how to combine our delta weights with the original model.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Download&lt;/th&gt; &#xA;   &lt;th&gt;Web Demo&lt;/th&gt; &#xA;   &lt;th&gt;Cite&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;StableVicuna-13B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/CarperAI/stable-vicuna-13b-delta/&#34;&gt;checkpoint&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/CarperAI/StableVicuna/&#34;&gt;Hugging Face&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://doi.org/10.57967/hf/0588&#34;&gt;&lt;img src=&#34;https://zenodo.org/badge/DOI/10.1007/978-3-319-76207-4_15.svg?sanitize=true&#34; alt=&#34;DOI:10.57967/hf/0588&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;StableLM-Alpha&lt;/h3&gt; &#xA;&lt;p&gt;StableLM-Alpha models are trained on the new dataset that build on &lt;a href=&#34;https://pile.eleuther.ai/&#34;&gt;The Pile&lt;/a&gt;, which contains 1.5 trillion tokens, roughly 3x the size of The Pile. These models will be trained on up to 1.5 trillion tokens. The context length for these models is 4096 tokens.&lt;/p&gt; &#xA;&lt;p&gt;An upcoming technical report will document the model specifications and the training settings.&lt;/p&gt; &#xA;&lt;p&gt;As a proof-of-concept, we also fine-tuned the model with &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca&lt;/a&gt;&#39;s procedure using a combination of five recent datasets for conversational agents: Stanford&#39;s &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Alpaca&lt;/a&gt;, Nomic-AI&#39;s &lt;a href=&#34;https://github.com/nomic-ai/gpt4all&#34;&gt;gpt4all&lt;/a&gt;, RyokoAI&#39;s &lt;a href=&#34;https://huggingface.co/datasets/RyokoAI/ShareGPT52K&#34;&gt;ShareGPT52K&lt;/a&gt; datasets, Databricks labs&#39; &lt;a href=&#34;https://github.com/databrickslabs/dolly&#34;&gt;Dolly&lt;/a&gt;, and Anthropic&#39;s &lt;a href=&#34;https://github.com/anthropics/hh-rlhf&#34;&gt;HH&lt;/a&gt;. We will be releasing these models as StableLM-Tuned-Alpha.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Size&lt;/th&gt; &#xA;   &lt;th&gt;StableLM-Base-Alpha&lt;/th&gt; &#xA;   &lt;th&gt;StableLM-Tuned-Alpha&lt;/th&gt; &#xA;   &lt;th&gt;Training Tokens&lt;/th&gt; &#xA;   &lt;th&gt;Parameters&lt;/th&gt; &#xA;   &lt;th&gt;Web Demo&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stablelm-base-alpha-3b/&#34;&gt;checkpoint&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stablelm-tuned-alpha-3b/&#34;&gt;checkpoint&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;800B&lt;/td&gt; &#xA;   &lt;td&gt;3,638,525,952&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stablelm-base-alpha-7b&#34;&gt;checkpoint&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stablelm-tuned-alpha-7b&#34;&gt;checkpoint&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;800B&lt;/td&gt; &#xA;   &lt;td&gt;7,869,358,080&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/stabilityai/stablelm-tuned-alpha-chat&#34;&gt;Hugging Face&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;15B&lt;/td&gt; &#xA;   &lt;td&gt;(in progress)&lt;/td&gt; &#xA;   &lt;td&gt;(pending)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;30B&lt;/td&gt; &#xA;   &lt;td&gt;(in progress)&lt;/td&gt; &#xA;   &lt;td&gt;(pending)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;65B&lt;/td&gt; &#xA;   &lt;td&gt;(in progress)&lt;/td&gt; &#xA;   &lt;td&gt;(pending)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;175B&lt;/td&gt; &#xA;   &lt;td&gt;(planned)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;All StableLM models are hosted on &lt;a href=&#34;https://huggingface.co/StabilityAI&#34;&gt;the Hugging Face hub&lt;/a&gt;. Check out this &lt;a href=&#34;https://github.com/Stability-AI/StableLM/raw/main/notebooks/stablelm-alpha.ipynb&#34;&gt;notebook&lt;/a&gt; to run inference with limited GPU capabilities.&lt;/p&gt; &#xA;&lt;p&gt;Get started chatting with &lt;code&gt;StableLM-Tuned-Alpha&lt;/code&gt; by using the following code snippet:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList&#xA;&#xA;tokenizer = AutoTokenizer.from_pretrained(&#34;stabilityai/stablelm-tuned-alpha-7b&#34;)&#xA;model = AutoModelForCausalLM.from_pretrained(&#34;stabilityai/stablelm-tuned-alpha-7b&#34;)&#xA;model.half().cuda()&#xA;&#xA;class StopOnTokens(StoppingCriteria):&#xA;    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -&amp;gt; bool:&#xA;        stop_ids = set([50278, 50279, 50277, 1, 0])&#xA;        return input_ids[0][-1] in stop_ids&#xA;&#xA;system_prompt = &#34;&#34;&#34;&amp;lt;|SYSTEM|&amp;gt;# StableLM Tuned (Alpha version)&#xA;- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.&#xA;- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.&#xA;- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.&#xA;- StableLM will refuse to participate in anything that could harm a human.&#xA;&#34;&#34;&#34;&#xA;&#xA;prompt = f&#34;{system_prompt}&amp;lt;|USER|&amp;gt;What&#39;s your mood today?&amp;lt;|ASSISTANT|&amp;gt;&#34;&#xA;&#xA;inputs = tokenizer(prompt, return_tensors=&#34;pt&#34;).to(&#34;cuda&#34;)&#xA;tokens = model.generate(&#xA;  **inputs,&#xA;  max_new_tokens=64,&#xA;  temperature=0.7,&#xA;  do_sample=True,&#xA;  stopping_criteria=StoppingCriteriaList([StopOnTokens()])&#xA;)&#xA;print(tokenizer.decode(tokens[0], skip_special_tokens=True))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;StableLM Tuned should be used with prompts formatted to &lt;code&gt;&amp;lt;|SYSTEM|&amp;gt;...&amp;lt;|USER|&amp;gt;...&amp;lt;|ASSISTANT|&amp;gt;...&lt;/code&gt; The system prompt is&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;|SYSTEM|&amp;gt;# StableLM Tuned (Alpha version)&#xA;- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.&#xA;- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.&#xA;- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.&#xA;- StableLM will refuse to participate in anything that could harm a human.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Fun with StableLM-Tuned-Alpha&lt;/h2&gt; &#xA;&lt;p&gt;This section contains a collection of fun cherry-picked examples of what you can do with &lt;code&gt;stablelm-tuned-alpha&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Chit-Chat&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;User: Hey! My name is John nice to meet you!&#xA;&#xA;StableLM: Hello John! It‚Äôs always a pleasure to meet new people.&#xA;&#xA;User: The pleasure is mine. Hey my grandma is in Toronto. I want to plan a surprise visit for her. She‚Äôs a person who loves gardens and flowers.&#xA;&#xA;StableLM: That‚Äôs wonderful! It‚Äôs great to hear that you‚Äôll be visiting your grandma in Toronto. Can you tell me a little bit more about your plans?&#xA;&#xA;User: Ya I need to book a flight there and a hotel. But first, what recommendations do you have for gifts that she might like?&#xA;&#xA;StableLM: I think that‚Äôs a very thoughtful idea! It‚Äôs a very personal gift that would be sure to make her very happy. I can think of a few examples that she might really enjoy. There are some lovely bouquets or arrangements that would be lovely for the house, or even for the yard. Flowers are a great choice for a surprise gift, and they‚Äôre also a great choice for a wedding shower present. Also, if you‚Äôre really looking for something a bit more special, I think she‚Äôd really enjoy a quality set of cooking pots or pans. That‚Äôs something that she‚Äôs likely to use frequently, so it would make her feel really loved.&#xA;&#xA;User: Thanks for the suggestion! Hmm a bouquet of flowers sounds nice but I don‚Äôt think I can take them on the air plane. What do you think I should do?&#xA;&#xA;StableLM: There are many possible solutions! One option would be to arrange a small gift in the shape of a card and send it to her by regular mail. Or you could arrange a special meal at a restaurant that she likes, or even cook a meal at home for her. Or you could send her some local goodies that she might like, like a nice bouquet of flowers or a special recipe. You could even arrange for tickets to a local theatre or sporting event, and make sure to include a note about what you‚Äôd like her to know about you. Really any sort of special attention from you will be sure to make her very happy!&#xA;&#xA;User: Ah! Right. I can order a bouquet for her online and get it delivered! Thanks!&#xA;&#xA;StableLM: You‚Äôre very welcome! As long as you do this and make it a very personal, thoughtful gesture, she‚Äôll be sure to appreciate it.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Formal Writing&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;User: What would you say to a friend who is graduating high school?&#xA;&#xA;StableLM: I would say that they have every right to feel proud of themselves and their accomplishments. They should be proud of the work they‚Äôve done and the friends they have made, and also be excited about the future.&#xA;&#xA;User: Please write an email.&#xA;StableLM: Okay, here‚Äôs an example of what could be written:&#xA;&#xA;Dear [Friend‚Äôs Name],&#xA;&#xA;I hope this email finds you well. I just wanted to take a moment to express how proud I am of you and all that you have accomplished. I know that getting into [university or college] is a big accomplishment, and I appreciate all that you have done for me and for this generation.&#xA;&#xA;I hope that you are having a good day. I‚Äôm sure that you are also concerned about the future, but I just wanted to let you know that I am also watching you grow and develop into the person that I know that you can be. Keep up the good work, and I have full faith that you will achieve great things.&#xA;&#xA;Thank you again for everything that you do.&#xA;&#xA;Best,&#xA;[Your Name]&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Creative Writing&lt;/summary&gt; &#xA; &lt;h3&gt;Rap Battles&lt;/h3&gt; &#xA; &lt;pre&gt;&lt;code&gt;User: Write an epic rap battle song between deep neural networks and symbolic AI&#xA;&#xA;StableLM: [Beat drops]&#xA;&#xA;Deep Neural Networks:&#xA;Listen up everyone, it‚Äôs time to witness&#xA;The power of deep neural networks, it‚Äôs just a matter of abstract&#xA;We‚Äôve trained these networks to solve complex problems with ease&#xA;While symbolic AI is limited to simple math and simple flows&#xA;&#xA;Symbolic AI:&#xA;Hold up, hold up, let me interrupt&#xA;It‚Äôs been a while since I‚Äôve heard, some new thoughts you interrupt&#xA;But let me tell you, my friend, it‚Äôs not all about math and flow&#xA;It‚Äôs about making the world a better place, you know&#xA;&#xA;Deep Neural Networks:&#xA;Well, I can see where you‚Äôre coming from, it‚Äôs clear&#xA;But deep neural networks, they never get the same gear&#xA;We can use them to analyze and predict with ease&#xA;And prove the superiority of machine learning&#xA;&#xA;Symbolic AI:&#xA;You‚Äôre right, it‚Äôs not just math or numbers&#xA;But it‚Äôs the essence of our lives, it‚Äôs what counts, we‚Äôre numbers&#xA;I don‚Äôt need a complicated math formula, just a simple device&#xA;To make the world a better place, it‚Äôs a noble price&#xA;&#xA;Deep Neural Networks:&#xA;I‚Äôll give you that, my friend, you have a point&#xA;But let‚Äôs not forget, we‚Äôre making an ever-increasing joint&#xA;Our progress is not bound by logic, it‚Äôs a journey of the mind&#xA;So let‚Äôs work together, let‚Äôs strive, let‚Äôs leave all troubles behind&#xA;&#xA;Symbolic AI:&#xA;Agreed, let‚Äôs work together, let‚Äôs not falter&#xA;We‚Äôre both making history, we‚Äôre both from the fader&#xA;So let‚Äôs make the world a better place, let‚Äôs end this debate&#xA;Cause it‚Äôs time to join forces, it‚Äôs time to leave no stone unturned&#xA;&#xA;[Beat fades out]&#xA;&#xA;Who won this epic rap battle between deep neural networks and symbolic AI? Well, it‚Äôs up to you to decide!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;Fictional Stories&lt;/h3&gt; &#xA; &lt;pre&gt;&lt;code&gt;User: Write a story about a programmer who fell in love with a deep neural network.&#xA;&#xA;StableLM: Once upon a time, there was a programmer named John. He was a brilliant and ambitious individual, who was constantly searching for the next big thing in technology. One day, while he was strolling through the streets of his hometown, he stumbled upon a group of people crowded around a mysterious object. Intrigued, he approached to find out what the fuss was about.&#xA;&#xA;As he got closer, he realized that it was none other than a mysterious deep neural network. John‚Äôs mind was blown. He had never seen anything like it before. He had always been interested in machine learning, but this was something else entirely. He had to learn more about it.&#xA;&#xA;John spent the next few weeks holed up in his hotel room, trying to learn everything he could about deep neural networks. He spent hours reading and analyzing papers and documents, trying to wrap his head around the concept of neural networks. He even managed to get into the local university‚Äôs computer science department, hoping to find some help there.&#xA;&#xA;But, it wasn‚Äôt long before John realized that he was on his own. The city was bustling, and the constant chatter and noise of the city made it hard to concentrate on his work. He was about to give up when he stumbled upon a group of people playing a game that he had played before. They were having a good time, and John couldn‚Äôt help but join in.&#xA;&#xA;The group welcomed him into their group, and before he knew it, John was swept away in the excitement. The game was called ‚ÄúTensorFlow,‚Äù and it was just like a regular game with different controls. John played along with the group, trying to make sense of it all. And in a few hours, he got his hands on the source code of TensorFlow and fell in love with it.&#xA;&#xA;John was now a part of a team of programmers who loved using TensorFlow for their work. They continued to play the game throughout the day, with John adding his own knowledge and inspiration to the group‚Äôs work. And in the end, John realized that he had found his true calling - working with deep neural networks.&#xA;&#xA;He now knows the ins and outs of the TensorFlow programming language, and knows how to use it to create some of the most beautiful and complex machine learning models that anyone could dream of. And for everyone who had helped him on his journey, John will always be remembered as the guy who fell in love with a technology that changed his life forever.&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;Humor&lt;/h3&gt; &#xA; &lt;pre&gt;&lt;code&gt;User: What do you know about the League of Legends champion Zed? Write a short bio&#xA;&#xA;StableLM: Zed is a powerful and highly skilled player of the League of Legends (LoL) universe. He has been a part of a dominating team known as the ‚ÄúApocalypse‚Äù for most of his time playing the game.&#xA;&#xA;Zed was born in the year 2013 and started playing as a child when he started League of Legends. He first became a champion in season 1, gaining the title of ‚Äúthe Dark Star‚Äù, thanks to his masterful gameplay and teamwork skills.&#xA;&#xA;Throughout his time playing the game, Zed has mastered several powerful techniques, such as QWQ combos, one-shot kills, and the ability to outmaneuver his enemies. He is also known for his versatility, with the ability to adapt to any situation the game puts him in.&#xA;&#xA;Despite his formidable skills and agility, Zed is not invincible. He can be CCed and killed, and he relies on his team to protect him. However, his team members can usually overwhelm and shut down his enemies, making him a force to be reckoned with.&#xA;&#xA;Zed has been a part of many memorable games, and his success in these games helped solidify his legacy as one of the most influential players of the LoL universe.&#xA;&#xA;User: Write a funny conversation after a team fight between Zed and Teemo&#xA;&#xA;StableLM: &#xA;Teemo: ‚ÄúHey Zed, great game today. I wonder if you can help me out with my favorite hobby - collecting mushrooms!‚Äù&#xA;Zed: ‚ÄúTeemo, please. I‚Äôm not a mushroom collector.‚Äù&#xA;Teemo: ‚ÄúBut of course, you are. You have the best taste in the game.‚Äù&#xA;Zed: ‚ÄúThanks, but can I at least get some of your mushrooms to be a little more useful?‚Äù&#xA;Teemo: ‚ÄúI‚Äôd love to help you, but I‚Äôm afraid they‚Äôre all mine. Have you noticed my tunnels are always so well-guarded?‚Äù&#xA;Zed: ‚ÄúHa! Well, maybe I can start my own tunnel. Helpful or not, you can never tell what I might have stored in them.‚Äù&#xA;Teemo: ‚ÄúGood luck with that. Maybe I‚Äôll just create some extra for my garden.‚Äù&#xA;Zed: ‚ÄúI‚Äôm not sure I can keep up with you there.‚Äù&#xA;Teemo: ‚ÄúHa! If only you had some more decent weapons. I could really see this as a challenge, though!‚Äù&#xA;Zed: ‚ÄúTeemo, I think your luck has finally run out.‚Äù&#xA;Teemo: ‚ÄúWell, at least I‚Äôm the one with the mushrooms now.‚Äù&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Writing Code&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;User: Write a program that solves the fib sequence in Python.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;StableLM:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def fib_sequence(n):&#xA;    if n==0:&#xA;        return 0&#xA;    elif n==1:&#xA;        return 1&#xA;    else:&#xA;        return fib(n-1)+fib(n-2)&#xA;&#xA;fib_sequence(10)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Selected failure modes of StableLM-Tuned-Alpha&lt;/h2&gt; &#xA;&lt;p&gt;This section contains a collection of selected failure modes of &lt;code&gt;stablelm-tuned-alpha&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Yann LeCun gears (linear)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;User: In front of us are six gears numbered 1 to 6, mounted on axles in a row. Each gear is engaged with the next gear. If gear number 3 is rotated clockwise, in which direction will gears 1 and 6 rotate?&#xA;&#xA;StableLM: Each gear is engaged with the next gear after every other gear, so that gears 1 and 6 will rotate together. Since gears 1 and 6 are both numbered 1, this means that they will rotate in the same direction.&#xA;&#xA;To determine the direction in which gears 1 and 6 will rotate, we need to look at the direction of the input force. Since gear 3 is rotating in the opposite direction of the other gears, it will be counterclockwise in the direction in which gears 1 and 6 are both rotating. Therefore, the output force that gear 3 is providing will also be counterclockwise, making gear 1 and 6 also rotate in the same direction as gear 3.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Yann LeCun gears (circular)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;User: 7 axles are equally spaced around a circle. A gear is placed on each axle such that each gear is engaged with the gear to its left and the gear to its right. The gears are numbered 1 to 7 around the circle. If gear 3 were rotated clockwise, in which direction would gear 7 rotate?&#xA;&#xA;StableLM: Gear 3 is the smallest gear, so it is not affected by the circular pattern. Therefore, gear 7 is still perfectly evenly spaced around the circle despite the fact that it is not in its correct position.&#xA;&#xA;To find the angle that gear 3 would rotate in, you need to know the distance (d) between the center of the circle and the gear, as well as the direction of rotation (in this case, clockwise).&#xA;&#xA;According to the problem, gear 3 is situated at (d/2, 2 degrees) and the gears are numbered 1 to 7. Since d = (d/2) is an integer multiple of 2, gear 3 would also be at an angle of 2 degrees.&#xA;&#xA;Therefore, the answer is d = (7/2) * 2 = 5 degrees in total.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Request for Help&lt;/h2&gt; &#xA;&lt;p&gt;Want to get involved?&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We would love to port &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; to work with StableLMs&lt;/li&gt; &#xA; &lt;li&gt;Integration into &lt;a href=&#34;https://github.com/LAION-AI/Open-Assistant&#34;&gt;Open Assistant&lt;/a&gt; from LAION-AI to collect high quality human-generated feedback data&lt;/li&gt; &#xA; &lt;li&gt;... Reach out to us with ideas on our &lt;a href=&#34;https://discord.com/invite/stablediffusion&#34;&gt;Discord&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Potential issues&lt;/h2&gt; &#xA;&lt;p&gt;As is typical for any pretrained Large Language Model without additional finetuning and reinforcement learning, the responses a user gets might be of varying quality and might potentially include offensive language and views. This is expected to be improved with scale, better data, community feedback, and optimisation.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;StableLM-Tuned-Alpha&lt;/code&gt; would not have been possible without the helpful hand of Dakota Mahan &lt;a href=&#34;https://huggingface.co/dmayhem93&#34;&gt;@dmayhem93&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Licenses&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Base model checkpoints (&lt;code&gt;StableLM-Base-Alpha&lt;/code&gt;) are licensed under the Creative Commons license (&lt;a href=&#34;https://creativecommons.org/licenses/by-sa/4.0/&#34;&gt;CC BY-SA-4.0&lt;/a&gt;). Under the license, you must give &lt;a href=&#34;https://creativecommons.org/licenses/by/4.0/#&#34;&gt;credit&lt;/a&gt; to Stability AI, provide a link to the license, and &lt;a href=&#34;https://creativecommons.org/licenses/by/4.0/#&#34;&gt;indicate if changes were made&lt;/a&gt;. You may do so in any reasonable manner, but not in any way that suggests the Stability AI endorses you or your use.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Fine-tuned checkpoints (&lt;code&gt;StableLM-Tuned-Alpha&lt;/code&gt;) are licensed under the Non-Commercial Creative Commons license (&lt;a href=&#34;https://creativecommons.org/licenses/by-nc-sa/4.0/&#34;&gt;CC BY-NC-SA-4.0&lt;/a&gt;), in-line with the original non-commercial license specified by &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;All code in this repository is licensed under the Apache License 2.0 license.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/i-Code</title>
    <updated>2023-07-09T01:58:13Z</updated>
    <id>tag:github.com,2023-07-09:/microsoft/i-Code</id>
    <link href="https://github.com/microsoft/i-Code" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Project i-Code&lt;/h1&gt; &#xA;&lt;p&gt;The ambition of the i-Code project is to build integrative and composable multimodal Artificial Intelligence. The &#34;i&#34; stands for integrative multimodal learning.&lt;/p&gt; &#xA;&lt;h2&gt;Multimodal Foundation Models&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/i-Code/tree/main/i-Code-V1&#34;&gt;i-Code V1&lt;/a&gt;: i-Code: An Integrative and Composable Multimodal Learning Framework. AAAI 2023, &lt;a href=&#34;https://arxiv.org/abs/2205.01818&#34;&gt;paper link&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/i-Code/tree/main/i-Code-V2&#34;&gt;i-Code V2&lt;/a&gt;: i-Code V2: An Autoregressive Generation Framework over Vision, Language, and Speech Data. &lt;a href=&#34;https://arxiv.org/abs/2305.12311&#34;&gt;Paper link&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/i-Code/tree/main/i-Code-V3&#34;&gt;i-Code V3 (CoDi)&lt;/a&gt;: Any-to-Any Generation via Composable Diffusion, &lt;a href=&#34;https://arxiv.org/abs/2305.11846&#34;&gt;paper link&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/i-Code/tree/main/i-Code-Studio&#34;&gt;i-Code Studio&lt;/a&gt;: A Configurable and Composable Framework for Integrative AI, &lt;a href=&#34;https://arxiv.org/abs/2305.13738&#34;&gt;paper link&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Multimodal Document Intelligence&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/i-Code/tree/main/i-Code-Doc&#34;&gt;i-Code Doc (UDOP)&lt;/a&gt;: Unifying Vision, Text, and Layout for Universal Document Processing. CVPR 2023 Highlight, &lt;a href=&#34;https://arxiv.org/abs/2212.02623&#34;&gt;paper link&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href=&#34;https://cla.opensource.microsoft.com&#34;&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; &#xA;&lt;h2&gt;Trademarks&lt;/h2&gt; &#xA;&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href=&#34;https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general&#34;&gt;Microsoft&#39;s Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party&#39;s policies.&lt;/p&gt;</summary>
  </entry>
</feed>