<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-10-22T01:54:48Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>brevdev/notebooks</title>
    <updated>2023-10-22T01:54:48Z</updated>
    <id>tag:github.com,2023-10-22:/brevdev/notebooks</id>
    <link href="https://github.com/brevdev/notebooks" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://uohmivykqgnnbiouffke.supabase.co/storage/v1/object/public/landingpage/brevdevnotebooks.png&#34; width=&#34;100%&#34;&gt; &#xA;&lt;!-- Links --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://console.brev.dev&#34; style=&#34;color: #06b6d4;&#34;&gt;Console&lt;/a&gt; • &lt;a href=&#34;https://brev.dev&#34; style=&#34;color: #06b6d4;&#34;&gt;Docs&lt;/a&gt; • &lt;a href=&#34;/&#34; style=&#34;color: #06b6d4;&#34;&gt;Templates&lt;/a&gt; • &lt;a href=&#34;https://discord.gg/NVDyv7TUgJ&#34; style=&#34;color: #06b6d4;&#34;&gt;Discord&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Brev.dev Notebooks&lt;/h1&gt; &#xA;&lt;p&gt;This repo contains helpful AI/ML notebook templates. Each notebook has been coupled with the minimum GPU specs required to use them + setup scripts making a Brev template. Click the deploy badge on any notebook to deploy it.&lt;/p&gt; &#xA;&lt;h2&gt;Notebooks&lt;/h2&gt; &#xA;&lt;!-- make a table  --&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Notebook&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Min. GPU&lt;/th&gt; &#xA;   &lt;th&gt;Deploy&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/brevdev/notebooks/raw/main/llama2-finetune.ipynb&#34;&gt;Fine-tune Llama 2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Fine-tune Llama 2 on your own dataset&lt;/td&gt; &#xA;   &lt;td&gt;1x A10G&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/file/d/1RB0xrrb1GuaTRZ2qjCKuoySmRn5d-lky/view?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://console.brev.dev/environment/new?instance=A10G:g5.xlarge&amp;amp;name=fine-tune-llama2&#34;&gt;&lt;img src=&#34;https://uohmivykqgnnbiouffke.supabase.co/storage/v1/object/public/landingpage/brevdeploynavy.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/brevdev/notebooks/raw/main/mistral-finetune.ipynb&#34;&gt;Fine-tune Mistral&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A Guide to Cheaply Fine-tuning Mistral&lt;/td&gt; &#xA;   &lt;td&gt;1x A10G&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/file/d/1mlr4apb3zM9mxkQMBlbC1CdjorqK6pIx/view?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://console.brev.dev/environment/new?instance=A10G:g5.xlarge&amp;amp;name=mistral-finetune&#34;&gt;&lt;img src=&#34;https://uohmivykqgnnbiouffke.supabase.co/storage/v1/object/public/landingpage/brevdeploynavy.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/brevdev/notebooks/raw/main/mistral-finetune-own-data.ipynb&#34;&gt;Fine-tune Mistral - Own Data&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Fine-tune Mistral on your own dataset&lt;/td&gt; &#xA;   &lt;td&gt;1x A10G&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/file/d/1n_XpTFn10_64NkcHmEF8CUm3E43sPQUo/view?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://console.brev.dev/environment/new?instance=A10G:g5.xlarge&amp;amp;name=mistral-finetune&#34;&gt;&lt;img src=&#34;https://uohmivykqgnnbiouffke.supabase.co/storage/v1/object/public/landingpage/brevdeploynavy.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/brevdev/notebooks/raw/main/julia-install.ipynb&#34;&gt;Julia Install&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Easily Install Julia + Notebooks&lt;/td&gt; &#xA;   &lt;td&gt;any || CPU&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/file/d/1RDnIu87b6a7Uu6Kc8EkJoenq8toVknkj/view?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://console.brev.dev/environment/new?instance=t3a.medium&amp;amp;name=julia&#34;&gt;&lt;img src=&#34;https://uohmivykqgnnbiouffke.supabase.co/storage/v1/object/public/landingpage/brevdeploynavy.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;What is Brev.dev?&lt;/h3&gt; &#xA;&lt;p&gt;Brev is a dev tool that makes it really easy to code on a GPU in the cloud. Brev does 3 things: provision, configure, and connect.&lt;/p&gt; &#xA;&lt;h4&gt;Provision:&lt;/h4&gt; &#xA;&lt;p&gt;Brev provisions a GPU for you. You don&#39;t have to worry about setting up a cloud account. We have solid GPU supply, but if you do have AWS or GCP, you can link them.&lt;/p&gt; &#xA;&lt;h4&gt;Configure:&lt;/h4&gt; &#xA;&lt;p&gt;Brev configures your GPU with the right drivers and libraries. Use our open source tool Verb to point and click the right python and CUDA versions.&lt;/p&gt; &#xA;&lt;h4&gt;Connect:&lt;/h4&gt; &#xA;&lt;p&gt;Brev.dev CLI automatically edits your ssh config so you can &lt;code&gt;ssh gpu-name&lt;/code&gt; or run &lt;code&gt;brev open gpu-name&lt;/code&gt; to open VS Code to the remote machine&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>roboflow/notebooks</title>
    <updated>2023-10-22T01:54:48Z</updated>
    <id>tag:github.com,2023-10-22:/roboflow/notebooks</id>
    <link href="https://github.com/roboflow/notebooks" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Examples and tutorials on using SOTA computer vision models and techniques. Learn everything from old-school ResNet, through YOLO and object-detection transformers like DETR, to the latest models like Grounding DINO and SAM.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt; &lt;a align=&#34;center&#34; href=&#34;&#34; target=&#34;_blank&#34;&gt; &lt;img width=&#34;850&#34; src=&#34;https://raw.githubusercontent.com/roboflow/notebooks/main/assets/roboflow-notebooks-banner.png&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA; &lt;br&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/roboflow/notebooks&#34;&gt;notebooks&lt;/a&gt; | &lt;a href=&#34;https://github.com/roboflow/inference&#34;&gt;inference&lt;/a&gt; | &lt;a href=&#34;https://github.com/autodistill/autodistill&#34;&gt;autodistill&lt;/a&gt; | &lt;a href=&#34;https://github.com/roboflow/roboflow-collect&#34;&gt;collect&lt;/a&gt;&lt;/p&gt; &#xA; &lt;br&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;a href=&#34;https://youtube.com/roboflow&#34;&gt; &lt;img src=&#34;https://media.roboflow.com/notebooks/template/icons/purple/youtube.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949634652&#34; width=&#34;3%&#34;&gt; &lt;/a&gt; &#xA;  &lt;img src=&#34;https://github.com/SkalskiP/SkalskiP/raw/master/icons/transparent.png&#34; width=&#34;3%&#34;&gt; &#xA;  &lt;a href=&#34;https://roboflow.com&#34;&gt; &lt;img src=&#34;https://media.roboflow.com/notebooks/template/icons/purple/roboflow-app.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949746649&#34; width=&#34;3%&#34;&gt; &lt;/a&gt; &#xA;  &lt;img src=&#34;https://github.com/SkalskiP/SkalskiP/raw/master/icons/transparent.png&#34; width=&#34;3%&#34;&gt; &#xA;  &lt;a href=&#34;https://www.linkedin.com/company/roboflow-ai/&#34;&gt; &lt;img src=&#34;https://media.roboflow.com/notebooks/template/icons/purple/linkedin.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949633691&#34; width=&#34;3%&#34;&gt; &lt;/a&gt; &#xA;  &lt;img src=&#34;https://github.com/SkalskiP/SkalskiP/raw/master/icons/transparent.png&#34; width=&#34;3%&#34;&gt; &#xA;  &lt;a href=&#34;https://docs.roboflow.com&#34;&gt; &lt;img src=&#34;https://media.roboflow.com/notebooks/template/icons/purple/knowledge.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949634511&#34; width=&#34;3%&#34;&gt; &lt;/a&gt; &#xA;  &lt;img src=&#34;https://github.com/SkalskiP/SkalskiP/raw/master/icons/transparent.png&#34; width=&#34;3%&#34;&gt; &#xA;  &lt;a href=&#34;https://discuss.roboflow.com&#34;&gt; &lt;img src=&#34;https://media.roboflow.com/notebooks/template/icons/purple/forum.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949633584&#34; width=&#34;3%&#34;&gt; &lt;img src=&#34;https://github.com/SkalskiP/SkalskiP/raw/master/icons/transparent.png&#34; width=&#34;3%&#34;&gt; &lt;/a&gt;&#xA;  &lt;a href=&#34;https://blog.roboflow.com&#34;&gt; &lt;img src=&#34;https://media.roboflow.com/notebooks/template/icons/purple/blog.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949633605&#34; width=&#34;3%&#34;&gt; &lt;/a&gt;  &#xA; &lt;/div&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;👋 hello&lt;/h2&gt; &#xA;&lt;p&gt;Over the years we have created dozens of Computer Vision tutorials. This repository contains examples and tutorials on using SOTA computer vision models and techniques. Learn everything from old-school ResNet, through YOLO and object-detection transformers like DETR, to the latest models like Grounding DINO and SAM.&lt;/p&gt; &#xA;&lt;!-- AUTOGENERATED-NOTEBOOKS-TABLE --&gt; &#xA;&lt;!--&#xA;   WARNING: DO NOT EDIT THIS TABLE MANUALLY. IT IS AUTOMATICALLY GENERATED.&#xA;   HEAD OVER TO CONTRIBUTING.MD FOR MORE DETAILS ON HOW TO MAKE CHANGES PROPERLY.&#xA;--&gt; &#xA;&lt;h2&gt;🚀 model tutorials (28 notebooks)&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;notebook&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;open in colab / kaggle / sagemaker studio lab&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;complementary materials&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;repository / paper&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-rtmdet-object-detection-on-custom-data.ipynb&#34;&gt;RTMDet Object Detection&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-rtmdet-object-detection-on-custom-data.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-rtmdet-object-detection-on-custom-data.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/roboflow-ai/notebooks/blob/main/notebooks/train-rtmdet-object-detection-on-custom-data.ipynb&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/sage-maker.svg?sanitize=true&#34; alt=&#34;SageMaker&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://blog.roboflow.com/how-to-train-rtmdet-on-a-custom-dataset&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&#34; alt=&#34;Roboflow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://youtu.be/5kgWyo6Sg4E&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/youtube.svg?sanitize=true&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmdetection&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/github.svg?sanitize=true&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2212.07784&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2212.07784-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-segment-anything-with-fast-sam.ipynb&#34;&gt;Fast Segment Anything Model (FastSAM)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-segment-anything-with-fast-sam.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-segment-anything-with-fast-sam.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-segment-anything-with-fast-sam.ipynb&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/sage-maker.svg?sanitize=true&#34; alt=&#34;SageMaker&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://blog.roboflow.com/how-to-use-fastsam&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&#34; alt=&#34;Roboflow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://youtu.be/yHNPyqazYYU&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/youtube.svg?sanitize=true&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/CASIA-IVA-Lab/FastSAM&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/github.svg?sanitize=true&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2306.12156&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2306.12156-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolo-nas-on-custom-dataset.ipynb&#34;&gt;YOLO-NAS Object Detection&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolo-nas-on-custom-dataset.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolo-nas-on-custom-dataset.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolo-nas-on-custom-dataset.ipynb&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/sage-maker.svg?sanitize=true&#34; alt=&#34;SageMaker&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://blog.roboflow.com/yolo-nas-how-to-train-on-custom-dataset&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&#34; alt=&#34;Roboflow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://youtu.be/V-H3eoPUnA8&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/youtube.svg?sanitize=true&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/YOLONAS.md&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/github.svg?sanitize=true&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-segment-anything-with-sam.ipynb&#34;&gt;Segment Anything Model (SAM)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-segment-anything-with-sam.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-segment-anything-with-sam.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-segment-anything-with-sam.ipynb&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/sage-maker.svg?sanitize=true&#34; alt=&#34;SageMaker&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://blog.roboflow.com/how-to-use-segment-anything-model-sam&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&#34; alt=&#34;Roboflow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://youtu.be/D-D6ZmadzPE&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/youtube.svg?sanitize=true&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/github.svg?sanitize=true&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2304.02643&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2304.02643-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/zero-shot-object-detection-with-grounding-dino.ipynb&#34;&gt;Zero-Shot Object Detection with Grounding DINO&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-grounding-dino.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/zero-shot-object-detection-with-grounding-dino.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-grounding-dino.ipynb&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/sage-maker.svg?sanitize=true&#34; alt=&#34;SageMaker&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://blog.roboflow.com/grounding-dino-zero-shot-object-detection&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&#34; alt=&#34;Roboflow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://youtu.be/cMa77r3YrDk&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/youtube.svg?sanitize=true&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/github.svg?sanitize=true&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2303.05499&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2303.05499-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-huggingface-detr-on-custom-dataset.ipynb&#34;&gt;DETR Transformer Object Detection&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-huggingface-detr-on-custom-dataset.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-huggingface-detr-on-custom-dataset.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/roboflow-ai/notebooks/blob/main/notebooks/train-huggingface-detr-on-custom-dataset.ipynb&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/sage-maker.svg?sanitize=true&#34; alt=&#34;SageMaker&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://youtu.be/AM8D4j9KoaU&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&#34; alt=&#34;Roboflow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://youtu.be/AM8D4j9KoaU&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/youtube.svg?sanitize=true&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/facebookresearch/detr&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/github.svg?sanitize=true&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2005.12872&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2005.12872-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/dinov2-classification.ipynb&#34;&gt;DINOv2 Image Classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/dinov2-classification.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/dinov2-classification.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://blog.roboflow.com/how-to-classify-images-with-dinov2/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&#34; alt=&#34;Roboflow&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/facebookresearch/dinov2/&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/github.svg?sanitize=true&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2304.07193&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2304.07193-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov8-object-detection-on-custom-dataset.ipynb&#34;&gt;YOLOv8 Object Detection&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov8-object-detection-on-custom-dataset.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov8-object-detection-on-custom-dataset.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov8-object-detection-on-custom-dataset.ipynb&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/sage-maker.svg?sanitize=true&#34; alt=&#34;SageMaker&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://blog.roboflow.com/how-to-train-yolov8-on-a-custom-dataset&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&#34; alt=&#34;Roboflow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://youtu.be/wuZtUMEiKWY&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/youtube.svg?sanitize=true&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ultralytics/ultralytics&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/github.svg?sanitize=true&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov8-instance-segmentation-on-custom-dataset.ipynb&#34;&gt;YOLOv8 Instance Segmentation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov8-instance-segmentation-on-custom-dataset.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov8-instance-segmentation-on-custom-dataset.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov8-instance-segmentation-on-custom-dataset.ipynb&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/sage-maker.svg?sanitize=true&#34; alt=&#34;SageMaker&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://blog.roboflow.com/how-to-train-yolov8-instance-segmentation/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&#34; alt=&#34;Roboflow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://youtu.be/pFiGSrRtaU4&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/youtube.svg?sanitize=true&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ultralytics/ultralytics&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/github.svg?sanitize=true&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov8-classification-on-custom-dataset.ipynb&#34;&gt;YOLOv8 Classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov8-classification-on-custom-dataset.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov8-classification-on-custom-dataset.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov8-classification-on-custom-dataset.ipynb&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/sage-maker.svg?sanitize=true&#34; alt=&#34;SageMaker&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://blog.roboflow.com/how-to-train-a-yolov8-classification-model/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&#34; alt=&#34;Roboflow&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ultralytics/ultralytics&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/github.svg?sanitize=true&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov7-object-detection-on-custom-data.ipynb&#34;&gt;YOLOv7 Object Detection&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov7-object-detection-on-custom-data.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov7-object-detection-on-custom-data.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://blog.roboflow.com/yolov7-custom-dataset-training-tutorial&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&#34; alt=&#34;Roboflow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=5nsmXLyDaU4&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/youtube.svg?sanitize=true&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/WongKinYiu/yolov7&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/github.svg?sanitize=true&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2207.02696&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2207.02696-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov7-instance-segmentation-on-custom-data.ipynb&#34;&gt;YOLOv7 Instance Segmentation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov7-instance-segmentation-on-custom-data.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov7-instance-segmentation-on-custom-data.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov7-instance-segmentation-on-custom-data.ipynb&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/sage-maker.svg?sanitize=true&#34; alt=&#34;SageMaker&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://blog.roboflow.com/train-yolov7-instance-segmentation-on-custom-data&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&#34; alt=&#34;Roboflow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=vFGxM2KLs10&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/youtube.svg?sanitize=true&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/WongKinYiu/yolov7&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/github.svg?sanitize=true&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2207.02696&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2207.02696-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov7-object-detection-on-custom-data-openvino-torch-ort.ipynb&#34;&gt;YOLOv7 Object Detection OpenVINO + TorchORT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov7-object-detection-on-custom-data-openvino-torch-ort.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov7-object-detection-on-custom-data-openvino-torch-ort.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://blog.roboflow.com/accelerate-pytorch-openvino-torch-ort&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&#34; alt=&#34;Roboflow&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow-ai/yolov7&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/github.svg?sanitize=true&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2207.02696&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2207.02696-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov6-object-detection-on-custom-data.ipynb&#34;&gt;MT-YOLOv6 Object Detection&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov6-object-detection-on-custom-data.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov6-object-detection-on-custom-data.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://blog.roboflow.com/how-to-train-yolov6-on-a-custom-dataset&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&#34; alt=&#34;Roboflow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=fFCWrMFH2UY&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/youtube.svg?sanitize=true&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/meituan/YOLOv6&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/github.svg?sanitize=true&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2209.02976&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2209.02976-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov5-object-detection-on-custom-data.ipynb&#34;&gt;YOLOv5 Object Detection&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov5-object-detection-on-custom-data.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov5-object-detection-on-custom-data.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://blog.roboflow.com/how-to-train-yolov5-on-a-custom-dataset&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&#34; alt=&#34;Roboflow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://youtu.be/watch?v=x0ThXHbtqCQ&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/youtube.svg?sanitize=true&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/github.svg?sanitize=true&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov5-classification-on-custom-data.ipynb&#34;&gt;YOLOv5 Classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov5-classification-on-custom-data.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov5-classification-on-custom-data.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://blog.roboflow.com/train-yolov5-classification-custom-data&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&#34; alt=&#34;Roboflow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=DPjp9Kq4qn8&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/youtube.svg?sanitize=true&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/github.svg?sanitize=true&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov5-instance-segmentation-on-custom-data.ipynb&#34;&gt;YOLOv5 Instance Segmentation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov5-instance-segmentation-on-custom-data.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov5-instance-segmentation-on-custom-data.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov5-instance-segmentation-on-custom-data.ipynb&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/sage-maker.svg?sanitize=true&#34; alt=&#34;SageMaker&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://blog.roboflow.com/train-yolov5-instance-segmentation-custom-dataset&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&#34; alt=&#34;Roboflow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=vKzfvtEtiYo&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/youtube.svg?sanitize=true&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/github.svg?sanitize=true&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-detectron2-segmentation-on-custom-data.ipynb&#34;&gt;Detection2 Instance Segmentation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-detectron2-segmentation-on-custom-data.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-detectron2-segmentation-on-custom-data.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://blog.roboflow.com/how-to-train-detectron2&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&#34; alt=&#34;Roboflow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://youtu.be/e8LPflX0nwQ&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/youtube.svg?sanitize=true&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/facebookresearch/detectron2&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/github.svg?sanitize=true&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/1703.06870v3&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-1703.06870v3-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-segformer-segmentation-on-custom-data.ipynb&#34;&gt;SegFormer Instance Segmentation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-segformer-segmentation-on-custom-data.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-segformer-segmentation-on-custom-data.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://blog.roboflow.com/how-to-train-segformer-on-a-custom-dataset-with-pytorch-lightning&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&#34; alt=&#34;Roboflow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=4HNkBMfw-2o&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/youtube.svg?sanitize=true&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/NVlabs/SegFormer&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/github.svg?sanitize=true&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2105.15203v3&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2105.15203v3-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-vision-transformer-classification-on-custom-data.ipynb&#34;&gt;Vision Transformer Classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-vision-transformer-classification-on-custom-data.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-vision-transformer-classification-on-custom-data.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://blog.roboflow.com/how-to-train-vision-transformer&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&#34; alt=&#34;Roboflow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=8yRE2Pa-8_I&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/youtube.svg?sanitize=true&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/lucidrains/vit-pytorch&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/github.svg?sanitize=true&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.11929&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2010.11929-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-scaled-yolov4-object-detection-on-custom-data.ipynb&#34;&gt;Scaled-YOLOv4 Object Detection&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-scaled-yolov4-object-detection-on-custom-data.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-scaled-yolov4-object-detection-on-custom-data.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://blog.roboflow.com/how-to-train-scaled-yolov4&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&#34; alt=&#34;Roboflow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=rEbpKxZbvIo&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/youtube.svg?sanitize=true&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/WongKinYiu/ScaledYOLOv4&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/github.svg?sanitize=true&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2004.10934&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2004.10934-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolos-huggingface-object-detection-on-custom-data.ipynb&#34;&gt;YOLOS Object Detection&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolos-huggingface-object-detection-on-custom-data.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolos-huggingface-object-detection-on-custom-data.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://blog.roboflow.com/train-yolos-transformer-custom-dataset&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&#34; alt=&#34;Roboflow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=N0V0xxSi6Xc&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/youtube.svg?sanitize=true&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/github.svg?sanitize=true&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2106.00666&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2106.00666-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolor-object-detection-on-custom-data.ipynb&#34;&gt;YOLOR Object Detection&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolor-object-detection-on-custom-data.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolor-object-detection-on-custom-data.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://blog.roboflow.com/train-yolor-on-a-custom-dataset&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&#34; alt=&#34;Roboflow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=sZ5DiXDOHEM&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/youtube.svg?sanitize=true&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow-ai/yolor&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/github.svg?sanitize=true&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/1506.02640&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-1506.02640-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolox-object-detection-on-custom-data.ipynb&#34;&gt;YOLOX Object Detection&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolox-object-detection-on-custom-data.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolox-object-detection-on-custom-data.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://blog.roboflow.com/how-to-train-yolox-on-a-custom-dataset&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&#34; alt=&#34;Roboflow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=q3RbFbaQQGw&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/youtube.svg?sanitize=true&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Megvii-BaseDetection/YOLOX&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/github.svg?sanitize=true&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2107.08430&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2107.08430-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-resnet34-classification.ipynb&#34;&gt;Resnet34 fast.ai Classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-resnet34-classification.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-resnet34-classification.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://blog.roboflow.com/how-to-train-a-custom-resnet34-model&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&#34; alt=&#34;Roboflow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=93kXzUOiYY4&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/youtube.svg?sanitize=true&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-use-openai-clip-classification.ipynb&#34;&gt;OpenAI Clip Classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-use-openai-clip-classification.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-use-openai-clip-classification.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://blog.roboflow.com/how-to-use-openai-clip&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&#34; alt=&#34;Roboflow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=8o701AEoZ8I&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/youtube.svg?sanitize=true&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/openai/CLIP&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/github.svg?sanitize=true&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2103.00020&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2103.00020-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov4-tiny-object-detection-on-custom-data.ipynb&#34;&gt;YOLOv4-tiny Darknet Object Detection&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov4-tiny-object-detection-on-custom-data.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov4-tiny-object-detection-on-custom-data.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://blog.roboflow.ai/train-yolov4-tiny-on-custom-data-lighting-fast-detection&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&#34; alt=&#34;Roboflow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=NTnZgLsk_DA&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/youtube.svg?sanitize=true&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow-ai/darknet&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/github.svg?sanitize=true&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2011.04244&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2011.04244-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-train-yolov8-classification-no-labeling.ipynb&#34;&gt;Train a YOLOv8 Classification Model with No Labeling&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-train-yolov8-classification-no-labeling.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-train-yolov8-classification-no-labeling.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-train-yolov8-classification-no-labeling.ipynb&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/sage-maker.svg?sanitize=true&#34; alt=&#34;SageMaker&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://blog.roboflow.com/train-classification-model-no-labeling/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&#34; alt=&#34;Roboflow&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ultralytics/ultralytics&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/github.svg?sanitize=true&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;📸 computer vision skills (14 notebooks)&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;notebook&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;open in colab / kaggle / sagemaker studio lab&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;complementary materials&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;repository / paper&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-auto-train-yolov8-model-with-autodistill.ipynb&#34;&gt;Auto Train YOLOv8 Model with Autodistill&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-auto-train-yolov8-model-with-autodistill.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-auto-train-yolov8-model-with-autodistill.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-auto-train-yolov8-model-with-autodistill.ipynb&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/sage-maker.svg?sanitize=true&#34; alt=&#34;SageMaker&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://blog.roboflow.com/autodistill&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&#34; alt=&#34;Roboflow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://youtu.be/gKTYMfwPo4M&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/youtube.svg?sanitize=true&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/autodistill/autodistill&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/github.svg?sanitize=true&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-segmentation-model-with-no-labeling.ipynb&#34;&gt;Train a Segmentation Model with No Labeling&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-segmentation-model-with-no-labeling.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-segmentation-model-with-no-labeling.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/roboflow-ai/notebooks/blob/main/notebooks/train-segmentation-model-with-no-labeling.ipynb&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/sage-maker.svg?sanitize=true&#34; alt=&#34;SageMaker&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://blog.roboflow.com/train-a-segmentation-model-no-labeling/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&#34; alt=&#34;Roboflow&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/autodistill/autodistill&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/github.svg?sanitize=true&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/dinov2-image-retrieval.ipynb&#34;&gt;DINOv2 Image Retrieval&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/dinov2-image-retrieval.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/dinov2-image-retrieval.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/facebookresearch/dinov2/&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/github.svg?sanitize=true&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2304.07193&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2304.07193-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/image_embeddings_analysis_part_1.ipynb&#34;&gt;Image Embeddings Analysis - Part 1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/image_embeddings_analysis_part_1.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/image_embeddings_analysis_part_1.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/roboflow-ai/notebooks/blob/main/notebooks/image_embeddings_analysis_part_1.ipynb&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/sage-maker.svg?sanitize=true&#34; alt=&#34;SageMaker&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://youtu.be/YxJkE6FvGF4&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/youtube.svg?sanitize=true&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/openai/CLIP&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/github.svg?sanitize=true&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2103.00020&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2103.00020-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/automated-dataset-annotation-and-evaluation-with-grounding-dino-and-sam.ipynb&#34;&gt;Automated Dataset Annotation and Evaluation with Grounding DINO and SAM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/automated-dataset-annotation-and-evaluation-with-grounding-dino-and-sam.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/automated-dataset-annotation-and-evaluation-with-grounding-dino-and-sam.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/roboflow-ai/notebooks/blob/main/notebooks/automated-dataset-annotation-and-evaluation-with-grounding-dino-and-sam.ipynb&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/sage-maker.svg?sanitize=true&#34; alt=&#34;SageMaker&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://blog.roboflow.com/enhance-image-annotation-with-grounding-dino-and-sam/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&#34; alt=&#34;Roboflow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://youtu.be/oEQYStnF2l8&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/youtube.svg?sanitize=true&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/github.svg?sanitize=true&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2303.05499&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2303.05499-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/automated-dataset-annotation-and-evaluation-with-grounding-dino.ipynb&#34;&gt;Automated Dataset Annotation and Evaluation with Grounding DINO&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/automated-dataset-annotation-and-evaluation-with-grounding-dino.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/automated-dataset-annotation-and-evaluation-with-grounding-dino.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/roboflow-ai/notebooks/blob/main/notebooks/automated-dataset-annotation-and-evaluation-with-grounding-dino.ipynb&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/sage-maker.svg?sanitize=true&#34; alt=&#34;SageMaker&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://youtu.be/C4NqaRBz_Kw&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/youtube.svg?sanitize=true&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/github.svg?sanitize=true&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2303.05499&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2303.05499-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/vector-analysis-with-sklearn-and-bokeh.ipynb&#34;&gt;Vector Analysis with Scikit-learn and Bokeh&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/vector-analysis-with-sklearn-and-bokeh.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/vector-analysis-with-sklearn-and-bokeh.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://blog.roboflow.com/vector-analysis&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&#34; alt=&#34;Roboflow&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-use-rf100.ipynb&#34;&gt;RF100 Object Detection Model Benchmarking&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-use-rf100.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-use-rf100.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://blog.roboflow.com/roboflow-100&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&#34; alt=&#34;Roboflow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://youtu.be/jIgZMr-PBMo&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/youtube.svg?sanitize=true&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow-ai/roboflow-100-benchmark&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/github.svg?sanitize=true&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2211.13523&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2211.13523-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-detect-and-count-objects-in-polygon-zone.ipynb&#34;&gt;Detect and Count Objects in Polygon Zone with YOLOv5 / YOLOv8 / Detectron2 + Supervision&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-detect-and-count-objects-in-polygon-zone.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-detect-and-count-objects-in-polygon-zone.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-detect-and-count-objects-in-polygon-zone.ipynb&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/sage-maker.svg?sanitize=true&#34; alt=&#34;SageMaker&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://youtu.be/l_kf9CfZ_8M&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/youtube.svg?sanitize=true&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow/supervision&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/github.svg?sanitize=true&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-track-and-count-vehicles-with-yolov8.ipynb&#34;&gt;Track and Count Vehicles with YOLOv8 + ByteTRACK + Supervision&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-track-and-count-vehicles-with-yolov8.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-track-and-count-vehicles-with-yolov8.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-track-and-count-vehicles-with-yolov8.ipynb&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/sage-maker.svg?sanitize=true&#34; alt=&#34;SageMaker&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://blog.roboflow.com/yolov8-tracking-and-counting/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&#34; alt=&#34;Roboflow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://youtu.be/OS5qI9YBkfk&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/youtube.svg?sanitize=true&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow/supervision&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/github.svg?sanitize=true&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2110.06864&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2110.06864-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-track-football-players.ipynb&#34;&gt;Football Players Tracking with YOLOv5 + ByteTRACK&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-track-football-players.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-track-football-players.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-track-football-players.ipynb&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/sage-maker.svg?sanitize=true&#34; alt=&#34;SageMaker&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://blog.roboflow.com/track-football-players&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&#34; alt=&#34;Roboflow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://youtu.be/QCG8QMhga9k&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/youtube.svg?sanitize=true&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ifzhang/ByteTrack&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/github.svg?sanitize=true&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2110.06864&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2110.06864-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-generate-segmentation-mask-with-roboflow.ipynb&#34;&gt;Create Segmentation Masks with Roboflow&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-generate-segmentation-mask-with-roboflow.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-generate-segmentation-mask-with-roboflow.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://blog.roboflow.com/how-to-create-segmentation-masks-with-roboflow&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&#34; alt=&#34;Roboflow&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-use-polygonzone-annotate-and-supervision.ipynb&#34;&gt;How to Use PolygonZone and Roboflow Supervision&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-use-polygonzone-annotate-and-supervision.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-use-polygonzone-annotate-and-supervision.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://blog.roboflow.com/polygonzone/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&#34; alt=&#34;Roboflow&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/image-to-image-search-clip-faiss.ipynb&#34;&gt;Image-to-Image Search with CLIP and faiss&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/image-to-image-search-clip-faiss.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/image-to-image-search-clip-faiss.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://blog.roboflow.com/clip-image-search-faiss/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&#34; alt=&#34;Roboflow&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;!-- AUTOGENERATED-NOTEBOOKS-TABLE --&gt; &#xA;&lt;h2&gt;🎬 videos&lt;/h2&gt; &#xA;&lt;p&gt;Almost every week we create tutorials showing you the hottest models in Computer Vision. 🔥 &lt;a href=&#34;https://www.youtube.com/@Roboflow&#34;&gt;Subscribe&lt;/a&gt;, and stay up to date with our latest YouTube videos!&lt;/p&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;a href=&#34;https://youtu.be/CilXrt3S-ws&#34; title=&#34;How to Choose the Best Computer Vision Model for Your Project&#34;&gt;&lt;img src=&#34;https://github.com/roboflow/notebooks/assets/26109316/73a01d3b-cf70-40c3-a5e4-e4bc5be38d42&#34; alt=&#34;How to Choose the Best Computer Vision Model for Your Project&#34; width=&#34;300px&#34; align=&#34;left&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://youtu.be/CilXrt3S-ws&#34; title=&#34;How to Choose the Best Computer Vision Model for Your Project&#34;&gt;&lt;strong&gt;How to Choose the Best Computer Vision Model for Your Project&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt;&#xA;&lt;div&gt;&#xA; &lt;strong&gt;Created: 26 May 2023&lt;/strong&gt; | &#xA; &lt;strong&gt;Updated: 26 May 2023&lt;/strong&gt;&#xA;&lt;/div&gt; &#xA;&lt;br&gt; In this video, we will dive into the complexity of choosing the right computer vision model for your unique project. From the importance of high-quality datasets to hardware considerations, interoperability, benchmarking, and licensing issues, this video covers it all... &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;a href=&#34;https://youtu.be/oEQYStnF2l8&#34; title=&#34;Accelerate Image Annotation with SAM and Grounding DINO&#34;&gt;&lt;img src=&#34;https://github.com/SkalskiP/SkalskiP/assets/26109316/ae1ca38e-40b7-4b35-8582-e8ea5de3806e&#34; alt=&#34;Accelerate Image Annotation with SAM and Grounding DINO&#34; width=&#34;300px&#34; align=&#34;left&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://youtu.be/oEQYStnF2l8&#34; title=&#34;Accelerate Image Annotation with SAM and Grounding DINO&#34;&gt;&lt;strong&gt;Accelerate Image Annotation with SAM and Grounding DINO&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt;&#xA;&lt;div&gt;&#xA; &lt;strong&gt;Created: 20 Apr 2023&lt;/strong&gt; | &#xA; &lt;strong&gt;Updated: 20 Apr 2023&lt;/strong&gt;&#xA;&lt;/div&gt; &#xA;&lt;br&gt; Discover how to speed up your image annotation process using Grounding DINO and Segment Anything Model (SAM). Learn how to convert object detection datasets into instance segmentation datasets, and see the potential of using these models to automatically annotate your datasets for real-time detectors like YOLOv8... &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;a href=&#34;https://youtu.be/D-D6ZmadzPE&#34; title=&#34;SAM - Segment Anything Model by Meta AI: Complete Guide&#34;&gt;&lt;img src=&#34;https://github.com/SkalskiP/SkalskiP/assets/26109316/6913ff11-53c6-4341-8d90-eaff3023c3fd&#34; alt=&#34;SAM - Segment Anything Model by Meta AI: Complete Guide&#34; width=&#34;300px&#34; align=&#34;left&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://youtu.be/D-D6ZmadzPE&#34; title=&#34;SAM - Segment Anything Model by Meta AI: Complete Guide&#34;&gt;&lt;strong&gt;SAM - Segment Anything Model by Meta AI: Complete Guide&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt;&#xA;&lt;div&gt;&#xA; &lt;strong&gt;Created: 11 Apr 2023&lt;/strong&gt; | &#xA; &lt;strong&gt;Updated: 11 Apr 2023&lt;/strong&gt;&#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;br&gt; Discover the incredible potential of Meta AI&#39;s Segment Anything Model (SAM)! We dive into SAM, an efficient and promptable model for image segmentation, which has revolutionized computer vision tasks. With over 1 billion masks on 11M licensed and privacy-respecting images, SAM&#39;s zero-shot performance is often superior to prior fully supervised results... &lt;/p&gt;&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h2&gt;💻 run locally&lt;/h2&gt; &#xA;&lt;p&gt;We try to make it as easy as possible to run Roboflow Notebooks in Colab and Kaggle, but if you still want to run them locally, below you will find instructions on how to do it. Remember don&#39;t install your dependencies globally, use &lt;a href=&#34;https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/&#34;&gt;venv&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;# clone repository and navigate to root directory&#xA;git clone git@github.com:roboflow-ai/notebooks.git&#xA;cd notebooks&#xA;&#xA;# setup python environment and activate it&#xA;python3 -m venv venv&#xA;source venv/bin/activate&#xA;&#xA;# install and run jupyter notebook&#xA;pip install notebook&#xA;jupyter notebook&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;☁️ run in sagemaker studio lab&lt;/h2&gt; &#xA;&lt;p&gt;You can now open our tutorial notebooks in &lt;a href=&#34;https://aws.amazon.com/sagemaker/studio-lab/&#34;&gt;Amazon SageMaker Studio Lab&lt;/a&gt; - a free machine learning development environment that provides the compute, storage, and security—all at no cost—for anyone to learn and experiment with ML.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Stable Diffusion Image Generation&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;YOLOv5 Custom Dataset Training&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;YOLOv7 Custom Dataset Training&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/roboflow-ai/notebooks/blob/main/notebooks/sagemaker-studiolab/stable-diffusion-image-generation.ipynb&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/sage-maker.svg?sanitize=true&#34; alt=&#34;SageMaker&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/roboflow-ai/notebooks/blob/main/notebooks/sagemaker-studiolab/yolov5-custom-training.ipynb&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/sage-maker.svg?sanitize=true&#34; alt=&#34;SageMaker&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/roboflow-ai/notebooks/blob/main/notebooks/sagemaker-studiolab/yolov7-custom-training.ipynb&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/sage-maker.svg?sanitize=true&#34; alt=&#34;SageMaker&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;🐞 bugs &amp;amp; 🦸 contribution&lt;/h2&gt; &#xA;&lt;p&gt;Computer Vision moves fast! Sometimes our notebooks lag a tad behind the ever-pushing forward libraries. If you notice that any of the notebooks is not working properly, create a &lt;a href=&#34;https://github.com/roboflow-ai/notebooks/issues/new?assignees=&amp;amp;labels=bug%2Ctriage&amp;amp;template=bug-report.yml&#34;&gt;bug report&lt;/a&gt; and let us know.&lt;/p&gt; &#xA;&lt;p&gt;If you have an idea for a new tutorial we should do, create a &lt;a href=&#34;https://github.com/roboflow-ai/notebooks/issues/new?assignees=&amp;amp;labels=enhancement&amp;amp;template=feature-request.yml&#34;&gt;feature request&lt;/a&gt;. We are constantly looking for new ideas. If you feel up to the task and want to create a tutorial yourself, please take a peek at our &lt;a href=&#34;https://github.com/roboflow-ai/notebooks/raw/main/CONTRIBUTING.md&#34;&gt;contribution guide&lt;/a&gt;. There you can find all the information you need.&lt;/p&gt; &#xA;&lt;p&gt;We are here for you, so don&#39;t hesitate to &lt;a href=&#34;https://github.com/roboflow-ai/notebooks/discussions&#34;&gt;reach out&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>shap/shap</title>
    <updated>2023-10-22T01:54:48Z</updated>
    <id>tag:github.com,2023-10-22:/shap/shap</id>
    <link href="https://github.com/shap/shap" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A game theoretic approach to explain the output of any machine learning model.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/shap/shap/master/docs/artwork/shap_header.svg?sanitize=true&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/shap/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/shap&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://anaconda.org/conda-forge/shap&#34;&gt;&lt;img src=&#34;https://img.shields.io/conda/vn/conda-forge/shap&#34; alt=&#34;Conda&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/license/shap/shap&#34; alt=&#34;License&#34;&gt; &lt;img src=&#34;https://github.com/shap/shap/actions/workflows/run_tests.yml/badge.svg?sanitize=true&#34; alt=&#34;Tests&#34;&gt; &lt;a href=&#34;https://mybinder.org/v2/gh/shap/shap/master&#34;&gt;&lt;img src=&#34;https://mybinder.org/badge_logo.svg?sanitize=true&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://shap.readthedocs.io/en/latest/?badge=latest&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/shap/badge/?version=latest&#34; alt=&#34;Documentation Status&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/pypi/dm/shap&#34; alt=&#34;Downloads&#34;&gt; &lt;a href=&#34;https://pypi.org/pypi/shap/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/shap&#34; alt=&#34;PyPI pyversions&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SHAP (SHapley Additive exPlanations)&lt;/strong&gt; is a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions (see &lt;a href=&#34;https://raw.githubusercontent.com/shap/shap/master/#citations&#34;&gt;papers&lt;/a&gt; for details and citations).&lt;/p&gt; &#xA;&lt;!--**SHAP (SHapley Additive exPlanations)** is a unified approach to explain the output of any machine learning model. SHAP connects game theory with local explanations, uniting several previous methods [1-7] and representing the only possible consistent and locally accurate additive feature attribution method based on expectations (see our [papers](#citations) for details and citations).--&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;SHAP can be installed from either &lt;a href=&#34;https://pypi.org/project/shap&#34;&gt;PyPI&lt;/a&gt; or &lt;a href=&#34;https://anaconda.org/conda-forge/shap&#34;&gt;conda-forge&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&#xA;pip install shap&#xA;&lt;i&gt;or&lt;/i&gt;&#xA;conda install -c conda-forge shap&#xA;&lt;/pre&gt; &#xA;&lt;h2&gt;Tree ensemble example (XGBoost/LightGBM/CatBoost/scikit-learn/pyspark models)&lt;/h2&gt; &#xA;&lt;p&gt;While SHAP can explain the output of any machine learning model, we have developed a high-speed exact algorithm for tree ensemble methods (see our &lt;a href=&#34;https://rdcu.be/b0z70&#34;&gt;Nature MI paper&lt;/a&gt;). Fast C++ implementations are supported for &lt;em&gt;XGBoost&lt;/em&gt;, &lt;em&gt;LightGBM&lt;/em&gt;, &lt;em&gt;CatBoost&lt;/em&gt;, &lt;em&gt;scikit-learn&lt;/em&gt; and &lt;em&gt;pyspark&lt;/em&gt; tree models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import xgboost&#xA;import shap&#xA;&#xA;# train an XGBoost model&#xA;X, y = shap.datasets.california()&#xA;model = xgboost.XGBRegressor().fit(X, y)&#xA;&#xA;# explain the model&#39;s predictions using SHAP&#xA;# (same syntax works for LightGBM, CatBoost, scikit-learn, transformers, Spark, etc.)&#xA;explainer = shap.Explainer(model)&#xA;shap_values = explainer(X)&#xA;&#xA;# visualize the first prediction&#39;s explanation&#xA;shap.plots.waterfall(shap_values[0])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;616&#34; src=&#34;https://raw.githubusercontent.com/shap/shap/master/docs/artwork/california_waterfall.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;The above explanation shows features each contributing to push the model output from the base value (the average model output over the training dataset we passed) to the model output. Features pushing the prediction higher are shown in red, those pushing the prediction lower are in blue. Another way to visualize the same explanation is to use a force plot (these are introduced in our &lt;a href=&#34;https://rdcu.be/baVbR&#34;&gt;Nature BME paper&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# visualize the first prediction&#39;s explanation with a force plot&#xA;shap.plots.force(shap_values[0])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;811&#34; src=&#34;https://raw.githubusercontent.com/shap/shap/master/docs/artwork/california_instance.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;If we take many force plot explanations such as the one shown above, rotate them 90 degrees, and then stack them horizontally, we can see explanations for an entire dataset (in the notebook this plot is interactive):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# visualize all the training set predictions&#xA;shap.plots.force(shap_values[:500])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;811&#34; src=&#34;https://raw.githubusercontent.com/shap/shap/master/docs/artwork/california_dataset.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;To understand how a single feature effects the output of the model we can plot the SHAP value of that feature vs. the value of the feature for all the examples in a dataset. Since SHAP values represent a feature&#39;s responsibility for a change in the model output, the plot below represents the change in predicted house price as the latitude changes. Vertical dispersion at a single value of latitude represents interaction effects with other features. To help reveal these interactions we can color by another feature. If we pass the whole explanation tensor to the &lt;code&gt;color&lt;/code&gt; argument the scatter plot will pick the best feature to color by. In this case it picks longitude.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# create a dependence scatter plot to show the effect of a single feature across the whole dataset&#xA;shap.plots.scatter(shap_values[:, &#34;Latitude&#34;], color=shap_values)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;544&#34; src=&#34;https://raw.githubusercontent.com/shap/shap/master/docs/artwork/california_scatter.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;To get an overview of which features are most important for a model we can plot the SHAP values of every feature for every sample. The plot below sorts features by the sum of SHAP value magnitudes over all samples, and uses SHAP values to show the distribution of the impacts each feature has on the model output. The color represents the feature value (red high, blue low). This reveals for example that higher median incomes improves the predicted home price.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# summarize the effects of all the features&#xA;shap.plots.beeswarm(shap_values)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;583&#34; src=&#34;https://raw.githubusercontent.com/shap/shap/master/docs/artwork/california_beeswarm.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;We can also just take the mean absolute value of the SHAP values for each feature to get a standard bar plot (produces stacked bars for multi-class outputs):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;shap.plots.bar(shap_values)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;570&#34; src=&#34;https://raw.githubusercontent.com/shap/shap/master/docs/artwork/california_global_bar.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Natural language example (transformers)&lt;/h2&gt; &#xA;&lt;p&gt;SHAP has specific support for natural language models like those in the Hugging Face transformers library. By adding coalitional rules to traditional Shapley values we can form games that explain large modern NLP model using very few function evaluations. Using this functionality is as simple as passing a supported transformers pipeline to SHAP:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import transformers&#xA;import shap&#xA;&#xA;# load a transformers pipeline model&#xA;model = transformers.pipeline(&#39;sentiment-analysis&#39;, return_all_scores=True)&#xA;&#xA;# explain the model on two sample inputs&#xA;explainer = shap.Explainer(model)&#xA;shap_values = explainer([&#34;What a great movie! ...if you have no taste.&#34;])&#xA;&#xA;# visualize the first prediction&#39;s explanation for the POSITIVE output class&#xA;shap.plots.text(shap_values[0, :, &#34;POSITIVE&#34;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;811&#34; src=&#34;https://raw.githubusercontent.com/shap/shap/master/docs/artwork/sentiment_analysis_plot.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Deep learning example with DeepExplainer (TensorFlow/Keras models)&lt;/h2&gt; &#xA;&lt;p&gt;Deep SHAP is a high-speed approximation algorithm for SHAP values in deep learning models that builds on a connection with &lt;a href=&#34;https://arxiv.org/abs/1704.02685&#34;&gt;DeepLIFT&lt;/a&gt; described in the SHAP NIPS paper. The implementation here differs from the original DeepLIFT by using a distribution of background samples instead of a single reference value, and using Shapley equations to linearize components such as max, softmax, products, divisions, etc. Note that some of these enhancements have also been since integrated into DeepLIFT. TensorFlow models and Keras models using the TensorFlow backend are supported (there is also preliminary support for PyTorch):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# ...include code from https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py&#xA;&#xA;import shap&#xA;import numpy as np&#xA;&#xA;# select a set of background examples to take an expectation over&#xA;background = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]&#xA;&#xA;# explain predictions of the model on four images&#xA;e = shap.DeepExplainer(model, background)&#xA;# ...or pass tensors directly&#xA;# e = shap.DeepExplainer((model.layers[0].input, model.layers[-1].output), background)&#xA;shap_values = e.shap_values(x_test[1:5])&#xA;&#xA;# plot the feature attributions&#xA;shap.image_plot(shap_values, -x_test[1:5])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;820&#34; src=&#34;https://raw.githubusercontent.com/shap/shap/master/docs/artwork/mnist_image_plot.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;The plot above explains ten outputs (digits 0-9) for four different images. Red pixels increase the model&#39;s output while blue pixels decrease the output. The input images are shown on the left, and as nearly transparent grayscale backings behind each of the explanations. The sum of the SHAP values equals the difference between the expected model output (averaged over the background dataset) and the current model output. Note that for the &#39;zero&#39; image the blank middle is important, while for the &#39;four&#39; image the lack of a connection on top makes it a four instead of a nine.&lt;/p&gt; &#xA;&lt;h2&gt;Deep learning example with GradientExplainer (TensorFlow/Keras/PyTorch models)&lt;/h2&gt; &#xA;&lt;p&gt;Expected gradients combines ideas from &lt;a href=&#34;https://arxiv.org/abs/1703.01365&#34;&gt;Integrated Gradients&lt;/a&gt;, SHAP, and &lt;a href=&#34;https://arxiv.org/abs/1706.03825&#34;&gt;SmoothGrad&lt;/a&gt; into a single expected value equation. This allows an entire dataset to be used as the background distribution (as opposed to a single reference value) and allows local smoothing. If we approximate the model with a linear function between each background data sample and the current input to be explained, and we assume the input features are independent then expected gradients will compute approximate SHAP values. In the example below we have explained how the 7th intermediate layer of the VGG16 ImageNet model impacts the output probabilities.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from keras.applications.vgg16 import VGG16&#xA;from keras.applications.vgg16 import preprocess_input&#xA;import keras.backend as K&#xA;import numpy as np&#xA;import json&#xA;import shap&#xA;&#xA;# load pre-trained model and choose two images to explain&#xA;model = VGG16(weights=&#39;imagenet&#39;, include_top=True)&#xA;X,y = shap.datasets.imagenet50()&#xA;to_explain = X[[39,41]]&#xA;&#xA;# load the ImageNet class names&#xA;url = &#34;https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json&#34;&#xA;fname = shap.datasets.cache(url)&#xA;with open(fname) as f:&#xA;    class_names = json.load(f)&#xA;&#xA;# explain how the input to the 7th layer of the model explains the top two classes&#xA;def map2layer(x, layer):&#xA;    feed_dict = dict(zip([model.layers[0].input], [preprocess_input(x.copy())]))&#xA;    return K.get_session().run(model.layers[layer].input, feed_dict)&#xA;e = shap.GradientExplainer(&#xA;    (model.layers[7].input, model.layers[-1].output),&#xA;    map2layer(X, 7),&#xA;    local_smoothing=0 # std dev of smoothing noise&#xA;)&#xA;shap_values,indexes = e.shap_values(map2layer(to_explain, 7), ranked_outputs=2)&#xA;&#xA;# get the names for the classes&#xA;index_names = np.vectorize(lambda x: class_names[str(x)][1])(indexes)&#xA;&#xA;# plot the explanations&#xA;shap.image_plot(shap_values, to_explain, index_names)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;500&#34; src=&#34;https://raw.githubusercontent.com/shap/shap/master/docs/artwork/gradient_imagenet_plot.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Predictions for two input images are explained in the plot above. Red pixels represent positive SHAP values that increase the probability of the class, while blue pixels represent negative SHAP values the reduce the probability of the class. By using &lt;code&gt;ranked_outputs=2&lt;/code&gt; we explain only the two most likely classes for each input (this spares us from explaining all 1,000 classes).&lt;/p&gt; &#xA;&lt;h2&gt;Model agnostic example with KernelExplainer (explains any function)&lt;/h2&gt; &#xA;&lt;p&gt;Kernel SHAP uses a specially-weighted local linear regression to estimate SHAP values for any model. Below is a simple example for explaining a multi-class SVM on the classic iris dataset.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import sklearn&#xA;import shap&#xA;from sklearn.model_selection import train_test_split&#xA;&#xA;# print the JS visualization code to the notebook&#xA;shap.initjs()&#xA;&#xA;# train a SVM classifier&#xA;X_train,X_test,Y_train,Y_test = train_test_split(*shap.datasets.iris(), test_size=0.2, random_state=0)&#xA;svm = sklearn.svm.SVC(kernel=&#39;rbf&#39;, probability=True)&#xA;svm.fit(X_train, Y_train)&#xA;&#xA;# use Kernel SHAP to explain test set predictions&#xA;explainer = shap.KernelExplainer(svm.predict_proba, X_train, link=&#34;logit&#34;)&#xA;shap_values = explainer.shap_values(X_test, nsamples=100)&#xA;&#xA;# plot the SHAP values for the Setosa output of the first instance&#xA;shap.force_plot(explainer.expected_value[0], shap_values[0][0,:], X_test.iloc[0,:], link=&#34;logit&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;810&#34; src=&#34;https://raw.githubusercontent.com/shap/shap/master/docs/artwork/iris_instance.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;The above explanation shows four features each contributing to push the model output from the base value (the average model output over the training dataset we passed) towards zero. If there were any features pushing the class label higher they would be shown in red.&lt;/p&gt; &#xA;&lt;p&gt;If we take many explanations such as the one shown above, rotate them 90 degrees, and then stack them horizontally, we can see explanations for an entire dataset. This is exactly what we do below for all the examples in the iris test set:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# plot the SHAP values for the Setosa output of all instances&#xA;shap.force_plot(explainer.expected_value[0], shap_values[0], X_test, link=&#34;logit&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;813&#34; src=&#34;https://raw.githubusercontent.com/shap/shap/master/docs/artwork/iris_dataset.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;SHAP Interaction Values&lt;/h2&gt; &#xA;&lt;p&gt;SHAP interaction values are a generalization of SHAP values to higher order interactions. Fast exact computation of pairwise interactions are implemented for tree models with &lt;code&gt;shap.TreeExplainer(model).shap_interaction_values(X)&lt;/code&gt;. This returns a matrix for every prediction, where the main effects are on the diagonal and the interaction effects are off-diagonal. These values often reveal interesting hidden relationships, such as how the increased risk of death peaks for men at age 60 (see the NHANES notebook for details):&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;483&#34; src=&#34;https://raw.githubusercontent.com/shap/shap/master/docs/artwork/nhanes_age_sex_interaction.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Sample notebooks&lt;/h2&gt; &#xA;&lt;p&gt;The notebooks below demonstrate different use cases for SHAP. Look inside the notebooks directory of the repository if you want to try playing with the original notebooks yourself.&lt;/p&gt; &#xA;&lt;h3&gt;TreeExplainer&lt;/h3&gt; &#xA;&lt;p&gt;An implementation of Tree SHAP, a fast and exact algorithm to compute SHAP values for trees and ensembles of trees.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://shap.github.io/shap/notebooks/NHANES%20I%20Survival%20Model.html&#34;&gt;&lt;strong&gt;NHANES survival model with XGBoost and SHAP interaction values&lt;/strong&gt;&lt;/a&gt; - Using mortality data from 20 years of followup this notebook demonstrates how to use XGBoost and &lt;code&gt;shap&lt;/code&gt; to uncover complex risk factor relationships.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://shap.github.io/shap/notebooks/tree_explainer/Census%20income%20classification%20with%20LightGBM.html&#34;&gt;&lt;strong&gt;Census income classification with LightGBM&lt;/strong&gt;&lt;/a&gt; - Using the standard adult census income dataset, this notebook trains a gradient boosting tree model with LightGBM and then explains predictions using &lt;code&gt;shap&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://shap.github.io/shap/notebooks/League%20of%20Legends%20Win%20Prediction%20with%20XGBoost.html&#34;&gt;&lt;strong&gt;League of Legends Win Prediction with XGBoost&lt;/strong&gt;&lt;/a&gt; - Using a Kaggle dataset of 180,000 ranked matches from League of Legends we train and explain a gradient boosting tree model with XGBoost to predict if a player will win their match.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;DeepExplainer&lt;/h3&gt; &#xA;&lt;p&gt;An implementation of Deep SHAP, a faster (but only approximate) algorithm to compute SHAP values for deep learning models that is based on connections between SHAP and the DeepLIFT algorithm.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://shap.github.io/shap/notebooks/deep_explainer/Front%20Page%20DeepExplainer%20MNIST%20Example.html&#34;&gt;&lt;strong&gt;MNIST Digit classification with Keras&lt;/strong&gt;&lt;/a&gt; - Using the MNIST handwriting recognition dataset, this notebook trains a neural network with Keras and then explains predictions using &lt;code&gt;shap&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://shap.github.io/shap/notebooks/deep_explainer/Keras%20LSTM%20for%20IMDB%20Sentiment%20Classification.html&#34;&gt;&lt;strong&gt;Keras LSTM for IMDB Sentiment Classification&lt;/strong&gt;&lt;/a&gt; - This notebook trains an LSTM with Keras on the IMDB text sentiment analysis dataset and then explains predictions using &lt;code&gt;shap&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;GradientExplainer&lt;/h3&gt; &#xA;&lt;p&gt;An implementation of expected gradients to approximate SHAP values for deep learning models. It is based on connections between SHAP and the Integrated Gradients algorithm. GradientExplainer is slower than DeepExplainer and makes different approximation assumptions.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://shap.github.io/shap/notebooks/gradient_explainer/Explain%20an%20Intermediate%20Layer%20of%20VGG16%20on%20ImageNet.html&#34;&gt;&lt;strong&gt;Explain an Intermediate Layer of VGG16 on ImageNet&lt;/strong&gt;&lt;/a&gt; - This notebook demonstrates how to explain the output of a pre-trained VGG16 ImageNet model using an internal convolutional layer.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;LinearExplainer&lt;/h3&gt; &#xA;&lt;p&gt;For a linear model with independent features we can analytically compute the exact SHAP values. We can also account for feature correlation if we are willing to estimate the feature covariance matrix. LinearExplainer supports both of these options.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://shap.github.io/shap/notebooks/linear_explainer/Sentiment%20Analysis%20with%20Logistic%20Regression.html&#34;&gt;&lt;strong&gt;Sentiment Analysis with Logistic Regression&lt;/strong&gt;&lt;/a&gt; - This notebook demonstrates how to explain a linear logistic regression sentiment analysis model.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;KernelExplainer&lt;/h3&gt; &#xA;&lt;p&gt;An implementation of Kernel SHAP, a model agnostic method to estimate SHAP values for any model. Because it makes no assumptions about the model type, KernelExplainer is slower than the other model type specific algorithms.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://shap.github.io/shap/notebooks/Census%20income%20classification%20with%20scikit-learn.html&#34;&gt;&lt;strong&gt;Census income classification with scikit-learn&lt;/strong&gt;&lt;/a&gt; - Using the standard adult census income dataset, this notebook trains a k-nearest neighbors classifier using scikit-learn and then explains predictions using &lt;code&gt;shap&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://shap.github.io/shap/notebooks/ImageNet%20VGG16%20Model%20with%20Keras.html&#34;&gt;&lt;strong&gt;ImageNet VGG16 Model with Keras&lt;/strong&gt;&lt;/a&gt; - Explain the classic VGG16 convolutional neural network&#39;s predictions for an image. This works by applying the model agnostic Kernel SHAP method to a super-pixel segmented image.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://shap.github.io/shap/notebooks/Iris%20classification%20with%20scikit-learn.html&#34;&gt;&lt;strong&gt;Iris classification&lt;/strong&gt;&lt;/a&gt; - A basic demonstration using the popular iris species dataset. It explains predictions from six different models in scikit-learn using &lt;code&gt;shap&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Documentation notebooks&lt;/h2&gt; &#xA;&lt;p&gt;These notebooks comprehensively demonstrate how to use specific functions and objects.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://shap.github.io/shap/notebooks/plots/decision_plot.html&#34;&gt;&lt;code&gt;shap.decision_plot&lt;/code&gt; and &lt;code&gt;shap.multioutput_decision_plot&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://shap.github.io/shap/notebooks/plots/dependence_plot.html&#34;&gt;&lt;code&gt;shap.dependence_plot&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Methods Unified by SHAP&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;em&gt;LIME:&lt;/em&gt; Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. &#34;Why should i trust you?: Explaining the predictions of any classifier.&#34; Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2016.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;em&gt;Shapley sampling values:&lt;/em&gt; Strumbelj, Erik, and Igor Kononenko. &#34;Explaining prediction models and individual predictions with feature contributions.&#34; Knowledge and information systems 41.3 (2014): 647-665.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;em&gt;DeepLIFT:&lt;/em&gt; Shrikumar, Avanti, Peyton Greenside, and Anshul Kundaje. &#34;Learning important features through propagating activation differences.&#34; arXiv preprint arXiv:1704.02685 (2017).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;em&gt;QII:&lt;/em&gt; Datta, Anupam, Shayak Sen, and Yair Zick. &#34;Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems.&#34; Security and Privacy (SP), 2016 IEEE Symposium on. IEEE, 2016.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;em&gt;Layer-wise relevance propagation:&lt;/em&gt; Bach, Sebastian, et al. &#34;On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation.&#34; PloS one 10.7 (2015): e0130140.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;em&gt;Shapley regression values:&lt;/em&gt; Lipovetsky, Stan, and Michael Conklin. &#34;Analysis of regression in game theory approach.&#34; Applied Stochastic Models in Business and Industry 17.4 (2001): 319-330.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;em&gt;Tree interpreter:&lt;/em&gt; Saabas, Ando. Interpreting random forests. &lt;a href=&#34;http://blog.datadive.net/interpreting-random-forests/&#34;&gt;http://blog.datadive.net/interpreting-random-forests/&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Citations&lt;/h2&gt; &#xA;&lt;p&gt;The algorithms and visualizations used in this package came primarily out of research in &lt;a href=&#34;https://suinlee.cs.washington.edu&#34;&gt;Su-In Lee&#39;s lab&lt;/a&gt; at the University of Washington, and Microsoft Research. If you use SHAP in your research we would appreciate a citation to the appropriate paper(s):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For general use of SHAP you can read/cite our &lt;a href=&#34;http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions&#34;&gt;NeurIPS paper&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/shap/shap/master/docs/references/shap_nips.bib&#34;&gt;bibtex&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;For TreeExplainer you can read/cite our &lt;a href=&#34;https://www.nature.com/articles/s42256-019-0138-9&#34;&gt;Nature Machine Intelligence paper&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/shap/shap/master/docs/references/tree_explainer.bib&#34;&gt;bibtex&lt;/a&gt;; &lt;a href=&#34;https://rdcu.be/b0z70&#34;&gt;free access&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;For GPUTreeExplainer you can read/cite &lt;a href=&#34;https://arxiv.org/abs/2010.13972&#34;&gt;this article&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For &lt;code&gt;force_plot&lt;/code&gt; visualizations and medical applications you can read/cite our &lt;a href=&#34;https://www.nature.com/articles/s41551-018-0304-0&#34;&gt;Nature Biomedical Engineering paper&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/shap/shap/master/docs/references/nature_bme.bib&#34;&gt;bibtex&lt;/a&gt;; &lt;a href=&#34;https://rdcu.be/baVbR&#34;&gt;free access&lt;/a&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img height=&#34;1&#34; width=&#34;1&#34; style=&#34;display:none&#34; src=&#34;https://www.facebook.com/tr?id=189147091855991&amp;amp;ev=PageView&amp;amp;noscript=1&#34;&gt;</summary>
  </entry>
</feed>