<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-12-04T01:44:18Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>huggingface/diffusion-models-class</title>
    <updated>2022-12-04T01:44:18Z</updated>
    <id>tag:github.com,2022-12-04:/huggingface/diffusion-models-class</id>
    <link href="https://github.com/huggingface/diffusion-models-class" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Materials for the Hugging Face Diffusion Models Course&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Hugging Face Diffusion Models Course&lt;/h1&gt; &#xA;&lt;p&gt;In this free course, you will:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üë©‚Äçüéì Study the theory behind diffusion models&lt;/li&gt; &#xA; &lt;li&gt;üß® Learn how to generate images and audio with the popular ü§ó Diffusers library&lt;/li&gt; &#xA; &lt;li&gt;üèãÔ∏è‚Äç‚ôÇÔ∏è Train your own diffusion models from scratch&lt;/li&gt; &#xA; &lt;li&gt;üìª Fine-tune existing diffusion models on new datasets&lt;/li&gt; &#xA; &lt;li&gt;üó∫ Explore conditional generation and guidance&lt;/li&gt; &#xA; &lt;li&gt;üßë‚Äçüî¨ Create your own custom diffusion model pipelines&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Register via the &lt;strong&gt;&lt;a href=&#34;https://huggingface.us17.list-manage.com/subscribe?u=7f57e683fa28b51bfc493d048&amp;amp;id=ef963b4162&#34;&gt;signup form&lt;/a&gt;&lt;/strong&gt; and then join us on &lt;strong&gt;&lt;a href=&#34;https://discord.gg/aYka4Yhff9&#34;&gt;Discord&lt;/a&gt;&lt;/strong&gt; to get the conversations started. Instructions on how to join specific categories/channels &lt;strong&gt;&lt;a href=&#34;https://discord.com/channels/879548962464493619/1014509271255367701&#34;&gt;are here.&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Syllabus&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;üìÜ Publishing date&lt;/th&gt; &#xA;   &lt;th&gt;üìò Unit&lt;/th&gt; &#xA;   &lt;th&gt;üë©‚Äçüíª Hands-on&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;November 28, 2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/huggingface/diffusion-models-class/tree/main/unit1&#34;&gt;An Introduction to Diffusion Models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Introduction to Diffusers and Diffusion Models From Scratch&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TBA&lt;/td&gt; &#xA;   &lt;td&gt;Fine-Tuning and Guidance&lt;/td&gt; &#xA;   &lt;td&gt;Fine-Tuning a Diffusion Model on New Data and Adding Guidance&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TBA&lt;/td&gt; &#xA;   &lt;td&gt;Stable Diffusion Intro&lt;/td&gt; &#xA;   &lt;td&gt;Exploring a Powerful Text-Conditioned Latent Diffusion Model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TBA&lt;/td&gt; &#xA;   &lt;td&gt;Stable Diffusion Deep Dive&lt;/td&gt; &#xA;   &lt;td&gt;Fine-Tuning, Sampling Tricks and Custom Pipelines&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;More information coming soon!&lt;/p&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Good skills in Python üêç&lt;/li&gt; &#xA; &lt;li&gt;Basics in Deep Learning and Pytorch&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If it&#39;s not the case yet, you can check these free resources:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python: &lt;a href=&#34;https://www.udacity.com/course/introduction-to-python--ud1110&#34;&gt;https://www.udacity.com/course/introduction-to-python--ud1110&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Intro to Deep Learning with PyTorch: &lt;a href=&#34;https://www.udacity.com/course/deep-learning-pytorch--ud188&#34;&gt;https://www.udacity.com/course/deep-learning-pytorch--ud188&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;PyTorch in 60min: &lt;a href=&#34;https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html&#34;&gt;https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Is this class free?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Yes, totally free ü•≥.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Do I need to have a Hugging Face account to follow the course?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Yes, to push your custom models and pipelines to the hub, you need an account (it&#39;s free) ü§ó.&lt;/p&gt; &#xA;&lt;p&gt;You can create one here üëâ &lt;a href=&#34;https://huggingface.co/join&#34;&gt;https://huggingface.co/join&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;What‚Äôs the format of the class?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The course will consist of at least &lt;strong&gt;4 Units.&lt;/strong&gt; More will be added as time goes on, on topics like diffusion for audio.&lt;/p&gt; &#xA;&lt;p&gt;Each unit consists of some theory and background alongisde one or more hands-on notebooks. Some units will also contain suggested projects and we&#39;ll have competitions and swag for the best pipelines and demos (more details TDB).&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>camenduru/stable-diffusion-webui-colab</title>
    <updated>2022-12-04T01:44:18Z</updated>
    <id>tag:github.com,2022-12-04:/camenduru/stable-diffusion-webui-colab</id>
    <link href="https://github.com/camenduru/stable-diffusion-webui-colab" rel="alternate"></link>
    <summary type="html">&lt;p&gt;stable diffusion webui colab&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;ü¶í Colab&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Colab Page&lt;/th&gt; &#xA;   &lt;th&gt;Model Page&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/camenduru/stable-diffusion-webui-colab/blob/main/stable_diffusion_webui_colab.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; stable_diffusion_webui_colab&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/CompVis/stable-diffusion-v-1-4-original&#34;&gt;https://huggingface.co/CompVis/stable-diffusion-v-1-4-original&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/camenduru/stable-diffusion-webui-colab/blob/main/waifu_diffusion_webui_colab.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; waifu_diffusion_webui_colab&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/hakurei/waifu-diffusion-v1-3&#34;&gt;https://huggingface.co/hakurei/waifu-diffusion-v1-3&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/camenduru/stable-diffusion-webui-colab/blob/main/stable_diffusion_inpainting_webui_colab.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; stable_diffusion_inpainting_webui_colab&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/runwayml/stable-diffusion-inpainting&#34;&gt;https://huggingface.co/runwayml/stable-diffusion-inpainting&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/camenduru/stable-diffusion-webui-colab/blob/main/stable_diffusion_1_5_webui_colab.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; stable_diffusion_1_5_webui_colab&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/runwayml/stable-diffusion-v1-5&#34;&gt;https://huggingface.co/runwayml/stable-diffusion-v1-5&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/camenduru/stable-diffusion-webui-colab/blob/main/mo_di_diffusion_webui_colab.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; mo_di_diffusion_webui_colab (Use the tokens &lt;code&gt;modern disney style&lt;/code&gt; in your prompts for the effect.)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/nitrosocke/mo-di-diffusion&#34;&gt;https://huggingface.co/nitrosocke/mo-di-diffusion&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/camenduru/stable-diffusion-webui-colab/blob/main/arcane_diffusion_3_webui_colab.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; arcane_diffusion_3_webui_colab (Use the tokens &lt;code&gt;arcane style&lt;/code&gt; in your prompts for the effect.)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/nitrosocke/Arcane-Diffusion&#34;&gt;https://huggingface.co/nitrosocke/Arcane-Diffusion&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/camenduru/stable-diffusion-webui-colab/blob/main/cyberpunk_anime_diffusion_webui_colab.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; cyberpunk_anime_diffusion_webui_colab (Use the tokens &lt;code&gt;dgs illustration style&lt;/code&gt; in your prompts for the effect.)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/DGSpitzer/Cyberpunk-Anime-Diffusion&#34;&gt;https://huggingface.co/DGSpitzer/Cyberpunk-Anime-Diffusion&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/camenduru/stable-diffusion-webui-colab/blob/main/midjourney_v4_diffusion_webui_colab.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; midjourney_v4_diffusion_webui_colab (Use the tokens &lt;code&gt;mdjrny-v4 style&lt;/code&gt; in your prompts for the effect.)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/prompthero/midjourney-v4-diffusion&#34;&gt;https://huggingface.co/prompthero/midjourney-v4-diffusion&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/camenduru/stable-diffusion-webui-colab/blob/main/papercut_diffusion_webui_colab.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; papercut_diffusion_webui_colab (Use the tokens &lt;code&gt;PaperCut&lt;/code&gt; in your prompts for the effect.)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Fictiverse/Stable_Diffusion_PaperCut_Model&#34;&gt;https://huggingface.co/Fictiverse/Stable_Diffusion_PaperCut_Model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/camenduru/stable-diffusion-webui-colab/blob/main/samdoesart_diffusion_webui_colab.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; samdoesart_diffusion_webui_colab (Use the tokens &lt;code&gt;samdoesarts style&lt;/code&gt; in your prompts for the effect.)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/lilpotat/sa&#34;&gt;https://huggingface.co/lilpotat/sa&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/camenduru/stable-diffusion-webui-colab/blob/main/anything_3_webui_colab.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; anything_3_webui_colab&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Linaqruf/anything-v3.0&#34;&gt;https://huggingface.co/Linaqruf/anything-v3.0&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/camenduru/stable-diffusion-webui-colab/blob/main/stable-diffusion-v2-webui-colab.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; stable-diffusion-v2-webui-colab&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-2&#34;&gt;https://huggingface.co/stabilityai/stable-diffusion-2&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;ü¶Ü Kaggle&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/code/camenduru/stable-diffusion-webui-kaggle&#34;&gt;https://www.kaggle.com/code/camenduru/stable-diffusion-webui-kaggle&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model Path&lt;/th&gt; &#xA;   &lt;th&gt;Model Page&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;models/PaperCut_v1.ckpt&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Fictiverse/Stable_Diffusion_PaperCut_Model&#34;&gt;https://huggingface.co/Fictiverse/Stable_Diffusion_PaperCut_Model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;models/moDi-v1-pruned.ckpt&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/nitrosocke/mo-di-diffusion&#34;&gt;https://huggingface.co/nitrosocke/mo-di-diffusion&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;models/sd-v1-5-inpainting.ckpt&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/runwayml/stable-diffusion-v1-5&#34;&gt;https://huggingface.co/runwayml/stable-diffusion-v1-5&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;models/v1-5-pruned-emaonly.ckpt&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/runwayml/stable-diffusion-v1-5&#34;&gt;https://huggingface.co/runwayml/stable-diffusion-v1-5&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;models/wd-v1-3-float32.ckpt&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/hakurei/waifu-diffusion-v1-3&#34;&gt;https://huggingface.co/hakurei/waifu-diffusion-v1-3&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;How to Use (Youtube Video)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://www.youtube.com/watch?v=ZUbLoMt_wa0&#34;&gt;&lt;img src=&#34;https://i.imgur.com/Bth3B2R.jpg&#34; alt=&#34;How to Use Stable Diffusion Webui Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Stable Diffusion Web UI&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui&#34;&gt;https://github.com/AUTOMATIC1111/stable-diffusion-webui&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki&#34;&gt;https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Models License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/spaces/CompVis/stable-diffusion-license&#34;&gt;https://huggingface.co/spaces/CompVis/stable-diffusion-license&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>invoke-ai/InvokeAI</title>
    <updated>2022-12-04T01:44:18Z</updated>
    <id>tag:github.com,2022-12-04:/invoke-ai/InvokeAI</id>
    <link href="https://github.com/invoke-ai/InvokeAI" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This version of Stable Diffusion features a slick WebGUI, an interactive command-line script that combines text2img and img2img functionality in a &#34;dream bot&#34; style interface, and multiple features and other enhancements. For more info, see the website link below.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;InvokeAI: A Stable Diffusion Toolkit&lt;/h1&gt; &#xA; &lt;p&gt;&lt;em&gt;Formerly known as lstein/stable-diffusion&lt;/em&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/invoke-ai/InvokeAI/main/docs/assets/logo.png&#34; alt=&#34;project logo&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://discord.gg/ZmtBAhwWhy&#34;&gt;&lt;img src=&#34;https://flat.badgen.net/discord/members/ZmtBAhwWhy?icon=discord&#34; alt=&#34;discord badge&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/invoke-ai/InvokeAI/releases&#34;&gt;&lt;img src=&#34;https://flat.badgen.net/github/release/invoke-ai/InvokeAI/development?icon=github&#34; alt=&#34;latest release badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/invoke-ai/InvokeAI/stargazers&#34;&gt;&lt;img src=&#34;https://flat.badgen.net/github/stars/invoke-ai/InvokeAI?icon=github&#34; alt=&#34;github stars badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://useful-forks.github.io/?repo=invoke-ai%2FInvokeAI&#34;&gt;&lt;img src=&#34;https://flat.badgen.net/github/forks/invoke-ai/InvokeAI?icon=github&#34; alt=&#34;github forks badge&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/invoke-ai/InvokeAI/actions/workflows/test-invoke-conda.yml&#34;&gt;&lt;img src=&#34;https://flat.badgen.net/github/checks/invoke-ai/InvokeAI/main?label=CI%20status%20on%20main&amp;amp;cache=900&amp;amp;icon=github&#34; alt=&#34;CI checks on main badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/invoke-ai/InvokeAI/actions?query=branch%3Adevelopment&#34;&gt;&lt;img src=&#34;https://flat.badgen.net/github/checks/invoke-ai/InvokeAI/development?label=CI%20status%20on%20dev&amp;amp;cache=900&amp;amp;icon=github&#34; alt=&#34;CI checks on dev badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/invoke-ai/InvokeAI/commits/development&#34;&gt;&lt;img src=&#34;https://flat.badgen.net/github/last-commit/invoke-ai/InvokeAI/development?icon=github&amp;amp;color=yellow&amp;amp;label=last%20dev%20commit&amp;amp;cache=900&#34; alt=&#34;latest commit to dev badge&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/invoke-ai/InvokeAI/issues?q=is%3Aissue+is%3Aopen&#34;&gt;&lt;img src=&#34;https://flat.badgen.net/github/open-issues/invoke-ai/InvokeAI?icon=github&#34; alt=&#34;github open issues badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/invoke-ai/InvokeAI/pulls?q=is%3Apr+is%3Aopen&#34;&gt;&lt;img src=&#34;https://flat.badgen.net/github/open-prs/invoke-ai/InvokeAI?icon=github&#34; alt=&#34;github open prs badge&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;This is a fork of &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;CompVis/stable-diffusion&lt;/a&gt;, the open source text-to-image generator. It provides a streamlined process with various new features and options to aid the image generation process. It runs on Windows, macOS and Linux machines, with GPU cards with as little as 4 GB of RAM. It provides both a polished Web interface (see below), and an easy-to-use command-line interface.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Quick links&lt;/strong&gt;: [&lt;a href=&#34;https://raw.githubusercontent.com/invoke-ai/InvokeAI/main/#installation&#34;&gt;How to Install&lt;/a&gt;] [&lt;a href=&#34;https://discord.gg/ZmtBAhwWhy&#34;&gt;Discord Server&lt;/a&gt;] [&lt;a href=&#34;https://invoke-ai.github.io/InvokeAI/&#34;&gt;Documentation and Tutorials&lt;/a&gt;] [&lt;a href=&#34;https://github.com/invoke-ai/InvokeAI/&#34;&gt;Code and Downloads&lt;/a&gt;] [&lt;a href=&#34;https://github.com/invoke-ai/InvokeAI/issues&#34;&gt;Bug Reports&lt;/a&gt;] [&lt;a href=&#34;https://github.com/invoke-ai/InvokeAI/discussions&#34;&gt;Discussion, Ideas &amp;amp; Q&amp;amp;A&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Note: InvokeAI is rapidly evolving. Please use the &lt;a href=&#34;https://github.com/invoke-ai/InvokeAI/issues&#34;&gt;Issues&lt;/a&gt; tab to report bugs and make feature requests. Be sure to use the provided templates. They will help us diagnose issues faster.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation Quick-Start&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Go to the bottom of the &lt;a href=&#34;https://github.com/invoke-ai/InvokeAI/releases/tag/v2.2.3&#34;&gt;Latest Release Page&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Download the .zip file for your OS (Windows/macOS/Linux).&lt;/li&gt; &#xA; &lt;li&gt;Unzip the file.&lt;/li&gt; &#xA; &lt;li&gt;If you are on Windows, double-click on the &lt;code&gt;install.bat&lt;/code&gt; script. On macOS, open a Terminal window, drag the file &lt;code&gt;install.sh&lt;/code&gt; from Finder into the Terminal, and press return. On Linux, run &lt;code&gt;install.sh&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Wait a while, until it is done.&lt;/li&gt; &#xA; &lt;li&gt;The folder where you ran the installer from will now be filled with lots of files. If you are on Windows, double-click on the &lt;code&gt;invoke.bat&lt;/code&gt; file. On macOS, open a Terminal window, drag &lt;code&gt;invoke.sh&lt;/code&gt; from the folder into the Terminal, and press return. On Linux, run &lt;code&gt;invoke.sh&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Press 2 to open the &#34;browser-based UI&#34;, press enter/return, wait a minute or two for Stable Diffusion to start up, then open your browser and go to &lt;a href=&#34;http://localhost:9090&#34;&gt;http://localhost:9090&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Type &lt;code&gt;banana sushi&lt;/code&gt; in the box on the top left and click &lt;code&gt;Invoke&lt;/code&gt;:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/invoke-ai/InvokeAI/main/docs/assets/invoke-web-server-1.png&#34; width=&#34;640&#34;&gt;&#xA;&lt;/div&gt; &#xA;&lt;p&gt;For full installation and upgrade instructions, please see: &lt;a href=&#34;https://invoke-ai.github.io/InvokeAI/installation/&#34;&gt;InvokeAI Installation Overview&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/invoke-ai/InvokeAI/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/invoke-ai/InvokeAI/main/#hardware-requirements&#34;&gt;Hardware Requirements&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/invoke-ai/InvokeAI/main/#features&#34;&gt;Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/invoke-ai/InvokeAI/main/#latest-changes&#34;&gt;Latest Changes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/invoke-ai/InvokeAI/main/#troubleshooting&#34;&gt;Troubleshooting&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/invoke-ai/InvokeAI/main/#contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/invoke-ai/InvokeAI/main/#contributors&#34;&gt;Contributors&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/invoke-ai/InvokeAI/main/#support&#34;&gt;Support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/invoke-ai/InvokeAI/main/#further-reading&#34;&gt;Further Reading&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;This fork is supported across Linux, Windows and Macintosh. Linux users can use either an Nvidia-based card (with CUDA support) or an AMD card (using the ROCm driver). For full installation and upgrade instructions, please see: &lt;a href=&#34;https://invoke-ai.github.io/InvokeAI/installation/INSTALL_SOURCE/&#34;&gt;InvokeAI Installation Overview&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Hardware Requirements&lt;/h3&gt; &#xA;&lt;p&gt;InvokeAI is supported across Linux, Windows and macOS. Linux users can use either an Nvidia-based card (with CUDA support) or an AMD card (using the ROCm driver).&lt;/p&gt; &#xA;&lt;h4&gt;System&lt;/h4&gt; &#xA;&lt;p&gt;You wil need one of the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;An NVIDIA-based graphics card with 4 GB or more VRAM memory.&lt;/li&gt; &#xA; &lt;li&gt;An Apple computer with an M1 chip.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We do not recommend the GTX 1650 or 1660 series video cards. They are unable to run in half-precision mode and do not have sufficient VRAM to render 512x512 images.&lt;/p&gt; &#xA;&lt;h4&gt;Memory&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;At least 12 GB Main Memory RAM.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Disk&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;At least 12 GB of free disk space for the machine learning model, Python, and all its dependencies.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you have a Nvidia 10xx series card (e.g. the 1080ti), please run the dream script in full-precision mode as shown below.&lt;/p&gt; &#xA;&lt;p&gt;Similarly, specify full-precision mode on Apple M1 hardware.&lt;/p&gt; &#xA;&lt;p&gt;Precision is auto configured based on the device. If however you encounter errors like &#39;expected type Float but found Half&#39; or &#39;not implemented for Half&#39; you can try starting &lt;code&gt;invoke.py&lt;/code&gt; with the &lt;code&gt;--precision=float32&lt;/code&gt; flag:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;(invokeai) ~/InvokeAI$ python scripts/invoke.py --precision=float32&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Features&lt;/h3&gt; &#xA;&lt;h4&gt;Major Features&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://invoke-ai.github.io/InvokeAI/features/WEB/&#34;&gt;Web Server&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://invoke-ai.github.io/InvokeAI/features/CLI/&#34;&gt;Interactive Command Line Interface&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://invoke-ai.github.io/InvokeAI/features/IMG2IMG/&#34;&gt;Image To Image&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://invoke-ai.github.io/InvokeAI/features/INPAINTING/&#34;&gt;Inpainting Support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://invoke-ai.github.io/InvokeAI/features/OUTPAINTING/&#34;&gt;Outpainting Support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://invoke-ai.github.io/InvokeAI/features/POSTPROCESS/&#34;&gt;Upscaling, face-restoration and outpainting&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://invoke-ai.github.io/InvokeAI/features/PROMPTS/#reading-prompts-from-a-file&#34;&gt;Reading Prompts From File&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://invoke-ai.github.io/InvokeAI/features/PROMPTS/#prompt-blending&#34;&gt;Prompt Blending&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://invoke-ai.github.io/InvokeAI/features/OTHER/#thresholding-and-perlin-noise-initialization-options&#34;&gt;Thresholding and Perlin Noise Initialization Options&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://invoke-ai.github.io/InvokeAI/features/PROMPTS/#negative-and-unconditioned-prompts&#34;&gt;Negative/Unconditioned Prompts&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://invoke-ai.github.io/InvokeAI/features/VARIATIONS/&#34;&gt;Variations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://invoke-ai.github.io/InvokeAI/features/TEXTUAL_INVERSION/&#34;&gt;Personalizing Text-to-Image Generation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://invoke-ai.github.io/InvokeAI/features/OTHER/#simplified-api&#34;&gt;Simplified API for text to image generation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Other Features&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://invoke-ai.github.io/InvokeAI/features/OTHER/#google-colab&#34;&gt;Google Colab&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://invoke-ai.github.io/InvokeAI/features/OTHER/#seamless-tiling&#34;&gt;Seamless Tiling&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://invoke-ai.github.io/InvokeAI/features/OTHER/#shortcuts-reusing-seeds&#34;&gt;Shortcut: Reusing Seeds&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://invoke-ai.github.io/InvokeAI/features/OTHER/#preload-models&#34;&gt;Preload Models&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Latest Changes&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;v2.0.1 (13 November 2022)&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;fix noisy images at high step count when using k* samplers&lt;/li&gt; &#xA;   &lt;li&gt;dream.py script now calls invoke.py module directly rather than via a new python process (which could break the environment)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;v2.0.0 (9 November 2022)&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;dream.py&lt;/code&gt; script renamed &lt;code&gt;invoke.py&lt;/code&gt;. A &lt;code&gt;dream.py&lt;/code&gt; script wrapper remains for backward compatibility.&lt;/li&gt; &#xA;   &lt;li&gt;Completely new WebGUI - launch with &lt;code&gt;python3 scripts/invoke.py --web&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Support for &lt;a href=&#34;https://invoke-ai.github.io/InvokeAI/features/INPAINTING/&#34;&gt;inpainting&lt;/a&gt; and &lt;a href=&#34;https://invoke-ai.github.io/InvokeAI/features/OUTPAINTING/&#34;&gt;outpainting&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;img2img runs on all k* samplers&lt;/li&gt; &#xA;   &lt;li&gt;Support for &lt;a href=&#34;https://invoke-ai.github.io/InvokeAI/features/PROMPTS/#negative-and-unconditioned-prompts&#34;&gt;negative prompts&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Support for CodeFormer face reconstruction&lt;/li&gt; &#xA;   &lt;li&gt;Support for Textual Inversion on macOS&lt;/li&gt; &#xA;   &lt;li&gt;Support in both WebGUI and CLI for &lt;a href=&#34;https://invoke-ai.github.io/InvokeAI/features/POSTPROCESS/&#34;&gt;post-processing of previously-generated images&lt;/a&gt; using facial reconstruction, ESRGAN upscaling, outcropping (similar to DALL-E infinite canvas), and &#34;embiggen&#34; upscaling. See the &lt;code&gt;!fix&lt;/code&gt; command.&lt;/li&gt; &#xA;   &lt;li&gt;New &lt;code&gt;--hires&lt;/code&gt; option on &lt;code&gt;invoke&amp;gt;&lt;/code&gt; line allows &lt;a href=&#34;https://invoke-ai.github.io/InvokeAI/features/CLI/#txt2img&#34;&gt;larger images to be created without duplicating elements&lt;/a&gt;, at the cost of some performance.&lt;/li&gt; &#xA;   &lt;li&gt;New &lt;code&gt;--perlin&lt;/code&gt; and &lt;code&gt;--threshold&lt;/code&gt; options allow you to add and control variation during image generation (see &lt;a href=&#34;https://github.com/invoke-ai/InvokeAI/raw/main/docs/features/OTHER.md#thresholding-and-perlin-noise-initialization-options&#34;&gt;Thresholding and Perlin Noise Initialization&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Extensive metadata now written into PNG files, allowing reliable regeneration of images and tweaking of previous settings.&lt;/li&gt; &#xA;   &lt;li&gt;Command-line completion in &lt;code&gt;invoke.py&lt;/code&gt; now works on Windows, Linux and macOS platforms.&lt;/li&gt; &#xA;   &lt;li&gt;Improved &lt;a href=&#34;https://invoke-ai.github.io/InvokeAI/features/CLI/&#34;&gt;command-line completion behavior&lt;/a&gt;. New commands added: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;List command-line history with &lt;code&gt;!history&lt;/code&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Search command-line history with &lt;code&gt;!search&lt;/code&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Clear history with &lt;code&gt;!clear&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Deprecated &lt;code&gt;--full_precision&lt;/code&gt; / &lt;code&gt;-F&lt;/code&gt;. Simply omit it and &lt;code&gt;invoke.py&lt;/code&gt; will auto configure. To switch away from auto use the new flag like &lt;code&gt;--precision=float32&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For older changelogs, please visit the &lt;strong&gt;&lt;a href=&#34;https://invoke-ai.github.io/InvokeAI/CHANGELOG#v114-11-september-2022&#34;&gt;CHANGELOG&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Troubleshooting&lt;/h3&gt; &#xA;&lt;p&gt;Please check out our &lt;strong&gt;&lt;a href=&#34;https://invoke-ai.github.io/InvokeAI/help/TROUBLESHOOT/#faq&#34;&gt;Q&amp;amp;A&lt;/a&gt;&lt;/strong&gt; to get solutions for common installation problems and other issues.&lt;/p&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;Anyone who wishes to contribute to this project, whether documentation, features, bug fixes, code cleanup, testing, or code reviews, is very much encouraged to do so. To join, just raise your hand on the InvokeAI Discord server or discussion board.&lt;/p&gt; &#xA;&lt;p&gt;If you are unfamiliar with how to contribute to GitHub projects, here is a &lt;a href=&#34;https://opensource.com/article/19/7/create-pull-request-github&#34;&gt;Getting Started Guide&lt;/a&gt;. A full set of contribution guidelines, along with templates, are in progress, but for now the most important thing is to &lt;strong&gt;make your pull request against the &#34;development&#34; branch&lt;/strong&gt;, and not against &#34;main&#34;. This will help keep public breakage to a minimum and will allow you to propose more radical changes.&lt;/p&gt; &#xA;&lt;p&gt;We hope you enjoy using our software as much as we enjoy creating it, and we hope that some of those of you who are reading this will elect to become part of our community.&lt;/p&gt; &#xA;&lt;p&gt;Welcome to InvokeAI!&lt;/p&gt; &#xA;&lt;h3&gt;Contributors&lt;/h3&gt; &#xA;&lt;p&gt;This fork is a combined effort of various people from across the world. &lt;a href=&#34;https://invoke-ai.github.io/InvokeAI/other/CONTRIBUTORS/&#34;&gt;Check out the list of all these amazing people&lt;/a&gt;. We thank them for their time, hard work and effort.&lt;/p&gt; &#xA;&lt;h3&gt;Support&lt;/h3&gt; &#xA;&lt;p&gt;For support, please use this repository&#39;s GitHub Issues tracking service. Feel free to send me an email if you use and like the script.&lt;/p&gt; &#xA;&lt;p&gt;Original portions of the software are Copyright (c) 2020 &lt;a href=&#34;https://github.com/lstein&#34;&gt;Lincoln D. Stein&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Further Reading&lt;/h3&gt; &#xA;&lt;p&gt;Please see the original README for more information on this software and underlying algorithm, located in the file &lt;a href=&#34;https://invoke-ai.github.io/InvokeAI/other/README-CompViz/&#34;&gt;README-CompViz.md&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>