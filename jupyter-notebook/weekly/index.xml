<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-06-01T02:12:48Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>jeffheaton/t81_558_deep_learning</title>
    <updated>2022-06-01T02:12:48Z</updated>
    <id>tag:github.com,2022-06-01:/jeffheaton/t81_558_deep_learning</id>
    <link href="https://github.com/jeffheaton/t81_558_deep_learning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Washington University (in St. Louis) Course T81-558: Applications of Deep Neural Networks&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;T81 558:Applications of Deep Neural Networks&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://www.wustl.edu&#34;&gt;Washington University in St. Louis&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Instructor: &lt;a href=&#34;https://sites.wustl.edu/jeffheaton/&#34;&gt;Jeff Heaton&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The content of this course changes as technology evolves&lt;/strong&gt;, to keep up to date with changes &lt;a href=&#34;https://github.com/jeffheaton&#34;&gt;follow me on GitHub&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Section 1. Fall 2022, Monday, 2:30 PM, Location: TBD&lt;/li&gt; &#xA; &lt;li&gt;Section 2. Fall 2022, Online&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Course Description&lt;/h1&gt; &#xA;&lt;p&gt;Deep learning is a group of exciting new technologies for neural networks. Through a combination of advanced training techniques and neural network architectural components, it is now possible to create neural networks that can handle tabular data, images, text, and audio as both input and output. Deep learning allows a neural network to learn hierarchies of information in a way that is like the function of the human brain. This course will introduce the student to classic neural network structures, Convolution Neural Networks (CNN), Long Short-Term Memory (LSTM), Gated Recurrent Neural Networks (GRU), General Adversarial Networks (GAN) and reinforcement learning. Application of these architectures to computer vision, time series, security, natural language processing (NLP), and data generation will be covered. High Performance Computing (HPC) aspects will demonstrate how deep learning can be leveraged both on graphical processing units (GPUs), as well as grids. Focus is primarily upon the application of deep learning to problems, with some introduction to mathematical foundations. Students will use the Python programming language to implement deep learning using Google TensorFlow and Keras. It is not necessary to know Python prior to this course; however, familiarity of at least one programming language is assumed. This course will be delivered in a hybrid format that includes both classroom and online instruction.&lt;/p&gt; &#xA;&lt;h1&gt;Textbook&lt;/h1&gt; &#xA;&lt;p&gt;The complete text for this course is here on GitHub. This same material is also available in &lt;a href=&#34;https://www.heatonresearch.com/book/applications-deep-neural-networks-keras.html&#34;&gt;book format&lt;/a&gt;. The course textbook is “Applications of Deep Neural networks with Keras“, ISBN 9798416344269.&lt;/p&gt; &#xA;&lt;p&gt;If you would like to cite the material from this course/book, please use the following BibTex citation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{heaton2020applications,&#xA;    title={Applications of Deep Neural Networks},&#xA;    author={Jeff Heaton},&#xA;    year={2020},&#xA;    eprint={2009.05673},&#xA;    archivePrefix={arXiv},&#xA;    primaryClass={cs.LG}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Objectives&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Explain how neural networks (deep and otherwise) compare to other machine learning models.&lt;/li&gt; &#xA; &lt;li&gt;Determine when a deep neural network would be a good choice for a particular problem.&lt;/li&gt; &#xA; &lt;li&gt;Demonstrate your understanding of the material through a final project uploaded to GitHub.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Syllabus&lt;/h1&gt; &#xA;&lt;p&gt;This syllabus presents the expected class schedule, due dates, and reading assignments. &lt;a href=&#34;https://data.heatonresearch.com/wustl/jheaton-t81-558-spring-2022-syllabus.pdf&#34;&gt;Download current syllabus.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Module&lt;/th&gt; &#xA;   &lt;th&gt;Content&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_01_1_overview.ipynb&#34;&gt;Module 1&lt;/a&gt;&lt;br&gt;&lt;strong&gt;Meet on 08/29/2022&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 1: Python Preliminaries&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 1.1: Course Overview&lt;/li&gt;&#xA;     &lt;li&gt;Part 1.2: Introduction to Python&lt;/li&gt;&#xA;     &lt;li&gt;Part 1.3: Python Lists, Dictionaries, Sets &amp;amp; JSON&lt;/li&gt;&#xA;     &lt;li&gt;Part 1.4: File Handling&lt;/li&gt;&#xA;     &lt;li&gt;Part 1.5: Functions, Lambdas, and Map/ReducePython Preliminaries&lt;/li&gt;&#xA;     &lt;li&gt;&lt;strong&gt;We will meet on campus this week! (first meeting)&lt;/strong&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_02_1_python_pandas.ipynb&#34;&gt;Module 2&lt;/a&gt;&lt;br&gt;Week of 09/12/2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 2: Python for Machine Learning&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt; Part 2.1: Introduction to Pandas for Deep Learning&lt;/li&gt;&#xA;     &lt;li&gt;Part 2.2: Encoding Categorical Values in Pandas&lt;/li&gt;&#xA;     &lt;li&gt;Part 2.3: Grouping, Sorting, and Shuffling&lt;/li&gt;&#xA;     &lt;li&gt;Part 2.4: Using Apply and Map in Pandas&lt;/li&gt;&#xA;     &lt;li&gt;Part 2.5: Feature Engineering in Padas&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class1.ipynb&#34;&gt;Module 1 Program&lt;/a&gt; due: 09/13/2022&lt;/li&gt;&#xA;     &lt;li&gt; Icebreaker due: 09/13/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_03_1_neural_net.ipynb&#34;&gt;Module 3&lt;/a&gt;&lt;br&gt;Week of 09/19/2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 3: TensorFlow and Keras for Neural Networks&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 3.1: Deep Learning and Neural Network Introduction&lt;/li&gt;&#xA;     &lt;li&gt;Part 3.2: Introduction to Tensorflow &amp;amp; Keras&lt;/li&gt;&#xA;     &lt;li&gt;Part 3.3: Saving and Loading a Keras Neural Network&lt;/li&gt;&#xA;     &lt;li&gt;Part 3.4: Early Stopping in Keras to Prevent Overfitting&lt;/li&gt;&#xA;     &lt;li&gt;Part 3.5: Extracting Keras Weights and Manual Neural Network Calculation&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class2.ipynb&#34;&gt;Module 2: Program&lt;/a&gt; due: 09/20/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_04_1_feature_encode.ipynb&#34;&gt;Module 4&lt;/a&gt;&lt;br&gt;Week of 09/26/2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 4: Training for Tabular Data&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 4.1: Encoding a Feature Vector for Keras Deep Learning&lt;/li&gt;&#xA;     &lt;li&gt;Part 4.2: Keras Multiclass Classification for Deep Neural Networks with ROC and AUC&lt;/li&gt;&#xA;     &lt;li&gt;Part 4.3: Keras Regression for Deep Neural Networks with RMSE&lt;/li&gt;&#xA;     &lt;li&gt;Part 4.4: Backpropagation, Nesterov Momentum, and ADAM Training&lt;/li&gt;&#xA;     &lt;li&gt;Part 4.5: Neural Network RMSE and Log Loss Error Calculation from Scratch&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class3.ipynb&#34;&gt;Module 3 Program&lt;/a&gt; due: 09/27/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_05_1_reg_ridge_lasso.ipynb&#34;&gt;Module 5&lt;/a&gt;&lt;br&gt;&lt;strong&gt;Meet on 10/03/2022&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 5: Regularization and Dropout&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 5.1: Introduction to Regularization: Ridge and Lasso&lt;/li&gt;&#xA;     &lt;li&gt;Part 5.2: Using K-Fold Cross Validation with Keras&lt;/li&gt;&#xA;     &lt;li&gt;Part 5.3: Using L1 and L2 Regularization with Keras to Decrease Overfitting&lt;/li&gt;&#xA;     &lt;li&gt;Part 5.4: Drop Out for Keras to Decrease Overfitting&lt;/li&gt;&#xA;     &lt;li&gt;Part 5.5: Bootstrapping and Benchmarking Hyperparameters&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class4.ipynb&#34;&gt;Module 4 Program&lt;/a&gt; due: 10/04/2022&lt;/li&gt;&#xA;     &lt;li&gt;&lt;strong&gt;We will meet on campus this week! (second meeting)&lt;/strong&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_06_1_python_images.ipynb&#34;&gt;Module 6&lt;/a&gt;&lt;br&gt;Week of 10/17/2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 6: CNN for Vision&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;      Part 6.1: Image Processing in Python&#xA;     &lt;li&gt;Part 6.2: Using Convolutional Networks with Keras&lt;/li&gt;&#xA;     &lt;li&gt;Part 6.3: Using Pretrained Neural Networks&lt;/li&gt;&#xA;     &lt;li&gt;Part 6.4: Looking at Keras Generators and Image Augmentation&lt;/li&gt;&#xA;     &lt;li&gt;Part 6.5: Recognizing Multiple Images with YOLOv5&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class5.ipynb&#34;&gt;Module 5 Program&lt;/a&gt; due: 10/18/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_07_1_gan_intro.ipynb&#34;&gt;Module 7&lt;/a&gt;&lt;br&gt;Week of 10/24/2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 7: Generative Adversarial Networks (GANs)&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 7.1: Introduction to GANS for Image and Data Generation&lt;/li&gt;&#xA;     &lt;li&gt;Part 7.2: Train StyleGAN3 with your Own Images&lt;/li&gt;&#xA;     &lt;li&gt;Part 7.3: Exploring the StyleGAN Latent Vector&lt;/li&gt;&#xA;     &lt;li&gt;Part 7.4: GANS to Enhance Old Photographs Deoldify&lt;/li&gt;&#xA;     &lt;li&gt;Part 7.5: GANs for Tabular Synthetic Data Generation&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class6.ipynb&#34;&gt;Module 6 Assignment&lt;/a&gt; due: 10/25/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_08_1_kaggle_intro.ipynb&#34;&gt;Module 8&lt;/a&gt;&lt;br&gt;&lt;strong&gt;Meet on 10/31/2022&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 8: Kaggle&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 8.1: Introduction to Kaggle&lt;/li&gt;&#xA;     &lt;li&gt;Part 8.2: Building Ensembles with Scikit-Learn and Keras&lt;/li&gt;&#xA;     &lt;li&gt;Part 8.3: How Should you Architect Your Keras Neural Network: Hyperparameters&lt;/li&gt;&#xA;     &lt;li&gt;Part 8.4: Bayesian Hyperparameter Optimization for Keras&lt;/li&gt;&#xA;     &lt;li&gt;Part 8.5: Current Semester&#39;s Kaggle&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class7.ipynb&#34;&gt;Module 7 Assignment&lt;/a&gt; due: 11/01/2022&lt;/li&gt;&#xA;     &lt;li&gt;&lt;strong&gt;We will meet on campus this week! (third meeting)&lt;/strong&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_09_1_keras_transfer.ipynb&#34;&gt;Module 9&lt;/a&gt;&lt;br&gt;Week of 11/07/2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 9: Transfer Learning&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 9.1: Introduction to Keras Transfer Learning&lt;/li&gt;&#xA;     &lt;li&gt;Part 9.2: Keras Transfer Learning for Computer Vision&lt;/li&gt;&#xA;     &lt;li&gt;Part 9.3: Transfer Learning for NLP with Keras&lt;/li&gt;&#xA;     &lt;li&gt;Part 9.4: Transfer Learning for Facial Feature Recognition&lt;/li&gt;&#xA;     &lt;li&gt;Part 9.5: Transfer Learning for Style Transfer&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class8.ipynb&#34;&gt;Module 8 Assignment&lt;/a&gt; due: 11/08/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_10_1_timeseries.ipynb&#34;&gt;Module 10&lt;/a&gt;&lt;br&gt;Week of 11/14/2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 10: Time Series in Keras&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 10.1: Time Series Data Encoding for Deep Learning, Keras&lt;/li&gt;&#xA;     &lt;li&gt;Part 10.2: Programming LSTM with Keras and&lt;/li&gt;&#xA;     &lt;li&gt;Part 10.3: Text Generation with Keras&lt;/li&gt;&#xA;     &lt;li&gt;Part 10.4: Introduction to Transformers&lt;/li&gt;&#xA;     &lt;li&gt;Part 10.5: Transformers for Timeseries&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class9.ipynb&#34;&gt;Module 9 Assignment&lt;/a&gt; due: 11/15/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_11_01_huggingface.ipynb&#34;&gt;Module 11&lt;/a&gt;&lt;br&gt;Week of 11/21/2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 11: Natural Language Processing&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 11.1: Hugging Face Introduction&lt;/li&gt;&#xA;     &lt;li&gt;Part 11.2: Hugging Face Tokenizers&lt;/li&gt;&#xA;     &lt;li&gt;Part 11.3: Hugging Face Data Sets&lt;/li&gt;&#xA;     &lt;li&gt;Part 11.4: Training a Model in Hugging Face&lt;/li&gt;&#xA;     &lt;li&gt;Part 11.5: What are Embedding Layers in Keras&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class10.ipynb&#34;&gt;Module 10 Assignment&lt;/a&gt; due: 11/22/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_12_01_ai_gym.ipynb&#34;&gt;Module 12&lt;/a&gt;&lt;br&gt;Week of 11/28/2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 12: Reinforcement Learning&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Kaggle Assignment due: 11/29/2022 (approx 4-6PM, due to Kaggle GMT timezone)&lt;/li&gt;&#xA;     &lt;li&gt;Part 12.1: Introduction to the OpenAI Gym&lt;/li&gt;&#xA;     &lt;li&gt;Part 12.2: Introduction to Q-Learning for Keras&lt;/li&gt;&#xA;     &lt;li&gt;Part 12.3: Keras Q-Learning in the OpenAI Gym&lt;/li&gt;&#xA;     &lt;li&gt;Part 12.4: Atari Games with Keras Neural Networks&lt;/li&gt;&#xA;     &lt;li&gt;Part 12.5: Application of Reinforcement Learning&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class11.ipynb&#34;&gt;Module 11 Assignment&lt;/a&gt; due: 11/29/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_13_01_flask.ipynb&#34;&gt;Module 13&lt;/a&gt;&lt;br&gt;&lt;strong&gt;Meet on 12/05/2022&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 13: Deployment and Monitoring&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 13.1: Flask and Deep Learning Web Services &lt;/li&gt;&#xA;     &lt;li&gt;Part 13.2: Interrupting and Continuing Training&lt;/li&gt;&#xA;     &lt;li&gt;Part 13.3: Using a Keras Deep Neural Network with a Web Application&lt;/li&gt;&#xA;     &lt;li&gt;Part 13.4: When to Retrain Your Neural Network&lt;/li&gt;&#xA;     &lt;li&gt;Part 13.5: Tensor Processing Units (TPUs)&lt;/li&gt;&#xA;     &lt;li&gt;&lt;strong&gt;We will meet on campus this week! (fourth meeting)&lt;/strong&gt;&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class12.ipynb&#34;&gt;Module 12 Assignment&lt;/a&gt; due: 12/06/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Datasets&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://data.heatonresearch.com/data/t81-558/index.html&#34;&gt;Datasets can be downloaded here&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>bryandlee/animegan2-pytorch</title>
    <updated>2022-06-01T02:12:48Z</updated>
    <id>tag:github.com,2022-06-01:/bryandlee/animegan2-pytorch</id>
    <link href="https://github.com/bryandlee/animegan2-pytorch" rel="alternate"></link>
    <summary type="html">&lt;p&gt;PyTorch implementation of AnimeGANv2&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;PyTorch Implementation of &lt;a href=&#34;https://github.com/TachibanaYoshino/AnimeGANv2&#34;&gt;AnimeGANv2&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Updates&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2021-10-17&lt;/code&gt; Add weights for &lt;a href=&#34;https://raw.githubusercontent.com/bryandlee/animegan2-pytorch/main/#additional-model-weights&#34;&gt;FacePortraitV2&lt;/a&gt;. &lt;a href=&#34;https://colab.research.google.com/github/bryandlee/animegan2-pytorch/blob/main/colab_demo.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/26464535/142294796-54394a4a-a566-47a1-b9ab-4e715b901442.gif&#34; alt=&#34;sample&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2021-11-07&lt;/code&gt; Thanks to &lt;a href=&#34;https://twitter.com/ak92501&#34;&gt;ak92501&lt;/a&gt;, a &lt;a href=&#34;https://huggingface.co/spaces/akhaliq/AnimeGANv2&#34;&gt;web demo&lt;/a&gt; is integrated to &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces&lt;/a&gt; with &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;. &lt;a href=&#34;https://huggingface.co/spaces/akhaliq/AnimeGANv2&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2021-11-07&lt;/code&gt; Thanks to &lt;a href=&#34;https://github.com/xhlulu&#34;&gt;xhlulu&lt;/a&gt;, the &lt;code&gt;torch.hub&lt;/code&gt; model is now available. See &lt;a href=&#34;https://raw.githubusercontent.com/bryandlee/animegan2-pytorch/main/#torch-hub-usage&#34;&gt;Torch Hub Usage&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Basic Usage&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Inference&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python test.py --input_dir [image_folder_path] --device [cpu/cuda]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Torch Hub Usage&lt;/h2&gt; &#xA;&lt;p&gt;You can load the model via &lt;code&gt;torch.hub&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;model = torch.hub.load(&#34;bryandlee/animegan2-pytorch&#34;, &#34;generator&#34;).eval()&#xA;out = model(img_tensor)  # BCHW tensor&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Currently, the following &lt;code&gt;pretrained&lt;/code&gt; shorthands are available:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = torch.hub.load(&#34;bryandlee/animegan2-pytorch:main&#34;, &#34;generator&#34;, pretrained=&#34;celeba_distill&#34;)&#xA;model = torch.hub.load(&#34;bryandlee/animegan2-pytorch:main&#34;, &#34;generator&#34;, pretrained=&#34;face_paint_512_v1&#34;)&#xA;model = torch.hub.load(&#34;bryandlee/animegan2-pytorch:main&#34;, &#34;generator&#34;, pretrained=&#34;face_paint_512_v2&#34;)&#xA;model = torch.hub.load(&#34;bryandlee/animegan2-pytorch:main&#34;, &#34;generator&#34;, pretrained=&#34;paprika&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also load the &lt;code&gt;face2paint&lt;/code&gt; util function:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from PIL import Image&#xA;&#xA;face2paint = torch.hub.load(&#34;bryandlee/animegan2-pytorch:main&#34;, &#34;face2paint&#34;, size=512)&#xA;&#xA;img = Image.open(...).convert(&#34;RGB&#34;)&#xA;out = face2paint(model, img)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;More details about &lt;code&gt;torch.hub&lt;/code&gt; is in &lt;a href=&#34;https://pytorch.org/docs/stable/hub.html&#34;&gt;the torch docs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Weight Conversion from the Original Repo (Tensorflow)&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install the &lt;a href=&#34;https://github.com/TachibanaYoshino/AnimeGANv2#requirements&#34;&gt;original repo&#39;s dependencies&lt;/a&gt;: python 3.6, tensorflow 1.15.0-gpu&lt;/li&gt; &#xA; &lt;li&gt;Install torch &amp;gt;= 1.7.1&lt;/li&gt; &#xA; &lt;li&gt;Clone the original repo &amp;amp; run&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/TachibanaYoshino/AnimeGANv2&#xA;python convert_weights.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;samples&lt;/summary&gt; &#xA; &lt;br&gt; Results from converted `Paprika` style model (input image, original tensorflow result, pytorch result from left to right) &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bryandlee/animegan2-pytorch/main/samples/compare/1.jpg&#34; width=&#34;960&#34;&gt; &amp;nbsp; &lt;img src=&#34;https://raw.githubusercontent.com/bryandlee/animegan2-pytorch/main/samples/compare/2.jpg&#34; width=&#34;960&#34;&gt; &amp;nbsp; &lt;img src=&#34;https://raw.githubusercontent.com/bryandlee/animegan2-pytorch/main/samples/compare/3.jpg&#34; width=&#34;960&#34;&gt; &amp;nbsp;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Results from converted weights slightly different due to the &lt;a href=&#34;https://github.com/pytorch/pytorch/issues/10604&#34;&gt;bilinear upsample issue&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Additional Model Weights&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Webtoon Face&lt;/strong&gt; &lt;a href=&#34;https://drive.google.com/file/d/10T6F3-_RFOCJn6lMb-6mRmcISuYWJXGc&#34;&gt;[ckpt]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;samples&lt;/summary&gt; &#xA; &lt;p&gt;Trained on &lt;b&gt;256x256&lt;/b&gt; face images. Distilled from &lt;a href=&#34;https://github.com/bryandlee/naver-webtoon-faces/raw/master/README.md#face2webtoon&#34;&gt;webtoon face model&lt;/a&gt; with L2 + VGG + GAN Loss and CelebA-HQ images.&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/26464535/143959011-1740d4d3-790b-4c4c-b875-24404ef9c614.jpg&#34; alt=&#34;face_results&#34;&gt; &amp;nbsp;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;&lt;strong&gt;Face Portrait v1&lt;/strong&gt; &lt;a href=&#34;https://drive.google.com/file/d/1WK5Mdt6mwlcsqCZMHkCUSDJxN1UyFi0-&#34;&gt;[ckpt]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;samples&lt;/summary&gt; &#xA; &lt;p&gt;Trained on &lt;b&gt;512x512&lt;/b&gt; face images.&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1jCqcKekdtKzW7cxiw_bjbbfLsPh-dEds?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/26464535/127134790-93595da2-4f8b-4aca-a9d7-98699c5e6914.jpg&#34; alt=&#34;samples&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://youtu.be/CbMfI-HNCzw?t=317&#34;&gt;📺&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/26464535/129888683-98bb6283-7bb8-4d1a-a04a-e795f5858dcf.gif&#34; alt=&#34;sample&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;&lt;strong&gt;Face Portrait v2&lt;/strong&gt; &lt;a href=&#34;https://drive.google.com/uc?id=18H3iK09_d54qEDoWIc82SyWB2xun4gjU&#34;&gt;[ckpt]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;samples&lt;/summary&gt; &#xA; &lt;p&gt;Trained on &lt;b&gt;512x512&lt;/b&gt; face images. Compared to v1, &lt;code&gt;🔻beautify&lt;/code&gt; &lt;code&gt;🔺robustness&lt;/code&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1jCqcKekdtKzW7cxiw_bjbbfLsPh-dEds?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/26464535/137619176-59620b59-4e20-4d98-9559-a424f86b7f24.jpg&#34; alt=&#34;face_portrait_v2_0&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/26464535/137619181-a45c9230-f5e7-4f3c-8002-7c266f89de45.jpg&#34; alt=&#34;face_portrait_v2_1&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;🦑 🎮 🔥&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/26464535/137619183-20e94f11-7a8e-4c3e-9b45-378ab63827ca.jpg&#34; alt=&#34;face_portrait_v2_squid_game&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt;</summary>
  </entry>
  <entry>
    <title>rlabbe/Kalman-and-Bayesian-Filters-in-Python</title>
    <updated>2022-06-01T02:12:48Z</updated>
    <id>tag:github.com,2022-06-01:/rlabbe/Kalman-and-Bayesian-Filters-in-Python</id>
    <link href="https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Kalman Filter book using Jupyter Notebook. Focuses on building intuition and experience, not formal proofs. Includes Kalman filters,extended Kalman filters, unscented Kalman filters, particle filters, and more. All exercises include solutions.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;a href=&#34;https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python&#34;&gt;Kalman and Bayesian Filters in Python&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;Introductory text for Kalman and Bayesian filters. All code is written in Python, and the book itself is written using Jupyter Notebook so that you can run and modify the code in your browser. What better way to learn?&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&#34;Kalman and Bayesian Filters in Python&#34; looks amazing! ... your book is just what I needed&lt;/strong&gt; - Allen Downey, Professor and O&#39;Reilly author.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Thanks for all your work on publishing your introductory text on Kalman Filtering, as well as the Python Kalman Filtering libraries. We’ve been using it internally to teach some key state estimation concepts to folks and it’s been a huge help.&lt;/strong&gt; - Sam Rodkey, SpaceX&lt;/p&gt; &#xA;&lt;p&gt;Start reading online now by clicking the binder or Azure badge below:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://beta.mybinder.org/v2/gh/rlabbe/Kalman-and-Bayesian-Filters-in-Python/master&#34;&gt;&lt;img src=&#34;http://mybinder.org/badge.svg?sanitize=true&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python/master/animations/05_dog_track.gif&#34; alt=&#34;alt tag&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;What are Kalman and Bayesian Filters?&lt;/h2&gt; &#xA;&lt;p&gt;Sensors are noisy. The world is full of data and events that we want to measure and track, but we cannot rely on sensors to give us perfect information. The GPS in my car reports altitude. Each time I pass the same point in the road it reports a slightly different altitude. My kitchen scale gives me different readings if I weigh the same object twice.&lt;/p&gt; &#xA;&lt;p&gt;In simple cases the solution is obvious. If my scale gives slightly different readings I can just take a few readings and average them. Or I can replace it with a more accurate scale. But what do we do when the sensor is very noisy, or the environment makes data collection difficult? We may be trying to track the movement of a low flying aircraft. We may want to create an autopilot for a drone, or ensure that our farm tractor seeded the entire field. I work on computer vision, and I need to track moving objects in images, and the computer vision algorithms create very noisy and unreliable results.&lt;/p&gt; &#xA;&lt;p&gt;This book teaches you how to solve these sorts of filtering problems. I use many different algorithms, but they are all based on Bayesian probability. In simple terms Bayesian probability determines what is likely to be true based on past information.&lt;/p&gt; &#xA;&lt;p&gt;If I asked you the heading of my car at this moment you would have no idea. You&#39;d prefer a number between 1° and 360° degrees, and have a 1 in 360 chance of being right. Now suppose I told you that 2 seconds ago its heading was 243°. In 2 seconds my car could not turn very far, so you could make a far more accurate prediction. You are using past information to more accurately infer information about the present or future.&lt;/p&gt; &#xA;&lt;p&gt;The world is also noisy. That prediction helps you make a better estimate, but it also subject to noise. I may have just braked for a dog or swerved around a pothole. Strong winds and ice on the road are external influences on the path of my car. In control literature we call this noise though you may not think of it that way.&lt;/p&gt; &#xA;&lt;p&gt;There is more to Bayesian probability, but you have the main idea. Knowledge is uncertain, and we alter our beliefs based on the strength of the evidence. Kalman and Bayesian filters blend our noisy and limited knowledge of how a system behaves with the noisy and limited sensor readings to produce the best possible estimate of the state of the system. Our principle is to never discard information.&lt;/p&gt; &#xA;&lt;p&gt;Say we are tracking an object and a sensor reports that it suddenly changed direction. Did it really turn, or is the data noisy? It depends. If this is a jet fighter we&#39;d be very inclined to believe the report of a sudden maneuver. If it is a freight train on a straight track we would discount it. We&#39;d further modify our belief depending on how accurate the sensor is. Our beliefs depend on the past and on our knowledge of the system we are tracking and on the characteristics of the sensors.&lt;/p&gt; &#xA;&lt;p&gt;The Kalman filter was invented by Rudolf Emil Kálmán to solve this sort of problem in a mathematically optimal way. Its first use was on the Apollo missions to the moon, and since then it has been used in an enormous variety of domains. There are Kalman filters in aircraft, on submarines, and on cruise missiles. Wall street uses them to track the market. They are used in robots, in IoT (Internet of Things) sensors, and in laboratory instruments. Chemical plants use them to control and monitor reactions. They are used to perform medical imaging and to remove noise from cardiac signals. If it involves a sensor and/or time-series data, a Kalman filter or a close relative to the Kalman filter is usually involved.&lt;/p&gt; &#xA;&lt;h2&gt;Motivation&lt;/h2&gt; &#xA;&lt;p&gt;The motivation for this book came out of my desire for a gentle introduction to Kalman filtering. I&#39;m a software engineer that spent almost two decades in the avionics field, and so I have always been &#39;bumping elbows&#39; with the Kalman filter, but never implemented one myself. As I moved into solving tracking problems with computer vision the need became urgent. There are classic textbooks in the field, such as Grewal and Andrew&#39;s excellent &lt;em&gt;Kalman Filtering&lt;/em&gt;. But sitting down and trying to read many of these books is a dismal experience if you do not have the required background. Typically the first few chapters fly through several years of undergraduate math, blithely referring you to textbooks on topics such as Itō calculus, and present an entire semester&#39;s worth of statistics in a few brief paragraphs. They are good texts for an upper undergraduate course, and an invaluable reference to researchers and professionals, but the going is truly difficult for the more casual reader. Symbology is introduced without explanation, different texts use different terms and variables for the same concept, and the books are almost devoid of examples or worked problems. I often found myself able to parse the words and comprehend the mathematics of a definition, but had no idea as to what real world phenomena they describe. &#34;But what does that &lt;em&gt;mean?&lt;/em&gt;&#34; was my repeated thought.&lt;/p&gt; &#xA;&lt;p&gt;However, as I began to finally understand the Kalman filter I realized the underlying concepts are quite straightforward. A few simple probability rules, some intuition about how we integrate disparate knowledge to explain events in our everyday life and the core concepts of the Kalman filter are accessible. Kalman filters have a reputation for difficulty, but shorn of much of the formal terminology the beauty of the subject and of their math became clear to me, and I fell in love with the topic.&lt;/p&gt; &#xA;&lt;p&gt;As I began to understand the math and theory more difficulties present themselves. A book or paper&#39;s author makes some statement of fact and presents a graph as proof. Unfortunately, why the statement is true is not clear to me, nor is the method for making that plot obvious. Or maybe I wonder &#34;is this true if R=0?&#34; Or the author provides pseudocode at such a high level that the implementation is not obvious. Some books offer Matlab code, but I do not have a license to that expensive package. Finally, many books end each chapter with many useful exercises. Exercises which you need to understand if you want to implement Kalman filters for yourself, but exercises with no answers. If you are using the book in a classroom, perhaps this is okay, but it is terrible for the independent reader. I loathe that an author withholds information from me, presumably to avoid &#39;cheating&#39; by the student in the classroom.&lt;/p&gt; &#xA;&lt;p&gt;From my point of view none of this is necessary. Certainly if you are designing a Kalman filter for an aircraft or missile you must thoroughly master all of the mathematics and topics in a typical Kalman filter textbook. I just want to track an image on a screen, or write some code for an Arduino project. I want to know how the plots in the book are made, and chose different parameters than the author chose. I want to run simulations. I want to inject more noise in the signal and see how a filter performs. There are thousands of opportunities for using Kalman filters in everyday code, and yet this fairly straightforward topic is the provenance of rocket scientists and academics.&lt;/p&gt; &#xA;&lt;p&gt;I wrote this book to address all of those needs. This is not the book for you if you program navigation computers for Boeing or design radars for Raytheon. Go get an advanced degree at Georgia Tech, UW, or the like, because you&#39;ll need it. This book is for the hobbyist, the curious, and the working engineer that needs to filter or smooth data.&lt;/p&gt; &#xA;&lt;p&gt;This book is interactive. While you can read it online as static content, I urge you to use it as intended. It is written using Jupyter Notebook, which allows me to combine text, math, Python, and Python output in one place. Every plot, every piece of data in this book is generated from Python that is available to you right inside the notebook. Want to double the value of a parameter? Click on the Python cell, change the parameter&#39;s value, and click &#39;Run&#39;. A new plot or printed output will appear in the book.&lt;/p&gt; &#xA;&lt;p&gt;This book has exercises, but it also has the answers. I trust you. If you just need an answer, go ahead and read the answer. If you want to internalize this knowledge, try to implement the exercise before you read the answer.&lt;/p&gt; &#xA;&lt;p&gt;This book has supporting libraries for computing statistics, plotting various things related to filters, and for the various filters that we cover. This does require a strong caveat; most of the code is written for didactic purposes. It is rare that I chose the most efficient solution (which often obscures the intent of the code), and in the first parts of the book I did not concern myself with numerical stability. This is important to understand - Kalman filters in aircraft are carefully designed and implemented to be numerically stable; the naive implementation is not stable in many cases. If you are serious about Kalman filters this book will not be the last book you need. My intention is to introduce you to the concepts and mathematics, and to get you to the point where the textbooks are approachable.&lt;/p&gt; &#xA;&lt;p&gt;Finally, this book is free. The cost for the books required to learn Kalman filtering is somewhat prohibitive even for a Silicon Valley engineer like myself; I cannot believe they are within the reach of someone in a depressed economy, or a financially struggling student. I have gained so much from free software like Python, and free books like those from Allen B. Downey &lt;a href=&#34;http://www.greenteapress.com/&#34;&gt;here&lt;/a&gt;. It&#39;s time to repay that. So, the book is free, it is hosted on free servers, and it uses only free and open software such as IPython and MathJax to create the book.&lt;/p&gt; &#xA;&lt;h2&gt;Reading Online&lt;/h2&gt; &#xA;&lt;p&gt;The book is written as a collection of Jupyter Notebooks, an interactive, browser based system that allows you to combine text, Python, and math into your browser. There are multiple ways to read these online, listed below.&lt;/p&gt; &#xA;&lt;h3&gt;binder&lt;/h3&gt; &#xA;&lt;p&gt;binder serves interactive notebooks online, so you can run the code and change the code within your browser without downloading the book or installing Jupyter.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://beta.mybinder.org/v2/gh/rlabbe/Kalman-and-Bayesian-Filters-in-Python/master&#34;&gt;&lt;img src=&#34;http://mybinder.org/badge.svg?sanitize=true&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;nbviewer&lt;/h3&gt; &#xA;&lt;p&gt;The website &lt;a href=&#34;http://nbviewer.org&#34;&gt;http://nbviewer.org&lt;/a&gt; provides a Jupyter Notebook server that renders notebooks stored at github (or elsewhere). The rendering is done in real time when you load the book. You may use &lt;a href=&#34;http://nbviewer.ipython.org/github/rlabbe/Kalman-and-Bayesian-Filters-in-Python/blob/master/table_of_contents.ipynb&#34;&gt;&lt;em&gt;this nbviewer link&lt;/em&gt;&lt;/a&gt; to access my book via nbviewer. If you read my book today, and then I make a change tomorrow, when you go back tomorrow you will see that change. Notebooks are rendered statically - you can read them, but not modify or run the code.&lt;/p&gt; &#xA;&lt;p&gt;nbviewer seems to lag the checked in version by a few days, so you might not be reading the most recent content.&lt;/p&gt; &#xA;&lt;h3&gt;GitHub&lt;/h3&gt; &#xA;&lt;p&gt;GitHub is able to render the notebooks directly. The quickest way to view a notebook is to just click on them above. However, it renders the math incorrectly, and I cannot recommend using it if you are doing more than just dipping into the book.&lt;/p&gt; &#xA;&lt;h2&gt;PDF Version&lt;/h2&gt; &#xA;&lt;p&gt;A PDF version of the book is available [here]&lt;a href=&#34;https://drive.google.com/file/d/0By_SW19c1BfhSVFzNHc0SjduNzg/view?usp=sharing&amp;amp;resourcekey=0-41olC9ht9xE3wQe2zHZ45A&#34;&gt;https://drive.google.com/file/d/0By_SW19c1BfhSVFzNHc0SjduNzg/view?usp=sharing&amp;amp;resourcekey=0-41olC9ht9xE3wQe2zHZ45A&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;The PDF will usually lag behind what is in github as I don&#39;t update it for every minor check in.&lt;/p&gt; &#xA;&lt;h2&gt;Downloading and Running the Book&lt;/h2&gt; &#xA;&lt;p&gt;However, this book is intended to be interactive and I recommend using it in that form. It&#39;s a little more effort to set up, but worth it. If you install IPython and some supporting libraries on your computer and then clone this book you will be able to run all of the code in the book yourself. You can perform experiments, see how filters react to different data, see how different filters react to the same data, and so on. I find this sort of immediate feedback both vital and invigorating. You do not have to wonder &#34;what happens if&#34;. Try it and see!&lt;/p&gt; &#xA;&lt;p&gt;The book and supporting software can be downloaded from GitHub by running this command on the command line:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone --depth=1 https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python.git&#xA;pip install filterpy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Instructions for installation of the IPython ecosystem can be found in the Installation appendix, found &lt;a href=&#34;http://nbviewer.ipython.org/github/rlabbe/Kalman-and-Bayesian-Filters-in-Python/blob/master/Appendix-A-Installation.ipynb&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Once the software is installed you can navigate to the installation directory and run Jupyter notebook with the command line instruction&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;jupyter notebook&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will open a browser window showing the contents of the base directory. The book is organized into chapters, each contained within one IPython Notebook (these notebook files have a .ipynb file extension). For example, to read Chapter 2, click on the file &lt;em&gt;02-Discrete-Bayes.ipynb&lt;/em&gt;. Sometimes there are supporting notebooks for doing things like generating animations that are displayed in the chapter. These are not intended to be read by the end user, but of course if you are curious as to how an animation is made go ahead and take a look. You can find these notebooks in the folder named &lt;em&gt;Supporting_Notebooks&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This is admittedly a somewhat cumbersome interface to a book; I am following in the footsteps of several other projects that are somewhat repurposing Jupyter Notebook to generate entire books. I feel the slight annoyances have a huge payoff - instead of having to download a separate code base and run it in an IDE while you try to read a book, all of the code and text is in one place. If you want to alter the code, you may do so and immediately see the effects of your change. If you find a bug, you can make a fix, and push it back to my repository so that everyone in the world benefits. And, of course, you will never encounter a problem I face all the time with traditional books - the book and the code are out of sync with each other, and you are left scratching your head as to which source to trust.&lt;/p&gt; &#xA;&lt;h2&gt;Companion Software&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://pypi.python.org/pypi/filterpy&#34;&gt;&lt;img src=&#34;http://img.shields.io/pypi/v/filterpy.svg?sanitize=true&#34; alt=&#34;Latest Version&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;I wrote an open source Bayesian filtering Python library called &lt;strong&gt;FilterPy&lt;/strong&gt;. I have made the project available on PyPi, the Python Package Index. To install from PyPi, at the command line issue the command&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install filterpy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you do not have pip, you may follow the instructions here: &lt;a href=&#34;https://pip.pypa.io/en/latest/installing.html&#34;&gt;https://pip.pypa.io/en/latest/installing.html&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;All of the filters used in this book as well as others not in this book are implemented in my Python library FilterPy, available &lt;a href=&#34;https://github.com/rlabbe/filterpy&#34;&gt;here&lt;/a&gt;. You do not need to download or install this to read the book, but you will likely want to use this library to write your own filters. It includes Kalman filters, Fading Memory filters, H infinity filters, Extended and Unscented filters, least square filters, and many more. It also includes helper routines that simplify the designing the matrices used by some of the filters, and other code such as Kalman based smoothers.&lt;/p&gt; &#xA;&lt;p&gt;FilterPy is hosted on github at (&lt;a href=&#34;https://github.com/rlabbe/filterpy&#34;&gt;https://github.com/rlabbe/filterpy&lt;/a&gt;). If you want the bleeding edge release you will want to grab a copy from github, and follow your Python installation&#39;s instructions for adding it to the Python search path. This might expose you to some instability since you might not get a tested release, but as a benefit you will also get all of the test scripts used to test the library. You can examine these scripts to see many examples of writing and running filters while not in the Jupyter Notebook environment.&lt;/p&gt; &#xA;&lt;h2&gt;Alternative Way of Running the Book in Conda environment&lt;/h2&gt; &#xA;&lt;p&gt;If you have conda or miniconda installed, you can create an environment by&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env update -f environment.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and use&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda activate kf_bf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda deactivate kf_bf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to activate and deactivate the environment.&lt;/p&gt; &#xA;&lt;h2&gt;Issues or Questions&lt;/h2&gt; &#xA;&lt;p&gt;If you have comments, you can write an issue at GitHub so that everyone can read it along with my response. Please don&#39;t view it as a way to report bugs only. Alternatively I&#39;ve created a gitter room for more informal discussion. &lt;a href=&#34;https://gitter.im/rlabbe/Kalman-and-Bayesian-Filters-in-Python?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/Join%20Chat.svg?sanitize=true&#34; alt=&#34;Join the chat at https://gitter.im/rlabbe/Kalman-and-Bayesian-Filters-in-Python&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a rel=&#34;license&#34; href=&#34;http://creativecommons.org/licenses/by/4.0/&#34;&gt;&lt;img alt=&#34;Creative Commons License&#34; style=&#34;border-width:0&#34; src=&#34;https://i.creativecommons.org/l/by/4.0/88x31.png&#34;&gt;&lt;/a&gt;&lt;br&gt;&lt;span xmlns:dct=&#34;http://purl.org/dc/terms/&#34; property=&#34;dct:title&#34;&gt;Kalman and Bayesian Filters in Python&lt;/span&gt; by &lt;a xmlns:cc=&#34;http://creativecommons.org/ns#&#34; href=&#34;https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python&#34; property=&#34;cc:attributionName&#34; rel=&#34;cc:attributionURL&#34;&gt;Roger R. Labbe&lt;/a&gt; is licensed under a &lt;a rel=&#34;license&#34; href=&#34;http://creativecommons.org/licenses/by/4.0/&#34;&gt;Creative Commons Attribution 4.0 International License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;All software in this book, software that supports this book (such as in the the code directory) or used in the generation of the book (in the pdf directory) that is contained in this repository is licensed under the following MIT license:&lt;/p&gt; &#xA;&lt;p&gt;The MIT License (MIT)&lt;/p&gt; &#xA;&lt;p&gt;Copyright (c) 2015 Roger R. Labbe Jr&lt;/p&gt; &#xA;&lt;p&gt;Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the &#34;Software&#34;), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:&lt;/p&gt; &#xA;&lt;p&gt;The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.&lt;/p&gt; &#xA;&lt;p&gt;THE SOFTWARE IS PROVIDED &#34;AS IS&#34;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.TION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;rlabbejr at gmail.com&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>jakevdp/PythonDataScienceHandbook</title>
    <updated>2022-06-01T02:12:48Z</updated>
    <id>tag:github.com,2022-06-01:/jakevdp/PythonDataScienceHandbook</id>
    <link href="https://github.com/jakevdp/PythonDataScienceHandbook" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Python Data Science Handbook: full text in Jupyter Notebooks&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Python Data Science Handbook&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://mybinder.org/v2/gh/jakevdp/PythonDataScienceHandbook/master?filepath=notebooks%2FIndex.ipynb&#34;&gt;&lt;img src=&#34;https://mybinder.org/badge.svg?sanitize=true&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/Index.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository contains the entire &lt;a href=&#34;http://shop.oreilly.com/product/0636920034919.do&#34;&gt;Python Data Science Handbook&lt;/a&gt;, in the form of (free!) Jupyter notebooks.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jakevdp/PythonDataScienceHandbook/master/notebooks/figures/PDSH-cover.png&#34; alt=&#34;cover image&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How to Use this Book&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Read the book in its entirety online at &lt;a href=&#34;https://jakevdp.github.io/PythonDataScienceHandbook/&#34;&gt;https://jakevdp.github.io/PythonDataScienceHandbook/&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the code using the Jupyter notebooks available in this repository&#39;s &lt;a href=&#34;https://raw.githubusercontent.com/jakevdp/PythonDataScienceHandbook/master/notebooks&#34;&gt;notebooks&lt;/a&gt; directory.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Launch executable versions of these notebooks using &lt;a href=&#34;http://colab.research.google.com&#34;&gt;Google Colab&lt;/a&gt;: &lt;a href=&#34;https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/Index.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Launch a live notebook server with these notebooks using &lt;a href=&#34;https://beta.mybinder.org/&#34;&gt;binder&lt;/a&gt;: &lt;a href=&#34;https://mybinder.org/v2/gh/jakevdp/PythonDataScienceHandbook/master?filepath=notebooks%2FIndex.ipynb&#34;&gt;&lt;img src=&#34;https://mybinder.org/badge.svg?sanitize=true&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Buy the printed book through &lt;a href=&#34;http://shop.oreilly.com/product/0636920034919.do&#34;&gt;O&#39;Reilly Media&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;About&lt;/h2&gt; &#xA;&lt;p&gt;The book was written and tested with Python 3.5, though other Python versions (including Python 2.7) should work in nearly all cases.&lt;/p&gt; &#xA;&lt;p&gt;The book introduces the core libraries essential for working with data in Python: particularly &lt;a href=&#34;http://ipython.org&#34;&gt;IPython&lt;/a&gt;, &lt;a href=&#34;http://numpy.org&#34;&gt;NumPy&lt;/a&gt;, &lt;a href=&#34;http://pandas.pydata.org&#34;&gt;Pandas&lt;/a&gt;, &lt;a href=&#34;http://matplotlib.org&#34;&gt;Matplotlib&lt;/a&gt;, &lt;a href=&#34;http://scikit-learn.org&#34;&gt;Scikit-Learn&lt;/a&gt;, and related packages. Familiarity with Python as a language is assumed; if you need a quick introduction to the language itself, see the free companion project, &lt;a href=&#34;https://github.com/jakevdp/WhirlwindTourOfPython&#34;&gt;A Whirlwind Tour of Python&lt;/a&gt;: it&#39;s a fast-paced introduction to the Python language aimed at researchers and scientists.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/Index.ipynb&#34;&gt;Index.ipynb&lt;/a&gt; for an index of the notebooks available to accompany the text.&lt;/p&gt; &#xA;&lt;h2&gt;Software&lt;/h2&gt; &#xA;&lt;p&gt;The code in the book was tested with Python 3.5, though most (but not all) will also work correctly with Python 2.7 and other older Python versions.&lt;/p&gt; &#xA;&lt;p&gt;The packages I used to run the code in the book are listed in &lt;a href=&#34;https://raw.githubusercontent.com/jakevdp/PythonDataScienceHandbook/master/requirements.txt&#34;&gt;requirements.txt&lt;/a&gt; (Note that some of these exact version numbers may not be available on your platform: you may have to tweak them for your own use). To install the requirements using &lt;a href=&#34;http://conda.pydata.org&#34;&gt;conda&lt;/a&gt;, run the following at the command-line:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ conda install --file requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To create a stand-alone environment named &lt;code&gt;PDSH&lt;/code&gt; with Python 3.5 and all the required package versions, run the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ conda create -n PDSH python=3.5 --file requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can read more about using conda environments in the &lt;a href=&#34;http://conda.pydata.org/docs/using/envs.html&#34;&gt;Managing Environments&lt;/a&gt; section of the conda documentation.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;h3&gt;Code&lt;/h3&gt; &#xA;&lt;p&gt;The code in this repository, including all code samples in the notebooks listed above, is released under the &lt;a href=&#34;https://raw.githubusercontent.com/jakevdp/PythonDataScienceHandbook/master/LICENSE-CODE&#34;&gt;MIT license&lt;/a&gt;. Read more at the &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;Open Source Initiative&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Text&lt;/h3&gt; &#xA;&lt;p&gt;The text content of the book is released under the &lt;a href=&#34;https://raw.githubusercontent.com/jakevdp/PythonDataScienceHandbook/master/LICENSE-TEXT&#34;&gt;CC-BY-NC-ND license&lt;/a&gt;. Read more at &lt;a href=&#34;https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode&#34;&gt;Creative Commons&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>alembics/disco-diffusion</title>
    <updated>2022-06-01T02:12:48Z</updated>
    <id>tag:github.com,2022-06-01:/alembics/disco-diffusion</id>
    <link href="https://github.com/alembics/disco-diffusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Disco Diffusion&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/alembics/disco-diffusion/blob/main/Disco_Diffusion.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open in Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A frankensteinian amalgamation of notebooks, models and techniques for the generation of AI Art and Animations.&lt;/p&gt; &#xA;&lt;p&gt;[to be updated with further info soon]&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This project uses a special conversion tool to convert the python files into notebooks for easier development.&lt;/p&gt; &#xA;&lt;p&gt;What this means is you do not have to touch the notebook directly to make changes to it&lt;/p&gt; &#xA;&lt;p&gt;the tool being used is called &lt;a href=&#34;https://github.com/MSFTserver/colab-convert&#34;&gt;Colab-Convert&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;install using &lt;code&gt;pip install colab-convert&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;convert .py to .ipynb &lt;code&gt;colab-convert /path/to/file.py /path/to/file.ipynb&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;convert .ipynb to .py &lt;code&gt;colab-convert /path/to/file.ipynb /path/to/file.py&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Changelog&lt;/h2&gt; &#xA;&lt;h4&gt;v1 Oct 29th 2021 - Somnai&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Initial QoL improvements added, including user friendly UI, settings+prompt saving and improved google drive folder organization.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v1.1 Nov 13th 2021 - Somnai&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Now includes sizing options, intermediate saves and fixed image prompts and perlin inits. unexposed batch option since it doesn&#39;t work&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v2 Update: Nov 22nd 2021 - Somnai&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Initial addition of Katherine Crowson&#39;s Secondary Model Method (&lt;a href=&#34;https://colab.research.google.com/drive/1mpkrhOjoyzPeSWy2r7T8EYRaU7amYOOi#scrollTo=X5gODNAMEUCR&#34;&gt;https://colab.research.google.com/drive/1mpkrhOjoyzPeSWy2r7T8EYRaU7amYOOi#scrollTo=X5gODNAMEUCR&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Fix for incorrectly named settings files&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v3 Update: Dec 24th 2021 - Somnai&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Implemented Dango&#39;s advanced cutout method&lt;/li&gt; &#xA; &lt;li&gt;Added SLIP models, thanks to NeuralDivergent&lt;/li&gt; &#xA; &lt;li&gt;Fixed issue with NaNs resulting in black images, with massive help and testing from @Softology&lt;/li&gt; &#xA; &lt;li&gt;Perlin now changes properly within batches (not sure where this perlin_regen code came from originally, but thank you)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v4 Update: Jan 2021 - Somnai&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Implemented Diffusion Zooming&lt;/li&gt; &#xA; &lt;li&gt;Added Chigozie keyframing&lt;/li&gt; &#xA; &lt;li&gt;Made a bunch of edits to processes&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v4.1 Update: Jan 14th 2021 - Somnai&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Added video input mode&lt;/li&gt; &#xA; &lt;li&gt;Added license that somehow went missing&lt;/li&gt; &#xA; &lt;li&gt;Added improved prompt keyframing, fixed image_prompts and multiple prompts&lt;/li&gt; &#xA; &lt;li&gt;Improved UI&lt;/li&gt; &#xA; &lt;li&gt;Significant under the hood cleanup and improvement&lt;/li&gt; &#xA; &lt;li&gt;Refined defaults for each mode&lt;/li&gt; &#xA; &lt;li&gt;Removed SLIP models for the time being due to import conflicts&lt;/li&gt; &#xA; &lt;li&gt;Added latent-diffusion SuperRes for sharpening&lt;/li&gt; &#xA; &lt;li&gt;Added resume run mode&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v5 Update: Feb 20th 2022 - gandamu / Adam Letts&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Added 3D animation mode. Uses weighted combination of AdaBins and MiDaS depth estimation models. Uses pytorch3d for 3D transforms on Colab and/or Linux.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v5.1 Update: Mar 30th 2022 - zippy / Chris Allen and gandamu / Adam Letts&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Integrated Turbo+Smooth features from Disco Diffusion Turbo -- just the implementation, without its defaults.&lt;/li&gt; &#xA; &lt;li&gt;Implemented resume of turbo animations in such a way that it&#39;s now possible to resume from different batch folders and batch numbers.&lt;/li&gt; &#xA; &lt;li&gt;3D rotation parameter units are now degrees (rather than radians)&lt;/li&gt; &#xA; &lt;li&gt;Corrected name collision in sampling_mode (now diffusion_sampling_mode for plms/ddim, and sampling_mode for 3D transform sampling)&lt;/li&gt; &#xA; &lt;li&gt;Added video_init_seed_continuity option to make init video animations more continuous&lt;/li&gt; &#xA; &lt;li&gt;Removed pytorch3d from needing to be compiled with a lite version specifically made for Disco Diffusion&lt;/li&gt; &#xA; &lt;li&gt;Remove Super Resolution&lt;/li&gt; &#xA; &lt;li&gt;Remove Slip Models&lt;/li&gt; &#xA; &lt;li&gt;Update for crossplatform support&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v5.1 Update: Apr 4th 2022 - MSFTserver aka HostsServer&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Removed pytorch3d from needing to be compiled with a lite version specifically made for Disco Diffusion&lt;/li&gt; &#xA; &lt;li&gt;Remove Super Resolution&lt;/li&gt; &#xA; &lt;li&gt;Remove Slip Models&lt;/li&gt; &#xA; &lt;li&gt;Update for crossplatform support&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v5.2 Update: Apr 10th 2022 - nin_artificial / Tom Mason&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;VR Mode&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Notebook Provenance&lt;/h2&gt; &#xA;&lt;p&gt;Original notebook by Katherine Crowson (&lt;a href=&#34;https://github.com/crowsonkb&#34;&gt;https://github.com/crowsonkb&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/RiversHaveWings&#34;&gt;https://twitter.com/RiversHaveWings&lt;/a&gt;). It uses either OpenAI&#39;s 256x256 unconditional ImageNet or Katherine Crowson&#39;s fine-tuned 512x512 diffusion model (&lt;a href=&#34;https://github.com/openai/guided-diffusion&#34;&gt;https://github.com/openai/guided-diffusion&lt;/a&gt;), together with CLIP (&lt;a href=&#34;https://github.com/openai/CLIP&#34;&gt;https://github.com/openai/CLIP&lt;/a&gt;) to connect text prompts with images.&lt;/p&gt; &#xA;&lt;p&gt;Modified by Daniel Russell (&lt;a href=&#34;https://github.com/russelldc&#34;&gt;https://github.com/russelldc&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/danielrussruss&#34;&gt;https://twitter.com/danielrussruss&lt;/a&gt;) to include (hopefully) optimal params for quick generations in 15-100 timesteps rather than 1000, as well as more robust augmentations.&lt;/p&gt; &#xA;&lt;p&gt;Further improvements from Dango233 and nsheppard helped improve the quality of diffusion in general, and especially so for shorter runs like this notebook aims to achieve.&lt;/p&gt; &#xA;&lt;p&gt;Vark added code to load in multiple Clip models at once, which all prompts are evaluated against, which may greatly improve accuracy.&lt;/p&gt; &#xA;&lt;p&gt;The latest zoom, pan, rotation, and keyframes features were taken from Chigozie Nri&#39;s VQGAN Zoom Notebook (&lt;a href=&#34;https://github.com/chigozienri&#34;&gt;https://github.com/chigozienri&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/chigozienri&#34;&gt;https://twitter.com/chigozienri&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;Advanced DangoCutn Cutout method is also from Dango223.&lt;/p&gt; &#xA;&lt;p&gt;--&lt;/p&gt; &#xA;&lt;p&gt;Somnai (&lt;a href=&#34;https://twitter.com/Somnai_dreams&#34;&gt;https://twitter.com/Somnai_dreams&lt;/a&gt;) added 2D Diffusion animation techniques, QoL improvements and various implementations of tech and techniques, mostly listed in the changelog below.&lt;/p&gt; &#xA;&lt;p&gt;3D animation implementation added by Adam Letts (&lt;a href=&#34;https://twitter.com/gandamu_ml&#34;&gt;https://twitter.com/gandamu_ml&lt;/a&gt;) in collaboration with Somnai.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mrdbourke/zero-to-mastery-ml</title>
    <updated>2022-06-01T02:12:48Z</updated>
    <id>tag:github.com,2022-06-01:/mrdbourke/zero-to-mastery-ml</id>
    <link href="https://github.com/mrdbourke/zero-to-mastery-ml" rel="alternate"></link>
    <summary type="html">&lt;p&gt;All course materials for the Zero to Mastery Machine Learning and Data Science course.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Zero to Mastery Machine Learning&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://mybinder.org/v2/gh/mrdbourke/zero-to-mastery-ml/master&#34;&gt;&lt;img src=&#34;https://mybinder.org/badge_logo.svg?sanitize=true&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.deepnote.com/launch?template=data-science&amp;amp;url=https://github.com/mrdbourke/zero-to-mastery-ml/raw/master/section-2-data-science-and-ml-tools/introduction-to-pandas.ipynb&#34;&gt;&lt;img src=&#34;https://deepnote.com/buttons/launch-in-deepnote.svg?sanitize=true&#34; alt=&#34;Deepnote&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/mrdbourke/zero-to-mastery-ml/blob/master&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Welcome! This repository contains all of the code, notebooks, images and other materials related to the &lt;a href=&#34;https://dbourke.link/mlcourse&#34;&gt;Zero to Mastery Machine Learning Course on Udemy&lt;/a&gt; and &lt;a href=&#34;https://dbourke.link/ZTMmlcourse&#34;&gt;zerotomastery.io&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;d like to see anything in particular, please send me an email: &lt;a href=&#34;mailto:daniel@mrdbourke.com&#34;&gt;daniel@mrdbourke.com&lt;/a&gt; or leave an issue.&lt;/p&gt; &#xA;&lt;h2&gt;What this course focuses on&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create a framework for working through problems (&lt;a href=&#34;https://github.com/mrdbourke/zero-to-mastery-ml/raw/master/section-1-getting-ready-for-machine-learning/a-6-step-framework-for-approaching-machine-learning-projects.md&#34;&gt;6 step machine learning modelling framework&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Find tools to fit the framework&lt;/li&gt; &#xA; &lt;li&gt;Targeted practice = use tools and framework steps to work on end-to-end machine learning modelling projects&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;How this course is structured&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Section 1 - Getting your mind and computer ready for machine learning (concepts, computer setup)&lt;/li&gt; &#xA; &lt;li&gt;Section 2 - Tools for machine learning and data science (pandas, NumPy, Matplotlib, Scikit-Learn)&lt;/li&gt; &#xA; &lt;li&gt;Section 3 - End-to-end structured data projects (classification and regression)&lt;/li&gt; &#xA; &lt;li&gt;Section 4 - Neural networks, deep learning and transfer learning with TensorFlow 2.0&lt;/li&gt; &#xA; &lt;li&gt;Section 5 - Communicating and sharing your work&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Student notes&lt;/h2&gt; &#xA;&lt;p&gt;Some students have taken and shared extensive notes on this course, see them below.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;d like to submit yours, leave a pull request.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Chester&#39;s notes - &lt;a href=&#34;https://github.com/chesterheng/machinelearning-datascience&#34;&gt;https://github.com/chesterheng/machinelearning-datascience&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Sophia&#39;s notes - &lt;a href=&#34;https://www.rockyourcode.com/tags/udemy-complete-machine-learning-and-data-science-zero-to-mastery/&#34;&gt;https://www.rockyourcode.com/tags/udemy-complete-machine-learning-and-data-science-zero-to-mastery/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>spmallick/learnopencv</title>
    <updated>2022-06-01T02:12:48Z</updated>
    <id>tag:github.com,2022-06-01:/spmallick/learnopencv</id>
    <link href="https://github.com/spmallick/learnopencv" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Learn OpenCV : C++ and Python Examples&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LearnOpenCV&lt;/h1&gt; &#xA;&lt;p&gt;This repo contains code for Computer Vision, Deep learning, and AI articles shared on our blog &lt;a href=&#34;https://www.LearnOpenCV.com&#34;&gt;LearnOpenCV.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Want to become an expert in AI? &lt;a href=&#34;https://opencv.org/courses/&#34;&gt;AI Courses by OpenCV&lt;/a&gt; is a great place to start.&lt;/p&gt; &#xA;&lt;a href=&#34;https://opencv.org/courses/&#34;&gt; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://www.learnopencv.com/wp-content/uploads/2020/04/AI-Courses-By-OpenCV-Github.png&#34;&gt; &lt;/p&gt; &lt;/a&gt; &#xA;&lt;h2&gt;List of Blog Posts&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Blog Post&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/deep-dive-into-tensorflow-model-optimization-toolkit/&#34;&gt;A Deep Dive into Tensorflow Model Optimization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/A-Deep-Dive-into-Tensorflow-Model-Optimization&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/anti-spoofing-face-recognition-system-using-oak-d-and-depthai/&#34;&gt;Anti-Spoofing Face Recognition System using OAK-D and DepthAI&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Anti-Spoofing-Face-Recognition-with-OAK-D&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/depthai-pipeline-overview-creating-a-complex-pipeline/&#34;&gt;DepthAI Pipeline Overview: Creating a Complex Pipeline&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/OAK-DepthAi-Pipeline-Overview&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/tensorflow-lite-model-maker-create-models-for-on-device-machine-learning/&#34;&gt;TensorFlow Lite Model Maker: Create Models for On-Device Machine Learning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Tensorflow-Lite-Model-Maker-Create-Models-for-On-Device-ML&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/tensorflow-lite-model-optimization-for-on-device-machine-learning&#34;&gt;TensorFlow Lite: Model Optimization for On Device Machine Learning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/TensorFlow-Lite-Model-Optimization-for-On-Device-MachineLearning&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/object-detection-with-depth-measurement-with-oak-d/&#34;&gt;Object detection with depth measurement using pre-trained models with OAK-D&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/OAK-Object-Detection-with-Depth&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/custom-object-detection-training-using-yolov5/&#34;&gt;Custom Object Detection Training using YOLOv5&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Custom-Object-Detection-Training-using-YOLOv5&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/object-detection-using-yolov5-and-opencv-dnn-in-c-and-python/&#34;&gt;Object Detection using Yolov5 and OpenCV DNN (C++/Python)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Object-Detection-using-YOLOv5-and-OpenCV-DNN-in-CPP-and-Python&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/create-snapchat-instagram-filters-using-mediapipe/&#34;&gt;Create Snapchat/Instagram filters using Mediapipe&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Create-AR-filters-using-Mediapipe&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/autosar-c-compliant-deep-learning-inference-with-tensorrt/&#34;&gt;AUTOSAR C++ compliant deep learning inference with TensorRT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/industrial_cv_TensorRT_cpp&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/nvidia-gtc-2022-day-4-highlights-meet-the-new-jetson-orin/&#34;&gt;NVIDIA GTC 2022 Day 4 Highlights: Meet the new Jetson Orin&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/nvidia-gtc-2022-day-3-highlights-deep-dive-into-hopper-architecture/&#34;&gt;NVIDIA GTC 2022 Day 3 Highlights: Deep Dive into Hopper architecture&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/nvidia-gtc-2022-day-2-highlights/&#34;&gt;NVIDIA GTC 2022 Day 2 Highlights: Jensen’s Keynote&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/gtc-day-1-highlights/&#34;&gt;NVIDIA GTC 2022 Day 1 Highlights: Brilliant Start&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/automatic-license-plate-recognition-using-deep-learning/&#34;&gt;Automatic License Plate Recognition using Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/ALPR&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/building-a-body-posture-analysis-system-using-mediapipe/&#34;&gt;Building a Body Posture Analysis System using MediaPipe&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Posture-analysis-system-using-MediaPipe-Pose&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/introduction-to-mediapipe/&#34;&gt;Introduction to MediaPipe&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Introduction-to-MediaPipe&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/disparity-estimation-using-deep-learning/&#34;&gt;Disparity Estimation using Deep Learning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Disparity-Estimation-Using-Deep-Learning&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/how-to-build-chrome-dino-game-bot-using-opencv-feature-matching/&#34;&gt;How to build Chrome Dino game bot using OpenCV Feature Matching&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Chrome-Dino-Bot-using-OpenCV-feature-matching&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/top-10-sources-to-find-computer-vision-and-ai-models/&#34;&gt;Top 10 Sources to Find Computer Vision and AI Models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/multi-attribute-and-graph-based-object-detection/&#34;&gt;Multi-Attribute and Graph-based Object Detection&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/plastic-waste-detection-with-deep-learning/&#34;&gt;Plastic Waste Detection with Deep Learning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Plastic-Waste-Detection-with-Deep-Learning&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/ensemble-deep-learning-based-defect-classification-and-detection-in-sem-images/&#34;&gt;Ensemble Deep Learning-based Defect Classification and Detection in SEM Images&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/building-industrial-embedded-deep-learning-inference-pipelines-with-tensorrt/&#34;&gt;Building Industrial embedded deep learning inference pipelines with TensorRT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/industrial_cv_TensorRT_python&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/transfer-learning-for-medical-images/&#34;&gt;Transfer Learning for Medical Images&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/stereo-vision-and-depth-estimation-using-opencv-ai-kit/&#34;&gt;Stereo Vision and Depth Estimation using OpenCV AI Kit&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/oak-getting-started&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/introduction-to-opencv-ai-kit-and-depthai/&#34;&gt;Introduction to OpenCV AI Kit and DepthAI&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/oak-getting-started&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/wechat-qr-code-scanner-in-opencv&#34;&gt;WeChat QR Code Scanner in OpenCV&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/WeChat-QRCode-Scanner-OpenCV&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/ai-behind-the-diwali-2021-not-just-a-cadbury-ad/&#34;&gt;AI behind the Diwali 2021 ‘Not just a Cadbury ad’&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/model-selection-and-benchmarking-with-modelplace-ai/&#34;&gt;Model Selection and Benchmarking with Modelplace.AI&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://modelplace.ai/&#34;&gt;Model Zoo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/real-time-style-transfer-in-a-zoom-meeting/&#34;&gt;Real-time style transfer in a zoom meeting&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/style-transfer-zoom&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/introduction-to-openvino-deep-learning-workbench/&#34;&gt;Introduction to OpenVino Deep Learning Workbench&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Introduction-to-OpenVino-Deep-Learning-Workbench&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/running-openvino-models-on-intel-integrated-gpu/&#34;&gt;Running OpenVino Models on Intel Integrated GPU&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Running-OpenVino-Models-on-Intel-Integrated-GPU&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/post-training-quantization-with-openvino-toolkit/&#34;&gt;Post Training Quantization with OpenVino Toolkit&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Post-Training-Quantization-with-OpenVino-Toolkit&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/introduction-to-intel-openvino-toolkit/&#34;&gt;Introduction to Intel OpenVINO Toolkit&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/human-action-recognition-using-detectron2-and-lstm/&#34;&gt;Human Action Recognition using Detectron2 and LSTM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Human-Action-Recognition-Using-Detectron2-And-Lstm&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/paired-image-to-image-translation-pix2pix/&#34;&gt;Pix2Pix:Image-to-Image Translation in PyTorch &amp;amp; TensorFlow&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Image-to-Image-Translation-with-GAN&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/conditional-gan-cgan-in-pytorch-and-tensorflow/&#34;&gt;Conditional GAN (cGAN) in PyTorch and TensorFlow&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Conditional-GAN-PyTorch-TensorFlow&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/deep-convolutional-gan-in-pytorch-and-tensorflow/&#34;&gt;Deep Convolutional GAN in PyTorch and TensorFlow&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Deep-Convolutional-GAN&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/introduction-to-generative-adversarial-networks/&#34;&gt;Introduction to Generative Adversarial Networks (GANs)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Intro-to-Generative-Adversarial-Network&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/human-pose-estimation-using-keypoint-rcnn-in-pytorch/&#34;&gt;Human Pose Estimation using Keypoint RCNN in PyTorch&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/PyTorch-Keypoint-RCNN&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/non-maximum-suppression-theory-and-implementation-in-pytorch&#34;&gt;Non Maximum Suppression: Theory and Implementation in PyTorch&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Non-Maximum-Suppression&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/mrnet-multitask-approach/&#34;&gt;MRNet – The Multi-Task Approach&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/MRnet-MultiTask-Approach&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/generative-and-discriminative-models/&#34;&gt;Generative and Discriminative Models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/playing-chromes-t-rex-game-with-facial-gestures/&#34;&gt;Playing Chrome&#39;s T-Rex Game with Facial Gestures&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Playing-Chrome-TRex-Game-with-Facial-Gestures&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/variational-autoencoder-in-tensorflow/&#34;&gt;Variational Autoencoder in TensorFlow&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Variational-Autoencoder-TensorFlow&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/autoencoder-in-tensorflow-2-beginners-guide/&#34;&gt;Autoencoder in TensorFlow 2: Beginner’s Guide&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Autoencoder-in-TensorFlow&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/deep-learning-with-opencvs-dnn-module-a-definitive-guide/&#34;&gt;Deep Learning with OpenCV DNN Module: A Definitive Guide&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Deep-Learning-with-OpenCV-DNN-Module&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/depth-perception-using-stereo-camera-python-c/&#34;&gt;Depth perception using stereo camera (Python/C++)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Depth-Perception-Using-Stereo-Camera&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/contour-detection-using-opencv-python-c/&#34;&gt;Contour Detection using OpenCV (Python/C++)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Contour-Detection-using-OpenCV&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/super-resolution-in-opencv/&#34;&gt;Super Resolution in OpenCV&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/raw/master/Super-Resolution-in-OpenCV&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/improving-illumination-in-night-time-images/&#34;&gt;Improving Illumination in Night Time Images&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Improving-Illumination-in-Night-Time-Images&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/introduction-to-video-classification-and-human-activity-recognition/&#34;&gt;Video Classification and Human Activity Recognition&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/video-classification-and-human-activity-recognition&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/how-to-use-opencv-dnn-module-with-nvidia-gpu-on-windows&#34;&gt;How to use OpenCV DNN Module with Nvidia GPU on Windows&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/OpenCV-dnn-gpu-support-Windows&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/opencv-dnn-with-gpu-support/&#34;&gt;How to use OpenCV DNN Module with NVIDIA GPUs&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/OpenCV-dnn-gpu-support-Linux&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/code-opencv-in-visual-studio/&#34;&gt;Code OpenCV in Visual Studio&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/install-opencv-on-windows/&#34;&gt;Install OpenCV on Windows – C++ / Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Install-OpenCV-Windows-exe&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/face-recognition-with-arcface/&#34;&gt;Face Recognition with ArcFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Face-Recognition-with-ArcFace&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/background-subtraction-with-opencv-and-bgs-libraries/&#34;&gt;Background Subtraction with OpenCV and BGS Libraries&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Background-Subtraction&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/optical-flow-using-deep-learning-raft/&#34;&gt;RAFT: Optical Flow estimation using Deep Learning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Optical-Flow-Estimation-using-Deep-Learning-RAFT&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/making-a-low-cost-stereo-camera-using-opencv/&#34;&gt;Making A Low-Cost Stereo Camera Using OpenCV&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/stereo-camera&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/optical-flow-in-opencv&#34;&gt;Optical Flow in OpenCV (C++/Python)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Optical-Flow-in-OpenCV&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/introduction-to-epipolar-geometry-and-stereo-vision/&#34;&gt;Introduction to Epipolar Geometry and Stereo Vision&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/EpipolarGeometryAndStereoVision&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/classification-with-localization/&#34;&gt;Classification With Localization: Convert any keras Classifier to a Detector&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Classification-with-localization-convert-any-keras-classifier-into-a-detector/README.md&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/photoshop-filters-in-opencv/&#34;&gt;Photoshop Filters in OpenCV&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Photoshop-Filters-in-OpenCV&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/tetris-with-opencv-python&#34;&gt;Tetris Game using OpenCV Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Tetris&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/image-classification-with-opencv-for-android/&#34;&gt;Image Classification with OpenCV for Android&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/DNN-OpenCV-Classification-Android&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/image-classification-with-opencv-java&#34;&gt;Image Classification with OpenCV Java&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/DNN-OpenCV-Classification-with-Java&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/pytorch-to-tensorflow-model-conversion/&#34;&gt;PyTorch to Tensorflow Model Conversion&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/PyTorch-to-TensorFlow-Model-Conversion&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/snake-game-with-opencv-python/&#34;&gt;Snake Game with OpenCV Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/SnakeGame&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/stanford-mrnet-challenge-classifying-knee-mris/&#34;&gt;Stanford MRNet Challenge: Classifying Knee MRIs&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/MRNet-Single-Model&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/experiment-logging-with-tensorboard-and-wandb&#34;&gt;Experiment Logging with TensorBoard and wandb&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/PyTorch-Vision-Experiment-Logging&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/understanding-lens-distortion/&#34;&gt;Understanding Lens Distortion&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/UnderstandingLensDistortion&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/image-matting-with-state-of-the-art-method-f-b-alpha-matting/&#34;&gt;Image Matting with state-of-the-art Method “F, B, Alpha Matting”&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/FBAMatting&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/bag-of-tricks-for-image-classification-lets-check-if-it-is-working-or-not/&#34;&gt;Bag Of Tricks For Image Classification - Let&#39;s check if it is working or not&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Bag-Of-Tricks-For-Image-Classification&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/getting-started-opencv-cuda-module/&#34;&gt;Getting Started with OpenCV CUDA Module&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Getting-Started-OpenCV-CUDA-Module&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/training-a-custom-object-detector-with-dlib-making-gesture-controlled-applications/&#34;&gt;Training a Custom Object Detector with DLIB &amp;amp; Making Gesture Controlled Applications&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Training_a_custom_hand_detector_with_dlib&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/how-to-run-inference-using-tensorrt-c-api/&#34;&gt;How To Run Inference Using TensorRT C++ API&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/PyTorch-ONNX-TensorRT-CPP&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/using-facial-landmarks-for-overlaying-faces-with-masks/&#34;&gt;Using Facial Landmarks for Overlaying Faces with Medical Masks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/FaceMaskOverlay&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/tensorboard-with-pytorch-lightning&#34;&gt;Tensorboard with PyTorch Lightning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/TensorBoard-With-Pytorch-Lightning&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/otsu-thresholding-with-opencv/&#34;&gt;Otsu&#39;s Thresholding with OpenCV&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/otsu-method&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/pytorch-to-coreml-model-conversion/&#34;&gt;PyTorch-to-CoreML-model-conversion&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/PyTorch-to-CoreML-model-conversion&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/playing-rock-paper-scissors-with-ai/&#34;&gt;Playing Rock, Paper, Scissors with AI&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Playing-rock-paper-scissors-with-AI&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/cnn-receptive-field-computation-using-backprop-with-tensorflow/&#34;&gt;CNN Receptive Field Computation Using Backprop with TensorFlow&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/TensorFlow-Receptive-Field-With-Backprop&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/cnn-fully-convolutional-image-classification-with-tensorflow&#34;&gt;CNN Fully Convolutional Image Classification with TensorFlow&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/TensorFlow-Fully-Convolutional-Image-Classification&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/how-to-convert-a-model-from-pytorch-to-tensorrt-and-speed-up-inference/&#34;&gt;How to convert a model from PyTorch to TensorRT and speed up inference&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/PyTorch-ONNX-TensorRT&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/efficient-image-loading/&#34;&gt;Efficient image loading&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Efficient-image-loading&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/graph-convolutional-networks-model-relations-in-data/&#34;&gt;Graph Convolutional Networks: Model Relations In Data&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Graph-Convolutional-Networks-Model-Relations-In-Data&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/federated-learning-using-pytorch-and-pysyft/&#34;&gt;Getting Started with Federated Learning with PyTorch and PySyft&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Federated-Learning-Intro&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/creating-a-virtual-pen-and-eraser-with-opencv/&#34;&gt;Creating a Virtual Pen &amp;amp; Eraser&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Creating-a-Virtual-Pen-and-Eraser&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/getting-started-with-pytorch-lightning/&#34;&gt;Getting Started with PyTorch Lightning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Pytorch-Lightning&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/multi-label-image-classification-with-pytorch-image-tagging/&#34;&gt;Multi-Label Image Classification with PyTorch: Image Tagging&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/PyTorch-Multi-Label-Image-Classification-Image-Tagging&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/Funny-Mirrors-Using-OpenCV/&#34;&gt;Funny Mirrors Using OpenCV&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/FunnyMirrors&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/t-sne-for-resnet-feature-visualization/&#34;&gt;t-SNE for ResNet feature visualization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/TSNE&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/multi-label-image-classification-with-pytorch/&#34;&gt;Multi-Label Image Classification with Pytorch&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/PyTorch-Multi-Label-Image-Classification&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/cnn-receptive-field-computation-using-backprop/&#34;&gt;CNN Receptive Field Computation Using Backprop&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/PyTorch-Receptive-Field-With-Backprop&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/cnn-receptive-field-computation-using-backprop-with-tensorflow/&#34;&gt;CNN Receptive Field Computation Using Backprop with TensorFlow&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/TensorFlow-Receptive-Field-With-Backprop&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/augmented-reality-using-aruco-markers-in-opencv-(c++-python)/&#34;&gt;Augmented Reality using AruCo Markers in OpenCV(C++ and Python)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/AugmentedRealityWithArucoMarkers&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/fully-convolutional-image-classification-on-arbitrary-sized-image/&#34;&gt;Fully Convolutional Image Classification on Arbitrary Sized Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/PyTorch-Fully-Convolutional-Image-Classification&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/camera-calibration-using-opencv/&#34;&gt;Camera Calibration using OpenCV&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/CameraCalibration&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/geometry-of-image-formation/&#34;&gt;Geometry of Image Formation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/ensuring-training-reproducibility-in-pytorch&#34;&gt;Ensuring Training Reproducibility in Pytorch&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/gaze-tracking/&#34;&gt;Gaze Tracking&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/simple-background-estimation-in-videos-using-opencv-c-python/&#34;&gt;Simple Background Estimation in Videos Using OpenCV&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/VideoBackgroundEstimation&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/applications-of-foreground-background-separation-with-semantic-segmentation/&#34;&gt;Applications of Foreground-Background separation with Semantic Segmentation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/app-seperation-semseg&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/efficientnet-theory-code&#34;&gt;EfficientNet: Theory + Code&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/EfficientNet&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/mask-r-cnn-instance-segmentation-with-pytorch/&#34;&gt;PyTorch for Beginners: Mask R-CNN Instance Segmentation with PyTorch&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spmallick/learnopencv/master/PyTorch-Mask-RCNN&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/faster-r-cnn-object-detection-with-pytorch&#34;&gt;PyTorch for Beginners: Faster R-CNN Object Detection with PyTorch&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/PyTorch-faster-RCNN&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/pytorch-for-beginners-semantic-segmentation-using-torchvision/&#34;&gt;PyTorch for Beginners: Semantic Segmentation using torchvision&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/PyTorch-Segmentation-torchvision&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/image-classification-using-pre-trained-models-using-pytorch/&#34;&gt;PyTorch for Beginners: Comparison of pre-trained models for Image Classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Image-classification-pre-trained-models/Image_Classification_using_pre_trained_models.ipynb&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/pytorch-for-beginners-basics/&#34;&gt;PyTorch for Beginners: Basics&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/PyTorch-for-Beginners/PyTorch_for_Beginners.ipynb&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/pytorch-model-inference-using-onnx-and-caffe2/&#34;&gt;PyTorch Model Inference using ONNX and Caffe2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Inference-for-PyTorch-Models/ONNX-Caffe2&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/image-classification-using-transfer-learning-in-pytorch/&#34;&gt;Image Classification Using Transfer Learning in PyTorch&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Image-Classification-in-PyTorch&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/hangman-creating-games-in-opencv/&#34;&gt;Hangman: Creating games in OpenCV&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Hangman&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/image-inpainting-with-opencv-c-python/&#34;&gt;Image Inpainting with OpenCV (C++/Python)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Image-Inpainting&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/hough-transform-with-opencv-c-python/&#34;&gt;Hough Transform with OpenCV (C++/Python)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Hough-Transform&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/xeus-cling-run-c-code-in-jupyter-notebook/&#34;&gt;Xeus-Cling: Run C++ code in Jupyter Notebook&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/XeusCling&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/age-gender-classification-using-opencv-deep-learning-c-python/&#34;&gt;Gender &amp;amp; Age Classification using OpenCV Deep Learning ( C++/Python )&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/AgeGender&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/invisibility-cloak-using-color-detection-and-segmentation-with-opencv/&#34;&gt;Invisibility Cloak using Color Detection and Segmentation with OpenCV&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/InvisibilityCloak&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/fast-image-downloader-for-open-images-v4/&#34;&gt;Fast Image Downloader for Open Images V4 (Python)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/downloadOpenImages&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/deep-learning-based-text-detection-using-opencv-c-python/&#34;&gt;Deep Learning based Text Detection Using OpenCV (C++/Python)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/TextDetectionEAST&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/video-stabilization-using-point-feature-matching-in-opencv/&#34;&gt;Video Stabilization Using Point Feature Matching in OpenCV&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/VideoStabilization&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/training-yolov3-deep-learning-based-custom-object-detector/&#34;&gt;Training YOLOv3 : Deep Learning based Custom Object Detector&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/YOLOv3-Training-Snowman-Detector&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/using-openvino-with-opencv/&#34;&gt;Using OpenVINO with OpenCV&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/OpenVINO-OpenCV&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/duplicate-search-on-quora-dataset/&#34;&gt;Duplicate Search on Quora Dataset&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Quora-Dataset-Duplicate-Search&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/shape-matching-using-hu-moments-c-python/&#34;&gt;Shape Matching using Hu Moments (C++/Python)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/HuMoments&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/install-opencv-4-on-centos-7/&#34;&gt;Install OpenCV 4 on CentOS (C++ and Python)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/raw/master/InstallScripts/installOpenCV-3-on-centos.sh&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/install-opencv-3-4-4-on-centos-7/&#34;&gt;Install OpenCV 3.4.4 on CentOS (C++ and Python)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/raw/master/InstallScripts/installOpenCV-3-on-centos.sh&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/install-opencv-3-4-4-on-red-hat/&#34;&gt;Install OpenCV 3.4.4 on Red Hat (C++ and Python)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/raw/master/InstallScripts/installOpenCV-3-on-red-hat.sh&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/install-opencv-4-on-red-hat/&#34;&gt;Install OpenCV 4 on Red Hat (C++ and Python)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/raw/master/InstallScripts/installOpenCV-4-on-red-hat.sh&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/install-opencv-4-on-macos/&#34;&gt;Install OpenCV 4 on macOS (C++ and Python)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/InstallScripts/installOpenCV-4-macos.sh&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/install-opencv-3-4-4-on-raspberry-pi/&#34;&gt;Install OpenCV 3.4.4 on Raspberry Pi&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/raw/master/InstallScripts/installOpenCV-3-raspberry-pi.sh&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/install-opencv-3-4-4-on-macos/&#34;&gt;Install OpenCV 3.4.4 on macOS (C++ and Python)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/raw/master/InstallScripts/installOpenCV-3-macos.sh&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/opencv-qr-code-scanner-c-and-python/&#34;&gt;OpenCV QR Code Scanner (C++ and Python)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/QRCode-OpenCV&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/install-opencv-3-4-4-on-windows/&#34;&gt;Install OpenCV 3.4.4 on Windows (C++ and Python)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/InstallScripts/Windows-3&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/install-opencv-3-4-4-on-ubuntu-16-04/&#34;&gt;Install OpenCV 3.4.4 on Ubuntu 16.04 (C++ and Python)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/raw/master/InstallScripts/installOpenCV-3-on-Ubuntu-16-04.sh&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/install-opencv-3-4-4-on-ubuntu-18-04/&#34;&gt;Install OpenCV 3.4.4 on Ubuntu 18.04 (C++ and Python)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/raw/master/InstallScripts/installOpenCV-3-on-Ubuntu-18-04.sh&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/universal-sentence-encoder&#34;&gt;Universal Sentence Encoder&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/raw/master/Universal-Sentence-Encoder&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/install-opencv-4-on-raspberry-pi/&#34;&gt;Install OpenCV 4 on Raspberry Pi&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/raw/master/InstallScripts/installOpenCV-4-raspberry-pi.sh&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/install-opencv-4-on-windows/&#34;&gt;Install OpenCV 4 on Windows (C++ and Python)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/InstallScripts/Windows-4&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/hand-keypoint-detection-using-deep-learning-and-opencv/&#34;&gt;Hand Keypoint Detection using Deep Learning and OpenCV&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/HandPose&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/deep-learning-based-object-detection-and-instance-segmentation-using-mask-r-cnn-in-opencv-python-c/&#34;&gt;Deep learning based Object Detection and Instance Segmentation using Mask R-CNN in OpenCV (Python / C++)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Mask-RCNN&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/install-opencv-4-on-ubuntu-18-04/&#34;&gt;Install OpenCV 4 on Ubuntu 18.04 (C++ and Python)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/raw/master/InstallScripts/installOpenCV-4-on-Ubuntu-18-04.sh&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/install-opencv-4-on-ubuntu-16-04/&#34;&gt;Install OpenCV 4 on Ubuntu 16.04 (C++ and Python)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/raw/master/InstallScripts/installOpenCV-4-on-Ubuntu-16-04.sh&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/multi-person-pose-estimation-in-opencv-using-openpose/&#34;&gt;Multi-Person Pose Estimation in OpenCV using OpenPose&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/OpenPose-Multi-Person&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/heatmap-for-logo-detection-using-opencv-python/&#34;&gt;Heatmap for Logo Detection using OpenCV (Python)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/heatmap&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/deep-learning-based-object-detection-using-yolov3-with-opencv-python-c/&#34;&gt;Deep Learning based Object Detection using YOLOv3 with OpenCV ( Python / C++ )&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/ObjectDetection-YOLO&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/convex-hull-using-opencv-in-python-and-c/&#34;&gt;Convex Hull using OpenCV in Python and C++&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/ConvexHull&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/multitracker-multiple-object-tracking-using-opencv-c-python/&#34;&gt;MultiTracker : Multiple Object Tracking using OpenCV (C++/Python)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/MultiObjectTracker&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/convolutional-neural-network-based-image-colorization-using-opencv/&#34;&gt;Convolutional Neural Network based Image Colorization using OpenCV&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Colorization&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/svm-using-scikit-learn-in-python/&#34;&gt;SVM using scikit-learn&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/SVM-using-Python&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/goturn-deep-learning-based-object-tracking/&#34;&gt;GOTURN: Deep Learning based Object Tracking&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/GOTURN&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/find-center-of-blob-centroid-using-opencv-cpp-python/&#34;&gt;Find the Center of a Blob (Centroid) using OpenCV (C++/Python)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/CenterofBlob&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/support-vector-machines-svm/&#34;&gt;Support Vector Machines (SVM)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/SVM-using-Python&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/batch-normalization-in-deep-networks/&#34;&gt;Batch Normalization in Deep Networks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/BatchNormalization&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/deep-learning-character-classification-using-synthetic-dataset/&#34;&gt;Deep Learning based Character Classification using Synthetic Dataset&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/CharClassification&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/image-quality-assessment-brisque/&#34;&gt;Image Quality Assessment : BRISQUE&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/ImageMetrics&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/understanding-alexnet/&#34;&gt;Understanding AlexNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/deep-learning-based-text-recognition-ocr-using-tesseract-and-opencv/&#34;&gt;Deep Learning based Text Recognition (OCR) using Tesseract and OpenCV&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/OCR&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/deep-learning-based-human-pose-estimation-using-opencv-cpp-python/&#34;&gt;Deep Learning based Human Pose Estimation using OpenCV ( C++ / Python )&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/OpenPose&#34;&gt; Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/number-of-parameters-and-tensor-sizes-in-convolutional-neural-network/&#34;&gt;Number of Parameters and Tensor Sizes in a Convolutional Neural Network (CNN)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/how-to-convert-your-opencv-c-code-into-a-python-module/&#34;&gt;How to convert your OpenCV C++ code into a Python module&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/pymodule&#34;&gt; Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/cv4faces-best-project-award-2018/&#34;&gt;CV4Faces : Best Project Award 2018&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/facemark-facial-landmark-detection-using-opencv/&#34;&gt;Facemark : Facial Landmark Detection using OpenCV&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/FacialLandmarkDetection&#34;&gt; Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/image-alignment-feature-based-using-opencv-c-python/&#34;&gt;Image Alignment (Feature Based) using OpenCV (C++/Python)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/ImageAlignment-FeatureBased&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/barcode-and-qr-code-scanner-using-zbar-and-opencv/&#34;&gt;Barcode and QR code Scanner using ZBar and OpenCV&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/barcode-QRcodeScanner&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/keras-tutorial-fine-tuning-using-pre-trained-models/&#34;&gt;Keras Tutorial : Fine-tuning using pre-trained models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Keras-Fine-Tuning&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/opencv-transparent-api/&#34;&gt;OpenCV Transparent API&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/face-reconstruction-using-eigenfaces-cpp-python/&#34;&gt;Face Reconstruction using EigenFaces (C++/Python)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/ReconstructFaceUsingEigenFaces&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/eigenface-using-opencv-c-python/&#34;&gt;Eigenface using OpenCV (C++/Python)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/EigenFace&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/principal-component-analysis/&#34;&gt;Principal Component Analysis&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/keras-tutorial-transfer-learning-using-pre-trained-models/&#34;&gt;Keras Tutorial : Transfer Learning using pre-trained models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Keras-Transfer-Learning&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/keras-tutorial-using-pre-trained-imagenet-models/&#34;&gt;Keras Tutorial : Using pre-trained Imagenet models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Keras-ImageNet-Models&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/technical-aspects-of-a-digital-slr/&#34;&gt;Technical Aspects of a Digital SLR&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/using-harry-potter-interactive-wand-with-opencv-to-create-magic/&#34;&gt;Using Harry Potter interactive wand with OpenCV to create magic&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/install-opencv-3-and-dlib-on-windows-python-only/&#34;&gt;Install OpenCV 3 and Dlib on Windows ( Python only )&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/image-classification-using-convolutional-neural-networks-in-keras&#34;&gt;Image Classification using Convolutional Neural Networks in Keras&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/KerasCNN-CIFAR&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/understanding-autoencoders-using-tensorflow-python/&#34;&gt;Understanding Autoencoders using Tensorflow (Python)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/DenoisingAutoencoder&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/best-project-award-computer-vision-for-faces/&#34;&gt;Best Project Award : Computer Vision for Faces&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/understanding-activation-functions-in-deep-learning/&#34;&gt;Understanding Activation Functions in Deep Learning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/image-classification-using-feedforward-neural-network-in-keras/&#34;&gt;Image Classification using Feedforward Neural Network in Keras&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/KerasMLP-MNIST&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/exposure-fusion-using-opencv-cpp-python/&#34;&gt;Exposure Fusion using OpenCV (C++/Python)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/ExposureFusion&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnopencv.com/understanding-feedforward-neural-networks/&#34;&gt;Understanding Feedforward Neural Networks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/high-dynamic-range-hdr-imaging-using-opencv-cpp-python&#34;&gt;High Dynamic Range (HDR) Imaging using OpenCV (C++/Python)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/hdr&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/deep-learning-using-keras-the-basics&#34;&gt;Deep learning using Keras – The Basics&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/keras-linear-regression&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/selective-search-for-object-detection-cpp-python/&#34;&gt;Selective Search for Object Detection (C++ / Python)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/SelectiveSearch&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/installing-deep-learning-frameworks-on-ubuntu-with-cuda-support/&#34;&gt;Installing Deep Learning Frameworks on Ubuntu with CUDA support&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/parallel-pixel-access-in-opencv-using-foreach/&#34;&gt;Parallel Pixel Access in OpenCV using forEach&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/forEach&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/cvui-gui-lib-built-on-top-of-opencv-drawing-primitives/&#34;&gt;cvui: A GUI lib built on top of OpenCV drawing primitives&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/UI-cvui&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/install-dlib-on-windows/&#34;&gt;Install Dlib on Windows&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/install-dlib-on-ubuntu/&#34;&gt;Install Dlib on Ubuntu&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/install-opencv3-on-ubuntu/&#34;&gt;Install OpenCV3 on Ubuntu&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/read-write-and-display-a-video-using-opencv-cpp-python/&#34;&gt;Read, Write and Display a video using OpenCV ( C++/ Python )&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/VideoReadWriteDisplay&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/install-dlib-on-macos/&#34;&gt;Install Dlib on MacOS&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/install-opencv3-on-macos/&#34;&gt;Install OpenCV 3 on MacOS&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/install-opencv3-on-windows/&#34;&gt;Install OpenCV 3 on Windows&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/get-opencv-build-information-getbuildinformation/&#34;&gt;Get OpenCV Build Information ( getBuildInformation )&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/color-spaces-in-opencv-cpp-python/&#34;&gt;Color spaces in OpenCV (C++ / Python)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/ColorSpaces&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/neural-networks-a-30000-feet-view-for-beginners/&#34;&gt;Neural Networks : A 30,000 Feet View for Beginners&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/alpha-blending-using-opencv-cpp-python/&#34;&gt;Alpha Blending using OpenCV (C++ / Python)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/AlphaBlending&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/user-stories-how-readers-of-this-blog-are-applying-their-knowledge-to-build-applications/&#34;&gt;User stories : How readers of this blog are applying their knowledge to build applications&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/how-to-select-a-bounding-box-roi-in-opencv-cpp-python/&#34;&gt;How to select a bounding box ( ROI ) in OpenCV (C++/Python) ?&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/automatic-red-eye-remover-using-opencv-cpp-python/&#34;&gt;Automatic Red Eye Remover using OpenCV (C++ / Python)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/RedEyeRemover&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/bias-variance-tradeoff-in-machine-learning/&#34;&gt;Bias-Variance Tradeoff in Machine Learning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/embedded-computer-vision-which-device-should-you-choose/&#34;&gt;Embedded Computer Vision: Which device should you choose?&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/object-tracking-using-opencv-cpp-python/&#34;&gt;Object Tracking using OpenCV (C++/Python)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/tracking&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/handwritten-digits-classification-an-opencv-c-python-tutorial/&#34;&gt;Handwritten Digits Classification : An OpenCV ( C++ / Python ) Tutorial&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/digits-classification&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/training-better-haar-lbp-cascade-eye-detector-opencv/&#34;&gt;Training a better Haar and LBP cascade based Eye Detector using OpenCV&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/deep-learning-book-gift-recipients/&#34;&gt;Deep Learning Book Gift Recipients&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/minified-opencv-haar-and-lbp-cascades/&#34;&gt;Minified OpenCV Haar and LBP Cascades&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/ninjaEyeDetector&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/deep-learning-book-gift/&#34;&gt;Deep Learning Book Gift&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/histogram-of-oriented-gradients/&#34;&gt;Histogram of Oriented Gradients&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/image-recognition-and-object-detection-part1/&#34;&gt;Image Recognition and Object Detection : Part 1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/head-pose-estimation-using-opencv-and-dlib/&#34;&gt;Head Pose Estimation using OpenCV and Dlib&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/HeadPose&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/live-cv/&#34;&gt;Live CV : A Computer Vision Coding Application&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/approximate-focal-length-for-webcams-and-cell-phone-cameras/&#34;&gt;Approximate Focal Length for Webcams and Cell Phone Cameras&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/configuring-qt-for-opencv-on-osx/&#34;&gt;Configuring Qt for OpenCV on OSX&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/qt-test&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/rotation-matrix-to-euler-angles/&#34;&gt;Rotation Matrix To Euler Angles&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/RotationMatrixToEulerAngles&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/speeding-up-dlib-facial-landmark-detector/&#34;&gt;Speeding up Dlib’s Facial Landmark Detector&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/warp-one-triangle-to-another-using-opencv-c-python/&#34;&gt;Warp one triangle to another using OpenCV ( C++ / Python )&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/WarpTriangle&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/average-face-opencv-c-python-tutorial/&#34;&gt;Average Face : OpenCV ( C++ / Python ) Tutorial&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/FaceAverage&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/face-swap-using-opencv-c-python/&#34;&gt;Face Swap using OpenCV ( C++ / Python )&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/FaceSwap&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/face-morph-using-opencv-cpp-python/&#34;&gt;Face Morph Using OpenCV — C++ / Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/FaceMorph&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/deep-learning-example-using-nvidia-digits-3-on-ec2/&#34;&gt;Deep Learning Example using NVIDIA DIGITS 3 on EC2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/nvidia-digits-3-on-ec2/&#34;&gt;NVIDIA DIGITS 3 on EC2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/homography-examples-using-opencv-python-c/&#34;&gt;Homography Examples using OpenCV ( Python / C ++ )&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Homography&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/filling-holes-in-an-image-using-opencv-python-c/&#34;&gt;Filling holes in an image using OpenCV ( Python / C++ )&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Holes&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/how-to-find-frame-rate-or-frames-per-second-fps-in-opencv-python-cpp/&#34;&gt;How to find frame rate or frames per second (fps) in OpenCV ( Python / C++ ) ?&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/FPS&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/delaunay-triangulation-and-voronoi-diagram-using-opencv-c-python/&#34;&gt;Delaunay Triangulation and Voronoi Diagram using OpenCV ( C++ / Python) &lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Delaunay&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/opencv-c-vs-python-vs-matlab-for-computer-vision/&#34;&gt;OpenCV (C++ vs Python) vs MATLAB for Computer Vision&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/facial-landmark-detection/&#34;&gt;Facial Landmark Detection&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/why-does-opencv-use-bgr-color-format/&#34;&gt;Why does OpenCV use BGR color format ?&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/computer-vision-for-predicting-facial-attractiveness/&#34;&gt;Computer Vision for Predicting Facial Attractiveness&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/FacialAttractiveness&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/applycolormap-for-pseudocoloring-in-opencv-c-python/&#34;&gt;applyColorMap for pseudocoloring in OpenCV ( C++ / Python )&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Colormap&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/image-alignment-ecc-in-opencv-c-python/&#34;&gt;Image Alignment (ECC) in OpenCV ( C++ / Python )&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/ImageAlignment&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/how-to-find-opencv-version-python-cpp/&#34;&gt;How to find OpenCV version in Python and C++ ?&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/baidu-banned-from-ilsvrc-2015/&#34;&gt;Baidu banned from ILSVRC 2015&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/opencv-transparent-api/&#34;&gt;OpenCV Transparent API&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/how-computer-vision-solved-the-greatest-soccer-mystery-of-all-times/&#34;&gt;How Computer Vision Solved the Greatest Soccer Mystery of All Time&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/embedded-vision-summit-2015/&#34;&gt;Embedded Vision Summit 2015&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/read-an-image-in-opencv-python-cpp/&#34;&gt;Read an Image in OpenCV ( Python, C++ )&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/imread&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/non-photorealistic-rendering-using-opencv-python-c/&#34;&gt;Non-Photorealistic Rendering using OpenCV ( Python, C++ )&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/NonPhotorealisticRendering&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/seamless-cloning-using-opencv-python-cpp/&#34;&gt;Seamless Cloning using OpenCV ( Python , C++ )&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/SeamlessCloning&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/opencv-threshold-python-cpp/&#34;&gt;OpenCV Threshold ( Python , C++ )&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Threshold&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/blob-detection-using-opencv-python-c/&#34;&gt;Blob Detection Using OpenCV ( Python, C++ )&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/BlobDetector&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/turn-your-opencv-Code-into-a-web-api-in-under-10-minutes-part-1/&#34;&gt;Turn your OpenCV Code into a Web API in under 10 minutes — Part 1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/how-to-compile-opencv-sample-Code/&#34;&gt;How to compile OpenCV sample Code ?&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.learnopencv.com/install-opencv-3-on-yosemite-osx-10-10-x/&#34;&gt;Install OpenCV 3 on Yosemite ( OSX 10.10.x )&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/create-snapchat-instagram-filters-using-mediapipe/&#34;&gt;Create Snapchat/Instagram Filters Using Medipipe&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Create-AR-filters-using-Mediapipe&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/object-detection-with-depth-measurement-with-oak-d/&#34;&gt;Object detection with depth measurement using pre-trained models with OAK-D&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/OAK-Object-Detection-with-Depth&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/depthai-pipeline-overview-creating-a-complex-pipeline/&#34;&gt;DepthAi Pipeline Overview&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/OAK-DepthAi-Pipeline-Overview&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://learnopencv.com/anti-spoofing-face-recognition-system-using-oak-d-and-depthai/&#34;&gt;Anti-Spoofing Face Recognition with OAK-D&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv/tree/master/Anti-Spoofing-Face-Recognition-with-OAK-D&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt;</summary>
  </entry>
  <entry>
    <title>CoreyMSchafer/code_snippets</title>
    <updated>2022-06-01T02:12:48Z</updated>
    <id>tag:github.com,2022-06-01:/CoreyMSchafer/code_snippets</id>
    <link href="https://github.com/CoreyMSchafer/code_snippets" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;code_snippets&lt;/h1&gt;</summary>
  </entry>
  <entry>
    <title>deepmind/deepmind-research</title>
    <updated>2022-06-01T02:12:48Z</updated>
    <id>tag:github.com,2022-06-01:/deepmind/deepmind-research</id>
    <link href="https://github.com/deepmind/deepmind-research" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This repository contains implementations and illustrative code to accompany DeepMind publications&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DeepMind Research&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains implementations and illustrative code to accompany DeepMind publications. Along with publishing papers to accompany research conducted at DeepMind, we release open-source &lt;a href=&#34;https://deepmind.com/research/open-source/open-source-environments/&#34;&gt;environments&lt;/a&gt;, &lt;a href=&#34;https://deepmind.com/research/open-source/open-source-datasets/&#34;&gt;data sets&lt;/a&gt;, and &lt;a href=&#34;https://deepmind.com/research/open-source/open-source-code/&#34;&gt;code&lt;/a&gt; to enable the broader research community to engage with our work and build upon it, with the ultimate goal of accelerating scientific progress to benefit society. For example, you can build on our implementations of the &lt;a href=&#34;https://github.com/deepmind/dqn&#34;&gt;Deep Q-Network&lt;/a&gt; or &lt;a href=&#34;https://github.com/deepmind/dnc&#34;&gt;Differential Neural Computer&lt;/a&gt;, or experiment in the same environments we use for our research, such as &lt;a href=&#34;https://github.com/deepmind/lab&#34;&gt;DeepMind Lab&lt;/a&gt; or &lt;a href=&#34;https://github.com/deepmind/pysc2&#34;&gt;StarCraft II&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you enjoy building tools, environments, software libraries, and other infrastructure of the kind listed below, you can view open positions to work in related areas on our &lt;a href=&#34;https://deepmind.com/careers/&#34;&gt;careers page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For a full list of our publications, please see &lt;a href=&#34;https://deepmind.com/research/publications/&#34;&gt;https://deepmind.com/research/publications/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Projects&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/fusion_tcv&#34;&gt;Magnetic control of tokamak plasmas through deep reinforcement learning&lt;/a&gt;, Nature 2022&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/density_functional_approximation_dm21&#34;&gt;Pushing the Frontiers of Density Functionals by Solving the Fractional Electron Problem&lt;/a&gt;, Science 2021&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/pitfalls_static_language_models&#34;&gt;Mind the Gap: Assessing Temporal Generalization in Neural Language Models&lt;/a&gt;, NeurIPS 2021&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/tandem_dqn&#34;&gt;The Difficulty of Passive Learning in Deep Reinforcement Learning&lt;/a&gt;, NeurIPS 2021&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/nowcasting&#34;&gt;Skilful precipitation nowcasting using deep generative models of radar&lt;/a&gt;, Nature 2021&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/cadl&#34;&gt;Compute-Aided Design as Language&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/continual_learning&#34;&gt;Encoders and ensembles for continual learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/hierarchical_transformer_memory&#34;&gt;Towards mental time travel: a hierarchical memory for reinforcement learning agents&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/perceiver&#34;&gt;Perceiver IO: A General Architecture for Structured Inputs &amp;amp; Outputs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/neural_mip_solving&#34;&gt;Solving Mixed Integer Programs Using Neural Networks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/noisy_label&#34;&gt;A Realistic Simulation Framework for Learning with Label Noise&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/rapid_task_solving&#34;&gt;Rapid Task-Solving in Novel Environments&lt;/a&gt;, ICLR 2021&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/wikigraphs&#34;&gt;WikiGraphs: A Wikipedia - Knowledge Graph Paired Dataset&lt;/a&gt;, TextGraphs 2021&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/box_arrangement&#34;&gt;Behavior Priors for Efficient Reinforcement Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/meshgraphnets&#34;&gt;Learning Mesh-Based Simulation with Graph Networks&lt;/a&gt;, ICLR 2021&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/ogb_lsc&#34;&gt;Open Graph Benchmark - Large-Scale Challenge (OGB-LSC)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/synthetic_returns&#34;&gt;Synthetic Returns for Long-Term Credit Assignment&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/galaxy_mergers&#34;&gt;A Deep Learning Approach for Characterizing Major Galaxy Mergers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/kfac_ferminet_alpha&#34;&gt;Better, Faster Fermionic Neural Networks&lt;/a&gt; (KFAC implementation)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/object_attention_for_reasoning&#34;&gt;Object-based attention for spatio-temporal reasoning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/enformer&#34;&gt;Effective gene expression prediction from sequence by integrating long-range interactions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/satore&#34;&gt;Satore: First-order logic saturation with atom rewriting&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/nfnets&#34;&gt;Characterizing signal propagation to close the performance gap in unnormalized ResNets&lt;/a&gt;, ICLR 2021&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/adversarial_robustness&#34;&gt;Uncovering the Limits of Adversarial Training against Norm-Bounded Adversarial Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/cmtouch&#34;&gt;Learning rich touch representations through cross-modal self-supervision&lt;/a&gt;, CoRL 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/functional_regularisation_for_continual_learning&#34;&gt;Functional Regularisation for Continual Learning&lt;/a&gt;, ICLR 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/avae&#34;&gt;The Autoencoding Variational Autoencoder&lt;/a&gt;, NeurIPS 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/mmv&#34;&gt;Self-Supervised MultiModal Versatile Networks&lt;/a&gt;, NeurIPS 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/ode_gan&#34;&gt;ODE-GAN: Training GANs by Solving Ordinary Differential Equations&lt;/a&gt;, NeurIPS 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/causal_reasoning&#34;&gt;Algorithms for Causal Reasoning in Probability Trees&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/gated_linear_networks&#34;&gt;Gated Linear Networks&lt;/a&gt;, NeurIPS 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/himo&#34;&gt;Value-driven Hindsight Modelling&lt;/a&gt;, NeurIPS 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/learned_free_energy_estimation&#34;&gt;Targeted free energy estimation via learned mappings&lt;/a&gt;, Journal of Chemical Physics 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/learning_to_simulate&#34;&gt;Learning to Simulate Complex Physics with Graph Networks&lt;/a&gt;, ICML 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/physics_planning_games&#34;&gt;Physically Embedded Planning Problems&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/polygen&#34;&gt;PolyGen: PolyGen: An Autoregressive Generative Model of 3D Meshes&lt;/a&gt;, ICML 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/byol&#34;&gt;Bootstrap Your Own Latent&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/catch_carry&#34;&gt;Catch &amp;amp; Carry: Reusable Neural Controllers for Vision-Guided Whole-Body Tasks&lt;/a&gt;, SIGGRAPH 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/memo&#34;&gt;MEMO: A Deep Network For Flexible Combination Of Episodic Memories&lt;/a&gt;, ICLR 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/rl_unplugged&#34;&gt;RL Unplugged: Benchmarks for Offline Reinforcement Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/geomancer&#34;&gt;Disentangling by Subspace Diffusion (GEOMANCER)&lt;/a&gt;, NeurIPS 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/affordances_theory&#34;&gt;What can I do here? A theory of affordances in reinforcement learning&lt;/a&gt;, ICML 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/sketchy&#34;&gt;Scaling data-driven robotics with reward sketching and batch reinforcement learning&lt;/a&gt;, RSS 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/counterfactual_fairness&#34;&gt;Path-Specific Counterfactual Fairness&lt;/a&gt;, AAAI 2019&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/option_keyboard&#34;&gt;The Option Keyboard: Combining Skills in Reinforcement Learning&lt;/a&gt;, NeurIPS 2019&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/visr&#34;&gt;VISR - Fast Task Inference with Variational Intrinsic Successor Features&lt;/a&gt;, ICLR 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/glassy_dynamics&#34;&gt;Unveiling the predictive power of static structure in glassy systems&lt;/a&gt;, Nature Physics 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/iodine&#34;&gt;Multi-Object Representation Learning with Iterative Variational Inference (IODINE)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/alphafold_casp13&#34;&gt;AlphaFold CASP13&lt;/a&gt;, Nature 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/unrestricted_advx&#34;&gt;Unrestricted Adversarial Challenge&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/hierarchical_probabilistic_unet&#34;&gt;Hierarchical Probabilistic U-Net (HPU-Net)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/scratchgan&#34;&gt;Training Language GANs from Scratch&lt;/a&gt;, NeurIPS 2019&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/tvt&#34;&gt;Temporal Value Transport&lt;/a&gt;, Nature Communications 2019&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/curl&#34;&gt;Continual Unsupervised Representation Learning (CURL)&lt;/a&gt;, NeurIPS 2019&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/transporter&#34;&gt;Unsupervised Learning of Object Keypoints (Transporter)&lt;/a&gt;, NeurIPS 2019&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/bigbigan&#34;&gt;BigBiGAN&lt;/a&gt;, NeurIPS 2019&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/cs_gan&#34;&gt;Deep Compressed Sensing&lt;/a&gt;, ICML 2019&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/side_effects_penalties&#34;&gt;Side Effects Penalties&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/PrediNet&#34;&gt;PrediNet Architecture and Relations Game Datasets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/unsupervised_adversarial_training&#34;&gt;Unsupervised Adversarial Training&lt;/a&gt;, NeurIPS 2019&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/graph_matching_networks&#34;&gt;Graph Matching Networks for Learning the Similarity of Graph Structured Objects&lt;/a&gt;, ICML 2019&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/regal&#34;&gt;REGAL: Transfer Learning for Fast Optimization of Computation Graphs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/ensemble_loss_landscape&#34;&gt;Deep Ensembles: A Loss Landscape Perspective&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/powerpropagation&#34;&gt;Powerpropagation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/physics_inspired_models&#34;&gt;Physics Inspired Models&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;This is not an official Google product.&lt;/em&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>huggingface/notebooks</title>
    <updated>2022-06-01T02:12:48Z</updated>
    <id>tag:github.com,2022-06-01:/huggingface/notebooks</id>
    <link href="https://github.com/huggingface/notebooks" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Notebooks using the Hugging Face libraries 🤗&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;notebooks&lt;/h1&gt; &#xA;&lt;p&gt;Notebooks using the Hugging Face libraries 🤗&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>NielsRogge/Transformers-Tutorials</title>
    <updated>2022-06-01T02:12:48Z</updated>
    <id>tag:github.com,2022-06-01:/NielsRogge/Transformers-Tutorials</id>
    <link href="https://github.com/NielsRogge/Transformers-Tutorials" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This repository contains demos I made with the Transformers library by HuggingFace.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Transformers-Tutorials&lt;/h1&gt; &#xA;&lt;p&gt;Hi there!&lt;/p&gt; &#xA;&lt;p&gt;This repository contains demos I made with the &lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;Transformers library&lt;/a&gt; by 🤗 HuggingFace. Currently, all of them are implemented in PyTorch.&lt;/p&gt; &#xA;&lt;p&gt;NOTE: if you are not familiar with HuggingFace and/or Transformers, I highly recommend to check out our &lt;a href=&#34;https://huggingface.co/course/chapter1&#34;&gt;free course&lt;/a&gt;, which introduces you to several Transformer architectures (such as BERT, GPT-2, T5, BART, etc.), as well as an overview of the HuggingFace libraries, including &lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;Transformers&lt;/a&gt;, &lt;a href=&#34;https://github.com/huggingface/tokenizers&#34;&gt;Tokenizers&lt;/a&gt;, &lt;a href=&#34;https://github.com/huggingface/datasets&#34;&gt;Datasets&lt;/a&gt;, &lt;a href=&#34;https://github.com/huggingface/accelerate&#34;&gt;Accelerate&lt;/a&gt; and the &lt;a href=&#34;https://huggingface.co/&#34;&gt;hub&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Currently, it contains the following demos:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;BERT (&lt;a href=&#34;https://arxiv.org/abs/1810.04805&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;BertForTokenClassification&lt;/code&gt; on a named entity recognition (NER) dataset. &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Custom_Named_Entity_Recognition_with_BERT_only_first_wordpiece.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;BertForSequenceClassification&lt;/code&gt; for multi-label text classification. &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;BEiT (&lt;a href=&#34;https://raw.githubusercontent.com/NielsRogge/Transformers-Tutorials/master/%5Bhttps://arxiv.org/abs/2103.06874%5D(https://arxiv.org/abs/2106.08254)&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;understanding &lt;code&gt;BeitForMaskedImageModeling&lt;/code&gt; &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BEiT/Understanding_BeitForMaskedImageModeling.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;CANINE (&lt;a href=&#34;https://arxiv.org/abs/2103.06874&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;CanineForSequenceClassification&lt;/code&gt; on IMDb &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/CANINE/Fine_tune_CANINE_on_IMDb_(movie_review_binary_classification).ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;ConvNeXT (&lt;a href=&#34;https://arxiv.org/abs/2201.03545&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;fine-tuning (and performing inference with) &lt;code&gt;ConvNextForImageClassification&lt;/code&gt; &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/ConvNeXT/Fine_tune_ConvNeXT_for_image_classification.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;DPT (&lt;a href=&#34;https://arxiv.org/abs/2103.13413&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;performing inference with DPT for monocular depth estimation &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DPT/DPT_inference_notebook_(depth_estimation).ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;performing inference with DPT for semantic segmentation &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DPT/DPT_inference_notebook_(semantic_segmentation).ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;DETR (&lt;a href=&#34;https://arxiv.org/abs/2005.12872&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;performing inference with &lt;code&gt;DetrForObjectDetection&lt;/code&gt; &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DETR/DETR_minimal_example_(with_DetrFeatureExtractor).ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;DetrForObjectDetection&lt;/code&gt; on a custom object detection dataset &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DETR/Fine_tuning_DetrForObjectDetection_on_custom_dataset_(balloon).ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;evaluating &lt;code&gt;DetrForObjectDetection&lt;/code&gt; on the COCO detection 2017 validation set &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DETR/Evaluating_DETR_on_COCO_validation_2017.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;performing inference with &lt;code&gt;DetrForSegmentation&lt;/code&gt; &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DETR/DETR_panoptic_segmentation_minimal_example_(with_DetrFeatureExtractor).ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;DetrForSegmentation&lt;/code&gt; on COCO panoptic 2017 &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DETR/Fine_tuning_DetrForSegmentation_on_custom_dataset_end_to_end_approach.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;DiT (&lt;a href=&#34;https://arxiv.org/abs/2203.02378&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;performing inference with DiT for document image classification &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DiT/Inference_with_DiT_(Document_Image_Transformer)_for_document_image_classification.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;GLPN (&lt;a href=&#34;https://arxiv.org/abs/2201.07436&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;performing inference with &lt;code&gt;GLPNForDepthEstimation&lt;/code&gt; to illustrate monocular depth estimation &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/GLPN/GLPN_inference_(depth_estimation).ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;GPT-J-6B (&lt;a href=&#34;https://github.com/kingoflolz/mesh-transformer-jax&#34;&gt;repository&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;performing inference with &lt;code&gt;GPTJForCausalLM&lt;/code&gt; to illustrate few-shot learning and code generation &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/GPT-J-6B/Inference_with_GPT_J_6B.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;ImageGPT (&lt;a href=&#34;https://openai.com/blog/image-gpt/&#34;&gt;blog post&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;(un)conditional image generation with &lt;code&gt;ImageGPTForCausalLM&lt;/code&gt; &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/ImageGPT/(Un)conditional_image_generation_with_ImageGPT.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;linear probing with ImageGPT &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/ImageGPT/Linear_probing_with_ImageGPT.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;LUKE (&lt;a href=&#34;https://arxiv.org/abs/2010.01057&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;LukeForEntityPairClassification&lt;/code&gt; on a custom relation extraction dataset using PyTorch Lightning &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LUKE/Supervised_relation_extraction_with_LukeForEntityPairClassification.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;LayoutLM (&lt;a href=&#34;https://arxiv.org/abs/1912.13318&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;LayoutLMForTokenClassification&lt;/code&gt; on the &lt;a href=&#34;https://guillaumejaume.github.io/FUNSD/&#34;&gt;FUNSD&lt;/a&gt; dataset &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForTokenClassification_on_FUNSD.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;LayoutLMForSequenceClassification&lt;/code&gt; on the &lt;a href=&#34;https://www.cs.cmu.edu/~aharley/rvl-cdip/&#34;&gt;RVL-CDIP&lt;/a&gt; dataset &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForSequenceClassification_on_RVL_CDIP.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;adding image embeddings to LayoutLM during fine-tuning on the &lt;a href=&#34;https://guillaumejaume.github.io/FUNSD/&#34;&gt;FUNSD&lt;/a&gt; dataset &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Add_image_embeddings_to_LayoutLM.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;LayoutLMv2 (&lt;a href=&#34;https://arxiv.org/abs/2012.14740&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;LayoutLMv2ForSequenceClassification&lt;/code&gt; on RVL-CDIP &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/RVL-CDIP/Fine_tuning_LayoutLMv2ForSequenceClassification_on_RVL_CDIP.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;LayoutLMv2ForTokenClassification&lt;/code&gt; on FUNSD &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/FUNSD/Fine_tuning_LayoutLMv2ForTokenClassification_on_FUNSD.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;LayoutLMv2ForTokenClassification&lt;/code&gt; on FUNSD using the 🤗 Trainer &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/FUNSD/Fine_tuning_LayoutLMv2ForTokenClassification_on_FUNSD_using_HuggingFace_Trainer.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;performing inference with &lt;code&gt;LayoutLMv2ForTokenClassification&lt;/code&gt; on FUNSD &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/FUNSD/Inference_with_LayoutLMv2ForTokenClassification.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;true inference with &lt;code&gt;LayoutLMv2ForTokenClassification&lt;/code&gt; (when no labels are available) + Gradio demo &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/FUNSD/True_inference_with_LayoutLMv2ForTokenClassification_%2B_Gradio_demo.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;LayoutLMv2ForTokenClassification&lt;/code&gt; on CORD &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/CORD/Fine_tuning_LayoutLMv2ForTokenClassification_on_CORD.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;LayoutLMv2ForQuestionAnswering&lt;/code&gt; on DOCVQA &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/DocVQA/Fine_tuning_LayoutLMv2ForQuestionAnswering_on_DocVQA.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;LayoutLMv3 (&lt;a href=&#34;https://arxiv.org/abs/2204.08387&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;LayoutLMv3ForTokenClassification&lt;/code&gt; on the &lt;a href=&#34;https://guillaumejaume.github.io/FUNSD/&#34;&gt;FUNSD&lt;/a&gt; dataset &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv3/Fine_tune_LayoutLMv3_on_FUNSD_(HuggingFace_Trainer).ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;MaskFormer (&lt;a href=&#34;https://arxiv.org/abs/2107.06278&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;performing inference with &lt;code&gt;MaskFormer&lt;/code&gt; (both semantic and panoptic segmentation): &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/MaskFormer/maskformer_minimal_example(with_MaskFormerFeatureExtractor).ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;MaskFormer&lt;/code&gt; on a custom dataset for semantic segmentation &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/MaskFormer/Fine_tune_MaskFormer_on_custom_dataset.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Perceiver IO (&lt;a href=&#34;https://arxiv.org/abs/2107.14795&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;showcasing masked language modeling and image classification with the Perceiver &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/Perceiver/Perceiver_for_masked_language_modeling_and_image_classification.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning the Perceiver for image classification &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/Perceiver/Fine_tune_the_Perceiver_for_image_classification.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning the Perceiver for text classification &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/Perceiver/Fine_tune_Perceiver_for_text_classification.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;predicting optical flow between a pair of images with &lt;code&gt;PerceiverForOpticalFlow&lt;/code&gt;&lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/Perceiver/Perceiver_for_Optical_Flow.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;auto-encoding a video (images, audio, labels) with &lt;code&gt;PerceiverForMultimodalAutoencoding&lt;/code&gt; &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/Perceiver/Perceiver_for_Multimodal_Autoencoding.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;SegFormer (&lt;a href=&#34;https://arxiv.org/abs/2105.15203&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;performing inference with &lt;code&gt;SegformerForSemanticSegmentation&lt;/code&gt; &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/SegFormer/Segformer_inference_notebook.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;SegformerForSemanticSegmentation&lt;/code&gt; on custom data using native PyTorch &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/SegFormer/Fine_tune_SegFormer_on_custom_dataset.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;T5 (&lt;a href=&#34;https://arxiv.org/abs/1910.10683&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;T5ForConditionalGeneration&lt;/code&gt; on a Dutch summarization dataset on TPU using HuggingFace Accelerate &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/tree/master/T5&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;T5ForConditionalGeneration&lt;/code&gt; (CodeT5) for Ruby code summarization using PyTorch Lightning &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/T5/Fine_tune_CodeT5_for_generating_docstrings_from_Ruby_code.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;TAPAS (&lt;a href=&#34;https://arxiv.org/abs/2004.02349&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;TapasForQuestionAnswering&lt;/code&gt; on the Microsoft &lt;a href=&#34;https://www.microsoft.com/en-us/download/details.aspx?id=54253&#34;&gt;Sequential Question Answering (SQA)&lt;/a&gt; dataset &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;evaluating &lt;code&gt;TapasForSequenceClassification&lt;/code&gt; on the &lt;a href=&#34;https://tabfact.github.io/&#34;&gt;Table Fact Checking (TabFact)&lt;/a&gt; dataset &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Evaluating_TAPAS_on_the_Tabfact_test_set.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;TrOCR (&lt;a href=&#34;https://arxiv.org/abs/2109.10282&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;performing inference with &lt;code&gt;TrOCR&lt;/code&gt; to illustrate optical character recognition with Transformers, as well as making a Gradio demo &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TrOCR/Inference_with_TrOCR_%2B_Gradio_demo.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;TrOCR&lt;/code&gt; on the IAM dataset using the Seq2SeqTrainer &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TrOCR/Fine_tune_TrOCR_on_IAM_Handwriting_Database_using_Seq2SeqTrainer.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;TrOCR&lt;/code&gt; on the IAM dataset using native PyTorch &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TrOCR/Fine_tune_TrOCR_on_IAM_Handwriting_Database_using_native_PyTorch.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;evaluating &lt;code&gt;TrOCR&lt;/code&gt; on the IAM test set &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TrOCR/Evaluating_TrOCR_base_handwritten_on_the_IAM_test_set.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;ViLT (&lt;a href=&#34;https://arxiv.org/abs/2102.03334&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;ViLT&lt;/code&gt; for visual question answering (VQA) &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/ViLT/Fine_tuning_ViLT_for_VQA.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;performing inference with &lt;code&gt;ViLT&lt;/code&gt; to illustrate visual question answering (VQA) &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/ViLT/Inference_with_ViLT_(visual_question_answering).ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;masked language modeling (MLM) with a pre-trained &lt;code&gt;ViLT&lt;/code&gt; model &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/ViLT/Masked_language_modeling_with_ViLT.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;performing inference with &lt;code&gt;ViLT&lt;/code&gt; for image-text retrieval &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/ViLT/Using_ViLT_for_image_text_retrieval.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;performing inference with &lt;code&gt;ViLT&lt;/code&gt; to illustrate natural language for visual reasoning (NLVR) &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/ViLT/ViLT_for_natural_language_visual_reasoning.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;ViTMAE (&lt;a href=&#34;https://arxiv.org/abs/2111.06377&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;reconstructing pixel values with &lt;code&gt;ViTMAEForPreTraining&lt;/code&gt; &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/ViTMAE/ViT_MAE_visualization_demo.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Vision Transformer (&lt;a href=&#34;https://arxiv.org/abs/2010.11929&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;performing inference with &lt;code&gt;ViTForImageClassification&lt;/code&gt; &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Quick_demo_of_HuggingFace_version_of_Vision_Transformer_inference.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;ViTForImageClassification&lt;/code&gt; on &lt;a href=&#34;https://www.cs.toronto.edu/~kriz/cifar.html&#34;&gt;CIFAR-10&lt;/a&gt; using PyTorch Lightning &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_PyTorch_Lightning.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;ViTForImageClassification&lt;/code&gt; on &lt;a href=&#34;https://www.cs.toronto.edu/~kriz/cifar.html&#34;&gt;CIFAR-10&lt;/a&gt; using the 🤗 Trainer &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_the_%F0%9F%A4%97_Trainer.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;YOLOS (&lt;a href=&#34;https://arxiv.org/abs/2106.00666&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;YolosForObjectDetection&lt;/code&gt; on a custom dataset &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/YOLOS/Fine_tuning_YOLOS_for_object_detection_on_custom_dataset_(balloon).ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;inference with &lt;code&gt;YolosForObjectDetection&lt;/code&gt; &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/YOLOS/YOLOS_minimal_inference_example.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;... more to come! 🤗&lt;/p&gt; &#xA;&lt;p&gt;If you have any questions regarding these demos, feel free to open an issue on this repository.&lt;/p&gt; &#xA;&lt;p&gt;Btw, I was also the main contributor to add the following algorithms to the library:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;TAbular PArSing (TAPAS) by Google AI&lt;/li&gt; &#xA; &lt;li&gt;Vision Transformer (ViT) by Google AI&lt;/li&gt; &#xA; &lt;li&gt;DINO by Facebook AI&lt;/li&gt; &#xA; &lt;li&gt;Data-efficient Image Transformers (DeiT) by Facebook AI&lt;/li&gt; &#xA; &lt;li&gt;LUKE by Studio Ousia&lt;/li&gt; &#xA; &lt;li&gt;DEtection TRansformers (DETR) by Facebook AI&lt;/li&gt; &#xA; &lt;li&gt;CANINE by Google AI&lt;/li&gt; &#xA; &lt;li&gt;BEiT by Microsoft Research&lt;/li&gt; &#xA; &lt;li&gt;LayoutLMv2 (and LayoutXLM) by Microsoft Research&lt;/li&gt; &#xA; &lt;li&gt;TrOCR by Microsoft Research&lt;/li&gt; &#xA; &lt;li&gt;SegFormer by NVIDIA&lt;/li&gt; &#xA; &lt;li&gt;ImageGPT by OpenAI&lt;/li&gt; &#xA; &lt;li&gt;Perceiver by Deepmind&lt;/li&gt; &#xA; &lt;li&gt;MAE by Facebook AI&lt;/li&gt; &#xA; &lt;li&gt;ViLT by NAVER AI Lab&lt;/li&gt; &#xA; &lt;li&gt;ConvNeXT by Facebook AI&lt;/li&gt; &#xA; &lt;li&gt;DiT By Microsoft Research&lt;/li&gt; &#xA; &lt;li&gt;GLPN by KAIST&lt;/li&gt; &#xA; &lt;li&gt;DPT by Intel Labs&lt;/li&gt; &#xA; &lt;li&gt;TAPEX by Microsoft Research&lt;/li&gt; &#xA; &lt;li&gt;LayoutLMv3 by Microsoft Research&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;All of them were an incredible learning experience. I can recommend anyone to contribute an AI algorithm to the library!&lt;/p&gt; &#xA;&lt;h2&gt;Data preprocessing&lt;/h2&gt; &#xA;&lt;p&gt;Regarding preparing your data for a PyTorch model, there are a few options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;a native PyTorch dataset + dataloader. This is the standard way to prepare data for a PyTorch model, namely by subclassing &lt;code&gt;torch.utils.data.Dataset&lt;/code&gt;, and then a creating corresponding &lt;code&gt;DataLoader&lt;/code&gt; (which is a Python generator that allows to loop over the items of a dataset). When subclassing the &lt;code&gt;Dataset&lt;/code&gt; class, one needs to implement 3 methods: &lt;code&gt;__init__&lt;/code&gt;, &lt;code&gt;__len__&lt;/code&gt; (which returns the number of examples of the dataset) and &lt;code&gt;__getitem__&lt;/code&gt; (which returns an example of the dataset, given an integer index). Here&#39;s an example of creating a basic text classification dataset (assuming one has a CSV that contains 2 columns, namely &#34;text&#34; and &#34;label&#34;):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from torch.utils.data import Dataset&#xA;&#xA;class CustomTrainDataset(Dataset):&#xA;    def __init__(self, df, tokenizer):&#xA;        self.df = df&#xA;        self.tokenizer = tokenizer&#xA;&#xA;    def __len__(self):&#xA;        return len(self.df)&#xA;&#xA;    def __getitem__(self, idx):&#xA;        # get item&#xA;        item = df.iloc[idx]&#xA;        text = item[&#39;text&#39;]&#xA;        label = item[&#39;label&#39;]&#xA;        # encode text&#xA;        encoding = self.tokenizer(text, padding=&#34;max_length&#34;, max_length=128, truncation=True, return_tensors=&#34;pt&#34;)&#xA;        # remove batch dimension which the tokenizer automatically adds&#xA;        encoding = {k:v.squeeze() for k,v in encoding.items()}&#xA;        # add label&#xA;        encoding[&#34;label&#34;] = torch.tensor(label)&#xA;        &#xA;        return encoding&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Instantiating the dataset then happens as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import BertTokenizer&#xA;import pandas as pd&#xA;&#xA;tokenizer = BertTokenizer.from_pretrained(&#34;bert-base-uncased&#34;)&#xA;df = pd.read_csv(&#34;path_to_your_csv&#34;)&#xA;&#xA;train_dataset = CustomTrainDataset(df=df tokenizer=tokenizer)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Accessing the first example of the dataset can then be done as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;encoding = train_dataset[0]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In practice, one creates a corresponding &lt;code&gt;DataLoader&lt;/code&gt;, that allows to get batches from the dataset:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from torch.utils.data import DataLoader&#xA;&#xA;train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;I often check whether the data is created correctly by fetching the first batch from the data loader, and then printing out the shapes of the tensors, decoding the input_ids back to text, etc.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;batch = next(iter(train_dataloader))&#xA;for k,v in batch.items():&#xA;    print(k, v.shape)&#xA;# decode the input_ids of the first example of the batch&#xA;print(tokenizer.decode(batch[&#39;input_ids&#39;][0].tolist())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/datasets/&#34;&gt;HuggingFace Datasets&lt;/a&gt;. Datasets is a library by HuggingFace that allows to easily load and process data in a very fast and memory-efficient way. It is backed by &lt;a href=&#34;https://arrow.apache.org/&#34;&gt;Apache Arrow&lt;/a&gt;, and has cool features such as memory-mapping, which allow you to only load data into RAM when it is required. It only has deep interoperability with the &lt;a href=&#34;https://huggingface.co/datasets&#34;&gt;HuggingFace hub&lt;/a&gt;, allowing to easily load well-known datasets as well as share your own with the community.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Loading a custom dataset as a Dataset object can be done as follows (you can install datasets using &lt;code&gt;pip install datasets&lt;/code&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from datasets import load_dataset&#xA;&#xA;dataset = load_dataset(&#39;csv&#39;, data_files={&#39;train&#39;: [&#39;my_train_file_1.csv&#39;, &#39;my_train_file_2.csv&#39;] &#39;test&#39;: &#39;my_test_file.csv&#39;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here I&#39;m loading local CSV files, but there are other formats supported (including JSON, Parquet, txt) as well as loading data from a local Pandas dataframe or dictionary for instance. You can check out the &lt;a href=&#34;https://huggingface.co/docs/datasets/loading.html#local-and-remote-files&#34;&gt;docs&lt;/a&gt; for all details.&lt;/p&gt; &#xA;&lt;h2&gt;Training frameworks&lt;/h2&gt; &#xA;&lt;p&gt;Regarding fine-tuning Transformer models (or more generally, PyTorch models), there are a few options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;using native PyTorch. This is the most basic way to train a model, and requires the user to manually write the training loop. The advantage is that this is very easy to debug. The disadvantage is that one needs to implement training him/herself, such as setting the model in the appropriate mode (&lt;code&gt;model.train()&lt;/code&gt;/&lt;code&gt;model.eval()&lt;/code&gt;), handle device placement (&lt;code&gt;model.to(device)&lt;/code&gt;), etc. A typical training loop in PyTorch looks as follows (inspired by &lt;a href=&#34;&#34;&gt;this great PyTorch intro tutorial&lt;/a&gt;):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;&#xA;model = ...&#xA;&#xA;# I almost always use a learning rate of 5e-5 when fine-tuning Transformer based models&#xA;optimizer = torch.optim.Adam(model.parameters(), lr=5-e5)&#xA;&#xA;# put model on GPU, if available&#xA;device = torch.device(&#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34;)&#xA;model.to(device)&#xA;&#xA;for epoch in range(epochs):&#xA;    model.train()&#xA;    train_loss = 0.0&#xA;    for batch in train_dataloader:&#xA;        # put batch on device&#xA;        batch = {k:v.to(device) for k,v in batch.items()}&#xA;        &#xA;        # forward pass&#xA;        outputs = model(**batch)&#xA;        loss = outputs.loss&#xA;        &#xA;        train_loss += loss.item()&#xA;        &#xA;        loss.backward()&#xA;        optimizer.step()&#xA;        optimizer.zero_grad()&#xA;&#xA;    print(&#34;Loss after epoch {epoch}:&#34;, train_loss/len(train_dataloader))&#xA;    &#xA;    model.eval()&#xA;    val_loss = 0.0&#xA;    with torch.no_grad():&#xA;        for batch in eval_dataloader:&#xA;            # put batch on device&#xA;            batch = {k:v.to(device) for k,v in batch.items()}&#xA;            &#xA;            # forward pass&#xA;            outputs = model(**batch)&#xA;            loss = outputs.logits&#xA;            &#xA;            val_loss += loss.item()&#xA;                  &#xA;    print(&#34;Validation loss after epoch {epoch}:&#34;, val_loss/len(eval_dataloader))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.pytorchlightning.ai/&#34;&gt;PyTorch Lightning (PL)&lt;/a&gt;. PyTorch Lightning is a framework that automates the training loop written above, by abstracting it away in a Trainer object. Users don&#39;t need to write the training loop themselves anymore, instead they can just do &lt;code&gt;trainer = Trainer()&lt;/code&gt; and then &lt;code&gt;trainer.fit(model)&lt;/code&gt;. The advantage is that you can start training models very quickly (hence the name lightning), as all training-related code is handled by the &lt;code&gt;Trainer&lt;/code&gt; object. The disadvantage is that it may be more difficult to debug your model, as the training and evaluation is now abstracted away.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/transformers/main_classes/trainer.html&#34;&gt;HuggingFace Trainer&lt;/a&gt;. The HuggingFace Trainer API can be seen as a framework similar to PyTorch Lightning in the sense that it also abstracts the training away using a Trainer object. However, contrary to PyTorch Lightning, it is not meant not be a general framework. Rather, it is made especially for fine-tuning Transformer-based models available in the HuggingFace Transformers library. The Trainer also has an extension called &lt;code&gt;Seq2SeqTrainer&lt;/code&gt; for encoder-decoder models, such as BART, T5 and the &lt;code&gt;EncoderDecoderModel&lt;/code&gt; classes. Note that all &lt;a href=&#34;https://github.com/huggingface/transformers/tree/master/examples/pytorch&#34;&gt;PyTorch example scripts&lt;/a&gt; of the Transformers library make use of the Trainer.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/accelerate&#34;&gt;HuggingFace Accelerate&lt;/a&gt;: Accelerate is a new project, that is made for people who still want to write their own training loop (as shown above), but would like to make it work automatically irregardless of the hardware (i.e. multiple GPUs, TPU pods, mixed precision, etc.).&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>amanchadha/coursera-deep-learning-specialization</title>
    <updated>2022-06-01T02:12:48Z</updated>
    <id>tag:github.com,2022-06-01:/amanchadha/coursera-deep-learning-specialization</id>
    <link href="https://github.com/amanchadha/coursera-deep-learning-specialization" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Notes, programming assignments and quizzes from all courses within the Coursera Deep Learning specialization offered by deeplearning.ai: (i) Neural Networks and Deep Learning; (ii) Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization; (iii) Structuring Machine Learning Projects; (iv) Convolutional Neural Network…&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Deep Learning Specialization on Coursera (offered by deeplearning.ai)&lt;/h1&gt; &#xA;&lt;p&gt;Programming assignments and quizzes from all courses in the Coursera &lt;a href=&#34;https://www.coursera.org/specializations/deep-learning&#34;&gt;Deep Learning specialization&lt;/a&gt; offered by &lt;code&gt;deeplearning.ai&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Instructor: &lt;a href=&#34;http://www.andrewng.org/&#34;&gt;Andrew Ng&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Notes&lt;/h2&gt; &#xA;&lt;h3&gt;For detailed interview-ready notes on all courses in the Coursera Deep Learning specialization, refer &lt;a href=&#34;https://aman.ai/&#34;&gt;www.aman.ai&lt;/a&gt;.&lt;/h3&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;Run &lt;code&gt;setup.sh&lt;/code&gt; to (i) download a pre-trained VGG-19 dataset and (ii) extract the zip&#39;d pre-trained models and datasets that are needed for all the assignments.&lt;/p&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;p&gt;This repo contains my work for this specialization. The code base, quiz questions and diagrams are taken from the &lt;a href=&#34;https://www.coursera.org/specializations/deep-learning&#34;&gt;Deep Learning Specialization on Coursera&lt;/a&gt;, unless specified otherwise.&lt;/p&gt; &#xA;&lt;h2&gt;2021 Version&lt;/h2&gt; &#xA;&lt;p&gt;This specialization was updated in April 2021 to include developments in deep learning and programming frameworks, with the biggest change being shifting from TensorFlow 1 to TensorFlow 2. This repo has been updated accordingly as well.&lt;/p&gt; &#xA;&lt;h2&gt;Programming Assignments&lt;/h2&gt; &#xA;&lt;h3&gt;Course 1: Neural Networks and Deep Learning&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%202/Python%20Basics%20with%20Numpy/Python_Basics_With_Numpy_v3a.ipynb&#34;&gt;Week 2 - PA 1 - Python Basics with Numpy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%202/Logistic%20Regression%20as%20a%20Neural%20Network/Logistic_Regression_with_a_Neural_Network_mindset_v6a.ipynb&#34;&gt;Week 2 - PA 2 - Logistic Regression with a Neural Network mindset&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%203/Planar%20data%20classification%20with%20one%20hidden%20layer/Planar_data_classification_with_onehidden_layer_v6c.ipynb&#34;&gt;Week 3 - PA 3 - Planar data classification with one hidden layer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%204/Building%20your%20Deep%20Neural%20Network%20-%20Step%20by%20Step/Building_your_Deep_Neural_Network_Step_by_Step_v8a.ipynb&#34;&gt;Week 4 - PA 4 - Building your Deep Neural Network: Step by Step&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%204/Deep%20Neural%20Network%20Application_%20Image%20Classification/Deep%20Neural%20Network%20-%20Application%20v8.ipynb&#34;&gt;Week 4 - PA 5 - Deep Neural Network for Image Classification: Application&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Course 2: Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%201/Initialization/Initialization.ipynb&#34;&gt;Week 1 - PA 1 - Initialization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%201/Regularization/Regularization_v2a.ipynb&#34;&gt;Week 1 - PA 2 - Regularization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%201/Gradient%20Checking/Gradient%20Checking%20v1.ipynb&#34;&gt;Week 1 - PA 3 - Gradient Checking&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%202/Optimization_methods_v1b.ipynb&#34;&gt;Week 2 - PA 4 - Optimization Methods&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%203/Tensorflow_introduction.ipynb&#34;&gt;Week 3 - PA 5 - TensorFlow Tutorial&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Course 3: Structuring Machine Learning Projects&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;There are no programming assignments for this course. But this course comes with very interesting case study quizzes (below).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Course 4: Convolutional Neural Networks&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%201/Convolution_model_Step_by_Step_v1.ipynb&#34;&gt;Week 1 - PA 1 - Convolutional Model: step by step&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%201/Convolution_model_Application.ipynb&#34;&gt;Week 1 - PA 2 - Convolutional Neural Networks: Application&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%202/KerasTutorial/Keras%20-%20Tutorial%20-%20Happy%20House%20v2.ipynb&#34;&gt;Week 2 - PA 1 - Keras - Tutorial - Happy House&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%202/ResNets/Residual_Networks.ipynb&#34;&gt;Week 2 - PA 2 - Residual Networks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%202/Transfer%20Learning%20with%20MobileNet/Transfer_learning_with_MobileNet_v1.ipynb&#34;&gt;Week 2 - PA 2 - Transfer Learning with MobileNet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%203/Car%20detection%20for%20Autonomous%20Driving/Autonomous_driving_application_Car_detection.ipynb&#34;&gt;Week 3 - PA 1 - Car detection with YOLO for Autonomous Driving&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%203/Image%20Segmentation%20Unet/Image_segmentation_Unet_v2.ipynb&#34;&gt;Week 3 - PA 2 - Image Segmentation Unet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%204/Neural%20Style%20Transfer/Art_Generation_with_Neural_Style_Transfer.ipynb&#34;&gt;Week 4 - PA 1 - Art Generation with Neural Style Transfer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%204/Face%20Recognition/Face_Recognition.ipynb&#34;&gt;Week 4 - PA 2 - Face Recognition&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Course 5: Sequence Models&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%201/Building%20a%20Recurrent%20Neural%20Network%20-%20Step%20by%20Step/Building_a_Recurrent_Neural_Network_Step_by_Step.ipynb&#34;&gt;Week 1 - PA 1 - Building a Recurrent Neural Network - Step by Step&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%201/Dinosaur%20Island%20--%20Character-level%20language%20model/Dinosaurus_Island_Character_level_language_model.ipynb&#34;&gt;Week 1 - PA 2 - Dinosaur Land -- Character-level Language Modeling&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%201/Jazz%20improvisation%20with%20LSTM/Improvise_a_Jazz_Solo_with_an_LSTM_Network_v4_Solution.ipynb&#34;&gt;Week 1 - PA 3 - Jazz improvisation with LSTM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%202/Word%20Vector%20Representation/Operations_on_word_vectors_v2a.ipynb&#34;&gt;Week 2 - PA 1 - Word Vector Representation and Debiasing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%202/Emojify/Emoji_v3a.ipynb&#34;&gt;Week 2 - PA 2 - Emojify!&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%203/Machine%20Translation/Neural_machine_translation_with_attention_v4a.ipynb&#34;&gt;Week 3 - PA 1 - Neural Machine Translation with Attention&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%203/Trigger%20word%20detection/Trigger_word_detection_v2a.ipynb&#34;&gt;Week 3 - PA 2 - Trigger Word Detection&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%204/Transformer%20Subclass/C5_W4_A1_Transformer_Subclass_v1.ipynb&#34;&gt;Week 4 - PA 1 - Transformer Network&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%203/Named%20Entity%20Recognition/Transformer_application_Named_Entity_Recognition.ipynb&#34;&gt;Week 3 - PA 2 - Transformer Network Application: Named-Entity Recognition&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%203/Question%20Answering/QA_transformer.ipynb&#34;&gt;Week 3 - PA 2 - Transformer Network Application: Question Answering&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quiz Solutions&lt;/h2&gt; &#xA;&lt;h3&gt;Course 1: Neural Networks and Deep Learning&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Week 1 Quiz - Introduction to deep learning: &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%201/Week%201%20Quiz%20-%20Introduction%20to%20deep%20learning.md&#34;&gt;Text&lt;/a&gt; | &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%201/Week%201%20Quiz%20-%20Introduction%20to%20deep%20learning.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Week 2 Quiz - Neural Network Basics: &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%202/Week%202%20Quiz%20-%20Neural%20Network%20Basics.md&#34;&gt;Text&lt;/a&gt; | &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%202/Week%202%20Quiz%20-%20Neural%20Network%20Basics.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Week 3 Quiz - Shallow Neural Networks: &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%203/Week%203%20Quiz%20-%20Shallow%20Neural%20Networks.md&#34;&gt;Text&lt;/a&gt; | &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%203/Week%203%20Quiz%20-%20Shallow%20Neural%20Networks.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Week 4 Quiz - Key concepts on Deep Neural Networks: &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%204/Week%204%20Quiz%20-%20Key%20concepts%20on%20Deep%20Neural%20Networks.md&#34;&gt;Text&lt;/a&gt; | &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%204/Week%204%20Quiz%20-%20Key%20concepts%20on%20Deep%20Neural%20Networks.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Course 2: Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Week 1 Quiz - Practical aspects of deep learning: &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%201/Week%201%20Quiz%20-%20Practical%20aspects%20of%20deep%20learning.md&#34;&gt;Text&lt;/a&gt; | &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%201/Week%201%20Quiz%20-%20Practical%20aspects%20of%20deep%20learning.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Week 2 Quiz - Optimization algorithms: &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%202/Week%202%20Quiz%20-%20Optimization%20algorithms.md&#34;&gt;Text&lt;/a&gt; | &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%202/Week%202%20Quiz%20-%20Optimization%20algorithms.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Week 3 Quiz - Hyperparameter tuning, Batch Normalization, Programming Frameworks: &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%203/Week%203%20Quiz%20-%20Hyperparameter%20tuning%2C%20Batch%20Normalization%2C%20Programming%20Frameworks.md&#34;&gt;Text&lt;/a&gt; | &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%203/Week%203%20Quiz%20-%20Hyperparameter%20tuning%2C%20Batch%20Normalization%2C%20Programming%20Frameworks.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Course 3: Structuring Machine Learning Projects&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Week 1 Quiz - Bird recognition in the city of Peacetopia (case study): &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C3%20-%20Structuring%20Machine%20Learning%20Projects/Week%201%20Quiz%20-%20Bird%20recognition%20in%20the%20city%20of%20Peacetopia%20(case%20study).md&#34;&gt;Text&lt;/a&gt; | &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C3%20-%20Structuring%20Machine%20Learning%20Projects/Week%201%20Quiz%20-%20Bird%20recognition%20in%20the%20city%20of%20Peacetopia%20(case%20study).pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Week 2 Quiz - Autonomous driving (case study): &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C3%20-%20Structuring%20Machine%20Learning%20Projects/Week%202%20Quiz%20-%20Autonomous%20driving%20(case%20study).md&#34;&gt;Text&lt;/a&gt; | &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C3%20-%20Structuring%20Machine%20Learning%20Projects/Week%202%20Quiz%20-%20Autonomous%20driving%20(case%20study).pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Course 4: Convolutional Neural Networks&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Week 1 Quiz - The basics of ConvNets: &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%201/Week%201%20Quiz%20-%20The%20basics%20of%20ConvNets.md&#34;&gt;Text&lt;/a&gt; | &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%201/Week%201%20Quiz%20-%20The%20basics%20of%20ConvNets.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Week 2 Quiz - Deep convolutional models: &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%202/Week%202%20Quiz%20-%20Deep%20convolutional%20models.md&#34;&gt;Text&lt;/a&gt; | &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%202/Week%202%20Quiz%20-%20Deep%20convolutional%20models.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Week 3 Quiz - Detection algorithms: &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%203/Week%203%20Quiz%20-%20Detection%20algorithms.md&#34;&gt;Text&lt;/a&gt; | &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%203/Week%203%20Quiz%20-%20Detection%20algorithms.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Week 4 Quiz - Special applications: Face recognition &amp;amp; Neural style transfer: &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%204/Week%204%20Quiz%20-%20Special%20applications%20Face%20Recognition%20and%20Neural%20Style%20Transfer.md&#34;&gt;Text&lt;/a&gt; | &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%204/Week%204%20Quiz%20-%20Special%20applications%20Face%20Recognition%20and%20Neural%20Style%20Transfer.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Course 5: Sequence Models&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Week 1 Quiz - Recurrent Neural Networks: &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%201/Week%201%20Quiz%20-%20Recurrent%20Neural%20Networks.md&#34;&gt;Text&lt;/a&gt; | &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%201/Week%201%20Quiz%20-%20Recurrent%20Neural%20Networks.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Week 2 Quiz - Natural Language Processing &amp;amp; Word Embeddings: &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%202/Week%202%20Quiz%20-%20Natural%20Language%20Processing%20%26%20Word%20Embeddings.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Week 3 Quiz - Sequence models &amp;amp; Attention mechanism: &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%203/Week%203%20Quiz%20-%20Sequence%20models%20%26%20Attention%20mechanisms.md&#34;&gt;Text&lt;/a&gt; | &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%203/Week%203%20Quiz%20-%20Sequence%20models%20%26%20Attention%20mechanisms.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;I recognize the time people spend on building intuition, understanding new concepts and debugging assignments. The solutions uploaded here are &lt;strong&gt;only for reference&lt;/strong&gt;. They are meant to unblock you if you get stuck somewhere. Please do not copy any part of the code as-is (the programming assignments are fairly easy if you read the instructions carefully). Similarly, try out the quizzes yourself before you refer to the quiz solutions. This course is the most straight-forward deep learning course I have ever taken, with fabulous course content and structure. It&#39;s a treasure by the deeplearning.ai team.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>saic-mdal/lama</title>
    <updated>2022-06-01T02:12:48Z</updated>
    <id>tag:github.com,2022-06-01:/saic-mdal/lama</id>
    <link href="https://github.com/saic-mdal/lama" rel="alternate"></link>
    <summary type="html">&lt;p&gt;🦙 LaMa Image Inpainting, Resolution-robust Large Mask Inpainting with Fourier Convolutions, WACV 2022&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;🦙 LaMa: Resolution-robust Large Mask Inpainting with Fourier Convolutions&lt;/h1&gt; &#xA;&lt;p&gt;Official implementation by Samsung Research&lt;/p&gt; &#xA;&lt;p&gt;by Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, Victor Lempitsky.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34; &#34;font-size:30px;&#34;&gt; 🔥🔥🔥 &lt;br&gt; &lt;b&gt; LaMa generalizes surprisingly well to much higher resolutions (~2k❗️) than it saw during training (256x256), and achieves the excellent performance even in challenging scenarios, e.g. completion of periodic structures.&lt;/b&gt; &lt;/p&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://saic-mdal.github.io/lama-project/&#34;&gt;Project page&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/2109.07161&#34;&gt;arXiv&lt;/a&gt;] [&lt;a href=&#34;https://ashukha.com/projects/lama_21/lama_supmat_2021.pdf&#34;&gt;Supplementary&lt;/a&gt;] [&lt;a href=&#34;https://senya-ashukha.github.io/projects/lama_21/paper.txt&#34;&gt;BibTeX&lt;/a&gt;] [&lt;a href=&#34;https://www.casualganpapers.com/large-masks-fourier-convolutions-inpainting/LaMa-explained.html&#34;&gt;Casual GAN Papers Summary&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://colab.research.google.com/github/saic-mdal/lama/blob/master//colab/LaMa_inpainting.ipynb&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;br&gt; Try out in Google Colab &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/senya-ashukha/senya-ashukha.github.io/master/projects/lama_21/ezgif-4-0db51df695a8.gif&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/senya-ashukha/senya-ashukha.github.io/master/projects/lama_21/gif_for_lightning_v1_white.gif&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Non-official 3rd party apps:&lt;/h1&gt; &#xA;&lt;p&gt;(Feel free to share your app/implementation/demo by creating an issue)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cleanup.pictures/&#34;&gt;https://cleanup.pictures&lt;/a&gt; - a simple interactive object removal tool by &lt;a href=&#34;https://twitter.com/cyrildiagne&#34;&gt;@cyrildiagne&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/Sanster/lama-cleaner&#34;&gt;lama-cleaner&lt;/a&gt; by &lt;a href=&#34;https://github.com/Sanster/lama-cleaner&#34;&gt;@Sanster&lt;/a&gt; is a self-host version of &lt;a href=&#34;https://cleanup.pictures/&#34;&gt;https://cleanup.pictures&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Integrated to &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces&lt;/a&gt; with &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;. See demo: &lt;a href=&#34;https://huggingface.co/spaces/akhaliq/lama&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt; by &lt;a href=&#34;https://github.com/AK391&#34;&gt;@AK391&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Telegram bot &lt;a href=&#34;https://t.me/MagicEraserBot&#34;&gt;@MagicEraserBot&lt;/a&gt; by &lt;a href=&#34;https://github.com/Moldoteck&#34;&gt;@Moldoteck&lt;/a&gt;, &lt;a href=&#34;https://github.com/Moldoteck/MagicEraser&#34;&gt;code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/andy971022/auto-lama&#34;&gt;Auto-LaMa&lt;/a&gt; = DE:TR object detection + LaMa inpainting by &lt;a href=&#34;https://github.com/andy971022&#34;&gt;@andy971022&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zhaoyun0071/LAMA-Magic-Eraser-Local&#34;&gt;LAMA-Magic-Eraser-Local&lt;/a&gt; = a standalone inpainting application built with PyQt5 by &lt;a href=&#34;https://github.com/zhaoyun0071&#34;&gt;@zhaoyun0071&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.hama.app/&#34;&gt;Hama&lt;/a&gt; - object removal with a smart brush which simplifies mask drawing.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Environment setup&lt;/h1&gt; &#xA;&lt;p&gt;Clone the repo: &lt;code&gt;git clone https://github.com/saic-mdal/lama.git&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;There are three options of an environment:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Python virtualenv:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;virtualenv inpenv --python=/usr/bin/python3&#xA;source inpenv/bin/activate&#xA;pip install torch==1.8.0 torchvision==0.9.0&#xA;&#xA;cd lama&#xA;pip install -r requirements.txt &#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Conda&lt;/p&gt; &lt;pre&gt;&lt;code&gt;% Install conda for Linux, for other OS download miniconda at https://docs.conda.io/en/latest/miniconda.html&#xA;wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh&#xA;bash Miniconda3-latest-Linux-x86_64.sh -b -p $HOME/miniconda&#xA;$HOME/miniconda/bin/conda init bash&#xA;&#xA;cd lama&#xA;conda env create -f conda_env.yml&#xA;conda activate lama&#xA;conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch -y&#xA;pip install pytorch-lightning==1.2.9&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Docker: No actions are needed 🎉.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Inference &lt;a name=&#34;prediction&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;Run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd lama&#xA;export TORCH_HOME=$(pwd) &amp;amp;&amp;amp; export PYTHONPATH=$(pwd)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;1. Download pre-trained models&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Install tool for yandex disk link extraction:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip3 install wldhx.yadisk-direct&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The best model (Places2, Places Challenge):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -L $(yadisk-direct https://disk.yandex.ru/d/ouP6l8VJ0HpMZg) -o big-lama.zip&#xA;unzip big-lama.zip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;All models (Places &amp;amp; CelebA-HQ):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -L $(yadisk-direct https://disk.yandex.ru/d/EgqaSnLohjuzAg) -o lama-models.zip&#xA;unzip lama-models.zip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;2. Prepare images and masks&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Download test images:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -L $(yadisk-direct https://disk.yandex.ru/d/xKQJZeVRk5vLlQ) -o LaMa_test_images.zip&#xA;unzip LaMa_test_images.zip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;OR prepare your data:&lt;/summary&gt; 1) Create masks named as `[images_name]_maskXXX[image_suffix]`, put images and masks in the same folder. &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;You can use the &lt;a href=&#34;https://github.com/saic-mdal/lama/raw/main/bin/gen_mask_dataset.py&#34;&gt;script&lt;/a&gt; for random masks generation.&lt;/li&gt; &#xA;  &lt;li&gt;Check the format of the files: &lt;pre&gt;&lt;code&gt;image1_mask001.png&#xA;image1.png&#xA;image2_mask001.png&#xA;image2.png&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;Specify &lt;code&gt;image_suffix&lt;/code&gt;, e.g. &lt;code&gt;.png&lt;/code&gt; or &lt;code&gt;.jpg&lt;/code&gt; or &lt;code&gt;_input.jpg&lt;/code&gt; in &lt;code&gt;configs/prediction/default.yaml&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;&lt;strong&gt;3. Predict&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;On the host machine:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 bin/predict.py model.path=$(pwd)/big-lama indir=$(pwd)/LaMa_test_images outdir=$(pwd)/output&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;OR&lt;/strong&gt; in the docker&lt;/p&gt; &#xA;&lt;p&gt;The following command will pull the docker image from Docker Hub and execute the prediction script&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash docker/2_predict.sh $(pwd)/big-lama $(pwd)/LaMa_test_images $(pwd)/output device=cpu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Docker cuda: TODO&lt;/p&gt; &#xA;&lt;h1&gt;Train and Eval&lt;/h1&gt; &#xA;&lt;p&gt;⚠️ Warning: The training is not fully tested yet, e.g., did not re-training after refactoring ⚠️&lt;/p&gt; &#xA;&lt;p&gt;Make sure you run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd lama&#xA;export TORCH_HOME=$(pwd) &amp;amp;&amp;amp; export PYTHONPATH=$(pwd)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then download models for &lt;em&gt;perceptual loss&lt;/em&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mkdir -p ade20k/ade20k-resnet50dilated-ppm_deepsup/&#xA;wget -P ade20k/ade20k-resnet50dilated-ppm_deepsup/ http://sceneparsing.csail.mit.edu/model/pytorch/ade20k-resnet50dilated-ppm_deepsup/encoder_epoch_20.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Places&lt;/h2&gt; &#xA;&lt;p&gt;⚠️ NB: FID/SSIM/LPIPS metric values for Places that we see in LaMa paper are computed on 30000 images that we produce in evaluation section below. For more details on evaluation data check [&lt;a href=&#34;https://ashukha.com/projects/lama_21/lama_supmat_2021.pdf#subsection.3.1&#34;&gt;Section 3. Dataset splits in Supplementary&lt;/a&gt;] ⚠️&lt;/p&gt; &#xA;&lt;p&gt;On the host machine:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Download data from http://places2.csail.mit.edu/download.html&#xA;# Places365-Standard: Train(105GB)/Test(19GB)/Val(2.1GB) from High-resolution images section&#xA;wget http://data.csail.mit.edu/places/places365/train_large_places365standard.tar&#xA;wget http://data.csail.mit.edu/places/places365/val_large.tar&#xA;wget http://data.csail.mit.edu/places/places365/test_large.tar&#xA;&#xA;# Unpack train/test/val data and create .yaml config for it&#xA;bash fetch_data/places_standard_train_prepare.sh&#xA;bash fetch_data/places_standard_test_val_prepare.sh&#xA;&#xA;# Sample images for test and viz at the end of epoch&#xA;bash fetch_data/places_standard_test_val_sample.sh&#xA;bash fetch_data/places_standard_test_val_gen_masks.sh&#xA;&#xA;# Run training&#xA;python3 bin/train.py -cn lama-fourier location=places_standard&#xA;&#xA;# To evaluate trained model and report metrics as in our paper&#xA;# we need to sample previously unseen 30k images and generate masks for them&#xA;bash fetch_data/places_standard_evaluation_prepare_data.sh&#xA;&#xA;# Infer model on thick/thin/medium masks in 256 and 512 and run evaluation &#xA;# like this:&#xA;python3 bin/predict.py \&#xA;model.path=$(pwd)/experiments/&amp;lt;user&amp;gt;_&amp;lt;date:time&amp;gt;_lama-fourier_/ \&#xA;indir=$(pwd)/places_standard_dataset/evaluation/random_thick_512/ \&#xA;outdir=$(pwd)/inference/random_thick_512 model.checkpoint=last.ckpt&#xA;&#xA;python3 bin/evaluate_predicts.py \&#xA;$(pwd)/configs/eval2_gpu.yaml \&#xA;$(pwd)/places_standard_dataset/evaluation/random_thick_512/ \&#xA;$(pwd)/inference/random_thick_512 \&#xA;$(pwd)/inference/random_thick_512_metrics.csv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Docker: TODO&lt;/p&gt; &#xA;&lt;h2&gt;CelebA&lt;/h2&gt; &#xA;&lt;p&gt;On the host machine:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Make shure you are in lama folder&#xA;cd lama&#xA;export TORCH_HOME=$(pwd) &amp;amp;&amp;amp; export PYTHONPATH=$(pwd)&#xA;&#xA;# Download CelebA-HQ dataset&#xA;# Download data256x256.zip from https://drive.google.com/drive/folders/11Vz0fqHS2rXDb5pprgTjpD7S2BAJhi1P&#xA;&#xA;# unzip &amp;amp; split into train/test/visualization &amp;amp; create config for it&#xA;bash fetch_data/celebahq_dataset_prepare.sh&#xA;&#xA;# generate masks for test and visual_test at the end of epoch&#xA;bash fetch_data/celebahq_gen_masks.sh&#xA;&#xA;# Run training&#xA;python3 bin/train.py -cn lama-fourier-celeba data.batch_size=10&#xA;&#xA;# Infer model on thick/thin/medium masks in 256 and run evaluation &#xA;# like this:&#xA;python3 bin/predict.py \&#xA;model.path=$(pwd)/experiments/&amp;lt;user&amp;gt;_&amp;lt;date:time&amp;gt;_lama-fourier-celeba_/ \&#xA;indir=$(pwd)/celeba-hq-dataset/visual_test_256/random_thick_256/ \&#xA;outdir=$(pwd)/inference/celeba_random_thick_256 model.checkpoint=last.ckpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Docker: TODO&lt;/p&gt; &#xA;&lt;h2&gt;Places Challenge&lt;/h2&gt; &#xA;&lt;p&gt;On the host machine:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# This script downloads multiple .tar files in parallel and unpacks them&#xA;# Places365-Challenge: Train(476GB) from High-resolution images (to train Big-Lama) &#xA;bash places_challenge_train_download.sh&#xA;&#xA;TODO: prepare&#xA;TODO: train &#xA;TODO: eval&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Docker: TODO&lt;/p&gt; &#xA;&lt;h2&gt;Create your data&lt;/h2&gt; &#xA;&lt;p&gt;Please check bash scripts for data preparation and mask generation from CelebaHQ section, if you stuck at one of the following steps.&lt;/p&gt; &#xA;&lt;p&gt;On the host machine:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Make shure you are in lama folder&#xA;cd lama&#xA;export TORCH_HOME=$(pwd) &amp;amp;&amp;amp; export PYTHONPATH=$(pwd)&#xA;&#xA;# You need to prepare following image folders:&#xA;$ ls my_dataset&#xA;train&#xA;val_source # 2000 or more images&#xA;visual_test_source # 100 or more images&#xA;eval_source # 2000 or more images&#xA;&#xA;# LaMa generates random masks for the train data on the flight,&#xA;# but needs fixed masks for test and visual_test for consistency of evaluation.&#xA;&#xA;# Suppose, we want to evaluate and pick best models &#xA;# on 512x512 val dataset  with thick/thin/medium masks &#xA;# And your images have .jpg extention:&#xA;&#xA;python3 bin/gen_mask_dataset.py \&#xA;$(pwd)/configs/data_gen/random_&amp;lt;size&amp;gt;_512.yaml \ # thick, thin, medium&#xA;my_dataset/val_source/ \&#xA;my_dataset/val/random_&amp;lt;size&amp;gt;_512.yaml \# thick, thin, medium&#xA;--ext jpg&#xA;&#xA;# So the mask generator will: &#xA;# 1. resize and crop val images and save them as .png&#xA;# 2. generate masks&#xA;&#xA;ls my_dataset/val/random_medium_512/&#xA;image1_crop000_mask000.png&#xA;image1_crop000.png&#xA;image2_crop000_mask000.png&#xA;image2_crop000.png&#xA;...&#xA;&#xA;# Generate thick, thin, medium masks for visual_test folder:&#xA;&#xA;python3 bin/gen_mask_dataset.py \&#xA;$(pwd)/configs/data_gen/random_&amp;lt;size&amp;gt;_512.yaml \  #thick, thin, medium&#xA;my_dataset/visual_test_source/ \&#xA;my_dataset/visual_test/random_&amp;lt;size&amp;gt;_512/ \ #thick, thin, medium&#xA;--ext jpg&#xA;&#xA;&#xA;ls my_dataset/visual_test/random_thick_512/&#xA;image1_crop000_mask000.png&#xA;image1_crop000.png&#xA;image2_crop000_mask000.png&#xA;image2_crop000.png&#xA;...&#xA;&#xA;# Same process for eval_source image folder:&#xA;&#xA;python3 bin/gen_mask_dataset.py \&#xA;$(pwd)/configs/data_gen/random_&amp;lt;size&amp;gt;_512.yaml \  #thick, thin, medium&#xA;my_dataset/eval_source/ \&#xA;my_dataset/eval/random_&amp;lt;size&amp;gt;_512/ \ #thick, thin, medium&#xA;--ext jpg&#xA;&#xA;&#xA;&#xA;# Generate location config file which locate these folders:&#xA;&#xA;touch my_dataset.yaml&#xA;echo &#34;data_root_dir: $(pwd)/my_dataset/&#34; &amp;gt;&amp;gt; my_dataset.yaml&#xA;echo &#34;out_root_dir: $(pwd)/experiments/&#34; &amp;gt;&amp;gt; my_dataset.yaml&#xA;echo &#34;tb_dir: $(pwd)/tb_logs/&#34; &amp;gt;&amp;gt; my_dataset.yaml&#xA;mv my_dataset.yaml ${PWD}/configs/training/location/&#xA;&#xA;&#xA;# Check data config for consistency with my_dataset folder structure:&#xA;$ cat ${PWD}/configs/training/data/abl-04-256-mh-dist&#xA;...&#xA;train:&#xA;  indir: ${location.data_root_dir}/train&#xA;  ...&#xA;val:&#xA;  indir: ${location.data_root_dir}/val&#xA;  img_suffix: .png&#xA;visual_test:&#xA;  indir: ${location.data_root_dir}/visual_test&#xA;  img_suffix: .png&#xA;&#xA;&#xA;# Run training&#xA;python3 bin/train.py -cn lama-fourier location=my_dataset data.batch_size=10&#xA;&#xA;# Evaluation: LaMa training procedure picks best few models according to &#xA;# scores on my_dataset/val/ &#xA;&#xA;# To evaluate one of your best models (i.e. at epoch=32) &#xA;# on previously unseen my_dataset/eval do the following &#xA;# for thin, thick and medium:&#xA;&#xA;# infer:&#xA;python3 bin/predict.py \&#xA;model.path=$(pwd)/experiments/&amp;lt;user&amp;gt;_&amp;lt;date:time&amp;gt;_lama-fourier_/ \&#xA;indir=$(pwd)/my_dataset/eval/random_&amp;lt;size&amp;gt;_512/ \&#xA;outdir=$(pwd)/inference/my_dataset/random_&amp;lt;size&amp;gt;_512 \&#xA;model.checkpoint=epoch32.ckpt&#xA;&#xA;# metrics calculation:&#xA;python3 bin/evaluate_predicts.py \&#xA;$(pwd)/configs/eval2_gpu.yaml \&#xA;$(pwd)/my_dataset/eval/random_&amp;lt;size&amp;gt;_512/ \&#xA;$(pwd)/inference/my_dataset/random_&amp;lt;size&amp;gt;_512 \&#xA;$(pwd)/inference/my_dataset/random_&amp;lt;size&amp;gt;_512_metrics.csv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;OR&lt;/strong&gt; in the docker:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;TODO: train&#xA;TODO: eval&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Hints&lt;/h1&gt; &#xA;&lt;h3&gt;Generate different kinds of masks&lt;/h3&gt; &#xA;&lt;p&gt;The following command will execute a script that generates random masks.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash docker/1_generate_masks_from_raw_images.sh \&#xA;    configs/data_gen/random_medium_512.yaml \&#xA;    /directory_with_input_images \&#xA;    /directory_where_to_store_images_and_masks \&#xA;    --ext png&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The test data generation command stores images in the format, which is suitable for &lt;a href=&#34;https://raw.githubusercontent.com/saic-mdal/lama/main/#prediction&#34;&gt;prediction&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The table below describes which configs we used to generate different test sets from the paper. Note that we &lt;em&gt;do not fix a random seed&lt;/em&gt;, so the results will be slightly different each time.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Places 512x512&lt;/th&gt; &#xA;   &lt;th&gt;CelebA 256x256&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Narrow&lt;/td&gt; &#xA;   &lt;td&gt;random_thin_512.yaml&lt;/td&gt; &#xA;   &lt;td&gt;random_thin_256.yaml&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Medium&lt;/td&gt; &#xA;   &lt;td&gt;random_medium_512.yaml&lt;/td&gt; &#xA;   &lt;td&gt;random_medium_256.yaml&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Wide&lt;/td&gt; &#xA;   &lt;td&gt;random_thick_512.yaml&lt;/td&gt; &#xA;   &lt;td&gt;random_thick_256.yaml&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Feel free to change the config path (argument #1) to any other config in &lt;code&gt;configs/data_gen&lt;/code&gt; or adjust config files themselves.&lt;/p&gt; &#xA;&lt;h3&gt;Override parameters in configs&lt;/h3&gt; &#xA;&lt;p&gt;Also you can override parameters in config like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 bin/train.py -cn &amp;lt;config&amp;gt; data.batch_size=10 run_title=my-title&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Where .yaml file extension is omitted&lt;/p&gt; &#xA;&lt;h3&gt;Models options&lt;/h3&gt; &#xA;&lt;p&gt;Config names for models from paper (substitude into the training command):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;* big-lama&#xA;* big-lama-regular&#xA;* lama-fourier&#xA;* lama-regular&#xA;* lama_small_train_masks&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Which are seated in configs/training/folder&lt;/p&gt; &#xA;&lt;h3&gt;Links&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;All the data (models, test images, etc.) &lt;a href=&#34;https://disk.yandex.ru/d/AmdeG-bIjmvSug&#34;&gt;https://disk.yandex.ru/d/AmdeG-bIjmvSug&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Test images from the paper &lt;a href=&#34;https://disk.yandex.ru/d/xKQJZeVRk5vLlQ&#34;&gt;https://disk.yandex.ru/d/xKQJZeVRk5vLlQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;The pre-trained models &lt;a href=&#34;https://disk.yandex.ru/d/EgqaSnLohjuzAg&#34;&gt;https://disk.yandex.ru/d/EgqaSnLohjuzAg&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;The models for perceptual loss &lt;a href=&#34;https://disk.yandex.ru/d/ncVmQlmT_kTemQ&#34;&gt;https://disk.yandex.ru/d/ncVmQlmT_kTemQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Our training logs are available at &lt;a href=&#34;https://disk.yandex.ru/d/9Bt1wNSDS4jDkQ&#34;&gt;https://disk.yandex.ru/d/9Bt1wNSDS4jDkQ&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Training time &amp;amp; resources&lt;/h3&gt; &#xA;&lt;p&gt;TODO&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Segmentation code and models if form &lt;a href=&#34;https://github.com/CSAILVision/semantic-segmentation-pytorch&#34;&gt;CSAILVision&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;LPIPS metric is from &lt;a href=&#34;https://github.com/richzhang/PerceptualSimilarity&#34;&gt;richzhang&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;SSIM is from &lt;a href=&#34;https://github.com/Po-Hsun-Su/pytorch-ssim&#34;&gt;Po-Hsun-Su&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;FID is from &lt;a href=&#34;https://github.com/mseitzer/pytorch-fid&#34;&gt;mseitzer&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you found this code helpful, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{suvorov2021resolution,&#xA;  title={Resolution-robust Large Mask Inpainting with Fourier Convolutions},&#xA;  author={Suvorov, Roman and Logacheva, Elizaveta and Mashikhin, Anton and Remizova, Anastasia and Ashukha, Arsenii and Silvestrov, Aleksei and Kong, Naejin and Goka, Harshith and Park, Kiwoong and Lempitsky, Victor},&#xA;  journal={arXiv preprint arXiv:2109.07161},&#xA;  year={2021}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;div style=&#34;text-align:center&#34; align=&#34;center&#34;&gt; &#xA; &lt;br&gt; &#xA; &lt;br&gt; &#xA; &lt;img loading=&#34;lazy&#34; height=&#34;50px&#34; src=&#34;https://raw.githubusercontent.com/saic-mdal/lama-project/main/docs/img/samsung_ai.png&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;p style=&#34;font-weight:normal; font-size: 16pt;text-align:center&#34; align=&#34;center&#34;&gt;Copyright © 2021&lt;/p&gt; &#xA;&lt;br&gt;</summary>
  </entry>
  <entry>
    <title>ine-rmotr-curriculum/FreeCodeCamp-Pandas-Real-Life-Example</title>
    <updated>2022-06-01T02:12:48Z</updated>
    <id>tag:github.com,2022-06-01:/ine-rmotr-curriculum/FreeCodeCamp-Pandas-Real-Life-Example</id>
    <link href="https://github.com/ine-rmotr-curriculum/FreeCodeCamp-Pandas-Real-Life-Example" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h3&gt;rmotr.com&lt;/h3&gt; &#xA;&lt;h1&gt;Data Science with Python Course&lt;/h1&gt; &#xA;&lt;p&gt;This material is created for our &lt;a href=&#34;https://rmotr.com/data-science-python-course&#34;&gt;Data Science with Python Course&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>gedeck/practical-statistics-for-data-scientists</title>
    <updated>2022-06-01T02:12:48Z</updated>
    <id>tag:github.com,2022-06-01:/gedeck/practical-statistics-for-data-scientists</id>
    <link href="https://github.com/gedeck/practical-statistics-for-data-scientists" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code repository for O&#39;Reilly book&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://github.com/gedeck/dmba/actions/workflows/build.yml/badge.svg?sanitize=true&#34; alt=&#34;Python&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Code repository&lt;/h1&gt; &#xA;&lt;table width=&#34;100%&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gedeck/practical-statistics-for-data-scientists/master/images/OReilly-english.jpg&#34; width=&#34;300&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;Practical Statistics for Data Scientists:&lt;/b&gt; &lt;p&gt;50+ Essential Concepts Using R and Python&lt;/p&gt; &lt;p&gt;by Peter Bruce, Andrew Bruce, and &lt;a href=&#34;https://www.amazon.com/Peter-Gedeck/e/B082BJZJKX/&#34;&gt;Peter Gedeck&lt;/a&gt;&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Publisher: &lt;a href=&#34;https://oreil.ly/practicalStats_dataSci_2e&#34;&gt;O&#39;Reilly Media&lt;/a&gt;; 2 edition (June 9, 2020)&lt;/li&gt; &#xA;     &lt;li&gt;ISBN-13: 978-1492072942&lt;/li&gt; &#xA;     &lt;li&gt;Buy on &lt;a href=&#34;https://www.amazon.com/Practical-Statistics-Data-Scientists-Essential/dp/149207294X&#34;&gt;Amazon&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Errata: &lt;a href=&#34;http://oreilly.com/catalog/errata.csp?isbn=9781492072942&#34;&gt;http://oreilly.com/catalog/errata.csp?isbn=9781492072942&lt;/a&gt; &lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt;   &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Online&lt;/h2&gt; &#xA;&lt;p&gt;View the notebooks online: &lt;a href=&#34;https://nbviewer.jupyter.org/github/gedeck/practical-statistics-for-data-scientists/tree/master/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg?sanitize=true&#34; alt=&#34;nbviewer&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Excecute the notebooks in Binder: &lt;a href=&#34;https://mybinder.org/v2/gh/gedeck/practical-statistics-for-data-scientists/HEAD&#34;&gt;&lt;img src=&#34;https://mybinder.org/badge_logo.svg?sanitize=true&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This can take some time if the binder environment needs to be rebuilt.&lt;/p&gt; &#xA;&lt;h2&gt;Other language versions&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gedeck/practical-statistics-for-data-scientists/master/images/OReilly-english.jpg&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;English:&lt;/b&gt;&lt;br&gt; Practical Statistics for Data Scientists: 50+ Essential Concepts Using R and Python&lt;br&gt; 2020: ISBN 149207294X&lt;br&gt; &lt;a href=&#34;https://www.google.com/books/edition/Practical_Statistics_for_Data_Scientists/F2bcDwAAQBAJ?hl=en&#34;&gt;Google books&lt;/a&gt;, &lt;a href=&#34;https://www.amazon.com/Practical-Statistics-Data-Scientists-Essential/dp/149207294X&#34;&gt;Amazon&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gedeck/practical-statistics-for-data-scientists/master/images/OReilly-japanese.jpg&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;Japanese (2020-06-11):&lt;/b&gt;&lt;br&gt; データサイエンスのための統計学入門 第2版 ―予測、分類、統計モデリング、統計的機械学習とR/Pythonプログラミング &lt;br&gt; 2020: ISBN 978-4-873-11926-7, Shinya Ohashi (supervised), Toshiaki Kurokawa (translated), O&#39;Reilly Japan Inc.&lt;br&gt; &lt;a href=&#34;https://www.google.com/books/edition/%E3%83%87%E3%83%BC%E3%82%BF%E3%82%B5%E3%82%A4%E3%82%A8%E3%83%B3%E3%82%B9%E3%81%AE%E3%81%9F%E3%82%81%E3%81%AE%E7%B5%B1/d7EJzgEACAAJ?hl=en&#34;&gt;Google books&lt;/a&gt;, &lt;a href=&#34;https://www.amazon.co.jp/%E3%83%87%E3%83%BC%E3%82%BF%E3%82%B5%E3%82%A4%E3%82%A8%E3%83%B3%E3%82%B9%E3%81%AE%E3%81%9F%E3%82%81%E3%81%AE%E7%B5%B1%E8%A8%88%E5%AD%A6%E5%85%A5%E9%96%80-%E2%80%95%E4%BA%88%E6%B8%AC%E3%80%81%E5%88%86%E9%A1%9E%E3%80%81%E7%B5%B1%E8%A8%88%E3%83%A2%E3%83%87%E3%83%AA%E3%83%B3%E3%82%B0%E3%80%81%E7%B5%B1%E8%A8%88%E7%9A%84%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%81%A8R-Python%E3%83%97%E3%83%AD%E3%82%B0%E3%83%A9%E3%83%9F%E3%83%B3%E3%82%B0-Peter-Bruce/dp/487311926X&#34;&gt;Amazon&lt;/a&gt;, &lt;a href=&#34;https://www.oreilly.co.jp/books/9784873119267/&#34;&gt;Order here&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gedeck/practical-statistics-for-data-scientists/master/images/OReilly-german.jpg&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;German (2021-03-29):&lt;/b&gt;&lt;br&gt; Praktische Statistik für Data Scientists: 50+ essenzielle Konzepte mit R und Python&amp;nbsp;&lt;br&gt; 2021: ISBN 978-3-960-09153-0, Marcus Fraaß&amp;nbsp;(Übersetzer), dpunkt.verlag GmbH&lt;br&gt; &lt;a href=&#34;https://www.google.com/books/edition/Praktische_Statistik_f%C3%BCr_Data_Scientist/yeMCzgEACAAJ?hl=en&#34;&gt;Google books&lt;/a&gt;, &lt;a href=&#34;https://www.amazon.de/Praktische-Statistik-f%C3%BCr-Data-Scientists/dp/3960091532&#34;&gt;Amazon&lt;/a&gt; &lt;a href=&#34;https://dpunkt.de/produkt/praktische-statistik-fuer-data-scientists/&#34;&gt;Order here&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gedeck/practical-statistics-for-data-scientists/master/images/OReilly-korean.jpg&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;Korean (2021-05-07):&lt;/b&gt;&lt;br&gt; Practical Statistics for Data Scientists: 데이터 과학을 위한 통계(2판)&lt;br&gt; 2021: ISBN 979-1-162-24418-0, Junyong Lee (translation), Hanbit Media, Inc. &lt;br&gt; &lt;a href=&#34;https://www.google.com/books/edition/%EB%8D%B0%EC%9D%B4%ED%84%B0_%EA%B3%BC%ED%95%99%EC%9D%84_%EC%9C%84%ED%95%9C_%ED%86%B5%EA%B3%84_2%ED%8C%90/9E9qzgEACAAJ?hl=en&#34;&gt;Google books&lt;/a&gt;, &lt;a href=&#34;https://www.hanbit.co.kr/store/books/look.php?p_code=B2862122581&#34;&gt;Order here&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gedeck/practical-statistics-for-data-scientists/master/images/OReilly-polish.jpg&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;Polish (2021-06-16):&lt;/b&gt;&lt;br&gt; Statystyka praktyczna w data science. 50 kluczowych zagadnien w jezykach R i Python&lt;br&gt; 2021: ISBN 978-8-328-37427-0, Helion &lt;br&gt; &lt;a href=&#34;https://www.google.com/books/edition/Statystyka_praktyczna_w_data_science/GyqSzgEACAAJ&#34;&gt;Google books&lt;/a&gt;, &lt;a href=&#34;https://www.amazon.com/Statystyka-praktyczna-science-kluczowych-zagadnien/dp/8328374277/&#34;&gt;Amazon&lt;/a&gt;, &lt;a href=&#34;https://helion.pl/ksiazki/statystyka-praktyczna-w-data-science-50-kluczowych-zagadnien-w-jezykach-r-i-python-wydanie-ii-peter-bruce-andrew-bruce-peter-gedeck,stpra2.htm&#34;&gt;Order here&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gedeck/practical-statistics-for-data-scientists/master/images/OReilly-russian.png&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;Russian (2021-05-31):&lt;/b&gt;&lt;br&gt; Практическая статистика для специалистов Data Science, 2-е изд.&lt;br&gt; 2021: ISBN 978-5-9775-6705-3, BHV St Petersburg &lt;br&gt; &lt;a href=&#34;https://www.google.com/books/edition/%D0%9F%D1%80%D0%B0%D0%BA%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B0%D1%8F_%D1%81%D1%82%D0%B0%D1%82%D0%B8%D1%81%D1%82/l_6MDwAAQBAJ&#34;&gt;Google books&lt;/a&gt;, &lt;a href=&#34;https://bhv.ru/product/prakticheskaya-statistika-dlya-spetsialistov-data-science-2-e-izd/&#34;&gt;Order here&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gedeck/practical-statistics-for-data-scientists/master/images/OReilly-chinese-complex.png&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;Chinese complex (2021-07-29):&lt;/b&gt;&lt;br&gt; Practical Statistics for Data Scientists: 資料科學家的實用統計學 第二版&lt;br&gt; 2021: ISBN 978-9-865-02841-1, Hong Weien (translation), GoTop Information Inc. &lt;br&gt; &#xA;    &lt;!-- &lt;a href=&#39;https://www.google.com/books/edition/&#39;&gt;Google books&lt;/a&gt;, --&gt; &lt;a href=&#34;http://books.gotop.com.tw/o_A643&#34;&gt;Order here&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gedeck/practical-statistics-for-data-scientists/master/images/OReilly-chinese.png&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;Chinese simplified (2021-10-15):&lt;/b&gt;&lt;br&gt; Practical Statistics for Data Scientists: 数据科学中的实用统计学（第2版）&lt;br&gt; 2021: ISBN 978-7-115-56902-8, Chen Guangxin (translation), Posts &amp;amp; Telecom Press &lt;br&gt; &#xA;    &lt;!-- &lt;a href=&#39;https://www.google.com/books/edition/&#39;&gt;Google books&lt;/a&gt;, --&gt; &lt;a href=&#34;https://item.jd.com/12971155.html&#34;&gt;Order here&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gedeck/practical-statistics-for-data-scientists/master/images/SPD-english.png&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;English (Indian subcontinent &amp;amp; select countries only):&lt;/b&gt;&lt;br&gt; Practical Statistics for Data Scientists: 50+ Essential Concepts Using R And Python, Second Edition&lt;br&gt; 2021: ISBN 978-8-194-43500-6, Shroff Publishers and Distributors Pvt. Ltd. &lt;br&gt; &#xA;    &lt;!-- &lt;a href=&#39;https://www.google.com/books/edition/&#39;&gt;Google books&lt;/a&gt;, --&gt; &lt;a href=&#34;https://www.shroffpublishers.com/books/9788194435006/&#34;&gt;Order here&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gedeck/practical-statistics-for-data-scientists/master/images/OReilly-spanish.png&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;Spanish (2022-02-22):&lt;/b&gt;&lt;br&gt; Estadística práctica para ciencia de datos con R y Python, Second Edition&lt;br&gt; 2022: ISBN 978-8-426-73443-3, Marcombo S.A. &lt;br&gt; &lt;a href=&#34;https://books.google.com/books?id=IZxeEAAAQBAJ&#34;&gt;Google books&lt;/a&gt;, &lt;a href=&#34;https://www.amazon.com/Estad%C3%ADstica-pr%C3%A1ctica-ciencia-datos-Python/dp/842673443X/&#34;&gt;Amazon&lt;/a&gt;, &lt;a href=&#34;https://www.marcombo.com/estadistica-practica-para-ciencia-de-datos-con-r-y-python-9788426734433/&#34;&gt;Order here&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;See also&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The code repository for the first edition is at: &lt;a href=&#34;https://github.com/andrewgbruce/statistics-for-data-scientists&#34;&gt;https://github.com/andrewgbruce/statistics-for-data-scientists&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Setup R and Python environments&lt;/h1&gt; &#xA;&lt;h2&gt;R&lt;/h2&gt; &#xA;&lt;p&gt;Run the following commands in R to install all required packages&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;if (!require(vioplot)) install.packages(&#39;vioplot&#39;)&#xA;if (!require(corrplot)) install.packages(&#39;corrplot&#39;)&#xA;if (!require(gmodels)) install.packages(&#39;gmodels&#39;)&#xA;if (!require(matrixStats)) install.packages(&#39;matrixStats&#39;)&#xA;&#xA;if (!require(lmPerm)) install.packages(&#39;lmPerm&#39;)&#xA;if (!require(pwr)) install.packages(&#39;pwr&#39;)&#xA;&#xA;if (!require(FNN)) install.packages(&#39;FNN&#39;)&#xA;if (!require(klaR)) install.packages(&#39;klaR&#39;)&#xA;if (!require(DMwR)) install.packages(&#39;DMwR&#39;)&#xA;&#xA;if (!require(xgboost)) install.packages(&#39;xgboost&#39;)&#xA;&#xA;if (!require(ellipse)) install.packages(&#39;ellipse&#39;)&#xA;if (!require(mclust)) install.packages(&#39;mclust&#39;)&#xA;if (!require(ca)) install.packages(&#39;ca&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Python&lt;/h2&gt; &#xA;&lt;p&gt;We recommend to use a conda environment to run the Python code.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n sfds python&#xA;conda activate sfds&#xA;conda env update -n sfds -f environment.yml&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>Pierian-Data/Complete-Python-3-Bootcamp</title>
    <updated>2022-06-01T02:12:48Z</updated>
    <id>tag:github.com,2022-06-01:/Pierian-Data/Complete-Python-3-Bootcamp</id>
    <link href="https://github.com/Pierian-Data/Complete-Python-3-Bootcamp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Course Files for Complete Python 3 Bootcamp Course on Udemy&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Complete-Python-3-Bootcamp&lt;/h1&gt; &#xA;&lt;p&gt;Course Files for Complete Python 3 Bootcamp Course on Udemy&lt;/p&gt; &#xA;&lt;p&gt;Copyright(©) by Pierian Data Inc.&lt;/p&gt; &#xA;&lt;p&gt;Get it now for 95% off with the link: &lt;a href=&#34;https://www.udemy.com/complete-python-bootcamp/?couponCode=COMPLETE_GITHUB&#34;&gt;https://www.udemy.com/complete-python-bootcamp/?couponCode=COMPLETE_GITHUB&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Thanks!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/Data-Science-For-Beginners</title>
    <updated>2022-06-01T02:12:48Z</updated>
    <id>tag:github.com,2022-06-01:/microsoft/Data-Science-For-Beginners</id>
    <link href="https://github.com/microsoft/Data-Science-For-Beginners" rel="alternate"></link>
    <summary type="html">&lt;p&gt;10 Weeks, 20 Lessons, Data Science for All!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Data Science for Beginners - A Curriculum&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/Data-Science-For-Beginners/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/microsoft/Data-Science-For-Beginners.svg?sanitize=true&#34; alt=&#34;GitHub license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/Data-Science-For-Beginners/graphs/contributors/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/microsoft/Data-Science-For-Beginners.svg?sanitize=true&#34; alt=&#34;GitHub contributors&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/Data-Science-For-Beginners/issues/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/microsoft/Data-Science-For-Beginners.svg?sanitize=true&#34; alt=&#34;GitHub issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/Data-Science-For-Beginners/pulls/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-pr/microsoft/Data-Science-For-Beginners.svg?sanitize=true&#34; alt=&#34;GitHub pull-requests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://makeapullrequest.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square&#34; alt=&#34;PRs Welcome&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://GitHub.com/microsoft/Data-Science-For-Beginners/watchers/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/watchers/microsoft/Data-Science-For-Beginners.svg?style=social&amp;amp;label=Watch&#34; alt=&#34;GitHub watchers&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/Data-Science-For-Beginners/network/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/forks/microsoft/Data-Science-For-Beginners.svg?style=social&amp;amp;label=Fork&#34; alt=&#34;GitHub forks&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/Data-Science-For-Beginners/stargazers/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/microsoft/Data-Science-For-Beginners.svg?style=social&amp;amp;label=Star&#34; alt=&#34;GitHub stars&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Azure Cloud Advocates at Microsoft are pleased to offer a 10-week, 20-lesson curriculum all about Data Science. Each lesson includes pre-lesson and post-lesson quizzes, written instructions to complete the lesson, a solution, and an assignment. Our project-based pedagogy allows you to learn while building, a proven way for new skills to &#39;stick&#39;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Hearty thanks to our authors:&lt;/strong&gt; &lt;a href=&#34;https://www.twitter.com/paladique&#34;&gt;Jasmine Greenaway&lt;/a&gt;, &lt;a href=&#34;http://soshnikov.com&#34;&gt;Dmitry Soshnikov&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/nitya&#34;&gt;Nitya Narasimhan&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/JalenMcG&#34;&gt;Jalen McGee&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/jenlooper&#34;&gt;Jen Looper&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/maudstweets&#34;&gt;Maud Levy&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/TiffanySouterre&#34;&gt;Tiffany Souterre&lt;/a&gt;, &lt;a href=&#34;https://www.twitter.com/geektrainer&#34;&gt;Christopher Harrison&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;🙏 Special thanks 🙏 to our &lt;a href=&#34;https://studentambassadors.microsoft.com/&#34;&gt;Microsoft Student Ambassador&lt;/a&gt; authors, reviewers and content contributors,&lt;/strong&gt; notably Aaryan Arora, &lt;a href=&#34;https://github.com/AdityaGarg00&#34;&gt;Aditya Garg&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/alondra-sanchez-molina/&#34;&gt;Alondra Sanchez&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/ankitasingh007&#34;&gt;Ankita Singh&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/anupam--mishra/&#34;&gt;Anupam Mishra&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/arpitadas01/&#34;&gt;Arpita Das&lt;/a&gt;, ChhailBihari Dubey, &lt;a href=&#34;https://www.linkedin.com/in/dibrinsofor&#34;&gt;Dibri Nsofor&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/dishita-bhasin-7065281bb&#34;&gt;Dishita Bhasin&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/majd-s/&#34;&gt;Majd Safi&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/max-blum-6036a1186/&#34;&gt;Max Blum&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/miguelmque/&#34;&gt;Miguel Correa&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/iftu119&#34;&gt;Mohamma Iftekher (Iftu) Ebne Jalal&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/nawrin-tabassum&#34;&gt;Nawrin Tabassum&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/raymond-wp/&#34;&gt;Raymond Wangsa Putra&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/rty2423&#34;&gt;Rohit Yadav&lt;/a&gt;, Samridhi Sharma, &lt;a href=&#34;https://www.linkedin.com/mwlite/in/sanya-sinha-13aab1200&#34;&gt;Sanya Sinha&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/sheena-narua-n/&#34;&gt;Sheena Narula&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/tauqeerahmad5201/&#34;&gt;Tauqeer Ahmad&lt;/a&gt;, Yogendrasingh Pawar , &lt;a href=&#34;https://www.linkedin.com/in/vidushi-gupta07/&#34;&gt;Vidushi Gupta&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/jasleen-sondhi/&#34;&gt;Jasleen Sondhi&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/sketchnotes/00-Title.png&#34; alt=&#34; Sketchnote by (@sketchthedocs) &#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data Science For Beginners - &lt;em&gt;Sketchnote by &lt;a href=&#34;https://twitter.com/nitya&#34;&gt;@nitya&lt;/a&gt;&lt;/em&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Getting Started&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Teachers&lt;/strong&gt;: we have &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/for-teachers.md&#34;&gt;included some suggestions&lt;/a&gt; on how to use this curriculum. We&#39;d love your feedback &lt;a href=&#34;https://github.com/microsoft/Data-Science-For-Beginners/discussions&#34;&gt;in our discussion forum&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://aka.ms/student-page&#34;&gt;Students&lt;/a&gt;&lt;/strong&gt;: to use this curriculum on your own, fork the entire repo and complete the exercises on your own, starting with a pre-lecture quiz. Then read the lecture and complete the rest of the activities. Try to create the projects by comprehending the lessons rather than copying the solution code; however, that code is available in the /solutions folders in each project-oriented lesson. Another idea would be to form a study group with friends and go through the content together. For further study, we recommend &lt;a href=&#34;https://docs.microsoft.com/en-us/users/jenlooper-2911/collections/qprpajyoy3x0g7?WT.mc_id=academic-40229-cxa&#34;&gt;Microsoft Learn&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Meet the Team&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/8mzavjQSMM4&#34; title=&#34;Promo video&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/ds-for-beginners.gif&#34; alt=&#34;Promo video&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Gif by&lt;/strong&gt; &lt;a href=&#34;https://www.linkedin.com/in/mohitjaisal&#34;&gt;Mohit Jaisal&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;🎥 Click the image above for a video about the project the folks who created it!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Pedagogy&lt;/h2&gt; &#xA;&lt;p&gt;We have chosen two pedagogical tenets while building this curriculum: ensuring that it is project-based and that it includes frequent quizzes. By the end of this series, students will have learned basic principles of data science, including ethical concepts, data preparation, different ways of working with data, data visualization, data analysis, real-world use cases of data science, and more.&lt;/p&gt; &#xA;&lt;p&gt;In addition, a low-stakes quiz before a class sets the intention of the student towards learning a topic, while a second quiz after class ensures further retention. This curriculum was designed to be flexible and fun and can be taken in whole or in part. The projects start small and become increasingly complex by the end of the 10 week cycle.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Find our &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/CODE_OF_CONDUCT.md&#34;&gt;Code of Conduct&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/CONTRIBUTING.md&#34;&gt;Contributing&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/TRANSLATIONS.md&#34;&gt;Translation&lt;/a&gt; guidelines. We welcome your constructive feedback!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Each lesson includes:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Optional sketchnote&lt;/li&gt; &#xA; &lt;li&gt;Optional supplemental video&lt;/li&gt; &#xA; &lt;li&gt;Pre-lesson warmup quiz&lt;/li&gt; &#xA; &lt;li&gt;Written lesson&lt;/li&gt; &#xA; &lt;li&gt;For project-based lessons, step-by-step guides on how to build the project&lt;/li&gt; &#xA; &lt;li&gt;Knowledge checks&lt;/li&gt; &#xA; &lt;li&gt;A challenge&lt;/li&gt; &#xA; &lt;li&gt;Supplemental reading&lt;/li&gt; &#xA; &lt;li&gt;Assignment&lt;/li&gt; &#xA; &lt;li&gt;Post-lesson quiz&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;A note about quizzes&lt;/strong&gt;: All quizzes are contained &lt;a href=&#34;https://red-water-0103e7a0f.azurestaticapps.net/&#34;&gt;in this app&lt;/a&gt;, for 40 total quizzes of three questions each. They are linked from within the lessons, but the quiz app can be run locally; follow the instruction in the &lt;code&gt;quiz-app&lt;/code&gt; folder. They are gradually being localized.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Lessons&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/sketchnotes/00-Roadmap.png&#34; alt=&#34; Sketchnote by (@sketchthedocs) &#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data Science For Beginners: Roadmap - &lt;em&gt;Sketchnote by &lt;a href=&#34;https://twitter.com/nitya&#34;&gt;@nitya&lt;/a&gt;&lt;/em&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Lesson Number&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Topic&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Lesson Grouping&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Learning Objectives&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Linked Lesson&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Author&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;01&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Defining Data Science&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/README.md&#34;&gt;Introduction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Learn the basic concepts behind data science and how it’s related to artificial intelligence, machine learning, and big data.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/01-defining-data-science/README.md&#34;&gt;lesson&lt;/a&gt; &lt;a href=&#34;https://youtu.be/beZ7Mb_oz9I&#34;&gt;video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://soshnikov.com&#34;&gt;Dmitry&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;02&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data Science Ethics&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/README.md&#34;&gt;Introduction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data Ethics Concepts, Challenges &amp;amp; Frameworks.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/02-ethics/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/nitya&#34;&gt;Nitya&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;03&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Defining Data&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/README.md&#34;&gt;Introduction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;How data is classified and its common sources.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/03-defining-data/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.twitter.com/paladique&#34;&gt;Jasmine&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;04&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to Statistics &amp;amp; Probability&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/README.md&#34;&gt;Introduction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;The mathematical techniques of probability and statistics to understand data.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/04-stats-and-probability/README.md&#34;&gt;lesson&lt;/a&gt; &lt;a href=&#34;https://youtu.be/Z5Zy85g4Yjw&#34;&gt;video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://soshnikov.com&#34;&gt;Dmitry&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;05&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Working with Relational Data&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/README.md&#34;&gt;Working With Data&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to relational data and the basics of exploring and analyzing relational data with the Structured Query Language, also known as SQL (pronounced “see-quell”).&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/05-relational-databases/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.twitter.com/geektrainer&#34;&gt;Christopher&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;06&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Working with NoSQL Data&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/README.md&#34;&gt;Working With Data&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to non-relational data, its various types and the basics of exploring and analyzing document databases.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/06-non-relational/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/paladique&#34;&gt;Jasmine&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;07&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Working with Python&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/README.md&#34;&gt;Working With Data&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Basics of using Python for data exploration with libraries such as Pandas. Foundational understanding of Python programming is recommended.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/07-python/README.md&#34;&gt;lesson&lt;/a&gt; &lt;a href=&#34;https://youtu.be/dZjWOGbsN4Y&#34;&gt;video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://soshnikov.com&#34;&gt;Dmitry&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;08&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data Preparation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/README.md&#34;&gt;Working With Data&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Topics on data techniques for cleaning and transforming the data to handle challenges of missing, inaccurate, or incomplete data.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/08-data-preparation/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.twitter.com/paladique&#34;&gt;Jasmine&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;09&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Visualizing Quantities&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/README.md&#34;&gt;Data Visualization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Learn how to use Matplotlib to visualize bird data 🦆&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/09-visualization-quantities/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/jenlooper&#34;&gt;Jen&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;10&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Visualizing Distributions of Data&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/README.md&#34;&gt;Data Visualization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Visualizing observations and trends within an interval.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/10-visualization-distributions/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/jenlooper&#34;&gt;Jen&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;11&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Visualizing Proportions&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/README.md&#34;&gt;Data Visualization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Visualizing discrete and grouped percentages.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/11-visualization-proportions/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/jenlooper&#34;&gt;Jen&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Visualizing Relationships&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/README.md&#34;&gt;Data Visualization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Visualizing connections and correlations between sets of data and their variables.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/12-visualization-relationships/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/jenlooper&#34;&gt;Jen&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Meaningful Visualizations&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/README.md&#34;&gt;Data Visualization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Techniques and guidance for making your visualizations valuable for effective problem solving and insights.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/13-meaningful-visualizations/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/jenlooper&#34;&gt;Jen&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to the Data Science lifecycle&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/README.md&#34;&gt;Lifecycle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to the data science lifecycle and its first step of acquiring and extracting data.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/14-Introduction/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/paladique&#34;&gt;Jasmine&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Analyzing&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/README.md&#34;&gt;Lifecycle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;This phase of the data science lifecycle focuses on techniques to analyze data.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/15-analyzing/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/paladique&#34;&gt;Jasmine&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Communication&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/README.md&#34;&gt;Lifecycle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;This phase of the data science lifecycle focuses on presenting the insights from the data in a way that makes it easier for decision makers to understand.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/16-communication/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/JalenMcG&#34;&gt;Jalen&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data Science in the Cloud&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/README.md&#34;&gt;Cloud Data&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;This series of lessons introduces data science in the cloud and its benefits.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/17-Introduction/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/TiffanySouterre&#34;&gt;Tiffany&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/maudstweets&#34;&gt;Maud&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data Science in the Cloud&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/README.md&#34;&gt;Cloud Data&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Training models using Low Code tools.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/18-Low-Code/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/TiffanySouterre&#34;&gt;Tiffany&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/maudstweets&#34;&gt;Maud&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;19&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data Science in the Cloud&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/README.md&#34;&gt;Cloud Data&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Deploying models with Azure Machine Learning Studio.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/19-Azure/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/TiffanySouterre&#34;&gt;Tiffany&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/maudstweets&#34;&gt;Maud&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data Science in the Wild&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/6-Data-Science-In-Wild/README.md&#34;&gt;In the Wild&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data science driven projects in the real world.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/6-Data-Science-In-Wild/20-Real-World-Examples/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/nitya&#34;&gt;Nitya&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Offline access&lt;/h2&gt; &#xA;&lt;p&gt;You can run this documentation offline by using &lt;a href=&#34;https://docsify.js.org/#/&#34;&gt;Docsify&lt;/a&gt;. Fork this repo, &lt;a href=&#34;https://docsify.js.org/#/quickstart&#34;&gt;install Docsify&lt;/a&gt; on your local machine, then in the root folder of this repo, type &lt;code&gt;docsify serve&lt;/code&gt;. The website will be served on port 3000 on your localhost: &lt;code&gt;localhost:3000&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note, notebooks will not be rendered via Docsify, so when you need to run a notebook, do that separately in VS Code running a Python kernel.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;PDF&lt;/h2&gt; &#xA;&lt;p&gt;A PDF of all of the lessons can be found &lt;a href=&#34;https://microsoft.github.io/Data-Science-For-Beginners/pdf/readme.pdf&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Help Wanted!&lt;/h2&gt; &#xA;&lt;p&gt;If you would like to translate all or part of the curriculum, please follow our &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/TRANSLATIONS.md&#34;&gt;Translations&lt;/a&gt; guide.&lt;/p&gt; &#xA;&lt;h2&gt;Other Curricula&lt;/h2&gt; &#xA;&lt;p&gt;Our team produces other curricula! Check out:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/ml-beginners&#34;&gt;Machine Learning for Beginners&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/iot-beginners&#34;&gt;IoT for Beginners&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/webdev-beginners&#34;&gt;Web Dev for Beginners&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/ai-beginners&#34;&gt;AI for Beginners&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>kubernetes/community</title>
    <updated>2022-06-01T02:12:48Z</updated>
    <id>tag:github.com,2022-06-01:/kubernetes/community</id>
    <link href="https://github.com/kubernetes/community" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Kubernetes community content&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Kubernetes Community&lt;/h1&gt; &#xA;&lt;p&gt;Welcome to the Kubernetes community!&lt;/p&gt; &#xA;&lt;p&gt;This is the starting point for joining and contributing to the Kubernetes community - improving docs, improving code, giving talks etc.&lt;/p&gt; &#xA;&lt;p&gt;To learn more about the project structure and organization, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/governance.md&#34;&gt;Project Governance&lt;/a&gt; information.&lt;/p&gt; &#xA;&lt;h2&gt;Communicating&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/communication/&#34;&gt;communication&lt;/a&gt; page lists communication channels like chat, issues, mailing lists, conferences, etc.&lt;/p&gt; &#xA;&lt;p&gt;For more specific topics, try a SIG.&lt;/p&gt; &#xA;&lt;h2&gt;Governance&lt;/h2&gt; &#xA;&lt;p&gt;Kubernetes has the following types of groups that are officially supported:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Committees&lt;/strong&gt; are named sets of people that are chartered to take on sensitive topics. This group is encouraged to be as open as possible while achieving its mission but, because of the nature of the topics discussed, private communications are allowed. Examples of committees include the steering committee and things like security or code of conduct.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Special Interest Groups (SIGs)&lt;/strong&gt; are persistent open groups that focus on a part of the project. SIGs must have open and transparent proceedings. Anyone is welcome to participate and contribute provided they follow the Kubernetes Code of Conduct. The purpose of a SIG is to own and develop a set of &lt;strong&gt;subprojects&lt;/strong&gt;. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Subprojects&lt;/strong&gt; Each SIG can have a set of subprojects. These are smaller groups that can work independently. Some subprojects will be part of the main Kubernetes deliverables while others will be more speculative and live in the &lt;code&gt;kubernetes-sigs&lt;/code&gt; github org.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Working Groups&lt;/strong&gt; are temporary groups that are formed to address issues that cross SIG boundaries. Working groups do not own any code or other long term artifacts. Working groups can report back and act through involved SIGs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;User Groups&lt;/strong&gt; are groups for facilitating communication and discovery of information related to topics that have long term relevance to large groups of Kubernetes users. They do not have ownership of parts of the Kubernetes code base.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/governance.md&#34;&gt;full governance doc&lt;/a&gt; for more details on these groups.&lt;/p&gt; &#xA;&lt;p&gt;A SIG can have its own policy for contribution, described in a &lt;code&gt;README&lt;/code&gt; or &lt;code&gt;CONTRIBUTING&lt;/code&gt; file in the SIG folder in this repo (e.g. &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/sig-cli/CONTRIBUTING.md&#34;&gt;sig-cli/CONTRIBUTING.md&lt;/a&gt;), and its own mailing list, slack channel, etc.&lt;/p&gt; &#xA;&lt;p&gt;If you want to edit details about a SIG (e.g. its weekly meeting time or its leads), please follow &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/generator&#34;&gt;these instructions&lt;/a&gt; that detail how our docs are auto-generated.&lt;/p&gt; &#xA;&lt;h2&gt;Learn to Build&lt;/h2&gt; &#xA;&lt;p&gt;Links in &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/contributors/devel/README.md&#34;&gt;contributors/devel/README.md&lt;/a&gt; lead to many relevant technical topics.&lt;/p&gt; &#xA;&lt;h2&gt;Contribute&lt;/h2&gt; &#xA;&lt;p&gt;A first step to contributing is to pick from the &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/sig-list.md&#34;&gt;list of kubernetes SIGs&lt;/a&gt;. Start attending SIG meetings, join the slack channel and subscribe to the mailing list. SIGs will often have a set of &#34;help wanted&#34; issues that can help new contributors get involved.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/contributors/guide/README.md&#34;&gt;Contributor Guide&lt;/a&gt; provides detailed instruction on how to get your ideas and bug fixes seen and accepted, including:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;How to &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/contributors/guide/first-contribution.md#file-an-issue&#34;&gt;file an issue&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;How to &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/contributors/guide/first-contribution.md#find-something-to-work-on&#34;&gt;find something to work on&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;How to &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/contributors/guide/contributing.md#opening-a-pull-request&#34;&gt;open a pull request&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Membership&lt;/h2&gt; &#xA;&lt;p&gt;We encourage all contributors to become members. We aim to grow an active, healthy community of contributors, reviewers, and code owners. Learn more about requirements and responsibilities of membership in our &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/community-membership.md&#34;&gt;Community Membership&lt;/a&gt; page.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>guipsamora/pandas_exercises</title>
    <updated>2022-06-01T02:12:48Z</updated>
    <id>tag:github.com,2022-06-01:/guipsamora/pandas_exercises</id>
    <link href="https://github.com/guipsamora/pandas_exercises" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Practice your pandas skills!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Pandas Exercises&lt;/h1&gt; &#xA;&lt;p&gt;Fed up with a ton of tutorials but no easy way to find exercises I decided to create a repo just with exercises to practice pandas. Don&#39;t get me wrong, tutorials are great resources, but to learn is to do. So unless you practice you won&#39;t learn.&lt;/p&gt; &#xA;&lt;p&gt;There will be three different types of files:&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1. Exercise instructions&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2. Solutions without code&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3. Solutions with code and comments&lt;/p&gt; &#xA;&lt;p&gt;My suggestion is that you learn a topic in a tutorial, video or documentation and then do the first exercises. Learn one more topic and do more exercises. If you are stuck, don&#39;t go directly to the solution with code files. Check the solutions only and try to get the correct answer.&lt;/p&gt; &#xA;&lt;p&gt;Suggestions and collaborations are more than welcome.🙂 Please open an issue or make a PR indicating the exercise and your problem/solution.&lt;/p&gt; &#xA;&lt;h1&gt;Lessons&lt;/h1&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/#getting-and-knowing&#34;&gt;Getting and knowing&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/#merge&#34;&gt;Merge&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/#time-series&#34;&gt;Time Series&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/#filtering-and-sorting&#34;&gt;Filtering and Sorting&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/#stats&#34;&gt;Stats&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/#deleting&#34;&gt;Deleting&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/#grouping&#34;&gt;Grouping&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/#visualization&#34;&gt;Visualization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Indexing&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/#apply&#34;&gt;Apply&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/#creating-series-and-dataframes&#34;&gt;Creating Series and DataFrames&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Exporting&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/01_Getting_%26_Knowing_Your_Data&#34;&gt;Getting and knowing&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/01_Getting_%26_Knowing_Your_Data/Chipotle&#34;&gt;Chipotle&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/01_Getting_%26_Knowing_Your_Data/Occupation&#34;&gt;Occupation&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/01_Getting_%26_Knowing_Your_Data/World%20Food%20Facts&#34;&gt;World Food Facts&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/02_Filtering_%26_Sorting&#34;&gt;Filtering and Sorting&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/02_Filtering_%26_Sorting/Chipotle&#34;&gt;Chipotle&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/02_Filtering_%26_Sorting/Euro12&#34;&gt;Euro12&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/02_Filtering_%26_Sorting/Fictional%20Army&#34;&gt;Fictional Army&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/03_Grouping&#34;&gt;Grouping&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/03_Grouping/Alcohol_Consumption&#34;&gt;Alcohol Consumption&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/03_Grouping/Occupation&#34;&gt;Occupation&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/03_Grouping/Regiment&#34;&gt;Regiment&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/04_Apply&#34;&gt;Apply&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/04_Apply/Students_Alcohol_Consumption&#34;&gt;Students Alcohol Consumption&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/04_Apply/US_Crime_Rates&#34;&gt;US_Crime_Rates&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/05_Merge&#34;&gt;Merge&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/05_Merge/Auto_MPG&#34;&gt;Auto_MPG&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/05_Merge/Fictitous%20Names&#34;&gt;Fictitious Names&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/05_Merge/Housing%20Market&#34;&gt;House Market&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/06_Stats&#34;&gt;Stats&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/06_Stats/US_Baby_Names&#34;&gt;US_Baby_Names&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/06_Stats/Wind_Stats&#34;&gt;Wind_Stats&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/07_Visualization&#34;&gt;Visualization&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/07_Visualization/Chipotle&#34;&gt;Chipotle&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/07_Visualization/Titanic_Desaster&#34;&gt;Titanic Disaster&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/07_Visualization/Scores&#34;&gt;Scores&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/07_Visualization/Online_Retail&#34;&gt;Online Retail&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/07_Visualization/Tips&#34;&gt;Tips&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/08_Creating_Series_and_DataFrames&#34;&gt;Creating Series and DataFrames&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/08_Creating_Series_and_DataFrames/Pokemon&#34;&gt;Pokemon&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/09_Time_Series&#34;&gt;Time Series&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/09_Time_Series/Apple_Stock&#34;&gt;Apple_Stock&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/09_Time_Series/Getting_Financial_Data&#34;&gt;Getting_Financial_Data&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/09_Time_Series/Getting_Financial_Data&#34;&gt;Investor_Flow_of_Funds_US&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/10_Deleting&#34;&gt;Deleting&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/10_Deleting/Iris&#34;&gt;Iris&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/10_Deleting/Wine&#34;&gt;Wine&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Video Solutions&lt;/h1&gt; &#xA;&lt;p&gt;Video tutorials of data scientists working through the above exercises:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=pu3IpU937xs&amp;amp;list=PLgJhDSE2ZLxaY_DigHeiIDC1cD09rXgJv&#34;&gt;Data Talks - Pandas Learning By Doing&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>multimodalart/majesty-diffusion</title>
    <updated>2022-06-01T02:12:48Z</updated>
    <id>tag:github.com,2022-06-01:/multimodalart/majesty-diffusion</id>
    <link href="https://github.com/multimodalart/majesty-diffusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Majesty Diffusion by @Dango233 and @apolinario (@multimodalart)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Majesty Diffusion 👑&lt;/h1&gt; &#xA;&lt;h3&gt;Generate images from text with majesty&lt;/h3&gt; &#xA;&lt;h4&gt;Formerly known as Princess Generator&lt;/h4&gt; &#xA;&lt;p&gt;Majesty Diffusion are implementations of text-to-image diffusion models with a royal touch 👸&lt;/p&gt; &#xA;&lt;p&gt;Access our &lt;a href=&#34;https://multimodal.art/majesty-diffusion&#34;&gt;Majestic Guide&lt;/a&gt; (&lt;em&gt;under construction&lt;/em&gt;), join our community on &lt;a href=&#34;https://discord.gg/yNBtQBEDfZ&#34;&gt;Discord&lt;/a&gt; or reach out via &lt;a href=&#34;https://twitter.com/multimodalart&#34;&gt;@multimodalart on Twitter&lt;/a&gt;). Share your settings sending PRs to the settings libraries!&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/788417/169711951-1ea0d0e6-2581-474a-b79f-e62b3fd8c3dd.png&#34; width=&#34;30%&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/788417/169711813-804ebfa2-d9ee-4dd6-bdbf-8cb0b211d45c.png&#34; width=&#34;30%&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/788417/169711793-6ac0fb54-ca06-490b-8fb2-10a3edc507ab.png&#34; width=&#34;30%&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/788417/169712054-fe3bf4bd-4473-4070-ba69-0f74f5c3e475.png&#34; width=&#34;30%&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/788417/169711818-474fa21d-e20d-4ee9-8ebd-333ac964b333.png&#34; width=&#34;30%&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/788417/169711832-96456604-25b2-4cc0-8a5c-26bcd56e3993.png&#34; width=&#34;30%&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Current implementations:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/multimodalart/majesty-diffusion/main/#latent-majesty-diffusion-v12&#34;&gt;Latent Majesty Diffusion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/multimodalart/majesty-diffusion/main/#v-majesty-diffusion-v12&#34;&gt;V-Majesty Diffusion&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Latent Majesty Diffusion v1.3&lt;/h2&gt; &#xA;&lt;h5&gt;Formerly known as Latent Princess Generator&lt;/h5&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/multimodalart/MajestyDiffusion/blob/main/latent.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A &lt;a href=&#34;https://github.com/Dango233&#34;&gt;Dango233&lt;/a&gt; and &lt;a href=&#34;https://github.com/multimodalart&#34;&gt;apolinario (@multimodalart)&lt;/a&gt; Colab notebook implementing &lt;a href=&#34;https://github.com/CompVis&#34;&gt;CompVis&lt;/a&gt;&#39; Latent Diffusion, with the following changes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Added &lt;a href=&#34;https://github.com/Dango233&#34;&gt;Dango233&lt;/a&gt; CLIP Guidance&lt;/li&gt; &#xA; &lt;li&gt;Added &lt;a href=&#34;https://github.com/Dango233&#34;&gt;Dango233&lt;/a&gt; magical &lt;strong&gt;new&lt;/strong&gt; step and upscaling scheduling&lt;/li&gt; &#xA; &lt;li&gt;Added &lt;a href=&#34;https://github.com/Dango233&#34;&gt;Dango233&lt;/a&gt; cuts, augs and attributes scheduling&lt;/li&gt; &#xA; &lt;li&gt;Added &lt;a href=&#34;https://github.com/Dango233&#34;&gt;Dango233&lt;/a&gt; mag and clamp settings&lt;/li&gt; &#xA; &lt;li&gt;Added &lt;a href=&#34;https://github.com/Dango233&#34;&gt;Dango233&lt;/a&gt; linear ETA scheduling&lt;/li&gt; &#xA; &lt;li&gt;Added &lt;a href=&#34;https://github.com/Dango233&#34;&gt;Dango233&lt;/a&gt; negative prompts for Latent Diffusion Guidance&lt;/li&gt; &#xA; &lt;li&gt;Added &lt;a href=&#34;https://github.com/Jack000&#34;&gt;Jack000&lt;/a&gt; &lt;a href=&#34;https://github.com/Jack000/glid-3-xl&#34;&gt;GLID-3 XL&lt;/a&gt; watermark free fine-tuned model&lt;/li&gt; &#xA; &lt;li&gt;Added &lt;a href=&#34;https://github.com/dmarx/&#34;&gt;dmarx&lt;/a&gt; &lt;a href=&#34;https://github.com/dmarx/Multi-Modal-Comparators&#34;&gt;Multi-Modal-Comparators&lt;/a&gt; for CLIP and CLIP-like models&lt;/li&gt; &#xA; &lt;li&gt;Added &lt;a href=&#34;https://github.com/mlfoundations/open_clip&#34;&gt;open_clip&lt;/a&gt; gradient checkpointing&lt;/li&gt; &#xA; &lt;li&gt;Added &lt;a href=&#34;https://github.com/crowsonkb/v-diffusion-pytorch&#34;&gt;crowsonkb&lt;/a&gt; aesthetic models&lt;/li&gt; &#xA; &lt;li&gt;Added &lt;a href=&#34;https://github.com/LAION-AI/aesthetic-predictor&#34;&gt;LAION-AI&lt;/a&gt; aesthetic predictor embeddings&lt;/li&gt; &#xA; &lt;li&gt;Added &lt;a href=&#34;https://github.com/Dango233&#34;&gt;Dango233&lt;/a&gt; inpainting mode&lt;/li&gt; &#xA; &lt;li&gt;Added &lt;a href=&#34;https://github.com/multimodalart&#34;&gt;apolinario (@multimodalart)&lt;/a&gt; savable settings and setting library (including &lt;code&gt;colab-free-default&lt;/code&gt;, &lt;code&gt;dango233-princesses&lt;/code&gt;, &lt;code&gt;the-other-zippy&lt;/code&gt; and &lt;code&gt;makaitrad&lt;/code&gt; shared settings. Share yours with us too with a pull request!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;V-Majesty Diffusion v1.2&lt;/h2&gt; &#xA;&lt;h5&gt;Formerly known as Princess Generator ver. Victoria&lt;/h5&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/multimodalart/MajestyDiffusion/blob/main/v.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A &lt;a href=&#34;https://github.com/Dango233&#34;&gt;Dango233&lt;/a&gt; and &lt;a href=&#34;https://github.com/multimodalart&#34;&gt;apolinario (@multimodalart)&lt;/a&gt; Colab notebook implementing &lt;a href=&#34;https://github.com/crowsonkb/v-diffusion-pytorch&#34;&gt;crowsonkb&lt;/a&gt;&#39;s V-Objective Diffusion, with the following changes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Added &lt;a href=&#34;https://github.com/Dango233&#34;&gt;Dango233&lt;/a&gt; parallel multi-model diffusion (e.g.: run &lt;code&gt;cc12m_1&lt;/code&gt; and &lt;code&gt;yfcc_2&lt;/code&gt; at the same time - with or without lerping)&lt;/li&gt; &#xA; &lt;li&gt;Added &lt;a href=&#34;https://github.com/Dango233&#34;&gt;Dango233&lt;/a&gt; cuts, augs and attributes scheduling&lt;/li&gt; &#xA; &lt;li&gt;Added &lt;a href=&#34;https://github.com/Dango233&#34;&gt;Dango233&lt;/a&gt; mag and clamp settings&lt;/li&gt; &#xA; &lt;li&gt;Added &lt;a href=&#34;https://github.com/multimodalart&#34;&gt;apolinario (@multimodalart)&lt;/a&gt; ETA scheduling&lt;/li&gt; &#xA; &lt;li&gt;Added &lt;a href=&#34;https://github.com/nshepperd&#34;&gt;nshepperd&lt;/a&gt; v-diffusion imagenet512 and danbooru models&lt;/li&gt; &#xA; &lt;li&gt;Added &lt;a href=&#34;https://github.com/dmarx&#34;&gt;dmarx&lt;/a&gt; &lt;a href=&#34;https://github.com/dmarx/Multi-Modal-Comparators&#34;&gt;Multi-Modal-Comparators&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Added &lt;a href=&#34;https://github.com/crowsonkb&#34;&gt;crowsonkb&lt;/a&gt; AVA and Simulacra bot aesthetic models&lt;/li&gt; &#xA; &lt;li&gt;Added &lt;a href=&#34;https://github.com/LAION-AI/aesthetic-predictor&#34;&gt;LAION-AI&lt;/a&gt; aesthetic pre-calculated embeddings&lt;/li&gt; &#xA; &lt;li&gt;Added &lt;a href=&#34;https://github.com/mlfoundations/open_clip&#34;&gt;open_clip&lt;/a&gt; gradient checkpointing&lt;/li&gt; &#xA; &lt;li&gt;Added &lt;a href=&#34;https://github.com/Dango233&#34;&gt;Dango233&lt;/a&gt; inpainting mode&lt;/li&gt; &#xA; &lt;li&gt;Added &lt;a href=&#34;https://github.com/multimodalart&#34;&gt;apolinario (@multimodalart)&lt;/a&gt; &#34;internal upscaling&#34; (upscales the output with &lt;code&gt;yfcc_2&lt;/code&gt; or &lt;code&gt;openimages&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Added &lt;a href=&#34;https://github.com/multimodalart&#34;&gt;apolinario (@multimodalart)&lt;/a&gt; savable settings and setting library (including &lt;code&gt;defaults&lt;/code&gt;, &lt;code&gt;disco-diffusion-defaults&lt;/code&gt; default settings). Share yours with us too with a pull request!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;h3&gt;Please feel free to help us in any of these tasks!&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Figure out better defaults and add more settings to the settings library (contribute with a PR!)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add all notebooks to a single pipeline where on model can be the output of the other (similar to &lt;a href=&#34;https://github.com/Zalring/Centipede_Diffusion&#34;&gt;Centipede Diffusion&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add all notebooks to the &lt;a href=&#34;https://raw.githubusercontent.com/multimodalart/majesty-diffusion/main/multimodal.art/mindseye&#34;&gt;MindsEye UI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Modularise everything&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Create a command line version&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add an inpainting UI&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Improve performance, both in speed and VRAM consumption&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; More technical issues will be listed on &lt;a href=&#34;https://raw.githubusercontent.com/multimodalart/majesty-diffusion/main/issues&#34;&gt;https://github.com/multimodalart/majesty-diffusion/issues&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;Some functions and methods are from various code masters - including but not limited to &lt;a href=&#34;https://twitter.com/advadnoun&#34;&gt;advadnoun&lt;/a&gt;, &lt;a href=&#34;https://github.com/crowsonkb&#34;&gt;crowsonkb&lt;/a&gt;, &lt;a href=&#34;https://github.com/nshepperd&#34;&gt;nshepperd&lt;/a&gt;, &lt;a href=&#34;https://github.com/russelldc&#34;&gt;russelldc&lt;/a&gt;, &lt;a href=&#34;https://github.com/Dango233&#34;&gt;Dango233&lt;/a&gt; and many others&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>google-research/google-research</title>
    <updated>2022-06-01T02:12:48Z</updated>
    <id>tag:github.com,2022-06-01:/google-research/google-research</id>
    <link href="https://github.com/google-research/google-research" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Google Research&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Google Research&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains code released by &lt;a href=&#34;https://research.google&#34;&gt;Google Research&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;All datasets in this repository are released under the CC BY 4.0 International license, which can be found here: &lt;a href=&#34;https://creativecommons.org/licenses/by/4.0/legalcode&#34;&gt;https://creativecommons.org/licenses/by/4.0/legalcode&lt;/a&gt;. All source files in this repository are released under the Apache 2.0 license, the text of which can be found in the LICENSE file.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Because the repo is large, we recommend you download only the subdirectory of interest:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;SUBDIR=foo&#xA;svn export https://github.com/google-research/google-research/trunk/$SUBDIR&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you&#39;d like to submit a pull request, you&#39;ll need to clone the repository; we recommend making a shallow clone (without history).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone git@github.com:google-research/google-research.git --depth=1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;em&gt;Disclaimer: This is not an official Google product.&lt;/em&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/ML-For-Beginners</title>
    <updated>2022-06-01T02:12:48Z</updated>
    <id>tag:github.com,2022-06-01:/microsoft/ML-For-Beginners</id>
    <link href="https://github.com/microsoft/ML-For-Beginners" rel="alternate"></link>
    <summary type="html">&lt;p&gt;12 weeks, 26 lessons, 52 quizzes, classic Machine Learning for all&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/ML-For-Beginners/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/microsoft/ML-For-Beginners.svg?sanitize=true&#34; alt=&#34;GitHub license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/ML-For-Beginners/graphs/contributors/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/microsoft/ML-For-Beginners.svg?sanitize=true&#34; alt=&#34;GitHub contributors&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/ML-For-Beginners/issues/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/microsoft/ML-For-Beginners.svg?sanitize=true&#34; alt=&#34;GitHub issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/ML-For-Beginners/pulls/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-pr/microsoft/ML-For-Beginners.svg?sanitize=true&#34; alt=&#34;GitHub pull-requests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://makeapullrequest.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square&#34; alt=&#34;PRs Welcome&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://GitHub.com/microsoft/ML-For-Beginners/watchers/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/watchers/microsoft/ML-For-Beginners.svg?style=social&amp;amp;label=Watch&#34; alt=&#34;GitHub watchers&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/ML-For-Beginners/network/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/forks/microsoft/ML-For-Beginners.svg?style=social&amp;amp;label=Fork&#34; alt=&#34;GitHub forks&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/ML-For-Beginners/stargazers/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/microsoft/ML-For-Beginners.svg?style=social&amp;amp;label=Star&#34; alt=&#34;GitHub stars&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Machine Learning for Beginners - A Curriculum&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;🌍 Travel around the world as we explore Machine Learning by means of world cultures 🌍&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Azure Cloud Advocates at Microsoft are pleased to offer a 12-week, 26-lesson curriculum all about &lt;strong&gt;Machine Learning&lt;/strong&gt;. In this curriculum, you will learn about what is sometimes called &lt;strong&gt;classic machine learning&lt;/strong&gt;, using primarily Scikit-learn as a library and avoiding deep learning, which is covered in our forthcoming &#39;AI for Beginners&#39; curriculum. Pair these lessons with our &lt;a href=&#34;https://aka.ms/datascience-beginners&#34;&gt;&#39;Data Science for Beginners&#39; curriculum&lt;/a&gt;, as well!&lt;/p&gt; &#xA;&lt;p&gt;Travel with us around the world as we apply these classic techniques to data from many areas of the world. Each lesson includes pre- and post-lesson quizzes, written instructions to complete the lesson, a solution, an assignment, and more. Our project-based pedagogy allows you to learn while building, a proven way for new skills to &#39;stick&#39;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;✍️ Hearty thanks to our authors&lt;/strong&gt; Jen Looper, Stephen Howell, Francesca Lazzeri, Tomomi Imura, Cassie Breviu, Dmitry Soshnikov, Chris Noring, Anirban Mukherjee, Ornella Altunyan, and Amy Boyd&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;🎨 Thanks as well to our illustrators&lt;/strong&gt; Tomomi Imura, Dasani Madipalli, and Jen Looper&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;🙏 Special thanks 🙏 to our Microsoft Student Ambassador authors, reviewers, and content contributors&lt;/strong&gt;, notably Rishit Dagli, Muhammad Sakib Khan Inan, Rohan Raj, Alexandru Petrescu, Abhishek Jaiswal, Nawrin Tabassum, Ioan Samuila, and Snigdha Agarwal&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;🤩 Extra gratitude to Microsoft Student Ambassador Eric Wanjau for our R lessons!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Getting Started&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://aka.ms/student-page&#34;&gt;Students&lt;/a&gt;&lt;/strong&gt;, to use this curriculum, fork the entire repo to your own GitHub account and complete the exercises on your own or with a group:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Start with a pre-lecture quiz.&lt;/li&gt; &#xA; &lt;li&gt;Read the lecture and complete the activities, pausing and reflecting at each knowledge check.&lt;/li&gt; &#xA; &lt;li&gt;Try to create the projects by comprehending the lessons rather than running the solution code; however that code is available in the &lt;code&gt;/solution&lt;/code&gt; folders in each project-oriented lesson.&lt;/li&gt; &#xA; &lt;li&gt;Take the post-lecture quiz.&lt;/li&gt; &#xA; &lt;li&gt;Complete the challenge.&lt;/li&gt; &#xA; &lt;li&gt;Complete the assignment.&lt;/li&gt; &#xA; &lt;li&gt;After completing a lesson group, visit the &lt;a href=&#34;https://github.com/microsoft/ML-For-Beginners/discussions&#34;&gt;Discussion Board&lt;/a&gt; and &#34;learn out loud&#34; by filling out the appropriate PAT rubric. A &#39;PAT&#39; is a Progress Assessment Tool that is a rubric you fill out to further your learning. You can also react to other PATs so we can learn together.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;For further study, we recommend following these &lt;a href=&#34;https://docs.microsoft.com/en-us/users/jenlooper-2911/collections/k7o7tg1gp306q4?WT.mc_id=academic-15963-cxa&#34;&gt;Microsoft Learn&lt;/a&gt; modules and learning paths.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;Teachers&lt;/strong&gt;, we have &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/for-teachers.md&#34;&gt;included some suggestions&lt;/a&gt; on how to use this curriculum.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Meet the Team&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/Tj1XWrDSYJU&#34; title=&#34;Promo video&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/ml.gif&#34; alt=&#34;Promo video&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Gif by&lt;/strong&gt; &lt;a href=&#34;https://linkedin.com/in/mohitjaisal&#34;&gt;Mohit Jaisal&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;🎥 Click the image above for a video about the project and the folks who created it!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Pedagogy&lt;/h2&gt; &#xA;&lt;p&gt;We have chosen two pedagogical tenets while building this curriculum: ensuring that it is hands-on &lt;strong&gt;project-based&lt;/strong&gt; and that it includes &lt;strong&gt;frequent quizzes&lt;/strong&gt;. In addition, this curriculum has a common &lt;strong&gt;theme&lt;/strong&gt; to give it cohesion.&lt;/p&gt; &#xA;&lt;p&gt;By ensuring that the content aligns with projects, the process is made more engaging for students and retention of concepts will be augmented. In addition, a low-stakes quiz before a class sets the intention of the student towards learning a topic, while a second quiz after class ensures further retention. This curriculum was designed to be flexible and fun and can be taken in whole or in part. The projects start small and become increasingly complex by the end of the 12-week cycle. This curriculum also includes a postscript on real-world applications of ML, which can be used as extra credit or as a basis for discussion.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Find our &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/CODE_OF_CONDUCT.md&#34;&gt;Code of Conduct&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/CONTRIBUTING.md&#34;&gt;Contributing&lt;/a&gt;, and &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/TRANSLATIONS.md&#34;&gt;Translation&lt;/a&gt; guidelines. We welcome your constructive feedback!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Each lesson includes:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;optional sketchnote&lt;/li&gt; &#xA; &lt;li&gt;optional supplemental video&lt;/li&gt; &#xA; &lt;li&gt;pre-lecture warmup quiz&lt;/li&gt; &#xA; &lt;li&gt;written lesson&lt;/li&gt; &#xA; &lt;li&gt;for project-based lessons, step-by-step guides on how to build the project&lt;/li&gt; &#xA; &lt;li&gt;knowledge checks&lt;/li&gt; &#xA; &lt;li&gt;a challenge&lt;/li&gt; &#xA; &lt;li&gt;supplemental reading&lt;/li&gt; &#xA; &lt;li&gt;assignment&lt;/li&gt; &#xA; &lt;li&gt;post-lecture quiz&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;A note about languages&lt;/strong&gt;: These lessons are primarily written in Python, but many are also available in R. To complete an R lesson, go to the &lt;code&gt;/solution&lt;/code&gt; folder and look for R lessons. They include an .rmd extension that represents an &lt;strong&gt;R Markdown&lt;/strong&gt; file which can be simply defined as an embedding of &lt;code&gt;code chunks&lt;/code&gt; (of R or other languages) and a &lt;code&gt;YAML header&lt;/code&gt; (that guides how to format outputs such as PDF) in a &lt;code&gt;Markdown document&lt;/code&gt;. As such, it serves as an exemplary authoring framework for data science since it allows you to combine your code, its output, and your thoughts by allowing you to write them down in Markdown. Moreover, R Markdown documents can be rendered to output formats such as PDF, HTML, or Word.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;A note about quizzes&lt;/strong&gt;: All quizzes are contained &lt;a href=&#34;https://white-water-09ec41f0f.azurestaticapps.net/&#34;&gt;in this app&lt;/a&gt;, for 52 total quizzes of three questions each. They are linked from within the lessons but the quiz app can be run locally; follow the instruction in the &lt;code&gt;quiz-app&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Lesson Number&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Topic&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Lesson Grouping&lt;/th&gt; &#xA;   &lt;th&gt;Learning Objectives&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Linked Lesson&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Author&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;01&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to machine learning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/1-Introduction/README.md&#34;&gt;Introduction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Learn the basic concepts behind machine learning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/1-Introduction/1-intro-to-ML/README.md&#34;&gt;Lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Muhammad&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;02&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;The History of machine learning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/1-Introduction/README.md&#34;&gt;Introduction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Learn the history underlying this field&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/1-Introduction/2-history-of-ML/README.md&#34;&gt;Lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Jen and Amy&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;03&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Fairness and machine learning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/1-Introduction/README.md&#34;&gt;Introduction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;What are the important philosophical issues around fairness that students should consider when building and applying ML models?&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/1-Introduction/3-fairness/README.md&#34;&gt;Lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Tomomi&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;04&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Techniques for machine learning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/1-Introduction/README.md&#34;&gt;Introduction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;What techniques do ML researchers use to build ML models?&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/1-Introduction/4-techniques-of-ML/README.md&#34;&gt;Lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Chris and Jen&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;05&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to regression&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/README.md&#34;&gt;Regression&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Get started with Python and Scikit-learn for regression models&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/1-Tools/README.md&#34;&gt;Python&lt;/a&gt;&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/1-Tools/solution/R/lesson_1-R.ipynb&#34;&gt;R&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Jen&lt;/li&gt;&#xA;     &lt;li&gt;Eric Wanjau&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;06&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;North American pumpkin prices 🎃&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/README.md&#34;&gt;Regression&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Visualize and clean data in preparation for ML&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/2-Data/README.md&#34;&gt;Python&lt;/a&gt;&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/2-Data/solution/R/lesson_2-R.ipynb&#34;&gt;R&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Jen&lt;/li&gt;&#xA;     &lt;li&gt;Eric Wanjau&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;07&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;North American pumpkin prices 🎃&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/README.md&#34;&gt;Regression&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Build linear and polynomial regression models&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/3-Linear/README.md&#34;&gt;Python&lt;/a&gt;&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/3-Linear/solution/R/lesson_3-R.ipynb&#34;&gt;R&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Jen and Dmitry&lt;/li&gt;&#xA;     &lt;li&gt;Eric Wanjau&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;08&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;North American pumpkin prices 🎃&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/README.md&#34;&gt;Regression&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Build a logistic regression model&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/4-Logistic/README.md&#34;&gt;Python&lt;/a&gt; &lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/4-Logistic/solution/R/lesson_4-R.ipynb&#34;&gt;R&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Jen&lt;/li&gt;&#xA;     &lt;li&gt;Eric Wanjau&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;09&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;A Web App 🔌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/3-Web-App/README.md&#34;&gt;Web App&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Build a web app to use your trained model&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/3-Web-App/1-Web-App/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Jen&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;10&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to classification&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/README.md&#34;&gt;Classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Clean, prep, and visualize your data; introduction to classification&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt; &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/1-Introduction/README.md&#34;&gt;Python&lt;/a&gt; &lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/1-Introduction/solution/R/lesson_10-R.ipynb&#34;&gt;R&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Jen and Cassie&lt;/li&gt;&#xA;     &lt;li&gt;Eric Wanjau&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;11&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Delicious Asian and Indian cuisines 🍜&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/README.md&#34;&gt;Classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Introduction to classifiers&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt; &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/2-Classifiers-1/README.md&#34;&gt;Python&lt;/a&gt;&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/2-Classifiers-1/solution/R/lesson_11-R.ipynb&#34;&gt;R&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Jen and Cassie&lt;/li&gt;&#xA;     &lt;li&gt;Eric Wanjau&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Delicious Asian and Indian cuisines 🍜&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/README.md&#34;&gt;Classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;More classifiers&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt; &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/3-Classifiers-2/README.md&#34;&gt;Python&lt;/a&gt;&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/3-Classifiers-2/solution/R/lesson_12-R.ipynb&#34;&gt;R&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Jen and Cassie&lt;/li&gt;&#xA;     &lt;li&gt;Eric Wanjau&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Delicious Asian and Indian cuisines 🍜&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/README.md&#34;&gt;Classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Build a recommender web app using your model&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/4-Applied/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Jen&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to clustering&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/5-Clustering/README.md&#34;&gt;Clustering&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Clean, prep, and visualize your data; Introduction to clustering&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt; &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/5-Clustering/1-Visualize/README.md&#34;&gt;Python&lt;/a&gt;&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/5-Clustering/1-Visualize/solution/R/lesson_14-R.ipynb&#34;&gt;R&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Jen&lt;/li&gt;&#xA;     &lt;li&gt;Eric Wanjau&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Exploring Nigerian Musical Tastes 🎧&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/5-Clustering/README.md&#34;&gt;Clustering&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Explore the K-Means clustering method&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt; &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/5-Clustering/2-K-Means/README.md&#34;&gt;Python&lt;/a&gt;&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/5-Clustering/2-K-Means/solution/R/lesson_15-R.ipynb&#34;&gt;R&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Jen&lt;/li&gt;&#xA;     &lt;li&gt;Eric Wanjau&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to natural language processing ☕️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/6-NLP/README.md&#34;&gt;Natural language processing&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Learn the basics about NLP by building a simple bot&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/6-NLP/1-Introduction-to-NLP/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Stephen&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Common NLP Tasks ☕️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/6-NLP/README.md&#34;&gt;Natural language processing&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Deepen your NLP knowledge by understanding common tasks required when dealing with language structures&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/6-NLP/2-Tasks/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Stephen&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Translation and sentiment analysis ♥️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/6-NLP/README.md&#34;&gt;Natural language processing&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Translation and sentiment analysis with Jane Austen&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/6-NLP/3-Translation-Sentiment/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Stephen&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;19&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Romantic hotels of Europe ♥️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/6-NLP/README.md&#34;&gt;Natural language processing&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Sentiment analysis with hotel reviews 1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/6-NLP/4-Hotel-Reviews-1/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Stephen&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Romantic hotels of Europe ♥️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/6-NLP/README.md&#34;&gt;Natural language processing&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Sentiment analysis with hotel reviews 2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/6-NLP/5-Hotel-Reviews-2/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Stephen&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;21&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to time series forecasting&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/7-TimeSeries/README.md&#34;&gt;Time series&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Introduction to time series forecasting&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/7-TimeSeries/1-Introduction/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Francesca&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;⚡️ World Power Usage ⚡️ - time series forecasting with ARIMA&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/7-TimeSeries/README.md&#34;&gt;Time series&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Time series forecasting with ARIMA&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/7-TimeSeries/2-ARIMA/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Francesca&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;23&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;⚡️ World Power Usage ⚡️ - time series forecasting with SVR&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/7-TimeSeries/README.md&#34;&gt;Time series&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Time series forecasting with Support Vector Regressor&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/7-TimeSeries/3-SVR/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Anirban&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to reinforcement learning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/8-Reinforcement/README.md&#34;&gt;Reinforcement learning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Introduction to reinforcement learning with Q-Learning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/8-Reinforcement/1-QLearning/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Dmitry&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Help Peter avoid the wolf! 🐺&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/8-Reinforcement/README.md&#34;&gt;Reinforcement learning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Reinforcement learning Gym&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/8-Reinforcement/2-Gym/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Dmitry&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Postscript&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Real-World ML scenarios and applications&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/9-Real-World/README.md&#34;&gt;ML in the Wild&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Interesting and revealing real-world applications of classical ML&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/9-Real-World/1-Applications/README.md&#34;&gt;Lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Team&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Offline access&lt;/h2&gt; &#xA;&lt;p&gt;You can run this documentation offline by using &lt;a href=&#34;https://docsify.js.org/#/&#34;&gt;Docsify&lt;/a&gt;. Fork this repo, &lt;a href=&#34;https://docsify.js.org/#/quickstart&#34;&gt;install Docsify&lt;/a&gt; on your local machine, and then in the root folder of this repo, type &lt;code&gt;docsify serve&lt;/code&gt;. The website will be served on port 3000 on your localhost: &lt;code&gt;localhost:3000&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;PDFs&lt;/h2&gt; &#xA;&lt;p&gt;Find a pdf of the curriculum with links &lt;a href=&#34;https://microsoft.github.io/ML-For-Beginners/pdf/readme.pdf&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Help Wanted!&lt;/h2&gt; &#xA;&lt;p&gt;Would you like to contribute a translation? Please read our &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/TRANSLATIONS.md&#34;&gt;translation guidelines&lt;/a&gt; and add a templated issue to manage the workload &lt;a href=&#34;https://github.com/microsoft/ML-For-Beginners/issues&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Other Curricula&lt;/h2&gt; &#xA;&lt;p&gt;Our team produces other curricula! Check out:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/webdev-beginners&#34;&gt;Web Dev for Beginners&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/iot-beginners&#34;&gt;IoT for Beginners&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/datascience-beginners&#34;&gt;Data Science for Beginners&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/ai-beginners&#34;&gt;AI for Beginners&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>aws/amazon-sagemaker-examples</title>
    <updated>2022-06-01T02:12:48Z</updated>
    <id>tag:github.com,2022-06-01:/aws/amazon-sagemaker-examples</id>
    <link href="https://github.com/aws/amazon-sagemaker-examples" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Example 📓 Jupyter notebooks that demonstrate how to build, train, and deploy machine learning models using 🧠 Amazon SageMaker.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://github.com/aws/amazon-sagemaker-examples/raw/main/_static/sagemaker-banner.png&#34; alt=&#34;SageMaker&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Amazon SageMaker Examples&lt;/h1&gt; &#xA;&lt;p&gt;Example Jupyter notebooks that demonstrate how to build, train, and deploy machine learning models using Amazon SageMaker.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;📚&lt;/span&gt; Background&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://aws.amazon.com/sagemaker/&#34;&gt;Amazon SageMaker&lt;/a&gt; is a fully managed service for data science and machine learning (ML) workflows. You can use Amazon SageMaker to simplify the process of building, training, and deploying ML models.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://sagemaker-examples.readthedocs.io/en/latest/&#34;&gt;SageMaker example notebooks&lt;/a&gt; are Jupyter notebooks that demonstrate the usage of Amazon SageMaker.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;🛠&lt;/span&gt; Setup&lt;/h2&gt; &#xA;&lt;p&gt;The quickest setup to run example notebooks includes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;An &lt;a href=&#34;http://docs.aws.amazon.com/sagemaker/latest/dg/gs-account.html&#34;&gt;AWS account&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Proper &lt;a href=&#34;http://docs.aws.amazon.com/sagemaker/latest/dg/authentication-and-access-control.html&#34;&gt;IAM User and Role&lt;/a&gt; setup&lt;/li&gt; &#xA; &lt;li&gt;An &lt;a href=&#34;http://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html&#34;&gt;Amazon SageMaker Notebook Instance&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;An &lt;a href=&#34;http://docs.aws.amazon.com/sagemaker/latest/dg/gs-config-permissions.html&#34;&gt;S3 bucket&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;span&gt;💻&lt;/span&gt; Usage&lt;/h2&gt; &#xA;&lt;p&gt;These example notebooks are automatically loaded into SageMaker Notebook Instances. They can be accessed by clicking on the &lt;code&gt;SageMaker Examples&lt;/code&gt; tab in Jupyter or the SageMaker logo in JupyterLab.&lt;/p&gt; &#xA;&lt;p&gt;Although most examples utilize key Amazon SageMaker functionality like distributed, managed training or real-time hosted endpoints, these notebooks can be run outside of Amazon SageMaker Notebook Instances with minimal modification (updating IAM role definition and installing the necessary libraries).&lt;/p&gt; &#xA;&lt;p&gt;As of February 7, 2022, the default branch is named &#34;main&#34;. See our &lt;a href=&#34;https://github.com/aws/amazon-sagemaker-examples/discussions/3131&#34;&gt;announcement&lt;/a&gt; for details and how to update your existing clone.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;📓&lt;/span&gt; Examples&lt;/h2&gt; &#xA;&lt;h3&gt;Introduction to Ground Truth Labeling Jobs&lt;/h3&gt; &#xA;&lt;p&gt;These examples provide quick walkthroughs to get you up and running with the labeling job workflow for Amazon SageMaker Ground Truth.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/ground_truth_labeling_jobs/bring_your_own_model_for_sagemaker_labeling_workflows_with_active_learning&#34;&gt;Bring your own model for SageMaker labeling workflows with active learning&lt;/a&gt; is an end-to-end example that shows how to bring your custom training, inference logic and active learning to the Amazon SageMaker ecosystem.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/ground_truth_labeling_jobs/from_unlabeled_data_to_deployed_machine_learning_model_ground_truth_demo_image_classification&#34;&gt;From Unlabeled Data to a Deployed Machine Learning Model: A SageMaker Ground Truth Demonstration for Image Classification&lt;/a&gt; is an end-to-end example that starts with an unlabeled dataset, labels it using the Ground Truth API, analyzes the results, trains an image classification neural net using the annotated dataset, and finally uses the trained model to perform batch and online inference.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/ground_truth_labeling_jobs/ground_truth_object_detection_tutorial&#34;&gt;Ground Truth Object Detection Tutorial&lt;/a&gt; is a similar end-to-end example but for an object detection task.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/ground_truth_labeling_jobs/data_analysis_of_ground_truth_image_classification_output&#34;&gt;Basic Data Analysis of an Image Classification Output Manifest&lt;/a&gt; presents charts to visualize the number of annotations for each class, differentiating between human annotations and automatic labels (if your job used auto-labeling). It also displays sample images in each class, and creates a pdf which concisely displays the full results.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/ground_truth_labeling_jobs/object_detection_augmented_manifest_training&#34;&gt;Training a Machine Learning Model Using an Output Manifest&lt;/a&gt; introduces the concept of an &#34;augmented manifest&#34; and demonstrates that the output file of a labeling job can be immediately used as the input file to train a SageMaker machine learning model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/ground_truth_labeling_jobs/annotation_consolidation&#34;&gt;Annotation Consolidation&lt;/a&gt; demonstrates Amazon SageMaker Ground Truth annotation consolidation techniques for image classification for a completed labeling job.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Introduction to Applying Machine Learning&lt;/h3&gt; &#xA;&lt;p&gt;These examples provide a gentle introduction to machine learning concepts as they are applied in practical use cases across a variety of sectors.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_applying_machine_learning/xgboost_customer_churn&#34;&gt;Predicting Customer Churn&lt;/a&gt; uses customer interaction and service usage data to find those most likely to churn, and then walks through the cost/benefit trade-offs of providing retention incentives. This uses Amazon SageMaker&#39;s implementation of &lt;a href=&#34;https://github.com/dmlc/xgboost&#34;&gt;XGBoost&lt;/a&gt; to create a highly predictive model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_applying_machine_learning/breast_cancer_prediction&#34;&gt;Cancer Prediction&lt;/a&gt; predicts Breast Cancer based on features derived from images, using SageMaker&#39;s Linear Learner.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_applying_machine_learning/ensemble_modeling&#34;&gt;Ensembling&lt;/a&gt; predicts income using two Amazon SageMaker models to show the advantages in ensembling.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_applying_machine_learning/video_game_sales&#34;&gt;Video Game Sales&lt;/a&gt; develops a binary prediction model for the success of video games based on review scores.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_applying_machine_learning/gluon_recommender_system&#34;&gt;MXNet Gluon Recommender System&lt;/a&gt; uses neural network embeddings for non-linear matrix factorization to predict user movie ratings on Amazon digital reviews.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_applying_machine_learning/fair_linear_learner&#34;&gt;Fair Linear Learner&lt;/a&gt; is an example of an effective way to create fair linear models with respect to sensitive features.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_applying_machine_learning/US-census_population_segmentation_PCA_Kmeans&#34;&gt;Population Segmentation of US Census Data using PCA and Kmeans&lt;/a&gt; analyzes US census data and reduces dimensionality using PCA then clusters US counties using KMeans to identify segments of similar counties.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_applying_machine_learning/object2vec_document_embedding&#34;&gt;Document Embedding using Object2Vec&lt;/a&gt; is an example to embed a large collection of documents in a common low-dimensional space, so that the semantic distances between these documents are preserved.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_applying_machine_learning/deepar_chicago_traffic_violations&#34;&gt;Traffic violations forecasting using DeepAR&lt;/a&gt; is an example to use daily traffic violation data to predict pattern and seasonality to use Amazon DeepAR alogorithm.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;SageMaker Automatic Model Tuning&lt;/h3&gt; &#xA;&lt;p&gt;These examples introduce SageMaker&#39;s hyperparameter tuning functionality which helps deliver the best possible predictions by running a large number of training jobs to determine which hyperparameter values are the most impactful.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/hyperparameter_tuning/xgboost_direct_marketing&#34;&gt;XGBoost Tuning&lt;/a&gt; shows how to use SageMaker hyperparameter tuning to improve your model fit.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/hyperparameter_tuning/blazingtext_text_classification_20_newsgroups&#34;&gt;BlazingText Tuning&lt;/a&gt; shows how to use SageMaker hyperparameter tuning with the BlazingText built-in algorithm and 20_newsgroups dataset..&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/hyperparameter_tuning/tensorflow_mnist&#34;&gt;TensorFlow Tuning&lt;/a&gt; shows how to use SageMaker hyperparameter tuning with the pre-built TensorFlow container and MNIST dataset.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/hyperparameter_tuning/mxnet_mnist&#34;&gt;MXNet Tuning&lt;/a&gt; shows how to use SageMaker hyperparameter tuning with the pre-built MXNet container and MNIST dataset.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/hyperparameter_tuning/huggingface_multiclass_text_classification_20_newsgroups&#34;&gt;HuggingFace Tuning&lt;/a&gt; shows how to use SageMaker hyperparameter tuning with the pre-built HuggingFace container and 20_newsgroups dataset.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/hyperparameter_tuning/keras_bring_your_own&#34;&gt;Keras BYO Tuning&lt;/a&gt; shows how to use SageMaker hyperparameter tuning with a custom container running a Keras convolutional network on CIFAR-10 data.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/hyperparameter_tuning/r_bring_your_own&#34;&gt;R BYO Tuning&lt;/a&gt; shows how to use SageMaker hyperparameter tuning with the custom container from the &lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/r_bring_your_own&#34;&gt;Bring Your Own R Algorithm&lt;/a&gt; example.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/hyperparameter_tuning/analyze_results&#34;&gt;Analyzing Results&lt;/a&gt; is a shared notebook that can be used after each of the above notebooks to provide analysis on how training jobs with different hyperparameters performed.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;SageMaker Autopilot&lt;/h3&gt; &#xA;&lt;p&gt;These examples introduce SageMaker Autopilot. Autopilot automatically performs feature engineering, model selection, model tuning (hyperparameter optimization) and allows you to directly deploy the best model to an endpoint to serve inference requests.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/autopilot/&#34;&gt;Customer Churn AutoML&lt;/a&gt; shows how to use SageMaker Autopilot to automatically train a model for the &lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_applying_machine_learning/xgboost_customer_churn&#34;&gt;Predicting Customer Churn&lt;/a&gt; task.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/autopilot/&#34;&gt;Targeted Direct Marketing AutoML&lt;/a&gt; shows how to use SageMaker Autopilot to automatically train a model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-autopilot/housing_prices&#34;&gt;Housing Prices AutoML&lt;/a&gt; shows how to use SageMaker Autopilot for a linear regression problem (predict housing prices).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Introduction to Amazon Algorithms&lt;/h3&gt; &#xA;&lt;p&gt;These examples provide quick walkthroughs to get you up and running with Amazon SageMaker&#39;s custom developed algorithms. Most of these algorithms can train on distributed hardware, scale incredibly well, and are faster and cheaper than popular alternatives.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-python-sdk/1P_kmeans_highlevel&#34;&gt;k-means&lt;/a&gt; is our introductory example for Amazon SageMaker. It walks through the process of clustering MNIST images of handwritten digits using Amazon SageMaker k-means.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/factorization_machines_mnist&#34;&gt;Factorization Machines&lt;/a&gt; showcases Amazon SageMaker&#39;s implementation of the algorithm to predict whether a handwritten digit from the MNIST dataset is a 0 or not using a binary classifier.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/lda_topic_modeling&#34;&gt;Latent Dirichlet Allocation (LDA)&lt;/a&gt; introduces topic modeling using Amazon SageMaker Latent Dirichlet Allocation (LDA) on a synthetic dataset.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/linear_learner_mnist&#34;&gt;Linear Learner&lt;/a&gt; predicts whether a handwritten digit from the MNIST dataset is a 0 or not using a binary classifier from Amazon SageMaker Linear Learner.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/ntm_synthetic&#34;&gt;Neural Topic Model (NTM)&lt;/a&gt; uses Amazon SageMaker Neural Topic Model (NTM) to uncover topics in documents from a synthetic data source, where topic distributions are known.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/pca_mnist&#34;&gt;Principal Components Analysis (PCA)&lt;/a&gt; uses Amazon SageMaker PCA to calculate eigendigits from MNIST.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/seq2seq_translation_en-de&#34;&gt;Seq2Seq&lt;/a&gt; uses the Amazon SageMaker Seq2Seq algorithm that&#39;s built on top of &lt;a href=&#34;https://github.com/awslabs/sockeye&#34;&gt;Sockeye&lt;/a&gt;, which is a sequence-to-sequence framework for Neural Machine Translation based on MXNet. Seq2Seq implements state-of-the-art encoder-decoder architectures which can also be used for tasks like Abstractive Summarization in addition to Machine Translation. This notebook shows translation from English to German text.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/imageclassification_caltech&#34;&gt;Image Classification&lt;/a&gt; includes full training and transfer learning examples of Amazon SageMaker&#39;s Image Classification algorithm. This uses a ResNet deep convolutional neural network to classify images from the caltech dataset.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/xgboost_abalone&#34;&gt;XGBoost for regression&lt;/a&gt; predicts the age of abalone (&lt;a href=&#34;https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression.html&#34;&gt;Abalone dataset&lt;/a&gt;) using regression from Amazon SageMaker&#39;s implementation of &lt;a href=&#34;https://github.com/dmlc/xgboost&#34;&gt;XGBoost&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/xgboost_mnist&#34;&gt;XGBoost for multi-class classification&lt;/a&gt; uses Amazon SageMaker&#39;s implementation of &lt;a href=&#34;https://github.com/dmlc/xgboost&#34;&gt;XGBoost&lt;/a&gt; to classify handwritten digits from the MNIST dataset as one of the ten digits using a multi-class classifier. Both single machine and distributed use-cases are presented.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/deepar_synthetic&#34;&gt;DeepAR for time series forecasting&lt;/a&gt; illustrates how to use the Amazon SageMaker DeepAR algorithm for time series forecasting on a synthetically generated data set.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/blazingtext_word2vec_text8&#34;&gt;BlazingText Word2Vec&lt;/a&gt; generates Word2Vec embeddings from a cleaned text dump of Wikipedia articles using SageMaker&#39;s fast and scalable BlazingText implementation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/object_detection_birds&#34;&gt;Object detection for bird images&lt;/a&gt; demonstrates how to use the Amazon SageMaker Object Detection algorithm with a public dataset of Bird images.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/object2vec_movie_recommendation&#34;&gt;Object2Vec for movie recommendation&lt;/a&gt; demonstrates how Object2Vec can be used to model data consisting of pairs of singleton tokens using movie recommendation as a running example.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/object2vec_multilabel_genre_classification&#34;&gt;Object2Vec for multi-label classification&lt;/a&gt; shows how ObjectToVec algorithm can train on data consisting of pairs of sequences and singleton tokens using the setting of genre prediction of movies based on their plot descriptions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/object2vec_sentence_similarity&#34;&gt;Object2Vec for sentence similarity&lt;/a&gt; explains how to train Object2Vec using sequence pairs as input using sentence similarity analysis as the application.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/ipinsights_login&#34;&gt;IP Insights for suspicious logins&lt;/a&gt; shows how to train IP Insights on a login events for a web server to identify suspicious login attempts.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/semantic_segmentation_pascalvoc&#34;&gt;Semantic Segmentation&lt;/a&gt; shows how to train a semantic segmentation algorithm using the Amazon SageMaker Semantic Segmentation algorithm. It also demonstrates how to host the model and produce segmentation masks and probability of segmentation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/jumpstart_instance_segmentation&#34;&gt;JumpStart Instance Segmentation&lt;/a&gt; demonstrates how to use a pre-trained Instance Segmentation model available in JumpStart for inference.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/jumpstart_semantic_segmentation&#34;&gt;JumpStart Semantic Segmentation&lt;/a&gt; demonstrates how to use a pre-trained Semantic Segmentation model available in JumpStart for inference, how to finetune the pre-trained model on a custom dataset using JumpStart transfer learning algorithm, and how to use fine-tuned model for inference.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/jumpstart_text_generation&#34;&gt;JumpStart Text Generation&lt;/a&gt; shows how to use JumpStart to generate text that appears indistinguishable from the hand-written text.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/jumpstart_text_summarization&#34;&gt;JumpStart Text Summarization&lt;/a&gt; shows how to use JumpStart to summarize the text to contain only the important information.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/jumpstart_image_embedding&#34;&gt;JumpStart Image Embedding&lt;/a&gt; demonstrates how to use a pre-trained model available in JumpStart for image embedding.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/jumpstart_text_embedding&#34;&gt;JumpStart Text Embedding&lt;/a&gt; demonstrates how to use a pre-trained model available in JumpStart for text embedding.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/jumpstart_object_detection&#34;&gt;JumpStart Object Detection&lt;/a&gt; demonstrates how to use a pre-trained Object Detection model available in JumpStart for inference, how to finetune the pre-trained model on a custom dataset using JumpStart transfer learning algorithm, and how to use fine-tuned model for inference.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/jumpstart_machine_translation&#34;&gt;JumpStart Machine Translation&lt;/a&gt; demonstrates how to translate text from one language to another language in JumpStart.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/jumpstart_named_entity_recognition&#34;&gt;JumpStart Named Entity Recognition&lt;/a&gt; demonstrates how to identify named entities such as names, locations etc. in the text in JumpStart.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Amazon SageMaker RL&lt;/h3&gt; &#xA;&lt;p&gt;The following provide examples demonstrating different capabilities of Amazon SageMaker RL.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/reinforcement_learning/rl_cartpole_coach&#34;&gt;Cartpole using Coach&lt;/a&gt; demonstrates the simplest usecase of Amazon SageMaker RL using Intel&#39;s RL Coach.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/reinforcement_learning/rl_deepracer_robomaker_coach_gazebo&#34;&gt;AWS DeepRacer&lt;/a&gt; demonstrates AWS DeepRacer trainig using RL Coach in the Gazebo environment.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/reinforcement_learning/rl_hvac_coach_energyplus&#34;&gt;HVAC using EnergyPlus&lt;/a&gt; demonstrates the training of HVAC systems using the EnergyPlus environment.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/reinforcement_learning/rl_knapsack_coach_custom&#34;&gt;Knapsack Problem&lt;/a&gt; demonstrates how to solve the knapsack problem using a custom environment.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/reinforcement_learning/rl_mountain_car_coach_gymEnv&#34;&gt;Mountain Car&lt;/a&gt; Mountain car is a classic RL problem. This notebook explains how to solve this using the OpenAI Gym environment.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/reinforcement_learning/rl_network_compression_ray_custom&#34;&gt;Distributed Neural Network Compression&lt;/a&gt; This notebook explains how to compress ResNets using RL, using a custom environment and the RLLib toolkit.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/reinforcement_learning/rl_portfolio_management_coach_customEnv&#34;&gt;Portfolio Management&lt;/a&gt; This notebook uses a custom Gym environment to manage multiple financial investments.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/reinforcement_learning/rl_predictive_autoscaling_coach_customEnv&#34;&gt;Autoscaling&lt;/a&gt; demonstrates how to adjust load depending on demand. This uses RL Coach and a custom environment.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/reinforcement_learning/rl_roboschool_ray&#34;&gt;Roboschool&lt;/a&gt; is an open source physics simulator that is commonly used to train RL policies for robotic systems. This notebook demonstrates training a few agents using it.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/reinforcement_learning/rl_roboschool_stable_baselines&#34;&gt;Stable Baselines&lt;/a&gt; In this notebook example, we will make the HalfCheetah agent learn to walk using the stable-baselines, which are a set of improved implementations of Reinforcement Learning (RL) algorithms based on OpenAI Baselines.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/reinforcement_learning/rl_traveling_salesman_vehicle_routing_coach&#34;&gt;Travelling Salesman&lt;/a&gt; is a classic NP hard problem, which this notebook solves with AWS SageMaker RL.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/reinforcement_learning/rl_tic_tac_toe_coach_customEnv&#34;&gt;Tic-tac-toe&lt;/a&gt; is a simple implementation of a custom Gym environment to train and deploy an RL agent in Coach that then plays tic-tac-toe interactively in a Jupyter Notebook.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/reinforcement_learning/rl_unity_ray&#34;&gt;Unity Game Agent&lt;/a&gt; shows how to use RL algorithms to train an agent to play Unity3D game.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Scientific Details of Algorithms&lt;/h3&gt; &#xA;&lt;p&gt;These examples provide more thorough mathematical treatment on a select group of algorithms.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/scientific_details_of_algorithms/streaming_median&#34;&gt;Streaming Median&lt;/a&gt; sequentially introduces concepts used in streaming algorithms, which many SageMaker algorithms rely on to deliver speed and scalability.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/scientific_details_of_algorithms/lda_topic_modeling&#34;&gt;Latent Dirichlet Allocation (LDA)&lt;/a&gt; dives into Amazon SageMaker&#39;s spectral decomposition approach to LDA.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/scientific_details_of_algorithms/linear_learner_class_weights_loss_functions&#34;&gt;Linear Learner features&lt;/a&gt; shows how to use the class weights and loss functions features of the SageMaker Linear Learner algorithm to improve performance on a credit card fraud prediction task&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Amazon SageMaker Debugger&lt;/h3&gt; &#xA;&lt;p&gt;These examples provide and introduction to SageMaker Debugger which allows debugging and monitoring capabilities for training of machine learning and deep learning algorithms. Note that although these notebooks focus on a specific framework, the same approach works with all the frameworks that Amazon SageMaker Debugger supports. The notebooks below are listed in the order in which we recommend you review them.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-debugger/tensorflow_builtin_rule/&#34;&gt;Using a built-in rule with TensorFlow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-debugger/tensorflow_keras_custom_rule/&#34;&gt;Using a custom rule with TensorFlow Keras&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-debugger/mnist_tensor_analysis/&#34;&gt;Interactive tensor analysis in notebook with MXNet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-debugger/mnist_tensor_plot/&#34;&gt;Visualizing Debugging Tensors of MXNet training&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-debugger/mxnet_realtime_analysis/&#34;&gt;Real-time analysis in notebook with MXNet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-debugger/xgboost_builtin_rules/&#34;&gt;Using a built in rule with XGBoost&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-debugger/xgboost_realtime_analysis/&#34;&gt;Real-time analysis in notebook with XGBoost&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-debugger/mxnet_spot_training/&#34;&gt;Using SageMaker Debugger with Managed Spot Training and MXNet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-debugger/tensorflow_action_on_rule/&#34;&gt;Reacting to CloudWatch Events from Rules to take an action based on status with TensorFlow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-debugger/pytorch_custom_container/&#34;&gt;Using SageMaker Debugger with a custom PyTorch container&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Amazon SageMaker Clarify&lt;/h3&gt; &#xA;&lt;p&gt;These examples provide an introduction to SageMaker Clarify which provides machine learning developers with greater visibility into their training data and models so they can identify and limit bias and explain predictions.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker_processing/fairness_and_explainability&#34;&gt;Fairness and Explainability with SageMaker Clarify&lt;/a&gt; shows how to use SageMaker Clarify Processor API to measure the pre-training bias of a dataset and post-training bias of a model, and explain the importance of the input features on the model&#39;s decision.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker_model_monitor/fairness_and_explainability&#34;&gt;Amazon SageMaker Clarify Model Monitors&lt;/a&gt; shows how to use SageMaker Clarify Model Monitor API to schedule bias monitor to monitor predictions for bias drift on a regular basis, and schedule explainability monitor to monitor predictions for feature attribution drift on a regular basis.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Publishing content from RStudio on Amazon SageMaker to RStudio Connect&lt;/h3&gt; &#xA;&lt;p&gt;These examples show you how to run R examples, and publish applications in RStudio on Amazon SageMaker to RStudio Connect.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/r_examples/rsconnect_rmarkdown/&#34;&gt;Publishing R Markdown&lt;/a&gt; shows how you can author an R Markdown document (.Rmd, .Rpres) within RStudio on Amazon SageMaker and publish to RStudio Connect for wide consumption.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/r_examples/rsconnect_shiny/&#34;&gt;Publishing R Shiny Apps&lt;/a&gt; shows how you can author an R Shiny application within RStudio on Amazon SageMaker and publish to RStudio Connect for wide consumption.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/r_examples/rsconnect_streamlit/&#34;&gt;Publishing Streamlit Apps&lt;/a&gt; shows how you can author a streamlit application withing Amazon SageMaker Studio and publish to RStudio Connect for wide consumption.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Advanced Amazon SageMaker Functionality&lt;/h3&gt; &#xA;&lt;p&gt;These examples showcase unique functionality available in Amazon SageMaker. They cover a broad range of topics and utilize a variety of methods, but aim to provide the user with sufficient insight or inspiration to develop within Amazon SageMaker.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/data_distribution_types&#34;&gt;Data Distribution Types&lt;/a&gt; showcases the difference between two methods for sending data from S3 to Amazon SageMaker Training instances. This has particular implication for scalability and accuracy of distributed training.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/handling_kms_encrypted_data&#34;&gt;Encrypting Your Data&lt;/a&gt; shows how to use Server Side KMS encrypted data with Amazon SageMaker training. The IAM role used for S3 access needs to have permissions to encrypt and decrypt data with the KMS key.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/parquet_to_recordio_protobuf&#34;&gt;Using Parquet Data&lt;/a&gt; shows how to bring &lt;a href=&#34;https://parquet.apache.org/&#34;&gt;Parquet&lt;/a&gt; data sitting in S3 into an Amazon SageMaker Notebook and convert it into the recordIO-protobuf format that many SageMaker algorithms consume.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/working_with_redshift_data&#34;&gt;Connecting to Redshift&lt;/a&gt; demonstrates how to copy data from Redshift to S3 and vice-versa without leaving Amazon SageMaker Notebooks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/xgboost_bring_your_own_model&#34;&gt;Bring Your Own XGBoost Model&lt;/a&gt; shows how to use Amazon SageMaker Algorithms containers to bring a pre-trained model to a realtime hosted endpoint without ever needing to think about REST APIs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/kmeans_bring_your_own_model&#34;&gt;Bring Your Own k-means Model&lt;/a&gt; shows how to take a model that&#39;s been fit elsewhere and use Amazon SageMaker Algorithms containers to host it.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/r_bring_your_own&#34;&gt;Bring Your Own R Algorithm&lt;/a&gt; shows how to bring your own algorithm container to Amazon SageMaker using the R language.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/install_r_kernel&#34;&gt;Installing the R Kernel&lt;/a&gt; shows how to install the R kernel into an Amazon SageMaker Notebook Instance.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/scikit_bring_your_own&#34;&gt;Bring Your Own scikit Algorithm&lt;/a&gt; provides a detailed walkthrough on how to package a scikit learn algorithm for training and production-ready hosting.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/mxnet_mnist_byom&#34;&gt;Bring Your Own MXNet Model&lt;/a&gt; shows how to bring a model trained anywhere using MXNet into Amazon SageMaker.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/tensorflow_iris_byom&#34;&gt;Bring Your Own TensorFlow Model&lt;/a&gt; shows how to bring a model trained anywhere using TensorFlow into Amazon SageMaker.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/search&#34;&gt;Experiment Management Capabilities with Search&lt;/a&gt; shows how to organize Training Jobs into projects, and track relationships between Models, Endpoints, and Training Jobs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/multi_model_bring_your_own&#34;&gt;Host Multiple Models with Your Own Algorithm&lt;/a&gt; shows how to deploy multiple models to a realtime hosted endpoint with your own custom algorithm.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/multi_model_xgboost_home_value&#34;&gt;Host Multiple Models with XGBoost&lt;/a&gt; shows how to deploy multiple models to a realtime hosted endpoint using a multi-model enabled XGBoost container.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/multi_model_sklearn_home_value&#34;&gt;Host Multiple Models with SKLearn&lt;/a&gt; shows how to deploy multiple models to a realtime hosted endpoint using a multi-model enabled SKLearn container.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-script-mode&#34;&gt;SageMaker Training and Inference with Script Mode&lt;/a&gt; shows how to use custom training and inference scripts, similar to those you would use outside of SageMaker, with SageMaker&#39;s prebuilt containers for various frameworks like Scikit-learn, PyTorch, and XGBoost.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-triton&#34;&gt;Host Models with NVidia Triton Server&lt;/a&gt; shows how to deploy models to a realtime hosted endpoint using &lt;a href=&#34;https://developer.nvidia.com/nvidia-triton-inference-server&#34;&gt;Triton&lt;/a&gt; as the model inference server.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Amazon SageMaker Neo Compilation Jobs&lt;/h3&gt; &#xA;&lt;p&gt;These examples provide an introduction to how to use Neo to compile and optimize deep learning models.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker_neo_compilation_jobs/gluoncv_ssd_mobilenet&#34;&gt;GluonCV SSD Mobilenet&lt;/a&gt; shows how to train GluonCV SSD MobileNet and use Amazon SageMaker Neo to compile and optimize the trained model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker_neo_compilation_jobs/imageclassification_caltech&#34;&gt;Image Classification&lt;/a&gt; Adapts from &lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/imageclassification_caltech&#34;&gt;image classification&lt;/a&gt; including Neo API and comparison against the uncompiled baseline.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker_neo_compilation_jobs/mxnet_mnist&#34;&gt;MNIST with MXNet&lt;/a&gt; Adapts from &lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-python-sdk/mxnet_mnist&#34;&gt;MXNet MNIST&lt;/a&gt; including Neo API and comparison against the uncompiled baseline.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker_neo_compilation_jobs/pytorch_torchvision&#34;&gt;Deploying pre-trained PyTorch vision models&lt;/a&gt; shows how to use Amazon SageMaker Neo to compile and optimize pre-trained PyTorch models from TorchVision.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker_neo_compilation_jobs/tensorflow_distributed_mnist&#34;&gt;Distributed TensorFlow&lt;/a&gt; includes Neo API and comparison against the uncompiled baseline.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker_neo_compilation_jobs/xgboost_customer_churn&#34;&gt;Predicting Customer Churn&lt;/a&gt; Adapts from &lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_applying_machine_learning/xgboost_customer_churn&#34;&gt;XGBoost customer churn&lt;/a&gt; including Neo API and comparison against the uncompiled baseline.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Amazon SageMaker Processing&lt;/h3&gt; &#xA;&lt;p&gt;These examples show you how to use SageMaker Processing jobs to run data processing workloads.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker_processing/scikit_learn_data_processing_and_model_evaluation&#34;&gt;Scikit-Learn Data Processing and Model Evaluation&lt;/a&gt; shows how to use SageMaker Processing and the Scikit-Learn container to run data preprocessing and model evaluation workloads.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker_processing/feature_transformation_with_sagemaker_processing&#34;&gt;Feature transformation with Amazon SageMaker Processing and SparkML&lt;/a&gt; shows how to use SageMaker Processing to run data processing workloads using SparkML prior to training.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker_processing/feature_transformation_with_sagemaker_processing_dask&#34;&gt;Feature transformation with Amazon SageMaker Processing and Dask&lt;/a&gt; shows how to use SageMaker Processing to transform data using Dask distributed clusters&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker_processing/spark_distributed_data_processing&#34;&gt;Distributed Data Processing using Apache Spark and SageMaker Processing&lt;/a&gt; shows how to use the built-in Spark container on SageMaker Processing using the SageMaker Python SDK.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Amazon SageMaker Pipelines&lt;/h3&gt; &#xA;&lt;p&gt;These examples show you how to use &lt;a href=&#34;https://aws.amazon.com/sagemaker/pipelines&#34;&gt;SageMaker Pipelines&lt;/a&gt; to create, automate and manage end-to-end Machine Learning workflows.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-pipelines/nlp/amazon_comprehend_sagemaker_pipeline&#34;&gt;Amazon Comprehend with SageMaker Pipelines&lt;/a&gt; shows how to deploy a custom text classification using Amazon Comprehend and SageMaker Pipelines.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-pipelines/time_series_forecasting/amazon_forecast_pipeline&#34;&gt;Amazon Forecast with SageMaker Pipelines&lt;/a&gt; shows how you can create a dataset, dataset group and predictor with Amazon Forecast and SageMaker Pipelines.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Amazon SageMaker Pre-Built Framework Containers and the Python SDK&lt;/h3&gt; &#xA;&lt;h4&gt;Pre-Built Deep Learning Framework Containers&lt;/h4&gt; &#xA;&lt;p&gt;These examples show you how to train and host in pre-built deep learning framework containers using the SageMaker Python SDK.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-python-sdk/chainer_cifar10&#34;&gt;Chainer CIFAR-10&lt;/a&gt; trains a VGG image classification network on CIFAR-10 using Chainer (both single machine and multi-machine versions are included)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-python-sdk/chainer_mnist&#34;&gt;Chainer MNIST&lt;/a&gt; trains a basic neural network on MNIST using Chainer (shows how to use local mode)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-python-sdk/chainer_sentiment_analysis&#34;&gt;Chainer sentiment analysis&lt;/a&gt; trains a LSTM network with embeddings to predict text sentiment using Chainer&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-python-sdk/scikit_learn_iris&#34;&gt;IRIS with Scikit-learn&lt;/a&gt; trains a Scikit-learn classifier on IRIS data&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-python-sdk/scikit_learn_model_registry_batch_transform&#34;&gt;Model Registry and Batch Transform with Scikit-learn&lt;/a&gt; trains a Scikit-learn Random Forest model, registers it in Model Registry, and runs a Batch Transform Job.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-python-sdk/mxnet_gluon_mnist&#34;&gt;MNIST with MXNet Gluon&lt;/a&gt; trains a basic neural network on the MNIST handwritten digit dataset using MXNet Gluon&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-python-sdk/mxnet_mnist&#34;&gt;MNIST with MXNet&lt;/a&gt; trains a basic neural network on the MNIST handwritten digit data using MXNet&#39;s symbolic syntax&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-python-sdk/mxnet_gluon_sentiment&#34;&gt;Sentiment Analysis with MXNet Gluon&lt;/a&gt; trains a text classifier using embeddings with MXNet Gluon&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-python-sdk/tensorflow_script_mode_training_and_serving&#34;&gt;TensorFlow training and serving&lt;/a&gt; trains a basic neural network on MNIST&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-python-sdk/tensorflow_script_mode_horovod&#34;&gt;TensorFlow with Horovod&lt;/a&gt; trains on MNIST using Horovod for distributed training&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-python-sdk/tensorflow_script_mode_using_shell_commands&#34;&gt;TensorFlow using shell commands&lt;/a&gt; shows how to use a shell script for the container&#39;s entry point&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Pre-Built Machine Learning Framework Containers&lt;/h4&gt; &#xA;&lt;p&gt;These examples show you how to build Machine Learning models with frameworks like Apache Spark or Scikit-learn using SageMaker Python SDK.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-python-sdk/sparkml_serving_emr_mleap_abalone&#34;&gt;Inference with SparkML Serving&lt;/a&gt; shows how to build an ML model with Apache Spark using Amazon EMR on Abalone dataset and deploy in SageMaker with SageMaker SparkML Serving.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-python-sdk/scikit_learn_inference_pipeline&#34;&gt;Pipeline Inference with Scikit-learn and LinearLearner&lt;/a&gt; builds a ML pipeline using Scikit-learn preprocessing and LinearLearner algorithm in single endpoint&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Using Amazon SageMaker with Apache Spark&lt;/h3&gt; &#xA;&lt;p&gt;These examples show how to use Amazon SageMaker for model training, hosting, and inference through Apache Spark using &lt;a href=&#34;https://github.com/aws/sagemaker-spark&#34;&gt;SageMaker Spark&lt;/a&gt;. SageMaker Spark allows you to interleave Spark Pipeline stages with Pipeline stages that interact with Amazon SageMaker.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-spark/pyspark_mnist&#34;&gt;MNIST with SageMaker PySpark&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Using Amazon SageMaker with Amazon Keyspaces (for Apache Cassandra)&lt;/h3&gt; &#xA;&lt;p&gt;These examples show how to use Amazon SageMaker to read data from &lt;a href=&#34;https://docs.aws.amazon.com/keyspaces/&#34;&gt;Amazon Keyspaces&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/ingest_data/sagemaker-keyspaces&#34;&gt;Train Machine Learning Models using Amazon Keyspaces as a Data Source&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;AWS Marketplace&lt;/h3&gt; &#xA;&lt;h4&gt;Create algorithms/model packages for listing in AWS Marketplace for machine learning.&lt;/h4&gt; &#xA;&lt;p&gt;These example notebooks show you how to package a model or algorithm for listing in AWS Marketplace for machine learning.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/creating_marketplace_products&#34;&gt;Creating Marketplace Products&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/creating_marketplace_products/models&#34;&gt;Creating a Model Package - Listing on AWS Marketplace&lt;/a&gt; provides a detailed walkthrough on how to package a pre-trained model as a SageMaker Model Package that can be listed on AWS Marketplace.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/creating_marketplace_products/algorithms&#34;&gt;Creating Algorithm and Model Package - Listing on AWS Marketplace&lt;/a&gt; provides a detailed walkthrough on how to package a scikit learn algorithm to create SageMaker Algorithm and SageMaker Model Package entities that can be used with the enhanced SageMaker Train/Transform/Hosting/Tuning APIs and listed on AWS Marketplace.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Once you have created an algorithm or a model package to be listed in the AWS Marketplace, the next step is to list it in AWS Marketplace, and provide a sample notebook that customers can use to try your algorithm or model package.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/curating_aws_marketplace_listing_and_sample_notebook/ModelPackage&#34;&gt;Curate your AWS Marketplace model package listing and sample notebook&lt;/a&gt; provides instructions on how to craft a sample notebook to be associated with your listing and how to curate a good AWS Marketplace listing that makes it easy for AWS customers to consume your model package.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/curating_aws_marketplace_listing_and_sample_notebook/Algorithm&#34;&gt;Curate your AWS Marketplace algorithm listing and sample notebook&lt;/a&gt; provides instructions on how to craft a sample notebook to be associated with your listing and how to curate a good AWS Marketplace listing that makes it easy for your customers to consume your algorithm.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Use algorithms, data, and model packages from AWS Marketplace.&lt;/h4&gt; &#xA;&lt;p&gt;These examples show you how to use model-packages and algorithms from AWS Marketplace and dataset products from AWS Data Exchange, for machine learning.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/using_algorithms&#34;&gt;Using Algorithms&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/using_algorithms/amazon_demo_product&#34;&gt;Using Algorithm From AWS Marketplace&lt;/a&gt; provides a detailed walkthrough on how to use Algorithm with the enhanced SageMaker Train/Transform/Hosting/Tuning APIs by choosing a canonical product listed on AWS Marketplace.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/using_algorithms/automl&#34;&gt;Using AutoML algorithm&lt;/a&gt; provides a detailed walkthrough on how to use AutoML algorithm from AWS Marketplace.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/using_model_packages&#34;&gt;Using Model Packages&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/using_model_packages/generic_sample_notebook&#34;&gt;Using Model Packages From AWS Marketplace&lt;/a&gt; is a generic notebook which provides sample code snippets you can modify and use for performing inference on Model Packages from AWS Marketplace, using Amazon SageMaker.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/using_model_packages/amazon_demo_product&#34;&gt;Using Amazon Demo product From AWS Marketplace&lt;/a&gt; provides a detailed walkthrough on how to use Model Package entities with the enhanced SageMaker Transform/Hosting APIs by choosing a canonical product listed on AWS Marketplace.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/using_model_packages/auto_insurance&#34;&gt;Using models for extracting vehicle metadata&lt;/a&gt; provides a detailed walkthrough on how to use pre-trained models from AWS Marketplace for extracting metadata for a sample use-case of auto-insurance claim processing.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/using_model_packages/improving_industrial_workplace_safety&#34;&gt;Using models for identifying non-compliance at a workplace&lt;/a&gt; provides a detailed walkthrough on how to use pre-trained models from AWS Marketplace for extracting metadata for a sample use-case of generating summary reports for identifying non-compliance at a construction/industrial workplace.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/using_model_packages/creative-writing-using-gpt-2-text-generation&#34;&gt;Creative writing using GPT-2 Text Generation&lt;/a&gt; will show you how to use AWS Marketplace GPT-2-XL pre-trained model on Amazon SageMaker to generate text based on your prompt to help you author prose and poetry.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/using_model_packages/amazon_augmented_ai_with_aws_marketplace_ml_models&#34;&gt;Amazon Augmented AI with AWS Marketplace ML models&lt;/a&gt; will show you how to use AWS Marketplace pre-trained ML models with Amazon Augmented AI to implement human-in-loop workflow reviews with your ML model predictions.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/using_model_packages/data_quality_monitoring&#34;&gt;Monitoring data quality in third-party models from AWS Marketplace&lt;/a&gt; will show you how to perform Data Quality monitoring on a pre-trained third-party model from AWS Marketplace.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/using_model_packages/evaluating_aws_marketplace_models_for_person_counting_use_case&#34;&gt;Evaluating ML models from AWS Marketplace for person counting use case&lt;/a&gt; will show you how to use two AWS Marketplace GluonCV pre-trained ML models for person counting use case and evaluate each model for performance in different types of crowd images.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/using_model_packages/preprocessing-audio-data-using-a-machine-learning-model&#34;&gt;Preprocessing audio data using a pre-trained machine learning model&lt;/a&gt; demonstrates the usage of a pre-trained audio track separation model to create synthetic features and improve an acoustic classification model.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/using_data&#34;&gt;Using Dataset Products&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/using_data/using_data_with_ml_model&#34;&gt;Using Dataset Product from AWS Data Exchange with ML model from AWS Marketplace&lt;/a&gt; is a sample notebook which shows how a dataset from AWS Data Exchange can be used with an ML Model Package from AWS Marketplace.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/using_data/image_classification_with_shutterstock_image_datasets&#34;&gt;Using Shutterstock Image Datasets to train Image Classification Models&lt;/a&gt; provides a detailed walkthrough on how to use the &lt;a href=&#34;https://aws.amazon.com/marketplace/pp/prodview-y6xuddt42fmbu?qid=1623195111604&amp;amp;sr=0-1&amp;amp;ref_=srh_res_product_title#offers&#34;&gt;Free Sample: Images &amp;amp; Metadata of “Whole Foods” Shoppers&lt;/a&gt; from Shutterstock&#39;s Image Datasets to train a multi-label image classification model using Shutterstock&#39;s pre-labeled image assets. You can learn more about this implementation &lt;a href=&#34;https://aws.amazon.com/blogs/awsmarketplace/using-shutterstocks-image-datasets-to-train-your-computer-vision-models/&#34;&gt;from this blog post&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;span&gt;⚖&lt;/span&gt; License&lt;/h2&gt; &#xA;&lt;p&gt;This library is licensed under the &lt;a href=&#34;http://aws.amazon.com/apache2.0/&#34;&gt;Apache 2.0 License&lt;/a&gt;. For more details, please take a look at the &lt;a href=&#34;https://github.com/aws/amazon-sagemaker-examples/raw/master/LICENSE.txt&#34;&gt;LICENSE&lt;/a&gt; file.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;🤝&lt;/span&gt; Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Although we&#39;re extremely excited to receive contributions from the community, we&#39;re still working on the best mechanism to take in examples from external sources. Please bear with us in the short-term if pull requests take longer than expected or are closed. Please read our &lt;a href=&#34;https://github.com/aws/amazon-sagemaker-examples/raw/master/CONTRIBUTING.md&#34;&gt;contributing guidelines&lt;/a&gt; if you&#39;d like to open an issue or submit a pull request.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Azure/MachineLearningNotebooks</title>
    <updated>2022-06-01T02:12:48Z</updated>
    <id>tag:github.com,2022-06-01:/Azure/MachineLearningNotebooks</id>
    <link href="https://github.com/Azure/MachineLearningNotebooks" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Python notebooks with ML and deep learning examples with Azure Machine Learning Python SDK | Microsoft&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Azure Machine Learning Python SDK notebooks&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;a community-driven repository of examples using mlflow for tracking can be found at &lt;a href=&#34;https://github.com/Azure/azureml-examples&#34;&gt;https://github.com/Azure/azureml-examples&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Welcome to the Azure Machine Learning Python SDK notebooks repository!&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;These notebooks are recommended for use in an Azure Machine Learning &lt;a href=&#34;https://docs.microsoft.com/azure/machine-learning/concept-compute-instance&#34;&gt;Compute Instance&lt;/a&gt;, where you can run them without any additional set up.&lt;/p&gt; &#xA;&lt;p&gt;However, the notebooks can be run in any development environment with the correct &lt;code&gt;azureml&lt;/code&gt; packages installed.&lt;/p&gt; &#xA;&lt;p&gt;Install the &lt;code&gt;azureml.core&lt;/code&gt; Python package:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install azureml-core&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install additional packages as needed:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install azureml-mlflow&#xA;pip install azureml-dataset-runtime&#xA;pip install azureml-automl-runtime&#xA;pip install azureml-pipeline&#xA;pip install azureml-pipeline-steps&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We recommend starting with one of the &lt;a href=&#34;https://raw.githubusercontent.com/Azure/MachineLearningNotebooks/master/tutorials/compute-instance-quickstarts&#34;&gt;quickstarts&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This repository is a push-only mirror. Pull requests are ignored.&lt;/p&gt; &#xA;&lt;h2&gt;Code of Conduct&lt;/h2&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. Please see the &lt;a href=&#34;https://raw.githubusercontent.com/Azure/MachineLearningNotebooks/master/CODE_OF_CONDUCT.md&#34;&gt;code of conduct&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;h2&gt;Reference&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/azure/machine-learning&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>nianticlabs/monodepth2</title>
    <updated>2022-06-01T02:12:48Z</updated>
    <id>tag:github.com,2022-06-01:/nianticlabs/monodepth2</id>
    <link href="https://github.com/nianticlabs/monodepth2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[ICCV 2019] Monocular depth estimation from a single image&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Monodepth2&lt;/h1&gt; &#xA;&lt;p&gt;This is the reference PyTorch implementation for training and testing depth estimation models using the method described in&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Digging into Self-Supervised Monocular Depth Prediction&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;http://www0.cs.ucl.ac.uk/staff/C.Godard/&#34;&gt;Clément Godard&lt;/a&gt;, &lt;a href=&#34;http://vision.caltech.edu/~macaodha/&#34;&gt;Oisin Mac Aodha&lt;/a&gt;, &lt;a href=&#34;http://www.michaelfirman.co.uk&#34;&gt;Michael Firman&lt;/a&gt; and &lt;a href=&#34;http://www0.cs.ucl.ac.uk/staff/g.brostow/&#34;&gt;Gabriel J. Brostow&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1806.01260&#34;&gt;ICCV 2019 (arXiv pdf)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/nianticlabs/monodepth2/master/assets/teaser.gif&#34; alt=&#34;example input output gif&#34; width=&#34;600&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;This code is for non-commercial use; please see the &lt;a href=&#34;https://raw.githubusercontent.com/nianticlabs/monodepth2/master/LICENSE&#34;&gt;license file&lt;/a&gt; for terms.&lt;/p&gt; &#xA;&lt;p&gt;If you find our work useful in your research please consider citing our paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{monodepth2,&#xA;  title     = {Digging into Self-Supervised Monocular Depth Prediction},&#xA;  author    = {Cl{\&#39;{e}}ment Godard and&#xA;               Oisin {Mac Aodha} and&#xA;               Michael Firman and&#xA;               Gabriel J. Brostow},&#xA;  booktitle = {The International Conference on Computer Vision (ICCV)},&#xA;  month = {October},&#xA;year = {2019}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;⚙️ Setup&lt;/h2&gt; &#xA;&lt;p&gt;Assuming a fresh &lt;a href=&#34;https://www.anaconda.com/download/&#34;&gt;Anaconda&lt;/a&gt; distribution, you can install the dependencies with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda install pytorch=0.4.1 torchvision=0.2.1 -c pytorch&#xA;pip install tensorboardX==1.4&#xA;conda install opencv=3.3.1   # just needed for evaluation&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We ran our experiments with PyTorch 0.4.1, CUDA 9.1, Python 3.6.6 and Ubuntu 18.04. We have also successfully trained models with PyTorch 1.0, and our code is compatible with Python 2.7. You may have issues installing OpenCV version 3.3.1 if you use Python 3.7, we recommend to create a virtual environment with Python 3.6.6 &lt;code&gt;conda create -n monodepth2 python=3.6.6 anaconda &lt;/code&gt;.&lt;/p&gt; &#xA;&lt;!-- We recommend using a [conda environment](https://conda.io/docs/user-guide/tasks/manage-environments.html) to avoid dependency conflicts.&#xA;&#xA;We also recommend using `pillow-simd` instead of `pillow` for faster image preprocessing in the dataloaders. --&gt; &#xA;&lt;h2&gt;🖼️ Prediction for a single image&lt;/h2&gt; &#xA;&lt;p&gt;You can predict scaled disparity for a single image with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python test_simple.py --image_path assets/test_image.jpg --model_name mono+stereo_640x192&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or, if you are using a stereo-trained model, you can estimate metric depth with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python test_simple.py --image_path assets/test_image.jpg --model_name mono+stereo_640x192 --pred_metric_depth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On its first run either of these commands will download the &lt;code&gt;mono+stereo_640x192&lt;/code&gt; pretrained model (99MB) into the &lt;code&gt;models/&lt;/code&gt; folder. We provide the following options for &lt;code&gt;--model_name&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;code&gt;--model_name&lt;/code&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Training modality&lt;/th&gt; &#xA;   &lt;th&gt;Imagenet pretrained?&lt;/th&gt; &#xA;   &lt;th&gt;Model resolution&lt;/th&gt; &#xA;   &lt;th&gt;KITTI abs. rel. error&lt;/th&gt; &#xA;   &lt;th&gt;delta &amp;lt; 1.25&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_640x192.zip&#34;&gt;&lt;code&gt;mono_640x192&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Mono&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;640 x 192&lt;/td&gt; &#xA;   &lt;td&gt;0.115&lt;/td&gt; &#xA;   &lt;td&gt;0.877&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_640x192.zip&#34;&gt;&lt;code&gt;stereo_640x192&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Stereo&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;640 x 192&lt;/td&gt; &#xA;   &lt;td&gt;0.109&lt;/td&gt; &#xA;   &lt;td&gt;0.864&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_640x192.zip&#34;&gt;&lt;code&gt;mono+stereo_640x192&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Mono + Stereo&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;640 x 192&lt;/td&gt; &#xA;   &lt;td&gt;0.106&lt;/td&gt; &#xA;   &lt;td&gt;0.874&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_1024x320.zip&#34;&gt;&lt;code&gt;mono_1024x320&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Mono&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;1024 x 320&lt;/td&gt; &#xA;   &lt;td&gt;0.115&lt;/td&gt; &#xA;   &lt;td&gt;0.879&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_1024x320.zip&#34;&gt;&lt;code&gt;stereo_1024x320&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Stereo&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;1024 x 320&lt;/td&gt; &#xA;   &lt;td&gt;0.107&lt;/td&gt; &#xA;   &lt;td&gt;0.874&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_1024x320.zip&#34;&gt;&lt;code&gt;mono+stereo_1024x320&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Mono + Stereo&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;1024 x 320&lt;/td&gt; &#xA;   &lt;td&gt;0.106&lt;/td&gt; &#xA;   &lt;td&gt;0.876&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_no_pt_640x192.zip&#34;&gt;&lt;code&gt;mono_no_pt_640x192&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Mono&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;640 x 192&lt;/td&gt; &#xA;   &lt;td&gt;0.132&lt;/td&gt; &#xA;   &lt;td&gt;0.845&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_no_pt_640x192.zip&#34;&gt;&lt;code&gt;stereo_no_pt_640x192&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Stereo&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;640 x 192&lt;/td&gt; &#xA;   &lt;td&gt;0.130&lt;/td&gt; &#xA;   &lt;td&gt;0.831&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_no_pt_640x192.zip&#34;&gt;&lt;code&gt;mono+stereo_no_pt_640x192&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Mono + Stereo&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;640 x 192&lt;/td&gt; &#xA;   &lt;td&gt;0.127&lt;/td&gt; &#xA;   &lt;td&gt;0.836&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;You can also download models trained on the odometry split with &lt;a href=&#34;https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_odom_640x192.zip&#34;&gt;monocular&lt;/a&gt; and &lt;a href=&#34;https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_odom_640x192.zip&#34;&gt;mono+stereo&lt;/a&gt; training modalities.&lt;/p&gt; &#xA;&lt;p&gt;Finally, we provide resnet 50 depth estimation models trained with &lt;a href=&#34;https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_resnet50_640x192.zip&#34;&gt;ImageNet pretrained weights&lt;/a&gt; and &lt;a href=&#34;https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_resnet50_no_pt_640x192.zip&#34;&gt;trained from scratch&lt;/a&gt;. Make sure to set &lt;code&gt;--num_layers 50&lt;/code&gt; if using these.&lt;/p&gt; &#xA;&lt;h2&gt;💾 KITTI training data&lt;/h2&gt; &#xA;&lt;p&gt;You can download the entire &lt;a href=&#34;http://www.cvlibs.net/datasets/kitti/raw_data.php&#34;&gt;raw KITTI dataset&lt;/a&gt; by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;wget -i splits/kitti_archives_to_download.txt -P kitti_data/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then unzip with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd kitti_data&#xA;unzip &#34;*.zip&#34;&#xA;cd ..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Warning:&lt;/strong&gt; it weighs about &lt;strong&gt;175GB&lt;/strong&gt;, so make sure you have enough space to unzip too!&lt;/p&gt; &#xA;&lt;p&gt;Our default settings expect that you have converted the png images to jpeg with this command, &lt;strong&gt;which also deletes the raw KITTI &lt;code&gt;.png&lt;/code&gt; files&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;find kitti_data/ -name &#39;*.png&#39; | parallel &#39;convert -quality 92 -sampling-factor 2x2,1x1,1x1 {.}.png {.}.jpg &amp;amp;&amp;amp; rm {}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;or&lt;/strong&gt; you can skip this conversion step and train from raw png files by adding the flag &lt;code&gt;--png&lt;/code&gt; when training, at the expense of slower load times.&lt;/p&gt; &#xA;&lt;p&gt;The above conversion command creates images which match our experiments, where KITTI &lt;code&gt;.png&lt;/code&gt; images were converted to &lt;code&gt;.jpg&lt;/code&gt; on Ubuntu 16.04 with default chroma subsampling &lt;code&gt;2x2,1x1,1x1&lt;/code&gt;. We found that Ubuntu 18.04 defaults to &lt;code&gt;2x2,2x2,2x2&lt;/code&gt;, which gives different results, hence the explicit parameter in the conversion command.&lt;/p&gt; &#xA;&lt;p&gt;You can also place the KITTI dataset wherever you like and point towards it with the &lt;code&gt;--data_path&lt;/code&gt; flag during training and evaluation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Splits&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The train/test/validation splits are defined in the &lt;code&gt;splits/&lt;/code&gt; folder. By default, the code will train a depth model using &lt;a href=&#34;https://github.com/tinghuiz/SfMLearner&#34;&gt;Zhou&#39;s subset&lt;/a&gt; of the standard Eigen split of KITTI, which is designed for monocular training. You can also train a model using the new &lt;a href=&#34;http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction&#34;&gt;benchmark split&lt;/a&gt; or the &lt;a href=&#34;http://www.cvlibs.net/datasets/kitti/eval_odometry.php&#34;&gt;odometry split&lt;/a&gt; by setting the &lt;code&gt;--split&lt;/code&gt; flag.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Custom dataset&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can train on a custom monocular or stereo dataset by writing a new dataloader class which inherits from &lt;code&gt;MonoDataset&lt;/code&gt; – see the &lt;code&gt;KITTIDataset&lt;/code&gt; class in &lt;code&gt;datasets/kitti_dataset.py&lt;/code&gt; for an example.&lt;/p&gt; &#xA;&lt;h2&gt;⏳ Training&lt;/h2&gt; &#xA;&lt;p&gt;By default models and tensorboard event files are saved to &lt;code&gt;~/tmp/&amp;lt;model_name&amp;gt;&lt;/code&gt;. This can be changed with the &lt;code&gt;--log_dir&lt;/code&gt; flag.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Monocular training:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python train.py --model_name mono_model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stereo training:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Our code defaults to using Zhou&#39;s subsampled Eigen training data. For stereo-only training we have to specify that we want to use the full Eigen training set – see paper for details.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python train.py --model_name stereo_model \&#xA;  --frame_ids 0 --use_stereo --split eigen_full&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Monocular + stereo training:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python train.py --model_name mono+stereo_model \&#xA;  --frame_ids 0 -1 1 --use_stereo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;GPUs&lt;/h3&gt; &#xA;&lt;p&gt;The code can only be run on a single GPU. You can specify which GPU to use with the &lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt; environment variable:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=2 python train.py --model_name mono_model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;All our experiments were performed on a single NVIDIA Titan Xp.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Training modality&lt;/th&gt; &#xA;   &lt;th&gt;Approximate GPU memory&lt;/th&gt; &#xA;   &lt;th&gt;Approximate training time&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mono&lt;/td&gt; &#xA;   &lt;td&gt;9GB&lt;/td&gt; &#xA;   &lt;td&gt;12 hours&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Stereo&lt;/td&gt; &#xA;   &lt;td&gt;6GB&lt;/td&gt; &#xA;   &lt;td&gt;8 hours&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mono + Stereo&lt;/td&gt; &#xA;   &lt;td&gt;11GB&lt;/td&gt; &#xA;   &lt;td&gt;15 hours&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;💽 Finetuning a pretrained model&lt;/h3&gt; &#xA;&lt;p&gt;Add the following to the training command to load an existing model for finetuning:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python train.py --model_name finetuned_mono --load_weights_folder ~/tmp/mono_model/models/weights_19&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;🔧 Other training options&lt;/h3&gt; &#xA;&lt;p&gt;Run &lt;code&gt;python train.py -h&lt;/code&gt; (or look at &lt;code&gt;options.py&lt;/code&gt;) to see the range of other training options, such as learning rates and ablation settings.&lt;/p&gt; &#xA;&lt;h2&gt;📊 KITTI evaluation&lt;/h2&gt; &#xA;&lt;p&gt;To prepare the ground truth depth maps run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python export_gt_depth.py --data_path kitti_data --split eigen&#xA;python export_gt_depth.py --data_path kitti_data --split eigen_benchmark&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;...assuming that you have placed the KITTI dataset in the default location of &lt;code&gt;./kitti_data/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The following example command evaluates the epoch 19 weights of a model named &lt;code&gt;mono_model&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python evaluate_depth.py --load_weights_folder ~/tmp/mono_model/models/weights_19/ --eval_mono&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For stereo models, you must use the &lt;code&gt;--eval_stereo&lt;/code&gt; flag (see note below):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python evaluate_depth.py --load_weights_folder ~/tmp/stereo_model/models/weights_19/ --eval_stereo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you train your own model with our code you are likely to see slight differences to the publication results due to randomization in the weights initialization and data loading.&lt;/p&gt; &#xA;&lt;p&gt;An additional parameter &lt;code&gt;--eval_split&lt;/code&gt; can be set. The three different values possible for &lt;code&gt;eval_split&lt;/code&gt; are explained here:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;code&gt;--eval_split&lt;/code&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Test set size&lt;/th&gt; &#xA;   &lt;th&gt;For models trained with...&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;&lt;code&gt;eigen&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;697&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;--split eigen_zhou&lt;/code&gt; (default) or &lt;code&gt;--split eigen_full&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The standard Eigen test files&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;&lt;code&gt;eigen_benchmark&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;652&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;--split eigen_zhou&lt;/code&gt; (default) or &lt;code&gt;--split eigen_full&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Evaluate with the improved ground truth from the &lt;a href=&#34;http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction&#34;&gt;new KITTI depth benchmark&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;&lt;code&gt;benchmark&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;500&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;--split benchmark&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The &lt;a href=&#34;http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction&#34;&gt;new KITTI depth benchmark&lt;/a&gt; test files.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Because no ground truth is available for the new KITTI depth benchmark, no scores will be reported when &lt;code&gt;--eval_split benchmark&lt;/code&gt; is set. Instead, a set of &lt;code&gt;.png&lt;/code&gt; images will be saved to disk ready for upload to the evaluation server.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;External disparities evaluation&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Finally you can also use &lt;code&gt;evaluate_depth.py&lt;/code&gt; to evaluate raw disparities (or inverse depth) from other methods by using the &lt;code&gt;--ext_disp_to_eval&lt;/code&gt; flag:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python evaluate_depth.py --ext_disp_to_eval ~/other_method_disp.npy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;📷📷 Note on stereo evaluation&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Our stereo models are trained with an effective baseline of &lt;code&gt;0.1&lt;/code&gt; units, while the actual KITTI stereo rig has a baseline of &lt;code&gt;0.54m&lt;/code&gt;. This means a scaling of &lt;code&gt;5.4&lt;/code&gt; must be applied for evaluation. In addition, for models trained with stereo supervision we disable median scaling. Setting the &lt;code&gt;--eval_stereo&lt;/code&gt; flag when evaluating will automatically disable median scaling and scale predicted depths by &lt;code&gt;5.4&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;⤴️⤵️ Odometry evaluation&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We include code for evaluating poses predicted by models trained with &lt;code&gt;--split odom --dataset kitti_odom --data_path /path/to/kitti/odometry/dataset&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For this evaluation, the &lt;a href=&#34;http://www.cvlibs.net/datasets/kitti/eval_odometry.php&#34;&gt;KITTI odometry dataset&lt;/a&gt; &lt;strong&gt;(color, 65GB)&lt;/strong&gt; and &lt;strong&gt;ground truth poses&lt;/strong&gt; zip files must be downloaded. As above, we assume that the pngs have been converted to jpgs.&lt;/p&gt; &#xA;&lt;p&gt;If this data has been unzipped to folder &lt;code&gt;kitti_odom&lt;/code&gt;, a model can be evaluated with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python evaluate_pose.py --eval_split odom_9 --load_weights_folder ./odom_split.M/models/weights_29 --data_path kitti_odom/&#xA;python evaluate_pose.py --eval_split odom_10 --load_weights_folder ./odom_split.M/models/weights_29 --data_path kitti_odom/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;📦 Precomputed results&lt;/h2&gt; &#xA;&lt;p&gt;You can download our precomputed disparity predictions from the following links:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Training modality&lt;/th&gt; &#xA;   &lt;th&gt;Input size&lt;/th&gt; &#xA;   &lt;th&gt;&lt;code&gt;.npy&lt;/code&gt; filesize&lt;/th&gt; &#xA;   &lt;th&gt;Eigen disparities&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mono&lt;/td&gt; &#xA;   &lt;td&gt;640 x 192&lt;/td&gt; &#xA;   &lt;td&gt;343 MB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_640x192_eigen.npy&#34;&gt;Download 🔗&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Stereo&lt;/td&gt; &#xA;   &lt;td&gt;640 x 192&lt;/td&gt; &#xA;   &lt;td&gt;343 MB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_640x192_eigen.npy&#34;&gt;Download 🔗&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mono + Stereo&lt;/td&gt; &#xA;   &lt;td&gt;640 x 192&lt;/td&gt; &#xA;   &lt;td&gt;343 MB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_640x192_eigen.npy&#34;&gt;Download 🔗&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mono&lt;/td&gt; &#xA;   &lt;td&gt;1024 x 320&lt;/td&gt; &#xA;   &lt;td&gt;914 MB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_1024x320_eigen.npy&#34;&gt;Download 🔗&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Stereo&lt;/td&gt; &#xA;   &lt;td&gt;1024 x 320&lt;/td&gt; &#xA;   &lt;td&gt;914 MB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_1024x320_eigen.npy&#34;&gt;Download 🔗&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mono + Stereo&lt;/td&gt; &#xA;   &lt;td&gt;1024 x 320&lt;/td&gt; &#xA;   &lt;td&gt;914 MB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_1024x320_eigen.npy&#34;&gt;Download 🔗&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;👩‍⚖️ License&lt;/h2&gt; &#xA;&lt;p&gt;Copyright © Niantic, Inc. 2019. Patent Pending. All rights reserved. Please see the &lt;a href=&#34;https://raw.githubusercontent.com/nianticlabs/monodepth2/master/LICENSE&#34;&gt;license file&lt;/a&gt; for terms.&lt;/p&gt;</summary>
  </entry>
</feed>