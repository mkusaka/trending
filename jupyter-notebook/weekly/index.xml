<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-07T01:57:25Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>lichao-sun/Mora</title>
    <updated>2024-04-07T01:57:25Z</updated>
    <id>tag:github.com,2024-04-07:/lichao-sun/Mora</id>
    <link href="https://github.com/lichao-sun/Mora" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Mora: More like Sora for Generalist Video Generation&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Mora: More like Sora for Generalist Video Generation&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;üîç See our newest Video Generation paper: &lt;a href=&#34;http://arxiv.org/abs/2403.13248&#34;&gt;&lt;strong&gt;&#34;Mora: Enabling Generalist Video Generation via A Multi-Agent Framework&#34;&lt;/strong&gt;&lt;/a&gt; &lt;a href=&#34;http://arxiv.org/abs/2403.13248&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Paper-%F0%9F%8E%93-lightblue?style=flat-square&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/lichao-sun/Mora&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Gtihub-%F0%9F%8E%93-lightblue?style=flat-square&#34; alt=&#34;GitHub&#34;&gt;)&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;üìß Please let us know if you find a mistake or have any suggestions by e-mail: &lt;a href=&#34;mailto:lis221@lehigh.edu&#34;&gt;lis221@lehigh.edu&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;üì∞ News&lt;/h2&gt; &#xA;&lt;p&gt;üöÄÔ∏è Mar 20: Our paper &#34;&lt;a href=&#34;https://arxiv.org/abs/2403.13248&#34;&gt;Mora: Enabling Generalist Video Generation via A Multi-Agent Framework&lt;/a&gt;&#34; is released!&lt;/p&gt; &#xA;&lt;h2&gt;What is Mora&lt;/h2&gt; &#xA;&lt;p&gt;Mora is a multi-agent framework designed to facilitate generalist video generation tasks, leveraging a collaborative approach with multiple visual agents. It aims to replicate and extend the capabilities of OpenAI&#39;s Sora. &lt;img src=&#34;https://raw.githubusercontent.com/lichao-sun/Mora/main/image/task.jpg&#34; alt=&#34;Task&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üìπ Demo for Artist Creation&lt;/h2&gt; &#xA;&lt;p&gt;Inspired by OpenAI &lt;a href=&#34;https://openai.com/blog/sora-first-impressions&#34;&gt;Sora: First Impressions&lt;/a&gt;, we utilize Mora to generate Shy kids video. Even though Mora has reached the similar level as Sora in terms of video duration, 80s, Mora still has a significant gap in terms of resolution, object consistency, motion smoothness, etc.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/JHL328/test/assets/55661930/abe276f7-12d3-4d24-aff3-7474296e854e&#34;&gt;https://github.com/JHL328/test/assets/55661930/abe276f7-12d3-4d24-aff3-7474296e854e&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üé• Demo (1024√ó576 resolution, 12 seconds and more!)&lt;/h2&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/lichao-sun/Mora/main/image/demo1.gif&#34; width=&#34;49%&#34; height=&#34;auto&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/lichao-sun/Mora/main/image/demo2.gif&#34; width=&#34;49%&#34; height=&#34;auto&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/lichao-sun/Mora/main/image/demo3.gif&#34; width=&#34;49%&#34; height=&#34;auto&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/lichao-sun/Mora/main/image/demo4.gif&#34; width=&#34;49%&#34; height=&#34;auto&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Mora: A Multi-Agent Framework for Video Generation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lichao-sun/Mora/main/image/method.jpg&#34; alt=&#34;test image&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-Agent Collaboration&lt;/strong&gt;: Utilizes several advanced visual AI agents, each specializing in different aspects of the video generation process, to achieve high-quality outcomes across various tasks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Broad Spectrum of Tasks&lt;/strong&gt;: Capable of performing text-to-video generation, text-conditional image-to-video generation, extending generated videos, video-to-video editing, connecting videos, and simulating digital worlds, thereby covering an extensive range of video generation applications.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Open-Source and Extendable&lt;/strong&gt;: Mora‚Äôs open-source nature fosters innovation and collaboration within the community, allowing for continuous improvement and customization.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Proven Performance&lt;/strong&gt;: Experimental results demonstrate Mora&#39;s ability to achieve performance that is close to that of Sora in various tasks, making it a compelling open-source alternative for the video generation domain.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Results&lt;/h2&gt; &#xA;&lt;h3&gt;Text-to-video generation&lt;/h3&gt; &#xA;&lt;table class=&#34;left&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;b&gt;Input prompt&lt;/b&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;b&gt;Output video&lt;/b&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;A vibrant coral reef teeming with life under the crystal-clear blue ocean, with colorful fish swimming among the coral, rays of sunlight filtering through the water, and a gentle current moving the sea plants. &lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lichao-sun/Mora/main/image/task_1_demo_1.gif&#34; width=&#34;480&#34; height=&#34;auto&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;A majestic mountain range covered in snow, with the peaks touching the clouds and a crystal-clear lake at its base, reflecting the mountains and the sky, creating a breathtaking natural mirror.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lichao-sun/Mora/main/image/task_1_demo_2.gif&#34; width=&#34;480&#34; height=&#34;auto&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;In the middle of a vast desert, a golden desert city appears on the horizon, its architecture a blend of ancient Egyptian and futuristic elements.The city is surrounded by a radiant energy barrier, while in the air, seve&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lichao-sun/Mora/main/image/task_1_demo_3.gif&#34; width=&#34;480&#34; height=&#34;auto&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Text-conditional image-to-video generation&lt;/h3&gt; &#xA;&lt;table class=&#34;left&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;b&gt;Input prompt&lt;/b&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;b&gt;Input image&lt;/b&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;b&gt;Mora generated Video&lt;/b&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;b&gt;Sora generated Video&lt;/b&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Monster Illustration in the flat design style of a diverse family of monsters. The group includes a furry brown monster, a sleek black monster with antennas, a spotted green monster, and a tiny polka-dotted monster, all interacting in a playful environment. &lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lichao-sun/Mora/main/image/input1.jpg&#34; width=&#34;600&#34; height=&#34;90&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lichao-sun/Mora/main/image/task2_demo1.gif&#34; width=&#34;160&#34; height=&#34;90&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lichao-sun/Mora/main/image/sora_demo1.gif&#34; width=&#34;160&#34; height=&#34;90&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;An image of a realistic cloud that spells ‚ÄúSORA‚Äù.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lichao-sun/Mora/main/image/input2.jpg&#34; width=&#34;600&#34; height=&#34;90&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lichao-sun/Mora/main/image/task2_demo2.gif&#34; width=&#34;160&#34; height=&#34;90&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lichao-sun/Mora/main/image/sora_demo2.gif&#34; width=&#34;160&#34; height=&#34;90&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Extend generated video&lt;/h3&gt; &#xA;&lt;table class=&#34;left&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;b&gt;Original video&lt;/b&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;b&gt;Mora extended video&lt;/b&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;b&gt;Sora extended video&lt;/b&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;./image/original video.gif&#34; width=&#34;330&#34; height=&#34;auto&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lichao-sun/Mora/main/image/mora_task3.gif&#34; width=&#34;330&#34; height=&#34;auto&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lichao-sun/Mora/main/image/task3_sora.gif&#34; width=&#34;330&#34; height=&#34;auto&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Video-to-video editing&lt;/h3&gt; &#xA;&lt;table class=&#34;left&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;b&gt;Instruction&lt;/b&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;b&gt;Original video&lt;/b&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;b&gt;Mora edited Video&lt;/b&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;b&gt;Sora edited Video&lt;/b&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Change the setting to the 1920s with an old school car. make sure to keep the red color.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lichao-sun/Mora/main/image/task4_original.gif&#34; width=&#34;240&#34; height=&#34;auto&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lichao-sun/Mora/main/image/task4_mora_1920.gif&#34; width=&#34;240&#34; height=&#34;auto&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lichao-sun/Mora/main/image/task4_sora_1920.gif&#34; width=&#34;240&#34; height=&#34;auto&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Put the video in space with a rainbow road&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lichao-sun/Mora/main/image/task4_original.gif&#34; width=&#34;240&#34; height=&#34;auto&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lichao-sun/Mora/main/image/task4_mora_rainbow.gif&#34; width=&#34;240&#34; height=&#34;auto&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lichao-sun/Mora/main/image/task4_sora_rainbow.gif&#34; width=&#34;240&#34; height=&#34;auto&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Connect videos&lt;/h3&gt; &#xA;&lt;table class=&#34;left&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;b&gt;Input previous video&lt;/b&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;b&gt;Input next video&lt;/b&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;b&gt;Output connect Video&lt;/b&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lichao-sun/Mora/main/image/task5_mora1.gif&#34; width=&#34;300&#34; height=&#34;auto&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lichao-sun/Mora/main/image/task5_mora2.gif&#34; width=&#34;300&#34; height=&#34;auto&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lichao-sun/Mora/main/image/task5_mora.gif&#34; width=&#34;300&#34; height=&#34;auto&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lichao-sun/Mora/main/image/task5_sora1.gif&#34; width=&#34;300&#34; height=&#34;auto&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lichao-sun/Mora/main/image/task5_sora2.gif&#34; width=&#34;300&#34; height=&#34;auto&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lichao-sun/Mora/main/image/task5_sora.gif&#34; width=&#34;300&#34; height=&#34;auto&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Simulate digital worlds&lt;/h3&gt; &#xA;&lt;table class=&#34;left&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;b&gt;Mora simulating video&lt;/b&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;b&gt;Sora simulating video&lt;/b&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lichao-sun/Mora/main/image/task6_mora1.gif&#34; width=&#34;100%&#34; height=&#34;auto&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lichao-sun/Mora/main/image/task6_sora1.gif&#34; width=&#34;100%&#34; height=&#34;auto&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lichao-sun/Mora/main/image/task6_mora2.gif&#34; width=&#34;100%&#34; height=&#34;auto&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lichao-sun/Mora/main/image/task6_sora2.gif&#34; width=&#34;100%&#34; height=&#34;auto&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Code will be released as soon as possible!&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{yuan2024mora,&#xA;  title={Mora: Enabling Generalist Video Generation via A Multi-Agent Framework},&#xA;  author={Yuan, Zhengqing and Chen, Ruoxi and Li, Zhaoxu and Jia, Haolong and He, Lifang and Wang, Chi and Sun, Lichao},&#xA;  journal={arXiv preprint arXiv:2403.13248},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{liu2024sora,&#xA;  title={Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models},&#xA;  author={Liu, Yixin and Zhang, Kai and Li, Yuan and Yan, Zhiling and Gao, Chujie and Chen, Ruoxi and Yuan, Zhengqing and Huang, Yue and Sun, Hanchi and Gao, Jianfeng and others},&#xA;  journal={arXiv preprint arXiv:2402.17177},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{openai2024sorareport,&#xA;  title={Video generation models as world simulators},&#xA;  author={OpenAI},&#xA;  year={2024},&#xA;  howpublished={https://openai.com/research/video-generation-models-as-world-simulators},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>ora-io/keras2circom</title>
    <updated>2024-04-07T01:57:25Z</updated>
    <id>tag:github.com,2024-04-07:/ora-io/keras2circom</id>
    <link href="https://github.com/ora-io/keras2circom" rel="alternate"></link>
    <summary type="html">&lt;p&gt;python tool to transpile a tf.keras model into a circom circuit&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;keras2circom&lt;/h1&gt; &#xA;&lt;p&gt;keras2circom is a python tool that transpiles a tf.keras model into a circom circuit.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;First, clone the repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/socathie/keras2circom.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, install the dependencies. You can use pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you use conda, you can also create a new environment with the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda env create -f environment.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You will also need to install circom and snarkjs. You can run the following commands to install them:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash setup-circom.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Last but not least, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;To use the package, you can run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python main.py &amp;lt;model_path&amp;gt; [-o &amp;lt;output_dir&amp;gt;] [--raw]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For example, to transpile the model in &lt;code&gt;models/model.h5&lt;/code&gt; into a circom circuit, you can run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python main.py models/model.h5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The output will be in the &lt;code&gt;output&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;p&gt;If you want to transpile the model into a circom circuit with &#34;raw&#34; output, i.e. no ArgMax at the end, you can run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python main.py models/model.h5 --raw&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Testing&lt;/h2&gt; &#xA;&lt;p&gt;To test the package, you can run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm test&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>mshumer/gpt-llm-trainer</title>
    <updated>2024-04-07T01:57:25Z</updated>
    <id>tag:github.com,2024-04-07:/mshumer/gpt-llm-trainer</id>
    <link href="https://github.com/mshumer/gpt-llm-trainer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;gpt-llm-trainer&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://twitter.com/mattshumer_&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/mattshumer_?style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;NEW: Claude 3 -&amp;gt; LLaMA 2 7B Fine-Tuning version: &lt;a href=&#34;https://colab.research.google.com/drive/1eLe0t8Alu997w5Ewnw9mE96dtaC-qEho?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open Claude -&gt; LLaMA Version In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;LLaMA 2 7B Fine-Tuning version: &lt;a href=&#34;https://colab.research.google.com/drive/1mV9sAY4QBKLmS58dpFGHgwCXQKRASR31?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open LLaMA Version In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;GPT-3.5 Fine-Tuning version: &lt;a href=&#34;https://colab.research.google.com/drive/1NLqxHHCv3kFyw45t8k_CUfNlcepMdeDW?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open GPT-3.5 Version In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;Training models is hard. You have to collect a dataset, clean it, get it in the right format, select a model, write the training code and train it. And that&#39;s the best-case scenario.&lt;/p&gt; &#xA;&lt;p&gt;The goal of this project is to explore an experimental new pipeline to train a high-performing task-specific model. We try to abstract away all the complexity, so it&#39;s as easy as possible to go from idea -&amp;gt; performant fully-trained model.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Simply input a description of your task, and the system will generate a dataset from scratch, parse it into the right format, and fine-tune a LLaMA 2 or GPT-3.5 model for you.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Dataset Generation&lt;/strong&gt;: Using Claude 3 or GPT-4, &lt;code&gt;gpt-llm-trainer&lt;/code&gt; will generate a variety of prompts and responses based on the provided use-case.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;System Message Generation&lt;/strong&gt;: &lt;code&gt;gpt-llm-trainer&lt;/code&gt; will generate an effective system prompt for your model.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Fine-Tuning&lt;/strong&gt;: After your dataset has been generated, the system will automatically split it into training and validation sets, fine-tune a model for you, and get it ready for inference.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1mV9sAY4QBKLmS58dpFGHgwCXQKRASR31?usp=sharing&#34;&gt;Open the notebook in Google Colab&lt;/a&gt; or in a local Jupyter notebook.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you&#39;re using Colab, switch to the best GPU available (go to Runtime -&amp;gt; change runtime type).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add your OpenAI API key to the line &lt;code&gt;openai.api_key = &#34;YOUR KEY HERE&#34;&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;How to Use&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Define your &lt;code&gt;prompt&lt;/code&gt;. The &lt;code&gt;prompt&lt;/code&gt; is a description of what you want the trained AI to do. The more descriptive and clear you can be, the better. Additionally, set the temperature we will use to generate your dataset (high=creative, low=precise), and the number of examples you want to generate (100 is a good starting point).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;prompt = &#34;A model that takes in a puzzle-like reasoning-heavy question in English, and responds with a well-reasoned, step-by-step thought out response in Spanish.&#34;&#xA;temperature = .4&#xA;number_of_examples = 100&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Run all the cells (stop at &lt;code&gt;Merge the model and store in Google Drive&lt;/code&gt; if using the LLaMA 2 version).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;em&gt;It&#39;ll take some time (from 10 minutes to a couple of hours, depending on how many examples you generate), but soon, you&#39;ll have your fine-tuned model!&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;After your model is trained, you can use the &lt;code&gt;Run Inference&lt;/code&gt; cell (on the LLaMA 2 version) or the &lt;code&gt;Let&#39;s try it out!&lt;/code&gt; cell (on the GPT-3.5 version) to test the model, and the cells below that allow you to save and load the model to and from Google Drive for later use if you are using the LLaMA version. If you&#39;re using the OpenAI version, your model will be available for use via the API or in the OpenAI Playground.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Contributions are welcome! Some ideas:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;improve the example generation pipeline for efficiency/cost reduction (using n=)&lt;/li&gt; &#xA; &lt;li&gt;add additional example generation prompts to create more diverse examples&lt;/li&gt; &#xA; &lt;li&gt;add example pruning, removing very similar examples to improve performance&lt;/li&gt; &#xA; &lt;li&gt;use GPT-4 to automatically choose the training hyperparameters (and potentially, even the model to fine-tune!) based on a few examples + high-level dataset details (i.e. number of examples)&lt;/li&gt; &#xA; &lt;li&gt;train multiple model variants and choose the one with the lowest eval loss&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Huge shoutout to &lt;a href=&#34;https://twitter.com/maximelabonne&#34;&gt;Maxime Labonne&lt;/a&gt; for the training code that was used in this repo!&lt;/h2&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is &lt;a href=&#34;https://github.com/mshumer/gpt-llm-trainer/raw/master/LICENSE&#34;&gt;MIT&lt;/a&gt; licensed.&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;Matt Shumer - &lt;a href=&#34;https://twitter.com/mattshumer_&#34;&gt;@mattshumer_&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Lastly, if you want to try something even cooler than this, sign up for &lt;a href=&#34;https://www.hyperwriteai.com/personal-assistant&#34;&gt;Personal Assistant&lt;/a&gt; (most of my time is spent on this). It&#39;s basically an AI that can operate your web browser to complete tasks for you.&lt;/p&gt;</summary>
  </entry>
</feed>