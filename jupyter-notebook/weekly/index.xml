<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-06-08T01:43:30Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>google-ai-edge/ai-edge-torch</title>
    <updated>2025-06-08T01:43:30Z</updated>
    <id>tag:github.com,2025-06-08:/google-ai-edge/ai-edge-torch</id>
    <link href="https://github.com/google-ai-edge/ai-edge-torch" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Supporting PyTorch models with the Google AI Edge TFLite runtime.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AI Edge Torch&lt;/h1&gt; &#xA;&lt;p&gt;AI Edge Torch is a python library that supports converting PyTorch models into a .tflite format, which can then be run with TensorFlow Lite and MediaPipe. This enables applications for Android, iOS and IOT that can run models completely on-device. AI Edge Torch offers broad CPU coverage, with initial GPU and NPU support. AI Edge Torch seeks to closely integrate with PyTorch, building on top of torch.export() and providing good coverage of Core ATen operators.&lt;/p&gt; &#xA;&lt;p&gt;To get started converting PyTorch models to TF Lite, see additional details in the &lt;a href=&#34;https://raw.githubusercontent.com/google-ai-edge/ai-edge-torch/main/#pytorch-converter&#34;&gt;PyTorch converter&lt;/a&gt; section. For the particular case of Large Language Models (LLMs) and transformer-based models, the &lt;a href=&#34;https://raw.githubusercontent.com/google-ai-edge/ai-edge-torch/main/#generative-api&#34;&gt;Generative API&lt;/a&gt; supports model authoring and quantization to enable improved on device performance.&lt;/p&gt; &#xA;&lt;p&gt;Although part of the same PyPi package, the PyTorch converter is a Beta release, while the Generative API is an Alpha release. Please see the &lt;a href=&#34;https://github.com/google-ai-edge/ai-edge-torch/releases/&#34;&gt;release notes&lt;/a&gt; for additional information.&lt;/p&gt; &#xA;&lt;h2&gt;PyTorch Converter&lt;/h2&gt; &#xA;&lt;p&gt;Here are the steps needed to convert a PyTorch model to a TFLite flatbuffer:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;import torchvision&#xA;import ai_edge_torch&#xA;&#xA;# Use resnet18 with pre-trained weights.&#xA;resnet18 = torchvision.models.resnet18(torchvision.models.ResNet18_Weights.IMAGENET1K_V1)&#xA;sample_inputs = (torch.randn(1, 3, 224, 224),)&#xA;&#xA;# Convert and serialize PyTorch model to a tflite flatbuffer. Note that we&#xA;# are setting the model to evaluation mode prior to conversion.&#xA;edge_model = ai_edge_torch.convert(resnet18.eval(), sample_inputs)&#xA;edge_model.export(&#34;resnet18.tflite&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/google-ai-edge/ai-edge-torch/main/docs/pytorch_converter/getting_started.ipynb&#34;&gt;getting started&lt;/a&gt; Jupyter notebook gives an initial walkthrough of the conversion process and can be tried out with Google Colab.&lt;/p&gt; &#xA;&lt;p&gt;Additional technical details of the PyTorch Converter are &lt;a href=&#34;https://raw.githubusercontent.com/google-ai-edge/ai-edge-torch/main/docs/pytorch_converter/README.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Generative API&lt;/h2&gt; &#xA;&lt;p&gt;The AI Edge Torch Generative API is a Torch native library for authoring mobile-optimized PyTorch Transformer models, which can be converted to TFLite, allowing users to easily deploy Large Language Models (LLMs) on mobile devices. Users can convert the models using the AI Edge Torch PyTorch Converter, and run them via the TensorFlow Lite runtime. See &lt;a href=&#34;https://raw.githubusercontent.com/google-ai-edge/ai-edge-torch/main/ai_edge_torch/generative/examples/cpp&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Mobile app developers can also use the Edge Generative API to integrate PyTorch LLMs directly with the MediaPipe LLM Inference API for easy integration within their application code. See &lt;a href=&#34;http://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference#ai_edge_model_conversion&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;More detailed documentation can be found &lt;a href=&#34;https://raw.githubusercontent.com/google-ai-edge/ai-edge-torch/main/ai_edge_torch/generative&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The Generative API is currently CPU-only, with planned support for GPU and NPU. A further future direction is to collaborate with the PyTorch community to ensure that frequently used transformer abstractions can be directly supported without reauthoring.&lt;/p&gt; &#xA;&lt;h2&gt;Build Status&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Build Type&lt;/th&gt; &#xA;   &lt;th&gt;Status&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Generative API (Linux)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/google-ai-edge/ai-edge-torch/actions/workflows/nightly_generative_api.yml&#34;&gt;&lt;img src=&#34;https://github.com/google-ai-edge/ai-edge-torch/actions/workflows/nightly_generative_api.yml/badge.svg?branch=main&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Model Coverage (Linux)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/google-ai-edge/ai-edge-torch/actions/workflows/nightly_model_coverage.yml&#34;&gt;&lt;img src=&#34;https://github.com/google-ai-edge/ai-edge-torch/actions/workflows/nightly_model_coverage.yml/badge.svg?branch=main&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Unit tests (Linux)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/google-ai-edge/ai-edge-torch/actions/workflows/nightly_unittests.yml&#34;&gt;&lt;img src=&#34;https://github.com/google-ai-edge/ai-edge-torch/actions/workflows/nightly_unittests.yml/badge.svg?branch=main&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Nightly Release&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/google-ai-edge/ai-edge-torch/actions/workflows/nightly_release.yml&#34;&gt;&lt;img src=&#34;https://github.com/google-ai-edge/ai-edge-torch/actions/workflows/nightly_release.yml/badge.svg?branch=main&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Requirements and Dependencies&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python versions: &amp;gt;=3.10&lt;/li&gt; &#xA; &lt;li&gt;Operating system: Linux&lt;/li&gt; &#xA; &lt;li&gt;PyTorch: &lt;a href=&#34;https://pypi.org/project/torch/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/torch-%3E=2.4.0-blue&#34; alt=&#34;torch&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;TensorFlow: &lt;a href=&#34;https://pypi.org/project/tf-nightly/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/tf--nightly-latest-blue&#34; alt=&#34;tf-nightly&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- requirement badges are updated by ci/update_nightly_versions.py --&gt; &#xA;&lt;h3&gt;Python Virtual Env&lt;/h3&gt; &#xA;&lt;p&gt;Set up a Python virtualenv:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m venv --prompt ai-edge-torch venv&#xA;source venv/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The latest stable release can be installed with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install ai-edge-torch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternately, the nightly version can be installed with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install ai-edge-torch-nightly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The list of versioned releases can be seen &lt;a href=&#34;https://github.com/google-ai-edge/ai-edge-torch/releases&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The full list of PyPi releases (including nightly builds) can be seen &lt;a href=&#34;https://pypi.org/project/ai-edge-torch/#history&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;See our &lt;a href=&#34;https://raw.githubusercontent.com/google-ai-edge/ai-edge-torch/main/CONTRIBUTING.md&#34;&gt;contribution documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Getting Help&lt;/h1&gt; &#xA;&lt;p&gt;Please &lt;a href=&#34;https://github.com/google-ai-edge/ai-edge-torch/issues/new/choose&#34;&gt;create a GitHub issue&lt;/a&gt; with any questions.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>SakanaAI/AI-Scientist</title>
    <updated>2025-06-08T01:43:30Z</updated>
    <id>tag:github.com,2025-06-08:/SakanaAI/AI-Scientist</id>
    <link href="https://github.com/SakanaAI/AI-Scientist" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery üßë‚Äçüî¨&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/SakanaAI/AI-Scientist/raw/main/docs/logo_2.png&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/docs/logo_2.png&#34; width=&#34;215&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;b&gt;The AI Scientist: Towards Fully Automated&lt;/b&gt;&lt;br&gt; &lt;b&gt;Open-Ended Scientific Discovery üßë‚Äçüî¨&lt;/b&gt;&lt;br&gt; &lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; üìö &lt;a href=&#34;https://arxiv.org/abs/2408.06292&#34;&gt;[Paper]&lt;/a&gt; | üìù &lt;a href=&#34;https://sakana.ai/ai-scientist/&#34;&gt;[Blog Post]&lt;/a&gt; | üìÇ &lt;a href=&#34;https://drive.google.com/drive/folders/1G7A0wTqfXVa-cpexjk0oaXakaSJwffEt&#34;&gt;[Drive Folder]&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;One of the grand challenges of artificial intelligence is developing agents capable of conducting scientific research and discovering new knowledge. While frontier models have already been used to aid human scientists‚Äîfor example, for brainstorming ideas or writing code‚Äîthey still require extensive manual supervision or are heavily constrained to specific tasks.&lt;/p&gt; &#xA;&lt;p&gt;We&#39;re excited to introduce &lt;strong&gt;The AI Scientist&lt;/strong&gt;, the first comprehensive system for fully automatic scientific discovery, enabling Foundation Models such as Large Language Models (LLMs) to perform research independently.&lt;/p&gt; &#xA;&lt;p&gt;We provide all runs and data from our paper &lt;a href=&#34;https://drive.google.com/drive/folders/1G7A0wTqfXVa-cpexjk0oaXakaSJwffEt?usp=sharing&#34;&gt;here&lt;/a&gt;, where we run each base model on each template for approximately 50 ideas. We &lt;em&gt;highly&lt;/em&gt; recommend reading through some of the &lt;a href=&#34;https://drive.google.com/drive/folders/1Mmpz6M1FK4q8e-SewgZcUzdeD0Q2zC39?usp=sharing&#34;&gt;Claude papers&lt;/a&gt; to get a sense of the system&#39;s strengths and weaknesses. Here are some example papers generated by &lt;strong&gt;The AI Scientist&lt;/strong&gt; üìù:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/SakanaAI/AI-Scientist/raw/main/example_papers/adaptive_dual_scale_denoising.pdf&#34;&gt;DualScale Diffusion: Adaptive Feature Balancing for Low-Dimensional Generative Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/SakanaAI/AI-Scientist/raw/main/example_papers/grid_based_noise_adaptation.pdf&#34;&gt;Multi-scale Grid Noise Adaptation: Enhancing Diffusion Models For Low-dimensional Data&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/SakanaAI/AI-Scientist/raw/main/example_papers/gan_diffusion.pdf&#34;&gt;GAN-Enhanced Diffusion: Boosting Sample Quality and Diversity&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/SakanaAI/AI-Scientist/tree/main/example_papers/dual_expert_denoiser.pdf&#34;&gt;DualDiff: Enhancing Mode Capture in Low-dimensional Diffusion Models via Dual-expert Denoising&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/SakanaAI/AI-Scientist/raw/main/example_papers/multi_style_adapter.pdf&#34;&gt;StyleFusion: Adaptive Multi-style Generation in Character-Level Language Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/SakanaAI/AI-Scientist/tree/main/example_papers/rl_lr_adaptation.pdf&#34;&gt;Adaptive Learning Rates for Transformers via Q-Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/SakanaAI/AI-Scientist/tree/main/example_papers/weight_initialization_grokking.pdf&#34;&gt;Unlocking Grokking: A Comparative Study of Weight Initialization Strategies in Transformer Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/SakanaAI/AI-Scientist/tree/main/example_papers/layerwise_lr_grokking.pdf&#34;&gt;Grokking Accelerated: Layer-wise Learning Rates for Transformer Generalization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/SakanaAI/AI-Scientist/tree/main/example_papers/mdl_grokking_correlation.pdf&#34;&gt;Grokking Through Compression: Unveiling Sudden Generalization via Minimal Description Length&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/SakanaAI/AI-Scientist/tree/main/example_papers/data_augmentation_grokking.pdf&#34;&gt;Accelerating Mathematical Insight: Boosting Grokking Through Strategic Data Augmentation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;&lt;br&gt; &lt;strong&gt;Caution!&lt;/strong&gt; This codebase will execute LLM-written code. There are various risks and challenges associated with this autonomy, including the use of potentially dangerous packages, web access, and potential spawning of processes. Use at your own discretion. Please make sure to &lt;a href=&#34;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#containerization&#34;&gt;containerize&lt;/a&gt; and restrict web access appropriately.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/SakanaAI/AI-Scientist/raw/main/example_papers/adaptive_dual_scale_denoising/adaptive_dual_scale_denoising.pdf&#34;&gt;&lt;img src=&#34;https://github.com/SakanaAI/AI-Scientist/raw/main/docs/anim-ai-scientist.gif&#34; alt=&#34;Adaptive Dual Scale Denoising&#34; width=&#34;80%&#34;&gt; &lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#requirements&#34;&gt;Requirements&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#supported-models-and-api-keys&#34;&gt;Supported Models and API Keys&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#setting-up-the-templates&#34;&gt;Setting Up the Templates&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#nanogpt-template&#34;&gt;NanoGPT Template&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#2d-diffusion-template&#34;&gt;2D Diffusion Template&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#grokking-template&#34;&gt;Grokking Template&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#run-ai-scientist-paper-generation-experiments&#34;&gt;Run AI Scientist Paper Generation Experiments&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#getting-an-llm-generated-paper-review&#34;&gt;Getting an LLM-Generated Paper Review&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#making-your-own-template&#34;&gt;Making Your Own Template&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#community-contributed-templates&#34;&gt;Community-Contributed Templates&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#template-resources&#34;&gt;Template Resources&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#citing-the-ai-scientist&#34;&gt;Citing The AI Scientist&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#frequently-asked-questions&#34;&gt;Frequently Asked Questions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#containerization&#34;&gt;Containerization&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;We provide three templates, which were used in our paper, covering the following domains: &lt;strong&gt;NanoGPT&lt;/strong&gt;, &lt;strong&gt;2D Diffusion&lt;/strong&gt;, and &lt;strong&gt;Grokking&lt;/strong&gt;. These templates enable The AI Scientist to generate ideas and conduct experiments in these areas. We accept contributions of new templates from the community, but please note that they are not maintained by us. All other templates beyond the three provided are community contributions.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;This code is designed to run on Linux with NVIDIA GPUs using CUDA and PyTorch. Support for other GPU architectures may be possible by following the &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;PyTorch guidelines&lt;/a&gt;. The current templates would likely take an infeasible amount of time on CPU-only machines. Running on other operating systems may require significant adjustments.&lt;/p&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n ai_scientist python=3.11&#xA;conda activate ai_scientist&#xA;# Install pdflatex&#xA;sudo apt-get install texlive-full&#xA;&#xA;# Install PyPI requirements&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Installing &lt;code&gt;texlive-full&lt;/code&gt; can take a long time. You may need to &lt;a href=&#34;https://askubuntu.com/questions/956006/pregenerating-context-markiv-format-this-may-take-some-time-takes-forever&#34;&gt;hold Enter&lt;/a&gt; during the installation.&lt;/p&gt; &#xA;&lt;h3&gt;Supported Models and API Keys&lt;/h3&gt; &#xA;&lt;p&gt;We support a wide variety of models, including open-weight and API-only models. In general, we recommend using only frontier models above the capability of the original GPT-4. To see a full list of supported models, see &lt;a href=&#34;https://github.com/SakanaAI/AI-Scientist/raw/main/ai_scientist/llm.py&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;OpenAI API (GPT-4o, GPT-4o-mini, o1 models)&lt;/h4&gt; &#xA;&lt;p&gt;By default, this uses the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; environment variable.&lt;/p&gt; &#xA;&lt;h4&gt;Anthropic API (Claude Sonnet 3.5)&lt;/h4&gt; &#xA;&lt;p&gt;By default, this uses the &lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt; environment variable.&lt;/p&gt; &#xA;&lt;h5&gt;Claude Models via Bedrock&lt;/h5&gt; &#xA;&lt;p&gt;For Claude models provided by &lt;a href=&#34;https://aws.amazon.com/bedrock/&#34;&gt;Amazon Bedrock&lt;/a&gt;, please install these additional packages:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install anthropic[bedrock]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, specify a set of valid &lt;a href=&#34;https://docs.aws.amazon.com/cli/v1/userguide/cli-configure-envvars.html&#34;&gt;AWS Credentials&lt;/a&gt; and the target &lt;a href=&#34;https://docs.aws.amazon.com/bedrock/latest/userguide/bedrock-regions.html&#34;&gt;AWS Region&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;p&gt;Set the environment variables: &lt;code&gt;AWS_ACCESS_KEY_ID&lt;/code&gt;, &lt;code&gt;AWS_SECRET_ACCESS_KEY&lt;/code&gt;, &lt;code&gt;AWS_REGION_NAME&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h5&gt;Claude Models via Vertex AI&lt;/h5&gt; &#xA;&lt;p&gt;For Claude models provided by &lt;a href=&#34;https://cloud.google.com/model-garden?hl=en&#34;&gt;Vertex AI Model Garden&lt;/a&gt;, please install these additional packages:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install google-cloud-aiplatform&#xA;pip install anthropic[vertex]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, set up valid authentication for a &lt;a href=&#34;https://cloud.google.com/vertex-ai/docs/authentication&#34;&gt;Google Cloud project&lt;/a&gt;, for example by providing the region and project ID:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CLOUD_ML_REGION=&#34;REGION&#34;           # for Model Garden call&#xA;export ANTHROPIC_VERTEX_PROJECT_ID=&#34;PROJECT_ID&#34;  # for Model Garden call&#xA;export VERTEXAI_LOCATION=&#34;REGION&#34;         # for Aider/LiteLLM call&#xA;export VERTEXAI_PROJECT=&#34;PROJECT_ID&#34;      # for Aider/LiteLLM call&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;DeepSeek API (deepseek-chat, deepseek-reasoner)&lt;/h4&gt; &#xA;&lt;p&gt;By default, this uses the &lt;code&gt;DEEPSEEK_API_KEY&lt;/code&gt; environment variable.&lt;/p&gt; &#xA;&lt;h4&gt;OpenRouter API (Llama3.1)&lt;/h4&gt; &#xA;&lt;p&gt;By default, this uses the &lt;code&gt;OPENROUTER_API_KEY&lt;/code&gt; environment variable.&lt;/p&gt; &#xA;&lt;h4&gt;Google Gemini&lt;/h4&gt; &#xA;&lt;p&gt;We support Google Gemini models (e.g., &#34;gemini-1.5-flash&#34;, &#34;gemini-1.5-pro&#34;) via the &lt;a href=&#34;https://pypi.org/project/google-generativeai&#34;&gt;google-generativeai&lt;/a&gt; Python library. By default, it uses the environment variable:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export GEMINI_API_KEY=&#34;YOUR GEMINI API KEY&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Semantic Scholar API (Literature Search)&lt;/h4&gt; &#xA;&lt;p&gt;Our code can also optionally use a Semantic Scholar API Key (&lt;code&gt;S2_API_KEY&lt;/code&gt;) for higher throughput &lt;a href=&#34;https://www.semanticscholar.org/product/api&#34;&gt;if you have one&lt;/a&gt;, though it should work without it in principle. If you have problems with Semantic Scholar, you can skip the literature search and citation phases of paper generation.&lt;/p&gt; &#xA;&lt;p&gt;Be sure to provide the key for the model used for your runs, e.g.:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export OPENAI_API_KEY=&#34;YOUR KEY HERE&#34;&#xA;export S2_API_KEY=&#34;YOUR KEY HERE&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;OpenAlex API (Literature Search Alternative)&lt;/h4&gt; &#xA;&lt;p&gt;OpenAlex API can be used as an alternative if you do not have a Semantic Scholar API Key. OpenAlex does not require API key.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install pyalex&#xA;export OPENALEX_MAIL_ADDRESS=&#34;YOUR EMAIL ADDRESS&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And specify &lt;code&gt;--engine openalex&lt;/code&gt; when you execute the AI Scientist code.&lt;/p&gt; &#xA;&lt;p&gt;Note that this is experimental for those who do not have a Semantic Scholar API Key.&lt;/p&gt; &#xA;&lt;h2&gt;Setting Up the Templates&lt;/h2&gt; &#xA;&lt;p&gt;This section provides instructions for setting up each of the three templates used in our paper. Before running The AI Scientist experiments, please ensure you have completed the setup steps for the templates you are interested in.&lt;/p&gt; &#xA;&lt;h3&gt;NanoGPT Template&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Description:&lt;/strong&gt; This template investigates transformer-based autoregressive next-token prediction tasks.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Setup Steps:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Prepare the data:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python data/enwik8/prepare.py&#xA;python data/shakespeare_char/prepare.py&#xA;python data/text8/prepare.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Create baseline runs (machine dependent):&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Set up NanoGPT baseline run&#xA;# NOTE: YOU MUST FIRST RUN THE PREPARE SCRIPTS ABOVE!&#xA;cd templates/nanoGPT&#xA;python experiment.py --out_dir run_0&#xA;python plot.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;2D Diffusion Template&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Description:&lt;/strong&gt; This template studies improving the performance of diffusion generative models on low-dimensional datasets.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Setup Steps:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install dependencies:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Set up 2D Diffusion&#xA;git clone https://github.com/gregversteeg/NPEET.git&#xA;cd NPEET&#xA;pip install .&#xA;pip install scikit-learn&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Create baseline runs:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Set up 2D Diffusion baseline run&#xA;cd templates/2d_diffusion&#xA;python experiment.py --out_dir run_0&#xA;python plot.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Grokking Template&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Description:&lt;/strong&gt; This template investigates questions about generalization and learning speed in deep neural networks.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Setup Steps:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install dependencies:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Set up Grokking&#xA;pip install einops&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Create baseline runs:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Set up Grokking baseline run&#xA;cd templates/grokking&#xA;python experiment.py --out_dir run_0&#xA;python plot.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Run AI Scientist Paper Generation Experiments&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Please ensure the setup steps above are completed before running these experiments.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda activate ai_scientist&#xA;# Run the paper generation.&#xA;python launch_scientist.py --model &#34;gpt-4o-2024-05-13&#34; --experiment nanoGPT_lite --num-ideas 2&#xA;python launch_scientist.py --model &#34;claude-3-5-sonnet-20241022&#34; --experiment nanoGPT_lite --num-ideas 2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you have more than one GPU, use the &lt;code&gt;--parallel&lt;/code&gt; option to parallelize ideas across multiple GPUs.&lt;/p&gt; &#xA;&lt;h2&gt;Getting an LLM-Generated Paper Review&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import openai&#xA;from ai_scientist.perform_review import load_paper, perform_review&#xA;&#xA;client = openai.OpenAI()&#xA;model = &#34;gpt-4o-2024-05-13&#34;&#xA;&#xA;# Load paper from PDF file (raw text)&#xA;paper_txt = load_paper(&#34;report.pdf&#34;)&#xA;&#xA;# Get the review dictionary&#xA;review = perform_review(&#xA;    paper_txt,&#xA;    model,&#xA;    client,&#xA;    num_reflections=5,&#xA;    num_fs_examples=1,&#xA;    num_reviews_ensemble=5,&#xA;    temperature=0.1,&#xA;)&#xA;&#xA;# Inspect review results&#xA;review[&#34;Overall&#34;]    # Overall score (1-10)&#xA;review[&#34;Decision&#34;]   # &#39;Accept&#39; or &#39;Reject&#39;&#xA;review[&#34;Weaknesses&#34;] # List of weaknesses (strings)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run batch analysis:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd review_iclr_bench&#xA;python iclr_analysis.py --num_reviews 500 --batch_size 100 --num_fs_examples 1 --num_reflections 5 --temperature 0.1 --num_reviews_ensemble 5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Making Your Own Template&lt;/h2&gt; &#xA;&lt;p&gt;If there is an area of study you would like &lt;strong&gt;The AI Scientist&lt;/strong&gt; to explore, it is straightforward to create your own templates. In general, follow the structure of the existing templates, which consist of:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;experiment.py&lt;/code&gt; ‚Äî This is the main script where the core content is. It takes an argument &lt;code&gt;--out_dir&lt;/code&gt;, which specifies where it should create the folder and save the relevant information from the run.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;plot.py&lt;/code&gt; ‚Äî This script takes the information from the &lt;code&gt;run&lt;/code&gt; folders and creates plots. The code should be clear and easy to edit.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;prompt.json&lt;/code&gt; ‚Äî Put information about your template here.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;seed_ideas.json&lt;/code&gt; ‚Äî Place example ideas here. You can also try to generate ideas without any examples and then pick the best one or two to put here.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;latex/template.tex&lt;/code&gt; ‚Äî We recommend using our LaTeX folder but be sure to replace the pre-loaded citations with ones that you expect to be more relevant.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The key to making new templates work is matching the base filenames and output JSONs to the existing format; everything else is free to change. You should also ensure that the &lt;code&gt;template.tex&lt;/code&gt; file is updated to use the correct citation style / base plots for your template.&lt;/p&gt; &#xA;&lt;h3&gt;Community-Contributed Templates&lt;/h3&gt; &#xA;&lt;p&gt;We welcome community contributions in the form of new templates. While these are not maintained by us, we are delighted to highlight your templates to others. Below, we list community-contributed templates along with links to their pull requests (PRs):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Infectious Disease Modeling (&lt;code&gt;seir&lt;/code&gt;) - &lt;a href=&#34;https://github.com/SakanaAI/AI-Scientist/pull/137&#34;&gt;PR #137&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Image Classification with MobileNetV3 (&lt;code&gt;mobilenetV3&lt;/code&gt;) - &lt;a href=&#34;https://github.com/SakanaAI/AI-Scientist/pull/141&#34;&gt;PR #141&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Sketch RNN (&lt;code&gt;sketch_rnn&lt;/code&gt;) - &lt;a href=&#34;https://github.com/SakanaAI/AI-Scientist/pull/143&#34;&gt;PR #143&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;AI in Quantum Chemistry (&lt;code&gt;MACE&lt;/code&gt;) - &lt;a href=&#34;https://github.com/SakanaAI/AI-Scientist/pull/157&#34;&gt;PR#157&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Earthquake Prediction (&lt;code&gt;earthquake-prediction&lt;/code&gt;) - &lt;a href=&#34;https://github.com/SakanaAI/AI-Scientist/pull/167&#34;&gt;PR #167&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Tensorial Radiance Fields (&lt;code&gt;tensorf&lt;/code&gt;) - &lt;a href=&#34;https://github.com/SakanaAI/AI-Scientist/pull/175&#34;&gt;PR #175&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Large Language Model Steering / Probes (&lt;code&gt;probes&lt;/code&gt;) - &lt;a href=&#34;https://github.com/SakanaAI/AI-Scientist/pull/215&#34;&gt;PR #215&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;This section is reserved for community contributions. Please submit a pull request to add your template to the list! Please describe the template in the PR description, and also show examples of the generated papers.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Template Resources&lt;/h2&gt; &#xA;&lt;p&gt;We provide three templates, which heavily use code from other repositories, credited below:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;NanoGPT Template&lt;/strong&gt; uses code from &lt;a href=&#34;https://github.com/karpathy/nanoGPT&#34;&gt;NanoGPT&lt;/a&gt; and this &lt;a href=&#34;https://github.com/karpathy/nanoGPT/pull/254&#34;&gt;PR&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2D Diffusion Template&lt;/strong&gt; uses code from &lt;a href=&#34;https://github.com/tanelp/tiny-diffusion&#34;&gt;tiny-diffusion&lt;/a&gt;, &lt;a href=&#34;https://github.com/lucidrains/ema-pytorch&#34;&gt;ema-pytorch&lt;/a&gt;, and &lt;a href=&#34;https://www.research.autodesk.com/publications/same-stats-different-graphs/&#34;&gt;Datasaur&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Grokking Template&lt;/strong&gt; uses code from &lt;a href=&#34;https://github.com/Sea-Snell/grokking&#34;&gt;Sea-Snell/grokking&lt;/a&gt; and &lt;a href=&#34;https://github.com/danielmamay/grokking&#34;&gt;danielmamay/grokking&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We would like to thank the developers of the open-source models and packages for their contributions and for making their work available.&lt;/p&gt; &#xA;&lt;h2&gt;Citing The AI Scientist&lt;/h2&gt; &#xA;&lt;p&gt;If you use &lt;strong&gt;The AI Scientist&lt;/strong&gt; in your research, please cite it as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{lu2024aiscientist,&#xA;  title={The {AI} {S}cientist: Towards Fully Automated Open-Ended Scientific Discovery},&#xA;  author={Lu, Chris and Lu, Cong and Lange, Robert Tjarko and Foerster, Jakob and Clune, Jeff and Ha, David},&#xA;  journal={arXiv preprint arXiv:2408.06292},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Frequently Asked Questions&lt;/h2&gt; &#xA;&lt;p&gt;We recommend reading our paper first for any questions you have on The AI Scientist.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Why am I missing files when running The AI Scientist?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Ensure you have completed all the setup and preparation steps before the main experiment script.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Why has a PDF or a review not been generated?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The AI Scientist finishes an idea with a success rate that depends on the template, the base foundation model, and the complexity of the idea. We advise referring to our main paper. The highest success rates are observed with Claude Sonnet 3.5. Reviews are best done with GPT-4o; all other models have issues with positivity bias or failure to conform to required outputs.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;What is the cost of each idea generated?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Typically less than $15 per paper with Claude Sonnet 3.5. We recommend DeepSeek Coder V2 for a much more cost-effective approach. A good place to look for new models is the &lt;a href=&#34;https://aider.chat/docs/leaderboards/&#34;&gt;Aider leaderboard&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;How do I change the base conference format associated with the write-ups?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Change the base &lt;code&gt;template.tex&lt;/code&gt; files contained within each template.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;How do I run The AI Scientist for different subject fields?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Please refer to the instructions for different templates. In this current iteration, this is restricted to ideas that can be expressed in code. However, lifting this restriction would represent exciting future work! :)&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;How do I add support for a new foundation model?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You may modify &lt;code&gt;ai_scientist/llm.py&lt;/code&gt; to add support for a new foundation model. We do not advise using any model that is significantly weaker than GPT-4 level for &lt;strong&gt;The AI Scientist&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Why do I need to run the baseline runs myself?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;These appear as &lt;code&gt;run_0&lt;/code&gt; and should be run per machine you execute &lt;strong&gt;The AI Scientist&lt;/strong&gt; on for accurate run-time comparisons due to hardware differences.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;What if I have problems accessing the Semantic Scholar API?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We use the Semantic Scholar API to check ideas for novelty and collect citations for the paper write-up. You may be able to skip these phases if you don&#39;t have an API key or the API is slow to access.&lt;/p&gt; &#xA;&lt;h2&gt;Containerization&lt;/h2&gt; &#xA;&lt;p&gt;We include a &lt;a href=&#34;https://github.com/SakanaAI/AI-Scientist/pull/21&#34;&gt;community-contributed&lt;/a&gt; Docker image that may assist with your containerization efforts in &lt;code&gt;experimental/Dockerfile&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can use this image like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Endpoint Script&#xA;docker run -e OPENAI_API_KEY=$OPENAI_API_KEY -v `pwd`/templates:/app/AI-Scientist/templates &amp;lt;AI_SCIENTIST_IMAGE&amp;gt; \&#xA;  --model gpt-4o-2024-05-13 \&#xA;  --experiment 2d_diffusion \&#xA;  --num-ideas 2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Interactive&#xA;docker run -it -e OPENAI_API_KEY=$OPENAI_API_KEY \&#xA;  --entrypoint /bin/bash \&#xA;  &amp;lt;AI_SCIENTIST_IMAGE&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#SakanaAI/AI-Scientist&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=SakanaAI/AI-Scientist&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>unslothai/notebooks</title>
    <updated>2025-06-08T01:43:30Z</updated>
    <id>tag:github.com,2025-06-08:/unslothai/notebooks</id>
    <link href="https://github.com/unslothai/notebooks" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Fine-tune LLMs for free with guided Notebooks on Google Colab, Kaggle, and more.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://unsloth.ai&#34;&gt;&#xA;   &lt;picture&gt; &#xA;    &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20white%20text.png&#34;&gt; &#xA;    &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png&#34;&gt; &#xA;    &lt;img alt=&#34;unsloth logo&#34; src=&#34;https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png&#34; height=&#34;110&#34; style=&#34;max-width: 100%;&#34;&gt; &#xA;   &lt;/picture&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/unslothai/unsloth/main/images/start%20free%20finetune%20button.png&#34; height=&#34;48&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/unsloth&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord%20button.png&#34; height=&#34;48&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://docs.unsloth.ai&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/unslothai/unsloth/refs/heads/main/images/Documentation%20Button.png&#34; height=&#34;48&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;!-- üõë üö® DO NOT EDIT MANUALLY THIS SECTION UNTIL `end of notebook links`!! üõë üö® --&gt; &#xA;&lt;!-- üõë üö® THIS SECTION IS GENERATED BY `update_all_notebooks.py` AUTOMATICALLY üõë üö®  --&gt; &#xA;&lt;h2&gt;üìí Fine-tuning Notebooks&lt;/h2&gt; &#xA;&lt;p&gt;Below are our notebooks for Google Colab categorized by model. You can view our &lt;a href=&#34;https://github.com/unslothai/notebooks/#-kaggle-notebooks&#34;&gt;Kaggle notebooks here&lt;/a&gt;.&lt;br&gt;Use our guided notebooks to prep data, train, evaluate, and save your model. View our main &lt;a href=&#34;https://github.com/unslothai/unsloth&#34;&gt;GitHub repo here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Main Notebooks&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Notebook Link&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Qwen3 (14B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Conversational&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_%2814B%29-Reasoning-Conversational.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Qwen3-Base (4B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GRPO&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_%284B%29-GRPO.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Gemma 3 (4B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Conversational&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_%284B%29.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Llama 3.2 (3B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Conversational&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_%281B_and_3B%29-Conversational.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Phi-4 (14B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Conversational&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Llama 3.2 Vision (11B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Vision&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_%2811B%29-Vision.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Llama 3.1 (8B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Alpaca&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_%288B%29-Alpaca.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Mistral v0.3 (7B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Conversational&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_%287B%29-Conversational.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;DeepSeek-R1-0528-Qwen3 (8B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GRPO&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/DeepSeek_R1_0528_Qwen3_(8B)_GRPO.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Llama 3.2 (3B) by Meta&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Synthetic Data&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Meta_Synthetic_Data_Llama3_2_%283B%29.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Sesame-CSM (1B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;TTS&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Sesame_CSM_%281B%29-TTS.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Llama Notebooks&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Notebook Link&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Llama 3.2 (1B and 3B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Conversational&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_%281B_and_3B%29-Conversational.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Llama 3.2 (1B and 3B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GRPO&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Advanced_Llama3_2_(3B)_GRPO_LoRA.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Llama 3.1 (8B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GRPO&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_%288B%29-GRPO.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Llama 3.2 (11B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Vision&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_%2811B%29-Vision.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Llama 3.2 (1B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;RAFT&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_%281B%29-RAFT.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Llama 3.1 (8B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Inference&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_%288B%29-Inference.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Llama 3.1 (8B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Alpaca&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_%288B%29-Alpaca.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Llama 3 (8B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Ollama&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_%288B%29-Ollama.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Llama 3 (8B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Alpaca&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_%288B%29-Alpaca.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Llama 3 (8B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ORPO&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_%288B%29-ORPO.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Llama 3 (8B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Conversational&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_%288B%29-Conversational.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Meta Synthetic Data Llama 3.2 (3B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Data&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Meta_Synthetic_Data_Llama3_2_%283B%29.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Qwen Notebooks&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Notebook Link&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Qwen3 (14B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Reasoning Conversational&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_%2814B%29-Reasoning-Conversational.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Qwen3-Base (14B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Alpaca&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_%2814B%29-Alpaca.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Qwen 2.5 (3B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GRPO&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_%283B%29-GRPO.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Qwen2.5 Coder (1.5B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Tool Calling&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_Coder_%281.5B%29-Tool_Calling.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Qwen2.5 VL (7B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Vision&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_VL_%287B%29-Vision.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Qwen2.5 Coder (14B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Conversational&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_Coder_%2814B%29-Conversational.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Qwen2.5 (7B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Alpaca&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_%287B%29-Alpaca.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Qwen2 VL (7B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Vision&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_VL_%287B%29-Vision.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Qwen2 (7B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Alpaca&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_%287B%29-Alpaca.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Gemma Notebooks&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Notebook Link&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Gemma 3 (4B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Conversational&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_%284B%29.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Gemma 3 (1B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GRPO&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_%281B%29-GRPO.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Gemma 2 (2B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Alpaca&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma2_%282B%29-Alpaca.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Gemma 2 (9B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Alpaca&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma2_%289B%29-Alpaca.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;CodeGemma (7B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Conversational&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/CodeGemma_%287B%29-Conversational.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Mistral Notebooks&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Notebook Link&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Mistral v0.3 (7B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Conversational&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_%287B%29-Conversational.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Mistral v0.3 (7B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;CPT&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_%287B%29-CPT.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Mistral v0.3 (7B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GRPO&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_%287B%29-GRPO.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Mistral v0.3 (7B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Alpaca&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_%287B%29-Alpaca.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Pixtral (12B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Vision&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Pixtral_%2812B%29-Vision.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Zephyr (7B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;DPO&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Zephyr_%287B%29-DPO.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Mistral Small (22B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Alpaca&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_Small_%2822B%29-Alpaca.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Mistral Nemo (12B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Alpaca&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_Nemo_%2812B%29-Alpaca.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Mistral (7B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Text Completion&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_%287B%29-Text_Completion.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Phi Notebooks&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Notebook Link&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Phi-4&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Conversational&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Phi-4 (14B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GRPO&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4_%2814B%29-GRPO.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Phi-3.5 Mini&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Conversational&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_3.5_Mini-Conversational.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Phi-3 Medium&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Conversational&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_3_Medium-Conversational.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Text-to-Speech (TTS) Notebooks&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Notebook Link&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Sesame-CSM&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;TTS&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Sesame_CSM_%281B%29-TTS.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Orpheus-TTS&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;TTS&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Orpheus_%283B%29-TTS.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Spark-TTS&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;TTS&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Spark_TTS_%280_5B%29.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Oute-TTS&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;TTS&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Oute_TTS_%281B%29.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Oute-TTS&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;TTS&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Oute_TTS_%281B%29.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Llasa TTS (1B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;TTS&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llasa_TTS_%281B%29.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Llasa TTS (3B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;TTS&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llasa_TTS_%283B%29.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Whisper-Large-V3&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;STT&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Whisper.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Vision (Multimodal) Notebooks&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Notebook Link&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Llama 3.2 (11B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Vision&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_%2811B%29-Vision.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Qwen2.5 VL (7B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Vision&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_VL_%287B%29-Vision.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Pixtral (12B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Vision&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Pixtral_%2812B%29-Vision.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;BERT Notebooks&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Notebook Link&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;ModernBERT-large&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/timothelaborie/text_classification_scripts/blob/main/bert_classification.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Specific use-case Notebooks&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Usecase&lt;/th&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Notebook Link&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Text Classification&lt;/td&gt; &#xA;   &lt;td&gt;Llama 3.1 (8B)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/timothelaborie/text_classification_scripts/blob/main/unsloth_classification.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Tool Calling&lt;/td&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder (1.5B)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_Coder_(1.5B)-Tool_Calling.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Multiple Datasets&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1njCCbE1YVal9xC83hjdo2hiGItpY_D6t?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;KTO&lt;/td&gt; &#xA;   &lt;td&gt;Qwen2.5-Instruct (1.5B)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1MRgGtLWuZX4ypSfGguFgC-IblTvO2ivM?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Inference Chat UI&lt;/td&gt; &#xA;   &lt;td&gt;LLaMa 3.2 Vision&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Unsloth_Studio.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Conversational&lt;/td&gt; &#xA;   &lt;td&gt;LLaMa 3.2 (1B and 3B)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChatML&lt;/td&gt; &#xA;   &lt;td&gt;Mistral (7B)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/15F1xyn8497_dUbxZP4zWmPZ3PJx1Oymv?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Text Completion&lt;/td&gt; &#xA;   &lt;td&gt;Mistral (7B)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_(7B)-Text_Completion.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;GRPO Notebooks&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Notebook Link&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;DeepSeek-R1-0528-Qwen3 (8B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GRPO&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/DeepSeek_R1_0528_Qwen3_(8B)_GRPO.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Phi-4 (14B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GRPO&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4_%2814B%29-GRPO.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Llama 3.1 (8B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GRPO&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_%288B%29-GRPO.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Gemma 3 (1B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GRPO&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_%281B%29-GRPO.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Qwen3-Base (4B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GRPO&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_%284B%29-GRPO.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Qwen 2.5 (3B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GRPO&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_%283B%29-GRPO.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Mistral v0.3 (7B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GRPO&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_%287B%29-GRPO.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Other Notebooks&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Notebook Link&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Qwen2.5 Coder (1.5B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Tool Calling&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_Coder_%281.5B%29-Tool_Calling.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;TinyLlama (1.1B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Alpaca&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/TinyLlama_%281.1B%29-Alpaca.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Meta Synthetic Data Llama 3.1 (8B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GRPO&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Meta-Synthetic-Data-Llama3.1_%288B%29.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Unsloth&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Studio&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Unsloth_Studio.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;CodeForces cot Finetune for Reasoning on CodeForces&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Reasoning&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/CodeForces-cot-Finetune_for_Reasoning_on_CodeForces.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;üìí Kaggle Notebooks&lt;/h1&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Click for all our Kaggle notebooks categorized by model: &lt;/summary&gt; &#xA; &lt;h3&gt;GRPO Notebooks&lt;/h3&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Model&lt;/th&gt; &#xA;    &lt;th&gt;Type&lt;/th&gt; &#xA;    &lt;th&gt;Kaggle Link&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Phi 4&lt;/td&gt; &#xA;    &lt;td&gt;GRPO&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Phi_4_(14B)-GRPO.ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Llama3.1&lt;/td&gt; &#xA;    &lt;td&gt;GRPO&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Llama3.1_(8B)-GRPO.ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Meta Synthetic Data Llama3.1&lt;/td&gt; &#xA;    &lt;td&gt;GRPO&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Meta-Synthetic-Data-Llama3.1_(8B).ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Gemma3&lt;/td&gt; &#xA;    &lt;td&gt;GRPO&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Gemma3_(1B)-GRPO.ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Meta Synthetic Data Llama3 2&lt;/td&gt; &#xA;    &lt;td&gt;GRPO&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Meta_Synthetic_Data_Llama3_2_(3B).ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Qwen3&lt;/td&gt; &#xA;    &lt;td&gt;GRPO&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Qwen3_(4B)-GRPO.ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Qwen2.5&lt;/td&gt; &#xA;    &lt;td&gt;GRPO&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Qwen2.5_(3B)-GRPO.ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Mistral v0.3&lt;/td&gt; &#xA;    &lt;td&gt;GRPO&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Mistral_v0.3_(7B)-GRPO.ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;h3&gt;Gemma Notebooks&lt;/h3&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Model&lt;/th&gt; &#xA;    &lt;th&gt;Type&lt;/th&gt; &#xA;    &lt;th&gt;Kaggle Link&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Gemma3&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Gemma3_(4B).ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Gemma2&lt;/td&gt; &#xA;    &lt;td&gt;Alpaca&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Gemma2_(9B)-Alpaca.ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Gemma2&lt;/td&gt; &#xA;    &lt;td&gt;Alpaca&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Gemma2_(2B)-Alpaca.ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle CodeGemma&lt;/td&gt; &#xA;    &lt;td&gt;Conversational&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-CodeGemma_(7B)-Conversational.ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;h3&gt;Llama Notebooks&lt;/h3&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Model&lt;/th&gt; &#xA;    &lt;th&gt;Type&lt;/th&gt; &#xA;    &lt;th&gt;Kaggle Link&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Llama3.2&lt;/td&gt; &#xA;    &lt;td&gt;Vision&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Llama3.2_(11B)-Vision.ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Llama3.2&lt;/td&gt; &#xA;    &lt;td&gt;Conversational&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Llama3.2_(1B_and_3B)-Conversational.ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Llama3.2&lt;/td&gt; &#xA;    &lt;td&gt;RAFT&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Llama3.2_(1B)-RAFT.ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Llama3.1&lt;/td&gt; &#xA;    &lt;td&gt;Alpaca&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Llama3.1_(8B)-Alpaca.ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Llama3.1&lt;/td&gt; &#xA;    &lt;td&gt;Inference&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Llama3.1_(8B)-Inference.ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Llama3&lt;/td&gt; &#xA;    &lt;td&gt;Alpaca&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Llama3_(8B)-Alpaca.ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Llama3&lt;/td&gt; &#xA;    &lt;td&gt;Ollama&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Llama3_(8B)-Ollama.ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Llama3&lt;/td&gt; &#xA;    &lt;td&gt;Conversational&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Llama3_(8B)-Conversational.ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Llama3&lt;/td&gt; &#xA;    &lt;td&gt;ORPO&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Llama3_(8B)-ORPO.ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Llasa TTS&lt;/td&gt; &#xA;    &lt;td&gt;TTS&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Llasa_TTS_(3B).ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle TinyLlama&lt;/td&gt; &#xA;    &lt;td&gt;Alpaca&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-TinyLlama_(1.1B)-Alpaca.ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Llasa TTS&lt;/td&gt; &#xA;    &lt;td&gt;TTS&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Llasa_TTS_(1B).ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;h3&gt;Mistral Notebooks&lt;/h3&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Model&lt;/th&gt; &#xA;    &lt;th&gt;Type&lt;/th&gt; &#xA;    &lt;th&gt;Kaggle Link&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Mistral v0.3&lt;/td&gt; &#xA;    &lt;td&gt;Conversational&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Mistral_v0.3_(7B)-Conversational.ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Mistral v0.3&lt;/td&gt; &#xA;    &lt;td&gt;Alpaca&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Mistral_v0.3_(7B)-Alpaca.ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Mistral v0.3&lt;/td&gt; &#xA;    &lt;td&gt;CPT&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Mistral_v0.3_(7B)-CPT.ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Mistral&lt;/td&gt; &#xA;    &lt;td&gt;Text Completion&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Mistral_(7B)-Text_Completion.ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Pixtral&lt;/td&gt; &#xA;    &lt;td&gt;Vision&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Pixtral_(12B)-Vision.ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Mistral Nemo&lt;/td&gt; &#xA;    &lt;td&gt;Alpaca&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Mistral_Nemo_(12B)-Alpaca.ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Zephyr&lt;/td&gt; &#xA;    &lt;td&gt;DPO&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Zephyr_(7B)-DPO.ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Mistral Small&lt;/td&gt; &#xA;    &lt;td&gt;Alpaca&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Mistral_Small_(22B)-Alpaca.ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;h3&gt;Orpheus Notebooks&lt;/h3&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Model&lt;/th&gt; &#xA;    &lt;th&gt;Type&lt;/th&gt; &#xA;    &lt;th&gt;Kaggle Link&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Orpheus&lt;/td&gt; &#xA;    &lt;td&gt;TTS&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Orpheus_(3B)-TTS.ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;h3&gt;Oute Notebooks&lt;/h3&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Model&lt;/th&gt; &#xA;    &lt;th&gt;Type&lt;/th&gt; &#xA;    &lt;th&gt;Kaggle Link&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Oute TTS&lt;/td&gt; &#xA;    &lt;td&gt;TTS&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Oute_TTS_(1B).ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;h3&gt;Phi Notebooks&lt;/h3&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Model&lt;/th&gt; &#xA;    &lt;th&gt;Type&lt;/th&gt; &#xA;    &lt;th&gt;Kaggle Link&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Phi 4&lt;/td&gt; &#xA;    &lt;td&gt;Conversational&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Phi_4-Conversational.ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Phi 3.5 Mini&lt;/td&gt; &#xA;    &lt;td&gt;Conversational&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Phi_3.5_Mini-Conversational.ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Phi 3 Medium&lt;/td&gt; &#xA;    &lt;td&gt;Conversational&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Phi_3_Medium-Conversational.ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;h3&gt;Qwen Notebooks&lt;/h3&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Model&lt;/th&gt; &#xA;    &lt;th&gt;Type&lt;/th&gt; &#xA;    &lt;th&gt;Kaggle Link&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Qwen3&lt;/td&gt; &#xA;    &lt;td&gt;Reasoning Conversational&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Qwen3_(14B)-Reasoning-Conversational.ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Qwen3&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Qwen3_(14B).ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Qwen3&lt;/td&gt; &#xA;    &lt;td&gt;Alpaca&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Qwen3_(14B)-Alpaca.ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Qwen2.5&lt;/td&gt; &#xA;    &lt;td&gt;Alpaca&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Qwen2.5_(7B)-Alpaca.ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Qwen2.5 Coder&lt;/td&gt; &#xA;    &lt;td&gt;Conversational&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Qwen2.5_Coder_(14B)-Conversational.ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Qwen2.5 Coder&lt;/td&gt; &#xA;    &lt;td&gt;Tool Calling&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Qwen2.5_Coder_(1.5B)-Tool_Calling.ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Qwen2.5 VL&lt;/td&gt; &#xA;    &lt;td&gt;Vision&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Qwen2.5_VL_(7B)-Vision.ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Qwen2 VL&lt;/td&gt; &#xA;    &lt;td&gt;Vision&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Qwen2_VL_(7B)-Vision.ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Qwen2&lt;/td&gt; &#xA;    &lt;td&gt;Alpaca&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Qwen2_(7B)-Alpaca.ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;h3&gt;Spark Notebooks&lt;/h3&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Model&lt;/th&gt; &#xA;    &lt;th&gt;Type&lt;/th&gt; &#xA;    &lt;th&gt;Kaggle Link&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Spark TTS&lt;/td&gt; &#xA;    &lt;td&gt;TTS&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Spark_TTS_(0_5B).ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;h3&gt;Whisper Notebooks&lt;/h3&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Model&lt;/th&gt; &#xA;    &lt;th&gt;Type&lt;/th&gt; &#xA;    &lt;th&gt;Kaggle Link&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Whisper&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Whisper.ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;h3&gt;Other notebooks Notebooks&lt;/h3&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Model&lt;/th&gt; &#xA;    &lt;th&gt;Type&lt;/th&gt; &#xA;    &lt;th&gt;Kaggle Link&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle CodeForces cot Finetune for Reasoning on CodeForces&lt;/td&gt; &#xA;    &lt;td&gt;Reasoning&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-CodeForces-cot-Finetune_for_Reasoning_on_CodeForces.ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Unsloth&lt;/td&gt; &#xA;    &lt;td&gt;Studio&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Unsloth_Studio.ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Kaggle Sesame CSM&lt;/td&gt; &#xA;    &lt;td&gt;TTS&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Sesame_CSM_(1B)-TTS.ipynb&amp;amp;accelerator=nvidiaTeslaT4&#34;&gt;Open in Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;!-- End of Notebook Links --&gt; &#xA;&lt;h1&gt;‚ú® Contributing to Notebooks&lt;/h1&gt; &#xA;&lt;p&gt;If you&#39;d like to contribute to our notebooks, here&#39;s a guide to get you started:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Find the Template:&lt;/strong&gt; We&#39;ve provided a template notebook called &lt;code&gt;Template_Notebook.ipynb&lt;/code&gt; in the root directory of this project. This template contains the basic structure and formatting guidelines for all notebooks in this collection.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Create Your Notebook:&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Make a copy of &lt;code&gt;Template_Notebook.ipynb&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Rename the copied file to follow this naming convention: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;strong&gt;LLM Notebooks:&lt;/strong&gt; &lt;code&gt;&amp;lt;Model Name&amp;gt;-&amp;lt;Type&amp;gt;.ipynb&lt;/code&gt; (e.g., &lt;code&gt;Mistral_v0.3_(7B)-Alpaca.ipynb&lt;/code&gt;)&lt;/li&gt; &#xA;     &lt;li&gt;&lt;strong&gt;Vision Notebooks:&lt;/strong&gt; &lt;code&gt;&amp;lt;Model Name&amp;gt;-Vision.ipynb&lt;/code&gt; (e.g., &lt;code&gt;Llava_v1.6_(7B)-Vision.ipynb&lt;/code&gt;)&lt;/li&gt; &#xA;     &lt;li&gt;&lt;strong&gt;Example of &lt;code&gt;&amp;lt;Type&amp;gt;&lt;/code&gt;:&lt;/strong&gt; &lt;code&gt;Alpaca&lt;/code&gt;, &lt;code&gt;Conversational&lt;/code&gt;, &lt;code&gt;CPT&lt;/code&gt;, &lt;code&gt;DPO&lt;/code&gt;, &lt;code&gt;ORPO&lt;/code&gt;, &lt;code&gt;Text_Completion&lt;/code&gt;, &lt;code&gt;CSV&lt;/code&gt;, &lt;code&gt;Inference&lt;/code&gt;, &lt;code&gt;Unsloth_Studio&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;!-- *   Modify the content of your notebook, adding your code, explanations, and any other relevant information. Make sure to follow the structure and guidelines from the template. --&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Place in &lt;code&gt;original_template&lt;/code&gt;:&lt;/strong&gt; Once your notebook is ready, move it to the &lt;code&gt;original_template&lt;/code&gt; directory.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Update Notebooks:&lt;/strong&gt; Run the following command in your terminal: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python update_all_notebooks.py&#xA;&lt;/code&gt;&lt;/pre&gt; This script will automatically: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Copy your notebook from &lt;code&gt;original_template&lt;/code&gt; to the &lt;code&gt;notebooks&lt;/code&gt; directory.&lt;/li&gt; &#xA;   &lt;li&gt;Update the notebook&#39;s internal sections (like Installation, News) to ensure consistency.&lt;/li&gt; &#xA;   &lt;li&gt;Add your notebook to the appropriate list in this &lt;code&gt;README.md&lt;/code&gt; file.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Create a Pull Request:&lt;/strong&gt; After that, just create a pull request (PR) to merge your changes, making it available for everyone! &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;We appreciate your contributions and look forward to reviewing your notebooks!&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
</feed>