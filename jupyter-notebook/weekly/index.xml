<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-09-17T01:52:43Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>emptycrown/llama-hub</title>
    <updated>2023-09-17T01:52:43Z</updated>
    <id>tag:github.com,2023-09-17:/emptycrown/llama-hub</id>
    <link href="https://github.com/emptycrown/llama-hub" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A library of data loaders for LLMs made by the community -- to be used with GPT Index and/or LangChain&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LlamaHub ðŸ¦™&lt;/h1&gt; &#xA;&lt;p&gt;This is a simple library of all the data loaders / readers / tools that have been created by the community. The goal is to make it extremely easy to connect large language models to a large variety of knowledge sources. These are general-purpose utilities that are meant to be used in &lt;a href=&#34;https://github.com/jerryjliu/llama_index&#34;&gt;LlamaIndex&lt;/a&gt; and &lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;LangChain&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Loaders and readers allow you to easily ingest data for search and retrieval by a large language models, while tools allow the models to both read and write to third party data services and sources. Ultimately, this allows you to create your own customized data agent to intelligently work with you and your data to unlock the full capaibility of next level large language models.&lt;/p&gt; &#xA;&lt;p&gt;For a variety of examples on data agents, see the &lt;a href=&#34;https://github.com/emptycrown/llama-hub/tree/main/llama_hub/tools/notebooks&#34;&gt;notebooks directory&lt;/a&gt;. You can find example Jupyter notebooks for creating data agents that can load and parse data from Google Docs, SQL Databases, Notion, Slack and also manage you Google Calendar, Gmail inbox, or read and use OpenAPI specs.&lt;/p&gt; &#xA;&lt;p&gt;For an easier way to browse the integrations available, checkout the website here: &lt;a href=&#34;https://llamahub.ai/&#34;&gt;https://llamahub.ai/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;img width=&#34;1465&#34; alt=&#34;Screenshot 2023-07-17 at 6 12 32 PM&#34; src=&#34;https://github.com/ajhofmann/llama-hub/assets/10040285/5e344de4-4aca-4f6c-9944-46c00baa5eb2&#34;&gt; &#xA;&lt;h2&gt;Usage (Use &lt;code&gt;llama-hub&lt;/code&gt; as PyPI package)&lt;/h2&gt; &#xA;&lt;p&gt;These general-purpose loaders are designed to be used as a way to load data into &lt;a href=&#34;https://github.com/jerryjliu/llama_index&#34;&gt;LlamaIndex&lt;/a&gt; and/or subsequently used in &lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;LangChain&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install llama-hub&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;LlamaIndex&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from llama_index import GPTVectorStoreIndex&#xA;from llama_hub.google_docs.base import GoogleDocsReader&#xA;&#xA;gdoc_ids = [&#39;1wf-y2pd9C878Oh-FmLH7Q_BQkljdm6TQal-c1pUfrec&#39;]&#xA;loader = GoogleDocsReader()&#xA;documents = loader.load_data(document_ids=gdoc_ids)&#xA;index = GPTVectorStoreIndex.from_documents(documents)&#xA;index.query(&#39;Where did the author go to school?&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;LlamaIndex Data Agent&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from llama_index.agent import OpenAIAgent&#xA;import openai&#xA;openai.api_key = &#39;sk-api-key&#39;&#xA;&#xA;from llama_hub.tools.google_calendar.base import GoogleCalendarToolSpec&#xA;tool_spec = GoogleCalendarToolSpec()&#xA;&#xA;agent = OpenAIAgent.from_tools(tool_spec.to_tool_list())&#xA;agent.chat(&#39;what is the first thing on my calendar today&#39;)&#xA;agent.chat(&#34;Please create an event for tomorrow at 4pm to review pull requests&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For a variety of examples on creating and using data agents, see the &lt;a href=&#34;https://github.com/emptycrown/llama-hub/tree/main/llama_hub/tools/notebooks&#34;&gt;notebooks directory&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;LangChain&lt;/h3&gt; &#xA;&lt;p&gt;Note: Make sure you change the description of the &lt;code&gt;Tool&lt;/code&gt; to match your use-case.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from llama_index import GPTVectorStoreIndex&#xA;from llama_hub.google_docs.base import GoogleDocsReader&#xA;from langchain.llms import OpenAI&#xA;from langchain.chains.question_answering import load_qa_chain&#xA;&#xA;# load documents&#xA;gdoc_ids = [&#39;1wf-y2pd9C878Oh-FmLH7Q_BQkljdm6TQal-c1pUfrec&#39;]&#xA;loader = GoogleDocsReader()&#xA;documents = loader.load_data(document_ids=gdoc_ids)&#xA;langchain_documents = [d.to_langchain_format() for d in documents]&#xA;&#xA;# initialize sample QA chain&#xA;llm = OpenAI(temperature=0)&#xA;qa_chain = load_qa_chain(llm)&#xA;question=&#34;&amp;lt;query here&amp;gt;&#34;&#xA;answer = qa_chain.run(input_documents=langchain_documents, question=question)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Loader Usage (Use &lt;code&gt;download_loader&lt;/code&gt; from LlamaIndex)&lt;/h2&gt; &#xA;&lt;p&gt;You can also use the loaders with &lt;code&gt;download_loader&lt;/code&gt; from LlamaIndex in a single line of code.&lt;/p&gt; &#xA;&lt;p&gt;For example, see the code snippets below using the Google Docs Loader.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from llama_index import GPTVectorStoreIndex, download_loader&#xA;&#xA;GoogleDocsReader = download_loader(&#39;GoogleDocsReader&#39;)&#xA;&#xA;gdoc_ids = [&#39;1wf-y2pd9C878Oh-FmLH7Q_BQkljdm6TQal-c1pUfrec&#39;]&#xA;loader = GoogleDocsReader()&#xA;documents = loader.load_data(document_ids=gdoc_ids)&#xA;index = GPTVectorStoreIndex.from_documents(documents)&#xA;index.query(&#39;Where did the author go to school?&#39;)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;How to add a loader or tool&lt;/h2&gt; &#xA;&lt;p&gt;Adding a loader or tool simply requires forking this repo and making a Pull Request. The Llama Hub website will update automatically. However, please keep in mind the following guidelines when making your PR.&lt;/p&gt; &#xA;&lt;h3&gt;Step 0: Setup virtual environment, install Poetry and dependencies&lt;/h3&gt; &#xA;&lt;p&gt;Create a new Python virtual environment. The command below creates an environment in &lt;code&gt;.venv&lt;/code&gt;, and activates it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m venv .venv&#xA;source .venv/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;if you are in windows, use the following to activate your virtual environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;.venv\scripts\activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install poetry:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install poetry&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install the required dependencies (this will also install &lt;code&gt;llama_index&lt;/code&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will create an editable install of &lt;code&gt;llama-hub&lt;/code&gt; in your venv.&lt;/p&gt; &#xA;&lt;h3&gt;Step 1: Create a new directory&lt;/h3&gt; &#xA;&lt;p&gt;For loaders, create a new directory in &lt;code&gt;llama_hub&lt;/code&gt;, and for tools create a directory in &lt;code&gt;llama_hub/tools&lt;/code&gt; It can be nested within another, but name it something unique because the name of the directory will become the identifier for your loader (e.g. &lt;code&gt;google_docs&lt;/code&gt;). Inside your new directory, create a &lt;code&gt;__init__.py&lt;/code&gt; file, which can be empty, a &lt;code&gt;base.py&lt;/code&gt; file which will contain your loader implementation, and, if needed, a &lt;code&gt;requirements.txt&lt;/code&gt; file to list the package dependencies of your loader. Those packages will automatically be installed when your loader is used, so no need to worry about that anymore!&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;d like, you can create the new directory and files by running the following script in the &lt;code&gt;llama_hub&lt;/code&gt; directory. Just remember to put your dependencies into a &lt;code&gt;requirements.txt&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./add_loader.sh [NAME_OF_NEW_DIRECTORY]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step 2: Write your README&lt;/h3&gt; &#xA;&lt;p&gt;Inside your new directory, create a &lt;code&gt;README.md&lt;/code&gt; that mirrors that of the existing ones. It should have a summary of what your loader or tool does, its inputs, and how its used in the context of LlamaIndex and LangChain.&lt;/p&gt; &#xA;&lt;h3&gt;Step 3: Add your loader to the library.json file&lt;/h3&gt; &#xA;&lt;p&gt;Finally, add your loader to the &lt;code&gt;llama_hub/library.json&lt;/code&gt; file (for tools, add them to the &lt;code&gt;llama_hub/tools/library.json&lt;/code&gt;) so that it may be used by others. As is exemplified by the current file, add in the class name of your loader or tool, along with its id, author, etc. This file is referenced by the Llama Hub website and the download function within LlamaIndex.&lt;/p&gt; &#xA;&lt;h3&gt;Step 4: Make a Pull Request!&lt;/h3&gt; &#xA;&lt;p&gt;Create a PR against the main branch. We typically review the PR within a day. To help expedite the process, it may be helpful to provide screenshots (either in the PR or in the README directly) showing your data loader or tool in action!&lt;/p&gt; &#xA;&lt;h2&gt;Running tests&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python3.9 -m venv .venv&#xA;source .venv/bin/activate &#xA;pip3 install -r test_requirements.txt&#xA;&#xA;poetry run pytest tests &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Changelog&lt;/h2&gt; &#xA;&lt;p&gt;If you want to track the latest version updates / see which loaders are added to each release, take a look at our &lt;a href=&#34;https://github.com/emptycrown/llama-hub/raw/main/CHANGELOG.md&#34;&gt;full changelog here&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;h3&gt;How do I test my loader before it&#39;s merged?&lt;/h3&gt; &#xA;&lt;p&gt;There is an argument called &lt;code&gt;loader_hub_url&lt;/code&gt; in &lt;a href=&#34;https://github.com/jerryjliu/llama_index/raw/main/llama_index/readers/download.py&#34;&gt;&lt;code&gt;download_loader&lt;/code&gt;&lt;/a&gt; that defaults to the main branch of this repo. You can set it to your branch or fork to test your new loader.&lt;/p&gt; &#xA;&lt;h3&gt;Should I create a PR against LlamaHub or the LlamaIndex repo directly?&lt;/h3&gt; &#xA;&lt;p&gt;If you have a data loader PR, by default let&#39;s try to create it against LlamaHub! We will make exceptions in certain cases (for instance, if we think the data loader should be core to the LlamaIndex repo).&lt;/p&gt; &#xA;&lt;p&gt;For all other PR&#39;s relevant to LlamaIndex, let&#39;s create it directly against the &lt;a href=&#34;https://github.com/jerryjliu/llama_index&#34;&gt;LlamaIndex repo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Other questions?&lt;/h3&gt; &#xA;&lt;p&gt;Feel free to hop into the &lt;a href=&#34;https://discord.gg/dGcwcsnxhU&#34;&gt;community Discord&lt;/a&gt; or tag the official &lt;a href=&#34;https://twitter.com/llama_index&#34;&gt;Twitter account&lt;/a&gt;!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>amueller/introduction_to_ml_with_python</title>
    <updated>2023-09-17T01:52:43Z</updated>
    <id>tag:github.com,2023-09-17:/amueller/introduction_to_ml_with_python</id>
    <link href="https://github.com/amueller/introduction_to_ml_with_python" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Notebooks and code for the book &#34;Introduction to Machine Learning with Python&#34;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://mybinder.org/v2/gh/amueller/introduction_to_ml_with_python/master&#34;&gt;&lt;img src=&#34;https://mybinder.org/badge.svg?sanitize=true&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Introduction to Machine Learning with Python&lt;/h1&gt; &#xA;&lt;p&gt;This repository holds the code for the forthcoming book &#34;Introduction to Machine Learning with Python&#34; by &lt;a href=&#34;http://amueller.io&#34;&gt;Andreas Mueller&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/sarah_guido&#34;&gt;Sarah Guido&lt;/a&gt;. You can find details about the book on the &lt;a href=&#34;http://shop.oreilly.com/product/0636920030515.do&#34;&gt;O&#39;Reilly website&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The book requires the current stable version of scikit-learn, that is 0.20.0. Most of the book can also be used with previous versions of scikit-learn, though you need to adjust the import for everything from the &lt;code&gt;model_selection&lt;/code&gt; module, mostly &lt;code&gt;cross_val_score&lt;/code&gt;, &lt;code&gt;train_test_split&lt;/code&gt; and &lt;code&gt;GridSearchCV&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This repository provides the notebooks from which the book is created, together with the &lt;code&gt;mglearn&lt;/code&gt; library of helper functions to create figures and datasets.&lt;/p&gt; &#xA;&lt;p&gt;For the curious ones, the cover depicts a &lt;a href=&#34;https://en.wikipedia.org/wiki/Hellbender&#34;&gt;hellbender&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;All datasets are included in the repository, with the exception of the aclImdb dataset, which you can download from the page of &lt;a href=&#34;http://ai.stanford.edu/~amaas/data/sentiment/&#34;&gt;Andrew Maas&lt;/a&gt;. See the book for details.&lt;/p&gt; &#xA;&lt;p&gt;If you get &lt;code&gt;ImportError: No module named mglearn&lt;/code&gt; you can try to install mglearn into your python environment using the command &lt;code&gt;pip install mglearn&lt;/code&gt; in your terminal or &lt;code&gt;!pip install mglearn&lt;/code&gt; in Jupyter Notebook.&lt;/p&gt; &#xA;&lt;h2&gt;Errata&lt;/h2&gt; &#xA;&lt;p&gt;Please note that the first print of the book is missing the following line when listing the assumed imports:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from IPython.display import display&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please add this line if you see an error involving &lt;code&gt;display&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The first print of the book used a function called &lt;code&gt;plot_group_kfold&lt;/code&gt;. This has been renamed to &lt;code&gt;plot_label_kfold&lt;/code&gt; because of a rename in scikit-learn.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;To run the code, you need the packages &lt;code&gt;numpy&lt;/code&gt;, &lt;code&gt;scipy&lt;/code&gt;, &lt;code&gt;scikit-learn&lt;/code&gt;, &lt;code&gt;matplotlib&lt;/code&gt;, &lt;code&gt;pandas&lt;/code&gt; and &lt;code&gt;pillow&lt;/code&gt;. Some of the visualizations of decision trees and neural networks structures also require &lt;code&gt;graphviz&lt;/code&gt;. The chapter on text processing also requires &lt;code&gt;nltk&lt;/code&gt; and &lt;code&gt;spacy&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The easiest way to set up an environment is by installing &lt;a href=&#34;https://www.continuum.io/downloads&#34;&gt;Anaconda&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Installing packages with conda:&lt;/h3&gt; &#xA;&lt;p&gt;If you already have a Python environment set up, and you are using the &lt;code&gt;conda&lt;/code&gt; package manager, you can get all packages by running&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda install numpy scipy scikit-learn matplotlib pandas pillow graphviz python-graphviz&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For the chapter on text processing you also need to install &lt;code&gt;nltk&lt;/code&gt; and &lt;code&gt;spacy&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda install nltk spacy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Installing packages with pip&lt;/h3&gt; &#xA;&lt;p&gt;If you already have a Python environment and are using pip to install packages, you need to run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install numpy scipy scikit-learn matplotlib pandas pillow graphviz&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You also need to install the graphiz C-library, which is easiest using a package manager. If you are using OS X and homebrew, you can &lt;code&gt;brew install graphviz&lt;/code&gt;. If you are on Ubuntu or debian, you can &lt;code&gt;apt-get install graphviz&lt;/code&gt;. Installing graphviz on Windows can be tricky and using conda / anaconda is recommended. For the chapter on text processing you also need to install &lt;code&gt;nltk&lt;/code&gt; and &lt;code&gt;spacy&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install nltk spacy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Downloading English language model&lt;/h3&gt; &#xA;&lt;p&gt;For the text processing chapter, you need to download the English language model for spacy using&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m spacy download en&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Submitting Errata&lt;/h2&gt; &#xA;&lt;p&gt;If you have errata for the (e-)book, please submit them via the &lt;a href=&#34;http://www.oreilly.com/catalog/errata.csp?isbn=0636920030515&#34;&gt;O&#39;Reilly Website&lt;/a&gt;. You can submit fixes to the code as pull-requests here, but I&#39;d appreciate it if you would also submit them there, as this repository doesn&#39;t hold the &#34;master notebooks&#34;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/amueller/introduction_to_ml_with_python/master/cover.jpg&#34; alt=&#34;cover&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>google/prompt-to-prompt</title>
    <updated>2023-09-17T01:52:43Z</updated>
    <id>tag:github.com,2023-09-17:/google/prompt-to-prompt</id>
    <link href="https://github.com/google/prompt-to-prompt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Prompt-to-Prompt&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;em&gt;Latent Diffusion&lt;/em&gt; and &lt;em&gt;Stable Diffusion&lt;/em&gt; Implementation&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;&lt;span&gt;ðŸ¥³&lt;/span&gt; &lt;em&gt;&lt;strong&gt;New:&lt;/strong&gt;&lt;/em&gt; &lt;span&gt;ðŸ¥³&lt;/span&gt; Code for Null-Text Inversion is now provided &lt;a href=&#34;https://raw.githubusercontent.com/google/prompt-to-prompt/main/#null-text-inversion-for-editing-real-images&#34;&gt;here&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/google/prompt-to-prompt/main/docs/teaser.png&#34; alt=&#34;teaser&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://prompt-to-prompt.github.io&#34;&gt;Project Page&lt;/a&gt;â€‚â€‚â€‚&lt;a href=&#34;https://prompt-to-prompt.github.io/ptp_files/Prompt-to-Prompt_preprint.pdf&#34;&gt;Paper&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;This code was tested with Python 3.8, &lt;a href=&#34;https://pytorch.org/&#34;&gt;Pytorch&lt;/a&gt; 1.11 using pre-trained models through &lt;a href=&#34;https://github.com/huggingface/diffusers#readme&#34;&gt;huggingface / diffusers&lt;/a&gt;. Specifically, we implemented our method over &lt;a href=&#34;https://huggingface.co/CompVis/ldm-text2im-large-256&#34;&gt;Latent Diffusion&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/CompVis/stable-diffusion-v1-4&#34;&gt;Stable Diffusion&lt;/a&gt;. Additional required packages are listed in the requirements file. The code was tested on a Tesla V100 16GB but should work on other cards with at least &lt;strong&gt;12GB&lt;/strong&gt; VRAM.&lt;/p&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;In order to get started, we recommend taking a look at our notebooks: &lt;a href=&#34;https://raw.githubusercontent.com/google/prompt-to-prompt/main/prompt-to-prompt_ldm.ipynb&#34;&gt;&lt;strong&gt;prompt-to-prompt_ldm&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/google/prompt-to-prompt/main/prompt-to-prompt_stable.ipynb&#34;&gt;&lt;strong&gt;prompt-to-prompt_stable&lt;/strong&gt;&lt;/a&gt;. The notebooks contain end-to-end examples of usage of prompt-to-prompt on top of &lt;em&gt;Latent Diffusion&lt;/em&gt; and &lt;em&gt;Stable Diffusion&lt;/em&gt; respectively. Take a look at these notebooks to learn how to use the different types of prompt edits and understand the API.&lt;/p&gt; &#xA;&lt;h2&gt;Prompt Edits&lt;/h2&gt; &#xA;&lt;p&gt;In our notebooks, we perform our main logic by implementing the abstract class &lt;code&gt;AttentionControl&lt;/code&gt; object, of the following form:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class AttentionControl(abc.ABC):&#xA;    @abc.abstractmethod&#xA;    def forward (self, attn, is_cross: bool, place_in_unet: str):&#xA;        raise NotImplementedError&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;forward&lt;/code&gt; method is called in each attention layer of the diffusion model during the image generation, and we use it to modify the weights of the attention. Our method (See Section 3 of our &lt;a href=&#34;https://arxiv.org/abs/2208.01626&#34;&gt;paper&lt;/a&gt;) edits images with the procedure above, and each different prompt edit type modifies the weights of the attention in a different manner.&lt;/p&gt; &#xA;&lt;p&gt;The general flow of our code is as follows, with variations based on the attention control type:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;prompts = [&#34;A painting of a squirrel eating a burger&#34;, ...]&#xA;controller = AttentionControl(prompts, ...)&#xA;run_and_display(prompts, controller, ...)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Replacement&lt;/h3&gt; &#xA;&lt;p&gt;In this case, the user swaps tokens of the original prompt with others, e.g., the editing the prompt &lt;code&gt;&#34;A painting of a squirrel eating a burger&#34;&lt;/code&gt; to &lt;code&gt;&#34;A painting of a squirrel eating a lasagna&#34;&lt;/code&gt; or &lt;code&gt;&#34;A painting of a lion eating a burger&#34;&lt;/code&gt;. For this we define the class &lt;code&gt;AttentionReplace&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Refinement&lt;/h3&gt; &#xA;&lt;p&gt;In this case, the user adds new tokens to the prompt, e.g., editing the prompt &lt;code&gt;&#34;A painting of a squirrel eating a burger&#34;&lt;/code&gt; to &lt;code&gt;&#34;A watercolor painting of a squirrel eating a burger&#34;&lt;/code&gt;. For this we define the class &lt;code&gt;AttentionEditRefine&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Re-weight&lt;/h3&gt; &#xA;&lt;p&gt;In this case, the user changes the weight of certain tokens in the prompt, e.g., for the prompt &lt;code&gt;&#34;A photo of a poppy field at night&#34;&lt;/code&gt;, strengthen or weaken the extent to which the word &lt;code&gt;night&lt;/code&gt; affects the resulting image. For this we define the class &lt;code&gt;AttentionReweight&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Attention Control Options&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;cross_replace_steps&lt;/code&gt;: specifies the fraction of steps to edit the cross attention maps. Can also be set to a dictionary &lt;code&gt;[str:float]&lt;/code&gt; which specifies fractions for different words in the prompt.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;self_replace_steps&lt;/code&gt;: specifies the fraction of steps to replace the self attention maps.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;local_blend&lt;/code&gt; (optional): &lt;code&gt;LocalBlend&lt;/code&gt; object which is used to make local edits. &lt;code&gt;LocalBlend&lt;/code&gt; is initialized with the words from each prompt that correspond with the region in the image we want to edit.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;equalizer&lt;/code&gt;: used for attention Re-weighting only. A vector of coefficients to multiply each cross-attention weight&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{hertz2022prompt,&#xA;  title = {Prompt-to-Prompt Image Editing with Cross Attention Control},&#xA;  author = {Hertz, Amir and Mokady, Ron and Tenenbaum, Jay and Aberman, Kfir and Pritch, Yael and Cohen-Or, Daniel},&#xA;  journal = {arXiv preprint arXiv:2208.01626},&#xA;  year = {2022},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Null-Text Inversion for Editing Real Images&lt;/h1&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://null-text-inversion.github.io/&#34;&gt;Project Page&lt;/a&gt;â€‚â€‚â€‚&lt;a href=&#34;https://arxiv.org/abs/2211.09794&#34;&gt;Paper&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Null-text inversion enables intuitive text-based editing of &lt;strong&gt;real images&lt;/strong&gt; with the Stable Diffusion model. We use an initial DDIM inversion as an anchor for our optimization which only tunes the null-text embedding used in classifier-free guidance.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/google/prompt-to-prompt/main/docs/null_text_teaser.png&#34; alt=&#34;teaser&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Editing Real Images&lt;/h2&gt; &#xA;&lt;p&gt;Prompt-to-Prompt editing of real images by first using Null-text inversion is provided in this &lt;a href=&#34;https://raw.githubusercontent.com/google/prompt-to-prompt/main/null_text_w_ptp.ipynb&#34;&gt;&lt;strong&gt;Notebooke&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{mokady2022null,&#xA;  title={Null-text Inversion for Editing Real Images using Guided Diffusion Models},&#xA;  author={Mokady, Ron and Hertz, Amir and Aberman, Kfir and Pritch, Yael and Cohen-Or, Daniel},&#xA;  journal={arXiv preprint arXiv:2211.09794},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This is not an officially supported Google product.&lt;/p&gt;</summary>
  </entry>
</feed>