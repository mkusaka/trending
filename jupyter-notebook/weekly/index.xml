<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-07-14T01:38:46Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>NVIDIA/GenerativeAIExamples</title>
    <updated>2024-07-14T01:38:46Z</updated>
    <id>tag:github.com,2024-07-14:/NVIDIA/GenerativeAIExamples</id>
    <link href="https://github.com/NVIDIA/GenerativeAIExamples" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Generative AI reference workflows optimized for accelerated infrastructure and microservice architecture.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;NVIDIA Generative AI Examples&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://nvidia.github.io/GenerativeAIExamples/latest&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/documentation-blue.svg?sanitize=true&#34; alt=&#34;documentation&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;State-of-the-art Generative AI examples that are easy to deploy, test, and extend. All examples run on the high performance NVIDIA CUDA-X software stack and NVIDIA GPUs.&lt;/p&gt; &#xA;&lt;h2&gt;What&#39;s new?&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Tips for Building a RAG Pipeline with NVIDIA AI LangChain AI Endpoints by Amit Bleiweiss. [&lt;a href=&#34;https://developer.nvidia.com/blog/tips-for-building-a-rag-pipeline-with-nvidia-ai-langchain-ai-endpoints/&#34;&gt;Blog&lt;/a&gt;, &lt;a href=&#34;https://github.com/NVIDIA/GenerativeAIExamples/raw/v0.7.0/notebooks/08_RAG_Langchain_with_Local_NIM.ipynb&#34;&gt;notebook&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Experimental examples: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/NVIDIA/GenerativeAIExamples/raw/v0.7.0/experimental/rag-developer-chatbot&#34;&gt;How to create a developer-focused RAG chatbot using RAPIDS cuDF&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/NVIDIA/GenerativeAIExamples/raw/v0.7.0/experimental/event-driven-rag-cve-analysis&#34;&gt;NVIDIA Morpheus, NIMs, and RAG pipelines integrated to create LLM-based agent pipelines&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;NVIDIA NGC&lt;/h2&gt; &#xA;&lt;p&gt;Generative AI Examples can use models and GPUs from the &lt;a href=&#34;https://catalog.ngc.nvidia.com&#34;&gt;NVIDIA API Catalog&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Sign up for a &lt;a href=&#34;https://ngc.nvidia.com/signin&#34;&gt;free NGC developer account&lt;/a&gt; to access:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GPU-optimized containers used in these examples&lt;/li&gt; &#xA; &lt;li&gt;Release notes and developer documentation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Retrieval Augmented Generation (RAG)&lt;/h2&gt; &#xA;&lt;p&gt;A RAG pipeline embeds multimodal data -- such as documents, images, and video -- into a database connected to a LLM. RAG lets users chat with their data!&lt;/p&gt; &#xA;&lt;h3&gt;Developer RAG Examples&lt;/h3&gt; &#xA;&lt;p&gt;The developer RAG examples run on a single VM. The examples demonstrate how to combine NVIDIA GPU acceleration with popular LLM programming frameworks using NVIDIA&#39;s &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/#open-source-integrations&#34;&gt;open source connectors&lt;/a&gt;. The examples are easy to deploy with &lt;a href=&#34;https://docs.docker.com/compose/&#34;&gt;Docker Compose&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Examples support local and remote inference endpoints. If you have a GPU, you can inference locally with an &lt;a href=&#34;https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nim/containers/nim_llm&#34;&gt;NVIDIA NIM for LLMs&lt;/a&gt;. If you don&#39;t have a GPU, you can inference and embed remotely with &lt;a href=&#34;https://build.nvidia.com/explore/discover&#34;&gt;NVIDIA API Catalog endpoints&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Embedding&lt;/th&gt; &#xA;   &lt;th&gt;Framework&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Multi-GPU&lt;/th&gt; &#xA;   &lt;th&gt;TRT-LLM&lt;/th&gt; &#xA;   &lt;th&gt;NVIDIA Endpoints&lt;/th&gt; &#xA;   &lt;th&gt;Triton&lt;/th&gt; &#xA;   &lt;th&gt;Vector Database&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;llama3-70b&lt;/td&gt; &#xA;   &lt;td&gt;snowflake-arctic-embed-l&lt;/td&gt; &#xA;   &lt;td&gt;LangChain&lt;/td&gt; &#xA;   &lt;td&gt;NVIDIA API Catalog endpoints chat bot [&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/RetrievalAugmentedGeneration/examples/nvidia_api_catalog/&#34;&gt;code&lt;/a&gt;, &lt;a href=&#34;https://nvidia.github.io/GenerativeAIExamples/latest/api-catalog.html&#34;&gt;docs&lt;/a&gt;]&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Milvus or pgvector&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;llama3-8b&lt;/td&gt; &#xA;   &lt;td&gt;snowflake-arctic-embed-l&lt;/td&gt; &#xA;   &lt;td&gt;LlamaIndex&lt;/td&gt; &#xA;   &lt;td&gt;Canonical QA Chatbot [&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/RetrievalAugmentedGeneration/examples/developer_rag/&#34;&gt;code&lt;/a&gt;, &lt;a href=&#34;https://nvidia.github.io/GenerativeAIExamples/latest/api-catalog.html#using-the-llamaindex-data-framework&#34;&gt;docs&lt;/a&gt;]&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://nvidia.github.io/GenerativeAIExamples/latest/multi-gpu.html&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Milvus or pgvector&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;llama3-70b&lt;/td&gt; &#xA;   &lt;td&gt;snowflake-arctic-embed-l&lt;/td&gt; &#xA;   &lt;td&gt;LangChain&lt;/td&gt; &#xA;   &lt;td&gt;Chat bot with query decomposition agent [&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/RetrievalAugmentedGeneration/examples/query_decomposition_rag/&#34;&gt;code&lt;/a&gt;, &lt;a href=&#34;https://nvidia.github.io/GenerativeAIExamples/latest/query-decomposition.html&#34;&gt;docs&lt;/a&gt;]&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Milvus or pgvector&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;llama3-70b&lt;/td&gt; &#xA;   &lt;td&gt;ai-embed-qa-4&lt;/td&gt; &#xA;   &lt;td&gt;LangChain&lt;/td&gt; &#xA;   &lt;td&gt;Minimilastic example: RAG with NVIDIA AI Foundation Models [&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/examples/5_mins_rag_no_gpu/&#34;&gt;code&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/examples/README.md#rag-in-5-minutes-example&#34;&gt;README&lt;/a&gt;]&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;FAISS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;llama3-8b&lt;br&gt;Deplot&lt;br&gt;Neva-22b&lt;/td&gt; &#xA;   &lt;td&gt;snowflake-arctic-embed-l&lt;/td&gt; &#xA;   &lt;td&gt;Custom&lt;/td&gt; &#xA;   &lt;td&gt;Chat bot with multimodal data [&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/RetrievalAugmentedGeneration/examples/multimodal_rag/&#34;&gt;code&lt;/a&gt;, &lt;a href=&#34;https://nvidia.github.io/GenerativeAIExamples/latest/multimodal-data.html&#34;&gt;docs&lt;/a&gt;]&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Milvus or pvgector&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;llama3-70b&lt;/td&gt; &#xA;   &lt;td&gt;none&lt;/td&gt; &#xA;   &lt;td&gt;PandasAI&lt;/td&gt; &#xA;   &lt;td&gt;Chat bot with structured data [&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/RetrievalAugmentedGeneration/examples/structured_data_rag/&#34;&gt;code&lt;/a&gt;, &lt;a href=&#34;https://nvidia.github.io/GenerativeAIExamples/latest/structured-data.html&#34;&gt;docs&lt;/a&gt;]&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;none&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;llama3-8b&lt;/td&gt; &#xA;   &lt;td&gt;snowflake-arctic-embed-l&lt;/td&gt; &#xA;   &lt;td&gt;LangChain&lt;/td&gt; &#xA;   &lt;td&gt;Chat bot with multi-turn conversation [&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/RetrievalAugmentedGeneration/examples/multi_turn_rag/&#34;&gt;code&lt;/a&gt;, &lt;a href=&#34;https://nvidia.github.io/GenerativeAIExamples/latest/multi-turn.html&#34;&gt;docs&lt;/a&gt;]&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Milvus or pgvector&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Enterprise RAG Examples&lt;/h3&gt; &#xA;&lt;p&gt;The enterprise RAG examples run as microservices distributed across multiple VMs and GPUs. These examples show how to orchestrate RAG pipelines with &lt;a href=&#34;https://kubernetes.io/&#34;&gt;Kubernetes&lt;/a&gt; and deployed with &lt;a href=&#34;https://helm.sh/&#34;&gt;Helm&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Enterprise RAG examples include a &lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/operator/&#34;&gt;Kubernetes operator&lt;/a&gt; for LLM lifecycle management. It is compatible with the &lt;a href=&#34;https://catalog.ngc.nvidia.com/orgs/nvidia/containers/gpu-operator&#34;&gt;NVIDIA GPU Operator&lt;/a&gt; that automates GPU discovery and lifecycle management in a Kubernetes cluster.&lt;/p&gt; &#xA;&lt;p&gt;Enterprise RAG examples also support local and remote inference with &lt;a href=&#34;https://github.com/NVIDIA/TensorRT-LLM&#34;&gt;TensorRT-LLM&lt;/a&gt; and &lt;a href=&#34;https://build.nvidia.com/explore/discover&#34;&gt;NVIDIA API Catalog endpoints&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Embedding&lt;/th&gt; &#xA;   &lt;th&gt;Framework&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Multi-GPU&lt;/th&gt; &#xA;   &lt;th&gt;Multi-node&lt;/th&gt; &#xA;   &lt;th&gt;TRT-LLM&lt;/th&gt; &#xA;   &lt;th&gt;NVIDIA Endpoints&lt;/th&gt; &#xA;   &lt;th&gt;Triton&lt;/th&gt; &#xA;   &lt;th&gt;Vector Database&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;llama-3&lt;/td&gt; &#xA;   &lt;td&gt;nv-embed-qa-4&lt;/td&gt; &#xA;   &lt;td&gt;LlamaIndex&lt;/td&gt; &#xA;   &lt;td&gt;Chat bot, Kubernetes deployment [&lt;a href=&#34;https://registry.ngc.nvidia.com/orgs/ohlfw0olaadg/teams/ea-participants/helm-charts/rag-app-text-chatbot&#34;&gt;chart&lt;/a&gt;]&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Milvus&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Generative AI Model Examples&lt;/h3&gt; &#xA;&lt;p&gt;The generative AI model examples include end-to-end steps for pre-training, customizing, aligning and running inference on state-of-the-art generative AI models leveraging the &lt;a href=&#34;https://github.com/NVIDIA/NeMo&#34;&gt;NVIDIA NeMo Framework&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Resources(s)&lt;/th&gt; &#xA;   &lt;th&gt;Framework&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gemma&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/models/Gemma/&#34;&gt;Docs&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/models/Gemma/lora.ipynb&#34;&gt;LoRA&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/models/Gemma/sft.ipynb&#34;&gt;SFT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;NeMo&lt;/td&gt; &#xA;   &lt;td&gt;Aligning and customizing Gemma, and exporting to TensorRT-LLM format for inference&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;codegemma&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/models/Codegemma/&#34;&gt;Docs&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/models/Codegemma/lora.ipynb&#34;&gt;LoRA&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;NeMo&lt;/td&gt; &#xA;   &lt;td&gt;Customizing Codegemma, and exporting to TensorRT-LLM format for inference&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;starcoder-2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/models/StarCoder2/lora.ipynb&#34;&gt;LoRA&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/models/StarCoder2/inference.ipynb&#34;&gt;Inference&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;NeMo&lt;/td&gt; &#xA;   &lt;td&gt;Customizing Starcoder-2 with NeMo Framework, optimizing with NVIDIA TensorRT-LLM, and deploying with NVIDIA Triton Inference Server&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;small language models (SLMs)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/models/NeMo/slm/&#34;&gt;Docs&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/models/NeMo/slm/slm_pretraining_sft.ipynb&#34;&gt;Pre-training and SFT&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/models/NeMo/slm/megatron_gpt_eval_server.ipynb&#34;&gt;Eval&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;NeMo&lt;/td&gt; &#xA;   &lt;td&gt;Training, alignment, and running evaluation on SLMs using various techniques&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Tools&lt;/h2&gt; &#xA;&lt;p&gt;Example tools and tutorials to enhance LLM development and productivity when using NVIDIA RAG pipelines.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;NVIDIA Endpoints&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Evaluation&lt;/td&gt; &#xA;   &lt;td&gt;RAG evaluation using synthetic data generation and LLM-as-a-judge [&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/tools/evaluation/&#34;&gt;code&lt;/a&gt;, &lt;a href=&#34;https://nvidia.github.io/GenerativeAIExamples/latest/evaluation.html&#34;&gt;docs&lt;/a&gt;]&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Observability&lt;/td&gt; &#xA;   &lt;td&gt;Monitoring and debugging RAG pipelines [&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/tools/observability/&#34;&gt;code&lt;/a&gt;, &lt;a href=&#34;https://nvidia.github.io/GenerativeAIExamples/latest/observability.html&#34;&gt;docs&lt;/a&gt;]&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Open Source Integrations&lt;/h2&gt; &#xA;&lt;p&gt;These are open source connectors for NVIDIA-hosted and self-hosted API endpoints. These open source connectors are maintained and tested by NVIDIA engineers.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Framework&lt;/th&gt; &#xA;   &lt;th&gt;Chat&lt;/th&gt; &#xA;   &lt;th&gt;Text Embedding&lt;/th&gt; &#xA;   &lt;th&gt;Python&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://python.langchain.com/docs/integrations/providers/nvidia&#34;&gt;NVIDIA AI Foundation Endpoints&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.langchain.com/&#34;&gt;Langchain&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://python.langchain.com/docs/integrations/chat/nvidia_ai_endpoints&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://python.langchain.com/docs/integrations/text_embedding/nvidia_ai_endpoints&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pypi.org/project/langchain-nvidia-ai-endpoints/&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Easy access to NVIDIA hosted models. Supports chat, embedding, code generation, steerLM, multimodal, and RAG.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/langchain-ai/langchain/tree/master/libs/partners/nvidia-trt&#34;&gt;NVIDIA Triton + TensorRT-LLM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.langchain.com/&#34;&gt;Langchain&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/langchain-ai/langchain-nvidia/raw/main/libs/trt/docs/llms.ipynb&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/langchain-ai/langchain-nvidia/raw/main/libs/trt/docs/llms.ipynb&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pypi.org/project/langchain-nvidia-trt/&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;This connector allows Langchain to remotely interact with a Triton inference server over GRPC or HTTP for optimized LLM inference.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.llamaindex.ai/en/stable/examples/llm/nvidia_triton.html&#34;&gt;NVIDIA Triton Inference Server&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.llamaindex.ai/&#34;&gt;LlamaIndex&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Triton inference server provides API access to hosted LLM models over gRPC.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.llamaindex.ai/en/stable/examples/llm/nvidia_tensorrt.html&#34;&gt;NVIDIA TensorRT-LLM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.llamaindex.ai/&#34;&gt;LlamaIndex&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;TensorRT-LLM provides a Python API to build TensorRT engines with state-of-the-art optimizations for LLM inference on NVIDIA GPUs.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Related NVIDIA RAG Projects&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://docs.nvidia.com/ace/latest/workflows/tokkio/text/Tokkio_LLM_RAG_Bot.html&#34;&gt;NVIDIA Tokkio LLM-RAG&lt;/a&gt;: Use Tokkio to add avatar animation for RAG responses.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/NVIDIA/ChatRTX&#34;&gt;RAG on Windows using TensorRT-LLM and LlamaIndex&lt;/a&gt;: Create RAG chatbots on Windows using TensorRT-LLM.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/NVIDIA/workbench-example-hybrid-rag&#34;&gt;Hybrid RAG Project on AI Workbench&lt;/a&gt;: Run an NVIDIA AI Workbench example project for RAG.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Refer to the &lt;a href=&#34;https://github.com/NVIDIA/GenerativeAIExamples/releases&#34;&gt;releases page&lt;/a&gt; for information about previous releases.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Support, Feedback, and Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We&#39;re posting these examples on GitHub to support the NVIDIA LLM community and facilitate feedback. We invite contributions via GitHub Issues or pull requests!&lt;/p&gt; &#xA;&lt;h2&gt;Known Issues&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Some known issues are identified as TODOs in the Python code.&lt;/li&gt; &#xA; &lt;li&gt;The datasets provided as part of this project are under a different license for research and evaluation purposes.&lt;/li&gt; &#xA; &lt;li&gt;This project downloads and installs third-party open source software projects. Review the license terms of these open source projects before use.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>datawhalechina/llm-cookbook</title>
    <updated>2024-07-14T01:38:46Z</updated>
    <id>tag:github.com,2024-07-14:/datawhalechina/llm-cookbook</id>
    <link href="https://github.com/datawhalechina/llm-cookbook" rel="alternate"></link>
    <summary type="html">&lt;p&gt;面向开发者的 LLM 入门教程，吴恩达大模型系列课程中文版&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/datawhalechina/llm-cookbook/main/figures/readme.jpg&#34; alt=&#34;figures/readme.jpg&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;面向开发者的大模型手册 - LLM Cookbook&lt;/h1&gt; &#xA;&lt;h2&gt;项目简介&lt;/h2&gt; &#xA;&lt;p&gt;本项目是一个面向开发者的大模型手册，针对国内开发者的实际需求，主打 LLM 全方位入门实践。本项目基于吴恩达老师大模型系列课程内容，对原课程内容进行筛选、翻译、复现和调优，覆盖从 Prompt Engineering 到 RAG 开发、模型微调的全部流程，用最适合国内学习者的方式，指导国内开发者如何学习、入门 LLM 相关项目。&lt;/p&gt; &#xA;&lt;p&gt;针对不同内容的特点，我们对共计 11 门吴恩达老师的大模型课程进行了翻译复现，并结合国内学习者的实际情况，对不同课程进行了分级和排序，初学者可以先系统学习我们的必修类课程，掌握入门 LLM 所有方向都需要掌握的基础技能和概念，再选择性地学习我们的选修类课程，在自己感兴趣的方向上不断探索和学习。&lt;/p&gt; &#xA;&lt;p&gt;如果有你非常喜欢但我们还没有进行复现的吴恩达老师大模型课程，我们欢迎每一位开发者参考我们已有课程的格式和写法来对课程进行复现并提交 PR，在 PR 审核通过后，我们会根据课程内容将课程进行分级合并。欢迎每一位开发者的贡献！&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;在线阅读地址：&lt;a href=&#34;https://datawhalechina.github.io/llm-cookbook/&#34;&gt;面向开发者的 LLM 入门课程-在线阅读&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PDF下载地址：&lt;a href=&#34;https://github.com/datawhalechina/llm-cookbook/releases/tag/v1%2C0%2C0&#34;&gt;面向开发者的 LLM 入门教程-PDF&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;英文原版地址：&lt;a href=&#34;https://learn.deeplearning.ai&#34;&gt;吴恩达关于大模型的系列课程&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;项目意义&lt;/h2&gt; &#xA;&lt;p&gt;LLM 正在逐步改变人们的生活，而对于开发者，如何基于 LLM 提供的 API 快速、便捷地开发一些具备更强能力、集成LLM 的应用，来便捷地实现一些更新颖、更实用的能力，是一个急需学习的重要能力。&lt;/p&gt; &#xA;&lt;p&gt;由吴恩达老师与 OpenAI 合作推出的大模型系列教程，从大模型时代开发者的基础技能出发，深入浅出地介绍了如何基于大模型 API、LangChain 架构快速开发结合大模型强大能力的应用。其中，《Prompt Engineering for Developers》教程面向入门 LLM 的开发者，深入浅出地介绍了对于开发者，如何构造 Prompt 并基于 OpenAI 提供的 API 实现包括总结、推断、转换等多种常用功能，是入门 LLM 开发的经典教程；《Building Systems with the ChatGPT API》教程面向想要基于 LLM 开发应用程序的开发者，简洁有效而又系统全面地介绍了如何基于 ChatGPT API 打造完整的对话系统；《LangChain for LLM Application Development》教程结合经典大模型开源框架 LangChain，介绍了如何基于 LangChain 框架开发具备实用功能、能力全面的应用程序，《LangChain Chat With Your Data》教程则在此基础上进一步介绍了如何使用 LangChain 架构结合个人私有数据开发个性化大模型应用；《Building Generative AI Applications with Gradio》、《Evaluating and Debugging Generative AI》教程分别介绍了两个实用工具 Gradio 与 W&amp;amp;B，指导开发者如何结合这两个工具来打造、评估生成式 AI 应用。&lt;/p&gt; &#xA;&lt;p&gt;上述教程非常适用于开发者学习以开启基于 LLM 实际搭建应用程序之路。因此，我们将该系列课程翻译为中文，并复现其范例代码，也为其中一个视频增加了中文字幕，支持国内中文学习者直接使用，以帮助中文学习者更好地学习 LLM 开发；我们也同时实现了效果大致相当的中文 Prompt，支持学习者感受中文语境下 LLM 的学习使用，对比掌握多语言语境下的 Prompt 设计与 LLM 开发。未来，我们也将加入更多 Prompt 高级技巧，以丰富本课程内容，帮助开发者掌握更多、更巧妙的 Prompt 技能。&lt;/p&gt; &#xA;&lt;h2&gt;项目受众&lt;/h2&gt; &#xA;&lt;p&gt;所有具备基础 Python 能力，想要入门 LLM 的开发者。&lt;/p&gt; &#xA;&lt;h2&gt;项目亮点&lt;/h2&gt; &#xA;&lt;p&gt;《ChatGPT Prompt Engineering for Developers》、《Building Systems with the ChatGPT API》等教程作为由吴恩达老师与 OpenAI 联合推出的官方教程，在可预见的未来会成为 LLM 的重要入门教程，但是目前还只支持英文版且国内访问受限，打造中文版且国内流畅访问的教程具有重要意义；同时，GPT 对中文、英文具有不同的理解能力，本教程在多次对比、实验之后确定了效果大致相当的中文 Prompt，支持学习者研究如何提升 ChatGPT 在中文语境下的理解与生成能力。&lt;/p&gt; &#xA;&lt;h2&gt;学习指南&lt;/h2&gt; &#xA;&lt;p&gt;本教程适用于所有具备基础 Python 能力，想要入门 LLM 的开发者。&lt;/p&gt; &#xA;&lt;p&gt;如果你想要开始学习本教程，你需要提前具备：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;至少一个 LLM API（最好是 OpenAI，如果是其他 API，你可能需要参考&lt;a href=&#34;https://github.com/datawhalechina/llm-universe&#34;&gt;其他教程&lt;/a&gt;对 API 调用代码进行修改）&lt;/li&gt; &#xA; &lt;li&gt;能够使用 Python Jupyter Notebook&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;本教程共包括 11 门课程，分为必修类、选修类两个类别。必修类课程是我们认为最适合初学者学习以入门 LLM 的课程，包括了入门 LLM 所有方向都需要掌握的基础技能和概念，我们也针对必修类课程制作了适合阅读的在线阅读和 PDF 版本，在学习必修类课程时，我们建议学习者按照我们列出的顺序进行学习；选修类课程是在必修类课程上的拓展延伸，包括了 RAG 开发、模型微调、模型评估等多个方面，适合学习者在掌握了必修类课程之后选择自己感兴趣的方向和课程进行学习。&lt;/p&gt; &#xA;&lt;p&gt;必修类课程包括：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;面向开发者的 Prompt Engineering。基于吴恩达老师《ChatGPT Prompt Engineering for Developers》课程打造，面向入门 LLM 的开发者，深入浅出地介绍了对于开发者，如何构造 Prompt 并基于 OpenAI 提供的 API 实现包括总结、推断、转换等多种常用功能，是入门 LLM 开发的第一步。&lt;/li&gt; &#xA; &lt;li&gt;搭建基于 ChatGPT 的问答系统。基于吴恩达老师《Building Systems with the ChatGPT API》课程打造，指导开发者如何基于 ChatGPT 提供的 API 开发一个完整的、全面的智能问答系统。通过代码实践，实现了基于 ChatGPT 开发问答系统的全流程，介绍了基于大模型开发的新范式，是大模型开发的实践基础。&lt;/li&gt; &#xA; &lt;li&gt;使用 LangChain 开发应用程序。基于吴恩达老师《LangChain for LLM Application Development》课程打造，对 LangChain 展开深入介绍，帮助学习者了解如何使用 LangChain，并基于 LangChain 开发完整的、具备强大能力的应用程序。&lt;/li&gt; &#xA; &lt;li&gt;使用 LangChain 访问个人数据。基于吴恩达老师《LangChain Chat with Your Data》课程打造，深入拓展 LangChain 提供的个人数据访问能力，指导开发者如何使用 LangChain 开发能够访问用户个人数据、提供个性化服务的大模型应用。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;选修类课程包括：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;使用 Gradio 搭建生成式 AI 应用。基于吴恩达老师《Building Generative AI Applications with Gradio》课程打造，指导开发者如何使用 Gradio 通过 Python 接口程序快速、高效地为生成式 AI 构建用户界面。&lt;/li&gt; &#xA; &lt;li&gt;评估改进生成式 AI。基于吴恩达老师《Evaluating and Debugging Generative AI》课程打造，结合 wandb，提供一套系统化的方法和工具，帮助开发者有效地跟踪和调试生成式 AI 模型。&lt;/li&gt; &#xA; &lt;li&gt;微调大语言模型。基于吴恩达老师《Finetuning Large Language Model》课程打造，结合 lamini 框架，讲述如何便捷高效地在本地基于个人数据微调开源大语言模型。&lt;/li&gt; &#xA; &lt;li&gt;大模型与语义检索。基于吴恩达老师《Large Language Models with Semantic Search》课程打造，针对检索增强生成，讲述了多种高级检索技巧以实现更准确、高效的检索增强 LLM 生成效果。&lt;/li&gt; &#xA; &lt;li&gt;基于 Chroma 的高级检索。基于吴恩达老师《Advanced Retrieval for AI with Chroma》课程打造，旨在介绍基于 Chroma 的高级检索技术，提升检索结果的准确性。&lt;/li&gt; &#xA; &lt;li&gt;搭建和评估高级 RAG 应用。基于吴恩达老师《Building and Evaluating Advanced RAG Applications》课程打造，介绍构建和实现高质量RAG系统所需的关键技术和评估框架。&lt;/li&gt; &#xA; &lt;li&gt;LangChain 的 Functions、Tools 和 Agents。基于吴恩达老师《Functions, Tools and Agents with LangChain》课程打造，介绍如何基于 LangChain 的新语法构建 Agent。&lt;/li&gt; &#xA; &lt;li&gt;Prompt 高级技巧。包括 CoT、自我一致性等多种 Prompt 高级技巧的基础理论与代码实现。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;其他资料包括：&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;双语字幕视频地址：&lt;a href=&#34;https://www.bilibili.com/video/BV1Bo4y1A7FU/?share_source=copy_web&#34;&gt;吴恩达 x OpenAI的Prompt Engineering课程专业翻译版&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;中英双语字幕下载：&lt;a href=&#34;https://github.com/GitHubDaily/ChatGPT-Prompt-Engineering-for-Developers-in-Chinese&#34;&gt;《ChatGPT提示工程》非官方版中英双语字幕&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;视频讲解：&lt;a href=&#34;https://www.bilibili.com/video/BV1PN4y1k7y2/?spm_id_from=333.999.0.0&#34;&gt;面向开发者的 Prompt Engineering 讲解（数字游民大会）&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;目录结构说明：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;content：基于原课程复现的双语版代码，可运行的 Notebook，更新频率最高，更新速度最快。&#xA;&#xA;docs：必修类课程文字教程版在线阅读源码，适合阅读的 md。&#xA;&#xA;figures：图片文件。&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;致谢&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;核心贡献者&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/logan-zou&#34;&gt;邹雨衡-项目负责人&lt;/a&gt;（Datawhale成员-对外经济贸易大学研究生）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://yam.gift/&#34;&gt;长琴-项目发起人&lt;/a&gt;（内容创作者-Datawhale成员-AI算法工程师）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Sophia-Huang&#34;&gt;玉琳-项目发起人&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/xuhu0115&#34;&gt;徐虎-教程编撰者&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Weihong-Liu&#34;&gt;刘伟鸿-教程编撰者&lt;/a&gt;（内容创作者-江南大学非全研究生）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://Joyenjoye.com&#34;&gt;Joye-教程编撰者&lt;/a&gt;（内容创作者-数据科学家）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/0-yy-0&#34;&gt;高立业&lt;/a&gt;（内容创作者-DataWhale成员-算法工程师）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/GKDGKD&#34;&gt;邓宇文&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/wisdom-pan&#34;&gt;魂兮&lt;/a&gt;（内容创作者-前端工程师）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KMnO4-zx&#34;&gt;宋志学&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/YikunHan42&#34;&gt;韩颐堃&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/6forwater29&#34;&gt;陈逸涵&lt;/a&gt; (内容创作者-Datawhale意向成员-AI爱好者)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ztgg0228&#34;&gt;仲泰&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/leason-wan&#34;&gt;万礼行&lt;/a&gt;（内容创作者-视频翻译者）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Bald0Wang&#34;&gt;王熠明&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://yetingyun.blog.csdn.net&#34;&gt;曾浩龙&lt;/a&gt;（内容创作者-Datawhale 意向成员-JLU AI 研究生）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/xinqi-fan&#34;&gt;小饭同学&lt;/a&gt;（内容创作者）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sunhanyu714%5D&#34;&gt;孙韩玉&lt;/a&gt;（内容创作者-算法量化部署工程师）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/YinHan-Zhang&#34;&gt;张银晗&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/LinChentang&#34;&gt;左春生&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Jin-Zhang-Yaoguang&#34;&gt;张晋&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Aphasia0515&#34;&gt;李娇娇&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Kedreamix&#34;&gt;邓恺俊&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Zhiyuan-Fan&#34;&gt;范致远&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Beyondzjl&#34;&gt;周景林&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/very-very-very&#34;&gt;诸世纪&lt;/a&gt;（内容创作者-算法工程师）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/YixinZ-NUS&#34;&gt;Zhang Yixin&lt;/a&gt;（内容创作者-IT爱好者）&lt;/li&gt; &#xA; &lt;li&gt;Sarai（内容创作者-AI应用爱好者）&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;其他&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;特别感谢 &lt;a href=&#34;https://github.com/Sm1les&#34;&gt;@Sm1les&lt;/a&gt;、&lt;a href=&#34;https://github.com/LSGOMYP&#34;&gt;@LSGOMYP&lt;/a&gt; 对本项目的帮助与支持；&lt;/li&gt; &#xA; &lt;li&gt;感谢 &lt;a href=&#34;https://github.com/GitHubDaily&#34;&gt;GithubDaily&lt;/a&gt; 提供的双语字幕；&lt;/li&gt; &#xA; &lt;li&gt;如果有任何想法可以联系我们 DataWhale 也欢迎大家多多提出 issue；&lt;/li&gt; &#xA; &lt;li&gt;特别感谢以下为教程做出贡献的同学！&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;a href=&#34;https://datawhalechina.github.io/llm-cookbook/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=datawhalechina/llm-cookbook&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;Made with &lt;a href=&#34;https://contrib.rocks&#34;&gt;contrib.rocks&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#datawhalechina/llm-cookbook&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=datawhalechina/llm-cookbook&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;关注我们&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;扫描下方二维码关注公众号：Datawhale&lt;/p&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/datawhalechina/llm-cookbook/main/figures/qrcode.jpeg&#34; width=&#34;180&#34; height=&#34;180&#34;&gt; &#xA;&lt;/div&gt; Datawhale 是一个专注于数据科学与 AI 领域的开源组织，汇集了众多领域院校和知名企业的优秀学习者，聚合了一群有开源精神和探索精神的团队成员。微信搜索公众号Datawhale可以加入我们。 &#xA;&lt;h2&gt;LICENSE&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a rel=&#34;license&#34; href=&#34;http://creativecommons.org/licenses/by-nc-sa/4.0/&#34;&gt;&lt;img alt=&#34;知识共享许可协议&#34; style=&#34;border-width:0&#34; src=&#34;https://img.shields.io/badge/license-CC%20BY--NC--SA%204.0-lightgrey&#34;&gt;&lt;/a&gt;&lt;br&gt;本作品采用&lt;a rel=&#34;license&#34; href=&#34;http://creativecommons.org/licenses/by-nc-sa/4.0/&#34;&gt;知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议&lt;/a&gt;进行许可。&lt;/p&gt;</summary>
  </entry>
</feed>