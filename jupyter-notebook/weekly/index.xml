<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-24T01:48:09Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>langchain-ai/rag-from-scratch</title>
    <updated>2024-03-24T01:48:09Z</updated>
    <id>tag:github.com,2024-03-24:/langchain-ai/rag-from-scratch</id>
    <link href="https://github.com/langchain-ai/rag-from-scratch" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;RAG From Scratch&lt;/h1&gt; &#xA;&lt;p&gt;Retrieval augmented generation (RAG) comes is a general methodology for connecting LLMs with external data sources. These notebooks accompany a video series will build up an understanding of RAG from scratch, starting with the basics of indexing, retrieval, and generation. It will build up to more advanced techniques to address edge cases or challenges in RAG: &lt;img src=&#34;https://github.com/langchain-ai/rag-from-scratch/assets/122662504/54a2d76c-b07e-49e7-b4ce-fc45667360a1&#34; alt=&#34;rag_detail_v2&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Video playlist: &lt;a href=&#34;https://www.youtube.com/watch?v=wd7TZ4w1mSw&amp;amp;feature=youtu.be&#34;&gt;https://www.youtube.com/watch?v=wd7TZ4w1mSw&amp;amp;feature=youtu.be&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>abdullahtarek/tennis_analysis</title>
    <updated>2024-03-24T01:48:09Z</updated>
    <id>tag:github.com,2024-03-24:/abdullahtarek/tennis_analysis</id>
    <link href="https://github.com/abdullahtarek/tennis_analysis" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This project analyzes Tennis players in a video to measure their speed, ball shot speed and number of shots. This project will detect players and the tennis ball using YOLO and also utilizes CNNs to extract court keypoints. This hands on project is perfect for polishing your machine learning, and computer vision skills.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Tennis Analysis&lt;/h1&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;This project analyzes Tennis players in a video to measure their speed, ball shot speed and number of shots. This project will detect players and the tennis ball using YOLO and also utilizes CNNs to extract court keypoints. This hands on project is perfect for polishing your machine learning, and computer vision skills.&lt;/p&gt; &#xA;&lt;h2&gt;Output Videos&lt;/h2&gt; &#xA;&lt;p&gt;Here is a screenshot from one of the output videos:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/abdullahtarek/tennis_analysis/main/output_videos/screenshot.jpeg&#34; alt=&#34;Screenshot&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Models Used&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;YOLO v8 for player detection&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Fine Tuned YOLO for tennis ball detection&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Court Key point extraction&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Trained YOLOV5 model: &lt;a href=&#34;https://drive.google.com/file/d/1UZwiG1jkWgce9lNhxJ2L0NVjX1vGM05U/view?usp=sharing&#34;&gt;https://drive.google.com/file/d/1UZwiG1jkWgce9lNhxJ2L0NVjX1vGM05U/view?usp=sharing&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Trained tennis court key point model: &lt;a href=&#34;https://drive.google.com/file/d/1QrTOF1ToQ4plsSZbkBs3zOLkVt3MBlta/view?usp=sharing&#34;&gt;https://drive.google.com/file/d/1QrTOF1ToQ4plsSZbkBs3zOLkVt3MBlta/view?usp=sharing&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Tennis ball detetcor with YOLO: training/tennis_ball_detector_training.ipynb&lt;/li&gt; &#xA; &lt;li&gt;Tennis court keypoint with Pytorch: training/tennis_court_keypoints_training.ipynb&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;python3.8&lt;/li&gt; &#xA; &lt;li&gt;ultralytics&lt;/li&gt; &#xA; &lt;li&gt;pytroch&lt;/li&gt; &#xA; &lt;li&gt;pandas&lt;/li&gt; &#xA; &lt;li&gt;numpy&lt;/li&gt; &#xA; &lt;li&gt;opencv&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>guidance-ai/guidance</title>
    <updated>2024-03-24T01:48:09Z</updated>
    <id>tag:github.com,2024-03-24:/guidance-ai/guidance</id>
    <link href="https://github.com/guidance-ai/guidance" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A guidance language for controlling large language models.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;right&#34;&gt;&#xA; &lt;a href=&#34;https://guidance.readthedocs.org&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/guidance/badge/?version=latest&amp;amp;style=flat&#34;&gt;&lt;/a&gt;&#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;docs/figures/guidance_logo_blue_dark.svg&#34;&gt; &#xA;  &lt;img alt=&#34;guidance&#34; src=&#34;https://raw.githubusercontent.com/guidance-ai/guidance/main/docs/figures/guidance_logo_blue.svg?sanitize=true&#34; width=&#34;300&amp;quot;&#34;&gt; &#xA; &lt;/picture&gt;&#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;em&gt;Note that v0.1 is a dramatically new version developed while releases had to be paused over the summer. If you are looking for the old version based on handlebars, you can use v0.0.64, but you should instead try porting over to the much better new version :)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;guidance&lt;/code&gt;&lt;/strong&gt; is a programming paradigm that offers superior control and efficiency compared to conventional prompting and chaining. It allows users to constrain generation (e.g. with regex and CFGs) as well as to interleave control (conditional, loops) and generation seamlessly. Here are some important features:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Pure, beautiful python&lt;/strong&gt; with additional LM functionality. E.g. here is &lt;a href=&#34;https://raw.githubusercontent.com/guidance-ai/guidance/main/#basic-generation&#34;&gt;basic generation&lt;/a&gt;:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from guidance import models, gen&#xA;&#xA;# load a model (could be Transformers, LlamaCpp, VertexAI, OpenAI...)&#xA;llama2 = models.LlamaCpp(path) &#xA;&#xA;# append text or generations to the model&#xA;llama2 + f&#39;Do you want a joke or a poem? &#39; + gen(stop=&#39;.&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img alt=&#34;Do you want a joke or a poem? I&#39;ll give you a poem&#34; src=&#34;https://raw.githubusercontent.com/guidance-ai/guidance/main/docs/figures/simple_gen_llama2_7b.png&#34; width=&#34;354&#34;&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guidance-ai/guidance/main/#constrained-generation&#34;&gt;&lt;strong&gt;Constrained generation&lt;/strong&gt;&lt;/a&gt; with &lt;a href=&#34;https://raw.githubusercontent.com/guidance-ai/guidance/main/#select-basic&#34;&gt;selects&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/guidance-ai/guidance/main/#regular-expressions&#34;&gt;regular expressions&lt;/a&gt;, and &lt;a href=&#34;https://raw.githubusercontent.com/guidance-ai/guidance/main/#context-free-grammars&#34;&gt;context-free grammars&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from guidance import select&#xA;&#xA;# a simple select between two options&#xA;llama2 + f&#39;Do you want a joke or a poem? A &#39; + select([&#39;joke&#39;, &#39;poem&#39;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img alt=&#34;Do you want a joke or a poem? A poem&#34; src=&#34;https://raw.githubusercontent.com/guidance-ai/guidance/main/docs/figures/simple_select_llama2_7b.png&#34; width=&#34;277&#34;&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Rich templates with f-strings&lt;/strong&gt;:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;llama2 + f&#39;&#39;&#39;\&#xA;Do you want a joke or a poem? A {select([&#39;joke&#39;, &#39;poem&#39;])}.&#xA;Okay, here is a one-liner: &#34;{gen(stop=&#39;&#34;&#39;)}&#34;&#xA;&#39;&#39;&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img width=&#34;358&#34; alt=&#34;image&#34; src=&#34;https://github.com/guidance-ai/guidance/assets/3740613/486ca968-89b1-4c02-b914-3b9714fe5890&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guidance-ai/guidance/main/#stateful-control--generation&#34;&gt;&lt;strong&gt;Stateful control + generation&lt;/strong&gt;&lt;/a&gt; makes it easy to interleave prompting / logic / generation, no need for intermediate parsers:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# capture our selection under the name &#39;answer&#39;&#xA;lm = llama2 + f&#34;Do you want a joke or a poem? A {select([&#39;joke&#39;, &#39;poem&#39;], name=&#39;answer&#39;)}.\n&#34;&#xA;&#xA;# make a choice based on the model&#39;s previous selection&#xA;if lm[&#34;answer&#34;] == &#34;joke&#34;:&#xA;    lm += f&#34;Here is a one-line joke about cats: &#34; + gen(&#39;output&#39;, stop=&#39;\n&#39;)&#xA;else:&#xA;    lm += f&#34;Here is a one-line poem about dogs: &#34; + gen(&#39;output&#39;, stop=&#39;\n&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img width=&#34;393&#34; alt=&#34;image&#34; src=&#34;https://github.com/guidance-ai/guidance/assets/3740613/66d47ce7-1d5a-4dbd-b676-66b9c1094184&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Abstract chat interface&lt;/strong&gt; that uses the correct special tokens for any chat model:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from guidance import user, assistant&#xA;&#xA;# load a chat model&#xA;chat_lm = models.LlamaCppChat(path)&#xA;&#xA;# wrap with chat block contexts&#xA;with user():&#xA;    lm = chat_lm + &#39;Do you want a joke or a poem?&#39;&#xA;&#xA;with assistant():&#xA;    lm += f&#34;A {select([&#39;joke&#39;, &#39;poem&#39;])}.&#34;`&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img width=&#34;331&#34; alt=&#34;image&#34; src=&#34;https://github.com/guidance-ai/guidance/assets/3740613/89c3e0e2-ed0a-4715-8366-2efca74b7b71&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;ol start=&#34;6&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Easy to write reusable components&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import guidance&#xA;&#xA;@guidance&#xA;def one_line_thing(lm, thing, topic):&#xA;    lm += f&#39;Here is a one-line {thing} about {topic}: &#39; + gen(stop=&#39;\n&#39;)&#xA;    return lm # return our updated model&#xA;&#xA;# pick either a joke or a poem&#xA;lm = llama2 + f&#34;Do you want a joke or a poem? A {select([&#39;joke&#39;, &#39;poem&#39;], name=&#39;thing&#39;)}.\n&#34;&#xA;&#xA;# call our guidance function&#xA;lm += one_line_thing(lm[&#39;thing&#39;], &#39;cats&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img width=&#34;386&#34; alt=&#34;image&#34; src=&#34;https://github.com/guidance-ai/guidance/assets/3740613/60071680-8bbb-4fa5-a298-613d4fd55fa7&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;ol start=&#34;7&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;A library of pre-built components&lt;/strong&gt;, e.g. substring:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from guidance import substring&#xA;&#xA;# define a set of possible statements&#xA;text = &#39;guidance is awesome. guidance is so great. guidance is the best thing since sliced bread.&#39;&#xA;&#xA;# force the model to make an exact quote&#xA;llama2 + f&#39;Here is a true statement about the guidance library: &#34;{substring(text)}&#34;&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img width=&#34;589&#34; alt=&#34;image&#34; src=&#34;https://github.com/guidance-ai/guidance/assets/3740613/9a7178ad-ed73-4e6b-b418-f9d2a3a76b88&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;ol start=&#34;8&#34;&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guidance-ai/guidance/main/#automatic-interleaving-of-control-and-generation-tool-use&#34;&gt;&lt;strong&gt;Easy tool use&lt;/strong&gt;&lt;/a&gt;, where the model stops generation when a tool is called, calls the tool, then resumes generation. For example, here is a simple version of a calculator, via four separate &#39;tools&#39;:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@guidance&#xA;def add(lm, input1, input2):&#xA;    lm += f&#39; = {int(input1) + int(input2)}&#39;&#xA;    return lm&#xA;@guidance&#xA;def subtract(lm, input1, input2):&#xA;    lm += f&#39; = {int(input1) - int(input2)}&#39;&#xA;    return lm&#xA;@guidance&#xA;def multiply(lm, input1, input2):&#xA;    lm += f&#39; = {float(input1) * float(input2)}&#39;&#xA;    return lm&#xA;@guidance&#xA;def divide(lm, input1, input2):&#xA;    lm += f&#39; = {float(input1) / float(input2)}&#39;&#xA;    return lm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now we call &lt;code&gt;gen&lt;/code&gt; with these tools as options. Notice how generation is stopped and restarted automatically:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;lm = llama2 + &#39;&#39;&#39;\&#xA;1 + 1 = add(1, 1) = 2&#xA;2 - 3 = subtract(2, 3) = -1&#xA;&#39;&#39;&#39;&#xA;lm + gen(max_tokens=15, tools=[add, subtract, multiply, divide])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img width=&#34;201&#34; alt=&#34;image&#34; src=&#34;https://github.com/guidance-ai/guidance/assets/3740613/646e1a7d-0206-419b-8206-1d835c3a0e0a&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;ol start=&#34;9&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Speed&lt;/strong&gt;: In contrast to chaining, &lt;code&gt;guidance&lt;/code&gt; programs are the equivalent of a single LLM call. More so, whatever non-generated text that gets appended is batched, so that &lt;code&gt;guidance&lt;/code&gt; programs are &lt;strong&gt;faster&lt;/strong&gt; than having the LM generate intermediate text when you have a set structure.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Token healing&lt;/strong&gt;: Users deal with text (or bytes) rather than tokens, and thus don&#39;t have to worry about &lt;a href=&#34;https://towardsdatascience.com/the-art-of-prompt-design-prompt-boundaries-and-token-healing-3b2448b0be38&#34;&gt;perverse token boundaries issues&lt;/a&gt; such as &#39;prompt ending in whitespace&#39;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Streaming support&lt;/strong&gt;, also integrated with jupyter notebooks:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;lm = llama2 + &#39;Here is a cute 5-line poem about cats and dogs:\n&#39;&#xA;for i in range(5):&#xA;    lm += f&#34;LINE {i+1}: &#34; + gen(temperature=0.8, suffix=&#34;\n&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/guidance-ai/guidance/main/docs/figures/simple_streaming_example.gif&#34; width=&#34;337&#34;&gt; &#xA;&lt;ol start=&#34;13&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;High compatibility:&lt;/strong&gt; works with Transformers, llama.cpp, VertexAI, OpenAI. Users can write one guidance program and execute it on many backends. (note that the most powerful control features require endpoint integration, and for now work best with Transformers and llama.cpp).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;gpt = models.OpenAI(&#34;gpt-3.5-turbo&#34;)&#xA;&#xA;with user():&#xA;    lm = gpt + &#34;What is the capital of France?&#34;&#xA;&#xA;with assistant():&#xA;    lm += gen(&#34;capital&#34;)&#xA;&#xA;with user():&#xA;    lm += &#34;What is one short surprising fact about it?&#34;&#xA;&#xA;with assistant():&#xA;    lm += gen(&#34;fact&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img width=&#34;645&#34; alt=&#34;image&#34; src=&#34;https://github.com/guidance-ai/guidance/assets/3740613/f31ed7b8-1868-44d2-b14c-4842b0a40e5c&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;ol start=&#34;14&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-modal support.&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from guidance import image&#xA;&#xA;gemini = models.VertexAI(&#34;gemini-pro-vision&#34;)&#xA;&#xA;with user():&#xA;    lm = gemini + &#34;What is this a picture of?&#34; + image(&#34;longs_peak.jpg&#34;)&#xA;&#xA;with assistant():&#xA;    lm += gen(&#34;answer&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img width=&#34;673&#34; alt=&#34;image&#34; src=&#34;https://github.com/guidance-ai/guidance/assets/3740613/6450d05d-52e9-4ef5-b280-8b57e733d46d&#34;&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guidance-ai/guidance/main/#install&#34;&gt;Install&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guidance-ai/guidance/main/#loading-models&#34;&gt;Loading models&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guidance-ai/guidance/main/#llamacpp&#34;&gt;llama.cpp&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guidance-ai/guidance/main/#transformers&#34;&gt;transformers&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guidance-ai/guidance/main/#vertex-ai&#34;&gt;Vertex&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guidance-ai/guidance/main/#openai&#34;&gt;OpenAI&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guidance-ai/guidance/main/#example-notebooks&#34;&gt;Example notebooks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guidance-ai/guidance/main/#basic-generation&#34;&gt;Basic generation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guidance-ai/guidance/main/#constrained-generation&#34;&gt;Constrained Generation&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guidance-ai/guidance/main/#select-basic&#34;&gt;Select (basic)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guidance-ai/guidance/main/#regular-expressions&#34;&gt;Regular expressions&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guidance-ai/guidance/main/#regex-to-constrain-generation&#34;&gt;Regex to constrain generation&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guidance-ai/guidance/main/#regex-as-stopping-criterion&#34;&gt;Regex as stopping criterion&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guidance-ai/guidance/main/#context-free-grammars&#34;&gt;Context-free grammars&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guidance-ai/guidance/main/#stateful-control--generation&#34;&gt;Stateful control + generation&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guidance-ai/guidance/main/#state-in-immutable-objects&#34;&gt;State in immutable objects&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guidance-ai/guidance/main/#stateful-guidance-functions&#34;&gt;Stateful guidance functions&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guidance-ai/guidance/main/#example-react&#34;&gt;Example: ReAct&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guidance-ai/guidance/main/#example-changing-intermediate-step-of-a-chat-session&#34;&gt;Example: Changing intermediate step of a Chat session&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guidance-ai/guidance/main/#automatic-interleaving-of-control-and-generation-tool-use&#34;&gt;Automatic interleaving of control and generation: tool use&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guidance-ai/guidance/main/#gsm8k-example&#34;&gt;Gsm8k example&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guidance-ai/guidance/main/#automatic-call-grammar-for-guidance-functions&#34;&gt;Automatic call grammar for @guidance functions&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guidance-ai/guidance/main/#text-not-tokens&#34;&gt;Text, not tokens&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guidance-ai/guidance/main/#fast&#34;&gt;Fast&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guidance-ai/guidance/main/#integrated-stateful-control-is-faster&#34;&gt;Integrated stateful control is faster&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guidance-ai/guidance/main/#guidance-acceleration&#34;&gt;Guidance acceleration&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install guidance&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Loading models&lt;/h2&gt; &#xA;&lt;h3&gt;llama.cpp&lt;/h3&gt; &#xA;&lt;p&gt;Install the python bindings:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CMAKE_ARGS=&#34;-DLLAMA_CUBLAS=on&#34; pip install llama-cpp-python&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Loading the model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from guidance import models&#xA;lm = models.LlamaCpp(path_to_model, n_gpu_layers=-1)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Transformers&lt;/h3&gt; &#xA;&lt;p&gt;Install transformers:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from guidance import models&#xA;lm = models.Transformers(model_name_or_path)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Vertex AI&lt;/h3&gt; &#xA;&lt;p&gt;Remote endpoints that don&#39;t have explicit guidance integration are run &#34;optimistically&#34;. This means that all the text that can be forced is given to the model as a prompt (or chat context) and then the model is run in streaming mode without hard constrants (since the remote API doesn&#39;t support them). If the model ever violates the contraints then the model stream is stopped and we optionally try it again at that point. This means that all the API-supported control work as expected, and more complex controls/parsing that is not supported by the API work if the model stays consistent with the program.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;palm2 = models.VertexAI(&#34;text-bison@001&#34;)&#xA;&#xA;with instruction():&#xA;    lm = palm2 + &#34;What is one funny fact about Seattle?&#34;&#xA;&#xA;lm + gen(&#34;fact&#34;, max_tokens=100)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img width=&#34;635&#34; alt=&#34;image&#34; src=&#34;https://github.com/guidance-ai/guidance/assets/3740613/693ae08f-68f7-4368-bd25-19afc9bfc0a5&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h3&gt;OpenAI&lt;/h3&gt; &#xA;&lt;p&gt;OpenAI endpoint don&#39;t have direct support for guidance grammars, but through optimistic running we can still control them in ways that match the model type:&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Legacy completion models:&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;curie = models.OpenAI(&#34;text-curie-001&#34;)&#xA;&#xA;curie + &#34;The smallest cats are&#34; + gen(stop=&#34;.&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img width=&#34;263&#34; alt=&#34;image&#34; src=&#34;https://github.com/guidance-ai/guidance/assets/3740613/116a906c-ea77-4a13-a83a-682029d5e5c8&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Instruct tuned models:&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;gpt_instruct = models.OpenAI(&#34;gpt-3.5-turbo-instruct&#34;)&#xA;&#xA;with instruction():&#xA;    lm = gpt_instruct + &#34;What are the smallest cats?&#34;&#xA;    &#xA;lm += gen(stop=&#34;.&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img width=&#34;574&#34; alt=&#34;image&#34; src=&#34;https://github.com/guidance-ai/guidance/assets/3740613/56a53ce1-89f5-4e9d-bdb8-86fb3eebf309&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Chat models:&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;gpt = models.OpenAI(&#34;gpt-3.5-turbo&#34;)&#xA;&#xA;with system():&#xA;    lm = gpt + &#34;You are a cat expert.&#34;&#xA;&#xA;with user():&#xA;    lm += &#34;What are the smallest cats?&#34;&#xA;&#xA;with assistant():&#xA;    lm += gen(&#34;answer&#34;, stop=&#34;.&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img width=&#34;367&#34; alt=&#34;image&#34; src=&#34;https://github.com/guidance-ai/guidance/assets/3740613/46102f0f-37dc-4bb1-99b7-e5895bdee772&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Example notebooks&lt;/h2&gt; &#xA;&lt;p&gt;We are working on updating our example notebooks. The following ones have been updated:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guidance-ai/guidance/main/notebooks/tutorials/intro_to_guidance.ipynb&#34;&gt;Basic tutorial&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guidance-ai/guidance/main/notebooks/chat_with_search.ipynb&#34;&gt;Chatbot with search&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;More coming soon&lt;/p&gt; &#xA;&lt;h2&gt;Basic generation&lt;/h2&gt; &#xA;&lt;p&gt;An &lt;code&gt;lm&lt;/code&gt; object is immutable, so you change it by creating new copies of it. By default, when you append things to &lt;code&gt;lm&lt;/code&gt;, it creates a copy, e.g.:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from guidance import models, gen, select&#xA;llama2 = models.LlamaCpp(model)&#xA;&#xA;# llama2 is not modified, `lm` is a copy of `llama2` with &#39;This is a prompt&#39; appended to its state&#xA;lm = llama2 + &#39;This is a prompt&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img width=&#34;124&#34; alt=&#34;image&#34; src=&#34;https://github.com/guidance-ai/guidance/assets/3740613/c1e96b2b-8f4a-44ee-a8f4-a694a8d7784b&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can append &lt;em&gt;generation&lt;/em&gt; calls to model objects, e.g.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;lm = llama2 + &#39;This is a prompt&#39; + gen(max_tokens=10)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img width=&#34;267&#34; alt=&#34;image&#34; src=&#34;https://github.com/guidance-ai/guidance/assets/3740613/d2e5ed34-ba9d-4bdd-872d-2b76f8e3cf85&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can also interleave generation calls with plain text, or control flows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Note how we set stop tokens&#xA;lm = llama2 + &#39;I like to play with my &#39; + gen(stop=&#39; &#39;) + &#39; in&#39; + gen(stop=[&#39;\n&#39;, &#39;.&#39;, &#39;!&#39;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img width=&#34;279&#34; alt=&#34;image&#34; src=&#34;https://github.com/guidance-ai/guidance/assets/3740613/2d47fd65-1982-4dd8-9ba9-a01e62fba455&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Constrained Generation&lt;/h2&gt; &#xA;&lt;h3&gt;Select (basic)&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;select&lt;/code&gt; constrains generation to a set of options:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;lm = llama2 + &#39;I like the color &#39; + select([&#39;red&#39;, &#39;blue&#39;, &#39;green&#39;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img width=&#34;137&#34; alt=&#34;image&#34; src=&#34;https://github.com/guidance-ai/guidance/assets/3740613/f0b97629-78a9-439d-90b2-06af31fdc40e&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Regular expressions&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;gen&lt;/code&gt; has optional arguments &lt;code&gt;regex&lt;/code&gt; and &lt;code&gt;stop_regex&lt;/code&gt;, which allow generation (and stopping, respectively) to be controlled by a regex.&lt;/p&gt; &#xA;&lt;h4&gt;Regex to constrain generation&lt;/h4&gt; &#xA;&lt;p&gt;Unconstrained:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;lm = llama2 + &#39;Question: Luke has ten balls. He gives three to his brother.\n&#39;&#xA;lm += &#39;How many balls does he have left?\n&#39;&#xA;lm += &#39;Answer: &#39; + gen(stop=&#39;\n&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img width=&#34;405&#34; alt=&#34;image&#34; src=&#34;https://github.com/guidance-ai/guidance/assets/3740613/55fb66ea-a717-417a-8a70-14c46eba4c66&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;Constrained by regex:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;lm = llama2 + &#39;Question: Luke has ten balls. He gives three to his brother.\n&#39;&#xA;lm += &#39;How many balls does he have left?\n&#39;&#xA;lm += &#39;Answer: &#39; + gen(regex=&#39;\d+&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img width=&#34;404&#34; alt=&#34;image&#34; src=&#34;https://github.com/guidance-ai/guidance/assets/3740613/b45a5a79-55e0-4c15-884a-fba830c0a153&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Regex as stopping criterion&lt;/h4&gt; &#xA;&lt;p&gt;Unconstrained:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;lm = llama2 + &#39;19, 18,&#39; + gen(max_tokens=50)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img width=&#34;359&#34; alt=&#34;image&#34; src=&#34;https://github.com/guidance-ai/guidance/assets/3740613/5dd13454-cc42-4e27-a52c-19a31237891c&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;Stop with traditional stop text, whenever the model generates the number 7:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;lm = llama2 + &#39;19, 18,&#39; + gen(max_tokens=50, stop=&#39;7&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img width=&#34;73&#34; alt=&#34;image&#34; src=&#34;https://github.com/guidance-ai/guidance/assets/3740613/fc96d7c3-381d-4766-8bee-c930669f518a&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;Stop whenever the model generates the character &lt;code&gt;7&lt;/code&gt; without any numbers around it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;lm = llama2 + &#39;19, 18,&#39; + gen(max_tokens=50, stop_regex=&#39;[^\d]7[^\d]&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img width=&#34;293&#34; alt=&#34;image&#34; src=&#34;https://github.com/guidance-ai/guidance/assets/3740613/a657e566-b1a4-447a-82a5-b88977b5fedf&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Context-free grammars&lt;/h3&gt; &#xA;&lt;p&gt;We expose a variety of operators that make it easy to define CFGs, which in turn can be used to constrain generation. For example, we can use the &lt;code&gt;select&lt;/code&gt; operator (it accepts CFGs as options), &lt;code&gt;zero_or_more&lt;/code&gt; and &lt;code&gt;one_or_more&lt;/code&gt; to define a grammar for mathematical expressions:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import guidance&#xA;from guidance import one_or_more, select, zero_or_more&#xA;# stateless=True indicates this function does not depend on LLM generations&#xA;@guidance(stateless=True)&#xA;def number(lm):&#xA;    n = one_or_more(select([&#39;0&#39;, &#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;, &#39;5&#39;, &#39;6&#39;, &#39;7&#39;, &#39;8&#39;, &#39;9&#39;]))&#xA;    # Allow for negative or positive numbers&#xA;    return lm + select([&#39;-&#39; + n, n])&#xA;&#xA;@guidance(stateless=True)&#xA;def operator(lm):&#xA;    return lm + select([&#39;+&#39; , &#39;*&#39;, &#39;**&#39;, &#39;/&#39;, &#39;-&#39;])&#xA;&#xA;@guidance(stateless=True)&#xA;def expression(lm):&#xA;    # Either&#xA;    # 1. A number (terminal)&#xA;    # 2. two expressions with an operator and optional whitespace&#xA;    # 3. An expression with parentheses around it&#xA;    return lm + select([&#xA;        number(),&#xA;        expression() + zero_or_more(&#39; &#39;) +  operator() + zero_or_more(&#39; &#39;) +  expression(),&#xA;        &#39;(&#39; + expression() + &#39;)&#39;&#xA;    ])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;@guidance(stateless=True)&lt;/code&gt; decorator makes it such that a function (e.g. &lt;code&gt;expression&lt;/code&gt;) lives as a stateless grammar that does not get &#39;executed&#39; until we call call &lt;code&gt;lm + expression()&lt;/code&gt; or &lt;code&gt;lm += expression()&lt;/code&gt;. For example, here is an example of &lt;em&gt;unconstrained&lt;/em&gt; generation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Without constraints&#xA;lm = llama2 + &#39;Problem: Luke has a hundred and six balls. He then loses thirty six.\n&#39;&#xA;lm += &#39;Equivalent arithmetic expression: &#39; + gen(stop=&#39;\n&#39;) + &#39;\n&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img width=&#34;462&#34; alt=&#34;image&#34; src=&#34;https://github.com/guidance-ai/guidance/assets/3740613/54af1909-cad4-4fb1-8987-dfdfc02f8f42&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;Notice how the model wrote the right equation but solved it (incorrectly). If we wanted to constrain the model such that it only writes valid expressions (without trying to solve them), we can just append our grammar to it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;grammar = expression()&#xA;lm = llama2 + &#39;Problem: Luke has a hundred and six balls. He then loses thirty six.\n&#39;&#xA;lm += &#39;Equivalent arithmetic expression: &#39; + grammar + &#39;\n&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img width=&#34;460&#34; alt=&#34;image&#34; src=&#34;https://github.com/guidance-ai/guidance/assets/3740613/dbda0ff8-8edd-4384-b63d-fc98792e0689&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;Grammars are very easy to compose. For example, let&#39;s say we want a grammar that generates either a mathematical expression or an expression followed by a solution followed by another expression. Creating this grammar is easy:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from guidance import regex&#xA;grammar = select([expression(), expression() +  regex(&#39; = \d+; &#39;) + expression()])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We can generate according to it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;llama2 + &#39;Here is a math expression for two plus two: &#39; + grammar&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img width=&#34;346&#34; alt=&#34;image&#34; src=&#34;https://github.com/guidance-ai/guidance/assets/3740613/283e6973-0b8d-4153-a82b-9f5db1460da9&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;llama2 + &#39;2 + 2 = 4; 3+3\n&#39; + grammar&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img width=&#34;109&#34; alt=&#34;image&#34; src=&#34;https://github.com/guidance-ai/guidance/assets/3740613/d584a93c-bf24-43d5-8f8d-501e7eb88422&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;Even if you don&#39;t like thinking in terms of recursive grammars, this formalism makes it easy to constrain generation. For example, let&#39;s say we have the following one-shot prompt:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@guidance(stateless=True)&#xA;def ner_instruction(lm, input):&#xA;    lm += f&#39;&#39;&#39;\&#xA;    Please tag each word in the input with PER, ORG, LOC, or nothing&#xA;    ---&#xA;    Input: John worked at Apple.&#xA;    Output:&#xA;    John: PER&#xA;    worked: &#xA;    at: &#xA;    Apple: ORG&#xA;    .: &#xA;    ---&#xA;    Input: {input}&#xA;    Output:&#xA;    &#39;&#39;&#39;&#xA;    return lm&#xA;input = &#39;Julia never went to Morocco in her life!!&#39;&#xA;llama2 + ner_instruction(input) + gen(stop=&#39;---&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img width=&#34;465&#34; alt=&#34;image&#34; src=&#34;https://github.com/guidance-ai/guidance/assets/3740613/8ecf5ad4-68b8-4e7a-b107-b1a5613e4c68&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;Notice that the model did not spell the word &#39;Morocco&#39; correctly. Sometimes the model might also hallucinate a tag that doesn&#39;t exist. We can improve this by adding more few-shot examples, etc, but we can also constrain generation to the exact format we want:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import re&#xA;&#xA;@guidance(stateless=True)&#xA;def constrained_ner(lm, input):&#xA;    # Split into words&#xA;    words = [x for x in re.split(&#39;([^a-zA-Z0-9])&#39;, input) if x and not re.match(&#39;\s&#39;, x)]&#xA;    ret = &#39;&#39;&#xA;    for x in words:&#xA;        ret += x + &#39;: &#39; + select([&#39;PER&#39;, &#39;ORG&#39;, &#39;LOC&#39;, &#39;&#39;]) + &#39;\n&#39;&#xA;    return lm + ret&#xA;llama2 + ner_instruction(input) + constrained_ner(input)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img width=&#34;462&#34; alt=&#34;image&#34; src=&#34;https://github.com/guidance-ai/guidance/assets/3740613/72545093-ef16-479a-b666-bd97c54a5dc7&#34;&gt; &#xA;&lt;p&gt;While &lt;code&gt;constrained_ner(input)&lt;/code&gt; &lt;strong&gt;is&lt;/strong&gt; a grammar that constrains the model generation, it &lt;em&gt;feels&lt;/em&gt; like you&#39;re just writing normal imperative python code with &lt;code&gt;+=&lt;/code&gt; and &lt;code&gt;selects&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Stateful control + generation&lt;/h2&gt; &#xA;&lt;h3&gt;State in immutable objects&lt;/h3&gt; &#xA;&lt;p&gt;Whenever you do &lt;code&gt;lm + grammar&lt;/code&gt; or &lt;code&gt;lm + gen&lt;/code&gt;, &lt;code&gt;lm + select&lt;/code&gt;, etc, you return a new lm object with additional state. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;lm = llama2 + &#39;This is a prompt&#39; + gen(name=&#39;test&#39;, max_tokens=10)&#xA;lm += select([&#39;this&#39;, &#39;that&#39;], name=&#39;test2&#39;)&#xA;lm[&#39;test&#39;], lm[&#39;test2&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img width=&#34;296&#34; alt=&#34;image&#34; src=&#34;https://github.com/guidance-ai/guidance/assets/3740613/f0f9d180-6209-40df-9401-40da35d46e1a&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Stateful &lt;code&gt;guidance&lt;/code&gt; functions&lt;/h3&gt; &#xA;&lt;p&gt;The guidance decorator is &lt;code&gt;@guidance(stateless=False)&lt;/code&gt; by default, meaning that a function with this decorator depends on the lm state to execute (either prior state or state generated within the function). For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@guidance(stateless=False)&#xA;def test(lm):&#xA;    lm += &#39;Should I say &#34;Scott&#34;?\n&#39; + select([&#39;yes&#39;, &#39;no&#39;], name=&#39;answer&#39;) + &#39;\n&#39;&#xA;    if lm[&#39;answer&#39;] == &#39;yes&#39;:&#xA;        lm += &#39;Scott&#39;&#xA;    else:&#xA;        lm += &#39;Not Scott&#39;&#xA;    return lm&#xA;llama2 + test()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img width=&#34;159&#34; alt=&#34;image&#34; src=&#34;https://github.com/guidance-ai/guidance/assets/3740613/5a55496b-aea0-46e9-8de6-b63655027653&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Example: ReAct&lt;/h3&gt; &#xA;&lt;p&gt;A big advantage of stateful control is that you don&#39;t have to write any intermediate parsers, and adding follow-up &#39;prompting&#39; is easy, even if the follow up depends on what the model generates. For example, let&#39;s say we want to implement the first example of ReAct prompt in &lt;a href=&#34;https://www.promptingguide.ai/techniques/react&#34;&gt;this&lt;/a&gt;, and let&#39;s say the valid acts are only &#39;Search&#39; or &#39;Finish&#39;. We might write it like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@guidance&#xA;def react_prompt_example(lm, question, max_rounds=10):&#xA;    lm += f&#39;Question: {question}\n&#39;&#xA;    i = 1&#xA;    while True:&#xA;        lm += f&#39;Thought {i}: &#39; + gen(suffix=&#39;\n&#39;)&#xA;        lm += f&#39;Act {i}: &#39; + select([&#39;Search&#39;, &#39;Finish&#39;], name=&#39;act&#39;) &#xA;        lm += &#39;[&#39; + gen(name=&#39;arg&#39;, suffix=&#39;]&#39;) + &#39;\n&#39;&#xA;        if lm[&#39;act&#39;] == &#39;Finish&#39; or i == max_rounds:&#xA;            break&#xA;        else:&#xA;            lm += f&#39;Observation {i}: &#39; + search(lm[&#39;arg&#39;]) + &#39;\n&#39;&#xA;        i += 1&#xA;    return lm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Notice how we don&#39;t have to write a parser for Act and argument and hope that the model generates something valid: we enforce it. Notice also that the loop only stops once the model chooses to act with &#39;Finish&#39; (or once we hit a maximum number of rounds).&lt;/p&gt; &#xA;&lt;h3&gt;Example: Changing intermediate step of a Chat session&lt;/h3&gt; &#xA;&lt;p&gt;We can also hide or change some of what the model generates. For example, below we get a Chat model (notice we use special &lt;code&gt;role&lt;/code&gt; blocks) to name some experts to answer a question, but we always remove &#39;Ferriss&#39; from the list if he is mentioned:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from guidance import user, system, assistant&#xA;lm = llama2&#xA;query = &#39;How can I be more productive?&#39;&#xA;with system():&#xA;    lm += &#39;You are a helpful and terse assistant.&#39;&#xA;with user():&#xA;    lm += f&#39;I want a response to the following question:\n{query}\n&#39;&#xA;    lm += &#39;Name 3 world-class experts (past or present) who would be great at answering this.&#39;&#xA;with assistant():&#xA;    temp_lm = lm&#xA;    for i in range(1, 4):&#xA;        # This regex only allows strings that look like names (where every word is capitalized)&#xA;        # list_append appends the result to a list&#xA;        temp_lm += f&#39;{i}. &#39; + gen(regex=&#39;([A-Z][a-z]*\s*)+&#39;, suffix=&#39;\n&#39;,&#xA;                                  name=&#39;experts&#39;, list_append=True)&#xA;    experts = [x for x in temp_lm[&#39;experts&#39;] if &#39;Ferriss&#39; not in x]&#xA;    # Notice that even if the model generates &#39;Ferriss&#39; above,&#xA;    # it doesn&#39;t get added to `lm`, only to `temp_lm`&#xA;    lm += &#39;, &#39;.join(experts)&#xA;with user():&#xA;    lm += &#39;Please answer the question as if these experts had collaborated in writing an anonymous answer.&#39;&#xA;with assistant():&#xA;    lm += gen(max_tokens=100)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img width=&#34;688&#34; alt=&#34;image&#34; src=&#34;https://github.com/guidance-ai/guidance/assets/3740613/d274f8b8-52e7-41a5-9635-b34f70ed50e0&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Automatic interleaving of control and generation: tool use&lt;/h3&gt; &#xA;&lt;p&gt;Tool use is a common case of stateful control. To make it easy to do so, &lt;code&gt;gen&lt;/code&gt; calls take &lt;code&gt;tools&lt;/code&gt; as an optional argument, where each tool is defined by (1) a grammar that triggers its call and captures the arguments (if any), and (2) the actual tool call. Then, as generation unrolls, whenever the model generates something that matches the grammar of a tool call, it (1) stops generation, (2) calls the tool (which can append whatever it wants to the LM session), and (3) continues generation.&lt;/p&gt; &#xA;&lt;p&gt;For example, here is how we might implement a calculator tool, leveraging our &lt;code&gt;expression&lt;/code&gt; grammar above:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from guidance import capture, Tool&#xA;@guidance(stateless=True)&#xA;def calculator_call(lm):&#xA;    # capture just &#39;names&#39; the expression, to be saved in the LM state&#xA;    return lm + &#39;calculator(&#39; + capture(expression(), &#39;tool_args&#39;) + &#39;)&#39;&#xA;&#xA;@guidance&#xA;def calculator(lm):&#xA;    expression = lm[&#39;tool_args&#39;]&#xA;    # You typically don&#39;t want to run eval directly for save reasons&#xA;    # Here we are guaranteed to only have mathematical expressions&#xA;    lm += f&#39; = {eval(expression)}&#39;&#xA;    return lm&#xA;calculator_tool = Tool(calculator_call(), calculator)&#xA;lm = llama2 + &#39;Here are five expressions:\ncalculator(3 *3) = 33\ncalculator(2 + 1 * 3) = 5\n&#39;&#xA;lm += gen(max_tokens=30, tools=[calculator_tool], stop=&#39;\n\n&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img width=&#34;201&#34; alt=&#34;image&#34; src=&#34;https://github.com/guidance-ai/guidance/assets/3740613/2d9b840a-4fad-4dab-b3e7-20887539b447&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Gsm8k example&lt;/h3&gt; &#xA;&lt;p&gt;Notice that the calculator is just called seamlessly during generation. Here is a more realistic exampe of the model solving a gsm8k question:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@guidance&#xA;def math_with_calc(lm, question):&#xA;    # Two-shot example&#xA;    lm += &#39;&#39;&#39;\&#xA;    Question: John starts with 2 balls. He then quintupled his number of balls. Then he lost half of them. He then gave 3 to his brother. How many does he have left?&#xA;    Reasoning:&#xA;    1. He quintupled his balls. So he has calculator(2 * 5) = 10 balls.&#xA;    1. He lost half. So he has calculator(10 / 2) = 5 balls.&#xA;    3. He gave 3 to his brother. So he has calculator(5 - 3) = 2 balls.&#xA;    Answer: 2&#xA;&#xA;    Question: Jill get 7 dollars a day in allowance. She uses 1 each day to by a bus pass, then gives half away. How much does she have left each day?&#xA;    Reasoning:&#xA;    1. She gets 7 dollars a day.&#xA;    1. She spends 1 on a bus pass. So she has calculator(5 - 1) = 6.&#xA;    3. She gives half away. So that makes calculator(6 / 2) = 3.&#xA;    Answer: 3&#xA;&#xA;    &#39;&#39;&#39;&#xA;    lm += f&#39;Question: {question}\n&#39;&#xA;    lm += &#39;Reasoning:\n&#39; + gen(max_tokens=200, tools=[calculator_tool], stop=&#39;Answer&#39;)&#xA;    # Only numbers or commas&#xA;    lm += &#39;Answer: &#39; + gen(regex=&#39;[-\d,]+&#39;)&#xA;    return lm&#xA;&#xA;question = &#39;&#39;&#39;Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers&#39; market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers&#39; market?&#39;&#39;&#39;&#xA;llama2 + math_with_calc(question)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img width=&#34;685&#34; alt=&#34;image&#34; src=&#34;https://github.com/guidance-ai/guidance/assets/3740613/0c7b8da0-b295-46cd-a312-604ecfba7b33&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Automatic call grammar for @guidance functions&lt;/h3&gt; &#xA;&lt;p&gt;You can also initialize a &lt;code&gt;Tool&lt;/code&gt; with any &lt;code&gt;@guidance&lt;/code&gt;-decorated function, and the default call grammar will be like a python call. Here is an example of using multiple such tools in the same &lt;code&gt;gen&lt;/code&gt; call:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@guidance&#xA;def say_scott(lm, n):&#xA;    lm += &#39;\n&#39;&#xA;    for _ in range(int(n)):&#xA;        lm += &#39;Scott\n&#39;&#xA;    return lm&#xA;&#xA;@guidance&#xA;def say_marco(lm, n):&#xA;    lm += &#39;\n&#39;&#xA;    for _ in range(int(n)):&#xA;        lm += &#39;marco\n&#39;&#xA;    return lm&#xA;&#xA;tools = [Tool(callable=say_scott), Tool(callable=say_marco)]&#xA;llama2 + &#39;&#39;&#39;\&#xA;I am going to call say_scott and say_marco a few times:&#xA;say_scott(1)&#xA;Scott&#xA;&#39;&#39;&#39; + gen(max_tokens=20, tools=tools)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img width=&#34;395&#34; alt=&#34;image&#34; src=&#34;https://github.com/guidance-ai/guidance/assets/3740613/8025699b-59a1-4a3f-8b1e-a895a54924e2&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Text, not tokens&lt;/h2&gt; &#xA;&lt;p&gt;The standard greedy tokenizations used by most language models introduce a variety of subtle and powerful biases, which that can have all kinds of unintended consequences for your prompts. For example, take the following prompt, given to gpt-2 (standard greedy tokenization):&lt;/p&gt; &#xA;&lt;p&gt;hf_gen(prompt, max_tokens=10)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import pipeline&#xA;pipe = pipeline(&#34;text-generation&#34;, model=&#34;gpt2&#34;)&#xA;def hf_gen(prompt, max_tokens=100):&#xA;    return pipe(prompt, do_sample=False, max_length=max_tokens, return_full_text=False)[0][&#39;generated_text&#39;]&#xA;&#xA;prompt = &#39;http:&#39;&#xA;hf_gen(prompt, max_tokens=10)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img width=&#34;198&#34; alt=&#34;image&#34; src=&#34;https://github.com/guidance-ai/guidance/assets/3740613/a0fe3e81-89e0-4b4a-8981-edf8b1a8a723&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;Notice how the output generated by the LLM does not complete the URL with the obvious next characters (two forward slashes). It instead creates an invalid URL string with a space in the middle. Why? Because the string &lt;code&gt;://&lt;/code&gt; is its own token, and so once the model sees a colon by itself, it assumes that the next characters cannot be &lt;code&gt;//&lt;/code&gt;; otherwise, the tokenizer would not have used &lt;code&gt;:&lt;/code&gt;, and instead would have used &lt;code&gt;://&lt;/code&gt;. This is why there are warnings about ending prompts in whitespace, but the problem is way more pervasive than that: any boundary that may span multiple tokens will cause problems, e.g. notice how a partial word causes incorrect completion:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;prompt = &#39;John is a&#39;&#xA;hf_gen(prompt, max_tokens=5)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img width=&#34;133&#34; alt=&#34;image&#34; src=&#34;https://github.com/guidance-ai/guidance/assets/3740613/44906e57-c4ca-4dc3-a1c3-2fdba040259b&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;prompt = &#39;John is a fo&#39;&#xA;hf_gen(prompt, max_tokens=5)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img width=&#34;52&#34; alt=&#34;image&#34; src=&#34;https://github.com/guidance-ai/guidance/assets/3740613/df649320-ec8e-468a-bb2f-e1994f16c9b6&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;While problematic enough for normal prompts, these problems would be a disaster in the kinds of prompts we wrote in this readme, where there is interleaving of prompting and generation happening multiple times (and thus multiple opportunities for problems). This is why &lt;code&gt;guidance&lt;/code&gt; implements &lt;a href=&#34;https://towardsdatascience.com/the-art-of-prompt-design-prompt-boundaries-and-token-healing-3b2448b0be38&#34;&gt;token healing&lt;/a&gt;, a feature that deals with prompt boundaries automatically, allowing users to just think in terms of &lt;strong&gt;text&lt;/strong&gt; rather than tokens. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from guidance import models&#xA;gpt = models.Transformers(&#39;gpt2&#39;)&#xA;prompt = &#39;http:&#39;&#xA;gpt + prompt + gen(max_tokens=10)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img width=&#34;244&#34; alt=&#34;image&#34; src=&#34;https://github.com/guidance-ai/guidance/assets/3740613/c9f26a58-52f2-457c-958a-e048f68eb388&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;prompt = &#39;John is a fo&#39;&#xA;gpt + prompt + gen(max_tokens=2)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img width=&#34;186&#34; alt=&#34;image&#34; src=&#34;https://github.com/guidance-ai/guidance/assets/3740613/bc5e4cd4-9b82-4c09-9db2-9e890dad1d69&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Fast&lt;/h2&gt; &#xA;&lt;h3&gt;Integrated stateful control is faster&lt;/h3&gt; &#xA;&lt;p&gt;We have full control of the decoding loop in our integration with &lt;code&gt;transformers&lt;/code&gt; and &lt;code&gt;llamacpp&lt;/code&gt;, allowing us to add control and additional prompt without any extra cost.&lt;br&gt; If instead we&#39;re calling a server, we pay the extra cost of making additional requests, which might be ok if the server has caching, but quickly becomes impractical if the server does not have fine-grained caching. For example, note again the output from the &lt;a href=&#34;https://raw.githubusercontent.com/guidance-ai/guidance/main/#gsm8k-example&#34;&gt;gsm8k example with calculator&lt;/a&gt; above:&lt;/p&gt; &#xA;&lt;img width=&#34;624&#34; alt=&#34;image&#34; src=&#34;https://github.com/guidance-ai/guidance/assets/3740613/2c75b0f2-6997-43d9-b10e-cb9f6f2e2de5&#34;&gt; &#xA;&lt;p&gt;Every time we call &lt;code&gt;calculator&lt;/code&gt;, we have to stop generation, append the result to the prompt, and resume generation. To avoid slowing down after the first call, a server would need to keep the KV cache up to &#39;3 for breakfast. So she has calculator(16 - 3)&#39;, then roll forward generation from that point on. Even servers that &lt;em&gt;do&lt;/em&gt; have caching often don&#39;t have a way to guarantee state is preserved at each stop and start, and so user&#39;s pay a significant overhead at each interruption. The normal approach of considering everything as a new prompt would cause significant slow downs every time &lt;code&gt;calculator&lt;/code&gt; is called.&lt;/p&gt; &#xA;&lt;h3&gt;Guidance acceleration&lt;/h3&gt; &#xA;&lt;p&gt;In addition to the benefit above, &lt;code&gt;guidance&lt;/code&gt; calls are often &lt;strong&gt;faster&lt;/strong&gt; than running equivalent prompts the traditional way, because we can batch any additional text that is added by the user as execution unrolls (rather than generating it). Take the example below, where we generate a json with a GGUF compressed &lt;code&gt;llama2&lt;/code&gt; 7B executed using llama.cpp:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@guidance&#xA;def character_maker(lm, id, description, valid_weapons):&#xA;    lm += f&#34;&#34;&#34;\&#xA;    The following is a character profile for an RPG game in JSON format.&#xA;    ```json&#xA;    {{&#xA;        &#34;id&#34;: &#34;{id}&#34;,&#xA;        &#34;description&#34;: &#34;{description}&#34;,&#xA;        &#34;name&#34;: &#34;{gen(&#39;name&#39;, stop=&#39;&#34;&#39;)}&#34;,&#xA;        &#34;age&#34;: {gen(&#39;age&#39;, regex=&#39;[0-9]+&#39;, stop=&#39;,&#39;)},&#xA;        &#34;armor&#34;: &#34;{select(options=[&#39;leather&#39;, &#39;chainmail&#39;, &#39;plate&#39;], name=&#39;armor&#39;)}&#34;,&#xA;        &#34;weapon&#34;: &#34;{select(options=valid_weapons, name=&#39;weapon&#39;)}&#34;,&#xA;        &#34;class&#34;: &#34;{gen(&#39;class&#39;, stop=&#39;&#34;&#39;)}&#34;,&#xA;        &#34;mantra&#34;: &#34;{gen(&#39;mantra&#39;, stop=&#39;&#34;&#39;)}&#34;,&#xA;        &#34;strength&#34;: {gen(&#39;strength&#39;, regex=&#39;[0-9]+&#39;, stop=&#39;,&#39;)},&#xA;        &#34;items&#34;: [&#34;{gen(&#39;item&#39;, list_append=True, stop=&#39;&#34;&#39;)}&#34;, &#34;{gen(&#39;item&#39;, list_append=True, stop=&#39;&#34;&#39;)}&#34;, &#34;{gen(&#39;item&#39;, list_append=True, stop=&#39;&#34;&#39;)}&#34;]&#xA;    }}```&#34;&#34;&#34;&#xA;    return lm&#xA;a = time.time()&#xA;lm = llama2 + character_maker(1, &#39;A nimble fighter&#39;, [&#39;axe&#39;, &#39;sword&#39;, &#39;bow&#39;])&#xA;time.time() - a&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img width=&#34;480&#34; alt=&#34;image&#34; src=&#34;https://github.com/guidance-ai/guidance/assets/3740613/85b5a181-6e6a-4582-9203-730f49353aeb&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;Everything that is not green is not actually generated by the model, and is thus batched (much faster). This prompt takes about 1.2 seconds on an A100 GPU. Now, if we let the model generate everything (as in the roughly equivalent prompt below), it takes roughly &lt;code&gt;2.6&lt;/code&gt; seconds (not only is it slower, we also have less control over generation).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@guidance&#xA;def character_maker2(lm, id, description):&#xA;    lm += f&#34;&#34;&#34;\&#xA;    The following is a character profile for an RPG game in JSON format. It has fields &#39;id&#39;, &#39;description&#39;, &#39;name&#39;, &#39;age&#39;, &#39;armor&#39;, weapon&#39;, &#39;class&#39;, &#39;mantra&#39;, &#39;strength&#39;, and &#39;items (just the names of 3 items)&#39;&#xA;    please set description to &#39;{description}&#39;&#xA;    ```json&#34;&#34;&#34; + gen(stop=&#39;```&#39;)&#xA;    return lm&#xA;a = time.time()&#xA;lm = llama2 + character_maker2(1, &#39;A nimble fighter&#39;)&#xA;time.time() - a&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img width=&#34;586&#34; alt=&#34;image&#34; src=&#34;https://github.com/guidance-ai/guidance/assets/3740613/9c55500d-4c90-4f42-9343-43aa2a25efa4&#34;&gt;&lt;br&gt;&lt;/p&gt;</summary>
  </entry>
</feed>