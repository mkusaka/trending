<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-19T01:58:58Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>pinecone-io/examples</title>
    <updated>2023-03-19T01:58:58Z</updated>
    <id>tag:github.com,2023-03-19:/pinecone-io/examples</id>
    <link href="https://github.com/pinecone-io/examples" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;</summary>
  </entry>
  <entry>
    <title>tensorflow/docs</title>
    <updated>2023-03-19T01:58:58Z</updated>
    <id>tag:github.com,2023-03-19:/tensorflow/docs</id>
    <link href="https://github.com/tensorflow/docs" rel="alternate"></link>
    <summary type="html">&lt;p&gt;TensorFlow documentation&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TensorFlow Documentation&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://www.tensorflow.org/images/tf_logo_horizontal.png&#34;&gt;&#xA; &lt;br&gt;&#xA; &lt;br&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;These are the source files for the guide and tutorials on &lt;a href=&#34;https://www.tensorflow.org/overview&#34;&gt;tensorflow.org&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To contribute to the TensorFlow documentation, please read &lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/docs/master/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;, the &lt;a href=&#34;https://www.tensorflow.org/community/contribute/docs&#34;&gt;TensorFlow docs contributor guide&lt;/a&gt;, and the &lt;a href=&#34;https://www.tensorflow.org/community/contribute/docs_style&#34;&gt;style guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To file a docs issue, use the issue tracker in the &lt;a href=&#34;https://github.com/tensorflow/tensorflow/issues/new?template=20-documentation-issue.md&#34;&gt;tensorflow/tensorflow&lt;/a&gt; repo.&lt;/p&gt; &#xA;&lt;p&gt;And join the TensorFlow documentation contributors on the &lt;a href=&#34;https://groups.google.com/a/tensorflow.org/forum/#!forum/docs&#34;&gt;docs@tensorflow.org mailing list&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Community translations&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.tensorflow.org/community/contribute/docs#community_translations&#34;&gt;Community translations&lt;/a&gt; are located in the &lt;a href=&#34;https://github.com/tensorflow/docs-l10n&#34;&gt;tensorflow/docs-l10n&lt;/a&gt; repo. These docs are contributed, reviewed, and maintained by the community as &lt;em&gt;best-effort&lt;/em&gt;. To participate as a translator or reviewer, see the &lt;code&gt;site/&amp;lt;lang&amp;gt;/README.md&lt;/code&gt;, join the language mailing list, and submit a pull request.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/docs/master/LICENSE&#34;&gt;Apache License 2.0&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>innnky/emotional-vits</title>
    <updated>2023-03-19T01:58:58Z</updated>
    <id>tag:github.com,2023-03-19:/innnky/emotional-vits</id>
    <link href="https://github.com/innnky/emotional-vits" rel="alternate"></link>
    <summary type="html">&lt;p&gt;无需情感标注的情感可控语音合成模型，基于VITS&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Emotional VITS&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/spaces/innnky/nene-emotion&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;在线demo ↑↑↑ &lt;a href=&#34;https://www.bilibili.com/video/BV1Vg411h7of&#34;&gt;bilibili demo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;数据集无需任何情感标注，通过&lt;a href=&#34;https://github.com/audeering/w2v2-how-to&#34;&gt;情感提取模型&lt;/a&gt; 提取语句情感embedding输入网络，实现情感可控的VITS合成&lt;/p&gt; &#xA;&lt;h2&gt;模型结构&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;相对于原版VITS仅修改了TextEncoder部分 &lt;img src=&#34;https://raw.githubusercontent.com/innnky/emotional-vits/main/resources/out.png&#34; alt=&#34;image-20221029104949567&#34;&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;模型的优缺点介绍&lt;/h2&gt; &#xA;&lt;p&gt;该模型缺点：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;推理时需要指定一个音频作为情感的&lt;strong&gt;参考音频&lt;/strong&gt;才能够合成音频，而模型本身&lt;strong&gt;并不知道&lt;/strong&gt;“激动”、“平静”这类表示情绪的词语对应的情感特征是什么。&lt;/li&gt; &#xA; &lt;li&gt;对于只有一个角色的模型，可以通过&lt;strong&gt;预先筛选&lt;/strong&gt;的方式，即手动挑选几条“激动”、“平静”、“小声”之类的音频，手动实现情感文本-&amp;gt;情感embedding的对应关系 （这个过程可以用&lt;a href=&#34;https://raw.githubusercontent.com/innnky/emotional-vits/main/emotion_clustering.ipynb&#34;&gt;聚类算法&lt;/a&gt; 简化筛选）&lt;/li&gt; &#xA; &lt;li&gt;对于有&lt;strong&gt;多个角色&lt;/strong&gt;的模型，上述预筛选的方式有&lt;strong&gt;局限性&lt;/strong&gt;，因为例如同样对于“平静”这一个情感而言，不同角色对应的情感embedding可能会不同，导致建立情感文本-&amp;gt;情感embedding的映射关系很繁琐，很难通过一套统一的标准去描述不同角色之间的相似情感&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;该模型的优点：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;任何&lt;/strong&gt;普通的TTS数据集均可以完成情感控制。&lt;strong&gt;无需&lt;/strong&gt;手动打情感标签。&lt;/li&gt; &#xA; &lt;li&gt;由于在训练时候并没有指定情感的文本与embedding的对应关系，所有的情感特征embedding均在一个连续的空间内&lt;/li&gt; &#xA; &lt;li&gt;因此理论上对于任意角色数据集中出现的情感，推理时均可以通过该模型实现合成，只需要输入目标情感音频对应的embedding即可，而不会受到情感分类数量限制&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;快速挑选各个情感对应的音频&lt;/h2&gt; &#xA;&lt;p&gt;可以使用 &lt;strong&gt;聚类算法&lt;/strong&gt; 自动对音频的情感embedding进行分类，大致上可以区分出&lt;strong&gt;情感差异较大&lt;/strong&gt;的各个类别，具体使用请参考 &lt;a href=&#34;https://raw.githubusercontent.com/innnky/emotional-vits/main/emotion_clustering.ipynb&#34;&gt;emotion_clustering.ipynb&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Pre-requisites&lt;/h2&gt; &#xA;&lt;ol start=&#34;0&#34;&gt; &#xA; &lt;li&gt;Python &amp;gt;= 3.6&lt;/li&gt; &#xA; &lt;li&gt;Clone this repository&lt;/li&gt; &#xA; &lt;li&gt;Install python requirements. Please refer &lt;a href=&#34;https://raw.githubusercontent.com/innnky/emotional-vits/main/requirements.txt&#34;&gt;requirements.txt&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;prepare datasets&lt;/li&gt; &#xA; &lt;li&gt;Build Monotonic Alignment Search and run preprocessing if you use your own datasets.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Cython-version Monotonoic Alignment Search&#xA;cd monotonic_align&#xA;python setup.py build_ext --inplace&#xA;&#xA;# Preprocessing (g2p) for your own datasets. Preprocessed phonemes for nene have been already provided.&#xA;python preprocess.py --text_index 2 --filelists filelists/train.txt filelists/val.txt --text_cleaners japanese_cleaners&#xA;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;extract emotional embeddings, this will generate *.emo.npy for each wav file.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python emotion_extract.py --filelists filelists/train.txt filelists/val.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training Exmaple&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;&#xA;# nene&#xA;python train_ms.py -c configs/nene.json -m nene&#xA;&#xA;# if you are fine tuning pretrained original VITS checkpoint ,&#xA;python train_ms.py -c configs/nene.json -m nene --ckptD /path/to/D_xxxx.pth --ckptG /path/to/G_xxxx.pth&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Inference Example&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/innnky/emotional-vits/main/inference.ipynb&#34;&gt;inference.ipynb&lt;/a&gt; or use &lt;a href=&#34;https://github.com/CjangCjengh/MoeGoe&#34;&gt;MoeGoe&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>