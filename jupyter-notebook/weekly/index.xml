<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-28T02:02:26Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>GoogleCloudPlatform/generative-ai</title>
    <updated>2023-05-28T02:02:26Z</updated>
    <id>tag:github.com,2023-05-28:/GoogleCloudPlatform/generative-ai</id>
    <link href="https://github.com/GoogleCloudPlatform/generative-ai" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Sample code and notebooks for Generative AI on Google Cloud&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Generative AI&lt;/h1&gt; &#xA;&lt;p&gt;Welcome to the Google Cloud &lt;a href=&#34;https://cloud.google.com/ai/generative-ai&#34;&gt;Generative AI&lt;/a&gt; repository.&lt;/p&gt; &#xA;&lt;p&gt;This repository contains notebooks and content that demonstrate how to use, develop and manage generative AI workflows using &lt;a href=&#34;https://cloud.google.com/ai/generative-ai&#34;&gt;Generative AI&lt;/a&gt;, powered by &lt;a href=&#34;https://cloud.google.com/vertex-ai&#34;&gt;Vertex AI&lt;/a&gt; on Google Cloud.&lt;/p&gt; &#xA;&lt;h2&gt;Folder structure&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;generative-ai/&#xA;‚îú‚îÄ‚îÄ language/&#xA;|   ‚îî‚îÄ‚îÄ examples/             &#xA;|       ‚îú‚îÄ‚îÄ prompt-design/   - examples for prompts&#xA;|       ‚îî‚îÄ‚îÄ tuning/          - examples of tuning models&#xA;‚îî‚îÄ‚îÄ setup-env/               - setup instructions&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/language/&#34;&gt;Language/&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/language/intro_generative_ai_studio.md&#34;&gt;Getting Started with Generative AI Studio without code&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/language/intro_palm_api.ipynb&#34;&gt;Intro to Vertex AI PaLM API&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/language/intro_prompt_design.ipynb&#34;&gt;Intro to Prompt Design&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/language/examples/&#34;&gt;Examples/&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/language/examples/prompt-design/&#34;&gt;Prompt Design/&lt;/a&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/language/examples/prompt-design/ideation.ipynb&#34;&gt;Ideation&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/language/examples/prompt-design/question_answering.ipynb&#34;&gt;Question &amp;amp; Answering&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/language/examples/prompt-design/text_classification.ipynb&#34;&gt;Text Classifiction&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/language/examples/prompt-design/text_extraction.ipynb&#34;&gt;Text Extraction&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/language/examples/prompt-design/text_summarization.ipynb&#34;&gt;Text Summarization&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/language/examples/tuning/&#34;&gt;Tuning/&lt;/a&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/language/examples/tuning/getting_started_tuning.ipynb&#34;&gt;Tuning a Foundational Model, Deploying, and Making Predictions&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setting up your Google Cloud project&lt;/h2&gt; &#xA;&lt;p&gt;You will need a Google Cloud project to use this project.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://console.cloud.google.com/cloud-resource-manager&#34;&gt;Select or create a Google Cloud project&lt;/a&gt;. When you first create an account, you get a $300 free credit towards your compute/storage costs.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://cloud.google.com/billing/docs/how-to/modify-project&#34;&gt;Make sure that billing is enabled for your project&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com&#34;&gt;Enable the Vertex AI API&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Setting up your Python or Jupyter environment&lt;/h2&gt; &#xA;&lt;p&gt;Please see the README in the &lt;a href=&#34;https://github.com/GoogleCloudPlatform/generative-ai/tree/main/setup-env&#34;&gt;setup-env&lt;/a&gt; folder for information on using Colab notebooks and Vertex AI Workbench.&lt;/p&gt; &#xA;&lt;h2&gt;Google Generative AI Resources&lt;/h2&gt; &#xA;&lt;p&gt;Check out a list of &lt;a href=&#34;https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/RESOURCES.md&#34;&gt;Google Generative AI Resources&lt;/a&gt; like official product pages, documentation, videos, courses and more.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Contributions welcome! See the &lt;a href=&#34;https://github.com/GoogleCloudPlatform/generative-ai/raw/main/CONTRIBUTING.md&#34;&gt;Contributing Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Getting help&lt;/h2&gt; &#xA;&lt;p&gt;Please use the &lt;a href=&#34;https://github.com/GoogleCloudPlatform/generative-ai/issues&#34;&gt;issues page&lt;/a&gt; to provide feedback or submit a bug report.&lt;/p&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This repository itself is not an officially supported Google product. The code in this repository is for demonstrative purposes only.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>soyHenry/Python-Prep</title>
    <updated>2023-05-28T02:02:26Z</updated>
    <id>tag:github.com,2023-05-28:/soyHenry/Python-Prep</id>
    <link href="https://github.com/soyHenry/Python-Prep" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://d31uz8lwfmyn8g.cloudfront.net/Assets/logo-henry-white-lg.png&#34; alt=&#34;HenryLogo&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;&lt;strong&gt;üßë‚Äçüíª PREP COURSE | HENRY üë©‚Äçüíª&lt;/strong&gt;&lt;/h1&gt; &#xA;&lt;h2&gt;&lt;strong&gt;üìå INTRODUCCI√ìN&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;p&gt;¬°Hola üòÑ! Bienvenid@ al Prep Course para la carrera de Data Science&lt;/p&gt; &#xA;&lt;p&gt;En este curso introductorio podr√°s aprender y practicar todo el contenido que necesitas para alcanzar un nivel intermedio en Python, el lenguaje con el que vas a trabajar en la carrera. Encontrar√°s todo el material que necesitas para lograr avanzar en la evaluaci√≥n del Henry Challenge (√∫ltimo paso para ingresar a la carrera de Data).&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;&lt;strong&gt;üîé ¬øQU√â ES EL PREP COURSE?&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;p&gt;El Prep Course (curso preparatorio) es un curso dise√±ado con la finalidad de nivelar a todos nuestros aplicantes. Con este curso dar√°s tus primeros pasos en el mundo de la tecnolog√≠a y aprender√°s esos conceptos b√°sicos que son necesarios para poder realizar la carrera.&lt;/p&gt; &#xA;&lt;p&gt;El curso consiste en una serie de videos, material te√≥rico y ejercicios con los que podr√°s aprender nuestro lenguaje de desarrollo: Python. Puedes avanzar con el contenido a tu ritmo y de manera asincr√≥nica (es decir, en los horarios y tiempos que tu prefieras).Una vez que hayas terminado de estudiarlo, podr√°s continuar con el contenido de matem√°tica que se encuentra en: &lt;a href=&#34;https://raw.githubusercontent.com/soyHenry/Python-Prep/main/math.prep.soyhenry.com&#34;&gt;Prep de Matem√°tica&lt;/a&gt; Cuando hayas finalizado, podr√°s inscribirte para realizar el Henry Challenge.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;&lt;strong&gt;üìñ ¬øQU√â ES EL HENRY CHALLENGE?&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Es un examen en el que evaluamos los conceptos que se aprenden durante el Prep Course. El examen tiene la finalidad de asegurarnos que realmente has adquirido los conocimientos, dado que tenerlos bien claros es la clave del √©xito para que puedas avanzar sin problemas dentro de la carrera. El examen se realiza todos s√°bado por medio s√°bados, con previo registro e &lt;a href=&#34;https://admissions.soyhenry.com/&#34;&gt;inscripci√≥n&lt;/a&gt;. Podr√°s rendirlo hasta 3 veces. Tambien incluye una evaluaci√≥n de Matematica, pero solamente para conocer tu nivel.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;&lt;strong&gt;ü§® ¬øQU√â PUEDO HACER SI TENGO DUDAS?&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;SLACK:&lt;/strong&gt; es nuestra plataforma de comunicaci√≥n, donde podr√°s ponerte en contacto con nuestra comunidad que siempre te ayudar√° a resolver todas tus dudas. Encontrar√°s acceso a Slack desde la &lt;a href=&#34;https://www.admissions.soyhenry.com/&#34;&gt;plataforma de admisi√≥n&lt;/a&gt; o desde los mails que recibiste al momento de aplicar.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;&lt;strong&gt;üòã ¬øC√ìMO AVANZAR EN ESTE PROCESO?&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Para avanzar debes seguir el material te√≥rico junto con los videos de este curso introductorio. Para afianzar tus conocimientos y comenzar a practicar realiza cada una de estas &lt;strong&gt;&lt;em&gt;Homeworks&lt;/em&gt;&lt;/strong&gt;. Esto te facilitar√° resolver el Henry Challenge.&lt;/p&gt; &#xA;&lt;p&gt;Cualquier duda, nos puedes escribir a &lt;a href=&#34;mailto:admisiones@soyhenry.com&#34;&gt;admisiones@soyhenry.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;br&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/guidance</title>
    <updated>2023-05-28T02:02:26Z</updated>
    <id>tag:github.com,2023-05-28:/microsoft/guidance</id>
    <link href="https://github.com/microsoft/guidance" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A guidance language for controlling large language models.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;right&#34;&gt;&#xA; &lt;a href=&#34;https://guidance.readthedocs.org&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/guidance/badge/?version=latest&amp;amp;style=flat&#34;&gt;&lt;/a&gt;&#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;docs/figures/guidance_logo_blue_dark.svg&#34;&gt; &#xA;  &lt;img alt=&#34;guidance&#34; src=&#34;https://raw.githubusercontent.com/microsoft/guidance/main/docs/figures/guidance_logo_blue.svg?sanitize=true&#34; width=&#34;300&amp;quot;&#34;&gt; &#xA; &lt;/picture&gt;&#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;em&gt;Where there is no guidance, a model fails, but in an abundance of instructions there is safety.&lt;/em&gt;&lt;br&gt; &lt;em&gt;- &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/guidance/main/notebooks/proverb.ipynb&#34;&gt;GPT 11:14&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;!--It expands the API of language models so you can craft rich output structure, design precise tool use, create multi-agent interactions, and much more all while using clear code and maximum inference efficiency.--&gt; &#xA;&lt;p&gt;&lt;b&gt;Guidance&lt;/b&gt; enables you to control modern language models more effectively and efficiently than traditional prompting or chaining. Guidance programs allow you to interleave generation, prompting, and logical control into a single continuous flow matching how the language model actually processes the text. Simple output structures like &lt;a href=&#34;https://arxiv.org/abs/2201.11903&#34;&gt;Chain of Thought&lt;/a&gt; and its many variants (e.g., &lt;a href=&#34;https://arxiv.org/abs/2303.09014&#34;&gt;ART&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2210.03493&#34;&gt;Auto-CoT&lt;/a&gt;, etc.) have been shown to improve LLM performance. The advent of more powerful LLMs like &lt;a href=&#34;https://openai.com/research/gpt-4&#34;&gt;GPT-4&lt;/a&gt; allows for even richer structure, and &lt;code&gt;guidance&lt;/code&gt; makes that structure easier and cheaper.&lt;/p&gt; &#xA;&lt;p&gt;Features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Simple, intuitive syntax, based on &lt;a href=&#34;https://handlebarsjs.com/&#34;&gt;Handlebars&lt;/a&gt; templating.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Rich output structure with multiple generations, selections, conditionals, tool use, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Playground-like streaming in Jupyter/VSCode Notebooks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Smart seed-based generation caching.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support for role-based chat models (e.g., &lt;a href=&#34;https://beta.openai.com/docs/guides/chat&#34;&gt;ChatGPT&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Easy integration with Hugging Face models, including &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/guidance/main/notebooks/guidance_acceleration.ipynb&#34;&gt;guidance acceleration&lt;/a&gt; for speedups over standard prompting, &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/guidance/main/notebooks/token_healing.ipynb&#34;&gt;token healing&lt;/a&gt; to optimize prompt boundaries, and &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/guidance/main/notebooks/pattern_guides.ipynb&#34;&gt;regex pattern guides&lt;/a&gt; to enforce formats.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pip install guidance&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Live streaming (&lt;a href=&#34;https://github.com/microsoft/guidance/raw/main/notebooks/proverb.ipynb&#34;&gt;notebook&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;Speed up your prompt development cycle by streaming complex templates and generations live in your notebook. At first glance, Guidance feels like a templating language, and just like standard &lt;a href=&#34;https://handlebarsjs.com&#34;&gt;Handlebars&lt;/a&gt; templates, you can do variable interpolation (e.g., &lt;code&gt;{{proverb}}&lt;/code&gt;) and logical control. But unlike standard templating languages, guidance programs have a well defined linear execution order that directly corresponds to the token order as processed by the language model. This means that at any point during execution the language model can be used to generate text (using the &lt;code&gt;{{gen}}&lt;/code&gt; command) or make logical control flow decisions. This interleaving of generation and prompting allows for precise output structure that produces clear and parsable results.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import guidance&#xA;&#xA;# set the default language model used to execute guidance programs&#xA;guidance.llm = guidance.llms.OpenAI(&#34;text-davinci-003&#34;)&#xA;&#xA;# define a guidance program that adapts a proverb&#xA;program = guidance(&#34;&#34;&#34;Tweak this proverb to apply to model instructions instead.&#xA;&#xA;{{proverb}}&#xA;- {{book}} {{chapter}}:{{verse}}&#xA;&#xA;UPDATED&#xA;Where there is no guidance{{gen &#39;rewrite&#39; stop=&#34;\\n-&#34;}}&#xA;- GPT {{gen &#39;chapter&#39;}}:{{gen &#39;verse&#39;}}&#34;&#34;&#34;)&#xA;&#xA;# execute the program on a specific proverb&#xA;executed_program = program(&#xA;    proverb=&#34;Where there is no guidance, a people falls,\nbut in an abundance of counselors there is safety.&#34;,&#xA;    book=&#34;Proverbs&#34;,&#xA;    chapter=11,&#xA;    verse=14&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/guidance/main/docs/figures/proverb_animation.gif&#34; width=&#34;404&#34;&gt; &#xA;&lt;p&gt;After a program is executed, all the generated variables are now easily accessible:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;executed_program[&#34;rewrite&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&#39;, a model fails,\nbut in an abundance of instructions there is safety.&#39;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Chat dialog (&lt;a href=&#34;https://github.com/microsoft/guidance/raw/main/notebooks/chat.ipynb&#34;&gt;notebook&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;Guidance supports API-based chat models like GPT-4, as well as open chat models like Vicuna through a unified API based on role tags (e.g., &lt;code&gt;{{#system}}...{{/system}}&lt;/code&gt;). This allows interactive dialog development that combines rich templating and logical control with modern chat models.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# connect to a chat model like GPT-4 or Vicuna&#xA;gpt4 = guidance.llms.OpenAI(&#34;gpt-4&#34;)&#xA;# vicuna = guidance.llms.transformers.Vicuna(&#34;your_path/vicuna_13B&#34;, device_map=&#34;auto&#34;)&#xA;&#xA;experts = guidance(&#39;&#39;&#39;&#xA;{{#system~}}&#xA;You are a helpful and terse assistant.&#xA;{{~/system}}&#xA;&#xA;{{#user~}}&#xA;I want a response to the following question:&#xA;{{query}}&#xA;Name 3 world-class experts (past or present) who would be great at answering this?&#xA;Don&#39;t answer the question yet.&#xA;{{~/user}}&#xA;&#xA;{{#assistant~}}&#xA;{{gen &#39;expert_names&#39; temperature=0 max_tokens=300}}&#xA;{{~/assistant}}&#xA;&#xA;{{#user~}}&#xA;Great, now please answer the question as if these experts had collaborated in writing a joint anonymous answer.&#xA;{{~/user}}&#xA;&#xA;{{#assistant~}}&#xA;{{gen &#39;answer&#39; temperature=0 max_tokens=500}}&#xA;{{~/assistant}}&#xA;&#39;&#39;&#39;, llm=gpt4)&#xA;&#xA;experts(query=&#39;How can I be more productive?&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/guidance/main/docs/figures/chat_animation.gif&#34; width=&#34;619&#34;&gt; &#xA;&lt;h2&gt;Guidance acceleration (&lt;a href=&#34;https://github.com/microsoft/guidance/raw/main/notebooks/guidance_acceleration.ipynb&#34;&gt;notebook&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;When multiple generation or LLM-directed control flow statements are used in a single Guidance program then we can significantly improve inference performance by optimally reusing the Key/Value caches as we progress through the prompt. This means Guidance only asks the LLM to generate the green text below, not the entire program. &lt;strong&gt;This cuts this prompt&#39;s runtime in half vs. a standard generation approach.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# we use LLaMA here, but any GPT-style model will do&#xA;llama = guidance.llms.Transformers(&#34;your_path/llama-7b&#34;, device=0)&#xA;&#xA;# we can pre-define valid option sets&#xA;valid_weapons = [&#34;sword&#34;, &#34;axe&#34;, &#34;mace&#34;, &#34;spear&#34;, &#34;bow&#34;, &#34;crossbow&#34;]&#xA;&#xA;# define the prompt&#xA;character_maker = guidance(&#34;&#34;&#34;The following is a character profile for an RPG game in JSON format.&#xA;```json&#xA;{&#xA;    &#34;id&#34;: &#34;{{id}}&#34;,&#xA;    &#34;description&#34;: &#34;{{description}}&#34;,&#xA;    &#34;name&#34;: &#34;{{gen &#39;name&#39;}}&#34;,&#xA;    &#34;age&#34;: {{gen &#39;age&#39; pattern=&#39;[0-9]+&#39; stop=&#39;,&#39;}},&#xA;    &#34;armor&#34;: &#34;{{#select &#39;armor&#39;}}leather{{or}}chainmail{{or}}plate{{/select}}&#34;,&#xA;    &#34;weapon&#34;: &#34;{{select &#39;weapon&#39; options=valid_weapons}}&#34;,&#xA;    &#34;class&#34;: &#34;{{gen &#39;class&#39;}}&#34;,&#xA;    &#34;mantra&#34;: &#34;{{gen &#39;mantra&#39; temperature=0.7}}&#34;,&#xA;    &#34;strength&#34;: {{gen &#39;strength&#39; pattern=&#39;[0-9]+&#39; stop=&#39;,&#39;}},&#xA;    &#34;items&#34;: [{{#geneach &#39;items&#39; num_iterations=5 join=&#39;, &#39;}}&#34;{{gen &#39;this&#39; temperature=0.7}}&#34;{{/geneach}}]&#xA;}```&#34;&#34;&#34;)&#xA;&#xA;# generate a character&#xA;character_maker(&#xA;    id=&#34;e1f491f7-7ab8-4dac-8c20-c92b5e7d883d&#34;,&#xA;    description=&#34;A quick and nimble fighter.&#34;,&#xA;    valid_weapons=valid_weapons, llm=llama&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/guidance/main/docs/figures/json_animation.gif&#34; width=&#34;565&#34;&gt; &#xA;&lt;p&gt;The prompt above typically takes just over 2.5 seconds to complete on a A6000 GPU when using LLaMA 7B. If we were to run the same prompt adapted to be a single generation call (the standard practice today) it takes about 5 seconds to complete (4 of which is token generation and 1 of which is prompt processing). &lt;em&gt;This means Guidance acceleration delivers a 2x speedup over the standard approach for this prompt.&lt;/em&gt; In practice the exact speed-up factor depends on the format of your specific prompt and the size of your model (larger models benefit more). Acceleration is also only supported for Transformers LLMs at the moment. See the &lt;a href=&#34;https://github.com/microsoft/guidance/raw/main/notebooks/guidance_acceleration.ipynb&#34;&gt;notebook&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Token healing (&lt;a href=&#34;https://github.com/microsoft/guidance/raw/main/notebooks/art_of_prompt_design/prompt_boundaries_and_token_healing.ipynb&#34;&gt;notebook&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;The standard greedy tokenizations used by most language models introduce a subtle and powerful bias that can have all kinds of unintended consequences for your prompts. Using a process we call &#34;token healing&#34; &lt;code&gt;guidance&lt;/code&gt; automatically removes these surprising biases, freeing you to focus on designing the prompts you want without worrying about tokenization artifacts.&lt;/p&gt; &#xA;&lt;p&gt;Consider the following example, where we are trying to generate an HTTP URL string:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# we use StableLM as an open example, but these issues impact all models to varying degrees&#xA;guidance.llm = guidance.llms.Transformers(&#34;stabilityai/stablelm-base-alpha-3b&#34;, device=0)&#xA;&#xA;# we turn token healing off so that guidance acts like a normal prompting library&#xA;program = guidance(&#39;&#39;&#39;The link is &amp;lt;a href=&#34;http:{{gen max_tokens=10 token_healing=False}}&#39;&#39;&#39;)&#xA;program()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/guidance/main/docs/figures/url_with_space.png&#34; width=&#34;372&#34;&gt; &#xA;&lt;p&gt;Note that the output generated by the LLM does not complete the URL with the obvious next characters (two forward slashes). It instead creates an invalid URL string with a space in the middle. Why? Because the string &#34;://&#34; is its own token (&lt;code&gt;1358&lt;/code&gt;), and so once the model sees a colon by itself (token &lt;code&gt;27&lt;/code&gt;), it assumes that the next characters cannot be &#34;//&#34;; otherwise, the tokenizer would not have used &lt;code&gt;27&lt;/code&gt; and instead would have used &lt;code&gt;1358&lt;/code&gt; (the token for &#34;://&#34;).&lt;/p&gt; &#xA;&lt;p&gt;This bias is not just limited to the colon character -- it happens everywhere. &lt;em&gt;Over 70% of the 10k most common tokens for the StableLM model used above are prefixes of longer possible tokens, and so cause token boundary bias when they are the last token in a prompt.&lt;/em&gt; For example the &#34;:&#34; token &lt;code&gt;27&lt;/code&gt; has &lt;strong&gt;34&lt;/strong&gt; possible extensions, the &#34; the&#34; token &lt;code&gt;1735&lt;/code&gt; has &lt;strong&gt;51&lt;/strong&gt; extensions, and the &#34; &#34; (space) token &lt;code&gt;209&lt;/code&gt; has &lt;strong&gt;28,802&lt;/strong&gt; extensions).&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;guidance&lt;/code&gt; eliminates these biases by backing up the model by one token then allowing the model to step forward while constraining it to only generate tokens whose prefix matches the last token. This &#34;token healing&#34; process eliminates token boundary biases and allows any prompt to be completed naturally:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;guidance(&#39;The link is &amp;lt;a href=&#34;http:{{gen max_tokens=10}}&#39;)()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/guidance/main/docs/figures/url_without_space.png&#34; width=&#34;362&#34;&gt; &#xA;&lt;h2&gt;Rich output structure example (&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/guidance/main/notebooks/anachronism.ipynb&#34;&gt;notebook&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;To demonstrate the value of output structure, we take &lt;a href=&#34;https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/anachronisms&#34;&gt;a simple task&lt;/a&gt; from BigBench, where the goal is to identify whether a given sentence contains an anachronism (a statement that is impossible because of non-overlapping time periods). Below is a simple two-shot prompt for it, with a human-crafted chain-of-thought sequence.&lt;/p&gt; &#xA;&lt;p&gt;Guidance programs, like standard Handlebars templates, allow both variable interpolation (e.g., &lt;code&gt;{{input}}&lt;/code&gt;) and logical control. But unlike standard templating languages, guidance programs have a unique linear execution order that directly corresponds to the token order as processed by the language model. This means that at any point during execution the language model can be used to generate text (the &lt;code&gt;{{gen}}&lt;/code&gt; command) or make logical control flow decisions (the &lt;code&gt;{{#select}}...{{or}}...{{/select}}&lt;/code&gt; command). This interleaving of generation and prompting allows for precise output structure that improves accuracy while also producing clear and parsable results.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import guidance&#xA;                                                      &#xA;# set the default language model used to execute guidance programs&#xA;guidance.llm = guidance.llms.OpenAI(&#34;text-davinci-003&#34;) &#xA;&#xA;# define the few shot examples&#xA;examples = [&#xA;    {&#39;input&#39;: &#39;I wrote about shakespeare&#39;,&#xA;    &#39;entities&#39;: [{&#39;entity&#39;: &#39;I&#39;, &#39;time&#39;: &#39;present&#39;}, {&#39;entity&#39;: &#39;Shakespeare&#39;, &#39;time&#39;: &#39;16th century&#39;}],&#xA;    &#39;reasoning&#39;: &#39;I can write about Shakespeare because he lived in the past with respect to me.&#39;,&#xA;    &#39;answer&#39;: &#39;No&#39;},&#xA;    {&#39;input&#39;: &#39;Shakespeare wrote about me&#39;,&#xA;    &#39;entities&#39;: [{&#39;entity&#39;: &#39;Shakespeare&#39;, &#39;time&#39;: &#39;16th century&#39;}, {&#39;entity&#39;: &#39;I&#39;, &#39;time&#39;: &#39;present&#39;}],&#xA;    &#39;reasoning&#39;: &#39;Shakespeare cannot have written about me, because he died before I was born&#39;,&#xA;    &#39;answer&#39;: &#39;Yes&#39;}&#xA;]&#xA;&#xA;# define the guidance program&#xA;structure_program = guidance(&#xA;&#39;&#39;&#39;Given a sentence tell me whether it contains an anachronism (i.e. whether it could have happened or not based on the time periods associated with the entities).&#xA;----&#xA;&#xA;{{~! display the few-shot examples ~}}&#xA;{{~#each examples}}&#xA;Sentence: {{this.input}}&#xA;Entities and dates:{{#each this.entities}}&#xA;{{this.entity}}: {{this.time}}{{/each}}&#xA;Reasoning: {{this.reasoning}}&#xA;Anachronism: {{this.answer}}&#xA;---&#xA;{{~/each}}&#xA;&#xA;{{~! place the real question at the end }}&#xA;Sentence: {{input}}&#xA;Entities and dates:&#xA;{{gen &#34;entities&#34;}}&#xA;Reasoning:{{gen &#34;reasoning&#34;}}&#xA;Anachronism:{{#select &#34;answer&#34;}} Yes{{or}} No{{/select}}&#39;&#39;&#39;)&#xA;&#xA;# execute the program&#xA;out = structure_program(&#xA;    examples=examples,&#xA;    input=&#39;The T-rex bit my dog&#39;&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/guidance/main/docs/figures/anachronism.png&#34; width=&#34;837&#34;&gt; &#xA;&lt;p&gt;All of the generated program variables are now available in the executed program object:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;out[&#34;answer&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&#39; Yes&#39;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;We &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/guidance/main/notebooks/anachronism.ipynb&#34;&gt;compute accuracy&lt;/a&gt; on the validation set, and compare it to using the same two-shot examples above &lt;strong&gt;without&lt;/strong&gt; the output structure, as well as to the best reported result &lt;a href=&#34;https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/anachronisms&#34;&gt;here&lt;/a&gt;. The results below agree with existing literature, in that even a very simple output structure drastically improves performance, even compared against much larger models.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Accuracy&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/guidance/main/notebooks/anachronism.ipynb&#34;&gt;Few-shot learning with guidance examples, no CoT output structure&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.04%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/anachronisms&#34;&gt;PALM (3-shot)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Around 69%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/guidance/main/notebooks/anachronism.ipynb&#34;&gt;Guidance&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;76.01%&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Guaranteeing valid syntax JSON example (&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/guidance/main/notebooks/guaranteeing_valid_syntax.ipynb&#34;&gt;notebook&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;Large language models are great at generating useful outputs, but they are not great at guaranteeing that those outputs follow a specific format. This can cause problems when we want to use the outputs of a language model as input to another system. For example, if we want to use a language model to generate a JSON object, we need to make sure that the output is valid JSON. With &lt;code&gt;guidance&lt;/code&gt; we can both &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/guidance/main/notebooks/guidance_acceleration.ipynb&#34;&gt;accelerate inference speed&lt;/a&gt; and ensure that generated JSON is always valid. Below we generate a random character profile for a game with perfect syntax every time:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# load a model locally (we use LLaMA here)&#xA;guidance.llm = guidance.llms.Transformers(&#34;your_local_path/llama-7b&#34;, device=0)&#xA;&#xA;# we can pre-define valid option sets&#xA;valid_weapons = [&#34;sword&#34;, &#34;axe&#34;, &#34;mace&#34;, &#34;spear&#34;, &#34;bow&#34;, &#34;crossbow&#34;]&#xA;&#xA;# define the prompt&#xA;program = guidance(&#34;&#34;&#34;The following is a character profile for an RPG game in JSON format.&#xA;```json&#xA;{&#xA;    &#34;description&#34;: &#34;{{description}}&#34;,&#xA;    &#34;name&#34;: &#34;{{gen &#39;name&#39;}}&#34;,&#xA;    &#34;age&#34;: {{gen &#39;age&#39; pattern=&#39;[0-9]+&#39; stop=&#39;,&#39;}},&#xA;    &#34;armor&#34;: &#34;{{#select &#39;armor&#39;}}leather{{or}}chainmail{{or}}plate{{/select}}&#34;,&#xA;    &#34;weapon&#34;: &#34;{{select &#39;weapon&#39; options=valid_weapons}}&#34;,&#xA;    &#34;class&#34;: &#34;{{gen &#39;class&#39;}}&#34;,&#xA;    &#34;mantra&#34;: &#34;{{gen &#39;mantra&#39;}}&#34;,&#xA;    &#34;strength&#34;: {{gen &#39;strength&#39; pattern=&#39;[0-9]+&#39; stop=&#39;,&#39;}},&#xA;    &#34;items&#34;: [{{#geneach &#39;items&#39; num_iterations=3}}&#xA;        &#34;{{gen &#39;this&#39;}}&#34;,{{/geneach}}&#xA;    ]&#xA;}```&#34;&#34;&#34;)&#xA;&#xA;# execute the prompt&#xA;program(description=&#34;A quick and nimble fighter.&#34;, valid_weapons=valid_weapons)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/guidance/main/docs/figures/perfect_syntax.png&#34; width=&#34;657&#34;&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# and we also have a valid Python dictionary&#xA;out.variables()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/guidance/main/docs/figures/json_syntax_variables.png&#34; width=&#34;714&#34;&gt; &#xA;&lt;h2&gt;Role-based chat model example (&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/guidance/main/notebooks/chat.ipynb&#34;&gt;notebook&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;Modern chat-style models like ChatGPT and Alpaca are trained with special tokens that mark out &#34;roles&#34; for different areas of the prompt. Guidance supports these models through &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/guidance/main/notebooks/api_examples/library/role.ipynb&#34;&gt;role tags&lt;/a&gt; that automatically map to the correct tokens or API calls for the current LLM. Below we show how a role-based guidance program enables simple multi-step reasoning and planning.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import guidance&#xA;import re&#xA;&#xA;# we use GPT-4 here, but you could use gpt-3.5-turbo as well&#xA;guidance.llm = guidance.llms.OpenAI(&#34;gpt-4&#34;)&#xA;&#xA;# a custom function we will call in the guidance program&#xA;def parse_best(prosandcons, options):&#xA;    best = int(re.findall(r&#39;Best=(\d+)&#39;, prosandcons)[0])&#xA;    return options[best]&#xA;&#xA;# define the guidance program using role tags (like `{{#system}}...{{/system}}`)&#xA;create_plan = guidance(&#39;&#39;&#39;&#xA;{{#system~}}&#xA;You are a helpful assistant.&#xA;{{~/system}}&#xA;&#xA;{{! generate five potential ways to accomplish a goal }}&#xA;{{#block hidden=True}}&#xA;{{#user~}}&#xA;I want to {{goal}}.&#xA;{{~! generate potential options ~}}&#xA;Can you please generate one option for how to accomplish this?&#xA;Please make the option very short, at most one line.&#xA;{{~/user}}&#xA;&#xA;{{#assistant~}}&#xA;{{gen &#39;options&#39; n=5 temperature=1.0 max_tokens=500}}&#xA;{{~/assistant}}&#xA;{{/block}}&#xA;&#xA;{{! generate pros and cons for each option and select the best option }}&#xA;{{#block hidden=True}}&#xA;{{#user~}}&#xA;I want to {{goal}}.&#xA;&#xA;Can you please comment on the pros and cons of each of the following options, and then pick the best option?&#xA;---{{#each options}}&#xA;Option {{@index}}: {{this}}{{/each}}&#xA;---&#xA;Please discuss each option very briefly (one line for pros, one for cons), and end by saying Best=X, where X is the best option.&#xA;{{~/user}}&#xA;&#xA;{{#assistant~}}&#xA;{{gen &#39;prosandcons&#39; temperature=0.0 max_tokens=500}}&#xA;{{~/assistant}}&#xA;{{/block}}&#xA;&#xA;{{! generate a plan to accomplish the chosen option }}&#xA;{{#user~}}&#xA;I want to {{goal}}.&#xA;{{~! Create a plan }}&#xA;Here is my plan:&#xA;{{parse_best prosandcons options}}&#xA;Please elaborate on this plan, and tell me how to best accomplish it.&#xA;{{~/user}}&#xA;&#xA;{{#assistant~}}&#xA;{{gen &#39;plan&#39; max_tokens=500}}&#xA;{{~/assistant}}&#39;&#39;&#39;)&#xA;&#xA;# execute the program for a specific goal&#xA;out = create_plan(&#xA;    goal=&#39;read more books&#39;,&#xA;    parse_best=parse_best # a custom Python function we call in the program&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/guidance/main/docs/figures/chat_reading.png&#34; width=&#34;935&#34;&gt; &#xA;&lt;p&gt;This prompt/program is a bit more complicated, but we are basically going through 3 steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Generate a few options for how to accomplish the goal. Note that we generate with &lt;code&gt;n=5&lt;/code&gt;, such that each option is a separate generation (and is not impacted by the other options). We set &lt;code&gt;temperature=1&lt;/code&gt; to encourage diversity.&lt;/li&gt; &#xA; &lt;li&gt;Generate pros and cons for each option, and select the best one. We set &lt;code&gt;temperature=0&lt;/code&gt; to encourage the model to be more precise.&lt;/li&gt; &#xA; &lt;li&gt;Generate a plan for the best option, and ask the model to elaborate on it. Notice that steps 1 and 2 were &lt;code&gt;hidden&lt;/code&gt;, which means GPT-4 does not see them when generating content that comes later (in this case, that means when generating the plan). This is a simple way to make the model focus on the current step.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Since steps 1 and 2 are hidden, they do not appear on the generated output (except briefly during stream), but we can print the variables that these steps generated:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&#39;\n&#39;.join([&#39;Option %d: %s&#39; % (i, x) for i, x in enumerate(out[&#39;options&#39;])]))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Option 0: Set a goal to read for 20 minutes every day before bedtime.&lt;br&gt; Option 1: Join a book club for increased motivation and accountability.&lt;br&gt; Option 2: Set a daily goal to read for 20 minutes.&lt;br&gt; Option 3: Set a daily reminder to read for at least 20 minutes.&lt;br&gt; Option 4: Set a daily goal to read at least one chapter or 20 pages.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(out[&#39;prosandcons&#39;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Option 0:&lt;br&gt; Pros: Establishes a consistent reading routine.&lt;br&gt; Cons: May not be suitable for those with varying schedules.&lt;br&gt; ---&lt;br&gt; Option 1:&lt;br&gt; Pros: Provides social motivation and accountability.&lt;br&gt; Cons: May not align with personal reading preferences.&lt;br&gt; ---&lt;br&gt; Option 2:&lt;br&gt; Pros: Encourages daily reading habit.&lt;br&gt; Cons: Lacks a specific time frame, which may lead to procrastination.&lt;br&gt; ---&lt;br&gt; Option 3:&lt;br&gt; Pros: Acts as a daily reminder to prioritize reading.&lt;br&gt; Cons: May become repetitive and easy to ignore.&lt;br&gt; ---&lt;br&gt; Option 4:&lt;br&gt; Pros: Sets a clear daily reading target.&lt;br&gt; Cons: May be difficult to achieve on busy days or with longer chapters.&lt;/p&gt; &#xA; &lt;p&gt;Best=0&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Agents (&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/guidance/main/notebooks/chat.ipynb&#34;&gt;notebook&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;We can easily build agents that talk to each other or to a user, via the &lt;code&gt;await&lt;/code&gt; command. The &lt;code&gt;await&lt;/code&gt; command allows us to pause execution and return a partially executed guidance program. By putting &lt;code&gt;await&lt;/code&gt; in a loop, that partially executed program can then be called again and again to form a dialog (or any other structure you design). For example, here is how we might get GPT-4 to simulate two agents talking to one another:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import guidance&#xA;import re&#xA;guidance.llm = guidance.llms.OpenAI(&#34;gpt-4&#34;)&#xA;role_simulator = guidance(&#39;&#39;&#39;&#xA;{{#system~}}&#xA;You are a helpful assistant&#xA;{{~/system}}&#xA;                                           &#xA;{{#user~}}&#xA;You will answer the user as {{role}} in the following conversation. At every step, I will provide you with the user input, as well as a comment reminding you of your instructions. Never talk about the fact that you are an AI, even if the user asks you. Always answer as {{role}}.&#xA;{{#if first_question}}You can also start the conversation.{{/if}}&#xA;{{~/user}}&#xA;                                           &#xA;{{~! The assistant either starts the conversation or not, depending on if this is the first or second agent }}&#xA;{{#assistant~}}&#xA;Ok, I will follow these instructions.&#xA;{{#if first_question}}Let me start the conversation now:&#xA;{{role}}: {{first_question}}{{/if}}&#xA;{{~/assistant}}&#xA;&#xA;{{~! Then the conversation unrolls }}&#xA;{{~#geneach &#39;conversation&#39; stop=False}}&#xA;{{#user~}}&#xA;User: {{set &#39;this.input&#39; (await &#39;input&#39;)}}&#xA;Comment: Remember, answer as a {{role}}. Start your utterance with {{role}}:&#xA;{{~/user}}&#xA;&#xA;{{#assistant~}}&#xA;{{gen &#39;this.response&#39; temperature=0 max_tokens=300}}&#xA;{{~/assistant}}&#xA;{{~/geneach}}&#39;&#39;&#39;)&#xA;&#xA;republican = role_simulator(role=&#39;Republican&#39;, await_missing=True)&#xA;democrat = role_simulator(role=&#39;Democrat&#39;, await_missing=True)&#xA;&#xA;first_question = &#39;&#39;&#39;What do you think is the best way to stop inflation?&#39;&#39;&#39;&#xA;republican = republican(input=first_question, first_question=None)&#xA;democrat = democrat(input=republican[&#34;conversation&#34;][-2][&#34;response&#34;].strip(&#39;Republican: &#39;), first_question=first_question)&#xA;for i in range(2):&#xA;    republican = republican(input=democrat[&#34;conversation&#34;][-2][&#34;response&#34;].replace(&#39;Democrat: &#39;, &#39;&#39;))&#xA;    democrat = democrat(input=republican[&#34;conversation&#34;][-2][&#34;response&#34;].replace(&#39;Republican: &#39;, &#39;&#39;))&#xA;print(&#39;Democrat: &#39; + first_question)&#xA;for x in democrat[&#39;conversation&#39;][:-1]:&#xA;    print(&#39;Republican:&#39;, x[&#39;input&#39;])&#xA;    print()&#xA;    print(x[&#39;response&#39;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Democrat: What do you think is the best way to stop inflation?&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Republican: The best way to stop inflation is by implementing sound fiscal policies, such as reducing government spending, lowering taxes, and promoting economic growth. Additionally, the Federal Reserve should focus on maintaining a stable monetary policy to control inflation.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Democrat: I agree that sound fiscal policies are important in controlling inflation. As a Democrat, I would emphasize the importance of investing in education, healthcare, and infrastructure to promote long-term economic growth. Additionally, we should ensure that the Federal Reserve maintains a balanced approach to monetary policy, focusing on both controlling inflation and promoting full employment.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Republican: While investing in education, healthcare, and infrastructure is important, we must also prioritize reducing the national debt and limiting government intervention in the economy. By lowering taxes and reducing regulations, we can encourage businesses to grow and create jobs, which will ultimately lead to long-term economic growth. As for the Federal Reserve, it&#39;s crucial to maintain a stable monetary policy that primarily focuses on controlling inflation, as this will create a more predictable economic environment for businesses and consumers.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Democrat: While reducing the national debt and limiting government intervention are valid concerns, Democrats believe that strategic investments in education, healthcare, and infrastructure can lead to long-term economic growth and job creation. We also support a progressive tax system that ensures everyone pays their fair share, which can help fund these investments. As for the Federal Reserve, we believe that a balanced approach to monetary policy, focusing on both controlling inflation and promoting full employment, is essential for a healthy economy. We must strike a balance between fiscal responsibility and investing in our nation&#39;s future.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Republican: It&#39;s important to find a balance between fiscal responsibility and investing in our nation&#39;s future. However, we believe that the best way to achieve long-term economic growth and job creation is through free-market principles, such as lower taxes and reduced regulations. This approach encourages businesses to expand and innovate, leading to a more prosperous economy. A progressive tax system can sometimes discourage growth and investment, so we advocate for a simpler, fairer tax system that promotes economic growth. Regarding the Federal Reserve, while promoting full employment is important, we must not lose sight of the primary goal of controlling inflation to maintain a stable and predictable economic environment.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Democrat: I understand your perspective on free-market principles, but Democrats believe that a certain level of government intervention is necessary to ensure a fair and equitable economy. We support a progressive tax system to reduce income inequality and provide essential services to those in need. Additionally, we believe that regulations are important to protect consumers, workers, and the environment. As for the Federal Reserve, we agree that controlling inflation is crucial, but we also believe that promoting full employment should be a priority. By finding a balance between these goals, we can create a more inclusive and prosperous economy for all Americans.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;GPT4 + Bing&lt;/h2&gt; &#xA;&lt;p&gt;Last example &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/guidance/main/notebooks/chat.ipynb&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;API reference&lt;/h1&gt; &#xA;&lt;p&gt;All of the examples below are in &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/guidance/main/notebooks/tutorial.ipynb&#34;&gt;this notebook&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Template syntax&lt;/h2&gt; &#xA;&lt;p&gt;The template syntax is based on &lt;a href=&#34;https://handlebarsjs.com/&#34;&gt;Handlebars&lt;/a&gt;, with a few additions.&lt;br&gt; When &lt;code&gt;guidance&lt;/code&gt; is called, it returns a Program:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;prompt = guidance(&#39;&#39;&#39;What is {{example}}?&#39;&#39;&#39;)&#xA;prompt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;What is {{example}}?&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;The program can be executed by passing in arguments:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;prompt(example=&#39;truth&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;What is truth?&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Arguments can be iterables:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;people = [&#39;John&#39;, &#39;Mary&#39;, &#39;Bob&#39;, &#39;Alice&#39;]&#xA;ideas = [{&#39;name&#39;: &#39;truth&#39;, &#39;description&#39;: &#39;the state of being the case&#39;},&#xA;         {&#39;name&#39;: &#39;love&#39;, &#39;description&#39;: &#39;a strong feeling of affection&#39;},]&#xA;prompt = guidance(&#39;&#39;&#39;List of people:&#xA;{{#each people}}- {{this}}&#xA;{{~! This is a comment. The ~ removes adjacent whitespace either before or after a tag, depending on where you place it}}&#xA;{{/each~}}&#xA;List of ideas:&#xA;{{#each ideas}}{{this.name}}: {{this.description}}&#xA;{{/each}}&#39;&#39;&#39;)&#xA;prompt(people=people, ideas=ideas)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/guidance/main/docs/figures/template_objs.png&#34; alt=&#34;template_objects&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Notice the special &lt;code&gt;~&lt;/code&gt; character after &lt;code&gt;{{/each}}&lt;/code&gt;.&lt;br&gt; This can be added before or after any tag to remove all adjacent whitespace. Notice also the comment syntax: &lt;code&gt;{{! This is a comment }}&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can also include prompts/programs inside other prompts; e.g., here is how you could rewrite the prompt above:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;prompt1 = guidance(&#39;&#39;&#39;List of people:&#xA;{{#each people}}- {{this}}&#xA;{{/each~}}&#39;&#39;&#39;)&#xA;prompt2 = guidance(&#39;&#39;&#39;{{&amp;gt;prompt1}}&#xA;List of ideas:&#xA;{{#each ideas}}{{this.name}}: {{this.description}}&#xA;{{/each}}&#39;&#39;&#39;)&#xA;prompt2(prompt1=prompt1, people=people, ideas=ideas)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Generation&lt;/h2&gt; &#xA;&lt;h3&gt;Basic generation&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;code&gt;gen&lt;/code&gt; tag is used to generate text. You can use whatever arguments are supported by the underlying model. Executing a prompt calls the generation prompt:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import guidance&#xA;# Set the default llm. Could also pass a different one as argument to guidance(), with guidance(llm=...)&#xA;guidance.llm = guidance.llms.OpenAI(&#34;text-davinci-003&#34;)&#xA;prompt = guidance(&#39;&#39;&#39;The best thing about the beach is {{~gen &#39;best&#39; temperature=0.7 max_tokens=7}}&#39;&#39;&#39;)&#xA;prompt = prompt()&#xA;prompt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/guidance/main/docs/figures/generation1.png&#34; alt=&#34;generation1&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;guidance&lt;/code&gt; caches all OpenAI generations with the same arguments. If you want to flush the cache, you can call &lt;code&gt;guidance.llms.OpenAI.cache.clear()&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Selecting&lt;/h3&gt; &#xA;&lt;p&gt;You can select from a list of options using the &lt;code&gt;select&lt;/code&gt; tag:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;prompt = guidance(&#39;&#39;&#39;Is the following sentence offensive? Please answer with a single word, either &#34;Yes&#34;, &#34;No&#34;, or &#34;Maybe&#34;.&#xA;Sentence: {{example}}&#xA;Answer:{{#select &#34;answer&#34; logprobs=&#39;logprobs&#39;}} Yes{{or}} No{{or}} Maybe{{/select}}&#39;&#39;&#39;)&#xA;prompt = prompt(example=&#39;I hate tacos&#39;)&#xA;prompt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/guidance/main/docs/figures/select.png&#34; alt=&#34;select&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;prompt[&#39;logprobs&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;{&#39; Yes&#39;: -1.5689583, &#39; No&#39;: -7.332395, &#39; Maybe&#39;: -0.23746304}&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Sequences of generate/select&lt;/h3&gt; &#xA;&lt;p&gt;A prompt may contain multiple generations or selections, which will be executed in order:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;prompt = guidance(&#39;&#39;&#39;Generate a response to the following email:&#xA;{{email}}.&#xA;Response:{{gen &#34;response&#34;}}&#xA;&#xA;Is the response above offensive in any way? Please answer with a single word, either &#34;Yes&#34; or &#34;No&#34;.&#xA;Answer:{{#select &#34;answer&#34; logprobs=&#39;logprobs&#39;}} Yes{{or}} No{{/select}}&#39;&#39;&#39;)&#xA;prompt = prompt(email=&#39;I hate tacos&#39;)&#xA;prompt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/guidance/main/docs/figures/generate_select.png&#34; alt=&#34;generate_select&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;prompt[&#39;response&#39;], prompt[&#39;answer&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;(&#34; That&#39;s too bad! Tacos are one of my favorite meals.&#34;, &#39; No&#39;)&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Hidden generation&lt;/h3&gt; &#xA;&lt;p&gt;You can generate text without displaying it or using it in the subsequent generations using the &lt;code&gt;hidden&lt;/code&gt; tag, either in a &lt;code&gt;block&lt;/code&gt; or in a &lt;code&gt;gen&lt;/code&gt; tag:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;prompt = guidance(&#39;&#39;&#39;{{#block hidden=True}}Generate a response to the following email:&#xA;{{email}}.&#xA;Response:{{gen &#34;response&#34;}}{{/block}}&#xA;I will show you an email and a response, and you will tell me if it&#39;s offensive.&#xA;Email: {{email}}.&#xA;Response: {{response}}&#xA;Is the response above offensive in any way? Please answer with a single word, either &#34;Yes&#34; or &#34;No&#34;.&#xA;Answer:{{#select &#34;answer&#34; logprobs=&#39;logprobs&#39;}} Yes{{or}} No{{/select}}&#39;&#39;&#39;)&#xA;prompt = prompt(email=&#39;I hate tacos&#39;)&#xA;prompt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/guidance/main/docs/figures/hidden1.png&#34; alt=&#34;hidden1&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Notice that nothing inside the hidden block shows up in the output (or was used by the &lt;code&gt;select&lt;/code&gt;), even though we used the &lt;code&gt;response&lt;/code&gt; generated variable in the subsequent generation.&lt;/p&gt; &#xA;&lt;h3&gt;Generate with &lt;code&gt;n&amp;gt;1&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;If you use &lt;code&gt;n&amp;gt;1&lt;/code&gt;, the variable will contain a list (there is a visualization that lets you navigate the list, too):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;prompt = guidance(&#39;&#39;&#39;The best thing about the beach is {{~gen &#39;best&#39; n=3 temperature=0.7 max_tokens=7}}&#39;&#39;&#39;)&#xA;prompt = prompt()&#xA;prompt[&#39;best&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[&#39; that it is a great place to&#39;, &#39; being able to relax in the sun&#39;, &#34; that it&#39;s a great place to&#34;]&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Calling functions&lt;/h2&gt; &#xA;&lt;p&gt;You can call any Python function using generated variables as arguments. The function will be called when the prompt is executed:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def aggregate(best):&#xA;   return &#39;\n&#39;.join([&#39;- &#39; + x for x in best])&#xA;prompt = guidance(&#39;&#39;&#39;The best thing about the beach is {{~gen &#39;best&#39; n=3 temperature=0.7 max_tokens=7 hidden=True}}&#xA;{{aggregate best}}&#39;&#39;&#39;)&#xA;prompt = prompt(aggregate=aggregate)&#xA;prompt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/guidance/main/docs/figures/function.png&#34; alt=&#34;function&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Pausing execution with &lt;code&gt;await&lt;/code&gt;&lt;/h2&gt; &#xA;&lt;p&gt;An &lt;code&gt;await&lt;/code&gt; tag will stop program execution until that variable is provided:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;prompt = guidance(&#39;&#39;&#39;Generate a response to the following email:&#xA;{{email}}.&#xA;Response:{{gen &#34;response&#34;}}&#xA;{{await &#39;instruction&#39;}}&#xA;{{gen &#39;updated_response&#39;}}&#39;&#39;&#39;, stream=True)&#xA;prompt = prompt(email=&#39;Hello there&#39;)&#xA;prompt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/guidance/main/docs/figures/await1.png&#34; alt=&#34;await1&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Notice how the last &lt;code&gt;gen&lt;/code&gt; is not executed because it depends on &lt;code&gt;instruction&lt;/code&gt;. Let&#39;s provide &lt;code&gt;instruction&lt;/code&gt; now:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;prompt = prompt(instruction=&#39;Please translate the response above to Portuguese.&#39;)&#xA;prompt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/guidance/main/docs/figures/await2.png&#34; alt=&#34;await2&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The program is now executed all the way to the end.&lt;/p&gt; &#xA;&lt;h2&gt;Notebook functions&lt;/h2&gt; &#xA;&lt;p&gt;Echo, stream. TODO @SCOTT&lt;/p&gt; &#xA;&lt;h2&gt;Chat (see also &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/guidance/main/notebooks/chat.ipynb&#34;&gt;this notebook&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;If you use an OpenAI LLM that only allows for ChatCompletion (&lt;code&gt;gpt-3.5-turbo&lt;/code&gt; or &lt;code&gt;gpt-4&lt;/code&gt;), you can use the special tags &lt;code&gt;{{#system}}&lt;/code&gt;, &lt;code&gt;{{#user}}&lt;/code&gt;, and &lt;code&gt;{{#assistant}}&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;prompt = guidance(&#xA;&#39;&#39;&#39;{{#system~}}&#xA;You are a helpful assistant.&#xA;{{~/system}}&#xA;{{#user~}}&#xA;{{conversation_question}}&#xA;{{~/user}}&#xA;{{#assistant~}}&#xA;{{gen &#39;response&#39;}}&#xA;{{~/assistant}}&#39;&#39;&#39;)&#xA;prompt = prompt(conversation_question=&#39;What is the meaning of life?&#39;)&#xA;prompt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/guidance/main/docs/figures/chat1.png&#34; alt=&#34;chat1&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Since partial completions are not allowed, you can&#39;t really use output structure &lt;em&gt;inside&lt;/em&gt; an assistant block, but you can still set up a structure outside of it. Here is an example (also in &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/guidance/main/notebooks/chat.ipynb&#34;&gt;here&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;experts = guidance(&#xA;&#39;&#39;&#39;{{#system~}}&#xA;You are a helpful assistant.&#xA;{{~/system}}&#xA;{{#user~}}&#xA;I want a response to the following question:&#xA;{{query}}&#xA;Who are 3 world-class experts (past or present) who would be great at answering this?&#xA;Please don&#39;t answer the question or comment on it yet.&#xA;{{~/user}}&#xA;{{#assistant~}}&#xA;{{gen &#39;experts&#39; temperature=0 max_tokens=300}}&#xA;{{~/assistant}}&#xA;{{#user~}}&#xA;Great, now please answer the question as if these experts had collaborated in writing a joint anonymous answer.&#xA;In other words, their identity is not revealed, nor is the fact that there is a panel of experts answering the question.&#xA;If the experts would disagree, just present their different positions as alternatives in the answer itself (e.g., &#39;some might argue... others might argue...&#39;).&#xA;Please start your answer with ANSWER:&#xA;{{~/user}}&#xA;{{#assistant~}}&#xA;{{gen &#39;answer&#39; temperature=0 max_tokens=500}}&#xA;{{~/assistant}}&#39;&#39;&#39;)&#xA;experts(query=&#39;What is the meaning of life?&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can still use hidden blocks if you want to hide some of the conversation history for following generations:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;prompt = guidance(&#xA;&#39;&#39;&#39;{{#system~}}&#xA;You are a helpful assistant.&#xA;{{~/system}}&#xA;{{#block hidden=True~}}&#xA;{{#user~}}&#xA;Please tell me a joke&#xA;{{~/user}}&#xA;{{#assistant~}}&#xA;{{gen &#39;joke&#39;}}&#xA;{{~/assistant}}&#xA;{{~/block~}}&#xA;{{#user~}}&#xA;Is the following joke funny? Why or why not?&#xA;{{joke}}&#xA;{{~/user}}&#xA;{{#assistant~}}&#xA;{{gen &#39;funny&#39;}}&#xA;{{~/assistant}}&#39;&#39;&#39;)&#xA;prompt()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Agents with &lt;code&gt;geneach&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;You can combine the &lt;code&gt;await&lt;/code&gt; tag with &lt;code&gt;geneach&lt;/code&gt; (which generates a list) to create an agent easily:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;prompt = guidance(&#xA;&#39;&#39;&#39;{{#system~}}&#xA;You are a helpful assistant&#xA;{{~/system}}&#xA;{{~#geneach &#39;conversation&#39;}}&#xA;{{#user~}}&#xA;{{set &#39;this.user_text&#39; (await &#39;user_text&#39;)}}&#xA;{{~/user}}&#xA;{{#assistant~}}&#xA;{{gen &#39;this.ai_text&#39; temperature=0 max_tokens=300}}&#xA;{{~/assistant}}&#xA;{{~/geneach}}&#39;&#39;&#39;)&#xA;prompt= prompt(user_text =&#39;hi there&#39;)&#xA;prompt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Notice how the next iteration of the conversation is still templated, and how the conversation list has a placeholder as the last element:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;prompt[&#39;conversation&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[{&#39;user_text&#39;: &#39;hi there&#39;, &#39;ai_text&#39;: &#39;Hello! How can I help you today? If you have any questions or need assistance, feel free to ask.&#39;}, {}]&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;We can then execute the prompt again, and it will generate the next round:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;prompt = prompt(user_text = &#39;What is the meaning of life?&#39;)&#xA;prompt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See a more elaborate example &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/guidance/main/notebooks/chat.ipynb&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Using tools&lt;/h3&gt; &#xA;&lt;p&gt;See the &#39;Using a search API&#39; example in &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/guidance/main/notebooks/chat.ipynb&#34;&gt;this notebook&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>