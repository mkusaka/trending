<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-07-27T01:44:02Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>maxim5/cs229-2018-autumn</title>
    <updated>2025-07-27T01:44:02Z</updated>
    <id>tag:github.com,2025-07-27:/maxim5/cs229-2018-autumn</id>
    <link href="https://github.com/maxim5/cs229-2018-autumn" rel="alternate"></link>
    <summary type="html">&lt;p&gt;All notes and materials for the CS229: Machine Learning course by Stanford University&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CS229 Autumn 2018&lt;/h1&gt; &#xA;&lt;p&gt;All lecture notes, slides and assignments for &lt;a href=&#34;http://cs229.stanford.edu/&#34;&gt;CS229: Machine Learning&lt;/a&gt; course by Stanford University.&lt;/p&gt; &#xA;&lt;p&gt;The videos of all lectures are available &lt;a href=&#34;https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&#34;&gt;on YouTube&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Useful links:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/maxim5/cs229-2019-summer&#34;&gt;CS229 Summer 2019 edition&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>AllenDowney/ThinkPython</title>
    <updated>2025-07-27T01:44:02Z</updated>
    <id>tag:github.com,2025-07-27:/AllenDowney/ThinkPython</id>
    <link href="https://github.com/AllenDowney/ThinkPython" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Jupyter notebooks and other resources for Think Python by Allen Downey, published by O&#39;Reilly Media.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Think Python, 3rd edition&lt;/h1&gt; &#xA;&lt;p&gt;Jupyter notebooks and other material for the 3rd edition of &lt;em&gt;Think Python: How to Think Like a Computer Scientist&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;by Allen B. Downey&lt;/p&gt; &#xA;&lt;p&gt;You can order print and electronic versions of &lt;em&gt;Think Python 3e&lt;/em&gt; from &lt;a href=&#34;https://bookshop.org/a/98697/9781098155438&#34;&gt;Bookshop.org&lt;/a&gt; and &lt;a href=&#34;https://www.amazon.com/_/dp/1098155432?smid=ATVPDKIKX0DER&amp;amp;_encoding=UTF8&amp;amp;tag=oreilly20-20&amp;amp;_encoding=UTF8&amp;amp;tag=greenteapre01-20&amp;amp;linkCode=ur2&amp;amp;linkId=e2a529f94920295d27ec8a06e757dc7c&amp;amp;camp=1789&amp;amp;creative=9325&#34;&gt;Amazon&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The home page for the book is at &lt;a href=&#34;http://thinkpython.com&#34;&gt;Green Tea Press&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/OmniParser</title>
    <updated>2025-07-27T01:44:02Z</updated>
    <id>tag:github.com,2025-07-27:/microsoft/OmniParser</id>
    <link href="https://github.com/microsoft/OmniParser" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A simple screen parsing tool towards pure vision based GUI agent&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OmniParser: Screen Parsing tool for Pure Vision Based GUI Agent&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/OmniParser/master/imgs/logo.png&#34; alt=&#34;Logo&#34;&gt; &lt;/p&gt; &#xA;&lt;!-- &lt;a href=&#34;https://trendshift.io/repositories/12975&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://trendshift.io/api/badge/repositories/12975&#34; alt=&#34;microsoft%2FOmniParser | Trendshift&#34; style=&#34;width: 250px; height: 55px;&#34; width=&#34;250&#34; height=&#34;55&#34;/&gt;&lt;/a&gt; --&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2408.00203&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Paper-green&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;ðŸ“¢ [&lt;a href=&#34;https://microsoft.github.io/OmniParser/&#34;&gt;Project Page&lt;/a&gt;] [&lt;a href=&#34;https://www.microsoft.com/en-us/research/articles/omniparser-v2-turning-any-llm-into-a-computer-use-agent/&#34;&gt;V2 Blog Post&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/microsoft/OmniParser-v2.0&#34;&gt;Models V2&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/microsoft/OmniParser&#34;&gt;Models V1.5&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/spaces/microsoft/OmniParser-v2&#34;&gt;HuggingFace Space Demo&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;OmniParser&lt;/strong&gt; is a comprehensive method for parsing user interface screenshots into structured and easy-to-understand elements, which significantly enhances the ability of GPT-4V to generate actions that can be accurately grounded in the corresponding regions of the interface.&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2025/3] We support local logging of trajecotry so that you can use OmniParser+OmniTool to build training data pipeline for your favorate agent in your domain. [Documentation WIP]&lt;/li&gt; &#xA; &lt;li&gt;[2025/3] We are gradually adding multi agents orchstration and improving user interface in OmniTool for better experience.&lt;/li&gt; &#xA; &lt;li&gt;[2025/2] We release OmniParser V2 &lt;a href=&#34;https://huggingface.co/microsoft/OmniParser-v2.0&#34;&gt;checkpoints&lt;/a&gt;. &lt;a href=&#34;https://1drv.ms/v/c/650b027c18d5a573/EWXbVESKWo9Buu6OYCwg06wBeoM97C6EOTG6RjvWLEN1Qg?e=alnHGC&#34;&gt;Watch Video&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[2025/2] We introduce OmniTool: Control a Windows 11 VM with OmniParser + your vision model of choice. OmniTool supports out of the box the following large language models - OpenAI (4o/o1/o3-mini), DeepSeek (R1), Qwen (2.5VL) or Anthropic Computer Use. &lt;a href=&#34;https://1drv.ms/v/c/650b027c18d5a573/EehZ7RzY69ZHn-MeQHrnnR4BCj3by-cLLpUVlxMjF4O65Q?e=8LxMgX&#34;&gt;Watch Video&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[2025/1] V2 is coming. We achieve new state of the art results 39.5% on the new grounding benchmark &lt;a href=&#34;https://github.com/likaixin2000/ScreenSpot-Pro-GUI-Grounding/tree/main&#34;&gt;Screen Spot Pro&lt;/a&gt; with OmniParser v2 (will be released soon)! Read more details &lt;a href=&#34;https://github.com/microsoft/OmniParser/tree/master/docs/Evaluation.md&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2024/11] We release an updated version, OmniParser V1.5 which features 1) more fine grained/small icon detection, 2) prediction of whether each screen element is interactable or not. Examples in the demo.ipynb.&lt;/li&gt; &#xA; &lt;li&gt;[2024/10] OmniParser was the #1 trending model on huggingface model hub (starting 10/29/2024).&lt;/li&gt; &#xA; &lt;li&gt;[2024/10] Feel free to checkout our demo on &lt;a href=&#34;https://huggingface.co/spaces/microsoft/OmniParser&#34;&gt;huggingface space&lt;/a&gt;! (stay tuned for OmniParser + Claude Computer Use)&lt;/li&gt; &#xA; &lt;li&gt;[2024/10] Both Interactive Region Detection Model and Icon functional description model are released! &lt;a href=&#34;https://huggingface.co/microsoft/OmniParser&#34;&gt;Hugginface models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[2024/09] OmniParser achieves the best performance on &lt;a href=&#34;https://microsoft.github.io/WindowsAgentArena/&#34;&gt;Windows Agent Arena&lt;/a&gt;!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;First clone the repo, and then install environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cd OmniParser&#xA;conda create -n &#34;omni&#34; python==3.12&#xA;conda activate omni&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Ensure you have the V2 weights downloaded in weights folder (ensure caption weights folder is called icon_caption_florence). If not download them with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;   # download the model checkpoints to local directory OmniParser/weights/&#xA;   for f in icon_detect/{train_args.yaml,model.pt,model.yaml} icon_caption/{config.json,generation_config.json,model.safetensors}; do huggingface-cli download microsoft/OmniParser-v2.0 &#34;$f&#34; --local-dir weights; done&#xA;   mv weights/icon_caption weights/icon_caption_florence&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;!-- ## [deprecated]&#xA;Then download the model ckpts files in: https://huggingface.co/microsoft/OmniParser, and put them under weights/, default folder structure is: weights/icon_detect, weights/icon_caption_florence, weights/icon_caption_blip2. &#xA;&#xA;For v1: &#xA;convert the safetensor to .pt file. &#xA;```python&#xA;python weights/convert_safetensor_to_pt.py&#xA;&#xA;For v1.5: &#xA;download &#39;model_v1_5.pt&#39; from https://huggingface.co/microsoft/OmniParser/tree/main/icon_detect_v1_5, make a new dir: weights/icon_detect_v1_5, and put it inside the folder. No weight conversion is needed. &#xA;``` --&gt; &#xA;&lt;h2&gt;Examples:&lt;/h2&gt; &#xA;&lt;p&gt;We put together a few simple examples in the demo.ipynb.&lt;/p&gt; &#xA;&lt;h2&gt;Gradio Demo&lt;/h2&gt; &#xA;&lt;p&gt;To run gradio demo, simply run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python gradio_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Model Weights License&lt;/h2&gt; &#xA;&lt;p&gt;For the model checkpoints on huggingface model hub, please note that icon_detect model is under AGPL license since it is a license inherited from the original yolo model. And icon_caption_blip2 &amp;amp; icon_caption_florence is under MIT license. Please refer to the LICENSE file in the folder of each model: &lt;a href=&#34;https://huggingface.co/microsoft/OmniParser&#34;&gt;https://huggingface.co/microsoft/OmniParser&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;ðŸ“š Citation&lt;/h2&gt; &#xA;&lt;p&gt;Our technical report can be found &lt;a href=&#34;https://arxiv.org/abs/2408.00203&#34;&gt;here&lt;/a&gt;. If you find our work useful, please consider citing our work:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{lu2024omniparserpurevisionbased,&#xA;      title={OmniParser for Pure Vision Based GUI Agent}, &#xA;      author={Yadong Lu and Jianwei Yang and Yelong Shen and Ahmed Awadallah},&#xA;      year={2024},&#xA;      eprint={2408.00203},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV},&#xA;      url={https://arxiv.org/abs/2408.00203}, &#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>