<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-26T01:56:12Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>gkamradt/langchain-tutorials</title>
    <updated>2023-03-26T01:56:12Z</updated>
    <id>tag:github.com,2023-03-26:/gkamradt/langchain-tutorials</id>
    <link href="https://github.com/gkamradt/langchain-tutorials" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Overview and tutorial of the LangChain Library&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LangChain Tutorials&lt;/h1&gt; &#xA;&lt;p&gt;Overview and tutorial of the &lt;a href=&#34;https://langchain.readthedocs.io/en/latest/&#34;&gt;LangChain library&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;See the accompanying YouTube tutorials @ &lt;a href=&#34;https://www.youtube.com/channel/UCyR2Ct3pDOeZSRyZH5hPO-Q&#34;&gt;https://www.youtube.com/channel/UCyR2Ct3pDOeZSRyZH5hPO-Q&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to get updated when new tutorials are out, sign up at &lt;a href=&#34;https://raw.githubusercontent.com/gkamradt/langchain-tutorials/main/www.dataindependent.com&#34;&gt;DataIndependent&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This series is provided by and associated with &lt;a href=&#34;https://raw.githubusercontent.com/gkamradt/langchain-tutorials/main/www.dataindependent.com&#34;&gt;DataIndependent&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>tloen/alpaca-lora</title>
    <updated>2023-03-26T01:56:12Z</updated>
    <id>tag:github.com,2023-03-26:/tloen/alpaca-lora</id>
    <link href="https://github.com/tloen/alpaca-lora" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Instruct-tune LLaMA on consumer hardware&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;ü¶ôüå≤ü§è Alpaca-LoRA: Low-Rank LLaMA Instruct-Tuning&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ü§ó &lt;strong&gt;Try the pretrained model out &lt;a href=&#34;https://huggingface.co/spaces/tloen/alpaca-lora&#34;&gt;here&lt;/a&gt;, courtesy of a GPU grant from Huggingface!&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Users have created a Discord server for discussion and support &lt;a href=&#34;https://discord.gg/prbq284xX5&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This repository contains code for reproducing the &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca&lt;/a&gt; results using &lt;a href=&#34;https://arxiv.org/pdf/2106.09685.pdf&#34;&gt;low-rank adaptation (LoRA)&lt;/a&gt;. We provide an Instruct model of similar quality to &lt;code&gt;text-davinci-003&lt;/code&gt; that can run &lt;a href=&#34;https://twitter.com/miolini/status/1634982361757790209&#34;&gt;on a Raspberry Pi&lt;/a&gt; (for research), and the code is easily extended to the &lt;code&gt;13b&lt;/code&gt;, &lt;code&gt;30b&lt;/code&gt;, and &lt;code&gt;65b&lt;/code&gt; models.&lt;/p&gt; &#xA;&lt;p&gt;In addition to the training code, which runs within five hours on a single RTX 4090, we publish a script for downloading and inference on the foundation model and LoRA, as well as the resulting &lt;a href=&#34;https://huggingface.co/tloen/alpaca-lora-7b/tree/main&#34;&gt;LoRA weights themselves&lt;/a&gt;. To fine-tune cheaply and efficiently, we use Hugging Face&#39;s &lt;a href=&#34;https://github.com/huggingface/peft&#34;&gt;PEFT&lt;/a&gt; as well as Tim Dettmers&#39; &lt;a href=&#34;https://github.com/TimDettmers/bitsandbytes&#34;&gt;bitsandbytes&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Without hyperparameter tuning, the LoRA model produces outputs comparable to the Stanford Alpaca model. (Please see the outputs included below.) Further tuning might be able to achieve better performance; I invite interested users to give it a try and report their results.&lt;/p&gt; &#xA;&lt;h3&gt;Setup&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install dependencies&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;If bitsandbytes doesn&#39;t work, &lt;a href=&#34;https://github.com/TimDettmers/bitsandbytes/raw/main/compile_from_source.md&#34;&gt;install it from source.&lt;/a&gt; Windows users can follow &lt;a href=&#34;https://github.com/tloen/alpaca-lora/issues/17&#34;&gt;these instructions&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Training (&lt;code&gt;finetune.py&lt;/code&gt;)&lt;/h3&gt; &#xA;&lt;p&gt;This file contains a straightforward application of PEFT to the LLaMA model, as well as some code related to prompt construction and tokenization. PRs adapting this code to support larger models are always welcome.&lt;/p&gt; &#xA;&lt;p&gt;Example usage:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python finetune.py \&#xA;    --base_model &#39;decapoda-research/llama-7b-hf&#39; \&#xA;    --data_path &#39;./alpaca_data_cleaned.json&#39; \&#xA;    --output_dir &#39;./lora-alpaca&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We can also tweak our hyperparameters:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python finetune.py \&#xA;    --base_model &#39;decapoda-research/llama-7b-hf&#39; \&#xA;    --data_path &#39;./alpaca_data_cleaned.json&#39; \&#xA;    --output_dir &#39;./lora-alpaca&#39; \&#xA;    --batch_size 128 \&#xA;    --micro_batch_size 4 \&#xA;    --num_epochs 3 \&#xA;    --learning_rate 1e-4 \&#xA;    --cutoff_len 512 \&#xA;    --val_set_size 2000 \&#xA;    --lora_r 8 \&#xA;    --lora_alpha 16 \&#xA;    --lora_dropout 0.05 \&#xA;    --lora_target_modules &#39;[q_proj,v_proj]&#39; \&#xA;    --train_on_inputs \&#xA;    --group_by_length&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Inference (&lt;code&gt;generate.py&lt;/code&gt;)&lt;/h3&gt; &#xA;&lt;p&gt;This file reads the foundation model from the Hugging Face model hub and the LoRA weights from &lt;code&gt;tloen/alpaca-lora-7b&lt;/code&gt;, and runs a Gradio interface for inference on a specified input. Users should treat this as example code for the use of the model, and modify it as needed.&lt;/p&gt; &#xA;&lt;p&gt;Example usage:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python generate.py \&#xA;    --load_8bit \&#xA;    --base_model &#39;decapoda-research/llama-7b-hf&#39; \&#xA;    --lora_weights &#39;tloen/alpaca-lora-7b&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Checkpoint export (&lt;code&gt;export_*_checkpoint.py&lt;/code&gt;)&lt;/h3&gt; &#xA;&lt;p&gt;These files contain scripts that merge the LoRA weights back into the base model for export to Hugging Face format and to PyTorch &lt;code&gt;state_dicts&lt;/code&gt;. They should help users who want to run inference in projects like &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; or &lt;a href=&#34;https://github.com/antimatter15/alpaca.cpp&#34;&gt;alpaca.cpp&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Dataset&lt;/h3&gt; &#xA;&lt;p&gt;In addition to &lt;code&gt;alpaca_data.json&lt;/code&gt;, which contains the original Stanford Alpaca dataset, we also include &lt;code&gt;alpaca_data_cleaned.json&lt;/code&gt;, which has been &lt;a href=&#34;https://github.com/tloen/alpaca-lora/pull/32&#34;&gt;stripped of various tokenization artifacts&lt;/a&gt; with the help of @gururise. This file is now used by default in the training script.&lt;/p&gt; &#xA;&lt;p&gt;@AndriyMulyar has also provided interactive, embedding-based visualizations of the original dataset&#39;s &lt;a href=&#34;https://atlas.nomic.ai/map/alpaca_instructions&#34;&gt;instructions&lt;/a&gt; and &lt;a href=&#34;https://atlas.nomic.ai/map/alpaca_outputs&#34;&gt;outputs&lt;/a&gt;, as well as &lt;a href=&#34;https://atlas.nomic.ai/map/d2139cc3-bc1c-441c-8d6f-3e6ffbbc2eda/838019ff-8fe2-42ba-809a-d86d2b98cd50/-18.11668742841587/-11.348087116836096/-20.88850316347706/-17.680468640801223/774455612&#34;&gt;clusters of bad examples&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Notes&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We can likely improve our model performance significantly if we had a better dataset. Consider supporting the &lt;a href=&#34;https://open-assistant.io/&#34;&gt;LAION Open Assistant&lt;/a&gt; effort to produce a high-quality dataset for supervised fine-tuning (or bugging them to release their data).&lt;/li&gt; &#xA; &lt;li&gt;We&#39;re continually fixing bugs and conducting training runs, and the weights on the Hugging Face Hub are being updated accordingly. In particular, those facing issues with response lengths should make sure that they have the latest version of the weights and code.&lt;/li&gt; &#xA; &lt;li&gt;Users with multiple GPUs should take a look &lt;a href=&#34;https://github.com/tloen/alpaca-lora/issues/8#issuecomment-1477490259&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Resources&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/antimatter15/alpaca.cpp&#34;&gt;alpaca.cpp&lt;/a&gt;, a native client for running Alpaca models on the CPU&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/deep-diver/Alpaca-LoRA-Serve&#34;&gt;Alpaca-LoRA-Serve&lt;/a&gt;, a ChatGPT-style interface for Alpaca models&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/gururise/AlpacaDataCleaned&#34;&gt;AlpacaDataCleaned&lt;/a&gt;, a project to improve the quality of the Alpaca dataset&lt;/li&gt; &#xA; &lt;li&gt;Various adapter weights (download at own risk): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;7B: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/tloen/alpaca-lora-7b&#34;&gt;https://huggingface.co/tloen/alpaca-lora-7b&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/samwit/alpaca7B-lora&#34;&gt;https://huggingface.co/samwit/alpaca7B-lora&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;üáßüá∑ &lt;a href=&#34;https://huggingface.co/22h/cabrita-lora-v0-1&#34;&gt;https://huggingface.co/22h/cabrita-lora-v0-1&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;üá®üá≥ &lt;a href=&#34;https://huggingface.co/qychen/luotuo-lora-7b-0.1&#34;&gt;https://huggingface.co/qychen/luotuo-lora-7b-0.1&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;üáØüáµ &lt;a href=&#34;https://huggingface.co/kunishou/Japanese-Alapaca-LoRA-7b-v0&#34;&gt;https://huggingface.co/kunishou/Japanese-Alapaca-LoRA-7b-v0&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;üá´üá∑ &lt;a href=&#34;https://huggingface.co/bofenghuang/vigogne-lora-7b&#34;&gt;https://huggingface.co/bofenghuang/vigogne-lora-7b&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;üáπüá≠ &lt;a href=&#34;https://huggingface.co/Thaweewat/thai-buffala-lora-7b-v0-1&#34;&gt;https://huggingface.co/Thaweewat/thai-buffala-lora-7b-v0-1&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;üá©üá™ &lt;a href=&#34;https://huggingface.co/thisserand/alpaca_lora_german&#34;&gt;https://huggingface.co/thisserand/alpaca_lora_german&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;üáÆüáπ &lt;a href=&#34;https://huggingface.co/teelinsan/camoscio-7b-llama&#34;&gt;https://huggingface.co/teelinsan/camoscio-7b-llama&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;13B: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/chansung/alpaca-lora-13b&#34;&gt;https://huggingface.co/chansung/alpaca-lora-13b&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/mattreid/alpaca-lora-13b&#34;&gt;https://huggingface.co/mattreid/alpaca-lora-13b&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/samwit/alpaca13B-lora&#34;&gt;https://huggingface.co/samwit/alpaca13B-lora&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;üáØüáµ &lt;a href=&#34;https://huggingface.co/kunishou/Japanese-Alapaca-LoRA-13b-v0&#34;&gt;https://huggingface.co/kunishou/Japanese-Alapaca-LoRA-13b-v0&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;üá∞üá∑ &lt;a href=&#34;https://huggingface.co/chansung/koalpaca-lora-13b&#34;&gt;https://huggingface.co/chansung/koalpaca-lora-13b&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;30B: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/baseten/alpaca-30b&#34;&gt;https://huggingface.co/baseten/alpaca-30b&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/chansung/alpaca-lora-30b&#34;&gt;https://huggingface.co/chansung/alpaca-lora-30b&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;üáØüáµ &lt;a href=&#34;https://huggingface.co/kunishou/Japanese-Alapaca-LoRA-30b-v0&#34;&gt;https://huggingface.co/kunishou/Japanese-Alapaca-LoRA-30b-v0&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/chavinlo/alpaca-native&#34;&gt;alpaca-native&lt;/a&gt;, a replication using the original Alpaca code&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Example outputs&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Instruction&lt;/strong&gt;: Tell me about alpacas.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Alpaca-LoRA&lt;/strong&gt;: Alpacas are members of the camelid family and are native to the Andes Mountains of South America. They are known for their soft, luxurious fleece, which is used to make clothing, blankets, and other items. Alpacas are herbivores and graze on grasses and other plants. They are social animals and live in herds of up to 20 individuals.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stanford Alpaca&lt;/strong&gt;: Alpacas are small, fluffy animals related to camels and llamas. They are native to Peru and Bolivia, and were first domesticated around 5,000 years ago. They are kept mainly for their fine, soft fleece, which is used to make knitwear and other garments. Alpacas are herd animals and live in small family groups, led by an older male. They are highly social animals and can form strong bonds with their owners.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;text-davinci-003&lt;/strong&gt;: Alpacas are a domesticated species of South American camelid, similar to llamas. They are native to the Andes Mountains in South America and are kept as livestock for their soft, luxurious wool. Alpacas are social animals and live in herds of up to 20 individuals. They are typically kept in small herds of two to five animals, and are relatively easy to care for. Alpacas are herbivores and feed on grass, hay, and other vegetation. They are also known for their gentle and friendly nature, making them popular as pets.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Instruction&lt;/strong&gt;: Tell me about the president of Mexico in 2019.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Alpaca-LoRA&lt;/strong&gt;: The president of Mexico in 2019 was Andr√©s Manuel L√≥pez Obrador, who took office on December 1, 2018. He is a member of the National Regeneration Movement (MORENA) political party and is the first left-wing president of Mexico since 1946. He is known for his anti-corruption and anti-neoliberal policies, as well as his commitment to improving the living conditions of the Mexican people.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stanford Alpaca&lt;/strong&gt;: In 2019, Andres Manuel Lopez Obrador was sworn in as President of Mexico. He is the country‚Äôs 58th president and the first from the left-wing Morena party. He was elected in 2018 on a promise to combat corruption and reduce poverty and has faced criticism for his handling of the economy, crime and the country‚Äôs relationship with the United States. He is also known for his ambitious social welfare programs and has been praised for raising the minimum wage and providing aid to low-income families. He is seen as a champion of democracy and human rights in Mexico.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;text-davinci-003&lt;/strong&gt;: The president of Mexico in 2019 is Andr√©s Manuel L√≥pez Obrador, commonly known as AMLO. He is a Mexican politician who has been the President of Mexico since December 1, 2018. He is a member of the National Regeneration Movement (MORENA) party and previously served as the Head of Government of Mexico City from 2000 to 2005. He is a left-wing populist and has promised to reduce corruption and inequality in Mexico. He has also promised to increase public spending on social programs and infrastructure projects.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Instruction&lt;/strong&gt;: Tell me about the king of France in 2019.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Alpaca-LoRA&lt;/strong&gt;: The King of France in 2019 is Emmanuel Macron, who was elected in May 2017. He is the youngest president in the history of the Fifth Republic and the first president to be born after World War II. He is also the first president to have never held elected office before.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stanford Alpaca&lt;/strong&gt;: In 2019, the King of France was Emmanuel Macron. He was sworn in as President of France on May 14, 2017, and has been leading the country ever since. He is a member of the centrist La R√©publique En Marche! Party. He is known for his reformist agenda and has faced a number of protests from both the right and left of the political spectrum. He is the youngest French president in history.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;text-davinci-003&lt;/strong&gt;: The King of France is a largely symbolic role, as France is a republic. The current President of France is Emmanuel Macron, who was elected in 2017.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Instruction&lt;/strong&gt;: List all Canadian provinces in alphabetical order.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Alpaca-LoRA&lt;/strong&gt;: Alberta, British Columbia, Manitoba, New Brunswick, Newfoundland and Labrador, Nova Scotia, Ontario, Prince Edward Island, Quebec, Saskatchewan&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stanford Alpaca&lt;/strong&gt;: Alberta, British Columbia, Manitoba, New Brunswick, Newfoundland and Labrador, Nova Scotia, Ontario, Prince Edward Island, Quebec, Saskatchewan, Northwest Territories, Nunavut, Yukon.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;text-davinci-003&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Alberta&lt;/li&gt; &#xA; &lt;li&gt;British Columbia&lt;/li&gt; &#xA; &lt;li&gt;Manitoba&lt;/li&gt; &#xA; &lt;li&gt;New Brunswick&lt;/li&gt; &#xA; &lt;li&gt;Newfoundland and Labrador&lt;/li&gt; &#xA; &lt;li&gt;Northwest Territories&lt;/li&gt; &#xA; &lt;li&gt;Nova Scotia&lt;/li&gt; &#xA; &lt;li&gt;Nunavut&lt;/li&gt; &#xA; &lt;li&gt;Ontario&lt;/li&gt; &#xA; &lt;li&gt;Prince Edward Island&lt;/li&gt; &#xA; &lt;li&gt;Quebec&lt;/li&gt; &#xA; &lt;li&gt;Saskatchewan&lt;/li&gt; &#xA; &lt;li&gt;Yukon&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Instruction&lt;/strong&gt;: Write a Python program that prints the first 10 Fibonacci numbers.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Alpaca-LoRA&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def fibonacci(n):&#xA;    if n == 0:&#xA;        return 0&#xA;    elif n == 1:&#xA;        return 1&#xA;    else:&#xA;        return fibonacci(n-1) + fibonacci(n-2)&#xA;&#xA;print(fibonacci(10))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stanford Alpaca&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;numbers = [0, 1]&#xA;&#xA;for i in range(2, 11):&#xA;    numbers.append(numbers[i-2] + numbers[i-1])&#xA;&#xA;print(numbers)&#xA;# Output: [0, 1, 1, 2, 3, 5, 8, 13, 21, 34]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;text-davinci-003&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Fibonacci numbers are the numbers in the following integer sequence, called the Fibonacci sequence, and characterized by the fact that every number after the first two is the sum of the two preceding ones.&#xA;&#xA;# Fibonacci sequence: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, ...&#xA;&#xA;# Program:&#xA;&#xA;# Initialize the first two numbers of the Fibonacci sequence&#xA;a = 0&#xA;b = 1&#xA;&#xA;# Print the first 10 numbers of the Fibonacci sequence&#xA;print(a)&#xA;print(b)&#xA;&#xA;for i in range(2,10):&#xA;    c = a + b&#xA;    a = b&#xA;    b = c&#xA;    print(c)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Instruction&lt;/strong&gt;: Write a program that prints the numbers from 1 to 100. But for multiples of three print &#39;Fizz&#39; instead of the number and for the multiples of five print &#39;Buzz&#39;. For numbers which are multiples of both three and five print &#39;FizzBuzz&#39;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Alpaca-LoRA&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i in range(1, 101):&#xA;    if i % 3 == 0 and i % 5 == 0:&#xA;        print(&#34;FizzBuzz&#34;)&#xA;    elif i % 3 == 0:&#xA;        print(&#34;Fizz&#34;)&#xA;    elif i % 5 == 0:&#xA;        print(&#34;Buzz&#34;)&#xA;    else:&#xA;        print(i)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stanford Alpaca&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i in range(1, 101):&#xA;    if i % 3 == 0 and i % 5 == 0:&#xA;        print(&#39;FizzBuzz&#39;)&#xA;    elif i % 3 == 0:&#xA;        print(&#39;Fizz&#39;)&#xA;    elif i % 5 == 0:&#xA;        print(&#39;Buzz&#39;)&#xA;    else:&#xA;        print(i)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;text-davinci-003&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;for (let i = 1; i &amp;lt;= 100; i++) {&#xA;  if (i % 3 === 0 &amp;amp;&amp;amp; i % 5 === 0) {&#xA;    console.log(&#34;FizzBuzz&#34;);&#xA;  } else if (i % 3 === 0) {&#xA;    console.log(&#34;Fizz&#34;);&#xA;  } else if (i % 5 === 0) {&#xA;    console.log(&#34;Buzz&#34;);&#xA;  } else {&#xA;    console.log(i);&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Instruction&lt;/strong&gt;: Tell me five words that rhyme with &#39;shock&#39;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Alpaca-LoRA&lt;/strong&gt;: Flock, lock, rock, stock, and sock.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stanford Alpaca&lt;/strong&gt;: Five words that rhyme with shock are: rock, pop, shock, cook, and snock.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;text-davinci-003&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Block&lt;/li&gt; &#xA; &lt;li&gt;Lock&lt;/li&gt; &#xA; &lt;li&gt;Dock&lt;/li&gt; &#xA; &lt;li&gt;Clock&lt;/li&gt; &#xA; &lt;li&gt;Rock&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Instruction&lt;/strong&gt;: Translate the sentence &#39;I have no mouth but I must scream&#39; into Spanish.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Alpaca-LoRA&lt;/strong&gt;: No tengo boca pero tengo que gritar.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stanford Alpaca&lt;/strong&gt;: No tengo boca, pero debo gritar.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;text-davinci-003&lt;/strong&gt;: No tengo boca pero debo gritar.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>huawei-noah/HEBO</title>
    <updated>2023-03-26T01:56:12Z</updated>
    <id>tag:github.com,2023-03-26:/huawei-noah/HEBO</id>
    <link href="https://github.com/huawei-noah/HEBO" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Bayesian optimisation library developped by Huawei Noah&#39;s Ark Lab&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Bayesian Optimisation &amp;amp; Reinforcement Learning Research&lt;/h1&gt; &#xA;&lt;p&gt;This directory contains official implementations for Bayesian optimisation &amp;amp; Reinforcement Learning works developped by Huawei, Noah&#39;s Ark Lab.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bayesian Optimisation Research &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huawei-noah/HEBO/master/HEBO&#34;&gt;HEBO: Heteroscedastic Evolutionary Bayesian Optimisation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huawei-noah/HEBO/master/T-LBO&#34;&gt;T-LBO&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huawei-noah/HEBO/master/BOiLS&#34;&gt;BOiLS: Bayesian Optimisation for Logic Synthesis&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huawei-noah/HEBO/master/CompBO&#34;&gt;Bayesian Optimisation with Compositional Optimisers&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huawei-noah/HEBO/master/AntBO&#34;&gt;AntBO: Antibody Design with Combinatorial Bayesian Optimisation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Reinforcement Learning Research &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huawei-noah/HEBO/master/SIMMER&#34;&gt;Saut√© RL and Simmer RL: Safe Reinforcement Learning Using Safety State Augmentation &lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huawei-noah/HEBO/master/PMDB&#34;&gt;Model-Based Offline Reinforcement Learning with Pessimism-Modulated Dynamics Belief&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Further instructions are provided in the README files associated to each project.&lt;/p&gt; &#xA;&lt;h1&gt;Bayesian Optimisation Research&lt;/h1&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huawei-noah/HEBO/master/HEBO&#34;&gt;HEBO&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/huawei-noah/HEBO/master/HEBO/hebo.png&#34; alt=&#34;drawing&#34; width=&#34;400&#34;&gt; &#xA;&lt;p&gt;Bayesian optimsation library developped by Huawei Noahs Ark Decision Making and Reasoning (DMnR) lab. The &lt;strong&gt; winning submission &lt;/strong&gt; to the &lt;a href=&#34;https://bbochallenge.com/leaderboard&#34;&gt;NeurIPS 2020 Black-Box Optimisation Challenge&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huawei-noah/HEBO/master/T-LBO&#34;&gt;T-LBO&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p float=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/huawei-noah/HEBO/master/T-LBO/figures/LSBO.png&#34; width=&#34;400&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/huawei-noah/HEBO/master/T-LBO/figures/magnets.png&#34; width=&#34;400&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Codebase associated to: &lt;a href=&#34;https://arxiv.org/abs/2106.03609&#34;&gt;High-Dimensional Bayesian Optimisation with Variational Autoencoders and Deep Metric Learning&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h5&gt;Abstract&lt;/h5&gt; &#xA;&lt;p&gt;We introduce a method based on deep metric learning to perform Bayesian optimisation over high-dimensional, structured input spaces using variational autoencoders (VAEs). By extending ideas from supervised deep metric learning, we address a longstanding problem in high-dimensional VAE Bayesian optimisation, namely how to enforce a discriminative latent space as an inductive bias. Importantly, we achieve such an inductive bias using just 1% of the available labelled data relative to previous work, highlighting the sample efficiency of our approach. As a theoretical contribution, we present a proof of vanishing regret for our method. As an empirical contribution, we present state-of-the-art results on real-world high-dimensional black-box optimisation problems including property-guided molecule generation. It is the hope that the results presented in this paper can act as a guiding principle for realising effective high-dimensional Bayesian optimisation.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huawei-noah/HEBO/master/BOiLS&#34;&gt;BOiLS: Bayesian Optimisation for Logic Synthesis&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/huawei-noah/HEBO/master/BOiLS/results/sample-eff-1.png&#34; alt=&#34;drawing&#34; width=&#34;500&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Codebase associated to: &lt;a href=&#34;https://arxiv.org/abs/2111.06178&#34;&gt;BOiLS: Bayesian Optimisation for Logic Synthesis&lt;/a&gt; accepted at &lt;strong&gt;DATE22&lt;/strong&gt; conference.&lt;/p&gt; &#xA;&lt;h5&gt;Abstract&lt;/h5&gt; &#xA;&lt;p&gt;Optimising the quality-of-results (QoR) of circuits during logic synthesis is a formidable challenge necessitating the exploration of exponentially sized search spaces. While expert-designed operations aid in uncovering effective sequences, the increase in complexity of logic circuits favours automated procedures. Inspired by the successes of machine learning, researchers adapted deep learning and reinforcement learning to logic synthesis applications. However successful, those techniques suffer from high sample complexities preventing widespread adoption. To enable efficient and scalable solutions, we propose BOiLS, the first algorithm adapting modern Bayesian optimisation to navigate the space of synthesis operations. BOiLS requires no human intervention and effectively trades-off exploration versus exploitation through novel Gaussian process kernels and trust-region constrained acquisitions. In a set of experiments on EPFL benchmarks, we demonstrate BOiLS&#39;s superior performance compared to state-of-the-art in terms of both sample efficiency and QoR values.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huawei-noah/HEBO/master/CompBO&#34;&gt;Bayesian Optimisation with Compositional Optimisers&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;div style=&#34;text-align:center&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/huawei-noah/HEBO/master/CompBO/image/summary-Best-performance-on-Synthetic-tasks-matern-52-3.png&#34; alt=&#34;drawing&#34; width=&#34;600&#34;&gt; &#xA; &lt;div style=&#34;text-align:left&#34;&gt; &#xA;  &lt;p&gt;Codebase associated to: &lt;a href=&#34;https://www.jmlr.org/papers/v22/20-1422.html&#34;&gt;Are we Forgetting about Compositional Optimisers in Bayesian Optimisation?&lt;/a&gt; accepted at &lt;strong&gt;JMLR&lt;/strong&gt;.&lt;/p&gt; &#xA;  &lt;h5&gt;Abstract&lt;/h5&gt; &#xA;  &lt;p&gt;Bayesian optimisation presents a sample-efficient methodology for global optimisation. Within this framework, a crucial performance-determining subroutine is the maximisation of the acquisition function, a task complicated by the fact that acquisition functions tend to be non-convex and thus nontrivial to optimise. In this paper, we undertake a comprehensive empirical study of approaches to maximise the acquisition function. Additionally, by deriving novel, yet mathematically equivalent, compositional forms for popular acquisition functions, we recast the maximisation task as a compositional optimisation problem, allowing us to benefit from the extensive literature in this field. We highlight the empirical advantages of the compositional approach to acquisition function maximisation across 3958 individual experiments comprising synthetic optimisation tasks as well as tasks from Bayesmark. Given the generality of the acquisition function maximisation subroutine, we posit that the adoption of compositional optimisers has the potential to yield performance improvements across all domains in which Bayesian optimisation is currently being applied.&lt;/p&gt; &#xA;  &lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huawei-noah/HEBO/master/AntBO&#34;&gt;AntBO: Antibody Design with Combinatorial Bayesian Optimisation&lt;/a&gt;&lt;/h2&gt; &#xA;  &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/huawei-noah/HEBO/master/AntBO/figures/AntBO_illustrationPNG.PNG?raw=true&#34; alt=&#34;AntBO overview&#34;&gt;&lt;/p&gt; &#xA;  &lt;p&gt;Codebase associated to: &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S2667237522002764&#34;&gt;AntBO: Towards Real-World Automated Antibody Design with Combinatorial Bayesian Optimisation&lt;/a&gt;.&lt;/p&gt; &#xA;  &lt;h5&gt;Abstract&lt;/h5&gt; &#xA;  &lt;p&gt;Antibodies are canonically Y-shaped multimeric proteins capable of highly specific molecular recognition. The CDRH3 region located at the tip of variable chains of an antibody dominates antigen-binding specificity. Therefore, it is a priority to design optimal antigen-specific CDRH3 regions to develop therapeutic antibodies to combat harmful pathogens. However, the combinatorial nature of CDRH3 sequence space makes it impossible to search for an optimal binding sequence exhaustively and efficiently, especially not experimentally. Here, we present AntBO: a Combinatorial Bayesian Optimisation framework enabling efficient in silico design of the CDRH3 region. Ideally, antibodies should bind to their target antigen and be free from any harmful outcomes. Therefore, we introduce the CDRH3 trust region that restricts the search to sequences with feasible developability scores. To benchmark AntBO, we use the Absolut! software suite as a black-box oracle because it can score the target specificity and affinity of designed antibodies in silico in an unconstrained fashion. The results across 188 antigens demonstrate the benefit of AntBO in designing CDRH3 regions with diverse biophysical properties. In under 200 protein designs, AntBO can suggest antibody sequences that outperform the best binding sequence drawn from 6.9 million experimentally obtained CDRH3s and a commonly used genetic algorithm baseline. Additionally, AntBO finds very-high affinity CDRH3 sequences in only 38 protein designs whilst requiring no domain knowledge. We conclude AntBO brings automated antibody design methods closer to what is practically viable for in vitro experimentation.&lt;/p&gt; &#xA;  &lt;h1&gt;Reinforcement Learning Research&lt;/h1&gt; &#xA;  &lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huawei-noah/HEBO/master/SIMMER&#34;&gt;Saut√© RL and Simmer RL: Safe Reinforcement Learning Using Safety State Augmentation&lt;/a&gt;&lt;/h2&gt; &#xA;  &lt;p&gt;Codebase associated to: &lt;a href=&#34;https://arxiv.org/pdf/2202.06558.pdf&#34;&gt;Saut√© RL: Almost Surely Safe RL Using State Augmentation&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/pdf/2206.02675.pdf&#34;&gt;Enhancing Safe Exploration Using Safety State Augmentation&lt;/a&gt;.&lt;/p&gt; &#xA;  &lt;h5&gt;Abstract for Saut√© RL: Almost Surely Safe RL Using State Augmentation (ICML 2022)&lt;/h5&gt; &#xA;  &lt;p&gt;Satisfying safety constraints almost surely (or with probability one) can be critical for deployment of Reinforcement Learning (RL) in real-life applications. For example, plane landing and take-off should ideally occur with probability one. We address the problem by introducing Safety Augmented (Saute) Markov Decision Processes (MDPs), where the safety constraints are eliminated by augmenting them into the state-space and reshaping the objective. We show that Saute MDP satisfies the Bellman equation and moves us closer to solving Safe RL with constraints satisfied almost surely. We argue that Saute MDP allows to view Safe RL problem from a different perspective enabling new features. For instance, our approach has a plug-and-play nature, i.e., any RL algorithm can be &#34;sauteed&#34;. Additionally, state augmentation allows for policy generalization across safety constraints. We finally show that Saute RL algorithms can outperform their state-of-the-art counterparts when constraint satisfaction is of high importance.&lt;/p&gt; &#xA;  &lt;h5&gt;Abstract for Effects of Safety State Augmentation on Safe Exploration (NeurIPS 2022)&lt;/h5&gt; &#xA;  &lt;p&gt;Safe exploration is a challenging and important problem in model-free reinforcement learning (RL). Often the safety cost is sparse and unknown, which unavoidably leads to constraint violations -- a phenomenon ideally to be avoided in safety-critical applications. We tackle this problem by augmenting the state-space with a safety state, which is nonnegative if and only if the constraint is satisfied. The value of this state also serves as a distance toward constraint violation, while its initial value indicates the available safety budget. This idea allows us to derive policies for scheduling the safety budget during training. We call our approach Simmer (Safe policy IMproveMEnt for RL) to reflect the careful nature of these schedules. We apply this idea to two safe RL problems: RL with constraints imposed on an average cost, and RL with constraints imposed on a cost with probability one. Our experiments suggest that simmering a safe algorithm can improve safety during training for both settings. We further show that Simmer can stabilize training and improve the performance of safe RL with average constraints.&lt;/p&gt; &#xA;  &lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huawei-noah/HEBO/master/PMDB&#34;&gt;Model-Based Offline Reinforcement Learning with Pessimism-Modulated Dynamics Belief&lt;/a&gt;&lt;/h2&gt; &#xA;  &lt;p&gt;Code associdated to: &lt;a href=&#34;https://nips.cc/Conferences/2022/Schedule?showEvent=54842&#34;&gt;Model-Based Offline Reinforcement Learning with Pessimism-Modulated Dynamics Belief&lt;/a&gt; accepted at &lt;strong&gt;NeurIPS22&lt;/strong&gt; conference.&lt;/p&gt; &#xA;  &lt;h4&gt;Abstract&lt;/h4&gt; &#xA;  &lt;p&gt;Model-based offline reinforcement learning (RL) aims to find highly rewarding policy, by leveraging a previously collected static dataset and a dynamics model. While learned through reuse of static dataset, the dynamics model&#39;s generalization ability hopefully promotes policy learning if properly utilized. To that end, several works propose to quantify the uncertainty of predicted dynamics, and explicitly apply it to penalize reward. However, as the dynamics and the reward are intrinsically different factors in context of MDP, characterizing the impact of dynamics uncertainty through reward penalty may incur unexpected tradeoff between model utilization and risk avoidance. In this work, we instead maintain a belief distribution over dynamics, and evaluate/optimize policy through biased sampling from the belief. The sampling procedure, biased towards pessimism, is derived based on an alternating Markov game formulation of offline RL. We formally show that the biased sampling naturally induces an updated dynamics belief with policy-dependent reweighting factor, termed &lt;em&gt;Pessimism-Modulated Dynamics Belief&lt;/em&gt;. To improve policy, we devise an iterative regularized policy optimization algorithm for the game, with guarantee of monotonous improvement under certain condition. To make practical, we further devise an offline RL algorithm to approximately find the solution. Empirical results show that the proposed approach achieves state-of-the-art performance on a wide range of benchmark tasks.&lt;/p&gt; &#xA;  &lt;hr&gt; &#xA;  &lt;h2&gt;Codebase Contributors&lt;/h2&gt; &#xA;  &lt;p&gt;&lt;strong&gt; Current contributors: &lt;/strong&gt; Antoine Grosnit, Alexandre Max Maravel, Taher Jafferjee, Wenlong Lyu, Kaiyang Guo.&lt;/p&gt; &#xA;  &lt;p&gt;&lt;strong&gt; Alumni contributors: &lt;/strong&gt; Alexander I. Cowen-Rivers, Aivar Sootla, Ryan Rhys Griffiths, Zhi Wang.&lt;/p&gt; &#xA; &lt;/div&gt;&#xA;&lt;/div&gt;</summary>
  </entry>
</feed>