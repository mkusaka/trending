<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-02-12T01:58:36Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>AttendAndExcite/Attend-and-Excite</title>
    <updated>2023-02-12T01:58:36Z</updated>
    <id>tag:github.com,2023-02-12:/AttendAndExcite/Attend-and-Excite</id>
    <link href="https://github.com/AttendAndExcite/Attend-and-Excite" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official Implementation for &#34;Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Recent text-to-image generative models have demonstrated an unparalleled ability to generate diverse and creative imagery guided by a target text prompt. While revolutionary, current state-of-the-art diffusion models may still fail in generating images that fully convey the semantics in the given text prompt. We analyze the publicly available Stable Diffusion model and assess the existence of catastrophic neglect, where the model fails to generate one or more of the subjects from the input prompt. Moreover, we find that in some cases the model also fails to correctly bind attributes (e.g., colors) to their corresponding subjects. To help mitigate these failure cases, we introduce the concept of Generative Semantic Nursing (GSN), where we seek to intervene in the generative process on the fly during inference time to improve the faithfulness of the generated images. Using an attention-based formulation of GSN, dubbed Attend-and-Excite, we guide the model to refine the cross-attention units to attend to all subject tokens in the text prompt and strengthen — or excite — their activations, encouraging the model to generate all subjects described in the text prompt. We compare our approach to alternative approaches and demonstrate that it conveys the desired concepts more faithfully across a range of text prompts.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2301.13826&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2301.13826-b31b1b.svg?sanitize=true&#34; height=&#34;20.5&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://attendandexcite.github.io/Attend-and-Excite/&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Project&amp;amp;message=Website&amp;amp;color=red&#34; height=&#34;20.5&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://youtu.be/9EWs2IX4cus&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=5-Minute&amp;amp;message=Video&amp;amp;color=darkgreen&#34; height=&#34;20.5&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/hysts/Attend-and-Excite&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/AttendAndExcite/Attend-and-Excite/main/docs/teaser.jpg&#34; width=&#34;800px&#34;&gt; &lt;br&gt; Given a pre-trained text-to-image diffusion model (e.g., Stable Diffusion) our method, Attend-and-Excite, guides the generative model to modify the cross-attention values during the image synthesis process to generate images that more faithfully depict the input text prompt. Stable Diffusion alone (top row) struggles to generate multiple objects (e.g., a horse and a dog). However, by incorporating Attend-and-Excite (bottom row) to strengthen the subject tokens (marked in blue), we achieve images that are more semantically faithful with respect to the input text prompts. &lt;/p&gt; &#xA;&lt;h2&gt;Description&lt;/h2&gt; &#xA;&lt;p&gt;Official implementation of our Attend-and-Excite paper.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;h3&gt;Environment&lt;/h3&gt; &#xA;&lt;p&gt;Our code builds on the requirement of the official &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;Stable Diffusion repository&lt;/a&gt;. To set up their environment, please run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env create -f environment/environment.yaml&#xA;conda activate ldm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On top of these requirements, we add several requirements which can be found in &lt;code&gt;environment/requirements.txt&lt;/code&gt;. These requirements will be installed in the above command.&lt;/p&gt; &#xA;&lt;h3&gt;Hugging Face Diffusers Library&lt;/h3&gt; &#xA;&lt;p&gt;Our code relies also on Hugging Face&#39;s &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;diffusers&lt;/a&gt; library for downloading the Stable Diffusion v1.4 model.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/AttendAndExcite/Attend-and-Excite/main/docs/results.jpg&#34; width=&#34;800px&#34;&gt; &lt;br&gt; Example generations outputted by Stable Diffusion with Attend-and-Excite. &lt;/p&gt; &#xA;&lt;p&gt;To generate an image, you can simply run the &lt;code&gt;run.py&lt;/code&gt; script. For example,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run.py --prompt &#34;a cat and a dog&#34; --seeds [0] --token_indices [2,5]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Notes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You may run multiple seeds by passing a list of seeds. For example, &lt;code&gt;--seeds [0,1,2,3]&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;If you do not provide a list of which token indices to alter using &lt;code&gt;--token_indices&lt;/code&gt;, we will split the text according to the Stable Diffusion&#39;s tokenizer and display the index of each token. You will then be able to input which indices you wish to alter.&lt;/li&gt; &#xA; &lt;li&gt;If you wish to run the standard Stable Diffusion model without Attend-and-Excite, you can do so by passing &lt;code&gt;--run_standard_sd True&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;All parameters are defined in &lt;code&gt;config.py&lt;/code&gt; and are set to their defaults according to the official paper.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;All generated images will be saved to the path &lt;code&gt;&#34;{config.output_path}/{prompt}&#34;&lt;/code&gt;. We will also save a grid of all images (in the case of multiple seeds) under &lt;code&gt;config.output_path&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Notebooks&lt;/h2&gt; &#xA;&lt;p&gt;We provide Jupyter notebooks to reproduce the results from the paper for image generation and explainability via the cross-attention maps.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/AttendAndExcite/Attend-and-Excite/main/docs/explainability.jpg&#34; width=&#34;450px&#34;&gt; &lt;br&gt; Example cross-attention visualizations. &lt;/p&gt; &#xA;&lt;h3&gt;Generation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;notebooks/generate_images.ipynb&lt;/code&gt; enables image generation using a free-form text prompt with and without Attend-and-Excite.&lt;/p&gt; &#xA;&lt;h3&gt;Explainability&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;notebooks/explain.ipynb&lt;/code&gt; produces a comparison of the cross-attention maps before and after applying Attend-and-Excite as seen in the illustration above. This notebook can be used to provide an explanation for the generations produced by Attend-and-Excite.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;This code is builds on the code from the &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;diffusers&lt;/a&gt; library as well as the &lt;a href=&#34;https://github.com/google/prompt-to-prompt/&#34;&gt;Prompt-to-Prompt&lt;/a&gt; codebase.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Lux-AI-Challenge/Lux-Design-S2</title>
    <updated>2023-02-12T01:58:36Z</updated>
    <id>tag:github.com,2023-02-12:/Lux-AI-Challenge/Lux-Design-S2</id>
    <link href="https://github.com/Lux-AI-Challenge/Lux-Design-S2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Repository for the Lux AI Challenge, season 2&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Lux-Design-S2&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://badge.fury.io/py/luxai_s2&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/luxai_s2.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Welcome to the Lux AI Challenge Season 2!&lt;/p&gt; &#xA;&lt;p&gt;The Lux AI Challenge is a competition where competitors design agents to tackle a multi-variable optimization, resource gathering, and allocation problem in a 1v1 scenario against other competitors. In addition to optimization, successful agents must be capable of analyzing their opponents and developing appropriate policies to get the upper hand.&lt;/p&gt; &#xA;&lt;p&gt;Key features this season!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GPU/TPU optimized environment via Jax&lt;/li&gt; &#xA; &lt;li&gt;Asymmetric maps and novel mechanics (action efficiency and planning)&lt;/li&gt; &#xA; &lt;li&gt;$55,000 Prize Pool&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Go to our &lt;a href=&#34;https://raw.githubusercontent.com/Lux-AI-Challenge/Lux-Design-S2/main/#getting-started&#34;&gt;Getting Started&lt;/a&gt; section to get started programming a bot. The official competition runs until April 24th and submissions are due at 11:59PM UTC on the competition page: &lt;a href=&#34;https://www.kaggle.com/competitions/lux-ai-season-2&#34;&gt;https://www.kaggle.com/competitions/lux-ai-season-2&lt;/a&gt;. There is a &lt;strong&gt;$55,000&lt;/strong&gt; prize pool this year thanks to contributions from Kaggle, and our sponsors &lt;a href=&#34;https://quantco.com/&#34;&gt;QuantCo&lt;/a&gt;, &lt;a href=&#34;https://www.regression.gg/&#34;&gt;Regression Games&lt;/a&gt;, and &lt;a href=&#34;https://tsvcap.com&#34;&gt;TSVC&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Make sure to join our community discord at &lt;a href=&#34;https://discord.gg/aWJt3UAcgn&#34;&gt;https://discord.gg/aWJt3UAcgn&lt;/a&gt; to chat, strategize, and learn with other competitors! We will be posting announcements on the Kaggle Forums and on the discord.&lt;/p&gt; &#xA;&lt;p&gt;Season 2 specifications can be found here: &lt;a href=&#34;https://lux-ai.org/specs-s2&#34;&gt;https://lux-ai.org/specs-s2&lt;/a&gt;. These detail how the game works and what rules your agent must abide by.&lt;/p&gt; &#xA;&lt;p&gt;Interested in Season 1? Check out &lt;a href=&#34;https://github.com/Lux-AI-Challenge/Lux-Design-2021&#34;&gt;last year&#39;s repository&lt;/a&gt; where we received 22,000+ submissions from 1,100+ teams around the world ranging from scripted agents to Deep Reinforcement Learning.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;You will need Python &amp;gt;=3.7, &amp;lt;3.11 installed on your system. Once installed, you can install the Lux AI season 2 environment and optionally the GPU version with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install --upgrade luxai_s2&#xA;pip install juxai-s2 # installs the GPU version, requires a compatible GPU&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To verify your installation, you can run the CLI tool by replacing &lt;code&gt;path/to/bot/main.py&lt;/code&gt; with a path to a bot (e.g. the starter kit in &lt;code&gt;kits/python/main.py&lt;/code&gt;) and run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;luxai-s2 path/to/bot/main.py path/to/bot/main.py -v 2 -o replay.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will turn on logging to level 2, and store the replay file at &lt;code&gt;replay.json&lt;/code&gt;. For documentation on the luxai-s2 tool, see the &lt;a href=&#34;https://github.com/Lux-AI-Challenge/Lux-Design-S2/tree/main/luxai_s2/luxai_runner/README.md&#34;&gt;tool&#39;s README&lt;/a&gt;, which also includes details on how to run a local tournament to mass evaluate your agents. To watch the replay, upload &lt;code&gt;replay.json&lt;/code&gt; to &lt;a href=&#34;https://s2vis.lux-ai.org/&#34;&gt;https://s2vis.lux-ai.org/&lt;/a&gt; (or change &lt;code&gt;-o replay.json&lt;/code&gt; to &lt;code&gt;-o replay.html&lt;/code&gt;)&lt;/p&gt; &#xA;&lt;p&gt;Each supported programming language/solution type has its own starter kit, you can find general &lt;a href=&#34;https://github.com/Lux-AI-Challenge/Lux-Design-S2/tree/main/kits&#34;&gt;API documentation here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The kits folder in this repository holds all of the available starter kits you can use to start competing and building an AI agent. The readme shows you how to get started with your language of choice and run a match. We strongly recommend reading through the documentation for your language of choice in the links below&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Lux-AI-Challenge/Lux-Design-S2/tree/main/kits/python/&#34;&gt;Python&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Lux-AI-Challenge/Lux-Design-S2/tree/main/kits/rl/&#34;&gt;Reinforcement Learning (Python)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Lux-AI-Challenge/Lux-Design-S2/tree/main/kits/cpp/&#34;&gt;C++&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Lux-AI-Challenge/Lux-Design-S2/tree/main/kits/js/&#34;&gt;Javascript&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Lux-AI-Challenge/Lux-Design-S2/tree/main/kits/java/&#34;&gt;Java&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rooklift/golux2/&#34;&gt;Go&lt;/a&gt; - (A working bare-bones Go kit)&lt;/li&gt; &#xA; &lt;li&gt;Typescript - TBA&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Want to use another language but it&#39;s not supported? Feel free to suggest that language to our issues or even better, create a starter kit for the community to use and make a PR to this repository. See our &lt;a href=&#34;https://github.com/Lux-AI-Challenge/Lux-Design-S2/tree/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; document for more information on this.&lt;/p&gt; &#xA;&lt;!-- Finally, if you want to learn how to use the GPU optimized env see https://github.com/Lux-AI-Challenge/Lux-Design-S2/tree/main/examples/jax_env_tutorial.ipynb&#xA;&#xA;For the RL starter kit that trains using the jax env, see https://github.com/Lux-AI-Challenge/Lux-Design-S2/tree/main/kits/rl-sb3-jax-env/ --&gt; &#xA;&lt;p&gt;To stay up to date on changes and updates to the competition and the engine, watch for announcements on the forums or the &lt;a href=&#34;https://discord.gg/aWJt3UAcgn&#34;&gt;Discord&lt;/a&gt;. See &lt;a href=&#34;https://github.com/Lux-AI-Challenge/Lux-Design-S2/raw/main/ChangeLog.md&#34;&gt;ChangeLog.md&lt;/a&gt; for a full change log.&lt;/p&gt; &#xA;&lt;h2&gt;Community Tools&lt;/h2&gt; &#xA;&lt;p&gt;As the community builds tools for the competition, we will post them here!&lt;/p&gt; &#xA;&lt;p&gt;3rd Party Viewer (This has now been merged into the main repo so check out the lux-eye-s2 folder) - &lt;a href=&#34;https://github.com/jmerle/lux-eye-2022&#34;&gt;https://github.com/jmerle/lux-eye-2022&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://github.com/Lux-AI-Challenge/Lux-Design-S2/raw/main/CONTRIBUTING.md&#34;&gt;guide on contributing&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Sponsors&lt;/h2&gt; &#xA;&lt;p&gt;We are proud to announce our sponsors &lt;a href=&#34;https://quantco.com/&#34;&gt;QuantCo&lt;/a&gt;, &lt;a href=&#34;https://www.regression.gg/&#34;&gt;Regression Games&lt;/a&gt;, and &lt;a href=&#34;https://tsvcap.com&#34;&gt;TSVC&lt;/a&gt;. They help contribute to the prize pool and provide exciting opportunities to our competitors! For more information about them check out &lt;a href=&#34;https://www.lux-ai.org/sponsors-s2&#34;&gt;https://www.lux-ai.org/sponsors-s2&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Core Contributors&lt;/h2&gt; &#xA;&lt;p&gt;We like to extend thanks to some of our early core contributors: &lt;a href=&#34;https://github.com/duanwilliam&#34;&gt;@duanwilliam&lt;/a&gt; (Frontend), &lt;a href=&#34;https://github.com/programjames&#34;&gt;@programjames&lt;/a&gt; (Map generation, Engine optimization), and &lt;a href=&#34;https://github.com/themmj&#34;&gt;@themmj&lt;/a&gt; (C++ kit, Go kit, Engine optimization).&lt;/p&gt; &#xA;&lt;p&gt;We further like to extend thanks to some of our core contributors during the beta period: &lt;a href=&#34;https://github.com/LeFiz&#34;&gt;@LeFiz&lt;/a&gt; (Game Design/Architecture), &lt;a href=&#34;https://github.com/jmerle&#34;&gt;@jmerle&lt;/a&gt; (Visualizer)&lt;/p&gt; &#xA;&lt;p&gt;We further like to thank the following contributors during the official competition: &lt;a href=&#34;https://github.com/paradite&#34;&gt;@aradite&lt;/a&gt;(JS Kit), &lt;a href=&#34;https://github.com/MountainOrc&#34;&gt;@MountainOrc&lt;/a&gt;(Java Kit), &lt;a href=&#34;https://github.com/ArturBloch&#34;&gt;@ArturBloch&lt;/a&gt;(Java Kit), &lt;a href=&#34;https://github.com/rooklift&#34;&gt;@rooklift&lt;/a&gt;(Go Kit)&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use the Lux AI Season 2 environment in your work, please cite this repository as so&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@software{Lux_AI_Challenge_S1,&#xA;  author = {Tao, Stone and Doerschuk-Tiberi, Bovard},&#xA;  month = {10},&#xA;  title = {{Lux AI Challenge Season 2}},&#xA;  url = {https://github.com/Lux-AI-Challenge/Lux-Design-S2},&#xA;  version = {1.0.0},&#xA;  year = {2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>github/codespaces-jupyter</title>
    <updated>2023-02-12T01:58:36Z</updated>
    <id>tag:github.com,2023-02-12:/github/codespaces-jupyter</id>
    <link href="https://github.com/github/codespaces-jupyter" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Explore machine learning and data science with Codespaces&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GitHub Codespaces ♥️ Jupyter Notebooks&lt;/h1&gt; &#xA;&lt;p&gt;Welcome to your shiny new codespace! We&#39;ve got everything fired up and running for you to explore Python and Jupyter notebooks.&lt;/p&gt; &#xA;&lt;p&gt;You&#39;ve got a blank canvas to work on from a git perspective as well. There&#39;s a single initial commit with what you&#39;re seeing right now - where you go from here is up to you!&lt;/p&gt; &#xA;&lt;p&gt;Everything you do here is contained within this one codespace. There is no repository on GitHub yet. If and when you’re ready you can click &#34;Publish Branch&#34; and we’ll create your repository and push up your project. If you were just exploring then and have no further need for this code then you can simply delete your codespace and it&#39;s gone forever.&lt;/p&gt;</summary>
  </entry>
</feed>