<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-09-10T01:57:14Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>microsoft/BatteryML</title>
    <updated>2023-09-10T01:57:14Z</updated>
    <id>tag:github.com,2023-09-10:/microsoft/BatteryML</id>
    <link href="https://github.com/microsoft/BatteryML" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/BatteryML/main/image/Logo_RGB.png&#34; width=&#34;300&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;BatteryML: An Open-Source Tool for Machine Learning on Battery Degradation&lt;/h1&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;The performance degradation of lithium batteries is a complex electrochemical process, involving factors such as the growth of solid electrolyte interface, lithium precipitation, loss of active materials, etc. Furthermore, this inevitable performance degradation can have a significant impact on critical commercial scenarios, such as causing &#39;range anxiety&#39; for electric vehicle users and affecting the power stability of energy storage systems. Therefore, effectively analyzing and predicting the performance degradation of lithium batteries to provide guidance for early prevention and intervention has become a crucial research topic.&lt;/p&gt; &#xA;&lt;p&gt;To this end, we open source the BatteryML tool to facilitate the research and development of machine learning on battery degradation. We hope BatteryML can empower both battery researchers and data scientists to gain deeper insights from battery degradation data and build more powerful models for accurate predictions and early interventions.&lt;/p&gt; &#xA;&lt;h2&gt;Framework&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/BatteryML/main/image/framework.png&#34; width=&#34;800&#34;&gt; &#xA;&lt;h2&gt;Highlights:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Open-source and Community-driven:&lt;/strong&gt; BatteryML is an open-source project for battery degradation modeling, encouraging contributions and collaboration from the communities of both computer science and battery research to push the frontiers of this crucial field.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Comprehensive Dataset Collection:&lt;/strong&gt; BatteryML includes a comprehensive dataset collection, allowing easy accesses to most publicly available battery data.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Preprocessing and Feature Engineering:&lt;/strong&gt; Our tool offers built-in data preprocessing and feature engineering capabilities, making it easier for researchers and developers to prepare ready-to-use battery datasets for machine learning.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Wide Range of Models:&lt;/strong&gt; BatteryML already includes most classic models in the literature, enabling developers to quickly compare and benchmark different approaches.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Extensible and Customizable:&lt;/strong&gt; BatteryML provides flexible interfaces to support further extensions and customizations, making it a versatile tool for potential applications in battery research.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;h3&gt;Install the dependencies&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Download Raw Data and Run Preprocessing Scripts&lt;/h3&gt; &#xA;&lt;p&gt;Download the raw data and execute the preprocessing scripts as per the provided &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/BatteryML/main/dataprepare.md&#34;&gt;instruction&lt;/a&gt;. You can also use the code below to download public datasets and convert them to BatteryML&#39;s uniform data format.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scripts.download import download_raw_data&#xA;from scripts.preprocess import convert_data_format&#xA;&#xA;# Download battery public datasets. If `dataset_name` is set, only the specified dataset will be downloaded. If not set (None), all available preset datasets will be downloaded, including HUST, MATR, CALCE and RWTH.&#xA;download_raw_data(dataset_name=&#39;matr&#39;)&#xA;&#xA;# Convert data format to BatteryML&#39;s unified data format. If `dataset_name` is set, only the specified dataset will be converted. If not set (None), all datasets in the BatteryML/data/raw folder will be processed.&#xA;convert_data_format(dataset_name=&#39;matr&#39;)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Run Pipeline&lt;/h3&gt; &#xA;&lt;p&gt;To get started, simply configure the data, features, models, etc. in the config file. Once you&#39;ve set everything up, run the following code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scripts.pipeline import Pipeline&#xA;&#xA;pipeline = Pipeline(config_path=`path/to/your/config`,&#xA;                    device=&#39;cuda&#39;,&#xA;                    metric=&#39;RMSE&#39;,&#xA;                    workspace=&#39;workspaces&#39;)&#xA;&#xA;train_loss , test_loss = pipeline.train()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: Replace &lt;code&gt;path/to/your/config&lt;/code&gt; with the actual config_path.&lt;/p&gt; &#xA;&lt;p&gt;Besides, we have prepared an example &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/BatteryML/main/baseline.ipynb&#34;&gt;baseline&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;By leveraging BatteryML, researchers can gain valuable insights into the latest advancements in battery prediction and materials science, enabling them to conduct experiments efficiently and effectively. We invite you to join us in our journey to accelerate battery research and innovation by contributing to and utilizing BatteryML for your research endeavors.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/lida</title>
    <updated>2023-09-10T01:57:14Z</updated>
    <id>tag:github.com,2023-09-10:/microsoft/lida</id>
    <link href="https://github.com/microsoft/lida" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Automatic Generation of Visualizations and Infographics using Large Language Models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LIDA: Automatic Generation of Visualizations and Infographics using Large Language Models&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://badge.fury.io/py/lida&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/lida.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2303.02927&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2303.02927-%3CCOLOR%3E.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/microsoft/lida/blob/main/notebooks/tutorial.ipynb&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt; &lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- &lt;img src=&#34;docs/images/lidascreen.png&#34; width=&#34;100%&#34; /&gt; --&gt; &#xA;&lt;p&gt;LIDA is a library for generating data visualizations and data-faithful infographics. LIDA is grammar agnostic (will work with any programming language and visualization libraries e.g. matplotlib, seaborn, altair, d3 etc) and works with multiple large language model providers (OpenAI, PaLM, Cohere, Huggingface). Details on the components of LIDA are described in the &lt;a href=&#34;https://arxiv.org/abs/2303.02927&#34;&gt;paper here&lt;/a&gt; and in this tutorial &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/lida/main/notebooks/tutorial.ipynb&#34;&gt;notebook&lt;/a&gt;. See the project page &lt;a href=&#34;https://microsoft.github.io/lida/&#34;&gt;here&lt;/a&gt; for updates!.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note on Code Execution:&lt;/strong&gt; To create visualizations, LIDA &lt;em&gt;generates&lt;/em&gt; and &lt;em&gt;executes&lt;/em&gt; code. Ensure that you run LIDA in a secure environment.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/lida/main/docs/images/lidamodules.jpg&#34; alt=&#34;lida components&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;LIDA treats &lt;em&gt;&lt;strong&gt;visualizations as code&lt;/strong&gt;&lt;/em&gt; and provides a clean api for generating, executing, editing, explaining, evaluating and repairing visualization code.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Data Summarization&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Goal Generation&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Visualization Generation&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Visualization Editing&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Visualization Explanation&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Visualization Evaluation and Repair&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Visualization Recommendation&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Infographic Generation (beta) # pip install lida[infographics]&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;from lida import Manager, llm&#xA;&#xA;lida = Manager(text_gen = llm(&#34;openai&#34;)) # palm, cohere ..&#xA;summary = lida.summarize(&#34;data/cars.csv&#34;)&#xA;goals = lida.goals(summary, n=2) # exploratory data analysis&#xA;charts = lida.visualize(summary=summary, goal=goals[0]) # exploratory data analysis&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Setup and verify that your python environment is &lt;strong&gt;&lt;code&gt;python 3.10&lt;/code&gt;&lt;/strong&gt; or higher (preferably, use &lt;a href=&#34;https://docs.conda.io/en/main/miniconda.html#installing&#34;&gt;Conda&lt;/a&gt;). Install the library via pip.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install lida&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once requirements are met, setup your api key. Learn more about setting up keys for other LLM providers &lt;a href=&#34;https://github.com/victordibia/llmx&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export OPENAI_API_KEY=&amp;lt;your key&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively you can install the library in dev model by cloning this repo and running &lt;code&gt;pip install -e .&lt;/code&gt; in the repository root.&lt;/p&gt; &#xA;&lt;h2&gt;Web API and UI&lt;/h2&gt; &#xA;&lt;p&gt;LIDA comes with an optional bundled ui and web api that you can explore by running the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;lida ui  --port=8080 --docs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then navigate to &lt;a href=&#34;http://localhost:8080/&#34;&gt;http://localhost:8080/&lt;/a&gt; in your browser. To view the web api specification, add the &lt;code&gt;--docs&lt;/code&gt; option to the cli command, and navigate to &lt;code&gt;http://localhost:8080/api/docs&lt;/code&gt; in your browser.&lt;/p&gt; &#xA;&lt;p&gt;The fastest and recommended way to get started after installation will be to try out the web ui above or run the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/lida/main/notebooks/tutorial.ipynb&#34;&gt;tutorial notebook&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Data Summarization&lt;/h3&gt; &#xA;&lt;p&gt;Given a dataset, generate a compact summary of the data.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from lida import Manager&#xA;&#xA;lida = Manager()&#xA;summary = lida.summarize(&#34;data/cars.json&#34;) # generate data summary&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Goal Generation&lt;/h3&gt; &#xA;&lt;p&gt;Generate a set of visualization goals given a data summary.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;goals = lida.goals(summary, n=5) # generate goals&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Visualization Generation&lt;/h3&gt; &#xA;&lt;p&gt;Generate, refine, execute and filter visualization code given a data summary and visualization goal. Note that LIDA represents &lt;strong&gt;visualizations as code&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# generate charts (generate and execute visualization code)&#xA;charts = lida.visualize(summary=summary, goal=goals[0], library=&#34;matplotlib&#34;) # seaborn, ggplot ..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Visualization Editing&lt;/h3&gt; &#xA;&lt;p&gt;Given a visualization, edit the visualization using natural language.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# modify chart using natural language&#xA;instructions = [&#34;convert this to a bar chart&#34;, &#34;change the color to red&#34;, &#34;change y axes label to Fuel Efficiency&#34;, &#34;translate the title to french&#34;]&#xA;edited_charts = lida.edit(code=code,  summary=summary, instructions=instructions, library=library, textgen_config=textgen_config)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Visualization Explanation&lt;/h3&gt; &#xA;&lt;p&gt;Given a visualization, generate a natural language explanation of the visualization code (accessibility, data transformations applied, visualization code)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# generate explanation for chart&#xA;explanation = lida.explain(code=charts[0].code, summary=summary)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Visualization Evaluation and Repair&lt;/h3&gt; &#xA;&lt;p&gt;Given a visualization, evaluate to find repair instructions (which may be human authored, or generated), repair the visualization.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;evaluations = lida.evaluate(code=code,  goal=goals[i], library=library)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Visualization Recommendation&lt;/h3&gt; &#xA;&lt;p&gt;Given a dataset, generate a set of recommended visualizations.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;recommendations = lida.recommend(code=code, summary=summary, n=2,  textgen_config=textgen_config)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Infographic Generation [WIP]&lt;/h3&gt; &#xA;&lt;p&gt;Given a visualization, generate a data-faithful infographic. This methods should be considered experimental, and uses stable diffusion models from the &lt;a href=&#34;https://github.com/victordibia/peacasso&#34;&gt;peacasso&lt;/a&gt; library. You will need to run &lt;code&gt;pip install lida[infographics]&lt;/code&gt; to install the required dependencies.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;infographics = lida.infographics(visualization = charts[0].raster, n=3, style_prompt=&#34;line art&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Important Notes / Caveats / FAQs&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;LIDA generates and executes code based on provided input. Ensure that you run LIDA in a secure environment with appropriate permissions.&lt;/li&gt; &#xA; &lt;li&gt;LIDA currently works best with datasets that have a small number of columns (&amp;lt;= 10). This is mainly due to the limited context size for most models. For larger datasets, consider preprocessing your dataset to use a subset of the columns.&lt;/li&gt; &#xA; &lt;li&gt;LIDA assumes the dataset exists and is in a format that can be loaded into a pandas dataframe. For example, a csv file, or a json file with a list of objects. In practices the right dataset may need to be curated and preprocessed to ensure that it is suitable for the task at hand.&lt;/li&gt; &#xA; &lt;li&gt;Smaller LLMs (e.g., OSS LLMs on Huggingface) have limited instruction following capabilities and may not work well with LIDA. LIDA works best with larger LLMs (e.g., OpenAI GPT 3.5, GPT 4).&lt;/li&gt; &#xA; &lt;li&gt;How reliable is the LIDA approach? The LIDA &lt;a href=&#34;https://aclanthology.org/2023.acl-demo.11/&#34;&gt;paper&lt;/a&gt; describes experiments that evaluate the reliability of LIDA using a visualization error rate metric. With the current version of prompts, data summarization techniques, preprocessing/postprocessing logic and LLMs, LIDA has an error rate of &amp;lt; 3.5% on over 2200 visualizations generated (compared to a baseline of over 10% error rate). This area is work in progress.&lt;/li&gt; &#xA; &lt;li&gt;Can I build my own apps with LIDA? Yes! You can either use the python api directly in your app or setup a web api endpoint and use the web api in your app. See the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/lida/main/#web-api-and-ui&#34;&gt;web api&lt;/a&gt; section for more details.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Naturally, some of these limitations could be addressed by a much welcomed PR.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation and Citation&lt;/h2&gt; &#xA;&lt;p&gt;A short paper describing LIDA (Accepted at ACL 2023 Conference) is available &lt;a href=&#34;https://arxiv.org/abs/2303.02927&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{dibia2023lida,&#xA;    title = &#34;{LIDA}: A Tool for Automatic Generation of Grammar-Agnostic Visualizations and Infographics using Large Language Models&#34;,&#xA;    author = &#34;Dibia, Victor&#34;,&#xA;    booktitle = &#34;Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)&#34;,&#xA;    month = jul,&#xA;    year = &#34;2023&#34;,&#xA;    address = &#34;Toronto, Canada&#34;,&#xA;    publisher = &#34;Association for Computational Linguistics&#34;,&#xA;    url = &#34;https://aclanthology.org/2023.acl-demo.11&#34;,&#xA;    doi = &#34;10.18653/v1/2023.acl-demo.11&#34;,&#xA;    pages = &#34;113--126&#34;,&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;LIDA builds on insights in automatic generation of visualization from an earlier paper - &lt;a href=&#34;https://arxiv.org/abs/1804.03126&#34;&gt;Data2Vis: Automatic Generation of Data Visualizations Using Sequence to Sequence Recurrent Neural Networks&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>stanfordnlp/dspy</title>
    <updated>2023-09-10T01:57:14Z</updated>
    <id>tag:github.com,2023-09-10:/stanfordnlp/dspy</id>
    <link href="https://github.com/stanfordnlp/dspy" rel="alternate"></link>
    <summary type="html">&lt;p&gt;DSPy: The framework for programming with foundation models&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img align=&#34;center&#34; src=&#34;https://raw.githubusercontent.com/stanfordnlp/dspy/main/docs/images/DSPy8.png&#34; width=&#34;460px&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;/p&gt;&#xA;&lt;h2&gt;DSPy: &lt;em&gt;Programming&lt;/em&gt;â€”not promptingâ€”Foundation Models&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/stanfordnlp/dspy/blob/main/intro.ipynb&#34;&gt;&lt;img align=&#34;center&#34; src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DSPy&lt;/strong&gt; is the framework for solving advanced tasks with language models (LMs) and retrieval models (RMs). &lt;strong&gt;DSPy&lt;/strong&gt; unifies techniques for &lt;strong&gt;prompting&lt;/strong&gt; and &lt;strong&gt;fine-tuning&lt;/strong&gt; LMs â€” and approaches for &lt;strong&gt;reasoning&lt;/strong&gt; and &lt;strong&gt;tool/retrieval augmentation&lt;/strong&gt;. In DSPy, all of these are expressed through a small set of Pythonic modules &lt;em&gt;that compose and learn&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To make this possible:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;DSPy&lt;/strong&gt; provides &lt;strong&gt;composable and declarative modules&lt;/strong&gt; for instructing LMs in a familiar Pythonic syntax. It upgrades &#34;prompting techniques&#34; like chain-of-thought and self-reflection from hand-adapted &lt;em&gt;string manipulation tricks&lt;/em&gt; into truly modular &lt;em&gt;generalized operations that learn to adapt to your task&lt;/em&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;DSPy&lt;/strong&gt; introduces an &lt;strong&gt;automatic compiler that teaches LMs&lt;/strong&gt; how to conduct the declarative steps in your program. Specifically, the &lt;strong&gt;DSPy compiler&lt;/strong&gt; will internally &lt;em&gt;trace&lt;/em&gt; your program and then &lt;strong&gt;craft high-quality prompts for large LMs (or train automatic finetunes for small LMs)&lt;/strong&gt; to teach them the steps of your task.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The &lt;strong&gt;DSPy compiler&lt;/strong&gt; &lt;em&gt;bootstraps&lt;/em&gt; prompts and finetunes from minimal data &lt;strong&gt;without needing manual labels for the intermediate steps&lt;/strong&gt; in your program. Instead of brittle &#34;prompt engineering&#34; with hacky string manipulation, you can explore a systematic space of modular and trainable pieces.&lt;/p&gt; &#xA;&lt;p&gt;For complex tasks, &lt;strong&gt;DSPy&lt;/strong&gt; can routinely teach powerful models like &lt;code&gt;GPT-3.5&lt;/code&gt; and local models like &lt;code&gt;T5-base&lt;/code&gt; or &lt;code&gt;Llama2-13b&lt;/code&gt; to be much more reliable at tasks. &lt;strong&gt;DSPy&lt;/strong&gt; will compile the &lt;em&gt;same program&lt;/em&gt; into different few-shot prompts and/or finetunes for each LM.&lt;/p&gt; &#xA;&lt;p&gt;If you want to see &lt;strong&gt;DSPy&lt;/strong&gt; in action, &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stanfordnlp/dspy/main/intro.ipynb&#34;&gt;open our intro tutorial notebook&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Table of Contents&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stanfordnlp/dspy/main/#1-installation&#34;&gt;Installation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stanfordnlp/dspy/main/#2-syntax-youre-in-charge-of-the-workflowits-free-form-python-code&#34;&gt;Framework Syntax&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stanfordnlp/dspy/main/#3-two-powerful-concepts-signatures--teleprompters&#34;&gt;Compiling: Two Powerful Concepts&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stanfordnlp/dspy/main/#4-documentation--tutorials&#34;&gt;Tutorials &amp;amp; Documentation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stanfordnlp/dspy/main/#5-faq-is-dspy-right-for-me&#34;&gt;FAQ: Is DSPy right for me?&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Analogy to Neural Networks&lt;/h3&gt; &#xA;&lt;p&gt;When we build neural networks, we don&#39;t write manual &lt;em&gt;for-loops&lt;/em&gt; over lists of &lt;em&gt;hand-tuned&lt;/em&gt; floats. Instead, you might use a framework like &lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch&lt;/a&gt; to compose declarative layers (e.g., &lt;code&gt;Convolution&lt;/code&gt; or &lt;code&gt;Dropout&lt;/code&gt;) and then use optimizers (e.g., SGD or Adam) to learn the parameters of the network.&lt;/p&gt; &#xA;&lt;p&gt;Ditto! &lt;strong&gt;DSPy&lt;/strong&gt; gives you the right general-purpose modules (e.g., &lt;code&gt;ChainOfThought&lt;/code&gt;, &lt;code&gt;Retrieve&lt;/code&gt;, etc.) and takes care of optimizing their prompts &lt;em&gt;for your program&lt;/em&gt; and your metric, whatever they aim to do. Whenever you modify your code, your data, or your validation constraints, you can &lt;em&gt;compile&lt;/em&gt; your program again and &lt;strong&gt;DSPy&lt;/strong&gt; will create new effective prompts that fit your changes.&lt;/p&gt; &#xA;&lt;h2&gt;1) Installation&lt;/h2&gt; &#xA;&lt;p&gt;All you need is:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install dspy-ai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or open our intro notebook in Google Colab: &lt;a href=&#34;https://colab.research.google.com/github/stanfordnlp/dspy/blob/main/intro.ipynb&#34;&gt;&lt;img align=&#34;center&#34; src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;em&gt;Note: If you&#39;re looking for Demonstrate-Search-Predict (DSP), which is the previous version of DSPy, you can find it on the &lt;a href=&#34;https://github.com/stanfordnlp/dspy/tree/v1&#34;&gt;v1&lt;/a&gt; branch of this repo.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;2) Syntax: You&#39;re in charge of the workflowâ€”it&#39;s free-form Python code!&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;DSPy&lt;/strong&gt; hides tedious prompt engineering, but it cleanly exposes the important decisions you need to make: &lt;strong&gt;[1]&lt;/strong&gt; what&#39;s your system design going to look like? &lt;strong&gt;[2]&lt;/strong&gt; what are the important constraints on the behavior of your program?&lt;/p&gt; &#xA;&lt;p&gt;You express your system as free-form Pythonic modules. &lt;strong&gt;DSPy&lt;/strong&gt; will tune the quality of your program &lt;em&gt;in whatever way&lt;/em&gt; you use foundation models: you can code with loops, &lt;code&gt;if&lt;/code&gt; statements, or exceptions, and use &lt;strong&gt;DSPy&lt;/strong&gt; modules within any Python control flow you think works for your task.&lt;/p&gt; &#xA;&lt;p&gt;Suppose you want to build a simple retrieval-augmented generation (RAG) system for question answering. You can define your own &lt;code&gt;RAG&lt;/code&gt; program like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class RAG(dspy.Module):&#xA;    def __init__(self, num_passages=3):&#xA;        super().__init__()&#xA;        self.retrieve = dspy.Retrieve(k=num_passages)&#xA;        self.generate_answer = dspy.ChainOfThought(&#34;context, question -&amp;gt; answer&#34;)&#xA;    &#xA;    def forward(self, question):&#xA;        context = self.retrieve(question).passages&#xA;        answer = self.generate_answer(context=context, question=question)&#xA;        return answer&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A program has two key methods, which you can edit to fit your needs.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Your &lt;code&gt;__init__&lt;/code&gt; method&lt;/strong&gt; declares the modules you will use. Here, &lt;code&gt;RAG&lt;/code&gt; will use the built-in &lt;code&gt;Retrieve&lt;/code&gt; for retrieval and &lt;code&gt;ChainOfThought&lt;/code&gt; for generating answers. &lt;strong&gt;DSPy&lt;/strong&gt; offers general-purpose modules that take the shape of &lt;em&gt;your own&lt;/em&gt; sub-tasks â€” and not pre-built functions for specific applications.&lt;/p&gt; &#xA;&lt;p&gt;Modules that use the LM, like &lt;code&gt;ChainOfThought&lt;/code&gt;, require a &lt;em&gt;signature&lt;/em&gt;. That is a declarative spec that tells the module what it&#39;s expected to do. In this example, we use the short-hand signature notation &lt;code&gt;context, question -&amp;gt; answer&lt;/code&gt; to tell &lt;code&gt;ChainOfThought&lt;/code&gt; it will be given some &lt;code&gt;context&lt;/code&gt; and a &lt;code&gt;question&lt;/code&gt; and must produce an &lt;code&gt;answer&lt;/code&gt;. We will discuss more advanced &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stanfordnlp/dspy/main/#3a-declaring-the-inputoutput-behavior-of-lms-with-dspysignature&#34;&gt;signatures&lt;/a&gt;&lt;/strong&gt; below.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Your &lt;code&gt;forward&lt;/code&gt; method&lt;/strong&gt; expresses any computation you want to do with your modules. In this case, we use the modules &lt;code&gt;self.retrieve&lt;/code&gt; and &lt;code&gt;self.generate_answer&lt;/code&gt; to search for some &lt;code&gt;context&lt;/code&gt; and then use the &lt;code&gt;context&lt;/code&gt; and &lt;code&gt;question&lt;/code&gt; to generate the &lt;code&gt;answer&lt;/code&gt;!&lt;/p&gt; &#xA;&lt;p&gt;You can now either use this &lt;code&gt;RAG&lt;/code&gt; program in &lt;strong&gt;zero-shot mode&lt;/strong&gt;. Or &lt;strong&gt;compile&lt;/strong&gt; it to obtain higher quality. Zero-shot usage is simple. Just define an instance of your program and then call it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;rag = RAG()  # zero-shot, uncompiled version of RAG&#xA;rag(&#34;what is the capital of France?&#34;).answer  # -&amp;gt; &#34;Paris&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The next section will discuss how to compile our simple &lt;code&gt;RAG&lt;/code&gt; program. When we compile it, the &lt;strong&gt;DSPy compiler&lt;/strong&gt; will annotate &lt;em&gt;demonstrations&lt;/em&gt; of its steps: (1) retrieval, (2) using context, and (3) using &lt;em&gt;chain-of-thought&lt;/em&gt; to answer questions. From these demonstrations, the &lt;strong&gt;DSPy compiler&lt;/strong&gt; will make sure it produces an effective few-shot prompt that works well with your LM, retrieval model, and data. If you&#39;re working with small models, it&#39;ll finetune your model (instead of prompting) to do this task.&lt;/p&gt; &#xA;&lt;p&gt;If you later decide you need another step in your pipeline, just add another module and compile again. Maybe add a module that takes the chat history into account during search?&lt;/p&gt; &#xA;&lt;h2&gt;3) Two Powerful Concepts: Signatures &amp;amp; Teleprompters&lt;/h2&gt; &#xA;&lt;p&gt;To make it possible to compile any program you write, &lt;strong&gt;DSPy&lt;/strong&gt; introduces two simple concepts: Signatures and Teleprompters.&lt;/p&gt; &#xA;&lt;h4&gt;3.a) Declaring the input/output behavior of LMs with &lt;code&gt;dspy.Signature&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;When we assign tasks to LMs in &lt;strong&gt;DSPy&lt;/strong&gt;, we specify the behavior we need as a &lt;strong&gt;Signature&lt;/strong&gt;. A signature is a declarative specification of input/output behavior of a &lt;strong&gt;DSPy module&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Instead of investing effort into &lt;em&gt;how&lt;/em&gt; to get your LM to do a sub-task, signatures enable you to inform &lt;strong&gt;DSPy&lt;/strong&gt; &lt;em&gt;what&lt;/em&gt; the sub-task is. Later, the &lt;strong&gt;DSPy compiler&lt;/strong&gt; will figure out how to build a complex prompt for your large LM (or finetune your small LM) specifically for your signature, on your data, and within your pipeline.&lt;/p&gt; &#xA;&lt;p&gt;A signature consists of three simple elements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A minimal description of the sub-task the LM is supposed to solve.&lt;/li&gt; &#xA; &lt;li&gt;A description of one or more input fields (e.g., input question) that will we will give to the LM.&lt;/li&gt; &#xA; &lt;li&gt;A description of one or more output fields (e.g., the question&#39;s answer) that we will expect from the LM.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We support two notations for expressing signatures. The &lt;strong&gt;short-hand signature notation&lt;/strong&gt; is for quick development. You just provide your module (e.g., &lt;code&gt;dspy.ChainOfThought&lt;/code&gt;) with a string with &lt;code&gt;input_field_name_1, ... -&amp;gt; output_field_name_1, ...&lt;/code&gt; with the fields separated by commas.&lt;/p&gt; &#xA;&lt;p&gt;In the &lt;code&gt;RAG&lt;/code&gt; class earlier, we saw:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;self.generate_answer = dspy.ChainOfThought(&#34;context, question -&amp;gt; answer&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In many cases, this barebones signature is sufficient. However, sometimes you need more control. In these cases, we can use the full notation to express a more fully-fledged signature below.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class GenerateSearchQuery(dspy.Signature):&#xA;    &#34;&#34;&#34;Write a simple search query that will help answer a complex question.&#34;&#34;&#34;&#xA;&#xA;    context = dspy.InputField(desc=&#34;may contain relevant facts&#34;)&#xA;    question = dspy.InputField()&#xA;    query = dspy.OutputField()&#xA;&#xA;### inside your program&#39;s __init__ function&#xA;self.generate_answer = dspy.ChainOfThought(GenerateSearchQuery)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can optionally provide a &lt;code&gt;prefix&lt;/code&gt; and/or &lt;code&gt;desc&lt;/code&gt; key for each input or output field to refine or constraint the behavior of modules using your signature.&lt;/p&gt; &#xA;&lt;h4&gt;3.b) Asking &lt;strong&gt;DSPy&lt;/strong&gt; to automatically optimize your program with &lt;code&gt;dspy.teleprompt.*&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;After defining the &lt;code&gt;RAG&lt;/code&gt; program, we can &lt;strong&gt;compile&lt;/strong&gt; it. Compiling a program will update the parameters stored in each module. For large LMs, this is primarily in the form of creating and validating good demonstrations for inclusion in your prompt(s).&lt;/p&gt; &#xA;&lt;p&gt;Compiling depends on three things: a (potentially tiny) training set, a metric for validation, and your choice of teleprompter from &lt;strong&gt;DSPy&lt;/strong&gt;. &lt;strong&gt;Teleprompters&lt;/strong&gt; are powerful optimizers (included in &lt;strong&gt;DSPy&lt;/strong&gt;) that can learn to bootstrap and select effective prompts for the modules of any program. (The &#34;tele-&#34; in the name means &#34;at a distance&#34;, i.e., automatic prompting at a distance.)&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DSPy&lt;/strong&gt; typically requires very minimal labeling. For example, our &lt;code&gt;RAG&lt;/code&gt; pipeline may work well with just a handful of examples that contain a &lt;strong&gt;question&lt;/strong&gt; and its (human-annotated) &lt;strong&gt;answer&lt;/strong&gt;. Your pipeline may involve multiple complex steps: our basic &lt;code&gt;RAG&lt;/code&gt; example includes a retrieved context, a chain of thought, and the answer. However, you only need labels for the initial question and the final answer. &lt;strong&gt;DSPy&lt;/strong&gt; will bootstrap any intermediate labels needed to support your pipeline. If you change your pipeline in any way, the data bootstrapped will change accordingly!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;my_rag_trainset = [&#xA;  dspy.Example(&#xA;    question=&#34;Which award did Gary Zukav&#39;s first book receive?&#34;,&#xA;    answer=&#34;National Book Award&#34;&#xA;  ),&#xA;  ...&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Second, define your validation logic, which will express some constraints on the behavior of your program or individual modules. For &lt;code&gt;RAG&lt;/code&gt;, we might express a simple check like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def validate_context_and_answer(example, pred, trace=None):&#xA;    # check the gold label and the predicted answer are the same&#xA;    answer_match = example.answer.lower() == pred.answer.lower()&#xA;&#xA;    # check the predicted answer comes from one of the retrieved contexts&#xA;    context_match = any((pred.answer.lower() in c) for c in pred.context)&#xA;&#xA;    return answer_match and context_match&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Different teleprompters offer various tradeoffs in terms of how much they optimize cost versus quality, etc. For &lt;code&gt;RAG&lt;/code&gt;, we might use the simple teleprompter called &lt;code&gt;BootstrapFewShot&lt;/code&gt;. To do so, we instantiate the teleprompter itself with a validation function &lt;code&gt;my_rag_validation_logic&lt;/code&gt; and then compile against some training set &lt;code&gt;my_rag_trainset&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from dspy.teleprompt import BootstrapFewShot&#xA;&#xA;teleprompter = BootstrapFewShot(metric=my_rag_validation_logic)&#xA;compiled_rag = teleprompter.compile(RAG(), trainset=my_rag_trainset)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If we now use &lt;code&gt;compiled_rag&lt;/code&gt;, it will invoke our LM with rich prompts with few-shot demonstrations of chain-of-thought retrieval-augmented question answering on our data.&lt;/p&gt; &#xA;&lt;h2&gt;4) Documentation &amp;amp; Tutorials&lt;/h2&gt; &#xA;&lt;p&gt;While we work on new tutorials and documentation, please check out &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stanfordnlp/dspy/main/intro.ipynb&#34;&gt;our intro notebook&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Or open it directly in free Google Colab: &lt;a href=&#34;https://colab.research.google.com/github/stanfordnlp/dspy/blob/main/intro.ipynb&#34;&gt;&lt;img align=&#34;center&#34; src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h3 style=&#34;display: inline&#34;&gt;Intro Tutorial [coming soon]&lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;p&gt;&lt;strong&gt;[Intro-01] Getting Started: High Quality Pipelined Prompts with Minimal Effort&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;[Intro-02] Using DSPy For Your Own Task: Building Blocks&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;[Intro-03] Adding Complexity: Multi-stage Programs&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;[Intro-04] Adding Complexity for Your Own Task: Design Patterns&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h3 style=&#34;display: inline&#34;&gt;Advanced Demos [coming soon]&lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;p&gt;&lt;strong&gt;[Advanced-01] Long-Form QA &amp;amp; Programmatic Evaluation.&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;[Advanced-02] Programmatic Evaluation II &amp;amp; Dataset Creation.&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;[Advanced-03] Compiling &amp;amp; Teleprompters.&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;[Advanced-04] Extending DSPy with Modules or Teleprompters.&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;[Advanced-05]: Agents and General Tool Use in DSPy.&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;[Advanced-06]: Reproducibility, Saving Programs, and Advanced Caching.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h3 style=&#34;display: inline&#34;&gt;Module Reference [coming soon]&lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;h4&gt;Language Model Clients&lt;/h4&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;dspy.OpenAI&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;dspy.Cohere&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;dspy.TGI&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;dspy.VLLM&lt;/code&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h4&gt;Retrieval Model Clients&lt;/h4&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;dspy.ColBERTv2&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;dspy.AzureCognitiveSearch&lt;/code&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h4&gt;Signatures&lt;/h4&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;dspy.Signature&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;dspy.InputField&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;dspy.OutputField&lt;/code&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h4&gt;Modules&lt;/h4&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;dspy.Predict&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;dspy.Retrieve&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;dspy.ChainOfThought&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;dspy.SelfConsistency&lt;/code&gt; [use functional &lt;code&gt;dspy.majority&lt;/code&gt; now]&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;dspy.MultiChainReasoning&lt;/code&gt; [coming soon]&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;dspy.SelfCritique&lt;/code&gt; [coming soon]&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;dspy.SelfRevision&lt;/code&gt; [coming soon]&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h4&gt;Teleprompters&lt;/h4&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;dspy.teleprompt.LabeledFewShot&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;dspy.teleprompt.BootstrapFewShot&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;dspy.teleprompt.BootstrapFewShotWithRandomSearch&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;dspy.teleprompt.BootstrapFinetune&lt;/code&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;5) FAQ: Is DSPy right for me?&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;strong&gt;DSPy&lt;/strong&gt; philosophy and abstraction differ significantly from other libraries and frameworks, so it&#39;s usually straightforward to decide when &lt;strong&gt;DSPy&lt;/strong&gt; is (or isn&#39;t) the right framework for your usecase.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;re a NLP/AI researcher (or a practitioner exploring new pipelines or new tasks), the answer is generally an invariable &lt;strong&gt;yes&lt;/strong&gt;. If you&#39;re a practitioner doing other things, please read on.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;/h4&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h4 style=&#34;display: inline&#34;&gt;[5.a] DSPy vs. thin wrappers around prompts (OpenAI API, MiniChain, basic templating, etc.)&lt;/h4&gt;&lt;/summary&gt; &#xA; &lt;p&gt;In other words: &lt;em&gt;Why can&#39;t I just write my prompts directly as string templates?&lt;/em&gt; Well, for extremely simple settings, this &lt;em&gt;might&lt;/em&gt; work just fine. (If you&#39;re familiar with neural networks, this is like expressing a tiny two-layer NN as a Python for-loop. It kinda works.)&lt;/p&gt; &#xA; &lt;p&gt;However, when you need higher quality (or manageable cost), then you need to iteratively explore multi-stage decomposition, improved prompting, data bootstrapping, careful finetuning, retrieval augmentation, and/or using smaller (or cheaper, or local) models. The true expressive power of building with foundation models lies in the interactions between these pieces. But every time you change one piece, you likely break (or weaken) multiple other components.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;DSPy&lt;/strong&gt; cleanly abstracts away (&lt;em&gt;and&lt;/em&gt; powerfully optimizes) the parts of these interactions that are external to your actual system design. It lets you focus on designing the module-level interactions: the &lt;em&gt;same program&lt;/em&gt; expressed in 10 or 20 lines of &lt;strong&gt;DSPy&lt;/strong&gt; can easily be compiled into multi-stage instructions for &lt;code&gt;GPT-4&lt;/code&gt;, detailed prompts for &lt;code&gt;Llama2-13b&lt;/code&gt;, or finetunes for &lt;code&gt;T5-base&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;p&gt;Oh, and you wouldn&#39;t need to maintain long, brittle, model-specific strings at the core of your project anymore.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;&lt;/h4&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h4 style=&#34;display: inline&#34;&gt;[5.b] DSPy vs. application development libraries like LangChain, LlamaIndex&lt;/h4&gt;&lt;/summary&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;&lt;em&gt;Note: If you use LangChain as a thin wrapper around your own prompt strings, refer to answer [5.a] instead.&lt;/em&gt;&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;p&gt;LangChain and LlamaIndex are popular libraries that target high-level application development with LMs. They offer many &lt;em&gt;batteries-included&lt;/em&gt;, pre-built application modules that plug in with your data or configuration. In practice, indeed, many usecases genuinely &lt;em&gt;don&#39;t need&lt;/em&gt; any special components. If you&#39;d be happy to use someone&#39;s generic, off-the-shelf prompt for question answering over PDFs or standard text-to-SQL as long as it&#39;s easy to set up on your data, then you will probably find a very rich ecosystem in these libraries.&lt;/p&gt; &#xA; &lt;p&gt;Unlike these libraries, &lt;strong&gt;DSPy&lt;/strong&gt; doesn&#39;t internally contain hand-crafted prompts that target specific applications you can build. Instead, &lt;strong&gt;DSPy&lt;/strong&gt; introduces a very small set of much more powerful and general-purpose modules &lt;em&gt;that can learn to prompt (or finetune) your LM within your pipeline on your data&lt;/em&gt;.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;DSPy&lt;/strong&gt; offers a whole different degree of modularity: when you change your data, make tweaks to your program&#39;s control flow, or change your target LM, the &lt;strong&gt;DSPy compiler&lt;/strong&gt; can map your program into a new set of prompts (or finetunes) that are optimized specifically for this pipeline. Because of this, you may find that &lt;strong&gt;DSPy&lt;/strong&gt; obtains the highest quality for your task, with the least effort, provided you&#39;re willing to implement (or extend) your own short program. In short, &lt;strong&gt;DSPy&lt;/strong&gt; is for when you need a lightweight but automatically-optimizing programming model â€” not a library of predefined prompts and integrations.&lt;/p&gt; &#xA; &lt;p&gt;If you&#39;re familiar with neural networks:&lt;/p&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;This is like the difference between PyTorch (i.e., representing &lt;strong&gt;DSPy&lt;/strong&gt;) and HuggingFace Transformers (i.e., representing the higher-level libraries). If you simply want to use off-the-shelf &lt;code&gt;BERT-base-uncased&lt;/code&gt; or &lt;code&gt;GPT2-large&lt;/code&gt; or apply minimal finetuning to them, HF Transformers makes it very straightforward. If, however, you&#39;re looking to build your own architecture (or extend an existing one significantly), you have to quickly drop down into something much more modular like PyTorch. Luckily, HF Transformers &lt;em&gt;is&lt;/em&gt; implemented in backends like PyTorch. We are similarly excited about high-level wrapper around &lt;strong&gt;DSPy&lt;/strong&gt; for common applications. If this is implemented using &lt;strong&gt;DSPy&lt;/strong&gt;, your high-level application can also adapt significantly to your data in a way that static prompt chains won&#39;t. Please &lt;a href=&#34;https://github.com/stanfordnlp/dspy/issues/new&#34;&gt;open an issue&lt;/a&gt; if this is something you want to help with.&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;&lt;/h4&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h4 style=&#34;display: inline&#34;&gt;[5.c] DSPy vs. generation control libraries like Guidance, LMQL, RELM, Outlines&lt;/h4&gt;&lt;/summary&gt; &#xA; &lt;p&gt;Guidance, LMQL, RELM, and Outlines are all exciting new libraries for controlling the individual completions of LMs, e.g., if you want to enforce JSON output schema or constrain sampling to a particular regular expression.&lt;/p&gt; &#xA; &lt;p&gt;This is very useful in many settings, but it&#39;s generally focused on low-level, structured control of a single LM call. It doesn&#39;t help ensure the JSON (or structured output) you get is going to be correct or useful for your task.&lt;/p&gt; &#xA; &lt;p&gt;In contrast, &lt;strong&gt;DSPy&lt;/strong&gt; automatically optimizes the prompts in your programs to align them with various task needs, which may also include producing valid structured ouputs. That said, we are considering allowing &lt;strong&gt;Signatures&lt;/strong&gt; in &lt;strong&gt;DSPy&lt;/strong&gt; to express regex-like constraints that are implemented by these libraries.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Contributors &amp;amp; Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;DSPy&lt;/strong&gt; is led by &lt;strong&gt;Omar Khattab&lt;/strong&gt; at Stanford NLP with &lt;strong&gt;Chris Potts&lt;/strong&gt; and &lt;strong&gt;Matei Zaharia&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Key contributors and team members include &lt;strong&gt;Arnav Singhvi&lt;/strong&gt;, &lt;strong&gt;Paridhi Maheshwari&lt;/strong&gt;, &lt;strong&gt;Keshav Santhanam&lt;/strong&gt;, &lt;strong&gt;Sri Vardhamanan&lt;/strong&gt;, &lt;strong&gt;Eric Zhang&lt;/strong&gt;, &lt;strong&gt;Hanna Moazam&lt;/strong&gt;, and &lt;strong&gt;Thomas Joshi&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DSPy&lt;/strong&gt; includes important contributions from &lt;strong&gt;Rick Battle&lt;/strong&gt; and &lt;strong&gt;Igor Kotenkov&lt;/strong&gt;. It reflects discussions with &lt;strong&gt;Lisa Li&lt;/strong&gt;, &lt;strong&gt;David Hall&lt;/strong&gt;, &lt;strong&gt;Ashwin Paranjape&lt;/strong&gt;, &lt;strong&gt;Heather Miller&lt;/strong&gt;, &lt;strong&gt;Chris Manning&lt;/strong&gt;, &lt;strong&gt;Percy Liang&lt;/strong&gt;, and many others.&lt;/p&gt; &#xA;&lt;h2&gt;ðŸ“œ Citation &amp;amp; Reading More&lt;/h2&gt; &#xA;&lt;p&gt;To stay up to date or learn more, follow &lt;a href=&#34;https://twitter.com/lateinteraction&#34;&gt;@lateinteraction&lt;/a&gt; on Twitter.&lt;/p&gt; &#xA;&lt;p&gt;If you use DSPy (or DSPv1) in a research paper, please cite our work as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{khattab2022demonstrate,&#xA;  title={Demonstrate-Search-Predict: Composing Retrieval and Language Models for Knowledge-Intensive {NLP}},&#xA;  author={Khattab, Omar and Santhanam, Keshav and Li, Xiang Lisa and Hall, David and Liang, Percy and Potts, Christopher and Zaharia, Matei},&#xA;  journal={arXiv preprint arXiv:2212.14024},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also read more about the old v1 of our framework (Demonstrateâ€“Searchâ€“Predict, or DSP):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2212.14024.pdf&#34;&gt;&lt;strong&gt;Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP&lt;/strong&gt;&lt;/a&gt; (Academic Paper, Dec 2022)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/lateinteraction/status/1617953413576425472&#34;&gt;&lt;strong&gt;Introducing DSP&lt;/strong&gt;&lt;/a&gt; (Twitter Thread, Jan 2023)&lt;/li&gt; &#xA; &lt;li&gt;Thread &lt;a href=&#34;https://twitter.com/lateinteraction/status/1625231662849073160&#34;&gt;&lt;strong&gt;Releasing the DSP Compiler (v0.1)&lt;/strong&gt;&lt;/a&gt; (Twitter Thread, Feb 2023)&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>