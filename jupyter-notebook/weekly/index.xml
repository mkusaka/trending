<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-05-04T01:41:54Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>facebookresearch/audiocraft</title>
    <updated>2025-05-04T01:41:54Z</updated>
    <id>tag:github.com,2025-05-04:/facebookresearch/audiocraft</id>
    <link href="https://github.com/facebookresearch/audiocraft" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Audiocraft is a library for audio processing and generation with deep learning. It features the state-of-the-art EnCodec audio compressor / tokenizer, along with MusicGen, a simple and controllable music generation LM with textual and melodic conditioning.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AudioCraft&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/facebookresearch/audiocraft/workflows/audiocraft_docs/badge.svg?sanitize=true&#34; alt=&#34;docs badge&#34;&gt; &lt;img src=&#34;https://github.com/facebookresearch/audiocraft/workflows/audiocraft_linter/badge.svg?sanitize=true&#34; alt=&#34;linter badge&#34;&gt; &lt;img src=&#34;https://github.com/facebookresearch/audiocraft/workflows/audiocraft_tests/badge.svg?sanitize=true&#34; alt=&#34;tests badge&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;AudioCraft is a PyTorch library for deep learning research on audio generation. AudioCraft contains inference and training code for two state-of-the-art AI generative models producing high-quality audio: AudioGen and MusicGen.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;AudioCraft requires Python 3.9, PyTorch 2.1.0. To install AudioCraft, you can run the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Best to make sure you have torch installed first, in particular before installing xformers.&#xA;# Don&#39;t run this if you already have PyTorch installed.&#xA;python -m pip install &#39;torch==2.1.0&#39;&#xA;# You might need the following before trying to install the packages&#xA;python -m pip install setuptools wheel&#xA;# Then proceed to one of the following&#xA;python -m pip install -U audiocraft  # stable release&#xA;python -m pip install -U git+https://git@github.com/facebookresearch/audiocraft#egg=audiocraft  # bleeding edge&#xA;python -m pip install -e .  # or if you cloned the repo locally (mandatory if you want to train).&#xA;python -m pip install -e &#39;.[wm]&#39;  # if you want to train a watermarking model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We also recommend having &lt;code&gt;ffmpeg&lt;/code&gt; installed, either through your system or Anaconda:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt-get install ffmpeg&#xA;# Or if you are using Anaconda or Miniconda&#xA;conda install &#34;ffmpeg&amp;lt;5&#34; -c conda-forge&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Models&lt;/h2&gt; &#xA;&lt;p&gt;At the moment, AudioCraft contains the training code and inference code for:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/docs/MUSICGEN.md&#34;&gt;MusicGen&lt;/a&gt;: A state-of-the-art controllable text-to-music model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/docs/AUDIOGEN.md&#34;&gt;AudioGen&lt;/a&gt;: A state-of-the-art text-to-sound model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/docs/ENCODEC.md&#34;&gt;EnCodec&lt;/a&gt;: A state-of-the-art high fidelity neural audio codec.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/docs/MBD.md&#34;&gt;Multi Band Diffusion&lt;/a&gt;: An EnCodec compatible decoder using diffusion.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/docs/MAGNET.md&#34;&gt;MAGNeT&lt;/a&gt;: A state-of-the-art non-autoregressive model for text-to-music and text-to-sound.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/docs/WATERMARKING.md&#34;&gt;AudioSeal&lt;/a&gt;: A state-of-the-art audio watermarking.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/docs/MUSICGEN_STYLE.md&#34;&gt;MusicGen Style&lt;/a&gt;: A state-of-the-art text-and-style-to-music model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/docs/JASCO.md&#34;&gt;JASCO&lt;/a&gt;: &#34;High quality text-to-music model conditioned on chords, melodies and drum tracks&#34;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Training code&lt;/h2&gt; &#xA;&lt;p&gt;AudioCraft contains PyTorch components for deep learning research in audio and training pipelines for the developed models. For a general introduction of AudioCraft design principles and instructions to develop your own training pipeline, refer to the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/docs/TRAINING.md&#34;&gt;AudioCraft training documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For reproducing existing work and using the developed training pipelines, refer to the instructions for each specific model that provides pointers to configuration, example grids and model/task-specific information and FAQ.&lt;/p&gt; &#xA;&lt;h2&gt;API documentation&lt;/h2&gt; &#xA;&lt;p&gt;We provide some &lt;a href=&#34;https://facebookresearch.github.io/audiocraft/api_docs/audiocraft/index.html&#34;&gt;API documentation&lt;/a&gt; for AudioCraft.&lt;/p&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;h4&gt;Is the training code available?&lt;/h4&gt; &#xA;&lt;p&gt;Yes! We provide the training code for &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/docs/ENCODEC.md&#34;&gt;EnCodec&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/docs/MUSICGEN.md&#34;&gt;MusicGen&lt;/a&gt;,&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/docs/MBD.md&#34;&gt;Multi Band Diffusion&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/docs/JASCO.md&#34;&gt;JASCO&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Where are the models stored?&lt;/h4&gt; &#xA;&lt;p&gt;Hugging Face stored the model in a specific location, which can be overridden by setting the &lt;code&gt;AUDIOCRAFT_CACHE_DIR&lt;/code&gt; environment variable for the AudioCraft models. In order to change the cache location of the other Hugging Face models, please check out the &lt;a href=&#34;https://huggingface.co/docs/transformers/installation#cache-setup&#34;&gt;Hugging Face Transformers documentation for the cache setup&lt;/a&gt;. Finally, if you use a model that relies on Demucs (e.g. &lt;code&gt;musicgen-melody&lt;/code&gt;) and want to change the download location for Demucs, refer to the &lt;a href=&#34;https://pytorch.org/docs/stable/hub.html#where-are-my-downloaded-models-saved&#34;&gt;Torch Hub documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The code in this repository is released under the MIT license as found in the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/LICENSE&#34;&gt;LICENSE file&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The models weights in this repository are released under the CC-BY-NC 4.0 license as found in the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/LICENSE_weights&#34;&gt;LICENSE_weights file&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;For the general framework of AudioCraft, please cite the following.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{copet2023simple,&#xA;    title={Simple and Controllable Music Generation},&#xA;    author={Jade Copet and Felix Kreuk and Itai Gat and Tal Remez and David Kant and Gabriel Synnaeve and Yossi Adi and Alexandre DÃ©fossez},&#xA;    booktitle={Thirty-seventh Conference on Neural Information Processing Systems},&#xA;    year={2023},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When referring to a specific model, please cite as mentioned in the model specific README, e.g &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/docs/MUSICGEN.md&#34;&gt;./docs/MUSICGEN.md&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/docs/AUDIOGEN.md&#34;&gt;./docs/AUDIOGEN.md&lt;/a&gt;, etc.&lt;/p&gt;</summary>
  </entry>
</feed>