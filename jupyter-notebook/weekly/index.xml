<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-19T01:40:14Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>SynodicMonth/ChebyKAN</title>
    <updated>2024-05-19T01:40:14Z</updated>
    <id>tag:github.com,2024-05-19:/SynodicMonth/ChebyKAN</id>
    <link href="https://github.com/SynodicMonth/ChebyKAN" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Kolmogorov-Arnold Networks (KAN) using Chebyshev polynomials instead of B-splines.&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;This is a VERY COARSE version and absolutely NOT FULLY TESTED! it&#39;s only intended for experiementing! Any discussion and criticism are welcome! Check the issues for more information!&lt;/h2&gt; &#xA;&lt;h1&gt;ChebyKAN&lt;/h1&gt; &#xA;&lt;p&gt;Kolmogorov-Arnold Networks (KAN) using Chebyshev polynomials instead of B-splines.&lt;/p&gt; &#xA;&lt;p&gt;This is inspired by Kolmogorov-Arnold Networks &lt;a href=&#34;https://arxiv.org/abs/2404.19756v2&#34;&gt;https://arxiv.org/abs/2404.19756v2&lt;/a&gt;, which uses B-splines to approximate functions. B-splines are poor in performance and not very intuitive to use. I&#39;m trying to replace B-splines with Chebyshev polynomials.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Chebyshev_polynomials&#34;&gt;Chebyshev polynomials&lt;/a&gt; are orthogonal polynomials defined on the interval [-1, 1]. They are very good at approximating functions and can be calculated recursively.&lt;/p&gt; &#xA;&lt;p&gt;A simple (and naive) implementation of ChebyKANLayer is provided in &lt;code&gt;chebyKANLayer_unoptimized.py&lt;/code&gt;. Its reserved for a brief understanding.&lt;/p&gt; &#xA;&lt;p&gt;Thanks @JanRocketMan for proving ChebyKAN = Linear + custom activation function. (see issue #3 for more information) Thanks @iiisak and @K-H-Ismail for providing an optimized version by replace recurrent definition with trigonometric definition and vectorization. The optimized version is in &lt;code&gt;ChebyKANLayer.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;p&gt;Just copy &lt;code&gt;ChebyKANLayer.py&lt;/code&gt; to your project and import it.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from ChebyKANLayer import ChebyKANLayer&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Example&lt;/h1&gt; &#xA;&lt;p&gt;Construct a ChebyKAN for MNIST&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class MNISTChebyKAN(nn.Module):&#xA;    def __init__(self):&#xA;        super(MNISTChebyKAN, self).__init__()&#xA;        self.chebykan1 = ChebyKANLayer(28*28, 32, 4)&#xA;        self.ln1 = nn.LayerNorm(32) # To avoid gradient vanishing caused by tanh&#xA;        self.chebykan2 = ChebyKANLayer(32, 16, 4)&#xA;        self.ln2 = nn.LayerNorm(16)&#xA;        self.chebykan3 = ChebyKANLayer(16, 10, 4)&#xA;&#xA;    def forward(self, x):&#xA;        x = x.view(-1, 28*28)  # Flatten the images&#xA;        x = self.chebykan1(x)&#xA;        x = self.ln1(x)&#xA;        x = self.chebykan2(x)&#xA;        x = self.ln2(x)&#xA;        x = self.chebykan3(x)&#xA;        return x&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Since Chebyshev polynomials are defined on the interval [-1, 1], we need to use tanh to keep the input in that range. We also use LayerNorm to avoid gradient vanishing caused by tanh. Removing LayerNorm will cause the network really hard to train.&lt;/p&gt; &#xA;&lt;p&gt;Have a look at &lt;code&gt;Cheby-KAN_MNIST.ipynb&lt;/code&gt;, &lt;code&gt;Function_Interpolation_Test.ipynb&lt;/code&gt;, and &lt;code&gt;Multivar_Interpolation_Test.ipynb&lt;/code&gt; for more examples.&lt;/p&gt; &#xA;&lt;h1&gt;Experiment Results&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;MNIST:&lt;/strong&gt; ~97% accuracy after about 20 epochs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Epoch 1, Train Loss: 1.1218, Test Loss: 0.4689, Test Acc: 0.91&#xA;Epoch 2, Train Loss: 0.3302, Test Loss: 0.2599, Test Acc: 0.93&#xA;Epoch 3, Train Loss: 0.2170, Test Loss: 0.2359, Test Acc: 0.94&#xA;Epoch 4, Train Loss: 0.1696, Test Loss: 0.1857, Test Acc: 0.95&#xA;Epoch 5, Train Loss: 0.1422, Test Loss: 0.1574, Test Acc: 0.96&#xA;Epoch 6, Train Loss: 0.1241, Test Loss: 0.1597, Test Acc: 0.95&#xA;Epoch 7, Train Loss: 0.1052, Test Loss: 0.1475, Test Acc: 0.96&#xA;Epoch 8, Train Loss: 0.0932, Test Loss: 0.1321, Test Acc: 0.96&#xA;Epoch 9, Train Loss: 0.0879, Test Loss: 0.1553, Test Acc: 0.95&#xA;Epoch 10, Train Loss: 0.0780, Test Loss: 0.1239, Test Acc: 0.96&#xA;Epoch 11, Train Loss: 0.0722, Test Loss: 0.1283, Test Acc: 0.96&#xA;Epoch 12, Train Loss: 0.0629, Test Loss: 0.1236, Test Acc: 0.96&#xA;Epoch 13, Train Loss: 0.0612, Test Loss: 0.1271, Test Acc: 0.96&#xA;Epoch 14, Train Loss: 0.0521, Test Loss: 0.1390, Test Acc: 0.96&#xA;Epoch 15, Train Loss: 0.0488, Test Loss: 0.1374, Test Acc: 0.96&#xA;Epoch 16, Train Loss: 0.0487, Test Loss: 0.1309, Test Acc: 0.96&#xA;Epoch 17, Train Loss: 0.0416, Test Loss: 0.1253, Test Acc: 0.96&#xA;Epoch 18, Train Loss: 0.0402, Test Loss: 0.1346, Test Acc: 0.96&#xA;Epoch 19, Train Loss: 0.0373, Test Loss: 0.1199, Test Acc: 0.97&#xA;Epoch 20, Train Loss: 0.0346, Test Loss: 0.1434, Test Acc: 0.96&#xA;Epoch 21, Train Loss: 0.0314, Test Loss: 0.1142, Test Acc: 0.97&#xA;Epoch 22, Train Loss: 0.0285, Test Loss: 0.1258, Test Acc: 0.97&#xA;Epoch 23, Train Loss: 0.0289, Test Loss: 0.1192, Test Acc: 0.97&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/SynodicMonth/ChebyKAN/main/img/MNIST.png&#34; alt=&#34;MNIST&#34;&gt; The network parameters are [28*28, 32, 16, 10] with 4 degree Chebyshev polynomials.&lt;/p&gt; &#xA;&lt;p&gt;It needs a low learning rate (2e-4) to train. The network is very sensitive to the learning rate.&lt;/p&gt; &#xA;&lt;p&gt;Note that it&#39;s still not as good as MLPs. Detailed comparison is on the way.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;del&gt;&lt;strong&gt;Function Interpolation:&lt;/strong&gt; much better than MLPs when the function is (mostly) smooth, very effective in discovering mathematical laws.&lt;/del&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/SynodicMonth/ChebyKAN/main/img/Interpolation.png&#34; alt=&#34;alt text&#34;&gt; &lt;del&gt;ChebyKAN: [1, 8, 1] with 8 degree.&lt;/del&gt; &lt;del&gt;MLP: [1, 1024, 512, 1] with ReLU&lt;/del&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Edit: The comparison above is not fair.&lt;/strong&gt; &lt;strong&gt;Thanks @usamec for pointing out the mistake that the MLP was too big and not trained properly.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;!-- ~~Edit: Adding noise to the data does not affect the ChebyKAN&#39;s performance.~~ --&gt; &#xA;&lt;!-- ![alt text](img/Interpolation_noise.png) --&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fixed version:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Function Interpolation:&lt;/strong&gt; converge faster than MLPs when the function is (mostly) smooth.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/SynodicMonth/ChebyKAN/main/img/Interpolation_fix.png&#34; alt=&#34;alt text&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;ChebyKAN: [1, 8, 1] with 8 degree. MLP: [1, 128, 1] with Tanh.&lt;/p&gt; &#xA;&lt;p&gt;With decent training, the MLP can achieve similar performance as ChebyKAN. Note that ChebyKAN shows some overfitting.&lt;/p&gt; &#xA;&lt;p&gt;However ChebyKAN converges much faster than MLP.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/SynodicMonth/ChebyKAN/main/img/Convergence_Speed.png&#34; alt=&#34;alt text&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;ChebyKAN: Adam, lr=0.01. MLP: Adam, lr=0.03.&lt;/p&gt; &#xA;&lt;p&gt;@5000 epoch, ChebyKAN has already converged, while MLP is still far from convergence.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/SynodicMonth/ChebyKAN/main/img/Early_Stopping.png&#34; alt=&#34;alt text&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Future Work&lt;/h1&gt; &#xA;&lt;p&gt;More experiments and optimizations are needed to prove the correctness and effectiveness of ChebyKAN. Not sure if the current parameters initialization is optimal. Maybe Xavier initialization is better. I&#39;m not sure if the current implementation is correct. Any suggestions are welcome.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>aamini/introtodeeplearning</title>
    <updated>2024-05-19T01:40:14Z</updated>
    <id>tag:github.com,2024-05-19:/aamini/introtodeeplearning</id>
    <link href="https://github.com/aamini/introtodeeplearning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Lab Materials for MIT 6.S191: Introduction to Deep Learning&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;http://introtodeeplearning.com&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aamini/introtodeeplearning/master/assets/banner.png&#34; alt=&#34;banner&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository contains all of the code and software labs for &lt;a href=&#34;http://introtodeeplearning.com&#34;&gt;MIT Introduction to Deep Learning&lt;/a&gt;! All lecture slides and videos are available on the program website.&lt;/p&gt; &#xA;&lt;h1&gt;Instructions&lt;/h1&gt; &#xA;&lt;p&gt;MIT Introduction to Deep Learning software labs are designed to be completed at your own pace. At the end of each of the labs, there will be instructions on how you can submit your materials as part of the lab competitions. These instructions include what information must be submitted and in what format.&lt;/p&gt; &#xA;&lt;h2&gt;Opening the labs in Google Colaboratory:&lt;/h2&gt; &#xA;&lt;p&gt;The 2023 Introduction to Deep Learning labs will be run in Google&#39;s Colaboratory, a Jupyter notebook environment that runs entirely in the cloud, so you don&#39;t need to download anything. To run these labs, you must have a Google account.&lt;/p&gt; &#xA;&lt;p&gt;On this Github repo, navigate to the lab folder you want to run (&lt;code&gt;lab1&lt;/code&gt;, &lt;code&gt;lab2&lt;/code&gt;, &lt;code&gt;lab3&lt;/code&gt;) and open the appropriate python notebook (*.ipynb). Click the &#34;Run in Colab&#34; link on the top of the lab. That&#39;s it!&lt;/p&gt; &#xA;&lt;h2&gt;Running the labs&lt;/h2&gt; &#xA;&lt;p&gt;Now, to run the labs, open the Jupyter notebook on Colab. Navigate to the &#34;Runtime&#34; tab --&amp;gt; &#34;Change runtime type&#34;. In the pop-up window, under &#34;Runtime type&#34; select &#34;Python 3&#34;, and under &#34;Hardware accelerator&#34; select &#34;GPU&#34;. Go through the notebooks and fill in the &lt;code&gt;#TODO&lt;/code&gt; cells to get the code to compile for yourself!&lt;/p&gt; &#xA;&lt;h3&gt;MIT Deep Learning package&lt;/h3&gt; &#xA;&lt;p&gt;You might notice that inside the labs we install the &lt;code&gt;mitdeeplearning&lt;/code&gt; python package from the Python Package repository:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install mitdeeplearning&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;This package contains convienence functions that we use throughout the course and can be imported like any other Python package.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import mitdeeplearning as mdl&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;We do this for you in each of the labs, but the package is also open source under the same license so you can also use it outside the class.&lt;/p&gt; &#xA;&lt;h2&gt;Lecture Videos&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=njKP3FqW3Sk&amp;amp;list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&amp;amp;index=1&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aamini/introtodeeplearning/master/assets/video_play.png&#34; width=&#34;500&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;All lecture videos are available publicly online and linked above! Use and/or modification of lecture slides outside of MIT Introduction to Deep Learning must reference:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;¬© MIT Introduction to Deep Learning&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;http://introtodeeplearning.com&#34;&gt;http://introtodeeplearning.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;All code in this repository is copyright 2023 &lt;a href=&#34;http://introtodeeplearning.com&#34;&gt;MIT Introduction to Deep Learning&lt;/a&gt;. All Rights Reserved.&lt;/p&gt; &#xA;&lt;p&gt;Licensed under the MIT License. You may not use this file except in compliance with the License. Use and/or modification of this code outside of MIT Introduction to Deep Learning must reference:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;¬© MIT Introduction to Deep Learning&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;http://introtodeeplearning.com&#34;&gt;http://introtodeeplearning.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt;</summary>
  </entry>
  <entry>
    <title>advimman/lama</title>
    <updated>2024-05-19T01:40:14Z</updated>
    <id>tag:github.com,2024-05-19:/advimman/lama</id>
    <link href="https://github.com/advimman/lama" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ü¶ô LaMa Image Inpainting, Resolution-robust Large Mask Inpainting with Fourier Convolutions, WACV 2022&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ü¶ô LaMa: Resolution-robust Large Mask Inpainting with Fourier Convolutions&lt;/h1&gt; &#xA;&lt;p&gt;by Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, Victor Lempitsky.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34; &#34;font-size:30px;&#34;&gt; üî•üî•üî• &lt;br&gt; &lt;b&gt; LaMa generalizes surprisingly well to much higher resolutions (~2k‚ùóÔ∏è) than it saw during training (256x256), and achieves the excellent performance even in challenging scenarios, e.g. completion of periodic structures.&lt;/b&gt; &lt;/p&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://advimman.github.io/lama-project/&#34;&gt;Project page&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/2109.07161&#34;&gt;arXiv&lt;/a&gt;] [&lt;a href=&#34;https://ashukha.com/projects/lama_21/lama_supmat_2021.pdf&#34;&gt;Supplementary&lt;/a&gt;] [&lt;a href=&#34;https://senya-ashukha.github.io/projects/lama_21/paper.txt&#34;&gt;BibTeX&lt;/a&gt;] [&lt;a href=&#34;https://www.casualganpapers.com/large-masks-fourier-convolutions-inpainting/LaMa-explained.html&#34;&gt;Casual GAN Papers Summary&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://colab.research.google.com/drive/15KTEIScUbVZtUP6w2tCDMVpE-b1r9pkZ?usp=drive_link&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;br&gt; Try out in Google Colab &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/senya-ashukha/senya-ashukha.github.io/master/projects/lama_21/ezgif-4-0db51df695a8.gif&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/senya-ashukha/senya-ashukha.github.io/master/projects/lama_21/gif_for_lightning_v1_white.gif&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;LaMa development&lt;/h1&gt; &#xA;&lt;p&gt;(Feel free to share your paper by creating an issue)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/geekyutao/Inpaint-Anything&#34;&gt;https://github.com/geekyutao/Inpaint-Anything&lt;/a&gt; --- Inpaint Anything: Segment Anything Meets Image Inpainting&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/geekyutao/Inpaint-Anything/main/example/MainFramework.png&#34;&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2206.13644&#34;&gt;Feature Refinement to Improve High Resolution Image Inpainting&lt;/a&gt; / &lt;a href=&#34;https://www.youtube.com/watch?v=gEukhOheWgE&#34;&gt;video&lt;/a&gt; / code &lt;a href=&#34;https://github.com/advimman/lama/pull/112&#34;&gt;https://github.com/advimman/lama/pull/112&lt;/a&gt; / by Geomagical Labs (&lt;a href=&#34;https://raw.githubusercontent.com/advimman/lama/main/geomagical.com&#34;&gt;geomagical.com&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/senya-ashukha/senya-ashukha.github.io/master/images/FeatureRefinement.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Non-official 3rd party apps:&lt;/h1&gt; &#xA;&lt;p&gt;(Feel free to share your app/implementation/demo by creating an issue)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/enesmsahin/simple-lama-inpainting&#34;&gt;https://github.com/enesmsahin/simple-lama-inpainting&lt;/a&gt; - a simple pip package for LaMa inpainting.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mallman/CoreMLaMa&#34;&gt;https://github.com/mallman/CoreMLaMa&lt;/a&gt; - Apple&#39;s Core ML model format&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cleanup.pictures/&#34;&gt;https://cleanup.pictures&lt;/a&gt; - a simple interactive object removal tool by &lt;a href=&#34;https://twitter.com/cyrildiagne&#34;&gt;@cyrildiagne&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/Sanster/lama-cleaner&#34;&gt;lama-cleaner&lt;/a&gt; by &lt;a href=&#34;https://github.com/Sanster/lama-cleaner&#34;&gt;@Sanster&lt;/a&gt; is a self-host version of &lt;a href=&#34;https://cleanup.pictures/&#34;&gt;https://cleanup.pictures&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Integrated to &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces&lt;/a&gt; with &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;. See demo: &lt;a href=&#34;https://huggingface.co/spaces/akhaliq/lama&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt; by &lt;a href=&#34;https://github.com/AK391&#34;&gt;@AK391&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Telegram bot &lt;a href=&#34;https://t.me/MagicEraserBot&#34;&gt;@MagicEraserBot&lt;/a&gt; by &lt;a href=&#34;https://github.com/Moldoteck&#34;&gt;@Moldoteck&lt;/a&gt;, &lt;a href=&#34;https://github.com/Moldoteck/MagicEraser&#34;&gt;code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/andy971022/auto-lama&#34;&gt;Auto-LaMa&lt;/a&gt; = DE:TR object detection + LaMa inpainting by &lt;a href=&#34;https://github.com/andy971022&#34;&gt;@andy971022&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zhaoyun0071/LAMA-Magic-Eraser-Local&#34;&gt;LAMA-Magic-Eraser-Local&lt;/a&gt; = a standalone inpainting application built with PyQt5 by &lt;a href=&#34;https://github.com/zhaoyun0071&#34;&gt;@zhaoyun0071&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.hama.app/&#34;&gt;Hama&lt;/a&gt; - object removal with a smart brush which simplifies mask drawing.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.modelscope.cn/models/damo/cv_fft_inpainting_lama/summary&#34;&gt;ModelScope&lt;/a&gt; = the largest Model Community in Chinese by &lt;a href=&#34;https://github.com/chenbinghui1&#34;&gt;@chenbinghui1&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/qwopqwop200/lama-with-maskdino&#34;&gt;LaMa with MaskDINO&lt;/a&gt; = MaskDINO object detection + LaMa inpainting with refinement by &lt;a href=&#34;https://github.com/qwopqwop200&#34;&gt;@qwopqwop200&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mallman/CoreMLaMa&#34;&gt;CoreMLaMa&lt;/a&gt; - a script to convert Lama Cleaner&#39;s port of LaMa to Apple&#39;s Core ML model format.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Environment setup&lt;/h1&gt; &#xA;&lt;p&gt;Clone the repo: &lt;code&gt;git clone https://github.com/advimman/lama.git&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;There are three options of an environment:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Python virtualenv:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;virtualenv inpenv --python=/usr/bin/python3&#xA;source inpenv/bin/activate&#xA;pip install torch==1.8.0 torchvision==0.9.0&#xA;&#xA;cd lama&#xA;pip install -r requirements.txt &#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Conda&lt;/p&gt; &lt;pre&gt;&lt;code&gt;% Install conda for Linux, for other OS download miniconda at https://docs.conda.io/en/latest/miniconda.html&#xA;wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh&#xA;bash Miniconda3-latest-Linux-x86_64.sh -b -p $HOME/miniconda&#xA;$HOME/miniconda/bin/conda init bash&#xA;&#xA;cd lama&#xA;conda env create -f conda_env.yml&#xA;conda activate lama&#xA;conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch -y&#xA;pip install pytorch-lightning==1.2.9&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Docker: No actions are needed üéâ.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Inference &lt;a name=&#34;prediction&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;Run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd lama&#xA;export TORCH_HOME=$(pwd) &amp;amp;&amp;amp; export PYTHONPATH=$(pwd)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;1. Download pre-trained models&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The best model (Places2, Places Challenge):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -LJO https://huggingface.co/smartywu/big-lama/resolve/main/big-lama.zip&#xA;unzip big-lama.zip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;All models (Places &amp;amp; CelebA-HQ):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;download [https://drive.google.com/drive/folders/1B2x7eQDgecTL0oh3LSIBDGj0fTxs6Ips?usp=drive_link]&#xA;unzip lama-models.zip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;2. Prepare images and masks&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Download test images:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;unzip LaMa_test_images.zip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;OR prepare your data:&lt;/summary&gt; 1) Create masks named as `[images_name]_maskXXX[image_suffix]`, put images and masks in the same folder. &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;You can use the &lt;a href=&#34;https://github.com/advimman/lama/raw/main/bin/gen_mask_dataset.py&#34;&gt;script&lt;/a&gt; for random masks generation.&lt;/li&gt; &#xA;  &lt;li&gt;Check the format of the files: &lt;pre&gt;&lt;code&gt;image1_mask001.png&#xA;image1.png&#xA;image2_mask001.png&#xA;image2.png&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;Specify &lt;code&gt;image_suffix&lt;/code&gt;, e.g. &lt;code&gt;.png&lt;/code&gt; or &lt;code&gt;.jpg&lt;/code&gt; or &lt;code&gt;_input.jpg&lt;/code&gt; in &lt;code&gt;configs/prediction/default.yaml&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;&lt;strong&gt;3. Predict&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;On the host machine:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 bin/predict.py model.path=$(pwd)/big-lama indir=$(pwd)/LaMa_test_images outdir=$(pwd)/output&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;OR&lt;/strong&gt; in the docker&lt;/p&gt; &#xA;&lt;p&gt;The following command will pull the docker image from Docker Hub and execute the prediction script&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash docker/2_predict.sh $(pwd)/big-lama $(pwd)/LaMa_test_images $(pwd)/output device=cpu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Docker cuda:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash docker/2_predict_with_gpu.sh $(pwd)/big-lama $(pwd)/LaMa_test_images $(pwd)/output&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;4. Predict with Refinement&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;On the host machine:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 bin/predict.py refine=True model.path=$(pwd)/big-lama indir=$(pwd)/LaMa_test_images outdir=$(pwd)/output&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Train and Eval&lt;/h1&gt; &#xA;&lt;p&gt;Make sure you run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd lama&#xA;export TORCH_HOME=$(pwd) &amp;amp;&amp;amp; export PYTHONPATH=$(pwd)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then download models for &lt;em&gt;perceptual loss&lt;/em&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mkdir -p ade20k/ade20k-resnet50dilated-ppm_deepsup/&#xA;wget -P ade20k/ade20k-resnet50dilated-ppm_deepsup/ http://sceneparsing.csail.mit.edu/model/pytorch/ade20k-resnet50dilated-ppm_deepsup/encoder_epoch_20.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Places&lt;/h2&gt; &#xA;&lt;p&gt;‚ö†Ô∏è NB: FID/SSIM/LPIPS metric values for Places that we see in LaMa paper are computed on 30000 images that we produce in evaluation section below. For more details on evaluation data check [&lt;a href=&#34;https://ashukha.com/projects/lama_21/lama_supmat_2021.pdf#subsection.3.1&#34;&gt;Section 3. Dataset splits in Supplementary&lt;/a&gt;] ‚ö†Ô∏è&lt;/p&gt; &#xA;&lt;p&gt;On the host machine:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Download data from http://places2.csail.mit.edu/download.html&#xA;# Places365-Standard: Train(105GB)/Test(19GB)/Val(2.1GB) from High-resolution images section&#xA;wget http://data.csail.mit.edu/places/places365/train_large_places365standard.tar&#xA;wget http://data.csail.mit.edu/places/places365/val_large.tar&#xA;wget http://data.csail.mit.edu/places/places365/test_large.tar&#xA;&#xA;# Unpack train/test/val data and create .yaml config for it&#xA;bash fetch_data/places_standard_train_prepare.sh&#xA;bash fetch_data/places_standard_test_val_prepare.sh&#xA;&#xA;# Sample images for test and viz at the end of epoch&#xA;bash fetch_data/places_standard_test_val_sample.sh&#xA;bash fetch_data/places_standard_test_val_gen_masks.sh&#xA;&#xA;# Run training&#xA;python3 bin/train.py -cn lama-fourier location=places_standard&#xA;&#xA;# To evaluate trained model and report metrics as in our paper&#xA;# we need to sample previously unseen 30k images and generate masks for them&#xA;bash fetch_data/places_standard_evaluation_prepare_data.sh&#xA;&#xA;# Infer model on thick/thin/medium masks in 256 and 512 and run evaluation &#xA;# like this:&#xA;python3 bin/predict.py \&#xA;model.path=$(pwd)/experiments/&amp;lt;user&amp;gt;_&amp;lt;date:time&amp;gt;_lama-fourier_/ \&#xA;indir=$(pwd)/places_standard_dataset/evaluation/random_thick_512/ \&#xA;outdir=$(pwd)/inference/random_thick_512 model.checkpoint=last.ckpt&#xA;&#xA;python3 bin/evaluate_predicts.py \&#xA;$(pwd)/configs/eval2_gpu.yaml \&#xA;$(pwd)/places_standard_dataset/evaluation/random_thick_512/ \&#xA;$(pwd)/inference/random_thick_512 \&#xA;$(pwd)/inference/random_thick_512_metrics.csv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Docker: TODO&lt;/p&gt; &#xA;&lt;h2&gt;CelebA&lt;/h2&gt; &#xA;&lt;p&gt;On the host machine:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Make shure you are in lama folder&#xA;cd lama&#xA;export TORCH_HOME=$(pwd) &amp;amp;&amp;amp; export PYTHONPATH=$(pwd)&#xA;&#xA;# Download CelebA-HQ dataset&#xA;# Download data256x256.zip from https://drive.google.com/drive/folders/11Vz0fqHS2rXDb5pprgTjpD7S2BAJhi1P&#xA;&#xA;# unzip &amp;amp; split into train/test/visualization &amp;amp; create config for it&#xA;bash fetch_data/celebahq_dataset_prepare.sh&#xA;&#xA;# generate masks for test and visual_test at the end of epoch&#xA;bash fetch_data/celebahq_gen_masks.sh&#xA;&#xA;# Run training&#xA;python3 bin/train.py -cn lama-fourier-celeba data.batch_size=10&#xA;&#xA;# Infer model on thick/thin/medium masks in 256 and run evaluation &#xA;# like this:&#xA;python3 bin/predict.py \&#xA;model.path=$(pwd)/experiments/&amp;lt;user&amp;gt;_&amp;lt;date:time&amp;gt;_lama-fourier-celeba_/ \&#xA;indir=$(pwd)/celeba-hq-dataset/visual_test_256/random_thick_256/ \&#xA;outdir=$(pwd)/inference/celeba_random_thick_256 model.checkpoint=last.ckpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Docker: TODO&lt;/p&gt; &#xA;&lt;h2&gt;Places Challenge&lt;/h2&gt; &#xA;&lt;p&gt;On the host machine:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# This script downloads multiple .tar files in parallel and unpacks them&#xA;# Places365-Challenge: Train(476GB) from High-resolution images (to train Big-Lama) &#xA;bash places_challenge_train_download.sh&#xA;&#xA;TODO: prepare&#xA;TODO: train &#xA;TODO: eval&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Docker: TODO&lt;/p&gt; &#xA;&lt;h2&gt;Create your data&lt;/h2&gt; &#xA;&lt;p&gt;Please check bash scripts for data preparation and mask generation from CelebaHQ section, if you stuck at one of the following steps.&lt;/p&gt; &#xA;&lt;p&gt;On the host machine:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Make shure you are in lama folder&#xA;cd lama&#xA;export TORCH_HOME=$(pwd) &amp;amp;&amp;amp; export PYTHONPATH=$(pwd)&#xA;&#xA;# You need to prepare following image folders:&#xA;$ ls my_dataset&#xA;train&#xA;val_source # 2000 or more images&#xA;visual_test_source # 100 or more images&#xA;eval_source # 2000 or more images&#xA;&#xA;# LaMa generates random masks for the train data on the flight,&#xA;# but needs fixed masks for test and visual_test for consistency of evaluation.&#xA;&#xA;# Suppose, we want to evaluate and pick best models &#xA;# on 512x512 val dataset  with thick/thin/medium masks &#xA;# And your images have .jpg extention:&#xA;&#xA;python3 bin/gen_mask_dataset.py \&#xA;$(pwd)/configs/data_gen/random_&amp;lt;size&amp;gt;_512.yaml \ # thick, thin, medium&#xA;my_dataset/val_source/ \&#xA;my_dataset/val/random_&amp;lt;size&amp;gt;_512.yaml \# thick, thin, medium&#xA;--ext jpg&#xA;&#xA;# So the mask generator will: &#xA;# 1. resize and crop val images and save them as .png&#xA;# 2. generate masks&#xA;&#xA;ls my_dataset/val/random_medium_512/&#xA;image1_crop000_mask000.png&#xA;image1_crop000.png&#xA;image2_crop000_mask000.png&#xA;image2_crop000.png&#xA;...&#xA;&#xA;# Generate thick, thin, medium masks for visual_test folder:&#xA;&#xA;python3 bin/gen_mask_dataset.py \&#xA;$(pwd)/configs/data_gen/random_&amp;lt;size&amp;gt;_512.yaml \  #thick, thin, medium&#xA;my_dataset/visual_test_source/ \&#xA;my_dataset/visual_test/random_&amp;lt;size&amp;gt;_512/ \ #thick, thin, medium&#xA;--ext jpg&#xA;&#xA;&#xA;ls my_dataset/visual_test/random_thick_512/&#xA;image1_crop000_mask000.png&#xA;image1_crop000.png&#xA;image2_crop000_mask000.png&#xA;image2_crop000.png&#xA;...&#xA;&#xA;# Same process for eval_source image folder:&#xA;&#xA;python3 bin/gen_mask_dataset.py \&#xA;$(pwd)/configs/data_gen/random_&amp;lt;size&amp;gt;_512.yaml \  #thick, thin, medium&#xA;my_dataset/eval_source/ \&#xA;my_dataset/eval/random_&amp;lt;size&amp;gt;_512/ \ #thick, thin, medium&#xA;--ext jpg&#xA;&#xA;&#xA;&#xA;# Generate location config file which locate these folders:&#xA;&#xA;touch my_dataset.yaml&#xA;echo &#34;data_root_dir: $(pwd)/my_dataset/&#34; &amp;gt;&amp;gt; my_dataset.yaml&#xA;echo &#34;out_root_dir: $(pwd)/experiments/&#34; &amp;gt;&amp;gt; my_dataset.yaml&#xA;echo &#34;tb_dir: $(pwd)/tb_logs/&#34; &amp;gt;&amp;gt; my_dataset.yaml&#xA;mv my_dataset.yaml ${PWD}/configs/training/location/&#xA;&#xA;&#xA;# Check data config for consistency with my_dataset folder structure:&#xA;$ cat ${PWD}/configs/training/data/abl-04-256-mh-dist&#xA;...&#xA;train:&#xA;  indir: ${location.data_root_dir}/train&#xA;  ...&#xA;val:&#xA;  indir: ${location.data_root_dir}/val&#xA;  img_suffix: .png&#xA;visual_test:&#xA;  indir: ${location.data_root_dir}/visual_test&#xA;  img_suffix: .png&#xA;&#xA;&#xA;# Run training&#xA;python3 bin/train.py -cn lama-fourier location=my_dataset data.batch_size=10&#xA;&#xA;# Evaluation: LaMa training procedure picks best few models according to &#xA;# scores on my_dataset/val/ &#xA;&#xA;# To evaluate one of your best models (i.e. at epoch=32) &#xA;# on previously unseen my_dataset/eval do the following &#xA;# for thin, thick and medium:&#xA;&#xA;# infer:&#xA;python3 bin/predict.py \&#xA;model.path=$(pwd)/experiments/&amp;lt;user&amp;gt;_&amp;lt;date:time&amp;gt;_lama-fourier_/ \&#xA;indir=$(pwd)/my_dataset/eval/random_&amp;lt;size&amp;gt;_512/ \&#xA;outdir=$(pwd)/inference/my_dataset/random_&amp;lt;size&amp;gt;_512 \&#xA;model.checkpoint=epoch32.ckpt&#xA;&#xA;# metrics calculation:&#xA;python3 bin/evaluate_predicts.py \&#xA;$(pwd)/configs/eval2_gpu.yaml \&#xA;$(pwd)/my_dataset/eval/random_&amp;lt;size&amp;gt;_512/ \&#xA;$(pwd)/inference/my_dataset/random_&amp;lt;size&amp;gt;_512 \&#xA;$(pwd)/inference/my_dataset/random_&amp;lt;size&amp;gt;_512_metrics.csv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;OR&lt;/strong&gt; in the docker:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;TODO: train&#xA;TODO: eval&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Hints&lt;/h1&gt; &#xA;&lt;h3&gt;Generate different kinds of masks&lt;/h3&gt; &#xA;&lt;p&gt;The following command will execute a script that generates random masks.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash docker/1_generate_masks_from_raw_images.sh \&#xA;    configs/data_gen/random_medium_512.yaml \&#xA;    /directory_with_input_images \&#xA;    /directory_where_to_store_images_and_masks \&#xA;    --ext png&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The test data generation command stores images in the format, which is suitable for &lt;a href=&#34;https://raw.githubusercontent.com/advimman/lama/main/#prediction&#34;&gt;prediction&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The table below describes which configs we used to generate different test sets from the paper. Note that we &lt;em&gt;do not fix a random seed&lt;/em&gt;, so the results will be slightly different each time.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Places 512x512&lt;/th&gt; &#xA;   &lt;th&gt;CelebA 256x256&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Narrow&lt;/td&gt; &#xA;   &lt;td&gt;random_thin_512.yaml&lt;/td&gt; &#xA;   &lt;td&gt;random_thin_256.yaml&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Medium&lt;/td&gt; &#xA;   &lt;td&gt;random_medium_512.yaml&lt;/td&gt; &#xA;   &lt;td&gt;random_medium_256.yaml&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Wide&lt;/td&gt; &#xA;   &lt;td&gt;random_thick_512.yaml&lt;/td&gt; &#xA;   &lt;td&gt;random_thick_256.yaml&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Feel free to change the config path (argument #1) to any other config in &lt;code&gt;configs/data_gen&lt;/code&gt; or adjust config files themselves.&lt;/p&gt; &#xA;&lt;h3&gt;Override parameters in configs&lt;/h3&gt; &#xA;&lt;p&gt;Also you can override parameters in config like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 bin/train.py -cn &amp;lt;config&amp;gt; data.batch_size=10 run_title=my-title&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Where .yaml file extension is omitted&lt;/p&gt; &#xA;&lt;h3&gt;Models options&lt;/h3&gt; &#xA;&lt;p&gt;Config names for models from paper (substitude into the training command):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;* big-lama&#xA;* big-lama-regular&#xA;* lama-fourier&#xA;* lama-regular&#xA;* lama_small_train_masks&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Which are seated in configs/training/folder&lt;/p&gt; &#xA;&lt;h3&gt;Links&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;All the data (models, test images, etc.) &lt;a href=&#34;https://disk.yandex.ru/d/AmdeG-bIjmvSug&#34;&gt;https://disk.yandex.ru/d/AmdeG-bIjmvSug&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Test images from the paper &lt;a href=&#34;https://disk.yandex.ru/d/xKQJZeVRk5vLlQ&#34;&gt;https://disk.yandex.ru/d/xKQJZeVRk5vLlQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;The pre-trained models &lt;a href=&#34;https://disk.yandex.ru/d/EgqaSnLohjuzAg&#34;&gt;https://disk.yandex.ru/d/EgqaSnLohjuzAg&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;The models for perceptual loss &lt;a href=&#34;https://disk.yandex.ru/d/ncVmQlmT_kTemQ&#34;&gt;https://disk.yandex.ru/d/ncVmQlmT_kTemQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Our training logs are available at &lt;a href=&#34;https://disk.yandex.ru/d/9Bt1wNSDS4jDkQ&#34;&gt;https://disk.yandex.ru/d/9Bt1wNSDS4jDkQ&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Training time &amp;amp; resources&lt;/h3&gt; &#xA;&lt;p&gt;TODO&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Segmentation code and models if form &lt;a href=&#34;https://github.com/CSAILVision/semantic-segmentation-pytorch&#34;&gt;CSAILVision&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;LPIPS metric is from &lt;a href=&#34;https://github.com/richzhang/PerceptualSimilarity&#34;&gt;richzhang&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;SSIM is from &lt;a href=&#34;https://github.com/Po-Hsun-Su/pytorch-ssim&#34;&gt;Po-Hsun-Su&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;FID is from &lt;a href=&#34;https://github.com/mseitzer/pytorch-fid&#34;&gt;mseitzer&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you found this code helpful, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{suvorov2021resolution,&#xA;  title={Resolution-robust Large Mask Inpainting with Fourier Convolutions},&#xA;  author={Suvorov, Roman and Logacheva, Elizaveta and Mashikhin, Anton and Remizova, Anastasia and Ashukha, Arsenii and Silvestrov, Aleksei and Kong, Naejin and Goka, Harshith and Park, Kiwoong and Lempitsky, Victor},&#xA;  journal={arXiv preprint arXiv:2109.07161},&#xA;  year={2021}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>