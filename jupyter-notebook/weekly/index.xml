<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-10T01:54:15Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>evo-design/evo</title>
    <updated>2024-03-10T01:54:15Z</updated>
    <id>tag:github.com,2024-03-10:/evo-design/evo</id>
    <link href="https://github.com/evo-design/evo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;DNA foundation modeling from molecular to genome scale&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Evo: DNA foundation modeling from molecular to genome scale&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/evo-design/evo/main/evo.jpg&#34; alt=&#34;Evo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Evo is a biological foundation model capable of long-context modeling and design. Evo uses the &lt;a href=&#34;https://github.com/togethercomputer/stripedhyena&#34;&gt;StripedHyena architecture&lt;/a&gt; to enable modeling of sequences at a single-nucleotide, byte-level resolution with near-linear scaling of compute and memory relative to context length. Evo has 7 billion parameters and is trained on OpenGenome, a prokaryotic whole-genome dataset containing ~300 billion tokens.&lt;/p&gt; &#xA;&lt;p&gt;We describe Evo in the paper &lt;a href=&#34;https://www.biorxiv.org/content/10.1101/2024.02.27.582234v1&#34;&gt;“Sequence modeling and design from molecular to genome scale with Evo”&lt;/a&gt; and in the &lt;a href=&#34;https://arcinstitute.org/news/blog/evo&#34;&gt;accompanying blog post&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We provide the following model checkpoints:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Checkpoint Name&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;evo-1-8k-base&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A model pretrained with 8,192 context. We use this model as the base model for molecular-scale finetuning tasks.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;evo-1-131k-base&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A model pretrained with 131,072 context using &lt;code&gt;evo-1-8k-base&lt;/code&gt; as the base model. We use this model to reason about and generate sequences at the genome scale.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/evo-design/evo/main/#setup&#34;&gt;Setup&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/evo-design/evo/main/#requirements&#34;&gt;Requirements&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/evo-design/evo/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/evo-design/evo/main/#usage&#34;&gt;Usage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/evo-design/evo/main/#huggingface&#34;&gt;HuggingFace&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://api.together.xyz/playground/language/togethercomputer/evo-1-131k-base&#34;&gt;Together web UI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/evo-design/evo/main/#together-api&#34;&gt;Together API&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/evo-design/evo/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;p&gt;Evo is based on &lt;a href=&#34;https://github.com/togethercomputer/stripedhyena/tree/main&#34;&gt;StripedHyena&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Evo uses &lt;a href=&#34;https://github.com/Dao-AILab/flash-attention&#34;&gt;FlashAttention-2&lt;/a&gt;, which may not work on all GPU architectures. Please consult the &lt;a href=&#34;https://github.com/Dao-AILab/flash-attention#installation-and-features&#34;&gt;FlashAttention GitHub repository&lt;/a&gt; for the current list of supported GPUs.&lt;/p&gt; &#xA;&lt;p&gt;Make sure to install the correct &lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch version&lt;/a&gt; on your system.&lt;/p&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;You can install Evo using &lt;code&gt;pip&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install evo-model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or directly from the GitHub source&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/evo-design/evo.git&#xA;cd evo/&#xA;pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We recommend that you install the PyTorch library first, before installing all other dependencies (due to dependency issues of the &lt;code&gt;flash-attn&lt;/code&gt; library; see, e.g., this &lt;a href=&#34;https://github.com/Dao-AILab/flash-attention/issues/246&#34;&gt;issue&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;One of our &lt;a href=&#34;https://raw.githubusercontent.com/evo-design/evo/main/scripts/&#34;&gt;example scripts&lt;/a&gt;, demonstrating how to go from generating sequences with Evo to folding proteins (&lt;a href=&#34;https://raw.githubusercontent.com/evo-design/evo/main/scripts/generation_to_folding.py&#34;&gt;scripts/generation_to_folding.py&lt;/a&gt;), further requires the installation of &lt;code&gt;prodigal&lt;/code&gt;. We have created an &lt;a href=&#34;https://raw.githubusercontent.com/evo-design/evo/main/environment.yml&#34;&gt;environment.yml&lt;/a&gt; file for this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda env create -f environment.yml&#xA;conda activate evo-design&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Below is an example of how to download Evo and use it locally through the Python API.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from evo import Evo&#xA;import torch&#xA;&#xA;device = &#39;cuda:0&#39;&#xA;&#xA;evo_model = Evo(&#39;evo-1-131k-base&#39;)&#xA;model, tokenizer = evo_model.model, evo_model.tokenizer&#xA;model.to(device)&#xA;model.eval()&#xA;&#xA;sequence = &#39;ACGT&#39;&#xA;input_ids = torch.tensor(&#xA;    tokenizer.tokenize(sequence),&#xA;    dtype=torch.int,&#xA;).to(device).unsqueeze(0)&#xA;logits, _ = model(input_ids) # (batch, length, vocab)&#xA;&#xA;print(&#39;Logits: &#39;, logits)&#xA;print(&#39;Shape (batch, length, vocab): &#39;, logits.shape)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;An example of batched inference can be found in &lt;a href=&#34;https://raw.githubusercontent.com/evo-design/evo/main/scripts/example_inference.py&#34;&gt;&lt;code&gt;scripts/example_inference.py&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We provide an &lt;a href=&#34;https://raw.githubusercontent.com/evo-design/evo/main/scripts/generate.py&#34;&gt;example script&lt;/a&gt; for how to prompt the model and sample a set of sequences given the prompt.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m scripts.generate \&#xA;    --model-name &#39;evo-1-131k-base&#39; \&#xA;    --prompt ACGT \&#xA;    --n-samples 10 \&#xA;    --n-tokens 100 \&#xA;    --temperature 1. \&#xA;    --top-k 4 \&#xA;    --device cuda:0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We also provide an &lt;a href=&#34;https://raw.githubusercontent.com/evo-design/evo/main/scripts/generate.py&#34;&gt;example script&lt;/a&gt; for using the model to score the log-likelihoods of a set of sequences.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m scripts.score \&#xA;    --input-fasta examples/example_seqs.fasta \&#xA;    --output-tsv scores.tsv \&#xA;    --model-name &#39;evo-1-131k-base&#39; \&#xA;    --device cuda:0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;HuggingFace&lt;/h2&gt; &#xA;&lt;p&gt;Evo is integrated with &lt;a href=&#34;https://huggingface.co/togethercomputer/evo-1-131k-base&#34;&gt;HuggingFace&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoConfig, AutoModelForCausalLM&#xA;&#xA;model_name = &#39;togethercomputer/evo-1-8k-base&#39;&#xA;&#xA;model_config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)&#xA;model_config.use_cache = True&#xA;&#xA;model = AutoModelForCausalLM.from_pretrained(&#xA;    model_name,&#xA;    config=model_config,&#xA;    trust_remote_code=True,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Together API&lt;/h2&gt; &#xA;&lt;p&gt;Evo is available through Together AI with a &lt;a href=&#34;https://api.together.xyz/playground/language/togethercomputer/evo-1-131k-base&#34;&gt;web UI&lt;/a&gt;, where you can generate DNA sequences with a chat-like interface.&lt;/p&gt; &#xA;&lt;p&gt;For more detailed or batch workflows, you can call the Together API with a simple example below.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import openai&#xA;import os&#xA;&#xA;# Fill in your API information here.&#xA;client = openai.OpenAI(&#xA;  api_key=TOGETHER_API_KEY,&#xA;  base_url=&#39;https://api.together.xyz&#39;,&#xA;)&#xA;&#xA;chat_completion = client.chat.completions.create(&#xA;  messages=[&#xA;    {&#xA;      &#34;role&#34;: &#34;system&#34;,&#xA;      &#34;content&#34;: &#34;&#34;&#xA;    },&#xA;    {&#xA;      &#34;role&#34;: &#34;user&#34;,&#xA;      &#34;content&#34;: &#34;ACGT&#34;, # Prompt the model with a sequence.&#xA;    }&#xA;  ],&#xA;  model=&#34;togethercomputer/evo-1-131k-base&#34;,&#xA;  max_tokens=128, # Sample some number of new tokens.&#xA;  logprobs=True&#xA;)&#xA;print(&#xA;    chat_completion.choices[0].logprobs.token_logprobs,&#xA;    chat_completion.choices[0].message.content&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Please cite the following preprint when referencing Evo.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article {nguyen2024sequence,&#xA;    author = {Eric Nguyen and Michael Poli and Matthew G Durrant and Armin W Thomas and Brian Kang and Jeremy Sullivan and Madelena Y Ng and Ashley Lewis and Aman Patel and Aaron Lou and Stefano Ermon and Stephen A Baccus and Tina Hernandez-Boussard and Christopher Ré and Patrick D Hsu and Brian L Hie},&#xA;    title = {Sequence modeling and design from molecular to genome scale with Evo},&#xA;    year = {2024},&#xA;    doi = {10.1101/2024.02.27.582234},&#xA;    publisher = {Cold Spring Harbor Laboratory},&#xA;    URL = {https://www.biorxiv.org/content/early/2024/02/27/2024.02.27.582234},&#xA;    journal = {bioRxiv}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>yformer/EfficientSAM</title>
    <updated>2024-03-10T01:54:15Z</updated>
    <id>tag:github.com,2024-03-10:/yformer/EfficientSAM</id>
    <link href="https://github.com/yformer/EfficientSAM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;EfficientSAM&lt;/h1&gt; &#xA;&lt;p&gt;EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;p&gt;[Jan.12 2024] ONNX version of EfficientSAM including separate encoder and decoder is available on the &lt;a href=&#34;https://huggingface.co/spaces/yunyangx/EfficientSAM/tree/main&#34;&gt;Hugging Face Space&lt;/a&gt; (thanks to @wkentaro Kentaro Wada for implementing onnx export)&lt;/p&gt; &#xA;&lt;p&gt;[Dec.31 2023] EfficientSAM is integrated into the annotation tool, &lt;a href=&#34;https://github.com/labelmeai/labelme&#34;&gt;Labelme&lt;/a&gt; (huge thanks to lableme team and @wkentaro Kentaro Wada)&lt;/p&gt; &#xA;&lt;p&gt;[Dec.11 2023] The EfficientSAM model code with checkpoints is fully available in this repository. The &lt;a href=&#34;https://github.com/yformer/EfficientSAM/raw/main/EfficientSAM_example.py&#34;&gt;example&lt;/a&gt; script shows how to instantiate the model with checkpoint and query points on an image.&lt;/p&gt; &#xA;&lt;p&gt;[Dec.10 2023] Grounded EfficientSAM demo is available on &lt;a href=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything/tree/main/EfficientSAM&#34;&gt;Grounded-Efficient-Segment-Anything&lt;/a&gt; (huge thanks to IDEA-Research team and @rentainhe for supporting &lt;a href=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything/raw/main/EfficientSAM/grounded_efficient_sam.py&#34;&gt;grounded-efficient-sam demo&lt;/a&gt; under &lt;a href=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything&#34;&gt;Grounded-Segment-Anything&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;[Dec.6 2023] EfficientSAM demo is available on the &lt;a href=&#34;https://huggingface.co/spaces/yunyangx/EfficientSAM&#34;&gt;Hugging Face Space&lt;/a&gt; (huge thanks to all the HF team for their support).&lt;/p&gt; &#xA;&lt;p&gt;[Dec.5 2023] We release the torchscript version of EfficientSAM and share a colab.&lt;/p&gt; &#xA;&lt;h2&gt;Online Demo &amp;amp; Examples&lt;/h2&gt; &#xA;&lt;p&gt;Online demo and examples can be found in the &lt;a href=&#34;https://yformer.github.io/efficient-sam/&#34;&gt;project page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;EfficientSAM Instance Segmentation Examples&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Point-prompt&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yformer/EfficientSAM/main/figs/examples/demo_point.png&#34; alt=&#34;point-prompt&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Box-prompt&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yformer/EfficientSAM/main/figs/examples/demo_box.png&#34; alt=&#34;box-prompt&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Segment everything&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yformer/EfficientSAM/main/figs/examples/demo_everything.png&#34; alt=&#34;segment everything&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Saliency&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yformer/EfficientSAM/main/figs/examples/demo_saliency.png&#34; alt=&#34;Saliency&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Model&lt;/h2&gt; &#xA;&lt;p&gt;EfficientSAM checkpoints are available under the weights folder of this github repository. Example instantiations and run of the models can be found in &lt;a href=&#34;https://github.com/yformer/EfficientSAM/raw/main/EfficientSAM_example.py&#34;&gt;EfficientSAM_example.py&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;EfficientSAM-S&lt;/th&gt; &#xA;   &lt;th&gt;EfficientSAM-Ti&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/yformer/EfficientSAM/raw/main/weights/efficient_sam_vits.pt.zip&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/yformer/EfficientSAM/raw/main/weights/efficient_sam_vitt.pt&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;You can directly use EfficientSAM with checkpoints,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;from efficient_sam.build_efficient_sam import build_efficient_sam_vitt, build_efficient_sam_vits&#xA;efficientsam = build_efficient_sam_vitt()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Jupyter Notebook Example&lt;/h2&gt; &#xA;&lt;p&gt;The notebook is shared &lt;a href=&#34;https://github.com/yformer/EfficientSAM/raw/main/notebooks&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;SAM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ChaoningZhang/MobileSAM&#34;&gt;MobileSAM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/CASIA-IVA-Lab/FastSAM&#34;&gt;FastSAM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/xuebinqin/U-2-Net&#34;&gt;U-2-Net&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you&#39;re using EfficientSAM in your research or applications, please cite using this BibTeX:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;&#xA;&#xA;@article{xiong2023efficientsam,&#xA;  title={EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything},&#xA;  author={Yunyang Xiong, Bala Varadarajan, Lemeng Wu, Xiaoyu Xiang, Fanyi Xiao, Chenchen Zhu, Xiaoliang Dai, Dilin Wang, Fei Sun, Forrest Iandola, Raghuraman Krishnamoorthi, Vikas Chandra},&#xA;  journal={arXiv:2312.00863},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>google-research/vision_transformer</title>
    <updated>2024-03-10T01:54:15Z</updated>
    <id>tag:github.com,2024-03-10:/google-research/vision_transformer</id>
    <link href="https://github.com/google-research/vision_transformer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Vision Transformer and MLP-Mixer Architectures&lt;/h1&gt; &#xA;&lt;p&gt;In this repository we release models from the papers&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2010.11929&#34;&gt;An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2105.01601&#34;&gt;MLP-Mixer: An all-MLP Architecture for Vision&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.10270&#34;&gt;How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.01548&#34;&gt;When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2111.07991&#34;&gt;LiT: Zero-Shot Transfer with Locked-image text Tuning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2203.08065&#34;&gt;Surrogate Gap Minimization Improves Sharpness-Aware Training&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The models were pre-trained on the &lt;a href=&#34;http://www.image-net.org/&#34;&gt;ImageNet&lt;/a&gt; and &lt;a href=&#34;http://www.image-net.org/&#34;&gt;ImageNet-21k&lt;/a&gt; datasets. We provide the code for fine-tuning the released models in &lt;a href=&#34;https://jax.readthedocs.io&#34;&gt;JAX&lt;/a&gt;/&lt;a href=&#34;http://flax.readthedocs.io&#34;&gt;Flax&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The models from this codebase were originally trained in &lt;a href=&#34;https://github.com/google-research/big_vision/&#34;&gt;https://github.com/google-research/big_vision/&lt;/a&gt; where you can find more advanced code (e.g. multi-host training), as well as some of the original training scripts (e.g. &lt;a href=&#34;https://github.com/google-research/big_vision/raw/main/big_vision/configs/vit_i21k.py&#34;&gt;configs/vit_i21k.py&lt;/a&gt; for pre-training a ViT, or &lt;a href=&#34;https://github.com/google-research/big_vision/raw/main/big_vision/configs/transfer.py&#34;&gt;configs/transfer.py&lt;/a&gt; for transfering a model).&lt;/p&gt; &#xA;&lt;p&gt;Table of contents:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-research/vision_transformer/main/#vision-transformer-and-mlp-mixer-architectures&#34;&gt;Vision Transformer and MLP-Mixer Architectures&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-research/vision_transformer/main/#colab&#34;&gt;Colab&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-research/vision_transformer/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-research/vision_transformer/main/#fine-tuning-a-model&#34;&gt;Fine-tuning a model&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-research/vision_transformer/main/#vision-transformer&#34;&gt;Vision Transformer&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-research/vision_transformer/main/#available-vit-models&#34;&gt;Available ViT models&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-research/vision_transformer/main/#expected-vit-results&#34;&gt;Expected ViT results&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-research/vision_transformer/main/#mlp-mixer&#34;&gt;MLP-Mixer&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-research/vision_transformer/main/#available-mixer-models&#34;&gt;Available Mixer models&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-research/vision_transformer/main/#expected-mixer-results&#34;&gt;Expected Mixer results&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-research/vision_transformer/main/#lit-models&#34;&gt;LiT models&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-research/vision_transformer/main/#running-on-cloud&#34;&gt;Running on cloud&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-research/vision_transformer/main/#create-a-vm&#34;&gt;Create a VM&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-research/vision_transformer/main/#setup-vm&#34;&gt;Setup VM&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-research/vision_transformer/main/#bibtex&#34;&gt;Bibtex&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-research/vision_transformer/main/#disclaimers&#34;&gt;Disclaimers&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-research/vision_transformer/main/#changelog&#34;&gt;Changelog&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Colab&lt;/h2&gt; &#xA;&lt;p&gt;Below Colabs run both with GPUs, and TPUs (8 cores, data parallelism).&lt;/p&gt; &#xA;&lt;p&gt;The first Colab demonstrates the JAX code of Vision Transformers and MLP Mixers. This Colab allows you to edit the files from the repository directly in the Colab UI and has annotated Colab cells that walk you through the code step by step, and lets you interact with the data.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/google-research/vision_transformer/blob/main/vit_jax.ipynb&#34;&gt;https://colab.research.google.com/github/google-research/vision_transformer/blob/main/vit_jax.ipynb&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The second Colab allows you to explore the &amp;gt;50k Vision Transformer and hybrid checkpoints that were used to generate the data of the third paper &#34;How to train your ViT? ...&#34;. The Colab includes code to explore and select checkpoints, and to do inference both using the JAX code from this repo, and also using the popular &lt;a href=&#34;https://github.com/rwightman/pytorch-image-models&#34;&gt;&lt;code&gt;timm&lt;/code&gt;&lt;/a&gt; PyTorch library that can directly load these checkpoints as well. Note that a handful of models are also available directly from TF-Hub: &lt;a href=&#34;https://tfhub.dev/sayakpaul/collections/vision_transformer&#34;&gt;sayakpaul/collections/vision_transformer&lt;/a&gt; (external contribution by &lt;a href=&#34;https://github.com/sayakpaul&#34;&gt;Sayak Paul&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;The second Colab also lets you fine-tune the checkpoints on any tfds dataset and your own dataset with examples in individual JPEG files (optionally directly reading from Google Drive).&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/google-research/vision_transformer/blob/main/vit_jax_augreg.ipynb&#34;&gt;https://colab.research.google.com/github/google-research/vision_transformer/blob/main/vit_jax_augreg.ipynb&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: As for now (6/20/21) Google Colab only supports a single GPU (Nvidia Tesla T4), and TPUs (currently TPUv2-8) are attached indirectly to the Colab VM and communicate over slow network, which leads to pretty bad training speed. You would usually want to set up a dedicated machine if you have a non-trivial amount of data to fine-tune on. For details see the &lt;a href=&#34;https://raw.githubusercontent.com/google-research/vision_transformer/main/#running-on-cloud&#34;&gt;Running on cloud&lt;/a&gt; section.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Make sure you have &lt;code&gt;Python&amp;gt;=3.10&lt;/code&gt; installed on your machine.&lt;/p&gt; &#xA;&lt;p&gt;Install JAX and python dependencies by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# If using GPU:&#xA;pip install -r vit_jax/requirements.txt&#xA;&#xA;# If using TPU:&#xA;pip install -r vit_jax/requirements-tpu.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For newer versions of &lt;a href=&#34;https://github.com/google/jax&#34;&gt;JAX&lt;/a&gt;, follow the instructions provided in the corresponding repository linked here. Note that installation instructions for CPU, GPU and TPU differs slightly.&lt;/p&gt; &#xA;&lt;p&gt;Install &lt;a href=&#34;https://github.com/google/flaxformer&#34;&gt;Flaxformer&lt;/a&gt;, follow the instructions provided in the corresponding repository linked here.&lt;/p&gt; &#xA;&lt;p&gt;For more details refer to the section &lt;a href=&#34;https://raw.githubusercontent.com/google-research/vision_transformer/main/#running-on-cloud&#34;&gt;Running on cloud&lt;/a&gt; below.&lt;/p&gt; &#xA;&lt;h2&gt;Fine-tuning a model&lt;/h2&gt; &#xA;&lt;p&gt;You can run fine-tuning of the downloaded model on your dataset of interest. All models share the same command line interface.&lt;/p&gt; &#xA;&lt;p&gt;For example for fine-tuning a ViT-B/16 (pre-trained on imagenet21k) on CIFAR10 (note how we specify &lt;code&gt;b16,cifar10&lt;/code&gt; as arguments to the config, and how we instruct the code to access the models directly from a GCS bucket instead of first downloading them into the local directory):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m vit_jax.main --workdir=/tmp/vit-$(date +%s) \&#xA;    --config=$(pwd)/vit_jax/configs/vit.py:b16,cifar10 \&#xA;    --config.pretrained_dir=&#39;gs://vit_models/imagenet21k&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In order to fine-tune a Mixer-B/16 (pre-trained on imagenet21k) on CIFAR10:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m vit_jax.main --workdir=/tmp/vit-$(date +%s) \&#xA;    --config=$(pwd)/vit_jax/configs/mixer_base16_cifar10.py \&#xA;    --config.pretrained_dir=&#39;gs://mixer_models/imagenet21k&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &#34;How to train your ViT? ...&#34; paper added &amp;gt;50k checkpoints that you can fine-tune with the &lt;a href=&#34;https://github.com/google-research/vision_transformer/raw/main/vit_jax/configs/augreg.py&#34;&gt;&lt;code&gt;configs/augreg.py&lt;/code&gt;&lt;/a&gt; config. When you only specify the model name (the &lt;code&gt;config.name&lt;/code&gt; value from &lt;a href=&#34;https://github.com/google-research/vision_transformer/raw/main/vit_jax/configs/models.py&#34;&gt;&lt;code&gt;configs/model.py&lt;/code&gt;&lt;/a&gt;), then the best i21k checkpoint by upstream validation accuracy (&#34;recommended&#34; checkpoint, see section 4.5 of the paper) is chosen. To make up your mind which model you want to use, have a look at Figure 3 in the paper. It&#39;s also possible to choose a different checkpoint (see Colab &lt;a href=&#34;https://colab.research.google.com/github/google-research/vision_transformer/blob/main/vit_jax_augreg.ipynb&#34;&gt;&lt;code&gt;vit_jax_augreg.ipynb&lt;/code&gt;&lt;/a&gt;) and then specify the value from the &lt;code&gt;filename&lt;/code&gt; or &lt;code&gt;adapt_filename&lt;/code&gt; column, which correspond to the filenames without &lt;code&gt;.npz&lt;/code&gt; from the &lt;a href=&#34;https://console.cloud.google.com/storage/browser/vit_models/augreg/&#34;&gt;&lt;code&gt;gs://vit_models/augreg&lt;/code&gt;&lt;/a&gt; directory.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m vit_jax.main --workdir=/tmp/vit-$(date +%s) \&#xA;    --config=$(pwd)/vit_jax/configs/augreg.py:R_Ti_16 \&#xA;    --config.dataset=oxford_iiit_pet \&#xA;    --config.base_lr=0.01&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Currently, the code will automatically download CIFAR-10 and CIFAR-100 datasets. Other public or custom datasets can be easily integrated, using &lt;a href=&#34;https://github.com/tensorflow/datasets/&#34;&gt;tensorflow datasets library&lt;/a&gt;. Note that you will also need to update &lt;code&gt;vit_jax/input_pipeline.py&lt;/code&gt; to specify some parameters about any added dataset.&lt;/p&gt; &#xA;&lt;p&gt;Note that our code uses all available GPUs/TPUs for fine-tuning.&lt;/p&gt; &#xA;&lt;p&gt;To see a detailed list of all available flags, run &lt;code&gt;python3 -m vit_jax.train --help&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Notes on memory:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Different models require different amount of memory. Available memory also depends on the accelerator configuration (both type and count). If you encounter an out-of-memory error you can increase the value of &lt;code&gt;--config.accum_steps=8&lt;/code&gt; -- alternatively, you could also decrease the &lt;code&gt;--config.batch=512&lt;/code&gt; (and decrease &lt;code&gt;--config.base_lr&lt;/code&gt; accordingly).&lt;/li&gt; &#xA; &lt;li&gt;The host keeps a shuffle buffer in memory. If you encounter a host OOM (as opposed to an accelerator OOM), you can decrease the default &lt;code&gt;--config.shuffle_buffer=50000&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Vision Transformer&lt;/h2&gt; &#xA;&lt;p&gt;by Alexey Dosovitskiy*†, Lucas Beyer*, Alexander Kolesnikov*, Dirk Weissenborn*, Xiaohua Zhai*, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit and Neil Houlsby*†.&lt;/p&gt; &#xA;&lt;p&gt;(*) equal technical contribution, (†) equal advising.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/google-research/vision_transformer/main/vit_figure.png&#34; alt=&#34;Figure 1 from paper&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Overview of the model: we split an image into fixed-size patches, linearly embed each of them, add position embeddings, and feed the resulting sequence of vectors to a standard Transformer encoder. In order to perform classification, we use the standard approach of adding an extra learnable &#34;classification token&#34; to the sequence.&lt;/p&gt; &#xA;&lt;h3&gt;Available ViT models&lt;/h3&gt; &#xA;&lt;p&gt;We provide a variety of ViT models in different GCS buckets. The models can be downloaded with e.g.:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;wget https://storage.googleapis.com/vit_models/imagenet21k/ViT-B_16.npz&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The model filenames (without the &lt;code&gt;.npz&lt;/code&gt; extension) correspond to the &lt;code&gt;config.model_name&lt;/code&gt; in &lt;a href=&#34;https://github.com/google-research/vision_transformer/raw/main/vit_jax/configs/models.py&#34;&gt;&lt;code&gt;vit_jax/configs/models.py&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://console.cloud.google.com/storage/browser/vit_models/imagenet21k/&#34;&gt;&lt;code&gt;gs://vit_models/imagenet21k&lt;/code&gt;&lt;/a&gt; - Models pre-trained on ImageNet-21k.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://console.cloud.google.com/storage/browser/vit_models/imagenet21k+imagenet2012/&#34;&gt;&lt;code&gt;gs://vit_models/imagenet21k+imagenet2012&lt;/code&gt;&lt;/a&gt; - Models pre-trained on ImageNet-21k and fine-tuned on ImageNet.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://console.cloud.google.com/storage/browser/vit_models/augreg/&#34;&gt;&lt;code&gt;gs://vit_models/augreg&lt;/code&gt;&lt;/a&gt; - Models pre-trained on ImageNet-21k, applying varying amounts of &lt;a href=&#34;https://arxiv.org/abs/2106.10270&#34;&gt;AugReg&lt;/a&gt;. Improved performance.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://console.cloud.google.com/storage/browser/vit_models/sam/&#34;&gt;&lt;code&gt;gs://vit_models/sam&lt;/code&gt;&lt;/a&gt; - Models pre-trained on ImageNet with &lt;a href=&#34;https://arxiv.org/abs/2010.01412&#34;&gt;SAM&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://console.cloud.google.com/storage/browser/vit_models/gsam/&#34;&gt;&lt;code&gt;gs://vit_models/gsam&lt;/code&gt;&lt;/a&gt; - Models pre-trained on ImageNet with &lt;a href=&#34;https://arxiv.org/abs/2203.08065&#34;&gt;GSAM&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We recommend using the following checkpoints, trained with &lt;a href=&#34;https://arxiv.org/abs/2106.10270&#34;&gt;AugReg&lt;/a&gt; that have the best pre-training metrics:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Pre-trained checkpoint&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Size&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Fine-tuned checkpoint&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Resolution&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Img/sec&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Imagenet accuracy&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;L/16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;gs://vit_models/augreg/L_16-i21k-300ep-lr_0.001-aug_strong1-wd_0.1-do_0.0-sd_0.0.npz&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1243 MiB&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;gs://vit_models/augreg/L_16-i21k-300ep-lr_0.001-aug_strong1-wd_0.1-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.01-res_384.npz&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;384&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;50&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;85.59%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;B/16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;gs://vit_models/augreg/B_16-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.0-sd_0.0.npz&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;391 MiB&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;gs://vit_models/augreg/B_16-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_384.npz&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;384&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;138&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;85.49%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;S/16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;gs://vit_models/augreg/S_16-i21k-300ep-lr_0.001-aug_light1-wd_0.03-do_0.0-sd_0.0.npz&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;115 MiB&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;gs://vit_models/augreg/S_16-i21k-300ep-lr_0.001-aug_light1-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_384.npz&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;384&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;300&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;83.73%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;R50+L/32&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;gs://vit_models/augreg/R50_L_32-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.1-sd_0.1.npz&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1337 MiB&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;gs://vit_models/augreg/R50_L_32-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.1-sd_0.1--imagenet2012-steps_20k-lr_0.01-res_384.npz&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;384&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;327&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;85.99%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;R26+S/32&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;gs://vit_models/augreg/R26_S_32-i21k-300ep-lr_0.001-aug_light1-wd_0.1-do_0.0-sd_0.0.npz&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;170 MiB&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;gs://vit_models/augreg/R26_S_32-i21k-300ep-lr_0.001-aug_light1-wd_0.1-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.01-res_384.npz&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;384&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;560&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;83.85%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Ti/16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;gs://vit_models/augreg/Ti_16-i21k-300ep-lr_0.001-aug_none-wd_0.03-do_0.0-sd_0.0.npz&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;37 MiB&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;gs://vit_models/augreg/Ti_16-i21k-300ep-lr_0.001-aug_none-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_384.npz&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;384&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;610&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;78.22%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;B/32&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;gs://vit_models/augreg/B_32-i21k-300ep-lr_0.001-aug_light1-wd_0.1-do_0.0-sd_0.0.npz&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;398 MiB&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;gs://vit_models/augreg/B_32-i21k-300ep-lr_0.001-aug_light1-wd_0.1-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.01-res_384.npz&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;384&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;955&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;83.59%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;S/32&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;gs://vit_models/augreg/S_32-i21k-300ep-lr_0.001-aug_none-wd_0.1-do_0.0-sd_0.0.npz&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;118 MiB&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;gs://vit_models/augreg/S_32-i21k-300ep-lr_0.001-aug_none-wd_0.1-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.01-res_384.npz&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;384&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2154&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;79.58%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;R+Ti/16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;gs://vit_models/augreg/R_Ti_16-i21k-300ep-lr_0.001-aug_none-wd_0.03-do_0.0-sd_0.0.npz&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;40 MiB&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;gs://vit_models/augreg/R_Ti_16-i21k-300ep-lr_0.001-aug_none-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_384.npz&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;384&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2426&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;75.40%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The results from the original ViT paper (&lt;a href=&#34;https://arxiv.org/abs/2010.11929&#34;&gt;https://arxiv.org/abs/2010.11929&lt;/a&gt;) have been replicated using the models from &lt;a href=&#34;https://console.cloud.google.com/storage/browser/vit_models/imagenet21k/&#34;&gt;&lt;code&gt;gs://vit_models/imagenet21k&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;model&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;dataset&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;dropout=0.0&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;dropout=0.1&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;R50+ViT-B_16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;cifar10&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;98.72%, 3.9h (A100), &lt;a href=&#34;https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&amp;amp;regexInput=%5ER50.ViT-B_16/cifar10/do_0.0&amp;amp;_smoothingWeight=0&#34;&gt;tb.dev&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;98.94%, 10.1h (V100), &lt;a href=&#34;https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&amp;amp;regexInput=%5ER50.ViT-B_16/cifar10/do_0.1&amp;amp;_smoothingWeight=0&#34;&gt;tb.dev&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;R50+ViT-B_16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;cifar100&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;90.88%, 4.1h (A100), &lt;a href=&#34;https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&amp;amp;regexInput=%5ER50.ViT-B_16/cifar100/do_0.0&amp;amp;_smoothingWeight=0&#34;&gt;tb.dev&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;92.30%, 10.1h (V100), &lt;a href=&#34;https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&amp;amp;regexInput=%5ER50.ViT-B_16/cifar100/do_0.1&amp;amp;_smoothingWeight=0&#34;&gt;tb.dev&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;R50+ViT-B_16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;imagenet2012&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;83.72%, 9.9h (A100), &lt;a href=&#34;https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&amp;amp;regexInput=%5ER50.ViT-B_16/imagenet2012/do_0.0&amp;amp;_smoothingWeight=0&#34;&gt;tb.dev&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;85.08%, 24.2h (V100), &lt;a href=&#34;https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&amp;amp;regexInput=%5ER50.ViT-B_16/imagenet2012/do_0.1&amp;amp;_smoothingWeight=0&#34;&gt;tb.dev&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ViT-B_16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;cifar10&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;99.02%, 2.2h (A100), &lt;a href=&#34;https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&amp;amp;regexInput=%5EViT-B_16/cifar10/do_0.0&amp;amp;_smoothingWeight=0&#34;&gt;tb.dev&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;98.76%, 7.8h (V100), &lt;a href=&#34;https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&amp;amp;regexInput=%5EViT-B_16/cifar10/do_0.1&amp;amp;_smoothingWeight=0&#34;&gt;tb.dev&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ViT-B_16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;cifar100&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;92.06%, 2.2h (A100), &lt;a href=&#34;https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&amp;amp;regexInput=%5EViT-B_16/cifar100/do_0.0&amp;amp;_smoothingWeight=0&#34;&gt;tb.dev&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;91.92%, 7.8h (V100), &lt;a href=&#34;https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&amp;amp;regexInput=%5EViT-B_16/cifar100/do_0.1&amp;amp;_smoothingWeight=0&#34;&gt;tb.dev&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ViT-B_16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;imagenet2012&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;84.53%, 6.5h (A100), &lt;a href=&#34;https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&amp;amp;regexInput=%5EViT-B_16/imagenet2012/do_0.0&amp;amp;_smoothingWeight=0&#34;&gt;tb.dev&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;84.12%, 19.3h (V100), &lt;a href=&#34;https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&amp;amp;regexInput=%5EViT-B_16/imagenet2012/do_0.1&amp;amp;_smoothingWeight=0&#34;&gt;tb.dev&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ViT-B_32&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;cifar10&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;98.88%, 0.8h (A100), &lt;a href=&#34;https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&amp;amp;regexInput=%5EViT-B_32/cifar10/do_0.0&amp;amp;_smoothingWeight=0&#34;&gt;tb.dev&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;98.75%, 1.8h (V100), &lt;a href=&#34;https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&amp;amp;regexInput=%5EViT-B_32/cifar10/do_0.1&amp;amp;_smoothingWeight=0&#34;&gt;tb.dev&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ViT-B_32&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;cifar100&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;92.31%, 0.8h (A100), &lt;a href=&#34;https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&amp;amp;regexInput=%5EViT-B_32/cifar100/do_0.0&amp;amp;_smoothingWeight=0&#34;&gt;tb.dev&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;92.05%, 1.8h (V100), &lt;a href=&#34;https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&amp;amp;regexInput=%5EViT-B_32/cifar100/do_0.1&amp;amp;_smoothingWeight=0&#34;&gt;tb.dev&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ViT-B_32&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;imagenet2012&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;81.66%, 3.3h (A100), &lt;a href=&#34;https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&amp;amp;regexInput=%5EViT-B_32/imagenet2012/do_0.0&amp;amp;_smoothingWeight=0&#34;&gt;tb.dev&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;81.31%, 4.9h (V100), &lt;a href=&#34;https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&amp;amp;regexInput=%5EViT-B_32/imagenet2012/do_0.1&amp;amp;_smoothingWeight=0&#34;&gt;tb.dev&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ViT-L_16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;cifar10&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;99.13%, 6.9h (A100), &lt;a href=&#34;https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&amp;amp;regexInput=%5EViT-L_16/cifar10/do_0.0&amp;amp;_smoothingWeight=0&#34;&gt;tb.dev&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;99.14%, 24.7h (V100), &lt;a href=&#34;https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&amp;amp;regexInput=%5EViT-L_16/cifar10/do_0.1&amp;amp;_smoothingWeight=0&#34;&gt;tb.dev&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ViT-L_16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;cifar100&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;92.91%, 7.1h (A100), &lt;a href=&#34;https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&amp;amp;regexInput=%5EViT-L_16/cifar100/do_0.0&amp;amp;_smoothingWeight=0&#34;&gt;tb.dev&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;93.22%, 24.4h (V100), &lt;a href=&#34;https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&amp;amp;regexInput=%5EViT-L_16/cifar100/do_0.1&amp;amp;_smoothingWeight=0&#34;&gt;tb.dev&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ViT-L_16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;imagenet2012&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;84.47%, 16.8h (A100), &lt;a href=&#34;https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&amp;amp;regexInput=%5EViT-L_16/imagenet2012/do_0.0&amp;amp;_smoothingWeight=0&#34;&gt;tb.dev&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;85.05%, 59.7h (V100), &lt;a href=&#34;https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&amp;amp;regexInput=%5EViT-L_16/imagenet2012/do_0.1&amp;amp;_smoothingWeight=0&#34;&gt;tb.dev&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ViT-L_32&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;cifar10&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;99.06%, 1.9h (A100), &lt;a href=&#34;https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&amp;amp;regexInput=%5EViT-L_32/cifar10/do_0.0&amp;amp;_smoothingWeight=0&#34;&gt;tb.dev&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;99.09%, 6.1h (V100), &lt;a href=&#34;https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&amp;amp;regexInput=%5EViT-L_32/cifar10/do_0.1&amp;amp;_smoothingWeight=0&#34;&gt;tb.dev&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ViT-L_32&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;cifar100&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;93.29%, 1.9h (A100), &lt;a href=&#34;https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&amp;amp;regexInput=%5EViT-L_32/cifar100/do_0.0&amp;amp;_smoothingWeight=0&#34;&gt;tb.dev&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;93.34%, 6.2h (V100), &lt;a href=&#34;https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&amp;amp;regexInput=%5EViT-L_32/cifar100/do_0.1&amp;amp;_smoothingWeight=0&#34;&gt;tb.dev&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ViT-L_32&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;imagenet2012&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;81.89%, 7.5h (A100), &lt;a href=&#34;https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&amp;amp;regexInput=%5EViT-L_32/imagenet2012/do_0.0&amp;amp;_smoothingWeight=0&#34;&gt;tb.dev&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;81.13%, 15.0h (V100), &lt;a href=&#34;https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&amp;amp;regexInput=%5EViT-L_32/imagenet2012/do_0.1&amp;amp;_smoothingWeight=0&#34;&gt;tb.dev&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;We also would like to emphasize that high-quality results can be achieved with shorter training schedules and encourage users of our code to play with hyper-parameters to trade-off accuracy and computational budget. Some examples for CIFAR-10/100 datasets are presented in the table below.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;upstream&lt;/th&gt; &#xA;   &lt;th&gt;model&lt;/th&gt; &#xA;   &lt;th&gt;dataset&lt;/th&gt; &#xA;   &lt;th&gt;total_steps / warmup_steps&lt;/th&gt; &#xA;   &lt;th&gt;accuracy&lt;/th&gt; &#xA;   &lt;th&gt;wall-clock time&lt;/th&gt; &#xA;   &lt;th&gt;link&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;imagenet21k&lt;/td&gt; &#xA;   &lt;td&gt;ViT-B_16&lt;/td&gt; &#xA;   &lt;td&gt;cifar10&lt;/td&gt; &#xA;   &lt;td&gt;500 / 50&lt;/td&gt; &#xA;   &lt;td&gt;98.59%&lt;/td&gt; &#xA;   &lt;td&gt;17m&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://tensorboard.dev/experiment/QgkpiW53RPmjkabe1ME31g/&#34;&gt;tensorboard.dev&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;imagenet21k&lt;/td&gt; &#xA;   &lt;td&gt;ViT-B_16&lt;/td&gt; &#xA;   &lt;td&gt;cifar10&lt;/td&gt; &#xA;   &lt;td&gt;1000 / 100&lt;/td&gt; &#xA;   &lt;td&gt;98.86%&lt;/td&gt; &#xA;   &lt;td&gt;39m&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://tensorboard.dev/experiment/w8DQkDeJTOqJW5js80gOQg/&#34;&gt;tensorboard.dev&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;imagenet21k&lt;/td&gt; &#xA;   &lt;td&gt;ViT-B_16&lt;/td&gt; &#xA;   &lt;td&gt;cifar100&lt;/td&gt; &#xA;   &lt;td&gt;500 / 50&lt;/td&gt; &#xA;   &lt;td&gt;89.17%&lt;/td&gt; &#xA;   &lt;td&gt;17m&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://tensorboard.dev/experiment/5hM4GrnAR0KEZg725Ewnqg/&#34;&gt;tensorboard.dev&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;imagenet21k&lt;/td&gt; &#xA;   &lt;td&gt;ViT-B_16&lt;/td&gt; &#xA;   &lt;td&gt;cifar100&lt;/td&gt; &#xA;   &lt;td&gt;1000 / 100&lt;/td&gt; &#xA;   &lt;td&gt;91.15%&lt;/td&gt; &#xA;   &lt;td&gt;39m&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://tensorboard.dev/experiment/QLQTaaIoT9uEcAjtA0eRwg/&#34;&gt;tensorboard.dev&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;MLP-Mixer&lt;/h2&gt; &#xA;&lt;p&gt;by Ilya Tolstikhin*, Neil Houlsby*, Alexander Kolesnikov*, Lucas Beyer*, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, Alexey Dosovitskiy.&lt;/p&gt; &#xA;&lt;p&gt;(*) equal contribution.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/google-research/vision_transformer/main/mixer_figure.png&#34; alt=&#34;Figure 1 from paper&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;MLP-Mixer (&lt;em&gt;Mixer&lt;/em&gt; for short) consists of per-patch linear embeddings, Mixer layers, and a classifier head. Mixer layers contain one token-mixing MLP and one channel-mixing MLP, each consisting of two fully-connected layers and a GELU nonlinearity. Other components include: skip-connections, dropout, and linear classifier head.&lt;/p&gt; &#xA;&lt;p&gt;For installation follow &lt;a href=&#34;https://raw.githubusercontent.com/google-research/vision_transformer/main/#installation&#34;&gt;the same steps&lt;/a&gt; as above.&lt;/p&gt; &#xA;&lt;h3&gt;Available Mixer models&lt;/h3&gt; &#xA;&lt;p&gt;We provide the Mixer-B/16 and Mixer-L/16 models pre-trained on the ImageNet and ImageNet-21k datasets. Details can be found in Table 3 of the Mixer paper. All the models can be found at:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://console.cloud.google.com/storage/mixer_models/&#34;&gt;https://console.cloud.google.com/storage/mixer_models/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Note that these models are also available directly from TF-Hub: &lt;a href=&#34;https://tfhub.dev/sayakpaul/collections/mlp-mixer&#34;&gt;sayakpaul/collections/mlp-mixer&lt;/a&gt; (external contribution by &lt;a href=&#34;https://github.com/sayakpaul&#34;&gt;Sayak Paul&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;h3&gt;Expected Mixer results&lt;/h3&gt; &#xA;&lt;p&gt;We ran the fine-tuning code on Google Cloud machine with four V100 GPUs with the default adaption parameters from this repository. Here are the results:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;upstream&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;model&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;dataset&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;accuracy&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;wall_clock_time&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;link&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ImageNet&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Mixer-B/16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;cifar10&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;96.72%&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3.0h&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://tensorboard.dev/experiment/j9zCYt9yQVm93nqnsDZayA/&#34;&gt;tensorboard.dev&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ImageNet&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Mixer-L/16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;cifar10&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;96.59%&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3.0h&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://tensorboard.dev/experiment/Q4feeErzRGGop5XzAvYj2g/&#34;&gt;tensorboard.dev&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ImageNet-21k&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Mixer-B/16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;cifar10&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;96.82%&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;9.6h&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://tensorboard.dev/experiment/mvP4McV2SEGFeIww20ie5Q/&#34;&gt;tensorboard.dev&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ImageNet-21k&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Mixer-L/16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;cifar10&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;98.34%&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;10.0h&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://tensorboard.dev/experiment/dolAJyQYTYmudytjalF6Jg/&#34;&gt;tensorboard.dev&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;LiT models&lt;/h2&gt; &#xA;&lt;p&gt;For details, refer to the Google AI blog post &lt;a href=&#34;http://ai.googleblog.com/2022/04/locked-image-tuning-adding-language.html&#34;&gt;LiT: adding language understanding to image models&lt;/a&gt;, or read the CVPR paper &#34;LiT: Zero-Shot Transfer with Locked-image text Tuning&#34; (&lt;a href=&#34;https://arxiv.org/abs/2111.07991&#34;&gt;https://arxiv.org/abs/2111.07991&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;We published a Transformer B/16-base model with an ImageNet zeroshot accuracy of 72.1%, and a L/16-large model with an ImageNet zeroshot accuracy of 75.7%. For more details about these models, please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/google-research/vision_transformer/main/model_cards/lit.md&#34;&gt;LiT model card&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We provide a in-browser demo with small text encoders for interactive use (the smallest models should even run on a modern cell phone):&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://google-research.github.io/vision_transformer/lit/&#34;&gt;https://google-research.github.io/vision_transformer/lit/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;And finally a Colab to use the JAX models with both image and text encoders:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/google-research/vision_transformer/blob/main/lit.ipynb&#34;&gt;https://colab.research.google.com/github/google-research/vision_transformer/blob/main/lit.ipynb&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Note that none of above models support multi-lingual inputs yet, but we&#39;re working on publishing such models and will update this repository once they become available.&lt;/p&gt; &#xA;&lt;p&gt;This repository only contains evaluation code for LiT models. You can find the training code in the &lt;code&gt;big_vision&lt;/code&gt; repository:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/google-research/big_vision/tree/main/big_vision/configs/proj/image_text&#34;&gt;https://github.com/google-research/big_vision/tree/main/big_vision/configs/proj/image_text&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Expected zeroshot results from &lt;a href=&#34;https://github.com/google-research/vision_transformer/raw/main/model_cards/lit.md&#34;&gt;&lt;code&gt;model_cards/lit.md&lt;/code&gt;&lt;/a&gt; (note that the zeroshot evaluation is slightly different from the simplified evaluation in the Colab):&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;B16B_2&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;L16L&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ImageNet zero-shot&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;73.9%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;75.7%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ImageNet v2 zero-shot&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;65.1%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;66.6%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;CIFAR100 zero-shot&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;79.0%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;80.5%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Pets37 zero-shot&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;83.3%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;83.3%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Resisc45 zero-shot&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;25.3%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;25.6%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;MS-COCO Captions image-to-text retrieval&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;51.6%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;48.5%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;MS-COCO Captions text-to-image retrieval&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;31.8%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;31.1%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Running on cloud&lt;/h2&gt; &#xA;&lt;p&gt;While above &lt;a href=&#34;https://raw.githubusercontent.com/google-research/vision_transformer/main/#colab&#34;&gt;colabs&lt;/a&gt; are pretty useful to get started, you would usually want to train on a larger machine with more powerful accelerators.&lt;/p&gt; &#xA;&lt;h3&gt;Create a VM&lt;/h3&gt; &#xA;&lt;p&gt;You can use the following commands to setup a VM with GPUs on Google Cloud:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Set variables used by all commands below.&#xA;# Note that project must have accounting set up.&#xA;# For a list of zones with GPUs refer to&#xA;# https://cloud.google.com/compute/docs/gpus/gpu-regions-zones&#xA;PROJECT=my-awesome-gcp-project  # Project must have billing enabled.&#xA;VM_NAME=vit-jax-vm-gpu&#xA;ZONE=europe-west4-b&#xA;&#xA;# Below settings have been tested with this repository. You can choose other&#xA;# combinations of images &amp;amp; machines (e.g.), refer to the corresponding gcloud commands:&#xA;# gcloud compute images list --project ml-images&#xA;# gcloud compute machine-types list&#xA;# etc.&#xA;gcloud compute instances create $VM_NAME \&#xA;    --project=$PROJECT --zone=$ZONE \&#xA;    --image=c1-deeplearning-tf-2-5-cu110-v20210527-debian-10 \&#xA;    --image-project=ml-images --machine-type=n1-standard-96 \&#xA;    --scopes=cloud-platform,storage-full --boot-disk-size=256GB \&#xA;    --boot-disk-type=pd-ssd --metadata=install-nvidia-driver=True \&#xA;    --maintenance-policy=TERMINATE \&#xA;    --accelerator=type=nvidia-tesla-v100,count=8&#xA;&#xA;# Connect to VM (after some minutes needed to setup &amp;amp; start the machine).&#xA;gcloud compute ssh --project $PROJECT --zone $ZONE $VM_NAME&#xA;&#xA;# Stop the VM after use (only storage is billed for a stopped VM).&#xA;gcloud compute instances stop --project $PROJECT --zone $ZONE $VM_NAME&#xA;&#xA;# Delete VM after use (this will also remove all data stored on VM).&#xA;gcloud compute instances delete --project $PROJECT --zone $ZONE $VM_NAME&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, you can use the following similar commands to set up a Cloud VM with TPUs attached to them (below commands copied from the &lt;a href=&#34;https://cloud.google.com/tpu/docs/jax-quickstart-tpu-vm&#34;&gt;TPU tutorial&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;PROJECT=my-awesome-gcp-project  # Project must have billing enabled.&#xA;VM_NAME=vit-jax-vm-tpu&#xA;ZONE=europe-west4-a&#xA;&#xA;# Required to set up service identity initially.&#xA;gcloud beta services identity create --service tpu.googleapis.com&#xA;&#xA;# Create a VM with TPUs directly attached to it.&#xA;gcloud alpha compute tpus tpu-vm create $VM_NAME \&#xA;    --project=$PROJECT --zone=$ZONE \&#xA;    --accelerator-type v3-8 \&#xA;    --version tpu-vm-base&#xA;&#xA;# Connect to VM (after some minutes needed to setup &amp;amp; start the machine).&#xA;gcloud alpha compute tpus tpu-vm ssh --project $PROJECT --zone $ZONE $VM_NAME&#xA;&#xA;# Stop the VM after use (only storage is billed for a stopped VM).&#xA;gcloud alpha compute tpus tpu-vm stop --project $PROJECT --zone $ZONE $VM_NAME&#xA;&#xA;# Delete VM after use (this will also remove all data stored on VM).&#xA;gcloud alpha compute tpus tpu-vm delete --project $PROJECT --zone $ZONE $VM_NAME&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Setup VM&lt;/h3&gt; &#xA;&lt;p&gt;And then fetch the repository and the install dependencies (including &lt;code&gt;jaxlib&lt;/code&gt; with TPU support) as usual:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone --depth=1 --branch=master https://github.com/google-research/vision_transformer&#xA;cd vision_transformer&#xA;&#xA;# optional: install virtualenv&#xA;pip3 install virtualenv&#xA;python3 -m virtualenv env&#xA;. env/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you&#39;re connected to a VM with GPUs attached, install JAX and other dependencies with the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r vit_jax/requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you&#39;re connected to a VM with TPUs attached, install JAX and other dependencies with the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r vit_jax/requirements-tpu.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install &lt;a href=&#34;https://github.com/google/flaxformer&#34;&gt;Flaxformer&lt;/a&gt;, follow the instructions provided in the corresponding repository linked here.&lt;/p&gt; &#xA;&lt;p&gt;For both GPUs and TPUs, Check that JAX can connect to attached accelerators with the command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -c &#39;import jax; print(jax.devices())&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And finally execute one of the commands mentioned in the section &lt;a href=&#34;https://raw.githubusercontent.com/google-research/vision_transformer/main/#fine-tuning-a-model&#34;&gt;fine-tuning a model&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Bibtex&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{dosovitskiy2020vit,&#xA;  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},&#xA;  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and  Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},&#xA;  journal={ICLR},&#xA;  year={2021}&#xA;}&#xA;&#xA;@article{tolstikhin2021mixer,&#xA;  title={MLP-Mixer: An all-MLP Architecture for Vision},&#xA;  author={Tolstikhin, Ilya and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and Lucic, Mario and Dosovitskiy, Alexey},&#xA;  journal={arXiv preprint arXiv:2105.01601},&#xA;  year={2021}&#xA;}&#xA;&#xA;@article{steiner2021augreg,&#xA;  title={How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers},&#xA;  author={Steiner, Andreas and Kolesnikov, Alexander and and Zhai, Xiaohua and Wightman, Ross and Uszkoreit, Jakob and Beyer, Lucas},&#xA;  journal={arXiv preprint arXiv:2106.10270},&#xA;  year={2021}&#xA;}&#xA;&#xA;@article{chen2021outperform,&#xA;  title={When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations},&#xA;  author={Chen, Xiangning and Hsieh, Cho-Jui and Gong, Boqing},&#xA;  journal={arXiv preprint arXiv:2106.01548},&#xA;  year={2021},&#xA;}&#xA;&#xA;@article{zhuang2022gsam,&#xA;  title={Surrogate Gap Minimization Improves Sharpness-Aware Training},&#xA;  author={Zhuang, Juntang and Gong, Boqing and Yuan, Liangzhe and Cui, Yin and Adam, Hartwig and Dvornek, Nicha and Tatikonda, Sekhar and Duncan, James and Liu, Ting},&#xA;  journal={ICLR},&#xA;  year={2022},&#xA;}&#xA;&#xA;@article{zhai2022lit,&#xA;  title={LiT: Zero-Shot Transfer with Locked-image Text Tuning},&#xA;  author={Zhai, Xiaohua and Wang, Xiao and Mustafa, Basil and Steiner, Andreas and Keysers, Daniel and Kolesnikov, Alexander and Beyer, Lucas},&#xA;  journal={CVPR},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Changelog&lt;/h2&gt; &#xA;&lt;p&gt;In reverse chronological order:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;2022-08-18: Added LiT-B16B_2 model that was trained for 60k steps (LiT_B16B: 30k) without linear head on the image side (LiT_B16B: 768) and has better performance.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2022-06-09: Added the ViT and Mixer models trained from scratch using &lt;a href=&#34;https://arxiv.org/abs/2203.08065&#34;&gt;GSAM&lt;/a&gt; on ImageNet without strong data augmentations. The resultant ViTs outperform those of similar sizes trained using AdamW optimizer or the original &lt;a href=&#34;https://arxiv.org/abs/2010.01412&#34;&gt;SAM&lt;/a&gt; algorithm, or with strong data augmentations.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2022-04-14: Added models and Colab for &lt;a href=&#34;https://raw.githubusercontent.com/google-research/vision_transformer/main/#lit-models&#34;&gt;LiT models&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2021-07-29: Added ViT-B/8 AugReg models (3 upstream checkpoints and adaptations with resolution=224).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2021-07-02: Added the &#34;When Vision Transformers Outperform ResNets...&#34; paper&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2021-07-02: Added &lt;a href=&#34;https://arxiv.org/abs/2010.01412&#34;&gt;SAM&lt;/a&gt; (Sharpness-Aware Minimization) optimized ViT and MLP-Mixer checkpoints.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2021-06-20: Added the &#34;How to train your ViT? ...&#34; paper, and a new Colab to explore the &amp;gt;50k pre-trained and fine-tuned checkpoints mentioned in the paper.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2021-06-18: This repository was rewritten to use Flax Linen API and &lt;code&gt;ml_collections.ConfigDict&lt;/code&gt; for configuration.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2021-05-19: With publication of the &#34;How to train your ViT? ...&#34; paper, we added more than 50k ViT and hybrid models pre-trained on ImageNet and ImageNet-21k with various degrees of data augmentation and model regularization, and fine-tuned on ImageNet, Pets37, Kitti-distance, CIFAR-100, and Resisc45. Check out &lt;a href=&#34;https://colab.research.google.com/github/google-research/vision_transformer/blob/main/vit_jax_augreg.ipynb&#34;&gt;&lt;code&gt;vit_jax_augreg.ipynb&lt;/code&gt;&lt;/a&gt; to navigate this treasure trove of models! For example, you can use that Colab to fetch the filenames of recommended pre-trained and fine-tuned checkpoints from the &lt;code&gt;i21k_300&lt;/code&gt; column of Table 3 in the paper.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2020-12-01: Added the R50+ViT-B/16 hybrid model (ViT-B/16 on top of a Resnet-50 backbone). When pretrained on imagenet21k, this model achieves almost the performance of the L/16 model with less than half the computational finetuning cost. Note that &#34;R50&#34; is somewhat modified for the B/16 variant: The original ResNet-50 has [3,4,6,3] blocks, each reducing the resolution of the image by a factor of two. In combination with the ResNet stem this would result in a reduction of 32x so even with a patch size of (1,1) the ViT-B/16 variant cannot be realized anymore. For this reason we instead use [3,4,9] blocks for the R50+B/16 variant.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2020-11-09: Added the ViT-L/16 model.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2020-10-29: Added ViT-B/16 and ViT-L/16 models pretrained on ImageNet-21k and then fine-tuned on ImageNet at 224x224 resolution (instead of default 384x384). These models have the suffix &#34;-224&#34; in their name. They are expected to achieve 81.2% and 82.7% top-1 accuracies respectively.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Disclaimers&lt;/h2&gt; &#xA;&lt;p&gt;Open source release prepared by Andreas Steiner.&lt;/p&gt; &#xA;&lt;p&gt;Note: This repository was forked and modified from &lt;a href=&#34;https://github.com/google-research/big_transfer&#34;&gt;google-research/big_transfer&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;This is not an official Google product.&lt;/strong&gt;&lt;/p&gt;</summary>
  </entry>
</feed>