<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-10-27T01:40:11Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>anthropics/courses</title>
    <updated>2024-10-27T01:40:11Z</updated>
    <id>tag:github.com,2024-10-27:/anthropics/courses</id>
    <link href="https://github.com/anthropics/courses" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Anthropic&#39;s educational courses&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Anthropic courses&lt;/h1&gt; &#xA;&lt;p&gt;Welcome to Anthropic&#39;s educational courses. This repository currently contains five courses. We suggest completing the courses in the following order:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/anthropics/courses/master/anthropic_api_fundamentals/README.md&#34;&gt;Anthropic API fundamentals&lt;/a&gt; - teaches the essentials of working with the Claude SDK: getting an API key, working with model parameters, writing multimodal prompts, streaming responses, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/anthropics/courses/master/prompt_engineering_interactive_tutorial/README.md&#34;&gt;Prompt engineering interactive tutorial&lt;/a&gt; - a comprehensive step-by-step guide to key prompting techniques. [&lt;a href=&#34;https://catalog.us-east-1.prod.workshops.aws/workshops/0644c9e9-5b82-45f2-8835-3b5aa30b1848/en-US&#34;&gt;AWS Workshop version&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/anthropics/courses/master/real_world_prompting/README.md&#34;&gt;Real world prompting&lt;/a&gt; - learn how to incorporate prompting techniques into complex, real world prompts. [&lt;a href=&#34;https://github.com/anthropics/courses/tree/vertex/real_world_prompting&#34;&gt;Google Vertex version&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/anthropics/courses/master/prompt_evaluations/README.md&#34;&gt;Prompt evaluations&lt;/a&gt; - learn how to write production prompt evaluations to measure the quality of your prompts.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/anthropics/courses/master/tool_use/README.md&#34;&gt;Tool use&lt;/a&gt; - teaches everything you need to know to implement tool use successfully in your workflows with Claude.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;Please note that these courses often favor our lowest-cost model, Claude 3 Haiku, to keep API costs down for students following along with the materials. Feel free to use other Claude models if you prefer.&lt;/strong&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/co-tracker</title>
    <updated>2024-10-27T01:40:11Z</updated>
    <id>tag:github.com,2024-10-27:/facebookresearch/co-tracker</id>
    <link href="https://github.com/facebookresearch/co-tracker" rel="alternate"></link>
    <summary type="html">&lt;p&gt;CoTracker is a model for tracking any point (pixel) on a video.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CoTracker3: Simpler and Better Point Tracking by Pseudo-Labelling Real Videos&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://ai.facebook.com/research/&#34;&gt;Meta AI Research, GenAI&lt;/a&gt;&lt;/strong&gt;; &lt;strong&gt;&lt;a href=&#34;https://www.robots.ox.ac.uk/~vgg/&#34;&gt;University of Oxford, VGG&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://nikitakaraevv.github.io/&#34;&gt;Nikita Karaev&lt;/a&gt;, &lt;a href=&#34;https://linkedin.com/in/lvoursl&#34;&gt;Iurii Makarov&lt;/a&gt;, &lt;a href=&#34;https://jytime.github.io/&#34;&gt;Jianyuan Wang&lt;/a&gt;, &lt;a href=&#34;https://www.irocco.info/&#34;&gt;Ignacio Rocco&lt;/a&gt;, &lt;a href=&#34;https://ai.facebook.com/people/benjamin-graham/&#34;&gt;Benjamin Graham&lt;/a&gt;, &lt;a href=&#34;https://nneverova.github.io/&#34;&gt;Natalia Neverova&lt;/a&gt;, &lt;a href=&#34;https://www.robots.ox.ac.uk/~vedaldi/&#34;&gt;Andrea Vedaldi&lt;/a&gt;, &lt;a href=&#34;https://chrirupp.github.io/&#34;&gt;Christian Rupprecht&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://cotracker3.github.io/&#34;&gt;Project Page&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2307.07635&#34;&gt;Paper #1&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2410.11831&#34;&gt;Paper #2&lt;/a&gt; | &lt;a href=&#34;https://twitter.com/n_karaev/status/1742638906355470772&#34;&gt;X Thread&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/co-tracker/main/#citing-cotracker&#34;&gt;BibTeX&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/facebookresearch/co-tracker/blob/main/notebooks/demo.ipynb&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt; &lt;/a&gt; &#xA;&lt;a href=&#34;https://huggingface.co/spaces/facebook/cotracker&#34;&gt; &lt;img alt=&#34;Spaces&#34; src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34;&gt; &lt;/a&gt; &#xA;&lt;img width=&#34;1100&#34; src=&#34;https://raw.githubusercontent.com/facebookresearch/co-tracker/main/assets/teaser.png&#34;&gt; &#xA;&lt;p&gt;&lt;strong&gt;CoTracker&lt;/strong&gt; is a fast transformer-based model that can track any point in a video. It brings to tracking some of the benefits of Optical Flow.&lt;/p&gt; &#xA;&lt;p&gt;CoTracker can track:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Any pixel&lt;/strong&gt; in a video&lt;/li&gt; &#xA; &lt;li&gt;A &lt;strong&gt;quasi-dense&lt;/strong&gt; set of pixels together&lt;/li&gt; &#xA; &lt;li&gt;Points can be manually selected or sampled on a grid in any video frame&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Try these tracking modes for yourself with our &lt;a href=&#34;https://colab.research.google.com/github/facebookresearch/co-tracker/blob/master/notebooks/demo.ipynb&#34;&gt;Colab demo&lt;/a&gt; or in the &lt;a href=&#34;https://huggingface.co/spaces/facebook/cotracker&#34;&gt;Hugging Face Space 🤗&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Updates:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;[October 15, 2024] 📣 We&#39;re releasing CoTracker3! State-of-the-art point tracking with a lightweight architecture trained with 1000x less data than previous top-performing models. Code for baseline models and the pseudo-labeling pipeline are available in the repo, as well as model checkpoints. Check out our &lt;a href=&#34;https://arxiv.org/abs/2410.11831&#34;&gt;paper&lt;/a&gt; for more details.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[September 25, 2024] CoTracker2.1 is now available! This model has better performance on TAP-Vid benchmarks and follows the architecture of the original CoTracker. Try it out!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[June 14, 2024] We have released the code for &lt;a href=&#34;https://github.com/facebookresearch/vggsfm&#34;&gt;VGGSfM&lt;/a&gt;, a model for recovering camera poses and 3D structure from any image sequences based on point tracking! VGGSfM is the first fully differentiable SfM framework that unlocks scalability and outperforms conventional SfM methods on standard benchmarks.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[December 27, 2023] CoTracker2 is now available! It can now track many more (up to &lt;strong&gt;265*265&lt;/strong&gt;!) points jointly and it has a cleaner and more memory-efficient implementation. It also supports online processing. See the &lt;a href=&#34;https://arxiv.org/abs/2307.07635&#34;&gt;updated paper&lt;/a&gt; for more details. The old version remains available &lt;a href=&#34;https://github.com/facebookresearch/co-tracker/tree/8d364031971f6b3efec945dd15c468a183e58212&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[September 5, 2023] You can now run our Gradio demo &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/co-tracker/main/gradio_demo/app.py&#34;&gt;locally&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick start&lt;/h2&gt; &#xA;&lt;p&gt;The easiest way to use CoTracker is to load a pretrained model from &lt;code&gt;torch.hub&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;h3&gt;Offline mode:&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install imageio[ffmpeg]&lt;/code&gt;, then:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;# Download the video&#xA;url = &#39;https://github.com/facebookresearch/co-tracker/raw/refs/heads/main/assets/apple.mp4&#39;&#xA;&#xA;import imageio.v3 as iio&#xA;frames = iio.imread(url, plugin=&#34;FFMPEG&#34;)  # plugin=&#34;pyav&#34;&#xA;&#xA;device = &#39;cuda&#39;&#xA;grid_size = 10&#xA;video = torch.tensor(frames).permute(0, 3, 1, 2)[None].float().to(device)  # B T C H W&#xA;&#xA;# Run Offline CoTracker:&#xA;cotracker = torch.hub.load(&#34;facebookresearch/co-tracker&#34;, &#34;cotracker3_offline&#34;).to(device)&#xA;pred_tracks, pred_visibility = cotracker(video, grid_size=grid_size) # B T N 2,  B T N 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Online mode:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cotracker = torch.hub.load(&#34;facebookresearch/co-tracker&#34;, &#34;cotracker3_online&#34;).to(device)&#xA;&#xA;# Run Online CoTracker, the same model with a different API:&#xA;# Initialize online processing&#xA;cotracker(video_chunk=video, is_first_step=True, grid_size=grid_size)  &#xA;&#xA;# Process the video&#xA;for ind in range(0, video.shape[1] - cotracker.step, cotracker.step):&#xA;    pred_tracks, pred_visibility = cotracker(&#xA;        video_chunk=video[:, ind : ind + cotracker.step * 2]&#xA;    )  # B T N 2,  B T N 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Online processing is more memory-efficient and allows for the processing of longer videos. However, in the example provided above, the video length is known! See &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/co-tracker/main/online_demo.py&#34;&gt;the online demo&lt;/a&gt; for an example of tracking from an online stream with an unknown video length.&lt;/p&gt; &#xA;&lt;h3&gt;Visualize predicted tracks:&lt;/h3&gt; &#xA;&lt;p&gt;After &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/co-tracker/main/#installation-instructions&#34;&gt;installing&lt;/a&gt; CoTracker, you can visualize tracks with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from cotracker.utils.visualizer import Visualizer&#xA;&#xA;vis = Visualizer(save_dir=&#34;./saved_videos&#34;, pad_value=120, linewidth=3)&#xA;vis.visualize(video, pred_tracks, pred_visibility)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We offer a number of other ways to interact with CoTracker:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Interactive Gradio demo: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;A demo is available in the &lt;a href=&#34;https://huggingface.co/spaces/facebook/cotracker&#34;&gt;&lt;code&gt;facebook/cotracker&lt;/code&gt; Hugging Face Space 🤗&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;You can use the gradio demo locally by running &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/co-tracker/main/gradio_demo/app.py&#34;&gt;&lt;code&gt;python -m gradio_demo.app&lt;/code&gt;&lt;/a&gt; after installing the required packages: &lt;code&gt;pip install -r gradio_demo/requirements.txt&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Jupyter notebook: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;You can run the notebook in &lt;a href=&#34;https://colab.research.google.com/github/facebookresearch/co-tracker/blob/master/notebooks/demo.ipynb&#34;&gt;Google Colab&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Or explore the notebook located at &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/co-tracker/main/notebooks/demo.ipynb&#34;&gt;&lt;code&gt;notebooks/demo.ipynb&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;You can &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/co-tracker/main/#installation-instructions&#34;&gt;install&lt;/a&gt; CoTracker &lt;em&gt;locally&lt;/em&gt; and then: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Run an &lt;em&gt;offline&lt;/em&gt; demo with 10 ⨉ 10 points sampled on a grid on the first frame of a video (results will be saved to &lt;code&gt;./saved_videos/demo.mp4&lt;/code&gt;)):&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python demo.py --grid_size 10&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Run an &lt;em&gt;online&lt;/em&gt; demo:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python online_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;A GPU is strongly recommended for using CoTracker locally.&lt;/p&gt; &#xA;&lt;img width=&#34;500&#34; src=&#34;https://raw.githubusercontent.com/facebookresearch/co-tracker/main/assets/bmx-bumps.gif&#34;&gt; &#xA;&lt;h2&gt;Installation Instructions&lt;/h2&gt; &#xA;&lt;p&gt;You can use a Pretrained Model via PyTorch Hub, as described above, or install CoTracker from this GitHub repo. This is the best way if you need to run our local demo or evaluate/train CoTracker.&lt;/p&gt; &#xA;&lt;p&gt;Ensure you have both &lt;em&gt;PyTorch&lt;/em&gt; and &lt;em&gt;TorchVision&lt;/em&gt; installed on your system. Follow the instructions &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;here&lt;/a&gt; for the installation. We strongly recommend installing both PyTorch and TorchVision with CUDA support, although for small tasks CoTracker can be run on CPU.&lt;/p&gt; &#xA;&lt;h3&gt;Install a Development Version&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/facebookresearch/co-tracker&#xA;cd co-tracker&#xA;pip install -e .&#xA;pip install matplotlib flow_vis tqdm tensorboard&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can manually download all CoTracker3 checkpoints (baseline and scaled models, as well as single and sliding window architectures) from the links below and place them in the &lt;code&gt;checkpoints&lt;/code&gt; folder as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p checkpoints&#xA;cd checkpoints&#xA;# download the online (multi window) model&#xA;wget https://huggingface.co/facebook/cotracker3/resolve/main/scaled_online.pth&#xA;# download the offline (single window) model&#xA;wget https://huggingface.co/facebook/cotracker3/resolve/main/scaled_offline.pth&#xA;cd ..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also download CoTracker3 checkpoints trained only on Kubric:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# download the online (sliding window) model&#xA;wget https://huggingface.co/facebook/cotracker3/resolve/main/baseline_online.pth&#xA;# download the offline (single window) model&#xA;wget https://huggingface.co/facebook/cotracker3/resolve/main/baseline_offline.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For old checkpoints, see &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/co-tracker/main/#previous-version&#34;&gt;this section&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;To reproduce the results presented in the paper, download the following datasets:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/deepmind/tapnet&#34;&gt;TAP-Vid&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dynamic-stereo.github.io/&#34;&gt;Dynamic Replica&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;And install the necessary dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install hydra-core==1.1.0 mediapy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, execute the following command to evaluate the online model on TAP-Vid DAVIS:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python ./cotracker/evaluation/evaluate.py --config-name eval_tapvid_davis_first exp_dir=./eval_outputs dataset_root=your/tapvid/path&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And the offline model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python ./cotracker/evaluation/evaluate.py --config-name eval_tapvid_davis_first exp_dir=./eval_outputs dataset_root=/fsx-repligen/shared/datasets/tapvid offline_model=True window_len=60 checkpoint=./checkpoints/scaled_offline.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We run evaluations jointly on all the target points at a time for faster inference. With such evaluations, the numbers are similar to those presented in the paper. If you want to reproduce the exact numbers from the paper, add the flag &lt;code&gt;single_point=True&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;These are the numbers that you should be able to reproduce using the released checkpoint and the current version of the codebase:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Kinetics, $\delta_\text{avg}^\text{vis}$&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;DAVIS, $\delta_\text{avg}^\text{vis}$&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;RoboTAP, $\delta_\text{avg}^\text{vis}$&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;RGB-S, $\delta_\text{avg}^\text{vis}$&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;CoTracker2, 27.12.23&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;61.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;74.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;69.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;73.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;CoTracker2.1, 25.09.24&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;76.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;70.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;79.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;CoTracker3 offline, 15.10.24&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;67.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;76.9&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;78.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;85.0&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;CoTracker3 online, 15.10.24&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;68.3&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;76.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;78.8&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;82.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;h3&gt;Baseline&lt;/h3&gt; &#xA;&lt;p&gt;To train the CoTracker as described in our paper, you first need to generate annotations for &lt;a href=&#34;https://github.com/google-research/kubric&#34;&gt;Google Kubric&lt;/a&gt; MOVI-f dataset. Instructions for annotation generation can be found &lt;a href=&#34;https://github.com/deepmind/tapnet&#34;&gt;here&lt;/a&gt;. You can also find a discussion on dataset generation in &lt;a href=&#34;https://github.com/facebookresearch/co-tracker/issues/8&#34;&gt;this issue&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Once you have the annotated dataset, you need to make sure you followed the steps for evaluation setup and install the training dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install pip==24.0&#xA;pip install pytorch_lightning==1.6.0 tensorboard opencv-python&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now you can launch training on Kubric. Our model was trained for 50000 iterations on 32 GPUs (4 nodes with 8 GPUs). Modify &lt;em&gt;dataset_root&lt;/em&gt; and &lt;em&gt;ckpt_path&lt;/em&gt; accordingly before running this command. For training on 4 nodes, add &lt;code&gt;--num_nodes 4&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Here is an example of how to launch training of the online model on Kubric:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt; python train_on_kubric.py --batch_size 1 --num_steps 50000 \&#xA; --ckpt_path ./ --model_name cotracker_three --save_freq 200 --sequence_len 64 \&#xA;  --eval_datasets tapvid_davis_first tapvid_stacking --traj_per_sample 384 \&#xA;  --sliding_window_len 16 --train_datasets kubric --save_every_n_epoch 5 \&#xA;  --evaluate_every_n_epoch 5 --model_stride 4 --dataset_root ${path_to_your_dataset} \&#xA;   --num_nodes 4 --num_virtual_tracks 64 --mixed_precision --corr_radius 3 \ &#xA;   --wdecay 0.0005 --linear_layer_for_vis_conf --validate_at_start --add_huber_loss&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Training the offline model on Kubric:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python train_on_kubric.py --batch_size 1 --num_steps 50000 \&#xA; --ckpt_path ./ --model_name cotracker_three --save_freq 200 --sequence_len 60 \&#xA; --eval_datasets tapvid_davis_first tapvid_stacking --traj_per_sample 512 \&#xA; --sliding_window_len 60 --train_datasets kubric --save_every_n_epoch 5 \&#xA; --evaluate_every_n_epoch 5 --model_stride 4 --dataset_root ${path_to_your_dataset} \&#xA; --num_nodes 4 --num_virtual_tracks 64 --mixed_precision --offline_model \&#xA; --random_frame_rate --query_sampling_method random --corr_radius 3 \&#xA; --wdecay 0.0005 --random_seq_len --linear_layer_for_vis_conf \&#xA; --validate_at_start --add_huber_loss&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Fine-tuning with pseudo labels&lt;/h3&gt; &#xA;&lt;p&gt;In order to launch training with pseudo-labelling, you need to collect your own dataset of real videos. There is a sample class available in &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/co-tracker/main/cotracker/datasets/real_dataset.py&#34;&gt;&lt;code&gt;cotracker/datasets/real_dataset.py&lt;/code&gt;&lt;/a&gt; with keyword-based filtering that we used for training. Your class should implement loading a video and storing it in the &lt;code&gt;CoTrackerData&lt;/code&gt; class as a field, while pseudo labels will be generated in &lt;code&gt;train_on_real_data.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You should have an existing Kubric-trained model for fine-tuning with pseudo labels. Here is an example of how you can launch fine-tuning of the online model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python ./train_on_real_data.py --batch_size 1 --num_steps 15000 \&#xA; --ckpt_path ./ --model_name cotracker_three --save_freq 200 --sequence_len 64 \&#xA; --eval_datasets tapvid_stacking tapvid_davis_first --traj_per_sample 384 \&#xA; --save_every_n_epoch 15 --evaluate_every_n_epoch 15 --model_stride 4 \&#xA; --dataset_root ${path_to_your_dataset} --num_nodes 4 --real_data_splits 0 \&#xA; --num_virtual_tracks 64 --mixed_precision --random_frame_rate \&#xA; --restore_ckpt ./checkpoints/baseline_online.pth \&#xA; --lr 0.00005 --real_data_filter_sift --validate_at_start \&#xA; --sliding_window_len 16 --limit_samples 15000&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And the offline model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python train_on_real_data.py --batch_size 1 --num_steps 15000 \&#xA; --ckpt_path ./ --model_name cotracker_three --save_freq 200 --sequence_len 80 \&#xA; --eval_datasets tapvid_stacking tapvid_davis_first --traj_per_sample 384 --save_every_n_epoch 15 \&#xA; --evaluate_every_n_epoch 15 --model_stride 4 --dataset_root ${path_to_your_dataset} \&#xA;  --num_nodes 4 --real_data_splits 0 --num_virtual_tracks 64 --mixed_precision \&#xA;  --random_frame_rate --restore_ckpt ./checkpoints/baseline_offline.pth --lr 0.00005 \&#xA;  --real_data_filter_sift --validate_at_start --offline_model --limit_samples 15000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;h3&gt;Building the documentation&lt;/h3&gt; &#xA;&lt;p&gt;To build CoTracker documentation, first install the dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install sphinx&#xA;pip install sphinxcontrib-bibtex&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you can use this command to generate the documentation in the &lt;code&gt;docs/_build/html&lt;/code&gt; folder:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make -C docs html&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Previous versions&lt;/h2&gt; &#xA;&lt;h3&gt;CoTracker v2&lt;/h3&gt; &#xA;&lt;p&gt;You could use CoTracker v2 with torch.hub in both offline and online modes.&lt;/p&gt; &#xA;&lt;h4&gt;Offline mode:&lt;/h4&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install imageio[ffmpeg]&lt;/code&gt;, then:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;# Download the video&#xA;url = &#39;https://github.com/facebookresearch/co-tracker/blob/main/assets/apple.mp4&#39;&#xA;&#xA;import imageio.v3 as iio&#xA;frames = iio.imread(url, plugin=&#34;FFMPEG&#34;)  # plugin=&#34;pyav&#34;&#xA;&#xA;device = &#39;cuda&#39;&#xA;grid_size = 10&#xA;video = torch.tensor(frames).permute(0, 3, 1, 2)[None].float().to(device)  # B T C H W&#xA;&#xA;# Run Offline CoTracker:&#xA;cotracker = torch.hub.load(&#34;facebookresearch/co-tracker&#34;, &#34;cotracker2&#34;).to(device)&#xA;pred_tracks, pred_visibility = cotracker(video, grid_size=grid_size) # B T N 2,  B T N 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Online mode:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cotracker = torch.hub.load(&#34;facebookresearch/co-tracker&#34;, &#34;cotracker2_online&#34;).to(device)&#xA;&#xA;# Run Online CoTracker, the same model with a different API:&#xA;# Initialize online processing&#xA;cotracker(video_chunk=video, is_first_step=True, grid_size=grid_size)  &#xA;&#xA;# Process the video&#xA;for ind in range(0, video.shape[1] - cotracker.step, cotracker.step):&#xA;    pred_tracks, pred_visibility = cotracker(&#xA;        video_chunk=video[:, ind : ind + cotracker.step * 2]&#xA;    )  # B T N 2,  B T N 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Checkpoint for v2 could be downloaded with the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget https://huggingface.co/facebook/cotracker/resolve/main/cotracker2.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;CoTracker v1&lt;/h3&gt; &#xA;&lt;p&gt;It is directly available via pytorch hub:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;import einops&#xA;import timm&#xA;import tqdm&#xA;&#xA;cotracker = torch.hub.load(&#34;facebookresearch/co-tracker:v1.0&#34;, &#34;cotracker_w8&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The old version of the code is available &lt;a href=&#34;https://github.com/facebookresearch/co-tracker/tree/8d364031971f6b3efec945dd15c468a183e58212&#34;&gt;here&lt;/a&gt;. You can also download the corresponding checkpoints:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget https://dl.fbaipublicfiles.com/cotracker/cotracker_stride_4_wind_8.pth&#xA;wget https://dl.fbaipublicfiles.com/cotracker/cotracker_stride_4_wind_12.pth&#xA;wget https://dl.fbaipublicfiles.com/cotracker/cotracker_stride_8_wind_16.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The majority of CoTracker is licensed under CC-BY-NC, however portions of the project are available under separate license terms: Particle Video Revisited is licensed under the MIT license, TAP-Vid and LocoTrack are licensed under the Apache 2.0 license.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;We would like to thank &lt;a href=&#34;https://github.com/aharley/pips&#34;&gt;PIPs&lt;/a&gt;, &lt;a href=&#34;https://github.com/deepmind/tapnet&#34;&gt;TAP-Vid&lt;/a&gt;, &lt;a href=&#34;https://github.com/cvlab-kaist/locotrack&#34;&gt;LocoTrack&lt;/a&gt; for publicly releasing their code and data. We also want to thank &lt;a href=&#34;https://lukemelas.github.io/&#34;&gt;Luke Melas-Kyriazi&lt;/a&gt; for proofreading the paper, &lt;a href=&#34;https://jytime.github.io/&#34;&gt;Jianyuan Wang&lt;/a&gt;, &lt;a href=&#34;https://shapovalov.ro/&#34;&gt;Roman Shapovalov&lt;/a&gt; and &lt;a href=&#34;https://adamharley.com/&#34;&gt;Adam W. Harley&lt;/a&gt; for the insightful discussions.&lt;/p&gt; &#xA;&lt;h2&gt;Citing CoTracker&lt;/h2&gt; &#xA;&lt;p&gt;If you find our repository useful, please consider giving it a star ⭐ and citing our research papers in your work:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{karaev23cotracker,&#xA;  title     = {CoTracker: It is Better to Track Together},&#xA;  author    = {Nikita Karaev and Ignacio Rocco and Benjamin Graham and Natalia Neverova and Andrea Vedaldi and Christian Rupprecht},&#xA;  booktitle = {Proc. {ECCV}},&#xA;  year      = {2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{karaev24cotracker3,&#xA;  title     = {CoTracker3: Simpler and Better Point Tracking by Pseudo-Labelling Real Videos},&#xA;  author    = {Nikita Karaev and Iurii Makarov and Jianyuan Wang and Natalia Neverova and Andrea Vedaldi and Christian Rupprecht},&#xA;  booktitle = {Proc. {arXiv:2410.11831}},&#xA;  year      = {2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>google-deepmind/mujoco_menagerie</title>
    <updated>2024-10-27T01:40:11Z</updated>
    <id>tag:github.com,2024-10-27:/google-deepmind/mujoco_menagerie</id>
    <link href="https://github.com/google-deepmind/mujoco_menagerie" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A collection of high-quality models for the MuJoCo physics engine, curated by Google DeepMind.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt; &lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/#&#34;&gt;&lt;img alt=&#34;MuJoCo Menagerie&#34; src=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/assets/banner.png&#34; width=&#34;100%&#34;&gt;&lt;/a&gt; &lt;/h1&gt; &#xA;&lt;p&gt; &lt;a href=&#34;https://github.com/google-deepmind/mujoco_menagerie/actions/workflows/build.yml?query=branch%3Amain&#34; alt=&#34;GitHub Actions&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/google-deepmind/mujoco_menagerie/build.yml?branch=main&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://mujoco.readthedocs.io/en/latest/models.html&#34; alt=&#34;Documentation&#34;&gt; &lt;img src=&#34;https://readthedocs.org/projects/mujoco/badge/?version=latest&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/google-deepmind/mujoco_menagerie/raw/main/CONTRIBUTING.md&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-green.svg?sanitize=true&#34; alt=&#34;PRs&#34; height=&#34;20&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Menagerie&lt;/strong&gt; is a collection of high-quality models for the &lt;a href=&#34;https://github.com/google-deepmind/mujoco&#34;&gt;MuJoCo&lt;/a&gt; physics engine, curated by Google DeepMind.&lt;/p&gt; &#xA;&lt;p&gt;A physics simulator is only as good as the model it is simulating, and in a powerful simulator like MuJoCo with many modeling options, it is easy to create &#34;bad&#34; models which do not behave as expected. The goal of this collection is to provide the community with a curated library of well-designed models that work well right out of the gate.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/assets/franka_fr3-fr3.png&#34; width=&#34;100&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/assets/kinova_gen3-gen3.png&#34; width=&#34;100&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/assets/kuka_iiwa_14-iiwa14.png&#34; width=&#34;100&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/assets/ufactory_lite6-lite6.png&#34; width=&#34;100&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/assets/franka_emika_panda-panda.png&#34; width=&#34;100&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/assets/rethink_robotics_sawyer-sawyer.png&#34; width=&#34;100&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/assets/universal_robots_ur10e-ur10e.png&#34; width=&#34;100&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/assets/universal_robots_ur5e-ur5e.png&#34; width=&#34;100&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/assets/trossen_vx300s-vx300s.png&#34; width=&#34;100&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/assets/ufactory_xarm7-xarm7.png&#34; width=&#34;100&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/assets/unitree_z1-z1.png&#34; width=&#34;100&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/assets/aloha-aloha.png&#34; width=&#34;100&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/assets/robotiq_2f85-2f85.png&#34; width=&#34;100&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/assets/franka_emika_panda-hand.png&#34; width=&#34;100&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/assets/ufactory_xarm7-hand.png&#34; width=&#34;100&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/assets/wonik_allegro-left_hand.png&#34; width=&#34;100&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/assets/shadow_hand-left_hand.png&#34; width=&#34;100&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/assets/leap_hand-left_hand.png&#34; width=&#34;100&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/assets/shadow_dexee-shadow_dexee.png&#34; width=&#34;100&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/assets/google_robot-robot.png&#34; width=&#34;100&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/assets/hello_robot_stretch-stretch.png&#34; width=&#34;100&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/assets/unitree_a1-a1.png&#34; width=&#34;100&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/assets/anybotics_anymal_b-anymal_b.png&#34; width=&#34;100&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/assets/anybotics_anymal_c-anymal_c.png&#34; width=&#34;100&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/assets/google_barkour_v0-barkour_v0.png&#34; width=&#34;100&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/assets/google_barkour_vb-barkour_vb.png&#34; width=&#34;100&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/assets/unitree_go1-go1.png&#34; width=&#34;100&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/assets/unitree_go2-go2.png&#34; width=&#34;100&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/assets/boston_dynamics_spot-spot_arm.png&#34; width=&#34;100&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/assets/agility_cassie-cassie.png&#34; width=&#34;100&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/assets/unitree_g1-g1.png&#34; width=&#34;100&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/assets/unitree_h1-h1.png&#34; width=&#34;100&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/assets/robotis_op3-op3.png&#34; width=&#34;100&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/assets/pal_talos-talos.png&#34; width=&#34;100&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/assets/bitcraze_crazyflie_2-cf2.png&#34; width=&#34;100&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/assets/skydio_x2-x2.png&#34; width=&#34;100&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/assets/flybody-fruitfly.png&#34; width=&#34;100&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/assets/realsense_d435i-d435i.png&#34; width=&#34;100&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/#getting-started&#34;&gt;Getting Started&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/#prerequisites&#34;&gt;Prerequisites&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/#overview&#34;&gt;Overview&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/#usage&#34;&gt;Usage&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/#via-robot-descriptions&#34;&gt;Via &lt;code&gt;robot-descriptions&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/#via-git-clone&#34;&gt;Via &lt;code&gt;git clone&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/#model-quality-and-contributing&#34;&gt;Model Quality and Contributing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/#menagerie-models&#34;&gt;Menagerie Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/#citing-menagerie&#34;&gt;Citing Menagerie&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/#acknowledgments&#34;&gt;Acknowledgments&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/#license-and-disclaimer&#34;&gt;License and Disclaimer&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;p&gt;The minimum required MuJoCo version for each model is specified in its respective README. You can download prebuilt binaries for MuJoCo from the GitHub &lt;a href=&#34;https://github.com/google-deepmind/mujoco/releases/&#34;&gt;releases page&lt;/a&gt;, or if you are working with Python, you can install the native bindings from &lt;a href=&#34;https://pypi.org/project/mujoco/&#34;&gt;PyPI&lt;/a&gt; via &lt;code&gt;pip install mujoco&lt;/code&gt;. For alternative installation instructions, see &lt;a href=&#34;https://github.com/google-deepmind/mujoco#installation&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Overview&lt;/h3&gt; &#xA;&lt;p&gt;The structure of Menagerie is illustrated below. For brevity, we have only included one model directory since all others follow the exact same pattern.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;├── unitree_go2&#xA;│&amp;nbsp;&amp;nbsp; ├── assets&#xA;│&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; ├── base_0.obj&#xA;│&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; ├── ...&#xA;│&amp;nbsp;&amp;nbsp; ├── go2.png&#xA;│&amp;nbsp;&amp;nbsp; ├── go2.xml&#xA;│&amp;nbsp;&amp;nbsp; ├── LICENSE&#xA;│&amp;nbsp;&amp;nbsp; ├── README.md&#xA;│&amp;nbsp;&amp;nbsp; └── scene.xml&#xA;│&amp;nbsp;&amp;nbsp; └── go2_mjx.xml&#xA;│&amp;nbsp;&amp;nbsp; └── scene_mjx.xml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;assets&lt;/code&gt;: stores the 3D meshes (.stl or .obj) of the model used for visual and collision purposes&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;LICENSE&lt;/code&gt;: describes the copyright and licensing terms of the model&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;README.md&lt;/code&gt;: contains detailed steps describing how the model&#39;s MJCF XML file was generated&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;model&amp;gt;.xml&lt;/code&gt;: contains the MJCF definition of the model&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;scene.xml&lt;/code&gt;: includes &lt;code&gt;&amp;lt;model&amp;gt;.xml&lt;/code&gt; with a plane, a light source and potentially other objects&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;model&amp;gt;.png&lt;/code&gt;: a PNG image of &lt;code&gt;scene.xml&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;model&amp;gt;_mjx.xml&lt;/code&gt;: contains an MJX-compatible version of the model. Not all models have an MJX variant (see &lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/#menagerie-models&#34;&gt;Menagerie Models&lt;/a&gt; for more information).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;scene_mjx.xml&lt;/code&gt;: same as &lt;code&gt;scene.xml&lt;/code&gt; but loads the MJX variant&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Note that &lt;code&gt;&amp;lt;model&amp;gt;.xml&lt;/code&gt; solely describes the model, i.e., no other entity is defined in the kinematic tree. We leave additional body definitions for the &lt;code&gt;scene.xml&lt;/code&gt; file, as can be seen in the Shadow Hand &lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/shadow_hand/scene_right.xml&#34;&gt;&lt;code&gt;scene.xml&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Usage&lt;/h3&gt; &#xA;&lt;h4&gt;Via &lt;code&gt;robot-descriptions&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;You can use the opensource &lt;a href=&#34;https://github.com/robot-descriptions/robot_descriptions.py&#34;&gt;&lt;code&gt;robot_descriptions&lt;/code&gt;&lt;/a&gt; package to load any model in Menagerie. It is available on PyPI and can be installed via &lt;code&gt;pip install robot_descriptions&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Once installed, you can load a model of your choice as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import mujoco&#xA;&#xA;# Loading a specific model description as an imported module.&#xA;from robot_descriptions import panda_mj_description&#xA;model = mujoco.MjModel.from_xml_path(panda_mj_description.MJCF_PATH)&#xA;&#xA;# Directly loading an instance of MjModel.&#xA;from robot_descriptions.loaders.mujoco import load_robot_description&#xA;model = load_robot_description(&#34;panda_mj_description&#34;)&#xA;&#xA;# Loading a variant of the model, e.g. panda without a gripper.&#xA;model = load_robot_description(&#34;panda_mj_description&#34;, variant=&#34;panda_nohand&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Via &lt;code&gt;git clone&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;You can also directly clone this repository in the directory of your choice:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/google-deepmind/mujoco_menagerie.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can then interactively explore the model using the Python viewer:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m mujoco.viewer --mjcf mujoco_menagerie/unitree_go2/scene.xml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you have further questions, please check out our &lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/FAQ.md&#34;&gt;FAQ&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Model Quality and Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Our goal is to eventually make all Menagerie models as faithful as possible to the real system they are being modeled after. Improving model quality is an ongoing effort, and the current state of many models is not necessarily as good as it could be.&lt;/p&gt; &#xA;&lt;p&gt;However, by releasing Menagerie in its current state, we hope to consolidate and increase visibility for community contributions. To help Menagerie users set proper expectations around the quality of each model, we introduce the following grading system:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Grade&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;A+&lt;/td&gt; &#xA;   &lt;td&gt;Values are the product of proper system identification&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;A&lt;/td&gt; &#xA;   &lt;td&gt;Values are realistic, but have not been properly identified&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;B&lt;/td&gt; &#xA;   &lt;td&gt;Stable, but some values are unrealistic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;C&lt;/td&gt; &#xA;   &lt;td&gt;Conditionally stable, can be significantly improved&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The grading system will be applied to each model once a proper system identification toolbox is created. We are currently planning to release this toolbox later this year.&lt;/p&gt; &#xA;&lt;p&gt;For more information regarding contributions, for example to add a new model to Menagerie, see &lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Menagerie Models&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Arms.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Maker&lt;/th&gt; &#xA;   &lt;th&gt;DoFs&lt;/th&gt; &#xA;   &lt;th&gt;License&lt;/th&gt; &#xA;   &lt;th&gt;MJX&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FR3&lt;/td&gt; &#xA;   &lt;td&gt;Franka Robotics&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/franka_fr3/LICENSE&#34;&gt;Apache-2.0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;iiwa14&lt;/td&gt; &#xA;   &lt;td&gt;KUKA&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/kuka_iiwa_14/LICENSE&#34;&gt;BSD-3-Clause&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Lite6&lt;/td&gt; &#xA;   &lt;td&gt;UFACTORY&lt;/td&gt; &#xA;   &lt;td&gt;6&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/ufactory_lite6/LICENSE&#34;&gt;BSD-3-Clause&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Panda&lt;/td&gt; &#xA;   &lt;td&gt;Franka Robotics&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/franka_emika_panda/LICENSE&#34;&gt;BSD-3-Clause&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✔️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Sawyer&lt;/td&gt; &#xA;   &lt;td&gt;Rethink Robotics&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/rethink_robotics_sawyer/LICENSE&#34;&gt;Apache-2.0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Unitree Z1&lt;/td&gt; &#xA;   &lt;td&gt;Unitree Robotics&lt;/td&gt; &#xA;   &lt;td&gt;6&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/unitree_z1/LICENSE&#34;&gt;BSD-3-Clause&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;UR5e&lt;/td&gt; &#xA;   &lt;td&gt;Universal Robots&lt;/td&gt; &#xA;   &lt;td&gt;6&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/universal_robots_ur5e/LICENSE&#34;&gt;BSD-3-Clause&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;UR10e&lt;/td&gt; &#xA;   &lt;td&gt;Universal Robots&lt;/td&gt; &#xA;   &lt;td&gt;6&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/universal_robots_ur10e/LICENSE&#34;&gt;BSD-3-Clause&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViperX 300&lt;/td&gt; &#xA;   &lt;td&gt;Trossen Robotics&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/trossen_vx300s/LICENSE&#34;&gt;BSD-3-Clause&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WidowX 250&lt;/td&gt; &#xA;   &lt;td&gt;Trossen Robotics&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/trossen_wx250s/LICENSE&#34;&gt;BSD-3-Clause&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;xarm7&lt;/td&gt; &#xA;   &lt;td&gt;UFACTORY&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/ufactory_xarm7/LICENSE&#34;&gt;BSD-3-Clause&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Gen3&lt;/td&gt; &#xA;   &lt;td&gt;Kinova Robotics&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/kinova_gen3/LICENSE&#34;&gt;BSD-3-Clause&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Bipeds.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Maker&lt;/th&gt; &#xA;   &lt;th&gt;DoFs&lt;/th&gt; &#xA;   &lt;th&gt;License&lt;/th&gt; &#xA;   &lt;th&gt;MJX&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Cassie&lt;/td&gt; &#xA;   &lt;td&gt;Agility Robotics&lt;/td&gt; &#xA;   &lt;td&gt;28&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/agility_cassie/LICENSE&#34;&gt;BSD-3-Clause&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dual Arms.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Maker&lt;/th&gt; &#xA;   &lt;th&gt;DoFs&lt;/th&gt; &#xA;   &lt;th&gt;License&lt;/th&gt; &#xA;   &lt;th&gt;MJX&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ALOHA 2&lt;/td&gt; &#xA;   &lt;td&gt;Trossen Robotics, Google DeepMind&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/aloha/LICENSE&#34;&gt;BSD-3-Clause&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✔️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Drones.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Maker&lt;/th&gt; &#xA;   &lt;th&gt;DoFs&lt;/th&gt; &#xA;   &lt;th&gt;License&lt;/th&gt; &#xA;   &lt;th&gt;MJX&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Crazyflie 2&lt;/td&gt; &#xA;   &lt;td&gt;Bitcraze&lt;/td&gt; &#xA;   &lt;td&gt;0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/bitcraze_crazyflie_2/LICENSE&#34;&gt;MIT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Skydio X2&lt;/td&gt; &#xA;   &lt;td&gt;Skydio&lt;/td&gt; &#xA;   &lt;td&gt;0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/skydio_x2/LICENSE&#34;&gt;Apache-2.0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;End-effectors.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Maker&lt;/th&gt; &#xA;   &lt;th&gt;DoFs&lt;/th&gt; &#xA;   &lt;th&gt;License&lt;/th&gt; &#xA;   &lt;th&gt;MJX&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Allegro Hand V3&lt;/td&gt; &#xA;   &lt;td&gt;Wonik Robotics&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/wonik_allegro/LICENSE&#34;&gt;BSD-2-Clause&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LEAP Hand&lt;/td&gt; &#xA;   &lt;td&gt;Carnegie Mellon University&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/leap_hand/LICENSE&#34;&gt;MIT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Robotiq 2F-85&lt;/td&gt; &#xA;   &lt;td&gt;Robotiq&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/robotiq_2f85/LICENSE&#34;&gt;BSD-2-Clause&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Shadow Hand EM35&lt;/td&gt; &#xA;   &lt;td&gt;Shadow Robot Company&lt;/td&gt; &#xA;   &lt;td&gt;24&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/shadow_hand/LICENSE&#34;&gt;Apache-2.0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Shadow DEX-EE Hand&lt;/td&gt; &#xA;   &lt;td&gt;Shadow Robot Company&lt;/td&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/shadow_dexee/LICENSE&#34;&gt;Apache-2.0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Mobile Manipulators.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Maker&lt;/th&gt; &#xA;   &lt;th&gt;DoFs&lt;/th&gt; &#xA;   &lt;th&gt;License&lt;/th&gt; &#xA;   &lt;th&gt;MJX&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Google Robot&lt;/td&gt; &#xA;   &lt;td&gt;Google DeepMind&lt;/td&gt; &#xA;   &lt;td&gt;9&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/google_robot/LICENSE&#34;&gt;Apache-2.0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Stretch 2&lt;/td&gt; &#xA;   &lt;td&gt;Hello Robot&lt;/td&gt; &#xA;   &lt;td&gt;17&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/hello_robot_stretch/LICENSE&#34;&gt;Clear BSD&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Stretch 3&lt;/td&gt; &#xA;   &lt;td&gt;Hello Robot&lt;/td&gt; &#xA;   &lt;td&gt;17&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/hello_robot_stretch_3/LICENSE&#34;&gt;Apache-2.0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Humanoids.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Maker&lt;/th&gt; &#xA;   &lt;th&gt;DoFs&lt;/th&gt; &#xA;   &lt;th&gt;License&lt;/th&gt; &#xA;   &lt;th&gt;MJX&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Robotis OP3&lt;/td&gt; &#xA;   &lt;td&gt;Robotis&lt;/td&gt; &#xA;   &lt;td&gt;20&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/robotis_op3/LICENSE&#34;&gt;Apache-2.0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Unitree G1&lt;/td&gt; &#xA;   &lt;td&gt;Unitree Robotics&lt;/td&gt; &#xA;   &lt;td&gt;37&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/unitree_g1/LICENSE&#34;&gt;BSD-3-Clause&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Unitree H1&lt;/td&gt; &#xA;   &lt;td&gt;Unitree Robotics&lt;/td&gt; &#xA;   &lt;td&gt;19&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/unitree_h1/LICENSE&#34;&gt;BSD-3-Clause&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TALOS&lt;/td&gt; &#xA;   &lt;td&gt;PAL Robotics&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/pal_talos/LICENSE&#34;&gt;Apache-2.0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Quadrupeds.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Maker&lt;/th&gt; &#xA;   &lt;th&gt;DoFs&lt;/th&gt; &#xA;   &lt;th&gt;License&lt;/th&gt; &#xA;   &lt;th&gt;MJX&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ANYmal B&lt;/td&gt; &#xA;   &lt;td&gt;ANYbotics&lt;/td&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/anybotics_anymal_b/LICENSE&#34;&gt;BSD-3-Clause&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ANYmal C&lt;/td&gt; &#xA;   &lt;td&gt;ANYbotics&lt;/td&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/anybotics_anymal_c/LICENSE&#34;&gt;BSD-3-Clause&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✔️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Spot&lt;/td&gt; &#xA;   &lt;td&gt;Boston Dynamics&lt;/td&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/boston_dynamics_spot/LICENSE&#34;&gt;BSD-3-Clause&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Unitree A1&lt;/td&gt; &#xA;   &lt;td&gt;Unitree Robotics&lt;/td&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/unitree_a1/LICENSE&#34;&gt;BSD-3-Clause&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Unitree Go1&lt;/td&gt; &#xA;   &lt;td&gt;Unitree Robotics&lt;/td&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/unitree_go1/LICENSE&#34;&gt;BSD-3-Clause&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Unitree Go2&lt;/td&gt; &#xA;   &lt;td&gt;Unitree Robotics&lt;/td&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/unitree_go2/LICENSE&#34;&gt;BSD-3-Clause&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✔️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Google Barkour v0&lt;/td&gt; &#xA;   &lt;td&gt;Google DeepMind&lt;/td&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/google_barkour_v0/LICENSE&#34;&gt;Apache-2.0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✔️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Google Barkour vB&lt;/td&gt; &#xA;   &lt;td&gt;Google DeepMind&lt;/td&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/google_barkour_vb/LICENSE&#34;&gt;Apache-2.0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✔️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Biomechanical.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Maker&lt;/th&gt; &#xA;   &lt;th&gt;DoFs&lt;/th&gt; &#xA;   &lt;th&gt;License&lt;/th&gt; &#xA;   &lt;th&gt;MJX&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;flybody&lt;/td&gt; &#xA;   &lt;td&gt;Google DeepMind, HHMI Janelia Research Campus&lt;/td&gt; &#xA;   &lt;td&gt;102&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/flybody/LICENSE&#34;&gt;Apache-2.0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Miscellaneous.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Maker&lt;/th&gt; &#xA;   &lt;th&gt;DoFs&lt;/th&gt; &#xA;   &lt;th&gt;License&lt;/th&gt; &#xA;   &lt;th&gt;MJX&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;D435i&lt;/td&gt; &#xA;   &lt;td&gt;Intel Realsense&lt;/td&gt; &#xA;   &lt;td&gt;0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_menagerie/main/realsense_d435i/LICENSE&#34;&gt;Apache-2.0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Citing Menagerie&lt;/h2&gt; &#xA;&lt;p&gt;If you use Menagerie in your work, please use the following citation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@software{menagerie2022github,&#xA;  author = {Zakka, Kevin and Tassa, Yuval and {MuJoCo Menagerie Contributors}},&#xA;  title = {{MuJoCo Menagerie: A collection of high-quality simulation models for MuJoCo}},&#xA;  url = {http://github.com/google-deepmind/mujoco_menagerie},&#xA;  year = {2022},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;The models in this repository are based on third-party models designed by many talented people, and would not have been possible without their generous open-source contributions. We would like to acknowledge all the designers and engineers who made MuJoCo Menagerie possible.&lt;/p&gt; &#xA;&lt;p&gt;We&#39;d like to thank Pedro Vergani for his help with visuals and design.&lt;/p&gt; &#xA;&lt;p&gt;The main effort required to make this repository publicly available was undertaken by &lt;a href=&#34;https://kzakka.com/&#34;&gt;Kevin Zakka&lt;/a&gt;, with help from the Robotics Simulation team at Google DeepMind.&lt;/p&gt; &#xA;&lt;h2&gt;License and Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;XML and asset files in each individual model directory of this repository are subject to different license terms. Please consult the &lt;code&gt;LICENSE&lt;/code&gt; files under each specific model subdirectory for the relevant license and copyright information.&lt;/p&gt; &#xA;&lt;p&gt;All other content is Copyright 2022 DeepMind Technologies Limited and licensed under the Apache License, Version 2.0. A copy of this license is provided in the top-level LICENSE file in this repository. You can also obtain it from &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;https://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This is not an officially supported Google product.&lt;/p&gt;</summary>
  </entry>
</feed>