<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-06-01T01:41:01Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>pathwaycom/llm-app</title>
    <updated>2025-06-01T01:41:01Z</updated>
    <id>tag:github.com,2025-06-01:/pathwaycom/llm-app</id>
    <link href="https://github.com/pathwaycom/llm-app" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Ready-to-run cloud templates for RAG, AI pipelines, and enterprise search with live data. üê≥Docker-friendly.‚ö°Always in sync with Sharepoint, Google Drive, S3, Kafka, PostgreSQL, real-time data APIs, and more.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;Pathway AI Pipelines&lt;/h1&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://trendshift.io/repositories/4400&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://trendshift.io/api/badge/repositories/4400&#34; alt=&#34;pathwaycom%2Fllm-app | Trendshift&#34; style=&#34;width: 250px; height: 55px;&#34; width=&#34;250&#34; height=&#34;55&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Linux-FCC624?style=for-the-badge&amp;amp;logo=linux&amp;amp;logoColor=black&#34; alt=&#34;Linux&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/mac%20os-000000?style=for-the-badge&amp;amp;logo=apple&amp;amp;logoColor=white&#34; alt=&#34;macOS&#34;&gt; &lt;a href=&#34;https://discord.gg/pathway&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Discord-5865F2?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white&#34; alt=&#34;chat on Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://x.com/intent/follow?screen_name=pathway_com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/X-000000?style=for-the-badge&amp;amp;logo=x&amp;amp;logoColor=white&#34; alt=&#34;follow on X&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Pathway&#39;s &lt;strong&gt;AI Pipelines&lt;/strong&gt; allow you to quickly put in production AI applications that offer &lt;strong&gt;high-accuracy RAG and AI enterprise search at scale&lt;/strong&gt; using the most &lt;strong&gt;up-to-date knowledge&lt;/strong&gt; available in your data sources. It provides you ready-to-deploy &lt;strong&gt;LLM (Large Language Model) App Templates&lt;/strong&gt;. You can test them on your own machine and deploy on-cloud (GCP, AWS, Azure, Render,...) or on-premises.&lt;/p&gt; &#xA;&lt;p&gt;The apps connect and sync (all new data additions, deletions, updates) with data sources on your &lt;strong&gt;file system, Google Drive, Sharepoint, S3, Kafka, PostgreSQL, real-time data APIs&lt;/strong&gt;. They come with no infrastructure dependencies that would need a separate setup. They include &lt;strong&gt;built-in data indexing&lt;/strong&gt; enabling vector search, hybrid search, and full-text search - all done in-memory, with cache.&lt;/p&gt; &#xA;&lt;h2&gt;Application Templates&lt;/h2&gt; &#xA;&lt;p&gt;The application templates provided in this repo scale up to &lt;strong&gt;millions of pages of documents&lt;/strong&gt;. Some of them are optimized for simplicity, some are optimized for amazing accuracy. Pick the one that suits you best. You can use it out of the box, or change some steps of the pipeline - for example, if you would like to add a new data source, or change a Vector Index into a Hybrid Index, it&#39;s just a one-line change.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Application (template)&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pathwaycom/llm-app/main/examples/pipelines/demo-question-answering/&#34;&gt;&lt;code&gt;Question-Answering RAG App&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Basic end-to-end RAG app. A question-answering pipeline that uses the GPT model of choice to provide answers to queries to your documents (PDF, DOCX,...) on a live connected data source (files, Google Drive, Sharepoint,...). You can also try out a &lt;a href=&#34;https://pathway.com/solutions/rag-pipelines#try-it-out&#34;&gt;demo REST endpoint&lt;/a&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pathwaycom/llm-app/main/examples/pipelines/demo-document-indexing/&#34;&gt;&lt;code&gt;Live Document Indexing (Vector Store / Retriever)&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A real-time document indexing pipeline for RAG that acts as a vector store service. It performs live indexing on your documents (PDF, DOCX,...) from a connected data source (files, Google Drive, Sharepoint,...). It can be used with any frontend, or integrated as a retriever backend for a &lt;a href=&#34;https://pathway.com/blog/langchain-integration&#34;&gt;Langchain&lt;/a&gt; or &lt;a href=&#34;https://pathway.com/blog/llamaindex-pathway&#34;&gt;Llamaindex&lt;/a&gt; application. You can also try out a &lt;a href=&#34;https://pathway.com/solutions/ai-contract-management#try-it-out&#34;&gt;demo REST endpoint&lt;/a&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pathwaycom/llm-app/main/examples/pipelines/gpt_4o_multimodal_rag/&#34;&gt;&lt;code&gt;Multimodal RAG pipeline with GPT4o&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Multimodal RAG using GPT-4o in the parsing stage to index PDFs and other documents from a connected data source files, Google Drive, Sharepoint,...). It is perfect for extracting information from unstructured financial documents in your folders (including charts and tables), updating results as documents change or new ones arrive.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pathwaycom/llm-app/main/examples/pipelines/unstructured_to_sql_on_the_fly/&#34;&gt;&lt;code&gt;Unstructured-to-SQL pipeline + SQL question-answering&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A RAG example which connects to unstructured financial data sources (financial report PDFs), structures the data into SQL, and loads it into a PostgreSQL table. It also answers natural language user queries to these financial documents by translating them into SQL using an LLM and executing the query on the PostgreSQL table.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pathwaycom/llm-app/main/examples/pipelines/adaptive-rag/&#34;&gt;&lt;code&gt;Adaptive RAG App&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A RAG application using Adaptive RAG, a technique developed by Pathway to reduce token cost in RAG up to 4x while maintaining accuracy.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pathwaycom/llm-app/main/examples/pipelines/private-rag/&#34;&gt;&lt;code&gt;Private RAG App with Mistral and Ollama&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A fully private (local) version of the &lt;code&gt;demo-question-answering&lt;/code&gt; RAG pipeline using Pathway, Mistral, and Ollama.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pathwaycom/llm-app/main/examples/pipelines/slides_ai_search/&#34;&gt;&lt;code&gt;Slides AI Search App&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;An indexing pipeline for retrieving slides. It performs multi-modal of PowerPoint and PDF and maintains live index of your slides.&#34;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;How do these AI Pipelines work?&lt;/h2&gt; &#xA;&lt;p&gt;The apps can be run as &lt;strong&gt;Docker containers&lt;/strong&gt;, and expose an &lt;strong&gt;HTTP API&lt;/strong&gt; to connect the frontend. To allow quick testing and demos, some app templates also include an optional Streamlit UI which connects to this API.&lt;/p&gt; &#xA;&lt;p&gt;The apps rely on the &lt;a href=&#34;https://github.com/pathwaycom/pathway&#34;&gt;Pathway Live Data framework&lt;/a&gt; for data source synchronization and for serving API requests (Pathway is a standalone Python library with a Rust engine built into it). They bring you a &lt;strong&gt;simple and unified application logic&lt;/strong&gt; for back-end, embedding, retrieval, LLM tech stack. There is no need to integrate and maintain separate modules for your Gen AI app: &lt;del&gt;Vector Database (e.g. Pinecone/Weaviate/Qdrant) + Cache (e.g. Redis) + API Framework (e.g. Fast API)&lt;/del&gt;. Pathway&#39;s default choice of &lt;strong&gt;built-in vector index&lt;/strong&gt; is based on the lightning-fast &lt;a href=&#34;https://github.com/unum-cloud/usearch&#34;&gt;usearch&lt;/a&gt; library, and &lt;strong&gt;hybrid full-text indexes&lt;/strong&gt; make use of &lt;a href=&#34;https://github.com/quickwit-oss/tantivy&#34;&gt;Tantivy&lt;/a&gt; library. Everything works out of the box.&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;Each of the &lt;a href=&#34;https://raw.githubusercontent.com/pathwaycom/llm-app/main/examples/pipelines/&#34;&gt;App templates&lt;/a&gt; in this repo contains a README.md with instructions on how to run it.&lt;/p&gt; &#xA;&lt;p&gt;You can also find &lt;a href=&#34;https://pathway.com/developers/templates/&#34;&gt;more ready-to-run code templates&lt;/a&gt; on the Pathway website.&lt;/p&gt; &#xA;&lt;h2&gt;Some visual highlights&lt;/h2&gt; &#xA;&lt;p&gt;Effortlessly extract and organize table and chart data from PDFs, docs, and more with multimodal RAG - in real-time:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/pathwaycom/llm-app/raw/main/examples/pipelines/gpt_4o_multimodal_rag/gpt4o_with_pathway_comparison.gif&#34; alt=&#34;Effortlessly extract and organize table and chart data from PDFs, docs, and more with multimodal RAG - in real-time&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;(Check out &lt;a href=&#34;https://raw.githubusercontent.com/pathwaycom/llm-app/main/examples/pipelines/gpt_4o_multimodal_rag/&#34;&gt;&lt;code&gt;Multimodal RAG pipeline with GPT4o&lt;/code&gt;&lt;/a&gt; to see the whole pipeline in the works. You may also check out the &lt;a href=&#34;https://raw.githubusercontent.com/pathwaycom/llm-app/main/examples/pipelines/unstructured_to_sql_on_the_fly/&#34;&gt;&lt;code&gt;Unstructured-to-SQL pipeline&lt;/code&gt;&lt;/a&gt; for a minimal example that works with non-multimodal models as well.)&lt;/p&gt; &#xA;&lt;p&gt;Automated real-time knowledge mining and alerting:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pathwaycom/llm-app/main/examples/pipelines/drive_alert/drive_alert_demo.gif&#34; alt=&#34;Automated real-time knowledge mining and alerting&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;(Check out the &lt;a href=&#34;https://github.com/pathwaycom/llm-app/tree/main/examples/pipelines/drive_alert&#34;&gt;&lt;code&gt;Alerting when answers change on Google Drive&lt;/code&gt;&lt;/a&gt; app example.)&lt;/p&gt; &#xA;&lt;h3&gt;Do-it-Yourself Videos&lt;/h3&gt; &#xA;&lt;p&gt;‚ñ∂Ô∏è &lt;a href=&#34;https://www.youtube.com/watch?v=kcrJSk00duw&#34;&gt;An introduction to building LLM apps with Pathway&lt;/a&gt; - by &lt;a href=&#34;https://scholar.google.com/citations?user=Yc94070AAAAJ&#34;&gt;Jan Chorowski&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;‚ñ∂Ô∏è &lt;a href=&#34;https://www.youtube.com/watch?v=k1XGo7ts4tI&#34;&gt;Let&#39;s build a real-world LLM app in 11 minutes&lt;/a&gt; - by &lt;a href=&#34;https://substack.com/@paulabartabajo&#34;&gt;Pau Labarta Bajo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Troubleshooting&lt;/h2&gt; &#xA;&lt;p&gt;To provide feedback or report a bug, please &lt;a href=&#34;https://github.com/pathwaycom/pathway/issues&#34;&gt;raise an issue on our issue tracker&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Anyone who wishes to contribute to this project, whether documentation, features, bug fixes, code cleanup, testing, or code reviews, is very much encouraged to do so. If this is your first contribution to a GitHub project, here is a &lt;a href=&#34;https://docs.github.com/en/get-started/quickstart/contributing-to-projects&#34;&gt;Get Started Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;d like to make a contribution that needs some more work, just raise your hand on the &lt;a href=&#34;https://discord.com/invite/pathway&#34;&gt;Pathway Discord server&lt;/a&gt; (#get-help) and let us know what you are planning!&lt;/p&gt; &#xA;&lt;h2&gt;Supported and maintained by&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/pathwaycom/&#34;&gt;&lt;img src=&#34;https://pathway.com/logo-light.svg?sanitize=true&#34; alt=&#34;Pathway&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://pathway.com/solutions/llm-app&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/See%20Pathway&#39;s%20offering%20for%20AI%20applications-0000FF&#34; alt=&#34;See Pathway&#39;s offering for AI applications&#34;&gt; &lt;/a&gt; &lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>vllm-project/aibrix</title>
    <updated>2025-06-01T01:41:01Z</updated>
    <id>tag:github.com,2025-06-01:/vllm-project/aibrix</id>
    <link href="https://github.com/vllm-project/aibrix" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Cost-efficient and pluggable Infrastructure components for GenAI inference&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AIBrix&lt;/h1&gt; &#xA;&lt;p&gt;Welcome to AIBrix, an open-source initiative designed to provide essential building blocks to construct scalable GenAI inference infrastructure. AIBrix delivers a cloud-native solution optimized for deploying, managing, and scaling large language model (LLM) inference, tailored specifically to enterprise needs.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; | &lt;a href=&#34;https://aibrix.readthedocs.io/latest/&#34;&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://aibrix.github.io/&#34;&gt;&lt;b&gt;Blog&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://github.com/vllm-project/aibrix/raw/main/docs/paper/AIBrix_White_Paper_0219_2025.pdf&#34;&gt;&lt;b&gt;White Paper&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://x.com/vllm_project&#34;&gt;&lt;b&gt;Twitter/X&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://vllm-dev.slack.com/archives/C08EQ883CSV&#34;&gt;&lt;b&gt;Developer Slack&lt;/b&gt;&lt;/a&gt; | &lt;/p&gt; &#xA;&lt;h2&gt;Latest News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2025-05-21]&lt;/strong&gt; AIBrix v0.3.0 is released. Check out the &lt;a href=&#34;https://github.com/vllm-project/aibrix/releases/tag/v0.3.0&#34;&gt;release notes&lt;/a&gt; for more details.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2025-03-09]&lt;/strong&gt; AIBrix v0.2.1 is released. DeepSeek-R1 full weights deployment is supported and gateway stability has been improved! Check &lt;a href=&#34;https://aibrix.github.io/posts/2025-03-10-deepseek-r1/&#34;&gt;Blog Post&lt;/a&gt; for more details.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2025-02-19]&lt;/strong&gt; AIBrix v0.2.0 is released. Check out the &lt;a href=&#34;https://github.com/vllm-project/aibrix/releases/tag/v0.2.0&#34;&gt;release notes&lt;/a&gt; for more details.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Key Features&lt;/h2&gt; &#xA;&lt;p&gt;The initial release includes the following key features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;High-Density LoRA Management&lt;/strong&gt;: Streamlined support for lightweight, low-rank adaptations of models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLM Gateway and Routing&lt;/strong&gt;: Efficiently manage and direct traffic across multiple models and replicas.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLM App-Tailored Autoscaler&lt;/strong&gt;: Dynamically scale inference resources based on real-time demand.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Unified AI Runtime&lt;/strong&gt;: A versatile sidecar enabling metric standardization, model downloading, and management.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Distributed Inference&lt;/strong&gt;: Scalable architecture to handle large workloads across multiple nodes.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Distributed KV Cache&lt;/strong&gt;: Enables high-capacity, cross-engine KV reuse.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Cost-efficient Heterogeneous Serving&lt;/strong&gt;: Enables mixed GPU inference to reduce costs with SLO guarantees.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;GPU Hardware Failure Detection&lt;/strong&gt;: Proactive detection of GPU hardware issues.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Architecture&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/vllm-project/aibrix/main/docs/source/assets/images/aibrix-architecture-v1.jpeg&#34; alt=&#34;aibrix-architecture-v1&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;To get started with AIBrix, clone this repository and follow the setup instructions in the documentation. Our comprehensive guide will help you configure and deploy your first LLM infrastructure seamlessly.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Local Testing&#xA;git clone https://github.com/vllm-project/aibrix.git&#xA;cd aibrix&#xA;&#xA;# Install nightly aibrix dependencies&#xA;kubectl apply -k config/dependency --server-side&#xA;&#xA;# Install nightly aibrix components&#xA;kubectl apply -k config/default&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install stable distribution&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Install component dependencies&#xA;kubectl apply -f &#34;https://github.com/vllm-project/aibrix/releases/download/v0.3.0/aibrix-dependency-v0.3.0.yaml&#34; --server-side&#xA;&#xA;# Install aibrix components&#xA;kubectl apply -f &#34;https://github.com/vllm-project/aibrix/releases/download/v0.3.0/aibrix-core-v0.3.0.yaml&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;For detailed documentation on installation, configuration, and usage, please visit our &lt;a href=&#34;https://aibrix.readthedocs.io/latest/&#34;&gt;documentation page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions from the community! Check out our &lt;a href=&#34;https://raw.githubusercontent.com/vllm-project/aibrix/main/CONTRIBUTING.md&#34;&gt;contributing guidelines&lt;/a&gt; to see how you can make a difference.&lt;/p&gt; &#xA;&lt;p&gt;Slack Channel: &lt;a href=&#34;https://vllm-dev.slack.com/archives/C08EQ883CSV&#34;&gt;#aibrix&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;AIBrix is licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/vllm-project/aibrix/main/LICENSE&#34;&gt;Apache 2.0 License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions or encounter any issues, please submit an issue on our &lt;a href=&#34;https://github.com/vllm-project/aibrix/issues&#34;&gt;GitHub issues page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Thank you for choosing AIBrix for your GenAI infrastructure needs!&lt;/p&gt;</summary>
  </entry>
</feed>