<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-09-11T01:44:35Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Stability-AI/stability-sdk</title>
    <updated>2022-09-11T01:44:35Z</updated>
    <id>tag:github.com,2022-09-11:/Stability-AI/stability-sdk</id>
    <link href="https://github.com/Stability-AI/stability-sdk" rel="alternate"></link>
    <summary type="html">&lt;p&gt;SDK for interacting with stability.ai APIs (e.g. stable diffusion inference)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;stability-clients&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/stability-ai/stability-sdk/blob/main/nbs/demo_colab.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Client implementations that interact with the Stability Generator API&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Install the PyPI package via:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install stability-sdk&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Python client&lt;/h1&gt; &#xA;&lt;p&gt;&lt;code&gt;client.py&lt;/code&gt; is both a command line client and an API class that wraps the gRPC based API. To try the client:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Use Python venv: &lt;code&gt;python3 -m venv pyenv&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Set up in venv dependencies: &lt;code&gt;pyenv/bin/pip3 install -r requirements.txt&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;pyenv/bin/activate&lt;/code&gt; to use the venv.&lt;/li&gt; &#xA; &lt;li&gt;Set the &lt;code&gt;STABILITY_HOST&lt;/code&gt; environment variable. This is by default set to the production endpoint &lt;code&gt;grpc.stability.ai:443&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Set the &lt;code&gt;STABILITY_KEY&lt;/code&gt; environment variable.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Then to invoke:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;python3 client.py -W 512 -H 512 &#34;A stunning house.&#34;&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;It will generate and put PNGs in your current directory.&lt;/p&gt; &#xA;&lt;h2&gt;SDK Usage&lt;/h2&gt; &#xA;&lt;p&gt;See usage demo notebooks in ./nbs&lt;/p&gt; &#xA;&lt;h2&gt;Command line usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;usage: python -m stability_sdk.client [-h] [--height HEIGHT] [--width WIDTH]&#xA;&#x9;  &#x9;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;  [--cfg_scale CFG_SCALE] [--sampler SAMPLER] [--steps STEPS]&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;  [--seed SEED] [--prefix PREFIX] [--no-store]&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;  [--num_samples NUM_SAMPLES] [--show]&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;  prompt [prompt ...]&#xA;&#xA;positional arguments:&#xA;  prompt&#xA;&#xA;optional arguments:&#xA;  -h, --help            show this help message and exit&#xA;  --height HEIGHT, -H HEIGHT&#xA;                        [512] height of image&#xA;  --width WIDTH, -W WIDTH&#xA;                        [512] width of image&#xA;  --cfg_scale CFG_SCALE, -C CFG_SCALE&#xA;                        [7.0] CFG scale factor&#xA;  --sampler SAMPLER, -A SAMPLER&#xA;                        [k_lms] (ddim, plms, k_euler, k_euler_ancestral,&#xA;                        k_heun, k_dpm_2, k_dpm_2_ancestral, k_lms)&#xA;  --steps STEPS, -s STEPS&#xA;                        [50] number of steps&#xA;  --seed SEED, -S SEED  random seed to use&#xA;  --prefix PREFIX, -p PREFIX&#xA;                        output prefixes for artifacts&#xA;  --no-store            do not write out artifacts&#xA;  --num_samples NUM_SAMPLES, -n NUM_SAMPLES&#xA;                        number of samples to generate&#xA;  --show                open artifacts using PIL&#xA;  --engine, -e          engine to use for inference&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Connecting to the API in using langauges other than python&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;src&lt;/code&gt; subdirectory contains pre-compiled gRPC stubs for the following languages:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Stability-AI/stability-sdk/tree/main/src/js&#34;&gt;Javascript/Typescript&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If a language you would like to connect to the API with is not listed above, you can use the following protobuf definition to compile stubs for your language:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Stability-AI/stability-sdk/tree/ecma_clients/src/proto&#34;&gt;protobuf spec&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>altryne/sd-webui-colab</title>
    <updated>2022-09-11T01:44:35Z</updated>
    <id>tag:github.com,2022-09-11:/altryne/sd-webui-colab</id>
    <link href="https://github.com/altryne/sd-webui-colab" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A repo for the maintenance of the Colab version of stable-diffusion-webui repo&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Colab version of the amazing stable diffusion web ui repo located &lt;a href=&#34;https://github.com/hlky/stable-diffusion&#34;&gt;here&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;See the original repo for all the features&lt;/p&gt; &#xA;&lt;p&gt;This repo is for the maintenance of the Colab script itself&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/altryne/sd-webui-colab/blob/main/Stable_Diffusion_WebUi_Altryne.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Super simple to get started&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/463317/187118517-6c5cbe01-84d1-4e39-97e7-9ffd85c00d2a.jpg&#34; alt=&#34;CleanShot 2022-08-28 at 21 40 05@2x&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;After it loads everything, it &lt;strong&gt;WILL&lt;/strong&gt; fail and send you a notification, it&#39;s ok&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download the stable diffusion model (s-d-v1-4.ckpt) file from &lt;a href=&#34;https://huggingface.co/CompVis/stable-diffusion-v1-4&#34;&gt;Hugging Face Stable Diffusion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Create a folder in the root of your Google Drive and name it &lt;code&gt;AI&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Upload the downloaded file into Ai/models (create the models folder)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Then click Runtime -&amp;gt; Run After It will ask you to connect your google drive to find the model.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/463317/187119142-860634f0-2d55-4e6b-b54d-a7d8b510db36.jpg&#34; alt=&#34;CleanShot 2022-08-28 at 21 47 03@2x&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The keep running the cells or again hit Runtime -&amp;gt; Run After&lt;/p&gt; &#xA;&lt;p&gt;After all cells have run, time to launcht he webui!&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/463317/187121016-26b24be2-3317-4f24-a8c0-5f917fa4ded2.jpg&#34; alt=&#34;CleanShot 2022-08-28 at 22 05 13@2x&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;TADA&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/463317/187121044-40210fd8-ca80-4bab-bd90-3b749e06c8fb.jpg&#34; alt=&#34;CleanShot 2022-08-28 at 22 06 42@2x&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>dennybritz/reinforcement-learning</title>
    <updated>2022-09-11T01:44:35Z</updated>
    <id>tag:github.com,2022-09-11:/dennybritz/reinforcement-learning</id>
    <link href="https://github.com/dennybritz/reinforcement-learning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Implementation of Reinforcement Learning Algorithms. Python, OpenAI Gym, Tensorflow. Exercises and Solutions to accompany Sutton&#39;s Book and David Silver&#39;s course.&lt;/p&gt;&lt;hr&gt;&lt;h3&gt;Overview&lt;/h3&gt; &#xA;&lt;p&gt;This repository provides code, exercises and solutions for popular Reinforcement Learning algorithms. These are meant to serve as a learning tool to complement the theoretical materials from&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://incompleteideas.net/book/RLbook2018.pdf&#34;&gt;Reinforcement Learning: An Introduction (2nd Edition)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html&#34;&gt;David Silver&#39;s Reinforcement Learning Course&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Each folder in corresponds to one or more chapters of the above textbook and/or course. In addition to exercises and solution, each folder also contains a list of learning goals, a brief concept summary, and links to the relevant readings.&lt;/p&gt; &#xA;&lt;p&gt;All code is written in Python 3 and uses RL environments from &lt;a href=&#34;https://gym.openai.com/&#34;&gt;OpenAI Gym&lt;/a&gt;. Advanced techniques use &lt;a href=&#34;https://www.tensorflow.org/&#34;&gt;Tensorflow&lt;/a&gt; for neural network implementations.&lt;/p&gt; &#xA;&lt;h3&gt;Table of Contents&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dennybritz/reinforcement-learning/master/Introduction/&#34;&gt;Introduction to RL problems &amp;amp; OpenAI Gym&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dennybritz/reinforcement-learning/master/MDP/&#34;&gt;MDPs and Bellman Equations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dennybritz/reinforcement-learning/master/DP/&#34;&gt;Dynamic Programming: Model-Based RL, Policy Iteration and Value Iteration&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dennybritz/reinforcement-learning/master/MC/&#34;&gt;Monte Carlo Model-Free Prediction &amp;amp; Control&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dennybritz/reinforcement-learning/master/TD/&#34;&gt;Temporal Difference Model-Free Prediction &amp;amp; Control&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dennybritz/reinforcement-learning/master/FA/&#34;&gt;Function Approximation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dennybritz/reinforcement-learning/master/DQN/&#34;&gt;Deep Q Learning&lt;/a&gt; (WIP)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dennybritz/reinforcement-learning/master/PolicyGradient/&#34;&gt;Policy Gradient Methods&lt;/a&gt; (WIP)&lt;/li&gt; &#xA; &lt;li&gt;Learning and Planning (WIP)&lt;/li&gt; &#xA; &lt;li&gt;Exploration and Exploitation (WIP)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;List of Implemented Algorithms&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dennybritz/reinforcement-learning/master/DP/Policy%20Evaluation%20Solution.ipynb&#34;&gt;Dynamic Programming Policy Evaluation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dennybritz/reinforcement-learning/master/DP/Policy%20Iteration%20Solution.ipynb&#34;&gt;Dynamic Programming Policy Iteration&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dennybritz/reinforcement-learning/master/DP/Value%20Iteration%20Solution.ipynb&#34;&gt;Dynamic Programming Value Iteration&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dennybritz/reinforcement-learning/master/MC/MC%20Prediction%20Solution.ipynb&#34;&gt;Monte Carlo Prediction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dennybritz/reinforcement-learning/master/MC/MC%20Control%20with%20Epsilon-Greedy%20Policies%20Solution.ipynb&#34;&gt;Monte Carlo Control with Epsilon-Greedy Policies&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dennybritz/reinforcement-learning/master/MC/Off-Policy%20MC%20Control%20with%20Weighted%20Importance%20Sampling%20Solution.ipynb&#34;&gt;Monte Carlo Off-Policy Control with Importance Sampling&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dennybritz/reinforcement-learning/master/TD/SARSA%20Solution.ipynb&#34;&gt;SARSA (On Policy TD Learning)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dennybritz/reinforcement-learning/master/TD/Q-Learning%20Solution.ipynb&#34;&gt;Q-Learning (Off Policy TD Learning)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dennybritz/reinforcement-learning/master/FA/Q-Learning%20with%20Value%20Function%20Approximation%20Solution.ipynb&#34;&gt;Q-Learning with Linear Function Approximation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dennybritz/reinforcement-learning/master/DQN/Deep%20Q%20Learning%20Solution.ipynb&#34;&gt;Deep Q-Learning for Atari Games&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dennybritz/reinforcement-learning/master/DQN/Double%20DQN%20Solution.ipynb&#34;&gt;Double Deep-Q Learning for Atari Games&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Deep Q-Learning with Prioritized Experience Replay (WIP)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dennybritz/reinforcement-learning/master/PolicyGradient/CliffWalk%20REINFORCE%20with%20Baseline%20Solution.ipynb&#34;&gt;Policy Gradient: REINFORCE with Baseline&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dennybritz/reinforcement-learning/master/PolicyGradient/CliffWalk%20Actor%20Critic%20Solution.ipynb&#34;&gt;Policy Gradient: Actor Critic with Baseline&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dennybritz/reinforcement-learning/master/PolicyGradient/Continuous%20MountainCar%20Actor%20Critic%20Solution.ipynb&#34;&gt;Policy Gradient: Actor Critic with Baseline for Continuous Action Spaces&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Deterministic Policy Gradients for Continuous Action Spaces (WIP)&lt;/li&gt; &#xA; &lt;li&gt;Deep Deterministic Policy Gradients (DDPG) (WIP)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dennybritz/reinforcement-learning/master/PolicyGradient/a3c&#34;&gt;Asynchronous Advantage Actor Critic (A3C)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Resources&lt;/h3&gt; &#xA;&lt;p&gt;Textbooks:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://incompleteideas.net/book/RLbook2018.pdf&#34;&gt;Reinforcement Learning: An Introduction (2nd Edition)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Classes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html&#34;&gt;David Silver&#39;s Reinforcement Learning Course (UCL, 2015)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://rll.berkeley.edu/deeprlcourse/&#34;&gt;CS294 - Deep Reinforcement Learning (Berkeley, Fall 2015)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.udacity.com/course/reinforcement-learning--ud600&#34;&gt;CS 8803 - Reinforcement Learning (Georgia Tech)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cs.uwaterloo.ca/~ppoupart/teaching/cs885-spring18/&#34;&gt;CS885 - Reinforcement Learning (UWaterloo), Spring 2018&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://rail.eecs.berkeley.edu/deeprlcourse/&#34;&gt;CS294-112 - Deep Reinforcement Learning (UC Berkeley)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Talks/Tutorials:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://videolectures.net/deeplearning2016_pineau_reinforcement_learning/&#34;&gt;Introduction to Reinforcement Learning (Joelle Pineau @ Deep Learning Summer School 2016)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://videolectures.net/deeplearning2016_abbeel_deep_reinforcement/&#34;&gt;Deep Reinforcement Learning (Pieter Abbeel @ Deep Learning Summer School 2016)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://techtalks.tv/talks/deep-reinforcement-learning/62360/&#34;&gt;Deep Reinforcement Learning ICML 2016 Tutorial (David Silver)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=ggqnxyjaKe4&#34;&gt;Tutorial: Introduction to Reinforcement Learning with Function Approximation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLjKEIQlKCTZYN3CYBlj8r58SbNorobqcp&#34;&gt;John Schulman - Deep Reinforcement Learning (4 Lectures)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://people.eecs.berkeley.edu/~pabbeel/nips-tutorial-policy-optimization-Schulman-Abbeel.pdf&#34;&gt;Deep Reinforcement Learning Slides @ NIPS 2016&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://spinningup.openai.com/en/latest/user/introduction.html&#34;&gt;OpenAI Spinning Up&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLqYmG7hTraZDNJre23vqCGIVpfZ_K2RZs&#34;&gt;Advanced Deep Learning &amp;amp; Reinforcement Learning (UCL 2018, DeepMind)&lt;/a&gt; -&lt;a href=&#34;https://sites.google.com/view/deep-rl-bootcamp/lectures&#34;&gt;Deep RL Bootcamp&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Other Projects:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/carpedm20/deep-rl-tensorflow&#34;&gt;carpedm20/deep-rl-tensorflow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/matthiasplappert/keras-rl&#34;&gt;matthiasplappert/keras-rl&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Selected Papers:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.readcube.com/articles/10.1038/nature14236&#34;&gt;Human-Level Control through Deep Reinforcement Learning (2015-02)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://arxiv.org/abs/1509.06461&#34;&gt;Deep Reinforcement Learning with Double Q-learning (2015-09)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1509.02971&#34;&gt;Continuous control with deep reinforcement learning (2015-09)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://arxiv.org/abs/1511.05952&#34;&gt;Prioritized Experience Replay (2015-11)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://arxiv.org/abs/1511.06581&#34;&gt;Dueling Network Architectures for Deep Reinforcement Learning (2015-11)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://arxiv.org/abs/1602.01783&#34;&gt;Asynchronous Methods for Deep Reinforcement Learning (2016-02)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://arxiv.org/abs/1603.01121&#34;&gt;Deep Reinforcement Learning from Self-Play in Imperfect-Information Games (2016-03)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gogameguru.com/i/2016/03/deepmind-mastering-go.pdf&#34;&gt;Mastering the game of Go with deep neural networks and tree search&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>