<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-17T01:56:52Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>state-spaces/s4</title>
    <updated>2023-12-17T01:56:52Z</updated>
    <id>tag:github.com,2023-12-17:/state-spaces/s4</id>
    <link href="https://github.com/state-spaces/s4" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Structured state space sequence models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Structured State Spaces for Sequence Modeling&lt;/h1&gt; &#xA;&lt;p&gt;This repository provides the official implementations and experiments for models related to &lt;a href=&#34;https://arxiv.org/abs/2111.00396&#34;&gt;S4&lt;/a&gt;, including &lt;a href=&#34;https://arxiv.org/abs/2008.07669&#34;&gt;HiPPO&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2110.13985&#34;&gt;LSSL&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2202.09729&#34;&gt;SaShiMi&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2203.14343&#34;&gt;DSS&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2206.12037&#34;&gt;HTTYH&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2206.11893&#34;&gt;S4D&lt;/a&gt;, and &lt;a href=&#34;https://arxiv.org/abs/2210.06583&#34;&gt;S4ND&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Project-specific information for each of these models, including overview of the source code and specific experiment reproductions, can be found under &lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/models/&#34;&gt;models/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;p&gt;Setting up the environment and porting S4 to external codebases:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/#setup&#34;&gt;Setup&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/#getting-started-with-s4&#34;&gt;Getting Started with S4&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Using this repository for training models:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/#training&#34;&gt;Training&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/#generation&#34;&gt;Generation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/#overall-repository-structure&#34;&gt;Repository Structure&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Changelog&lt;/h3&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/CHANGELOG.md&#34;&gt;CHANGELOG.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Roadmap&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;More documentation for training from scratch using this repository&lt;/li&gt; &#xA; &lt;li&gt;Compilation of S4 resources and implementations&lt;/li&gt; &#xA; &lt;li&gt;pip package&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;p&gt;This repository requires Python 3.9+ and Pytorch 1.10+. It has been tested up to Pytorch 1.13.1. Other packages are listed in &lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/requirements.txt&#34;&gt;requirements.txt&lt;/a&gt;. Some care may be needed to make some of the library versions compatible, particularly torch/torchvision/torchaudio/torchtext.&lt;/p&gt; &#xA;&lt;p&gt;Example installation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda install pytorch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 pytorch-cuda=11.6 -c pytorch -c nvidia&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Structured Kernels&lt;/h3&gt; &#xA;&lt;p&gt;A core operation of S4 are the Cauchy and Vandermonde kernels described in the &lt;a href=&#34;https://arxiv.org/abs/2111.00396&#34;&gt;paper&lt;/a&gt;. These are very simple matrix multiplications; a naive implementation of these operation can be found in the &lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/models/s4/s4.py&#34;&gt;standalone&lt;/a&gt; in the function &lt;code&gt;cauchy_naive&lt;/code&gt; and &lt;code&gt;log_vandermonde_naive&lt;/code&gt;. However, as the paper describes, this has suboptimal memory usage that currently requires a custom kernel to overcome in PyTorch.&lt;/p&gt; &#xA;&lt;p&gt;Two more efficient methods are supported. The code will automatically detect if either of these is installed and call the appropriate kernel.&lt;/p&gt; &#xA;&lt;h4&gt;Custom CUDA Kernel&lt;/h4&gt; &#xA;&lt;p&gt;This version is faster but requires manual compilation for each machine environment. Run &lt;code&gt;python setup.py install&lt;/code&gt; from the directory &lt;code&gt;extensions/kernels/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Pykeops&lt;/h4&gt; &#xA;&lt;p&gt;This version is provided by the &lt;a href=&#34;https://www.kernel-operations.io/keops/python/installation.html&#34;&gt;pykeops library&lt;/a&gt;. Installation usually works out of the box with &lt;code&gt;pip install pykeops cmake&lt;/code&gt; which are also listed in the requirements file.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started with S4&lt;/h2&gt; &#xA;&lt;h3&gt;S4 Module&lt;/h3&gt; &#xA;&lt;p&gt;Self-contained files for the S4 layer and variants can be found in &lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/models/s4/&#34;&gt;models/s4/&lt;/a&gt;, which includes instructions for calling the module.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/notebooks/&#34;&gt;notebooks/&lt;/a&gt; for visualizations explaining some concepts behind HiPPO and S4.&lt;/p&gt; &#xA;&lt;h3&gt;Example Train Script (External Usage)&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/example.py&#34;&gt;example.py&lt;/a&gt; is a self-contained training script for MNIST and CIFAR that imports the standalone S4 file. The default settings &lt;code&gt;python example.py&lt;/code&gt; reaches 88% accuracy on sequential CIFAR with a very simple S4D model of 200k parameters. This script can be used as an example for using S4 variants in external repositories.&lt;/p&gt; &#xA;&lt;h3&gt;Training with this Repository (Internal Usage)&lt;/h3&gt; &#xA;&lt;p&gt;This repository aims to provide a very flexible framework for training sequence models. Many models and datasets are supported.&lt;/p&gt; &#xA;&lt;p&gt;The basic entrypoint is &lt;code&gt;python -m train&lt;/code&gt;, or equivalently&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m train pipeline=mnist model=s4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;which trains an S4 model on the Permuted MNIST dataset. This should get to around 90% after 1 epoch which takes 1-3 minutes depending on GPU.&lt;/p&gt; &#xA;&lt;p&gt;More examples of using this repository are documented throughout. See &lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/#training&#34;&gt;Training&lt;/a&gt; for an overview.&lt;/p&gt; &#xA;&lt;h3&gt;Optimizer Hyperparameters&lt;/h3&gt; &#xA;&lt;p&gt;One important feature of this codebase is supporting parameters that require different optimizer hyperparameters. In particular, the SSM kernel is particularly sensitive to the $(A, B)$ (and sometimes $\Delta$ parameters), so the learning rate on these parameters is sometimes lowered and the weight decay is always set to $0$.&lt;/p&gt; &#xA;&lt;p&gt;See the method &lt;code&gt;register&lt;/code&gt; in the model (e.g. &lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/py&#34;&gt;s4d.py&lt;/a&gt;) and the function &lt;code&gt;setup_optimizer&lt;/code&gt; in the training script (e.g. &lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/example.py&#34;&gt;example.py&lt;/a&gt;) for an examples of how to implement this in external repos.&lt;/p&gt; &#xA;&lt;!--&#xA;Our logic for setting these parameters can be found in the `OptimModule` class under `src/models/sequence/ss/kernel.py` and the corresponding optimizer hook in `SequenceLightningModule.configure_optimizers` under `train.py`&#xA;--&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;The core training infrastructure of this repository is based on &lt;a href=&#34;https://pytorch-lightning.readthedocs.io/en/latest/&#34;&gt;Pytorch-Lightning&lt;/a&gt; with a configuration scheme based on &lt;a href=&#34;https://hydra.cc/docs/intro/&#34;&gt;Hydra&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The main entrypoint is &lt;code&gt;train.py&lt;/code&gt; and configs are found in &lt;code&gt;configs/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Data&lt;/h3&gt; &#xA;&lt;p&gt;Basic datasets are auto-downloaded, including MNIST, CIFAR, and Speech Commands. All logic for creating and loading datasets is in &lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/src/dataloaders/&#34;&gt;src/dataloaders&lt;/a&gt; directory. The README inside this subdirectory documents how to download and organize other datasets.&lt;/p&gt; &#xA;&lt;h3&gt;Models&lt;/h3&gt; &#xA;&lt;p&gt;Models are defined in &lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/src/models&#34;&gt;src/models&lt;/a&gt;. See the README in this subdirectory for an overview.&lt;/p&gt; &#xA;&lt;h3&gt;Configs and Hyperparameters&lt;/h3&gt; &#xA;&lt;p&gt;Pre-defined configs reproducing end-to-end experiments from the papers are provided, found under project-specific information in &lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/models/&#34;&gt;models/&lt;/a&gt;, such as for the &lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/models/s4/experiments.md&#34;&gt;original S4 paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Configs can also be easily modified through the command line. An example experiment is&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m train pipeline=mnist dataset.permute=True model=s4 model.n_layers=3 model.d_model=128 model.norm=batch model.prenorm=True wandb=null&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This uses the Permuted MNIST task with an S4 model with a specified number of layers, backbone dimension, and normalization type.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/configs/&#34;&gt;configs/README.md&lt;/a&gt; for more detailed documentation about the configs.&lt;/p&gt; &#xA;&lt;h4&gt;Hydra&lt;/h4&gt; &#xA;&lt;p&gt;It is recommended to read the &lt;a href=&#34;https://hydra.cc/docs/intro/&#34;&gt;Hydra documentation&lt;/a&gt; to fully understand the configuration framework. For help launching specific experiments, please file an issue.&lt;/p&gt; &#xA;&lt;!--&#xA;#### Registries&#xA;&#xA;This codebase uses a modification of the hydra `instantiate` utility that provides shorthand names of different classes, for convenience in configuration and logging.&#xA;The mapping from shorthand to full path can be found in `src/utils/registry.py`.&#xA;--&gt; &#xA;&lt;h3&gt;Resuming&lt;/h3&gt; &#xA;&lt;p&gt;Each experiment will be logged to its own directory (generated by Hydra) of the form &lt;code&gt;./outputs/&amp;lt;date&amp;gt;/&amp;lt;time&amp;gt;/&lt;/code&gt;. Checkpoints will be saved here inside this folder and printed to console whenever a new checkpoint is created. To resume training, simply point to the desired &lt;code&gt;.ckpt&lt;/code&gt; file (a PyTorch Lightning checkpoint, e.g. &lt;code&gt;./outputs/&amp;lt;date&amp;gt;/&amp;lt;time&amp;gt;/checkpoints/val/loss.ckpt&lt;/code&gt;) and append the flag &lt;code&gt;train.ckpt=&amp;lt;path&amp;gt;/&amp;lt;to&amp;gt;/&amp;lt;checkpoint&amp;gt;.ckpt&lt;/code&gt; to the original training command.&lt;/p&gt; &#xA;&lt;h3&gt;PyTorch Lightning Trainer&lt;/h3&gt; &#xA;&lt;p&gt;The PTL &lt;a href=&#34;https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html&#34;&gt;Trainer&lt;/a&gt; class controls the overall training loop and also provides many useful pre-defined flags. Some useful examples are explained below. The full list of allowable flags can be found in the PTL documentation, as well as our &lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/configs/trainer/&#34;&gt;trainer configs&lt;/a&gt;. See the default trainer config &lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/configs/trainer/default.yaml&#34;&gt;configs/trainer/default.yaml&lt;/a&gt; for the most useful options.&lt;/p&gt; &#xA;&lt;h4&gt;Multi-GPU training&lt;/h4&gt; &#xA;&lt;p&gt;Simply pass in &lt;code&gt;trainer.gpus=2&lt;/code&gt; to train with 2 GPUs.&lt;/p&gt; &#xA;&lt;h4&gt;Inspect model layers&lt;/h4&gt; &#xA;&lt;p&gt;&lt;code&gt;trainer.weights_summary=full&lt;/code&gt; prints out every layer of the model with their parameter counts. Useful for debugging internals of models.&lt;/p&gt; &#xA;&lt;h4&gt;Data subsampling&lt;/h4&gt; &#xA;&lt;p&gt;&lt;code&gt;trainer.limit_{train,val}_batches={10,0.1}&lt;/code&gt; trains (validates) on only 10 batches (0.1 fraction of all batches). Useful for testing the train loop without going through all the data.&lt;/p&gt; &#xA;&lt;h3&gt;WandB&lt;/h3&gt; &#xA;&lt;p&gt;Logging with &lt;a href=&#34;https://wandb.ai/site&#34;&gt;WandB&lt;/a&gt; is built into this repository. In order to use this, simply set your &lt;code&gt;WANDB_API_KEY&lt;/code&gt; environment variable, and change the &lt;code&gt;wandb.project&lt;/code&gt; attribute of &lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/configs/config.yaml&#34;&gt;configs/config.yaml&lt;/a&gt; (or pass it on the command line e.g. &lt;code&gt;python -m train .... wandb.project=s4&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;p&gt;Set &lt;code&gt;wandb=null&lt;/code&gt; to turn off WandB logging.&lt;/p&gt; &#xA;&lt;h2&gt;Generation&lt;/h2&gt; &#xA;&lt;p&gt;Autoregressive generation can be performed with the &lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/generate.py&#34;&gt;generate.py&lt;/a&gt; script. This script can be used in two ways after training a model using this codebase.&lt;/p&gt; &#xA;&lt;h3&gt;Option 1: Checkpoint Path&lt;/h3&gt; &#xA;&lt;p&gt;The more flexible option requires the checkpoint path of the trained PyTorch Lightning model. The generation script accepts the same config options as the train script, with a few additional flags that are documented in &lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/configs/generate.yaml&#34;&gt;configs/generate.yaml&lt;/a&gt;. After training with &lt;code&gt;python -m train &amp;lt;train flags&amp;gt;&lt;/code&gt;, generate with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m generate &amp;lt;train flags&amp;gt; checkpoint_path=&amp;lt;path/to/model.ckpt&amp;gt; &amp;lt;generation flags&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Any of the flags found in the config can be overridden.&lt;/p&gt; &#xA;&lt;p&gt;Note: This option can be used with either &lt;code&gt;.ckpt&lt;/code&gt; checkpoints (PyTorch Lightning, which includes information for the Trainer) or &lt;code&gt;.pt&lt;/code&gt; checkpoints (PyTorch, which is just a model state dict).&lt;/p&gt; &#xA;&lt;h3&gt;Option 2: Experiment Path&lt;/h3&gt; &#xA;&lt;p&gt;The second option for generation does not require passing in training flags again, and instead reads the config from the Hydra experiment folder, along with a PyTorch Lightning checkpoint within the experiment folder.&lt;/p&gt; &#xA;&lt;h3&gt;Example 1 (Language)&lt;/h3&gt; &#xA;&lt;p&gt;Download the &lt;a href=&#34;https://huggingface.co/krandiash/sashimi-release/tree/main/checkpoints&#34;&gt;WikiText-103 model checkpoint&lt;/a&gt;, for example to &lt;code&gt;./checkpoints/s4-wt103.pt&lt;/code&gt;. This model was trained with the command &lt;code&gt;python -m train experiment=lm/s4-wt103&lt;/code&gt;. Note that from the config we can see that the model was trained with a receptive field of length 8192.&lt;/p&gt; &#xA;&lt;p&gt;To generate, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m generate experiment=lm/s4-wt103 checkpoint_path=checkpoints/s4-wt103.pt n_samples=1 l_sample=16384 l_prefix=8192 decode=text&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This generates a sample of length 16384 conditioned on a prefix of length 8192.&lt;/p&gt; &#xA;&lt;h3&gt;Example 2 (Audio)&lt;/h3&gt; &#xA;&lt;p&gt;Let&#39;s train a small SaShiMi model on the SC09 dataset. We can also reduce the number of training and validation batches to get a checkpoint faster:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m train experiment=audio/sashimi-sc09 model.n_layers=2 trainer.limit_train_batches=0.1 trainer.limit_val_batches=0.1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After the first epoch completes, a message is printed indicating where the checkpoint is saved.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Epoch 0, global step 96: val/loss reached 3.71754 (best 3.71754), saving model to &#34;&amp;lt;repository&amp;gt;/outputs/&amp;lt;date&amp;gt;/&amp;lt;time&amp;gt;/checkpoints/val/loss.ckpt&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Option 1:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m generate experiment=audio/sashimi-sc09 model.n_layers=2 checkpoint_path=&amp;lt;repository&amp;gt;/outputs/&amp;lt;date&amp;gt;/&amp;lt;time&amp;gt;/checkpoints/val/loss.ckpt n_samples=4 l_sample=16000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This option redefines the full config so that the model and dataset can be constructed.&lt;/p&gt; &#xA;&lt;p&gt;Option 2:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m generate experiment_path=&amp;lt;repository&amp;gt;/outputs/&amp;lt;date&amp;gt;/&amp;lt;time&amp;gt; checkpoint_path=checkpoints/val/loss.ckpt n_samples=4 l_sample=16000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This option only needs the path to the Hydra experiment folder and the desired checkpoint within.&lt;/p&gt; &#xA;&lt;h2&gt;Overall Repository Structure&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;configs/         Config files for model, data pipeline, training loop, etc.&#xA;data/            Default location of raw data&#xA;extensions/      CUDA extensions (Cauchy and Vandermonde kernels)&#xA;src/             Main source code for models, datasets, etc.&#xA;  callbacks/     Training loop utilities (e.g. checkpointing)&#xA;  dataloaders/   Dataset and dataloader definitions&#xA;  models/        Model definitions&#xA;  tasks/         Encoder/decoder modules to interface between data and model backbone&#xA;  utils/&#xA;models/          Model-specific information (code, experiments, additional resources)&#xA;example.py       Example training script for using S4 externally&#xA;train.py         Training entrypoint for this repo&#xA;generate.py      Autoregressive generation script&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use this codebase, or otherwise found our work valuable, please cite S4 and &lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/s4/main/models/README.md#citations&#34;&gt;other relevant papers&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{gu2022efficiently,&#xA;  title={Efficiently Modeling Long Sequences with Structured State Spaces},&#xA;  author={Gu, Albert and Goel, Karan and R\&#39;e, Christopher},&#xA;  booktitle={The International Conference on Learning Representations ({ICLR})},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>open-mmlab/mmagic</title>
    <updated>2023-12-17T01:56:52Z</updated>
    <id>tag:github.com,2023-12-17:/open-mmlab/mmagic</id>
    <link href="https://github.com/open-mmlab/mmagic" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OpenMMLab Multimodal Advanced, Generative, and Intelligent Creation Toolbox. Unlock the magic ü™Ñ: Generative-AI (AIGC), easy-to-use APIs, awsome model zoo, diffusion models, for text-to-image generation, image/video restoration/enhancement, etc.&lt;/p&gt;&lt;hr&gt;&lt;div id=&#34;top&#34; align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/docs/en/_static/image/mmagic-logo.png&#34; width=&#34;500px&#34;&gt; &#xA; &lt;div&gt;&#xA;  &amp;nbsp;&#xA; &lt;/div&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;font size=&#34;10&#34;&gt;&lt;b&gt;M&lt;/b&gt;ultimodal &lt;b&gt;A&lt;/b&gt;dvanced, &lt;b&gt;G&lt;/b&gt;enerative, and &lt;b&gt;I&lt;/b&gt;ntelligent &lt;b&gt;C&lt;/b&gt;reation (MMagic [em&#39;m√¶d í…™k])&lt;/font&gt; &#xA; &lt;/div&gt; &#xA; &lt;div&gt;&#xA;  &amp;nbsp;&#xA; &lt;/div&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;b&gt;&lt;font size=&#34;5&#34;&gt;OpenMMLab website&lt;/font&gt;&lt;/b&gt; &#xA;  &lt;sup&gt; &lt;a href=&#34;https://openmmlab.com&#34;&gt; &lt;i&gt;&lt;font size=&#34;4&#34;&gt;HOT&lt;/font&gt;&lt;/i&gt; &lt;/a&gt; &lt;/sup&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &#xA;  &lt;b&gt;&lt;font size=&#34;5&#34;&gt;OpenMMLab platform&lt;/font&gt;&lt;/b&gt; &#xA;  &lt;sup&gt; &lt;a href=&#34;https://platform.openmmlab.com&#34;&gt; &lt;i&gt;&lt;font size=&#34;4&#34;&gt;TRY IT OUT&lt;/font&gt;&lt;/i&gt; &lt;/a&gt; &lt;/sup&gt; &#xA; &lt;/div&gt; &#xA; &lt;div&gt;&#xA;  &amp;nbsp;&#xA; &lt;/div&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://pypi.org/project/mmagic/&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/mmagic.svg?sanitize=true&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://mmagic.readthedocs.io/en/latest/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/docs-latest-blue&#34; alt=&#34;docs&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmagic/actions&#34;&gt;&lt;img src=&#34;https://github.com/open-mmlab/mmagic/workflows/build/badge.svg?sanitize=true&#34; alt=&#34;badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/open-mmlab/mmagic&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/open-mmlab/mmagic/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmagic/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/open-mmlab/mmagic.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmagic/issues&#34;&gt;&lt;img src=&#34;https://isitmaintained.com/badge/open/open-mmlab/mmagic.svg?sanitize=true&#34; alt=&#34;open issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmagic/issues&#34;&gt;&lt;img src=&#34;https://isitmaintained.com/badge/resolution/open-mmlab/mmagic.svg?sanitize=true&#34; alt=&#34;issue resolution&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://openxlab.org.cn/apps?search=mmagic&#34;&gt;&lt;img src=&#34;https://cdn-static.openxlab.org.cn/app-center/openxlab_demo.svg?sanitize=true&#34; alt=&#34;Open in OpenXLab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://mmagic.readthedocs.io/en/latest/&#34;&gt;üìòDocumentation&lt;/a&gt; | &lt;a href=&#34;https://mmagic.readthedocs.io/en/latest/get_started/install.html&#34;&gt;üõ†Ô∏èInstallation&lt;/a&gt; | &lt;a href=&#34;https://mmagic.readthedocs.io/en/latest/model_zoo/overview.html&#34;&gt;üìäModel Zoo&lt;/a&gt; | &lt;a href=&#34;https://mmagic.readthedocs.io/en/latest/changelog.html&#34;&gt;üÜïUpdate News&lt;/a&gt; | &lt;a href=&#34;https://github.com/open-mmlab/mmagic/projects&#34;&gt;üöÄOngoing Projects&lt;/a&gt; | &lt;a href=&#34;https://github.com/open-mmlab/mmagic/issues&#34;&gt;ü§îReporting Issues&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;English | &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/README_zh-CN.md&#34;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://openmmlab.medium.com/&#34; style=&#34;text-decoration:none;&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/25839884/218352562-cdded397-b0f3-4ca1-b8dd-a60df8dca75b.png&#34; width=&#34;3%&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png&#34; width=&#34;3%&#34; alt=&#34;&#34;&gt; &#xA; &lt;a href=&#34;https://discord.gg/raweFPmdzG&#34; style=&#34;text-decoration:none;&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/25839884/218347213-c080267f-cbb6-443e-8532-8e1ed9a58ea9.png&#34; width=&#34;3%&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png&#34; width=&#34;3%&#34; alt=&#34;&#34;&gt; &#xA; &lt;a href=&#34;https://twitter.com/OpenMMLab&#34; style=&#34;text-decoration:none;&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/25839884/218346637-d30c8a0f-3eba-4699-8131-512fb06d46db.png&#34; width=&#34;3%&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png&#34; width=&#34;3%&#34; alt=&#34;&#34;&gt; &#xA; &lt;a href=&#34;https://www.youtube.com/openmmlab&#34; style=&#34;text-decoration:none;&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/25839884/218346691-ceb2116a-465a-40af-8424-9f30d2348ca9.png&#34; width=&#34;3%&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;üöÄ What&#39;s New &lt;a&gt;&lt;img width=&#34;35&#34; height=&#34;20&#34; src=&#34;https://user-images.githubusercontent.com/12782558/212848161-5e783dd6-11e8-4fe0-bbba-39ffb77730be.png&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h3&gt;New release &lt;a href=&#34;https://github.com/open-mmlab/mmagic/releases/tag/v1.1.0&#34;&gt;&lt;strong&gt;MMagic v1.1.0&lt;/strong&gt;&lt;/a&gt; [22/09/2023]:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support ViCo, a new SD personalization method. &lt;a href=&#34;https://github.com/open-mmlab/mmagic/raw/main/configs/vico/README.md&#34;&gt;Click to View&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Support AnimateDiff, a popular text2animation method. &lt;a href=&#34;https://github.com/open-mmlab/mmagic/raw/main/configs/animatediff/README.md&#34;&gt;Click to View&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Support SDXLÔºàStable Diffusion XLÔºâ. &lt;a href=&#34;https://github.com/open-mmlab/mmagic/raw/main/configs/stable_diffusion_xl/README.md&#34;&gt;Click to View&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Support DragGAN implementation with MMagic. &lt;a href=&#34;https://github.com/open-mmlab/mmagic/raw/main/configs/draggan/README.md&#34;&gt;Click to View&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Support FastComposer, a new multi-subject text-to-image generation method. &lt;a href=&#34;https://github.com/open-mmlab/mmagic/raw/main/configs/fastcomposer/README.md&#34;&gt;Click to View&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We are excited to announce the release of MMagic v1.0.0 that inherits from &lt;a href=&#34;https://github.com/open-mmlab/mmediting&#34;&gt;MMEditing&lt;/a&gt; and &lt;a href=&#34;https://github.com/open-mmlab/mmgeneration&#34;&gt;MMGeneration&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;After iterative updates with OpenMMLab 2.0 framework and merged with MMGeneration, MMEditing has become a powerful tool that supports low-level algorithms based on both GAN and CNN. Today, MMEditing embraces Generative AI and transforms into a more advanced and comprehensive AIGC toolkit: &lt;strong&gt;MMagic&lt;/strong&gt; (&lt;strong&gt;M&lt;/strong&gt;ultimodal &lt;strong&gt;A&lt;/strong&gt;dvanced, &lt;strong&gt;G&lt;/strong&gt;enerative, and &lt;strong&gt;I&lt;/strong&gt;ntelligent &lt;strong&gt;C&lt;/strong&gt;reation). MMagic will provide more agile and flexible experimental support for researchers and AIGC enthusiasts, and help you on your AIGC exploration journey.&lt;/p&gt; &#xA;&lt;p&gt;We highlight the following new features.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;1. New Models&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We support 11 new models in 4 new tasks.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Text2Image / Diffusion &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ControlNet&lt;/li&gt; &#xA;   &lt;li&gt;DreamBooth&lt;/li&gt; &#xA;   &lt;li&gt;Stable Diffusion&lt;/li&gt; &#xA;   &lt;li&gt;Disco Diffusion&lt;/li&gt; &#xA;   &lt;li&gt;GLIDE&lt;/li&gt; &#xA;   &lt;li&gt;Guided Diffusion&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;3D-aware Generation &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;EG3D&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Image Restoration &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;NAFNet&lt;/li&gt; &#xA;   &lt;li&gt;Restormer&lt;/li&gt; &#xA;   &lt;li&gt;SwinIR&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Image Colorization &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;InstColorization&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;2. Magic Diffusion Model&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;For the Diffusion Model, we provide the following &#34;magic&#34; :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support image generation based on Stable Diffusion and Disco Diffusion.&lt;/li&gt; &#xA; &lt;li&gt;Support Finetune methods such as Dreambooth and DreamBooth LoRA.&lt;/li&gt; &#xA; &lt;li&gt;Support controllability in text-to-image generation using ControlNet.&lt;/li&gt; &#xA; &lt;li&gt;Support acceleration and optimization strategies based on xFormers to improve training and inference efficiency.&lt;/li&gt; &#xA; &lt;li&gt;Support video generation based on MultiFrame Render.&lt;/li&gt; &#xA; &lt;li&gt;Support calling basic models and sampling strategies through DiffuserWrapper.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;3. Upgraded Framework&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;By using MMEngine and MMCV of OpenMMLab 2.0 framework, MMagic has upgraded in the following new features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Refactor DataSample to support the combination and splitting of batch dimensions.&lt;/li&gt; &#xA; &lt;li&gt;Refactor DataPreprocessor and unify the data format for various tasks during training and inference.&lt;/li&gt; &#xA; &lt;li&gt;Refactor MultiValLoop and MultiTestLoop, supporting the evaluation of both generation-type metrics (e.g. FID) and reconstruction-type metrics (e.g. SSIM), and supporting the evaluation of multiple datasets at once.&lt;/li&gt; &#xA; &lt;li&gt;Support visualization on local files or using tensorboard and wandb.&lt;/li&gt; &#xA; &lt;li&gt;Support for 33+ algorithms accelerated by Pytorch 2.0.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;MMagic&lt;/strong&gt; has supported all the tasks, models, metrics, and losses in &lt;a href=&#34;https://github.com/open-mmlab/mmediting&#34;&gt;MMEditing&lt;/a&gt; and &lt;a href=&#34;https://github.com/open-mmlab/mmgeneration&#34;&gt;MMGeneration&lt;/a&gt; and unifies interfaces of all components based on &lt;a href=&#34;https://github.com/open-mmlab/mmengine&#34;&gt;MMEngine&lt;/a&gt; üòç.&lt;/p&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/docs/en/changelog.md&#34;&gt;changelog.md&lt;/a&gt; for details and release history.&lt;/p&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/docs/en/migration/overview.md&#34;&gt;migration documents&lt;/a&gt; to migrate from &lt;a href=&#34;https://github.com/open-mmlab/mmagic/tree/0.x&#34;&gt;old version&lt;/a&gt; MMEditing 0.x to new version MMagic 1.x .&lt;/p&gt; &#xA;&lt;div id=&#34;table&#34; align=&#34;center&#34;&gt;&lt;/div&gt; &#xA;&lt;h2&gt;üìÑ Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#-introduction&#34;&gt;üìñ Introduction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#-contributing&#34;&gt;üôå Contributing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#%EF%B8%8F-installation&#34;&gt;üõ†Ô∏è Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#-model-zoo&#34;&gt;üìä Model Zoo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#-acknowledgement&#34;&gt;ü§ù Acknowledgement&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#%EF%B8%8F-citation&#34;&gt;üñäÔ∏è Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#-license&#34;&gt;üé´ License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#%EF%B8%8F-%EF%B8%8Fopenmmlab-family&#34;&gt;üèóÔ∏è Ô∏èOpenMMLab Family&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üìñ Introduction&lt;/h2&gt; &#xA;&lt;p&gt;MMagic (&lt;strong&gt;M&lt;/strong&gt;ultimodal &lt;strong&gt;A&lt;/strong&gt;dvanced, &lt;strong&gt;G&lt;/strong&gt;enerative, and &lt;strong&gt;I&lt;/strong&gt;ntelligent &lt;strong&gt;C&lt;/strong&gt;reation) is an advanced and comprehensive AIGC toolkit that inherits from &lt;a href=&#34;https://github.com/open-mmlab/mmediting&#34;&gt;MMEditing&lt;/a&gt; and &lt;a href=&#34;https://github.com/open-mmlab/mmgeneration&#34;&gt;MMGeneration&lt;/a&gt;. It is an open-source image and video editing&amp;amp;generating toolbox based on PyTorch. It is a part of the &lt;a href=&#34;https://openmmlab.com/&#34;&gt;OpenMMLab&lt;/a&gt; project.&lt;/p&gt; &#xA;&lt;p&gt;Currently, MMagic support multiple image and video generation/editing tasks.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/49083766/233564593-7d3d48ed-e843-4432-b610-35e3d257765c.mp4&#34;&gt;https://user-images.githubusercontent.com/49083766/233564593-7d3d48ed-e843-4432-b610-35e3d257765c.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;‚ú® Major features&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;State of the Art Models&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;MMagic provides state-of-the-art generative models to process, edit and synthesize images and videos.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Powerful and Popular Applications&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;MMagic supports popular and contemporary image restoration, text-to-image, 3D-aware generation, inpainting, matting, super-resolution and generation applications. Specifically, MMagic supports fine-tuning for stable diffusion and many exciting diffusion&#39;s application such as ControlNet Animation with SAM. MMagic also supports GAN interpolation, GAN projection, GAN manipulations and many other popular GAN‚Äôs applications. It‚Äôs time to begin your AIGC exploration journey!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Efficient Framework&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;By using MMEngine and MMCV of OpenMMLab 2.0 framework, MMagic decompose the editing framework into different modules and one can easily construct a customized editor framework by combining different modules. We can define the training process just like playing with Legos and provide rich components and strategies. In MMagic, you can complete controls on the training process with different levels of APIs. With the support of &lt;a href=&#34;https://github.com/open-mmlab/mmengine/raw/main/mmengine/model/wrappers/seperate_distributed.py&#34;&gt;MMSeparateDistributedDataParallel&lt;/a&gt;, distributed training for dynamic architectures can be easily implemented.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;‚ú® Best Practice&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The best practice on our main branch works with &lt;strong&gt;Python 3.9+&lt;/strong&gt; and &lt;strong&gt;PyTorch 2.0+&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#table&#34;&gt;üîùBack to Table of Contents&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üôå Contributing&lt;/h2&gt; &#xA;&lt;p&gt;More and more community contributors are joining us to make our repo better. Some recent projects are contributed by the community including:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/stable_diffusion_xl/README.md&#34;&gt;SDXL&lt;/a&gt; is contributed by @okotaku.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/animatediff/README.md&#34;&gt;AnimateDiff&lt;/a&gt; is contributed by @ElliotQi.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/vico/README.md&#34;&gt;ViCo&lt;/a&gt; is contributed by @FerryHuang.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/draggan/README.md&#34;&gt;DragGan&lt;/a&gt; is contributed by @qsun1.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/fastcomposer/README.md&#34;&gt;FastComposer&lt;/a&gt; is contributed by @xiaomile.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/projects/README.md&#34;&gt;Projects&lt;/a&gt; is opened to make it easier for everyone to add projects to MMagic.&lt;/p&gt; &#xA;&lt;p&gt;We appreciate all contributions to improve MMagic. Please refer to &lt;a href=&#34;https://github.com/open-mmlab/mmcv/raw/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; in MMCV and &lt;a href=&#34;https://github.com/open-mmlab/mmengine/raw/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; in MMEngine for more details about the contributing guideline.&lt;/p&gt; &#xA;&lt;p align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#table&#34;&gt;üîùBack to Table of Contents&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üõ†Ô∏è Installation&lt;/h2&gt; &#xA;&lt;p&gt;MMagic depends on &lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch&lt;/a&gt;, &lt;a href=&#34;https://github.com/open-mmlab/mmengine&#34;&gt;MMEngine&lt;/a&gt; and &lt;a href=&#34;https://github.com/open-mmlab/mmcv&#34;&gt;MMCV&lt;/a&gt;. Below are quick steps for installation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step 1.&lt;/strong&gt; Install PyTorch following &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;official instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step 2.&lt;/strong&gt; Install MMCV, MMEngine and MMagic with &lt;a href=&#34;https://github.com/open-mmlab/mim&#34;&gt;MIM&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip3 install openmim&#xA;mim install mmcv&amp;gt;=2.0.0&#xA;mim install mmengine&#xA;mim install mmagic&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step 3.&lt;/strong&gt; Verify MMagic has been successfully installed.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd ~&#xA;python -c &#34;import mmagic; print(mmagic.__version__)&#34;&#xA;# Example output: 1.0.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Getting Started&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;After installing MMagic successfully, now you are able to play with MMagic! To generate an image from text, you only need several lines of codes by MMagic!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from mmagic.apis import MMagicInferencer&#xA;sd_inferencer = MMagicInferencer(model_name=&#39;stable_diffusion&#39;)&#xA;text_prompts = &#39;A panda is having dinner at KFC&#39;&#xA;result_out_dir = &#39;output/sd_res.png&#39;&#xA;sd_inferencer.infer(text=text_prompts, result_out_dir=result_out_dir)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/docs/en/get_started/quick_run.md&#34;&gt;quick run&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/docs/en/user_guides/inference.md&#34;&gt;inference&lt;/a&gt; for the basic usage of MMagic.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Install MMagic from source&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can also experiment on the latest developed version rather than the stable release by installing MMagic from source with the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/open-mmlab/mmagic.git&#xA;cd mmagic&#xA;pip3 install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/docs/en/get_started/install.md&#34;&gt;installation&lt;/a&gt; for more detailed instruction.&lt;/p&gt; &#xA;&lt;p align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#table&#34;&gt;üîùBack to Table of Contents&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üìä Model Zoo&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;b&gt;Supported algorithms&lt;/b&gt; &#xA;&lt;/div&gt; &#xA;&lt;table align=&#34;center&#34;&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr align=&#34;center&#34; valign=&#34;bottom&#34;&gt; &#xA;   &lt;td&gt; &lt;b&gt;Conditional GANs&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;Unconditional GANs&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;Image Restoration&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;Image Super-Resolution&lt;/b&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr valign=&#34;top&#34;&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/sngan_proj/README.md&#34;&gt;SNGAN/Projection GAN (ICLR&#39;2018)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/sagan/README.md&#34;&gt;SAGAN (ICML&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/biggan/README.md&#34;&gt;BIGGAN/BIGGAN-DEEP (ICLR&#39;2018)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/dcgan/README.md&#34;&gt;DCGAN (ICLR&#39;2016)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/wgan-gp/README.md&#34;&gt;WGAN-GP (NeurIPS&#39;2017)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/lsgan/README.md&#34;&gt;LSGAN (ICCV&#39;2017)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/ggan/README.md&#34;&gt;GGAN (ArXiv&#39;2017)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/pggan/README.md&#34;&gt;PGGAN (ICLR&#39;2018)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/singan/README.md&#34;&gt;SinGAN (ICCV&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/styleganv1/README.md&#34;&gt;StyleGANV1 (CVPR&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/styleganv2/README.md&#34;&gt;StyleGANV2 (CVPR&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/styleganv3/README.md&#34;&gt;StyleGANV3 (NeurIPS&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/draggan/README.md&#34;&gt;DragGan (2023)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/swinir/README.md&#34;&gt;SwinIR (ICCVW&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/nafnet/README.md&#34;&gt;NAFNet (ECCV&#39;2022)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/restormer/README.md&#34;&gt;Restormer (CVPR&#39;2022)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/srcnn/README.md&#34;&gt;SRCNN (TPAMI&#39;2015)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/srgan_resnet/README.md&#34;&gt;SRResNet&amp;amp;SRGAN (CVPR&#39;2016)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/edsr/README.md&#34;&gt;EDSR (CVPR&#39;2017)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/esrgan/README.md&#34;&gt;ESRGAN (ECCV&#39;2018)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/rdn/README.md&#34;&gt;RDN (CVPR&#39;2018)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/dic/README.md&#34;&gt;DIC (CVPR&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/ttsr/README.md&#34;&gt;TTSR (CVPR&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/glean/README.md&#34;&gt;GLEAN (CVPR&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/liif/README.md&#34;&gt;LIIF (CVPR&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/real_esrgan/README.md&#34;&gt;Real-ESRGAN (ICCVW&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt;   &#xA; &lt;/tbody&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr align=&#34;center&#34; valign=&#34;bottom&#34;&gt; &#xA;   &lt;td&gt; &lt;b&gt;Video Super-Resolution&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;Video Interpolation&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;Image Colorization&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;Image Translation&lt;/b&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr valign=&#34;top&#34;&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/edvr/README.md&#34;&gt;EDVR (CVPR&#39;2018)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/tof/README.md&#34;&gt;TOF (IJCV&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/tdan/README.md&#34;&gt;TDAN (CVPR&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/basicvsr/README.md&#34;&gt;BasicVSR (CVPR&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/iconvsr/README.md&#34;&gt;IconVSR (CVPR&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/basicvsr_pp/README.md&#34;&gt;BasicVSR++ (CVPR&#39;2022)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/real_basicvsr/README.md&#34;&gt;RealBasicVSR (CVPR&#39;2022)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/tof/README.md&#34;&gt;TOFlow (IJCV&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/cain/README.md&#34;&gt;CAIN (AAAI&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/flavr/README.md&#34;&gt;FLAVR (CVPR&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/inst_colorization/README.md&#34;&gt;InstColorization (CVPR&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/pix2pix/README.md&#34;&gt;Pix2Pix (CVPR&#39;2017)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/cyclegan/README.md&#34;&gt;CycleGAN (ICCV&#39;2017)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt;   &#xA; &lt;/tbody&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr align=&#34;center&#34; valign=&#34;bottom&#34;&gt; &#xA;   &lt;td&gt; &lt;b&gt;Inpainting&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;Matting&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;Text-to-Image(Video)&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;3D-aware Generation&lt;/b&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr valign=&#34;top&#34;&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/global_local/README.md&#34;&gt;Global&amp;amp;Local (ToG&#39;2017)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/deepfillv1/README.md&#34;&gt;DeepFillv1 (CVPR&#39;2018)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/partial_conv/README.md&#34;&gt;PConv (ECCV&#39;2018)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/deepfillv2/README.md&#34;&gt;DeepFillv2 (CVPR&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/aot_gan/README.md&#34;&gt;AOT-GAN (TVCG&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/stable_diffusion/README.md&#34;&gt;Stable Diffusion Inpainting (CVPR&#39;2022)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/dim/README.md&#34;&gt;DIM (CVPR&#39;2017)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/indexnet/README.md&#34;&gt;IndexNet (ICCV&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/gca/README.md&#34;&gt;GCA (AAAI&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/projects/glide/configs/README.md&#34;&gt;GLIDE (NeurIPS&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/guided_diffusion/README.md&#34;&gt;Guided Diffusion (NeurIPS&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/disco_diffusion/README.md&#34;&gt;Disco-Diffusion (2022)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/stable_diffusion/README.md&#34;&gt;Stable-Diffusion (2022)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/dreambooth/README.md&#34;&gt;DreamBooth (2022)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/textual_inversion/README.md&#34;&gt;Textual Inversion (2022)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/projects/prompt_to_prompt/README.md&#34;&gt;Prompt-to-Prompt (2022)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/projects/prompt_to_prompt/README.md&#34;&gt;Null-text Inversion (2022)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/controlnet/README.md&#34;&gt;ControlNet (2023)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/controlnet_animation/README.md&#34;&gt;ControlNet Animation (2023)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/stable_diffusion_xl/README.md&#34;&gt;Stable Diffusion XL (2023)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/animatediff/README.md&#34;&gt;AnimateDiff (2023)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/vico/README.md&#34;&gt;ViCo (2023)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/fastcomposer/README.md&#34;&gt;FastComposer (2023)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/eg3d/README.md&#34;&gt;EG3D (CVPR&#39;2022)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt;   &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://mmagic.readthedocs.io/en/latest/model_zoo/overview.html&#34;&gt;model_zoo&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;p align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#table&#34;&gt;üîùBack to Table of Contents&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ü§ù Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;MMagic is an open source project that is contributed by researchers and engineers from various colleges and companies. We wish that the toolbox and benchmark could serve the growing research community by providing a flexible toolkit to reimplement existing methods and develop their own new methods.&lt;/p&gt; &#xA;&lt;p&gt;We appreciate all the contributors who implement their methods or add new features, as well as users who give valuable feedbacks. Thank you all!&lt;/p&gt; &#xA;&lt;a href=&#34;https://github.com/open-mmlab/mmagic/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=open-mmlab/mmagic&#34;&gt; &lt;/a&gt; &#xA;&lt;p align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#table&#34;&gt;üîùBack to Table of Contents&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üñäÔ∏è Citation&lt;/h2&gt; &#xA;&lt;p&gt;If MMagic is helpful to your research, please cite it as below.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{mmagic2023,&#xA;    title = {{MMagic}: {OpenMMLab} Multimodal Advanced, Generative, and Intelligent Creation Toolbox},&#xA;    author = {{MMagic Contributors}},&#xA;    howpublished = {\url{https://github.com/open-mmlab/mmagic}},&#xA;    year = {2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{mmediting2022,&#xA;    title = {{MMEditing}: {OpenMMLab} Image and Video Editing Toolbox},&#xA;    author = {{MMEditing Contributors}},&#xA;    howpublished = {\url{https://github.com/open-mmlab/mmediting}},&#xA;    year = {2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#table&#34;&gt;üîùBack to Table of Contents&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üé´ License&lt;/h2&gt; &#xA;&lt;p&gt;This project is released under the &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/LICENSE&#34;&gt;Apache 2.0 license&lt;/a&gt;. Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/LICENSE&#34;&gt;LICENSES&lt;/a&gt; for the careful check, if you are using our code for commercial matters.&lt;/p&gt; &#xA;&lt;p align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#table&#34;&gt;üîùBack to Table of Contents&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üèóÔ∏è Ô∏èOpenMMLab Family&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmengine&#34;&gt;MMEngine&lt;/a&gt;: OpenMMLab foundational library for training deep learning models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmcv&#34;&gt;MMCV&lt;/a&gt;: OpenMMLab foundational library for computer vision.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mim&#34;&gt;MIM&lt;/a&gt;: MIM installs OpenMMLab packages.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmpretrain&#34;&gt;MMPreTrain&lt;/a&gt;: OpenMMLab Pre-training Toolbox and Benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmdetection&#34;&gt;MMDetection&lt;/a&gt;: OpenMMLab detection toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmdetection3d&#34;&gt;MMDetection3D&lt;/a&gt;: OpenMMLab&#39;s next-generation platform for general 3D object detection.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmrotate&#34;&gt;MMRotate&lt;/a&gt;: OpenMMLab rotated object detection toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmsegmentation&#34;&gt;MMSegmentation&lt;/a&gt;: OpenMMLab semantic segmentation toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmocr&#34;&gt;MMOCR&lt;/a&gt;: OpenMMLab text detection, recognition, and understanding toolbox.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmpose&#34;&gt;MMPose&lt;/a&gt;: OpenMMLab pose estimation toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmhuman3d&#34;&gt;MMHuman3D&lt;/a&gt;: OpenMMLab 3D human parametric model toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmselfsup&#34;&gt;MMSelfSup&lt;/a&gt;: OpenMMLab self-supervised learning toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmrazor&#34;&gt;MMRazor&lt;/a&gt;: OpenMMLab model compression toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmfewshot&#34;&gt;MMFewShot&lt;/a&gt;: OpenMMLab fewshot learning toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2&#34;&gt;MMAction2&lt;/a&gt;: OpenMMLab&#39;s next-generation action understanding toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmtracking&#34;&gt;MMTracking&lt;/a&gt;: OpenMMLab video perception toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmflow&#34;&gt;MMFlow&lt;/a&gt;: OpenMMLab optical flow toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmagic&#34;&gt;MMagic&lt;/a&gt;: OpenMMLab Multimodal Advanced, Generative, and Intelligent Creation Toolbox.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmdeploy&#34;&gt;MMDeploy&lt;/a&gt;: OpenMMLab model deployment framework.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#table&#34;&gt;üîùBack to Table of Contents&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>PRIS-CV/DemoFusion</title>
    <updated>2023-12-17T01:56:52Z</updated>
    <id>tag:github.com,2023-12-17:/PRIS-CV/DemoFusion</id>
    <link href="https://github.com/PRIS-CV/DemoFusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Let us democratise high-resolution generation! (arXiv 2023)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DemoFusion&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ruoyidu.github.io/demofusion/demofusion.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Page-green.svg?sanitize=true&#34; alt=&#34;Project Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/pdf/2311.16973.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2311.16973-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://replicate.com/lucataco/demofusion&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-%F0%9F%9A%80%20Replicate-blue&#34; alt=&#34;Replicate&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/camenduru/DemoFusion-colab/blob/main/DemoFusion_colab.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/radames/Enhance-This-DemoFusion-SDXL&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/i2i-%F0%9F%A4%97%20Hugging%20Face-blue&#34; alt=&#34;Hugging Face&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badges.toozhao.com/stats/01HFMAPCVTA1T32KN2PASNYGYK&#34; title=&#34;Get your own page views count badge on badges.toozhao.com&#34;&gt;&lt;img src=&#34;https://badges.toozhao.com/badges/01HFMAPCVTA1T32KN2PASNYGYK/blue.svg?sanitize=true&#34; alt=&#34;Page Views Count&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Code release for &#34;DemoFusion: Democratising High-Resolution Image Generation With No üí∞&#34; (arXiv 2023)&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/PRIS-CV/DemoFusion/main/figures/illustration.jpg&#34; width=&#34;800&#34;&gt; &#xA;&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;: High-resolution image generation with Generative Artificial Intelligence (GenAI) has immense potential but, due to the enormous capital investment required for training, it is increasingly centralised to a few large corporations, and hidden behind paywalls. This paper aims to democratise high-resolution GenAI by advancing the frontier of high-resolution generation while remaining accessible to a broad audience. We demonstrate that existing Latent Diffusion Models (LDMs) possess untapped potential for higher-resolution image generation. Our novel DemoFusion framework seamlessly extends open-source GenAI models, employing Progressive Upscaling, Skip Residual, and Dilated Sampling mechanisms to achieve higher-resolution image generation. The progressive nature of DemoFusion requires more passes, but the intermediate results can serve as &#34;previews&#34;, facilitating rapid prompt iteration.&lt;/p&gt; &#xA;&lt;h1&gt;News&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;2023.12.12&lt;/strong&gt;: ‚ú® DemoFusion with ControNet is availabe now! Check it out at &lt;code&gt;pipeline_demofusion_sdxl_controlnet&lt;/code&gt;! The local &lt;a href=&#34;https://github.com/PRIS-CV/DemoFusion#DemoFusionControlNet-with-local-Gradio-demo&#34;&gt;Gradio Demo&lt;/a&gt; is also available.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2023.12.10&lt;/strong&gt;: ‚ú® Image2Image is supported by &lt;code&gt;pipeline_demofusion_sdxl&lt;/code&gt; now! The local &lt;a href=&#34;https://github.com/PRIS-CV/DemoFusion#Image2Image-with-local-Gradio-demo&#34;&gt;Gradio Demo&lt;/a&gt; is also available.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2023.12.08&lt;/strong&gt;: üöÄ A HuggingFace Demo for Img2Img is now available! &lt;a href=&#34;https://huggingface.co/spaces/radames/Enhance-This-DemoFusion-SDXL&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/i2i-%F0%9F%A4%97%20Hugging%20Face-blue&#34; alt=&#34;Hugging Face&#34;&gt;&lt;/a&gt; Thank &lt;a href=&#34;https://github.com/radames&#34;&gt;Radam√©s&lt;/a&gt; for the implementation and &lt;a href=&#34;https://huggingface.co/docs/diffusers/index&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Hugging%20Face-Diffusers-orange.svg?sanitize=true&#34; alt=&#34;Hugging Face&#34;&gt;&lt;/a&gt; for the support!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2023.12.07&lt;/strong&gt;: üöÄ Add Colab demo &lt;a href=&#34;https://colab.research.google.com/github/camenduru/DemoFusion-colab/blob/main/DemoFusion_colab.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;. Check it out! Thank &lt;a href=&#34;https://github.com/camenduru&#34;&gt;camenduru&lt;/a&gt; for the implementation!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2023.12.06&lt;/strong&gt;: ‚ú® The local &lt;a href=&#34;https://github.com/PRIS-CV/DemoFusion#Text2Image-with-local-Gradio-demo&#34;&gt;Gradio Demo&lt;/a&gt; is now available! Better interaction and presentation!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2023.12.04&lt;/strong&gt;: ‚ú® A &lt;a href=&#34;https://github.com/PRIS-CV/DemoFusion#Text2Image-on-Windows-with-8-GB-of-VRAM&#34;&gt;low-vram version&lt;/a&gt; of DemoFusion is available! Thank &lt;a href=&#34;https://github.com/klimaleksus&#34;&gt;klimaleksus&lt;/a&gt; for the implementation!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2023.12.01&lt;/strong&gt;: üöÄ Integrated to &lt;a href=&#34;https://replicate.com/explore&#34;&gt;Replicate&lt;/a&gt;. Check out the online demo: &lt;a href=&#34;https://replicate.com/lucataco/demofusion&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-%F0%9F%9A%80%20Replicate-blue&#34; alt=&#34;Replicate&#34;&gt;&lt;/a&gt; Thank &lt;a href=&#34;https://github.com/lucataco&#34;&gt;Luis C.&lt;/a&gt; for the implementation!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2023.11.29&lt;/strong&gt;: üí∞ &lt;code&gt;pipeline_demofusion_sdxl&lt;/code&gt; is released.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;h2&gt;A quick try with integrated demos&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;HuggingFace Space: Try Text2Image generation at &lt;a href=&#34;https://huggingface.co/spaces/fffiloni/DemoFusion&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/t2i-%F0%9F%A4%97%20Hugging%20Face-blue&#34; alt=&#34;Hugging Face&#34;&gt;&lt;/a&gt; and Image2Image enhancement at &lt;a href=&#34;https://huggingface.co/spaces/radames/Enhance-This-DemoFusion-SDXL&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/i2i-%F0%9F%A4%97%20Hugging%20Face-blue&#34; alt=&#34;Hugging Face&#34;&gt;&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Colab: Try Text2Image generation at &lt;a href=&#34;https://colab.research.google.com/github/camenduru/DemoFusion-colab/blob/main/DemoFusion_colab.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; and Image2Image enhancement at &lt;a href=&#34;https://colab.research.google.com/github/camenduru/DemoFusion-colab/blob/main/DemoFusion_img2img_colab.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Replicate: Try Text2Image generation at &lt;a href=&#34;https://replicate.com/lucataco/demofusion&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-%F0%9F%9A%80%20Replicate-blue&#34; alt=&#34;Replicate&#34;&gt;&lt;/a&gt; and Image2Image enhancement at &lt;a href=&#34;https://replicate.com/lucataco/demofusion-enhance&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-%F0%9F%9A%80%20Replicate-blue&#34; alt=&#34;Replicate&#34;&gt;&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Starting with our code&lt;/h2&gt; &#xA;&lt;h3&gt;Hyper-parameters&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;view_batch_size&lt;/code&gt; (&lt;code&gt;int&lt;/code&gt;, defaults to 16): The batch size for multiple denoising paths. Typically, a larger batch size can result in higher efficiency but comes with increased GPU memory requirements.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;stride&lt;/code&gt; (&lt;code&gt;int&lt;/code&gt;, defaults to 64): The stride of moving local patches. A smaller stride is better for alleviating seam issues, but it also introduces additional computational overhead and inference time.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cosine_scale_1&lt;/code&gt; (&lt;code&gt;float&lt;/code&gt;, defaults to 3): Control the decreasing rate of skip-residual. A smaller value results in better consistency with low-resolution results, but it may lead to more pronounced upsampling noise. Please refer to Appendix C in the DemoFusion paper.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cosine_scale_2&lt;/code&gt; (&lt;code&gt;float&lt;/code&gt;, defaults to 1): Control the decreasing rate of dilated sampling. A smaller value can better address the repetition issue, but it may lead to grainy images. For specific impacts, please refer to Appendix C in the DemoFusion paper.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cosine_scale_3&lt;/code&gt; (&lt;code&gt;float&lt;/code&gt;, defaults to 1): Control the decrease rate of the Gaussian filter. A smaller value results in less grainy images, but it may lead to over-smoothing images. Please refer to Appendix C in the DemoFusion paper.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;sigma&lt;/code&gt; (&lt;code&gt;float&lt;/code&gt;, defaults to 1): The standard value of the Gaussian filter. A larger sigma promotes the global guidance of dilated sampling, but it has the potential of over-smoothing.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;multi_decoder&lt;/code&gt; (&lt;code&gt;bool&lt;/code&gt;, defaults to True): Determine whether to use a tiled decoder. Generally, a tiled decoder becomes necessary when the resolution exceeds 3072*3072 on an RTX 3090 GPU.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;show_image&lt;/code&gt; (&lt;code&gt;bool&lt;/code&gt;, defaults to False): Determine whether to show intermediate results during generation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Text2Image (will take about 17 GB of VRAM)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Set up the dependencies as:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n demofusion python=3.9&#xA;conda activate demofusion&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download &lt;code&gt;pipeline_demofusion_sdxl.py&lt;/code&gt; and run it as follows. A use case can be found in &lt;code&gt;demo.ipynb&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;from pipeline_demofusion_sdxl import DemoFusionSDXLPipeline&#xA;&#xA;model_ckpt = &#34;stabilityai/stable-diffusion-xl-base-1.0&#34;&#xA;pipe = DemoFusionSDXLPipeline.from_pretrained(model_ckpt, torch_dtype=torch.float16)&#xA;pipe = pipe.to(&#34;cuda&#34;)&#xA;&#xA;prompt = &#34;Envision a portrait of an elderly woman, her face a canvas of time, framed by a headscarf with muted tones of rust and cream. Her eyes, blue like faded denim. Her attire, simple yet dignified.&#34;&#xA;negative_prompt = &#34;blurry, ugly, duplicate, poorly drawn, deformed, mosaic&#34;&#xA;&#xA;images = pipe(prompt, negative_prompt=negative_prompt,&#xA;              height=3072, width=3072, view_batch_size=16, stride=64,&#xA;              num_inference_steps=50, guidance_scale=7.5,&#xA;              cosine_scale_1=3, cosine_scale_2=1, cosine_scale_3=1, sigma=0.8,&#xA;              multi_decoder=True, show_image=True&#xA;             )&#xA;&#xA;for i, image in enumerate(images):&#xA;    image.save(&#39;image_&#39; + str(i) + &#39;.png&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;‚ö†Ô∏è When you have enough VRAM (e.g., generating 2048*2048 images on hardware with more than 18GB RAM), you can set &lt;code&gt;multi_decoder=False&lt;/code&gt;, which can make the decoding process faster.&lt;/li&gt; &#xA; &lt;li&gt;Please feel free to try different prompts and resolutions.&lt;/li&gt; &#xA; &lt;li&gt;Default hyper-parameters are recommended, but they may not be optimal for all cases. For specific impacts of each hyper-parameter, please refer to Appendix C in the DemoFusion paper.&lt;/li&gt; &#xA; &lt;li&gt;The code was cleaned before the release. If you encounter any issues, please contact us.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Text2Image on Windows with 8 GB of VRAM&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Set up the environment as:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;cmd&#xA;git clone &#34;https://github.com/PRIS-CV/DemoFusion&#34;&#xA;cd DemoFusion&#xA;python -m venv venv&#xA;venv\Scripts\activate&#xA;pip install -U &#34;xformers==0.0.22.post7+cu118&#34; --index-url https://download.pytorch.org/whl/cu118&#xA;pip install &#34;diffusers==0.21.4&#34; &#34;matplotlib==3.8.2&#34; &#34;transformers==4.35.2&#34; &#34;accelerate==0.25.0&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Launch DemoFusion as follows. The use case can be found in &lt;code&gt;demo_lowvram.py&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;python&#xA;from pipeline_demofusion_sdxl import DemoFusionSDXLPipeline&#xA;&#xA;import torch&#xA;from diffusers.models import AutoencoderKL&#xA;vae = AutoencoderKL.from_pretrained(&#34;madebyollin/sdxl-vae-fp16-fix&#34;, torch_dtype=torch.float16)&#xA;&#xA;model_ckpt = &#34;stabilityai/stable-diffusion-xl-base-1.0&#34;&#xA;pipe = DemoFusionSDXLPipeline.from_pretrained(model_ckpt, torch_dtype=torch.float16, vae=vae)&#xA;pipe = pipe.to(&#34;cuda&#34;)&#xA;&#xA;prompt = &#34;Envision a portrait of an elderly woman, her face a canvas of time, framed by a headscarf with muted tones of rust and cream. Her eyes, blue like faded denim. Her attire, simple yet dignified.&#34;&#xA;negative_prompt = &#34;blurry, ugly, duplicate, poorly drawn, deformed, mosaic&#34;&#xA;&#xA;images = pipe(prompt, negative_prompt=negative_prompt,&#xA;              height=2048, width=2048, view_batch_size=4, stride=64,&#xA;              num_inference_steps=40, guidance_scale=7.5,&#xA;              cosine_scale_1=3, cosine_scale_2=1, cosine_scale_3=1, sigma=0.8,&#xA;              multi_decoder=True, show_image=False, lowvram=True&#xA;             )&#xA;&#xA;for i, image in enumerate(images):&#xA;    image.save(&#39;image_&#39; + str(i) + &#39;.png&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Text2Image with local Gradio demo&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Make sure you have installed &lt;code&gt;gradio&lt;/code&gt; and &lt;code&gt;gradio_imageslider&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Launch DemoFusion via Gradio demo now -- try &lt;code&gt;python gradio_demo.py&lt;/code&gt;! Better Interaction and PresentationÔºÅ&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/PRIS-CV/DemoFusion/main/figures/gradio_demo.png&#34; width=&#34;600&#34;&gt; &#xA;&lt;h3&gt;Image2Image with local Gradio demo&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Make sure you have installed &lt;code&gt;gradio&lt;/code&gt; and &lt;code&gt;gradio_imageslider&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Launch DemoFusion Image2Image by &lt;code&gt;python gradio_demo_img2img.py&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/PRIS-CV/DemoFusion/main/figures/gradio_demo_img2img.png&#34; width=&#34;600&#34;&gt; - ‚ö†Ô∏è Please note that, as a tuning-free framework, DemoFusion&#39;s Image2Image capability is strongly correlated with the SDXL&#39;s training data distribution and will show a significant bias. An accurate prompt to describe the content and style of the input also significantly improves performance. Have fun and regard it as a side application of text+image based generation. &#xA;&lt;h3&gt;DemoFusion+ControlNet with local Gradio demo&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Make sure you have installed &lt;code&gt;gradio&lt;/code&gt; and &lt;code&gt;gradio_imageslider&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Launch DemoFusion+ControNet Text2Image by &lt;code&gt;python gradio_demo.py&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt; &lt;img src=&#34;https://raw.githubusercontent.com/PRIS-CV/DemoFusion/main/figures/gradio_demo_controlnet.png&#34; width=&#34;600&#34;&gt; &lt;/li&gt; &#xA; &lt;li&gt;Launch DemoFusion+ControNet Image2Image by &lt;code&gt;python gradio_demo_img2img.py&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt; &lt;img src=&#34;https://raw.githubusercontent.com/PRIS-CV/DemoFusion/main/figures/gradio_demo_controlnet_img2img.png&#34; width=&#34;600&#34;&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this paper useful in your research, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{du2023demofusion,&#xA;  title={DemoFusion: Democratising High-Resolution Image Generation With No $$$},&#xA;  author={Du, Ruoyi and Chang, Dongliang and Hospedales, Timothy and Song, Yi-Zhe and Ma, Zhanyu},&#xA;  journal={arXiv preprint arXiv:2311.16973},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>