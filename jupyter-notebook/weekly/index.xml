<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-24T01:53:16Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>salesforce/LAVIS</title>
    <updated>2023-12-24T01:53:16Z</updated>
    <id>tag:github.com,2023-12-24:/salesforce/LAVIS</id>
    <link href="https://github.com/salesforce/LAVIS" rel="alternate"></link>
    <summary type="html">&lt;p&gt;LAVIS - A One-stop Library for Language-Vision Intelligence&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/salesforce/LAVIS/main/docs/_static/logo_final.png&#34; width=&#34;400&#34;&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/salesforce/LAVIS/releases&#34;&gt;&lt;img alt=&#34;Latest Release&#34; src=&#34;https://img.shields.io/github/release/salesforce/LAVIS.svg?sanitize=true&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://opensource.salesforce.com/LAVIS/index.html&#34;&gt; &lt;img alt=&#34;docs&#34; src=&#34;https://github.com/salesforce/LAVIS/actions/workflows/docs.yaml/badge.svg?sanitize=true&#34;&gt; &lt;/a&gt;&#xA; &lt;a href=&#34;https://opensource.org/licenses/BSD-3-Clause&#34;&gt; &lt;img alt=&#34;license&#34; src=&#34;https://img.shields.io/badge/License-BSD_3--Clause-blue.svg?sanitize=true&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://pepy.tech/project/salesforce-lavis&#34;&gt; &lt;img alt=&#34;Downloads&#34; src=&#34;https://pepy.tech/badge/salesforce-lavis&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://opensource.salesforce.com/LAVIS//latest/benchmark.html&#34;&gt;Benchmark&lt;/a&gt;, &#xA; &lt;a href=&#34;https://arxiv.org/abs/2209.09019&#34;&gt;Technical Report&lt;/a&gt;, &#xA; &lt;a href=&#34;https://opensource.salesforce.com/LAVIS//latest/index.html&#34;&gt;Documentation&lt;/a&gt;, &#xA; &lt;a href=&#34;https://github.com/salesforce/LAVIS/tree/main/examples&#34;&gt;Jupyter Notebook Examples&lt;/a&gt;, &#xA; &lt;a href=&#34;https://blog.salesforceairesearch.com/lavis-language-vision-library/&#34;&gt;Blog&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;LAVIS - A Library for Language-Vision Intelligence&lt;/h1&gt; &#xA;&lt;h2&gt;What&#39;s New: 🎉&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[Model Release] November 2023, released implementation of &lt;strong&gt;X-InstructBLIP&lt;/strong&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2311.18799.pdf&#34;&gt;Paper&lt;/a&gt;, &lt;a href=&#34;https://github.com/salesforce/LAVIS/tree/main/projects/xinstructblip&#34;&gt;Project Page&lt;/a&gt;, &lt;a href=&#34;https://artemisp.github.io/X-InstructBLIP-page/&#34;&gt;Website&lt;/a&gt;, &lt;a href=&#34;https://colab.research.google.com/github/salesforce/LAVIS/blob/main/projects/xinstructblip/demo/run_demo.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;A simple, yet effective, cross-modality framework built atop frozen LLMs that allows the integration of various modalities (image, video, audio, 3D) without extensive modality-specific customization.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[Model Release] July 2023, released implementation of &lt;strong&gt;BLIP-Diffusion&lt;/strong&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2305.06500&#34;&gt;Paper&lt;/a&gt;, &lt;a href=&#34;https://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion&#34;&gt;Project Page&lt;/a&gt;, &lt;a href=&#34;https://dxli94.github.io/BLIP-Diffusion-website/&#34;&gt;Website&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;A text-to-image generation model that trains 20x than DreamBooth. Also facilitates zero-shot subject-driven generation and editing.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[Model Release] May 2023, released implementation of &lt;strong&gt;InstructBLIP&lt;/strong&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2305.06500&#34;&gt;Paper&lt;/a&gt;, &lt;a href=&#34;https://github.com/salesforce/LAVIS/tree/main/projects/instructblip&#34;&gt;Project Page&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;A new vision-language instruction-tuning framework using BLIP-2 models, achieving state-of-the-art zero-shot generalization performance on a wide range of vision-language tasks.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[Model Release] Jan 2023, released implementation of &lt;strong&gt;BLIP-2&lt;/strong&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2301.12597&#34;&gt;Paper&lt;/a&gt;, &lt;a href=&#34;https://github.com/salesforce/LAVIS/tree/main/projects/blip2&#34;&gt;Project Page&lt;/a&gt;, &lt;a href=&#34;https://colab.research.google.com/github/salesforce/LAVIS/blob/main/examples/blip2_instructed_generation.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;A generic and efficient pre-training strategy that easily harvests development of pretrained vision models and large language models (LLMs) for vision-language pretraining. BLIP-2 beats Flamingo on zero-shot VQAv2 (&lt;strong&gt;65.0&lt;/strong&gt; vs &lt;strong&gt;56.3&lt;/strong&gt;), establishing new state-of-the-art on zero-shot captioning (on NoCaps &lt;strong&gt;121.6&lt;/strong&gt; CIDEr score vs previous best &lt;strong&gt;113.2&lt;/strong&gt;). In addition, equipped with powerful LLMs (e.g. OPT, FlanT5), BLIP-2 also unlocks the new &lt;strong&gt;zero-shot instructed vision-to-language generation&lt;/strong&gt; capabilities for various interesting applications!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Jan 2023, LAVIS is now available on &lt;a href=&#34;https://pypi.org/project/salesforce-lavis/&#34;&gt;PyPI&lt;/a&gt; for installation!&lt;/li&gt; &#xA; &lt;li&gt;[Model Release] Dec 2022, released implementation of &lt;strong&gt;Img2LLM-VQA&lt;/strong&gt; (&lt;strong&gt;CVPR 2023&lt;/strong&gt;, &lt;em&gt;&#34;From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models&#34;&lt;/em&gt;, by Jiaxian Guo et al) &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2212.10846.pdf&#34;&gt;Paper&lt;/a&gt;, &lt;a href=&#34;https://github.com/salesforce/LAVIS/tree/main/projects/img2llm-vqa&#34;&gt;Project Page&lt;/a&gt;, &lt;a href=&#34;https://colab.research.google.com/github/salesforce/LAVIS/blob/main/projects/img2llm-vqa/img2llm_vqa.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;A plug-and-play module that enables off-the-shelf use of Large Language Models (LLMs) for visual question answering (VQA). Img2LLM-VQA surpasses Flamingo on zero-shot VQA on VQAv2 (61.9 vs 56.3), while in contrast requiring no end-to-end training!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[Model Release] Oct 2022, released implementation of &lt;strong&gt;PNP-VQA&lt;/strong&gt; (&lt;strong&gt;EMNLP Findings 2022&lt;/strong&gt;, &lt;em&gt;&#34;Plug-and-Play VQA: Zero-shot VQA by Conjoining Large Pretrained Models with Zero Training&#34;&lt;/em&gt;, by Anthony T.M.H. et al), &lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2210.08773&#34;&gt;Paper&lt;/a&gt;, &lt;a href=&#34;https://github.com/salesforce/LAVIS/tree/main/projects/pnp-vqa&#34;&gt;Project Page&lt;/a&gt;, &lt;a href=&#34;https://colab.research.google.com/github/salesforce/LAVIS/blob/main/projects/pnp-vqa/pnp_vqa.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;A modular zero-shot VQA framework that requires no PLMs training, achieving SoTA zero-shot VQA performance.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Technical Report and Citing LAVIS&lt;/h2&gt; &#xA;&lt;p&gt;You can find more details in our &lt;a href=&#34;https://arxiv.org/abs/2209.09019&#34;&gt;technical report&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;If you&#39;re using LAVIS in your research or applications, please cite it using this BibTeX&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{li-etal-2023-lavis,&#xA;    title = &#34;{LAVIS}: A One-stop Library for Language-Vision Intelligence&#34;,&#xA;    author = &#34;Li, Dongxu  and&#xA;      Li, Junnan  and&#xA;      Le, Hung  and&#xA;      Wang, Guangsen  and&#xA;      Savarese, Silvio  and&#xA;      Hoi, Steven C.H.&#34;,&#xA;    booktitle = &#34;Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)&#34;,&#xA;    month = jul,&#xA;    year = &#34;2023&#34;,&#xA;    address = &#34;Toronto, Canada&#34;,&#xA;    publisher = &#34;Association for Computational Linguistics&#34;,&#xA;    url = &#34;https://aclanthology.org/2023.acl-demo.3&#34;,&#xA;    pages = &#34;31--41&#34;,&#xA;    abstract = &#34;We introduce LAVIS, an open-source deep learning library for LAnguage-VISion research and applications. LAVIS aims to serve as a one-stop comprehensive library that brings recent advancements in the language-vision field accessible for researchers and practitioners, as well as fertilizing future research and development. It features a unified interface to easily access state-of-the-art image-language, video-language models and common datasets. LAVIS supports training, evaluation and benchmarking on a rich variety of tasks, including multimodal classification, retrieval, captioning, visual question answering, dialogue and pre-training. In the meantime, the library is also highly extensible and configurable, facilitating future development and customization. In this technical report, we describe design principles, key components and functionalities of the library, and also present benchmarking results across common language-vision tasks.&#34;,&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/salesforce/LAVIS/main/#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/salesforce/LAVIS/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/salesforce/LAVIS/main/#getting-started&#34;&gt;Getting Started&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/salesforce/LAVIS/main/#model-zoo&#34;&gt;Model Zoo&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/salesforce/LAVIS/main/#image-captioning&#34;&gt;Image Captioning&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/salesforce/LAVIS/main/#visual-question-answering-vqa&#34;&gt;Visual question answering (VQA)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/salesforce/LAVIS/main/#unified-feature-extraction-interface&#34;&gt;Unified Feature Extraction Interface&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/salesforce/LAVIS/main/#load-datasets&#34;&gt;Load Datasets&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/salesforce/LAVIS/main/#jupyter-notebook-examples&#34;&gt;Jupyter Notebook Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/salesforce/LAVIS/main/#resources-and-tools&#34;&gt;Resources and Tools&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/salesforce/LAVIS/main/#documentations&#34;&gt;Documentations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/salesforce/LAVIS/main/#ethical-and-responsible-use&#34;&gt;Ethical and Responsible Use&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/salesforce/LAVIS/main/#technical-report-and-citing-lavis&#34;&gt;Technical Report and Citing LAVIS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/salesforce/LAVIS/main/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;LAVIS is a Python deep learning library for LAnguage-and-VISion intelligence research and applications. This library aims to provide engineers and researchers with a one-stop solution to rapidly develop models for their specific multimodal scenarios, and benchmark them across standard and customized datasets. It features a unified interface design to access&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;10+&lt;/strong&gt; tasks (retrieval, captioning, visual question answering, multimodal classification etc.);&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;20+&lt;/strong&gt; datasets (COCO, Flickr, Nocaps, Conceptual Commons, SBU, etc.);&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;30+&lt;/strong&gt; pretrained weights of state-of-the-art foundation language-vision models and their task-specific adaptations, including &lt;a href=&#34;https://arxiv.org/pdf/2107.07651.pdf&#34;&gt;ALBEF&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/2201.12086.pdf&#34;&gt;BLIP&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/2112.09583.pdf&#34;&gt;ALPRO&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/2103.00020.pdf&#34;&gt;CLIP&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/salesforce/LAVIS/main/assets/demo-6.png&#34;&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p&gt;Key features of LAVIS include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Unified and Modular Interface&lt;/strong&gt;: facilitating to easily leverage and repurpose existing modules (datasets, models, preprocessors), also to add new modules.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Easy Off-the-shelf Inference and Feature Extraction&lt;/strong&gt;: readily available pre-trained models let you take advantage of state-of-the-art multimodal understanding and generation capabilities on your own data.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Reproducible Model Zoo and Training Recipes&lt;/strong&gt;: easily replicate and extend state-of-the-art models on existing and new tasks.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Dataset Zoo and Automatic Downloading Tools&lt;/strong&gt;: it can be a hassle to prepare the many language-vision datasets. LAVIS provides automatic downloading scripts to help prepare a large variety of datasets and their annotations.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The following table shows the supported tasks, datasets and models in our library. This is a continuing effort and we are working on further growing the list.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Tasks&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Supported Models&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Supported Datasets&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Image-text Pre-training&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ALBEF, BLIP&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO, VisualGenome, SBU ConceptualCaptions&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Image-text Retrieval&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ALBEF, BLIP, CLIP&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO, Flickr30k&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Text-image Retrieval&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ALBEF, BLIP, CLIP&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO, Flickr30k&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Visual Question Answering&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ALBEF, BLIP&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;VQAv2, OKVQA, A-OKVQA&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Image Captioning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;BLIP&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO, NoCaps&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Image Classification&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;CLIP&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Natural Language Visual Reasoning (NLVR)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ALBEF, BLIP&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;NLVR2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Visual Entailment (VE)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ALBEF&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SNLI-VE&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Visual Dialogue&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;BLIP&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;VisDial&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Video-text Retrieval&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;BLIP, ALPRO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;MSRVTT, DiDeMo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Text-video Retrieval&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;BLIP, ALPRO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;MSRVTT, DiDeMo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Video Question Answering (VideoQA)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;BLIP, ALPRO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;MSRVTT, MSVD&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Video Dialogue&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;VGD-GPT&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;AVSD&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Multimodal Feature Extraction&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ALBEF, CLIP, BLIP, ALPRO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;customized&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Text-to-image Generation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;[COMING SOON]&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;(Optional) Creating conda environment&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n lavis python=3.8&#xA;conda activate lavis&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;install from &lt;a href=&#34;https://pypi.org/project/salesforce-lavis/&#34;&gt;PyPI&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install salesforce-lavis&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Or, for development, you may build from source&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/salesforce/LAVIS.git&#xA;cd LAVIS&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Model Zoo&lt;/h3&gt; &#xA;&lt;p&gt;Model zoo summarizes supported models in LAVIS, to view:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from lavis.models import model_zoo&#xA;print(model_zoo)&#xA;# ==================================================&#xA;# Architectures                  Types&#xA;# ==================================================&#xA;# albef_classification           ve&#xA;# albef_feature_extractor        base&#xA;# albef_nlvr                     nlvr&#xA;# albef_pretrain                 base&#xA;# albef_retrieval                coco, flickr&#xA;# albef_vqa                      vqav2&#xA;# alpro_qa                       msrvtt, msvd&#xA;# alpro_retrieval                msrvtt, didemo&#xA;# blip_caption                   base_coco, large_coco&#xA;# blip_classification            base&#xA;# blip_feature_extractor         base&#xA;# blip_nlvr                      nlvr&#xA;# blip_pretrain                  base&#xA;# blip_retrieval                 coco, flickr&#xA;# blip_vqa                       vqav2, okvqa, aokvqa&#xA;# clip_feature_extractor         ViT-B-32, ViT-B-16, ViT-L-14, ViT-L-14-336, RN50&#xA;# clip                           ViT-B-32, ViT-B-16, ViT-L-14, ViT-L-14-336, RN50&#xA;# gpt_dialogue                   base&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Let’s see how to use models in LAVIS to perform inference on example data. We first load a sample image from local.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from PIL import Image&#xA;# setup device to use&#xA;device = torch.device(&#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34;)&#xA;# load sample image&#xA;raw_image = Image.open(&#34;docs/_static/merlion.png&#34;).convert(&#34;RGB&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This example image shows &lt;a href=&#34;https://en.wikipedia.org/wiki/Merlion&#34;&gt;Merlion park&lt;/a&gt; (&lt;a href=&#34;https://theculturetrip.com/asia/singapore/articles/what-exactly-is-singapores-merlion-anyway/&#34;&gt;source&lt;/a&gt;), a landmark in Singapore.&lt;/p&gt; &#xA;&lt;h3&gt;Image Captioning&lt;/h3&gt; &#xA;&lt;p&gt;In this example, we use the BLIP model to generate a caption for the image. To make inference even easier, we also associate each pre-trained model with its preprocessors (transforms), accessed via &lt;code&gt;load_model_and_preprocess()&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from lavis.models import load_model_and_preprocess&#xA;device = torch.device(&#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34;)&#xA;# loads BLIP caption base model, with finetuned checkpoints on MSCOCO captioning dataset.&#xA;# this also loads the associated image processors&#xA;model, vis_processors, _ = load_model_and_preprocess(name=&#34;blip_caption&#34;, model_type=&#34;base_coco&#34;, is_eval=True, device=device)&#xA;# preprocess the image&#xA;# vis_processors stores image transforms for &#34;train&#34; and &#34;eval&#34; (validation / testing / inference)&#xA;image = vis_processors[&#34;eval&#34;](raw_image).unsqueeze(0).to(device)&#xA;# generate caption&#xA;model.generate({&#34;image&#34;: image})&#xA;# [&#39;a large fountain spewing water into the air&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Visual question answering (VQA)&lt;/h3&gt; &#xA;&lt;p&gt;BLIP model is able to answer free-form questions about images in natural language. To access the VQA model, simply replace the &lt;code&gt;name&lt;/code&gt; and &lt;code&gt;model_type&lt;/code&gt; arguments passed to &lt;code&gt;load_model_and_preprocess()&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from lavis.models import load_model_and_preprocess&#xA;model, vis_processors, txt_processors = load_model_and_preprocess(name=&#34;blip_vqa&#34;, model_type=&#34;vqav2&#34;, is_eval=True, device=device)&#xA;# ask a random question.&#xA;question = &#34;Which city is this photo taken?&#34;&#xA;image = vis_processors[&#34;eval&#34;](raw_image).unsqueeze(0).to(device)&#xA;question = txt_processors[&#34;eval&#34;](question)&#xA;model.predict_answers(samples={&#34;image&#34;: image, &#34;text_input&#34;: question}, inference_method=&#34;generate&#34;)&#xA;# [&#39;singapore&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Unified Feature Extraction Interface&lt;/h3&gt; &#xA;&lt;p&gt;LAVIS provides a unified interface to extract features from each architecture. To extract features, we load the feature extractor variants of each model. The multimodal feature can be used for multimodal classification. The low-dimensional unimodal features can be used to compute cross-modal similarity.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from lavis.models import load_model_and_preprocess&#xA;model, vis_processors, txt_processors = load_model_and_preprocess(name=&#34;blip_feature_extractor&#34;, model_type=&#34;base&#34;, is_eval=True, device=device)&#xA;caption = &#34;a large fountain spewing water into the air&#34;&#xA;image = vis_processors[&#34;eval&#34;](raw_image).unsqueeze(0).to(device)&#xA;text_input = txt_processors[&#34;eval&#34;](caption)&#xA;sample = {&#34;image&#34;: image, &#34;text_input&#34;: [text_input]}&#xA;&#xA;features_multimodal = model.extract_features(sample)&#xA;print(features_multimodal.multimodal_embeds.shape)&#xA;# torch.Size([1, 12, 768]), use features_multimodal[:,0,:] for multimodal classification tasks&#xA;&#xA;features_image = model.extract_features(sample, mode=&#34;image&#34;)&#xA;features_text = model.extract_features(sample, mode=&#34;text&#34;)&#xA;print(features_image.image_embeds.shape)&#xA;# torch.Size([1, 197, 768])&#xA;print(features_text.text_embeds.shape)&#xA;# torch.Size([1, 12, 768])&#xA;&#xA;# low-dimensional projected features&#xA;print(features_image.image_embeds_proj.shape)&#xA;# torch.Size([1, 197, 256])&#xA;print(features_text.text_embeds_proj.shape)&#xA;# torch.Size([1, 12, 256])&#xA;similarity = features_image.image_embeds_proj[:,0,:] @ features_text.text_embeds_proj[:,0,:].t()&#xA;print(similarity)&#xA;# tensor([[0.2622]])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Load Datasets&lt;/h3&gt; &#xA;&lt;p&gt;LAVIS inherently supports a wide variety of common language-vision datasets by providing &lt;a href=&#34;https://opensource.salesforce.com/LAVIS//latest/benchmark&#34;&gt;automatic download tools&lt;/a&gt; to help download and organize these datasets. After downloading, to load the datasets, use the following code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from lavis.datasets.builders import dataset_zoo&#xA;dataset_names = dataset_zoo.get_names()&#xA;print(dataset_names)&#xA;# [&#39;aok_vqa&#39;, &#39;coco_caption&#39;, &#39;coco_retrieval&#39;, &#39;coco_vqa&#39;, &#39;conceptual_caption_12m&#39;,&#xA;#  &#39;conceptual_caption_3m&#39;, &#39;didemo_retrieval&#39;, &#39;flickr30k&#39;, &#39;imagenet&#39;, &#39;laion2B_multi&#39;,&#xA;#  &#39;msrvtt_caption&#39;, &#39;msrvtt_qa&#39;, &#39;msrvtt_retrieval&#39;, &#39;msvd_caption&#39;, &#39;msvd_qa&#39;, &#39;nlvr&#39;,&#xA;#  &#39;nocaps&#39;, &#39;ok_vqa&#39;, &#39;sbu_caption&#39;, &#39;snli_ve&#39;, &#39;vatex_caption&#39;, &#39;vg_caption&#39;, &#39;vg_vqa&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After downloading the images, we can use &lt;code&gt;load_dataset()&lt;/code&gt; to obtain the dataset.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from lavis.datasets.builders import load_dataset&#xA;coco_dataset = load_dataset(&#34;coco_caption&#34;)&#xA;print(coco_dataset.keys())&#xA;# dict_keys([&#39;train&#39;, &#39;val&#39;, &#39;test&#39;])&#xA;print(len(coco_dataset[&#34;train&#34;]))&#xA;# 566747&#xA;print(coco_dataset[&#34;train&#34;][0])&#xA;# {&#39;image&#39;: &amp;lt;PIL.Image.Image image mode=RGB size=640x480&amp;gt;,&#xA;#  &#39;text_input&#39;: &#39;A woman wearing a net on her head cutting a cake. &#39;,&#xA;#  &#39;image_id&#39;: 0}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you already host a local copy of the dataset, you can pass in the &lt;code&gt;vis_path&lt;/code&gt; argument to change the default location to load images.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;coco_dataset = load_dataset(&#34;coco_caption&#34;, vis_path=YOUR_LOCAL_PATH)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Jupyter Notebook Examples&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/salesforce/LAVIS/tree/main/examples&#34;&gt;examples&lt;/a&gt; for more inference examples, e.g. captioning, feature extraction, VQA, GradCam, zeros-shot classification.&lt;/p&gt; &#xA;&lt;h2&gt;Resources and Tools&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Benchmarks&lt;/strong&gt;: see &lt;a href=&#34;https://opensource.salesforce.com/LAVIS//latest/benchmark&#34;&gt;Benchmark&lt;/a&gt; for instructions to evaluate and train supported models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Dataset Download and Browsing&lt;/strong&gt;: see &lt;a href=&#34;https://opensource.salesforce.com/LAVIS//latest/benchmark&#34;&gt;Dataset Download&lt;/a&gt; for instructions and automatic tools on download common language-vision datasets.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;GUI Demo&lt;/strong&gt;: to run the demo locally, run &lt;code&gt;bash run_scripts/run_demo.sh&lt;/code&gt; and then follow the instruction on the prompts to view in browser. A web demo is coming soon.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Documentations&lt;/h2&gt; &#xA;&lt;p&gt;For more details and advanced usages, please refer to &lt;a href=&#34;https://opensource.salesforce.com/LAVIS//latest/index.html#&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Ethical and Responsible Use&lt;/h2&gt; &#xA;&lt;p&gt;We note that models in LAVIS provide no guarantees on their multimodal abilities; incorrect or biased predictions may be observed. In particular, the datasets and pretrained models utilized in LAVIS may contain socioeconomic biases which could result in misclassification and other unwanted behaviors such as offensive or inappropriate speech. We strongly recommend that users review the pre-trained models and overall system in LAVIS before practical adoption. We plan to improve the library by investigating and mitigating these potential biases and inappropriate behaviors in the future.&lt;/p&gt; &#xA;&lt;h2&gt;Contact us&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions, comments or suggestions, please do not hesitate to contact us at &lt;a href=&#34;mailto:lavis@salesforce.com&#34;&gt;lavis@salesforce.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/salesforce/LAVIS/main/LICENSE.txt&#34;&gt;BSD 3-Clause License&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>rahulnyk/knowledge_graph</title>
    <updated>2023-12-24T01:53:16Z</updated>
    <id>tag:github.com,2023-12-24:/rahulnyk/knowledge_graph</id>
    <link href="https://github.com/rahulnyk/knowledge_graph" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Convert any text to a graph of knowledge. This can be used for Graph Augmented Generation or Knowledge Graph based QnA&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Convert any Corpus of Text into a &lt;em&gt;Graph of Knowledge&lt;/em&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rahulnyk/knowledge_graph/main/assets/KG_banner.png&#34; alt=&#34;Knowledge Graph Banner&#34;&gt; &lt;em&gt;A knowledge graph generated using this code&lt;/em&gt; ghpages link of this graph: &lt;a href=&#34;https://rahulnyk.github.io/knowledge_graph/&#34;&gt;https://rahulnyk.github.io/knowledge_graph/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;What is a knowledge graph?&lt;/h2&gt; &#xA;&lt;p&gt;A knowledge graph, also known as a semantic network, represents a network of real-world entities—i.e. objects, events, situations, or concepts—and illustrates the relationship between them. This information is usually stored in a graph database and visualized as a graph structure, prompting the term knowledge “graph.”&lt;/p&gt; &#xA;&lt;p&gt;Source: &lt;a href=&#34;https://www.ibm.com/topics/knowledge-graph&#34;&gt;https://www.ibm.com/topics/knowledge-graph&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How to create a simple knowledge graph from a body of work?&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clean the text corpus (The body of work).&lt;/li&gt; &#xA; &lt;li&gt;Extract concepts and entities from the body of work.&lt;/li&gt; &#xA; &lt;li&gt;Extract relations between the entities.&lt;/li&gt; &#xA; &lt;li&gt;Convert a graph schema.&lt;/li&gt; &#xA; &lt;li&gt;Populate nodes (concepts) and edges (relations).&lt;/li&gt; &#xA; &lt;li&gt;Visualise and Query.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Step 6 is purely optional, but it has certain artistic gratification associated with it. Network graphs are beautiful objects (just look at the banner image above, isn&#39;t it beautiful?). Fortunately, there are a good number of Python libraries available for generating graph visualisations.&lt;/p&gt; &#xA;&lt;h2&gt;Why Graph?&lt;/h2&gt; &#xA;&lt;p&gt;Once the Knowledge Graph (KG) is build, we can use it for many purposes. We can run graph algorithms and calculate centralities of any node, to understand how important a concept (node) is to this body of work. We can calculate communities to bunch the concepts together to better analyse the text. We can understand the connectedness between seemingly disconnected concepts.&lt;/p&gt; &#xA;&lt;p&gt;The best of all, we can achieve &lt;strong&gt;Graph Retrieval Augmented Generation (GRAG)&lt;/strong&gt; and chat with our text in a much more profound way using Graph as a retriever. This is a new and improved version of &lt;strong&gt;Retrieval Augmented Generation (RAG)&lt;/strong&gt; where we use a vectory db as a retriever to chat with our documents.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;This project&lt;/h2&gt; &#xA;&lt;p&gt;Here I have created a simple knowledge graph from a PDF document. The process I follow here is very similar to what is outlined in the above sections, with some simplifications.&lt;/p&gt; &#xA;&lt;p&gt;First I split the entire text into chunks. Then I extract concepts mentioned within each chunk using an LLM. Note that I am not extracting entities using an NER model here. There is a difference between concepts and entities. For example &#39;Bangalore&#39; is an entity, and &#39;Pleasant weather in Bangalore&#39; is a concept. In my experience, concepts make more meaningful KG than entities.&lt;/p&gt; &#xA;&lt;p&gt;I assume that the concepts that are mentioned in the vicinity of each other are related. So every edge in the KG is a text chunk in which the two connected concepts are mentioned.&lt;/p&gt; &#xA;&lt;p&gt;Once the nodes (concepts) and the edges (text chunks) are calculated, It is easy to create a graph out of them using the libraries mentioned here. All the components I used here are set up locally, so this project can be run very easily on a personal machine. I have adopted a no-GPT approach here to keep things economical. I am using the fantastic Mistral 7B openorca instruct, which crushes this use case wonderfully. The model can be set up locally using Ollama so generating the KG is basically free (No calls to GPT).&lt;/p&gt; &#xA;&lt;p&gt;To generate a graph this the notebook you have to tweak.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/rahulnyk/knowledge_graph/raw/main/extract_graph.ipynb&#34;&gt;extract_graph.ipynb&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The notebook implements the method outlined in the following flowchart.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/rahulnyk/knowledge_graph/main/assets/Method.png&#34;&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Split the corpus of text into chunks. Assign a chunk_id to each of these chunks.&lt;/li&gt; &#xA; &lt;li&gt;For every text chunk extract concepts and their semantic relationships using an LLM. Let’s assign this relation a weightage of W1. There can be multiple relationships between the same pair of concepts. Every such relation is an edge between a pair of concepts.&lt;/li&gt; &#xA; &lt;li&gt;Consider that the concepts that occur in the same text chunk are also related by their contextual proximity. Let’s assign this relation a weightage of W2. Note that the same pair of concepts may occur in multiple chunks.&lt;/li&gt; &#xA; &lt;li&gt;Group similar pairs, sum their weights, and concatenate their relationships. So now we have only one edge between any distinct pair of concepts. The edge has a certain weight and a list of relations as its name.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Additional it also calculates the Degree of each node, and Communities of nodes, for sizing and coloring the nodes in the graph respectively.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://medium.com/towards-data-science/how-to-convert-any-text-into-a-graph-of-concepts-110844f22a1a&#34;&gt;Here is a Medium article explaining the method in detail &lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Tech Stack&lt;/h2&gt; &#xA;&lt;h3&gt;Mistral 7B&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://mistral.ai/news/announcing-mistral-7b/&#34;&gt;&lt;img src=&#34;https://mistral.ai/images/logo_hubc88c4ece131b91c7cb753f40e9e1cc5_2589_256x0_resize_q97_h2_lanczos_3.webp&#34; height=&#34;50&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;I am using the &lt;a href=&#34;https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca&#34;&gt;Mistral 7B Openorca&lt;/a&gt; for extracting concepts out of text chunks. It can follow the system prompt instructions very well.&lt;/p&gt; &#xA;&lt;h3&gt;Ollama&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ollama.ai&#34;&gt;&lt;img src=&#34;https://github.com/jmorganca/ollama/assets/3325447/0d0b44e2-8f4a-4e99-9b52-a5c1c741c8f7 &#34; height=&#34;50&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Ollama makes it easy to host any model locally. Mistral 7B OpenOrca version is already available with Ollama to use out of the box.&lt;/p&gt; &#xA;&lt;p&gt;To set up this project, you must install Ollama on your local machine.&lt;/p&gt; &#xA;&lt;p&gt;Step 1: Install Ollama &lt;a href=&#34;https://ollama.ai&#34;&gt;https://ollama.ai&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Step 2: run &lt;code&gt;ollama run zephyr&lt;/code&gt; in your terminal. This will pull the zephyr model to your local machine and start the Ollama server.&lt;/p&gt; &#xA;&lt;h3&gt;Pandas&lt;/h3&gt; &#xA;&lt;p&gt;dataframes for graph schema (can use a graphdb at a later stage).&lt;/p&gt; &#xA;&lt;h3&gt;NetworkX&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://networkx.org&#34;&gt;&lt;img src=&#34;https://networkx.org/_static/networkx_logo.svg?sanitize=true&#34; height=&#34;50&#34;&gt;&lt;/a&gt;&lt;a&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is a python library that makes dealing with graphs super easy&lt;/p&gt; &#xA;&lt;h3&gt;Pyvis&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/WestHealth/pyvis/tree/master&#34;&gt;Pyvis python library&lt;/a&gt; for visualisation. Pyvis generates Javascript Graph visualisations using python, so the final graphs can be hosted on the web. For example the &lt;a href=&#34;https://rahulnyk.github.io/knowledge_graph/&#34;&gt;github link of this repo&lt;/a&gt; is a graph generated by pyvis&lt;/p&gt; &#xA;&lt;h1&gt;Looking for contributions&lt;/h1&gt; &#xA;&lt;p&gt;This project needs a lot more work. There are some wonderful ideas suggested by folks on medium and here on Github. If this interests you, Please join hands and lets&#39; build this together. Here are a few suggested imrpovements.&lt;/p&gt; &#xA;&lt;h3&gt;Back End&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;Use embeddings to deduplicate semantically similar concepts (&lt;strong&gt;Suggested by William Claude on the &lt;a href=&#34;https://medium.com/towards-data-science/how-to-convert-any-text-into-a-graph-of-concepts-110844f22a1a&#34;&gt;Medium Article&lt;/a&gt;&lt;/strong&gt;)&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Avoid having similar concepts written differently by the LLM (eg: &#34;doctor&#34; and &#34;doctors&#34;)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Reinforce the clustering of strongly similar concepts (eg: &#34;doctor&#34; and &#34;medical practitioner&#34;)?&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;Filter out the redundant, or outlier concepts that may not be useful in understanding the text. For example, generic concepts that occur too often in the text. (&lt;strong&gt;Suggested by Luke Chesley&lt;/strong&gt;)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;Better implement the concept of contextual proximity to avoide overweighting certain concepts that occur too frequently, or to weed out useless edges. (&lt;strong&gt;Suggested by Luke Chesley&lt;/strong&gt;)&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Front End&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Create a Frontend for rendering Graph of Concepts in a more useful way. for example here is a flow. (&lt;strong&gt;Suggested by David Garcia on the &lt;a href=&#34;https://medium.com/towards-data-science/how-to-convert-any-text-into-a-graph-of-concepts-110844f22a1a&#34;&gt;Medium Article&lt;/a&gt;&lt;/strong&gt;). &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;Provide a list concept/interest/topics&lt;/li&gt; &#xA;   &lt;li&gt;User selects what they&#39;re interested in&lt;/li&gt; &#xA;   &lt;li&gt;This expands to show sub-topics, sub-concepts, sub-x, etc.&lt;/li&gt; &#xA;   &lt;li&gt;This is how you get deep into a specialty&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>sugarforever/LangChain-Tutorials</title>
    <updated>2023-12-24T01:53:16Z</updated>
    <id>tag:github.com,2023-12-24:/sugarforever/LangChain-Tutorials</id>
    <link href="https://github.com/sugarforever/LangChain-Tutorials" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LangChain Tutorials&lt;/h1&gt;</summary>
  </entry>
</feed>