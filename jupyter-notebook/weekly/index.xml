<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-03T01:51:57Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>aymericdamien/TensorFlow-Examples</title>
    <updated>2023-12-03T01:51:57Z</updated>
    <id>tag:github.com,2023-12-03:/aymericdamien/TensorFlow-Examples</id>
    <link href="https://github.com/aymericdamien/TensorFlow-Examples" rel="alternate"></link>
    <summary type="html">&lt;p&gt;TensorFlow Tutorial and Examples for Beginners (support TF v1 &amp; v2)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TensorFlow Examples&lt;/h1&gt; &#xA;&lt;p&gt;This tutorial was designed for easily diving into TensorFlow, through examples. For readability, it includes both notebooks and source codes with explanation, for both TF v1 &amp;amp; v2.&lt;/p&gt; &#xA;&lt;p&gt;It is suitable for beginners who want to find clear and concise examples about TensorFlow. Besides the traditional &#39;raw&#39; TensorFlow implementations, you can also find the latest TensorFlow API practices (such as &lt;code&gt;layers&lt;/code&gt;, &lt;code&gt;estimator&lt;/code&gt;, &lt;code&gt;dataset&lt;/code&gt;, ...).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Update (05/16/2020):&lt;/strong&gt; Moving all default examples to TF2. For TF v1 examples: &lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1&#34;&gt;check here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Tutorial index&lt;/h2&gt; &#xA;&lt;h4&gt;0 - Prerequisite&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v2/notebooks/0_Prerequisite/ml_introduction.ipynb&#34;&gt;Introduction to Machine Learning&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v2/notebooks/0_Prerequisite/mnist_dataset_intro.ipynb&#34;&gt;Introduction to MNIST Dataset&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;1 - Introduction&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hello World&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v2/notebooks/1_Introduction/helloworld.ipynb&#34;&gt;notebook&lt;/a&gt;). Very simple example to learn how to print &#34;hello world&#34; using TensorFlow 2.0+.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Basic Operations&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v2/notebooks/1_Introduction/basic_operations.ipynb&#34;&gt;notebook&lt;/a&gt;). A simple example that cover TensorFlow 2.0+ basic operations.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;2 - Basic Models&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Linear Regression&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v2/notebooks/2_BasicModels/linear_regression.ipynb&#34;&gt;notebook&lt;/a&gt;). Implement a Linear Regression with TensorFlow 2.0+.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Logistic Regression&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v2/notebooks/2_BasicModels/logistic_regression.ipynb&#34;&gt;notebook&lt;/a&gt;). Implement a Logistic Regression with TensorFlow 2.0+.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Word2Vec (Word Embedding)&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v2/notebooks/2_BasicModels/word2vec.ipynb&#34;&gt;notebook&lt;/a&gt;). Build a Word Embedding Model (Word2Vec) from Wikipedia data, with TensorFlow 2.0+.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;GBDT (Gradient Boosted Decision Trees)&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v2/notebooks/2_BasicModels/gradient_boosted_trees.ipynb&#34;&gt;notebooks&lt;/a&gt;). Implement a Gradient Boosted Decision Trees with TensorFlow 2.0+ to predict house value using Boston Housing dataset.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;3 - Neural Networks&lt;/h4&gt; &#xA;&lt;h5&gt;Supervised&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Simple Neural Network&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v2/notebooks/3_NeuralNetworks/neural_network.ipynb&#34;&gt;notebook&lt;/a&gt;). Use TensorFlow 2.0 &#39;layers&#39; and &#39;model&#39; API to build a simple neural network to classify MNIST digits dataset.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Simple Neural Network (low-level)&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v2/notebooks/3_NeuralNetworks/neural_network_raw.ipynb&#34;&gt;notebook&lt;/a&gt;). Raw implementation of a simple neural network to classify MNIST digits dataset.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Convolutional Neural Network&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v2/notebooks/3_NeuralNetworks/convolutional_network.ipynb&#34;&gt;notebook&lt;/a&gt;). Use TensorFlow 2.0+ &#39;layers&#39; and &#39;model&#39; API to build a convolutional neural network to classify MNIST digits dataset.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Convolutional Neural Network (low-level)&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v2/notebooks/3_NeuralNetworks/convolutional_network_raw.ipynb&#34;&gt;notebook&lt;/a&gt;). Raw implementation of a convolutional neural network to classify MNIST digits dataset.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Recurrent Neural Network (LSTM)&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v2/notebooks/3_NeuralNetworks/recurrent_network.ipynb&#34;&gt;notebook&lt;/a&gt;). Build a recurrent neural network (LSTM) to classify MNIST digits dataset, using TensorFlow 2.0 &#39;layers&#39; and &#39;model&#39; API.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Bi-directional Recurrent Neural Network (LSTM)&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v2/notebooks/3_NeuralNetworks/bidirectional_rnn.ipynb&#34;&gt;notebook&lt;/a&gt;). Build a bi-directional recurrent neural network (LSTM) to classify MNIST digits dataset, using TensorFlow 2.0+ &#39;layers&#39; and &#39;model&#39; API.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Dynamic Recurrent Neural Network (LSTM)&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v2/notebooks/3_NeuralNetworks/dynamic_rnn.ipynb&#34;&gt;notebook&lt;/a&gt;). Build a recurrent neural network (LSTM) that performs dynamic calculation to classify sequences of variable length, using TensorFlow 2.0+ &#39;layers&#39; and &#39;model&#39; API.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h5&gt;Unsupervised&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Auto-Encoder&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v2/notebooks/3_NeuralNetworks/autoencoder.ipynb&#34;&gt;notebook&lt;/a&gt;). Build an auto-encoder to encode an image to a lower dimension and re-construct it.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;DCGAN (Deep Convolutional Generative Adversarial Networks)&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v2/notebooks/3_NeuralNetworks/dcgan.ipynb&#34;&gt;notebook&lt;/a&gt;). Build a Deep Convolutional Generative Adversarial Network (DCGAN) to generate images from noise.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;4 - Utilities&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Save and Restore a model&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v2/notebooks/4_Utils/save_restore_model.ipynb&#34;&gt;notebook&lt;/a&gt;). Save and Restore a model with TensorFlow 2.0+.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Build Custom Layers &amp;amp; Modules&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v2/notebooks/4_Utils/build_custom_layers.ipynb&#34;&gt;notebook&lt;/a&gt;). Learn how to build your own layers / modules and integrate them into TensorFlow 2.0+ Models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tensorboard&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v2/notebooks/4_Utils/tensorboard.ipynb&#34;&gt;notebook&lt;/a&gt;). Track and visualize neural network computation graph, metrics, weights and more using TensorFlow 2.0+ tensorboard.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;5 - Data Management&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Load and Parse data&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v2/notebooks/5_DataManagement/load_data.ipynb&#34;&gt;notebook&lt;/a&gt;). Build efficient data pipeline with TensorFlow 2.0 (Numpy arrays, Images, CSV files, custom data, ...).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Build and Load TFRecords&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v2/notebooks/5_DataManagement/tfrecords.ipynb&#34;&gt;notebook&lt;/a&gt;). Convert data into TFRecords format, and load them with TensorFlow 2.0+.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Image Transformation (i.e. Image Augmentation)&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v2/notebooks/5_DataManagement/image_transformation.ipynb&#34;&gt;notebook&lt;/a&gt;). Apply various image augmentation techniques with TensorFlow 2.0+, to generate distorted images for training.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;6 - Hardware&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-GPU Training&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v2/notebooks/6_Hardware/multigpu_training.ipynb&#34;&gt;notebook&lt;/a&gt;). Train a convolutional neural network with multiple GPUs on CIFAR-10 dataset.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TensorFlow v1&lt;/h2&gt; &#xA;&lt;p&gt;The tutorial index for TF v1 is available here: &lt;a href=&#34;https://raw.githubusercontent.com/aymericdamien/TensorFlow-Examples/master/tensorflow_v1&#34;&gt;TensorFlow v1.15 Examples&lt;/a&gt;. Or see below for a list of the examples.&lt;/p&gt; &#xA;&lt;h2&gt;Dataset&lt;/h2&gt; &#xA;&lt;p&gt;Some examples require MNIST dataset for training and testing. Don&#39;t worry, this dataset will automatically be downloaded when running examples. MNIST is a database of handwritten digits, for a quick description of that dataset, you can check &lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/notebooks/0_Prerequisite/mnist_dataset_intro.ipynb&#34;&gt;this notebook&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Official Website: &lt;a href=&#34;http://yann.lecun.com/exdb/mnist/&#34;&gt;http://yann.lecun.com/exdb/mnist/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;To download all the examples, simply clone this repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/aymericdamien/TensorFlow-Examples&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run them, you also need the latest version of TensorFlow. To install it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install tensorflow&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or (with GPU support):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install tensorflow_gpu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more details about TensorFlow installation, you can check &lt;a href=&#34;https://www.tensorflow.org/install/&#34;&gt;TensorFlow Installation Guide&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;TensorFlow v1 Examples - Index&lt;/h2&gt; &#xA;&lt;p&gt;The tutorial index for TF v1 is available here: &lt;a href=&#34;https://raw.githubusercontent.com/aymericdamien/TensorFlow-Examples/master/tensorflow_v1&#34;&gt;TensorFlow v1.15 Examples&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;0 - Prerequisite&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/notebooks/tensorflow_v1/0_Prerequisite/ml_introduction.ipynb&#34;&gt;Introduction to Machine Learning&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/notebooks/tensorflow_v1/0_Prerequisite/mnist_dataset_intro.ipynb&#34;&gt;Introduction to MNIST Dataset&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;1 - Introduction&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hello World&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/notebooks/1_Introduction/helloworld.ipynb&#34;&gt;notebook&lt;/a&gt;) (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/examples/1_Introduction/helloworld.py&#34;&gt;code&lt;/a&gt;). Very simple example to learn how to print &#34;hello world&#34; using TensorFlow.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Basic Operations&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/notebooks/tensorflow_v1/1_Introduction/basic_operations.ipynb&#34;&gt;notebook&lt;/a&gt;) (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-examples/Examples/blob/master/tensorflow_v1/1_Introduction/basic_operations.py&#34;&gt;code&lt;/a&gt;). A simple example that cover TensorFlow basic operations.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;TensorFlow Eager API basics&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/notebooks/1_Introduction/basic_eager_api.ipynb&#34;&gt;notebook&lt;/a&gt;) (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/examples/1_Introduction/basic_eager_api.py&#34;&gt;code&lt;/a&gt;). Get started with TensorFlow&#39;s Eager API.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;2 - Basic Models&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Linear Regression&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/notebooks/2_BasicModels/linear_regression.ipynb&#34;&gt;notebook&lt;/a&gt;) (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/examples/2_BasicModels/linear_regression.py&#34;&gt;code&lt;/a&gt;). Implement a Linear Regression with TensorFlow.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Linear Regression (eager api)&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/notebooks/2_BasicModels/linear_regression_eager_api.ipynb&#34;&gt;notebook&lt;/a&gt;) (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/examples/2_BasicModels/linear_regression_eager_api.py&#34;&gt;code&lt;/a&gt;). Implement a Linear Regression using TensorFlow&#39;s Eager API.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Logistic Regression&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/notebooks/2_BasicModels/logistic_regression.ipynb&#34;&gt;notebook&lt;/a&gt;) (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/examples/2_BasicModels/logistic_regression.py&#34;&gt;code&lt;/a&gt;). Implement a Logistic Regression with TensorFlow.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Logistic Regression (eager api)&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/notebooks/2_BasicModels/logistic_regression_eager_api.ipynb&#34;&gt;notebook&lt;/a&gt;) (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/examples/2_BasicModels/logistic_regression_eager_api.py&#34;&gt;code&lt;/a&gt;). Implement a Logistic Regression using TensorFlow&#39;s Eager API.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Nearest Neighbor&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/notebooks/2_BasicModels/nearest_neighbor.ipynb&#34;&gt;notebook&lt;/a&gt;) (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/examples/2_BasicModels/nearest_neighbor.py&#34;&gt;code&lt;/a&gt;). Implement Nearest Neighbor algorithm with TensorFlow.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;K-Means&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/notebooks/2_BasicModels/kmeans.ipynb&#34;&gt;notebook&lt;/a&gt;) (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/examples/2_BasicModels/kmeans.py&#34;&gt;code&lt;/a&gt;). Build a K-Means classifier with TensorFlow.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Random Forest&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/notebooks/2_BasicModels/random_forest.ipynb&#34;&gt;notebook&lt;/a&gt;) (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/examples/2_BasicModels/random_forest.py&#34;&gt;code&lt;/a&gt;). Build a Random Forest classifier with TensorFlow.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Gradient Boosted Decision Tree (GBDT)&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/notebooks/2_BasicModels/gradient_boosted_decision_tree.ipynb&#34;&gt;notebook&lt;/a&gt;) (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/examples/2_BasicModels/gradient_boosted_decision_tree.py&#34;&gt;code&lt;/a&gt;). Build a Gradient Boosted Decision Tree (GBDT) with TensorFlow.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Word2Vec (Word Embedding)&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/notebooks/2_BasicModels/word2vec.ipynb&#34;&gt;notebook&lt;/a&gt;) (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/examples/2_BasicModels/word2vec.py&#34;&gt;code&lt;/a&gt;). Build a Word Embedding Model (Word2Vec) from Wikipedia data, with TensorFlow.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;3 - Neural Networks&lt;/h4&gt; &#xA;&lt;h5&gt;Supervised&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Simple Neural Network&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/3_NeuralNetworks/notebooks/neural_network_raw.ipynb&#34;&gt;notebook&lt;/a&gt;) (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/examples/3_NeuralNetworks/neural_network_raw.py&#34;&gt;code&lt;/a&gt;). Build a simple neural network (a.k.a Multi-layer Perceptron) to classify MNIST digits dataset. Raw TensorFlow implementation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Simple Neural Network (tf.layers/estimator api)&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/notebooks/3_NeuralNetworks/neural_network.ipynb&#34;&gt;notebook&lt;/a&gt;) (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/examples/3_NeuralNetworks/neural_network.py&#34;&gt;code&lt;/a&gt;). Use TensorFlow &#39;layers&#39; and &#39;estimator&#39; API to build a simple neural network (a.k.a Multi-layer Perceptron) to classify MNIST digits dataset.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Simple Neural Network (eager api)&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/notebooks/3_NeuralNetworks/neural_network_eager_api.ipynb&#34;&gt;notebook&lt;/a&gt;) (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/examples/3_NeuralNetworks/neural_network_eager_api.py&#34;&gt;code&lt;/a&gt;). Use TensorFlow Eager API to build a simple neural network (a.k.a Multi-layer Perceptron) to classify MNIST digits dataset.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Convolutional Neural Network&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/notebooks/3_NeuralNetworks/convolutional_network_raw.ipynb&#34;&gt;notebook&lt;/a&gt;) (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/examples/3_NeuralNetworks/convolutional_network_raw.py&#34;&gt;code&lt;/a&gt;). Build a convolutional neural network to classify MNIST digits dataset. Raw TensorFlow implementation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Convolutional Neural Network (tf.layers/estimator api)&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/notebooks/3_NeuralNetworks/convolutional_network.ipynb&#34;&gt;notebook&lt;/a&gt;) (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/examples/3_NeuralNetworks/convolutional_network.py&#34;&gt;code&lt;/a&gt;). Use TensorFlow &#39;layers&#39; and &#39;estimator&#39; API to build a convolutional neural network to classify MNIST digits dataset.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Recurrent Neural Network (LSTM)&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/notebooks/3_NeuralNetworks/recurrent_network.ipynb&#34;&gt;notebook&lt;/a&gt;) (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/examples/3_NeuralNetworks/recurrent_network.py&#34;&gt;code&lt;/a&gt;). Build a recurrent neural network (LSTM) to classify MNIST digits dataset.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Bi-directional Recurrent Neural Network (LSTM)&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/notebooks/3_NeuralNetworks/bidirectional_rnn.ipynb&#34;&gt;notebook&lt;/a&gt;) (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/examples/3_NeuralNetworks/bidirectional_rnn.py&#34;&gt;code&lt;/a&gt;). Build a bi-directional recurrent neural network (LSTM) to classify MNIST digits dataset.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Dynamic Recurrent Neural Network (LSTM)&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/notebooks/3_NeuralNetworks/dynamic_rnn.ipynb&#34;&gt;notebook&lt;/a&gt;) (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/examples/3_NeuralNetworks/dynamic_rnn.py&#34;&gt;code&lt;/a&gt;). Build a recurrent neural network (LSTM) that performs dynamic calculation to classify sequences of different length.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h5&gt;Unsupervised&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Auto-Encoder&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/notebooks/3_NeuralNetworks/autoencoder.ipynb&#34;&gt;notebook&lt;/a&gt;) (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/examples/3_NeuralNetworks/autoencoder.py&#34;&gt;code&lt;/a&gt;). Build an auto-encoder to encode an image to a lower dimension and re-construct it.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Variational Auto-Encoder&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/notebooks/3_NeuralNetworks/variational_autoencoder.ipynb&#34;&gt;notebook&lt;/a&gt;) (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/examples/3_NeuralNetworks/variational_autoencoder.py&#34;&gt;code&lt;/a&gt;). Build a variational auto-encoder (VAE), to encode and generate images from noise.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;GAN (Generative Adversarial Networks)&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/notebooks/3_NeuralNetworks/gan.ipynb&#34;&gt;notebook&lt;/a&gt;) (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/examples/3_NeuralNetworks/gan.py&#34;&gt;code&lt;/a&gt;). Build a Generative Adversarial Network (GAN) to generate images from noise.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;DCGAN (Deep Convolutional Generative Adversarial Networks)&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/notebooks/3_NeuralNetworks/dcgan.ipynb&#34;&gt;notebook&lt;/a&gt;) (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/examples/3_NeuralNetworks/dcgan.py&#34;&gt;code&lt;/a&gt;). Build a Deep Convolutional Generative Adversarial Network (DCGAN) to generate images from noise.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;4 - Utilities&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Save and Restore a model&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/notebooks/4_Utils/save_restore_model.ipynb&#34;&gt;notebook&lt;/a&gt;) (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/examples/4_Utils/save_restore_model.py&#34;&gt;code&lt;/a&gt;). Save and Restore a model with TensorFlow.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tensorboard - Graph and loss visualization&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/notebooks/4_Utils/tensorboard_basic.ipynb&#34;&gt;notebook&lt;/a&gt;) (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/examples/4_Utils/tensorboard_basic.py&#34;&gt;code&lt;/a&gt;). Use Tensorboard to visualize the computation Graph and plot the loss.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tensorboard - Advanced visualization&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/notebooks/4_Utils/tensorboard_advanced.ipynb&#34;&gt;notebook&lt;/a&gt;) (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/examples/4_Utils/tensorboard_advanced.py&#34;&gt;code&lt;/a&gt;). Going deeper into Tensorboard; visualize the variables, gradients, and more...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;5 - Data Management&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Build an image dataset&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/notebooks/5_DataManagement/build_an_image_dataset.ipynb&#34;&gt;notebook&lt;/a&gt;) (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/examples/5_DataManagement/build_an_image_dataset.py&#34;&gt;code&lt;/a&gt;). Build your own images dataset with TensorFlow data queues, from image folders or a dataset file.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;TensorFlow Dataset API&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/notebooks/5_DataManagement/tensorflow_dataset_api.ipynb&#34;&gt;notebook&lt;/a&gt;) (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/examples/5_DataManagement/tensorflow_dataset_api.py&#34;&gt;code&lt;/a&gt;). Introducing TensorFlow Dataset API for optimizing the input data pipeline.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Load and Parse data&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/notebooks/5_DataManagement/load_data.ipynb&#34;&gt;notebook&lt;/a&gt;). Build efficient data pipeline (Numpy arrays, Images, CSV files, custom data, ...).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Build and Load TFRecords&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/notebooks/5_DataManagement/tfrecords.ipynb&#34;&gt;notebook&lt;/a&gt;). Convert data into TFRecords format, and load them.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Image Transformation (i.e. Image Augmentation)&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/notebooks/5_DataManagement/image_transformation.ipynb&#34;&gt;notebook&lt;/a&gt;). Apply various image augmentation techniques, to generate distorted images for training.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;6 - Multi GPU&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Basic Operations on multi-GPU&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/notebooks/6_MultiGPU/multigpu_basics.ipynb&#34;&gt;notebook&lt;/a&gt;) (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/examples/6_MultiGPU/multigpu_basics.py&#34;&gt;code&lt;/a&gt;). A simple example to introduce multi-GPU in TensorFlow.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Train a Neural Network on multi-GPU&lt;/strong&gt; (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/notebooks/6_MultiGPU/multigpu_cnn.ipynb&#34;&gt;notebook&lt;/a&gt;) (&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples/raw/master/tensorflow_v1/examples/6_MultiGPU/multigpu_cnn.py&#34;&gt;code&lt;/a&gt;). A clear and simple TensorFlow implementation to train a convolutional neural network on multiple GPUs.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;More Examples&lt;/h2&gt; &#xA;&lt;p&gt;The following examples are coming from &lt;a href=&#34;https://github.com/tflearn/tflearn&#34;&gt;TFLearn&lt;/a&gt;, a library that provides a simplified interface for TensorFlow. You can have a look, there are many &lt;a href=&#34;https://github.com/tflearn/tflearn/tree/master/examples&#34;&gt;examples&lt;/a&gt; and &lt;a href=&#34;http://tflearn.org/doc_index/#api&#34;&gt;pre-built operations and layers&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Tutorials&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tflearn/tflearn/raw/master/tutorials/intro/quickstart.md&#34;&gt;TFLearn Quickstart&lt;/a&gt;. Learn the basics of TFLearn through a concrete machine learning task. Build and train a deep neural network classifier.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tflearn/tflearn/raw/master/examples&#34;&gt;TFLearn Examples&lt;/a&gt;. A large collection of examples using TFLearn.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>mlabonne/llm-course</title>
    <updated>2023-12-03T01:51:57Z</updated>
    <id>tag:github.com,2023-12-03:/mlabonne/llm-course</id>
    <link href="https://github.com/mlabonne/llm-course" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Course to get into Large Language Models (LLMs) with roadmaps and Colab notebooks.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üó£Ô∏è Large Language Model Course&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/maximelabonne&#34;&gt;Follow me on X&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://mlabonne.github.io/blog&#34;&gt;Blog&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python&#34;&gt;Hands-on GNN&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The LLM course is divided into three parts:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;üß© &lt;strong&gt;LLM Fundamentals&lt;/strong&gt; covers essential knowledge about mathematics, Python, and neural networks.&lt;/li&gt; &#xA; &lt;li&gt;üßë‚Äçüî¨ &lt;strong&gt;The LLM Scientist&lt;/strong&gt; focuses on learning how to build the best possible LLMs using the latest techniques&lt;/li&gt; &#xA; &lt;li&gt;üë∑ &lt;strong&gt;The LLM Engineer&lt;/strong&gt; focuses on how to create LLM-based solutions and deploy them.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;üìù Notebooks&lt;/h2&gt; &#xA;&lt;p&gt;A list of notebooks and articles related to large language models.&lt;/p&gt; &#xA;&lt;h3&gt;Fine-tuning&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Notebook&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Article&lt;/th&gt; &#xA;   &lt;th&gt;Notebook&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Fine-tune Llama 2 in Google Colab&lt;/td&gt; &#xA;   &lt;td&gt;Step-by-step guide to fine-tune your first Llama 2 model.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://mlabonne.github.io/blog/posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html&#34;&gt;Article&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1PEQyJO1-f6j0S_XJ8DV50NkpzasXkrzd?usp=sharing&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mlabonne/llm-course/main/images/colab.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Fine-tune LLMs with Axolotl&lt;/td&gt; &#xA;   &lt;td&gt;End-to-end guide to the state-of-the-art tool for fine-tuning.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://mlabonne.github.io/blog/posts/A_Beginners_Guide_to_LLM_Finetuning.html&#34;&gt;Article&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;W.I.P.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Fine-tune a Mistral-7b model with DPO&lt;/td&gt; &#xA;   &lt;td&gt;Boost the performance of supervised fine-tuned models with DPO.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://twitter.com/maximelabonne/status/1729936514107290022&#34;&gt;Tweet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/15iFBr1xWgztXvhrj5I9fBv20c7CFOPBE?usp=sharing&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mlabonne/llm-course/main/images/colab.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Quantization&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Notebook&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Article&lt;/th&gt; &#xA;   &lt;th&gt;Notebook&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1. Introduction to Weight Quantization&lt;/td&gt; &#xA;   &lt;td&gt;Large language model optimization using 8-bit quantization.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html&#34;&gt;Article&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1DPr4mUQ92Cc-xf4GgAaB6dFcFnWIvqYi?usp=sharing&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mlabonne/llm-course/main/images/colab.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2. 4-bit LLM Quantization using GPTQ&lt;/td&gt; &#xA;   &lt;td&gt;Quantize your own open-source LLMs to run them on consumer hardware.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://mlabonne.github.io/blog/4bit_quantization/&#34;&gt;Article&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1lSvVDaRgqQp_mWK_jC9gydz6_-y6Aq4A?usp=sharing&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mlabonne/llm-course/main/images/colab.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3. Quantize Llama 2 models with GGUF and llama.cpp&lt;/td&gt; &#xA;   &lt;td&gt;Quantize Llama 2 models with llama.cpp and upload GGUF versions to the HF Hub.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html&#34;&gt;Article&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1pL8k7m04mgE5jo2NrjGi8atB0j_37aDD?usp=sharing&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mlabonne/llm-course/main/images/colab.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;4. ExLlamaV2: The Fastest Library to Run&amp;nbsp;LLMs&lt;/td&gt; &#xA;   &lt;td&gt;Quantize and run EXL2&amp;nbsp;models and upload them to the HF Hub.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://mlabonne.github.io/blog/posts/ExLlamaV2_The_Fastest_Library_to_Run%C2%A0LLMs.html&#34;&gt;Article&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1yrq4XBlxiA0fALtMoT2dwiACVc77PHou?usp=sharing&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mlabonne/llm-course/main/images/colab.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Other&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Notebook&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Article&lt;/th&gt; &#xA;   &lt;th&gt;Notebook&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Decoding Strategies in Large Language Models&lt;/td&gt; &#xA;   &lt;td&gt;A guide to text generation from beam search to nucleus sampling&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://mlabonne.github.io/blog/posts/2022-06-07-Decoding_strategies.html&#34;&gt;Article&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/19CJlOS5lI29g-B3dziNn93Enez1yiHk2?usp=sharing&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mlabonne/llm-course/main/images/colab.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Visualizing GPT-2&#39;s Loss Landscape&lt;/td&gt; &#xA;   &lt;td&gt;3D plot of the loss landscape based on weight pertubations.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://twitter.com/maximelabonne/status/1667618081844219904&#34;&gt;Tweet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1Fu1jikJzFxnSPzR_V2JJyDVWWJNXssaL?usp=sharing&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mlabonne/llm-course/main/images/colab.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Improve ChatGPT with Knowledge Graphs&lt;/td&gt; &#xA;   &lt;td&gt;Augment ChatGPT&#39;s answers with knowledge graphs.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://mlabonne.github.io/blog/posts/Article_Improve_ChatGPT_with_Knowledge_Graphs.html&#34;&gt;Article&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1mwhOSw9Y9bgEaIFKT4CLi0n18pXRM4cj?usp=sharing&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mlabonne/llm-course/main/images/colab.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;üß© LLM Fundamentals&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mlabonne/llm-course/main/images/roadmap_fundamentals.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;1. Mathematics for Machine Learning&lt;/h3&gt; &#xA;&lt;p&gt;Before mastering machine learning, it is important to understand the fundamental mathematical concepts that power these algorithms.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Linear Algebra&lt;/strong&gt;: This is crucial for understanding many algorithms, especially those used in deep learning. Key concepts include vectors, matrices, determinants, eigenvalues and eigenvectors, vector spaces, and linear transformations.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Calculus&lt;/strong&gt;: Many machine learning algorithms involve the optimization of continuous functions, which requires an understanding of derivatives, integrals, limits, and series. Multivariable calculus and the concept of gradients are also important.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Probability and Statistics&lt;/strong&gt;: These are crucial for understanding how models learn from data and make predictions. Key concepts include probability theory, random variables, probability distributions, expectations, variance, covariance, correlation, hypothesis testing, confidence intervals, maximum likelihood estimation, and Bayesian inference.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìö Resources:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=fNk_zzaMoSs&amp;amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&#34;&gt;3Blue1Brown - The Essence of Linear Algebra&lt;/a&gt;: Series of videos that give a geometric intuition to these concepts.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=qBigTkBLU6g&amp;amp;list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9&#34;&gt;StatQuest with Josh Starmer - Statistics Fundamentals&lt;/a&gt;: Offers simple and clear explanations for many statistical concepts.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://automata88.medium.com/list/cacc224d5e7d&#34;&gt;AP Statistics Intuition by Ms Aerin&lt;/a&gt;: List of Medium articles that provide the intuition behind every probability distribution.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://immersivemath.com/ila/learnmore.html&#34;&gt;Immersive Linear Algebra&lt;/a&gt;: Another visual interpretation of linear algebra.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.khanacademy.org/math/linear-algebra&#34;&gt;Khan Academy - Linear Algebra&lt;/a&gt;: Great for beginners as it explains the concepts in a very intuitive way.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.khanacademy.org/math/calculus-1&#34;&gt;Khan Academy - Calculus&lt;/a&gt;: An interactive course that covers all the basics of calculus.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.khanacademy.org/math/statistics-probability&#34;&gt;Khan Academy - Probability and Statistics&lt;/a&gt;: Delivers the material in an easy-to-understand format.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;2. Python for Machine Learning&lt;/h3&gt; &#xA;&lt;p&gt;Python is a powerful and flexible programming language that&#39;s particularly good for machine learning, thanks to its readability, consistency, and robust ecosystem of data science libraries.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Python Basics&lt;/strong&gt;: Understanding of Python&#39;s basic syntax, data types, error handling, and object-oriented programming is crucial.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Data Science Libraries&lt;/strong&gt;: Familiarity with NumPy for numerical operations, Pandas for data manipulation and analysis, Matplotlib and Seaborn for data visualization is a must.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Data Preprocessing&lt;/strong&gt;: This involves feature scaling and normalization, handling missing data, outlier detection, categorical data encoding, and splitting data into training, validation, and test sets.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Machine Learning Libraries&lt;/strong&gt;: Proficiency with Scikit-learn, a library providing a wide selection of supervised and unsupervised learning algorithms, is vital. Understanding how to implement algorithms like linear regression, logistic regression, decision trees, random forests, k-nearest neighbors (K-NN), and K-means clustering is important. Dimensionality reduction techniques like PCA and t-SNE are also very helpful for visualizing high-dimensional data.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìö Resources:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://realpython.com/&#34;&gt;Real Python&lt;/a&gt;: A comprehensive resource with articles and tutorials for both beginner and advanced Python concepts.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=rfscVS0vtbw&#34;&gt;freeCodeCamp - Learn Python&lt;/a&gt;: Long video that provides a full introduction into all of the core concepts in Python.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jakevdp.github.io/PythonDataScienceHandbook/&#34;&gt;Python Data Science Handbook&lt;/a&gt;: Free digital book that is a great resource for learning pandas, NumPy, matplotlib, and Seaborn.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtu.be/i_LwzRVP7bg&#34;&gt;freeCodeCamp - Machine Learning for Everybody&lt;/a&gt;: Practical introduction to different machine learning algorithms for beginners.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.udacity.com/course/intro-to-machine-learning--ud120&#34;&gt;Udacity - Intro to Machine Learning&lt;/a&gt;: Free course that covers PCA and several other machine learning concepts.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;3. Neural Networks&lt;/h3&gt; &#xA;&lt;p&gt;Neural networks are a fundamental part of many machine learning models, particularly in the realm of deep learning. To utilize them effectively, a comprehensive understanding of their design and mechanics is essential.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fundamentals&lt;/strong&gt;: This includes understanding the structure of a neural network such as layers, weights, biases, activation functions (sigmoid, tanh, ReLU, etc.)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Training and Optimization&lt;/strong&gt;: Familiarize yourself with backpropagation and different types of loss functions, like Mean Squared Error (MSE) and Cross-Entropy. Understand various optimization algorithms like Gradient Descent, Stochastic Gradient Descent, RMSprop, and Adam.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Overfitting&lt;/strong&gt;: It&#39;s crucial to comprehend the concept of overfitting (where a model performs well on training data but poorly on unseen data) and various regularization techniques to prevent it. Techniques include dropout, L1/L2 regularization, early stopping, and data augmentation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Implement a Multilayer Perceptron (MLP)&lt;/strong&gt;: Build an MLP, also known as a fully connected network, using PyTorch.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìö Resources:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=aircAruvnKk&#34;&gt;3Blue1Brown - But what is a Neural Network?&lt;/a&gt;: This video gives an intuitive explanation of neural networks and their inner workings.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=VyWAvY2CF9c&#34;&gt;freeCodeCamp - Deep Learning Crash Course&lt;/a&gt;: This video efficiently introduces all the most important concepts in deep learning.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://course.fast.ai/&#34;&gt;Fast.ai - Practical Deep Learning&lt;/a&gt;: Free course designed for people with coding experience who want to learn about deep learning.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&#34;&gt;Patrick Loeber - PyTorch Tutorials&lt;/a&gt;: Series of videos for complete beginners to learn about PyTorch.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;4. Natural Language Processing (NLP)&lt;/h3&gt; &#xA;&lt;p&gt;NLP is a fascinating branch of artificial intelligence that bridges the gap between human language and machine understanding. From simple text processing to understanding linguistic nuances, NLP plays a crucial role in many applications like translation, sentiment analysis, chatbots, and much more.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Text Preprocessing&lt;/strong&gt;: Learn various text preprocessing steps like tokenization (splitting text into words or sentences), stemming (reducing words to their root form), lemmatization (similar to stemming but considers the context), stop word removal, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Feature Extraction Techniques&lt;/strong&gt;: Become familiar with techniques to convert text data into a format that can be understood by machine learning algorithms. Key methods include Bag-of-words (BoW), Term Frequency-Inverse Document Frequency (TF-IDF), and n-grams.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Word Embeddings&lt;/strong&gt;: Word embeddings are a type of word representation that allows words with similar meanings to have similar representations. Key methods include Word2Vec, GloVe, and FastText.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Recurrent Neural Networks (RNNs)&lt;/strong&gt;: Understand the working of RNNs, a type of neural network designed to work with sequence data. Explore LSTMs and GRUs, two RNN variants that are capable of learning long-term dependencies.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìö Resources:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://realpython.com/natural-language-processing-spacy-python/&#34;&gt;RealPython - NLP with spaCy in Python&lt;/a&gt;: Exhaustive guide about the spaCy library for NLP tasks in Python.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.kaggle.com/learn-guide/natural-language-processing&#34;&gt;Kaggle - NLP Guide&lt;/a&gt;: A few notebooks and resources for a hands-on explanation of NLP in Python.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jalammar.github.io/illustrated-word2vec/&#34;&gt;Jay Alammar - The Illustration Word2Vec&lt;/a&gt;: A good reference to understand the famous Word2Vec architecture.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jaketae.github.io/study/pytorch-rnn/&#34;&gt;Jake Tae - PyTorch RNN from Scratch&lt;/a&gt;: Practical and simple implementation of RNN, LSTM, and GRU models in PyTorch.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colah.github.io/posts/2015-08-Understanding-LSTMs/&#34;&gt;colah&#39;s blog - Understanding LSTM Networks&lt;/a&gt;: A more theoretical article about the LSTM network.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üßë‚Äçüî¨ The LLM Scientist&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mlabonne/llm-course/main/images/roadmap_scientist.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;1. The LLM architecture&lt;/h3&gt; &#xA;&lt;p&gt;While an in-depth knowledge about the Transformer architecture is not required, it is important to have a good understanding of its inputs (tokens) and outputs (logits). The vanilla attention mechanism is another crucial component to master, as improved versions of it are introduced later on.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;High-level view&lt;/strong&gt;: Revisit the encoder-decoder Transformer architecture, and more specifically the decoder-only GPT architecture, which is used in every modern LLM.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tokenization&lt;/strong&gt;: Understand how to convert raw text data into a format that the model can understand, which involves splitting the text into tokens (usually words or subwords).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Attention mechanisms&lt;/strong&gt;: Grasp the theory behind attention mechanisms, including self-attention and scaled dot-product attention, which allows the model to focus on different parts of the input when producing an output.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Text generation&lt;/strong&gt;: Learn about the different ways the model can generate output sequences. Common strategies include greedy decoding, beam search, top-k sampling, and nucleus sampling.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìö &lt;strong&gt;References&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jalammar.github.io/illustrated-transformer/&#34;&gt;The Illustrated Transformer&lt;/a&gt; by Jay Alammar: A visual and intuitive explanation of the Transformer model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jalammar.github.io/illustrated-gpt2/&#34;&gt;The Illustrated GPT-2&lt;/a&gt; by Jay Alammar: Even more important than the previous article, it is focused on the GPT architecture, which is very similar to Llama&#39;s.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=kCc8FmEb1nY&#34;&gt;nanoGPT&lt;/a&gt; by Andrej Karpathy: A 2h-long YouTube video to reimplement GPT from scratch (for programmers).&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lilianweng.github.io/posts/2018-06-24-attention/&#34;&gt;Attention? Attention!&lt;/a&gt; by Lilian Weng: Introduce the need for attention in a more formal way.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mlabonne.github.io/blog/posts/2023-06-07-Decoding_strategies.html&#34;&gt;Decoding Strategies in LLMs&lt;/a&gt;: Provide code and a visual introduction to the different decoding strategies to generate text.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;2. Building an instruction dataset&lt;/h3&gt; &#xA;&lt;p&gt;While it&#39;s easy to find raw data from Wikipedia and other websites, it&#39;s difficult to collect pairs of instructions and answers in the wild. Like in traditional machine learning, the quality of the dataset will directly influence the quality of the model, which is why it might be the most important component in the fine-tuning process.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://crfm.stanford.edu/2023/03/13/alpaca.html&#34;&gt;Alpaca&lt;/a&gt;-like dataset&lt;/strong&gt;: Generate synthetic data from scratch with the OpenAI API (GPT). You can specify seeds and system prompts to create a diverse dataset.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Advanced techniques&lt;/strong&gt;: Learn how to improve existing datasets with &lt;a href=&#34;https://arxiv.org/abs/2304.12244&#34;&gt;Evol-Instruct&lt;/a&gt;, how to generate high-quality synthetic data like in the &lt;a href=&#34;https://arxiv.org/abs/2306.02707&#34;&gt;Orca&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2306.11644&#34;&gt;phi-1&lt;/a&gt; papers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Filtering data&lt;/strong&gt;: Traditional techniques involving regex, removing near-duplicates, focusing on answers with a high number of tokens, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Prompt templates&lt;/strong&gt;: There&#39;s no true standard way of formatting instructions and answers, which is why it&#39;s important to know about the different chat templates, such as &lt;a href=&#34;https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/chatgpt?tabs=python&amp;amp;pivots=programming-language-chat-ml&#34;&gt;ChatML&lt;/a&gt;, &lt;a href=&#34;https://crfm.stanford.edu/2023/03/13/alpaca.html&#34;&gt;Alpaca&lt;/a&gt;, etc.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìö &lt;strong&gt;References&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://wandb.ai/capecape/alpaca_ft/reports/How-to-Fine-Tune-an-LLM-Part-1-Preparing-a-Dataset-for-Instruction-Tuning--Vmlldzo1NTcxNzE2&#34;&gt;Preparing a Dataset for Instruction tuning&lt;/a&gt; by Thomas Capelle: Exploration of the Alpaca and Alpaca-GPT4 datasets and how to format them.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/mlearning-ai/generating-a-clinical-instruction-dataset-in-portuguese-with-langchain-and-gpt-4-6ee9abfa41ae&#34;&gt;Generating a Clinical Instruction Dataset&lt;/a&gt; by Solano Todeschini: Tutorial on how to create a synthetic instruction dataset using GPT-4.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/@kshitiz.sahay26/how-i-created-an-instruction-dataset-using-gpt-3-5-to-fine-tune-llama-2-for-news-classification-ed02fe41c81f&#34;&gt;GPT 3.5 for news classification&lt;/a&gt; by Kshitiz Sahay: Use GPT 3.5 to create an instruction dataset to fine-tune Llama 2 for news classification.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1GH8PW9-zAe4cXEZyOIE-T9uHXblIldAg?usp=sharing&#34;&gt;Dataset creation for fine-tuning LLM&lt;/a&gt;: Notebook that contains a few techniques to filter a dataset and upload the result.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/blog/chat-templates&#34;&gt;Chat Template&lt;/a&gt; by Matthew Carrigan: Hugging Face&#39;s page about prompt templates&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;3. Pre-training models&lt;/h3&gt; &#xA;&lt;p&gt;Pre-training is a very long and costly process, which is why this is not the focus of this course. It&#39;s good to have some level of understanding of what happens during pre-training, but hands-on experience is not required.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Data pipeline&lt;/strong&gt;: Pre-training requires huge datasets (e.g., &lt;a href=&#34;https://arxiv.org/abs/2307.09288&#34;&gt;Llama 2&lt;/a&gt; was trained on 2 trillion tokens) that need to be filtered, tokenized, and collated with a pre-defined vocabulary.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Causal language modeling&lt;/strong&gt;: Learn the difference between causal and masked language modeling, as well as the loss function used in this case. For efficient pre-training, learn more about &lt;a href=&#34;https://github.com/NVIDIA/Megatron-LM&#34;&gt;Megatron-LM&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Scaling laws&lt;/strong&gt;: The &lt;a href=&#34;https://arxiv.org/pdf/2001.08361.pdf&#34;&gt;scaling laws&lt;/a&gt; describe the expected model performance based on the model size, dataset size, and the amount of compute used for training.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;High-Performance Computing&lt;/strong&gt;: Out of scope here, but more knowledge about HPC is fundamental if you&#39;re planning to create your own LLM from scratch (hardware, distributed workload, etc.).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìö &lt;strong&gt;References&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Zjh-819/LLMDataHub&#34;&gt;LLMDataHub&lt;/a&gt; by Junhao Zhao: Curated list of datasets for pre-training, fine-tuning, and RLHF.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/learn/nlp-course/chapter7/6?fw=pt&#34;&gt;Training a causal language model from scratch&lt;/a&gt; by Hugging Face: Pre-train a GPT-2 model from scratch using the transformers library.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NVIDIA/Megatron-LM&#34;&gt;Megatron-LM&lt;/a&gt;: State-of-the-art library to efficiently pre-train models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jzhang38/TinyLlama&#34;&gt;TinyLlama&lt;/a&gt; by Zhang et al.: Check this project to get a good understanding of how a Llama model is trained from scratch.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/transformers/tasks/language_modeling&#34;&gt;Causal language modeling&lt;/a&gt; by Hugging Face: Explain the difference between causal and masked language modeling and how to quickly fine-tune a DistilGPT-2 model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications&#34;&gt;Chinchilla&#39;s wild implications&lt;/a&gt; by nostalgebraist: Discuss the scaling laws and explain what they mean to LLMs in general.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://bigscience.notion.site/BLOOM-BigScience-176B-Model-ad073ca07cdf479398d5f95d88e218c4&#34;&gt;BLOOM&lt;/a&gt; by BigScience: Notion pages that describes how the BLOOM model was built, with a lot of useful information about the engineering part and the problems that were encountered.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/metaseq/raw/main/projects/OPT/chronicles/OPT175B_Logbook.pdf&#34;&gt;OPT-175 Logbook&lt;/a&gt; by Meta: Research logs showing what went wrong and what went right. Useful if you&#39;re planning to pre-train a very large language model (in this case, 175B parameters).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;4. Supervised Fine-Tuning&lt;/h3&gt; &#xA;&lt;p&gt;Pre-trained models are only trained on a next-token prediction task, which is why they&#39;re not helpful assistants. SFT allows you to tweak them into responding to instructions. Moreover, it allows you to fine-tune your model on any data (private, not seen by GPT-4, etc.) and use it without having to pay for an API like OpenAI&#39;s.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Full fine-tuning&lt;/strong&gt;: Full fine-tuning refers to training all the parameters in the model. It is not an efficient technique, but it produces slightly better results.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.09685&#34;&gt;&lt;strong&gt;LoRA&lt;/strong&gt;&lt;/a&gt;: A parameter-efficient technique (PEFT) based on low-rank adapters. Instead of training all the parameters, we only train these adapters.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.14314&#34;&gt;&lt;strong&gt;QLoRA&lt;/strong&gt;&lt;/a&gt;: Another PEFT based on LoRA, which also quantizes the weights of the model in 4 bits and introduce paged optimizers to manage memory spikes.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/OpenAccess-AI-Collective/axolotl&#34;&gt;Axolotl&lt;/a&gt;&lt;/strong&gt;: A user-friendly and powerful fine-tuning tool that is used in a lot of state-of-the-art open-source models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.deepspeed.ai/&#34;&gt;&lt;strong&gt;DeepSpeed&lt;/strong&gt;&lt;/a&gt;: Efficient pre-training and fine-tuning of LLMs for multi-GPU and multi-node settings (implemented in Axolotl).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìö &lt;strong&gt;References&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://rentry.org/llm-training&#34;&gt;The Novice&#39;s LLM Training Guide&lt;/a&gt; by Alpin: Overview of the main concepts and parameters to consider when fine-tuning LLMs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lightning.ai/pages/community/lora-insights/&#34;&gt;LoRA insights&lt;/a&gt; by Sebastian Raschka: Practical insights about LoRA and how to select the best parameters.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mlabonne.github.io/blog/posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html&#34;&gt;Fine-Tune Your Own Llama 2 Model&lt;/a&gt;: Hands-on tutorial on how to fine-tune a Llama 2 model using Hugging Face libraries.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/padding-large-language-models-examples-with-llama-2-199fb10df8ff&#34;&gt;Padding Large Language Models&lt;/a&gt; by Benjamin Marie: Best practices to pad training examples for causal LLMs&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mlabonne.github.io/blog/posts/A_Beginners_Guide_to_LLM_Finetuning.html&#34;&gt;A Beginner&#39;s Guide to LLM Fine-Tuning&lt;/a&gt;: Tutorial on how to fine-tune a CodeLlama model using Axolotl.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;5. Reinforcement Learning from Human Feedback&lt;/h3&gt; &#xA;&lt;p&gt;After supervised fine-tuning, RLHF is a step used to align the LLM&#39;s answers with human expectations. The idea is to learn preferences from human (or artificial) feedback, which can be used to reduce biases, censor models, or make them act in a more useful way. It is more complex than SFT and often seen as optional.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Preference datasets&lt;/strong&gt;: These datasets typically contain several answers with some kind of ranking, which makes them more difficult to produce than instruction datasets.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1707.06347&#34;&gt;&lt;strong&gt;Proximal Policy Optimization&lt;/strong&gt;&lt;/a&gt;: This algorithm leverages a reward model that predicts whether a given text is highly ranked by humans. This prediction is then used to optimize the SFT model with a penalty based on KL divergence.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.18290&#34;&gt;Direct Preference Optimization&lt;/a&gt;&lt;/strong&gt;: DPO simplifies the process by reframing it as a classification problem. It uses a reference model instead of a reward model (no training needed) and only requires one hyperparameter, making it more stable and efficient.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìö &lt;strong&gt;References&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://wandb.ai/ayush-thakur/Intro-RLAIF/reports/An-Introduction-to-Training-LLMs-Using-Reinforcement-Learning-From-Human-Feedback-RLHF---VmlldzozMzYyNjcy&#34;&gt;An Introduction to Training LLMs using RLHF&lt;/a&gt; by Ayush Thakur: Explain why RLHF is desirable to reduce bias and increase performance in LLMs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/blog/rlhf&#34;&gt;Illustration RLHF&lt;/a&gt; by Hugging Face: Introduction to RLHF with reward model training and fine-tuning with reinforcement learning.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/blog/stackllama&#34;&gt;StackLLaMA&lt;/a&gt; by Hugging Face: Tutorial to efficiently align a LLaMA model with RLHF using the transformers library.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/blog/dpo-trl&#34;&gt;Fine-tune Llama 2 with DPO&lt;/a&gt; by Hugging Face: Tutorial to fine-tune a Llama 2 model with DPO.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://substack.com/profile/27393275-sebastian-raschka-phd&#34;&gt;LLM Training: RLHF and Its Alternatives&lt;/a&gt; by Sebastian Rashcka: Overview of the RLHF process and alternatives like RLAIF.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;6. Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;Evaluating LLMs is an undervalued part of the pipeline, which is time-consuming and moderately reliable. Your downstream task should dictate what you want to evaluate, but always remember the Goodhart&#39;s law: &#34;when a measure becomes a target, it ceases to be a good measure.&#34;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Traditional metrics&lt;/strong&gt;: Metrics like perplexity and BLEU score are not popular as they were because they&#39;re flawed in most contexts. It is still important to understand them and when they can be applied.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;General benchmarks&lt;/strong&gt;: Based on the &lt;a href=&#34;https://github.com/EleutherAI/lm-evaluation-harness&#34;&gt;Language Model Evaluation Harness&lt;/a&gt;, the &lt;a href=&#34;https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard&#34;&gt;Open LLM Leaderboard&lt;/a&gt; is the main benchmark for general-purpose LLMs (like ChatGPT). There are other popular benchmarks like &lt;a href=&#34;https://github.com/google/BIG-bench&#34;&gt;BigBench&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2306.05685&#34;&gt;MT-Bench&lt;/a&gt;, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Task-specific benchmarks&lt;/strong&gt;: Tasks like summarization, translation, question answering have dedicated benchmarks, metrics, and even subdomains (medical, financial, etc.), such as &lt;a href=&#34;https://pubmedqa.github.io/&#34;&gt;PubMedQA&lt;/a&gt; for biomedical question answering.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Human evaluation&lt;/strong&gt;: The most reliable evaluation is the acceptance rate by users or comparisons made by humans. If you want to know if a model performs well, the simplest but surest way is to use it yourself.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìö &lt;strong&gt;References&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/transformers/perplexity&#34;&gt;Perplexity of fixed-length models&lt;/a&gt; by Hugging Face: Overview of perplexity with code to implement it with the transformers library.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213&#34;&gt;BLEU at your own risk&lt;/a&gt; by Rachael Tatman: Overview of the BLEU score and its many issues with examples.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2307.03109&#34;&gt;A Survey on Evaluation of LLMs&lt;/a&gt; by Chang et al.: Comprehensive paper about what to evaluate, where to evaluate, and how to evaluate.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard&#34;&gt;Chatbot Arena Leaderboard&lt;/a&gt; by lmsys: Elo rating of general-purpose LLMs, based on comparisons made by humans.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;7. Quantization&lt;/h3&gt; &#xA;&lt;p&gt;Quantization is the process of converting the weights (and activations) of a model using a lower precision. For example, weights stored using 16 bits can be converted into a 4-bit representation. This technique has become increasingly important to reduce the computational and memory costs associated to LLMs.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Base techniques&lt;/strong&gt;: Learn the different levels of precision (FP32, FP16, INT8, etc.) and how to perform na√Øve quantization with absmax and zero-point techniques.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;GGUF and llama.cpp&lt;/strong&gt;: Originally designed to run on CPUs, &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; and the GGUF format have become the most popular tools to run LLMs on consumer-grade hardware.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;GPTQ and EXL2&lt;/strong&gt;: &lt;a href=&#34;https://arxiv.org/abs/2210.17323&#34;&gt;GPTQ&lt;/a&gt; and, more specifically, the &lt;a href=&#34;https://github.com/turboderp/exllamav2&#34;&gt;EXL2&lt;/a&gt; format offer an incredible speed but can only run on GPUs. Models also take a long time to be quantized.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;AWQ&lt;/strong&gt;: This new format is more accurate than GPTQ (lower perplexity) but uses a lot more VRAM and is not necessarily faster.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìö &lt;strong&gt;References&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html&#34;&gt;Introduction to quantization&lt;/a&gt;: Overview of quantization, absmax and zero-point quantization, and LLM.int8() with code.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html&#34;&gt;Quantize Llama models with llama.cpp&lt;/a&gt;: Tutorial on how to quantize a Llama 2 model using llama.cpp and the GGUF format.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html&#34;&gt;4-bit LLM Quantization with GPTQ&lt;/a&gt;: Tutorial on how to quantize an LLM using the GPTQ algorithm with AutoGPTQ.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mlabonne.github.io/blog/posts/ExLlamaV2_The_Fastest_Library_to_Run%C2%A0LLMs.html&#34;&gt;ExLlamaV2: The Fastest Library to Run LLMs&lt;/a&gt;: Guide on how to quantize a Mistral model using the EXL2 format and run it with the ExLlamaV2 library.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/friendliai/understanding-activation-aware-weight-quantization-awq-boosting-inference-serving-efficiency-in-10bb0faf63a8&#34;&gt;Understanding Activation-Aware Weight Quantization&lt;/a&gt; by FriendliAI: Overview of the AWQ technique and its benefits.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;8. Inference optimization&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Flash Attention&lt;/strong&gt;: Optimization of the attention mechanism to transform its complexity from quadratic to linear, speeding up both training and inference.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Key-value cache&lt;/strong&gt;: Understand the key-value cache and the improvements introduced in &lt;a href=&#34;https://arxiv.org/abs/1911.02150&#34;&gt;Multi-Query Attention&lt;/a&gt; (MQA) and &lt;a href=&#34;https://arxiv.org/abs/2305.13245&#34;&gt;Grouped-Query Attention&lt;/a&gt; (GQA).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Speculative decoding&lt;/strong&gt;: Use a small model to produce drafts that are then reviewed by a larger model to speed up text generation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Positional encoding&lt;/strong&gt;: Understand positional encodings in transformers, particularly relative schemes like &lt;a href=&#34;https://arxiv.org/abs/2104.09864&#34;&gt;RoPE&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2108.12409&#34;&gt;ALiBi&lt;/a&gt;, and &lt;a href=&#34;https://arxiv.org/abs/2309.00071&#34;&gt;YaRN&lt;/a&gt;. (Not directly connected to inference optimization but to longer context windows.)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìö &lt;strong&gt;References&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one&#34;&gt;GPU Inference&lt;/a&gt; by Hugging Face: Explain how to optimize inference on GPUs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/transformers/main/en/llm_tutorial_optimization&#34;&gt;Optimizing LLMs for Speed and Memory&lt;/a&gt; by Hugging Face: Explain three main techniques to optimize speed and memory, namely quantization, Flash Attention, and architectural innovations.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/blog/assisted-generation&#34;&gt;Assisted Generation&lt;/a&gt; by Hugging Face: HF&#39;s version of speculative decoding, it&#39;s an interesting blog post about how it works with code to implement it.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.eleuther.ai/yarn/&#34;&gt;Extending the RoPE&lt;/a&gt; by EleutherAI: Article that summarizes the different position-encoding techniques.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://kaiokendev.github.io/context&#34;&gt;Extending Context is Hard... but not Impossible&lt;/a&gt; by kaiokendev: This blog post introduces the SuperHOT technique and provides an excellent survey of related work.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üë∑ The LLM Engineer&lt;/h2&gt; &#xA;&lt;p&gt;W.I.P.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Contributions&lt;/h3&gt; &#xA;&lt;p&gt;Feel free to contact me if you think other topics should be mentioned or if the current architecture can be improved.&lt;/p&gt; &#xA;&lt;h3&gt;Acknowledgements&lt;/h3&gt; &#xA;&lt;p&gt;This roadmap was inspired by the excellent &lt;a href=&#34;https://github.com/milanm/DevOps-Roadmap&#34;&gt;DevOps Roadmap&lt;/a&gt; from Milan Milanoviƒá and Romano Roth.&lt;/p&gt; &#xA;&lt;p&gt;Special thanks to Thomas Thelen for motivating me to create a roadmap, and Andr√© Frade for his input and review of the first draft.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Disclaimer: I am not affiliated with any sources listed here.&lt;/em&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>run-llama/llama-hub</title>
    <updated>2023-12-03T01:51:57Z</updated>
    <id>tag:github.com,2023-12-03:/run-llama/llama-hub</id>
    <link href="https://github.com/run-llama/llama-hub" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A library of data loaders for LLMs made by the community -- to be used with LlamaIndex and/or LangChain&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LlamaHub ü¶ô&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Original creator&lt;/strong&gt;: Jesse Zhang (GH: &lt;a href=&#34;https://github.com/emptycrown&#34;&gt;emptycrown&lt;/a&gt;, Twitter: &lt;a href=&#34;https://twitter.com/thejessezhang&#34;&gt;@thejessezhang&lt;/a&gt;), who courteously donated the repo to LlamaIndex!&lt;/p&gt; &#xA;&lt;p&gt;This is a simple library of all the data loaders / readers / tools / llama-packs that have been created by the community. The goal is to make it extremely easy to connect large language models to a large variety of knowledge sources. These are general-purpose utilities that are meant to be used in &lt;a href=&#34;https://github.com/run-llama/llama_index&#34;&gt;LlamaIndex&lt;/a&gt;, &lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;LangChain&lt;/a&gt; and more!.&lt;/p&gt; &#xA;&lt;p&gt;Loaders and readers allow you to easily ingest data for search and retrieval by a large language model, while tools allow the models to both read and write to third party data services and sources. Ultimately, this allows you to create your own customized data agent to intelligently work with you and your data to unlock the full capability of next level large language models.&lt;/p&gt; &#xA;&lt;p&gt;For a variety of examples of data agents, see the &lt;a href=&#34;https://github.com/emptycrown/llama-hub/tree/main/llama_hub/tools/notebooks&#34;&gt;notebooks directory&lt;/a&gt;. You can find example Jupyter notebooks for creating data agents that can load and parse data from Google Docs, SQL Databases, Notion, and Slack, and also manage your Google Calendar, and Gmail inbox, or read and use OpenAPI specs.&lt;/p&gt; &#xA;&lt;p&gt;For an easier way to browse the integrations available, check out the website here: &lt;a href=&#34;https://llamahub.ai/&#34;&gt;https://llamahub.ai/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;img width=&#34;1465&#34; alt=&#34;Screenshot 2023-07-17 at 6 12 32 PM&#34; src=&#34;https://github.com/ajhofmann/llama-hub/assets/10040285/5e344de4-4aca-4f6c-9944-46c00baa5eb2&#34;&gt; &#xA;&lt;h2&gt;Usage (Use &lt;code&gt;llama-hub&lt;/code&gt; as PyPI package)&lt;/h2&gt; &#xA;&lt;p&gt;These general-purpose loaders are designed to be used as a way to load data into &lt;a href=&#34;https://github.com/jerryjliu/llama_index&#34;&gt;LlamaIndex&lt;/a&gt; and/or subsequently used in &lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;LangChain&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install llama-hub&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;LlamaIndex&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from llama_index import VectorStoreIndex&#xA;from llama_hub.google_docs import GoogleDocsReader&#xA;&#xA;gdoc_ids = [&#39;1wf-y2pd9C878Oh-FmLH7Q_BQkljdm6TQal-c1pUfrec&#39;]&#xA;loader = GoogleDocsReader()&#xA;documents = loader.load_data(document_ids=gdoc_ids)&#xA;index = VectorStoreIndex.from_documents(documents)&#xA;index.query(&#39;Where did the author go to school?&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;LlamaIndex Data Agent&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from llama_index.agent import OpenAIAgent&#xA;import openai&#xA;openai.api_key = &#39;sk-api-key&#39;&#xA;&#xA;from llama_hub.tools.google_calendar import GoogleCalendarToolSpec&#xA;tool_spec = GoogleCalendarToolSpec()&#xA;&#xA;agent = OpenAIAgent.from_tools(tool_spec.to_tool_list())&#xA;agent.chat(&#39;what is the first thing on my calendar today&#39;)&#xA;agent.chat(&#34;Please create an event for tomorrow at 4pm to review pull requests&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For a variety of examples of creating and using data agents, see the &lt;a href=&#34;https://github.com/emptycrown/llama-hub/tree/main/llama_hub/tools/notebooks&#34;&gt;notebooks directory&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;LangChain&lt;/h3&gt; &#xA;&lt;p&gt;Note: Make sure you change the description of the &lt;code&gt;Tool&lt;/code&gt; to match your use case.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from llama_index import VectorStoreIndex&#xA;from llama_hub.google_docs import GoogleDocsReader&#xA;from langchain.llms import OpenAI&#xA;from langchain.chains.question_answering import load_qa_chain&#xA;&#xA;# load documents&#xA;gdoc_ids = [&#39;1wf-y2pd9C878Oh-FmLH7Q_BQkljdm6TQal-c1pUfrec&#39;]&#xA;loader = GoogleDocsReader()&#xA;documents = loader.load_data(document_ids=gdoc_ids)&#xA;langchain_documents = [d.to_langchain_format() for d in documents]&#xA;&#xA;# initialize sample QA chain&#xA;llm = OpenAI(temperature=0)&#xA;qa_chain = load_qa_chain(llm)&#xA;question=&#34;&amp;lt;query here&amp;gt;&#34;&#xA;answer = qa_chain.run(input_documents=langchain_documents, question=question)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Loader Usage (Use &lt;code&gt;download_loader&lt;/code&gt; from LlamaIndex)&lt;/h2&gt; &#xA;&lt;p&gt;You can also use the loaders with &lt;code&gt;download_loader&lt;/code&gt; from LlamaIndex in a single line of code.&lt;/p&gt; &#xA;&lt;p&gt;For example, see the code snippets below using the Google Docs Loader.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from llama_index import VectorStoreIndex, download_loader&#xA;&#xA;GoogleDocsReader = download_loader(&#39;GoogleDocsReader&#39;)&#xA;&#xA;gdoc_ids = [&#39;1wf-y2pd9C878Oh-FmLH7Q_BQkljdm6TQal-c1pUfrec&#39;]&#xA;loader = GoogleDocsReader()&#xA;documents = loader.load_data(document_ids=gdoc_ids)&#xA;index = VectorStoreIndex.from_documents(documents)&#xA;index.query(&#39;Where did the author go to school?&#39;)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;LLama-Pack Usage&lt;/h2&gt; &#xA;&lt;p&gt;Llama-packs can be downloaded using the &lt;code&gt;llamaindex-cli&lt;/code&gt; tool that comes with &lt;code&gt;llama-index&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;llamaindex-cli download-llamapack ZephyrQueryEnginePack --download-dir ./zephyr_pack&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or with the &lt;code&gt;download_llama_pack&lt;/code&gt; function directly:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from llama_index.llama_packs import download_llama_pack&#xA;&#xA;# download and install dependencies&#xA;LlavaCompletionPack = download_llama_pack(&#xA;  &#34;LlavaCompletionPack&#34;, &#34;./llava_pack&#34;&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;How to add a loader/tool/llama-pack&lt;/h2&gt; &#xA;&lt;p&gt;Adding a loader/tool/llama-pack simply requires forking this repo and making a Pull Request. The Llama Hub website will update automatically when a new &lt;code&gt;llama-hub&lt;/code&gt; release is made. However, please keep in mind the following guidelines when making your PR.&lt;/p&gt; &#xA;&lt;h3&gt;Step 0: Setup virtual environment, install Poetry and dependencies&lt;/h3&gt; &#xA;&lt;p&gt;Create a new Python virtual environment. The command below creates an environment in &lt;code&gt;.venv&lt;/code&gt;, and activates it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m venv .venv&#xA;source .venv/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;if you are in windows, use the following to activate your virtual environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;.venv\scripts\activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install poetry:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install poetry&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install the required dependencies (this will also install &lt;code&gt;llama_index&lt;/code&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will create an editable install of &lt;code&gt;llama-hub&lt;/code&gt; in your venv.&lt;/p&gt; &#xA;&lt;h3&gt;Step 1: Create a new directory&lt;/h3&gt; &#xA;&lt;p&gt;For loaders, create a new directory in &lt;code&gt;llama_hub&lt;/code&gt;, for tools create a directory in &lt;code&gt;llama_hub/tools&lt;/code&gt;, and for llama-packs create a directory in &lt;code&gt;llama_hub/llama_packs&lt;/code&gt; It can be nested within another, but name it something unique because the name of the directory will become the identifier for your loader (e.g. &lt;code&gt;google_docs&lt;/code&gt;). Inside your new directory, create a &lt;code&gt;__init__.py&lt;/code&gt; file specifying the module&#39;s public interface with &lt;code&gt;__all__&lt;/code&gt;, a &lt;code&gt;base.py&lt;/code&gt; file which will contain your loader implementation, and, if needed, a &lt;code&gt;requirements.txt&lt;/code&gt; file to list the package dependencies of your loader. Those packages will automatically be installed when your loader is used, so no need to worry about that anymore!&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;d like, you can create the new directory and files by running the following script in the &lt;code&gt;llama_hub&lt;/code&gt; directory. Just remember to put your dependencies into a &lt;code&gt;requirements.txt&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./add_loader.sh [NAME_OF_NEW_DIRECTORY]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step 2: Write your README&lt;/h3&gt; &#xA;&lt;p&gt;Inside your new directory, create a &lt;code&gt;README.md&lt;/code&gt; that mirrors that of the existing ones. It should have a summary of what your loader or tool does, its inputs, and how it is used in the context of LlamaIndex and LangChain.&lt;/p&gt; &#xA;&lt;h3&gt;Step 3: Add your loader to the library.json file&lt;/h3&gt; &#xA;&lt;p&gt;Finally, add your loader to the &lt;code&gt;llama_hub/library.json&lt;/code&gt; file (or for the equivilant &lt;code&gt;library.json&lt;/code&gt; under &lt;code&gt;tools/&lt;/code&gt; or &lt;code&gt;llama-packs/&lt;/code&gt;) so that it may be used by others. As is exemplified by the current file, add the class name of your loader or tool, along with its ID, author, etc. This file is referenced by the Llama Hub website and the download function within LlamaIndex.&lt;/p&gt; &#xA;&lt;h3&gt;Step 4: Make a Pull Request!&lt;/h3&gt; &#xA;&lt;p&gt;Create a PR against the main branch. We typically review the PR within a day. To help expedite the process, it may be helpful to provide screenshots (either in the PR or in the README directly) Show your data loader or tool in action!&lt;/p&gt; &#xA;&lt;h2&gt;Running tests&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python3.9 -m venv .venv&#xA;source .venv/bin/activate &#xA;pip3 install -r test_requirements.txt&#xA;&#xA;poetry run pytest tests &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Changelog&lt;/h2&gt; &#xA;&lt;p&gt;If you want to track the latest version updates / see which loaders are added to each release, take a look at our &lt;a href=&#34;https://github.com/emptycrown/llama-hub/raw/main/CHANGELOG.md&#34;&gt;full changelog here&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;h3&gt;How do I test my loader before it&#39;s merged?&lt;/h3&gt; &#xA;&lt;p&gt;There is an argument called &lt;code&gt;loader_hub_url&lt;/code&gt; in &lt;a href=&#34;https://github.com/jerryjliu/llama_index/raw/main/llama_index/readers/download.py&#34;&gt;&lt;code&gt;download_loader&lt;/code&gt;&lt;/a&gt; that defaults to the main branch of this repo. You can set it to your branch or fork to test your new loader.&lt;/p&gt; &#xA;&lt;h3&gt;Should I create a PR against LlamaHub or the LlamaIndex repo directly?&lt;/h3&gt; &#xA;&lt;p&gt;If you have a data loader PR, by default let&#39;s try to create it against LlamaHub! We will make exceptions in certain cases (for instance, if we think the data loader should be core to the LlamaIndex repo).&lt;/p&gt; &#xA;&lt;p&gt;For all other PR&#39;s relevant to LlamaIndex, let&#39;s create it directly against the &lt;a href=&#34;https://github.com/jerryjliu/llama_index&#34;&gt;LlamaIndex repo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Other questions?&lt;/h3&gt; &#xA;&lt;p&gt;Feel free to hop into the &lt;a href=&#34;https://discord.gg/dGcwcsnxhU&#34;&gt;community Discord&lt;/a&gt; or tag the official &lt;a href=&#34;https://twitter.com/llama_index&#34;&gt;Twitter account&lt;/a&gt;!&lt;/p&gt;</summary>
  </entry>
</feed>