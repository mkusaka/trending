<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-11-20T01:44:16Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>tensorflow/examples</title>
    <updated>2022-11-20T01:44:16Z</updated>
    <id>tag:github.com,2022-11-20:/tensorflow/examples</id>
    <link href="https://github.com/tensorflow/examples" rel="alternate"></link>
    <summary type="html">&lt;p&gt;TensorFlow examples&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TensorFlow Examples&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://www.tensorflow.org/images/tf_logo_social.png&#34;&gt;&#xA; &lt;br&gt;&#xA; &lt;br&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Most important links!&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/examples/master/community&#34;&gt;Community examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/examples/master/courses/udacity_deep_learning&#34;&gt;Course materials&lt;/a&gt; for the &lt;a href=&#34;https://www.udacity.com/course/deep-learning--ud730&#34;&gt;Deep Learning&lt;/a&gt; class on Udacity&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you are looking to learn TensorFlow, don&#39;t miss the &lt;a href=&#34;http://github.com/tensorflow/docs&#34;&gt;core TensorFlow documentation&lt;/a&gt; which is largely runnable code. Those notebooks can be opened in Colab from &lt;a href=&#34;https://tensorflow.org&#34;&gt;tensorflow.org&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;What is this repo?&lt;/h2&gt; &#xA;&lt;p&gt;This is the TensorFlow example repo. It has several classes of material:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Showcase examples and documentation for our fantastic &lt;a href=&#34;https://tensorflow.org/community&#34;&gt;TensorFlow Community&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Provide examples mentioned on TensorFlow.org&lt;/li&gt; &#xA; &lt;li&gt;Publish material supporting official TensorFlow courses&lt;/li&gt; &#xA; &lt;li&gt;Publish supporting material for the &lt;a href=&#34;https://blog.tensorflow.org&#34;&gt;TensorFlow Blog&lt;/a&gt; and &lt;a href=&#34;https://youtube.com/tensorflow&#34;&gt;TensorFlow YouTube Channel&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We welcome community contributions, see &lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/examples/master/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; and, for style help, &lt;a href=&#34;https://www.tensorflow.org/community/contribute/docs_style&#34;&gt;Writing TensorFlow documentation&lt;/a&gt; guide.&lt;/p&gt; &#xA;&lt;p&gt;To file an issue, use the tracker in the &lt;a href=&#34;https://github.com/tensorflow/tensorflow/issues/new?template=20-documentation-issue.md&#34;&gt;tensorflow/tensorflow&lt;/a&gt; repo.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/examples/master/LICENSE&#34;&gt;Apache License 2.0&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>cloneofsimo/paint-with-words-sd</title>
    <updated>2022-11-20T01:44:16Z</updated>
    <id>tag:github.com,2022-11-20:/cloneofsimo/paint-with-words-sd</id>
    <link href="https://github.com/cloneofsimo/paint-with-words-sd" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Implementation of Paint-with-words with Stable Diffusion : method from eDiffi that let you generate image from text-labeled segmentation map.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Paint-with-Words, Implemented with Stable diffusion&lt;/h1&gt; &#xA;&lt;h2&gt;Subtle Control of the Image Generation&lt;/h2&gt; &#xA;&lt;!-- #region --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/cloneofsimo/paint-with-words-sd/master/contents/rabbit_mage.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;!-- #endregion --&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Notice how without PwW the cloud is missing.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;!-- #region --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/cloneofsimo/paint-with-words-sd/master/contents/road.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;!-- #endregion --&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Notice how without PwW, abandoned city is missing, and road becomes purple as well.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Shift the object : Same seed, just the segmentation map&#39;s positional difference&lt;/h2&gt; &#xA;&lt;!-- #region --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/cloneofsimo/paint-with-words-sd/master/contents/aurora_1_merged.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;!-- #endregion --&gt; &#xA;&lt;!-- #region --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/cloneofsimo/paint-with-words-sd/master/contents/aurora_2_merged.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;!-- #endregion --&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&#34;A digital painting of a half-frozen lake near mountains under a full moon and aurora. A boat is in the middle of the lake. Highly detailed.&#34;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Notice how nearly all of the composition remains the same, other than the position of the moon.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Recently, researchers from NVIDIA proposed &lt;a href=&#34;https://arxiv.org/abs/2211.01324&#34;&gt;eDiffi&lt;/a&gt;. In the paper, they suggested method that allows &#34;painting with word&#34;. Basically, this is like make-a-scene, but with just using adjusted cross-attention score. You can see the results and detailed method in the paper.&lt;/p&gt; &#xA;&lt;p&gt;Their paper and their method was not open-sourced. Yet, paint-with-words can be implemented with Stable Diffusion since they share common Cross Attention module. So, I implemented it with Stable Diffusion.&lt;/p&gt; &#xA;&lt;!-- #region --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/cloneofsimo/paint-with-words-sd/master/contents/paint_with_words_figure.png&#34;&gt; &lt;/p&gt; &#xA;&lt;!-- #endregion --&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install git+https://github.com/cloneofsimo/paint-with-words-sd.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Basic Usage&lt;/h1&gt; &#xA;&lt;p&gt;Before running, fill in the variable &lt;code&gt;HF_TOKEN&lt;/code&gt; in &lt;code&gt;.env&lt;/code&gt; file with Huggingface token for Stable Diffusion, and load_dotenv().&lt;/p&gt; &#xA;&lt;p&gt;Prepare segmentation map, and map-color : tag label such as below. keys are (R, G, B) format, and values are tag label.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;{&#xA;    (0, 0, 0): &#34;cat,1.0&#34;,&#xA;    (255, 255, 255): &#34;dog,1.0&#34;,&#xA;    (13, 255, 0): &#34;tree,1.5&#34;,&#xA;    (90, 206, 255): &#34;sky,0.2&#34;,&#xA;    (74, 18, 1): &#34;ground,0.2&#34;,&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You neeed to have them so that they are in format &#34;{label},{strength}&#34;, where strength is additional weight of the attention score you will give during generation, i.e., it will have more effect.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;import dotenv&#xA;from PIL import Image&#xA;&#xA;from paint_with_words import paint_with_words&#xA;&#xA;settings = {&#xA;    &#34;color_context&#34;: {&#xA;        (0, 0, 0): &#34;cat,1.0&#34;,&#xA;        (255, 255, 255): &#34;dog,1.0&#34;,&#xA;        (13, 255, 0): &#34;tree,1.5&#34;,&#xA;        (90, 206, 255): &#34;sky,0.2&#34;,&#xA;        (74, 18, 1): &#34;ground,0.2&#34;,&#xA;    },&#xA;    &#34;color_map_img_path&#34;: &#34;contents/example_input.png&#34;,&#xA;    &#34;input_prompt&#34;: &#34;realistic photo of a dog, cat, tree, with beautiful sky, on sandy ground&#34;,&#xA;    &#34;output_img_path&#34;: &#34;contents/output_cat_dog.png&#34;,&#xA;}&#xA;&#xA;&#xA;dotenv.load_dotenv()&#xA;&#xA;color_map_image = Image.open(settings[&#34;color_map_img_path&#34;]).convert(&#34;RGB&#34;)&#xA;color_context = settings[&#34;color_context&#34;]&#xA;input_prompt = settings[&#34;input_prompt&#34;]&#xA;&#xA;img = paint_with_words(&#xA;    color_context=color_context,&#xA;    color_map_image=color_map_image,&#xA;    input_prompt=input_prompt,&#xA;    num_inference_steps=30,&#xA;    guidance_scale=7.5,&#xA;    device=&#34;cuda:0&#34;,&#xA;)&#xA;&#xA;img.save(settings[&#34;output_img_path&#34;])&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;There is minimal working example in &lt;code&gt;runner.py&lt;/code&gt; that is self contained. Please have a look!&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Weight Scaling&lt;/h1&gt; &#xA;&lt;p&gt;In the paper, they used $w \log (1 + \sigma) \max (Q^T K)$ to scale appropriate attention weight. However, this wasn&#39;t optimal after few tests, found by &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/4406&#34;&gt;CookiePPP&lt;/a&gt;. You can check out the effect of the functions below:&lt;/p&gt; &#xA;&lt;!-- #region --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/cloneofsimo/paint-with-words-sd/master/contents/compare_std.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;!-- #endregion --&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;$w&#39; = w \log (1 + \sigma) std (Q^T K)$&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;!-- #region --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/cloneofsimo/paint-with-words-sd/master/contents/compare_max.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;!-- #endregion --&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;$w&#39; = w \log (1 + \sigma) \max (Q^T K)$&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;!-- #region --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/cloneofsimo/paint-with-words-sd/master/contents/compare_log2_std.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;!-- #endregion --&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;$w&#39; = w \log (1 + \sigma^2) std (Q^T K)$&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;You can define your own weight function and further tweak the configurations by defining &lt;code&gt;weight_function&lt;/code&gt; argument in &lt;code&gt;paint_with_words&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;w_f = lambda w, sigma, qk: 0.4 * w * math.log(sigma**2 + 1) * qk.std()&#xA;&#xA;img = paint_with_words(&#xA;    color_context=color_context,&#xA;    color_map_image=color_map_image,&#xA;    input_prompt=input_prompt,&#xA;    num_inference_steps=20,&#xA;    guidance_scale=7.5,&#xA;    device=&#34;cuda:0&#34;,&#xA;    preloaded_utils=loaded,&#xA;    weight_function=w_f&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;More on the weight function, (but higher)&lt;/h2&gt; &#xA;&lt;!-- #region --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/cloneofsimo/paint-with-words-sd/master/contents/compare_4_std.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;!-- #endregion --&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;$w&#39; = w \log (1 + \sigma) std (Q^T K)$&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;!-- #region --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/cloneofsimo/paint-with-words-sd/master/contents/compare_4_max.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;!-- #endregion --&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;$w&#39; = w \log (1 + \sigma) \max (Q^T K)$&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;!-- #region --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/cloneofsimo/paint-with-words-sd/master/contents/compare_4_log2_std.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;!-- #endregion --&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;$w&#39; = w \log (1 + \sigma^2) std (Q^T K)$&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h1&gt;Using other Fine-tuned models&lt;/h1&gt; &#xA;&lt;p&gt;If you are from Automatic1111 community, you maybe used to using native LDM checkpoint formats, not diffuser-checkpoint format. Luckily, there is a quick script that allows conversion. &lt;a href=&#34;https://github.com/huggingface/diffusers/raw/main/scripts/convert_original_stable_diffusion_to_diffusers.py&#34;&gt;this&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python change_model_path.py --checkpoint_path custom_model.ckpt --scheduler_type ddim --dump_path custom_model_diffusion_format&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now, use the converted model in &lt;code&gt;paint_with_words&lt;/code&gt; function.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from paint_with_words import paint_with_words, pww_load_tools&#xA;&#xA;loaded = pww_load_tools(&#xA;    &#34;cuda:0&#34;,&#xA;    scheduler_type=LMSDiscreteScheduler,&#xA;    local_model_path=&#34;./custom_model_diffusion_format&#34;&#xA;)&#xA;#...&#xA;img = paint_with_words(&#xA;    color_context=color_context,&#xA;    color_map_image=color_map_image,&#xA;    input_prompt=input_prompt,&#xA;    num_inference_steps=30,&#xA;    guidance_scale=7.5,&#xA;    device=&#34;cuda:0&#34;,&#xA;    weight_function=lambda w, sigma, qk: 0.4 * w * math.log(1 + sigma) * qk.max(),&#xA;    preloaded_utils=loaded&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Example Notebooks&lt;/h1&gt; &#xA;&lt;p&gt;You can view the minimal working notebook &lt;a href=&#34;https://raw.githubusercontent.com/cloneofsimo/paint-with-words-sd/master/contents/notebooks/paint_with_words.ipynb&#34;&gt;here&lt;/a&gt; or &lt;a href=&#34;https://colab.research.google.com/drive/1MZfGaY3aQQn5_T-6bkXFE1rI59A2nJlU?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cloneofsimo/paint-with-words-sd/master/contents/notebooks/paint_with_words.ipynb&#34;&gt;Painting with words&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cloneofsimo/paint-with-words-sd/master/contents/notebooks/paint_with_words_textual_inversion.ipynb&#34;&gt;Painting with words + Textual Inversion&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;TODO&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;I&#39;ll work on these after school exam is over&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Make extensive comparisons for different weight scaling functions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Create word latent-based cross-attention generations.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Check if statement &#34;making background weight smaller is better&#34; is justifiable, by using some standard metrics&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Create AUTOMATIC1111&#39;s interface&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Create Gradio interface&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Create tutorial&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; See if starting with some &#34;known image latent&#34; is helpful. If it is, we might as well hard-code some initial latent.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Region based seeding, where we set seed for each regions. Can be simply implemented with extra argument in &lt;code&gt;COLOR_CONTEXT&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; sentence wise text seperation. Currently token is the smallest unit that influences cross-attention. This needs to be fixed. (Can be done pretty trivially)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Allow different models to be used. use &lt;a href=&#34;https://github.com/huggingface/diffusers/raw/main/scripts/convert_original_stable_diffusion_to_diffusers.py&#34;&gt;this&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &#34;negative region&#34;, where we can set some region to &#34;not&#34; have some semantics. can be done with classifier-free guidance.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Img2ImgPaintWithWords -&amp;gt; Img2Img, but with extra text segmentation map for better control&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; InpaintPaintwithWords -&amp;gt; inpaint, but with extra text segmentation map for better control&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support for other schedulers&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>dataquestio/project-walkthroughs</title>
    <updated>2022-11-20T01:44:16Z</updated>
    <id>tag:github.com,2022-11-20:/dataquestio/project-walkthroughs</id>
    <link href="https://github.com/dataquestio/project-walkthroughs" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Overview&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains files, notebooks, and data used for live project walkthroughs on Dataquest. You can watch the project walkthroughs on &lt;a href=&#34;https://www.youtube.com/channel/UC_lePY0Lm0E2-_IkYUWpI5A&#34;&gt;Youtube&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;These walkthroughs help you build complete end-to-end projects that can go into your portfolio.&lt;/p&gt; &#xA;&lt;h1&gt;Prerequisites&lt;/h1&gt; &#xA;&lt;p&gt;To complete these projects, you&#39;ll need to have a good understanding of:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python syntax, including functions, if statements, and data structures&lt;/li&gt; &#xA; &lt;li&gt;Data cleaning&lt;/li&gt; &#xA; &lt;li&gt;Pandas syntax&lt;/li&gt; &#xA; &lt;li&gt;Using Jupyter notebook&lt;/li&gt; &#xA; &lt;li&gt;The basics of machine learning&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please make sure you&#39;ve completed these Dataquest courses (or know the material) before trying these projects:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.dataquest.io/course/introduction-to-python/&#34;&gt;Python Introduction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.dataquest.io/course/for-loops-and-conditional-statements-in-python/&#34;&gt;For Loops and If Statements&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.dataquest.io/course/dictionaries-frequency-tables-and-functions-in-python/&#34;&gt;Dictionaries In Python&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.dataquest.io/course/python-functions-and-jupyter-notebook/&#34;&gt;Functions and Jupyter Notebook&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.dataquest.io/course/python-for-data-science-intermediate/&#34;&gt;Python Intermediate&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.dataquest.io/course/pandas-fundamentals/&#34;&gt;Pandas and NumPy Fundamentals&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.dataquest.io/course/python-datacleaning/&#34;&gt;Data Cleaning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.dataquest.io/course/machine-learning-fundamentals/&#34;&gt;Machine Learning Fundamentals&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>