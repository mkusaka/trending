<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-06-22T01:42:00Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>NVIDIA/Isaac-GR00T</title>
    <updated>2025-06-22T01:42:00Z</updated>
    <id>tag:github.com,2025-06-22:/NVIDIA/Isaac-GR00T</id>
    <link href="https://github.com/NVIDIA/Isaac-GR00T" rel="alternate"></link>
    <summary type="html">&lt;p&gt;NVIDIA Isaac GR00T N1.5 is the world&#39;s first open foundation model for generalized humanoid robot reasoning and skills.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/media/header_compress.png&#34; width=&#34;800&#34; alt=&#34;NVIDIA Isaac GR00T N1.5 Header&#34;&gt; &#xA; &lt;!-- --- --&gt; &#xA; &lt;p style=&#34;font-size: 1.2em;&#34;&gt; &lt;a href=&#34;https://developer.nvidia.com/isaac/gr00t&#34;&gt;&lt;strong&gt;Website&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/nvidia/GR00T-N1.5-3B&#34;&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/datasets/nvidia/PhysicalAI-Robotics-GR00T-X-Embodiment-Sim&#34;&gt;&lt;strong&gt;Dataset&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2503.14734&#34;&gt;&lt;strong&gt;Paper&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/NVIDIA/Isaac-GR00T/actions/workflows/main.yml&#34;&gt;&lt;img src=&#34;https://github.com/NVIDIA/Isaac-GR00T/actions/workflows/main.yml/badge.svg?sanitize=true&#34; alt=&#34;CI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/psf/black&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true&#34; alt=&#34;Code style: black&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pycqa.github.io/isort/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&amp;amp;labelColor=ef8336&#34; alt=&#34;Imports: isort&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://star-history.com/#NVIDIA/Isaac-GR00T&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/NVIDIA/Isaac-GR00T?style=flat-square&#34; alt=&#34;GitHub star chart&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/NVIDIA/Isaac-GR00T/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-raw/NVIDIA/Isaac-GR00T?style=flat-square&#34; alt=&#34;Open Issues&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;NVIDIA Isaac GR00T&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/media/robot-demo.gif&#34; width=&#34;800&#34; alt=&#34;NVIDIA Isaac GR00T N1.5 Header&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;We just released GR00T N1.5, an updated version of GR00T N1 with improved performance and new features. Check out the release blog post (&lt;a href=&#34;https://research.nvidia.com/labs/gear/gr00t-n1_5/&#34;&gt;https://research.nvidia.com/labs/gear/gr00t-n1_5/&lt;/a&gt;) for more details.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;To use the older version, N1, please checkout the &lt;a href=&#34;https://github.com/NVIDIA/Isaac-GR00T/tree/n1-release&#34;&gt;n1-release&lt;/a&gt; release branch.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;NVIDIA Isaac GR00T N1.5 is an open foundation model for generalized humanoid robot reasoning and skills. This cross-embodiment model takes multimodal input, including language and images, to perform manipulation tasks in diverse environments.&lt;/p&gt; &#xA;&lt;p&gt;GR00T N1.5 is trained on an expansive humanoid dataset, consisting of real captured data, synthetic data generated using the components of NVIDIA Isaac GR00T Blueprint (&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/media/videos&#34;&gt;examples of neural-generated trajectories&lt;/a&gt;), and internet-scale video data. It is adaptable through post-training for specific embodiments, tasks and environments.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/media/real-data.gif&#34; height=&#34;150&#34; alt=&#34;real-robot-data&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/media/sim-data.gif&#34; height=&#34;150&#34; alt=&#34;sim-robot-data&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;The neural network architecture of GR00T N1.5 is a combination of vision-language foundation model and diffusion transformer head that denoises continuous actions. Here is a schematic diagram of the architecture:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/media/model-architecture.png&#34; width=&#34;800&#34; alt=&#34;model-architecture&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Here is the general procedure to use GR00T N1.5:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Assume the user has already collected a dataset of robot demonstrations in the form of (video, state, action) triplets.&lt;/li&gt; &#xA; &lt;li&gt;The user will first convert the demonstration data into the LeRobot compatible data schema (more info in &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/getting_started/LeRobot_compatible_data_schema.md&#34;&gt;&lt;code&gt;getting_started/LeRobot_compatible_data_schema.md&lt;/code&gt;&lt;/a&gt;), which is compatible with the upstream &lt;a href=&#34;https://github.com/huggingface/lerobot&#34;&gt;Huggingface LeRobot&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Our repo provides examples of different configurations for training with different robot embodiments.&lt;/li&gt; &#xA; &lt;li&gt;Our repo provides convenient scripts for finetuning the pre-trained GR00T N1.5 model on user&#39;s data, and running inference.&lt;/li&gt; &#xA; &lt;li&gt;The user will connect the &lt;code&gt;Gr00tPolicy&lt;/code&gt; to the robot controller to execute actions on their target hardware.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;What&#39;s New in GR00T N1.5&lt;/h2&gt; &#xA;&lt;p&gt;GR00T N1.5 represents a significant upgrade over GR00T N1, with improvements in both model architecture and data leading to better performance in many aspects.&lt;/p&gt; &#xA;&lt;h3&gt;Model and Data Improvements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Frozen VLM&lt;/strong&gt;: The vision-language model remains frozen during both pretraining and finetuning, preserving language understanding and improving generalization&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Enhanced VLM Grounding&lt;/strong&gt;: Updated to Eagle 2.5 with improved grounding capabilities and physical understanding, achieving 40.4 IoU on GR-1 grounding tasks (vs 35.5 for Qwen2.5VL).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Simplified Adapter&lt;/strong&gt;: Streamlined MLP connection between vision encoder and LLM with added layer normalization.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;FLARE Integration&lt;/strong&gt;: Added Future Latent Representation Alignment (&lt;a href=&#34;https://research.nvidia.com/labs/gear/flare&#34;&gt;FLARE&lt;/a&gt;) objective alongside flow matching loss, enabling effective learning from human ego videos&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;DreamGen Integration&lt;/strong&gt;: Incorporated synthetic neural trajectories generated via &lt;a href=&#34;https://research.nvidia.com/labs/gear/dreamgen&#34;&gt;DreamGen&lt;/a&gt; to enable generalization to novel behaviors and tasks beyond teleoperation data&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Performance Improvements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Language Following&lt;/strong&gt;: Significantly improved language command following versus N1 - 93.3% vs 46.6% on GR-1 manipulation tasks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Data Efficiency&lt;/strong&gt;: Better performance in low-data regimes (0-shot and few-shot scenarios)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Better Novel Object Generalization&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;New Embodiment Heads&lt;/strong&gt;: Added support for single arm robots with end-effector (EEF) control space via &lt;code&gt;EmbodimentTag.OXE_DROID&lt;/code&gt; head, and humanoid robots with grippers via &lt;code&gt;EmbodimentTag.AGIBOT_GENIE1&lt;/code&gt; head, expanding beyond joint space control to enable broader robot compatibility&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;These improvements make GR00T N1.5 particularly effective for applications requiring strong language understanding, few-shot adaptation, and generalization to novel objects and environments. See our GR00T N1.5 &lt;a href=&#34;https://research.nvidia.com/labs/gear/gr00t-n1_5&#34;&gt;tech blog&lt;/a&gt; for more details on the model and experimental results.&lt;/p&gt; &#xA;&lt;h2&gt;Target Audience&lt;/h2&gt; &#xA;&lt;p&gt;GR00T N1.5 is intended for researchers and professionals in humanoid robotics. This repository provides tools to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Leverage a pre-trained foundation model for robot control&lt;/li&gt; &#xA; &lt;li&gt;Fine-tune on small, custom datasets&lt;/li&gt; &#xA; &lt;li&gt;Adapt the model to specific robotics tasks with minimal data&lt;/li&gt; &#xA; &lt;li&gt;Deploy the model for inference&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The focus is on enabling customization of robot behaviors through finetuning.&lt;/p&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We have tested the code on Ubuntu 20.04 and 22.04, GPU: H100, L40, RTX 4090 and A6000 for finetuning and Python==3.10, CUDA version 12.4.&lt;/li&gt; &#xA; &lt;li&gt;For inference, we have tested on Ubuntu 20.04 and 22.04, GPU: RTX 3090, RTX 4090 and A6000.&lt;/li&gt; &#xA; &lt;li&gt;If you haven&#39;t installed CUDA 12.4, please follow the instructions &lt;a href=&#34;https://docs.nvidia.com/cuda/cuda-installation-guide-linux/&#34;&gt;here&lt;/a&gt; to install it.&lt;/li&gt; &#xA; &lt;li&gt;If you haven&#39;t installed tensorrt, please follow the instructions &lt;a href=&#34;https://docs.nvidia.com/deeplearning/tensorrt/latest/installing-tensorrt/installing.html#&#34;&gt;here&lt;/a&gt; to install it.&lt;/li&gt; &#xA; &lt;li&gt;Please make sure you have the following dependencies installed in your system: &lt;code&gt;ffmpeg&lt;/code&gt;, &lt;code&gt;libsm6&lt;/code&gt;, &lt;code&gt;libxext6&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation Guide&lt;/h2&gt; &#xA;&lt;p&gt;Clone the repo:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/NVIDIA/Isaac-GR00T&#xA;cd Isaac-GR00T&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Create a new conda environment and install the dependencies. We recommend Python 3.10:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note that, please make sure your CUDA version is 12.4. Otherwise, you may have a hard time with properly configuring flash-attn module.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;conda create -n gr00t python=3.10&#xA;conda activate gr00t&#xA;pip install --upgrade setuptools&#xA;pip install -e .[base]&#xA;pip install --no-build-isolation flash-attn==2.7.1.post4 &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Getting started with this repo&lt;/h2&gt; &#xA;&lt;p&gt;We provide accessible Jupyter notebooks and detailed documentation in the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/getting_started&#34;&gt;&lt;code&gt;./getting_started&lt;/code&gt;&lt;/a&gt; folder. Utility scripts can be found in the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/scripts&#34;&gt;&lt;code&gt;./scripts&lt;/code&gt;&lt;/a&gt; folder. Additionally, a comprehensive tutorial for finetuning the model on the SO-101 robot is available on &lt;a href=&#34;https://huggingface.co/blog/nvidia/gr00t-n1-5-so101-tuning&#34;&gt;HuggingFace&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;1. Data Format &amp;amp; Loading&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To load and process the data, we use &lt;a href=&#34;https://github.com/huggingface/lerobot&#34;&gt;Huggingface LeRobot data&lt;/a&gt;, but with a more detailed modality and annotation schema (we call it &#34;LeRobot compatible data schema&#34;).&lt;/li&gt; &#xA; &lt;li&gt;An example of LeRobot dataset is stored here: &lt;code&gt;./demo_data/robot_sim.PickNPlace&lt;/code&gt;. (with additional &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/demo_data/robot_sim.PickNPlace/meta/modality.json&#34;&gt;&lt;code&gt;modality.json&lt;/code&gt;&lt;/a&gt; file)&lt;/li&gt; &#xA; &lt;li&gt;Detailed explanation of the dataset format is available in &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/getting_started/LeRobot_compatible_data_schema.md&#34;&gt;&lt;code&gt;getting_started/LeRobot_compatible_data_schema.md&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;We support multiple embodiments with the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/getting_started/4_deeper_understanding.md#embodiment-action-head-fine-tuning&#34;&gt;&lt;code&gt;EmbodimentTag&lt;/code&gt;&lt;/a&gt; system.&lt;/li&gt; &#xA; &lt;li&gt;Once your data is organized in this format, you can load the data using &lt;code&gt;LeRobotSingleDataset&lt;/code&gt; class.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from gr00t.data.dataset import LeRobotSingleDataset&#xA;from gr00t.data.embodiment_tags import EmbodimentTag&#xA;from gr00t.data.dataset import ModalityConfig&#xA;from gr00t.experiment.data_config import DATA_CONFIG_MAP&#xA;&#xA;# get the data config&#xA;data_config = DATA_CONFIG_MAP[&#34;fourier_gr1_arms_only&#34;]&#xA;&#xA;# get the modality configs and transforms&#xA;modality_config = data_config.modality_config()&#xA;transforms = data_config.transform()&#xA;&#xA;# This is a LeRobotSingleDataset object that loads the data from the given dataset path.&#xA;dataset = LeRobotSingleDataset(&#xA;    dataset_path=&#34;demo_data/robot_sim.PickNPlace&#34;,&#xA;    modality_configs=modality_config,&#xA;    transforms=None,  # we can choose to not apply any transforms&#xA;    embodiment_tag=EmbodimentTag.GR1, # the embodiment to use&#xA;)&#xA;&#xA;# This is an example of how to access the data.&#xA;dataset[5]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/getting_started/0_load_dataset.ipynb&#34;&gt;&lt;code&gt;getting_started/0_load_dataset.ipynb&lt;/code&gt;&lt;/a&gt; is an interactive tutorial on how to load the data and process it to interface with the GR00T N1.5 model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/scripts/load_dataset.py&#34;&gt;&lt;code&gt;scripts/load_dataset.py&lt;/code&gt;&lt;/a&gt; is an executable script with the same content as the notebook.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Try run the script to load the dataset&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/load_dataset.py --dataset-path ./demo_data/robot_sim.PickNPlace&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;2. Inference&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The GR00T N1.5 model is hosted on &lt;a href=&#34;https://huggingface.co/nvidia/GR00T-N1.5-3B&#34;&gt;Huggingface&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Example cross embodiment dataset is available at &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/demo_data/robot_sim.PickNPlace&#34;&gt;demo_data/robot_sim.PickNPlace&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;2.1 Inference with PyTorch&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from gr00t.model.policy import Gr00tPolicy&#xA;from gr00t.data.embodiment_tags import EmbodimentTag&#xA;&#xA;# 1. Load the modality config and transforms, or use above&#xA;modality_config = ComposedModalityConfig(...)&#xA;transforms = ComposedModalityTransform(...)&#xA;&#xA;# 2. Load the dataset&#xA;dataset = LeRobotSingleDataset(.....&amp;lt;Same as above&amp;gt;....)&#xA;&#xA;# 3. Load pre-trained model&#xA;policy = Gr00tPolicy(&#xA;    model_path=&#34;nvidia/GR00T-N1.5-3B&#34;,&#xA;    modality_config=modality_config,&#xA;    modality_transform=transforms,&#xA;    embodiment_tag=EmbodimentTag.GR1,&#xA;    device=&#34;cuda&#34;&#xA;)&#xA;&#xA;# 4. Run inference&#xA;action_chunk = policy.get_action(dataset[0])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/getting_started/1_gr00t_inference.ipynb&#34;&gt;&lt;code&gt;getting_started/1_gr00t_inference.ipynb&lt;/code&gt;&lt;/a&gt; is an interactive Jupyter notebook tutorial to build an inference pipeline.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;User can also run the inference service using the provided script. The inference service can run in either server mode or client mode.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/inference_service.py --model-path nvidia/GR00T-N1.5-3B --server&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On a different terminal, run the client mode to send requests to the server.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/inference_service.py  --client&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2.2 Inference with Python TensorRT (Optional)&lt;/h3&gt; &#xA;&lt;p&gt;To inference with ONNX and TensorRT, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/deployment_scripts/README.md&#34;&gt;&lt;code&gt;deployment_scripts/README.md&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;3. Fine-Tuning&lt;/h2&gt; &#xA;&lt;p&gt;Users can run the finetuning script below to finetune the model with the example dataset. A tutorial is available in &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/getting_started/2_finetuning.ipynb&#34;&gt;&lt;code&gt;getting_started/2_finetuning.ipynb&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Then run the finetuning script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# first run --help to see the available arguments&#xA;python scripts/gr00t_finetune.py --help&#xA;&#xA;# then run the script&#xA;python scripts/gr00t_finetune.py --dataset-path ./demo_data/robot_sim.PickNPlace --num-gpus 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you are finetuning on a 4090, you need to pass the &lt;code&gt;--no-tune_diffusion_model&lt;/code&gt; flag when running &lt;code&gt;gr00t_finetune.py&lt;/code&gt; to avoid CUDA out of memory.&lt;/p&gt; &#xA;&lt;p&gt;You can also download a sample dataset from our huggingface sim data release &lt;a href=&#34;https://huggingface.co/datasets/nvidia/PhysicalAI-Robotics-GR00T-X-Embodiment-Sim&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;huggingface-cli download  nvidia/PhysicalAI-Robotics-GR00T-X-Embodiment-Sim \&#xA;  --repo-type dataset \&#xA;  --include &#34;gr1_arms_only.CanSort/**&#34; \&#xA;  --local-dir $HOME/gr00t_dataset&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The recommended finetuning configuration is to boost your batch size to the max, and train for 20k steps.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Hardware Performance Considerations&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Finetuning Performance&lt;/strong&gt;: We used 1 H100 node or L40 node for optimal finetuning. Other hardware configurations (e.g. A6000, RTX 4090) will also work but may take longer to converge. The exact batch size is dependent on the hardware, and on which component of the model is being tuned.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;LoRA finetuning&lt;/strong&gt;: We used 2 A6000 GPUs or 2 RTX 4090 GPUs for LoRA finetuning. Users can try out different configurations for effective finetuning.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Inference Performance&lt;/strong&gt;: For real-time inference, most modern GPUs perform similarly when processing a single sample. Our benchmarks show minimal difference between L40 and RTX 4090 for inference speed.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For new embodiment finetuning, checkout our notebook in &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/getting_started/3_0_new_embodiment_finetuning.md&#34;&gt;&lt;code&gt;getting_started/3_0_new_embodiment_finetuning.md&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Choosing the Right Embodiment Head&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/media/robots-banner.png&#34; width=&#34;1000&#34; alt=&#34;robots-banner&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;GR00T N1.5 provides three pretrained embodiment heads optimized for different robot configurations:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;EmbodimentTag.GR1&lt;/code&gt;&lt;/strong&gt;: Designed for humanoid robots with dexterous hands using absolute joint space control&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;EmbodimentTag.OXE_DROID&lt;/code&gt;&lt;/strong&gt;: Optimized for single arm robots using delta end-effector (EEF) control&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;EmbodimentTag.AGIBOT_GENIE1&lt;/code&gt;&lt;/strong&gt;: Built for humanoid robots with grippers using absolute joint space control&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;EmbodimentTag.NEW_EMBODIMENT&lt;/code&gt;&lt;/strong&gt;: (Non-pretrained) New embodiment head for finetuning on new robot embodiments&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Select the embodiment head that best matches your robot&#39;s configuration for optimal finetuning performance. For detailed information on the observation and action spaces, see &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/getting_started/4_deeper_understanding.md#embodiment-action-head-fine-tuning&#34;&gt;&lt;code&gt;EmbodimentTag&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;4. Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;To conduct an offline evaluation of the model, we provide a script that evaluates the model on a dataset and plots it out. Quick try: &lt;code&gt;python scripts/eval_policy.py --plot --model_path nvidia/GR00T-N1.5-3B&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Or you can run the newly trained model in client-server mode.&lt;/p&gt; &#xA;&lt;p&gt;Run the newly trained model&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/inference_service.py --server \&#xA;    --model-path &amp;lt;MODEL_PATH&amp;gt; \&#xA;    --embodiment-tag new_embodiment&#xA;    --data-config &amp;lt;DATA_CONFIG&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run the offline evaluation script&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/eval_policy.py --plot \&#xA;    --dataset-path &amp;lt;DATASET_PATH&amp;gt; \&#xA;    --embodiment-tag new_embodiment \&#xA;    --data-config &amp;lt;DATA_CONFIG&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You will then see a plot of Ground Truth vs Predicted actions, along with unnormed MSE of the actions. This would give you an indication if the policy is performing well on the dataset.&lt;/p&gt; &#xA;&lt;h2&gt;Jetson Deployment&lt;/h2&gt; &#xA;&lt;p&gt;A detailed guide for deploying GR00T N1.5 on Jetson is available in &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/deployment_scripts/README.md&#34;&gt;&lt;code&gt;deployment_scripts/README.md&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Here&#39;s comparison of E2E performance between PyTorch and TensorRT on Orin&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/media/orin-perf.png&#34; width=&#34;800&#34; alt=&#34;orin-perf&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Model latency measured by &lt;code&gt;trtexec&lt;/code&gt; with batch_size=1.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model Name&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Orin benchmark perf (ms)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Precision&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Action_Head - process_backbone_output&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5.17&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP16&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Action_Head - state_encoder&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.05&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP16&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Action_Head - action_encoder&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.20&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP16&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Action_Head - DiT&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7.77&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP16&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Action_Head - action_decoder&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.04&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP16&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;VLM - ViT&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;11.96&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP16&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;VLM - LLM&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17.25&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP16&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The module latency (e.g., DiT Block) in pipeline is slightly longer than the model latency in benchmark table above because the module (e.g., Action_Head - DiT) latency not only includes the model latency in table above but also accounts for the overhead of data transfer from PyTorch to TRT and returning from TRT to PyTorch.&lt;/p&gt; &#xA;&lt;h1&gt;FAQ&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;Does it work on CUDA ARM Linux?&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Yes, visit &lt;a href=&#34;https://github.com/dusty-nv/jetson-containers/tree/master/packages/robots/Isaac-GR00T&#34;&gt;jetson-containers&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;I have my own data, what should I do next for finetuning?&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This repo assumes that your data is already organized according to the LeRobot format.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;What is Modality Config? Embodiment Tag? and Transform Config?&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Embodiment Tag: Defines the robot embodiment used, non-pretrained embodiment tags are all considered as &lt;code&gt;new_embodiment&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Modality Config: Defines the modalities used in the dataset (e.g. video, state, action)&lt;/li&gt; &#xA; &lt;li&gt;Transform Config: Defines the Data Transforms applied to the data during dataloading.&lt;/li&gt; &#xA; &lt;li&gt;For more details, see &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/getting_started/4_deeper_understanding.md&#34;&gt;&lt;code&gt;getting_started/4_deeper_understanding.md&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;What is the inference speed for Gr00tPolicy?&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Below are benchmark results based on a single H100 GPU. Performance will be slightly slower on consumer GPUs like RTX 4090 for inference (single sample processing):&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Module&lt;/th&gt; &#xA;   &lt;th&gt;Inference Speed&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VLM Backbone&lt;/td&gt; &#xA;   &lt;td&gt;23.18 ms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Action Head with 4 diffusion steps&lt;/td&gt; &#xA;   &lt;td&gt;4 x 6.18 ms = 24.7 ms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Full Model&lt;/td&gt; &#xA;   &lt;td&gt;47.88 ms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;We noticed that 4 denoising steps are sufficient during inference.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;How to train with multiple datasets?&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can train with multiple datasets by providing a list of dataset paths to the &lt;code&gt;dataset_path&lt;/code&gt; argument.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/gr00t_finetune.py --dataset-path &amp;lt;DATASET1&amp;gt; &amp;lt;DATASET2&amp;gt; --num-gpus 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, the &lt;code&gt;gr00t_finetune.py&lt;/code&gt; imposes equal weights to all datasets, with &lt;code&gt;balance_dataset_weights&lt;/code&gt; and &lt;code&gt;balance_trajectory_weights&lt;/code&gt; set to &lt;code&gt;True&lt;/code&gt;. For more details, see the &lt;code&gt;LeRobotMixtureDataset&lt;/code&gt; class definition in &lt;code&gt;gr00t/data/dataset.py&lt;/code&gt;. Users can also use the &lt;code&gt;LeRobotMixtureDataset&lt;/code&gt; class directly to train with multiple datasets with different embodiments, transforms, and sampling weights.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Is LoRA finetuning supported?&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Yes, you can use LoRA finetuning to finetune the model. This can be enabled by indicating &lt;code&gt;--lora_rank 64 --lora_alpha 128&lt;/code&gt; in the finetuning script. However, we recommend using the full model finetuning for better performance.&lt;/p&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;For more details, see &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION &amp;amp; AFFILIATES. All rights reserved.&#xA;# SPDX-License-Identifier: Apache-2.0&#xA;#&#xA;# Licensed under the Apache License, Version 2.0 (the &#34;License&#34;);&#xA;# you may not use this file except in compliance with the License.&#xA;# You may obtain a copy of the License at&#xA;#&#xA;# http://www.apache.org/licenses/LICENSE-2.0&#xA;#&#xA;# Unless required by applicable law or agreed to in writing, software&#xA;# distributed under the License is distributed on an &#34;AS IS&#34; BASIS,&#xA;# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&#xA;# See the License for the specific language governing permissions and&#xA;# limitations under the License.&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>QwenLM/Qwen2.5-VL</title>
    <updated>2025-06-22T01:42:00Z</updated>
    <id>tag:github.com,2025-06-22:/QwenLM/Qwen2.5-VL</id>
    <link href="https://github.com/QwenLM/Qwen2.5-VL" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Qwen2.5-VL is the multimodal large language model series developed by Qwen team, Alibaba Cloud.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Qwen2.5-VL&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-VL/qwen2.5vl_logo.png&#34; width=&#34;400&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; üíú &lt;a href=&#34;https://chat.qwenlm.ai/&#34;&gt;&lt;b&gt;Qwen Chat&lt;/b&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü§ó &lt;a href=&#34;https://huggingface.co/collections/Qwen/qwen25-vl-6795ffac22b334a837c0f9a5&#34;&gt;Hugging Face&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü§ñ &lt;a href=&#34;https://modelscope.cn/organization/qwen&#34;&gt;ModelScope&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üìë &lt;a href=&#34;https://qwenlm.github.io/blog/qwen2.5-vl/&#34;&gt;Blog&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üìö &lt;a href=&#34;https://github.com/QwenLM/Qwen2.5-VL/tree/main/cookbooks&#34;&gt;Cookbooks&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üìë &lt;a href=&#34;https://arxiv.org/abs/2502.13923&#34;&gt;Paper&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;br&gt; üñ•Ô∏è &lt;a href=&#34;https://huggingface.co/spaces/Qwen/Qwen2.5-VL-72B-Instruct&#34;&gt;Demo&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üí¨ &lt;a href=&#34;https://github.com/QwenLM/Qwen/raw/main/assets/wechat.png&#34;&gt;WeChat (ÂæÆ‰ø°)&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü´® &lt;a href=&#34;https://discord.gg/CV4E9rpNSD&#34;&gt;Discord&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üìë &lt;a href=&#34;https://help.aliyun.com/zh/model-studio/developer-reference/qwen-vl-api&#34;&gt;API&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üñ•Ô∏è &lt;a href=&#34;https://gallery.pai-ml.com/#/preview/deepLearning/cv/qwen2.5-vl&#34;&gt;PAI-DSW&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;In the past five months since Qwen2-VL&#39;s release, numerous developers have built new models on the Qwen2-VL vision-language models, providing us with valuable feedback. During this period, we focused on building more useful vision-language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5-VL.&lt;/p&gt; &#xA;&lt;h4&gt;Key Enhancements:&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Powerful Document Parsing Capabilities&lt;/strong&gt;: Upgrade text recognition to omnidocument parsing, excelling in processing multi-scene, multilingual, and various built-in (handwriting, tables, charts, chemical formulas, and music sheets) documents.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Precise Object Grounding Across Formats&lt;/strong&gt;: Unlock improved accuracy in detecting, pointing, and counting objects, accommodating absolute coordinate and JSON formats for advanced spatial reasoning.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Ultra-long Video Understanding and Fine-grained Video Grounding&lt;/strong&gt;: Extend native dynamic resolution to the temporal dimension, enhancing the ability to understand videos lasting hours while extracting event segments in seconds.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Enhanced Agent Functionality for Computer and Mobile Devices&lt;/strong&gt;: Leverage advanced grounding, reasoning, and decision-making abilities, boosting the model with superior agent functionality on smartphones and computers.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Model Architecture Updates:&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Dynamic Resolution and Frame Rate Training for Video Understanding&lt;/strong&gt;:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We extend dynamic resolution to the temporal dimension by adopting dynamic FPS sampling, enabling the model to comprehend videos at various sampling rates. Accordingly, we update mRoPE in the time dimension with IDs and absolute time alignment, enabling the model to learn temporal sequence and speed, and ultimately acquire the ability to pinpoint specific moments.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-VL/qwen2.5vl_arc.jpeg&#34; width=&#34;80%&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Streamlined and Efficient Vision Encoder&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We enhance both training and inference speeds by strategically implementing window attention into the ViT. The ViT architecture is further optimized with SwiGLU and RMSNorm, aligning it with the structure of the Qwen2.5 LLM.&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2025.04.08: We provide the &lt;a href=&#34;https://github.com/QwenLM/Qwen2.5-VL/tree/main/qwen-vl-finetune&#34;&gt;code&lt;/a&gt; for fine-tuning Qwen2-VL and Qwen2.5-VL.&lt;/li&gt; &#xA; &lt;li&gt;2025.03.25: We have released the &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct&#34;&gt;Qwen2.5-VL-32B&lt;/a&gt;. It is smarter and its responses align more closely with human preferences. For more details, please check our &lt;a href=&#34;https://qwenlm.github.io/blog/qwen2.5-vl-32b/&#34;&gt;blog&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;2025.02.20: we have released the &lt;a href=&#34;https://arxiv.org/abs/2502.13923&#34;&gt;Qwen2.5-VL Technical Report&lt;/a&gt;. Alongside the report, we have also released AWQ-quantized models for Qwen2.5-VL in three different sizes: &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct-AWQ&#34;&gt;3B&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct-AWQ&#34;&gt;7B&lt;/a&gt; , and &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct-AWQ&#34;&gt;72B&lt;/a&gt; parameters.&lt;/li&gt; &#xA; &lt;li&gt;2025.01.28: We have released the &lt;a href=&#34;https://huggingface.co/Qwen&#34;&gt;Qwen2.5-VL series&lt;/a&gt;. For more details, please check our &lt;a href=&#34;https://qwenlm.github.io/blog/qwen2.5-vl/&#34;&gt;blog&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;2024.12.25: We have released the &lt;a href=&#34;https://huggingface.co/Qwen/QVQ-72B-Preview&#34;&gt;QvQ-72B-Preview&lt;/a&gt;. QvQ-72B-Preview is an experimental research model, focusing on enhancing visual reasoning capabilities. For more details, please check our &lt;a href=&#34;https://qwenlm.github.io/blog/qvq-72b-preview/&#34;&gt;blog&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;2024.09.19: The instruction-tuned &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct&#34;&gt;Qwen2-VL-72B model&lt;/a&gt; and its quantized version [&lt;a href=&#34;https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct-AWQ&#34;&gt;AWQ&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct-GPTQ-Int4&#34;&gt;GPTQ-Int4&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct-GPTQ-Int8&#34;&gt;GPTQ-Int8&lt;/a&gt;] are now available. We have also released the &lt;a href=&#34;https://arxiv.org/pdf/2409.12191&#34;&gt;Qwen2-VL paper&lt;/a&gt; simultaneously.&lt;/li&gt; &#xA; &lt;li&gt;2024.08.30: We have released the &lt;a href=&#34;https://huggingface.co/collections/Qwen/qwen2-vl-66cee7455501d7126940800d&#34;&gt;Qwen2-VL series&lt;/a&gt;. The 2B and 7B models are now available, and the 72B model for open source is coming soon. For more details, please check our &lt;a href=&#34;https://qwenlm.github.io/blog/qwen2-vl/&#34;&gt;blog&lt;/a&gt;!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Dataset&lt;/th&gt; &#xA;   &lt;th&gt;Qwen2.5-VL-3B&lt;br&gt;&lt;sup&gt;(&lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct&#34;&gt;ü§ó&lt;/a&gt;&lt;a href=&#34;https://modelscope.cn/models/qwen/Qwen2.5-VL-3B-Instruct&#34;&gt;ü§ñ&lt;/a&gt;)&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Qwen2.5-VL-7B&lt;br&gt;&lt;sup&gt;(&lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct&#34;&gt;ü§ó&lt;/a&gt;&lt;a href=&#34;https://modelscope.cn/models/qwen/Qwen2.5-VL-7B-Instruct&#34;&gt;ü§ñ&lt;/a&gt;)&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Qwen2.5-VL-32B&lt;br&gt;&lt;sup&gt;(&lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct&#34;&gt;ü§ó&lt;/a&gt;&lt;a href=&#34;https://modelscope.cn/models/qwen/Qwen2.5-VL-32B-Instruct&#34;&gt;ü§ñ&lt;/a&gt;)&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Qwen2.5-VL-72B&lt;br&gt;&lt;sup&gt;(&lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct&#34;&gt;ü§ó&lt;/a&gt;&lt;a href=&#34;https://modelscope.cn/models/qwen/Qwen2.5-VL-72B-Instruct&#34;&gt;ü§ñ&lt;/a&gt;)&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Gemini-2 Flash&lt;/th&gt; &#xA;   &lt;th&gt;GPT-4o&lt;/th&gt; &#xA;   &lt;th&gt;Claude3.5 Sonnet&lt;/th&gt; &#xA;   &lt;th&gt;Qwen2-VL 72B&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MMMU&lt;/td&gt; &#xA;   &lt;td&gt;53.1&lt;/td&gt; &#xA;   &lt;td&gt;58.6&lt;/td&gt; &#xA;   &lt;td&gt;70.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;70.2&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;70.7&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;70.3&lt;/td&gt; &#xA;   &lt;td&gt;70.4&lt;/td&gt; &#xA;   &lt;td&gt;64.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MMMU Pro&lt;/td&gt; &#xA;   &lt;td&gt;31.6&lt;/td&gt; &#xA;   &lt;td&gt;38.3&lt;/td&gt; &#xA;   &lt;td&gt;49.5&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;51.1&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;57&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;54.5&lt;/td&gt; &#xA;   &lt;td&gt;54.7&lt;/td&gt; &#xA;   &lt;td&gt;46.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DocVQA&lt;/td&gt; &#xA;   &lt;td&gt;93.9&lt;/td&gt; &#xA;   &lt;td&gt;95.7&lt;/td&gt; &#xA;   &lt;td&gt;94.8&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;96.4&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;92.1&lt;/td&gt; &#xA;   &lt;td&gt;91.1&lt;/td&gt; &#xA;   &lt;td&gt;95.2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;96.5&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;InfoVQA&lt;/td&gt; &#xA;   &lt;td&gt;77.1&lt;/td&gt; &#xA;   &lt;td&gt;82.6&lt;/td&gt; &#xA;   &lt;td&gt;83.4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;87.3&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;77.8&lt;/td&gt; &#xA;   &lt;td&gt;80.7&lt;/td&gt; &#xA;   &lt;td&gt;74.3&lt;/td&gt; &#xA;   &lt;td&gt;84.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CC-OCR&lt;/td&gt; &#xA;   &lt;td&gt;74.5&lt;/td&gt; &#xA;   &lt;td&gt;77.8&lt;/td&gt; &#xA;   &lt;td&gt;77.1&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;79.8&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;73.0&lt;/td&gt; &#xA;   &lt;td&gt;66.6&lt;/td&gt; &#xA;   &lt;td&gt;62.7&lt;/td&gt; &#xA;   &lt;td&gt;68.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OCRBenchV2&lt;/td&gt; &#xA;   &lt;td&gt;54.3/52.1&lt;/td&gt; &#xA;   &lt;td&gt;56.3/57.2&lt;/td&gt; &#xA;   &lt;td&gt;57.2/59.1&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;61.5/63.7&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;46.5/32.3&lt;/td&gt; &#xA;   &lt;td&gt;45.2/39.6&lt;/td&gt; &#xA;   &lt;td&gt;47.8/46.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MegaBench&lt;/td&gt; &#xA;   &lt;td&gt;28.9&lt;/td&gt; &#xA;   &lt;td&gt;36.8&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;51.3&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;55.2&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;54.2&lt;/td&gt; &#xA;   &lt;td&gt;52.1&lt;/td&gt; &#xA;   &lt;td&gt;46.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MMStar&lt;/td&gt; &#xA;   &lt;td&gt;55.8&lt;/td&gt; &#xA;   &lt;td&gt;63.9&lt;/td&gt; &#xA;   &lt;td&gt;69.5&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;70.8&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;69.4&lt;/td&gt; &#xA;   &lt;td&gt;64.7&lt;/td&gt; &#xA;   &lt;td&gt;65.1&lt;/td&gt; &#xA;   &lt;td&gt;68.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MMBench1.1&lt;/td&gt; &#xA;   &lt;td&gt;81.5&lt;/td&gt; &#xA;   &lt;td&gt;84.3&lt;/td&gt; &#xA;   &lt;td&gt;84.6&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;88.0&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;83.0&lt;/td&gt; &#xA;   &lt;td&gt;82.1&lt;/td&gt; &#xA;   &lt;td&gt;83.4&lt;/td&gt; &#xA;   &lt;td&gt;86.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MathVista&lt;/td&gt; &#xA;   &lt;td&gt;62.3&lt;/td&gt; &#xA;   &lt;td&gt;68.2&lt;/td&gt; &#xA;   &lt;td&gt;74.7&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;74.8&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;73.1&lt;/td&gt; &#xA;   &lt;td&gt;63.8&lt;/td&gt; &#xA;   &lt;td&gt;65.4&lt;/td&gt; &#xA;   &lt;td&gt;70.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MathVision&lt;/td&gt; &#xA;   &lt;td&gt;21.2&lt;/td&gt; &#xA;   &lt;td&gt;25.1&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;38.4&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;38.1&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;41.3&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;30.4&lt;/td&gt; &#xA;   &lt;td&gt;38.3&lt;/td&gt; &#xA;   &lt;td&gt;25.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VideoMME&lt;/td&gt; &#xA;   &lt;td&gt;61.5/67.6&lt;/td&gt; &#xA;   &lt;td&gt;65.1/71.6&lt;/td&gt; &#xA;   &lt;td&gt;70.5/77.9&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;73.3/79.1&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-/-&lt;/td&gt; &#xA;   &lt;td&gt;71.9/77.2&lt;/td&gt; &#xA;   &lt;td&gt;60/62.9&lt;/td&gt; &#xA;   &lt;td&gt;71.2/77.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MMBench-Video&lt;/td&gt; &#xA;   &lt;td&gt;1.63&lt;/td&gt; &#xA;   &lt;td&gt;1.79&lt;/td&gt; &#xA;   &lt;td&gt;1.93&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;2.02&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;1.68&lt;/td&gt; &#xA;   &lt;td&gt;1.38&lt;/td&gt; &#xA;   &lt;td&gt;1.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LVBench&lt;/td&gt; &#xA;   &lt;td&gt;43.3&lt;/td&gt; &#xA;   &lt;td&gt;45.3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;49.0&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;47.3&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;30.8&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CharadesSTA&lt;/td&gt; &#xA;   &lt;td&gt;38.8&lt;/td&gt; &#xA;   &lt;td&gt;43.6&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;54.2&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;50.9&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;35.7&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AITZ&lt;/td&gt; &#xA;   &lt;td&gt;76.9&lt;/td&gt; &#xA;   &lt;td&gt;81.9&lt;/td&gt; &#xA;   &lt;td&gt;83.1&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;83.2&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;35.3&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Android Control&lt;/td&gt; &#xA;   &lt;td&gt;63.7/90.8&lt;/td&gt; &#xA;   &lt;td&gt;60.1/91.4&lt;/td&gt; &#xA;   &lt;td&gt;69.6/93.3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;67.36/93.7&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;66.4/84.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ScreenSpot&lt;/td&gt; &#xA;   &lt;td&gt;55.5&lt;/td&gt; &#xA;   &lt;td&gt;84.7&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;88.5&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;87.1&lt;/td&gt; &#xA;   &lt;td&gt;84.0&lt;/td&gt; &#xA;   &lt;td&gt;18.1&lt;/td&gt; &#xA;   &lt;td&gt;83.0&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ScreenSpot Pro&lt;/td&gt; &#xA;   &lt;td&gt;23.9&lt;/td&gt; &#xA;   &lt;td&gt;29.0&lt;/td&gt; &#xA;   &lt;td&gt;39.4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;43.6&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;17.1&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AndroidWorld&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;22.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;35&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;34.5(SoM)&lt;/td&gt; &#xA;   &lt;td&gt;27.9&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OSWorld&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;5.92&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;8.83&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;5.03&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;14.9&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;Below, we provide simple examples to show how to use Qwen2.5-VL with ü§ñ ModelScope and ü§ó Transformers.&lt;/p&gt; &#xA;&lt;p&gt;The code of Qwen2.5-VL has been in the latest Hugging face transformers and we advise you to build from source with command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install transformers==4.51.3 accelerate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or you might encounter the following error:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;KeyError: &#39;qwen2_5_vl&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We offer a toolkit to help you handle various types of visual input more conveniently, as if you were using an API. This includes base64, URLs, and interleaved images and videos. You can install it using the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# It&#39;s highly recommended to use `[decord]` feature for faster video loading.&#xA;pip install qwen-vl-utils[decord]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Currently, &lt;code&gt;qwen-vl-utils&lt;/code&gt; supports three video decoding backends: &lt;code&gt;torchvision&lt;/code&gt;, &lt;code&gt;decord&lt;/code&gt;, and &lt;code&gt;torchcodec&lt;/code&gt;. While &lt;code&gt;decord&lt;/code&gt; and &lt;code&gt;torchcodec&lt;/code&gt; generally offer significantly faster decoding speeds compared to &lt;code&gt;torchvision&lt;/code&gt;, we recommend using &lt;code&gt;torchcodec&lt;/code&gt;. This is because &lt;code&gt;decord&lt;/code&gt; has known issues, such as decoding hangs, and its project is no longer actively maintained.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;For &lt;code&gt;decord&lt;/code&gt;, if you are not using Linux, you might not be able to install &lt;code&gt;decord&lt;/code&gt; from PyPI. In that case, you can use &lt;code&gt;pip install qwen-vl-utils&lt;/code&gt; which will fall back to using torchvision for video processing. However, you can still &lt;a href=&#34;https://github.com/dmlc/decord?tab=readme-ov-file#install-from-source&#34;&gt;install decord from source&lt;/a&gt; to get decord used when loading video.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;To use &lt;code&gt;torchcodec&lt;/code&gt; as the backend for video decoding, follow the installation instructions provided in the official &lt;a href=&#34;https://github.com/pytorch/torchcodec/tree/main?tab=readme-ov-file#installing-torchcodec&#34;&gt;torchcodec repository&lt;/a&gt; and install it manually. Note that &lt;code&gt;torchcodec&lt;/code&gt; depends on FFmpeg for decoding functionality.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Cookbooks&lt;/h2&gt; &#xA;&lt;p&gt;We are preparing &lt;a href=&#34;https://github.com/QwenLM/Qwen2.5-VL/tree/main/cookbooks&#34;&gt;cookbooks&lt;/a&gt; for many capabilities, including recognition, localization, document parsing, video understanding, key information extraction, and more. Welcome to learn more!&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Cookbook&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Open&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/QwenLM/Qwen2.5-VL/raw/main/cookbooks/universal_recognition.ipynb&#34;&gt;Universal Recognition&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Not only identify animals, plants, people, and scenic spots but also recognize various objects such as cars and merchandise.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/QwenLM/Qwen2.5-VL/blob/main/cookbooks/universal_recognition.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/QwenLM/Qwen2.5-VL/raw/main/cookbooks/document_parsing.ipynb&#34;&gt;Powerful Document Parsing Capabilities&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The parsing of documents has reached a higher level, including not only text but also layout position information and our Qwen HTML format.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/QwenLM/Qwen2.5-VL/blob/main/cookbooks/document_parsing.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/QwenLM/Qwen2.5-VL/raw/main/cookbooks/spatial_understanding.ipynb&#34;&gt;Precise Object Grounding Across Formats&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Using absolute position coordinates, it supports both boxes and points, allowing for diverse combinations of positioning and labeling tasks.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/QwenLM/Qwen2.5-VL/blob/main/cookbooks/spatial_understanding.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/QwenLM/Qwen2.5-VL/raw/main/cookbooks/ocr.ipynb&#34;&gt;General OCR and Key Information Extraction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Stronger text recognition capabilities in natural scenes and multiple languages, supporting diverse key information extraction needs.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/QwenLM/Qwen2.5-VL/blob/main/cookbooks/ocr.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/QwenLM/Qwen2.5-VL/raw/main/cookbooks/video_understanding.ipynb&#34;&gt;Video Understanding&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Better video OCR, long video understanding, and video grounding.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/QwenLM/Qwen2.5-VL/blob/main/cookbooks/video_understanding.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/QwenLM/Qwen2.5-VL/raw/main/cookbooks/mobile_agent.ipynb&#34;&gt;Mobile Agent&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Locate and think for mobile phone control.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/QwenLM/Qwen2.5-VL/blob/main/cookbooks/mobile_agent.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/QwenLM/Qwen2.5-VL/raw/main/cookbooks/computer_use.ipynb&#34;&gt;Computer-Use Agent&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Locate and think for controlling computers and Web.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/QwenLM/Qwen2.5-VL/blob/main/cookbooks/computer_use.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Using ü§ó Transformers to Chat&lt;/h3&gt; &#xA;&lt;p&gt;Here we show a code snippet to show you how to use the chat model with &lt;code&gt;transformers&lt;/code&gt; and &lt;code&gt;qwen_vl_utils&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor&#xA;from qwen_vl_utils import process_vision_info&#xA;&#xA;# default: Load the model on the available device(s)&#xA;model = Qwen2_5_VLForConditionalGeneration.from_pretrained(&#xA;    &#34;Qwen/Qwen2.5-VL-7B-Instruct&#34;, torch_dtype=&#34;auto&#34;, device_map=&#34;auto&#34;&#xA;)&#xA;&#xA;# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.&#xA;# model = Qwen2_5_VLForConditionalGeneration.from_pretrained(&#xA;#     &#34;Qwen/Qwen2.5-VL-7B-Instruct&#34;,&#xA;#     torch_dtype=torch.bfloat16,&#xA;#     attn_implementation=&#34;flash_attention_2&#34;,&#xA;#     device_map=&#34;auto&#34;,&#xA;# )&#xA;&#xA;# default processor&#xA;processor = AutoProcessor.from_pretrained(&#34;Qwen/Qwen2.5-VL-7B-Instruct&#34;)&#xA;&#xA;# The default range for the number of visual tokens per image in the model is 4-16384.&#xA;# You can set min_pixels and max_pixels according to your needs, such as a token range of 256-1280, to balance performance and cost.&#xA;# min_pixels = 256*28*28&#xA;# max_pixels = 1280*28*28&#xA;# processor = AutoProcessor.from_pretrained(&#34;Qwen/Qwen2.5-VL-7B-Instruct&#34;, min_pixels=min_pixels, max_pixels=max_pixels)&#xA;&#xA;messages = [&#xA;    {&#xA;        &#34;role&#34;: &#34;user&#34;,&#xA;        &#34;content&#34;: [&#xA;            {&#xA;                &#34;type&#34;: &#34;image&#34;,&#xA;                &#34;image&#34;: &#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg&#34;,&#xA;            },&#xA;            {&#34;type&#34;: &#34;text&#34;, &#34;text&#34;: &#34;Describe this image.&#34;},&#xA;        ],&#xA;    }&#xA;]&#xA;&#xA;# Preparation for inference&#xA;text = processor.apply_chat_template(&#xA;    messages, tokenize=False, add_generation_prompt=True&#xA;)&#xA;image_inputs, video_inputs = process_vision_info(messages)&#xA;inputs = processor(&#xA;    text=[text],&#xA;    images=image_inputs,&#xA;    videos=video_inputs,&#xA;    padding=True,&#xA;    return_tensors=&#34;pt&#34;,&#xA;)&#xA;inputs = inputs.to(model.device)&#xA;&#xA;# Inference: Generation of the output&#xA;generated_ids = model.generate(**inputs, max_new_tokens=128)&#xA;generated_ids_trimmed = [&#xA;    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)&#xA;]&#xA;output_text = processor.batch_decode(&#xA;    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False&#xA;)&#xA;print(output_text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Minimum VRAM requirements&lt;/summary&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Precision&lt;/th&gt; &#xA;    &lt;th&gt;Qwen2.5-VL-3B&lt;/th&gt; &#xA;    &lt;th&gt;Qwen2.5-VL-7B&lt;/th&gt; &#xA;    &lt;th&gt;Qwen2.5-VL-72B&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;FP32&lt;/td&gt; &#xA;    &lt;td&gt;11.5 GB&lt;/td&gt; &#xA;    &lt;td&gt;26.34 GB&lt;/td&gt; &#xA;    &lt;td&gt;266.21 GB&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;BF16&lt;/td&gt; &#xA;    &lt;td&gt;5.75 GB&lt;/td&gt; &#xA;    &lt;td&gt;13.17 GB&lt;/td&gt; &#xA;    &lt;td&gt;133.11 GB&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;INT8&lt;/td&gt; &#xA;    &lt;td&gt;2.87 GB&lt;/td&gt; &#xA;    &lt;td&gt;6.59 GB&lt;/td&gt; &#xA;    &lt;td&gt;66.5 GB&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;INT4&lt;/td&gt; &#xA;    &lt;td&gt;1.44 GB&lt;/td&gt; &#xA;    &lt;td&gt;3.29 GB&lt;/td&gt; &#xA;    &lt;td&gt;33.28 GB&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;p&gt;Note: The table above presents the theoretical minimum video memory requirements for inference with &lt;code&gt;transformers&lt;/code&gt;; however, in practice, the actual memory usage is typically at least 1.2 times higher. For more information, see the linked resource &lt;a href=&#34;https://huggingface.co/docs/accelerate/main/en/usage_guides/model_size_estimator&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Multi image inference&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Messages containing multiple images and a text query&#xA;messages = [&#xA;    {&#xA;        &#34;role&#34;: &#34;user&#34;,&#xA;        &#34;content&#34;: [&#xA;            {&#34;type&#34;: &#34;image&#34;, &#34;image&#34;: &#34;file:///path/to/image1.jpg&#34;},&#xA;            {&#34;type&#34;: &#34;image&#34;, &#34;image&#34;: &#34;file:///path/to/image2.jpg&#34;},&#xA;            {&#34;type&#34;: &#34;text&#34;, &#34;text&#34;: &#34;Identify the similarities between these images.&#34;},&#xA;        ],&#xA;    }&#xA;]&#xA;&#xA;# Preparation for inference&#xA;text = processor.apply_chat_template(&#xA;    messages, tokenize=False, add_generation_prompt=True&#xA;)&#xA;image_inputs, video_inputs = process_vision_info(messages)&#xA;inputs = processor(&#xA;    text=[text],&#xA;    images=image_inputs,&#xA;    videos=video_inputs,&#xA;    padding=True,&#xA;    return_tensors=&#34;pt&#34;,&#xA;)&#xA;inputs = inputs.to(&#34;cuda&#34;)&#xA;&#xA;# Inference&#xA;generated_ids = model.generate(**inputs, max_new_tokens=128)&#xA;generated_ids_trimmed = [&#xA;    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)&#xA;]&#xA;output_text = processor.batch_decode(&#xA;    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False&#xA;)&#xA;print(output_text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Video inference&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Messages containing a images list as a video and a text query&#xA;messages = [&#xA;    {&#xA;        &#34;role&#34;: &#34;user&#34;,&#xA;        &#34;content&#34;: [&#xA;            {&#xA;                &#34;type&#34;: &#34;video&#34;,&#xA;                &#34;video&#34;: [&#xA;                    &#34;file:///path/to/frame1.jpg&#34;,&#xA;                    &#34;file:///path/to/frame2.jpg&#34;,&#xA;                    &#34;file:///path/to/frame3.jpg&#34;,&#xA;                    &#34;file:///path/to/frame4.jpg&#34;,&#xA;                ],&#xA;            },&#xA;            {&#34;type&#34;: &#34;text&#34;, &#34;text&#34;: &#34;Describe this video.&#34;},&#xA;        ],&#xA;    }&#xA;]&#xA;&#xA;# Messages containing a local video path and a text query&#xA;messages = [&#xA;    {&#xA;        &#34;role&#34;: &#34;user&#34;,&#xA;        &#34;content&#34;: [&#xA;            {&#xA;                &#34;type&#34;: &#34;video&#34;,&#xA;                &#34;video&#34;: &#34;file:///path/to/video1.mp4&#34;,&#xA;                &#34;max_pixels&#34;: 360 * 420,&#xA;                &#34;fps&#34;: 1.0,&#xA;            },&#xA;            {&#34;type&#34;: &#34;text&#34;, &#34;text&#34;: &#34;Describe this video.&#34;},&#xA;        ],&#xA;    }&#xA;]&#xA;&#xA;# Messages containing a video url and a text query&#xA;messages = [&#xA;    {&#xA;        &#34;role&#34;: &#34;user&#34;,&#xA;        &#34;content&#34;: [&#xA;            {&#xA;                &#34;type&#34;: &#34;video&#34;,&#xA;                &#34;video&#34;: &#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4&#34;,&#xA;                &#34;min_pixels&#34;: 4 * 28 * 28,&#xA;                &#34;max_pixels&#34;: 256 * 28 * 28,&#xA;                &#34;total_pixels&#34;: 20480 * 28 * 28,&#xA;            },&#xA;            {&#34;type&#34;: &#34;text&#34;, &#34;text&#34;: &#34;Describe this video.&#34;},&#xA;        ],&#xA;    }&#xA;]&#xA;&#xA;# Preparation for inference&#xA;text = processor.apply_chat_template(&#xA;    messages, tokenize=False, add_generation_prompt=True&#xA;)&#xA;image_inputs, video_inputs, video_kwargs = process_vision_info(messages, return_video_kwargs=True)&#xA;inputs = processor(&#xA;    text=[text],&#xA;    images=image_inputs,&#xA;    videos=video_inputs,&#xA;    fps=fps,&#xA;    padding=True,&#xA;    return_tensors=&#34;pt&#34;,&#xA;    **video_kwargs,&#xA;)&#xA;inputs = inputs.to(&#34;cuda&#34;)&#xA;&#xA;# Inference&#xA;generated_ids = model.generate(**inputs, max_new_tokens=128)&#xA;generated_ids_trimmed = [&#xA;    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)&#xA;]&#xA;output_text = processor.batch_decode(&#xA;    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False&#xA;)&#xA;print(output_text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h4&gt;Video URL compatibility&lt;/h4&gt; &#xA; &lt;p&gt;Video URL compatibility is primarily determined by the version of the third-party library being used. For more details, refer to the table below. If you prefer not to use the default backend, you can switch it by setting &lt;code&gt;FORCE_QWENVL_VIDEO_READER&lt;/code&gt; to &lt;code&gt;torchvision&lt;/code&gt;, &lt;code&gt;decord&lt;/code&gt;, or &lt;code&gt;torchcodec&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Backend&lt;/th&gt; &#xA;    &lt;th&gt;HTTP&lt;/th&gt; &#xA;    &lt;th&gt;HTTPS&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;torchvision &amp;gt;= 0.19.0&lt;/td&gt; &#xA;    &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;    &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;torchvision &amp;lt; 0.19.0&lt;/td&gt; &#xA;    &lt;td&gt;‚ùå&lt;/td&gt; &#xA;    &lt;td&gt;‚ùå&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;decord&lt;/td&gt; &#xA;    &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;    &lt;td&gt;‚ùå&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;torchcodec&lt;/td&gt; &#xA;    &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;    &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;h4&gt;Configuration for adjusting video resolution&lt;/h4&gt; &#xA; &lt;p&gt;We recommend setting appropriate values for the &lt;code&gt;min_pixels&lt;/code&gt; and &lt;code&gt;max_pixels&lt;/code&gt; parameters based on available GPU memory and the specific application scenario to restrict the resolution of individual frames in the video. Alternatively, you can use the &lt;code&gt;total_pixels&lt;/code&gt; parameter to limit the total number of tokens in the video (it is recommended to set this value below 24576 * 28 * 28 to avoid excessively long input sequences). For more details on parameter usage and processing logic, please refer to the &lt;code&gt;fetch_video&lt;/code&gt; function in &lt;code&gt;qwen_vl_utils/vision_process.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Batch inference&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Sample messages for batch inference&#xA;messages1 = [&#xA;    {&#xA;        &#34;role&#34;: &#34;user&#34;,&#xA;        &#34;content&#34;: [&#xA;            {&#34;type&#34;: &#34;image&#34;, &#34;image&#34;: &#34;file:///path/to/image1.jpg&#34;},&#xA;            {&#34;type&#34;: &#34;image&#34;, &#34;image&#34;: &#34;file:///path/to/image2.jpg&#34;},&#xA;            {&#34;type&#34;: &#34;text&#34;, &#34;text&#34;: &#34;What are the common elements in these pictures?&#34;},&#xA;        ],&#xA;    }&#xA;]&#xA;messages2 = [&#xA;    {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: &#34;You are a helpful assistant.&#34;},&#xA;    {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Who are you?&#34;},&#xA;]&#xA;# Combine messages for batch processing&#xA;messages = [messages1, messages2]&#xA;&#xA;# Preparation for batch inference&#xA;texts = [&#xA;    processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)&#xA;    for msg in messages&#xA;]&#xA;image_inputs, video_inputs = process_vision_info(messages)&#xA;inputs = processor(&#xA;    text=texts,&#xA;    images=image_inputs,&#xA;    videos=video_inputs,&#xA;    padding=True,&#xA;    return_tensors=&#34;pt&#34;,&#xA;)&#xA;inputs = inputs.to(&#34;cuda&#34;)&#xA;&#xA;# Batch Inference&#xA;generated_ids = model.generate(**inputs, max_new_tokens=128)&#xA;generated_ids_trimmed = [&#xA;    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)&#xA;]&#xA;output_texts = processor.batch_decode(&#xA;    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False&#xA;)&#xA;print(output_texts)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;ü§ñ ModelScope&lt;/h3&gt; &#xA;&lt;p&gt;We strongly advise users especially those in mainland China to use ModelScope. &lt;code&gt;snapshot_download&lt;/code&gt; can help you solve issues concerning downloading checkpoints.&lt;/p&gt; &#xA;&lt;h3&gt;More Usage Tips&lt;/h3&gt; &#xA;&lt;p&gt;For input images, we support local files, base64, and URLs. For videos, we currently only support local files.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# You can directly insert a local file path, a URL, or a base64-encoded image into the position where you want in the text.&#xA;## Local file path&#xA;messages = [&#xA;    {&#xA;        &#34;role&#34;: &#34;user&#34;,&#xA;        &#34;content&#34;: [&#xA;            {&#34;type&#34;: &#34;image&#34;, &#34;image&#34;: &#34;file:///path/to/your/image.jpg&#34;},&#xA;            {&#34;type&#34;: &#34;text&#34;, &#34;text&#34;: &#34;Describe this image.&#34;},&#xA;        ],&#xA;    }&#xA;]&#xA;## Image URL&#xA;messages = [&#xA;    {&#xA;        &#34;role&#34;: &#34;user&#34;,&#xA;        &#34;content&#34;: [&#xA;            {&#34;type&#34;: &#34;image&#34;, &#34;image&#34;: &#34;http://path/to/your/image.jpg&#34;},&#xA;            {&#34;type&#34;: &#34;text&#34;, &#34;text&#34;: &#34;Describe this image.&#34;},&#xA;        ],&#xA;    }&#xA;]&#xA;## Base64 encoded image&#xA;messages = [&#xA;    {&#xA;        &#34;role&#34;: &#34;user&#34;,&#xA;        &#34;content&#34;: [&#xA;            {&#34;type&#34;: &#34;image&#34;, &#34;image&#34;: &#34;data:image;base64,/9j/...&#34;},&#xA;            {&#34;type&#34;: &#34;text&#34;, &#34;text&#34;: &#34;Describe this image.&#34;},&#xA;        ],&#xA;    }&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Image Resolution for performance boost&lt;/h4&gt; &#xA;&lt;p&gt;The model supports a wide range of resolution inputs. By default, it uses the native resolution for input, but higher resolutions can enhance performance at the cost of more computation. Users can set the minimum and maximum number of pixels to achieve an optimal configuration for their needs, such as a token count range of 256-1280, to balance speed and memory usage.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;min_pixels = 256 * 28 * 28&#xA;max_pixels = 1280 * 28 * 28&#xA;processor = AutoProcessor.from_pretrained(&#xA;    &#34;Qwen/Qwen2.5-VL-7B-Instruct&#34;, min_pixels=min_pixels, max_pixels=max_pixels&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Besides, We provide two methods for fine-grained control over the image size input to the model:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Specify exact dimensions: Directly set &lt;code&gt;resized_height&lt;/code&gt; and &lt;code&gt;resized_width&lt;/code&gt;. These values will be rounded to the nearest multiple of 28.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Define min_pixels and max_pixels: Images will be resized to maintain their aspect ratio within the range of min_pixels and max_pixels.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# resized_height and resized_width&#xA;messages = [&#xA;    {&#xA;        &#34;role&#34;: &#34;user&#34;,&#xA;        &#34;content&#34;: [&#xA;            {&#xA;                &#34;type&#34;: &#34;image&#34;,&#xA;                &#34;image&#34;: &#34;file:///path/to/your/image.jpg&#34;,&#xA;                &#34;resized_height&#34;: 280,&#xA;                &#34;resized_width&#34;: 420,&#xA;            },&#xA;            {&#34;type&#34;: &#34;text&#34;, &#34;text&#34;: &#34;Describe this image.&#34;},&#xA;        ],&#xA;    }&#xA;]&#xA;# min_pixels and max_pixels&#xA;messages = [&#xA;    {&#xA;        &#34;role&#34;: &#34;user&#34;,&#xA;        &#34;content&#34;: [&#xA;            {&#xA;                &#34;type&#34;: &#34;image&#34;,&#xA;                &#34;image&#34;: &#34;file:///path/to/your/image.jpg&#34;,&#xA;                &#34;min_pixels&#34;: 50176,&#xA;                &#34;max_pixels&#34;: 50176,&#xA;            },&#xA;            {&#34;type&#34;: &#34;text&#34;, &#34;text&#34;: &#34;Describe this image.&#34;},&#xA;        ],&#xA;    }&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Add ids for Multiple Image Inputs&lt;/h4&gt; &#xA;&lt;p&gt;By default, images and video content are directly included in the conversation. When handling multiple images, it&#39;s helpful to add labels to the images and videos for better reference. Users can control this behavior with the following settings:&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Add vision ids&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;conversation = [&#xA;    {&#xA;        &#34;role&#34;: &#34;user&#34;,&#xA;        &#34;content&#34;: [{&#34;type&#34;: &#34;image&#34;}, {&#34;type&#34;: &#34;text&#34;, &#34;text&#34;: &#34;Hello, how are you?&#34;}],&#xA;    },&#xA;    {&#xA;        &#34;role&#34;: &#34;assistant&#34;,&#xA;        &#34;content&#34;: &#34;I&#39;m doing well, thank you for asking. How can I assist you today?&#34;,&#xA;    },&#xA;    {&#xA;        &#34;role&#34;: &#34;user&#34;,&#xA;        &#34;content&#34;: [&#xA;            {&#34;type&#34;: &#34;text&#34;, &#34;text&#34;: &#34;Can you describe these images and video?&#34;},&#xA;            {&#34;type&#34;: &#34;image&#34;},&#xA;            {&#34;type&#34;: &#34;image&#34;},&#xA;            {&#34;type&#34;: &#34;video&#34;},&#xA;            {&#34;type&#34;: &#34;text&#34;, &#34;text&#34;: &#34;These are from my vacation.&#34;},&#xA;        ],&#xA;    },&#xA;    {&#xA;        &#34;role&#34;: &#34;assistant&#34;,&#xA;        &#34;content&#34;: &#34;I&#39;d be happy to describe the images and video for you. Could you please provide more context about your vacation?&#34;,&#xA;    },&#xA;    {&#xA;        &#34;role&#34;: &#34;user&#34;,&#xA;        &#34;content&#34;: &#34;It was a trip to the mountains. Can you see the details in the images and video?&#34;,&#xA;    },&#xA;]&#xA;&#xA;# default:&#xA;prompt_without_id = processor.apply_chat_template(&#xA;    conversation, add_generation_prompt=True&#xA;)&#xA;# Excepted output: &#39;&amp;lt;|im_start|&amp;gt;system\nYou are a helpful assistant.&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\n&amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;Hello, how are you?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\nI&#39;m doing well, thank you for asking. How can I assist you today?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\nCan you describe these images and video?&amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;&amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;&amp;lt;|vision_start|&amp;gt;&amp;lt;|video_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;These are from my vacation.&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\nI&#39;d be happy to describe the images and video for you. Could you please provide more context about your vacation?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\nIt was a trip to the mountains. Can you see the details in the images and video?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\n&#39;&#xA;&#xA;&#xA;# add ids&#xA;prompt_with_id = processor.apply_chat_template(&#xA;    conversation, add_generation_prompt=True, add_vision_id=True&#xA;)&#xA;# Excepted output: &#39;&amp;lt;|im_start|&amp;gt;system\nYou are a helpful assistant.&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\nPicture 1: &amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;Hello, how are you?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\nI&#39;m doing well, thank you for asking. How can I assist you today?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\nCan you describe these images and video?Picture 2: &amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;Picture 3: &amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;Video 1: &amp;lt;|vision_start|&amp;gt;&amp;lt;|video_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;These are from my vacation.&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\nI&#39;d be happy to describe the images and video for you. Could you please provide more context about your vacation?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\nIt was a trip to the mountains. Can you see the details in the images and video?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\n&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;Flash-Attention 2 to speed up generation&lt;/h4&gt; &#xA;&lt;p&gt;First, make sure to install the latest version of Flash Attention 2:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -U flash-attn --no-build-isolation&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Also, you should have a hardware that is compatible with Flash-Attention 2. Read more about it in the official documentation of the &lt;a href=&#34;https://github.com/Dao-AILab/flash-attention&#34;&gt;flash attention repository&lt;/a&gt;. FlashAttention-2 can only be used when a model is loaded in &lt;code&gt;torch.float16&lt;/code&gt; or &lt;code&gt;torch.bfloat16&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To load and run a model using Flash Attention-2, simply add &lt;code&gt;attn_implementation=&#34;flash_attention_2&#34;&lt;/code&gt; when loading the model as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import Qwen2_5_VLForConditionalGeneration&#xA;&#xA;model = Qwen2_5_VLForConditionalGeneration.from_pretrained(&#xA;    &#34;Qwen/Qwen2.5-VL-7B-Instruct&#34;, &#xA;    torch_dtype=torch.bfloat16, &#xA;    attn_implementation=&#34;flash_attention_2&#34;,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Processing Long Texts&lt;/h3&gt; &#xA;&lt;p&gt;The current &lt;code&gt;config.json&lt;/code&gt; is set for context length up to 32,768 tokens. To handle extensive inputs exceeding 32,768 tokens, we utilize &lt;a href=&#34;https://arxiv.org/abs/2309.00071&#34;&gt;YaRN&lt;/a&gt;, a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.&lt;/p&gt; &#xA;&lt;p&gt;For supported frameworks, you could add the following to &lt;code&gt;config.json&lt;/code&gt; to enable YaRN:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;{&#xA;&#x9;...,&#xA;    &#34;type&#34;: &#34;yarn&#34;,&#xA;    &#34;mrope_section&#34;: [&#xA;        16,&#xA;        24,&#xA;        24&#xA;    ],&#xA;    &#34;factor&#34;: 4,&#xA;    &#34;original_max_position_embeddings&#34;: 32768&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;However, it should be noted that this method has a significant impact on the performance of temporal and spatial localization tasks, and is therefore not recommended for use.&lt;/p&gt; &#xA;&lt;p&gt;At the same time, for long video inputs, since MRoPE itself is more economical with ids, the max_position_embeddings can be directly modified to a larger value, such as 64k.&lt;/p&gt; &#xA;&lt;h3&gt;Try Qwen2.5-VL-72B with API!&lt;/h3&gt; &#xA;&lt;p&gt;To explore Qwen2.5-VL-72B, a more fascinating multimodal model, we encourage you to test our cutting-edge API service. Let&#39;s start the exciting journey right now!&lt;/p&gt; &#xA;&lt;h4&gt;Installation&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install dashscope&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Examples&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import dashscope&#xA;&#xA;&#xA;dashscope.api_key = &#34;your_api_key&#34;&#xA;&#xA;messages = [{&#xA;    &#39;role&#39;: &#39;user&#39;,&#xA;    &#39;content&#39;: [&#xA;        {&#xA;            &#39;image&#39;: &#34;https://dashscope.oss-cn-beijing.aliyuncs.com/images/dog_and_girl.jpeg&#34;&#xA;        },&#xA;        {&#xA;            &#39;text&#39;: &#39;What are in the image?&#39;&#xA;        },&#xA;    ]&#xA;}]&#xA;&#xA;response = dashscope.MultiModalConversation.call(model=&#39;qwen2.5-vl-72b-instruct&#39;, messages=messages)&#xA;print(response)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more usage, please refer to the tutorial at &lt;a href=&#34;https://help.aliyun.com/zh/model-studio/developer-reference/qwen-vl-api&#34;&gt;aliyun&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;h3&gt;Web UI Example&lt;/h3&gt; &#xA;&lt;p&gt;In this section, we provide instructions for users to build a web-based user interface (UI) demo. This UI demo allows users to interact with a predefined model or application through a web browser. Follow the steps below to get started.&lt;/p&gt; &#xA;&lt;h4&gt;Installation&lt;/h4&gt; &#xA;&lt;p&gt;Before you begin, ensure that you have the required dependencies installed on your system. You can install them by running the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements_web_demo.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Running the Demo with FlashAttention-2&lt;/h4&gt; &#xA;&lt;p&gt;Once the required packages are installed, you can launch the web demo using the following command. This command will start a web server and provide you with a link to access the UI in your web browser.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Recommended&lt;/strong&gt;: For enhanced performance and efficiency, especially in multi-image and video processing scenarios, we strongly recommend using &lt;a href=&#34;https://github.com/Dao-AILab/flash-attention&#34;&gt;FlashAttention-2&lt;/a&gt;. FlashAttention-2 provides significant improvements in memory usage and speed, making it ideal for handling large-scale models and data processing.&lt;/p&gt; &#xA;&lt;p&gt;To enable FlashAttention-2, use the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python web_demo_mm.py --flash-attn2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will load the model with FlashAttention-2 enabled.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Default Usage&lt;/strong&gt;: If you prefer to run the demo without FlashAttention-2 or if you do not specify the &lt;code&gt;--flash-attn2&lt;/code&gt; option, the demo will load the model using the standard attention implementation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python web_demo_mm.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After running the command, you‚Äôll see a link generated in the terminal similar to this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Running on local: http://127.0.0.1:7860/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Copy this link and paste it into your browser to access the web UI, where you can interact with the model by inputting text, uploading images, or using any other provided functionalities.&lt;/p&gt; &#xA;&lt;h5&gt;Running the Streaming Video Chat Demo&lt;/h5&gt; &#xA;&lt;p&gt;An experimental streaming video chat demo is also available in the &lt;code&gt;web_demo_streaming&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;p&gt;To run the streaming video chat demo, use the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd web_demo_streaming/&#xA;python app.py --flash-attn2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you prefer to run the demo without FlashAttention-2, use the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd web_demo_streaming/&#xA;python app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This demo supports webcam/screen capture as its video input source. To support screen capture video input, we use code snippet from the following hugginface space: &lt;a href=&#34;https://huggingface.co/spaces/gstaff/gradio-screen-recorder/tree/main&#34;&gt;gstaff/gradio-screen-recorder&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Deployment&lt;/h2&gt; &#xA;&lt;p&gt;We recommend using vLLM for fast Qwen2.5-VL deployment and inference. You need to install &lt;code&gt;vllm&amp;gt;0.7.2&lt;/code&gt; to enable Qwen2.5-VL support. You can also use our &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen2.5-VL/main/#-docker&#34;&gt;official docker image&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can also check &lt;a href=&#34;https://docs.vllm.ai/en/latest/serving/multimodal_inputs.html&#34;&gt;vLLM official documentation&lt;/a&gt; for more details about online serving and offline inference.&lt;/p&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install git+https://github.com/huggingface/transformers@f3f6c86582611976e72be054675e2bf0abb5f775&#xA;pip install accelerate&#xA;pip install qwen-vl-utils&#xA;pip install &#39;vllm&amp;gt;0.7.2&#39;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Start an OpenAI API Service&lt;/h3&gt; &#xA;&lt;p&gt;Run the command below to start an OpenAI-compatible API service:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;vllm serve Qwen/Qwen2.5-VL-7B-Instruct --port 8000 --host 0.0.0.0 --dtype bfloat16 --limit-mm-per-prompt image=5,video=5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you can use the chat API as below (via curl or Python API):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl http://localhost:8000/v1/chat/completions \&#xA;    -H &#34;Content-Type: application/json&#34; \&#xA;    -d &#39;{&#xA;    &#34;model&#34;: &#34;Qwen/Qwen2.5-VL-7B-Instruct&#34;,&#xA;    &#34;messages&#34;: [&#xA;    {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: &#34;You are a helpful assistant.&#34;},&#xA;    {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: [&#xA;        {&#34;type&#34;: &#34;image_url&#34;, &#34;image_url&#34;: {&#34;url&#34;: &#34;https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png&#34;}},&#xA;        {&#34;type&#34;: &#34;text&#34;, &#34;text&#34;: &#34;What is the text in the illustrate?&#34;}&#xA;    ]}&#xA;    ]&#xA;    }&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from openai import OpenAI&#xA;&#xA;# Set OpenAI&#39;s API key and API base to use vLLM&#39;s API server.&#xA;openai_api_key = &#34;EMPTY&#34;&#xA;openai_api_base = &#34;http://localhost:8000/v1&#34;&#xA;&#xA;client = OpenAI(&#xA;    api_key=openai_api_key,&#xA;    base_url=openai_api_base,&#xA;)&#xA;&#xA;chat_response = client.chat.completions.create(&#xA;    model=&#34;Qwen/Qwen2.5-VL-7B-Instruct&#34;,&#xA;    messages=[&#xA;        {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: &#34;You are a helpful assistant.&#34;},&#xA;        {&#xA;            &#34;role&#34;: &#34;user&#34;,&#xA;            &#34;content&#34;: [&#xA;                {&#xA;                    &#34;type&#34;: &#34;image_url&#34;,&#xA;                    &#34;image_url&#34;: {&#xA;                        &#34;url&#34;: &#34;https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png&#34;&#xA;                    },&#xA;                },&#xA;                {&#34;type&#34;: &#34;text&#34;, &#34;text&#34;: &#34;What is the text in the illustrate?&#34;},&#xA;            ],&#xA;        },&#xA;    ],&#xA;)&#xA;print(&#34;Chat response:&#34;, chat_response)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also upload base64-encoded local images (see &lt;a href=&#34;https://platform.openai.com/docs/guides/vision/uploading-base-64-encoded-images&#34;&gt;OpenAI API protocol document&lt;/a&gt; for more details):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import base64&#xA;from openai import OpenAI&#xA;# Set OpenAI&#39;s API key and API base to use vLLM&#39;s API server.&#xA;openai_api_key = &#34;EMPTY&#34;&#xA;openai_api_base = &#34;http://localhost:8000/v1&#34;&#xA;client = OpenAI(&#xA;    api_key=openai_api_key,&#xA;    base_url=openai_api_base,&#xA;)&#xA;image_path = &#34;/path/to/local/image.png&#34;&#xA;with open(image_path, &#34;rb&#34;) as f:&#xA;    encoded_image = base64.b64encode(f.read())&#xA;encoded_image_text = encoded_image.decode(&#34;utf-8&#34;)&#xA;base64_qwen = f&#34;data:image;base64,{encoded_image_text}&#34;&#xA;chat_response = client.chat.completions.create(&#xA;    model=&#34;Qwen/Qwen2.5-VL-7B-Instruct&#34;,&#xA;    messages=[&#xA;        {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: &#34;You are a helpful assistant.&#34;},&#xA;        {&#xA;            &#34;role&#34;: &#34;user&#34;,&#xA;            &#34;content&#34;: [&#xA;                {&#xA;                    &#34;type&#34;: &#34;image_url&#34;,&#xA;                    &#34;image_url&#34;: {&#xA;                        &#34;url&#34;: base64_qwen&#xA;                    },&#xA;                },&#xA;                {&#34;type&#34;: &#34;text&#34;, &#34;text&#34;: &#34;What is the text in the illustrate?&#34;},&#xA;            ],&#xA;        },&#xA;    ],&#xA;)&#xA;print(&#34;Chat response:&#34;, chat_response)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For videos, you can use the chat API as followsÔºö&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import base64&#xA;import numpy as np&#xA;from PIL import Image&#xA;from io import BytesIO&#xA;from openai import OpenAI&#xA;from qwen_vl_utils import process_vision_info&#xA;&#xA;&#xA;# Set OpenAI&#39;s API key and API base to use vLLM&#39;s API server.&#xA;openai_api_key = &#34;EMPTY&#34;&#xA;openai_api_base = &#34;http://localhost:8000/v1&#34;&#xA;&#xA;client = OpenAI(&#xA;    api_key=openai_api_key,&#xA;    base_url=openai_api_base,&#xA;)&#xA;&#xA;&#xA;video_messages = [&#xA;    {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: &#34;You are a helpful assistant.&#34;},&#xA;    {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: [&#xA;        {&#34;type&#34;: &#34;text&#34;, &#34;text&#34;: &#34;ËØ∑Áî®Ë°®Ê†ºÊÄªÁªì‰∏Ä‰∏ãËßÜÈ¢ë‰∏≠ÁöÑÂïÜÂìÅÁâπÁÇπ&#34;},&#xA;        {&#xA;            &#34;type&#34;: &#34;video&#34;,&#xA;            &#34;video&#34;: &#34;https://duguang-labelling.oss-cn-shanghai.aliyuncs.com/qiansun/video_ocr/videos/50221078283.mp4&#34;,&#xA;            &#34;total_pixels&#34;: 20480 * 28 * 28, &#34;min_pixels&#34;: 16 * 28 * 2, &#xA;            &#39;fps&#39;: 3.0  # The default value is 2.0, but for demonstration purposes, we set it to 3.0.&#xA;        }]&#xA;    },&#xA;]&#xA;&#xA;&#xA;def prepare_message_for_vllm(content_messages):&#xA;    &#34;&#34;&#34;&#xA;    The frame extraction logic for videos in `vLLM` differs from that of `qwen_vl_utils`.&#xA;    Here, we utilize `qwen_vl_utils` to extract video frames, with the `media_typ`e of the video explicitly set to `video/jpeg`.&#xA;    By doing so, vLLM will no longer attempt to extract frames from the input base64-encoded images.&#xA;    &#34;&#34;&#34;&#xA;    vllm_messages, fps_list = [], []&#xA;    for message in content_messages:&#xA;        message_content_list = message[&#34;content&#34;]&#xA;        if not isinstance(message_content_list, list):&#xA;            vllm_messages.append(message)&#xA;            continue&#xA;&#xA;        new_content_list = []&#xA;        for part_message in message_content_list:&#xA;            if &#39;video&#39; in part_message:&#xA;                video_message = [{&#39;content&#39;: [part_message]}]&#xA;                image_inputs, video_inputs, video_kwargs = process_vision_info(video_message, return_video_kwargs=True)&#xA;                assert video_inputs is not None, &#34;video_inputs should not be None&#34;&#xA;                video_input = (video_inputs.pop()).permute(0, 2, 3, 1).numpy().astype(np.uint8)&#xA;                fps_list.extend(video_kwargs.get(&#39;fps&#39;, []))&#xA;&#xA;                # encode image with base64&#xA;                base64_frames = []&#xA;                for frame in video_input:&#xA;                    img = Image.fromarray(frame)&#xA;                    output_buffer = BytesIO()&#xA;                    img.save(output_buffer, format=&#34;jpeg&#34;)&#xA;                    byte_data = output_buffer.getvalue()&#xA;                    base64_str = base64.b64encode(byte_data).decode(&#34;utf-8&#34;)&#xA;                    base64_frames.append(base64_str)&#xA;&#xA;                part_message = {&#xA;                    &#34;type&#34;: &#34;video_url&#34;,&#xA;                    &#34;video_url&#34;: {&#34;url&#34;: f&#34;data:video/jpeg;base64,{&#39;,&#39;.join(base64_frames)}&#34;}&#xA;                }&#xA;            new_content_list.append(part_message)&#xA;        message[&#34;content&#34;] = new_content_list&#xA;        vllm_messages.append(message)&#xA;    return vllm_messages, {&#39;fps&#39;: fps_list}&#xA;&#xA;&#xA;video_messages, video_kwargs = prepare_message_for_vllm(video_messages)&#xA;chat_response = client.chat.completions.create(&#xA;    model=&#34;Qwen/Qwen2.5-VL-7B-Instruct&#34;,&#xA;    messages=video_messages,&#xA;    extra_body={&#xA;        &#34;mm_processor_kwargs&#34;: video_kwargs&#xA;    }&#xA;)&#xA;print(&#34;Chat response:&#34;, chat_response)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Inference Locally&lt;/h3&gt; &#xA;&lt;p&gt;You can also use vLLM to inference Qwen2.5-VL locally:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoProcessor&#xA;from vllm import LLM, SamplingParams&#xA;from qwen_vl_utils import process_vision_info&#xA;&#xA;MODEL_PATH = &#34;Qwen/Qwen2.5-VL-7B-Instruct&#34;&#xA;&#xA;llm = LLM(&#xA;    model=MODEL_PATH,&#xA;    limit_mm_per_prompt={&#34;image&#34;: 10, &#34;video&#34;: 10},&#xA;)&#xA;&#xA;sampling_params = SamplingParams(&#xA;    temperature=0.1,&#xA;    top_p=0.001,&#xA;    repetition_penalty=1.05,&#xA;    max_tokens=256,&#xA;    stop_token_ids=[],&#xA;)&#xA;&#xA;image_messages = [&#xA;    {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: &#34;You are a helpful assistant.&#34;},&#xA;    {&#xA;        &#34;role&#34;: &#34;user&#34;,&#xA;        &#34;content&#34;: [&#xA;            {&#xA;                &#34;type&#34;: &#34;image&#34;,&#xA;                &#34;image&#34;: &#34;https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png&#34;,&#xA;                &#34;min_pixels&#34;: 224 * 224,&#xA;                &#34;max_pixels&#34;: 1280 * 28 * 28,&#xA;            },&#xA;            {&#34;type&#34;: &#34;text&#34;, &#34;text&#34;: &#34;What is the text in the illustrate?&#34;},&#xA;        ],&#xA;    },&#xA;]&#xA;&#xA;&#xA;# For video input, you can pass following values instead:&#xA;# &#34;type&#34;: &#34;video&#34;,&#xA;# &#34;video&#34;: &#34;&amp;lt;video URL&amp;gt;&#34;,&#xA;video_messages = [&#xA;    {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: &#34;You are a helpful assistant.&#34;},&#xA;    {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: [&#xA;            {&#34;type&#34;: &#34;text&#34;, &#34;text&#34;: &#34;ËØ∑Áî®Ë°®Ê†ºÊÄªÁªì‰∏Ä‰∏ãËßÜÈ¢ë‰∏≠ÁöÑÂïÜÂìÅÁâπÁÇπ&#34;},&#xA;            {&#xA;                &#34;type&#34;: &#34;video&#34;, &#xA;                &#34;video&#34;: &#34;https://duguang-labelling.oss-cn-shanghai.aliyuncs.com/qiansun/video_ocr/videos/50221078283.mp4&#34;,&#xA;                &#34;total_pixels&#34;: 20480 * 28 * 28, &#34;min_pixels&#34;: 16 * 28 * 28&#xA;            }&#xA;        ]&#xA;    },&#xA;]&#xA;&#xA;# Here we use video messages as a demonstration&#xA;messages = video_messages&#xA;&#xA;processor = AutoProcessor.from_pretrained(MODEL_PATH)&#xA;prompt = processor.apply_chat_template(&#xA;    messages,&#xA;    tokenize=False,&#xA;    add_generation_prompt=True,&#xA;)&#xA;image_inputs, video_inputs, video_kwargs = process_vision_info(messages, return_video_kwargs=True)&#xA;&#xA;mm_data = {}&#xA;if image_inputs is not None:&#xA;    mm_data[&#34;image&#34;] = image_inputs&#xA;if video_inputs is not None:&#xA;    mm_data[&#34;video&#34;] = video_inputs&#xA;&#xA;llm_inputs = {&#xA;    &#34;prompt&#34;: prompt,&#xA;    &#34;multi_modal_data&#34;: mm_data,&#xA;&#xA;    # FPS will be returned in video_kwargs&#xA;    &#34;mm_processor_kwargs&#34;: video_kwargs,&#xA;}&#xA;&#xA;outputs = llm.generate([llm_inputs], sampling_params=sampling_params)&#xA;generated_text = outputs[0].outputs[0].text&#xA;&#xA;print(generated_text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üê≥ Docker&lt;/h2&gt; &#xA;&lt;p&gt;To simplify the deploy process, we provide docker images with pre-build environments: &lt;a href=&#34;https://hub.docker.com/r/qwenllm/qwenvl&#34;&gt;qwenllm/qwenvl&lt;/a&gt;. You only need to install the driver and download model files to launch demos.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run --gpus all --ipc=host --network=host --rm --name qwen2.5 -it qwenllm/qwenvl:2.5-cu121 bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our paper and code useful in your research, please consider giving a star &lt;span&gt;‚≠ê&lt;/span&gt; and citation &lt;span&gt;üìù&lt;/span&gt; :)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-BibTeX&#34;&gt;&#xA;@article{Qwen2.5-VL,&#xA;  title={Qwen2.5-VL Technical Report},&#xA;  author={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},&#xA;  journal={arXiv preprint arXiv:2502.13923},&#xA;  year={2025}&#xA;}&#xA;&#xA;@article{Qwen2-VL,&#xA;  title={Qwen2-VL: Enhancing Vision-Language Model&#39;s Perception of the World at Any Resolution},&#xA;  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},&#xA;  journal={arXiv preprint arXiv:2409.12191},&#xA;  year={2024}&#xA;}&#xA;&#xA;@article{Qwen-VL,&#xA;  title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},&#xA;  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},&#xA;  journal={arXiv preprint arXiv:2308.12966},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt;</summary>
  </entry>
</feed>