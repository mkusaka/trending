<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-07T01:58:23Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>TommyZihao/Train_Custom_Dataset</title>
    <updated>2023-05-07T01:58:23Z</updated>
    <id>tag:github.com,2023-05-07:/TommyZihao/Train_Custom_Dataset</id>
    <link href="https://github.com/TommyZihao/Train_Custom_Dataset" rel="alternate"></link>
    <summary type="html">&lt;p&gt;标注自己的数据集，训练、评估、测试、部署自己的人工智能算法&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;子豪兄带你两天搞定AI毕业设计&lt;/h1&gt; &#xA;&lt;p&gt;标注自己的数据集，训练、评估、测试、部署自己的人工智能算法&lt;/p&gt; &#xA;&lt;p&gt;作者：同济子豪兄 &lt;a href=&#34;https://space.bilibili.com/1900783&#34;&gt;https://space.bilibili.com/1900783&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;代码测试&lt;a href=&#34;https://featurize.cn?s=d7ce99f842414bfcaea5662a97581bd1&#34;&gt;云GPU环境&lt;/a&gt;：GPU RTX 3060、CUDA v11.2&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;本教程的数据集、代码、视频，倾注了子豪兄大量时间和心血。如果知识付费，卖两三千并不为过，但本着开源分享精神，全部免费开源，但仅可用于教学、科研、科普等非盈利用途，并需在转载引用时注明出处。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://zihao-download.obs.cn-east-3.myhuaweicloud.com/img_bed/20220803/cv_fund.png&#34; alt=&#34;计算机视觉解决的基本问题&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/TommyZihao/zihao_commercial&#34;&gt;优雅地感谢子豪兄&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;图像分类&lt;/h2&gt; &#xA;&lt;h3&gt;构建自己的图像分类数据集&lt;/h3&gt; &#xA;&lt;p&gt;收集图像、下载样例数据集，删除系统多余文件，划分训练集、测试集，统计图像尺寸、比例分布、拍照地点位置分布，统计各类别图像数量&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1Jd4y1T7rw&#34;&gt;https://www.bilibili.com/video/BV1Jd4y1T7rw&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;【Pytorch】ImageNet预训练图像分类模型预测&lt;/h3&gt; &#xA;&lt;p&gt;使用Pytorch自带的预训练图像分类模型，分别对单张图像、视频、摄像头实时画面运行图像分类预测&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1qe4y1D7zD&#34;&gt;https://www.bilibili.com/video/BV1qe4y1D7zD&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;【Pytorch】迁移学习Fine-tuning训练自己的图像分类模型&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1Ng411C7WY&#34;&gt;https://www.bilibili.com/video/BV1Ng411C7WY&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;【Pytorch】用训练得到的pytorch图像分类模型，识别图像、视频、摄像头画面&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV12d4y1P7xz&#34;&gt;https://www.bilibili.com/video/BV12d4y1P7xz&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;测试集评估&lt;/h3&gt; &#xA;&lt;p&gt;计算各类别分类评估指标，绘制混淆矩阵、PR曲线、ROC曲线。&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1bP411j7NK&#34;&gt;https://www.bilibili.com/video/BV1bP411j7NK&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;测试集语义特征降维可视化&lt;/h3&gt; &#xA;&lt;p&gt;抽取Pytorch训练得到的图像分类模型中间层的输出特征，作为输入图像的语义特征。计算测试集所有图像的语义特征，使用t-SNE和UMAP两种降维方法降维至二维和三维，可视化。&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1VB4y1z7xN&#34;&gt;https://www.bilibili.com/video/BV1VB4y1z7xN&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;可解释性分析、显著性分析&lt;/h3&gt; &#xA;&lt;p&gt;CAM热力图系列算法：&lt;a href=&#34;https://www.bilibili.com/video/BV1JG4y1s74x&#34;&gt;https://www.bilibili.com/video/BV1JG4y1s74x&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;答疑交流群&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://zdb.h5.xeknow.com/s/3MPxbI&#34;&gt;子豪兄图像分类答疑交流群（有问必答）&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;单目标追踪（蜜蜂追踪）&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1za411Y7Zm&#34;&gt;https://www.bilibili.com/video/BV1za411Y7Zm&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;视频人流量计数+足迹追踪&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1za411Y7Zm&#34;&gt;https://www.bilibili.com/video/BV1za411Y7Zm&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;大模型摘要生成&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1W44y1g7cB&#34;&gt;https://www.bilibili.com/video/BV1W44y1g7cB&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;CycleGAN图像风格迁移&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1wv4y1T71F&#34;&gt;https://www.bilibili.com/video/BV1wv4y1T71F&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;OCR文字识别&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1Ua411x7dB&#34;&gt;https://www.bilibili.com/video/BV1Ua411x7dB&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mkturkcan/generative-agents</title>
    <updated>2023-05-07T01:58:23Z</updated>
    <id>tag:github.com,2023-05-07:/mkturkcan/generative-agents</id>
    <link href="https://github.com/mkturkcan/generative-agents" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An attempt to build a working, locally-running cheap version of Generative Agents: Interactive Simulacra of Human Behavior&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Generative Large Language Models for Human-Like Behavior&lt;/h1&gt; &#xA;&lt;p&gt;This repository includes a working version of the type of model described in Generative Agents: Interactive Simulacra of Human Behavior.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;The models are distributed as notebooks that are easy to run locally, or on Google Colab. We recommend the use of Jupyter Lab if running locally. The notebook(s) should work as-is on Google Colab.&lt;/p&gt; &#xA;&lt;h1&gt;How to Use&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The most stable model is available at &lt;a href=&#34;https://github.com/mkturkcan/generative-agents/tree/main/notebook/Release&#34;&gt;https://github.com/mkturkcan/generative-agents/tree/main/notebook/Release&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;WIP models with the latest features will be available in &lt;a href=&#34;https://github.com/mkturkcan/generative-agents/tree/main/notebook/WIP&#34;&gt;https://github.com/mkturkcan/generative-agents/tree/main/notebook/WIP&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;A WIP library is available under &lt;a href=&#34;https://github.com/mkturkcan/generative-agents/tree/main/game_simulation&#34;&gt;https://github.com/mkturkcan/generative-agents/tree/main/game_simulation&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Model&lt;/h2&gt; &#xA;&lt;p&gt;The current model is a simulation of the town of Phandalin from an introductory D&amp;amp;D 5e adventure. This setting is chosen as it is much more free form than the simple scenario described in the original paper.&lt;/p&gt; &#xA;&lt;h2&gt;Limitations&lt;/h2&gt; &#xA;&lt;p&gt;The model, as described in the paper, requires access to a very high quality instruction model such as GPT-3. However, the model also requires many high-context queries to work, making it expensive to run. As such, in this work we use low-parameter, locally runnable models instead.&lt;/p&gt; &#xA;&lt;p&gt;We expect that with the advent of the next generation of instruction-tuned models, the model in this repo will perform better.&lt;/p&gt; &#xA;&lt;h2&gt;Future Steps&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Summarize agent decisions as emojis. (WIP)&lt;/li&gt; &#xA; &lt;li&gt;Create a family of questions to compress agent contexts better.&lt;/li&gt; &#xA; &lt;li&gt;Check if the agent contexts are compressed well with an another layer of prompts.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>z-x-yang/Segment-and-Track-Anything</title>
    <updated>2023-05-07T01:58:23Z</updated>
    <id>tag:github.com,2023-05-07:/z-x-yang/Segment-and-Track-Anything</id>
    <link href="https://github.com/z-x-yang/Segment-and-Track-Anything" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An open-source project dedicated to tracking and segmenting any objects in videos, either automatically or interactively. The primary algorithms utilized include the Segment Anything Model (SAM) for key-frame segmentation and Associating Objects with Transformers (AOT) for efficient tracking and propagation purposes.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Segment and Track Anything (SAM-Track)&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Online Demo:&lt;/strong&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1R10N70AJaslzADFqb-a5OihYkllWEVxB?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tutorial:&lt;/strong&gt; &lt;a href=&#34;https://raw.githubusercontent.com/z-x-yang/Segment-and-Track-Anything/main/tutorial/tutorial%20for%20WebUI-1.5-Version.md&#34;&gt;tutorial-v1.5 (Text)&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/z-x-yang/Segment-and-Track-Anything/main/tutorial/tutorial%20for%20WebUI-1.0-Version.md&#34;&gt;tutorial-v1.0 (Click &amp;amp; Brush)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/z-x-yang/Segment-and-Track-Anything/main/assets/top.gif&#34; width=&#34;880&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Segment and Track Anything&lt;/strong&gt; is an open-source project that focuses on the segmentation and tracking of any objects in videos, utilizing both automatic and interactive methods. The primary algorithms utilized include the &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;&lt;strong&gt;SAM&lt;/strong&gt; (Segment Anything Models)&lt;/a&gt; for automatic/interactive key-frame segmentation and the &lt;a href=&#34;https://github.com/yoxu515/aot-benchmark&#34;&gt;&lt;strong&gt;DeAOT&lt;/strong&gt; (Decoupling features in Associating Objects with Transformers)&lt;/a&gt; (NeurIPS2022) for efficient multi-object tracking and propagation. The SAM-Track pipeline enables dynamic and automatic detection and segmentation of new objects by SAM, while DeAOT is responsible for tracking all identified objects.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;📢&lt;/span&gt;New Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;[2023/4/29] We have added advanced arguments for AOT-L: &lt;code&gt;long_term_memory_gap&lt;/code&gt; and &lt;code&gt;max_len_long_term&lt;/code&gt;.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;long_term_memory_gap&lt;/code&gt; controls the frequency at which the AOT model adds new reference frames to its long-term memory. During mask propagation, AOT matches the current frame with the reference frames stored in the long-term memory.&lt;/li&gt; &#xA;   &lt;li&gt;Setting the gap value to a proper value helps to obtain better performance. To avoid memory explosion in long videos, we set a &lt;code&gt;max_len_long_term&lt;/code&gt; value for the long-term memory storage, i.e. when the number of memory frames reaches the &lt;code&gt;max_len_long_term value&lt;/code&gt;, the oldest memory frame will be discarded and a new frame will be added.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2023/4/26] &lt;strong&gt;Interactive WebUI 1.5-Version&lt;/strong&gt;: We have added new features based on Interactive WebUI-1.0 Version.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;We have added a new form of interactivity—text prompts—to SAMTrack.&lt;/li&gt; &#xA;   &lt;li&gt;From now on, multiple objects that need to be tracked can be interactively added.&lt;/li&gt; &#xA;   &lt;li&gt;Check out &lt;a href=&#34;https://raw.githubusercontent.com/z-x-yang/Segment-and-Track-Anything/main/tutorial/tutorial%20for%20WebUI-1.5-Version.md&#34;&gt;tutorial&lt;/a&gt; for Interactive WebUI 1.5-Version. More demos will be released in the next few days.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2023/4/26] &lt;strong&gt;Image-Sequence input&lt;/strong&gt;: The WebUI now has a new feature that allows for input of image sequences, which can be used to test video segmentation datasets. Get started with the &lt;a href=&#34;https://raw.githubusercontent.com/z-x-yang/Segment-and-Track-Anything/main/tutorial/tutorial%20for%20Image-Sequence%20input.md&#34;&gt;tutorial&lt;/a&gt; for Image-Sequence input.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2023/4/25] &lt;strong&gt;Online Demo:&lt;/strong&gt; You can easily use SAMTrack in &lt;a href=&#34;https://colab.research.google.com/drive/1R10N70AJaslzADFqb-a5OihYkllWEVxB?usp=sharing&#34;&gt;Colab&lt;/a&gt; for visual tracking tasks.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2023/4/23] &lt;strong&gt;Interactive WebUI:&lt;/strong&gt; We have introduced a new WebUI that allows interactive user segmentation through strokes and clicks. Feel free to explore and have fun with the &lt;a href=&#34;https://raw.githubusercontent.com/z-x-yang/Segment-and-Track-Anything/main/tutorial/tutorial%20for%20WebUI-1.0-Version.md&#34;&gt;tutorial&lt;/a&gt;!&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;[2023/4/24] &lt;strong&gt;Tutorial V1.0:&lt;/strong&gt; Check out our new video tutorials! &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;YouTube-Link: &lt;a href=&#34;https://www.youtube.com/watch?v=DF0iFSsX8KY&#34;&gt;Tutorial for Interactively modify single-object mask for first frame of video&lt;/a&gt;、&lt;a href=&#34;https://www.youtube.com/watch?v=UJvKPng9_DA&#34;&gt;Tutorial for Interactively add object by click&lt;/a&gt;、&lt;a href=&#34;https://www.youtube.com/watch?v=m1oFavjIaCM&#34;&gt;Tutorial for Interactively add object by stroke&lt;/a&gt;.&lt;/li&gt; &#xA;     &lt;li&gt;Bilibili Video Link:&lt;a href=&#34;https://www.bilibili.com/video/BV1tM4115791/?spm_id_from=333.999.0.0&#34;&gt;Tutorial for Interactively modify single-object mask for first frame of video&lt;/a&gt;、&lt;a href=&#34;https://www.bilibili.com/video/BV1Qs4y1A7d1/&#34;&gt;Tutorial for Interactively add object by click&lt;/a&gt;、&lt;a href=&#34;https://www.bilibili.com/video/BV1Lm4y117J4/?spm_id_from=333.999.0.0&#34;&gt;Tutorial for Interactively add object by stroke&lt;/a&gt;.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;1.0-Version is a developer version, please feel free to contact us if you encounter any bugs &lt;span&gt;🐛&lt;/span&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2023/4/17] &lt;strong&gt;SAMTrack&lt;/strong&gt;: Automatically segment and track anything in video!&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;span&gt;🔥&lt;/span&gt;Demos&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://youtu.be/UPhtpf1k6HA&#34; title=&#34;Segment-and-Track-Anything Versatile Demo&#34;&gt;&lt;img src=&#34;https://res.cloudinary.com/marcomontalbano/image/upload/v1681713095/video_to_markdown/images/youtube--UPhtpf1k6HA-c05b58ac6eb4c4700831b2b3070cd403.jpg&#34; alt=&#34;Segment-and-Track-Anything Versatile Demo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;This video showcases the segmentation and tracking capabilities of SAM-Track in various scenarios, such as street views, AR, cells, animations, aerial shots, and more.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;📆&lt;/span&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Colab notebook: Completed on April 25th, 2023.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 1.0-Version Interactive WebUI: Completed on April 23rd, 2023. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;We will create a feature that enables users to interactively modify the mask for the initial video frame according to their needs. The interactive segmentation capabilities of Segment-and-Track-Anything is demonstrated in &lt;a href=&#34;https://www.youtube.com/watch?v=Xyd54AngvV8&amp;amp;feature=youtu.be&#34;&gt;Demo8&lt;/a&gt; and &lt;a href=&#34;https://www.youtube.com/watch?v=eZrdna8JkoQ&#34;&gt;Demo9&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Bilibili Video Link: &lt;a href=&#34;https://www.bilibili.com/video/BV1JL411v7uE/&#34;&gt;Demo8&lt;/a&gt;, &lt;a href=&#34;https://www.bilibili.com/video/BV1Qs4y1w763/&#34;&gt;Demo9&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 1.5-Version Interactive WebUI: Completed on April 26th, 2023. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;We will develop a function that allows interactive modification of multi-object masks for the first frame of a video. This function will be based on Version 1.0. YouTube: &lt;a href=&#34;https://www.youtube.com/watch?v=UFtwFaOfx2I&amp;amp;feature=youtu.be&#34;&gt;Demo4&lt;/a&gt;, &lt;a href=&#34;https://www.youtube.com/watch?v=cK5MPFdJdSY&amp;amp;feature=youtu.be&#34;&gt;Demo5&lt;/a&gt;; Bilibili: &lt;a href=&#34;https://www.bilibili.com/video/BV17X4y127mJ/&#34;&gt;Demo4&lt;/a&gt;, &lt;a href=&#34;https://www.bilibili.com/video/BV1Pz4y1a7mC/&#34;&gt;Demo5&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Furthermore, we plan to include text prompts as an additional form of interaction. YouTube: &lt;a href=&#34;https://www.youtube.com/watch?v=5oieHqFIJPc&amp;amp;feature=youtu.be&#34;&gt;Demo1&lt;/a&gt;, &lt;a href=&#34;https://www.youtube.com/watch?v=nXfq17X6ohk&#34;&gt;Demo2&lt;/a&gt;; Bilibili: &lt;a href=&#34;https://www.bilibili.com/video/BV1hg4y157yd/?vd_source=fe3b5c0215d05cc44c8eb3d94abae3ca&#34;&gt;Demo1&lt;/a&gt;, &lt;a href=&#34;https://www.bilibili.com/video/BV1RV4y1k7i5/&#34;&gt;Demo2&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 2.x-Version Interactive WebUI &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;In version 2.x, the segmentation model will offer two options: SAM and SEEM.&lt;/li&gt; &#xA;   &lt;li&gt;We will develop a new function where the fixed-category object detection result can be displayed as a prompt.&lt;/li&gt; &#xA;   &lt;li&gt;We will enable SAM-Track to add and modify objects during tracking. YouTube: &lt;a href=&#34;https://www.youtube.com/watch?v=l7hXM1a3nEA&amp;amp;feature=youtu.be&#34;&gt;Demo6&lt;/a&gt;, &lt;a href=&#34;https://www.youtube.com/watch?v=hPjw28Ul4cw&amp;amp;feature=youtu.be&#34;&gt;Demo7&lt;/a&gt;; Bilibili: &lt;a href=&#34;https://www.bilibili.com/video/BV1nk4y1j7Am&#34;&gt;Demo6&lt;/a&gt;, &lt;a href=&#34;https://www.bilibili.com/video/BV1mk4y1E78s/?vd_source=fe3b5c0215d05cc44c8eb3d94abae3ca&#34;&gt;Demo7&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Demo1&lt;/strong&gt; showcases SAM-Track&#39;s ability to take the class of objects as prompt. The user gives the category text &#39;panda&#39; to enable instance-level segmentation and tracking of all objects belonging to this category.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=5oieHqFIJPc&amp;amp;feature=youtu.be&#34; title=&#34;demo1&#34;&gt;&lt;img src=&#34;https://res.cloudinary.com/marcomontalbano/image/upload/v1683347297/video_to_markdown/images/youtube--5oieHqFIJPc-c05b58ac6eb4c4700831b2b3070cd403.jpg&#34; alt=&#34;demo1&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;Demo2&lt;/strong&gt; showcases SAM-Track&#39;s ability to take the text description as prompt. SAM-Track could segment and track target objects given the input that &#39;panda on the far left&#39;.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=nXfq17X6ohk&#34; title=&#34;demo1&#34;&gt;&lt;img src=&#34;https://res.cloudinary.com/marcomontalbano/image/upload/v1683347643/video_to_markdown/images/youtube--nXfq17X6ohk-c05b58ac6eb4c4700831b2b3070cd403.jpg&#34; alt=&#34;demo1&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;Demo3&lt;/strong&gt; showcases SAM-Track&#39;s ability to track numerous objects at the same time. SAM-Track is capable of automatically detecting newly appearing objects.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=jMqFMq0tRP0&#34; title=&#34;demo1&#34;&gt;&lt;img src=&#34;https://res.cloudinary.com/marcomontalbano/image/upload/v1683347961/video_to_markdown/images/youtube--jMqFMq0tRP0-c05b58ac6eb4c4700831b2b3070cd403.jpg&#34; alt=&#34;demo1&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;Demo4&lt;/strong&gt; showcases SAM-Track&#39;s ability to take multiple modes of interactions as prompt. The user specified human and skateboard with click and brushstroke, respectively.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=UFtwFaOfx2I&amp;amp;feature=youtu.be&#34; title=&#34;demo1&#34;&gt;&lt;img src=&#34;https://res.cloudinary.com/marcomontalbano/image/upload/v1683348115/video_to_markdown/images/youtube--UFtwFaOfx2I-c05b58ac6eb4c4700831b2b3070cd403.jpg&#34; alt=&#34;demo1&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;Demo5&lt;/strong&gt; showcases SAM-Track&#39;s ability to refine the results of segment-everything. The user merges the tram as a whole with a single click.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=cK5MPFdJdSY&amp;amp;feature=youtu.be&#34; title=&#34;demo1&#34;&gt;&lt;img src=&#34;https://res.cloudinary.com/marcomontalbano/image/upload/v1683348276/video_to_markdown/images/youtube--cK5MPFdJdSY-c05b58ac6eb4c4700831b2b3070cd403.jpg&#34; alt=&#34;demo1&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;Demo6&lt;/strong&gt; showcases SAM-Track&#39;s ability to add new objects during tracking. The user annotates another car by rolling back to an intermediate frame.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=l7hXM1a3nEA&#34; title=&#34;demo1&#34;&gt;&lt;img src=&#34;https://res.cloudinary.com/marcomontalbano/image/upload/v1683348411/video_to_markdown/images/youtube--l7hXM1a3nEA-c05b58ac6eb4c4700831b2b3070cd403.jpg&#34; alt=&#34;demo1&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;Demo7&lt;/strong&gt; showcases SAM-Track&#39;s ability to refine the prediction during tracking. This feature is highly advantageous for segmentation and tracking under complex environments.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=hPjw28Ul4cw&amp;amp;feature=youtu.be&#34; title=&#34;demo1&#34;&gt;&lt;img src=&#34;https://res.cloudinary.com/marcomontalbano/image/upload/v1683348621/video_to_markdown/images/youtube--hPjw28Ul4cw-c05b58ac6eb4c4700831b2b3070cd403.jpg&#34; alt=&#34;demo1&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;Demo8&lt;/strong&gt; showcases SAM-Track&#39;s ability to interactively segment and track individual objects. The user specified that SAM-Track tracked a man playing street basketball.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Xyd54AngvV8&#34; title=&#34;Interactive Segment-and-Track-Anything Demo1&#34;&gt;&lt;img src=&#34;https://res.cloudinary.com/marcomontalbano/image/upload/v1681712022/video_to_markdown/images/youtube--Xyd54AngvV8-c05b58ac6eb4c4700831b2b3070cd403.jpg&#34; alt=&#34;Interactive Segment-and-Track-Anything Demo1&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;Demo9&lt;/strong&gt; showcases SAM-Track&#39;s ability to interactively add specified objects for tracking.The user customized the addition of objects to be tracked on top of the segmentation of everything in the scene using SAM-Track.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=eZrdna8JkoQ&#34; title=&#34;Interactive Segment-and-Track-Anything Demo2&#34;&gt;&lt;img src=&#34;https://res.cloudinary.com/marcomontalbano/image/upload/v1681712071/video_to_markdown/images/youtube--eZrdna8JkoQ-c05b58ac6eb4c4700831b2b3070cd403.jpg&#34; alt=&#34;Interactive Segment-and-Track-Anything Demo2&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;&lt;span&gt;💻&lt;/span&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;span&gt;📑&lt;/span&gt;Requirements&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;Segment-Anything&lt;/a&gt; repository has been cloned and renamed as sam, and the &lt;a href=&#34;https://github.com/yoxu515/aot-benchmark&#34;&gt;aot-benchmark&lt;/a&gt; repository has been cloned and renamed as aot.&lt;/p&gt; &#xA;&lt;p&gt;Please check the dependency requirements in &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;SAM&lt;/a&gt; and &lt;a href=&#34;https://github.com/yoxu515/aot-benchmark&#34;&gt;DeAOT&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The implementation is tested under python 3.9, as well as pytorch 1.10 and torchvision 0.11. &lt;strong&gt;We recommend equivalent or higher pytorch version&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Use the &lt;code&gt;install.sh&lt;/code&gt; to install the necessary libs for SAM-Track&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash script/install.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;span&gt;⭐&lt;/span&gt;Model Preparation&lt;/h3&gt; &#xA;&lt;p&gt;Download SAM model to ckpt, the default model is SAM-VIT-B (&lt;a href=&#34;https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth&#34;&gt;sam_vit_b_01ec64.pth&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;Download DeAOT/AOT model to ckpt, the default model is R50-DeAOT-L (&lt;a href=&#34;https://drive.google.com/file/d/1QoChMkTVxdYZ_eBlZhK2acq9KMQZccPJ/view&#34;&gt;R50_DeAOTL_PRE_YTB_DAV.pth&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;Download Grounding-Dino model to ckpt, the default model is GroundingDINO-T (&lt;a href=&#34;https://huggingface.co/ShilongLiu/GroundingDINO/resolve/main/groundingdino_swint_ogc.pth&#34;&gt;groundingdino_swint_ogc&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;You can download the default weights using the command line as shown below.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash script/download_ckpt.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;span&gt;❤️&lt;/span&gt;Run Demo&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The video to be processed can be put in ./assets.&lt;/li&gt; &#xA; &lt;li&gt;Then run &lt;strong&gt;demo.ipynb&lt;/strong&gt; step by step to generate results.&lt;/li&gt; &#xA; &lt;li&gt;The results will be saved as masks for each frame and a gif file for visualization.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The arguments for SAM-Track, DeAOT and SAM can be manually modified in model_args.py for purpose of using other models or controling the behavior of each model.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;span&gt;💪&lt;/span&gt;WebUI App&lt;/h3&gt; &#xA;&lt;p&gt;Our user-friendly visual interface allows you to easily obtain the results of your experiments. Simply initiate it using the command line.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Users can upload the video directly on the UI and use SegTracker to automatically/interactively track objects within that video. We use a video of a man playing basketball as an example.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/z-x-yang/Segment-and-Track-Anything/main/assets/interactive_webui.jpg&#34; alt=&#34;Interactive WebUI&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;SegTracker-Parameters:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;aot_model&lt;/strong&gt;: used to select which version of DeAOT/AOT to use for tracking and propagation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;sam_gap&lt;/strong&gt;: used to control how often SAM is used to add newly appearing objects at specified frame intervals. Increase to decrease the frequency of discovering new targets, but significantly improve speed of inference.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;points_per_side&lt;/strong&gt;: used to control the number of points per side used for generating masks by sampling a grid over the image. Increasing the size enhances the ability to detect small objects, but larger targets may be segmented into finer granularity.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;max_obj_num&lt;/strong&gt;: used to limit the maximum number of objects that SAM-Track can detect and track. A larger number of objects necessitates a greater utilization of memory, with approximately 16GB of memory capable of processing a maximum of 255 objects.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Usage: To see the details, please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/z-x-yang/Segment-and-Track-Anything/main/tutorial/tutorial%20for%20WebUI-1.0-Version.md&#34;&gt;tutorial for 1.0-Version WebUI&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;span&gt;🌝&lt;/span&gt;Credits&lt;/h3&gt; &#xA;&lt;p&gt;Licenses for borrowed code can be found in &lt;code&gt;licenses.md&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;DeAOT/AOT - &lt;a href=&#34;https://github.com/yoxu515/aot-benchmark&#34;&gt;https://github.com/yoxu515/aot-benchmark&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;SAM - &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;https://github.com/facebookresearch/segment-anything&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Gradio (for building WebUI) - &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;https://github.com/gradio-app/gradio&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Grounding-Dino - &lt;a href=&#34;https://github.com/yamy-cheng/GroundingDINO&#34;&gt;https://github.com/yamy-cheng/GroundingDINO&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;&lt;span&gt;🏫&lt;/span&gt;About us&lt;/h3&gt; &#xA;&lt;p&gt;Thank you for your interest in this project. The project is supervised by the ReLER Lab at Zhejiang University’s College of Computer Science and Technology. ReLER was established by Yang Yi, a Qiu Shi Distinguished Professor at Zhejiang University. Our dedicated team of contributors includes &lt;a href=&#34;https://github.com/yoxu515&#34;&gt;Yuanyou Xu&lt;/a&gt;, &lt;a href=&#34;https://github.com/yamy-cheng&#34;&gt;Yangming Cheng&lt;/a&gt;, &lt;a href=&#34;https://github.com/lingorX&#34;&gt;Liulei Li&lt;/a&gt;, &lt;a href=&#34;https://z-x-yang.github.io/&#34;&gt;Zongxin Yang&lt;/a&gt;, &lt;a href=&#34;https://sites.google.com/view/wenguanwang&#34;&gt;Wenguan Wang&lt;/a&gt; and &lt;a href=&#34;https://scholar.google.com/citations?user=RMSuNFwAAAAJ&amp;amp;hl=en&#34;&gt;Yi Yang&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>