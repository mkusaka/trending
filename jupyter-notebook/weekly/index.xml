<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-02-25T01:49:05Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Vaibhavs10/insanely-fast-whisper</title>
    <updated>2024-02-25T01:49:05Z</updated>
    <id>tag:github.com,2024-02-25:/Vaibhavs10/insanely-fast-whisper</id>
    <link href="https://github.com/Vaibhavs10/insanely-fast-whisper" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Insanely Fast Whisper&lt;/h1&gt; &#xA;&lt;p&gt;An opinionated CLI to transcribe Audio files w/ Whisper on-device! Powered by ü§ó &lt;em&gt;Transformers&lt;/em&gt;, &lt;em&gt;Optimum&lt;/em&gt; &amp;amp; &lt;em&gt;flash-attn&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; - Transcribe &lt;strong&gt;150&lt;/strong&gt; minutes (2.5 hours) of audio in less than &lt;strong&gt;98&lt;/strong&gt; seconds - with &lt;a href=&#34;https://huggingface.co/openai/whisper-large-v3&#34;&gt;OpenAI&#39;s Whisper Large v3&lt;/a&gt;. Blazingly fast transcription is now a reality!‚ö°Ô∏è&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://huggingface.co/datasets/reach-vb/random-images/resolve/main/insanely-fast-whisper-img.png&#34; width=&#34;615&#34; height=&#34;308&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Not convinced? Here are some benchmarks we ran on a Nvidia A100 - 80GB üëá&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Optimisation type&lt;/th&gt; &#xA;   &lt;th&gt;Time to Transcribe (150 mins of Audio)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;large-v3 (Transformers) (&lt;code&gt;fp32&lt;/code&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;~31 (&lt;em&gt;31 min 1 sec&lt;/em&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;large-v3 (Transformers) (&lt;code&gt;fp16&lt;/code&gt; + &lt;code&gt;batching [24]&lt;/code&gt; + &lt;code&gt;bettertransformer&lt;/code&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;~5 (&lt;em&gt;5 min 2 sec&lt;/em&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;large-v3 (Transformers) (&lt;code&gt;fp16&lt;/code&gt; + &lt;code&gt;batching [24]&lt;/code&gt; + &lt;code&gt;Flash Attention 2&lt;/code&gt;)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;~2 (&lt;em&gt;1 min 38 sec&lt;/em&gt;)&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;distil-large-v2 (Transformers) (&lt;code&gt;fp16&lt;/code&gt; + &lt;code&gt;batching [24]&lt;/code&gt; + &lt;code&gt;bettertransformer&lt;/code&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;~3 (&lt;em&gt;3 min 16 sec&lt;/em&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;distil-large-v2 (Transformers) (&lt;code&gt;fp16&lt;/code&gt; + &lt;code&gt;batching [24]&lt;/code&gt; + &lt;code&gt;Flash Attention 2&lt;/code&gt;)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;~1 (&lt;em&gt;1 min 18 sec&lt;/em&gt;)&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;large-v2 (Faster Whisper) (&lt;code&gt;fp16&lt;/code&gt; + &lt;code&gt;beam_size [1]&lt;/code&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;~9.23 (&lt;em&gt;9 min 23 sec&lt;/em&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;large-v2 (Faster Whisper) (&lt;code&gt;8-bit&lt;/code&gt; + &lt;code&gt;beam_size [1]&lt;/code&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;~8 (&lt;em&gt;8 min 15 sec&lt;/em&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;P.S. We also ran the benchmarks on a &lt;a href=&#34;https://raw.githubusercontent.com/Vaibhavs10/insanely-fast-whisper/main/notebooks/&#34;&gt;Google Colab T4 GPU&lt;/a&gt; instance too!&lt;/p&gt; &#xA;&lt;p&gt;P.P.S. This project originally started as a way to showcase benchmarks for Transformers, but has since evolved into a lightweight CLI for people to use. This is purely community driven. We add whatever community seems to have a strong demand for!&lt;/p&gt; &#xA;&lt;h2&gt;üÜï Blazingly fast transcriptions via your terminal! ‚ö°Ô∏è&lt;/h2&gt; &#xA;&lt;p&gt;We&#39;ve added a CLI to enable fast transcriptions. Here&#39;s how you can use it:&lt;/p&gt; &#xA;&lt;p&gt;Install &lt;code&gt;insanely-fast-whisper&lt;/code&gt; with &lt;code&gt;pipx&lt;/code&gt; (&lt;code&gt;pip install pipx&lt;/code&gt; or &lt;code&gt;brew install pipx&lt;/code&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pipx install insanely-fast-whisper&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Note: Due to a dependency on &lt;a href=&#34;https://github.com/microsoft/onnxruntime/issues/17842&#34;&gt;&lt;code&gt;onnxruntime&lt;/code&gt;, Python 3.12 is currently not supported&lt;/a&gt;. You can force a Python version (e.g. 3.11) by adding &lt;code&gt;--python python3.11&lt;/code&gt; to the command.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;‚ö†Ô∏è If you have python 3.11.XX installed, &lt;code&gt;pipx&lt;/code&gt; may parse the version incorrectly and install a very old version of &lt;code&gt;insanely-fast-whisper&lt;/code&gt; without telling you (version &lt;code&gt;0.0.8&lt;/code&gt;, which won&#39;t work anymore with the current &lt;code&gt;BetterTransformers&lt;/code&gt;). In that case, you can install the latest version by passing &lt;code&gt;--ignore-requires-python&lt;/code&gt; to &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pipx install insanely-fast-whisper --force --pip-args=&#34;--ignore-requires-python&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you&#39;re installing with &lt;code&gt;pip&lt;/code&gt;, you can pass the argument directly: &lt;code&gt;pip install insanely-fast-whisper --ignore-requires-python&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Run inference from any path on your computer:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;insanely-fast-whisper --file-name &amp;lt;filename or URL&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Note: if you are running on macOS, you also need to add &lt;code&gt;--device-id mps&lt;/code&gt; flag.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;üî• You can run &lt;a href=&#34;https://huggingface.co/openai/whisper-large-v3&#34;&gt;Whisper-large-v3&lt;/a&gt; w/ &lt;a href=&#34;https://github.com/Dao-AILab/flash-attention&#34;&gt;Flash Attention 2&lt;/a&gt; from this CLI too:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;insanely-fast-whisper --file-name &amp;lt;filename or URL&amp;gt; --flash True &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;üåü You can run &lt;a href=&#34;https://huggingface.co/distil-whisper&#34;&gt;distil-whisper&lt;/a&gt; directly from this CLI too:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;insanely-fast-whisper --model-name distil-whisper/large-v2 --file-name &amp;lt;filename or URL&amp;gt; &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Don&#39;t want to install &lt;code&gt;insanely-fast-whisper&lt;/code&gt;? Just use &lt;code&gt;pipx run&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pipx run insanely-fast-whisper --file-name &amp;lt;filename or URL&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] The CLI is highly opinionated and only works on NVIDIA GPUs &amp;amp; Mac. Make sure to check out the defaults and the list of options you can play around with to maximise your transcription throughput. Run &lt;code&gt;insanely-fast-whisper --help&lt;/code&gt; or &lt;code&gt;pipx run insanely-fast-whisper --help&lt;/code&gt; to get all the CLI arguments along with their defaults.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;CLI Options&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;insanely-fast-whisper&lt;/code&gt; repo provides an all round support for running Whisper in various settings. Note that as of today 26th Nov, &lt;code&gt;insanely-fast-whisper&lt;/code&gt; works on both CUDA and mps (mac) enabled devices.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;  -h, --help            show this help message and exit&#xA;  --file-name FILE_NAME&#xA;                        Path or URL to the audio file to be transcribed.&#xA;  --device-id DEVICE_ID&#xA;                        Device ID for your GPU. Just pass the device number when using CUDA, or &#34;mps&#34; for Macs with Apple Silicon. (default: &#34;0&#34;)&#xA;  --transcript-path TRANSCRIPT_PATH&#xA;                        Path to save the transcription output. (default: output.json)&#xA;  --model-name MODEL_NAME&#xA;                        Name of the pretrained model/ checkpoint to perform ASR. (default: openai/whisper-large-v3)&#xA;  --task {transcribe,translate}&#xA;                        Task to perform: transcribe or translate to another language. (default: transcribe)&#xA;  --language LANGUAGE   &#xA;                        Language of the input audio. (default: &#34;None&#34; (Whisper auto-detects the language))&#xA;  --batch-size BATCH_SIZE&#xA;                        Number of parallel batches you want to compute. Reduce if you face OOMs. (default: 24)&#xA;  --flash FLASH         &#xA;                        Use Flash Attention 2. Read the FAQs to see how to install FA2 correctly. (default: False)&#xA;  --timestamp {chunk,word}&#xA;                        Whisper supports both chunked as well as word level timestamps. (default: chunk)&#xA;  --hf_token TOKEN&#xA;                        Provide a hf.co/settings/token for Pyannote.audio to diarise the audio clips&#xA;  --diarization_model DIARIZATION_MODEL&#xA;                        Name of the pretrained model/ checkpoint to perform diarization. (default: pyannote/speaker-diarization)&#xA;  --num-speakers NUM_SPEAKERS&#xA;                        Specifies the exact number of speakers present in the audio file. Useful when the exact number of participants in the conversation is known. Must be at least 1. Cannot be used together with --min-speakers or --max-speakers. (default: None)&#xA;  --min-speakers MIN_SPEAKERS&#xA;                        Sets the minimum number of speakers that the system should consider during diarization. Must be at least 1. Cannot be used together with --num-speakers. Must be less than or equal to --max-speakers if both are specified. (default: None)&#xA;  --max-speakers MAX_SPEAKERS&#xA;                        Defines the maximum number of speakers that the system should consider in diarization. Must be at least 1. Cannot be used together with --num-speakers. Must be greater than or equal to --min-speakers if both are specified. (default: None)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Frequently Asked Questions&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;How to correctly install flash-attn to make it work with &lt;code&gt;insanely-fast-whisper&lt;/code&gt;?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Make sure to install it via &lt;code&gt;pipx runpip insanely-fast-whisper install flash-attn --no-build-isolation&lt;/code&gt;. Massive kudos to @li-yifei for helping with this.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;How to solve an &lt;code&gt;AssertionError: Torch not compiled with CUDA enabled&lt;/code&gt; error on Windows?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The root cause of this problem is still unknown, however, you can resolve this by manually installing torch in the virtualenv like &lt;code&gt;python -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121&lt;/code&gt;. Thanks to @pto2k for all tdebugging this.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;How to avoid Out-Of-Memory (OOM) exceptions on Mac?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The &lt;em&gt;mps&lt;/em&gt; backend isn&#39;t as optimised as CUDA, hence is way more memory hungry. Typically you can run with &lt;code&gt;--batch-size 4&lt;/code&gt; without any issues (should use roughly 12GB GPU VRAM). Don&#39;t forget to set &lt;code&gt;--device-id mps&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;How to use Whisper without a CLI?&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;All you need to run is the below snippet:&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;pip install --upgrade transformers optimum accelerate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from transformers import pipeline&#xA;from transformers.utils import is_flash_attn_2_available&#xA;&#xA;pipe = pipeline(&#xA;    &#34;automatic-speech-recognition&#34;,&#xA;    model=&#34;openai/whisper-large-v3&#34;, # select checkpoint from https://huggingface.co/openai/whisper-large-v3#model-details&#xA;    torch_dtype=torch.float16,&#xA;    device=&#34;cuda:0&#34;, # or mps for Mac devices&#xA;    model_kwargs={&#34;attn_implementation&#34;: &#34;flash_attention_2&#34;} if is_flash_attn_2_available() else {&#34;attn_implementation&#34;: &#34;sdpa&#34;},&#xA;)&#xA;&#xA;outputs = pipe(&#xA;    &#34;&amp;lt;FILE_NAME&amp;gt;&#34;,&#xA;    chunk_length_s=30,&#xA;    batch_size=24,&#xA;    return_timestamps=True,&#xA;)&#xA;&#xA;outputs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/whisper&#34;&gt;OpenAI Whisper&lt;/a&gt; team for open sourcing such a brilliant check point.&lt;/li&gt; &#xA; &lt;li&gt;Hugging Face Transformers team, specifically &lt;a href=&#34;https://github.com/ArthurZucker&#34;&gt;Arthur&lt;/a&gt;, &lt;a href=&#34;https://github.com/patrickvonplaten&#34;&gt;Patrick&lt;/a&gt;, &lt;a href=&#34;https://github.com/sanchit-gandhi&#34;&gt;Sanchit&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://github.com/ylacombe&#34;&gt;Yoach&lt;/a&gt; (alphabetical order) for continuing to maintain Whisper in Transformers.&lt;/li&gt; &#xA; &lt;li&gt;Hugging Face &lt;a href=&#34;https://github.com/huggingface/optimum&#34;&gt;Optimum&lt;/a&gt; team for making the BetterTransformer API so easily accessible.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/patrick91&#34;&gt;Patrick Arminio&lt;/a&gt; for helping me tremendously to put together this CLI.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Community showcase&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;@ochen1 created a brilliant MVP for a CLI here: &lt;a href=&#34;https://github.com/ochen1/insanely-fast-whisper-cli&#34;&gt;https://github.com/ochen1/insanely-fast-whisper-cli&lt;/a&gt; (Try it out now!)&lt;/li&gt; &#xA; &lt;li&gt;@arihanv created an app (Shush) using NextJS (Frontend) &amp;amp; Modal (Backend): &lt;a href=&#34;https://github.com/arihanv/Shush&#34;&gt;https://github.com/arihanv/Shush&lt;/a&gt; (Check it outtt!)&lt;/li&gt; &#xA; &lt;li&gt;@kadirnar created a python package on top of the transformers with optimisations: &lt;a href=&#34;https://github.com/kadirnar/whisper-plus&#34;&gt;https://github.com/kadirnar/whisper-plus&lt;/a&gt; (Go go go!!!)&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>huggingface/blog</title>
    <updated>2024-02-25T01:49:05Z</updated>
    <id>tag:github.com,2024-02-25:/huggingface/blog</id>
    <link href="https://github.com/huggingface/blog" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Public repo for HF blog posts&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;The Hugging Face Blog Repository ü§ó&lt;/h1&gt; &#xA;&lt;p&gt;This is the official repository of the &lt;a href=&#34;https://hf.co/blog&#34;&gt;Hugging Face Blog&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;If you are an external contributor&lt;/strong&gt;: If your blog post is not a collaboration post with Hugging Face, please consider creating a &lt;a href=&#34;https://huggingface.co/blog-explorers&#34;&gt;community blog&lt;/a&gt; instead. Community blog posts appear on our blogs main page just like the blogs in this repository.&lt;/p&gt; &#xA;&lt;h2&gt;How to write an article? üìù&lt;/h2&gt; &#xA;&lt;p&gt;1Ô∏è‚É£ Create a branch &lt;code&gt;YourName/Title&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;2Ô∏è‚É£ Create a md (markdown) file, &lt;strong&gt;use a short file name&lt;/strong&gt;. For instance, if your title is &#34;Introduction to Deep Reinforcement Learning&#34;, the md file name could be &lt;code&gt;intro-rl.md&lt;/code&gt;. This is important because the &lt;strong&gt;file name will be the blogpost&#39;s URL&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;3Ô∏è‚É£ Create a new folder in &lt;code&gt;assets&lt;/code&gt;. Use the same name as the name of the md file. Optionally you may add a numerical prefix to that folder, using the number that hasn&#39;t been used yet. But this is no longer required. i.e. the asset folder in this example could be &lt;code&gt;123_intro-rl&lt;/code&gt; or &lt;code&gt;intro-rl&lt;/code&gt;. This folder will contain &lt;strong&gt;your thumbnail only&lt;/strong&gt;. The folder number is mostly for (rough) ordering purposes, so it&#39;s no big deal if two concurrent articles use the same number.&lt;/p&gt; &#xA;&lt;p&gt;For the rest of your files, create a mirrored folder in the HuggingFace Documentation Images &lt;a href=&#34;https://huggingface.co/datasets/huggingface/documentation-images/tree/main/blog&#34;&gt;repo&lt;/a&gt;. This is to reduce bloat in the GitHub base repo when cloning and pulling.&lt;/p&gt; &#xA;&lt;p&gt;üñºÔ∏è: In terms of images, &lt;strong&gt;try to have small files&lt;/strong&gt; to avoid having a slow loading user experience:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Use compressed images, you can use this website: &lt;a href=&#34;https://tinypng.com&#34;&gt;https://tinypng.com&lt;/a&gt; or &lt;a href=&#34;https://www.iloveimg.com/compress-image&#34;&gt;https://www.iloveimg.com/compress-image&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;4Ô∏è‚É£ Copy and paste this to your md file and change the elements&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;title&lt;/li&gt; &#xA; &lt;li&gt;thumbnail&lt;/li&gt; &#xA; &lt;li&gt;authors&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;---&#xA;title: &#34;PUT YOUR TITLE HERE&#34; &#xA;thumbnail: /blog/assets/101_decision-transformers-train/thumbnail.gif&#xA;authors:&#xA;- user: your_hf_user&#xA;- user: your_coauthor&#xA;---&#xA;&#xA;# Train your first Decision Transformer&#xA;&#xA;Your content here [...]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When published, the Hub will insert the following UI elements right after the blogpost&#39;s main header (i.e. the line that starts with a single &lt;code&gt;#&lt;/code&gt;, aka. the &lt;code&gt;&amp;lt;h1&amp;gt;&lt;/code&gt;):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&#34;Published on [date]&#34;&lt;/li&gt; &#xA; &lt;li&gt;&#34;Update on GitHub&#34; button&lt;/li&gt; &#xA; &lt;li&gt;avatars of the authors that were listed in authors.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;5Ô∏è‚É£ Then, you can add your content. It&#39;s markdown system so if you wrote your text on notion just control shift v to copy/paste as markdown.&lt;/p&gt; &#xA;&lt;p&gt;6Ô∏è‚É£ Modify &lt;code&gt;_blog.yml&lt;/code&gt; to add your blogpost.&lt;/p&gt; &#xA;&lt;p&gt;7Ô∏è‚É£ When your article is ready, &lt;strong&gt;open a pull request&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;8Ô∏è‚É£ To check how your blog will look like before merging it, check out the &lt;a href=&#34;https://github.com/huggingface/moon-landing/tree/main#codespace&#34;&gt;CodeSpace instructions&lt;/a&gt; (internal for HF team)&lt;/p&gt; &#xA;&lt;p&gt;9Ô∏è‚É£ The article will be &lt;strong&gt;published automatically when you merge your pull request&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;How to get a nice responsive thumbnail?&lt;/h2&gt; &#xA;&lt;p&gt;1Ô∏è‚É£ Create a &lt;code&gt;1300x650&lt;/code&gt; image&lt;/p&gt; &#xA;&lt;p&gt;2Ô∏è‚É£ Use &lt;a href=&#34;https://github.com/huggingface/blog/raw/main/assets/thumbnail-template.svg&#34;&gt;this template&lt;/a&gt; and fill the content part.&lt;/p&gt; &#xA;&lt;p&gt;‚û°Ô∏è Or select a background you like and follow the instructions in &lt;a href=&#34;https://www.figma.com/file/sXrf9VtkkbWI7kCIesMkDY/HF-Blog-Template?node-id=351%3A39&#34;&gt;this Figma template&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Using LaTeX&lt;/h2&gt; &#xA;&lt;p&gt;Just add:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;\\(your_latex_here\\)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For instance:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;\\( Q(S_t, A_t) \\)&lt;/code&gt; ‚û°Ô∏è $Q(S_t, A_t)$&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Stability-AI/StableCascade</title>
    <updated>2024-02-25T01:49:05Z</updated>
    <id>tag:github.com,2024-02-25:/Stability-AI/StableCascade</id>
    <link href="https://github.com/Stability-AI/StableCascade" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official Code for Stable Cascade&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Stable Cascade&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/figures/collage_1.jpg&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;This is the official codebase for &lt;strong&gt;Stable Cascade&lt;/strong&gt;. We provide training &amp;amp; inference scripts, as well as a variety of different models you can use. &lt;br&gt;&lt;br&gt; This model is built upon the &lt;a href=&#34;https://openreview.net/forum?id=gU58d5QeGv&#34;&gt;W√ºrstchen&lt;/a&gt; architecture and its main difference to other models, like Stable Diffusion, is that it is working at a much smaller latent space. Why is this important? The smaller the latent space, the &lt;strong&gt;faster&lt;/strong&gt; you can run inference and the &lt;strong&gt;cheaper&lt;/strong&gt; the training becomes. How small is the latent space? Stable Diffusion uses a compression factor of 8, resulting in a 1024x1024 image being encoded to 128x128. Stable Cascade achieves a compression factor of 42, meaning that it is possible to encode a 1024x1024 image to 24x24, while maintaining crisp reconstructions. The text-conditional model is then trained in the highly compressed latent space. Previous versions of this architecture, achieved a 16x cost reduction over Stable Diffusion 1.5. &lt;br&gt; &lt;br&gt; Therefore, this kind of model is well suited for usages where efficiency is important. Furthermore, all known extensions like finetuning, LoRA, ControlNet, IP-Adapter, LCM etc. are possible with this method as well. A few of those are already provided (finetuning, ControlNet, LoRA) in the &lt;a href=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/train&#34;&gt;training&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/inference&#34;&gt;inference&lt;/a&gt; sections.&lt;/p&gt; &#xA;&lt;p&gt;Moreover, Stable Cascade achieves impressive results, both visually and evaluation wise. According to our evaluation, Stable Cascade performs best in both prompt alignment and aesthetic quality in almost all comparisons. The above picture shows the results from a human evaluation using a mix of parti-prompts (link) and aesthetic prompts. Specifically, Stable Cascade (30 inference steps) was compared against Playground v2 (50 inference steps), SDXL (50 inference steps), SDXL Turbo (1 inference step) and W√ºrstchen v2 (30 inference steps). &lt;br&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img height=&#34;300&#34; src=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/figures/comparison.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Stable Cascade¬¥s focus on efficiency is evidenced through its architecture and a higher compressed latent space. Despite the largest model containing 1.4 billion parameters more than Stable Diffusion XL, it still features faster inference times, as can be seen in the figure below.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img height=&#34;300&#34; src=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/figures/comparison-inference-speed.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/figures/collage_2.jpg&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Model Overview&lt;/h2&gt; &#xA;&lt;p&gt;Stable Cascade consists of three models: Stage A, Stage B and Stage C, representing a cascade for generating images, hence the name &#34;Stable Cascade&#34;. Stage A &amp;amp; B are used to compress images, similarly to what the job of the VAE is in Stable Diffusion. However, as mentioned before, with this setup a much higher compression of images can be achieved. Furthermore, Stage C is responsible for generating the small 24 x 24 latents given a text prompt. The following picture shows this visually. Note that Stage A is a VAE and both Stage B &amp;amp; C are diffusion models.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/figures/model-overview.jpg&#34; width=&#34;600&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;For this release, we are providing two checkpoints for Stage C, two for Stage B and one for Stage A. Stage C comes with a 1 billion and 3.6 billion parameter version, but we highly recommend using the 3.6 billion version, as most work was put into its finetuning. The two versions for Stage B amount to 700 million and 1.5 billion parameters. Both achieve great results, however the 1.5 billion excels at reconstructing small and fine details. Therefore, you will achieve the best results if you use the larger variant of each. Lastly, Stage A contains 20 million parameters and is fixed due to its small size.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;This section will briefly outline how you can get started with &lt;strong&gt;Stable Cascade&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Inference&lt;/h3&gt; &#xA;&lt;p&gt;Running the model can be done through the notebooks provided in the &lt;a href=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/inference&#34;&gt;inference&lt;/a&gt; section. You will find more details regarding downloading the models, compute requirements as well as some tutorials on how to use the models. Specifically, there are four notebooks provided for the following use-cases:&lt;/p&gt; &#xA;&lt;h4&gt;Text-to-Image&lt;/h4&gt; &#xA;&lt;p&gt;A compact &lt;a href=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/inference/text_to_image.ipynb&#34;&gt;notebook&lt;/a&gt; that provides you with basic functionality for text-to-image, image-variation and image-to-image.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Text-to-Image&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;code&gt;Cinematic photo of an anthropomorphic penguin sitting in a cafe reading a book and having a coffee.&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/figures/text-to-image-example-penguin.jpg&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Image Variation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The model can also understand image embeddings, which makes it possible to generate variations of a given image (left). There was no prompt given here.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/figures/image-variations-example-headset.jpg&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Image-to-Image&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This works just as usual, by noising an image up to a specific point and then letting the model generate from that starting point. Here the left image is noised to 80% and the caption is: &lt;code&gt;A person riding a rodent.&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/figures/image-to-image-example-rodent.jpg&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Furthermore, the model is also accessible in the diffusers ü§ó library. You can find the documentation and usage &lt;a href=&#34;https://huggingface.co/stabilityai/stable-cascade&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;ControlNet&lt;/h4&gt; &#xA;&lt;p&gt;This &lt;a href=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/inference/controlnet.ipynb&#34;&gt;notebook&lt;/a&gt; shows how to use ControlNets that were trained by us or how to use one that you trained yourself for Stable Cascade. With this release, we provide the following ControlNets:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Inpainting / Outpainting&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/figures/controlnet-paint.jpg&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Face Identity&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/figures/controlnet-face.jpg&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The Face Identity ControlNet will be released at a later point.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Canny&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/figures/controlnet-canny.jpg&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Super Resolution&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/figures/controlnet-sr.jpg&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;These can all be used through the same notebook and only require changing the config for each ControlNet. More information is provided in the &lt;a href=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/inference&#34;&gt;inference guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;LoRA&lt;/h4&gt; &#xA;&lt;p&gt;We also provide our own implementation for training and using LoRAs with Stable Cascade, which can be used to finetune the text-conditional model (Stage C). Specifically, you can add and learn new tokens and add LoRA layers to the model. This &lt;a href=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/inference/lora.ipynb&#34;&gt;notebook&lt;/a&gt; shows how you can use a trained LoRA. For example, training a LoRA on my dog with the following kind of training images:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/figures/fernando_original.jpg&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Lets me generate the following images of my dog given the prompt: &lt;code&gt;Cinematic photo of a dog [fernando] wearing a space suit.&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/figures/fernando.jpg&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;h4&gt;Image Reconstruction&lt;/h4&gt; &#xA;&lt;p&gt;Lastly, one thing that might be very interesting for people, especially if you want to train your own text-conditional model from scratch, maybe even with a completely different architecture than our Stage C, is to use the (Diffusion) Autoencoder that Stable Cascade uses to be able to work in the highly compressed space. Just like people use Stable Diffusion&#39;s VAE to train their own models (e.g. Dalle3), you could use Stage A &amp;amp; B in the same way, while benefiting from a much higher compression, allowing you to train and run models faster. &lt;br&gt; The notebook shows how to encode and decode images and what specific benefits you get. For example, say you have the following batch of images of dimension &lt;code&gt;4 x 3 x 1024 x 1024&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/figures/original.jpg&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;You can encode these images to a compressed size of &lt;code&gt;4 x 16 x 24 x 24&lt;/code&gt;, giving you a spatial compression factor of &lt;code&gt;1024 / 24 = 42.67&lt;/code&gt;. Afterwards you can use Stage A &amp;amp; B to decode the images back to &lt;code&gt;4 x 3 x 1024 x 1024&lt;/code&gt;, giving you the following output:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/figures/reconstructed.jpg&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;As you can see, the reconstructions are surprisingly close, even for small details. Such reconstructions are not possible with a standard VAE etc. The &lt;a href=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/inference/reconstruct_images.ipynb&#34;&gt;notebook&lt;/a&gt; gives you more information and easy code to try it out.&lt;/p&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;p&gt;We provide code for training Stable Cascade from scratch, finetuning, ControlNet and LoRA. You can find a comprehensive explanation for how to do so in the &lt;a href=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/train&#34;&gt;training folder&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Remarks&lt;/h2&gt; &#xA;&lt;p&gt;The codebase is in early development. You might encounter unexpected errors or not perfectly optimized training and inference code. We apologize for that in advance. If there is interest, we will continue releasing updates to it, aiming to bring in the latest improvements and optimizations. Moreover, we would be more than happy to receive ideas, feedback or even updates from people that would like to contribute. Cheers.&lt;/p&gt; &#xA;&lt;h2&gt;Gradio App&lt;/h2&gt; &#xA;&lt;p&gt;First install gradio and diffusers by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip3 install gradio&#xA;pip3 install accelerate # optionally&#xA;pip3 install git+https://github.com/kashif/diffusers.git@wuerstchen-v3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then from the root of the project run this command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;PYTHONPATH=./ python3 gradio_app/app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{pernias2023wuerstchen,&#xA;      title={Wuerstchen: An Efficient Architecture for Large-Scale Text-to-Image Diffusion Models}, &#xA;      author={Pablo Pernias and Dominic Rampas and Mats L. Richter and Christopher J. Pal and Marc Aubreville},&#xA;      year={2023},&#xA;      eprint={2306.00637},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;LICENSE&lt;/h2&gt; &#xA;&lt;p&gt;All the code from this repo is under an &lt;a href=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/LICENSE&#34;&gt;MIT LICENSE&lt;/a&gt;&lt;br&gt; The model weights, that you can get from Hugginface following &lt;a href=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/models/readme.md&#34;&gt;these instructions&lt;/a&gt;, are under a &lt;a href=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/WEIGHTS_LICENSE&#34;&gt;STABILITY AI NON-COMMERCIAL RESEARCH COMMUNITY LICENSE&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>