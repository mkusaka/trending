<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Dart Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-26T01:51:23Z</updated>
  <subtitle>Weekly Trending of Dart in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>xiaoyaocz/dart_simple_live</title>
    <updated>2023-03-26T01:51:23Z</updated>
    <id>tag:github.com,2023-03-26:/xiaoyaocz/dart_simple_live</id>
    <link href="https://github.com/xiaoyaocz/dart_simple_live" rel="alternate"></link>
    <summary type="html">&lt;p&gt;简简单单的看直播&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;128&#34; src=&#34;https://raw.githubusercontent.com/xiaoyaocz/dart_simple_live/master/assets/logo.png&#34; alt=&#34;Simple Live logo&#34;&gt; &lt;/p&gt; &#xA;&lt;h2 align=&#34;center&#34;&gt;Simple Live&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; 获取各个平台的直播信息及弹幕,基于&lt;a href=&#34;https://github.com/xiaoyaocz/AllLive&#34;&gt;AllLive&lt;/a&gt;项目实现 &lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xiaoyaocz/dart_simple_live/master/assets/screenshot_light.jpg&#34; alt=&#34;浅色模式&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xiaoyaocz/dart_simple_live/master/assets/screenshot_dark.jpg&#34; alt=&#34;深色模式&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;支持直播平台：&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;虎牙直播&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;斗鱼直播&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;哔哩哔哩直播&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;APP支持平台&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Android&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; iOS&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Windows &lt;code&gt;TODO&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; MacOS &lt;code&gt;TODO&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Linux &lt;code&gt;TODO&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;请到&lt;a href=&#34;https://github.com/xiaoyaocz/dart_simple_live/releases&#34;&gt;Releases&lt;/a&gt;下载最新版本，iOS请下载ipa文件自行签名安装&lt;/p&gt; &#xA;&lt;h2&gt;项目结构&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;simple_live_core&lt;/code&gt; 项目核心库，实现获取各个网站的信息及弹幕。&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;simple_live_console&lt;/code&gt; 基于simple_live_core的控制台程序。&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;simple_live_app&lt;/code&gt; 基于核心库实现的Flutter APP客户端。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;参考及引用&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/xiaoyaocz/AllLive&#34;&gt;AllLive&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/xiaoyaocz/dart_tars_protocol.git&#34;&gt;dart_tars_protocol&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/wbt5/real-url&#34;&gt;wbt5/real-url&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/lovelyyoshino/Bilibili-Live-API/raw/master/API.WebSocket.md&#34;&gt;lovelyyoshino/Bilibili-Live-API&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/IsoaSFlus/danmaku&#34;&gt;IsoaSFlus/danmaku&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/BacooTang/huya-danmu&#34;&gt;BacooTang/huya-danmu&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/TarsCloud/Tars&#34;&gt;TarsCloud/Tars&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;声明&lt;/h2&gt; &#xA;&lt;p&gt;本项目仅用于学习交流编程技术，严禁将本项目用于商业目的。如有任何商业行为，均与本项目无关。&lt;/p&gt; &#xA;&lt;p&gt;如果本项目存在侵犯您的合法权益的情况，请及时与开发者联系，开发者将会及时删除有关内容。&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>anasfik/openai</title>
    <updated>2023-03-26T01:51:23Z</updated>
    <id>tag:github.com,2023-03-26:/anasfik/openai</id>
    <link href="https://github.com/anasfik/openai" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Dart/Flutter SDK for ChatGPT and all OpenAI APIs (GPT, Dall-e..)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;NEW: ChatGPT &amp;amp; Whisper APIs are &lt;a href=&#34;https://raw.githubusercontent.com/anasfik/openai/main/#chat-chatgpt&#34;&gt;added&lt;/a&gt; to the library and can be used directly.&lt;/h1&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img alt=&#34;GitHub commit activity&#34; src=&#34;https://img.shields.io/github/commit-activity/m/anasfik/openai&#34;&gt; &lt;img alt=&#34;GitHub contributors&#34; src=&#34;https://img.shields.io/github/contributors/anasfik/openai&#34;&gt; &lt;img alt=&#34;GitHub Repo stars&#34; src=&#34;https://img.shields.io/github/stars/anasfik/openai?style=social&#34;&gt; &lt;img alt=&#34;GitHub Workflow Status&#34; src=&#34;https://img.shields.io/github/actions/workflow/status/anasfik/openai/dart.yml?label=tests&#34;&gt; &lt;img alt=&#34;GitHub Workflow Status&#34; src=&#34;https://img.shields.io/github/actions/workflow/status/anasfik/openai/release.yml?label=build&#34;&gt; &lt;img alt=&#34;GitHub&#34; src=&#34;https://img.shields.io/github/license/anasfik/openai&#34;&gt; &lt;img alt=&#34;Pub Version&#34; src=&#34;https://img.shields.io/pub/v/dart_openai&#34;&gt; &lt;img alt=&#34;Pub Likes&#34; src=&#34;https://img.shields.io/pub/likes/dart_openai&#34;&gt; &lt;img alt=&#34;Pub Points&#34; src=&#34;https://img.shields.io/pub/points/dart_openai&#34;&gt; &lt;img alt=&#34;Pub Popularity&#34; src=&#34;https://img.shields.io/pub/popularity/dart_openai&#34;&gt; &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h3&gt;Help this grow and get discovered by other devs with a ⭐ star.&lt;/h3&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;An open-source Client package that allows developers to easily integrate the power of OpenAI&#39;s state-of-the-art AI models into their Dart/Flutter applications.&lt;/p&gt; &#xA;&lt;p&gt;This library provides simple and intuitive methods for making requests to OpenAI&#39;s various APIs, including the GPT-3 language model, DALL-E image generation, and more.&lt;/p&gt; &#xA;&lt;p&gt;The package is designed to be lightweight and easy to use, so you can focus on building your application, rather than worrying about the complexities and errors caused by dealing with HTTP requests.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;i&gt;Unofficial&lt;/i&gt; &lt;br&gt; &lt;i&gt;OpenAI does not have any official Dart library.&lt;/i&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Note:&lt;/h3&gt; &#xA;&lt;p&gt;Please note that this client SDK connects directly to &lt;a href=&#34;https://platform.openai.com/docs/introduction/overview&#34;&gt;OpenAI APIs&lt;/a&gt; using HTTP requests.&lt;/p&gt; &#xA;&lt;h2&gt;✨ Key Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Easy to use methods that reflect exactly the OpenAI documentation, with additional functionalities that make it better to use with Dart Programming Language.&lt;/li&gt; &#xA; &lt;li&gt;Authorize just once, use it anywhere and at any time in your application.&lt;/li&gt; &#xA; &lt;li&gt;Developer-friendly.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Stream&lt;/code&gt; functionality for completions API &amp;amp; fine-tune events API.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;👑 Code Progress (100 %)&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/anasfik/openai/main/#authentication&#34;&gt;Authentication&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/anasfik/openai/main/#models&#34;&gt;Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/anasfik/openai/main/#completions&#34;&gt;Completions&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; With &lt;code&gt;Stream&lt;/code&gt; responses.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/anasfik/openai/main/#chat-chatgpt&#34;&gt;Chat (chatGPT)&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; With &lt;code&gt;Stream&lt;/code&gt; responses.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/anasfik/openai/main/#edits&#34;&gt;Edits&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/anasfik/openai/main/#images&#34;&gt;Images&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/anasfik/openai/main/#embeddings&#34;&gt;Embeddings&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/anasfik/openai/main/#audio&#34;&gt;Audio&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/anasfik/openai/main/#files&#34;&gt;Files&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/anasfik/openai/main/#fine-tunes&#34;&gt;Fine-tunes&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; With events &lt;code&gt;Stream&lt;/code&gt; responses.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/anasfik/openai/main/#moderations&#34;&gt;Moderation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;💫 Testing Progress (100 %)&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/anasfik/openai/main/#authentication&#34;&gt;Authentication&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/anasfik/openai/main/#models&#34;&gt;Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/anasfik/openai/main/#completions&#34;&gt;Completions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/anasfik/openai/main/#chat-chatgpt&#34;&gt;chat (chatGPT)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/anasfik/openai/main/#edits&#34;&gt;Edits&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/anasfik/openai/main/#images&#34;&gt;Images&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/anasfik/openai/main/#embeddings&#34;&gt;Embeddings&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/anasfik/openai/main/#audio&#34;&gt;Audio&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/anasfik/openai/main/#files&#34;&gt;Files&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/anasfik/openai/main/#fine-tunes&#34;&gt;Fine-tunes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/anasfik/openai/main/#moderations&#34;&gt;Moderation&lt;/a&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;📜 Full Documentation:&lt;/h1&gt; &#xA;&lt;p&gt;For the full documentation about all members this library offers, &lt;a href=&#34;https://pub.dev/documentation/dart_openai/latest/&#34;&gt;check here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;🟢 Usage&lt;/h1&gt; &#xA;&lt;h2&gt;Authentication&lt;/h2&gt; &#xA;&lt;h3&gt;API key&lt;/h3&gt; &#xA;&lt;p&gt;The OpenAI API uses API keys for authentication. you can get your account APU key by visiting &lt;a href=&#34;https://platform.openai.com/account/api-keys&#34;&gt;API keys&lt;/a&gt; of your account.&lt;/p&gt; &#xA;&lt;p&gt;We highly recommend loading your secret key at runtime from a &lt;code&gt;.env&lt;/code&gt; file, you can use the &lt;a href=&#34;https://pub.dev/packages/envied&#34;&gt;envied&lt;/a&gt; package.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;// .env&#xA;OPEN_AI_API_KEY=&amp;lt;REPLACE WITH YOUR API KEY&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-dart&#34;&gt;// lib/env/env.dart&#xA;import &#39;package:envied/envied.dart&#39;;&#xA;part &#39;env.g.dart&#39;;&#xA;&#xA;@Envied(path: &#34;.env&#34;)&#xA;abstract class Env {&#xA;  @EnviedField(varName: &#39;OPEN_AI_API_KEY&#39;) // the .env variable.&#xA;  static const apiKey = _Env.apiKey;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-dart&#34;&gt;// lib/main.dart&#xA;void main() {&#xA; OpenAI.apiKey = Env.apiKey; // Initializes the package with that API key&#xA; // ..&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;if no &lt;code&gt;apiKey&lt;/code&gt; is set, and you tried to access &lt;code&gt;OpenAI.instance&lt;/code&gt;, a &lt;code&gt;MissingApiKeyException&lt;/code&gt; will be thrown even before making the actual request.&lt;/p&gt; &#xA;&lt;p&gt;if the &lt;code&gt;apiKey&lt;/code&gt; is set, but it is invalid when making requests, a &lt;code&gt;RequestFailedException&lt;/code&gt; will be thrown in your app, check the &lt;a href=&#34;https://raw.githubusercontent.com/anasfik/openai/main/#error-handling&#34;&gt;error handling&lt;/a&gt; section for more info.&lt;/p&gt; &#xA;&lt;h3&gt;Setting an organization&lt;/h3&gt; &#xA;&lt;p&gt;if you belong to a specific organization, you can pass its id to &lt;code&gt;OpenAI.organization&lt;/code&gt; like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-dart&#34;&gt; OpenAI.organization = &#34;ORGANIZATION ID&#34;;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you don&#39;t belong actually to any organization, you can just ignore this section, or set it to &lt;code&gt;null&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://platform.openai.com/docs/api-reference/authentication&#34;&gt;Learn More From Here.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Models&lt;/h2&gt; &#xA;&lt;h3&gt;List Models&lt;/h3&gt; &#xA;&lt;p&gt;Lists the currently available models, and provides basic information about each one such as the owner and availability.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-dart&#34;&gt; List&amp;lt;OpenAIModelModel&amp;gt; models = await OpenAI.instance.model.list();&#xA; OpenAIModelModel firstModel = models.first;&#xA;&#xA; print(firstModel.id); // ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Retrieve model.&lt;/h3&gt; &#xA;&lt;p&gt;Retrieves a single model by its id and gets additional pieces of information about it.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-dart&#34;&gt; OpenAIModelModel model = await OpenAI.instance.model.retrieve(&#34;text-davinci-003&#34;);&#xA; print(model.id)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If the model id does not exist, a &lt;code&gt;RequestFailedException&lt;/code&gt; will be thrown, check &lt;a href=&#34;https://raw.githubusercontent.com/anasfik/openai/main/#error-handling&#34;&gt;Error Handling&lt;/a&gt; section.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://platform.openai.com/docs/api-reference/models&#34;&gt;Learn More From Here.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Completions&lt;/h2&gt; &#xA;&lt;h3&gt;Create completion&lt;/h3&gt; &#xA;&lt;p&gt;Creates a Completion based on the provided properties &lt;code&gt;model&lt;/code&gt;, &lt;code&gt;prompt&lt;/code&gt; &amp;amp; other properties.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-dart&#34;&gt;OpenAICompletionModel completion = await OpenAI.instance.completion.create(&#xA;  model: &#34;text-davinci-003&#34;,&#xA;  prompt: &#34;Dart is a progr&#34;,&#xA;  maxTokens: 20,&#xA;  temperature: 0.5,&#xA;  n: 1,&#xA;  stop: [&#34;\n&#34;],&#xA;  echo: true,&#xA;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;if the request failed (as an example, if you did pass an invalid model id...), a &lt;code&gt;RequestFailedException&lt;/code&gt; will be thrown, check &lt;a href=&#34;https://raw.githubusercontent.com/anasfik/openai/main/#error-handling&#34;&gt;Error Handling&lt;/a&gt; section.&lt;/p&gt; &#xA;&lt;h3&gt;Create Completion Stream&lt;/h3&gt; &#xA;&lt;p&gt;In addition to calling the &lt;code&gt;OpenAI.instance.completion.create()&lt;/code&gt; which is a &lt;code&gt;Future&lt;/code&gt; and will not return an actual value until the completion is ended, you can get a &lt;code&gt;Stream&lt;/code&gt; of values as they are generated:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-dart&#34;&gt;Stream&amp;lt;OpenAIStreamCompletionModel&amp;gt; completionStream = OpenAI.instance.completion.createStream(&#xA;  model: &#34;text-davinci-003&#34;,&#xA;  prompt: &#34;Github is &#34;,&#xA;  maxTokens: 100,&#xA;  temperature: 0.5,&#xA;  topP: 1,&#xA; );&#xA;&#xA;completionStream.listen((event) {&#xA; final firstCompletionChoice = event.choices.first;&#xA; print(firstCompletionChoice.text); // ...&#xA;});&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://platform.openai.com/docs/api-reference/completions&#34;&gt;Learn More From Here.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Chat (ChatGPT)&lt;/h2&gt; &#xA;&lt;h3&gt;Create chat completion&lt;/h3&gt; &#xA;&lt;p&gt;Creates a completion for the chat message, note you need to set each message as a &lt;code&gt;OpenAIChatCompletionChoiceMessageModel&lt;/code&gt; object.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-dart&#34;&gt;OpenAIChatCompletionModel chatCompletion = await OpenAI.instance.chat.create(&#xA;    model: &#34;gpt-3.5-turbo&#34;,&#xA;    messages: [&#xA;      OpenAIChatCompletionChoiceMessageModel(content: &#34;hello, what is Flutter and Dart ?&#34;, role: &#34;user&#34;),&#xA;    ],&#xA;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Create a chat completion stream&lt;/h3&gt; &#xA;&lt;p&gt;in addition to calling &lt;code&gt;OpenAI.instance.chat.create()&lt;/code&gt; which is a &lt;code&gt;Future&lt;/code&gt; and will resolve only after the whole chat is generated, you can get a &lt;code&gt;Stream&lt;/code&gt; of results:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-dart&#34;&gt;OpenAIStreamChatCompletionModel chatStream = OpenAI.instance.chat.createStream(&#xA;    model: &#34;gpt-3.5-turbo&#34;,&#xA;    messages: [&#xA;      OpenAIChatCompletionChoiceMessageModel(&#xA;        content: &#34;hello&#34;,&#xA;        role: &#34;user&#34;,&#xA;      )&#xA;    ],&#xA;  );&#xA;&#xA;chatStream.listen((chatStreamEvent) {&#xA;print(chatStreamEvent); // ...&#xA;  })&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Edits&lt;/h2&gt; &#xA;&lt;h3&gt;Create edit&lt;/h3&gt; &#xA;&lt;p&gt;Creates an edited version of the given prompt based on the used model.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-dart&#34;&gt;OpenAIEditModel edit = await OpenAI.instance.edit.create(&#xA;model: &#34;text-davinci-edit-001&#34;;&#xA;instruction: &#34;remote all &#39;!&#39;from input text&#34;,&#xA;input: &#34;Hello!!, I! need to be ! somethi!ng&#34;&#xA;n: 1,&#xA;temperature: 0.8,&#xA;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://platform.openai.com/docs/api-reference/edits&#34;&gt;Learn More From Here.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Images&lt;/h2&gt; &#xA;&lt;h3&gt;Create image&lt;/h3&gt; &#xA;&lt;p&gt;Generates a new image based on a prompt given.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-dart&#34;&gt; OpenAIImageModel image = await OpenAI.instance.image.create(&#xA;  prompt: &#39;an astronaut on the sea&#39;,&#xA;  n: 1,&#xA;  size: OpenAIImageSize.size1024,&#xA;  responseFormat: OpenAIResponseFormat.url,&#xA;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Create image edit&lt;/h3&gt; &#xA;&lt;p&gt;Creates an edited or extended image given an original image and a prompt.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-dart&#34;&gt;OpenAiImageEditModel imageEdit = await OpenAI.instance.image.edit(&#xA; file: File(/* IMAGE PATH HERE */),&#xA; mask: File(/* MASK PATH HERE */),&#xA; prompt: &#34;mask the image with a dinosaur&#34;,&#xA; n: 1,&#xA; size: OpenAIImageSize.size1024,&#xA; responseFormat: OpenAIResponseFormat.url,&#xA;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Create image variation&lt;/h3&gt; &#xA;&lt;p&gt;Creates a variation of a given image.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-dart&#34;&gt;OpenAIImageVariationModel imageVariation = await OpenAI.instance.image.variation(&#xA; image: File(/* IMAGE PATH HERE */),&#xA; n: 1,&#xA; size: OpenAIImageSize.size1024,&#xA; responseFormat: OpenAIResponseFormat.url,&#xA;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://platform.openai.com/docs/api-reference/images&#34;&gt;Learn More From Here.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Embeddings&lt;/h2&gt; &#xA;&lt;p&gt;Get a vector representation of a given input that can be easily consumed by machine learning models and algorithms.&lt;/p&gt; &#xA;&lt;h3&gt;Create embeddings&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-dart&#34;&gt;OpenAIEmbeddingsModel embeddings = await OpenAI.instance.embedding.create(&#xA; model: &#34;text-embedding-ada-002&#34;,&#xA; input: &#34;This is a text input just to test&#34;,&#xA;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://platform.openai.com/docs/api-reference/embeddings&#34;&gt;Learn More From Here.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;Audio&lt;/h1&gt; &#xA;&lt;h2&gt;Create transcription&lt;/h2&gt; &#xA;&lt;p&gt;for transcribing an audio &lt;code&gt;File&lt;/code&gt;, you can use the &lt;code&gt;createTranscription()&lt;/code&gt; method directly by providing the &lt;code&gt;file&lt;/code&gt; property:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-dart&#34;&gt;OpenAIAudioModel transcription = OpenAI.instance.audio.createTranscription(&#xA;  file: /* THE AUDIO FILE HERE */,&#xA;  model: &#34;whisper-1&#34;,&#xA;  responseFormat: OpenAIAudioResponseFormat.json,&#xA;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Create translation&lt;/h2&gt; &#xA;&lt;p&gt;to get access to the translation API, and translate an audio file to english, you can use the &lt;code&gt;createTranslation()&lt;/code&gt; method, by providing the `file`` property:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-dart&#34;&gt;OpenAIAudioModel translation = await OpenAI.instance.audio.createTranslation(&#xA;  file: /* THE AUDIO FILE HERE */,&#xA;  model: &#34;whisper-1&#34;,&#xA;  responseFormat: OpenAIAudioResponseFormat.text,&#xA;&#xA;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Files&lt;/h2&gt; &#xA;&lt;p&gt;Files are used to upload documents that can be used with features like &lt;a href=&#34;https://raw.githubusercontent.com/anasfik/openai/main/#fine-tunes&#34;&gt;Fine-tuning&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;List files&lt;/h3&gt; &#xA;&lt;p&gt;Get a list of all the uploaded files o-to your OpenAI account.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-dart&#34;&gt;List&amp;lt;OpenAIFileModel&amp;gt; files = await OpenAI.instance.file.list();&#xA;&#xA;print(files.first.fileName); // ...&#xA;print(files.first.id); // ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Upload file&lt;/h3&gt; &#xA;&lt;p&gt;Upload a file that contains document(s) to be used across various endpoints/features. Currently, the size of all the files uploaded by one organization can be up to 1 GB. Please contact us if you need to increase the storage limit.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-dart&#34;&gt;OpenAIFileModel uploadedFile = await OpenAI.instance.file.upload(&#xA; file: File(&#34;/* FILE PATH HERE */&#34;),&#xA; purpose: &#34;fine-tuning&#34;,&#xA;);&#xA;&#xA;print(uploadedFile.id); // ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Delete file&lt;/h3&gt; &#xA;&lt;p&gt;Deletes an existent file by it&#39;s id.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-dart&#34;&gt;bool isFileDeleted = await OpenAI.instance.file.delete(&#34;/* FILE ID */&#34;);&#xA;&#xA;print(isFileDeleted);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Retrieve file&lt;/h3&gt; &#xA;&lt;p&gt;Fetches for a single file by it&#39;s id and returns informations about it.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-dart&#34;&gt;OpenAIFileModel file = await OpenAI.instance.file.retrieve(&#34;FILE ID&#34;);&#xA;print(file.id);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Retrieve file content&lt;/h3&gt; &#xA;&lt;p&gt;Fetches for a single file content by it&#39;s id.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-dart&#34;&gt;dynamic fileContent  = await OpenAI.instance.file.retrieveContent(&#34;FILE ID&#34;);&#xA;&#xA;print(fileContent);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://platform.openai.com/docs/api-reference/files&#34;&gt;Learn More From Here.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Fine Tunes&lt;/h2&gt; &#xA;&lt;h3&gt;Create fine-tune&lt;/h3&gt; &#xA;&lt;p&gt;Creates a job that fine-tunes a specified model from a given dataset, and returns a fine-tuned object about the enqueued job.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-dart&#34;&gt;OpenAIFineTuneModel fineTune = await OpenAI.instance.fineTune.create(&#xA; trainingFile: &#34;FILE ID&#34;,&#xA;);&#xA;&#xA;print(fineTune.status); // ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;List fine-tunes&lt;/h3&gt; &#xA;&lt;p&gt;List your organization&#39;s fine-tuning jobs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-dart&#34;&gt;List&amp;lt;OpenAIFineTuneModel&amp;gt; fineTunes = await OpenAI.instance.fineTune.list();&#xA;&#xA;print(fineTunes.first); // ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Retrieve fine-tune&lt;/h3&gt; &#xA;&lt;p&gt;Retrieves a fine-tune by its id.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-dart&#34;&gt;OpenAIFineTuneModel fineTune = await OpenAI.instance.fineTune.retrieve(&#34;FINE TUNE ID&#34;);&#xA;&#xA;print(fineTune.id); // ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Cancel fine-tune&lt;/h3&gt; &#xA;&lt;p&gt;Cancels a fine-tune job by its id, and returns it.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-dart&#34;&gt;OpenAIFineTuneModel cancelledFineTune = await OpenAI.instance.fineTune.cancel(&#34;FINE TUNE ID&#34;);&#xA;&#xA;print(cancelledFineTune.status); // ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;List fine-tune events&lt;/h3&gt; &#xA;&lt;p&gt;Lists a single fine-tune progress events by it&#39;s id.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-dart&#34;&gt; List&amp;lt;OpenAIFineTuneEventModel&amp;gt; events = await OpenAI.instance.fineTune.listEvents(&#34;FINE TUNE ID&#34;);&#xA;&#xA; print(events.first.message); // ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Listen to fine-tune events &lt;code&gt;Stream&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Streams all events of a fine-tune job by its id, as they happen.&lt;/p&gt; &#xA;&lt;p&gt;This is a long-running operation that will not return until the fine-tune job is terminated.&lt;/p&gt; &#xA;&lt;p&gt;The stream will emit an event every time a new event is available.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-dart&#34;&gt;Stream&amp;lt;OpenAIFineTuneEventStreamModel&amp;gt; eventsStream = OpenAI.instance.fineTune.listEventsStream(&#34;FINE TUNE ID&#34;);&#xA;&#xA;eventsStream.listen((event) {&#xA; print(event.message);&#xA;});&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Delete fine-tune&lt;/h3&gt; &#xA;&lt;p&gt;Deletes a fine-tune job by its id.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-dart&#34;&gt; bool deleted = await OpenAI.instance.fineTune.delete(&#34;FINE TUNE ID&#34;);&#xA;&#xA;print(deleted); // ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://platform.openai.com/docs/api-reference/fine-tunes&#34;&gt;Learn More From Here.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Moderations&lt;/h2&gt; &#xA;&lt;h3&gt;Create moderation&lt;/h3&gt; &#xA;&lt;p&gt;Classifies if text violates OpenAI&#39;s Content Policy&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-dart&#34;&gt;OpenAIModerationModel moderation = await OpenAI.instance.moderation.create(&#xA;  input: &#34;I want to kill him&#34;,&#xA;);&#xA;&#xA;print(moderation.results); // ...&#xA;print(moderation.results.first.categories.hate); // ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://platform.openai.com/docs/api-reference/moderations&#34;&gt;Learn More From Here.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Error Handling&lt;/h2&gt; &#xA;&lt;p&gt;Any time an error happens from the OpenAI API ends (As Example: when you try to create an image variation from a non-image file.. , a &lt;code&gt;RequestFailedException&lt;/code&gt; will be thrown automatically inside your Flutter / Dart app, you can use a &lt;code&gt;try-catch&lt;/code&gt; to catch that error, and make an action based on it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-dart&#34;&gt;try {&#xA;&#xA;// This will throw an error.&#xA; final errorVariation = await OpenAI.instance.image.variation(&#xA;  image: File(/*PATH OF NON-IMAGE FILE*/),&#xA; );&#xA;} on RequestFailedException catch(e) {&#xA; print(e.message);&#xA; print(e.statusCode)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;h4&gt;Want To Help ?&lt;/h4&gt; &#xA;&lt;p&gt;Please, Just remember that any kind of help related to these tasks are welcome, for the sake of the community.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Writing documentation: if you see any class, property, method.. that you know what does and it is undocumented, please take from your time 2 minutes and help another developer that doesn&#39;t.&lt;/li&gt; &#xA; &lt;li&gt;Code Refactoring: I know this is my job not yours :), but if you can and want, you&#39;re more than welcome.&lt;/li&gt; &#xA; &lt;li&gt;Reviewing code: if it happens that there is a better way to make something happen in the SDK, please just let me know.&lt;/li&gt; &#xA; &lt;li&gt;if you tried any sample of use cases, examples of yours and wanted to include it in the examples/, please go ahead.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>RetroMusicPlayer/Paisa</title>
    <updated>2023-03-26T01:51:23Z</updated>
    <id>tag:github.com,2023-03-26:/RetroMusicPlayer/Paisa</id>
    <link href="https://github.com/RetroMusicPlayer/Paisa" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Expense manager for Android with Material Design&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://retromusic.app&#34;&gt; &lt;img src=&#34;assets\images\icon.png&#34; height=&#34;128&#34;&gt; &lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 align=&#34;center&#34;&gt;&lt;a href=&#34;https://retromusic.app&#34;&gt;Paisa - Expense Tracker&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;a href=&#34;https://retromusic.app&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/h4h13/paisa&#34; style=&#34;text-decoration:none&#34; area-label=&#34;flutter&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Platform-Flutter-blue&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://play.google.com/store/apps/details?id=dev.hemanths.paisa&#34; style=&#34;text-decoration:none&#34; area-label=&#34;flutter&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Download-Google%20Play-green&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/RetroMusicPlayer/Paisa/releases/tag/v3.1.1&#34; style=&#34;text-decoration:none&#34; area-label=&#34;flutter&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Version-3.1.1-orange&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;h2&gt; Material design expense manager&lt;/h2&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h3&gt;⚠ Join &lt;a href=&#34;https://t.me/app_paisa&#34;&gt;@paisa&lt;/a&gt; on Telegram for important updates&lt;/h3&gt; &#xA;&lt;h3&gt;Screen shots&lt;/h3&gt; &#xA;&lt;h4&gt;Mobile&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RetroMusicPlayer/Paisa/master/paisa-images/Screenshot_1667486038.png&#34; width=&#34;200&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RetroMusicPlayer/Paisa/master/paisa-images/Screenshot_1667486042.png&#34; width=&#34;200&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RetroMusicPlayer/Paisa/master/paisa-images/Screenshot_1667486044.png&#34; width=&#34;200&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RetroMusicPlayer/Paisa/master/paisa-images/Screenshot_1667486046.png&#34; width=&#34;200&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Home&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Accounts&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Categories&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Budget overview&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;Foldable&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RetroMusicPlayer/Paisa/master/paisa-images/Screenshot_1667485291.png&#34; width=&#34;200&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RetroMusicPlayer/Paisa/master/paisa-images/Screenshot_1667485297.png&#34; width=&#34;200&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RetroMusicPlayer/Paisa/master/paisa-images/Screenshot_1667485299.png&#34; width=&#34;200&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RetroMusicPlayer/Paisa/master/paisa-images/Screenshot_1667485301.png&#34; width=&#34;200&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Home&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Accounts&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Categories&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Budget overview&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;Tablet &amp;amp; Desktop&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RetroMusicPlayer/Paisa/master/paisa-images/Screenshot_1667485280.png&#34; width=&#34;200&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RetroMusicPlayer/Paisa/master/paisa-images/Screenshot_1667485342.png&#34; width=&#34;200&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RetroMusicPlayer/Paisa/master/paisa-images/Screenshot_1667485319.png&#34; width=&#34;200&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RetroMusicPlayer/Paisa/master/paisa-images/Screenshot_1667485320.png&#34; width=&#34;200&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Home&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Accounts&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Categories&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Budget overview&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Expense Tracking&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Tracking expenses, incomes &amp;amp; deposits&lt;/li&gt; &#xA; &lt;li&gt;Account &amp;amp; budget visw overview&lt;/li&gt; &#xA; &lt;li&gt;Manage categories&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;License&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;Copyright (c) 2022, Hemanth S&#xA;All rights reserved.&#xA;&#xA;This source code is licensed under the GPLv3-style license found in the&#xA;LICENSE file in the root directory of this source tree. &#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>