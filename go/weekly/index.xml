<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Go Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-12-11T01:42:02Z</updated>
  <subtitle>Weekly Trending of Go in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>thangchung/go-coffeeshop</title>
    <updated>2022-12-11T01:42:02Z</updated>
    <id>tag:github.com,2022-12-11:/thangchung/go-coffeeshop</id>
    <link href="https://github.com/thangchung/go-coffeeshop" rel="alternate"></link>
    <summary type="html">&lt;p&gt;☕ A practical event-driven microservices demo built with Golang. Nomad, Consul Connect, Vault, and Terraform for deployment&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;go-coffeeshop&lt;/h1&gt; &#xA;&lt;p&gt;A coffee shop application with event-driven microservices has been written in Golang. Nomad, Consul Connect, Vault, and Terraform for deployment&lt;/p&gt; &#xA;&lt;p&gt;Other version can be found at:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/thangchung/coffeeshop-on-nomad&#34;&gt;.NET CoffeeShop with Microservices approach&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/thangchung/coffeeshop-modular&#34;&gt;.NET CoffeeShop with Modular Monolith approach&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Technical stack&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Backend building blocks &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/grpc-ecosystem/grpc-gateway&#34;&gt;grpc-ecosystem/grpc-gateway/v2&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/labstack/echo&#34;&gt;labstack/echo/v4&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/rabbitmq/amqp091-go&#34;&gt;rabbitmq/amqp091-go&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/jackc/pgx&#34;&gt;jackc/pgx/v4&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/Masterminds/squirrel&#34;&gt;Masterminds/squirrel&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/georgysavva/scany&#34;&gt;georgysavva/scany&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/golang-migrate/migrate&#34;&gt;golang-migrate/migrate/v4&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Utils &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/ilyakaznacheev/cleanenv&#34;&gt;ilyakaznacheev/cleanenv&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/sirupsen/logrus&#34;&gt;sirupsen/logrus&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/samber/lo&#34;&gt;samber/lo&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;golang/glog&lt;/li&gt; &#xA;     &lt;li&gt;google/uuid&lt;/li&gt; &#xA;     &lt;li&gt;google.golang.org/genproto&lt;/li&gt; &#xA;     &lt;li&gt;google.golang.org/grpc&lt;/li&gt; &#xA;     &lt;li&gt;google.golang.org/protobuf&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Infrastructure &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Postgres, RabbitMQ&lt;/li&gt; &#xA;   &lt;li&gt;Hashicorp Nomad, Consul (Connect), Vault, Terraform&lt;/li&gt; &#xA;   &lt;li&gt;docker and docker-compose&lt;/li&gt; &#xA;   &lt;li&gt;devcontainer for reproducible development environment&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;CoffeeShop - Choreography Saga&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/thangchung/go-coffeeshop/main/docs/coffeeshop.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Services&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;No.&lt;/th&gt; &#xA;   &lt;th&gt;Service&lt;/th&gt; &#xA;   &lt;th&gt;URI&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;grpc-gateway&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://localhost:5000&#34;&gt;http://localhost:5000&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;product service&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://localhost:5001&#34;&gt;http://localhost:5001&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;counter service&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://localhost:5002&#34;&gt;http://localhost:5002&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;barista service&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://localhost:5003&#34;&gt;http://localhost:5003&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;kitchen service&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://localhost:5004&#34;&gt;http://localhost:5004&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;6&lt;/td&gt; &#xA;   &lt;td&gt;web&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://localhost:8080&#34;&gt;http://localhost:8080&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Starting project&lt;/h2&gt; &#xA;&lt;p&gt;Jump into &lt;code&gt;.devcontainer&lt;/code&gt;, then&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt; docker-compose -f docker-compose-full.yaml build&#xA;&amp;gt; docker-compose -f docker-compose-full.yaml up&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;From &lt;code&gt;vscode&lt;/code&gt; =&amp;gt; Press F1 =&amp;gt; Type &lt;code&gt;Simple Browser View&lt;/code&gt; =&amp;gt; Choose it and enter &lt;a href=&#34;http://localhost:8080&#34;&gt;http://localhost:8080&lt;/a&gt;. Enjoy!!!&lt;/p&gt; &#xA;&lt;h2&gt;Screenshots&lt;/h2&gt; &#xA;&lt;h3&gt;Home screen&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/thangchung/go-coffeeshop/main/docs/home_screen.png&#34; alt=&#34;home_screen&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Payment screen&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/thangchung/go-coffeeshop/main/docs/payment_screen.png&#34; alt=&#34;payment_screen&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Order list screen&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/thangchung/go-coffeeshop/main/docs/order_list_screen.png&#34; alt=&#34;order_list_screen&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;HashiCorp stack deployment&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/thangchung/go-coffeeshop/main/docs/coffeeshop_hashicorp.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The details of how to run it can be find at &lt;a href=&#34;https://raw.githubusercontent.com/thangchung/go-coffeeshop/main/build/README.md&#34;&gt;deployment with Nomad, Consult Connect and Vault&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Debug Apps&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/thangchung/go-coffeeshop/wiki/Golang#debug-app-in-monorepo&#34;&gt;Debug golang app in monorepo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Trouble shooting&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/thangchung/go-coffeeshop/wiki#trouble-shooting&#34;&gt;Development project trouble shooting&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Enhance project structure with DDD patterns&lt;/li&gt; &#xA; &lt;li&gt;Add testing&lt;/li&gt; &#xA; &lt;li&gt;Add and integrate with observability libs and tools&lt;/li&gt; &#xA; &lt;li&gt;Add user identity management (authentication and authorization)&lt;/li&gt; &#xA; &lt;li&gt;Add resiliency&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/golang-standards/project-layout&#34;&gt;project-layout&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://peter.bourgon.org/go-best-practices-2016/#repository-structure&#34;&gt;repository-structure&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/thockin/go-build-template&#34;&gt;go-build-template&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/evrone/go-clean-template&#34;&gt;go-clean-template&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/emsifa/tailwind-pos&#34;&gt;emsifa/tailwind-pos&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>seaweedfs/seaweedfs</title>
    <updated>2022-12-11T01:42:02Z</updated>
    <id>tag:github.com,2022-12-11:/seaweedfs/seaweedfs</id>
    <link href="https://github.com/seaweedfs/seaweedfs" rel="alternate"></link>
    <summary type="html">&lt;p&gt;SeaweedFS is a fast distributed storage system for blobs, objects, files, and data lake, for billions of files! Blob store has O(1) disk seek, cloud tiering. Filer supports Cloud Drive, cross-DC active-active replication, Kubernetes, POSIX FUSE mount, S3 API, S3 Gateway, Hadoop, WebDAV, encryption, Erasure Coding.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SeaweedFS&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://join.slack.com/t/seaweedfs/shared_invite/enQtMzI4MTMwMjU2MzA3LTEyYzZmZWYzOGQ3MDJlZWMzYmI0OTE4OTJiZjJjODBmMzUxNmYwODg0YjY3MTNlMjBmZDQ1NzQ5NDJhZWI2ZmY&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/slack-purple&#34; alt=&#34;Slack&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/intent/follow?screen_name=seaweedfs&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/seaweedfs.svg?style=social&amp;amp;label=Follow&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/seaweedfs/seaweedfs/actions/workflows/go.yml&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/workflow/status/chrislusf/seaweedfs/Go&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://godoc.org/github.com/seaweedfs/seaweedfs/weed&#34;&gt;&lt;img src=&#34;https://godoc.org/github.com/seaweedfs/seaweedfs/weed?status.svg?sanitize=true&#34; alt=&#34;GoDoc&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/seaweedfs/seaweedfs/wiki&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/docs-wiki-blue.svg?sanitize=true&#34; alt=&#34;Wiki&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/chrislusf/seaweedfs/&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/pulls/chrislusf/seaweedfs?maxAge=4800&#34; alt=&#34;Docker Pulls&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://search.maven.org/search?q=g:com.github.chrislusf&#34;&gt;&lt;img src=&#34;https://img.shields.io/maven-central/v/com.github.chrislusf/seaweedfs-client&#34; alt=&#34;SeaweedFS on Maven Central&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/note/seaweedfs.png&#34; alt=&#34;SeaweedFS Logo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2 align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.patreon.com/seaweedfs&#34;&gt;Sponsor SeaweedFS via Patreon&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;SeaweedFS is an independent Apache-licensed open source project with its ongoing development made possible entirely thanks to the support of these awesome &lt;a href=&#34;https://github.com/seaweedfs/seaweedfs/raw/master/backers.md&#34;&gt;backers&lt;/a&gt;. If you&#39;d like to grow SeaweedFS even stronger, please consider joining our &lt;a href=&#34;https://www.patreon.com/seaweedfs&#34;&gt;sponsors on Patreon&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Your support will be really appreciated by me and other supporters!&lt;/p&gt; &#xA;&lt;!--&#xA;&lt;h4 align=&#34;center&#34;&gt;Platinum&lt;/h4&gt;&#xA;&#xA;&lt;p align=&#34;center&#34;&gt;&#xA;  &lt;a href=&#34;&#34; target=&#34;_blank&#34;&gt;&#xA;    Add your name or icon here&#xA;  &lt;/a&gt;&#xA;&lt;/p&gt;&#xA;--&gt; &#xA;&lt;h3&gt;Gold Sponsors&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.nodion.com&#34;&gt;&lt;img src=&#34;https://www.nodion.com/img/logo.svg?sanitize=true&#34; alt=&#34;nodion&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.piknik.com&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/note/piknik.png&#34; alt=&#34;piknik&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/seaweedfs/seaweedfs/releases/latest&#34;&gt;Download Binaries for different platforms&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://join.slack.com/t/seaweedfs/shared_invite/enQtMzI4MTMwMjU2MzA3LTEyYzZmZWYzOGQ3MDJlZWMzYmI0OTE4OTJiZjJjODBmMzUxNmYwODg0YjY3MTNlMjBmZDQ1NzQ5NDJhZWI2ZmY&#34;&gt;SeaweedFS on Slack&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/SeaweedFS&#34;&gt;SeaweedFS on Twitter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://t.me/Seaweedfs&#34;&gt;SeaweedFS on Telegram&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.reddit.com/r/SeaweedFS/&#34;&gt;SeaweedFS on Reddit&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://groups.google.com/d/forum/seaweedfs&#34;&gt;SeaweedFS Mailing List&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/seaweedfs/seaweedfs/wiki&#34;&gt;Wiki Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/seaweedfs/seaweedfs/wiki/SeaweedFS_Architecture.pdf&#34;&gt;SeaweedFS White Paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/1DcxKWlINc-HNCjhYeERkpGXXm6nTCES8mi2W5G0Z4Ts/edit?usp=sharing&#34;&gt;SeaweedFS Introduction Slides 2021.5&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.slideshare.net/chrislusf/seaweedfs-introduction&#34;&gt;SeaweedFS Introduction Slides 2019.3&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Table of Contents&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#quick-start&#34;&gt;Quick Start&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#quick-start-for-s3-api-on-docker&#34;&gt;Quick Start for S3 API on Docker&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#quick-start-with-single-binary&#34;&gt;Quick Start with Single Binary&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#quick-start-seaweedfs-s3-on-aws&#34;&gt;Quick Start SeaweedFS S3 on AWS&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#features&#34;&gt;Features&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#additional-features&#34;&gt;Additional Features&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#filer-features&#34;&gt;Filer Features&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#example-using-seaweed-object-store&#34;&gt;Example: Using Seaweed Object Store&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#object-store-architecture&#34;&gt;Architecture&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#compared-to-other-file-systems&#34;&gt;Compared to Other File Systems&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#compared-to-hdfs&#34;&gt;Compared to HDFS&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#compared-to-glusterfs-ceph&#34;&gt;Compared to GlusterFS, Ceph&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#compared-to-glusterfs&#34;&gt;Compared to GlusterFS&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#compared-to-ceph&#34;&gt;Compared to Ceph&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#dev-plan&#34;&gt;Dev Plan&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#installation-guide&#34;&gt;Installation Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#disk-related-topics&#34;&gt;Disk Related Topics&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#benchmark&#34;&gt;Benchmark&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Quick Start&lt;/h1&gt; &#xA;&lt;h2&gt;Quick Start for S3 API on Docker&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;docker run -p 8333:8333 chrislusf/seaweedfs server -s3&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start with Single Binary&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download the latest binary from &lt;a href=&#34;https://github.com/seaweedfs/seaweedfs/releases&#34;&gt;https://github.com/seaweedfs/seaweedfs/releases&lt;/a&gt; and unzip a single binary file &lt;code&gt;weed&lt;/code&gt; or &lt;code&gt;weed.exe&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;weed server -dir=/some/data/dir -s3&lt;/code&gt; to start one master, one volume server, one filer, and one S3 gateway.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Also, to increase capacity, just add more volume servers by running &lt;code&gt;weed volume -dir=&#34;/some/data/dir2&#34; -mserver=&#34;&amp;lt;master_host&amp;gt;:9333&#34; -port=8081&lt;/code&gt; locally, or on a different machine, or on thousands of machines. That is it!&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start SeaweedFS S3 on AWS&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Setup fast production-ready &lt;a href=&#34;https://aws.amazon.com/marketplace/pp/prodview-nzelz5gprlrjc&#34;&gt;SeaweedFS S3 on AWS with cloudformation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Introduction&lt;/h1&gt; &#xA;&lt;p&gt;SeaweedFS is a simple and highly scalable distributed file system. There are two objectives:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;to store billions of files!&lt;/li&gt; &#xA; &lt;li&gt;to serve the files fast!&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;SeaweedFS started as an Object Store to handle small files efficiently. Instead of managing all file metadata in a central master, the central master only manages volumes on volume servers, and these volume servers manage files and their metadata. This relieves concurrency pressure from the central master and spreads file metadata into volume servers, allowing faster file access (O(1), usually just one disk read operation).&lt;/p&gt; &#xA;&lt;p&gt;There is only 40 bytes of disk storage overhead for each file&#39;s metadata. It is so simple with O(1) disk reads that you are welcome to challenge the performance with your actual use cases.&lt;/p&gt; &#xA;&lt;p&gt;SeaweedFS started by implementing &lt;a href=&#34;http://www.usenix.org/event/osdi10/tech/full_papers/Beaver.pdf&#34;&gt;Facebook&#39;s Haystack design paper&lt;/a&gt;. Also, SeaweedFS implements erasure coding with ideas from &lt;a href=&#34;https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-muralidhar.pdf&#34;&gt;f4: Facebook’s Warm BLOB Storage System&lt;/a&gt;, and has a lot of similarities with &lt;a href=&#34;https://www.usenix.org/system/files/fast21-pan.pdf&#34;&gt;Facebook’s Tectonic Filesystem&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;On top of the object store, optional &lt;a href=&#34;https://github.com/seaweedfs/seaweedfs/wiki/Directories-and-Files&#34;&gt;Filer&lt;/a&gt; can support directories and POSIX attributes. Filer is a separate linearly-scalable stateless server with customizable metadata stores, e.g., MySql, Postgres, Redis, Cassandra, HBase, Mongodb, Elastic Search, LevelDB, RocksDB, Sqlite, MemSql, TiDB, Etcd, CockroachDB, YDB, etc.&lt;/p&gt; &#xA;&lt;p&gt;For any distributed key value stores, the large values can be offloaded to SeaweedFS. With the fast access speed and linearly scalable capacity, SeaweedFS can work as a distributed &lt;a href=&#34;https://github.com/seaweedfs/seaweedfs/wiki/Filer-as-a-Key-Large-Value-Store&#34;&gt;Key-Large-Value store&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;SeaweedFS can transparently integrate with the cloud. With hot data on local cluster, and warm data on the cloud with O(1) access time, SeaweedFS can achieve both fast local access time and elastic cloud storage capacity. What&#39;s more, the cloud storage access API cost is minimized. Faster and Cheaper than direct cloud storage!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#table-of-contents&#34;&gt;Back to TOC&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Features&lt;/h1&gt; &#xA;&lt;h2&gt;Additional Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Can choose no replication or different replication levels, rack and data center aware.&lt;/li&gt; &#xA; &lt;li&gt;Automatic master servers failover - no single point of failure (SPOF).&lt;/li&gt; &#xA; &lt;li&gt;Automatic Gzip compression depending on file MIME type.&lt;/li&gt; &#xA; &lt;li&gt;Automatic compaction to reclaim disk space after deletion or update.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/seaweedfs/seaweedfs/wiki/Store-file-with-a-Time-To-Live&#34;&gt;Automatic entry TTL expiration&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Any server with some disk spaces can add to the total storage space.&lt;/li&gt; &#xA; &lt;li&gt;Adding/Removing servers does &lt;strong&gt;not&lt;/strong&gt; cause any data re-balancing unless triggered by admin commands.&lt;/li&gt; &#xA; &lt;li&gt;Optional picture resizing.&lt;/li&gt; &#xA; &lt;li&gt;Support ETag, Accept-Range, Last-Modified, etc.&lt;/li&gt; &#xA; &lt;li&gt;Support in-memory/leveldb/readonly mode tuning for memory/performance balance.&lt;/li&gt; &#xA; &lt;li&gt;Support rebalancing the writable and readonly volumes.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/seaweedfs/seaweedfs/wiki/Tiered-Storage&#34;&gt;Customizable Multiple Storage Tiers&lt;/a&gt;: Customizable storage disk types to balance performance and cost.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/seaweedfs/seaweedfs/wiki/Cloud-Tier&#34;&gt;Transparent cloud integration&lt;/a&gt;: unlimited capacity via tiered cloud storage for warm data.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/seaweedfs/seaweedfs/wiki/Erasure-coding-for-warm-storage&#34;&gt;Erasure Coding for warm storage&lt;/a&gt; Rack-Aware 10.4 erasure coding reduces storage cost and increases availability.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#table-of-contents&#34;&gt;Back to TOC&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Filer Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/seaweedfs/seaweedfs/wiki/Directories-and-Files&#34;&gt;Filer server&lt;/a&gt; provides &#34;normal&#34; directories and files via http.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/seaweedfs/seaweedfs/wiki/Filer-Stores&#34;&gt;File TTL&lt;/a&gt; automatically expires file metadata and actual file data.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/seaweedfs/seaweedfs/wiki/FUSE-Mount&#34;&gt;Mount filer&lt;/a&gt; reads and writes files directly as a local directory via FUSE.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/seaweedfs/seaweedfs/wiki/Filer-Store-Replication&#34;&gt;Filer Store Replication&lt;/a&gt; enables HA for filer meta data stores.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/seaweedfs/seaweedfs/wiki/Filer-Active-Active-cross-cluster-continuous-synchronization&#34;&gt;Active-Active Replication&lt;/a&gt; enables asynchronous one-way or two-way cross cluster continuous replication.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/seaweedfs/seaweedfs/wiki/Amazon-S3-API&#34;&gt;Amazon S3 compatible API&lt;/a&gt; accesses files with S3 tooling.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/seaweedfs/seaweedfs/wiki/Hadoop-Compatible-File-System&#34;&gt;Hadoop Compatible File System&lt;/a&gt; accesses files from Hadoop/Spark/Flink/etc or even runs HBase.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/seaweedfs/seaweedfs/wiki/Async-Replication-to-Cloud&#34;&gt;Async Replication To Cloud&lt;/a&gt; has extremely fast local access and backups to Amazon S3, Google Cloud Storage, Azure, BackBlaze.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/seaweedfs/seaweedfs/wiki/WebDAV&#34;&gt;WebDAV&lt;/a&gt; accesses as a mapped drive on Mac and Windows, or from mobile devices.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/seaweedfs/seaweedfs/wiki/Filer-Data-Encryption&#34;&gt;AES256-GCM Encrypted Storage&lt;/a&gt; safely stores the encrypted data.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/seaweedfs/seaweedfs/wiki/Data-Structure-for-Large-Files&#34;&gt;Super Large Files&lt;/a&gt; stores large or super large files in tens of TB.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/seaweedfs/seaweedfs/wiki/Cloud-Drive-Architecture&#34;&gt;Cloud Drive&lt;/a&gt; mounts cloud storage to local cluster, cached for fast read and write with asynchronous write back.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/seaweedfs/seaweedfs/wiki/Gateway-to-Remote-Object-Storage&#34;&gt;Gateway to Remote Object Store&lt;/a&gt; mirrors bucket operations to remote object storage, in addition to &lt;a href=&#34;https://github.com/seaweedfs/seaweedfs/wiki/Cloud-Drive-Architecture&#34;&gt;Cloud Drive&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Kubernetes&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/seaweedfs/seaweedfs-csi-driver&#34;&gt;Kubernetes CSI Driver&lt;/a&gt; A Container Storage Interface (CSI) Driver. &lt;a href=&#34;https://hub.docker.com/r/chrislusf/seaweedfs-csi-driver/&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/pulls/chrislusf/seaweedfs-csi-driver.svg?maxAge=4800&#34; alt=&#34;Docker Pulls&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/seaweedfs/seaweedfs-operator&#34;&gt;SeaweedFS Operator&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#table-of-contents&#34;&gt;Back to TOC&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Example: Using Seaweed Object Store&lt;/h2&gt; &#xA;&lt;p&gt;By default, the master node runs on port 9333, and the volume nodes run on port 8080. Let&#39;s start one master node, and two volume nodes on port 8080 and 8081. Ideally, they should be started from different machines. We&#39;ll use localhost as an example.&lt;/p&gt; &#xA;&lt;p&gt;SeaweedFS uses HTTP REST operations to read, write, and delete. The responses are in JSON or JSONP format.&lt;/p&gt; &#xA;&lt;h3&gt;Start Master Server&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; ./weed master&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Start Volume Servers&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; weed volume -dir=&#34;/tmp/data1&#34; -max=5  -mserver=&#34;localhost:9333&#34; -port=8080 &amp;amp;&#xA;&amp;gt; weed volume -dir=&#34;/tmp/data2&#34; -max=10 -mserver=&#34;localhost:9333&#34; -port=8081 &amp;amp;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Write File&lt;/h3&gt; &#xA;&lt;p&gt;To upload a file: first, send a HTTP POST, PUT, or GET request to &lt;code&gt;/dir/assign&lt;/code&gt; to get an &lt;code&gt;fid&lt;/code&gt; and a volume server URL:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; curl http://localhost:9333/dir/assign&#xA;{&#34;count&#34;:1,&#34;fid&#34;:&#34;3,01637037d6&#34;,&#34;url&#34;:&#34;127.0.0.1:8080&#34;,&#34;publicUrl&#34;:&#34;localhost:8080&#34;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Second, to store the file content, send a HTTP multi-part POST request to &lt;code&gt;url + &#39;/&#39; + fid&lt;/code&gt; from the response:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; curl -F file=@/home/chris/myphoto.jpg http://127.0.0.1:8080/3,01637037d6&#xA;{&#34;name&#34;:&#34;myphoto.jpg&#34;,&#34;size&#34;:43234,&#34;eTag&#34;:&#34;1cc0118e&#34;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To update, send another POST request with updated file content.&lt;/p&gt; &#xA;&lt;p&gt;For deletion, send an HTTP DELETE request to the same &lt;code&gt;url + &#39;/&#39; + fid&lt;/code&gt; URL:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; curl -X DELETE http://127.0.0.1:8080/3,01637037d6&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Save File Id&lt;/h3&gt; &#xA;&lt;p&gt;Now, you can save the &lt;code&gt;fid&lt;/code&gt;, 3,01637037d6 in this case, to a database field.&lt;/p&gt; &#xA;&lt;p&gt;The number 3 at the start represents a volume id. After the comma, it&#39;s one file key, 01, and a file cookie, 637037d6.&lt;/p&gt; &#xA;&lt;p&gt;The volume id is an unsigned 32-bit integer. The file key is an unsigned 64-bit integer. The file cookie is an unsigned 32-bit integer, used to prevent URL guessing.&lt;/p&gt; &#xA;&lt;p&gt;The file key and file cookie are both coded in hex. You can store the &amp;lt;volume id, file key, file cookie&amp;gt; tuple in your own format, or simply store the &lt;code&gt;fid&lt;/code&gt; as a string.&lt;/p&gt; &#xA;&lt;p&gt;If stored as a string, in theory, you would need 8+1+16+8=33 bytes. A char(33) would be enough, if not more than enough, since most uses will not need 2^32 volumes.&lt;/p&gt; &#xA;&lt;p&gt;If space is really a concern, you can store the file id in your own format. You would need one 4-byte integer for volume id, 8-byte long number for file key, and a 4-byte integer for the file cookie. So 16 bytes are more than enough.&lt;/p&gt; &#xA;&lt;h3&gt;Read File&lt;/h3&gt; &#xA;&lt;p&gt;Here is an example of how to render the URL.&lt;/p&gt; &#xA;&lt;p&gt;First look up the volume server&#39;s URLs by the file&#39;s volumeId:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; curl http://localhost:9333/dir/lookup?volumeId=3&#xA;{&#34;volumeId&#34;:&#34;3&#34;,&#34;locations&#34;:[{&#34;publicUrl&#34;:&#34;localhost:8080&#34;,&#34;url&#34;:&#34;localhost:8080&#34;}]}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Since (usually) there are not too many volume servers, and volumes don&#39;t move often, you can cache the results most of the time. Depending on the replication type, one volume can have multiple replica locations. Just randomly pick one location to read.&lt;/p&gt; &#xA;&lt;p&gt;Now you can take the public URL, render the URL or directly read from the volume server via URL:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt; http://localhost:8080/3,01637037d6.jpg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Notice we add a file extension &#34;.jpg&#34; here. It&#39;s optional and just one way for the client to specify the file content type.&lt;/p&gt; &#xA;&lt;p&gt;If you want a nicer URL, you can use one of these alternative URL formats:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt; http://localhost:8080/3/01637037d6/my_preferred_name.jpg&#xA; http://localhost:8080/3/01637037d6.jpg&#xA; http://localhost:8080/3,01637037d6.jpg&#xA; http://localhost:8080/3/01637037d6&#xA; http://localhost:8080/3,01637037d6&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to get a scaled version of an image, you can add some params:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;http://localhost:8080/3/01637037d6.jpg?height=200&amp;amp;width=200&#xA;http://localhost:8080/3/01637037d6.jpg?height=200&amp;amp;width=200&amp;amp;mode=fit&#xA;http://localhost:8080/3/01637037d6.jpg?height=200&amp;amp;width=200&amp;amp;mode=fill&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Rack-Aware and Data Center-Aware Replication&lt;/h3&gt; &#xA;&lt;p&gt;SeaweedFS applies the replication strategy at a volume level. So, when you are getting a file id, you can specify the replication strategy. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl http://localhost:9333/dir/assign?replication=001&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The replication parameter options are:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;000: no replication&#xA;001: replicate once on the same rack&#xA;010: replicate once on a different rack, but same data center&#xA;100: replicate once on a different data center&#xA;200: replicate twice on two different data center&#xA;110: replicate once on a different rack, and once on a different data center&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;More details about replication can be found &lt;a href=&#34;https://github.com/seaweedfs/seaweedfs/wiki/Replication&#34;&gt;on the wiki&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can also set the default replication strategy when starting the master server.&lt;/p&gt; &#xA;&lt;h3&gt;Allocate File Key on Specific Data Center&lt;/h3&gt; &#xA;&lt;p&gt;Volume servers can be started with a specific data center name:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt; weed volume -dir=/tmp/1 -port=8080 -dataCenter=dc1&#xA; weed volume -dir=/tmp/2 -port=8081 -dataCenter=dc2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When requesting a file key, an optional &#34;dataCenter&#34; parameter can limit the assigned volume to the specific data center. For example, this specifies that the assigned volume should be limited to &#39;dc1&#39;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt; http://localhost:9333/dir/assign?dataCenter=dc1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Other Features&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/seaweedfs/seaweedfs/wiki/Failover-Master-Server&#34;&gt;No Single Point of Failure&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/seaweedfs/seaweedfs/wiki/Optimization#insert-with-your-own-keys&#34;&gt;Insert with your own keys&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/seaweedfs/seaweedfs/wiki/Optimization#upload-large-files&#34;&gt;Chunking large files&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/seaweedfs/seaweedfs/wiki/Optimization#collection-as-a-simple-name-space&#34;&gt;Collection as a Simple Name Space&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#table-of-contents&#34;&gt;Back to TOC&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Object Store Architecture&lt;/h2&gt; &#xA;&lt;p&gt;Usually distributed file systems split each file into chunks, a central master keeps a mapping of filenames, chunk indices to chunk handles, and also which chunks each chunk server has.&lt;/p&gt; &#xA;&lt;p&gt;The main drawback is that the central master can&#39;t handle many small files efficiently, and since all read requests need to go through the chunk master, so it might not scale well for many concurrent users.&lt;/p&gt; &#xA;&lt;p&gt;Instead of managing chunks, SeaweedFS manages data volumes in the master server. Each data volume is 32GB in size, and can hold a lot of files. And each storage node can have many data volumes. So the master node only needs to store the metadata about the volumes, which is a fairly small amount of data and is generally stable.&lt;/p&gt; &#xA;&lt;p&gt;The actual file metadata is stored in each volume on volume servers. Since each volume server only manages metadata of files on its own disk, with only 16 bytes for each file, all file access can read file metadata just from memory and only needs one disk operation to actually read file data.&lt;/p&gt; &#xA;&lt;p&gt;For comparison, consider that an xfs inode structure in Linux is 536 bytes.&lt;/p&gt; &#xA;&lt;h3&gt;Master Server and Volume Server&lt;/h3&gt; &#xA;&lt;p&gt;The architecture is fairly simple. The actual data is stored in volumes on storage nodes. One volume server can have multiple volumes, and can both support read and write access with basic authentication.&lt;/p&gt; &#xA;&lt;p&gt;All volumes are managed by a master server. The master server contains the volume id to volume server mapping. This is fairly static information, and can be easily cached.&lt;/p&gt; &#xA;&lt;p&gt;On each write request, the master server also generates a file key, which is a growing 64-bit unsigned integer. Since write requests are not generally as frequent as read requests, one master server should be able to handle the concurrency well.&lt;/p&gt; &#xA;&lt;h3&gt;Write and Read files&lt;/h3&gt; &#xA;&lt;p&gt;When a client sends a write request, the master server returns (volume id, file key, file cookie, volume node URL) for the file. The client then contacts the volume node and POSTs the file content.&lt;/p&gt; &#xA;&lt;p&gt;When a client needs to read a file based on (volume id, file key, file cookie), it asks the master server by the volume id for the (volume node URL, volume node public URL), or retrieves this from a cache. Then the client can GET the content, or just render the URL on web pages and let browsers fetch the content.&lt;/p&gt; &#xA;&lt;p&gt;Please see the example for details on the write-read process.&lt;/p&gt; &#xA;&lt;h3&gt;Storage Size&lt;/h3&gt; &#xA;&lt;p&gt;In the current implementation, each volume can hold 32 gibibytes (32GiB or 8x2^32 bytes). This is because we align content to 8 bytes. We can easily increase this to 64GiB, or 128GiB, or more, by changing 2 lines of code, at the cost of some wasted padding space due to alignment.&lt;/p&gt; &#xA;&lt;p&gt;There can be 4 gibibytes (4GiB or 2^32 bytes) of volumes. So the total system size is 8 x 4GiB x 4GiB which is 128 exbibytes (128EiB or 2^67 bytes).&lt;/p&gt; &#xA;&lt;p&gt;Each individual file size is limited to the volume size.&lt;/p&gt; &#xA;&lt;h3&gt;Saving memory&lt;/h3&gt; &#xA;&lt;p&gt;All file meta information stored on an volume server is readable from memory without disk access. Each file takes just a 16-byte map entry of &amp;lt;64bit key, 32bit offset, 32bit size&amp;gt;. Of course, each map entry has its own space cost for the map. But usually the disk space runs out before the memory does.&lt;/p&gt; &#xA;&lt;h3&gt;Tiered Storage to the cloud&lt;/h3&gt; &#xA;&lt;p&gt;The local volume servers are much faster, while cloud storages have elastic capacity and are actually more cost-efficient if not accessed often (usually free to upload, but relatively costly to access). With the append-only structure and O(1) access time, SeaweedFS can take advantage of both local and cloud storage by offloading the warm data to the cloud.&lt;/p&gt; &#xA;&lt;p&gt;Usually hot data are fresh and warm data are old. SeaweedFS puts the newly created volumes on local servers, and optionally upload the older volumes on the cloud. If the older data are accessed less often, this literally gives you unlimited capacity with limited local servers, and still fast for new data.&lt;/p&gt; &#xA;&lt;p&gt;With the O(1) access time, the network latency cost is kept at minimum.&lt;/p&gt; &#xA;&lt;p&gt;If the hot/warm data is split as 20/80, with 20 servers, you can achieve storage capacity of 100 servers. That&#39;s a cost saving of 80%! Or you can repurpose the 80 servers to store new data also, and get 5X storage throughput.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#table-of-contents&#34;&gt;Back to TOC&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Compared to Other File Systems&lt;/h2&gt; &#xA;&lt;p&gt;Most other distributed file systems seem more complicated than necessary.&lt;/p&gt; &#xA;&lt;p&gt;SeaweedFS is meant to be fast and simple, in both setup and operation. If you do not understand how it works when you reach here, we&#39;ve failed! Please raise an issue with any questions or update this file with clarifications.&lt;/p&gt; &#xA;&lt;p&gt;SeaweedFS is constantly moving forward. Same with other systems. These comparisons can be outdated quickly. Please help to keep them updated.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#table-of-contents&#34;&gt;Back to TOC&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Compared to HDFS&lt;/h3&gt; &#xA;&lt;p&gt;HDFS uses the chunk approach for each file, and is ideal for storing large files.&lt;/p&gt; &#xA;&lt;p&gt;SeaweedFS is ideal for serving relatively smaller files quickly and concurrently.&lt;/p&gt; &#xA;&lt;p&gt;SeaweedFS can also store extra large files by splitting them into manageable data chunks, and store the file ids of the data chunks into a meta chunk. This is managed by &#34;weed upload/download&#34; tool, and the weed master or volume servers are agnostic about it.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#table-of-contents&#34;&gt;Back to TOC&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Compared to GlusterFS, Ceph&lt;/h3&gt; &#xA;&lt;p&gt;The architectures are mostly the same. SeaweedFS aims to store and read files fast, with a simple and flat architecture. The main differences are&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;SeaweedFS optimizes for small files, ensuring O(1) disk seek operation, and can also handle large files.&lt;/li&gt; &#xA; &lt;li&gt;SeaweedFS statically assigns a volume id for a file. Locating file content becomes just a lookup of the volume id, which can be easily cached.&lt;/li&gt; &#xA; &lt;li&gt;SeaweedFS Filer metadata store can be any well-known and proven data store, e.g., Redis, Cassandra, HBase, Mongodb, Elastic Search, MySql, Postgres, Sqlite, MemSql, TiDB, CockroachDB, Etcd, YDB etc, and is easy to customize.&lt;/li&gt; &#xA; &lt;li&gt;SeaweedFS Volume server also communicates directly with clients via HTTP, supporting range queries, direct uploads, etc.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;System&lt;/th&gt; &#xA;   &lt;th&gt;File Metadata&lt;/th&gt; &#xA;   &lt;th&gt;File Content Read&lt;/th&gt; &#xA;   &lt;th&gt;POSIX&lt;/th&gt; &#xA;   &lt;th&gt;REST API&lt;/th&gt; &#xA;   &lt;th&gt;Optimized for large number of small files&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SeaweedFS&lt;/td&gt; &#xA;   &lt;td&gt;lookup volume id, cacheable&lt;/td&gt; &#xA;   &lt;td&gt;O(1) disk seek&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SeaweedFS Filer&lt;/td&gt; &#xA;   &lt;td&gt;Linearly Scalable, Customizable&lt;/td&gt; &#xA;   &lt;td&gt;O(1) disk seek&lt;/td&gt; &#xA;   &lt;td&gt;FUSE&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GlusterFS&lt;/td&gt; &#xA;   &lt;td&gt;hashing&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;FUSE, NFS&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ceph&lt;/td&gt; &#xA;   &lt;td&gt;hashing + rules&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;FUSE&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MooseFS&lt;/td&gt; &#xA;   &lt;td&gt;in memory&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;FUSE&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MinIO&lt;/td&gt; &#xA;   &lt;td&gt;separate meta file for each file&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#table-of-contents&#34;&gt;Back to TOC&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Compared to GlusterFS&lt;/h3&gt; &#xA;&lt;p&gt;GlusterFS stores files, both directories and content, in configurable volumes called &#34;bricks&#34;.&lt;/p&gt; &#xA;&lt;p&gt;GlusterFS hashes the path and filename into ids, and assigned to virtual volumes, and then mapped to &#34;bricks&#34;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#table-of-contents&#34;&gt;Back to TOC&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Compared to MooseFS&lt;/h3&gt; &#xA;&lt;p&gt;MooseFS chooses to neglect small file issue. From moosefs 3.0 manual, &#34;even a small file will occupy 64KiB plus additionally 4KiB of checksums and 1KiB for the header&#34;, because it &#34;was initially designed for keeping large amounts (like several thousands) of very big files&#34;&lt;/p&gt; &#xA;&lt;p&gt;MooseFS Master Server keeps all meta data in memory. Same issue as HDFS namenode.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#table-of-contents&#34;&gt;Back to TOC&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Compared to Ceph&lt;/h3&gt; &#xA;&lt;p&gt;Ceph can be setup similar to SeaweedFS as a key-&amp;gt;blob store. It is much more complicated, with the need to support layers on top of it. &lt;a href=&#34;https://github.com/seaweedfs/seaweedfs/issues/120&#34;&gt;Here is a more detailed comparison&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;SeaweedFS has a centralized master group to look up free volumes, while Ceph uses hashing and metadata servers to locate its objects. Having a centralized master makes it easy to code and manage.&lt;/p&gt; &#xA;&lt;p&gt;Ceph, like SeaweedFS, is based on the object store RADOS. Ceph is rather complicated with mixed reviews.&lt;/p&gt; &#xA;&lt;p&gt;Ceph uses CRUSH hashing to automatically manage data placement, which is efficient to locate the data. But the data has to be placed according to the CRUSH algorithm. Any wrong configuration would cause data loss. Topology changes, such as adding new servers to increase capacity, will cause data migration with high IO cost to fit the CRUSH algorithm. SeaweedFS places data by assigning them to any writable volumes. If writes to one volume failed, just pick another volume to write. Adding more volumes is also as simple as it can be.&lt;/p&gt; &#xA;&lt;p&gt;SeaweedFS is optimized for small files. Small files are stored as one continuous block of content, with at most 8 unused bytes between files. Small file access is O(1) disk read.&lt;/p&gt; &#xA;&lt;p&gt;SeaweedFS Filer uses off-the-shelf stores, such as MySql, Postgres, Sqlite, Mongodb, Redis, Elastic Search, Cassandra, HBase, MemSql, TiDB, CockroachCB, Etcd, YDB, to manage file directories. These stores are proven, scalable, and easier to manage.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;SeaweedFS&lt;/th&gt; &#xA;   &lt;th&gt;comparable to Ceph&lt;/th&gt; &#xA;   &lt;th&gt;advantage&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Master&lt;/td&gt; &#xA;   &lt;td&gt;MDS&lt;/td&gt; &#xA;   &lt;td&gt;simpler&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Volume&lt;/td&gt; &#xA;   &lt;td&gt;OSD&lt;/td&gt; &#xA;   &lt;td&gt;optimized for small files&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Filer&lt;/td&gt; &#xA;   &lt;td&gt;Ceph FS&lt;/td&gt; &#xA;   &lt;td&gt;linearly scalable, Customizable, O(1) or O(logN)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#table-of-contents&#34;&gt;Back to TOC&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Compared to MinIO&lt;/h3&gt; &#xA;&lt;p&gt;MinIO follows AWS S3 closely and is ideal for testing for S3 API. It has good UI, policies, versionings, etc. SeaweedFS is trying to catch up here. It is also possible to put MinIO as a gateway in front of SeaweedFS later.&lt;/p&gt; &#xA;&lt;p&gt;MinIO metadata are in simple files. Each file write will incur extra writes to corresponding meta file.&lt;/p&gt; &#xA;&lt;p&gt;MinIO does not have optimization for lots of small files. The files are simply stored as is to local disks. Plus the extra meta file and shards for erasure coding, it only amplifies the LOSF problem.&lt;/p&gt; &#xA;&lt;p&gt;MinIO has multiple disk IO to read one file. SeaweedFS has O(1) disk reads, even for erasure coded files.&lt;/p&gt; &#xA;&lt;p&gt;MinIO has full-time erasure coding. SeaweedFS uses replication on hot data for faster speed and optionally applies erasure coding on warm data.&lt;/p&gt; &#xA;&lt;p&gt;MinIO does not have POSIX-like API support.&lt;/p&gt; &#xA;&lt;p&gt;MinIO has specific requirements on storage layout. It is not flexible to adjust capacity. In SeaweedFS, just start one volume server pointing to the master. That&#39;s all.&lt;/p&gt; &#xA;&lt;h2&gt;Dev Plan&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;More tools and documentation, on how to manage and scale the system.&lt;/li&gt; &#xA; &lt;li&gt;Read and write stream data.&lt;/li&gt; &#xA; &lt;li&gt;Support structured data.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This is a super exciting project! And we need helpers and &lt;a href=&#34;https://www.patreon.com/seaweedfs&#34;&gt;support&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#table-of-contents&#34;&gt;Back to TOC&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation Guide&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Installation guide for users who are not familiar with golang&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Step 1: install go on your machine and setup the environment by following the instructions at:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://golang.org/doc/install&#34;&gt;https://golang.org/doc/install&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;make sure to define your $GOPATH&lt;/p&gt; &#xA;&lt;p&gt;Step 2: checkout this repo:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/seaweedfs/seaweedfs.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Step 3: download, compile, and install the project by executing the following command&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd seaweedfs/weed &amp;amp;&amp;amp; make install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once this is done, you will find the executable &#34;weed&#34; in your &lt;code&gt;$GOPATH/bin&lt;/code&gt; directory&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#table-of-contents&#34;&gt;Back to TOC&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Disk Related Topics&lt;/h2&gt; &#xA;&lt;h3&gt;Hard Drive Performance&lt;/h3&gt; &#xA;&lt;p&gt;When testing read performance on SeaweedFS, it basically becomes a performance test of your hard drive&#39;s random read speed. Hard drives usually get 100MB/s~200MB/s.&lt;/p&gt; &#xA;&lt;h3&gt;Solid State Disk&lt;/h3&gt; &#xA;&lt;p&gt;To modify or delete small files, SSD must delete a whole block at a time, and move content in existing blocks to a new block. SSD is fast when brand new, but will get fragmented over time and you have to garbage collect, compacting blocks. SeaweedFS is friendly to SSD since it is append-only. Deletion and compaction are done on volume level in the background, not slowing reading and not causing fragmentation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#table-of-contents&#34;&gt;Back to TOC&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Benchmark&lt;/h2&gt; &#xA;&lt;p&gt;My Own Unscientific Single Machine Results on Mac Book with Solid State Disk, CPU: 1 Intel Core i7 2.6GHz.&lt;/p&gt; &#xA;&lt;p&gt;Write 1 million 1KB file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Concurrency Level:      16&#xA;Time taken for tests:   66.753 seconds&#xA;Completed requests:      1048576&#xA;Failed requests:        0&#xA;Total transferred:      1106789009 bytes&#xA;Requests per second:    15708.23 [#/sec]&#xA;Transfer rate:          16191.69 [Kbytes/sec]&#xA;&#xA;Connection Times (ms)&#xA;              min      avg        max      std&#xA;Total:        0.3      1.0       84.3      0.9&#xA;&#xA;Percentage of the requests served within a certain time (ms)&#xA;   50%      0.8 ms&#xA;   66%      1.0 ms&#xA;   75%      1.1 ms&#xA;   80%      1.2 ms&#xA;   90%      1.4 ms&#xA;   95%      1.7 ms&#xA;   98%      2.1 ms&#xA;   99%      2.6 ms&#xA;  100%     84.3 ms&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Randomly read 1 million files:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Concurrency Level:      16&#xA;Time taken for tests:   22.301 seconds&#xA;Completed requests:      1048576&#xA;Failed requests:        0&#xA;Total transferred:      1106812873 bytes&#xA;Requests per second:    47019.38 [#/sec]&#xA;Transfer rate:          48467.57 [Kbytes/sec]&#xA;&#xA;Connection Times (ms)&#xA;              min      avg        max      std&#xA;Total:        0.0      0.3       54.1      0.2&#xA;&#xA;Percentage of the requests served within a certain time (ms)&#xA;   50%      0.3 ms&#xA;   90%      0.4 ms&#xA;   98%      0.6 ms&#xA;   99%      0.7 ms&#xA;  100%     54.1 ms&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#table-of-contents&#34;&gt;Back to TOC&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Licensed under the Apache License, Version 2.0 (the &#34;License&#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;http://www.apache.org/licenses/LICENSE-2.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &#34;AS IS&#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.&lt;/p&gt; &#xA;&lt;p&gt;The text of this page is available for modification and reuse under the terms of the Creative Commons Attribution-Sharealike 3.0 Unported License and the GNU Free Documentation License (unversioned, with no invariant sections, front-cover texts, or back-cover texts).&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#table-of-contents&#34;&gt;Back to TOC&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Stargazers over time&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://starchart.cc/chrislusf/seaweedfs&#34;&gt;&lt;img src=&#34;https://starchart.cc/chrislusf/seaweedfs.svg?sanitize=true&#34; alt=&#34;Stargazers over time&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>gorilla/mux</title>
    <updated>2022-12-11T01:42:02Z</updated>
    <id>tag:github.com,2022-12-11:/gorilla/mux</id>
    <link href="https://github.com/gorilla/mux" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A powerful HTTP router and URL matcher for building Go web servers with 🦍&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;gorilla/mux&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://godoc.org/github.com/gorilla/mux&#34;&gt;&lt;img src=&#34;https://godoc.org/github.com/gorilla/mux?status.svg?sanitize=true&#34; alt=&#34;GoDoc&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://circleci.com/gh/gorilla/mux&#34;&gt;&lt;img src=&#34;https://circleci.com/gh/gorilla/mux.svg?style=svg&#34; alt=&#34;CircleCI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://sourcegraph.com/github.com/gorilla/mux?badge&#34;&gt;&lt;img src=&#34;https://sourcegraph.com/github.com/gorilla/mux/-/badge.svg?sanitize=true&#34; alt=&#34;Sourcegraph&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://cloud-cdn.questionable.services/gorilla-icon-64.png&#34; alt=&#34;Gorilla Logo&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;The Gorilla project has been archived, and is no longer under active maintainenance. You can read more here: &lt;a href=&#34;https://github.com/gorilla#gorilla-toolkit&#34;&gt;https://github.com/gorilla#gorilla-toolkit&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Package &lt;code&gt;gorilla/mux&lt;/code&gt; implements a request router and dispatcher for matching incoming requests to their respective handler.&lt;/p&gt; &#xA;&lt;p&gt;The name mux stands for &#34;HTTP request multiplexer&#34;. Like the standard &lt;code&gt;http.ServeMux&lt;/code&gt;, &lt;code&gt;mux.Router&lt;/code&gt; matches incoming requests against a list of registered routes and calls a handler for the route that matches the URL or other conditions. The main features are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;It implements the &lt;code&gt;http.Handler&lt;/code&gt; interface so it is compatible with the standard &lt;code&gt;http.ServeMux&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Requests can be matched based on URL host, path, path prefix, schemes, header and query values, HTTP methods or using custom matchers.&lt;/li&gt; &#xA; &lt;li&gt;URL hosts, paths and query values can have variables with an optional regular expression.&lt;/li&gt; &#xA; &lt;li&gt;Registered URLs can be built, or &#34;reversed&#34;, which helps maintaining references to resources.&lt;/li&gt; &#xA; &lt;li&gt;Routes can be used as subrouters: nested routes are only tested if the parent route matches. This is useful to define groups of routes that share common conditions like a host, a path prefix or other repeated attributes. As a bonus, this optimizes request matching.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/gorilla/mux/master/#install&#34;&gt;Install&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/gorilla/mux/master/#examples&#34;&gt;Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/gorilla/mux/master/#matching-routes&#34;&gt;Matching Routes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/gorilla/mux/master/#static-files&#34;&gt;Static Files&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/gorilla/mux/master/#serving-single-page-applications&#34;&gt;Serving Single Page Applications&lt;/a&gt; (e.g. React, Vue, Ember.js, etc.)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/gorilla/mux/master/#registered-urls&#34;&gt;Registered URLs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/gorilla/mux/master/#walking-routes&#34;&gt;Walking Routes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/gorilla/mux/master/#graceful-shutdown&#34;&gt;Graceful Shutdown&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/gorilla/mux/master/#middleware&#34;&gt;Middleware&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/gorilla/mux/master/#handling-cors-requests&#34;&gt;Handling CORS Requests&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/gorilla/mux/master/#testing-handlers&#34;&gt;Testing Handlers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/gorilla/mux/master/#full-example&#34;&gt;Full Example&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;With a &lt;a href=&#34;https://golang.org/doc/install#testing&#34;&gt;correctly configured&lt;/a&gt; Go toolchain:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;go get -u github.com/gorilla/mux&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;Let&#39;s start registering a couple of URL paths and handlers:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func main() {&#xA;    r := mux.NewRouter()&#xA;    r.HandleFunc(&#34;/&#34;, HomeHandler)&#xA;    r.HandleFunc(&#34;/products&#34;, ProductsHandler)&#xA;    r.HandleFunc(&#34;/articles&#34;, ArticlesHandler)&#xA;    http.Handle(&#34;/&#34;, r)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here we register three routes mapping URL paths to handlers. This is equivalent to how &lt;code&gt;http.HandleFunc()&lt;/code&gt; works: if an incoming request URL matches one of the paths, the corresponding handler is called passing (&lt;code&gt;http.ResponseWriter&lt;/code&gt;, &lt;code&gt;*http.Request&lt;/code&gt;) as parameters.&lt;/p&gt; &#xA;&lt;p&gt;Paths can have variables. They are defined using the format &lt;code&gt;{name}&lt;/code&gt; or &lt;code&gt;{name:pattern}&lt;/code&gt;. If a regular expression pattern is not defined, the matched variable will be anything until the next slash. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;r := mux.NewRouter()&#xA;r.HandleFunc(&#34;/products/{key}&#34;, ProductHandler)&#xA;r.HandleFunc(&#34;/articles/{category}/&#34;, ArticlesCategoryHandler)&#xA;r.HandleFunc(&#34;/articles/{category}/{id:[0-9]+}&#34;, ArticleHandler)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The names are used to create a map of route variables which can be retrieved calling &lt;code&gt;mux.Vars()&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func ArticlesCategoryHandler(w http.ResponseWriter, r *http.Request) {&#xA;    vars := mux.Vars(r)&#xA;    w.WriteHeader(http.StatusOK)&#xA;    fmt.Fprintf(w, &#34;Category: %v\n&#34;, vars[&#34;category&#34;])&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And this is all you need to know about the basic usage. More advanced options are explained below.&lt;/p&gt; &#xA;&lt;h3&gt;Matching Routes&lt;/h3&gt; &#xA;&lt;p&gt;Routes can also be restricted to a domain or subdomain. Just define a host pattern to be matched. They can also have variables:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;r := mux.NewRouter()&#xA;// Only matches if domain is &#34;www.example.com&#34;.&#xA;r.Host(&#34;www.example.com&#34;)&#xA;// Matches a dynamic subdomain.&#xA;r.Host(&#34;{subdomain:[a-z]+}.example.com&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;There are several other matchers that can be added. To match path prefixes:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;r.PathPrefix(&#34;/products/&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;...or HTTP methods:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;r.Methods(&#34;GET&#34;, &#34;POST&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;...or URL schemes:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;r.Schemes(&#34;https&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;...or header values:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;r.Headers(&#34;X-Requested-With&#34;, &#34;XMLHttpRequest&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;...or query values:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;r.Queries(&#34;key&#34;, &#34;value&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;...or to use a custom matcher function:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;r.MatcherFunc(func(r *http.Request, rm *RouteMatch) bool {&#xA;    return r.ProtoMajor == 0&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;...and finally, it is possible to combine several matchers in a single route:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;r.HandleFunc(&#34;/products&#34;, ProductsHandler).&#xA;  Host(&#34;www.example.com&#34;).&#xA;  Methods(&#34;GET&#34;).&#xA;  Schemes(&#34;http&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Routes are tested in the order they were added to the router. If two routes match, the first one wins:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;r := mux.NewRouter()&#xA;r.HandleFunc(&#34;/specific&#34;, specificHandler)&#xA;r.PathPrefix(&#34;/&#34;).Handler(catchAllHandler)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Setting the same matching conditions again and again can be boring, so we have a way to group several routes that share the same requirements. We call it &#34;subrouting&#34;.&lt;/p&gt; &#xA;&lt;p&gt;For example, let&#39;s say we have several URLs that should only match when the host is &lt;code&gt;www.example.com&lt;/code&gt;. Create a route for that host and get a &#34;subrouter&#34; from it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;r := mux.NewRouter()&#xA;s := r.Host(&#34;www.example.com&#34;).Subrouter()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then register routes in the subrouter:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;s.HandleFunc(&#34;/products/&#34;, ProductsHandler)&#xA;s.HandleFunc(&#34;/products/{key}&#34;, ProductHandler)&#xA;s.HandleFunc(&#34;/articles/{category}/{id:[0-9]+}&#34;, ArticleHandler)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The three URL paths we registered above will only be tested if the domain is &lt;code&gt;www.example.com&lt;/code&gt;, because the subrouter is tested first. This is not only convenient, but also optimizes request matching. You can create subrouters combining any attribute matchers accepted by a route.&lt;/p&gt; &#xA;&lt;p&gt;Subrouters can be used to create domain or path &#34;namespaces&#34;: you define subrouters in a central place and then parts of the app can register its paths relatively to a given subrouter.&lt;/p&gt; &#xA;&lt;p&gt;There&#39;s one more thing about subroutes. When a subrouter has a path prefix, the inner routes use it as base for their paths:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;r := mux.NewRouter()&#xA;s := r.PathPrefix(&#34;/products&#34;).Subrouter()&#xA;// &#34;/products/&#34;&#xA;s.HandleFunc(&#34;/&#34;, ProductsHandler)&#xA;// &#34;/products/{key}/&#34;&#xA;s.HandleFunc(&#34;/{key}/&#34;, ProductHandler)&#xA;// &#34;/products/{key}/details&#34;&#xA;s.HandleFunc(&#34;/{key}/details&#34;, ProductDetailsHandler)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Static Files&lt;/h3&gt; &#xA;&lt;p&gt;Note that the path provided to &lt;code&gt;PathPrefix()&lt;/code&gt; represents a &#34;wildcard&#34;: calling &lt;code&gt;PathPrefix(&#34;/static/&#34;).Handler(...)&lt;/code&gt; means that the handler will be passed any request that matches &#34;/static/*&#34;. This makes it easy to serve static files with mux:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func main() {&#xA;    var dir string&#xA;&#xA;    flag.StringVar(&amp;amp;dir, &#34;dir&#34;, &#34;.&#34;, &#34;the directory to serve files from. Defaults to the current dir&#34;)&#xA;    flag.Parse()&#xA;    r := mux.NewRouter()&#xA;&#xA;    // This will serve files under http://localhost:8000/static/&amp;lt;filename&amp;gt;&#xA;    r.PathPrefix(&#34;/static/&#34;).Handler(http.StripPrefix(&#34;/static/&#34;, http.FileServer(http.Dir(dir))))&#xA;&#xA;    srv := &amp;amp;http.Server{&#xA;        Handler:      r,&#xA;        Addr:         &#34;127.0.0.1:8000&#34;,&#xA;        // Good practice: enforce timeouts for servers you create!&#xA;        WriteTimeout: 15 * time.Second,&#xA;        ReadTimeout:  15 * time.Second,&#xA;    }&#xA;&#xA;    log.Fatal(srv.ListenAndServe())&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Serving Single Page Applications&lt;/h3&gt; &#xA;&lt;p&gt;Most of the time it makes sense to serve your SPA on a separate web server from your API, but sometimes it&#39;s desirable to serve them both from one place. It&#39;s possible to write a simple handler for serving your SPA (for use with React Router&#39;s &lt;a href=&#34;https://reacttraining.com/react-router/web/api/BrowserRouter&#34;&gt;BrowserRouter&lt;/a&gt; for example), and leverage mux&#39;s powerful routing for your API endpoints.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package main&#xA;&#xA;import (&#xA;&#x9;&#34;encoding/json&#34;&#xA;&#x9;&#34;log&#34;&#xA;&#x9;&#34;net/http&#34;&#xA;&#x9;&#34;os&#34;&#xA;&#x9;&#34;path/filepath&#34;&#xA;&#x9;&#34;time&#34;&#xA;&#xA;&#x9;&#34;github.com/gorilla/mux&#34;&#xA;)&#xA;&#xA;// spaHandler implements the http.Handler interface, so we can use it&#xA;// to respond to HTTP requests. The path to the static directory and&#xA;// path to the index file within that static directory are used to&#xA;// serve the SPA in the given static directory.&#xA;type spaHandler struct {&#xA;&#x9;staticPath string&#xA;&#x9;indexPath  string&#xA;}&#xA;&#xA;// ServeHTTP inspects the URL path to locate a file within the static dir&#xA;// on the SPA handler. If a file is found, it will be served. If not, the&#xA;// file located at the index path on the SPA handler will be served. This&#xA;// is suitable behavior for serving an SPA (single page application).&#xA;func (h spaHandler) ServeHTTP(w http.ResponseWriter, r *http.Request) {&#xA;    // get the absolute path to prevent directory traversal&#xA;&#x9;path, err := filepath.Abs(r.URL.Path)&#xA;&#x9;if err != nil {&#xA;        // if we failed to get the absolute path respond with a 400 bad request&#xA;        // and stop&#xA;&#x9;&#x9;http.Error(w, err.Error(), http.StatusBadRequest)&#xA;&#x9;&#x9;return&#xA;&#x9;}&#xA;&#xA;    // prepend the path with the path to the static directory&#xA;&#x9;path = filepath.Join(h.staticPath, path)&#xA;&#xA;    // check whether a file exists at the given path&#xA;&#x9;_, err = os.Stat(path)&#xA;&#x9;if os.IsNotExist(err) {&#xA;&#x9;&#x9;// file does not exist, serve index.html&#xA;&#x9;&#x9;http.ServeFile(w, r, filepath.Join(h.staticPath, h.indexPath))&#xA;&#x9;&#x9;return&#xA;&#x9;} else if err != nil {&#xA;        // if we got an error (that wasn&#39;t that the file doesn&#39;t exist) stating the&#xA;        // file, return a 500 internal server error and stop&#xA;&#x9;&#x9;http.Error(w, err.Error(), http.StatusInternalServerError)&#xA;&#x9;&#x9;return&#xA;&#x9;}&#xA;&#xA;    // otherwise, use http.FileServer to serve the static dir&#xA;&#x9;http.FileServer(http.Dir(h.staticPath)).ServeHTTP(w, r)&#xA;}&#xA;&#xA;func main() {&#xA;&#x9;router := mux.NewRouter()&#xA;&#xA;&#x9;router.HandleFunc(&#34;/api/health&#34;, func(w http.ResponseWriter, r *http.Request) {&#xA;&#x9;&#x9;// an example API handler&#xA;&#x9;&#x9;json.NewEncoder(w).Encode(map[string]bool{&#34;ok&#34;: true})&#xA;&#x9;})&#xA;&#xA;&#x9;spa := spaHandler{staticPath: &#34;build&#34;, indexPath: &#34;index.html&#34;}&#xA;&#x9;router.PathPrefix(&#34;/&#34;).Handler(spa)&#xA;&#xA;&#x9;srv := &amp;amp;http.Server{&#xA;&#x9;&#x9;Handler: router,&#xA;&#x9;&#x9;Addr:    &#34;127.0.0.1:8000&#34;,&#xA;&#x9;&#x9;// Good practice: enforce timeouts for servers you create!&#xA;&#x9;&#x9;WriteTimeout: 15 * time.Second,&#xA;&#x9;&#x9;ReadTimeout:  15 * time.Second,&#xA;&#x9;}&#xA;&#xA;&#x9;log.Fatal(srv.ListenAndServe())&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Registered URLs&lt;/h3&gt; &#xA;&lt;p&gt;Now let&#39;s see how to build registered URLs.&lt;/p&gt; &#xA;&lt;p&gt;Routes can be named. All routes that define a name can have their URLs built, or &#34;reversed&#34;. We define a name calling &lt;code&gt;Name()&lt;/code&gt; on a route. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;r := mux.NewRouter()&#xA;r.HandleFunc(&#34;/articles/{category}/{id:[0-9]+}&#34;, ArticleHandler).&#xA;  Name(&#34;article&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To build a URL, get the route and call the &lt;code&gt;URL()&lt;/code&gt; method, passing a sequence of key/value pairs for the route variables. For the previous route, we would do:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;url, err := r.Get(&#34;article&#34;).URL(&#34;category&#34;, &#34;technology&#34;, &#34;id&#34;, &#34;42&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;...and the result will be a &lt;code&gt;url.URL&lt;/code&gt; with the following path:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#34;/articles/technology/42&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This also works for host and query value variables:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;r := mux.NewRouter()&#xA;r.Host(&#34;{subdomain}.example.com&#34;).&#xA;  Path(&#34;/articles/{category}/{id:[0-9]+}&#34;).&#xA;  Queries(&#34;filter&#34;, &#34;{filter}&#34;).&#xA;  HandlerFunc(ArticleHandler).&#xA;  Name(&#34;article&#34;)&#xA;&#xA;// url.String() will be &#34;http://news.example.com/articles/technology/42?filter=gorilla&#34;&#xA;url, err := r.Get(&#34;article&#34;).URL(&#34;subdomain&#34;, &#34;news&#34;,&#xA;                                 &#34;category&#34;, &#34;technology&#34;,&#xA;                                 &#34;id&#34;, &#34;42&#34;,&#xA;                                 &#34;filter&#34;, &#34;gorilla&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;All variables defined in the route are required, and their values must conform to the corresponding patterns. These requirements guarantee that a generated URL will always match a registered route -- the only exception is for explicitly defined &#34;build-only&#34; routes which never match.&lt;/p&gt; &#xA;&lt;p&gt;Regex support also exists for matching Headers within a route. For example, we could do:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;r.HeadersRegexp(&#34;Content-Type&#34;, &#34;application/(text|json)&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;...and the route will match both requests with a Content-Type of &lt;code&gt;application/json&lt;/code&gt; as well as &lt;code&gt;application/text&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;There&#39;s also a way to build only the URL host or path for a route: use the methods &lt;code&gt;URLHost()&lt;/code&gt; or &lt;code&gt;URLPath()&lt;/code&gt; instead. For the previous route, we would do:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// &#34;http://news.example.com/&#34;&#xA;host, err := r.Get(&#34;article&#34;).URLHost(&#34;subdomain&#34;, &#34;news&#34;)&#xA;&#xA;// &#34;/articles/technology/42&#34;&#xA;path, err := r.Get(&#34;article&#34;).URLPath(&#34;category&#34;, &#34;technology&#34;, &#34;id&#34;, &#34;42&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And if you use subrouters, host and path defined separately can be built as well:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;r := mux.NewRouter()&#xA;s := r.Host(&#34;{subdomain}.example.com&#34;).Subrouter()&#xA;s.Path(&#34;/articles/{category}/{id:[0-9]+}&#34;).&#xA;  HandlerFunc(ArticleHandler).&#xA;  Name(&#34;article&#34;)&#xA;&#xA;// &#34;http://news.example.com/articles/technology/42&#34;&#xA;url, err := r.Get(&#34;article&#34;).URL(&#34;subdomain&#34;, &#34;news&#34;,&#xA;                                 &#34;category&#34;, &#34;technology&#34;,&#xA;                                 &#34;id&#34;, &#34;42&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Walking Routes&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;code&gt;Walk&lt;/code&gt; function on &lt;code&gt;mux.Router&lt;/code&gt; can be used to visit all of the routes that are registered on a router. For example, the following prints all of the registered routes:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package main&#xA;&#xA;import (&#xA;&#x9;&#34;fmt&#34;&#xA;&#x9;&#34;net/http&#34;&#xA;&#x9;&#34;strings&#34;&#xA;&#xA;&#x9;&#34;github.com/gorilla/mux&#34;&#xA;)&#xA;&#xA;func handler(w http.ResponseWriter, r *http.Request) {&#xA;&#x9;return&#xA;}&#xA;&#xA;func main() {&#xA;&#x9;r := mux.NewRouter()&#xA;&#x9;r.HandleFunc(&#34;/&#34;, handler)&#xA;&#x9;r.HandleFunc(&#34;/products&#34;, handler).Methods(&#34;POST&#34;)&#xA;&#x9;r.HandleFunc(&#34;/articles&#34;, handler).Methods(&#34;GET&#34;)&#xA;&#x9;r.HandleFunc(&#34;/articles/{id}&#34;, handler).Methods(&#34;GET&#34;, &#34;PUT&#34;)&#xA;&#x9;r.HandleFunc(&#34;/authors&#34;, handler).Queries(&#34;surname&#34;, &#34;{surname}&#34;)&#xA;&#x9;err := r.Walk(func(route *mux.Route, router *mux.Router, ancestors []*mux.Route) error {&#xA;&#x9;&#x9;pathTemplate, err := route.GetPathTemplate()&#xA;&#x9;&#x9;if err == nil {&#xA;&#x9;&#x9;&#x9;fmt.Println(&#34;ROUTE:&#34;, pathTemplate)&#xA;&#x9;&#x9;}&#xA;&#x9;&#x9;pathRegexp, err := route.GetPathRegexp()&#xA;&#x9;&#x9;if err == nil {&#xA;&#x9;&#x9;&#x9;fmt.Println(&#34;Path regexp:&#34;, pathRegexp)&#xA;&#x9;&#x9;}&#xA;&#x9;&#x9;queriesTemplates, err := route.GetQueriesTemplates()&#xA;&#x9;&#x9;if err == nil {&#xA;&#x9;&#x9;&#x9;fmt.Println(&#34;Queries templates:&#34;, strings.Join(queriesTemplates, &#34;,&#34;))&#xA;&#x9;&#x9;}&#xA;&#x9;&#x9;queriesRegexps, err := route.GetQueriesRegexp()&#xA;&#x9;&#x9;if err == nil {&#xA;&#x9;&#x9;&#x9;fmt.Println(&#34;Queries regexps:&#34;, strings.Join(queriesRegexps, &#34;,&#34;))&#xA;&#x9;&#x9;}&#xA;&#x9;&#x9;methods, err := route.GetMethods()&#xA;&#x9;&#x9;if err == nil {&#xA;&#x9;&#x9;&#x9;fmt.Println(&#34;Methods:&#34;, strings.Join(methods, &#34;,&#34;))&#xA;&#x9;&#x9;}&#xA;&#x9;&#x9;fmt.Println()&#xA;&#x9;&#x9;return nil&#xA;&#x9;})&#xA;&#xA;&#x9;if err != nil {&#xA;&#x9;&#x9;fmt.Println(err)&#xA;&#x9;}&#xA;&#xA;&#x9;http.Handle(&#34;/&#34;, r)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Graceful Shutdown&lt;/h3&gt; &#xA;&lt;p&gt;Go 1.8 introduced the ability to &lt;a href=&#34;https://golang.org/doc/go1.8#http_shutdown&#34;&gt;gracefully shutdown&lt;/a&gt; a &lt;code&gt;*http.Server&lt;/code&gt;. Here&#39;s how to do that alongside &lt;code&gt;mux&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package main&#xA;&#xA;import (&#xA;    &#34;context&#34;&#xA;    &#34;flag&#34;&#xA;    &#34;log&#34;&#xA;    &#34;net/http&#34;&#xA;    &#34;os&#34;&#xA;    &#34;os/signal&#34;&#xA;    &#34;time&#34;&#xA;&#xA;    &#34;github.com/gorilla/mux&#34;&#xA;)&#xA;&#xA;func main() {&#xA;    var wait time.Duration&#xA;    flag.DurationVar(&amp;amp;wait, &#34;graceful-timeout&#34;, time.Second * 15, &#34;the duration for which the server gracefully wait for existing connections to finish - e.g. 15s or 1m&#34;)&#xA;    flag.Parse()&#xA;&#xA;    r := mux.NewRouter()&#xA;    // Add your routes as needed&#xA;&#xA;    srv := &amp;amp;http.Server{&#xA;        Addr:         &#34;0.0.0.0:8080&#34;,&#xA;        // Good practice to set timeouts to avoid Slowloris attacks.&#xA;        WriteTimeout: time.Second * 15,&#xA;        ReadTimeout:  time.Second * 15,&#xA;        IdleTimeout:  time.Second * 60,&#xA;        Handler: r, // Pass our instance of gorilla/mux in.&#xA;    }&#xA;&#xA;    // Run our server in a goroutine so that it doesn&#39;t block.&#xA;    go func() {&#xA;        if err := srv.ListenAndServe(); err != nil {&#xA;            log.Println(err)&#xA;        }&#xA;    }()&#xA;&#xA;    c := make(chan os.Signal, 1)&#xA;    // We&#39;ll accept graceful shutdowns when quit via SIGINT (Ctrl+C)&#xA;    // SIGKILL, SIGQUIT or SIGTERM (Ctrl+/) will not be caught.&#xA;    signal.Notify(c, os.Interrupt)&#xA;&#xA;    // Block until we receive our signal.&#xA;    &amp;lt;-c&#xA;&#xA;    // Create a deadline to wait for.&#xA;    ctx, cancel := context.WithTimeout(context.Background(), wait)&#xA;    defer cancel()&#xA;    // Doesn&#39;t block if no connections, but will otherwise wait&#xA;    // until the timeout deadline.&#xA;    srv.Shutdown(ctx)&#xA;    // Optionally, you could run srv.Shutdown in a goroutine and block on&#xA;    // &amp;lt;-ctx.Done() if your application should wait for other services&#xA;    // to finalize based on context cancellation.&#xA;    log.Println(&#34;shutting down&#34;)&#xA;    os.Exit(0)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Middleware&lt;/h3&gt; &#xA;&lt;p&gt;Mux supports the addition of middlewares to a &lt;a href=&#34;https://godoc.org/github.com/gorilla/mux#Router&#34;&gt;Router&lt;/a&gt;, which are executed in the order they are added if a match is found, including its subrouters. Middlewares are (typically) small pieces of code which take one request, do something with it, and pass it down to another middleware or the final handler. Some common use cases for middleware are request logging, header manipulation, or &lt;code&gt;ResponseWriter&lt;/code&gt; hijacking.&lt;/p&gt; &#xA;&lt;p&gt;Mux middlewares are defined using the de facto standard type:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;type MiddlewareFunc func(http.Handler) http.Handler&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Typically, the returned handler is a closure which does something with the http.ResponseWriter and http.Request passed to it, and then calls the handler passed as parameter to the MiddlewareFunc. This takes advantage of closures being able access variables from the context where they are created, while retaining the signature enforced by the receivers.&lt;/p&gt; &#xA;&lt;p&gt;A very basic middleware which logs the URI of the request being handled could be written as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func loggingMiddleware(next http.Handler) http.Handler {&#xA;    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {&#xA;        // Do stuff here&#xA;        log.Println(r.RequestURI)&#xA;        // Call the next handler, which can be another middleware in the chain, or the final handler.&#xA;        next.ServeHTTP(w, r)&#xA;    })&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Middlewares can be added to a router using &lt;code&gt;Router.Use()&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;r := mux.NewRouter()&#xA;r.HandleFunc(&#34;/&#34;, handler)&#xA;r.Use(loggingMiddleware)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A more complex authentication middleware, which maps session token to users, could be written as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// Define our struct&#xA;type authenticationMiddleware struct {&#xA;&#x9;tokenUsers map[string]string&#xA;}&#xA;&#xA;// Initialize it somewhere&#xA;func (amw *authenticationMiddleware) Populate() {&#xA;&#x9;amw.tokenUsers[&#34;00000000&#34;] = &#34;user0&#34;&#xA;&#x9;amw.tokenUsers[&#34;aaaaaaaa&#34;] = &#34;userA&#34;&#xA;&#x9;amw.tokenUsers[&#34;05f717e5&#34;] = &#34;randomUser&#34;&#xA;&#x9;amw.tokenUsers[&#34;deadbeef&#34;] = &#34;user0&#34;&#xA;}&#xA;&#xA;// Middleware function, which will be called for each request&#xA;func (amw *authenticationMiddleware) Middleware(next http.Handler) http.Handler {&#xA;    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {&#xA;        token := r.Header.Get(&#34;X-Session-Token&#34;)&#xA;&#xA;        if user, found := amw.tokenUsers[token]; found {&#xA;        &#x9;// We found the token in our map&#xA;        &#x9;log.Printf(&#34;Authenticated user %s\n&#34;, user)&#xA;        &#x9;// Pass down the request to the next middleware (or final handler)&#xA;        &#x9;next.ServeHTTP(w, r)&#xA;        } else {&#xA;        &#x9;// Write an error and stop the handler chain&#xA;        &#x9;http.Error(w, &#34;Forbidden&#34;, http.StatusForbidden)&#xA;        }&#xA;    })&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;r := mux.NewRouter()&#xA;r.HandleFunc(&#34;/&#34;, handler)&#xA;&#xA;amw := authenticationMiddleware{tokenUsers: make(map[string]string)}&#xA;amw.Populate()&#xA;&#xA;r.Use(amw.Middleware)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: The handler chain will be stopped if your middleware doesn&#39;t call &lt;code&gt;next.ServeHTTP()&lt;/code&gt; with the corresponding parameters. This can be used to abort a request if the middleware writer wants to. Middlewares &lt;em&gt;should&lt;/em&gt; write to &lt;code&gt;ResponseWriter&lt;/code&gt; if they &lt;em&gt;are&lt;/em&gt; going to terminate the request, and they &lt;em&gt;should not&lt;/em&gt; write to &lt;code&gt;ResponseWriter&lt;/code&gt; if they &lt;em&gt;are not&lt;/em&gt; going to terminate it.&lt;/p&gt; &#xA;&lt;h3&gt;Handling CORS Requests&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://godoc.org/github.com/gorilla/mux#CORSMethodMiddleware&#34;&gt;CORSMethodMiddleware&lt;/a&gt; intends to make it easier to strictly set the &lt;code&gt;Access-Control-Allow-Methods&lt;/code&gt; response header.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You will still need to use your own CORS handler to set the other CORS headers such as &lt;code&gt;Access-Control-Allow-Origin&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;The middleware will set the &lt;code&gt;Access-Control-Allow-Methods&lt;/code&gt; header to all the method matchers (e.g. &lt;code&gt;r.Methods(http.MethodGet, http.MethodPut, http.MethodOptions)&lt;/code&gt; -&amp;gt; &lt;code&gt;Access-Control-Allow-Methods: GET,PUT,OPTIONS&lt;/code&gt;) on a route&lt;/li&gt; &#xA; &lt;li&gt;If you do not specify any methods, then:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;em&gt;Important&lt;/em&gt;: there must be an &lt;code&gt;OPTIONS&lt;/code&gt; method matcher for the middleware to set the headers.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Here is an example of using &lt;code&gt;CORSMethodMiddleware&lt;/code&gt; along with a custom &lt;code&gt;OPTIONS&lt;/code&gt; handler to set all the required CORS headers:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package main&#xA;&#xA;import (&#xA;&#x9;&#34;net/http&#34;&#xA;&#x9;&#34;github.com/gorilla/mux&#34;&#xA;)&#xA;&#xA;func main() {&#xA;    r := mux.NewRouter()&#xA;&#xA;    // IMPORTANT: you must specify an OPTIONS method matcher for the middleware to set CORS headers&#xA;    r.HandleFunc(&#34;/foo&#34;, fooHandler).Methods(http.MethodGet, http.MethodPut, http.MethodPatch, http.MethodOptions)&#xA;    r.Use(mux.CORSMethodMiddleware(r))&#xA;    &#xA;    http.ListenAndServe(&#34;:8080&#34;, r)&#xA;}&#xA;&#xA;func fooHandler(w http.ResponseWriter, r *http.Request) {&#xA;    w.Header().Set(&#34;Access-Control-Allow-Origin&#34;, &#34;*&#34;)&#xA;    if r.Method == http.MethodOptions {&#xA;        return&#xA;    }&#xA;&#xA;    w.Write([]byte(&#34;foo&#34;))&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And an request to &lt;code&gt;/foo&lt;/code&gt; using something like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl localhost:8080/foo -v&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Would look like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;*   Trying ::1...&#xA;* TCP_NODELAY set&#xA;* Connected to localhost (::1) port 8080 (#0)&#xA;&amp;gt; GET /foo HTTP/1.1&#xA;&amp;gt; Host: localhost:8080&#xA;&amp;gt; User-Agent: curl/7.59.0&#xA;&amp;gt; Accept: */*&#xA;&amp;gt; &#xA;&amp;lt; HTTP/1.1 200 OK&#xA;&amp;lt; Access-Control-Allow-Methods: GET,PUT,PATCH,OPTIONS&#xA;&amp;lt; Access-Control-Allow-Origin: *&#xA;&amp;lt; Date: Fri, 28 Jun 2019 20:13:30 GMT&#xA;&amp;lt; Content-Length: 3&#xA;&amp;lt; Content-Type: text/plain; charset=utf-8&#xA;&amp;lt; &#xA;* Connection #0 to host localhost left intact&#xA;foo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Testing Handlers&lt;/h3&gt; &#xA;&lt;p&gt;Testing handlers in a Go web application is straightforward, and &lt;em&gt;mux&lt;/em&gt; doesn&#39;t complicate this any further. Given two files: &lt;code&gt;endpoints.go&lt;/code&gt; and &lt;code&gt;endpoints_test.go&lt;/code&gt;, here&#39;s how we&#39;d test an application using &lt;em&gt;mux&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;First, our simple HTTP handler:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// endpoints.go&#xA;package main&#xA;&#xA;func HealthCheckHandler(w http.ResponseWriter, r *http.Request) {&#xA;    // A very simple health check.&#xA;    w.Header().Set(&#34;Content-Type&#34;, &#34;application/json&#34;)&#xA;    w.WriteHeader(http.StatusOK)&#xA;&#xA;    // In the future we could report back on the status of our DB, or our cache&#xA;    // (e.g. Redis) by performing a simple PING, and include them in the response.&#xA;    io.WriteString(w, `{&#34;alive&#34;: true}`)&#xA;}&#xA;&#xA;func main() {&#xA;    r := mux.NewRouter()&#xA;    r.HandleFunc(&#34;/health&#34;, HealthCheckHandler)&#xA;&#xA;    log.Fatal(http.ListenAndServe(&#34;localhost:8080&#34;, r))&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Our test code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// endpoints_test.go&#xA;package main&#xA;&#xA;import (&#xA;    &#34;net/http&#34;&#xA;    &#34;net/http/httptest&#34;&#xA;    &#34;testing&#34;&#xA;)&#xA;&#xA;func TestHealthCheckHandler(t *testing.T) {&#xA;    // Create a request to pass to our handler. We don&#39;t have any query parameters for now, so we&#39;ll&#xA;    // pass &#39;nil&#39; as the third parameter.&#xA;    req, err := http.NewRequest(&#34;GET&#34;, &#34;/health&#34;, nil)&#xA;    if err != nil {&#xA;        t.Fatal(err)&#xA;    }&#xA;&#xA;    // We create a ResponseRecorder (which satisfies http.ResponseWriter) to record the response.&#xA;    rr := httptest.NewRecorder()&#xA;    handler := http.HandlerFunc(HealthCheckHandler)&#xA;&#xA;    // Our handlers satisfy http.Handler, so we can call their ServeHTTP method&#xA;    // directly and pass in our Request and ResponseRecorder.&#xA;    handler.ServeHTTP(rr, req)&#xA;&#xA;    // Check the status code is what we expect.&#xA;    if status := rr.Code; status != http.StatusOK {&#xA;        t.Errorf(&#34;handler returned wrong status code: got %v want %v&#34;,&#xA;            status, http.StatusOK)&#xA;    }&#xA;&#xA;    // Check the response body is what we expect.&#xA;    expected := `{&#34;alive&#34;: true}`&#xA;    if rr.Body.String() != expected {&#xA;        t.Errorf(&#34;handler returned unexpected body: got %v want %v&#34;,&#xA;            rr.Body.String(), expected)&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In the case that our routes have &lt;a href=&#34;https://raw.githubusercontent.com/gorilla/mux/master/#examples&#34;&gt;variables&lt;/a&gt;, we can pass those in the request. We could write &lt;a href=&#34;https://dave.cheney.net/2013/06/09/writing-table-driven-tests-in-go&#34;&gt;table-driven tests&lt;/a&gt; to test multiple possible route variables as needed.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// endpoints.go&#xA;func main() {&#xA;    r := mux.NewRouter()&#xA;    // A route with a route variable:&#xA;    r.HandleFunc(&#34;/metrics/{type}&#34;, MetricsHandler)&#xA;&#xA;    log.Fatal(http.ListenAndServe(&#34;localhost:8080&#34;, r))&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Our test file, with a table-driven test of &lt;code&gt;routeVariables&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// endpoints_test.go&#xA;func TestMetricsHandler(t *testing.T) {&#xA;    tt := []struct{&#xA;        routeVariable string&#xA;        shouldPass bool&#xA;    }{&#xA;        {&#34;goroutines&#34;, true},&#xA;        {&#34;heap&#34;, true},&#xA;        {&#34;counters&#34;, true},&#xA;        {&#34;queries&#34;, true},&#xA;        {&#34;adhadaeqm3k&#34;, false},&#xA;    }&#xA;&#xA;    for _, tc := range tt {&#xA;        path := fmt.Sprintf(&#34;/metrics/%s&#34;, tc.routeVariable)&#xA;        req, err := http.NewRequest(&#34;GET&#34;, path, nil)&#xA;        if err != nil {&#xA;            t.Fatal(err)&#xA;        }&#xA;&#xA;        rr := httptest.NewRecorder()&#xA;&#x9;&#xA;&#x9;// Need to create a router that we can pass the request through so that the vars will be added to the context&#xA;&#x9;router := mux.NewRouter()&#xA;        router.HandleFunc(&#34;/metrics/{type}&#34;, MetricsHandler)&#xA;        router.ServeHTTP(rr, req)&#xA;&#xA;        // In this case, our MetricsHandler returns a non-200 response&#xA;        // for a route variable it doesn&#39;t know about.&#xA;        if rr.Code == http.StatusOK &amp;amp;&amp;amp; !tc.shouldPass {&#xA;            t.Errorf(&#34;handler should have failed on routeVariable %s: got %v want %v&#34;,&#xA;                tc.routeVariable, rr.Code, http.StatusOK)&#xA;        }&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Full Example&lt;/h2&gt; &#xA;&lt;p&gt;Here&#39;s a complete, runnable example of a small &lt;code&gt;mux&lt;/code&gt; based server:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package main&#xA;&#xA;import (&#xA;    &#34;net/http&#34;&#xA;    &#34;log&#34;&#xA;    &#34;github.com/gorilla/mux&#34;&#xA;)&#xA;&#xA;func YourHandler(w http.ResponseWriter, r *http.Request) {&#xA;    w.Write([]byte(&#34;Gorilla!\n&#34;))&#xA;}&#xA;&#xA;func main() {&#xA;    r := mux.NewRouter()&#xA;    // Routes consist of a path and a handler function.&#xA;    r.HandleFunc(&#34;/&#34;, YourHandler)&#xA;&#xA;    // Bind to a port and pass our router in&#xA;    log.Fatal(http.ListenAndServe(&#34;:8000&#34;, r))&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;BSD licensed. See the LICENSE file for details.&lt;/p&gt;</summary>
  </entry>
</feed>