<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Go Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-11-03T01:40:46Z</updated>
  <subtitle>Weekly Trending of Go in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>tsenart/vegeta</title>
    <updated>2024-11-03T01:40:46Z</updated>
    <id>tag:github.com,2024-11-03:/tsenart/vegeta</id>
    <link href="https://github.com/tsenart/vegeta" rel="alternate"></link>
    <summary type="html">&lt;p&gt;HTTP load testing tool and library. It&#39;s over 9000!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Vegeta &lt;a href=&#34;https://github.com/tsenart/vegeta/actions&#34;&gt;&lt;img src=&#34;https://github.com/tsenart/vegeta/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://goreportcard.com/report/github.com/tsenart/vegeta&#34;&gt;&lt;img src=&#34;https://goreportcard.com/badge/github.com/tsenart/vegeta&#34; alt=&#34;Go Report Card&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pkg.go.dev/github.com/tsenart/vegeta/v12/lib&#34;&gt;&lt;img src=&#34;https://pkg.go.dev/badge/github.com/tsenart/vegeta/v12/lib&#34; alt=&#34;PkgGoDev&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitter.im/tsenart/vegeta?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/Join%20Chat.svg?sanitize=true&#34; alt=&#34;Gitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/tsenart/vegeta/master/#donate&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/donate-bitcoin-yellow.svg?sanitize=true&#34; alt=&#34;Donate&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;Vegeta is a versatile HTTP load testing tool built out of a need to drill HTTP services with a constant request rate. &lt;a href=&#34;https://en.wikipedia.org/wiki/It&#39;s_Over_9000&#34;&gt;It&#39;s over 9000!&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;http://fc09.deviantart.net/fs49/i/2009/198/c/c/ssj2_vegeta_by_trunks24.jpg&#34; alt=&#34;Vegeta&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Usable as a command line tool and a Go library.&lt;/li&gt; &#xA; &lt;li&gt;CLI designed with UNIX composability in mind.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tsenart/vegeta/pull/92/files#r20198929&#34;&gt;Avoids&lt;/a&gt; nasty &lt;a href=&#34;http://highscalability.com/blog/2015/10/5/your-load-generator-is-probably-lying-to-you-take-the-red-pi.html&#34;&gt;Coordinated Omission&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Extensive reporting functionality.&lt;/li&gt; &#xA; &lt;li&gt;Simple to use for &lt;a href=&#34;https://kubernetes.io/blog/2015/11/one-million-requests-per-second-dependable-and-dynamic-distributed-systems-at-scale/&#34;&gt;distributed load testing&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Easy to install and run (static binary, package managers, etc).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;h3&gt;Pre-compiled executables&lt;/h3&gt; &#xA;&lt;p&gt;Get them &lt;a href=&#34;http://github.com/tsenart/vegeta/releases&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;macOS&lt;/h3&gt; &#xA;&lt;p&gt;You can install Vegeta using the &lt;a href=&#34;https://github.com/Homebrew/homebrew/&#34;&gt;Homebrew&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ brew update &amp;amp;&amp;amp; brew install vegeta&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or with &lt;a href=&#34;https://www.macports.org/&#34;&gt;MacPorts&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ port install vegeta&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Arch Linux&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ pacman -S vegeta&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;FreeBSD&lt;/h3&gt; &#xA;&lt;p&gt;On FreeBSD you can install Vegeta with the built in package manager because there is a &lt;a href=&#34;https://www.freshports.org/benchmarks/vegeta&#34;&gt;Vegeta Package&lt;/a&gt; available.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ pkg install vegeta&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Source&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/tsenart/vegeta&#xA;cd vegeta&#xA;make vegeta&#xA;mv vegeta ~/bin # Or elsewhere, up to you.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Versioning&lt;/h2&gt; &#xA;&lt;p&gt;Both the library and the CLI are versioned with &lt;a href=&#34;https://semver.org/spec/v2.0.0.html&#34;&gt;SemVer v2.0.0&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;After &lt;a href=&#34;https://github.com/tsenart/vegeta/tree/v8.0.0&#34;&gt;v8.0.0&lt;/a&gt;, the two components are versioned separately to better isolate breaking changes to each.&lt;/p&gt; &#xA;&lt;p&gt;CLI releases are tagged with &lt;code&gt;cli/vMAJOR.MINOR.PATCH&lt;/code&gt; and published on the &lt;a href=&#34;https://github.com/tsenart/vegeta/releases&#34;&gt;GitHub releases page&lt;/a&gt;. As for the library, new versions are tagged with both &lt;code&gt;lib/vMAJOR.MINOR.PATCH&lt;/code&gt; and &lt;code&gt;vMAJOR.MINOR.PATCH&lt;/code&gt;. The latter tag is required for compatibility with &lt;code&gt;go mod&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/tsenart/vegeta/master/.github/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Usage manual&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;Usage: vegeta [global flags] &amp;lt;command&amp;gt; [command flags]&#xA;&#xA;global flags:&#xA;  -cpus int&#xA;    &#x9;Number of CPUs to use (default = number of cpus)&#xA;  -profile string&#xA;    &#x9;Enable profiling of [cpu, heap]&#xA;  -version&#xA;    &#x9;Print version and exit&#xA;&#xA;attack command:&#xA;  -body string&#xA;    &#x9;Requests body file&#xA;  -cert string&#xA;    &#x9;TLS client PEM encoded certificate file&#xA;  -chunked&#xA;    &#x9;Send body with chunked transfer encoding&#xA;  -connect-to value&#xA;    &#x9;A mapping of (ip|host):port to use instead of a target URL&#39;s (ip|host):port. Can be repeated multiple times.&#xA;    &#x9;Identical src:port with different dst:port will round-robin over the different dst:port pairs.&#xA;    &#x9;Example: google.com:80:localhost:6060&#xA;  -connections int&#xA;    &#x9;Max open idle connections per target host (default 10000)&#xA;  -dns-ttl value&#xA;    &#x9;Cache DNS lookups for the given duration [-1 = disabled, 0 = forever] (default 0s)&#xA;  -duration duration&#xA;    &#x9;Duration of the test [0 = forever]&#xA;  -format string&#xA;    &#x9;Targets format [http, json] (default &#34;http&#34;)&#xA;  -h2c&#xA;    &#x9;Send HTTP/2 requests without TLS encryption&#xA;  -header value&#xA;    &#x9;Request header&#xA;  -http2&#xA;    &#x9;Send HTTP/2 requests when supported by the server (default true)&#xA;  -insecure&#xA;    &#x9;Ignore invalid server TLS certificates&#xA;  -keepalive&#xA;    &#x9;Use persistent connections (default true)&#xA;  -key string&#xA;    &#x9;TLS client PEM encoded private key file&#xA;  -laddr value&#xA;    &#x9;Local IP address (default 0.0.0.0)&#xA;  -lazy&#xA;    &#x9;Read targets lazily&#xA;  -max-body value&#xA;    &#x9;Maximum number of bytes to capture from response bodies. [-1 = no limit] (default -1)&#xA;  -max-connections int&#xA;    &#x9;Max connections per target host&#xA;  -max-workers uint&#xA;    &#x9;Maximum number of workers (default 18446744073709551615)&#xA;  -name string&#xA;    &#x9;Attack name&#xA;  -output string&#xA;    &#x9;Output file (default &#34;stdout&#34;)&#xA;  -prometheus-addr string&#xA;    &#x9;Prometheus exporter listen address [empty = disabled]. Example: 0.0.0.0:8880&#xA;  -proxy-header value&#xA;    &#x9;Proxy CONNECT header&#xA;  -rate value&#xA;    &#x9;Number of requests per time unit [0 = infinity] (default 50/1s)&#xA;  -redirects int&#xA;    &#x9;Number of redirects to follow. -1 will not follow but marks as success (default 10)&#xA;  -resolvers value&#xA;    &#x9;List of addresses (ip:port) to use for DNS resolution. Disables use of local system DNS. (comma separated list)&#xA;  -root-certs value&#xA;    &#x9;TLS root certificate files (comma separated list)&#xA;  -session-tickets&#xA;    &#x9;Enable TLS session resumption using session tickets&#xA;  -targets string&#xA;    &#x9;Targets file (default &#34;stdin&#34;)&#xA;  -timeout duration&#xA;    &#x9;Requests timeout (default 30s)&#xA;  -unix-socket string&#xA;    &#x9;Connect over a unix socket. This overrides the host address in target URLs&#xA;  -workers uint&#xA;    &#x9;Initial number of workers (default 10)&#xA;&#xA;encode command:&#xA;  -output string&#xA;    &#x9;Output file (default &#34;stdout&#34;)&#xA;  -to string&#xA;    &#x9;Output encoding [csv, gob, json] (default &#34;json&#34;)&#xA;&#xA;plot command:&#xA;  -output string&#xA;    &#x9;Output file (default &#34;stdout&#34;)&#xA;  -threshold int&#xA;    &#x9;Threshold of data points above which series are downsampled. (default 4000)&#xA;  -title string&#xA;    &#x9;Title and header of the resulting HTML page (default &#34;Vegeta Plot&#34;)&#xA;&#xA;report command:&#xA;  -buckets string&#xA;    &#x9;Histogram buckets, e.g.: &#34;[0,1ms,10ms]&#34;&#xA;  -every duration&#xA;    &#x9;Report interval&#xA;  -output string&#xA;    &#x9;Output file (default &#34;stdout&#34;)&#xA;  -type string&#xA;    &#x9;Report type to generate [text, json, hist[buckets], hdrplot] (default &#34;text&#34;)&#xA;&#xA;examples:&#xA;  echo &#34;GET http://localhost/&#34; | vegeta attack -duration=5s | tee results.bin | vegeta report&#xA;  vegeta report -type=json results.bin &amp;gt; metrics.json&#xA;  cat results.bin | vegeta plot &amp;gt; plot.html&#xA;  cat results.bin | vegeta report -type=&#34;hist[0,100ms,200ms,300ms]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;&lt;code&gt;-cpus&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Specifies the number of CPUs to be used internally. It defaults to the amount of CPUs available in the system.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;-profile&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Specifies which profiler to enable during execution. Both &lt;em&gt;cpu&lt;/em&gt; and &lt;em&gt;heap&lt;/em&gt; profiles are supported. It defaults to none.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;-version&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Prints the version and exits.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;attack&lt;/code&gt; command&lt;/h3&gt; &#xA;&lt;h4&gt;&lt;code&gt;-body&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Specifies the file whose content will be set as the body of every request unless overridden per attack target, see &lt;code&gt;-targets&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;-cert&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Specifies the PEM encoded TLS client certificate file to be used with HTTPS requests. If &lt;code&gt;-key&lt;/code&gt; isn&#39;t specified, it will be set to the value of this flag.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;-chunked&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Specifies whether to send request bodies with the chunked transfer encoding.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;-connections&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Specifies the maximum number of idle open connections per target host.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;-dns-ttl&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Specifies the duration to cache DNS lookups for. A zero value caches forever. A negative value disables caching altogether.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;-max-connections&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Specifies the maximum number of connections per target host.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;-duration&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Specifies the amount of time to issue request to the targets. The internal concurrency structure&#39;s setup has this value as a variable. The actual run time of the test can be longer than specified due to the responses delay. Use 0 for an infinite attack.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;-format&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Specifies the targets format to decode.&lt;/p&gt; &#xA;&lt;h5&gt;&lt;code&gt;json&lt;/code&gt; format&lt;/h5&gt; &#xA;&lt;p&gt;The JSON format makes integration with programs that produce targets dynamically easier. Each target is one JSON object in its own line. The method and url fields are required. If present, the body field must be base64 encoded. The generated &lt;a href=&#34;https://raw.githubusercontent.com/tsenart/vegeta/master/lib/target.schema.json&#34;&gt;JSON Schema&lt;/a&gt; defines the format in detail.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;jq -ncM &#39;{method: &#34;GET&#34;, url: &#34;http://goku&#34;, body: &#34;Punch!&#34; | @base64, header: {&#34;Content-Type&#34;: [&#34;text/plain&#34;]}}&#39; |&#xA;  vegeta attack -format=json -rate=100 | vegeta encode&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;&lt;code&gt;http&lt;/code&gt; format&lt;/h5&gt; &#xA;&lt;p&gt;The http format almost resembles the plain-text HTTP message format defined in &lt;a href=&#34;https://www.w3.org/Protocols/rfc2616/rfc2616-sec5.html&#34;&gt;RFC 2616&lt;/a&gt; but it doesn&#39;t support in-line HTTP bodies, only references to files that are loaded and used as request bodies (as exemplified below).&lt;/p&gt; &#xA;&lt;p&gt;Although targets in this format can be produced by other programs, it was originally meant to be used by people writing targets by hand for simple use cases.&lt;/p&gt; &#xA;&lt;p&gt;Here are a few examples of valid targets files in the http format:&lt;/p&gt; &#xA;&lt;h6&gt;Simple targets&lt;/h6&gt; &#xA;&lt;pre&gt;&lt;code&gt;GET http://goku:9090/path/to/dragon?item=ball&#xA;GET http://user:password@goku:9090/path/to&#xA;HEAD http://goku:9090/path/to/success&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h6&gt;Targets with custom headers&lt;/h6&gt; &#xA;&lt;pre&gt;&lt;code&gt;GET http://user:password@goku:9090/path/to&#xA;X-Account-ID: 8675309&#xA;&#xA;DELETE http://goku:9090/path/to/remove&#xA;Confirmation-Token: 90215&#xA;Authorization: Token DEADBEEF&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h6&gt;Targets with custom bodies&lt;/h6&gt; &#xA;&lt;pre&gt;&lt;code&gt;POST http://goku:9090/things&#xA;@/path/to/newthing.json&#xA;&#xA;PATCH http://goku:9090/thing/71988591&#xA;@/path/to/thing-71988591.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h6&gt;Targets with custom bodies and headers&lt;/h6&gt; &#xA;&lt;pre&gt;&lt;code&gt;POST http://goku:9090/things&#xA;X-Account-ID: 99&#xA;@/path/to/newthing.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h6&gt;Add comments&lt;/h6&gt; &#xA;&lt;p&gt;Lines starting with &lt;code&gt;#&lt;/code&gt; are ignored.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# get a dragon ball&#xA;GET http://goku:9090/path/to/dragon?item=ball&#xA;# specify a test account&#xA;X-Account-ID: 99&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;&lt;code&gt;-h2c&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Specifies that HTTP2 requests are to be sent over TCP without TLS encryption.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;-header&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Specifies a request header to be used in all targets defined, see &lt;code&gt;-targets&lt;/code&gt;. You can specify as many as needed by repeating the flag.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;-http2&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Specifies whether to enable HTTP/2 requests to servers which support it.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;-insecure&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Specifies whether to ignore invalid server TLS certificates.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;-keepalive&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Specifies whether to reuse TCP connections between HTTP requests.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;-key&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Specifies the PEM encoded TLS client certificate private key file to be used with HTTPS requests.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;-laddr&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Specifies the local IP address to be used.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;-lazy&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Specifies whether to read the input targets lazily instead of eagerly. This allows streaming targets into the attack command and reduces memory footprint. The trade-off is one of added latency in each hit against the targets.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;-max-body&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Specifies the maximum number of bytes to capture from the body of each response. Remaining unread bytes will be fully read but discarded. Set to -1 for no limit. It knows how to interpret values like these:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;&#34;10 MB&#34;&lt;/code&gt; -&amp;gt; &lt;code&gt;10MB&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&#34;10240 g&#34;&lt;/code&gt; -&amp;gt; &lt;code&gt;10TB&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&#34;2000&#34;&lt;/code&gt; -&amp;gt; &lt;code&gt;2000B&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&#34;1tB&#34;&lt;/code&gt; -&amp;gt; &lt;code&gt;1TB&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&#34;5 peta&#34;&lt;/code&gt; -&amp;gt; &lt;code&gt;5PB&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&#34;28 kilobytes&#34;&lt;/code&gt; -&amp;gt; &lt;code&gt;28KB&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&#34;1 gigabyte&#34;&lt;/code&gt; -&amp;gt; &lt;code&gt;1GB&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;&lt;code&gt;-name&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Specifies the name of the attack to be recorded in responses.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;-output&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Specifies the output file to which the binary results will be written to. Made to be piped to the report command input. Defaults to stdout.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;-rate&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Specifies the request rate per time unit to issue against the targets. The actual request rate can vary slightly due to things like garbage collection, but overall it should stay very close to the specified. If no time unit is provided, 1s is used.&lt;/p&gt; &#xA;&lt;p&gt;A &lt;code&gt;-rate&lt;/code&gt; of &lt;code&gt;0&lt;/code&gt; or &lt;code&gt;infinity&lt;/code&gt; means vegeta will send requests as fast as possible. Use together with &lt;code&gt;-max-workers&lt;/code&gt; to model a fixed set of concurrent users sending requests serially (i.e. waiting for a response before sending the next request).&lt;/p&gt; &#xA;&lt;p&gt;Setting &lt;code&gt;-max-workers&lt;/code&gt; to a very high number while setting &lt;code&gt;-rate=0&lt;/code&gt; can result in vegeta consuming too many resources and crashing. Use with care.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;-redirects&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Specifies the max number of redirects followed on each request. The default is 10. When the value is -1, redirects are not followed but the response is marked as successful.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;-resolvers&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Specifies custom DNS resolver addresses to use for name resolution instead of the ones configured by the operating system. Works only on non Windows systems.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;-root-certs&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Specifies the trusted TLS root CAs certificate files as a comma separated list. If unspecified, the default system CAs certificates will be used.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;-session-tickets&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Specifies whether to support TLS session resumption using session tickets.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;-targets&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Specifies the file from which to read targets, defaulting to stdin. See the &lt;a href=&#34;https://raw.githubusercontent.com/tsenart/vegeta/master/#-format&#34;&gt;&lt;code&gt;-format&lt;/code&gt;&lt;/a&gt; section to learn about the different target formats.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;-timeout&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Specifies the timeout for each request. A value of &lt;code&gt;0&lt;/code&gt; disables timeouts.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;-workers&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Specifies the initial number of workers used in the attack. The actual number of workers will increase if necessary in order to sustain the requested rate, unless it&#39;d go beyond &lt;code&gt;-max-workers&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;-max-workers&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Specifies the maximum number of workers used in the attack. It can be used to control the concurrency level used by an attack.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;report&lt;/code&gt; command&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;Usage: vegeta report [options] [&amp;lt;file&amp;gt;...]&#xA;&#xA;Outputs a report of attack results.&#xA;&#xA;Arguments:&#xA;  &amp;lt;file&amp;gt;  A file with vegeta attack results encoded with one of&#xA;          the supported encodings (gob | json | csv) [default: stdin]&#xA;&#xA;Options:&#xA;  --type    Which report type to generate (text | json | hist[buckets] | hdrplot).&#xA;            [default: text]&#xA;&#xA;  --buckets Histogram buckets, e.g.: &#39;[0,1ms,10ms]&#39;&#xA;&#xA;  --every   Write the report to --output at every given interval (e.g 100ms)&#xA;            The default of 0 means the report will only be written after&#xA;            all results have been processed. [default: 0]&#xA;&#xA;  --output  Output file [default: stdout]&#xA;&#xA;Examples:&#xA;  echo &#34;GET http://:80&#34; | vegeta attack -rate=10/s &amp;gt; results.gob&#xA;  echo &#34;GET http://:80&#34; | vegeta attack -rate=100/s | vegeta encode &amp;gt; results.json&#xA;  vegeta report results.*&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;&lt;code&gt;report -type=text&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;Requests      [total, rate, throughput] 1200, 120.00, 65.87&#xA;Duration      [total, attack, wait]     10.094965987s, 9.949883921s, 145.082066ms&#xA;Latencies     [min, mean, 50, 95, 99, max]  90.438129ms, 113.172398ms, 108.272568ms, 140.18235ms, 247.771566ms, 264.815246ms&#xA;Bytes In      [total, mean]             3714690, 3095.57&#xA;Bytes Out     [total, mean]             0, 0.00&#xA;Success       [ratio]                   55.42%&#xA;Status Codes  [code:count]              0:535  200:665&#xA;Error Set:&#xA;Get http://localhost:6060: dial tcp 127.0.0.1:6060: connection refused&#xA;Get http://localhost:6060: read tcp 127.0.0.1:6060: connection reset by peer&#xA;Get http://localhost:6060: dial tcp 127.0.0.1:6060: connection reset by peer&#xA;Get http://localhost:6060: write tcp 127.0.0.1:6060: broken pipe&#xA;Get http://localhost:6060: net/http: transport closed before response was received&#xA;Get http://localhost:6060: http: can&#39;t write HTTP request on broken connection&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;Requests&lt;/code&gt; row shows:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;code&gt;total&lt;/code&gt; number of issued requests.&lt;/li&gt; &#xA; &lt;li&gt;The real request &lt;code&gt;rate&lt;/code&gt; sustained during the &lt;code&gt;attack&lt;/code&gt; period.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;throughput&lt;/code&gt; of successful requests over the &lt;code&gt;total&lt;/code&gt; period.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The &lt;code&gt;Duration&lt;/code&gt; row shows:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;code&gt;attack&lt;/code&gt; time taken issuing all requests (&lt;code&gt;total&lt;/code&gt; - &lt;code&gt;wait&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;wait&lt;/code&gt; time waiting for the response to the last issued request (&lt;code&gt;total&lt;/code&gt; - &lt;code&gt;attack&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;total&lt;/code&gt; time taken in the attack (&lt;code&gt;attack&lt;/code&gt; + &lt;code&gt;wait&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Latency is the amount of time taken for a response to a request to be read (including the &lt;code&gt;-max-body&lt;/code&gt; bytes from the response body).&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;min&lt;/code&gt; is the minimum latency of all requests in an attack.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;mean&lt;/code&gt; is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Arithmetic_mean&#34;&gt;arithmetic mean / average&lt;/a&gt; of the latencies of all requests in an attack.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;50&lt;/code&gt;, &lt;code&gt;90&lt;/code&gt;, &lt;code&gt;95&lt;/code&gt;, &lt;code&gt;99&lt;/code&gt; are the 50th, 90th, 95th and 99th &lt;a href=&#34;https://en.wikipedia.org/wiki/Percentile&#34;&gt;percentiles&lt;/a&gt;, respectively, of the latencies of all requests in an attack. To understand more about why these are useful, I recommend &lt;a href=&#34;https://bravenewgeek.com/everything-you-know-about-latency-is-wrong/&#34;&gt;this article&lt;/a&gt; from @tylertreat.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;max&lt;/code&gt; is the maximum latency of all requests in an attack.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The &lt;code&gt;Bytes In&lt;/code&gt; and &lt;code&gt;Bytes Out&lt;/code&gt; rows shows:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;code&gt;total&lt;/code&gt; number of bytes sent (out) or received (in) with the request or response bodies.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;mean&lt;/code&gt; number of bytes sent (out) or received (in) with the request or response bodies.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The &lt;code&gt;Success&lt;/code&gt; ratio shows the percentage of requests whose responses didn&#39;t error and had status codes between &lt;strong&gt;200&lt;/strong&gt; and &lt;strong&gt;400&lt;/strong&gt; (non-inclusive).&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;Status Codes&lt;/code&gt; row shows a histogram of status codes. &lt;code&gt;0&lt;/code&gt; status codes mean a request failed to be sent.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;Error Set&lt;/code&gt; shows a unique set of errors returned by all issued requests. These include requests that got non-successful response status code.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;report -type=json&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;All duration like fields are in nanoseconds.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;latencies&#34;: {&#xA;    &#34;total&#34;: 237119463,&#xA;    &#34;mean&#34;: 2371194,&#xA;    &#34;50th&#34;: 2854306,&#xA;    &#34;90th&#34;: 3228223,&#xA;    &#34;95th&#34;: 3478629,&#xA;    &#34;99th&#34;: 3530000,&#xA;    &#34;max&#34;: 3660505,&#xA;    &#34;min&#34;: 1949582&#xA;  },&#xA;  &#34;buckets&#34;: {&#xA;    &#34;0&#34;: 9952,&#xA;    &#34;1000000&#34;: 40,&#xA;    &#34;2000000&#34;: 6,&#xA;    &#34;3000000&#34;: 0,&#xA;    &#34;4000000&#34;: 0,&#xA;    &#34;5000000&#34;: 2&#xA;  },&#xA;  &#34;bytes_in&#34;: {&#xA;    &#34;total&#34;: 606700,&#xA;    &#34;mean&#34;: 6067&#xA;  },&#xA;  &#34;bytes_out&#34;: {&#xA;    &#34;total&#34;: 0,&#xA;    &#34;mean&#34;: 0&#xA;  },&#xA;  &#34;earliest&#34;: &#34;2015-09-19T14:45:50.645818631+02:00&#34;,&#xA;  &#34;latest&#34;: &#34;2015-09-19T14:45:51.635818575+02:00&#34;,&#xA;  &#34;end&#34;: &#34;2015-09-19T14:45:51.639325797+02:00&#34;,&#xA;  &#34;duration&#34;: 989999944,&#xA;  &#34;wait&#34;: 3507222,&#xA;  &#34;requests&#34;: 100,&#xA;  &#34;rate&#34;: 101.01010672380401,&#xA;  &#34;throughput&#34;: 101.00012489812,&#xA;  &#34;success&#34;: 1,&#xA;  &#34;status_codes&#34;: {&#xA;    &#34;200&#34;: 100&#xA;  },&#xA;  &#34;errors&#34;: []&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In the &lt;code&gt;buckets&lt;/code&gt; field, each key is a nanosecond value representing the lower bound of a bucket. The upper bound is implied by the next higher bucket. Upper bounds are non-inclusive. The highest bucket is the overflow bucket; it has no upper bound. The values are counts of how many requests fell into that particular bucket. If the &lt;code&gt;-buckets&lt;/code&gt; parameter is not present, the &lt;code&gt;buckets&lt;/code&gt; field is omitted.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;report -type=hist&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Computes and prints a text based histogram for the given buckets. Each bucket upper bound is non-inclusive.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;cat results.bin | vegeta report -type=&#39;hist[0,2ms,4ms,6ms]&#39;&#xA;Bucket         #     %       Histogram&#xA;[0,     2ms]   6007  32.65%  ########################&#xA;[2ms,   4ms]   5505  29.92%  ######################&#xA;[4ms,   6ms]   2117  11.51%  ########&#xA;[6ms,   +Inf]  4771  25.93%  ###################&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;&lt;code&gt;report -type=hdrplot&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Writes out results in a format plottable by &lt;a href=&#34;https://hdrhistogram.github.io/HdrHistogram/plotFiles.html&#34;&gt;https://hdrhistogram.github.io/HdrHistogram/plotFiles.html&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Value(ms)  Percentile  TotalCount  1/(1-Percentile)&#xA;0.076715   0.000000    0           1.000000&#xA;0.439370   0.100000    200         1.111111&#xA;0.480836   0.200000    400         1.250000&#xA;0.495559   0.300000    599         1.428571&#xA;0.505101   0.400000    799         1.666667&#xA;0.513059   0.500000    999         2.000000&#xA;0.516664   0.550000    1099        2.222222&#xA;0.520455   0.600000    1199        2.500000&#xA;0.525008   0.650000    1299        2.857143&#xA;0.530174   0.700000    1399        3.333333&#xA;0.534891   0.750000    1499        4.000000&#xA;0.537572   0.775000    1548        4.444444&#xA;0.540340   0.800000    1598        5.000000&#xA;0.543763   0.825000    1648        5.714286&#xA;0.547164   0.850000    1698        6.666667&#xA;0.551432   0.875000    1748        8.000000&#xA;0.553444   0.887500    1773        8.888889&#xA;0.555774   0.900000    1798        10.000000&#xA;0.558454   0.912500    1823        11.428571&#xA;0.562123   0.925000    1848        13.333333&#xA;0.565563   0.937500    1873        16.000000&#xA;0.567831   0.943750    1886        17.777778&#xA;0.570617   0.950000    1898        20.000000&#xA;0.574522   0.956250    1911        22.857143&#xA;0.579046   0.962500    1923        26.666667&#xA;0.584426   0.968750    1936        32.000000&#xA;0.586695   0.971875    1942        35.555556&#xA;0.590451   0.975000    1948        40.000000&#xA;0.597543   0.978125    1954        45.714286&#xA;0.605637   0.981250    1961        53.333333&#xA;0.613564   0.984375    1967        64.000000&#xA;0.620393   0.985938    1970        71.113640&#xA;0.629121   0.987500    1973        80.000000&#xA;0.638060   0.989062    1976        91.424392&#xA;0.648085   0.990625    1979        106.666667&#xA;0.659689   0.992188    1982        128.008193&#xA;0.665870   0.992969    1984        142.227279&#xA;0.672985   0.993750    1986        160.000000&#xA;0.680101   0.994531    1987        182.848784&#xA;0.687810   0.995313    1989        213.356091&#xA;0.695729   0.996094    1990        256.016385&#xA;0.730641   0.996484    1991        284.414107&#xA;0.785516   0.996875    1992        320.000000&#xA;0.840392   0.997266    1993        365.764448&#xA;1.009646   0.997656    1993        426.621160&#xA;1.347020   0.998047    1994        512.032770&#xA;1.515276   0.998242    1994        568.828214&#xA;1.683532   0.998437    1995        639.795266&#xA;1.887487   0.998633    1995        731.528895&#xA;2.106249   0.998828    1996        853.242321&#xA;2.325011   0.999023    1996        1023.541453&#xA;2.434952   0.999121    1996        1137.656428&#xA;2.544894   0.999219    1996        1280.409731&#xA;2.589510   0.999316    1997        1461.988304&#xA;2.605192   0.999414    1997        1706.484642&#xA;2.620873   0.999512    1997        2049.180328&#xA;2.628713   0.999561    1997        2277.904328&#xA;2.636394   0.999609    1997        2557.544757&#xA;2.644234   0.999658    1997        2923.976608&#xA;2.652075   0.999707    1997        3412.969283&#xA;2.658916   0.999756    1998        4098.360656&#xA;2.658916   0.999780    1998        4545.454545&#xA;2.658916   0.999805    1998        5128.205128&#xA;2.658916   0.999829    1998        5847.953216&#xA;2.658916   0.999854    1998        6849.315068&#xA;2.658916   0.999878    1998        8196.721311&#xA;2.658916   0.999890    1998        9090.909091&#xA;2.658916   0.999902    1998        10204.081633&#xA;2.658916   0.999915    1998        11764.705882&#xA;2.658916   0.999927    1998        13698.630137&#xA;2.658916   0.999939    1998        16393.442623&#xA;2.658916   0.999945    1998        18181.818182&#xA;2.658916   0.999951    1998        20408.163265&#xA;2.658916   0.999957    1998        23255.813953&#xA;2.658916   0.999963    1998        27027.027027&#xA;2.658916   0.999969    1998        32258.064516&#xA;2.658916   0.999973    1998        37037.037037&#xA;2.658916   0.999976    1998        41666.666667&#xA;2.658916   0.999979    1998        47619.047619&#xA;2.658916   0.999982    1998        55555.555556&#xA;2.658916   0.999985    1998        66666.666667&#xA;2.658916   0.999986    1998        71428.571429&#xA;2.658916   0.999988    1998        83333.333333&#xA;2.658916   0.999989    1998        90909.090909&#xA;2.658916   0.999991    1998        111111.111111&#xA;2.658916   0.999992    1998        125000.000000&#xA;2.658916   0.999993    1998        142857.142858&#xA;2.658916   0.999994    1998        166666.666668&#xA;2.658916   0.999995    1998        199999.999999&#xA;2.658916   0.999996    1998        250000.000000&#xA;2.658916   0.999997    1998        333333.333336&#xA;2.658916   0.999998    1998        500000.000013&#xA;2.658916   0.999999    1998        999999.999971&#xA;2.658916   1.000000    1998        10000000.000000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;code&gt;encode&lt;/code&gt; command&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;Usage: vegeta encode [options] [&amp;lt;file&amp;gt;...]&#xA;&#xA;Encodes vegeta attack results from one encoding to another.&#xA;The supported encodings are Gob (binary), CSV and JSON.&#xA;Each input file may have a different encoding which is detected&#xA;automatically.&#xA;&#xA;The CSV encoder doesn&#39;t write a header. The columns written by it are:&#xA;&#xA;  1. Unix timestamp in nanoseconds since epoch&#xA;  2. HTTP status code&#xA;  3. Request latency in nanoseconds&#xA;  4. Bytes out&#xA;  5. Bytes in&#xA;  6. Error&#xA;  7. Base64 encoded response body&#xA;  8. Attack name&#xA;  9. Sequence number of request&#xA;  10. Method&#xA;  11. URL&#xA;  12. Base64 encoded response headers&#xA;&#xA;Arguments:&#xA;  &amp;lt;file&amp;gt;  A file with vegeta attack results encoded with one of&#xA;          the supported encodings (gob | json | csv) [default: stdin]&#xA;&#xA;Options:&#xA;  --to      Output encoding (gob | json | csv) [default: json]&#xA;  --output  Output file [default: stdout]&#xA;&#xA;Examples:&#xA;  echo &#34;GET http://:80&#34; | vegeta attack -rate=1/s &amp;gt; results.gob&#xA;  cat results.gob | vegeta encode | jq -c &#39;del(.body)&#39; | vegeta encode -to gob&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;code&gt;plot&lt;/code&gt; command&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/Jra1sNH.png&#34; alt=&#34;Plot&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Usage: vegeta plot [options] [&amp;lt;file&amp;gt;...]&#xA;&#xA;Outputs an HTML time series plot of request latencies over time.&#xA;The X axis represents elapsed time in seconds from the beginning&#xA;of the earliest attack in all input files. The Y axis represents&#xA;request latency in milliseconds.&#xA;&#xA;Click and drag to select a region to zoom into. Double click to zoom out.&#xA;Choose a different number on the bottom left corner input field&#xA;to change the moving average window size (in data points).&#xA;&#xA;Arguments:&#xA;  &amp;lt;file&amp;gt;  A file output by running vegeta attack [default: stdin]&#xA;&#xA;Options:&#xA;  --title      Title and header of the resulting HTML page.&#xA;               [default: Vegeta Plot]&#xA;  --threshold  Threshold of data points to downsample series to.&#xA;               Series with less than --threshold number of data&#xA;               points are not downsampled. [default: 4000]&#xA;&#xA;Examples:&#xA;  echo &#34;GET http://:80&#34; | vegeta attack -name=50qps -rate=50 -duration=5s &amp;gt; results.50qps.bin&#xA;  cat results.50qps.bin | vegeta plot &amp;gt; plot.50qps.html&#xA;  echo &#34;GET http://:80&#34; | vegeta attack -name=100qps -rate=100 -duration=5s &amp;gt; results.100qps.bin&#xA;  vegeta plot results.50qps.bin results.100qps.bin &amp;gt; plot.html&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage: Generated targets&lt;/h2&gt; &#xA;&lt;p&gt;Apart from accepting a static list of targets, Vegeta can be used together with another program that generates them in a streaming fashion. Here&#39;s an example of that using the &lt;code&gt;jq&lt;/code&gt; utility that generates targets with an incrementing id in their body.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;jq -ncM &#39;while(true; .+1) | {method: &#34;POST&#34;, url: &#34;http://:6060&#34;, body: {id: .} | @base64 }&#39; | \&#xA;  vegeta attack -rate=50/s -lazy -format=json -duration=30s | \&#xA;  tee results.bin | \&#xA;  vegeta report&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage: Distributed attacks&lt;/h2&gt; &#xA;&lt;p&gt;Whenever your load test can&#39;t be conducted due to Vegeta hitting machine limits such as open files, memory, CPU or network bandwidth, it&#39;s a good idea to use Vegeta in a distributed manner.&lt;/p&gt; &#xA;&lt;p&gt;In a hypothetical scenario where the desired attack rate is 60k requests per second, let&#39;s assume we have 3 machines with &lt;code&gt;vegeta&lt;/code&gt; installed.&lt;/p&gt; &#xA;&lt;p&gt;Make sure open file descriptor and process limits are set to a high number for your user &lt;strong&gt;on each machine&lt;/strong&gt; using the &lt;code&gt;ulimit&lt;/code&gt; command.&lt;/p&gt; &#xA;&lt;p&gt;We&#39;re ready to start the attack. All we need to do is to divide the intended rate by the number of machines, and use that number on each attack. Here we&#39;ll use &lt;a href=&#34;https://code.google.com/p/pdsh/&#34;&gt;pdsh&lt;/a&gt; for orchestration.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ PDSH_RCMD_TYPE=ssh pdsh -b -w &#39;10.0.1.1,10.0.2.1,10.0.3.1&#39; \&#xA;    &#39;echo &#34;GET http://target/&#34; | vegeta attack -rate=20000 -duration=60s &amp;gt; result.bin&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After the previous command finishes, we can gather the result files to use on our report.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ for machine in 10.0.1.1 10.0.2.1 10.0.3.1; do&#xA;    scp $machine:~/result.bin $machine.bin &amp;amp;&#xA;  done&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;report&lt;/code&gt; command accepts multiple result files. It&#39;ll read and sort them by timestamp before generating reports.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;vegeta report *.bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Another way to gather results in distributed tests is to use the built-in Prometheus Exporter and configure a Prometheus Server to get test results from all Vegeta instances. See &lt;code&gt;attack&lt;/code&gt; option &#34;prometheus-addr&#34; for more details and a complete example in the section &#34;Prometheus Support&#34;.&lt;/p&gt; &#xA;&lt;h2&gt;Usage: Real-time Analysis&lt;/h2&gt; &#xA;&lt;p&gt;If you are a happy user of iTerm, you can integrate vegeta with &lt;a href=&#34;https://github.com/rs/jplot&#34;&gt;jplot&lt;/a&gt; using &lt;a href=&#34;https://github.com/rs/jaggr&#34;&gt;jaggr&lt;/a&gt; to plot a vegeta report in real-time in the comfort of your terminal:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;echo &#39;GET http://localhost:8080&#39; | \&#xA;    vegeta attack -rate 5000 -duration 10m | vegeta encode | \&#xA;    jaggr @count=rps \&#xA;          hist\[100,200,300,400,500\]:code \&#xA;          p25,p50,p95:latency \&#xA;          sum:bytes_in \&#xA;          sum:bytes_out | \&#xA;    jplot rps+code.hist.100+code.hist.200+code.hist.300+code.hist.400+code.hist.500 \&#xA;          latency.p95+latency.p50+latency.p25 \&#xA;          bytes_in.sum+bytes_out.sum&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/ttBDsQS.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage: Library&lt;/h2&gt; &#xA;&lt;p&gt;The library versioning follows &lt;a href=&#34;https://semver.org/spec/v2.0.0.html&#34;&gt;SemVer v2.0.0&lt;/a&gt;. Since &lt;a href=&#34;https://github.com/tsenart/vegeta/tree/lib/v9.0.0&#34;&gt;lib/v9.0.0&lt;/a&gt;, the library and cli are versioned separately to better isolate breaking changes to each component.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/tsenart/vegeta/master/#Versioning&#34;&gt;Versioning&lt;/a&gt; for more details on git tag naming schemes and compatibility with &lt;code&gt;go mod&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package main&#xA;&#xA;import (&#xA;  &#34;fmt&#34;&#xA;  &#34;time&#34;&#xA;&#xA;  vegeta &#34;github.com/tsenart/vegeta/v12/lib&#34;&#xA;)&#xA;&#xA;func main() {&#xA;  rate := vegeta.Rate{Freq: 100, Per: time.Second}&#xA;  duration := 4 * time.Second&#xA;  targeter := vegeta.NewStaticTargeter(vegeta.Target{&#xA;    Method: &#34;GET&#34;,&#xA;    URL:    &#34;http://localhost:9100/&#34;,&#xA;  })&#xA;  attacker := vegeta.NewAttacker()&#xA;&#xA;  var metrics vegeta.Metrics&#xA;  for res := range attacker.Attack(targeter, rate, duration, &#34;Big Bang!&#34;) {&#xA;    metrics.Add(res)&#xA;  }&#xA;  metrics.Close()&#xA;&#xA;  fmt.Printf(&#34;99th percentile: %s\n&#34;, metrics.Latencies.P99)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Limitations&lt;/h4&gt; &#xA;&lt;p&gt;There will be an upper bound of the supported &lt;code&gt;rate&lt;/code&gt; which varies on the machine being used. You could be CPU bound (unlikely), memory bound (more likely) or have system resource limits being reached which ought to be tuned for the process execution. The important limits for us are file descriptors and processes. On a UNIX system you can get and set the current soft-limit values for a user.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ulimit -n # file descriptors&#xA;2560&#xA;$ ulimit -u # processes / threads&#xA;709&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Just pass a new number as the argument to change it.&lt;/p&gt; &#xA;&lt;h2&gt;Prometheus support&lt;/h2&gt; &#xA;&lt;p&gt;Vegeta has a built-in Prometheus Exporter that may be enabled during attacks so that you can point any Prometheus instance to Vegeta attack processes and monitor attack metrics.&lt;/p&gt; &#xA;&lt;p&gt;To enable the Prometheus Exporter on the command line, set the &#34;prometheus-addr&#34; flag.&lt;/p&gt; &#xA;&lt;p&gt;A Prometheus HTTP endpoint will be available only during the lifespan of an attack and will be closed right after the attack is finished.&lt;/p&gt; &#xA;&lt;p&gt;The following metrics are exposed:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;request_bytes_in&lt;/code&gt; - bytes count received from targeted servers by &#34;url&#34;, &#34;method&#34; and &#34;status&#34;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;request_bytes_out&lt;/code&gt; - bytes count sent to targeted server by &#34;url&#34;, &#34;method&#34; and &#34;status&#34;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;request_seconds&lt;/code&gt; - histogram with request latency and counters by &#34;url&#34;, &#34;method&#34; and &#34;status&#34;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;request_fail_count&lt;/code&gt; - count of failed requests by &#34;url&#34;, &#34;method&#34;, &#34;status&#34; and &#34;message&#34;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/tsenart/vegeta/master/lib/prom/prometheus-sample.png&#34; width=&#34;500&#34;&gt; &#xA;&lt;p&gt;Check file &lt;a href=&#34;https://raw.githubusercontent.com/tsenart/vegeta/master/lib/prom/grafana.json&#34;&gt;lib/prom/grafana.json&lt;/a&gt; with the source of this sample dashboard in Grafana.&lt;/p&gt; &#xA;&lt;h3&gt;Limitations&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Prometheus scrapes metrics from a running vegeta attack process and assigns timestamps to samples on its server. This means result timestamps aren&#39;t accurate (i.e. they&#39;re scraping time, not result time).&lt;/li&gt; &#xA; &lt;li&gt;Configuring Prometheus to scrape vegeta needs to happen out-of-band. That&#39;s a hassle!&lt;/li&gt; &#xA; &lt;li&gt;Since there&#39;s no coordination between a vegeta attack process and a Prometheus server, an attack process will finish before Prometheus has the chance to scrape the latest observations.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Why aren&#39;t we using pushgateway instead? See &lt;a href=&#34;https://github.com/tsenart/vegeta/pull/534#issuecomment-1629943731&#34;&gt;this comment&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;There&#39;s &lt;a href=&#34;https://github.com/tsenart/vegeta/issues/637&#34;&gt;an issue&lt;/a&gt; tracking the proper solution to all these limitations which is a remote write integration.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/tsenart/vegeta/master/LICENSE&#34;&gt;LICENSE&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Donate&lt;/h2&gt; &#xA;&lt;p&gt;If you use and love Vegeta, please consider sending some Satoshi to &lt;code&gt;1MDmKC51ve7Upxt75KoNM6x1qdXHFK6iW2&lt;/code&gt;. In case you want to be mentioned as a sponsor, let me know!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tsenart/vegeta/master/#donate&#34;&gt;&lt;img src=&#34;https://i.imgur.com/W9Vc51d.png&#34; alt=&#34;Donate Bitcoin&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ThreeDotsLabs/watermill</title>
    <updated>2024-11-03T01:40:46Z</updated>
    <id>tag:github.com,2024-11-03:/ThreeDotsLabs/watermill</id>
    <link href="https://github.com/ThreeDotsLabs/watermill" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Building event-driven applications the easy way in Go.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Watermill&lt;/h1&gt; &#xA;&lt;img align=&#34;right&#34; width=&#34;300&#34; src=&#34;https://watermill.io/img/gopher.svg?sanitize=true&#34;&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ThreeDotsLabs/watermill/actions/workflows/master.yml&#34;&gt;&lt;img src=&#34;https://github.com/ThreeDotsLabs/watermill/actions/workflows/master.yml/badge.svg?sanitize=true&#34; alt=&#34;CI Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pkg.go.dev/github.com/ThreeDotsLabs/watermill&#34;&gt;&lt;img src=&#34;https://pkg.go.dev/badge/github.com/ThreeDotsLabs/watermill.svg?sanitize=true&#34; alt=&#34;Go Reference&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://goreportcard.com/report/github.com/ThreeDotsLabs/watermill&#34;&gt;&lt;img src=&#34;https://goreportcard.com/badge/github.com/ThreeDotsLabs/watermill&#34; alt=&#34;Go Report Card&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/ThreeDotsLabs/watermill&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/ThreeDotsLabs/watermill/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Watermill is a Go library for working efficiently with message streams. It is intended for building event driven applications, enabling event sourcing, RPC over messages, sagas and basically whatever else comes to your mind. You can use conventional pub/sub implementations like Kafka or RabbitMQ, but also HTTP or MySQL binlog if that fits your use case.&lt;/p&gt; &#xA;&lt;h2&gt;Goals&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Easy&lt;/strong&gt; to understand.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Universal&lt;/strong&gt; - event-driven architecture, messaging, stream processing, CQRS - use it for whatever you need.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fast&lt;/strong&gt; (see &lt;a href=&#34;https://raw.githubusercontent.com/ThreeDotsLabs/watermill/master/#benchmarks&#34;&gt;Benchmarks&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Flexible&lt;/strong&gt; with middlewares, plugins and Pub/Sub configurations.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Resilient&lt;/strong&gt; - using proven technologies and passing stress tests (see &lt;a href=&#34;https://raw.githubusercontent.com/ThreeDotsLabs/watermill/master/#stability&#34;&gt;Stability&lt;/a&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Pick what you like the best or see in order:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Follow the &lt;a href=&#34;https://watermill.io/docs/getting-started/&#34;&gt;Getting Started guide&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;See examples below.&lt;/li&gt; &#xA; &lt;li&gt;Read the full documentation: &lt;a href=&#34;https://watermill.io/&#34;&gt;https://watermill.io/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Our online hands-on training&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://threedots.tech/event-driven/?utm_source=watermill-readme&#34;&gt;&lt;img align=&#34;center&#34; width=&#34;400&#34; src=&#34;https://threedots.tech/event-driven-banner.png&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Basic &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ThreeDotsLabs/watermill/master/_examples/basic/1-your-first-app&#34;&gt;Your first app&lt;/a&gt; - &lt;strong&gt;start here!&lt;/strong&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ThreeDotsLabs/watermill/master/_examples/basic/2-realtime-feed&#34;&gt;Realtime feed&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ThreeDotsLabs/watermill/master/_examples/basic/3-router&#34;&gt;Router&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ThreeDotsLabs/watermill/master/_examples/basic/4-metrics&#34;&gt;Metrics&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ThreeDotsLabs/watermill/master/_examples/basic/5-cqrs-protobuf&#34;&gt;CQRS with protobuf&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ThreeDotsLabs/watermill/master/_examples/pubsubs&#34;&gt;Pub/Subs usage&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;These examples are part of the &lt;a href=&#34;https://watermill.io/docs/getting-started/&#34;&gt;Getting started guide&lt;/a&gt; and show usage of a single Pub/Sub at a time.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Real-world examples &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ThreeDotsLabs/watermill/master/_examples/real-world-examples/exactly-once-delivery-counter&#34;&gt;Exactly-once delivery counter&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ThreeDotsLabs/watermill/master/_examples/real-world-examples/receiving-webhooks&#34;&gt;Receiving webhooks&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ThreeDotsLabs/watermill/master/_examples/real-world-examples/sending-webhooks&#34;&gt;Sending webhooks&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ThreeDotsLabs/watermill/master/_examples/real-world-examples/synchronizing-databases&#34;&gt;Synchronizing Databases&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ThreeDotsLabs/watermill/master/_examples/real-world-examples/persistent-event-log&#34;&gt;Persistent Event Log&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ThreeDotsLabs/watermill/master/_examples/real-world-examples/transactional-events&#34;&gt;Transactional Events&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ThreeDotsLabs/watermill/master/_examples/real-world-examples/server-sent-events&#34;&gt;Real-time HTTP updates with Server-Sent Events&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ThreeDotsLabs/watermill/master/_examples/real-world-examples/server-sent-events-htmx&#34;&gt;Real-time HTTP updates with Server-Sent Events and htmx&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Complete projects &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/ThreeDotsLabs/nats-example&#34;&gt;NATS example with live code reloading&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/ThreeDotsLabs/event-driven-example&#34;&gt;RabbitMQ, webhooks and Kafka integration&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Background&lt;/h2&gt; &#xA;&lt;p&gt;Building distributed and scalable services is rarely as easy as some may suggest. There is a lot of hidden knowledge that comes with writing such systems. Just like you don&#39;t need to know the whole TCP stack to create a HTTP REST server, you shouldn&#39;t need to study all of this knowledge to start with building message-driven applications.&lt;/p&gt; &#xA;&lt;p&gt;Watermill&#39;s goal is to make communication with messages as easy to use as HTTP routers. It provides the tools needed to begin working with event-driven architecture and allows you to learn the details on the go.&lt;/p&gt; &#xA;&lt;p&gt;At the heart of Watermill there is one simple interface:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func(*Message) ([]*Message, error)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Your handler receives a message and decides whether to publish new message(s) or return an error. What happens next is up to the middlewares you&#39;ve chosen.&lt;/p&gt; &#xA;&lt;p&gt;You can find more about our motivations in our &lt;a href=&#34;https://threedots.tech/post/introducing-watermill/&#34;&gt;&lt;em&gt;Introducing Watermill&lt;/em&gt; blog post&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Pub/Subs&lt;/h2&gt; &#xA;&lt;p&gt;All publishers and subscribers have to implement an interface:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;type Publisher interface {&#xA;&#x9;Publish(topic string, messages ...*Message) error&#xA;&#x9;Close() error&#xA;}&#xA;&#xA;type Subscriber interface {&#xA;&#x9;Subscribe(ctx context.Context, topic string) (&amp;lt;-chan *Message, error)&#xA;&#x9;Close() error&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Supported Pub/Subs:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;AMQP Pub/Sub &lt;a href=&#34;https://github.com/ThreeDotsLabs/watermill-amqp/&#34;&gt;(&lt;code&gt;github.com/ThreeDotsLabs/watermill-amqp/v2&lt;/code&gt;)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Bolt Pub/Sub &lt;a href=&#34;https://github.com/ThreeDotsLabs/watermill-bolt/&#34;&gt;(&lt;code&gt;github.com/ThreeDotsLabs/watermill-bolt&lt;/code&gt;)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Firestore Pub/Sub &lt;a href=&#34;https://github.com/ThreeDotsLabs/watermill-firestore/&#34;&gt;(&lt;code&gt;github.com/ThreeDotsLabs/watermill-firestore&lt;/code&gt;)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Google Cloud Pub/Sub &lt;a href=&#34;https://github.com/ThreeDotsLabs/watermill-googlecloud/&#34;&gt;(&lt;code&gt;github.com/ThreeDotsLabs/watermill-googlecloud&lt;/code&gt;)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;HTTP Pub/Sub &lt;a href=&#34;https://github.com/ThreeDotsLabs/watermill-http/&#34;&gt;(&lt;code&gt;github.com/ThreeDotsLabs/watermill-http&lt;/code&gt;)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;io.Reader/io.Writer Pub/Sub &lt;a href=&#34;https://github.com/ThreeDotsLabs/watermill-io/&#34;&gt;(&lt;code&gt;github.com/ThreeDotsLabs/watermill-io&lt;/code&gt;)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Kafka Pub/Sub &lt;a href=&#34;https://github.com/ThreeDotsLabs/watermill-kafka/&#34;&gt;(&lt;code&gt;github.com/ThreeDotsLabs/watermill-kafka/v2&lt;/code&gt;)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;NATS Pub/Sub &lt;a href=&#34;https://github.com/ThreeDotsLabs/watermill-nats/&#34;&gt;(&lt;code&gt;github.com/ThreeDotsLabs/watermill-nats&lt;/code&gt;)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Redis Stream Pub/Sub &lt;a href=&#34;https://github.com/ThreeDotsLabs/watermill-redisstream/&#34;&gt;(&lt;code&gt;github.com/ThreeDotsLabs/watermill-redisstream&lt;/code&gt;)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;SQL Pub/Sub &lt;a href=&#34;https://github.com/ThreeDotsLabs/watermill-sql/&#34;&gt;(&lt;code&gt;github.com/ThreeDotsLabs/watermill-sql/v2&lt;/code&gt;)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;All Pub/Subs implementation documentation can be found in the &lt;a href=&#34;https://watermill.io/pubsubs/&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Unofficial libraries&lt;/h2&gt; &#xA;&lt;p&gt;Can&#39;t find your favorite Pub/Sub or library integration? Check &lt;a href=&#34;https://watermill.io/docs/awesome/&#34;&gt;Awesome Watermill&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you know another library or are an author of one, please &lt;a href=&#34;https://github.com/ThreeDotsLabs/watermill/edit/master/docs/content/docs/awesome.md&#34;&gt;add it to the list&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Please check our &lt;a href=&#34;https://raw.githubusercontent.com/ThreeDotsLabs/watermill/master/CONTRIBUTING.md&#34;&gt;contributing guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Stability&lt;/h2&gt; &#xA;&lt;p&gt;Watermill v1.0.0 has been released and is production-ready. The public API is stable and will not change without changing the major version.&lt;/p&gt; &#xA;&lt;p&gt;To ensure that all Pub/Subs are stable and safe to use in production, we created a &lt;a href=&#34;https://github.com/ThreeDotsLabs/watermill/raw/master/pubsub/tests/test_pubsub.go#L34&#34;&gt;set of tests&lt;/a&gt; that need to pass for each of the implementations before merging to master. All tests are also executed in &lt;a href=&#34;https://github.com/ThreeDotsLabs/watermill/raw/master/pubsub/tests/test_pubsub.go#L171&#34;&gt;&lt;em&gt;stress&lt;/em&gt;&lt;/a&gt; mode - that means that we are running all the tests &lt;strong&gt;20x&lt;/strong&gt; in parallel.&lt;/p&gt; &#xA;&lt;p&gt;All tests are run with the race condition detector enabled (&lt;code&gt;-race&lt;/code&gt; flag in tests).&lt;/p&gt; &#xA;&lt;p&gt;For more information about debugging tests, you should check &lt;a href=&#34;http://watermill.io/docs/troubleshooting/#debugging-pubsub-tests&#34;&gt;tests troubleshooting guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Benchmarks&lt;/h2&gt; &#xA;&lt;p&gt;Initial tools for benchmarking Pub/Subs can be found in &lt;a href=&#34;https://github.com/ThreeDotsLabs/watermill-benchmark&#34;&gt;watermill-benchmark&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;All benchmarks are being done on a single 16 CPU VM instance, running one binary and dependencies in Docker Compose.&lt;/p&gt; &#xA;&lt;p&gt;These numbers are meant to serve as a rough estimate of how fast messages can be processed by different Pub/Subs. Keep in mind that the results can be vastly different, depending on the setup and configuration (both much lower and higher).&lt;/p&gt; &#xA;&lt;p&gt;Here&#39;s the short version for message size of 16 bytes.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Pub/Sub&lt;/th&gt; &#xA;   &lt;th&gt;Publish (messages / s)&lt;/th&gt; &#xA;   &lt;th&gt;Subscribe (messages / s)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GoChannel&lt;/td&gt; &#xA;   &lt;td&gt;331,882&lt;/td&gt; &#xA;   &lt;td&gt;118,943&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Redis Streams&lt;/td&gt; &#xA;   &lt;td&gt;61,642&lt;/td&gt; &#xA;   &lt;td&gt;11,213&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NATS Jetstream (16 Subscribers)&lt;/td&gt; &#xA;   &lt;td&gt;49,255&lt;/td&gt; &#xA;   &lt;td&gt;33,009&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Kafka (one node)&lt;/td&gt; &#xA;   &lt;td&gt;44,090&lt;/td&gt; &#xA;   &lt;td&gt;108,285&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SQL (MySQL)&lt;/td&gt; &#xA;   &lt;td&gt;5,599&lt;/td&gt; &#xA;   &lt;td&gt;167&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SQL (PostgreSQL, batch size=1)&lt;/td&gt; &#xA;   &lt;td&gt;3,834&lt;/td&gt; &#xA;   &lt;td&gt;455&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Google Cloud Pub/Sub&lt;/td&gt; &#xA;   &lt;td&gt;3,689&lt;/td&gt; &#xA;   &lt;td&gt;30,229&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AMQP&lt;/td&gt; &#xA;   &lt;td&gt;2,702&lt;/td&gt; &#xA;   &lt;td&gt;13,192&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;p&gt;If you didn&#39;t find the answer to your question in &lt;a href=&#34;https://watermill.io/&#34;&gt;the documentation&lt;/a&gt;, feel free to ask us directly!&lt;/p&gt; &#xA;&lt;p&gt;Please join us on the &lt;code&gt;#watermill&lt;/code&gt; channel on the &lt;a href=&#34;https://discord.gg/QV6VFg4YQE&#34;&gt;Three Dots Labs Discord&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Why the name?&lt;/h2&gt; &#xA;&lt;p&gt;It processes streams!&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ThreeDotsLabs/watermill/master/LICENSE&#34;&gt;MIT License&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>deepflowio/deepflow</title>
    <updated>2024-11-03T01:40:46Z</updated>
    <id>tag:github.com,2024-11-03:/deepflowio/deepflow</id>
    <link href="https://github.com/deepflowio/deepflow" rel="alternate"></link>
    <summary type="html">&lt;p&gt;eBPF Observability - Distributed Tracing and Profiling&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/deepflowio/deepflow/main/docs/deepflow-logo.png&#34; alt=&#34;DeepFlow&#34; width=&#34;300&#34;&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt;Instant Observability for Cloud &amp;amp; AI Applications&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;Zero Code, Full Stack, eBPF &amp;amp; Wasm&lt;/p&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://zenodo.org/badge/latestdoi/448599559&#34;&gt;&lt;img src=&#34;https://zenodo.org/badge/448599559.svg?sanitize=true&#34; alt=&#34;DOI&#34;&gt;&lt;/a&gt; &lt;img alt=&#34;GitHub Release&#34; src=&#34;https://img.shields.io/github/v/release/deepflowio/deepflow&#34;&gt;  &lt;img alt=&#34;docker pulls&#34; src=&#34;https://img.shields.io/docker/pulls/deepflowce/deepflow-agent?color=green?label=docker%20pulls&#34;&gt;  &lt;img alt=&#34;License&#34; src=&#34;https://img.shields.io/github/license/deepflowio/deepflow?color=purple&#34;&gt;  &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;English | &lt;a href=&#34;https://raw.githubusercontent.com/deepflowio/deepflow/main/README-CN.md&#34;&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/deepflowio/deepflow/main/README-JP.md&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;What is DeepFlow&lt;/h1&gt; &#xA;&lt;p&gt;The DeepFlow open-source project aims to provide deep observability for complex cloud-native and AI applications. DeepFlow implemented &lt;strong&gt;Zero Code&lt;/strong&gt; data collection with eBPF for metrics, distributed tracing, request logs and function profiling, and is further integrated with &lt;strong&gt;SmartEncoding&lt;/strong&gt; to achieve &lt;strong&gt;Full Stack&lt;/strong&gt; correlation and efficient access to all observability data. With DeepFlow, cloud-native and AI applications automatically gain deep observability, removing the heavy burden of developers continually instrumenting code and providing monitoring and diagnostic capabilities covering everything from code to infrastructure for DevOps/SRE teams.&lt;/p&gt; &#xA;&lt;h1&gt;Key Features&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Universal Map&lt;/strong&gt; for &lt;strong&gt;Any&lt;/strong&gt; Service: DeepFlow provides a universal map with &lt;strong&gt;Zero Code&lt;/strong&gt; by eBPF for production environments, including application services, AI services, and infrastructure services in any language. In addition to analyzing common protocols, Wasm plugins are supported for your private protocols. &lt;strong&gt;Full Stack&lt;/strong&gt; golden signals of applications and infrastructures are calculated, pinpointing performance bottlenecks at ease.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Distributed Tracing&lt;/strong&gt; for &lt;strong&gt;Any&lt;/strong&gt; Request: &lt;strong&gt;Zero Code&lt;/strong&gt; distributed tracing powered by eBPF supports applications in any language and infrastructures including gateways, service meshes, databases, message queues, DNS and NICs, leaving no blind spots. &lt;strong&gt;Full Stack&lt;/strong&gt; network performance metrics and file I/O events are automatically collected for each Span. Distributed tracing enters a new era: Zero Instrumentation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Continuous Profiling&lt;/strong&gt; for &lt;strong&gt;Any&lt;/strong&gt; Function: DeepFlow collects profiling data at a cost of below 1% with &lt;strong&gt;Zero Code&lt;/strong&gt;, plots OnCPU/OffCPU/GPU/Memory/Network function call stack flame graphs, locates &lt;strong&gt;Full Stack&lt;/strong&gt; performance bottleneck in business functions, library and framework functions, runtime functions, shared library functions, kernel function, CUDA functions, and automatically relates them to distrubuted tracing data.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Seamless Integration&lt;/strong&gt; with Popular Stack: DeepFlow can serve as storage backed for Prometheus, OpenTelemetry, SkyWalking and Pyroscope. It also provides &lt;strong&gt;SQL, PromQL and OLTP&lt;/strong&gt; APIs to work as data source in popular observability stacks. It injects meta tags for all observability signals including cloud resource, K8s container, K8s labels, K8s annotations, CMDB business attributes, etc., eliminating data silos.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Performance 10x ClickHouse&lt;/strong&gt;: &lt;strong&gt;SmartEncoding&lt;/strong&gt; injects standardized and pre-encoded meta tags into all observability data, reducing storage overhead by 10x compared to ClickHouse String or LowCard method. Custom tags and observability data are stored separately, making tags available for almost unlimited dimensions and cardinalities with uncompromised query experience like &lt;strong&gt;BigTable&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Documentation&lt;/h1&gt; &#xA;&lt;p&gt;For more information, please visit &lt;a href=&#34;https://deepflow.io/docs/?from=github&#34;&gt;the documentation website&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Quick start&lt;/h1&gt; &#xA;&lt;p&gt;There are three editions of DeepFlow:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;DeepFlow Community: for developers&lt;/li&gt; &#xA; &lt;li&gt;DeepFlow Enterprise: for organizations, solving team collaboration problems&lt;/li&gt; &#xA; &lt;li&gt;DeepFlow Cloud: SaaS service, currently in beta&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The DeepFlow Community Edition consists of the core components of the Enterprise Edition.&lt;/p&gt; &#xA;&lt;h2&gt;DeepFlow Community&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://deepflow.io/docs/ce-install/all-in-one/?from=github&#34;&gt;the deployment documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;At the same time, we have also built a complete &lt;a href=&#34;https://ce-demo.deepflow.yunshan.net/?from=github&#34;&gt;DeepFlow Community Demo&lt;/a&gt;, welcome to experience it. Login account/password: deepflow/deepflow.&lt;/p&gt; &#xA;&lt;h2&gt;DeepFlow Enterprise&lt;/h2&gt; &#xA;&lt;p&gt;You can visit the &lt;a href=&#34;https://deepflow.io/&#34;&gt;DeepFlow Enterprise Demo&lt;/a&gt;, currently available in Chinese only.&lt;/p&gt; &#xA;&lt;h1&gt;Compile DeepFlow from Source&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepflowio/deepflow/main/agent/build.md&#34;&gt;compile deepflow-agent&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Software Architecture&lt;/h1&gt; &#xA;&lt;p&gt;DeepFlow Community Edition consists of two components, Agent and Server. An Agent runs in each K8s node, legacy host and cloud host, and is responsible for AutoMetrics and AutoTracing data collection of all application processes on the host. Server runs in a K8s cluster and provides Agent management, tag injection, data ingest and query services.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/deepflowio/deepflow/main/docs/deepflow-architecture.png&#34; alt=&#34;DeepFlow Architecture&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Milestones&lt;/h1&gt; &#xA;&lt;p&gt;Here is our &lt;a href=&#34;https://deepflow.io/docs/about/milestone/?from=github&#34;&gt;future feature plan&lt;/a&gt;. Issues and Pull Requests are welcome.&lt;/p&gt; &#xA;&lt;h1&gt;Contact Us&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;DiscordClick &lt;a href=&#34;https://discord.gg/QJ7Dyj4wWM&#34;&gt;here&lt;/a&gt; to join our discussion.&lt;/li&gt; &#xA; &lt;li&gt;Twitter&lt;a href=&#34;https://twitter.com/deepflowio&#34;&gt;DeepFlow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;WeChat Group&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/deepflowio/deepflow/main/docs/wechat-group-keeper.png&#34; width=&#34;30%&#34;&gt; &#xA;&lt;h1&gt;Acknowledgments&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Thanks &lt;a href=&#34;https://ebpf.io/&#34;&gt;eBPF&lt;/a&gt;, a revolutionary Linux kernel technology.&lt;/li&gt; &#xA; &lt;li&gt;Thanks &lt;a href=&#34;https://opentelemetry.io/&#34;&gt;OpenTelemetry&lt;/a&gt;, provides vendor-neutral APIs to collect application telemetry data.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Honors&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The paper &lt;a href=&#34;https://dl.acm.org/doi/10.1145/3603269.3604823&#34;&gt;Network-Centric Distributed Tracing with DeepFlow: Troubleshooting Your Microservices in Zero Code&lt;/a&gt; has been accepted by ACM SIGCOMM 2023.&lt;/li&gt; &#xA; &lt;li&gt;DeepFlow enriches the &lt;a href=&#34;https://landscape.cncf.io/?selected=deep-flow&#34;&gt;CNCF CLOUD NATIVE Landscape&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;DeepFlow enriches the &lt;a href=&#34;https://landscape.cncf.io/?selected=deep-flow&amp;amp;group=cnai&amp;amp;item=cnai--model-llm-observability--deepflow&#34;&gt;CNCF CNAI (Cloud-Native AI) Landscape&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;DeepFlow enriches the &lt;a href=&#34;https://ebpf.io/applications#deepflow&#34;&gt;eBPF Project Landscape&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>