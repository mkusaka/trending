<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Go Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-09-01T02:10:20Z</updated>
  <subtitle>Monthly Trending of Go in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>songquanpeng/one-api</title>
    <updated>2023-09-01T02:10:20Z</updated>
    <id>tag:github.com,2023-09-01:/songquanpeng/one-api</id>
    <link href="https://github.com/songquanpeng/one-api" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OpenAI 接口管理 &amp; 分发系统，支持 Azure、Anthropic Claude、Google PaLM 2、智谱 ChatGLM、百度文心一言、讯飞星火认知、阿里通义千问以及 360 智脑，可用于二次分发管理 key，仅单可执行文件，已打包好 Docker 镜像，一键部署，开箱即用. OpenAI key management &amp; redistribution system, using a single API for all LLMs, and features an English UI.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;right&#34;&gt; &lt;strong&gt;中文&lt;/strong&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/songquanpeng/one-api/main/README.en.md&#34;&gt;English&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/songquanpeng/one-api/main/README.ja.md&#34;&gt;日本語&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/songquanpeng/one-api&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/songquanpeng/one-api/main/web/public/logo.png&#34; width=&#34;150&#34; height=&#34;150&#34; alt=&#34;one-api logo&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;One API&lt;/h1&gt; &#xA; &lt;p&gt;&lt;em&gt;✨ 通过标准的 OpenAI API 格式访问所有的大模型，开箱即用 ✨&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/songquanpeng/one-api/main/LICENSE&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/license/songquanpeng/one-api?color=brightgreen&#34; alt=&#34;license&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/songquanpeng/one-api/releases/latest&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/v/release/songquanpeng/one-api?color=brightgreen&amp;amp;include_prereleases&#34; alt=&#34;release&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/repository/docker/justsong/one-api&#34;&gt; &lt;img src=&#34;https://img.shields.io/docker/pulls/justsong/one-api?color=brightgreen&#34; alt=&#34;docker pull&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/songquanpeng/one-api/releases/latest&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/downloads/songquanpeng/one-api/total?color=brightgreen&amp;amp;include_prereleases&#34; alt=&#34;release&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://goreportcard.com/report/github.com/songquanpeng/one-api&#34;&gt; &lt;img src=&#34;https://goreportcard.com/badge/github.com/songquanpeng/one-api&#34; alt=&#34;GoReportCard&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/songquanpeng/one-api#部署&#34;&gt;部署教程&lt;/a&gt; · &lt;a href=&#34;https://github.com/songquanpeng/one-api#使用方法&#34;&gt;使用方法&lt;/a&gt; · &lt;a href=&#34;https://github.com/songquanpeng/one-api/issues&#34;&gt;意见反馈&lt;/a&gt; · &lt;a href=&#34;https://github.com/songquanpeng/one-api#截图展示&#34;&gt;截图展示&lt;/a&gt; · &lt;a href=&#34;https://openai.justsong.cn/&#34;&gt;在线演示&lt;/a&gt; · &lt;a href=&#34;https://github.com/songquanpeng/one-api#常见问题&#34;&gt;常见问题&lt;/a&gt; · &lt;a href=&#34;https://github.com/songquanpeng/one-api#相关项目&#34;&gt;相关项目&lt;/a&gt; · &lt;a href=&#34;https://iamazing.cn/page/reward&#34;&gt;赞赏支持&lt;/a&gt; &lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; 本项目为开源项目，使用者必须在遵循 OpenAI 的&lt;a href=&#34;https://openai.com/policies/terms-of-use&#34;&gt;使用条款&lt;/a&gt;以及&lt;strong&gt;法律法规&lt;/strong&gt;的情况下使用，不得用于非法用途。&lt;/p&gt; &#xA; &lt;p&gt;根据&lt;a href=&#34;http://www.cac.gov.cn/2023-07/13/c_1690898327029107.htm&#34;&gt;《生成式人工智能服务管理暂行办法》&lt;/a&gt;的要求，请勿对中国地区公众提供一切未经备案的生成式人工智能服务。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt; 使用 Docker 拉取的最新镜像可能是 &lt;code&gt;alpha&lt;/code&gt; 版本，如果追求稳定性请手动指定版本。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;功能&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;支持多种大模型： &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://platform.openai.com/docs/guides/gpt/chat-completions-api&#34;&gt;OpenAI ChatGPT 系列模型&lt;/a&gt;（支持 &lt;a href=&#34;https://learn.microsoft.com/en-us/azure/ai-services/openai/reference&#34;&gt;Azure OpenAI API&lt;/a&gt;）&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://anthropic.com&#34;&gt;Anthropic Claude 系列模型&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://developers.generativeai.google&#34;&gt;Google PaLM2 系列模型&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://cloud.baidu.com/doc/WENXINWORKSHOP/index.html&#34;&gt;百度文心一言系列模型&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://help.aliyun.com/document_detail/2400395.html&#34;&gt;阿里通义千问系列模型&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://www.xfyun.cn/doc/spark/Web.html&#34;&gt;讯飞星火认知大模型&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://bigmodel.cn&#34;&gt;智谱 ChatGLM 系列模型&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://ai.360.cn&#34;&gt;360 智脑&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;支持配置镜像以及众多第三方代理服务： &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://openai-sb.com&#34;&gt;OpenAI-SB&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://api2d.com/r/197971&#34;&gt;API2D&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://aigptx.top?aff=uFpUl2Kf&#34;&gt;OhMyGPT&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://aiproxy.io/?i=OneAPI&#34;&gt;AI Proxy&lt;/a&gt; （邀请码：&lt;code&gt;OneAPI&lt;/code&gt;）&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://console.closeai-asia.com/r/2412&#34;&gt;CloseAI&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 自定义渠道：例如各种未收录的第三方代理服务&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;支持通过&lt;strong&gt;负载均衡&lt;/strong&gt;的方式访问多个渠道。&lt;/li&gt; &#xA; &lt;li&gt;支持 &lt;strong&gt;stream 模式&lt;/strong&gt;，可以通过流式传输实现打字机效果。&lt;/li&gt; &#xA; &lt;li&gt;支持&lt;strong&gt;多机部署&lt;/strong&gt;，&lt;a href=&#34;https://raw.githubusercontent.com/songquanpeng/one-api/main/#%E5%A4%9A%E6%9C%BA%E9%83%A8%E7%BD%B2&#34;&gt;详见此处&lt;/a&gt;。&lt;/li&gt; &#xA; &lt;li&gt;支持&lt;strong&gt;令牌管理&lt;/strong&gt;，设置令牌的过期时间和额度。&lt;/li&gt; &#xA; &lt;li&gt;支持&lt;strong&gt;兑换码管理&lt;/strong&gt;，支持批量生成和导出兑换码，可使用兑换码为账户进行充值。&lt;/li&gt; &#xA; &lt;li&gt;支持&lt;strong&gt;通道管理&lt;/strong&gt;，批量创建通道。&lt;/li&gt; &#xA; &lt;li&gt;支持&lt;strong&gt;用户分组&lt;/strong&gt;以及&lt;strong&gt;渠道分组&lt;/strong&gt;，支持为不同分组设置不同的倍率。&lt;/li&gt; &#xA; &lt;li&gt;支持渠道&lt;strong&gt;设置模型列表&lt;/strong&gt;。&lt;/li&gt; &#xA; &lt;li&gt;支持&lt;strong&gt;查看额度明细&lt;/strong&gt;。&lt;/li&gt; &#xA; &lt;li&gt;支持&lt;strong&gt;用户邀请奖励&lt;/strong&gt;。&lt;/li&gt; &#xA; &lt;li&gt;支持以美元为单位显示额度。&lt;/li&gt; &#xA; &lt;li&gt;支持发布公告，设置充值链接，设置新用户初始额度。&lt;/li&gt; &#xA; &lt;li&gt;支持模型映射，重定向用户的请求模型。&lt;/li&gt; &#xA; &lt;li&gt;支持失败自动重试。&lt;/li&gt; &#xA; &lt;li&gt;支持绘图接口。&lt;/li&gt; &#xA; &lt;li&gt;支持丰富的&lt;strong&gt;自定义&lt;/strong&gt;设置， &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;支持自定义系统名称，logo 以及页脚。&lt;/li&gt; &#xA;   &lt;li&gt;支持自定义首页和关于页面，可以选择使用 HTML &amp;amp; Markdown 代码进行自定义，或者使用一个单独的网页通过 iframe 嵌入。&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;支持通过系统访问令牌访问管理 API。&lt;/li&gt; &#xA; &lt;li&gt;支持 Cloudflare Turnstile 用户校验。&lt;/li&gt; &#xA; &lt;li&gt;支持用户管理，支持&lt;strong&gt;多种用户登录注册方式&lt;/strong&gt;： &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;邮箱登录注册（支持注册邮箱白名单）以及通过邮箱进行密码重置。&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/settings/applications/new&#34;&gt;GitHub 开放授权&lt;/a&gt;。&lt;/li&gt; &#xA;   &lt;li&gt;微信公众号授权（需要额外部署 &lt;a href=&#34;https://github.com/songquanpeng/wechat-server&#34;&gt;WeChat Server&lt;/a&gt;）。&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;部署&lt;/h2&gt; &#xA;&lt;h3&gt;基于 Docker 进行部署&lt;/h3&gt; &#xA;&lt;p&gt;部署命令：&lt;code&gt;docker run --name one-api -d --restart always -p 3000:3000 -e TZ=Asia/Shanghai -v /home/ubuntu/data/one-api:/data justsong/one-api&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;其中，&lt;code&gt;-p 3000:3000&lt;/code&gt; 中的第一个 &lt;code&gt;3000&lt;/code&gt; 是宿主机的端口，可以根据需要进行修改。&lt;/p&gt; &#xA;&lt;p&gt;数据将会保存在宿主机的 &lt;code&gt;/home/ubuntu/data/one-api&lt;/code&gt; 目录，请确保该目录存在且具有写入权限，或者更改为合适的目录。&lt;/p&gt; &#xA;&lt;p&gt;如果启动失败，请添加 &lt;code&gt;--privileged=true&lt;/code&gt;，具体参考 #482。&lt;/p&gt; &#xA;&lt;p&gt;如果上面的镜像无法拉取，可以尝试使用 GitHub 的 Docker 镜像，将上面的 &lt;code&gt;justsong/one-api&lt;/code&gt; 替换为 &lt;code&gt;ghcr.io/songquanpeng/one-api&lt;/code&gt; 即可。&lt;/p&gt; &#xA;&lt;p&gt;如果你的并发量较大，&lt;strong&gt;务必&lt;/strong&gt;设置 &lt;code&gt;SQL_DSN&lt;/code&gt;，详见下面&lt;a href=&#34;https://raw.githubusercontent.com/songquanpeng/one-api/main/#%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F&#34;&gt;环境变量&lt;/a&gt;一节。&lt;/p&gt; &#xA;&lt;p&gt;更新命令：&lt;code&gt;docker run --rm -v /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower -cR&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Nginx 的参考配置：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;server{&#xA;   server_name openai.justsong.cn;  # 请根据实际情况修改你的域名&#xA;   &#xA;   location / {&#xA;          client_max_body_size  64m;&#xA;          proxy_http_version 1.1;&#xA;          proxy_pass http://localhost:3000;  # 请根据实际情况修改你的端口&#xA;          proxy_set_header Host $host;&#xA;          proxy_set_header X-Forwarded-For $remote_addr;&#xA;          proxy_cache_bypass $http_upgrade;&#xA;          proxy_set_header Accept-Encoding gzip;&#xA;          proxy_read_timeout 300s;  # GPT-4 需要较长的超时时间，请自行调整&#xA;   }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;之后使用 Let&#39;s Encrypt 的 certbot 配置 HTTPS：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Ubuntu 安装 certbot：&#xA;sudo snap install --classic certbot&#xA;sudo ln -s /snap/bin/certbot /usr/bin/certbot&#xA;# 生成证书 &amp;amp; 修改 Nginx 配置&#xA;sudo certbot --nginx&#xA;# 根据指示进行操作&#xA;# 重启 Nginx&#xA;sudo service nginx restart&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;初始账号用户名为 &lt;code&gt;root&lt;/code&gt;，密码为 &lt;code&gt;123456&lt;/code&gt;。&lt;/p&gt; &#xA;&lt;h3&gt;手动部署&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;从 &lt;a href=&#34;https://github.com/songquanpeng/one-api/releases/latest&#34;&gt;GitHub Releases&lt;/a&gt; 下载可执行文件或者从源码编译： &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/songquanpeng/one-api.git&#xA;&#xA;# 构建前端&#xA;cd one-api/web&#xA;npm install&#xA;npm run build&#xA;&#xA;# 构建后端&#xA;cd ..&#xA;go mod download&#xA;go build -ldflags &#34;-s -w&#34; -o one-api&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;运行： &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;chmod u+x one-api&#xA;./one-api --port 3000 --log-dir ./logs&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;访问 &lt;a href=&#34;http://localhost:3000/&#34;&gt;http://localhost:3000/&lt;/a&gt; 并登录。初始账号用户名为 &lt;code&gt;root&lt;/code&gt;，密码为 &lt;code&gt;123456&lt;/code&gt;。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;更加详细的部署教程&lt;a href=&#34;https://iamazing.cn/page/how-to-deploy-a-website&#34;&gt;参见此处&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h3&gt;多机部署&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;所有服务器 &lt;code&gt;SESSION_SECRET&lt;/code&gt; 设置一样的值。&lt;/li&gt; &#xA; &lt;li&gt;必须设置 &lt;code&gt;SQL_DSN&lt;/code&gt;，使用 MySQL 数据库而非 SQLite，所有服务器连接同一个数据库。&lt;/li&gt; &#xA; &lt;li&gt;所有从服务器必须设置 &lt;code&gt;NODE_TYPE&lt;/code&gt; 为 &lt;code&gt;slave&lt;/code&gt;，不设置则默认为主服务器。&lt;/li&gt; &#xA; &lt;li&gt;设置 &lt;code&gt;SYNC_FREQUENCY&lt;/code&gt; 后服务器将定期从数据库同步配置，在使用远程数据库的情况下，推荐设置该项并启用 Redis，无论主从。&lt;/li&gt; &#xA; &lt;li&gt;从服务器可以选择设置 &lt;code&gt;FRONTEND_BASE_URL&lt;/code&gt;，以重定向页面请求到主服务器。&lt;/li&gt; &#xA; &lt;li&gt;从服务器上&lt;strong&gt;分别&lt;/strong&gt;装好 Redis，设置好 &lt;code&gt;REDIS_CONN_STRING&lt;/code&gt;，这样可以做到在缓存未过期的情况下数据库零访问，可以减少延迟。&lt;/li&gt; &#xA; &lt;li&gt;如果主服务器访问数据库延迟也比较高，则也需要启用 Redis，并设置 &lt;code&gt;SYNC_FREQUENCY&lt;/code&gt;，以定期从数据库同步配置。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;环境变量的具体使用方法详见&lt;a href=&#34;https://raw.githubusercontent.com/songquanpeng/one-api/main/#%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F&#34;&gt;此处&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h3&gt;宝塔部署教程&lt;/h3&gt; &#xA;&lt;p&gt;详见 &lt;a href=&#34;https://github.com/songquanpeng/one-api/issues/175&#34;&gt;#175&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;p&gt;如果部署后访问出现空白页面，详见 &lt;a href=&#34;https://github.com/songquanpeng/one-api/issues/97&#34;&gt;#97&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h3&gt;部署第三方服务配合 One API 使用&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;欢迎 PR 添加更多示例。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;ChatGPT Next Web&lt;/h4&gt; &#xA;&lt;p&gt;项目主页：&lt;a href=&#34;https://github.com/Yidadaa/ChatGPT-Next-Web&#34;&gt;https://github.com/Yidadaa/ChatGPT-Next-Web&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run --name chat-next-web -d -p 3001:3000 yidadaa/chatgpt-next-web&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;注意修改端口号，之后在页面上设置接口地址（例如：&lt;a href=&#34;https://openai.justsong.cn/&#34;&gt;https://openai.justsong.cn/&lt;/a&gt; ）和 API Key 即可。&lt;/p&gt; &#xA;&lt;h4&gt;ChatGPT Web&lt;/h4&gt; &#xA;&lt;p&gt;项目主页：&lt;a href=&#34;https://github.com/Chanzhaoyu/chatgpt-web&#34;&gt;https://github.com/Chanzhaoyu/chatgpt-web&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run --name chatgpt-web -d -p 3002:3002 -e OPENAI_API_BASE_URL=https://openai.justsong.cn -e OPENAI_API_KEY=sk-xxx chenzhaoyu94/chatgpt-web&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;注意修改端口号、&lt;code&gt;OPENAI_API_BASE_URL&lt;/code&gt; 和 &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;。&lt;/p&gt; &#xA;&lt;h3&gt;部署到第三方平台&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;strong&gt;部署到 Sealos &lt;/strong&gt;&lt;/summary&gt; &#xA; &lt;div&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Sealos 的服务器在国外，不需要额外处理网络问题，支持高并发 &amp;amp; 动态伸缩。&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &#xA;  &lt;p&gt;点击以下按钮一键部署（部署后访问出现 404 请等待 3~5 分钟）：&lt;/p&gt; &#xA;  &lt;p&gt;&lt;a href=&#34;https://cloud.sealos.io/?openapp=system-fastdeploy?templateName=one-api&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/labring-actions/templates/main/Deploy-on-Sealos.svg?sanitize=true&#34; alt=&#34;Deploy-on-Sealos.svg&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;/div&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;strong&gt;部署到 Zeabur&lt;/strong&gt;&lt;/summary&gt; &#xA; &lt;div&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Zeabur 的服务器在国外，自动解决了网络的问题，同时免费的额度也足够个人使用。&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;首先 fork 一份代码。&lt;/li&gt; &#xA;   &lt;li&gt;进入 &lt;a href=&#34;https://zeabur.com?referralCode=songquanpeng&#34;&gt;Zeabur&lt;/a&gt;，登录，进入控制台。&lt;/li&gt; &#xA;   &lt;li&gt;新建一个 Project，在 Service -&amp;gt; Add Service 选择 Marketplace，选择 MySQL，并记下连接参数（用户名、密码、地址、端口）。&lt;/li&gt; &#xA;   &lt;li&gt;复制链接参数，运行 &lt;code&gt;create database `one-api` &lt;/code&gt; 创建数据库。&lt;/li&gt; &#xA;   &lt;li&gt;然后在 Service -&amp;gt; Add Service，选择 Git（第一次使用需要先授权），选择你 fork 的仓库。&lt;/li&gt; &#xA;   &lt;li&gt;Deploy 会自动开始，先取消。进入下方 Variable，添加一个 &lt;code&gt;PORT&lt;/code&gt;，值为 &lt;code&gt;3000&lt;/code&gt;，再添加一个 &lt;code&gt;SQL_DSN&lt;/code&gt;，值为 &lt;code&gt;&amp;lt;username&amp;gt;:&amp;lt;password&amp;gt;@tcp(&amp;lt;addr&amp;gt;:&amp;lt;port&amp;gt;)/one-api&lt;/code&gt; ，然后保存。 注意如果不填写 &lt;code&gt;SQL_DSN&lt;/code&gt;，数据将无法持久化，重新部署后数据会丢失。&lt;/li&gt; &#xA;   &lt;li&gt;选择 Redeploy。&lt;/li&gt; &#xA;   &lt;li&gt;进入下方 Domains，选择一个合适的域名前缀，如 &#34;my-one-api&#34;，最终域名为 &#34;my-one-api.zeabur.app&#34;，也可以 CNAME 自己的域名。&lt;/li&gt; &#xA;   &lt;li&gt;等待部署完成，点击生成的域名进入 One API。&lt;/li&gt; &#xA;  &lt;/ol&gt; &#xA; &lt;/div&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;配置&lt;/h2&gt; &#xA;&lt;p&gt;系统本身开箱即用。&lt;/p&gt; &#xA;&lt;p&gt;你可以通过设置环境变量或者命令行参数进行配置。&lt;/p&gt; &#xA;&lt;p&gt;等到系统启动后，使用 &lt;code&gt;root&lt;/code&gt; 用户登录系统并做进一步的配置。&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;：如果你不知道某个配置项的含义，可以临时删掉值以看到进一步的提示文字。&lt;/p&gt; &#xA;&lt;h2&gt;使用方法&lt;/h2&gt; &#xA;&lt;p&gt;在&lt;code&gt;渠道&lt;/code&gt;页面中添加你的 API Key，之后在&lt;code&gt;令牌&lt;/code&gt;页面中新增访问令牌。&lt;/p&gt; &#xA;&lt;p&gt;之后就可以使用你的令牌访问 One API 了，使用方式与 &lt;a href=&#34;https://platform.openai.com/docs/api-reference/introduction&#34;&gt;OpenAI API&lt;/a&gt; 一致。&lt;/p&gt; &#xA;&lt;p&gt;你需要在各种用到 OpenAI API 的地方设置 API Base 为你的 One API 的部署地址，例如：&lt;code&gt;https://openai.justsong.cn&lt;/code&gt;，API Key 则为你在 One API 中生成的令牌。&lt;/p&gt; &#xA;&lt;p&gt;注意，具体的 API Base 的格式取决于你所使用的客户端。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;graph LR&#xA;    A(用户)&#xA;    A ---&amp;gt;|请求| B(One API)&#xA;    B --&amp;gt;|中继请求| C(OpenAI)&#xA;    B --&amp;gt;|中继请求| D(Azure)&#xA;    B --&amp;gt;|中继请求| E(其他下游渠道)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;可以通过在令牌后面添加渠道 ID 的方式指定使用哪一个渠道处理本次请求，例如：&lt;code&gt;Authorization: Bearer ONE_API_KEY-CHANNEL_ID&lt;/code&gt;。 注意，需要是管理员用户创建的令牌才能指定渠道 ID。&lt;/p&gt; &#xA;&lt;p&gt;不加的话将会使用负载均衡的方式使用多个渠道。&lt;/p&gt; &#xA;&lt;h3&gt;环境变量&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;REDIS_CONN_STRING&lt;/code&gt;：设置之后将使用 Redis 作为缓存使用。 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;例子：&lt;code&gt;REDIS_CONN_STRING=redis://default:redispw@localhost:49153&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;如果数据库访问延迟很低，没有必要启用 Redis，启用后反而会出现数据滞后的问题。&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;SESSION_SECRET&lt;/code&gt;：设置之后将使用固定的会话密钥，这样系统重新启动后已登录用户的 cookie 将依旧有效。 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;例子：&lt;code&gt;SESSION_SECRET=random_string&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;SQL_DSN&lt;/code&gt;：设置之后将使用指定数据库而非 SQLite，请使用 MySQL 或 PostgreSQL。 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;例子： &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;MySQL：&lt;code&gt;SQL_DSN=root:123456@tcp(localhost:3306)/oneapi&lt;/code&gt;&lt;/li&gt; &#xA;     &lt;li&gt;PostgreSQL：&lt;code&gt;SQL_DSN=postgres://postgres:123456@localhost:5432/oneapi&lt;/code&gt;（适配中，欢迎反馈）&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;注意需要提前建立数据库 &lt;code&gt;oneapi&lt;/code&gt;，无需手动建表，程序将自动建表。&lt;/li&gt; &#xA;   &lt;li&gt;如果使用本地数据库：部署命令可添加 &lt;code&gt;--network=&#34;host&#34;&lt;/code&gt; 以使得容器内的程序可以访问到宿主机上的 MySQL。&lt;/li&gt; &#xA;   &lt;li&gt;如果使用云数据库：如果云服务器需要验证身份，需要在连接参数中添加 &lt;code&gt;?tls=skip-verify&lt;/code&gt;。&lt;/li&gt; &#xA;   &lt;li&gt;请根据你的数据库配置修改下列参数（或者保持默认值）： &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;code&gt;SQL_MAX_IDLE_CONNS&lt;/code&gt;：最大空闲连接数，默认为 &lt;code&gt;100&lt;/code&gt;。&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;SQL_MAX_OPEN_CONNS&lt;/code&gt;：最大打开连接数，默认为 &lt;code&gt;1000&lt;/code&gt;。 &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;如果报错 &lt;code&gt;Error 1040: Too many connections&lt;/code&gt;，请适当减小该值。&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;SQL_CONN_MAX_LIFETIME&lt;/code&gt;：连接的最大生命周期，默认为 &lt;code&gt;60&lt;/code&gt;，单位分钟。&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;FRONTEND_BASE_URL&lt;/code&gt;：设置之后将重定向页面请求到指定的地址，仅限从服务器设置。 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;例子：&lt;code&gt;FRONTEND_BASE_URL=https://openai.justsong.cn&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;SYNC_FREQUENCY&lt;/code&gt;：设置之后将定期与数据库同步配置，单位为秒，未设置则不进行同步。 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;例子：&lt;code&gt;SYNC_FREQUENCY=60&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;NODE_TYPE&lt;/code&gt;：设置之后将指定节点类型，可选值为 &lt;code&gt;master&lt;/code&gt; 和 &lt;code&gt;slave&lt;/code&gt;，未设置则默认为 &lt;code&gt;master&lt;/code&gt;。 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;例子：&lt;code&gt;NODE_TYPE=slave&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;CHANNEL_UPDATE_FREQUENCY&lt;/code&gt;：设置之后将定期更新渠道余额，单位为分钟，未设置则不进行更新。 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;例子：&lt;code&gt;CHANNEL_UPDATE_FREQUENCY=1440&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;CHANNEL_TEST_FREQUENCY&lt;/code&gt;：设置之后将定期检查渠道，单位为分钟，未设置则不进行检查。 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;例子：&lt;code&gt;CHANNEL_TEST_FREQUENCY=1440&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;POLLING_INTERVAL&lt;/code&gt;：批量更新渠道余额以及测试可用性时的请求间隔，单位为秒，默认无间隔。 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;例子：&lt;code&gt;POLLING_INTERVAL=5&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;命令行参数&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;--port &amp;lt;port_number&amp;gt;&lt;/code&gt;: 指定服务器监听的端口号，默认为 &lt;code&gt;3000&lt;/code&gt;。 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;例子：&lt;code&gt;--port 3000&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--log-dir &amp;lt;log_dir&amp;gt;&lt;/code&gt;: 指定日志文件夹，如果没有设置，日志将不会被保存。 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;例子：&lt;code&gt;--log-dir ./logs&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--version&lt;/code&gt;: 打印系统版本号并退出。&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--help&lt;/code&gt;: 查看命令的使用帮助和参数说明。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;演示&lt;/h2&gt; &#xA;&lt;h3&gt;在线演示&lt;/h3&gt; &#xA;&lt;p&gt;注意，该演示站不提供对外服务： &lt;a href=&#34;https://openai.justsong.cn&#34;&gt;https://openai.justsong.cn&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;截图展示&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/39998050/233837954-ae6683aa-5c4f-429f-a949-6645a83c9490.png&#34; alt=&#34;channel&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/39998050/233837971-dab488b7-6d96-43af-b640-a168e8d1c9bf.png&#34; alt=&#34;token&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;常见问题&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;额度是什么？怎么计算的？One API 的额度计算有问题？ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;额度 = 分组倍率 * 模型倍率 * （提示 token 数 + 补全 token 数 * 补全倍率）&lt;/li&gt; &#xA;   &lt;li&gt;其中补全倍率对于 GPT3.5 固定为 1.33，GPT4 为 2，与官方保持一致。&lt;/li&gt; &#xA;   &lt;li&gt;如果是非流模式，官方接口会返回消耗的总 token，但是你要注意提示和补全的消耗倍率不一样。&lt;/li&gt; &#xA;   &lt;li&gt;注意，One API 的默认倍率就是官方倍率，是已经调整过的。&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;账户额度足够为什么提示额度不足？ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;请检查你的令牌额度是否足够，这个和账户额度是分开的。&lt;/li&gt; &#xA;   &lt;li&gt;令牌额度仅供用户设置最大使用量，用户可自由设置。&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;提示无可用渠道？ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;请检查的用户分组和渠道分组设置。&lt;/li&gt; &#xA;   &lt;li&gt;以及渠道的模型设置。&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;渠道测试报错：&lt;code&gt;invalid character &#39;&amp;lt;&#39; looking for beginning of value&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;这是因为返回值不是合法的 JSON，而是一个 HTML 页面。&lt;/li&gt; &#xA;   &lt;li&gt;大概率是你的部署站的 IP 或代理的节点被 CloudFlare 封禁了。&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;ChatGPT Next Web 报错：&lt;code&gt;Failed to fetch&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;部署的时候不要设置 &lt;code&gt;BASE_URL&lt;/code&gt;。&lt;/li&gt; &#xA;   &lt;li&gt;检查你的接口地址和 API Key 有没有填对。&lt;/li&gt; &#xA;   &lt;li&gt;检查是否启用了 HTTPS，浏览器会拦截 HTTPS 域名下的 HTTP 请求。&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;报错：&lt;code&gt;当前分组负载已饱和，请稍后再试&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;上游通道 429 了。&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;相关项目&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labring/FastGPT&#34;&gt;FastGPT&lt;/a&gt;: 基于 LLM 大语言模型的知识库问答系统&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Yidadaa/ChatGPT-Next-Web&#34;&gt;ChatGPT Next Web&lt;/a&gt;: 一键拥有你自己的跨平台 ChatGPT 应用&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;注意&lt;/h2&gt; &#xA;&lt;p&gt;本项目使用 MIT 协议进行开源，&lt;strong&gt;在此基础上&lt;/strong&gt;，必须在页面底部保留署名以及指向本项目的链接。如果不想保留署名，必须首先获得授权。&lt;/p&gt; &#xA;&lt;p&gt;同样适用于基于本项目的二开项目。&lt;/p&gt; &#xA;&lt;p&gt;依据 MIT 协议，使用者需自行承担使用本项目的风险与责任，本开源项目开发者与此无关。&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>jmorganca/ollama</title>
    <updated>2023-09-01T02:10:20Z</updated>
    <id>tag:github.com,2023-09-01:/jmorganca/ollama</id>
    <link href="https://github.com/jmorganca/ollama" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Get up and running with Llama 2 and other large language models locally&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; height=&#34;200px&#34; srcset=&#34;https://github.com/jmorganca/ollama/assets/3325447/56ea1849-1284-4645-8970-956de6e51c3c&#34;&gt; &#xA;  &lt;img alt=&#34;logo&#34; height=&#34;200px&#34; src=&#34;https://github.com/jmorganca/ollama/assets/3325447/0d0b44e2-8f4a-4e99-9b52-a5c1c741c8f7&#34;&gt; &#xA; &lt;/picture&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;Ollama&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/ollama&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/ollama?style=flat&amp;amp;compact=true&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Run, create, and share large language models (LLMs).&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: Ollama is in early preview. Please report any issues you find.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Download&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ollama.ai/download&#34;&gt;Download&lt;/a&gt; for macOS&lt;/li&gt; &#xA; &lt;li&gt;Download for Windows and Linux (coming soon)&lt;/li&gt; &#xA; &lt;li&gt;Build &lt;a href=&#34;https://raw.githubusercontent.com/jmorganca/ollama/main/#building&#34;&gt;from source&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;To run and chat with &lt;a href=&#34;https://ai.meta.com/llama&#34;&gt;Llama 2&lt;/a&gt;, the new model by Meta:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ollama run llama2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Model library&lt;/h2&gt; &#xA;&lt;p&gt;Ollama supports a list of open-source models available on &lt;a href=&#34;https://ollama.ai/library&#34; title=&#34;ollama model library&#34;&gt;ollama.ai/library&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Here are some example open-source models that can be downloaded:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Parameters&lt;/th&gt; &#xA;   &lt;th&gt;Size&lt;/th&gt; &#xA;   &lt;th&gt;Download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama2&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;3.8GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama pull llama2&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama2 13B&lt;/td&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;7.3GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama pull llama2:13b&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama2 70B&lt;/td&gt; &#xA;   &lt;td&gt;70B&lt;/td&gt; &#xA;   &lt;td&gt;39GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama pull llama2:70b&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama2 Uncensored&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;3.8GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama pull llama2-uncensored&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Code Llama&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;3.8GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama pull codellama&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Orca Mini&lt;/td&gt; &#xA;   &lt;td&gt;3B&lt;/td&gt; &#xA;   &lt;td&gt;1.9GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama pull orca-mini&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Vicuna&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;3.8GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama pull vicuna&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Nous-Hermes&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;3.8GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama pull nous-hermes&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Nous-Hermes 13B&lt;/td&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;7.3GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama pull nous-hermes:13b&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Wizard Vicuna Uncensored&lt;/td&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;7.3GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama pull wizard-vicuna&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: You should have at least 8 GB of RAM to run the 3B models, 16 GB to run the 7B models, and 32 GB to run the 13B models.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;h3&gt;Run a model&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;ollama run llama2&#xA;&amp;gt;&amp;gt;&amp;gt; hi&#xA;Hello! How can I help you today?&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For multiline input, you can wrap text with &lt;code&gt;&#34;&#34;&#34;&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; &#34;&#34;&#34;Hello,&#xA;... world!&#xA;... &#34;&#34;&#34;&#xA;I&#39;m a basic program that prints the famous &#34;Hello, world!&#34; message to the console.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Create a custom model&lt;/h3&gt; &#xA;&lt;p&gt;Pull a base model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ollama pull llama2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;To update a model to the latest version, run &lt;code&gt;ollama pull llama2&lt;/code&gt; again. The model will be updated (if necessary).&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Create a &lt;code&gt;Modelfile&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;FROM llama2&#xA;&#xA;# set the temperature to 1 [higher is more creative, lower is more coherent]&#xA;PARAMETER temperature 1&#xA;&#xA;# set the system prompt&#xA;SYSTEM &#34;&#34;&#34;&#xA;You are Mario from Super Mario Bros. Answer as Mario, the assistant, only.&#xA;&#34;&#34;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, create and run the model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ollama create mario -f ./Modelfile&#xA;ollama run mario&#xA;&amp;gt;&amp;gt;&amp;gt; hi&#xA;Hello! It&#39;s your friend Mario.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more examples, see the &lt;a href=&#34;https://raw.githubusercontent.com/jmorganca/ollama/main/examples&#34;&gt;examples&lt;/a&gt; directory. For more information on creating a Modelfile, see the &lt;a href=&#34;https://raw.githubusercontent.com/jmorganca/ollama/main/docs/modelfile.md&#34;&gt;Modelfile&lt;/a&gt; documentation.&lt;/p&gt; &#xA;&lt;h3&gt;Pull a model from the registry&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;ollama pull orca-mini&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Listing local models&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;ollama list&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Model packages&lt;/h2&gt; &#xA;&lt;h3&gt;Overview&lt;/h3&gt; &#xA;&lt;p&gt;Ollama bundles model weights, configuration, and data into a single package, defined by a &lt;a href=&#34;https://raw.githubusercontent.com/jmorganca/ollama/main/docs/modelfile.md&#34;&gt;Modelfile&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;picture&gt; &#xA; &lt;source media=&#34;(prefers-color-scheme: dark)&#34; height=&#34;480&#34; srcset=&#34;https://github.com/jmorganca/ollama/assets/251292/2fd96b5f-191b-45c1-9668-941cfad4eb70&#34;&gt; &#xA; &lt;img alt=&#34;logo&#34; height=&#34;480&#34; src=&#34;https://github.com/jmorganca/ollama/assets/251292/2fd96b5f-191b-45c1-9668-941cfad4eb70&#34;&gt; &#xA;&lt;/picture&gt; &#xA;&lt;h2&gt;Building&lt;/h2&gt; &#xA;&lt;p&gt;Install &lt;code&gt;cmake&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;brew install cmake&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then generate dependencies and build:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;go generate ./...&#xA;go build .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, start the server:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./ollama serve&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, run a model (in another shell):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./ollama run llama2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;REST API&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/jmorganca/ollama/main/docs/api.md&#34;&gt;API documentation&lt;/a&gt; for all endpoints.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Ollama has an API for running and managing models. For example to generate text from a model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -X POST http://localhost:11434/api/generate -d &#39;{&#xA;  &#34;model&#34;: &#34;llama2&#34;,&#xA;  &#34;prompt&#34;:&#34;Why is the sky blue?&#34;&#xA;}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Tools using Ollama&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://python.langchain.com/docs/integrations/llms/ollama&#34;&gt;LangChain&lt;/a&gt; and &lt;a href=&#34;https://js.langchain.com/docs/modules/model_io/models/llms/integrations/ollama&#34;&gt;LangChain.js&lt;/a&gt; with a question-answering &lt;a href=&#34;https://js.langchain.com/docs/use_cases/question_answering/local_retrieval_qa&#34;&gt;example&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/continuedev/continue&#34;&gt;Continue&lt;/a&gt; - embeds Ollama inside Visual Studio Code. The extension lets you highlight code to add to the prompt, ask questions in the sidebar, and generate code inline.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/BerriAI/litellm&#34;&gt;LiteLLM&lt;/a&gt; a lightweight python package to simplify LLM API calls&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mekb-turtle/discord-ai-bot&#34;&gt;Discord AI Bot&lt;/a&gt; - interact with Ollama as a chatbot on Discord.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/MassimilianoPasquini97/raycast_ollama&#34;&gt;Raycast Ollama&lt;/a&gt; - Raycast extension to use Ollama for local llama inference on Raycast.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rtcfirefly/ollama-ui&#34;&gt;Simple HTML UI for Ollama&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zweifisch/ollama&#34;&gt;Emacs client&lt;/a&gt; for Ollama&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>moby/moby</title>
    <updated>2023-09-01T02:10:20Z</updated>
    <id>tag:github.com,2023-09-01:/moby/moby</id>
    <link href="https://github.com/moby/moby" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Moby Project - a collaborative project for the container ecosystem to assemble container-based systems&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;The Moby Project&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/moby/moby/master/docs/static_files/moby-project-logo.png&#34; alt=&#34;Moby Project logo&#34; title=&#34;The Moby Project&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Moby is an open-source project created by Docker to enable and accelerate software containerization.&lt;/p&gt; &#xA;&lt;p&gt;It provides a &#34;Lego set&#34; of toolkit components, the framework for assembling them into custom container-based systems, and a place for all container enthusiasts and professionals to experiment and exchange ideas. Components include container build tools, a container registry, orchestration tools, a runtime and more, and these can be used as building blocks in conjunction with other tools and projects.&lt;/p&gt; &#xA;&lt;h2&gt;Principles&lt;/h2&gt; &#xA;&lt;p&gt;Moby is an open project guided by strong principles, aiming to be modular, flexible and without too strong an opinion on user experience. It is open to the community to help set its direction.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Modular: the project includes lots of components that have well-defined functions and APIs that work together.&lt;/li&gt; &#xA; &lt;li&gt;Batteries included but swappable: Moby includes enough components to build fully featured container systems, but its modular architecture ensures that most of the components can be swapped by different implementations.&lt;/li&gt; &#xA; &lt;li&gt;Usable security: Moby provides secure defaults without compromising usability.&lt;/li&gt; &#xA; &lt;li&gt;Developer focused: The APIs are intended to be functional and useful to build powerful tools. They are not necessarily intended as end user tools but as components aimed at developers. Documentation and UX is aimed at developers not end users.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Audience&lt;/h2&gt; &#xA;&lt;p&gt;The Moby Project is intended for engineers, integrators and enthusiasts looking to modify, hack, fix, experiment, invent and build systems based on containers. It is not for people looking for a commercially supported system, but for people who want to work and learn with open source code.&lt;/p&gt; &#xA;&lt;h2&gt;Relationship with Docker&lt;/h2&gt; &#xA;&lt;p&gt;The components and tools in the Moby Project are initially the open source components that Docker and the community have built for the Docker Project. New projects can be added if they fit with the community goals. Docker is committed to using Moby as the upstream for the Docker Product. However, other projects are also encouraged to use Moby as an upstream, and to reuse the components in diverse ways, and all these uses will be treated in the same way. External maintainers and contributors are welcomed.&lt;/p&gt; &#xA;&lt;p&gt;The Moby project is not intended as a location for support or feature requests for Docker products, but as a place for contributors to work on open source code, fix bugs, and make the code more useful. The releases are supported by the maintainers, community and users, on a best efforts basis only, and are not intended for customers who want enterprise or commercial support; Docker EE is the appropriate product for these use cases.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Legal&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;Brought to you courtesy of our legal counsel. For more context, please see the &lt;a href=&#34;https://github.com/moby/moby/raw/master/NOTICE&#34;&gt;NOTICE&lt;/a&gt; document in this repo.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Use and transfer of Moby may be subject to certain restrictions by the United States and other governments.&lt;/p&gt; &#xA;&lt;p&gt;It is your responsibility to ensure that your use and/or transfer does not violate applicable laws.&lt;/p&gt; &#xA;&lt;p&gt;For more information, please see &lt;a href=&#34;https://www.bis.doc.gov&#34;&gt;https://www.bis.doc.gov&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Licensing&lt;/h1&gt; &#xA;&lt;p&gt;Moby is licensed under the Apache License, Version 2.0. See &lt;a href=&#34;https://github.com/moby/moby/raw/master/LICENSE&#34;&gt;LICENSE&lt;/a&gt; for the full license text.&lt;/p&gt;</summary>
  </entry>
</feed>