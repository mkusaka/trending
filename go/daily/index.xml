<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Go Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-06-27T01:31:16Z</updated>
  <subtitle>Daily Trending of Go in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>kubeflow/spark-operator</title>
    <updated>2024-06-27T01:31:16Z</updated>
    <id>tag:github.com,2024-06-27:/kubeflow/spark-operator</id>
    <link href="https://github.com/kubeflow/spark-operator" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Kubernetes operator for managing the lifecycle of Apache Spark applications on Kubernetes.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Kubeflow Spark Operator&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://goreportcard.com/report/github.com/kubeflow/spark-operator&#34;&gt;&lt;img src=&#34;https://goreportcard.com/badge/github.com/kubeflow/spark-operator&#34; alt=&#34;Go Report Card&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;The Kubernetes Operator for Apache Spark aims to make specifying and running &lt;a href=&#34;https://github.com/apache/spark&#34;&gt;Spark&lt;/a&gt; applications as easy and idiomatic as running other workloads on Kubernetes. It uses &lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/&#34;&gt;Kubernetes custom resources&lt;/a&gt; for specifying, running, and surfacing status of Spark applications. For a complete reference of the custom resource definitions, please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/kubeflow/spark-operator/master/docs/api-docs.md&#34;&gt;API Definition&lt;/a&gt;. For details on its design, please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/kubeflow/spark-operator/master/docs/design.md&#34;&gt;design doc&lt;/a&gt;. It requires Spark 2.3 and above that supports Kubernetes as a native scheduler backend.&lt;/p&gt; &#xA;&lt;p&gt;The Kubernetes Operator for Apache Spark currently supports the following list of features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Supports Spark 2.3 and up.&lt;/li&gt; &#xA; &lt;li&gt;Enables declarative application specification and management of applications through custom resources.&lt;/li&gt; &#xA; &lt;li&gt;Automatically runs &lt;code&gt;spark-submit&lt;/code&gt; on behalf of users for each &lt;code&gt;SparkApplication&lt;/code&gt; eligible for submission.&lt;/li&gt; &#xA; &lt;li&gt;Provides native &lt;a href=&#34;https://en.wikipedia.org/wiki/Cron&#34;&gt;cron&lt;/a&gt; support for running scheduled applications.&lt;/li&gt; &#xA; &lt;li&gt;Supports customization of Spark pods beyond what Spark natively is able to do through the mutating admission webhook, e.g., mounting ConfigMaps and volumes, and setting pod affinity/anti-affinity.&lt;/li&gt; &#xA; &lt;li&gt;Supports automatic application re-submission for updated &lt;code&gt;SparkApplication&lt;/code&gt; objects with updated specification.&lt;/li&gt; &#xA; &lt;li&gt;Supports automatic application restart with a configurable restart policy.&lt;/li&gt; &#xA; &lt;li&gt;Supports automatic retries of failed submissions with optional linear back-off.&lt;/li&gt; &#xA; &lt;li&gt;Supports mounting local Hadoop configuration as a Kubernetes ConfigMap automatically via &lt;code&gt;sparkctl&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Supports automatically staging local application dependencies to Google Cloud Storage (GCS) via &lt;code&gt;sparkctl&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Supports collecting and exporting application-level metrics and driver/executor metrics to Prometheus.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Project Status&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Project status:&lt;/strong&gt; &lt;em&gt;beta&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Current API version:&lt;/strong&gt; &lt;em&gt;&lt;code&gt;v1beta2&lt;/code&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;If you are currently using the &lt;code&gt;v1beta1&lt;/code&gt; version of the APIs in your manifests, please update them to use the &lt;code&gt;v1beta2&lt;/code&gt; version by changing &lt;code&gt;apiVersion: &#34;sparkoperator.k8s.io/&amp;lt;version&amp;gt;&#34;&lt;/code&gt; to &lt;code&gt;apiVersion: &#34;sparkoperator.k8s.io/v1beta2&#34;&lt;/code&gt;. You will also need to delete the &lt;code&gt;previous&lt;/code&gt; version of the CustomResourceDefinitions named &lt;code&gt;sparkapplications.sparkoperator.k8s.io&lt;/code&gt; and &lt;code&gt;scheduledsparkapplications.sparkoperator.k8s.io&lt;/code&gt;, and replace them with the &lt;code&gt;v1beta2&lt;/code&gt; version either by installing the latest version of the operator or by running &lt;code&gt;kubectl create -f manifest/crds&lt;/code&gt;.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Customization of Spark pods, e.g., mounting arbitrary volumes and setting pod affinity, is implemented using a Kubernetes &lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/&#34;&gt;Mutating Admission Webhook&lt;/a&gt;, which became beta in Kubernetes 1.9. The mutating admission webhook is disabled by default if you install the operator using the Helm &lt;a href=&#34;https://raw.githubusercontent.com/kubeflow/spark-operator/master/charts/spark-operator-chart&#34;&gt;chart&lt;/a&gt;. Check out the &lt;a href=&#34;https://raw.githubusercontent.com/kubeflow/spark-operator/master/docs/quick-start-guide.md#using-the-mutating-admission-webhook&#34;&gt;Quick Start Guide&lt;/a&gt; on how to enable the webhook.&lt;/p&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Version &amp;gt;= 1.13 of Kubernetes to use the &lt;a href=&#34;https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/#subresources&#34;&gt;&lt;code&gt;subresource&lt;/code&gt; support for CustomResourceDefinitions&lt;/a&gt;, which became beta in 1.13 and is enabled by default in 1.13 and higher.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Version &amp;gt;= 1.16 of Kubernetes to use the &lt;code&gt;MutatingWebhook&lt;/code&gt; and &lt;code&gt;ValidatingWebhook&lt;/code&gt; of &lt;code&gt;apiVersion: admissionregistration.k8s.io/v1&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;The easiest way to install the Kubernetes Operator for Apache Spark is to use the Helm &lt;a href=&#34;https://raw.githubusercontent.com/kubeflow/spark-operator/master/charts/spark-operator-chart/&#34;&gt;chart&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ helm repo add spark-operator https://kubeflow.github.io/spark-operator&#xA;&#xA;$ helm install my-release spark-operator/spark-operator --namespace spark-operator --create-namespace&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will install the Kubernetes Operator for Apache Spark into the namespace &lt;code&gt;spark-operator&lt;/code&gt;. The operator by default watches and handles &lt;code&gt;SparkApplication&lt;/code&gt;s in every namespaces. If you would like to limit the operator to watch and handle &lt;code&gt;SparkApplication&lt;/code&gt;s in a single namespace, e.g., &lt;code&gt;default&lt;/code&gt; instead, add the following option to the &lt;code&gt;helm install&lt;/code&gt; command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;--set &#34;sparkJobNamespaces={default}&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For configuration options available in the Helm chart, please refer to the chart&#39;s &lt;a href=&#34;https://raw.githubusercontent.com/kubeflow/spark-operator/master/charts/spark-operator-chart/README.md&#34;&gt;README&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Version Matrix&lt;/h2&gt; &#xA;&lt;p&gt;The following table lists the most recent few versions of the operator.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Operator Version&lt;/th&gt; &#xA;   &lt;th&gt;API Version&lt;/th&gt; &#xA;   &lt;th&gt;Kubernetes Version&lt;/th&gt; &#xA;   &lt;th&gt;Base Spark Version&lt;/th&gt; &#xA;   &lt;th&gt;Operator Image Tag&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;latest&lt;/code&gt; (master HEAD)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;v1beta2&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.13+&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;3.0.0&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;latest&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;v1beta2-1.3.3-3.1.1&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;v1beta2&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.16+&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;3.1.1&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;v1beta2-1.3.3-3.1.1&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;v1beta2-1.3.2-3.1.1&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;v1beta2&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.16+&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;3.1.1&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;v1beta2-1.3.2-3.1.1&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;v1beta2-1.3.0-3.1.1&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;v1beta2&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.16+&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;3.1.1&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;v1beta2-1.3.0-3.1.1&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;v1beta2-1.2.3-3.1.1&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;v1beta2&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.13+&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;3.1.1&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;v1beta2-1.2.3-3.1.1&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;v1beta2-1.2.0-3.0.0&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;v1beta2&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.13+&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;3.0.0&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;v1beta2-1.2.0-3.0.0&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;v1beta2-1.1.2-2.4.5&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;v1beta2&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.13+&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;2.4.5&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;v1beta2-1.1.2-2.4.5&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;v1beta2-1.0.1-2.4.4&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;v1beta2&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.13+&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;2.4.4&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;v1beta2-1.0.1-2.4.4&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;v1beta2-1.0.0-2.4.4&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;v1beta2&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.13+&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;2.4.4&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;v1beta2-1.0.0-2.4.4&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;v1beta1-0.9.0&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;v1beta1&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.13+&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;2.4.0&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;v2.4.0-v1beta1-0.9.0&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;When installing using the Helm chart, you can choose to use a specific image tag instead of the default one, using the following option:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;--set image.tag=&amp;lt;operator image tag&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;p&gt;Get started quickly with the Kubernetes Operator for Apache Spark using the &lt;a href=&#34;https://raw.githubusercontent.com/kubeflow/spark-operator/master/docs/quick-start-guide.md&#34;&gt;Quick Start Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you are running the Kubernetes Operator for Apache Spark on Google Kubernetes Engine and want to use Google Cloud Storage (GCS) and/or BigQuery for reading/writing data, also refer to the &lt;a href=&#34;https://raw.githubusercontent.com/kubeflow/spark-operator/master/docs/gcp.md&#34;&gt;GCP guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For more information, check the &lt;a href=&#34;https://raw.githubusercontent.com/kubeflow/spark-operator/master/docs/design.md&#34;&gt;Design&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/kubeflow/spark-operator/master/docs/api-docs.md&#34;&gt;API Specification&lt;/a&gt; and detailed &lt;a href=&#34;https://raw.githubusercontent.com/kubeflow/spark-operator/master/docs/user-guide.md&#34;&gt;User Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Please check &lt;a href=&#34;https://raw.githubusercontent.com/kubeflow/spark-operator/master/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; and the &lt;a href=&#34;https://raw.githubusercontent.com/kubeflow/spark-operator/master/docs/developer-guide.md&#34;&gt;Developer Guide&lt;/a&gt; out.&lt;/p&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Join the &lt;a href=&#34;https://www.kubeflow.org/docs/about/community/#kubeflow-slack-channels&#34;&gt;CNCF Slack Channel&lt;/a&gt; and then join &lt;code&gt;#kubeflow-spark-operator&lt;/code&gt; Channel.&lt;/li&gt; &#xA; &lt;li&gt;Check out our blog post &lt;a href=&#34;https://blog.kubeflow.org/operators/2024/04/15/kubeflow-spark-operator.html&#34;&gt;Announcing the Kubeflow Spark Operator: Building a Stronger Spark on Kubernetes Community&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Check out &lt;a href=&#34;https://raw.githubusercontent.com/kubeflow/spark-operator/master/docs/who-is-using.md&#34;&gt;who is using the Kubernetes Operator for Apache Spark&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>