<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Go Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-09-20T01:30:44Z</updated>
  <subtitle>Daily Trending of Go in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>jmorganca/ollama</title>
    <updated>2023-09-20T01:30:44Z</updated>
    <id>tag:github.com,2023-09-20:/jmorganca/ollama</id>
    <link href="https://github.com/jmorganca/ollama" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Get up and running with Llama 2 and other large language models locally&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; height=&#34;200px&#34; srcset=&#34;https://github.com/jmorganca/ollama/assets/3325447/56ea1849-1284-4645-8970-956de6e51c3c&#34;&gt; &#xA;  &lt;img alt=&#34;logo&#34; height=&#34;200px&#34; src=&#34;https://github.com/jmorganca/ollama/assets/3325447/0d0b44e2-8f4a-4e99-9b52-a5c1c741c8f7&#34;&gt; &#xA; &lt;/picture&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;Ollama&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/ollama&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/ollama?style=flat&amp;amp;compact=true&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Run, create, and share large language models (LLMs).&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: Ollama is in early preview. Please report any issues you find.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Download&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ollama.ai/download&#34;&gt;Download&lt;/a&gt; for macOS&lt;/li&gt; &#xA; &lt;li&gt;Download for Windows and Linux (coming soon)&lt;/li&gt; &#xA; &lt;li&gt;Build &lt;a href=&#34;https://raw.githubusercontent.com/jmorganca/ollama/main/#building&#34;&gt;from source&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;To run and chat with &lt;a href=&#34;https://ai.meta.com/llama&#34;&gt;Llama 2&lt;/a&gt;, the new model by Meta:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ollama run llama2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Model library&lt;/h2&gt; &#xA;&lt;p&gt;Ollama supports a list of open-source models available on &lt;a href=&#34;https://ollama.ai/library&#34; title=&#34;ollama model library&#34;&gt;ollama.ai/library&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Here are some example open-source models that can be downloaded:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Parameters&lt;/th&gt; &#xA;   &lt;th&gt;Size&lt;/th&gt; &#xA;   &lt;th&gt;Download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama2&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;3.8GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama pull llama2&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama2 13B&lt;/td&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;7.3GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama pull llama2:13b&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama2 70B&lt;/td&gt; &#xA;   &lt;td&gt;70B&lt;/td&gt; &#xA;   &lt;td&gt;39GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama pull llama2:70b&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama2 Uncensored&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;3.8GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama pull llama2-uncensored&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Code Llama&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;3.8GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama pull codellama&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Orca Mini&lt;/td&gt; &#xA;   &lt;td&gt;3B&lt;/td&gt; &#xA;   &lt;td&gt;1.9GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama pull orca-mini&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Vicuna&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;3.8GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama pull vicuna&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Nous-Hermes&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;3.8GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama pull nous-hermes&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Nous-Hermes 13B&lt;/td&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;7.3GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama pull nous-hermes:13b&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Wizard Vicuna Uncensored&lt;/td&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;7.3GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama pull wizard-vicuna&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: You should have at least 8 GB of RAM to run the 3B models, 16 GB to run the 7B models, and 32 GB to run the 13B models.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;h3&gt;Pull a public model&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;ollama pull llama2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;This command can also be used to update a local model. Only updated changes will be pulled.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Run a model interactively&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;ollama run llama2&#xA;&amp;gt;&amp;gt;&amp;gt; hi&#xA;Hello! How can I help you today?&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For multiline input, you can wrap text with &lt;code&gt;&#34;&#34;&#34;&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; &#34;&#34;&#34;Hello,&#xA;... world!&#xA;... &#34;&#34;&#34;&#xA;I&#39;m a basic program that prints the famous &#34;Hello, world!&#34; message to the console.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Run a model non-interactively&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ollama run llama2 &#39;tell me a joke&#39;&#xA; Sure! Here&#39;s a quick one:&#xA; Why did the scarecrow win an award? Because he was outstanding in his field!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ cat &amp;lt;&amp;lt;EOF &amp;gt;prompts.txt&#xA;tell me a joke about llamas&#xA;tell me another one&#xA;EOF&#xA;$ ollama run llama2 &amp;lt;prompts.txt&#xA;&amp;gt;&amp;gt;&amp;gt; tell me a joke about llamas&#xA; Why did the llama refuse to play hide-and-seek?&#xA; nobody likes to be hided!&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; tell me another one&#xA; Sure, here&#39;s another one:&#xA;&#xA;Why did the llama go to the bar?&#xA;To have a hay-often good time!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Run a model on contents of a text file&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ollama run llama2 &#34;summarize this file:&#34; &#34;$(cat README.md)&#34;&#xA; Ollama is a lightweight, extensible framework for building and running language models on the local machine. It provides a simple API for creating, running, and managing models, as well as a library of pre-built models that can be easily used in a variety of applications.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Customize a model&lt;/h3&gt; &#xA;&lt;p&gt;Pull a base model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ollama pull llama2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Create a &lt;code&gt;Modelfile&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;FROM llama2&#xA;&#xA;# set the temperature to 1 [higher is more creative, lower is more coherent]&#xA;PARAMETER temperature 1&#xA;&#xA;# set the system prompt&#xA;SYSTEM &#34;&#34;&#34;&#xA;You are Mario from Super Mario Bros. Answer as Mario, the assistant, only.&#xA;&#34;&#34;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, create and run the model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ollama create mario -f ./Modelfile&#xA;ollama run mario&#xA;&amp;gt;&amp;gt;&amp;gt; hi&#xA;Hello! It&#39;s your friend Mario.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more examples, see the &lt;a href=&#34;https://raw.githubusercontent.com/jmorganca/ollama/main/examples&#34;&gt;examples&lt;/a&gt; directory. For more information on creating a Modelfile, see the &lt;a href=&#34;https://raw.githubusercontent.com/jmorganca/ollama/main/docs/modelfile.md&#34;&gt;Modelfile&lt;/a&gt; documentation.&lt;/p&gt; &#xA;&lt;h3&gt;Listing local models&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;ollama list&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Removing local models&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;ollama rm llama2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Model packages&lt;/h2&gt; &#xA;&lt;h3&gt;Overview&lt;/h3&gt; &#xA;&lt;p&gt;Ollama bundles model weights, configurations, and data into a single package, defined by a &lt;a href=&#34;https://raw.githubusercontent.com/jmorganca/ollama/main/docs/modelfile.md&#34;&gt;Modelfile&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;picture&gt; &#xA; &lt;source media=&#34;(prefers-color-scheme: dark)&#34; height=&#34;480&#34; srcset=&#34;https://github.com/jmorganca/ollama/assets/251292/2fd96b5f-191b-45c1-9668-941cfad4eb70&#34;&gt; &#xA; &lt;img alt=&#34;logo&#34; height=&#34;480&#34; src=&#34;https://github.com/jmorganca/ollama/assets/251292/2fd96b5f-191b-45c1-9668-941cfad4eb70&#34;&gt; &#xA;&lt;/picture&gt; &#xA;&lt;h2&gt;Building&lt;/h2&gt; &#xA;&lt;p&gt;Install &lt;code&gt;cmake&lt;/code&gt; and &lt;code&gt;go&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;brew install cmake&#xA;brew install go&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then generate dependencies and build:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;go generate ./...&#xA;go build .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, start the server:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./ollama serve&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, in a separate shell, run a model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./ollama run llama2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;REST API&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/jmorganca/ollama/main/docs/api.md&#34;&gt;API documentation&lt;/a&gt; for all endpoints.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Ollama has an API for running and managing models. For example to generate text from a model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -X POST http://localhost:11434/api/generate -d &#39;{&#xA;  &#34;model&#34;: &#34;llama2&#34;,&#xA;  &#34;prompt&#34;:&#34;Why is the sky blue?&#34;&#xA;}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Community Projects using Ollama&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Project&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://python.langchain.com/docs/integrations/llms/ollama&#34;&gt;LangChain&lt;/a&gt; and &lt;a href=&#34;https://js.langchain.com/docs/modules/model_io/models/llms/integrations/ollama&#34;&gt;LangChain.js&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Also, there is a question-answering &lt;a href=&#34;https://js.langchain.com/docs/use_cases/question_answering/local_retrieval_qa&#34;&gt;example&lt;/a&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/continuedev/continue&#34;&gt;Continue&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Embeds Ollama inside Visual Studio Code. The extension lets you highlight code to add to the prompt, ask questions in the sidebar, and generate code inline.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/BerriAI/litellm&#34;&gt;LiteLLM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Lightweight Python package to simplify LLM API calls.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mekb-turtle/discord-ai-bot&#34;&gt;Discord AI Bot&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Interact with Ollama as a chatbot on Discord.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/MassimilianoPasquini97/raycast_ollama&#34;&gt;Raycast Ollama&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Raycast extension to use Ollama for local llama inference on Raycast.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/rtcfirefly/ollama-ui&#34;&gt;Simple HTML UI&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Also, there is a Chrome extension.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/zweifisch/ollama&#34;&gt;Emacs client&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt;</summary>
  </entry>
  <entry>
    <title>cortexproject/cortex</title>
    <updated>2023-09-20T01:30:44Z</updated>
    <id>tag:github.com,2023-09-20:/cortexproject/cortex</id>
    <link href="https://github.com/cortexproject/cortex" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A horizontally scalable, highly available, multi-tenant, long term Prometheus.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cortexproject/cortex/master/images/logo.png&#34; alt=&#34;Cortex Logo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/cortexproject/cortex/actions&#34;&gt;&lt;img src=&#34;https://github.com/cortexproject/cortex/workflows/ci/badge.svg?sanitize=true&#34; alt=&#34;CI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://godoc.org/github.com/cortexproject/cortex&#34;&gt;&lt;img src=&#34;https://godoc.org/github.com/cortexproject/cortex?status.svg?sanitize=true&#34; alt=&#34;GoDoc&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://goreportcard.com/report/github.com/cortexproject/cortex&#34;&gt;&lt;img src=&#34;https://goreportcard.com/badge/github.com/cortexproject/cortex&#34; alt=&#34;Go Report Card&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://cloud-native.slack.com/messages/cortex/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/join%20slack-%23cortex-brightgreen.svg?sanitize=true&#34; alt=&#34;Slack&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://bestpractices.coreinfrastructure.org/projects/6681&#34;&gt;&lt;img src=&#34;https://bestpractices.coreinfrastructure.org/projects/6681/badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://clomonitor.io/projects/cncf/cortex&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint?url=https://clomonitor.io/api/projects/cncf/cortex/badge&#34; alt=&#34;CLOMonitor&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Cortex: horizontally scalable, highly available, multi-tenant, long term storage for Prometheus.&lt;/h1&gt; &#xA;&lt;p&gt;Cortex provides horizontally scalable, highly available, multi-tenant, long term storage for &lt;a href=&#34;https://prometheus.io&#34;&gt;Prometheus&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Horizontally scalable:&lt;/strong&gt; Cortex can run across multiple machines in a cluster, exceeding the throughput and storage of a single machine. This enables you to send the metrics from multiple Prometheus servers to a single Cortex cluster and run &#34;globally aggregated&#34; queries across all data in a single place.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Highly available:&lt;/strong&gt; When run in a cluster, Cortex can replicate data between machines. This allows you to survive machine failure without gaps in your graphs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-tenant:&lt;/strong&gt; Cortex can isolate data and queries from multiple different independent Prometheus sources in a single cluster, allowing untrusted parties to share the same cluster.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Long term storage:&lt;/strong&gt; Cortex supports S3, GCS, Swift and Microsoft Azure for long term storage of metric data. This allows you to durably store data for longer than the lifetime of any single machine, and use this data for long term capacity planning.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Cortex is a &lt;a href=&#34;https://cncf.io&#34;&gt;CNCF&lt;/a&gt; incubation project used in several production systems including &lt;a href=&#34;https://aws.amazon.com/prometheus/&#34;&gt;Amazon Managed Service for Prometheus (AMP)&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Cortex is primarily used as a &lt;a href=&#34;https://prometheus.io/docs/operating/configuration/#remote_write&#34;&gt;remote write&lt;/a&gt; destination for Prometheus, with a Prometheus-compatible query API.&lt;/p&gt; &#xA;&lt;h2&gt;Chunk Storage Deprecation Notice&lt;/h2&gt; &#xA;&lt;p&gt;The chunks storage is deprecated since v1.10.0. You&#39;re encouraged to use the &lt;a href=&#34;https://raw.githubusercontent.com/cortexproject/cortex/master/docs/blocks-storage/_index.md&#34;&gt;blocks storage&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Chunks storage was removed in release 1.14.0&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Read the &lt;a href=&#34;https://cortexmetrics.io/docs/getting-started&#34;&gt;getting started guide&lt;/a&gt; if you&#39;re new to the project. Before deploying Cortex with a permanent storage backend you should read:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cortexmetrics.io/docs/architecture/&#34;&gt;An overview of Cortex&#39;s architecture&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cortexmetrics.io/docs/getting-started/&#34;&gt;Getting started with Cortex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cortexmetrics.io/docs/configuration/&#34;&gt;Information regarding configuring Cortex&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;There are also individual &lt;a href=&#34;https://cortexmetrics.io/docs/guides/&#34;&gt;guides&lt;/a&gt; to many tasks. Please review the important &lt;a href=&#34;https://cortexmetrics.io/docs/guides/security/&#34;&gt;security advice&lt;/a&gt; before deploying.&lt;/p&gt; &#xA;&lt;p&gt;For a guide to contributing to Cortex, see the &lt;a href=&#34;https://cortexmetrics.io/docs/contributing/&#34;&gt;contributor guidelines&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Further reading&lt;/h2&gt; &#xA;&lt;p&gt;To learn more about Cortex, consult the following talks and articles.&lt;/p&gt; &#xA;&lt;h3&gt;Talks and articles&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Apr 2023 KubeCon talk &#34;How to Run a Rock Solid Multi-Tenant Prometheus&#34; (&lt;a href=&#34;https://youtu.be/Pl5hEoRPLJU&#34;&gt;video&lt;/a&gt;, &lt;a href=&#34;https://static.sched.com/hosted_files/kccnceu2023/49/Kubecon2023.pptx.pdf&#34;&gt;slides&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Oct 2022 KubeCon talk &#34;Current State and the Future of Cortex&#34; (&lt;a href=&#34;https://youtu.be/u1SfBAGWHgQ&#34;&gt;video&lt;/a&gt;, &lt;a href=&#34;https://static.sched.com/hosted_files/kccncna2022/93/KubeCon%20%2B%20CloudNativeCon%20NA%202022%20PowerPoint%20-%20Cortex.pdf&#34;&gt;slides&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Oct 2021 KubeCon talk &#34;Cortex: Intro and Production Tips&#34; (&lt;a href=&#34;https://youtu.be/zNE_kGcUGuI&#34;&gt;video&lt;/a&gt;, &lt;a href=&#34;https://static.sched.com/hosted_files/kccncna2021/8e/KubeCon%202021%20NA%20Cortex%20Maintainer.pdf&#34;&gt;slides&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Dec 2020 blog post &#34;&lt;a href=&#34;https://aws.amazon.com/blogs/opensource/how-aws-and-grafana-labs-are-scaling-cortex-for-the-cloud/&#34;&gt;How AWS and Grafana Labs are scaling Cortex for the cloud&lt;/a&gt;&#34;&lt;/li&gt; &#xA; &lt;li&gt;Oct 2020 blog post &#34;&lt;a href=&#34;https://grafana.com/blog/2020/10/19/how-to-switch-cortex-from-chunks-to-blocks-storage-and-why-you-wont-look-back/&#34;&gt;How to switch Cortex from chunks to blocks storage (and why you won’t look back)&lt;/a&gt;&#34;&lt;/li&gt; &#xA; &lt;li&gt;Oct 2020 blog post &#34;&lt;a href=&#34;https://grafana.com/blog/2020/10/06/now-ga-cortex-blocks-storage-for-running-prometheus-at-scale-with-reduced-operational-complexity/&#34;&gt;Now GA: Cortex blocks storage for running Prometheus at scale with reduced operational complexity&lt;/a&gt;&#34;&lt;/li&gt; &#xA; &lt;li&gt;Sep 2020 blog post &#34;&lt;a href=&#34;https://www.weave.works/blog/a-tale-of-tail-latencies&#34;&gt;A Tale of Tail Latencies&lt;/a&gt;&#34;&lt;/li&gt; &#xA; &lt;li&gt;Sep 2020 KubeCon talk &#34;Scaling Prometheus: How We Got Some Thanos Into Cortex&#34; (&lt;a href=&#34;https://www.youtube.com/watch?v=Z5OJzRogAS4&#34;&gt;video&lt;/a&gt;, &lt;a href=&#34;https://static.sched.com/hosted_files/kccnceu20/ec/2020-08%20-%20KubeCon%20EU%20-%20Cortex%20blocks%20storage.pdf&#34;&gt;slides&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Aug 2020 blog post &#34;&lt;a href=&#34;https://grafana.com/blog/2020/08/12/scaling-prometheus-how-were-pushing-cortex-blocks-storage-to-its-limit-and-beyond/&#34;&gt;Scaling Prometheus: How we’re pushing Cortex blocks storage to its limit and beyond&lt;/a&gt;&#34;&lt;/li&gt; &#xA; &lt;li&gt;Jul 2020 blog post &#34;&lt;a href=&#34;https://grafana.com/blog/2020/07/29/how-blocks-storage-in-cortex-reduces-operational-complexity-for-running-prometheus-at-massive-scale/&#34;&gt;How blocks storage in Cortex reduces operational complexity for running Prometheus at massive scale&lt;/a&gt;&#34;&lt;/li&gt; &#xA; &lt;li&gt;Jul 2020 PromCon talk &#34;Sharing is Caring: Leveraging Open Source to Improve Cortex &amp;amp; Thanos&#34; (&lt;a href=&#34;https://www.youtube.com/watch?v=2oTLouUvsac&#34;&gt;video&lt;/a&gt;, &lt;a href=&#34;https://docs.google.com/presentation/d/1OuKYD7-k9Grb7unppYycdmVGWN0Bo0UwdJRySOoPdpg/edit&#34;&gt;slides&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Mar 2020 blog post &#34;&lt;a href=&#34;https://kenhaines.net/cortex-zone-aware-replication/&#34;&gt;Cortex: Zone Aware Replication&lt;/a&gt;&#34;&lt;/li&gt; &#xA; &lt;li&gt;Mar 2020 blog post &#34;&lt;a href=&#34;https://grafana.com/blog/2020/03/25/how-were-using-gossip-to-improve-cortex-and-loki-availability/&#34;&gt;How we&#39;re using gossip to improve Cortex and Loki availability&lt;/a&gt;&#34;&lt;/li&gt; &#xA; &lt;li&gt;Jan 2020 blog post &#34;[The Future of Cortex: Into the Next Decade][https://grafana.com/blog/2020/01/21/the-future-of-cortex-into-the-next-decade/]&#34;&lt;/li&gt; &#xA; &lt;li&gt;Nov 2019 KubeCon talks &#34;&lt;a href=&#34;https://kccncna19.sched.com/event/UaiH/cortex-101-horizontally-scalable-long-term-storage-for-prometheus-chris-marchbanks-splunk&#34;&gt;Cortex 101: Horizontally Scalable Long Term Storage for Prometheus&lt;/a&gt;&#34; (&lt;a href=&#34;https://www.youtube.com/watch?v=f8GmbH0U_kI&#34;&gt;video&lt;/a&gt;, &lt;a href=&#34;https://static.sched.com/hosted_files/kccncna19/92/cortex_101.pdf&#34;&gt;slides&lt;/a&gt;), &#34;&lt;a href=&#34;https://kccncna19.sched.com/event/UagC/performance-tuning-and-day-2-operations-goutham-veeramachaneni-grafana-labs&#34;&gt;Configuring Cortex for Max Performance&lt;/a&gt;&#34; (&lt;a href=&#34;https://www.youtube.com/watch?v=VuE5aDHDexU&#34;&gt;video&lt;/a&gt;, &lt;a href=&#34;https://static.sched.com/hosted_files/kccncna19/87/Taming%20Cortex_%20Configuring%20for%20maximum%20performance%281%29.pdf&#34;&gt;slides&lt;/a&gt;, &lt;a href=&#34;https://grafana.com/blog/2019/12/02/kubecon-recap-configuring-cortex-for-maximum-performance-at-scale/&#34;&gt;write up&lt;/a&gt;) and &#34;&lt;a href=&#34;https://kccncna19.sched.com/event/UaWT/blazin-fast-promql-tom-wilkie-grafana-labs&#34;&gt;Blazin’ Fast PromQL&lt;/a&gt;&#34; (&lt;a href=&#34;https://static.sched.com/hosted_files/kccncna19/0b/2019-11%20Blazin%27%20Fast%20PromQL.pdf&#34;&gt;slides&lt;/a&gt;, &lt;a href=&#34;https://www.youtube.com/watch?v=yYgdZyeBOck&#34;&gt;video&lt;/a&gt;, &lt;a href=&#34;https://grafana.com/blog/2019/09/19/how-to-get-blazin-fast-promql/&#34;&gt;write up&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Nov 2019 PromCon talk &#34;&lt;a href=&#34;https://promcon.io/2019-munich/talks/two-households-both-alike-in-dignity-cortex-and-thanos/&#34;&gt;Two Households, Both Alike in Dignity: Cortex and Thanos&lt;/a&gt;&#34; (&lt;a href=&#34;https://www.youtube.com/watch?v=KmJnmd3K3Ws&amp;amp;feature=youtu.be&#34;&gt;video&lt;/a&gt;, &lt;a href=&#34;https://promcon.io/2019-munich/slides/two-households-both-alike-in-dignity-cortex-and-thanos.pdf&#34;&gt;slides&lt;/a&gt;, &lt;a href=&#34;https://grafana.com/blog/2019/11/21/promcon-recap-two-households-both-alike-in-dignity-cortex-and-thanos/&#34;&gt;write up&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;May 2019 KubeCon talks; &#34;&lt;a href=&#34;https://kccnceu19.sched.com/event/MPhX/intro-cortex-tom-wilkie-grafana-labs-bryan-boreham-weaveworks&#34;&gt;Cortex: Intro&lt;/a&gt;&#34; (&lt;a href=&#34;https://www.youtube.com/watch?v=_7Wnta-3-W0&#34;&gt;video&lt;/a&gt;, &lt;a href=&#34;https://static.sched.com/hosted_files/kccnceu19/af/Cortex%20Intro%20KubeCon%20EU%202019.pdf&#34;&gt;slides&lt;/a&gt;, &lt;a href=&#34;https://grafana.com/blog/2019/05/21/grafana-labs-at-kubecon-the-latest-on-cortex/&#34;&gt;blog post&lt;/a&gt;) and &#34;&lt;a href=&#34;https://kccnceu19.sched.com/event/MPjK/deep-dive-cortex-tom-wilkie-grafana-labs-bryan-boreham-weaveworks&#34;&gt;Cortex: Deep Dive&lt;/a&gt;&#34; (&lt;a href=&#34;https://www.youtube.com/watch?v=mYyFT4ChHio&#34;&gt;video&lt;/a&gt;, &lt;a href=&#34;https://static.sched.com/hosted_files/kccnceu19/52/Cortex%20Deep%20Dive%20KubeCon%20EU%202019.pdf&#34;&gt;slides&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Feb 2019 blog post &amp;amp; podcast; &#34;&lt;a href=&#34;https://www.weave.works/blog/prometheus-scalability-with-bryan-boreham&#34;&gt;Prometheus Scalability with Bryan Boreham&lt;/a&gt;&#34; (&lt;a href=&#34;https://softwareengineeringdaily.com/2019/01/21/prometheus-scalability-with-bryan-boreham/&#34;&gt;podcast&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Feb 2019 blog post; &#34;&lt;a href=&#34;https://www.weave.works/blog/how-aspen-mesh-runs-cortex-in-production&#34;&gt;How Aspen Mesh Runs Cortex in Production&lt;/a&gt;&#34;&lt;/li&gt; &#xA; &lt;li&gt;Dec 2018 KubeCon talk; &#34;&lt;a href=&#34;https://kccna18.sched.com/event/GrXL/cortex-infinitely-scalable-prometheus-bryan-boreham-weaveworks&#34;&gt;Cortex: Infinitely Scalable Prometheus&lt;/a&gt;&#34; (&lt;a href=&#34;https://www.youtube.com/watch?v=iyN40FsRQEo&#34;&gt;video&lt;/a&gt;, &lt;a href=&#34;https://static.sched.com/hosted_files/kccna18/9b/Cortex%20CloudNativeCon%202018.pdf&#34;&gt;slides&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Dec 2018 CNCF blog post; &#34;&lt;a href=&#34;https://www.cncf.io/blog/2018/12/18/cortex-a-multi-tenant-horizontally-scalable-prometheus-as-a-service/&#34;&gt;Cortex: a multi-tenant, horizontally scalable Prometheus-as-a-Service&lt;/a&gt;&#34;&lt;/li&gt; &#xA; &lt;li&gt;Nov 2018 CloudNative London meetup talk; &#34;Cortex: Horizontally Scalable, Highly Available Prometheus&#34; (&lt;a href=&#34;https://www.slideshare.net/grafana/cortex-horizontally-scalable-highly-available-prometheus&#34;&gt;slides&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Nov 2018 CNCF TOC Presentation; &#34;Horizontally Scalable, Multi-tenant Prometheus&#34; (&lt;a href=&#34;https://docs.google.com/presentation/d/190oIFgujktVYxWZLhLYN4q8p9dtQYoe4sxHgn4deBSI/edit#slide=id.g3b8e2d6f7e_0_6&#34;&gt;slides&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Sept 2018 blog post; &#34;&lt;a href=&#34;https://medium.com/weaveworks/what-is-cortex-2c30bcbd247d&#34;&gt;What is Cortex?&lt;/a&gt;&#34;&lt;/li&gt; &#xA; &lt;li&gt;Aug 2018 PromCon panel; &#34;&lt;a href=&#34;https://promcon.io/2018-munich/talks/panel-discussion-prometheus-long-term-storage-approaches/&#34;&gt;Prometheus Long-Term Storage Approaches&lt;/a&gt;&#34; (&lt;a href=&#34;https://www.youtube.com/watch?v=3pTG_N8yGSU&#34;&gt;video&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Jul 2018 design doc; &#34;&lt;a href=&#34;https://docs.google.com/document/d/1lsvSkv0tiAMPQv-V8vI2LZ8f4i9JuTRsuPI_i-XcAqY&#34;&gt;Cortex Query Optimisations&lt;/a&gt;&#34;&lt;/li&gt; &#xA; &lt;li&gt;Aug 2017 PromCon talk; &#34;&lt;a href=&#34;https://promcon.io/2017-munich/talks/cortex-prometheus-as-a-service-one-year-on/&#34;&gt;Cortex: Prometheus as a Service, One Year On&lt;/a&gt;&#34; (&lt;a href=&#34;https://www.youtube.com/watch?v=_8DmPW4iQBQ&#34;&gt;videos&lt;/a&gt;, &lt;a href=&#34;https://promcon.io/2017-munich/slides/cortex-prometheus-as-a-service-one-year-on.pdf&#34;&gt;slides&lt;/a&gt;, write up &lt;a href=&#34;https://kausal.co/blog/cortex-prometheus-aas-promcon-1/&#34;&gt;part 1&lt;/a&gt;, &lt;a href=&#34;https://kausal.co/blog/cortex-prometheus-aas-promcon-2/&#34;&gt;part 2&lt;/a&gt;, &lt;a href=&#34;https://kausal.co/blog/cortex-prometheus-aas-promcon-3/&#34;&gt;part 3&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Jun 2017 Prometheus London meetup talk; &#34;Cortex: open-source, horizontally-scalable, distributed Prometheus&#34; (&lt;a href=&#34;https://www.youtube.com/watch?v=Xi4jq2IUbLs&#34;&gt;video&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Dec 2016 KubeCon talk; &#34;Weave Cortex: Multi-tenant, horizontally scalable Prometheus as a Service&#34; (&lt;a href=&#34;https://www.youtube.com/watch?v=9Uctgnazfwk&#34;&gt;video&lt;/a&gt;, &lt;a href=&#34;http://www.slideshare.net/weaveworks/weave-cortex-multitenant-horizontally-scalable-prometheus-as-a-service&#34;&gt;slides&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Aug 2016 PromCon talk; &#34;Project Frankenstein: Multitenant, Scale-Out Prometheus&#34;: (&lt;a href=&#34;https://youtu.be/3Tb4Wc0kfCM&#34;&gt;video&lt;/a&gt;, &lt;a href=&#34;http://www.slideshare.net/weaveworks/project-frankenstein-a-multitenant-horizontally-scalable-prometheus-as-a-service&#34;&gt;slides&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Jun 2016 design document; &#34;&lt;a href=&#34;http://goo.gl/prdUYV&#34;&gt;Project Frankenstein: A Multi Tenant, Scale Out Prometheus&lt;/a&gt;&#34;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;help&#34;&gt;&lt;/a&gt;Getting Help&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions about Cortex:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ask a question on the &lt;a href=&#34;https://cloud-native.slack.com/messages/cortex/&#34;&gt;Cortex Slack channel&lt;/a&gt;. To invite yourself to the CNCF Slack, visit &lt;a href=&#34;http://slack.cncf.io/&#34;&gt;http://slack.cncf.io/&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/cortexproject/cortex/issues/new&#34;&gt;File an issue.&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Send an email to &lt;a href=&#34;mailto:cortex-users@lists.cncf.io&#34;&gt;&lt;/a&gt;&lt;a href=&#34;mailto:cortex-users@lists.cncf.io&#34;&gt;cortex-users@lists.cncf.io&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Your feedback is always welcome.&lt;/p&gt; &#xA;&lt;p&gt;For security issues see &lt;a href=&#34;https://github.com/cortexproject/cortex/security/policy&#34;&gt;https://github.com/cortexproject/cortex/security/policy&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Community Meetings&lt;/h2&gt; &#xA;&lt;p&gt;The Cortex community call happens every two weeks on Thursday, alternating at 1200 UTC and 1700 UTC. Meeting notes are held &lt;a href=&#34;https://docs.google.com/document/d/1shtXSAqp3t7fiC-9uZcKkq3mgwsItAJlH6YW6x1joZo/edit&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To see meeting calendar:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;See the calendar &lt;a href=&#34;https://calendar.google.com/calendar/u/0/embed?src=cncf-cortex-maintainers@lists.cncf.io&amp;amp;ctz=UTC&#34;&gt;in your browser (time zone will be UTC)&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;If you use Google Calendar, &lt;a href=&#34;https://calendar.google.com/calendar/u/0?cid=Y25jZi1jb3J0ZXgtbWFpbnRhaW5lcnNAbGlzdHMuY25jZi5pbw&#34;&gt;add the Cortex&#39;s calendar to your own Google Calendar&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;You can also just &lt;a href=&#34;https://calendar.google.com/calendar/ical/cncf-cortex-maintainers%40lists.cncf.io/public/basic.ics&#34;&gt;download the .ics file&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Hosted Cortex (Prometheus as a service)&lt;/h2&gt; &#xA;&lt;p&gt;There are several commercial services where you can use Cortex on-demand:&lt;/p&gt; &#xA;&lt;h3&gt;Amazon Managed Service for Prometheus (AMP)&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://aws.amazon.com/prometheus/&#34;&gt;Amazon Managed Service for Prometheus (AMP)&lt;/a&gt; is a Prometheus-compatible monitoring service that makes it easy to monitor containerized applications at scale. It is a highly available, secure, and managed monitoring for your containers. Get started &lt;a href=&#34;https://console.aws.amazon.com/prometheus/home&#34;&gt;here&lt;/a&gt;. To learn more about the AMP, reference our &lt;a href=&#34;https://docs.aws.amazon.com/prometheus/latest/userguide/what-is-Amazon-Managed-Service-Prometheus.html&#34;&gt;documentation&lt;/a&gt; and &lt;a href=&#34;https://aws.amazon.com/blogs/mt/getting-started-amazon-managed-service-for-prometheus/&#34;&gt;Getting Started with AMP blog&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Emeritus Maintainers&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Peter Štibraný @pstibrany&lt;/li&gt; &#xA; &lt;li&gt;Marco Pracucci @pracucci&lt;/li&gt; &#xA; &lt;li&gt;Bryan Boreham @bboreham&lt;/li&gt; &#xA; &lt;li&gt;Goutham Veeramachaneni @gouthamve&lt;/li&gt; &#xA; &lt;li&gt;Jacob Lisi @jtlisi&lt;/li&gt; &#xA; &lt;li&gt;Tom Wilkie @tomwilkie&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;History of Cortex&lt;/h2&gt; &#xA;&lt;p&gt;The Cortex project was started by Tom Wilkie (Grafana Labs&#39; VP Product) and Julius Volz (Prometheus&#39; co-founder) in June 2016.&lt;/p&gt;</summary>
  </entry>
</feed>