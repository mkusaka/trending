<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Go Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-06T01:32:04Z</updated>
  <subtitle>Daily Trending of Go in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>gosom/google-maps-scraper</title>
    <updated>2023-12-06T01:32:04Z</updated>
    <id>tag:github.com,2023-12-06:/gosom/google-maps-scraper</id>
    <link href="https://github.com/gosom/google-maps-scraper" rel="alternate"></link>
    <summary type="html">&lt;p&gt;scrape data data from Google Maps. Extracts data such as the name, address, phone number, website URL, rating, reviews number, latitude and longitude, reviews,email and more for each place&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Google maps scraper&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/gosom/google-maps-scraper/actions/workflows/build.yml/badge.svg?sanitize=true&#34; alt=&#34;build&#34;&gt; &lt;a href=&#34;https://goreportcard.com/report/github.com/gosom/google-maps-scraper&#34;&gt;&lt;img src=&#34;https://goreportcard.com/badge/github.com/gosom/google-maps-scraper&#34; alt=&#34;Go Report Card&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/gosom/google-maps-scraper/raw/main/banner.png&#34; alt=&#34;Google maps scraper&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;A command line google maps scraper build using&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/gosom/scrapemate&#34;&gt;scrapemate&lt;/a&gt; web crawling framework.&lt;/p&gt; &#xA;&lt;p&gt;You can use this repository either as is, or you can use it&#39;s code as a base and customize it to your needs&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt; Added email extraction from business website support&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Extracts many data points from google maps&lt;/li&gt; &#xA; &lt;li&gt;Exports the data to CSV, JSON or PostgreSQL&lt;/li&gt; &#xA; &lt;li&gt;Perfomance about 55 urls per minute (-depth 1 -c 8)&lt;/li&gt; &#xA; &lt;li&gt;Extendable to write your own exporter&lt;/li&gt; &#xA; &lt;li&gt;Dockerized for easy run in multiple platforms&lt;/li&gt; &#xA; &lt;li&gt;Scalable in multiple machines&lt;/li&gt; &#xA; &lt;li&gt;Optionally extracts emails from the website of the business&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Notes on email extraction&lt;/h2&gt; &#xA;&lt;p&gt;By defaul email extraction is disabled.&lt;/p&gt; &#xA;&lt;p&gt;If you enable email extraction (see quickstart) then the scraper will visit the website of the business (if exists) and it will try to extract the emails from the page.&lt;/p&gt; &#xA;&lt;p&gt;For the moment it only checks only one page of the website (the one that is registered in Gmaps). At some point, it will be added support to try to extract from other pages like about, contact, impressum etc.&lt;/p&gt; &#xA;&lt;p&gt;Keep in mind that enabling email extraction results to larger processing time, since more pages are scraped.&lt;/p&gt; &#xA;&lt;h2&gt;Extracted Data Points&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;link&#xA;title&#xA;category&#xA;address&#xA;open_hours&#xA;popular_times&#xA;website&#xA;phone&#xA;plus_code&#xA;review_count&#xA;review_rating&#xA;reviews_per_rating&#xA;latitude&#xA;longitude&#xA;cid&#xA;status&#xA;descriptions&#xA;reviews_link&#xA;thumbnail&#xA;timezone&#xA;price_range&#xA;data_id&#xA;images&#xA;reservations&#xA;order_online&#xA;menu&#xA;owner&#xA;complete_address&#xA;about&#xA;user_reviews&#xA;emails&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: email is empty by default (see Usage)&lt;/p&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;h3&gt;Using docker:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;touch results.csv &amp;amp;&amp;amp; docker run -v $PWD/example-queries.txt:/example-queries -v $PWD/results.csv:/results.csv gosom/google-maps-scraper -depth 1 -input /example-queries -results /results.csv -exit-on-inactivity 3m&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;file &lt;code&gt;results.csv&lt;/code&gt; will contain the parsed results.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;If you want emails use additionally the &lt;code&gt;-email&lt;/code&gt; parameter&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;On your host&lt;/h3&gt; &#xA;&lt;p&gt;(tested only on Ubuntu 22.04)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/gosom/google-maps-scraper.git&#xA;cd google-maps-scraper&#xA;go mod download&#xA;go build&#xA;./google-maps-scraper -input example-queries.txt -results restaurants-in-cyprus.csv -exit-on-inactivity 3m&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Be a little bit patient. In the first run it downloads required libraries.&lt;/p&gt; &#xA;&lt;p&gt;The results are written when they arrive in the &lt;code&gt;results&lt;/code&gt; file you specified&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;If you want emails use additionally the &lt;code&gt;-email&lt;/code&gt; parameter&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Command line options&lt;/h3&gt; &#xA;&lt;p&gt;try &lt;code&gt;./google-maps-scraper -h&lt;/code&gt; to see the command line options available:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;  -c int&#xA;        sets the concurrency. By default it is set to half of the number of CPUs (default 8)&#xA;  -cache string&#xA;        sets the cache directory (no effect at the moment) (default &#34;cache&#34;)&#xA;  -debug&#xA;        Use this to perform a headfull crawl (it will open a browser window) [only when using without docker]&#xA;  -depth int&#xA;        is how much you allow the scraper to scroll in the search results. Experiment with that value (default 10)&#xA;  -dsn string&#xA;        Use this if you want to use a database provider&#xA;  -email&#xA;        Use this to extract emails from the websites&#xA;  -exit-on-inactivity duration&#xA;        program exits after this duration of inactivity(example value &#39;5m&#39;)&#xA;  -input string&#xA;        is the path to the file where the queries are stored (one query per line). By default it reads from stdin (default &#34;stdin&#34;)&#xA;  -json&#xA;        Use this to produce a json file instead of csv (not available when using db)&#xA;  -lang string&#xA;        is the languate code to use for google (the hl urlparam).Default is en . For example use de for German or el for Greek (default &#34;en&#34;)&#xA;  -produce&#xA;        produce seed jobs only (only valid with dsn)&#xA;  -results string&#xA;        is the path to the file where the results will be written (default &#34;stdout&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Using Database Provider (postgreSQL)&lt;/h2&gt; &#xA;&lt;p&gt;For running in your local machine:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker-compose -f docker-compose.dev.yaml up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The above starts a PostgreSQL contains and creates the required tables&lt;/p&gt; &#xA;&lt;p&gt;to access db:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;psql -h localhost -U postgres -d postgres&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Password is &lt;code&gt;postgres&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Then from your host run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;go run main.go -dsn &#34;postgres://postgres:postgres@localhost:5432/postgres&#34; -produce -input example-queries.txt --lang el&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(configure your queries and the desired language)&lt;/p&gt; &#xA;&lt;p&gt;This will populate the table &lt;code&gt;gmaps_jobs&lt;/code&gt; .&lt;/p&gt; &#xA;&lt;p&gt;you may run the scraper using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;go run main.go -c 2 -depth 1 -dsn &#34;postgres://postgres:postgres@localhost:5432/postgres&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you have a database server and several machines you can start multiple instances of the scraper as above.&lt;/p&gt; &#xA;&lt;h3&gt;Kubernetes&lt;/h3&gt; &#xA;&lt;p&gt;You may run the scraper in a kubernetes cluster. This helps to scale it easier.&lt;/p&gt; &#xA;&lt;p&gt;Assuming you have a kubernetes cluster and a database that is accessible from the cluster:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;First populate the database as shown above&lt;/li&gt; &#xA; &lt;li&gt;Create a deployment file &lt;code&gt;scraper.deployment&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1&#xA;kind: Deployment&#xA;metadata:&#xA;  name: google-maps-scraper&#xA;spec:&#xA;  selector:&#xA;    matchLabels:&#xA;      app: google-maps-scraper&#xA;  replicas: {NUM_OF_REPLICAS}&#xA;  template:&#xA;    metadata:&#xA;      labels:&#xA;        app: google-maps-scraper&#xA;    spec:&#xA;      containers:&#xA;      - name: google-maps-scraper&#xA;        image: gosom/google-maps-scraper:v0.9.3&#xA;        imagePullPolicy: IfNotPresent&#xA;        args: [&#34;-c&#34;, &#34;1&#34;, &#34;-depth&#34;, &#34;10&#34;, &#34;-dsn&#34;, &#34;postgres://{DBUSER}:{DBPASSWD@DBHOST}:{DBPORT}/{DBNAME}&#34;, &#34;-lang&#34;, &#34;{LANGUAGE_CODE}&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please replace the values or the command args accordingly&lt;/p&gt; &#xA;&lt;p&gt;Note: Keep in mind that because the application starts a headless browser it requires CPU and memory. Use an appropriate kubernetes cluster&lt;/p&gt; &#xA;&lt;h2&gt;Perfomance&lt;/h2&gt; &#xA;&lt;p&gt;Expected speed with concurrency of 8 and depth 1 is 55 jobs/per minute. Each search is 1 job + the number or results it contains.&lt;/p&gt; &#xA;&lt;p&gt;Based on the above: if we have 1000 keywords to search with each contains 16 results =&amp;gt; 1000 * 10 = 16000 jobs.&lt;/p&gt; &#xA;&lt;p&gt;We expect this to take about 10000/55 ~ 291 minutes ~ 5 hours&lt;/p&gt; &#xA;&lt;p&gt;If you want to scrape many keywords then it&#39;s better to use the Database Provider in combination with Kubernetes for convenience and start multipe scrapers in more than 1 machines.&lt;/p&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;p&gt;For more instruction you may also read the following links&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.gkomninos.com/how-to-extract-data-from-google-maps-using-golang&#34;&gt;https://blog.gkomninos.com/how-to-extract-data-from-google-maps-using-golang&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.gkomninos.com/distributed-google-maps-scraping&#34;&gt;https://blog.gkomninos.com/distributed-google-maps-scraping&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omkarcloud/google-maps-scraper/tree/master&#34;&gt;https://github.com/omkarcloud/google-maps-scraper/tree/master&lt;/a&gt; (also a nice project) [many thanks for the idea to extract the data by utilizing the JS objects]&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Licence&lt;/h2&gt; &#xA;&lt;p&gt;This code is licenced under the MIT Licence&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Please open an ISSUE or make a Pull Request&lt;/p&gt; &#xA;&lt;h2&gt;Notes&lt;/h2&gt; &#xA;&lt;p&gt;Please use this scraper responsibly&lt;/p&gt; &#xA;&lt;p&gt;banner is generated using OpenAI&#39;s DALE&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>dub-flow/sessionprobe</title>
    <updated>2023-12-06T01:32:04Z</updated>
    <id>tag:github.com,2023-12-06:/dub-flow/sessionprobe</id>
    <link href="https://github.com/dub-flow/sessionprobe" rel="alternate"></link>
    <summary type="html">&lt;p&gt;SessionProbe is a multi-threaded tool designed for penetration testing and bug bounty hunting. It evaluates user privileges in web applications by taking a session token and checking access across a list of URLs, highlighting potential authorization issues.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/go-mod/go-version/dub-flow/sessionprobe&#34; alt=&#34;Go Version&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/downloads/dub-flow/sessionprobe/total&#34; alt=&#34;GitHub Downloads&#34;&gt; &lt;img src=&#34;https://img.shields.io/docker/image-size/fw10/sessionprobe/latest&#34; alt=&#34;Docker Image Size&#34;&gt; &lt;img src=&#34;https://img.shields.io/docker/pulls/fw10/sessionprobe&#34; alt=&#34;Docker Pulls&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;SessionProbe 🚀⚡&lt;/h1&gt; &#xA;&lt;p&gt;&lt;code&gt;SessionProbe&lt;/code&gt; is a multi-threaded pentesting tool designed to assist in evaluating user privileges in web applications. It takes a user&#39;s session token and checks for a list of URLs if access is possible, highlighting potential authorization issues. &lt;code&gt;SessionProbe&lt;/code&gt; deduplicates URL lists and provides real-time logging and progress tracking.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;SessionProbe&lt;/code&gt; is intended to be used with &lt;code&gt;Burp Suite&#39;s&lt;/code&gt; &#34;Copy URLs in this host&#34; functionality in the &lt;code&gt;Target&lt;/code&gt; tab (available in the free &lt;code&gt;Community Edition&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: You may want to change the &lt;code&gt;filter&lt;/code&gt; in &lt;code&gt;Burps&#39;s&lt;/code&gt; &lt;code&gt;Target&lt;/code&gt; tab to include files or images. Otherwise, these &lt;code&gt;URLs&lt;/code&gt; would not be copied by &#34;Copy URLs in this host&#34; and would not be tested by &lt;code&gt;SessionProbe&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Built-in Help 🆘&lt;/h1&gt; &#xA;&lt;p&gt;Help is built-in!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;sessionprobe --help&lt;/code&gt; - outputs the help.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;How to Use ⚙&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;Usage:&#xA;    sessionprobe [flags]&#xA;&#xA;Flags:&#xA;  -u, --urls string             file containing the URLs to be checked (required)&#xA;  -H, --headers string          HTTP headers to be used in the requests in the format &#34;Key1:Value1;Key2:Value2;...&#34;&#xA;  -h, --help                    help for sessionprobe&#xA;      --ignore-css              ignore URLs ending with .css (default true)&#xA;      --ignore-js               ignore URLs ending with .js (default true)&#xA;  -o, --out string              output file (default &#34;output.txt&#34;)&#xA;  -p, --proxy string            proxy URL (default: &#34;&#34;)&#xA;  -r, --filter-regex string     exclude HTTP responses using a regex. Responses whose body matches this regex will not be part of the output.&#xA;  -l, --filter-lengths string   exclude HTTP responses by body length. You can specify lengths separated by commas (e.g., &#34;123,456,789&#34;).&#xA;      --skip-verification       skip verification of SSL certificates (default false)&#xA;  -t, --threads int             number of threads (default 10)&#xA;&#xA;Examples:&#xA;    ./sessionprobe -u ./urls.txt&#xA;    ./sessionprobe -u ./urls.txt --out ./unauthenticated-test.txt --threads 15&#xA;    ./sessionprobe -u ./urls.txt -H &#34;Cookie: .AspNetCore.Cookies=&amp;lt;cookie&amp;gt;&#34; -o ./output.txt&#xA;    ./sessionprobe -u ./urls.txt -H &#34;Authorization: Bearer &amp;lt;token&amp;gt;&#34; --proxy http://localhost:8080&#xA;    ./sessionprobe -u ./urls.txt -r &#34;Page Not Found&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Run via Docker 🐳&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Navigate into the directory where your &lt;code&gt;URLs file&lt;/code&gt; is.&lt;/li&gt; &#xA; &lt;li&gt;Run the below command:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;docker run -it --rm -v &#34;$(pwd):/app/files&#34; --name sessionprobe fw10/sessionprobe [flags]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Note that we are mounting the current directory in. This means that your &lt;code&gt;URLs file&lt;/code&gt; must be in the current directory and your &lt;code&gt;output file&lt;/code&gt; will also be in this directory.&lt;/li&gt; &#xA; &lt;li&gt;Also remember to have a &lt;code&gt;Burp listener&lt;/code&gt; run on all interfaces if you want to use the &lt;code&gt;--proxy&lt;/code&gt; option&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Setup ✅&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can simply run this tool from source via &lt;code&gt;go run .&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;You can build the tool yourself via &lt;code&gt;go build&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;You can build the docker image yourself via &lt;code&gt;docker build . -t fw10/sessionprobe&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Run Tests 🧪&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To run the tests, run &lt;code&gt;go test&lt;/code&gt; or &lt;code&gt;go test -v&lt;/code&gt; (for more details)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Features 🔎&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Test for authorization issues&lt;/li&gt; &#xA; &lt;li&gt;Automatically dedupes URLs&lt;/li&gt; &#xA; &lt;li&gt;Sorts the URLs by response status code and extension (e.g., &lt;code&gt;.css&lt;/code&gt;, &lt;code&gt;.js&lt;/code&gt;), and provides the length&lt;/li&gt; &#xA; &lt;li&gt;Multi-threaded&lt;/li&gt; &#xA; &lt;li&gt;Proxy functionality to pass all requests e.g. through &lt;code&gt;Burp&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Example Output 📋&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code&gt;Responses with Status Code: 200&#xA;&#xA;https://example.com/&amp;lt;some-path&amp;gt; =&amp;gt; Length: 12345&#xA;https://example.com/&amp;lt;some-path&amp;gt; =&amp;gt; Length: 40&#xA;...&#xA;&#xA;Responses with Status Code: 301&#xA;&#xA;https://example.com/&amp;lt;some-path&amp;gt; =&amp;gt; Length: 890&#xA;https://example.com/&amp;lt;some-path&amp;gt; =&amp;gt; Length: 434&#xA;...&#xA;&#xA;Responses with Status Code: 302&#xA;&#xA;https://example.com/&amp;lt;some-path&amp;gt; =&amp;gt; Length: 0&#xA;...&#xA;&#xA;Responses with Status Code: 404&#xA;&#xA;...&#xA;&#xA;Responses with Status Code: 502&#xA;&#xA;...&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Releases 🔑&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;code&gt;Releases&lt;/code&gt; section contains some already compiled binaries for you so that you might not have to build the tool yourself&lt;/li&gt; &#xA; &lt;li&gt;For the &lt;code&gt;Mac releases&lt;/code&gt;, your Mac may throw a warning (&lt;code&gt;&#34;cannot be opened because it is from an unidentified developer&#34;&lt;/code&gt;) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;To avoid this warning in the first place, you could simply build the app yourself (see &lt;code&gt;Setup&lt;/code&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;Alternatively, you may - at your own risk - bypass this warning following the guidance here: &lt;a href=&#34;https://support.apple.com/guide/mac-help/apple-cant-check-app-for-malicious-software-mchleab3a043/mac&#34;&gt;https://support.apple.com/guide/mac-help/apple-cant-check-app-for-malicious-software-mchleab3a043/mac&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Afterwards, you can simply run the binary from the command line and provide the required flags&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Bug Reports 🐞&lt;/h1&gt; &#xA;&lt;p&gt;If you find a bug, please file an Issue right here in GitHub, and I will try to resolve it in a timely manner.&lt;/p&gt;</summary>
  </entry>
</feed>