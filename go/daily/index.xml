<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Go Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-11-16T01:30:38Z</updated>
  <subtitle>Daily Trending of Go in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>openai/openai-go</title>
    <updated>2024-11-16T01:30:38Z</updated>
    <id>tag:github.com,2024-11-16:/openai/openai-go</id>
    <link href="https://github.com/openai/openai-go" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The official Go library for the OpenAI API&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenAI Go API Library&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pkg.go.dev/github.com/openai/openai-go&#34;&gt;&lt;img src=&#34;https://pkg.go.dev/badge/github.com/openai/openai-go.svg?sanitize=true&#34; alt=&#34;Go Reference&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!WARNING] &lt;strong&gt;This release is currently in alpha&lt;/strong&gt;. Minor breaking changes may occur.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;The OpenAI Go library provides convenient access to &lt;a href=&#34;https://platform.openai.com/docs&#34;&gt;the OpenAI REST API&lt;/a&gt; from applications written in Go. The full API of this library can be found in &lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-go/main/api.md&#34;&gt;api.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;!-- x-release-please-start-version --&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;import (&#xA;&#x9;&#34;github.com/openai/openai-go&#34; // imported as openai&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;!-- x-release-please-end --&gt; &#xA;&lt;p&gt;Or to pin the version:&lt;/p&gt; &#xA;&lt;!-- x-release-please-start-version --&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;go get -u &#39;github.com/openai/openai-go@v0.1.0-alpha.37&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;!-- x-release-please-end --&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;This library requires Go 1.18+.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;The full API of this library can be found in &lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-go/main/api.md&#34;&gt;api.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-go/main/examples/&#34;&gt;examples&lt;/a&gt; directory for complete and runnable examples.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package main&#xA;&#xA;import (&#xA;&#x9;&#34;context&#34;&#xA;&#xA;&#x9;&#34;github.com/openai/openai-go&#34;&#xA;&#x9;&#34;github.com/openai/openai-go/option&#34;&#xA;)&#xA;&#xA;func main() {&#xA;&#x9;client := openai.NewClient(&#xA;&#x9;&#x9;option.WithAPIKey(&#34;My API Key&#34;), // defaults to os.LookupEnv(&#34;OPENAI_API_KEY&#34;)&#xA;&#x9;)&#xA;&#x9;chatCompletion, err := client.Chat.Completions.New(context.TODO(), openai.ChatCompletionNewParams{&#xA;&#x9;&#x9;Messages: openai.F([]openai.ChatCompletionMessageParamUnion{&#xA;&#x9;&#x9;&#x9; openai.UserMessage(&#34;Say this is a test&#34;),&#xA;&#x9;&#x9;}),&#xA;&#x9;&#x9;Model: openai.F(openai.ChatModelGPT4o),&#xA;&#x9;})&#xA;&#x9;if err != nil {&#xA;&#x9;&#x9;panic(err.Error())&#xA;&#x9;}&#xA;&#x9;println(chatCompletion.Choices[0].Message.Content)&#xA;}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Conversations&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;param := openai.ChatCompletionNewParams{&#xA;&#x9;Messages: openai.F([]openai.ChatCompletionMessageParamUnion{&#xA;&#x9;&#x9;openai.UserMessage(&#34;What kind of houseplant is easy to take care of?&#34;),&#xA;  &#x9;}),&#xA;&#x9;Seed:     openai.Int(1),&#xA;&#x9;Model:    openai.F(openai.ChatModelGPT4o),&#xA;}&#xA;&#xA;completion, err := client.Chat.Completions.New(ctx, param)&#xA;&#xA;param.Messages.Value = append(param.Messages.Value, completion.Choices[0].Message)&#xA;param.Messages.Value = append(param.Messages.Value, openai.UserMessage(&#34;How big are those?&#34;))&#xA;&#xA;// continue the conversation&#xA;completion, err = client.Chat.Completions.New(ctx, param)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Streaming responses&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;question := &#34;Write an epic&#34;&#xA;&#xA;stream := client.Chat.Completions.NewStreaming(ctx, openai.ChatCompletionNewParams{&#xA;&#x9;Messages: openai.F([]openai.ChatCompletionMessageParamUnion{&#xA;&#x9;&#x9;openai.UserMessage(question),&#xA;&#x9;}),&#xA;&#x9;Seed:  openai.Int(0),&#xA;&#x9;Model: openai.F(openai.ChatModelGPT4o),&#xA;})&#xA;&#xA;// optionally, an accumulator helper can be used&#xA;acc := openai.ChatCompletionAccumulator{}&#xA;&#xA;for stream.Next() {&#xA;&#x9;chunk := stream.Current()&#xA;&#x9;acc.AddChunk(chunk)&#xA;&#xA;&#x9;if content, ok := acc.JustFinishedContent(); ok {&#xA;&#x9;&#x9;println(&#34;Content stream finished:&#34;, content)&#xA;&#x9;}&#xA;&#xA;&#x9;// if using tool calls&#xA;&#x9;if tool, ok := acc.JustFinishedToolCall(); ok {&#xA;&#x9;&#x9;println(&#34;Tool call stream finished:&#34;, tool.Index, tool.Name, tool.Arguments)&#xA;&#x9;}&#xA;&#xA;&#x9;if refusal, ok := acc.JustFinishedRefusal(); ok {&#xA;&#x9;&#x9;println(&#34;Refusal stream finished:&#34;, refusal)&#xA;&#x9;}&#xA;&#xA;&#x9;// it&#39;s best to use chunks after handling JustFinished events&#xA;&#x9;if len(chunk.Choices) &amp;gt; 0 {&#xA;&#x9;&#x9;println(chunk.Choices[0].Delta.Content)&#xA;&#x9;}&#xA;}&#xA;&#xA;if err := stream.Err(); err != nil {&#xA;&#x9;panic(err)&#xA;}&#xA;&#xA;// After the stream is finished, acc can be used like a ChatCompletion&#xA;_ = acc.Choices[0].Message.Content&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-go/main/examples/chat-completion-accumulating/main.go&#34;&gt;full streaming and accumulation example&lt;/a&gt;&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Tool calling&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;import (&#xA;&#x9;&#34;encoding/json&#34;&#xA;&#x9;// ...&#xA;)&#xA;&#xA;// ...&#xA;&#xA;question := &#34;What is the weather in New York City?&#34;&#xA;&#xA;params := openai.ChatCompletionNewParams{&#xA;&#x9;Messages: openai.F([]openai.ChatCompletionMessageParamUnion{&#xA;&#x9;&#x9;openai.UserMessage(question),&#xA;&#x9;}),&#xA;&#x9;Tools: openai.F([]openai.ChatCompletionToolParam{&#xA;&#x9;&#x9;{&#xA;&#x9;&#x9;&#x9;Type: openai.F(openai.ChatCompletionToolTypeFunction),&#xA;&#x9;&#x9;&#x9;Function: openai.F(openai.FunctionDefinitionParam{&#xA;&#x9;&#x9;&#x9;&#x9;Name:        openai.String(&#34;get_weather&#34;),&#xA;&#x9;&#x9;&#x9;&#x9;Description: openai.String(&#34;Get weather at the given location&#34;),&#xA;&#x9;&#x9;&#x9;&#x9;Parameters: openai.F(openai.FunctionParameters{&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;&#34;type&#34;: &#34;object&#34;,&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;&#34;properties&#34;: map[string]interface{}{&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;&#34;location&#34;: map[string]string{&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;&#34;type&#34;: &#34;string&#34;,&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;},&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;},&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;&#34;required&#34;: []string{&#34;location&#34;},&#xA;&#x9;&#x9;&#x9;&#x9;}),&#xA;&#x9;&#x9;&#x9;}),&#xA;&#x9;&#x9;},&#xA;&#x9;}),&#xA;&#x9;Model: openai.F(openai.ChatModelGPT4o),&#xA;}&#xA;&#xA;// chat completion request with tool calls&#xA;completion, _ := client.Chat.Completions.New(ctx, params)&#xA;&#xA;for _, toolCall := range completion.Choices[0].Message.ToolCalls {&#xA;&#x9;if toolCall.Function.Name == &#34;get_weather&#34; {&#xA;&#x9;&#x9;// extract the location from the function call arguments&#xA;&#x9;&#x9;var args map[string]interface{}&#xA;&#x9;&#x9;_ := json.Unmarshal([]byte(toolCall.Function.Arguments), &amp;amp;args)&#xA;&#xA;&#x9;&#x9;// call a weather API with the arguments requested by the model&#xA;&#x9;&#x9;weatherData := getWeather(args[&#34;location&#34;].(string))&#xA;&#x9;&#x9;params.Messages.Value = append(params.Messages.Value, openai.ToolMessage(toolCall.ID, weatherData))&#xA;&#x9;}&#xA;}&#xA;&#xA;// ... continue the conversation with the information provided by the tool&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-go/main/examples/chat-completion-tool-calling/main.go&#34;&gt;full tool calling example&lt;/a&gt;&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Structured outputs&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;import (&#xA;&#x9;&#34;encoding/json&#34;&#xA;&#x9;&#34;github.com/invopop/jsonschema&#34;&#xA;&#x9;// ...&#xA;)&#xA;&#xA;// A struct that will be converted to a Structured Outputs response schema&#xA;type HistoricalComputer struct {&#xA;&#x9;Origin       Origin   `json:&#34;origin&#34; jsonschema_description:&#34;The origin of the computer&#34;`&#xA;&#x9;Name         string   `json:&#34;full_name&#34; jsonschema_description:&#34;The name of the device model&#34;`&#xA;&#x9;NotableFacts []string `json:&#34;notable_facts&#34; jsonschema_description:&#34;A few key facts about the computer&#34;`&#xA;}&#xA;&#xA;type Origin struct {&#xA;&#x9;YearBuilt    int64  `json:&#34;year_of_construction&#34; jsonschema_description:&#34;The year it was made&#34;`&#xA;&#x9;Organization string `json:&#34;organization&#34; jsonschema_description:&#34;The organization that was in charge of its development&#34;`&#xA;}&#xA;&#xA;func GenerateSchema[T any]() interface{} {&#xA;&#x9;reflector := jsonschema.Reflector{&#xA;&#x9;&#x9;AllowAdditionalProperties: false,&#xA;&#x9;&#x9;DoNotReference:            true,&#xA;&#x9;}&#xA;&#x9;var v T&#xA;&#x9;schema := reflector.Reflect(v)&#xA;&#x9;return schema&#xA;}&#xA;&#xA;// Generate the JSON schema at initialization time&#xA;var HistoricalComputerResponseSchema = GenerateSchema[HistoricalComputer]()&#xA;&#xA;func main() {&#xA;&#xA;&#x9;// ...&#xA;&#xA;&#x9;question := &#34;What computer ran the first neural network?&#34;&#xA;&#xA;&#x9;schemaParam := openai.ResponseFormatJSONSchemaJSONSchemaParam{&#xA;&#x9;&#x9;Name:        openai.F(&#34;biography&#34;),&#xA;&#x9;&#x9;Description: openai.F(&#34;Notable information about a person&#34;),&#xA;&#x9;&#x9;Schema:      openai.F(HistoricalComputerResponseSchema),&#xA;&#x9;&#x9;Strict:      openai.Bool(true),&#xA;&#x9;}&#xA;&#xA;&#x9;chat, _ := client.Chat.Completions.New(ctx, openai.ChatCompletionNewParams{&#xA;&#x9;&#x9;// ...&#xA;&#x9;&#x9;ResponseFormat: openai.F[openai.ChatCompletionNewParamsResponseFormatUnion](&#xA;&#x9;&#x9;&#x9;openai.ResponseFormatJSONSchemaParam{&#xA;&#x9;&#x9;&#x9;&#x9;Type:       openai.F(openai.ResponseFormatJSONSchemaTypeJSONSchema),&#xA;&#x9;&#x9;&#x9;&#x9;JSONSchema: openai.F(schemaParam),&#xA;&#x9;&#x9;&#x9;},&#xA;&#x9;&#x9;),&#xA;&#x9;&#x9;// only certain models can perform structured outputs&#xA;&#x9;&#x9;Model: openai.F(openai.ChatModelGPT4o2024_08_06),&#xA;&#x9;})&#xA;&#xA;&#x9;// extract into a well-typed struct&#xA;&#x9;historicalComputer := HistoricalComputer{}&#xA;&#x9;_ = json.Unmarshal([]byte(chat.Choices[0].Message.Content), &amp;amp;historicalComputer)&#xA;&#xA;&#x9;historicalComputer.Name&#xA;&#x9;historicalComputer.Origin.YearBuilt&#xA;&#x9;historicalComputer.Origin.Organization&#xA;&#x9;for i, fact := range historicalComputer.NotableFacts {&#xA;&#x9;&#x9;// ...&#xA;&#x9;}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-go/main/examples/structured-outputs/main.go&#34;&gt;full structured outputs example&lt;/a&gt;&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Request fields&lt;/h3&gt; &#xA;&lt;p&gt;All request parameters are wrapped in a generic &lt;code&gt;Field&lt;/code&gt; type, which we use to distinguish zero values from null or omitted fields.&lt;/p&gt; &#xA;&lt;p&gt;This prevents accidentally sending a zero value if you forget a required parameter, and enables explicitly sending &lt;code&gt;null&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;, &lt;code&gt;&#39;&#39;&lt;/code&gt;, or &lt;code&gt;0&lt;/code&gt; on optional parameters. Any field not specified is not sent.&lt;/p&gt; &#xA;&lt;p&gt;To construct fields with values, use the helpers &lt;code&gt;String()&lt;/code&gt;, &lt;code&gt;Int()&lt;/code&gt;, &lt;code&gt;Float()&lt;/code&gt;, or most commonly, the generic &lt;code&gt;F[T]()&lt;/code&gt;. To send a null, use &lt;code&gt;Null[T]()&lt;/code&gt;, and to send a nonconforming value, use &lt;code&gt;Raw[T](any)&lt;/code&gt;. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;params := FooParams{&#xA;&#x9;Name: openai.F(&#34;hello&#34;),&#xA;&#xA;&#x9;// Explicitly send `&#34;description&#34;: null`&#xA;&#x9;Description: openai.Null[string](),&#xA;&#xA;&#x9;Point: openai.F(openai.Point{&#xA;&#x9;&#x9;X: openai.Int(0),&#xA;&#x9;&#x9;Y: openai.Int(1),&#xA;&#xA;&#x9;&#x9;// In cases where the API specifies a given type,&#xA;&#x9;&#x9;// but you want to send something else, use `Raw`:&#xA;&#x9;&#x9;Z: openai.Raw[int64](0.01), // sends a float&#xA;&#x9;}),&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Response objects&lt;/h3&gt; &#xA;&lt;p&gt;All fields in response structs are value types (not pointers or wrappers).&lt;/p&gt; &#xA;&lt;p&gt;If a given field is &lt;code&gt;null&lt;/code&gt;, not present, or invalid, the corresponding field will simply be its zero value.&lt;/p&gt; &#xA;&lt;p&gt;All response structs also include a special &lt;code&gt;JSON&lt;/code&gt; field, containing more detailed information about each property, which you can use like so:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;if res.Name == &#34;&#34; {&#xA;&#x9;// true if `&#34;name&#34;` is either not present or explicitly null&#xA;&#x9;res.JSON.Name.IsNull()&#xA;&#xA;&#x9;// true if the `&#34;name&#34;` key was not present in the repsonse JSON at all&#xA;&#x9;res.JSON.Name.IsMissing()&#xA;&#xA;&#x9;// When the API returns data that cannot be coerced to the expected type:&#xA;&#x9;if res.JSON.Name.IsInvalid() {&#xA;&#x9;&#x9;raw := res.JSON.Name.Raw()&#xA;&#xA;&#x9;&#x9;legacyName := struct{&#xA;&#x9;&#x9;&#x9;First string `json:&#34;first&#34;`&#xA;&#x9;&#x9;&#x9;Last  string `json:&#34;last&#34;`&#xA;&#x9;&#x9;}{}&#xA;&#x9;&#x9;json.Unmarshal([]byte(raw), &amp;amp;legacyName)&#xA;&#x9;&#x9;name = legacyName.First + &#34; &#34; + legacyName.Last&#xA;&#x9;}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;These &lt;code&gt;.JSON&lt;/code&gt; structs also include an &lt;code&gt;Extras&lt;/code&gt; map containing any properties in the json response that were not specified in the struct. This can be useful for API features not yet present in the SDK.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;body := res.JSON.ExtraFields[&#34;my_unexpected_field&#34;].Raw()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;RequestOptions&lt;/h3&gt; &#xA;&lt;p&gt;This library uses the functional options pattern. Functions defined in the &lt;code&gt;option&lt;/code&gt; package return a &lt;code&gt;RequestOption&lt;/code&gt;, which is a closure that mutates a &lt;code&gt;RequestConfig&lt;/code&gt;. These options can be supplied to the client or at individual requests. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;client := openai.NewClient(&#xA;&#x9;// Adds a header to every request made by the client&#xA;&#x9;option.WithHeader(&#34;X-Some-Header&#34;, &#34;custom_header_info&#34;),&#xA;)&#xA;&#xA;client.Chat.Completions.New(context.TODO(), ...,&#xA;&#x9;// Override the header&#xA;&#x9;option.WithHeader(&#34;X-Some-Header&#34;, &#34;some_other_custom_header_info&#34;),&#xA;&#x9;// Add an undocumented field to the request body, using sjson syntax&#xA;&#x9;option.WithJSONSet(&#34;some.json.path&#34;, map[string]string{&#34;my&#34;: &#34;object&#34;}),&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://pkg.go.dev/github.com/openai/openai-go/option&#34;&gt;full list of request options&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Pagination&lt;/h3&gt; &#xA;&lt;p&gt;This library provides some conveniences for working with paginated list endpoints.&lt;/p&gt; &#xA;&lt;p&gt;You can use &lt;code&gt;.ListAutoPaging()&lt;/code&gt; methods to iterate through items across all pages:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;iter := client.FineTuning.Jobs.ListAutoPaging(context.TODO(), openai.FineTuningJobListParams{&#xA;&#x9;Limit: openai.F(int64(20)),&#xA;})&#xA;// Automatically fetches more pages as needed.&#xA;for iter.Next() {&#xA;&#x9;fineTuningJob := iter.Current()&#xA;&#x9;fmt.Printf(&#34;%+v\n&#34;, fineTuningJob)&#xA;}&#xA;if err := iter.Err(); err != nil {&#xA;&#x9;panic(err.Error())&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or you can use simple &lt;code&gt;.List()&lt;/code&gt; methods to fetch a single page and receive a standard response object with additional helper methods like &lt;code&gt;.GetNextPage()&lt;/code&gt;, e.g.:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;page, err := client.FineTuning.Jobs.List(context.TODO(), openai.FineTuningJobListParams{&#xA;&#x9;Limit: openai.F(int64(20)),&#xA;})&#xA;for page != nil {&#xA;&#x9;for _, job := range page.Data {&#xA;&#x9;&#x9;fmt.Printf(&#34;%+v\n&#34;, job)&#xA;&#x9;}&#xA;&#x9;page, err = page.GetNextPage()&#xA;}&#xA;if err != nil {&#xA;&#x9;panic(err.Error())&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Errors&lt;/h3&gt; &#xA;&lt;p&gt;When the API returns a non-success status code, we return an error with type &lt;code&gt;*openai.Error&lt;/code&gt;. This contains the &lt;code&gt;StatusCode&lt;/code&gt;, &lt;code&gt;*http.Request&lt;/code&gt;, and &lt;code&gt;*http.Response&lt;/code&gt; values of the request, as well as the JSON of the error body (much like other response objects in the SDK).&lt;/p&gt; &#xA;&lt;p&gt;To handle errors, we recommend that you use the &lt;code&gt;errors.As&lt;/code&gt; pattern:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;_, err := client.FineTuning.Jobs.New(context.TODO(), openai.FineTuningJobNewParams{&#xA;&#x9;Model:        openai.F(openai.FineTuningJobNewParamsModelBabbage002),&#xA;&#x9;TrainingFile: openai.F(&#34;file-abc123&#34;),&#xA;})&#xA;if err != nil {&#xA;&#x9;var apierr *openai.Error&#xA;&#x9;if errors.As(err, &amp;amp;apierr) {&#xA;&#x9;&#x9;println(string(apierr.DumpRequest(true)))  // Prints the serialized HTTP request&#xA;&#x9;&#x9;println(string(apierr.DumpResponse(true))) // Prints the serialized HTTP response&#xA;&#x9;}&#xA;&#x9;panic(err.Error()) // GET &#34;/fine_tuning/jobs&#34;: 400 Bad Request { ... }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When other errors occur, they are returned unwrapped; for example, if HTTP transport fails, you might receive &lt;code&gt;*url.Error&lt;/code&gt; wrapping &lt;code&gt;*net.OpError&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Timeouts&lt;/h3&gt; &#xA;&lt;p&gt;Requests do not time out by default; use context to configure a timeout for a request lifecycle.&lt;/p&gt; &#xA;&lt;p&gt;Note that if a request is &lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-go/main/#retries&#34;&gt;retried&lt;/a&gt;, the context timeout does not start over. To set a per-retry timeout, use &lt;code&gt;option.WithRequestTimeout()&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// This sets the timeout for the request, including all the retries.&#xA;ctx, cancel := context.WithTimeout(context.Background(), 5*time.Minute)&#xA;defer cancel()&#xA;client.Chat.Completions.New(&#xA;&#x9;ctx,&#xA;&#x9;openai.ChatCompletionNewParams{&#xA;&#x9;&#x9;Messages: openai.F([]openai.ChatCompletionMessageParamUnion{&#xA;&#x9;&#x9;&#x9; openai.UserMessage(&#34;Say this is a test&#34;),&#xA;&#x9;&#x9;}),&#xA;&#x9;&#x9;Model: openai.F(openai.ChatModelGPT4o),&#xA;&#x9;},&#xA;&#x9;// This sets the per-retry timeout&#xA;&#x9;option.WithRequestTimeout(20*time.Second),&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;File uploads&lt;/h3&gt; &#xA;&lt;p&gt;Request parameters that correspond to file uploads in multipart requests are typed as &lt;code&gt;param.Field[io.Reader]&lt;/code&gt;. The contents of the &lt;code&gt;io.Reader&lt;/code&gt; will by default be sent as a multipart form part with the file name of &#34;anonymous_file&#34; and content-type of &#34;application/octet-stream&#34;.&lt;/p&gt; &#xA;&lt;p&gt;The file name and content-type can be customized by implementing &lt;code&gt;Name() string&lt;/code&gt; or &lt;code&gt;ContentType() string&lt;/code&gt; on the run-time type of &lt;code&gt;io.Reader&lt;/code&gt;. Note that &lt;code&gt;os.File&lt;/code&gt; implements &lt;code&gt;Name() string&lt;/code&gt;, so a file returned by &lt;code&gt;os.Open&lt;/code&gt; will be sent with the file name on disk.&lt;/p&gt; &#xA;&lt;p&gt;We also provide a helper &lt;code&gt;openai.FileParam(reader io.Reader, filename string, contentType string)&lt;/code&gt; which can be used to wrap any &lt;code&gt;io.Reader&lt;/code&gt; with the appropriate file name and content type.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// A file from the file system&#xA;file, err := os.Open(&#34;input.jsonl&#34;)&#xA;openai.FileNewParams{&#xA;&#x9;File:    openai.F[io.Reader](file),&#xA;&#x9;Purpose: openai.F(openai.FilePurposeFineTune),&#xA;}&#xA;&#xA;// A file from a string&#xA;openai.FileNewParams{&#xA;&#x9;File:    openai.F[io.Reader](strings.NewReader(&#34;my file contents&#34;)),&#xA;&#x9;Purpose: openai.F(openai.FilePurposeFineTune),&#xA;}&#xA;&#xA;// With a custom filename and contentType&#xA;openai.FileNewParams{&#xA;&#x9;File:    openai.FileParam(strings.NewReader(`{&#34;hello&#34;: &#34;foo&#34;}`), &#34;file.go&#34;, &#34;application/json&#34;),&#xA;&#x9;Purpose: openai.F(openai.FilePurposeFineTune),&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Retries&lt;/h3&gt; &#xA;&lt;p&gt;Certain errors will be automatically retried 2 times by default, with a short exponential backoff. We retry by default all connection errors, 408 Request Timeout, 409 Conflict, 429 Rate Limit, and &amp;gt;=500 Internal errors.&lt;/p&gt; &#xA;&lt;p&gt;You can use the &lt;code&gt;WithMaxRetries&lt;/code&gt; option to configure or disable this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// Configure the default for all requests:&#xA;client := openai.NewClient(&#xA;&#x9;option.WithMaxRetries(0), // default is 2&#xA;)&#xA;&#xA;// Override per-request:&#xA;client.Chat.Completions.New(&#xA;&#x9;context.TODO(),&#xA;&#x9;openai.ChatCompletionNewParams{&#xA;&#x9;&#x9;Messages: openai.F([]openai.ChatCompletionMessageParamUnion{&#xA;&#x9;&#x9;&#x9; openai.UserMessage(&#34;Say this is a test&#34;),&#xA;&#x9;&#x9;}),&#xA;&#x9;&#x9;Model: openai.F(openai.ChatModelGPT4o),&#xA;&#x9;},&#xA;&#x9;option.WithMaxRetries(5),&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Making custom/undocumented requests&lt;/h3&gt; &#xA;&lt;p&gt;This library is typed for convenient access to the documented API. If you need to access undocumented endpoints, params, or response properties, the library can still be used.&lt;/p&gt; &#xA;&lt;h4&gt;Undocumented endpoints&lt;/h4&gt; &#xA;&lt;p&gt;To make requests to undocumented endpoints, you can use &lt;code&gt;client.Get&lt;/code&gt;, &lt;code&gt;client.Post&lt;/code&gt;, and other HTTP verbs. &lt;code&gt;RequestOptions&lt;/code&gt; on the client, such as retries, will be respected when making these requests.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;var (&#xA;    // params can be an io.Reader, a []byte, an encoding/json serializable object,&#xA;    // or a &#34;‚Ä¶Params&#34; struct defined in this library.&#xA;    params map[string]interface{}&#xA;&#xA;    // result can be an []byte, *http.Response, a encoding/json deserializable object,&#xA;    // or a model defined in this library.&#xA;    result *http.Response&#xA;)&#xA;err := client.Post(context.Background(), &#34;/unspecified&#34;, params, &amp;amp;result)&#xA;if err != nil {&#xA;    ‚Ä¶&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Undocumented request params&lt;/h4&gt; &#xA;&lt;p&gt;To make requests using undocumented parameters, you may use either the &lt;code&gt;option.WithQuerySet()&lt;/code&gt; or the &lt;code&gt;option.WithJSONSet()&lt;/code&gt; methods.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;params := FooNewParams{&#xA;    ID:   openai.F(&#34;id_xxxx&#34;),&#xA;    Data: openai.F(FooNewParamsData{&#xA;        FirstName: openai.F(&#34;John&#34;),&#xA;    }),&#xA;}&#xA;client.Foo.New(context.Background(), params, option.WithJSONSet(&#34;data.last_name&#34;, &#34;Doe&#34;))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Undocumented response properties&lt;/h4&gt; &#xA;&lt;p&gt;To access undocumented response properties, you may either access the raw JSON of the response as a string with &lt;code&gt;result.JSON.RawJSON()&lt;/code&gt;, or get the raw JSON of a particular field on the result with &lt;code&gt;result.JSON.Foo.Raw()&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Any fields that are not present on the response struct will be saved and can be accessed by &lt;code&gt;result.JSON.ExtraFields()&lt;/code&gt; which returns the extra fields as a &lt;code&gt;map[string]Field&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Middleware&lt;/h3&gt; &#xA;&lt;p&gt;We provide &lt;code&gt;option.WithMiddleware&lt;/code&gt; which applies the given middleware to requests.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func Logger(req *http.Request, next option.MiddlewareNext) (res *http.Response, err error) {&#xA;&#x9;// Before the request&#xA;&#x9;start := time.Now()&#xA;&#x9;LogReq(req)&#xA;&#xA;&#x9;// Forward the request to the next handler&#xA;&#x9;res, err = next(req)&#xA;&#xA;&#x9;// Handle stuff after the request&#xA;&#x9;end := time.Now()&#xA;&#x9;LogRes(res, err, start - end)&#xA;&#xA;    return res, err&#xA;}&#xA;&#xA;client := openai.NewClient(&#xA;&#x9;option.WithMiddleware(Logger),&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When multiple middlewares are provided as variadic arguments, the middlewares are applied left to right. If &lt;code&gt;option.WithMiddleware&lt;/code&gt; is given multiple times, for example first in the client then the method, the middleware in the client will run first and the middleware given in the method will run next.&lt;/p&gt; &#xA;&lt;p&gt;You may also replace the default &lt;code&gt;http.Client&lt;/code&gt; with &lt;code&gt;option.WithHTTPClient(client)&lt;/code&gt;. Only one http client is accepted (this overwrites any previous client) and receives requests after any middleware has been applied.&lt;/p&gt; &#xA;&lt;h2&gt;Microsoft Azure OpenAI&lt;/h2&gt; &#xA;&lt;p&gt;To use this library with &lt;a href=&#34;https://learn.microsoft.com/azure/ai-services/openai/overview&#34;&gt;Azure OpenAI&lt;/a&gt;, use the option.RequestOption functions in the &lt;code&gt;azure&lt;/code&gt; package.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package main&#xA;&#xA;import (&#xA;&#x9;&#34;github.com/Azure/azure-sdk-for-go/sdk/azidentity&#34;&#xA;&#x9;&#34;github.com/openai/openai-go&#34;&#xA;&#x9;&#34;github.com/openai/openai-go/azure&#34;&#xA;)&#xA;&#xA;func main() {&#xA;&#x9;const azureOpenAIEndpoint = &#34;https://&amp;lt;azure-openai-resource&amp;gt;.openai.azure.com&#34;&#xA;&#xA;&#x9;// The latest API versions, including previews, can be found here:&#xA;&#x9;// https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning&#xA;&#x9;const azureOpenAIAPIVersion = &#34;2024-06-01&#34;&#xA;&#xA;&#x9;tokenCredential, err := azidentity.NewDefaultAzureCredential(nil)&#xA;&#xA;&#x9;if err != nil {&#xA;&#x9;&#x9;fmt.Printf(&#34;Failed to create the DefaultAzureCredential: %s&#34;, err)&#xA;&#x9;&#x9;os.Exit(1)&#xA;&#x9;}&#xA;&#xA;&#x9;client := openai.NewClient(&#xA;&#x9;&#x9;azure.WithEndpoint(azureOpenAIEndpoint, azureOpenAIAPIVersion),&#xA;&#xA;&#x9;&#x9;// Choose between authenticating using a TokenCredential or an API Key&#xA;&#x9;&#x9;azure.WithTokenCredential(tokenCredential),&#xA;&#x9;&#x9;// or azure.WithAPIKey(azureOpenAIAPIKey),&#xA;&#x9;)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Semantic versioning&lt;/h2&gt; &#xA;&lt;p&gt;This package generally follows &lt;a href=&#34;https://semver.org/spec/v2.0.0.html&#34;&gt;SemVer&lt;/a&gt; conventions, though certain backwards-incompatible changes may be released as minor versions:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Changes to library internals which are technically public but not intended or documented for external use. &lt;em&gt;(Please open a GitHub issue to let us know if you are relying on such internals)&lt;/em&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Changes that we do not expect to impact the vast majority of users in practice.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.&lt;/p&gt; &#xA;&lt;p&gt;We are keen for your feedback; please open an &lt;a href=&#34;https://www.github.com/openai/openai-go/issues&#34;&gt;issue&lt;/a&gt; with questions, bugs, or suggestions.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-go/main/CONTRIBUTING.md&#34;&gt;the contributing documentation&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mudler/LocalAI</title>
    <updated>2024-11-16T01:30:38Z</updated>
    <id>tag:github.com,2024-11-16:/mudler/LocalAI</id>
    <link href="https://github.com/mudler/LocalAI" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ü§ñ The free, Open Source alternative to OpenAI, Claude and others. Self-hosted and local-first. Drop-in replacement for OpenAI, running on consumer-grade hardware. No GPU required. Runs gguf, transformers, diffusers and many more models architectures. Features: Generate Text, Audio, Video, Images, Voice Cloning, Distributed, P2P inference&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; &lt;br&gt; &lt;img height=&#34;300&#34; src=&#34;https://github.com/go-skynet/LocalAI/assets/2420543/0966aa2a-166e-4f99-a3e5-6c915fc997dd&#34;&gt; &lt;br&gt; LocalAI &lt;br&gt; &lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/go-skynet/LocalAI/fork&#34; target=&#34;blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/forks/go-skynet/LocalAI?style=for-the-badge&#34; alt=&#34;LocalAI forks&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/go-skynet/LocalAI/stargazers&#34; target=&#34;blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/stars/go-skynet/LocalAI?style=for-the-badge&#34; alt=&#34;LocalAI stars&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/go-skynet/LocalAI/pulls&#34; target=&#34;blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/issues-pr/go-skynet/LocalAI?style=for-the-badge&#34; alt=&#34;LocalAI pull-requests&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/go-skynet/LocalAI/releases&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/release/go-skynet/LocalAI?&amp;amp;label=Latest&amp;amp;style=for-the-badge&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://hub.docker.com/r/localai/localai&#34; target=&#34;blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/dockerhub-images-important.svg?logo=Docker&#34; alt=&#34;LocalAI Docker hub&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://quay.io/repository/go-skynet/local-ai?tab=tags&amp;amp;tag=latest&#34; target=&#34;blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/quay.io-images-important.svg?&#34; alt=&#34;LocalAI Quay.io&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://twitter.com/LocalAI_API&#34; target=&#34;blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/twitter/follow/LocalAI_API?label=Follow:%20LocalAI_API&amp;amp;style=social&#34; alt=&#34;Follow LocalAI_API&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://discord.gg/uJAeKSAGDy&#34; target=&#34;blank&#34;&gt; &lt;img src=&#34;https://dcbadge.vercel.app/api/server/uJAeKSAGDy?style=flat-square&amp;amp;theme=default-inverted&#34; alt=&#34;Join LocalAI Discord Community&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;span&gt;üí°&lt;/span&gt; Get help - &lt;a href=&#34;https://localai.io/faq/&#34;&gt;‚ùìFAQ&lt;/a&gt; &lt;a href=&#34;https://github.com/go-skynet/LocalAI/discussions&#34;&gt;üí≠Discussions&lt;/a&gt; &lt;a href=&#34;https://discord.gg/uJAeKSAGDy&#34;&gt;&lt;span&gt;üí¨&lt;/span&gt; Discord&lt;/a&gt; &lt;a href=&#34;https://localai.io/&#34;&gt;&lt;span&gt;üìñ&lt;/span&gt; Documentation website&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://localai.io/basics/getting_started/&#34;&gt;üíª Quickstart&lt;/a&gt; &lt;a href=&#34;https://models.localai.io/&#34;&gt;üñºÔ∏è Models&lt;/a&gt; &lt;a href=&#34;https://github.com/mudler/LocalAI/issues?q=is%3Aissue+is%3Aopen+label%3Aroadmap&#34;&gt;üöÄ Roadmap&lt;/a&gt; &lt;a href=&#34;https://demo.localai.io&#34;&gt;ü•Ω Demo&lt;/a&gt; &lt;a href=&#34;https://explorer.localai.io&#34;&gt;üåç Explorer&lt;/a&gt; &lt;a href=&#34;https://github.com/mudler/LocalAI-examples&#34;&gt;üõ´ Examples&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/go-skynet/LocalAI/actions/workflows/test.yml&#34;&gt;&lt;img src=&#34;https://github.com/go-skynet/LocalAI/actions/workflows/test.yml/badge.svg?sanitize=true&#34; alt=&#34;tests&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://github.com/go-skynet/LocalAI/actions/workflows/release.yaml&#34;&gt;&lt;img src=&#34;https://github.com/go-skynet/LocalAI/actions/workflows/release.yaml/badge.svg?sanitize=true&#34; alt=&#34;Build and Release&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://github.com/go-skynet/LocalAI/actions/workflows/image.yml&#34;&gt;&lt;img src=&#34;https://github.com/go-skynet/LocalAI/actions/workflows/image.yml/badge.svg?sanitize=true&#34; alt=&#34;build container images&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://github.com/go-skynet/LocalAI/actions/workflows/bump_deps.yaml&#34;&gt;&lt;img src=&#34;https://github.com/go-skynet/LocalAI/actions/workflows/bump_deps.yaml/badge.svg?sanitize=true&#34; alt=&#34;Bump dependencies&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://artifacthub.io/packages/search?repo=localai&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/localai&#34; alt=&#34;Artifact Hub&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LocalAI&lt;/strong&gt; is the free, Open Source OpenAI alternative. LocalAI act as a drop-in replacement REST API that‚Äôs compatible with OpenAI (Elevenlabs, Anthropic... ) API specifications for local AI inferencing. It allows you to run LLMs, generate images, audio (and not only) locally or on-prem with consumer grade hardware, supporting multiple model families. Does not require GPU. It is created and maintained by &lt;a href=&#34;https://github.com/mudler&#34;&gt;Ettore Di Giacinto&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/mudler/LocalAI/assets/2420543/20b5ccd2-8393-44f0-aaf6-87a23806381e&#34; alt=&#34;screen&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Run the installer script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl https://localai.io/install.sh | sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or run with docker:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -ti --name local-ai -p 8080:8080 localai/localai:latest-aio-cpu&#xA;# Alternative images:&#xA;# - if you have an Nvidia GPU:&#xA;# docker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-aio-gpu-nvidia-cuda-12&#xA;# - without preconfigured models&#xA;# docker run -ti --name local-ai -p 8080:8080 localai/localai:latest&#xA;# - without preconfigured models for Nvidia GPUs&#xA;# docker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-gpu-nvidia-cuda-12 &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To load models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# From the model gallery (see available models with `local-ai models list`, in the WebUI from the model tab, or visiting https://models.localai.io)&#xA;local-ai run llama-3.2-1b-instruct:q4_k_m&#xA;# Start LocalAI with the phi-2 model directly from huggingface&#xA;local-ai run huggingface://TheBloke/phi-2-GGUF/phi-2.Q8_0.gguf&#xA;# Install and run a model from the Ollama OCI registry&#xA;local-ai run ollama://gemma:2b&#xA;# Run a model from a configuration file&#xA;local-ai run https://gist.githubusercontent.com/.../phi-2.yaml&#xA;# Install and run a model from a standard OCI registry (e.g., Docker Hub)&#xA;local-ai run oci://localai/phi-2:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://localai.io/basics/getting_started/index.html&#34;&gt;üíª Getting started&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üì∞ Latest project news&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Oct 2024: examples moved to &lt;a href=&#34;https://github.com/mudler/LocalAI-examples&#34;&gt;LocalAI-examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Aug 2024: üÜï FLUX-1, &lt;a href=&#34;https://explorer.localai.io&#34;&gt;P2P Explorer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;July 2024: üî•üî• üÜï P2P Dashboard, LocalAI Federated mode and AI Swarms: &lt;a href=&#34;https://github.com/mudler/LocalAI/pull/2723&#34;&gt;https://github.com/mudler/LocalAI/pull/2723&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;June 2024: üÜï You can browse now the model gallery without LocalAI! Check out &lt;a href=&#34;https://models.localai.io&#34;&gt;https://models.localai.io&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;June 2024: Support for models from OCI registries: &lt;a href=&#34;https://github.com/mudler/LocalAI/pull/2628&#34;&gt;https://github.com/mudler/LocalAI/pull/2628&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;May 2024: üî•üî• Decentralized P2P llama.cpp: &lt;a href=&#34;https://github.com/mudler/LocalAI/pull/2343&#34;&gt;https://github.com/mudler/LocalAI/pull/2343&lt;/a&gt; (peer2peer llama.cpp!) üëâ Docs &lt;a href=&#34;https://localai.io/features/distribute/&#34;&gt;https://localai.io/features/distribute/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;May 2024: üî•üî• Openvoice: &lt;a href=&#34;https://github.com/mudler/LocalAI/pull/2334&#34;&gt;https://github.com/mudler/LocalAI/pull/2334&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;May 2024: üÜï Function calls without grammars and mixed mode: &lt;a href=&#34;https://github.com/mudler/LocalAI/pull/2328&#34;&gt;https://github.com/mudler/LocalAI/pull/2328&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;May 2024: üî•üî• Distributed inferencing: &lt;a href=&#34;https://github.com/mudler/LocalAI/pull/2324&#34;&gt;https://github.com/mudler/LocalAI/pull/2324&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;May 2024: Chat, TTS, and Image generation in the WebUI: &lt;a href=&#34;https://github.com/mudler/LocalAI/pull/2222&#34;&gt;https://github.com/mudler/LocalAI/pull/2222&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;April 2024: Reranker API: &lt;a href=&#34;https://github.com/mudler/LocalAI/pull/2121&#34;&gt;https://github.com/mudler/LocalAI/pull/2121&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Roadmap items: &lt;a href=&#34;https://github.com/mudler/LocalAI/issues?q=is%3Aissue+is%3Aopen+label%3Aroadmap&#34;&gt;List of issues&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üî•üî• Hot topics (looking for help):&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Multimodal with vLLM and Video understanding: &lt;a href=&#34;https://github.com/mudler/LocalAI/pull/3729&#34;&gt;https://github.com/mudler/LocalAI/pull/3729&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Realtime API &lt;a href=&#34;https://github.com/mudler/LocalAI/issues/3714&#34;&gt;https://github.com/mudler/LocalAI/issues/3714&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üî•üî• Distributed, P2P Global community pools: &lt;a href=&#34;https://github.com/mudler/LocalAI/issues/3113&#34;&gt;https://github.com/mudler/LocalAI/issues/3113&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;WebUI improvements: &lt;a href=&#34;https://github.com/mudler/LocalAI/issues/2156&#34;&gt;https://github.com/mudler/LocalAI/issues/2156&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Backends v2: &lt;a href=&#34;https://github.com/mudler/LocalAI/issues/1126&#34;&gt;https://github.com/mudler/LocalAI/issues/1126&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Improving UX v2: &lt;a href=&#34;https://github.com/mudler/LocalAI/issues/1373&#34;&gt;https://github.com/mudler/LocalAI/issues/1373&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Assistant API: &lt;a href=&#34;https://github.com/mudler/LocalAI/issues/1273&#34;&gt;https://github.com/mudler/LocalAI/issues/1273&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Moderation endpoint: &lt;a href=&#34;https://github.com/mudler/LocalAI/issues/999&#34;&gt;https://github.com/mudler/LocalAI/issues/999&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Vulkan: &lt;a href=&#34;https://github.com/mudler/LocalAI/issues/1647&#34;&gt;https://github.com/mudler/LocalAI/issues/1647&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Anthropic API: &lt;a href=&#34;https://github.com/mudler/LocalAI/issues/1808&#34;&gt;https://github.com/mudler/LocalAI/issues/1808&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you want to help and contribute, issues up for grabs: &lt;a href=&#34;https://github.com/mudler/LocalAI/issues?q=is%3Aissue+is%3Aopen+label%3A%22up+for+grabs%22&#34;&gt;https://github.com/mudler/LocalAI/issues?q=is%3Aissue+is%3Aopen+label%3A%22up+for+grabs%22&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üöÄ &lt;a href=&#34;https://localai.io/features/&#34;&gt;Features&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üìñ &lt;a href=&#34;https://localai.io/features/text-generation/&#34;&gt;Text generation with GPTs&lt;/a&gt; (&lt;code&gt;llama.cpp&lt;/code&gt;, &lt;code&gt;gpt4all.cpp&lt;/code&gt;, ... &lt;a href=&#34;https://localai.io/model-compatibility/index.html#model-compatibility-table&#34;&gt;&lt;span&gt;üìñ&lt;/span&gt; and more&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;üó£ &lt;a href=&#34;https://localai.io/features/text-to-audio/&#34;&gt;Text to Audio&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üîà &lt;a href=&#34;https://localai.io/features/audio-to-text/&#34;&gt;Audio to Text&lt;/a&gt; (Audio transcription with &lt;code&gt;whisper.cpp&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;üé® &lt;a href=&#34;https://localai.io/features/image-generation&#34;&gt;Image generation with stable diffusion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üî• &lt;a href=&#34;https://localai.io/features/openai-functions/&#34;&gt;OpenAI-alike tools API&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üß† &lt;a href=&#34;https://localai.io/features/embeddings/&#34;&gt;Embeddings generation for vector databases&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚úçÔ∏è &lt;a href=&#34;https://localai.io/features/constrained_grammars/&#34;&gt;Constrained grammars&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üñºÔ∏è &lt;a href=&#34;https://localai.io/models/&#34;&gt;Download Models directly from Huggingface &lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ü•Ω &lt;a href=&#34;https://localai.io/features/gpt-vision/&#34;&gt;Vision API&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üìà &lt;a href=&#34;https://localai.io/features/reranker/&#34;&gt;Reranker API&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üÜïüñß &lt;a href=&#34;https://localai.io/features/distribute/&#34;&gt;P2P Inferencing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üåç Integrated WebUI!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üíª Usage&lt;/h2&gt; &#xA;&lt;p&gt;Check out the &lt;a href=&#34;https://localai.io/basics/getting_started/index.html&#34;&gt;Getting started&lt;/a&gt; section in our documentation.&lt;/p&gt; &#xA;&lt;h3&gt;üîó Community and integrations&lt;/h3&gt; &#xA;&lt;p&gt;Build and deploy custom containers:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sozercan/aikit&#34;&gt;https://github.com/sozercan/aikit&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;WebUIs:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Jirubizu/localai-admin&#34;&gt;https://github.com/Jirubizu/localai-admin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/go-skynet/LocalAI-frontend&#34;&gt;https://github.com/go-skynet/LocalAI-frontend&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;QA-Pilot(An interactive chat project that leverages LocalAI LLMs for rapid understanding and navigation of GitHub code repository) &lt;a href=&#34;https://github.com/reid41/QA-Pilot&#34;&gt;https://github.com/reid41/QA-Pilot&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Model galleries&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/go-skynet/model-gallery&#34;&gt;https://github.com/go-skynet/model-gallery&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Other:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Helm chart &lt;a href=&#34;https://github.com/go-skynet/helm-charts&#34;&gt;https://github.com/go-skynet/helm-charts&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;VSCode extension &lt;a href=&#34;https://github.com/badgooooor/localai-vscode-plugin&#34;&gt;https://github.com/badgooooor/localai-vscode-plugin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Terminal utility &lt;a href=&#34;https://github.com/djcopley/ShellOracle&#34;&gt;https://github.com/djcopley/ShellOracle&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Local Smart assistant &lt;a href=&#34;https://github.com/mudler/LocalAGI&#34;&gt;https://github.com/mudler/LocalAGI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Home Assistant &lt;a href=&#34;https://github.com/sammcj/homeassistant-localai&#34;&gt;https://github.com/sammcj/homeassistant-localai&lt;/a&gt; / &lt;a href=&#34;https://github.com/drndos/hass-openai-custom-conversation&#34;&gt;https://github.com/drndos/hass-openai-custom-conversation&lt;/a&gt; / &lt;a href=&#34;https://github.com/valentinfrlch/ha-gpt4vision&#34;&gt;https://github.com/valentinfrlch/ha-gpt4vision&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Discord bot &lt;a href=&#34;https://github.com/mudler/LocalAGI/tree/main/examples/discord&#34;&gt;https://github.com/mudler/LocalAGI/tree/main/examples/discord&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Slack bot &lt;a href=&#34;https://github.com/mudler/LocalAGI/tree/main/examples/slack&#34;&gt;https://github.com/mudler/LocalAGI/tree/main/examples/slack&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Shell-Pilot(Interact with LLM using LocalAI models via pure shell scripts on your Linux or MacOS system) &lt;a href=&#34;https://github.com/reid41/shell-pilot&#34;&gt;https://github.com/reid41/shell-pilot&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Telegram bot &lt;a href=&#34;https://github.com/mudler/LocalAI/tree/master/examples/telegram-bot&#34;&gt;https://github.com/mudler/LocalAI/tree/master/examples/telegram-bot&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Github Actions: &lt;a href=&#34;https://github.com/marketplace/actions/start-localai&#34;&gt;https://github.com/marketplace/actions/start-localai&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Examples: &lt;a href=&#34;https://github.com/mudler/LocalAI/tree/master/examples/&#34;&gt;https://github.com/mudler/LocalAI/tree/master/examples/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;üîó Resources&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://localai.io/docs/advanced/fine-tuning/&#34;&gt;LLM finetuning guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://localai.io/basics/build/index.html&#34;&gt;How to build locally&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://localai.io/basics/getting_started/index.html#run-localai-in-kubernetes&#34;&gt;How to install in Kubernetes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://localai.io/docs/integrations/&#34;&gt;Projects integrating LocalAI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://io.midori-ai.xyz/howtos/&#34;&gt;How tos section&lt;/a&gt; (curated by our community)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;span&gt;üìñ&lt;/span&gt; üé• &lt;a href=&#34;https://localai.io/basics/news/#media-blogs-social&#34;&gt;Media, Blogs, Social&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.suse.com/c/running-ai-locally/&#34;&gt;Run Visual studio code with LocalAI (SUSE)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üÜï &lt;a href=&#34;https://mudler.pm/posts/local-ai-jetson-nano-devkit/&#34;&gt;Run LocalAI on Jetson Nano Devkit&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.pulumi.com/blog/low-code-llm-apps-with-local-ai-flowise-and-pulumi/&#34;&gt;Run LocalAI on AWS EKS with Pulumi&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://staleks.hashnode.dev/installing-localai-on-aws-ec2-instance&#34;&gt;Run LocalAI on AWS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mudler.pm/posts/smart-slackbot-for-teams/&#34;&gt;Create a slackbot for teams and OSS projects that answer to documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=PKrDNuJ_dfE&#34;&gt;LocalAI meets k8sgpt&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mudler.pm/posts/localai-question-answering/&#34;&gt;Question Answering on Documents locally with LangChain, LocalAI, Chroma, and GPT4All&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/@tyler_97636/k8sgpt-localai-unlock-kubernetes-superpowers-for-free-584790de9b65&#34;&gt;Tutorial to use k8sgpt with LocalAI&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you utilize this repository, data in a downstream project, please consider citing it with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{localai,&#xA;  author = {Ettore Di Giacinto},&#xA;  title = {LocalAI: The free, Open source OpenAI alternative},&#xA;  year = {2023},&#xA;  publisher = {GitHub},&#xA;  journal = {GitHub repository},&#xA;  howpublished = {\url{https://github.com/go-skynet/LocalAI}},&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;‚ù§Ô∏è Sponsors&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Do you find LocalAI useful?&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Support the project by becoming &lt;a href=&#34;https://github.com/sponsors/mudler&#34;&gt;a backer or sponsor&lt;/a&gt;. Your logo will show up here with a link to your website.&lt;/p&gt; &#xA;&lt;p&gt;A huge thank you to our generous sponsors who support this project covering CI expenses, and our &lt;a href=&#34;https://github.com/sponsors/mudler&#34;&gt;Sponsor list&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.spectrocloud.com/&#34; target=&#34;blank&#34;&gt; &lt;img height=&#34;200&#34; src=&#34;https://github.com/go-skynet/LocalAI/assets/2420543/68a6f3cb-8a65-4a4d-99b5-6417a8905512&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://www.premai.io/&#34; target=&#34;blank&#34;&gt; &lt;img height=&#34;200&#34; src=&#34;https://github.com/mudler/LocalAI/assets/2420543/42e4ca83-661e-4f79-8e46-ae43689683d6&#34;&gt; &lt;br&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;üåü Star history&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#go-skynet/LocalAI&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=go-skynet/LocalAI&amp;amp;type=Date&#34; alt=&#34;LocalAI Star history Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üìñ License&lt;/h2&gt; &#xA;&lt;p&gt;LocalAI is a community-driven project created by &lt;a href=&#34;https://github.com/mudler/&#34;&gt;Ettore Di Giacinto&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;MIT - Author Ettore Di Giacinto &lt;a href=&#34;mailto:mudler@localai.io&#34;&gt;mudler@localai.io&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üôá Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;LocalAI couldn&#39;t have been built without the help of great software already available from the community. Thank you!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;https://github.com/tatsu-lab/stanford_alpaca&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/cornelk/llama-go&#34;&gt;https://github.com/cornelk/llama-go&lt;/a&gt; for the initial ideas&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/antimatter15/alpaca.cpp&#34;&gt;https://github.com/antimatter15/alpaca.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/EdVince/Stable-Diffusion-NCNN&#34;&gt;https://github.com/EdVince/Stable-Diffusion-NCNN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ggerganov/whisper.cpp&#34;&gt;https://github.com/ggerganov/whisper.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/saharNooby/rwkv.cpp&#34;&gt;https://github.com/saharNooby/rwkv.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rhasspy/piper&#34;&gt;https://github.com/rhasspy/piper&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ü§ó Contributors&lt;/h2&gt; &#xA;&lt;p&gt;This is a community project, a special thanks to our contributors! ü§ó &lt;a href=&#34;https://github.com/go-skynet/LocalAI/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=go-skynet/LocalAI&#34;&gt; &lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>