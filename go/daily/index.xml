<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Go Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-08-11T01:33:16Z</updated>
  <subtitle>Daily Trending of Go in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>sammcj/gollama</title>
    <updated>2025-08-11T01:33:16Z</updated>
    <id>tag:github.com,2025-08-11:/sammcj/gollama</id>
    <link href="https://github.com/sammcj/gollama" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Go manage your Ollama models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Gollama&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/sammcj/gollama/main/gollama-logo.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt; &#xA;&lt;p&gt;Gollama is a macOS / Linux tool for managing Ollama models.&lt;/p&gt; &#xA;&lt;p&gt;It provides a TUI (Text User Interface) for listing, inspecting, deleting, copying, and pushing Ollama models as well as bidirectional syncing with LM Studio*.&lt;/p&gt; &#xA;&lt;p&gt;The application allows users to interactively select models, sort, filter, edit, run, unload and perform actions on them using hotkeys.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/sammcj/gollama/main/screenshots/gollama-v1.0.0.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sammcj/gollama/main/#gollama&#34;&gt;Gollama&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sammcj/gollama/main/#table-of-contents&#34;&gt;Table of Contents&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sammcj/gollama/main/#features&#34;&gt;Features&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sammcj/gollama/main/#installation&#34;&gt;Installation&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sammcj/gollama/main/#go-install-recommended&#34;&gt;go install (recommended)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sammcj/gollama/main/#curl&#34;&gt;curl&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sammcj/gollama/main/#manually&#34;&gt;Manually&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sammcj/gollama/main/#if-command-not-found-gollama&#34;&gt;if &#34;command not found: gollama&#34;&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sammcj/gollama/main/#usage&#34;&gt;Usage&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sammcj/gollama/main/#key-bindings&#34;&gt;Key Bindings&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sammcj/gollama/main/#configuration&#34;&gt;Configuration&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sammcj/gollama/main/#installation-and-build-from-source&#34;&gt;Installation and build from source&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sammcj/gollama/main/#themes&#34;&gt;Themes&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sammcj/gollama/main/#logging&#34;&gt;Logging&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sammcj/gollama/main/#contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sammcj/gollama/main/#acknowledgements&#34;&gt;Acknowledgements&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sammcj/gollama/main/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;The project started off as a rewrite of my &lt;a href=&#34;https://smcleod.net/2024/03/llamalink-ollama-to-lm-studio-llm-model-linker/&#34;&gt;llamalink&lt;/a&gt; project, but I decided to expand it to include more features and make it more user-friendly.&lt;/p&gt; &#xA;&lt;p&gt;It&#39;s in active development, so there are some bugs and missing features, however I&#39;m finding it useful for managing my models every day, especially for cleaning up old models.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;List available models&lt;/li&gt; &#xA; &lt;li&gt;Display metadata such as size, quantisation level, model family, and modified date&lt;/li&gt; &#xA; &lt;li&gt;Edit / update a model&#39;s Modelfile&lt;/li&gt; &#xA; &lt;li&gt;Sort models by name, size, modification date, quantisation level, family etc&lt;/li&gt; &#xA; &lt;li&gt;Select and delete models&lt;/li&gt; &#xA; &lt;li&gt;Run and unload models&lt;/li&gt; &#xA; &lt;li&gt;Inspect model for additional details&lt;/li&gt; &#xA; &lt;li&gt;Calculate approximate vRAM usage for a model&lt;/li&gt; &#xA; &lt;li&gt;Bidirectional sync with LM Studio: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Link Ollama models to LM Studio&lt;/li&gt; &#xA;   &lt;li&gt;Create Ollama models from LM Studio models &lt;strong&gt;EXPERIMENTAL&lt;/strong&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Copy / rename models&lt;/li&gt; &#xA; &lt;li&gt;Push models to a registry&lt;/li&gt; &#xA; &lt;li&gt;Copy models to remote hosts (spit)&lt;/li&gt; &#xA; &lt;li&gt;Show running models&lt;/li&gt; &#xA; &lt;li&gt;Has some cool bugs&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See also - &lt;a href=&#34;https://github.com/sammcj/ingest&#34;&gt;ingest&lt;/a&gt; for passing directories/repos of code to markdown formatted for LLMs.&lt;/p&gt; &#xA;&lt;p&gt;Gollama Intro (&#34;Podcast&#34; Episode):&lt;/p&gt; &#xA;&lt;p&gt;&#xA; &lt;audio src=&#34;https://github.com/sammcj/smcleod_files/raw/refs/heads/master/audio/podcast-ep-sw/Podcast%20Episode%20-%20Gollama.mp3&#34; controls preload&gt;&lt;/audio&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;go install (recommended)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;go install github.com/sammcj/gollama@HEAD&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;curl&lt;/h3&gt; &#xA;&lt;p&gt;I don&#39;t recommend this method as it&#39;s not as easy to update, but you can use the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;curl -sL https://raw.githubusercontent.com/sammcj/gollama/refs/heads/main/scripts/install.sh | bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Manually&lt;/h3&gt; &#xA;&lt;p&gt;Download the most recent release from the &lt;a href=&#34;https://github.com/sammcj/gollama/releases&#34;&gt;releases page&lt;/a&gt; and extract the binary to a directory in your PATH.&lt;/p&gt; &#xA;&lt;p&gt;e.g. &lt;code&gt;zip -d gollama*.zip -d gollama &amp;amp;&amp;amp; mv gollama /usr/local/bin&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;if &#34;command not found: gollama&#34;&lt;/h3&gt; &#xA;&lt;p&gt;If you see this error, add environment variables to &lt;code&gt;.zshrc&lt;/code&gt; or &lt;code&gt;.bashrc&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;echo &#39;export PATH=$PATH:$HOME/go/bin&#39; &amp;gt;&amp;gt; ~/.zshrc&#xA;source ~/.zshrc&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;To run the &lt;code&gt;gollama&lt;/code&gt; application, use the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;gollama&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Tip&lt;/em&gt;: I like to alias gollama to &lt;code&gt;g&lt;/code&gt; for quick access:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;echo &#34;alias g=gollama&#34; &amp;gt;&amp;gt; ~/.zshrc&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Key Bindings&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;Space&lt;/code&gt;: Select&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Enter&lt;/code&gt;: Run model (Ollama run)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;i&lt;/code&gt;: Inspect model&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;t&lt;/code&gt;: Top (show running models)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;D&lt;/code&gt;: Delete model&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;e&lt;/code&gt;: Edit model&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;c&lt;/code&gt;: Copy model&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;U&lt;/code&gt;: Unload all models&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;p&lt;/code&gt;: Pull an existing model&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ctrl+k&lt;/code&gt;: Pull model &amp;amp; preserve user configuration&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ctrl+p&lt;/code&gt;: Pull (get) new model&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;P&lt;/code&gt;: Push model&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;n&lt;/code&gt;: Sort by name&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;s&lt;/code&gt;: Sort by size&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;m&lt;/code&gt;: Sort by modified&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;k&lt;/code&gt;: Sort by quantisation&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;f&lt;/code&gt;: Sort by family&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;B&lt;/code&gt;: Sort by parameter size&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;l&lt;/code&gt;: Link model to LM Studio&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;L&lt;/code&gt;: Link all models to LM Studio&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;r&lt;/code&gt;: Rename model &lt;em&gt;&lt;strong&gt;(Work in progress)&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;q&lt;/code&gt;: Quit&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Top&lt;/h4&gt; &#xA;&lt;p&gt;Top (&lt;code&gt;t&lt;/code&gt;)&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/sammcj/gollama/main/screenshots/gollama-top.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Inspect&lt;/h4&gt; &#xA;&lt;p&gt;Inspect (&lt;code&gt;i&lt;/code&gt;)&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/sammcj/gollama/main/screenshots/gollama-inspect.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Link&lt;/h4&gt; &#xA;&lt;p&gt;Gollama supports bidirectional syncing between Ollama and LM Studio:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Ollama â†’ LM Studio:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Link (&lt;code&gt;l&lt;/code&gt;): Link selected model to LM Studio&lt;/li&gt; &#xA; &lt;li&gt;Link All (&lt;code&gt;L&lt;/code&gt;): Link all models to LM Studio&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;LM Studio â†’ Ollama:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--link-lmstudio&lt;/code&gt;: Link LM Studio models to Ollama (creates symlinks)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;-C&lt;/code&gt; or &lt;code&gt;--create-from-lmstudio&lt;/code&gt;: Create Ollama models from LM Studio models&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Vision model support&lt;/strong&gt;: Automatically detects and handles mmproj files for vision models&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Smart filtering&lt;/strong&gt;: Skips models already linked between systems&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Safe operation&lt;/strong&gt;: Dry-run mode (&lt;code&gt;-n&lt;/code&gt; or &lt;code&gt;--dry-run&lt;/code&gt;) shows what would happen without making changes&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Configurable parameters&lt;/strong&gt;: Uses sensible defaults (16K context, 0.6 temperature, etc.)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;When linking models to LM Studio, Gollama creates a Modelfile with the template from LM-Studio and a set of default parameters that you can adjust.&lt;/p&gt; &#xA;&lt;p&gt;Note: Linking requires admin privileges if you&#39;re running Windows.&lt;/p&gt; &#xA;&lt;h4&gt;Spit (Copy to Remote)&lt;/h4&gt; &#xA;&lt;p&gt;The spit functionality allows you to copy Ollama models to remote hosts. This is useful for distributing models across multiple machines or creating backups.&lt;/p&gt; &#xA;&lt;p&gt;You can use the command-line interface:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Copy a specific model to a remote host&#xA;gollama --spit my-model --remote http://remote-host:11434&#xA;&#xA;# Copy all models to a remote host&#xA;gollama --spit-all --remote http://remote-host:11434&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This functionality uses the &lt;a href=&#34;https://github.com/sammcj/spitter&#34;&gt;spitter&lt;/a&gt; package to handle the model copying process.&lt;/p&gt; &#xA;&lt;h4&gt;Command-line Options&lt;/h4&gt; &#xA;&lt;p&gt;&lt;strong&gt;Model Management:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;-l&lt;/code&gt;: List all available Ollama models and exit&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;-s &amp;lt;search term&amp;gt;&lt;/code&gt;: Search for models by name &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;OR operator (&lt;code&gt;&#39;term1|term2&#39;&lt;/code&gt;) returns models that match either term&lt;/li&gt; &#xA;   &lt;li&gt;AND operator (&lt;code&gt;&#39;term1&amp;amp;term2&#39;&lt;/code&gt;) returns models that match both terms&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;-e &amp;lt;model&amp;gt;&lt;/code&gt;: Edit the Modelfile for a model&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;-u&lt;/code&gt;: Unload all running models&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;-v&lt;/code&gt;: Print the version and exit&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;LM Studio Integration:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;-L&lt;/code&gt;: Link all available Ollama models to LM Studio and exit&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--link-lmstudio&lt;/code&gt;: Link all available LM Studio models to Ollama and exit &lt;strong&gt;EXPERIMENTAL&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;-C&lt;/code&gt; or &lt;code&gt;--create-from-lmstudio&lt;/code&gt;: Create Ollama models from LM Studio models &lt;strong&gt;EXPERIMENTAL&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;-n&lt;/code&gt; or &lt;code&gt;--dry-run&lt;/code&gt;: Show what would happen without making any changes (works with all sync operations)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Configuration:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;-h&lt;/code&gt;, or &lt;code&gt;--host&lt;/code&gt;: Specify the host for the Ollama API&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;-H&lt;/code&gt;: Shortcut for &lt;code&gt;-h http://localhost:11434&lt;/code&gt; (connect to local Ollama API)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--ollama-dir&lt;/code&gt;: Custom Ollama models directory&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--lm-dir&lt;/code&gt;: Custom LM Studio models directory&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--log&lt;/code&gt; or &lt;code&gt;--log-level&lt;/code&gt;: Override log level (debug, info, warn, error)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Cleanup:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--cleanup&lt;/code&gt;: Remove all symlinked models and empty directories and exit&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--no-cleanup&lt;/code&gt;: Don&#39;t cleanup broken symlinks&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Remote Operations:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--spit &amp;lt;model&amp;gt;&lt;/code&gt;: Copy a model to a remote host&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--spit-all&lt;/code&gt;: Copy all models to a remote host&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--remote &amp;lt;url&amp;gt;&lt;/code&gt;: Remote host URL for spit operations (e.g., &lt;a href=&#34;http://remote-host:11434&#34;&gt;http://remote-host:11434&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;vRAM Analysis:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--vram&lt;/code&gt;: Estimate vRAM usage for a model. Accepts: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Ollama models (e.g. &lt;code&gt;llama3.1:8b-instruct-q6_K&lt;/code&gt;, &lt;code&gt;qwen2:14b-q4_0&lt;/code&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;HuggingFace models (e.g. &lt;code&gt;NousResearch/Hermes-2-Theta-Llama-3-8B&lt;/code&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;--fits&lt;/code&gt;: Available memory in GB for context calculation (e.g. &lt;code&gt;6&lt;/code&gt; for 6GB)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;--vram-to-nth&lt;/code&gt; or &lt;code&gt;--context&lt;/code&gt;: Maximum context length to analyze (e.g. &lt;code&gt;32k&lt;/code&gt; or &lt;code&gt;128k&lt;/code&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;--quant&lt;/code&gt;: Override quantisation level (e.g. &lt;code&gt;Q4_0&lt;/code&gt;, &lt;code&gt;Q5_K_M&lt;/code&gt;)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h5&gt;Simple model listing&lt;/h5&gt; &#xA;&lt;p&gt;Gollama can also be called with &lt;code&gt;-l&lt;/code&gt; to list models without the TUI.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;gollama -l&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;List (&lt;code&gt;gollama -l&lt;/code&gt;):&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/sammcj/gollama/main/screenshots/cli-list.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt; &#xA;&lt;h5&gt;Edit&lt;/h5&gt; &#xA;&lt;p&gt;Gollama can be called with &lt;code&gt;-e&lt;/code&gt; to edit the Modelfile for a model.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;gollama -e my-model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Search&lt;/h5&gt; &#xA;&lt;p&gt;Gollama can be called with &lt;code&gt;-s&lt;/code&gt; to search for models by name.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;gollama -s my-model # returns models that contain &#39;my-model&#39;&#xA;&#xA;gollama -s &#39;my-model|my-other-model&#39; # returns models that contain either &#39;my-model&#39; or &#39;my-other-model&#39;&#xA;&#xA;gollama -s &#39;my-model&amp;amp;instruct&#39; # returns models that contain both &#39;my-model&#39; and &#39;instruct&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;vRAM Estimation&lt;/h5&gt; &#xA;&lt;p&gt;Gollama includes a comprehensive vRAM estimation feature:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Calculate vRAM usage for a pulled Ollama model (e.g. &lt;code&gt;my-model:mytag&lt;/code&gt;), or huggingface model ID (e.g. &lt;code&gt;author/name&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Determine maximum context length for a given vRAM constraint&lt;/li&gt; &#xA; &lt;li&gt;Find the best quantisation setting for a given vRAM and context constraint&lt;/li&gt; &#xA; &lt;li&gt;Shows estimates for different k/v cache quantisation options (fp16, q8_0, q4_0)&lt;/li&gt; &#xA; &lt;li&gt;Automatic detection of available CUDA vRAM (&lt;strong&gt;coming soon!&lt;/strong&gt;) or system RAM&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/sammcj/gollama/main/screenshots/vram.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt; &#xA;&lt;p&gt;To estimate (v)RAM usage:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;gollama --vram llama3.1:8b-instruct-q6_K&#xA;&#xA;ðŸ“Š VRAM Estimation for Model: llama3.1:8b-instruct-q6_K&#xA;&#xA;| QUANT   | CTX  | BPW | 2K  | 8K              | 16K             | 32K             | 49K             | 64K |&#xA;| ------- | ---- | --- | --- | --------------- | --------------- | --------------- | --------------- |&#xA;| IQ1_S   | 1.56 | 2.2 | 2.8 | 3.7(3.7,3.7)    | 5.5(5.5,5.5)    | 7.3(7.3,7.3)    | 9.1(9.1,9.1)    |&#xA;| IQ2_XXS | 2.06 | 2.6 | 3.3 | 4.3(4.3,4.3)    | 6.1(6.1,6.1)    | 7.9(7.9,7.9)    | 9.8(9.8,9.8)    |&#xA;| IQ2_XS  | 2.31 | 2.9 | 3.6 | 4.5(4.5,4.5)    | 6.4(6.4,6.4)    | 8.2(8.2,8.2)    | 10.1(10.1,10.1) |&#xA;| IQ2_S   | 2.50 | 3.1 | 3.8 | 4.7(4.7,4.7)    | 6.6(6.6,6.6)    | 8.5(8.5,8.5)    | 10.4(10.4,10.4) |&#xA;| IQ2_M   | 2.70 | 3.2 | 4.0 | 4.9(4.9,4.9)    | 6.8(6.8,6.8)    | 8.7(8.7,8.7)    | 10.6(10.6,10.6) |&#xA;| IQ3_XXS | 3.06 | 3.6 | 4.3 | 5.3(5.3,5.3)    | 7.2(7.2,7.2)    | 9.2(9.2,9.2)    | 11.1(11.1,11.1) |&#xA;| IQ3_XS  | 3.30 | 3.8 | 4.5 | 5.5(5.5,5.5)    | 7.5(7.5,7.5)    | 9.5(9.5,9.5)    | 11.4(11.4,11.4) |&#xA;| Q2_K    | 3.35 | 3.9 | 4.6 | 5.6(5.6,5.6)    | 7.6(7.6,7.6)    | 9.5(9.5,9.5)    | 11.5(11.5,11.5) |&#xA;| Q3_K_S  | 3.50 | 4.0 | 4.8 | 5.7(5.7,5.7)    | 7.7(7.7,7.7)    | 9.7(9.7,9.7)    | 11.7(11.7,11.7) |&#xA;| IQ3_S   | 3.50 | 4.0 | 4.8 | 5.7(5.7,5.7)    | 7.7(7.7,7.7)    | 9.7(9.7,9.7)    | 11.7(11.7,11.7) |&#xA;| IQ3_M   | 3.70 | 4.2 | 5.0 | 6.0(6.0,6.0)    | 8.0(8.0,8.0)    | 9.9(9.9,9.9)    | 12.0(12.0,12.0) |&#xA;| Q3_K_M  | 3.91 | 4.4 | 5.2 | 6.2(6.2,6.2)    | 8.2(8.2,8.2)    | 10.2(10.2,10.2) | 12.2(12.2,12.2) |&#xA;| IQ4_XS  | 4.25 | 4.7 | 5.5 | 6.5(6.5,6.5)    | 8.6(8.6,8.6)    | 10.6(10.6,10.6) | 12.7(12.7,12.7) |&#xA;| Q3_K_L  | 4.27 | 4.7 | 5.5 | 6.5(6.5,6.5)    | 8.6(8.6,8.6)    | 10.7(10.7,10.7) | 12.7(12.7,12.7) |&#xA;| IQ4_NL  | 4.50 | 5.0 | 5.7 | 6.8(6.8,6.8)    | 8.9(8.9,8.9)    | 10.9(10.9,10.9) | 13.0(13.0,13.0) |&#xA;| Q4_0    | 4.55 | 5.0 | 5.8 | 6.8(6.8,6.8)    | 8.9(8.9,8.9)    | 11.0(11.0,11.0) | 13.1(13.1,13.1) |&#xA;| Q4_K_S  | 4.58 | 5.0 | 5.8 | 6.9(6.9,6.9)    | 8.9(8.9,8.9)    | 11.0(11.0,11.0) | 13.1(13.1,13.1) |&#xA;| Q4_K_M  | 4.85 | 5.3 | 6.1 | 7.1(7.1,7.1)    | 9.2(9.2,9.2)    | 11.4(11.4,11.4) | 13.5(13.5,13.5) |&#xA;| Q4_K_L  | 4.90 | 5.3 | 6.1 | 7.2(7.2,7.2)    | 9.3(9.3,9.3)    | 11.4(11.4,11.4) | 13.6(13.6,13.6) |&#xA;| Q5_K_S  | 5.54 | 5.9 | 6.8 | 7.8(7.8,7.8)    | 10.0(10.0,10.0) | 12.2(12.2,12.2) | 14.4(14.4,14.4) |&#xA;| Q5_0    | 5.54 | 5.9 | 6.8 | 7.8(7.8,7.8)    | 10.0(10.0,10.0) | 12.2(12.2,12.2) | 14.4(14.4,14.4) |&#xA;| Q5_K_M  | 5.69 | 6.1 | 6.9 | 8.0(8.0,8.0)    | 10.2(10.2,10.2) | 12.4(12.4,12.4) | 14.6(14.6,14.6) |&#xA;| Q5_K_L  | 5.75 | 6.1 | 7.0 | 8.1(8.1,8.1)    | 10.3(10.3,10.3) | 12.5(12.5,12.5) | 14.7(14.7,14.7) |&#xA;| Q6_K    | 6.59 | 7.0 | 8.0 | 9.4(9.4,9.4)    | 12.2(12.2,12.2) | 15.0(15.0,15.0) | 17.8(17.8,17.8) |&#xA;| Q8_0    | 8.50 | 8.8 | 9.9 | 11.4(11.4,11.4) | 14.4(14.4,14.4) | 17.4(17.4,17.4) | 20.3(20.3,20.3) |&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To find the best quantisation type for a given memory constraint (e.g. 6GB) you can provide &lt;code&gt;--fits &amp;lt;number of GB&amp;gt;&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;gollama --vram NousResearch/Hermes-2-Theta-Llama-3-8B --fits 6&#xA;&#xA;ðŸ“Š VRAM Estimation for Model: NousResearch/Hermes-2-Theta-Llama-3-8B&#xA;&#xA;| QUANT/CTX | BPW  | 2K  | 8K  | 16K          | 32K           | 49K            | 64K             |&#xA;| --------- | ---- | --- | --- | ------------ | ------------- | -------------- | --------------- |&#xA;| IQ1_S     | 1.56 | 2.4 | 3.8 | 5.7(4.7,4.2) | 9.5(7.5,6.5)  | 13.3(10.3,8.8) | 17.1(13.1,11.1) |&#xA;| IQ2_XXS   | 2.06 | 2.9 | 4.3 | 6.3(5.3,4.8) | 10.1(8.1,7.1) | 13.9(10.9,9.4) | 17.8(13.8,11.8) |&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will display a table showing vRAM usage for various quantisation types and context sizes.&lt;/p&gt; &#xA;&lt;p&gt;The vRAM estimator works by:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Fetching the model configuration from Hugging Face (if not cached locally)&lt;/li&gt; &#xA; &lt;li&gt;Calculating the memory requirements for model parameters, activations, and KV cache&lt;/li&gt; &#xA; &lt;li&gt;Adjusting calculations based on the specified quantisation settings&lt;/li&gt; &#xA; &lt;li&gt;Performing binary and linear searches to optimize for context length or quantisation settings&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Note: The estimator will attempt to use CUDA vRAM if available, otherwise it will fall back to system RAM for calculations.&lt;/p&gt; &#xA;&lt;h5&gt;LM Studio Integration Examples&lt;/h5&gt; &#xA;&lt;p&gt;&lt;strong&gt;Create Ollama models from LM Studio models:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;WARNING: EXPERIMENTAL, BACK UP YOUR MODELS FIRST!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Dry-run to see what would be created (recommended first step)&#xA;gollama -C -n&#xA;# or&#xA;gollama --create-from-lmstudio --dry-run&#xA;&#xA;# Actually create the models&#xA;gollama -C&#xA;# or&#xA;gollama --create-from-lmstudio&#xA;&#xA;# Create with debug logging&#xA;gollama -C --log debug&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Link LM Studio models to Ollama (symlinks):&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Dry-run first&#xA;gollama --link-lmstudio -n&#xA;&#xA;# Create symlinks&#xA;gollama --link-lmstudio&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Link Ollama models to LM Studio:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Link all models with dry-run&#xA;gollama -L -n&#xA;&#xA;# Actually link all models&#xA;gollama -L&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Key differences between linking and creating:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;--link-lmstudio&lt;/code&gt;&lt;/strong&gt;: Creates symlinks, LM Studio uses original Ollama files&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;-C&lt;/code&gt; / &lt;code&gt;--create-from-lmstudio&lt;/code&gt;&lt;/strong&gt;: Creates new Ollama models, independent copies with proper metadata&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Vision model support:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The create functionality automatically detects and handles vision models with mmproj files:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ gollama -C -n&#xA;[DRY RUN] Found 5 unlinked LM Studio models&#xA;[DRY RUN] Processing model publisher/vision-model... (vision model with 1 projection files) success!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;p&gt;Gollama uses a JSON configuration file located at &lt;code&gt;~/.config/gollama/config.json&lt;/code&gt;. The configuration file includes options for sorting, columns, API keys, log levels, theme etc...&lt;/p&gt; &#xA;&lt;p&gt;Example configuration:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;default_sort&#34;: &#34;modified&#34;,&#xA;  &#34;columns&#34;: [&#xA;    &#34;Name&#34;,&#xA;    &#34;Size&#34;,&#xA;    &#34;Quant&#34;,&#xA;    &#34;Family&#34;,&#xA;    &#34;Modified&#34;,&#xA;    &#34;ID&#34;&#xA;  ],&#xA;  &#34;ollama_api_key&#34;: &#34;&#34;,&#xA;  &#34;ollama_api_url&#34;: &#34;http://localhost:11434&#34;,&#xA;  &#34;lm_studio_file_paths&#34;: &#34;&#34;,&#xA;  &#34;log_level&#34;: &#34;info&#34;,&#xA;  &#34;log_file_path&#34;: &#34;/Users/username/.config/gollama/gollama.log&#34;,&#xA;  &#34;sort_order&#34;: &#34;Size&#34;,&#xA;  &#34;strip_string&#34;: &#34;my-private-registry.internal/&#34;,&#xA;  &#34;editor&#34;: &#34;/Applications/Visual Studio Code.app/Contents/Resources/app/bin/code&#34;,&#xA;  &#34;docker_container&#34;: &#34;&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;strip_string&lt;/code&gt; can be used to remove a prefix from model names as they are displayed in the TUI. This can be useful if you have a common prefix such as a private registry that you want to remove for display purposes.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;editor&lt;/code&gt; specifies which editor to use for editing modelfiles when pressing &#39;e&#39;. If empty, falls back to the &lt;code&gt;EDITOR&lt;/code&gt; environment variable, then defaults to &lt;code&gt;vim&lt;/code&gt;. External editors like VS Code are supported and will show a popup interface.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;docker_container&lt;/code&gt; - &lt;strong&gt;experimental&lt;/strong&gt; - if set, gollama will attempt to perform any run operations inside the specified container.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;theme&lt;/code&gt; - &lt;strong&gt;experimental&lt;/strong&gt; The name of the theme to use (without .json extension)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation and build from source&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone the repository:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/sammcj/gollama.git&#xA;cd gollama&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Build:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;go get&#xA;make build&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;./gollama&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Themes&lt;/h3&gt; &#xA;&lt;p&gt;Gollama has basic customisable theme support, themes are stored as JSON files in &lt;code&gt;~/.config/gollama/themes/&lt;/code&gt;. The active theme can be set via the &lt;code&gt;theme&lt;/code&gt; setting in your config file (without the .json extension).&lt;/p&gt; &#xA;&lt;p&gt;Default themes will be created if they don&#39;t exist:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;default&lt;/code&gt; - Dark theme with neon accents (default)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;light-neon&lt;/code&gt; - Light theme with neon accents, suitable for light terminal backgrounds&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To create a custom theme:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create a new JSON file in the themes directory (e.g. &lt;code&gt;~/.config/gollama/themes/my-theme.json&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Use the following structure:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;name&#34;: &#34;my-theme&#34;,&#xA;  &#34;description&#34;: &#34;My custom theme&#34;,&#xA;  &#34;colours&#34;: {&#xA;    &#34;header_foreground&#34;: &#34;#AA1493&#34;,&#xA;    &#34;header_border&#34;: &#34;#BA1B11&#34;,&#xA;    &#34;selected&#34;: &#34;#FFFFFF&#34;,&#xA;    ...&#xA;  },&#xA;  &#34;family&#34;: {&#xA;    &#34;llama&#34;: &#34;#FF1493&#34;,&#xA;    &#34;alpaca&#34;: &#34;#FF00FF&#34;,&#xA;    ...&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Colours can be specified as ANSI colour codes (e.g. &#34;241&#34;) or hex values (e.g. &#34;#FF00FF&#34;). The &lt;code&gt;family&lt;/code&gt; section defines colours for different model families in the list view.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Note: Using the VSCode extension &lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=naumovs.color-highlight&#34;&gt;&#39;Color Highlight&#39;&lt;/a&gt; makes it easier to find the hex values for colours.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Logging&lt;/h2&gt; &#xA;&lt;p&gt;Logs can be found in the &lt;code&gt;gollama.log&lt;/code&gt; which is stored in &lt;code&gt;$HOME/.config/gollama/gollama.log&lt;/code&gt; by default.&lt;/p&gt; &#xA;&lt;p&gt;The log level can be set in the configuration file or overridden via command-line:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Override log level for a single command&#xA;gollama -C --log debug&#xA;&#xA;# Or use the long form&#xA;gollama --create-from-lmstudio --log-level debug&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Available log levels: &lt;code&gt;debug&lt;/code&gt;, &lt;code&gt;info&lt;/code&gt;, &lt;code&gt;warn&lt;/code&gt;, &lt;code&gt;error&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Contributions are welcome! Please fork the repository and create a pull request with your changes.&lt;/p&gt; &#xA;&lt;!-- readme: contributors -start --&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/sammcj&#34;&gt; &lt;img src=&#34;https://avatars.githubusercontent.com/u/862951?v=4&#34; width=&#34;50;&#34; alt=&#34;sammcj&#34; /&gt; &lt;br /&gt; &lt;sub&gt;&lt;b&gt;Sam&lt;/b&gt;&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/Camsbury&#34;&gt; &lt;img src=&#34;https://avatars.githubusercontent.com/u/7485022?v=4&#34; width=&#34;50;&#34; alt=&#34;Camsbury&#34; /&gt; &lt;br /&gt; &lt;sub&gt;&lt;b&gt;Cameron Kingsbury&lt;/b&gt;&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/KimCookieYa&#34;&gt; &lt;img src=&#34;https://avatars.githubusercontent.com/u/45006957?v=4&#34; width=&#34;50;&#34; alt=&#34;KimCookieYa&#34; /&gt; &lt;br /&gt; &lt;sub&gt;&lt;b&gt;KimCookieYa&lt;/b&gt;&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/DenisBalan&#34;&gt; &lt;img src=&#34;https://avatars.githubusercontent.com/u/33955091?v=4&#34; width=&#34;50;&#34; alt=&#34;DenisBalan&#34; /&gt; &lt;br /&gt; &lt;sub&gt;&lt;b&gt;Denis Balan&lt;/b&gt;&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/erg&#34;&gt; &lt;img src=&#34;https://avatars.githubusercontent.com/u/27430?v=4&#34; width=&#34;50;&#34; alt=&#34;erg&#34; /&gt; &lt;br /&gt; &lt;sub&gt;&lt;b&gt;Doug Coleman&lt;/b&gt;&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/Impact123&#34;&gt; &lt;img src=&#34;https://avatars.githubusercontent.com/u/899193?v=4&#34; width=&#34;50;&#34; alt=&#34;Impact123&#34; /&gt; &lt;br /&gt; &lt;sub&gt;&lt;b&gt;Impact&lt;/b&gt;&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/josekasna&#34;&gt; &lt;img src=&#34;https://avatars.githubusercontent.com/u/138180151?v=4&#34; width=&#34;50;&#34; alt=&#34;josekasna&#34; /&gt; &lt;br /&gt; &lt;sub&gt;&lt;b&gt;Jose Almaraz&lt;/b&gt;&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/jralmaraz&#34;&gt; &lt;img src=&#34;https://avatars.githubusercontent.com/u/13877691?v=4&#34; width=&#34;50;&#34; alt=&#34;jralmaraz&#34; /&gt; &lt;br /&gt; &lt;sub&gt;&lt;b&gt;Jose Roberto Almaraz&lt;/b&gt;&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/Br1ght0ne&#34;&gt; &lt;img src=&#34;https://avatars.githubusercontent.com/u/12615679?v=4&#34; width=&#34;50;&#34; alt=&#34;Br1ght0ne&#34; /&gt; &lt;br /&gt; &lt;sub&gt;&lt;b&gt;Oleksii Filonenko&lt;/b&gt;&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/southwolf&#34;&gt; &lt;img src=&#34;https://avatars.githubusercontent.com/u/150648?v=4&#34; width=&#34;50;&#34; alt=&#34;southwolf&#34; /&gt; &lt;br /&gt; &lt;sub&gt;&lt;b&gt;SouthWolf&lt;/b&gt;&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/agustif&#34;&gt; &lt;img src=&#34;https://avatars.githubusercontent.com/u/6601142?v=4&#34; width=&#34;50;&#34; alt=&#34;agustif&#34; /&gt; &lt;br /&gt; &lt;sub&gt;&lt;b&gt;agustif&lt;/b&gt;&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/anrgct&#34;&gt; &lt;img src=&#34;https://avatars.githubusercontent.com/u/16172523?v=4&#34; width=&#34;50;&#34; alt=&#34;anrgct&#34; /&gt; &lt;br /&gt; &lt;sub&gt;&lt;b&gt;anrgct&lt;/b&gt;&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/fuho&#34;&gt; &lt;img src=&#34;https://avatars.githubusercontent.com/u/539452?v=4&#34; width=&#34;50;&#34; alt=&#34;fuho&#34; /&gt; &lt;br /&gt; &lt;sub&gt;&lt;b&gt;ondrej&lt;/b&gt;&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA; &lt;tbody&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;!-- readme: contributors -end --&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ollama.com/&#34;&gt;Ollama&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;Llama.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://charm.sh/&#34;&gt;Charmbracelet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sammcj/spitter&#34;&gt;Spitter&lt;/a&gt; - For model copying functionality&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Thank you to folks such as Matt Williams, Fahd Mirza and AI Code King for giving this a shot and providing feedback.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=T4uiTnacyhI&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/T4uiTnacyhI/0.jpg&#34; alt=&#34;AI Code King - Easiest &amp;amp; Interactive way to Manage &amp;amp; Run Ollama Models Locally&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=OCXuYm6LKgE&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/OCXuYm6LKgE/0.jpg&#34; alt=&#34;Matt Williams - My favourite way to run Ollama: Gollama&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=24yqFrQV-4Q&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/24yqFrQV-4Q/0.jpg&#34; alt=&#34;Fahd Mirza - Gollama - Manage Ollama Models Locally&#34; /&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Copyright Â© 2024 Sam McLeod&lt;/p&gt; &#xA;&lt;p&gt;This project is licensed under the MIT License. See the &lt;a href=&#34;https://raw.githubusercontent.com/sammcj/gollama/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</summary>
  </entry>
</feed>