<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Go Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-03-08T01:29:41Z</updated>
  <subtitle>Daily Trending of Go in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>mostlygeek/llama-swap</title>
    <updated>2025-03-08T01:29:41Z</updated>
    <id>tag:github.com,2025-03-08:/mostlygeek/llama-swap</id>
    <link href="https://github.com/mostlygeek/llama-swap" rel="alternate"></link>
    <summary type="html">&lt;p&gt;transparent proxy server for llama.cpp&#39;s server to provide automatic model swapping&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mostlygeek/llama-swap/main/header.jpeg&#34; alt=&#34;llama-swap header image&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;llama-swap&lt;/h1&gt; &#xA;&lt;p&gt;llama-swap is a light weight, transparent proxy server that provides automatic model swapping to llama.cpp&#39;s server.&lt;/p&gt; &#xA;&lt;p&gt;Written in golang, it is very easy to install (single binary with no dependancies) and configure (single yaml file). To get started, download a pre-built binary or use the provided docker images.&lt;/p&gt; &#xA;&lt;h2&gt;Features:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;✅ Easy to deploy: single binary with no dependencies&lt;/li&gt; &#xA; &lt;li&gt;✅ Easy to config: single yaml file&lt;/li&gt; &#xA; &lt;li&gt;✅ On-demand model switching&lt;/li&gt; &#xA; &lt;li&gt;✅ Full control over server settings per model&lt;/li&gt; &#xA; &lt;li&gt;✅ OpenAI API supported endpoints: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;v1/completions&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;v1/chat/completions&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;v1/embeddings&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;v1/rerank&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;v1/audio/speech&lt;/code&gt; (&lt;a href=&#34;https://github.com/mostlygeek/llama-swap/issues/36&#34;&gt;#36&lt;/a&gt;)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;✅ Run multiple models at once with &lt;code&gt;profiles&lt;/code&gt; (&lt;a href=&#34;https://github.com/mostlygeek/llama-swap/issues/53#issuecomment-2660761741&#34;&gt;docs&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;✅ Remote log monitoring at &lt;code&gt;/log&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;✅ Direct access to upstream HTTP server via &lt;code&gt;/upstream/:model_id&lt;/code&gt; (&lt;a href=&#34;https://github.com/mostlygeek/llama-swap/pull/31&#34;&gt;demo&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;✅ Manually unload models via &lt;code&gt;/unload&lt;/code&gt; endpoint (&lt;a href=&#34;https://github.com/mostlygeek/llama-swap/issues/58&#34;&gt;#58&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;✅ Automatic unloading of models after timeout by setting a &lt;code&gt;ttl&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;✅ Use any local OpenAI compatible server (llama.cpp, vllm, tabbyAPI, etc)&lt;/li&gt; &#xA; &lt;li&gt;✅ Docker and Podman support&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How does llama-swap work?&lt;/h2&gt; &#xA;&lt;p&gt;When a request is made to an OpenAI compatible endpoint, lama-swap will extract the &lt;code&gt;model&lt;/code&gt; value and load the appropriate server configuration to serve it. If the wrong upstream server is running, it will be replaced with the correct one. This is where the &#34;swap&#34; part comes in. The upstream server is automatically swapped to the correct one to serve the request.&lt;/p&gt; &#xA;&lt;p&gt;In the most basic configuration llama-swap handles one model at a time. For more advanced use cases, the &lt;code&gt;profiles&lt;/code&gt; feature can load multiple models at the same time. You have complete control over how your system resources are used.&lt;/p&gt; &#xA;&lt;h2&gt;config.yaml&lt;/h2&gt; &#xA;&lt;p&gt;llama-swap&#39;s configuration is purposefully simple.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;models:&#xA;  &#34;qwen2.5&#34;:&#xA;    proxy: &#34;http://127.0.0.1:9999&#34;&#xA;    cmd: &amp;gt;&#xA;      /app/llama-server&#xA;      -hf bartowski/Qwen2.5-0.5B-Instruct-GGUF:Q4_K_M&#xA;      --port 9999&#xA;&#xA;  &#34;smollm2&#34;:&#xA;    proxy: &#34;http://127.0.0.1:9999&#34;&#xA;    cmd: &amp;gt;&#xA;      /app/llama-server&#xA;      -hf bartowski/SmolLM2-135M-Instruct-GGUF:Q4_K_M&#xA;      --port 9999&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;But also very powerful ...&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;# Seconds to wait for llama.cpp to load and be ready to serve requests&#xA;# Default (and minimum) is 15 seconds&#xA;healthCheckTimeout: 60&#xA;&#xA;# Write HTTP logs (useful for troubleshooting), defaults to false&#xA;logRequests: true&#xA;&#xA;# define valid model values and the upstream server start&#xA;models:&#xA;  &#34;llama&#34;:&#xA;    cmd: llama-server --port 8999 -m Llama-3.2-1B-Instruct-Q4_K_M.gguf&#xA;&#xA;    # where to reach the server started by cmd, make sure the ports match&#xA;    proxy: http://127.0.0.1:8999&#xA;&#xA;    # aliases names to use this model for&#xA;    aliases:&#xA;      - &#34;gpt-4o-mini&#34;&#xA;      - &#34;gpt-3.5-turbo&#34;&#xA;&#xA;    # check this path for an HTTP 200 OK before serving requests&#xA;    # default: /health to match llama.cpp&#xA;    # use &#34;none&#34; to skip endpoint checking, but may cause HTTP errors&#xA;    # until the model is ready&#xA;    checkEndpoint: /custom-endpoint&#xA;&#xA;    # automatically unload the model after this many seconds&#xA;    # ttl values must be a value greater than 0&#xA;    # default: 0 = never unload model&#xA;    ttl: 60&#xA;&#xA;  &#34;qwen&#34;:&#xA;    # environment variables to pass to the command&#xA;    env:&#xA;      - &#34;CUDA_VISIBLE_DEVICES=0&#34;&#xA;&#xA;    # multiline for readability&#xA;    cmd: &amp;gt;&#xA;      llama-server --port 8999&#xA;      --model path/to/Qwen2.5-1.5B-Instruct-Q4_K_M.gguf&#xA;    proxy: http://127.0.0.1:8999&#xA;&#xA;  # unlisted models do not show up in /v1/models or /upstream lists&#xA;  # but they can still be requested as normal&#xA;  &#34;qwen-unlisted&#34;:&#xA;    unlisted: true&#xA;    cmd: llama-server --port 9999 -m Llama-3.2-1B-Instruct-Q4_K_M.gguf -ngl 0&#xA;&#xA;  # Docker Support (v26.1.4+ required!)&#xA;  &#34;docker-llama&#34;:&#xA;    proxy: &#34;http://127.0.0.1:9790&#34;&#xA;    cmd: &amp;gt;&#xA;      docker run --name dockertest&#xA;      --init --rm -p 9790:8080 -v /mnt/nvme/models:/models&#xA;      ghcr.io/ggerganov/llama.cpp:server&#xA;      --model &#39;/models/Qwen2.5-Coder-0.5B-Instruct-Q4_K_M.gguf&#39;&#xA;&#xA;# profiles make it easy to managing multi model (and gpu) configurations.&#xA;#&#xA;# Tips:&#xA;#  - each model must be listening on a unique address and port&#xA;#  - the model name is in this format: &#34;profile_name:model&#34;, like &#34;coding:qwen&#34;&#xA;#  - the profile will load and unload all models in the profile at the same time&#xA;profiles:&#xA;  coding:&#xA;    - &#34;qwen&#34;&#xA;    - &#34;llama&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;Use Case Examples&lt;/h3&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mostlygeek/llama-swap/main/config.example.yaml&#34;&gt;config.example.yaml&lt;/a&gt; includes example for supporting &lt;code&gt;v1/embeddings&lt;/code&gt; and &lt;code&gt;v1/rerank&lt;/code&gt; endpoints&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mostlygeek/llama-swap/main/examples/speculative-decoding/README.md&#34;&gt;Speculative Decoding&lt;/a&gt; - using a small draft model can increase inference speeds from 20% to 40%. This example includes a configurations Qwen2.5-Coder-32B (2.5x increase) and Llama-3.1-70B (1.4x increase) in the best cases.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mostlygeek/llama-swap/main/examples/benchmark-snakegame/README.md&#34;&gt;Optimizing Code Generation&lt;/a&gt; - find the optimal settings for your machine. This example demonstrates defining multiple configurations and testing which one is fastest.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mostlygeek/llama-swap/main/examples/restart-on-config-change/README.md&#34;&gt;Restart on Config Change&lt;/a&gt; - automatically restart llama-swap when trying out different configurations.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h2&gt;Configuration&lt;/h2&gt; &#xA; &lt;p&gt;llama-s&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Docker Install (&lt;a href=&#34;https://github.com/mostlygeek/llama-swap/pkgs/container/llama-swap&#34;&gt;download images&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;Docker is the quickest way to try out llama-swap:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# use CPU inference&#xA;$ docker run -it --rm -p 9292:8080 ghcr.io/mostlygeek/llama-swap:cpu&#xA;&#xA;&#xA;# qwen2.5 0.5B&#xA;$ curl -s http://localhost:9292/v1/chat/completions \&#xA;    -H &#34;Content-Type: application/json&#34; \&#xA;    -H &#34;Authorization: Bearer no-key&#34; \&#xA;    -d &#39;{&#34;model&#34;:&#34;qwen2.5&#34;,&#34;messages&#34;: [{&#34;role&#34;: &#34;user&#34;,&#34;content&#34;: &#34;tell me a joke&#34;}]}&#39; | \&#xA;    jq -r &#39;.choices[0].message.content&#39;&#xA;&#xA;&#xA;# SmolLM2 135M&#xA;$ curl -s http://localhost:9292/v1/chat/completions \&#xA;    -H &#34;Content-Type: application/json&#34; \&#xA;    -H &#34;Authorization: Bearer no-key&#34; \&#xA;    -d &#39;{&#34;model&#34;:&#34;smollm2&#34;,&#34;messages&#34;: [{&#34;role&#34;: &#34;user&#34;,&#34;content&#34;: &#34;tell me a joke&#34;}]}&#39; | \&#xA;    jq -r &#39;.choices[0].message.content&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Docker images are nightly ...&lt;/summary&gt; &#xA; &lt;p&gt;They include:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;ghcr.io/mostlygeek/llama-swap:cpu&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;ghcr.io/mostlygeek/llama-swap:cuda&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;ghcr.io/mostlygeek/llama-swap:intel&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;ghcr.io/mostlygeek/llama-swap:vulkan&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;ROCm disabled until fixed in llama.cpp container&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;Specific versions are also available and are tagged with the llama-swap, architecture and llama.cpp versions. For example: &lt;code&gt;ghcr.io/mostlygeek/llama-swap:v89-cuda-b4716&lt;/code&gt;&lt;/p&gt; &#xA; &lt;p&gt;Beyond the demo you will likely want to run the containers with your downloaded models and custom configuration.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;$ docker run -it --rm --runtime nvidia -p 9292:8080 \&#xA;  -v /path/to/models:/models \&#xA;  -v /path/to/custom/config.yaml:/app/config.yaml \&#xA;  ghcr.io/mostlygeek/llama-swap:cuda&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Bare metal Install (&lt;a href=&#34;https://github.com/mostlygeek/llama-swap/releases&#34;&gt;download&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;Pre-built binaries are available for Linux, FreeBSD and Darwin (OSX). These are automatically published and are likely a few hours ahead of the docker releases. The baremetal install works with any OpenAI compatible server, not just llama-server.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create a configuration file, see &lt;a href=&#34;https://raw.githubusercontent.com/mostlygeek/llama-swap/main/config.example.yaml&#34;&gt;config.example.yaml&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Download a &lt;a href=&#34;https://github.com/mostlygeek/llama-swap/releases&#34;&gt;release&lt;/a&gt; appropriate for your OS and architecture.&lt;/li&gt; &#xA; &lt;li&gt;Run the binary with &lt;code&gt;llama-swap --config path/to/config.yaml&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Building from source&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install golang for your system&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;git clone git@github.com:mostlygeek/llama-swap.git&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;make clean all&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Binaries will be in &lt;code&gt;build/&lt;/code&gt; subdirectory&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Monitoring Logs&lt;/h2&gt; &#xA;&lt;p&gt;Open the &lt;code&gt;http://&amp;lt;host&amp;gt;/logs&lt;/code&gt; with your browser to get a web interface with streaming logs.&lt;/p&gt; &#xA;&lt;p&gt;Of course, CLI access is also supported:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# sends up to the last 10KB of logs&#xA;curl http://host/logs&#39;&#xA;&#xA;# streams logs&#xA;curl -Ns &#39;http://host/logs/stream&#39;&#xA;&#xA;# stream and filter logs with linux pipes&#xA;curl -Ns http://host/logs/stream | grep &#39;eval time&#39;&#xA;&#xA;# skips history and just streams new log entries&#xA;curl -Ns &#39;http://host/logs/stream?no-history&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Do I need to use llama.cpp&#39;s server (llama-server)?&lt;/h2&gt; &#xA;&lt;p&gt;Any OpenAI compatible server would work. llama-swap was originally designed for llama-server and it is the best supported.&lt;/p&gt; &#xA;&lt;p&gt;For Python based inference servers like vllm or tabbyAPI it is recommended to run them via podman or docker. This provides clean environment isolation as well as responding correctly to &lt;code&gt;SIGTERM&lt;/code&gt; signals to shutdown.&lt;/p&gt; &#xA;&lt;h2&gt;Systemd Unit Files&lt;/h2&gt; &#xA;&lt;p&gt;Use this unit file to start llama-swap on boot. This is only tested on Ubuntu.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;/etc/systemd/system/llama-swap.service&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;[Unit]&#xA;Description=llama-swap&#xA;After=network.target&#xA;&#xA;[Service]&#xA;User=nobody&#xA;&#xA;# set this to match your environment&#xA;ExecStart=/path/to/llama-swap --config /path/to/llama-swap.config.yml&#xA;&#xA;Restart=on-failure&#xA;RestartSec=3&#xA;StartLimitBurst=3&#xA;StartLimitInterval=30&#xA;&#xA;[Install]&#xA;WantedBy=multi-user.target&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;picture&gt; &#xA; &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://api.star-history.com/svg?repos=mostlygeek/llama-swap&amp;amp;type=Date&amp;amp;theme=dark&#34;&gt; &#xA; &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;https://api.star-history.com/svg?repos=mostlygeek/llama-swap&amp;amp;type=Date&#34;&gt; &#xA; &lt;img alt=&#34;Star History Chart&#34; src=&#34;https://api.star-history.com/svg?repos=mostlygeek/llama-swap&amp;amp;type=Date&#34;&gt; &#xA;&lt;/picture&gt;</summary>
  </entry>
</feed>