<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Go Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-07-11T01:31:55Z</updated>
  <subtitle>Daily Trending of Go in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>beam-cloud/beta9</title>
    <updated>2025-07-11T01:31:55Z</updated>
    <id>tag:github.com,2025-07-11:/beam-cloud/beta9</id>
    <link href="https://github.com/beam-cloud/beta9" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Secure, high-performance AI infrastructure in Python.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img alt=&#34;Logo&#34; src=&#34;https://raw.githubusercontent.com/beam-cloud/beta9/main/static/beam-logo-white.png#gh-dark-mode-only&#34; width=&#34;30%&#34;&gt; &lt;img alt=&#34;Logo&#34; src=&#34;https://raw.githubusercontent.com/beam-cloud/beta9/main/static/beam-logo-dark.png#gh-light-mode-only&#34; width=&#34;30%&#34;&gt; &lt;/p&gt; &#xA; &lt;h2&gt;Run AI Workloads at Scale&lt;/h2&gt; &#xA; &lt;p align=&#34;center&#34;&gt;  &lt;a href=&#34;https://colab.research.google.com/drive/1jSDyYY7FY3Y3jJlCzkmHlH8vTyF-TEmB?usp=sharing&#34;&gt; &lt;img alt=&#34;Colab&#34; src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/beam-cloud/beta9/stargazers&#34;&gt; &lt;img alt=&#34;‚≠ê Star the Repo&#34; src=&#34;https://img.shields.io/github/stars/beam-cloud/beta9&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://docs.beam.cloud&#34;&gt; &lt;img alt=&#34;Documentation&#34; src=&#34;https://img.shields.io/badge/docs-quickstart-purple&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://join.slack.com/t/beam-cloud/shared_invite/zt-2uiks0hc6-UbBD97oZjz8_YnjQ2P7BEQ&#34;&gt; &lt;img alt=&#34;Join Slack&#34; src=&#34;https://img.shields.io/badge/Beam-Join%20Slack-orange?logo=slack&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://twitter.com/beam_cloud&#34;&gt; &lt;img alt=&#34;Twitter&#34; src=&#34;https://img.shields.io/twitter/follow/beam_cloud.svg?style=social&amp;amp;logo=twitter&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/beam-cloud/beta9?tab=AGPL-3.0-1-ov-file&#34;&gt; &lt;img alt=&#34;AGPL&#34; src=&#34;https://img.shields.io/badge/License-AGPL-green&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://beam.cloud?utm_source=github_readme&#34;&gt;Beam&lt;/a&gt;&lt;/strong&gt; is a fast, open-source runtime for serverless AI workloads. It gives you a Pythonic interface to deploy and scale AI applications with zero infrastructure overhead.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/beam-cloud/beta9/main/static/readme.gif&#34; alt=&#34;Watch the demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;‚ú® Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fast Image Builds&lt;/strong&gt;: Launch containers in under a second using a custom container runtime&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Parallelization and Concurrency&lt;/strong&gt;: Fan out workloads to 100s of containers&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;First-Class Developer Experience&lt;/strong&gt;: Hot-reloading, webhooks, and scheduled jobs&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Scale-to-Zero&lt;/strong&gt;: Workloads are serverless by default&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Volume Storage&lt;/strong&gt;: Mount distributed storage volumes&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;GPU Support&lt;/strong&gt;: Run on our cloud (4090s, H100s, and more) or bring your own GPUs&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üì¶ Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install beam-client&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;‚ö°Ô∏è Quickstart&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create an account &lt;a href=&#34;https://beam.cloud?utm_source=github_readme&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Follow our &lt;a href=&#34;https://platform.beam.cloud/onboarding?utm_source=github_readme&#34;&gt;Getting Started Guide&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Creating a sandbox&lt;/h2&gt; &#xA;&lt;p&gt;Spin up isolated containers to run LLM-generated code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from beam import Image, Sandbox&#xA;&#xA;&#xA;sandbox = Sandbox(image=Image()).create()&#xA;response = sandbox.process.run_code(&#34;print(&#39;I am running remotely&#39;)&#34;)&#xA;&#xA;print(response.result)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Deploy a serverless inference endpoint&lt;/h2&gt; &#xA;&lt;p&gt;Create an autoscaling endpoint for your custom model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from beam import Image, endpoint&#xA;from beam import QueueDepthAutoscaler&#xA;&#xA;@endpoint(&#xA;    image=Image(python_version=&#34;python3.11&#34;),&#xA;    gpu=&#34;A10G&#34;,&#xA;    cpu=2,&#xA;    memory=&#34;16Gi&#34;,&#xA;    autoscaler=QueueDepthAutoscaler(max_containers=5, tasks_per_container=30)&#xA;)&#xA;def handler():&#xA;    return {&#34;label&#34;: &#34;cat&#34;, &#34;confidence&#34;: 0.97}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Run background tasks&lt;/h2&gt; &#xA;&lt;p&gt;Schedule resilient background tasks (or replace your Celery queue) by adding a simple decorator:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from beam import Image, TaskPolicy, schema, task_queue&#xA;&#xA;&#xA;class Input(schema.Schema):&#xA;    image_url = schema.String()&#xA;&#xA;&#xA;@task_queue(&#xA;    name=&#34;image-processor&#34;,&#xA;    image=Image(python_version=&#34;python3.11&#34;),&#xA;    cpu=1,&#xA;    memory=1024,&#xA;    inputs=Input,&#xA;    task_policy=TaskPolicy(max_retries=3),&#xA;)&#xA;def my_background_task(input: Input, *, context):&#xA;    image_url = input.image_url&#xA;    print(f&#34;Processing image: {image_url}&#34;)&#xA;    return {&#34;image_url&#34;: image_url}&#xA;&#xA;&#xA;if __name__ == &#34;__main__&#34;:&#xA;    # Invoke a background task from your app (without deploying it)&#xA;    my_background_task.put(image_url=&#34;https://example.com/image.jpg&#34;)&#xA;&#xA;    # You can also deploy this behind a versioned endpoint with:&#xA;    # beam deploy app.py:my_background_task --name image-processor&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h2&gt;Self-Hosting vs Cloud&lt;/h2&gt; &#xA; &lt;p&gt;Beta9 is the open-source engine powering &lt;a href=&#34;https://beam.cloud&#34;&gt;Beam&lt;/a&gt;, our fully-managed cloud platform. You can self-host Beta9 for free or choose managed cloud hosting through Beam.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;üëã Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions big or small. These are the most helpful things for us:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Submit a &lt;a href=&#34;https://github.com/beam-cloud/beta9/issues/new?assignees=&amp;amp;labels=&amp;amp;projects=&amp;amp;template=feature-request.md&amp;amp;title=&#34;&gt;feature request&lt;/a&gt; or &lt;a href=&#34;https://github.com/beam-cloud/beta9/issues/new?assignees=&amp;amp;labels=&amp;amp;projects=&amp;amp;template=bug-report.md&amp;amp;title=&#34;&gt;bug report&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Open a PR with a new feature or improvement&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;‚ù§Ô∏è Thanks to Our Contributors&lt;/h2&gt; &#xA;&lt;a href=&#34;https://github.com/beam-cloud/beta9/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=beam-cloud/beta9&#34;&gt; &lt;/a&gt;</summary>
  </entry>
</feed>