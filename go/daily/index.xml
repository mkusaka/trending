<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Go Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-07-10T01:31:55Z</updated>
  <subtitle>Daily Trending of Go in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>vllm-project/aibrix</title>
    <updated>2025-07-10T01:31:55Z</updated>
    <id>tag:github.com,2025-07-10:/vllm-project/aibrix</id>
    <link href="https://github.com/vllm-project/aibrix" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Cost-efficient and pluggable Infrastructure components for GenAI inference&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AIBrix&lt;/h1&gt; &#xA;&lt;p&gt;Welcome to AIBrix, an open-source initiative designed to provide essential building blocks to construct scalable GenAI inference infrastructure. AIBrix delivers a cloud-native solution optimized for deploying, managing, and scaling large language model (LLM) inference, tailored specifically to enterprise needs.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; | &lt;a href=&#34;https://aibrix.readthedocs.io/latest/&#34;&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://aibrix.github.io/&#34;&gt;&lt;b&gt;Blog&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2504.03648&#34;&gt;&lt;b&gt;White Paper&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://x.com/vllm_project&#34;&gt;&lt;b&gt;Twitter/X&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://vllm-dev.slack.com/archives/C08EQ883CSV&#34;&gt;&lt;b&gt;Developer Slack&lt;/b&gt;&lt;/a&gt; | &lt;/p&gt; &#xA;&lt;h2&gt;Latest News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2025-06-10]&lt;/strong&gt; The AIBrix team delivered a talk at KubeCon China 2025 titled &lt;a href=&#34;https://kccncchn2025.sched.com/event/1x5im/introducing-aibrix-cost-effective-and-scalable-kubernetes-control-plane-for-vllm-jiaxin-shan-liguang-xie-bytedance&#34;&gt;AIBrix: Cost-Effective and Scalable Kubernetes Control Plane for vLLM&lt;/a&gt;, discussing how the framework optimizes vLLM deployment via Kubernetes for cost efficiency and scalability.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2025-05-21]&lt;/strong&gt; AIBrix v0.3.0 is released. Check out the &lt;a href=&#34;https://github.com/vllm-project/aibrix/releases/tag/v0.3.0&#34;&gt;release notes&lt;/a&gt; and &lt;a href=&#34;https://aibrix.github.io/posts/2025-05-21-v0.3.0-release/&#34;&gt;Blog Post&lt;/a&gt; for more details&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2025-04-04]&lt;/strong&gt; AIBrix co-delivered a KubeCon EU 2025 keynote with Google on &lt;a href=&#34;https://kccnceu2025.sched.com/event/1txC7/keynote-llm-aware-load-balancing-in-kubernetes-a-new-era-of-efficiency-clayton-coleman-distinguished-engineer-google-jiaxin-shan-software-engineer-bytedance&#34;&gt;LLM-Aware Load Balancing in Kubernetes: A New Era of Efficiency&lt;/a&gt;, focusing on LLM specific routing solutions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2025-03-30]&lt;/strong&gt; AIBrix was featured at the &lt;a href=&#34;http://asplos-conference.org/asplos2025/&#34;&gt;ASPLOS&#39;25&lt;/a&gt; workshop with the presentation &lt;a href=&#34;https://docs.google.com/presentation/d/1YDVsPFTIgGXnROGaJ1VKuDDAB4T5fzpE/edit&#34;&gt;AIBrix: An Open-Source, Large-Scale LLM Inference Infrastructure for System Research&lt;/a&gt;, showcasing its architecture for efficient LLM inference in system research scenarios.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2025-03-09]&lt;/strong&gt; AIBrix v0.2.1 is released. DeepSeek-R1 full weights deployment is supported and gateway stability has been improved! Check &lt;a href=&#34;https://aibrix.github.io/posts/2025-03-10-deepseek-r1/&#34;&gt;Blog Post&lt;/a&gt; for more details.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2025-02-19]&lt;/strong&gt; AIBrix v0.2.0 is released. Check out the &lt;a href=&#34;https://github.com/vllm-project/aibrix/releases/tag/v0.2.0&#34;&gt;release notes&lt;/a&gt; and &lt;a href=&#34;https://aibrix.github.io/posts/2025-02-05-v0.2.0-release/&#34;&gt;Blog Post&lt;/a&gt; for more details.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2025-11-13]&lt;/strong&gt; AIBrix v0.1.0 is released. Check out the &lt;a href=&#34;https://github.com/vllm-project/aibrix/releases/tag/v0.1.0&#34;&gt;release notes&lt;/a&gt; and &lt;a href=&#34;https://aibrix.github.io/posts/2024-11-12-v0.1.0-release/&#34;&gt;Blog Post&lt;/a&gt; for more details.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Key Features&lt;/h2&gt; &#xA;&lt;p&gt;The initial release includes the following key features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;High-Density LoRA Management&lt;/strong&gt;: Streamlined support for lightweight, low-rank adaptations of models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLM Gateway and Routing&lt;/strong&gt;: Efficiently manage and direct traffic across multiple models and replicas.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLM App-Tailored Autoscaler&lt;/strong&gt;: Dynamically scale inference resources based on real-time demand.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Unified AI Runtime&lt;/strong&gt;: A versatile sidecar enabling metric standardization, model downloading, and management.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Distributed Inference&lt;/strong&gt;: Scalable architecture to handle large workloads across multiple nodes.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Distributed KV Cache&lt;/strong&gt;: Enables high-capacity, cross-engine KV reuse.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Cost-efficient Heterogeneous Serving&lt;/strong&gt;: Enables mixed GPU inference to reduce costs with SLO guarantees.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;GPU Hardware Failure Detection&lt;/strong&gt;: Proactive detection of GPU hardware issues.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Architecture&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/vllm-project/aibrix/main/docs/source/assets/images/aibrix-architecture-v1.jpeg&#34; alt=&#34;aibrix-architecture-v1&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;To get started with AIBrix, clone this repository and follow the setup instructions in the documentation. Our comprehensive guide will help you configure and deploy your first LLM infrastructure seamlessly.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Local Testing&#xA;git clone https://github.com/vllm-project/aibrix.git&#xA;cd aibrix&#xA;&#xA;# Install nightly aibrix dependencies&#xA;kubectl apply -k config/dependency --server-side&#xA;&#xA;# Install nightly aibrix components&#xA;kubectl apply -k config/default&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install stable distribution&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Install component dependencies&#xA;kubectl apply -f &#34;https://github.com/vllm-project/aibrix/releases/download/v0.3.0/aibrix-dependency-v0.3.0.yaml&#34; --server-side&#xA;&#xA;# Install aibrix components&#xA;kubectl apply -f &#34;https://github.com/vllm-project/aibrix/releases/download/v0.3.0/aibrix-core-v0.3.0.yaml&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;For detailed documentation on installation, configuration, and usage, please visit our &lt;a href=&#34;https://aibrix.readthedocs.io/latest/&#34;&gt;documentation page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions from the community! Check out our &lt;a href=&#34;https://raw.githubusercontent.com/vllm-project/aibrix/main/CONTRIBUTING.md&#34;&gt;contributing guidelines&lt;/a&gt; to see how you can make a difference.&lt;/p&gt; &#xA;&lt;p&gt;Slack Channel: &lt;a href=&#34;https://vllm-dev.slack.com/archives/C08EQ883CSV&#34;&gt;#aibrix&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;AIBrix is licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/vllm-project/aibrix/main/LICENSE&#34;&gt;Apache 2.0 License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions or encounter any issues, please submit an issue on our &lt;a href=&#34;https://github.com/vllm-project/aibrix/issues&#34;&gt;GitHub issues page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Thank you for choosing AIBrix for your GenAI infrastructure needs!&lt;/p&gt;</summary>
  </entry>
</feed>