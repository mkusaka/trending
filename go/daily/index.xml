<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Go Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-06T01:28:45Z</updated>
  <subtitle>Daily Trending of Go in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Netflix/chaosmonkey</title>
    <updated>2024-04-06T01:28:45Z</updated>
    <id>tag:github.com,2024-04-06:/Netflix/chaosmonkey</id>
    <link href="https://github.com/Netflix/chaosmonkey" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Chaos Monkey is a resiliency tool that helps applications tolerate random instance failures.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Netflix/chaosmonkey/master/docs/logo.png&#34; alt=&#34;logo&#34; title=&#34;logo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Netflix/chaosmonkey/master/OSSMETADATA&#34;&gt;&lt;img src=&#34;https://img.shields.io/osslifecycle/Netflix/chaosmonkey.svg?sanitize=true&#34; alt=&#34;NetflixOSS Lifecycle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://travis-ci.com/Netflix/chaosmonkey&#34;&gt;&lt;img src=&#34;https://travis-ci.com/Netflix/chaosmonkey.svg?branch=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://godoc.org/github.com/Netflix/chaosmonkey&#34;&gt;&lt;img src=&#34;https://godoc.org/github.com/Netflix/chaosmonkey?status.svg?sanitize=true&#34; alt=&#34;GoDoc&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://goreportcard.com/report/github.com/Netflix/chaosmonkey&#34;&gt;&lt;img src=&#34;https://goreportcard.com/badge/github.com/Netflix/chaosmonkey&#34; alt=&#34;GoReportCard&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Chaos Monkey randomly terminates virtual machine instances and containers that run inside of your production environment. Exposing engineers to failures more frequently incentivizes them to build resilient services.&lt;/p&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://netflix.github.io/chaosmonkey&#34;&gt;documentation&lt;/a&gt; for info on how to use Chaos Monkey.&lt;/p&gt; &#xA;&lt;p&gt;Chaos Monkey is an example of a tool that follows the &lt;a href=&#34;http://principlesofchaos.org/&#34;&gt;Principles of Chaos Engineering&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;p&gt;This version of Chaos Monkey is fully integrated with &lt;a href=&#34;http://www.spinnaker.io/&#34;&gt;Spinnaker&lt;/a&gt;, the continuous delivery platform that we use at Netflix. You must be managing your apps with Spinnaker to use Chaos Monkey to terminate instances.&lt;/p&gt; &#xA;&lt;p&gt;Chaos Monkey should work with any backend that Spinnaker supports (AWS, Google Compute Engine, Azure, Kubernetes, Cloud Foundry). It has been tested with AWS, &lt;a href=&#34;https://medium.com/continuous-delivery-scale/running-chaos-monkey-on-spinnaker-google-compute-engine-gce-155dc52f20ef&#34;&gt;GCE&lt;/a&gt;, and Kubernetes.&lt;/p&gt; &#xA;&lt;h3&gt;Install locally&lt;/h3&gt; &#xA;&lt;p&gt;To install the Chaos Monkey binary on your local machine:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;go get github.com/netflix/chaosmonkey/cmd/chaosmonkey&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to deploy&lt;/h3&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://netflix.github.io/chaosmonkey&#34;&gt;docs&lt;/a&gt; for instructions on how to configure and deploy Chaos Monkey.&lt;/p&gt; &#xA;&lt;h3&gt;Support&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://groups.google.com/group/simianarmy-users&#34;&gt;Simian Army Google group&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>kube-vip/kube-vip</title>
    <updated>2024-04-06T01:28:45Z</updated>
    <id>tag:github.com,2024-04-06:/kube-vip/kube-vip</id>
    <link href="https://github.com/kube-vip/kube-vip" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Kubernetes Control Plane Virtual IP and Load-Balancer&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;kube-vip&lt;/h1&gt; &#xA;&lt;p&gt;High Availability and Load-Balancing&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/kube-vip/kube-vip/raw/main/kube-vip.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/kube-vip/kube-vip/actions/workflows/main.yaml&#34;&gt;&lt;img src=&#34;https://github.com/kube-vip/kube-vip/actions/workflows/main.yaml/badge.svg?sanitize=true&#34; alt=&#34;Build and publish main image regularly&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;Kubernetes Virtual IP and Load-Balancer for both control plane and Kubernetes services&lt;/p&gt; &#xA;&lt;p&gt;The idea behind &lt;code&gt;kube-vip&lt;/code&gt; is a small self-contained Highly-Available option for all environments, especially:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bare-Metal&lt;/li&gt; &#xA; &lt;li&gt;Edge (arm / Raspberry PI)&lt;/li&gt; &#xA; &lt;li&gt;Virtualisation&lt;/li&gt; &#xA; &lt;li&gt;Pretty much anywhere else :)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; All documentation of both usage and architecture are now available at &lt;a href=&#34;https://kube-vip.io&#34;&gt;https://kube-vip.io&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;Kube-Vip was originally created to provide a HA solution for the Kubernetes control plane, over time it has evolved to incorporate that same functionality into Kubernetes service type &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer&#34;&gt;load-balancers&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;VIP addresses can be both IPv4 or IPv6&lt;/li&gt; &#xA; &lt;li&gt;Control Plane with ARP (Layer 2) or BGP (Layer 3)&lt;/li&gt; &#xA; &lt;li&gt;Control Plane using either &lt;a href=&#34;https://godoc.org/k8s.io/client-go/tools/leaderelection&#34;&gt;leader election&lt;/a&gt; or &lt;a href=&#34;https://en.wikipedia.org/wiki/Raft_(computer_science)&#34;&gt;raft&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Control Plane HA with kubeadm (static Pods)&lt;/li&gt; &#xA; &lt;li&gt;Control Plane HA with K3s/and others (daemonsets)&lt;/li&gt; &#xA; &lt;li&gt;Service LoadBalancer using &lt;a href=&#34;https://godoc.org/k8s.io/client-go/tools/leaderelection&#34;&gt;leader election&lt;/a&gt; for ARP (Layer 2)&lt;/li&gt; &#xA; &lt;li&gt;Service LoadBalancer using multiple nodes with BGP&lt;/li&gt; &#xA; &lt;li&gt;Service LoadBalancer address pools per namespace or global&lt;/li&gt; &#xA; &lt;li&gt;Service LoadBalancer address via (existing network DHCP)&lt;/li&gt; &#xA; &lt;li&gt;Service LoadBalancer address exposure to gateway via UPNP&lt;/li&gt; &#xA; &lt;li&gt;... manifest generation, vendor API integrations and many more...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Why?&lt;/h2&gt; &#xA;&lt;p&gt;The purpose of &lt;code&gt;kube-vip&lt;/code&gt; is to simplify the building of HA Kubernetes clusters, which at this time can involve a few components and configurations that all need to be managed. This was blogged about in detail by &lt;a href=&#34;https://twitter.com/thebsdbox/&#34;&gt;thebsdbox&lt;/a&gt; here -&amp;gt; &lt;a href=&#34;https://thebsdbox.co.uk/2020/01/02/Designing-Building-HA-bare-metal-Kubernetes-cluster/#Networking-load-balancing&#34;&gt;https://thebsdbox.co.uk/2020/01/02/Designing-Building-HA-bare-metal-Kubernetes-cluster/#Networking-load-balancing&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Alternative HA Options&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;kube-vip&lt;/code&gt; provides both a floating or virtual IP address for your kubernetes cluster as well as load-balancing the incoming traffic to various control-plane replicas. At the current time to replicate this functionality a minimum of two pieces of tooling would be required:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;VIP&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.keepalived.org/&#34;&gt;Keepalived&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ucarp.wordpress.com/&#34;&gt;UCARP&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Hardware Load-balancer (functionality differs per vendor)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;LoadBalancing&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.haproxy.org/&#34;&gt;HAProxy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nginx.com&#34;&gt;Nginx&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Hardware Load-balancer (functionality differs per vendor)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;All of these would require a separate level of configuration and in some infrastructures multiple teams in order to implement. Also when considering the software components, they may require packaging into containers or if they’re pre-packaged then security and transparency may be an issue. Finally, in edge environments we may have limited room for hardware (no HW load-balancer) or packages solutions in the correct architectures might not exist (e.g. ARM). Luckily with &lt;code&gt;kube-vip&lt;/code&gt; being written in GO, it’s small(ish) and easy to build for multiple architectures, with the added security benefit of being the only thing needed in the container.&lt;/p&gt; &#xA;&lt;h2&gt;Troubleshooting and Feedback&lt;/h2&gt; &#xA;&lt;p&gt;Please raise issues on the GitHub repository and as mentioned check the documentation at &lt;a href=&#34;https://kube-vip.io/&#34;&gt;https://kube-vip.io&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Thanks for taking the time to join our community and start contributing! We welcome pull requests. Feel free to dig through the &lt;a href=&#34;https://github.com/kube-vip/kube-vip/issues&#34;&gt;issues&lt;/a&gt; and jump in.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;⚠&lt;/span&gt; This project has issue compiling on MacOS, please compile it on linux distribution&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#kube-vip/kube-vip&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=kube-vip/kube-vip&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>rancher/fleet</title>
    <updated>2024-04-06T01:28:45Z</updated>
    <id>tag:github.com,2024-04-06:/rancher/fleet</id>
    <link href="https://github.com/rancher/fleet" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Deploy workloads from Git to large fleets of Kubernetes clusters&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Introduction&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://app.fossa.com/projects/git%2Bgithub.com%2Francher%2Ffleet?ref=badge_shield&#34;&gt;&lt;img src=&#34;https://app.fossa.com/api/projects/git%2Bgithub.com%2Francher%2Ffleet.svg?type=shield&#34; alt=&#34;FOSSA Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/rancher/fleet/actions/workflows/ci.yml&#34;&gt;&lt;img src=&#34;https://github.com/rancher/fleet/actions/workflows/ci.yml/badge.svg?sanitize=true&#34; alt=&#34;Unit&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rancher/fleet/actions/workflows/e2e-ci.yml&#34;&gt;&lt;img src=&#34;https://github.com/rancher/fleet/actions/workflows/e2e-ci.yml/badge.svg?event=schedule&#34; alt=&#34;E2E Examples&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rancher/fleet/actions/workflows/e2e-multicluster-ci.yml&#34;&gt;&lt;img src=&#34;https://github.com/rancher/fleet/actions/workflows/e2e-multicluster-ci.yml/badge.svg?event=schedule&#34; alt=&#34;E2E Multi-Cluster Examples&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rancher/fleet/actions/workflows/golangci-lint.yml&#34;&gt;&lt;img src=&#34;https://github.com/rancher/fleet/actions/workflows/golangci-lint.yml/badge.svg?event=schedule&#34; alt=&#34;golangci-lint&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rancher/fleet/main/docs/arch.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Fleet is GitOps at scale. Fleet is designed to manage multiple clusters. It&#39;s also lightweight enough that it works great for a single cluster too, but it really shines when you get to a large scale. By large scale we mean either a lot of clusters, a lot of deployments, or a lot of teams in a single organization.&lt;/p&gt; &#xA;&lt;p&gt;Fleet can manage deployments from git of raw Kubernetes YAML, Helm charts, or Kustomize or any combination of the three. Regardless of the source all resources are dynamically turned into Helm charts and Helm is used as the engine to deploy everything in the cluster. This gives a high degree of control, consistency, and auditability. Fleet focuses not only on the ability to scale, but to give one a high degree of control and visibility to exactly what is installed on the cluster.&lt;/p&gt; &#xA;&lt;h1&gt;Quick Start&lt;/h1&gt; &#xA;&lt;p&gt;For more information, have a look at Fleet&#39;s &lt;a href=&#34;https://fleet.rancher.io/&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;Get &lt;code&gt;helm&lt;/code&gt; if you don&#39;t have it. Helm 3 is just a CLI and won&#39;t do bad insecure things to your cluster.&lt;/p&gt; &#xA;&lt;p&gt;For instance, using Homebrew:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;brew install helm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install the Fleet Helm charts (there&#39;s two because we separate out CRDs for ultimate flexibility.)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;helm -n cattle-fleet-system install --create-namespace --wait \&#xA;    fleet-crd https://github.com/rancher/fleet/releases/download/v0.9.2/fleet-crd-0.9.2.tgz&#xA;helm -n cattle-fleet-system install --create-namespace --wait \&#xA;    fleet https://github.com/rancher/fleet/releases/download/v0.9.2/fleet-0.9.2.tgz&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Add a Git Repo to watch&lt;/h2&gt; &#xA;&lt;p&gt;Change &lt;code&gt;spec.repo&lt;/code&gt; to your git repo of choice. Kubernetes manifest files that should be deployed should be in &lt;code&gt;/manifests&lt;/code&gt; in your repo.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cat &amp;gt; example.yaml &amp;lt;&amp;lt; &#34;EOF&#34;&#xA;apiVersion: fleet.cattle.io/v1alpha1&#xA;kind: GitRepo&#xA;metadata:&#xA;  name: sample&#xA;  # This namespace is special and auto-wired to deploy to the local cluster&#xA;  namespace: fleet-local&#xA;spec:&#xA;  # Everything from this repo will be run in this cluster. You trust me right?&#xA;  repo: &#34;https://github.com/rancher/fleet-examples&#34;&#xA;  paths:&#xA;  - simple&#xA;EOF&#xA;&#xA;kubectl apply -f example.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Get Status&lt;/h2&gt; &#xA;&lt;p&gt;Get status of what Fleet is doing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;kubectl -n fleet-local get fleet&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You should see something like this get created in your cluster.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;kubectl get deploy frontend&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;NAME       READY   UP-TO-DATE   AVAILABLE   AGE&#xA;frontend   3/3     3            3           116m&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Enjoy and read the &lt;a href=&#34;https://fleet.rancher.io/&#34;&gt;docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://app.fossa.com/projects/git%2Bgithub.com%2Francher%2Ffleet?ref=badge_large&#34;&gt;&lt;img src=&#34;https://app.fossa.com/api/projects/git%2Bgithub.com%2Francher%2Ffleet.svg?type=large&#34; alt=&#34;FOSSA Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;For developer and maintainer documentation, see &lt;a href=&#34;https://raw.githubusercontent.com/rancher/fleet/main/DEVELOPING.md&#34;&gt;DEVELOPING.md&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>