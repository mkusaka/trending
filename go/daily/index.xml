<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Go Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-05-30T01:32:00Z</updated>
  <subtitle>Daily Trending of Go in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>NVIDIA/KAI-Scheduler</title>
    <updated>2025-05-30T01:32:00Z</updated>
    <id>tag:github.com,2025-05-30:/NVIDIA/KAI-Scheduler</id>
    <link href="https://github.com/NVIDIA/KAI-Scheduler" rel="alternate"></link>
    <summary type="html">&lt;p&gt;KAI Scheduler is an open source Kubernetes Native scheduler for AI workloads at large scale&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/KAI-Scheduler/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache_2.0-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/NVIDIA/KAI-Scheduler/raw/main/.github/workflows/update-coverage-badge.yaml&#34;&gt;&lt;img src=&#34;https://github.com/NVIDIA/KAI-Scheduler/raw/coverage-badge/badges/coverage.svg?sanitize=true&#34; alt=&#34;Coverage&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;KAI Scheduler&lt;/h1&gt; &#xA;&lt;p&gt;KAI Scheduler is a robust, efficient, and scalable &lt;a href=&#34;https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/&#34;&gt;Kubernetes scheduler&lt;/a&gt; that optimizes GPU resource allocation for AI and machine learning workloads.&lt;/p&gt; &#xA;&lt;p&gt;Designed to manage large-scale GPU clusters, including thousands of nodes, and high-throughput of workloads, makes the KAI Scheduler ideal for extensive and demanding environments. KAI Scheduler allows administrators of Kubernetes clusters to dynamically allocate GPU resources to workloads.&lt;/p&gt; &#xA;&lt;p&gt;KAI Scheduler supports the entire AI lifecycle, from small, interactive jobs that require minimal resources to large training and inference, all within the same cluster. It ensures optimal resource allocation while maintaining resource fairness between the different consumers. It can run alongside other schedulers installed on the cluster.&lt;/p&gt; &#xA;&lt;h2&gt;Key Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/KAI-Scheduler/main/docs/batch/README.md&#34;&gt;Batch Scheduling&lt;/a&gt;: Ensure all pods in a group are scheduled simultaneously or not at all.&lt;/li&gt; &#xA; &lt;li&gt;Bin Packing &amp;amp; Spread Scheduling: Optimize node usage either by minimizing fragmentation (bin-packing) or increasing resiliency and load balancing (spread scheduling).&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/KAI-Scheduler/main/docs/priority/README.md&#34;&gt;Workload Priority&lt;/a&gt;: Prioritize workloads effectively within queues.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/KAI-Scheduler/main/docs/queues/README.md&#34;&gt;Hierarchical Queues&lt;/a&gt;: Manage workloads with two-level queue hierarchies for flexible organizational control.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/KAI-Scheduler/main/docs/fairness/README.md#resource-division-algorithm&#34;&gt;Resource distribution&lt;/a&gt;: Customize quotas, over-quota weights, limits, and priorities per queue.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/KAI-Scheduler/main/docs/fairness/README.md#reclaim-strategies&#34;&gt;Fairness Policies&lt;/a&gt;: Ensure equitable resource distribution using Dominant Resource Fairness (DRF) and resource reclamation across queues.&lt;/li&gt; &#xA; &lt;li&gt;Workload Consolidation: Reallocate running workloads intelligently to reduce fragmentation and increase cluster utilization.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/KAI-Scheduler/main/docs/elastic/README.md&#34;&gt;Elastic Workloads&lt;/a&gt;: Dynamically scale workloads within defined minimum and maximum pod counts.&lt;/li&gt; &#xA; &lt;li&gt;Dynamic Resource Allocation (DRA): Support vendor-specific hardware resources through Kubernetes ResourceClaims (e.g., GPUs from NVIDIA or AMD).&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/KAI-Scheduler/main/docs/gpu-sharing/README.md&#34;&gt;GPU Sharing&lt;/a&gt;: Allow multiple workloads to efficiently share single or multiple GPUs, maximizing resource utilization.&lt;/li&gt; &#xA; &lt;li&gt;Cloud &amp;amp; On-premise Support: Fully compatible with dynamic cloud infrastructures (including auto-scalers like Karpenter) as well as static on-premise deployments.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;p&gt;Before installing KAI Scheduler, ensure you have:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A running Kubernetes cluster&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://helm.sh/docs/intro/install&#34;&gt;Helm&lt;/a&gt; CLI installed&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NVIDIA/gpu-operator&#34;&gt;NVIDIA GPU-Operator&lt;/a&gt; installed in order to schedule workloads that request GPU resources&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;KAI Scheduler will be installed in &lt;code&gt;kai-scheduler&lt;/code&gt; namespace. When submitting workloads make sure to use a dedicated namespace.&lt;/p&gt; &#xA;&lt;h3&gt;Installation Methods&lt;/h3&gt; &#xA;&lt;p&gt;KAI Scheduler can be installed:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;From Production (Recommended)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;From Source (Build it Yourself)&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Install from Production&lt;/h4&gt; &#xA;&lt;p&gt;Locate the latest release version in &lt;a href=&#34;https://github.com/NVIDIA/KAI-Scheduler/releases&#34;&gt;releases&lt;/a&gt; page. Run the following command after replacing &lt;code&gt;&amp;lt;VERSION&amp;gt;&lt;/code&gt; with the desired release version:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;helm upgrade -i kai-scheduler oci://ghcr.io/nvidia/kai-scheduler/kai-scheduler -n kai-scheduler --create-namespace --version &amp;lt;VERSION&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Build from Source&lt;/h4&gt; &#xA;&lt;p&gt;Follow the instructions &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/KAI-Scheduler/main/docs/developer/building-from-source.md&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;To start scheduling workloads with KAI Scheduler, please continue to &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/KAI-Scheduler/main/docs/quickstart/README.md&#34;&gt;Quick Start example&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;h3&gt;High-level overview of the main priorities for 2025&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Refactor the codebase to enhance vendor neutrality&lt;/li&gt; &#xA; &lt;li&gt;Support Scheduling Gates &lt;a href=&#34;https://github.com/NVIDIA/KAI-Scheduler/issues/63&#34;&gt;https://github.com/NVIDIA/KAI-Scheduler/issues/63&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Research on possible integration with Kueue &lt;a href=&#34;https://github.com/NVIDIA/KAI-Scheduler/issues/68&#34;&gt;https://github.com/NVIDIA/KAI-Scheduler/issues/68&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Add Topology Aware Scheduling support of pod-group &lt;a href=&#34;https://github.com/NVIDIA/KAI-Scheduler/issues/66&#34;&gt;https://github.com/NVIDIA/KAI-Scheduler/issues/66&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Support Min Run Time per workloads&lt;/li&gt; &#xA; &lt;li&gt;Support Max Run Time per workload (with delayed requeue)&lt;/li&gt; &#xA; &lt;li&gt;Add more PriorityClasses as part of the default KAI install&lt;/li&gt; &#xA; &lt;li&gt;Support JobSet&lt;/li&gt; &#xA; &lt;li&gt;Support LWS (LeaderWorkerSet)&lt;/li&gt; &#xA; &lt;li&gt;Add metrics for pod and pod-group preemptions&lt;/li&gt; &#xA; &lt;li&gt;Decouple Priority and Preemption&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Long term goals&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support per queue time decay&lt;/li&gt; &#xA; &lt;li&gt;Hyper scale improvements&lt;/li&gt; &#xA; &lt;li&gt;Support Consolidation of Inference workloads for cluster defragmentation&lt;/li&gt; &#xA; &lt;li&gt;Support n-levels of hierarchical queues&lt;/li&gt; &#xA; &lt;li&gt;Graceful rollout of Inference workloads (new revision update using queue temporary over-quota)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Community, Discussion, and Support&lt;/h2&gt; &#xA;&lt;p&gt;Weâ€™d love to hear from you! Here are the best ways to connect:&lt;/p&gt; &#xA;&lt;h3&gt;Bi-weekly Community Call&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;When:&lt;/strong&gt; Every other Monday at 17:00 CEST&lt;br&gt; &lt;a href=&#34;https://dateful.com/time-zone-converter?t=17&amp;amp;tz2=Germany&#34;&gt;Convert to your time zone&lt;/a&gt; | &lt;a href=&#34;https://calendar.google.com/calendar/event?action=TEMPLATE&amp;amp;tmeid=N2Q2bjhoNXAzMGc0cWpnZTQ4OGtpdXFhanFfMjAyNTA2MDlUMTUwMDAwWiAxZjQ2OTZiOWVlM2JiMWE1ZWIzMTAwODBkNDZiZmMwMDZjNTUxYWFiZmU1YTM3ZGM2YTc0NTFhYmNhMmE1ODk0QGc&amp;amp;tmsrc=1f4696b9ee3bb1a5eb310080d46bfc006c551aabfe5a37dc6a7451abca2a5894%40group.calendar.google.com&amp;amp;scp=ALL&#34;&gt;Add to your calendar&lt;/a&gt; | &lt;a href=&#34;https://docs.google.com/document/d/13K7NGdPebOstlrsif1YLjGz1x-aJafMXeIgqbO7WghI/edit?usp=sharing&#34;&gt;Meeting notes &amp;amp; agenda&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Mailing List&lt;/h3&gt; &#xA;&lt;p&gt;Join the &lt;a href=&#34;https://groups.google.com/g/kai-scheduler&#34;&gt;kai-scheduler mailing list&lt;/a&gt; to receive updates on biweekly meetings.&lt;/p&gt; &#xA;&lt;h3&gt;Technical Issues &amp;amp; Feature Requests&lt;/h3&gt; &#xA;&lt;p&gt;Please open a &lt;a href=&#34;https://github.com/NVIDIA/KAI-Scheduler/issues/new/choose&#34;&gt;GitHub issue&lt;/a&gt; for bugs, feature suggestions, or technical help. This helps us keep track of requests and respond effectively.&lt;/p&gt; &#xA;&lt;h3&gt;Broader Conversations&lt;/h3&gt; &#xA;&lt;p&gt;For deeper discussions like roadmap planning, scheduling strategies, and working group coordination:&lt;br&gt; Join the &lt;a href=&#34;https://communityinviter.com/apps/cloud-native/cncf&#34;&gt;CNCF Slack&lt;/a&gt; and visit &lt;a href=&#34;https://cloud-native.slack.com/archives/C02Q5DFF3MM&#34;&gt;#batch-wg&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>