<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Go Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-09-01T01:31:06Z</updated>
  <subtitle>Daily Trending of Go in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>substratusai/kubeai</title>
    <updated>2024-09-01T01:31:06Z</updated>
    <id>tag:github.com,2024-09-01:/substratusai/kubeai</id>
    <link href="https://github.com/substratusai/kubeai" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Private Open AI on Kubernetes&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;KubeAI: Private Open AI on Kubernetes&lt;/h1&gt; &#xA;&lt;p&gt;The simple AI platform that runs on Kubernetes.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&#34;KubeAI is highly scalable, yet compact enough to fit on my old laptop.&#34; - Some Google Engineer&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;‚úÖÔ∏è Drop-in replacement for OpenAI with API compatibility&lt;br&gt; üöÄ Serve OSS LLMs on CPUs or GPUs&lt;br&gt; ‚öñÔ∏è Scale from zero, autoscale based on load&lt;br&gt; üõ†Ô∏è Zero dependencies (no Istio, Knative, etc.)&lt;br&gt; ü§ñ Operates OSS model servers (vLLM and Ollama)&lt;br&gt; üîã Additional OSS addons included (&lt;a href=&#34;https://github.com/open-webui/open-webui&#34;&gt;OpenWebUI&lt;/a&gt; i.e. ChatGPT UI)&lt;br&gt; ‚úâÔ∏è Plug-n-play with cloud messaging systems (Kafka, PubSub, etc.)&lt;/p&gt; &#xA;&lt;h2&gt;Architecture&lt;/h2&gt; &#xA;&lt;p&gt;KubeAI serves an OpenAI compatible HTTP API. Admins can configure ML models via &lt;code&gt;kind: Model&lt;/code&gt; Kubernetes Custom Resources. KubeAI can be thought of as a Model Operator (See &lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/operator/&#34;&gt;Operator Pattern&lt;/a&gt;) that manages &lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;vLLM&lt;/a&gt; and &lt;a href=&#34;https://github.com/ollama/ollama&#34;&gt;Ollama&lt;/a&gt; servers.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/substratusai/kubeai/main/diagrams/arch.excalidraw.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Local Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;&#xA; &lt;video controls src=&#34;https://github.com/user-attachments/assets/711d1279-6af9-4c6c-a052-e59e7730b757&#34; width=&#34;800&#34;&gt;&lt;/video&gt;&lt;/p&gt; &#xA;&lt;p&gt;Create a local cluster using &lt;a href=&#34;https://kind.sigs.k8s.io/&#34;&gt;kind&lt;/a&gt; or &lt;a href=&#34;https://minikube.sigs.k8s.io/docs/&#34;&gt;minikube&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;TIP: If you are using Podman for kind...&lt;/summary&gt; Make sure your Podman machine can use up to 6G of memory (by default it is capped at 2G): &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# You might need to stop and remove the existing machine:&#xA;podman machine stop&#xA;podman machine rm&#xA;&#xA;# Init and start a new machine:&#xA;podman machine init --memory 6144&#xA;podman machine start&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kind create cluster # OR: minikube start&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Add the KubeAI &lt;a href=&#34;https://helm.sh/docs/intro/install/&#34;&gt;Helm&lt;/a&gt; repository.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;helm repo add kubeai https://substratusai.github.io/kubeai/&#xA;helm repo update&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install KubeAI and wait for all components to be ready (may take a minute).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cat &amp;lt;&amp;lt;EOF &amp;gt; helm-values.yaml&#xA;models:&#xA;  catalog:&#xA;    gemma2-2b-cpu:&#xA;      enabled: true&#xA;      minReplicas: 1&#xA;    qwen2-500m-cpu:&#xA;      enabled: true&#xA;    nomic-embed-text-cpu:&#xA;      enabled: true&#xA;EOF&#xA;&#xA;helm upgrade --install kubeai kubeai/kubeai \&#xA;    -f ./helm-values.yaml \&#xA;    --wait --timeout 10m&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Before progressing to the next steps, start a watch on Pods in a standalone terminal to see how KubeAI deploys models.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl get pods --watch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Interact with Gemma2&lt;/h4&gt; &#xA;&lt;p&gt;Because we set &lt;code&gt;minReplicas: 1&lt;/code&gt; for the Gemma model you should see a model Pod already coming up.&lt;/p&gt; &#xA;&lt;p&gt;Start a local port-forward to the bundled chat UI.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl port-forward svc/openwebui 8000:80&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now open your browser to &lt;a href=&#34;http://localhost:8000&#34;&gt;localhost:8000&lt;/a&gt; and select the Gemma model to start chatting with.&lt;/p&gt; &#xA;&lt;h4&gt;Scale up Qwen2 from Zero&lt;/h4&gt; &#xA;&lt;p&gt;If you go back to the browser and start a chat with Qwen2, you will notice that it will take a while to respond at first. This is because we set &lt;code&gt;minReplicas: 0&lt;/code&gt; for this model and KubeAI needs to spin up a new Pod (you can verify with &lt;code&gt;kubectl get models -oyaml qwen2-500m-cpu&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;p&gt;NOTE: Autoscaling after initial scale-from-zero is not yet supported for the Ollama backend which we use in this local quickstart. KubeAI relies upon backend-specific metrics and the Ollama project has an open issue: &lt;a href=&#34;https://github.com/ollama/ollama/issues/3144&#34;&gt;https://github.com/ollama/ollama/issues/3144&lt;/a&gt;. To see autoscaling in action, checkout one of the &lt;a href=&#34;https://raw.githubusercontent.com/substratusai/kubeai/main/docs/cloud-install.md&#34;&gt;cloud install guides&lt;/a&gt; which uses the vLLM backend and autoscales across GPU resources.&lt;/p&gt; &#xA;&lt;h2&gt;Supported Models&lt;/h2&gt; &#xA;&lt;p&gt;Any vLLM or Ollama model can be served by KubeAI. Some examples of popular models served on KubeAI include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Llama v3.1 (8B, 70B, 405B)&lt;/li&gt; &#xA; &lt;li&gt;Gemma2 (2B, 9B, 27B)&lt;/li&gt; &#xA; &lt;li&gt;Qwen2 (1.5B, 7B, 72B)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Guides&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/substratusai/kubeai/main/installation/gke.md&#34;&gt;Installation on GKE&lt;/a&gt; - Deploy on Kubernetes clusters in the cloud&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/substratusai/kubeai/main/model-management.md&#34;&gt;Model Management&lt;/a&gt; - Manage ML models&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;OpenAI API Compatibility&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Implemented #&#xA;/v1/chat/completions&#xA;/v1/completions&#xA;/v1/embeddings&#xA;/v1/models&#xA;&#xA;# Planned #&#xA;# /v1/assistants/*&#xA;# /v1/batches/*&#xA;# /v1/fine_tuning/*&#xA;# /v1/images/*&#xA;# /v1/vector_stores/*&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Immediate Roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Model caching&lt;/li&gt; &#xA; &lt;li&gt;LoRA finetuning (compatible with OpenAI finetuning API)&lt;/li&gt; &#xA; &lt;li&gt;Image generation (compatible with OpenAI images API)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;Let us know about features you are interested in seeing or reach out with questions. &lt;a href=&#34;https://discord.gg/JeXhcmjZVm&#34;&gt;Visit our Discord channel&lt;/a&gt; to join the discussion!&lt;/p&gt; &#xA;&lt;p&gt;Or just reach out on LinkedIn if you want to connect:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/in/nstogner/&#34;&gt;Nick Stogner&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/in/samstoelinga/&#34;&gt;Sam Stoelinga&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>