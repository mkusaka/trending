<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Go Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-02-11T01:35:56Z</updated>
  <subtitle>Daily Trending of Go in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>open-telemetry/opentelemetry-operator</title>
    <updated>2023-02-11T01:35:56Z</updated>
    <id>tag:github.com,2023-02-11:/open-telemetry/opentelemetry-operator</id>
    <link href="https://github.com/open-telemetry/opentelemetry-operator" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Kubernetes Operator for OpenTelemetry Collector&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/open-telemetry/opentelemetry-operator/actions&#34;&gt;&lt;img src=&#34;https://github.com/open-telemetry/opentelemetry-operator/workflows/Continuous%20Integration/badge.svg?sanitize=true&#34; alt=&#34;Continuous Integration&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://goreportcard.com/report/github.com/open-telemetry/opentelemetry-operator&#34;&gt;&lt;img src=&#34;https://goreportcard.com/badge/github.com/open-telemetry/opentelemetry-operator&#34; alt=&#34;Go Report Card&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://godoc.org/github.com/open-telemetry/opentelemetry-operator/pkg/apis/opentelemetry/v1alpha1#OpenTelemetryCollector&#34;&gt;&lt;img src=&#34;https://godoc.org/github.com/open-telemetry/opentelemetry-operator?status.svg?sanitize=true&#34; alt=&#34;GoDoc&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;OpenTelemetry Operator for Kubernetes&lt;/h1&gt; &#xA;&lt;p&gt;The OpenTelemetry Operator is an implementation of a &lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/operator/&#34;&gt;Kubernetes Operator&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The operator manages:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-telemetry/opentelemetry-collector&#34;&gt;OpenTelemetry Collector&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;auto-instrumentation of the workloads using OpenTelemetry instrumentation libraries&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-telemetry/opentelemetry-operator/main/docs/api.md&#34;&gt;API docs&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Helm Charts&lt;/h2&gt; &#xA;&lt;p&gt;You can install Opentelemetry Operator via &lt;a href=&#34;https://github.com/open-telemetry/opentelemetry-helm-charts/tree/main/charts/opentelemetry-operator&#34;&gt;Helm Chart&lt;/a&gt; from the opentelemetry-helm-charts repository. More information is available in &lt;a href=&#34;https://github.com/open-telemetry/opentelemetry-helm-charts/tree/main/charts/opentelemetry-operator&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;To install the operator in an existing cluster, make sure you have &lt;a href=&#34;https://cert-manager.io/docs/installation/&#34;&gt;&lt;code&gt;cert-manager&lt;/code&gt; installed&lt;/a&gt; and run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl apply -f https://github.com/open-telemetry/opentelemetry-operator/releases/latest/download/opentelemetry-operator.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once the &lt;code&gt;opentelemetry-operator&lt;/code&gt; deployment is ready, create an OpenTelemetry Collector (otelcol) instance, like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;kubectl apply -f - &amp;lt;&amp;lt;EOF&#xA;apiVersion: opentelemetry.io/v1alpha1&#xA;kind: OpenTelemetryCollector&#xA;metadata:&#xA;  name: simplest&#xA;spec:&#xA;  config: |&#xA;    receivers:&#xA;      otlp:&#xA;        protocols:&#xA;          grpc:&#xA;          http:&#xA;    processors:&#xA;      memory_limiter:&#xA;        check_interval: 1s&#xA;        limit_percentage: 75&#xA;        spike_limit_percentage: 15&#xA;      batch:&#xA;        send_batch_size: 10000&#xA;        timeout: 10s&#xA;&#xA;    exporters:&#xA;      logging:&#xA;&#xA;    service:&#xA;      pipelines:&#xA;        traces:&#xA;          receivers: [otlp]&#xA;          processors: []&#xA;          exporters: [logging]&#xA;EOF&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;em&gt;WARNING:&lt;/em&gt;&lt;/strong&gt; Until the OpenTelemetry Collector format is stable, changes may be required in the above example to remain compatible with the latest version of the OpenTelemetry Collector image being referenced.&lt;/p&gt; &#xA;&lt;p&gt;This will create an OpenTelemetry Collector instance named &lt;code&gt;simplest&lt;/code&gt;, exposing a &lt;code&gt;jaeger-grpc&lt;/code&gt; port to consume spans from your instrumented applications and exporting those spans via &lt;code&gt;logging&lt;/code&gt;, which writes the spans to the console (&lt;code&gt;stdout&lt;/code&gt;) of the OpenTelemetry Collector instance that receives the span.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;config&lt;/code&gt; node holds the &lt;code&gt;YAML&lt;/code&gt; that should be passed down as-is to the underlying OpenTelemetry Collector instances. Refer to the &lt;a href=&#34;https://github.com/open-telemetry/opentelemetry-collector&#34;&gt;OpenTelemetry Collector&lt;/a&gt; documentation for a reference of the possible entries.&lt;/p&gt; &#xA;&lt;p&gt;At this point, the Operator does &lt;em&gt;not&lt;/em&gt; validate the contents of the configuration file: if the configuration is invalid, the instance will still be created but the underlying OpenTelemetry Collector might crash.&lt;/p&gt; &#xA;&lt;p&gt;The Operator does examine the configuration file to discover configured receivers and their ports. If it finds receivers with ports, it creates a pair of kubernetes services, one headless, exposing those ports within the cluster. The headless service contains a &lt;code&gt;service.beta.openshift.io/serving-cert-secret-name&lt;/code&gt; annotation that will cause OpenShift to create a secret containing a certificate and key. This secret can be mounted as a volume and the certificate and key used in those receivers&#39; TLS configurations.&lt;/p&gt; &#xA;&lt;h3&gt;Upgrades&lt;/h3&gt; &#xA;&lt;p&gt;As noted above, the OpenTelemetry Collector format is continuing to evolve. However, a best-effort attempt is made to upgrade all managed &lt;code&gt;OpenTelemetryCollector&lt;/code&gt; resources.&lt;/p&gt; &#xA;&lt;p&gt;In certain scenarios, it may be desirable to prevent the operator from upgrading certain &lt;code&gt;OpenTelemetryCollector&lt;/code&gt; resources. For example, when a resource is configured with a custom &lt;code&gt;.Spec.Image&lt;/code&gt;, end users may wish to manage configuration themselves as opposed to having the operator upgrade it. This can be configured on a resource by resource basis with the exposed property &lt;code&gt;.Spec.UpgradeStrategy&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;By configuring a resource&#39;s &lt;code&gt;.Spec.UpgradeStrategy&lt;/code&gt; to &lt;code&gt;none&lt;/code&gt;, the operator will skip the given instance during the upgrade routine.&lt;/p&gt; &#xA;&lt;p&gt;The default and only other acceptable value for &lt;code&gt;.Spec.UpgradeStrategy&lt;/code&gt; is &lt;code&gt;automatic&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Deployment modes&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;code&gt;CustomResource&lt;/code&gt; for the &lt;code&gt;OpenTelemetryCollector&lt;/code&gt; exposes a property named &lt;code&gt;.Spec.Mode&lt;/code&gt;, which can be used to specify whether the collector should run as a &lt;code&gt;DaemonSet&lt;/code&gt;, &lt;code&gt;Sidecar&lt;/code&gt;, or &lt;code&gt;Deployment&lt;/code&gt; (default). Look at &lt;a href=&#34;https://github.com/open-telemetry/opentelemetry-operator/raw/main/tests/e2e/daemonset-features/00-install.yaml&#34;&gt;this sample&lt;/a&gt; for reference.&lt;/p&gt; &#xA;&lt;h4&gt;Sidecar injection&lt;/h4&gt; &#xA;&lt;p&gt;A sidecar with the OpenTelemetry Collector can be injected into pod-based workloads by setting the pod annotation &lt;code&gt;sidecar.opentelemetry.io/inject&lt;/code&gt; to either &lt;code&gt;&#34;true&#34;&lt;/code&gt;, or to the name of a concrete &lt;code&gt;OpenTelemetryCollector&lt;/code&gt;, like in the following example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;kubectl apply -f - &amp;lt;&amp;lt;EOF&#xA;apiVersion: opentelemetry.io/v1alpha1&#xA;kind: OpenTelemetryCollector&#xA;metadata:&#xA;  name: sidecar-for-my-app&#xA;spec:&#xA;  mode: sidecar&#xA;  config: |&#xA;    receivers:&#xA;      jaeger:&#xA;        protocols:&#xA;          thrift_compact:&#xA;    processors:&#xA;&#xA;    exporters:&#xA;      logging:&#xA;&#xA;    service:&#xA;      pipelines:&#xA;        traces:&#xA;          receivers: [jaeger]&#xA;          processors: []&#xA;          exporters: [logging]&#xA;EOF&#xA;&#xA;kubectl apply -f - &amp;lt;&amp;lt;EOF&#xA;apiVersion: v1&#xA;kind: Pod&#xA;metadata:&#xA;  name: myapp&#xA;  annotations:&#xA;    sidecar.opentelemetry.io/inject: &#34;true&#34;&#xA;spec:&#xA;  containers:&#xA;  - name: myapp&#xA;    image: jaegertracing/vertx-create-span:operator-e2e-tests&#xA;    ports:&#xA;      - containerPort: 8080&#xA;        protocol: TCP&#xA;EOF&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When there are multiple &lt;code&gt;OpenTelemetryCollector&lt;/code&gt; resources with a mode set to &lt;code&gt;Sidecar&lt;/code&gt; in the same namespace, a concrete name should be used. When there&#39;s only one &lt;code&gt;Sidecar&lt;/code&gt; instance in the same namespace, this instance is used when the annotation is set to &lt;code&gt;&#34;true&#34;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The annotation value can come either from the namespace, or from the pod. The most specific annotation wins, in this order:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;the pod annotation is used when it&#39;s set to a concrete instance name or to &lt;code&gt;&#34;false&#34;&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;namespace annotation is used when the pod annotation is either absent or set to &lt;code&gt;&#34;true&#34;&lt;/code&gt;, and the namespace is set to a concrete instance or to &lt;code&gt;&#34;false&#34;&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The possible values for the annotation can be:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&#34;true&#34; - inject &lt;code&gt;OpenTelemetryCollector&lt;/code&gt; resource from the namespace.&lt;/li&gt; &#xA; &lt;li&gt;&#34;sidecar-for-my-app&#34; - name of &lt;code&gt;OpenTelemetryCollector&lt;/code&gt; CR instance in the current namespace.&lt;/li&gt; &#xA; &lt;li&gt;&#34;my-other-namespace/my-instrumentation&#34; - name and namespace of &lt;code&gt;OpenTelemetryCollector&lt;/code&gt; CR instance in another namespace.&lt;/li&gt; &#xA; &lt;li&gt;&#34;false&#34; - do not inject&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;When using a pod-based workload, such as &lt;code&gt;Deployment&lt;/code&gt; or &lt;code&gt;Statefulset&lt;/code&gt;, make sure to add the annotation to the &lt;code&gt;PodTemplate&lt;/code&gt; part. Like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;kubectl apply -f - &amp;lt;&amp;lt;EOF&#xA;apiVersion: apps/v1&#xA;kind: Deployment&#xA;metadata:&#xA;  name: my-app&#xA;  labels:&#xA;    app: my-app&#xA;  annotations:&#xA;    sidecar.opentelemetry.io/inject: &#34;true&#34; # WRONG&#xA;spec:&#xA;  selector:&#xA;    matchLabels:&#xA;      app: my-app&#xA;  replicas: 1&#xA;  template:&#xA;    metadata:&#xA;      labels:&#xA;        app: my-app&#xA;      annotations:&#xA;        sidecar.opentelemetry.io/inject: &#34;true&#34; # CORRECT&#xA;    spec:&#xA;      containers:&#xA;      - name: myapp&#xA;        image: jaegertracing/vertx-create-span:operator-e2e-tests&#xA;        ports:&#xA;          - containerPort: 8080&#xA;            protocol: TCP&#xA;EOF&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When using sidecar mode the OpenTelemetry collector container will have the environment variable &lt;code&gt;OTEL_RESOURCE_ATTRIBUTES&lt;/code&gt;set with Kubernetes resource attributes, ready to be consumed by the &lt;a href=&#34;https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/resourcedetectionprocessor&#34;&gt;resourcedetection&lt;/a&gt; processor.&lt;/p&gt; &#xA;&lt;h3&gt;OpenTelemetry auto-instrumentation injection&lt;/h3&gt; &#xA;&lt;p&gt;The operator can inject and configure OpenTelemetry auto-instrumentation libraries. Currently DotNet, Java, NodeJS and Python are supported.&lt;/p&gt; &#xA;&lt;p&gt;To use auto-instrumentation, configure an &lt;code&gt;Instrumentation&lt;/code&gt; resource with the configuration for the SDK and instrumentation.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;kubectl apply -f - &amp;lt;&amp;lt;EOF&#xA;apiVersion: opentelemetry.io/v1alpha1&#xA;kind: Instrumentation&#xA;metadata:&#xA;  name: my-instrumentation&#xA;spec:&#xA;  exporter:&#xA;    endpoint: http://otel-collector:4317&#xA;  propagators:&#xA;    - tracecontext&#xA;    - baggage&#xA;    - b3&#xA;  sampler:&#xA;    type: parentbased_traceidratio&#xA;    argument: &#34;0.25&#34;&#xA;  python:&#xA;    env:&#xA;      # Required if endpoint is set to 4317.&#xA;      # Python autoinstrumentation uses http/proto by default&#xA;      # so data must be sent to 4318 instead of 4137.&#xA;      - name: OTEL_EXPORTER_OTLP_ENDPOINT&#xA;        value: http://otel-collector:4318&#xA;EOF&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The above CR can be queried by &lt;code&gt;kubectl get otelinst&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Then add an annotation to a pod to enable injection. The annotation can be added to a namespace, so that all pods within that namespace wil get instrumentation, or by adding the annotation to individual PodSpec objects, available as part of Deployment, Statefulset, and other resources.&lt;/p&gt; &#xA;&lt;p&gt;Java:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;instrumentation.opentelemetry.io/inject-java: &#34;true&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;NodeJS:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;instrumentation.opentelemetry.io/inject-nodejs: &#34;true&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Python:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;instrumentation.opentelemetry.io/inject-python: &#34;true&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;DotNet:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;instrumentation.opentelemetry.io/inject-dotnet: &#34;true&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;OpenTelemetry SDK environment variables only:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;instrumentation.opentelemetry.io/inject-sdk: &#34;true&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The possible values for the annotation can be&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;&#34;true&#34;&lt;/code&gt; - inject and &lt;code&gt;Instrumentation&lt;/code&gt; resource from the namespace.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&#34;my-instrumentation&#34;&lt;/code&gt; - name of &lt;code&gt;Instrumentation&lt;/code&gt; CR instance in the current namespace.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&#34;my-other-namespace/my-instrumentation&#34;&lt;/code&gt; - name and namespace of &lt;code&gt;Instrumentation&lt;/code&gt; CR instance in another namespace.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&#34;false&#34;&lt;/code&gt; - do not inject&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Multi-container pods&lt;/h4&gt; &#xA;&lt;p&gt;If nothing else is specified, instrumentation is performed on the first container available in the pod spec. In some cases (for example in the case of the injection of an Istio sidecar) it becomes necessary to specify on which container(s) this injection must be performed.&lt;/p&gt; &#xA;&lt;p&gt;For this, it is possible to fine-tune the pod(s) on which the injection will be carried out.&lt;/p&gt; &#xA;&lt;p&gt;For this, we will use the &lt;code&gt;instrumentation.opentelemetry.io/container-names&lt;/code&gt; annotation for which we will indicate one or more pod names (&lt;code&gt;.spec.containers.name&lt;/code&gt;) on which the injection must be made:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: apps/v1&#xA;kind: Deployment&#xA;metadata:&#xA;  name: my-deployment-with-multiple-containers&#xA;spec:&#xA;  selector:&#xA;    matchLabels:&#xA;      app: my-pod-with-multiple-containers&#xA;  replicas: 1&#xA;  template:&#xA;    metadata:&#xA;      labels:&#xA;        app: my-pod-with-multiple-containers&#xA;      annotations:&#xA;        instrumentation.opentelemetry.io/inject-java: &#34;true&#34;&#xA;        instrumentation.opentelemetry.io/container-names: &#34;myapp,myapp2&#34;&#xA;    spec:&#xA;      containers:&#xA;      - name: myapp&#xA;        image: myImage1&#xA;      - name: myapp2&#xA;        image: myImage2&#xA;      - name: myapp3&#xA;        image: myImage3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In the above case, &lt;code&gt;myapp&lt;/code&gt; and &lt;code&gt;myapp2&lt;/code&gt; containers will be instrumented, &lt;code&gt;myapp3&lt;/code&gt; will not.&lt;/p&gt; &#xA;&lt;h4&gt;Use customized or vendor instrumentation&lt;/h4&gt; &#xA;&lt;p&gt;By default, the operator uses upstream auto-instrumentation libraries. Custom auto-instrumentation can be configured by overriding the image fields in a CR.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: opentelemetry.io/v1alpha1&#xA;kind: Instrumentation&#xA;metadata:&#xA;  name: my-instrumentation&#xA;spec:&#xA;  java:&#xA;    image: your-customized-auto-instrumentation-image:java&#xA;  nodejs:&#xA;    image: your-customized-auto-instrumentation-image:nodejs&#xA;  python:&#xA;    image: your-customized-auto-instrumentation-image:python&#xA;  dotnet:&#xA;    image: your-customized-auto-instrumentation-image:dotnet&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The Dockerfiles for auto-instrumentation can be found in &lt;a href=&#34;https://raw.githubusercontent.com/open-telemetry/opentelemetry-operator/main/autoinstrumentation&#34;&gt;autoinstrumentation directory&lt;/a&gt;. Follow the instructions in the Dockerfiles on how to build a custom container image.&lt;/p&gt; &#xA;&lt;h4&gt;Inject OpenTelemetry SDK environment variables only&lt;/h4&gt; &#xA;&lt;p&gt;You can configure the OpenTelemetry SDK for applications which can&#39;t currently be autoinstrumented by using &lt;code&gt;inject-sdk&lt;/code&gt; in place of (e.g.) &lt;code&gt;inject-python&lt;/code&gt; or &lt;code&gt;inject-java&lt;/code&gt;. This will inject environment variables like &lt;code&gt;OTEL_RESOURCE_ATTRIBUTES&lt;/code&gt;, &lt;code&gt;OTEL_TRACES_SAMPLER&lt;/code&gt;, and &lt;code&gt;OTEL_EXPORTER_OTLP_ENDPOINT&lt;/code&gt;, that you can configure in the &lt;code&gt;Instrumentation&lt;/code&gt;, but will not actually provide the SDK.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;instrumentation.opentelemetry.io/inject-sdk: &#34;true&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Target Allocator&lt;/h3&gt; &#xA;&lt;p&gt;The OpenTelemetry Operator comes with an optional component, the Target Allocator (TA). When creating an OpenTelemetryCollector Custom Resource (CR) and setting the TA as enabled, the Operator will create a new deployment and service to serve specific &lt;code&gt;http_sd_config&lt;/code&gt; directives for each Collector pod as part of that CR. It will also change the Prometheus receiver configuration in the CR, so that it uses the &lt;a href=&#34;https://prometheus.io/docs/prometheus/latest/http_sd/&#34;&gt;http_sd_config&lt;/a&gt; from the TA. The following example shows how to get started with the Target Allocator:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: opentelemetry.io/v1alpha1&#xA;kind: OpenTelemetryCollector&#xA;metadata:&#xA;  name: collector-with-ta&#xA;spec:&#xA;  mode: statefulset&#xA;  targetAllocator:&#xA;    enabled: true&#xA;  config: |&#xA;    receivers:&#xA;      prometheus:&#xA;        config:&#xA;          scrape_configs:&#xA;          - job_name: &#39;otel-collector&#39;&#xA;            scrape_interval: 10s&#xA;            static_configs:&#xA;            - targets: [ &#39;0.0.0.0:8888&#39; ]&#xA;&#xA;    exporters:&#xA;      logging:&#xA;&#xA;    service:&#xA;      pipelines:&#xA;        traces:&#xA;          receivers: [prometheus]&#xA;          processors: []&#xA;          exporters: [logging]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Behind the scenes, the OpenTelemetry Operator will convert the Collector’s configuration after the reconciliation into the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;    receivers:&#xA;      prometheus:&#xA;        config:&#xA;          global:&#xA;            scrape_interval: 1m&#xA;            scrape_timeout: 10s&#xA;            evaluation_interval: 1m&#xA;          scrape_configs:&#xA;          - job_name: otel-collector&#xA;            honor_timestamps: true&#xA;            scrape_interval: 10s&#xA;            scrape_timeout: 10s&#xA;            metrics_path: /metrics&#xA;            scheme: http&#xA;            follow_redirects: true&#xA;            http_sd_configs:&#xA;            - follow_redirects: false&#xA;              url: http://collector-with-ta-targetallocator:80/jobs/otel-collector/targets?collector_id=$POD_NAME&#xA;&#xA;    exporters:&#xA;      logging:&#xA;&#xA;    service:&#xA;      pipelines:&#xA;        traces:&#xA;          receivers: [prometheus]&#xA;          processors: []&#xA;          exporters: [logging]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note how the Operator added a &lt;code&gt;global&lt;/code&gt; section and a new &lt;code&gt;http_sd_configs&lt;/code&gt; to the &lt;code&gt;otel-collector&lt;/code&gt; scrape config, pointing to a Target Allocator instance it provisioned.&lt;/p&gt; &#xA;&lt;h2&gt;Compatibility matrix&lt;/h2&gt; &#xA;&lt;h3&gt;OpenTelemetry Operator vs. OpenTelemetry Collector&lt;/h3&gt; &#xA;&lt;p&gt;The OpenTelemetry Operator follows the same versioning as the operand (OpenTelemetry Collector) up to the minor part of the version. For example, the OpenTelemetry Operator v0.18.1 tracks OpenTelemetry Collector 0.18.0. The patch part of the version indicates the patch level of the operator itself, not that of OpenTelemetry Collector. Whenever a new patch version is released for OpenTelemetry Collector, we&#39;ll release a new patch version of the operator.&lt;/p&gt; &#xA;&lt;p&gt;By default, the OpenTelemetry Operator ensures consistent versioning between itself and the managed &lt;code&gt;OpenTelemetryCollector&lt;/code&gt; resources. That is, if the OpenTelemetry Operator is based on version &lt;code&gt;0.40.0&lt;/code&gt;, it will create resources with an underlying OpenTelemetry Collector at version &lt;code&gt;0.40.0&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When a custom &lt;code&gt;Spec.Image&lt;/code&gt; is used with an &lt;code&gt;OpenTelemetryCollector&lt;/code&gt; resource, the OpenTelemetry Operator will not manage this versioning and upgrading. In this scenario, it is best practice that the OpenTelemetry Operator version should match the underlying core version. Given a &lt;code&gt;OpenTelemetryCollector&lt;/code&gt; resource with a &lt;code&gt;Spec.Image&lt;/code&gt; configured to a custom image based on underlying OpenTelemetry Collector at version &lt;code&gt;0.40.0&lt;/code&gt;, it is recommended that the OpenTelemetry Operator is kept at version &lt;code&gt;0.40.0&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;OpenTelemetry Operator vs. Kubernetes vs. Cert Manager&lt;/h3&gt; &#xA;&lt;p&gt;We strive to be compatible with the widest range of Kubernetes versions as possible, but some changes to Kubernetes itself require us to break compatibility with older Kubernetes versions, be it because of code incompatibilities, or in the name of maintainability. Every released operator will support a specific range of Kubernetes versions, to be determined at the latest during the release.&lt;/p&gt; &#xA;&lt;p&gt;We use &lt;code&gt;cert-manager&lt;/code&gt; for some features of this operator and the third column shows the versions of the &lt;code&gt;cert-manager&lt;/code&gt; that are known to work with this operator&#39;s versions.&lt;/p&gt; &#xA;&lt;p&gt;The OpenTelemetry Operator &lt;em&gt;might&lt;/em&gt; work on versions outside of the given range, but when opening new issues, please make sure to test your scenario on a supported version.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;OpenTelemetry Operator&lt;/th&gt; &#xA;   &lt;th&gt;Kubernetes&lt;/th&gt; &#xA;   &lt;th&gt;Cert-Manager&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;v0.69.0&lt;/td&gt; &#xA;   &lt;td&gt;v1.19 to v1.25&lt;/td&gt; &#xA;   &lt;td&gt;v1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;v0.68.0&lt;/td&gt; &#xA;   &lt;td&gt;v1.19 to v1.25&lt;/td&gt; &#xA;   &lt;td&gt;v1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;v0.67.0&lt;/td&gt; &#xA;   &lt;td&gt;v1.19 to v1.25&lt;/td&gt; &#xA;   &lt;td&gt;v1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;v0.66.0&lt;/td&gt; &#xA;   &lt;td&gt;v1.19 to v1.25&lt;/td&gt; &#xA;   &lt;td&gt;v1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;v0.64.1&lt;/td&gt; &#xA;   &lt;td&gt;v1.19 to v1.25&lt;/td&gt; &#xA;   &lt;td&gt;v1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;v0.63.1&lt;/td&gt; &#xA;   &lt;td&gt;v1.19 to v1.25&lt;/td&gt; &#xA;   &lt;td&gt;v1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;v0.62.1&lt;/td&gt; &#xA;   &lt;td&gt;v1.19 to v1.25&lt;/td&gt; &#xA;   &lt;td&gt;v1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;v0.61.0&lt;/td&gt; &#xA;   &lt;td&gt;v1.19 to v1.25&lt;/td&gt; &#xA;   &lt;td&gt;v1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;v0.60.0&lt;/td&gt; &#xA;   &lt;td&gt;v1.19 to v1.25&lt;/td&gt; &#xA;   &lt;td&gt;v1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;v0.59.0&lt;/td&gt; &#xA;   &lt;td&gt;v1.19 to v1.24&lt;/td&gt; &#xA;   &lt;td&gt;v1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;v0.58.0&lt;/td&gt; &#xA;   &lt;td&gt;v1.19 to v1.24&lt;/td&gt; &#xA;   &lt;td&gt;v1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;v0.57.2&lt;/td&gt; &#xA;   &lt;td&gt;v1.19 to v1.24&lt;/td&gt; &#xA;   &lt;td&gt;v1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;v0.56.0&lt;/td&gt; &#xA;   &lt;td&gt;v1.19 to v1.24&lt;/td&gt; &#xA;   &lt;td&gt;v1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;v0.55.0&lt;/td&gt; &#xA;   &lt;td&gt;v1.19 to v1.24&lt;/td&gt; &#xA;   &lt;td&gt;v1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;v0.54.0&lt;/td&gt; &#xA;   &lt;td&gt;v1.19 to v1.24&lt;/td&gt; &#xA;   &lt;td&gt;v1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;v0.53.0&lt;/td&gt; &#xA;   &lt;td&gt;v1.19 to v1.24&lt;/td&gt; &#xA;   &lt;td&gt;v1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;v0.52.0&lt;/td&gt; &#xA;   &lt;td&gt;v1.19 to v1.23&lt;/td&gt; &#xA;   &lt;td&gt;v1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;v0.51.0&lt;/td&gt; &#xA;   &lt;td&gt;v1.19 to v1.23&lt;/td&gt; &#xA;   &lt;td&gt;v1alpha2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;v0.50.0&lt;/td&gt; &#xA;   &lt;td&gt;v1.19 to v1.23&lt;/td&gt; &#xA;   &lt;td&gt;v1alpha2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;v0.49.0&lt;/td&gt; &#xA;   &lt;td&gt;v1.19 to v1.23&lt;/td&gt; &#xA;   &lt;td&gt;v1alpha2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;v0.48.0&lt;/td&gt; &#xA;   &lt;td&gt;v1.19 to v1.23&lt;/td&gt; &#xA;   &lt;td&gt;v1alpha2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;v0.47.0&lt;/td&gt; &#xA;   &lt;td&gt;v1.19 to v1.23&lt;/td&gt; &#xA;   &lt;td&gt;v1alpha2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Contributing and Developing&lt;/h2&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://raw.githubusercontent.com/open-telemetry/opentelemetry-operator/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Approvers (&lt;a href=&#34;https://github.com/orgs/open-telemetry/teams/operator-approvers&#34;&gt;@open-telemetry/operator-approvers&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/frzifus&#34;&gt;Benedikt Bongartz&lt;/a&gt;, Red Hat&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yuriolisa&#34;&gt;Yuri Oliveira Sa&lt;/a&gt;, Red Hat&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Emeritus Approvers:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Aneurysm9&#34;&gt;Anthony Mirabella&lt;/a&gt;, AWS&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dmitryax&#34;&gt;Dmitrii Anoshin&lt;/a&gt;, Splunk&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jrcamp&#34;&gt;Jay Camp&lt;/a&gt;, Splunk&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/james-bebbington&#34;&gt;James Bebbington&lt;/a&gt;, Google&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/owais&#34;&gt;Owais Lone&lt;/a&gt;, Splunk&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mx-psi&#34;&gt;Pablo Baeyens&lt;/a&gt;, DataDog&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Target Allocator Maintainers (&lt;a href=&#34;https://github.com/orgs/open-telemetry/teams/operator-ta-maintainers&#34;&gt;@open-telemetry/operator-ta-maintainers&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Aneurysm9&#34;&gt;Anthony Mirabella&lt;/a&gt;, AWS&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kristinapathak&#34;&gt;Kristina Pathak&lt;/a&gt;, Lightstep&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/secustor&#34;&gt;Sebastian Poxhofer&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Maintainers (&lt;a href=&#34;https://github.com/orgs/open-telemetry/teams/operator-maintainers&#34;&gt;@open-telemetry/operator-maintainers&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jaronoff97&#34;&gt;Jacob Aronoff&lt;/a&gt;, Lightstep&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jpkrohling&#34;&gt;Juraci Paixão Kröhling&lt;/a&gt;, Grafana Labs&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pavolloffay&#34;&gt;Pavol Loffay&lt;/a&gt;, Red Hat&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/VineethReddy02&#34;&gt;Vineeth Pothulapati&lt;/a&gt;, Timescale&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Emeritus Maintainers&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/codeboten&#34;&gt;Alex Boten&lt;/a&gt;, Lightstep&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/BogdanDrutu&#34;&gt;Bogdan Drutu&lt;/a&gt;, Splunk&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tigrannajaryan&#34;&gt;Tigran Najaryan&lt;/a&gt;, Splunk&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Learn more about roles in the &lt;a href=&#34;https://github.com/open-telemetry/community/raw/main/community-membership.md&#34;&gt;community repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Thanks to all the people who already contributed!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/open-telemetry/opentelemetry-operator/graphs/contributors&#34;&gt;&lt;img src=&#34;https://contributors-img.web.app/image?repo=open-telemetry/opentelemetry-operator&#34; alt=&#34;Contributors&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-telemetry/opentelemetry-operator/main/LICENSE&#34;&gt;Apache 2.0 License&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>openziti/zrok</title>
    <updated>2023-02-11T01:35:56Z</updated>
    <id>tag:github.com,2023-02-11:/openziti/zrok</id>
    <link href="https://github.com/openziti/zrok" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Geo-scale, next-generation sharing platform built on top of OpenZiti.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/openziti/zrok/main/docs/images/zrok.png&#34; alt=&#34;zrok&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;zrok&lt;/code&gt; is a next-generation sharing platform built on top of &lt;a href=&#34;https://docs.openziti.io/docs/learn/introduction/&#34;&gt;OpenZiti&lt;/a&gt;, a programmable zero-trust network overlay. &lt;code&gt;zrok&lt;/code&gt; is a &lt;em&gt;Ziti Native Application&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;zrok&lt;/code&gt; facilitates sharing resources both publicly and privately, exposing them to an audience you can easily control.&lt;/p&gt; &#xA;&lt;p&gt;Like other offerings in this space, &lt;code&gt;zrok&lt;/code&gt; allows users to create ephemeral reverse proxies (&#34;tunnels&#34;) for &lt;code&gt;http&lt;/code&gt; resources. Additionally:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;zrok&lt;/code&gt; allows users to &lt;em&gt;privately&lt;/em&gt; share resources with other &lt;code&gt;zrok&lt;/code&gt; users; in &lt;em&gt;private&lt;/em&gt; usage scenarios, your private resources are not exposed to any public endpoints; all communication is securely and privately transported between &lt;code&gt;zrok&lt;/code&gt; environments&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;zrok&lt;/code&gt; allows sharing other types of resources; rather than just proxying &lt;code&gt;http&lt;/code&gt; endpoints, &lt;code&gt;zrok&lt;/code&gt; allows users to easily and rapidly share files and web content&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;zrok&lt;/code&gt; is ready to be extended to easily support many kinds of decentralized resource sharing; &lt;code&gt;zrok&lt;/code&gt; provides a framework that makes this kind of peer-to-peer resource sharing simple and secure&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/openziti/zrok/main/docs/images/zrok_deployment.png&#34; alt=&#34;zrok&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Frictionless&lt;/h2&gt; &#xA;&lt;p&gt;You can be up and sharing using the &lt;code&gt;zrok.io&lt;/code&gt; service in minutes. Here is a synopsis of what&#39;s involved.&lt;/p&gt; &#xA;&lt;h3&gt;First-time Setup&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download the binary for your platform &lt;a href=&#34;https://github.com/openziti/zrok/releases&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;zrok invite&lt;/code&gt; to create an account with the service&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;zrok enable&lt;/code&gt; to enable your shell environment for sharing with the service&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;And then... sharing...&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;zrok share&lt;/code&gt; to share resources immediately, simply and securely&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/openziti/zrok/main/docs/getting-started.md&#34;&gt;Concepts and Getting Started Guide&lt;/a&gt; for a full overview.&lt;/p&gt; &#xA;&lt;h2&gt;Self-Hosting&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;zrok&lt;/code&gt; is designed to scale up to support extremely large service instances. &lt;code&gt;zrok.io&lt;/code&gt; is a public service instance operated by NetFoundry using the same code base that is available to self-hosted environments.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;zrok&lt;/code&gt; is also designed to scale down to support extremely small deployments. Run &lt;code&gt;zrok&lt;/code&gt; and OpenZiti on a Raspberry Pi!&lt;/p&gt; &#xA;&lt;p&gt;The single &lt;code&gt;zrok&lt;/code&gt; binary contains everything you need to operate &lt;code&gt;zrok&lt;/code&gt; environments and also host your own service instances. Just add an OpenZiti network and you&#39;re up and running.&lt;/p&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/openziti/zrok/main/docs/guides/v0.3_self_hosting_guide.md&#34;&gt;Self-Hosting Guide&lt;/a&gt; for details on getting your own &lt;code&gt;zrok&lt;/code&gt; service instance running. This builds on top of the &lt;a href=&#34;https://docs.openziti.io/docs/learn/quickstarts/network/&#34;&gt;OpenZiti Quick Start&lt;/a&gt; to have a running &lt;code&gt;zrok&lt;/code&gt; service instance in minutes.&lt;/p&gt; &#xA;&lt;h2&gt;Building&lt;/h2&gt; &#xA;&lt;p&gt;If you are interested in building &lt;code&gt;zrok&lt;/code&gt; for yourself instead of using a released package, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/openziti/zrok/main/BUILD.md&#34;&gt;BUILD.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;d like to contribute back to &lt;code&gt;zrok&lt;/code&gt;, that&#39;d be great. Please see our &lt;a href=&#34;https://raw.githubusercontent.com/openziti/zrok/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; page and abide by the &lt;a href=&#34;https://raw.githubusercontent.com/openziti/zrok/main/CODE_OF_CONDUCT.md&#34;&gt;CODE_OF_CONDUCT.md&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>sashabaranov/go-gpt3</title>
    <updated>2023-02-11T01:35:56Z</updated>
    <id>tag:github.com,2023-02-11:/sashabaranov/go-gpt3</id>
    <link href="https://github.com/sashabaranov/go-gpt3" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OpenAI GPT-3 and DALL·E API wrapper for Go&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;go-gpt3&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://godoc.org/github.com/sashabaranov/go-gpt3&#34;&gt;&lt;img src=&#34;http://img.shields.io/badge/GoDoc-Reference-blue.svg?sanitize=true&#34; alt=&#34;GoDoc&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://goreportcard.com/report/github.com/sashabaranov/go-gpt3&#34;&gt;&lt;img src=&#34;https://goreportcard.com/badge/github.com/sashabaranov/go-gpt3&#34; alt=&#34;Go Report Card&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://beta.openai.com/&#34;&gt;OpenAI GPT-3&lt;/a&gt; API wrapper for Go&lt;/p&gt; &#xA;&lt;p&gt;Installation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;go get github.com/sashabaranov/go-gpt3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Example usage:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package main&#xA;&#xA;import (&#xA;&#x9;&#34;context&#34;&#xA;&#x9;&#34;fmt&#34;&#xA;&#x9;gogpt &#34;github.com/sashabaranov/go-gpt3&#34;&#xA;)&#xA;&#xA;func main() {&#xA;&#x9;c := gogpt.NewClient(&#34;your token&#34;)&#xA;&#x9;ctx := context.Background()&#xA;&#xA;&#x9;req := gogpt.CompletionRequest{&#xA;&#x9;&#x9;Model:     gogpt.GPT3Ada,&#xA;&#x9;&#x9;MaxTokens: 5,&#xA;&#x9;&#x9;Prompt:    &#34;Lorem ipsum&#34;,&#xA;&#x9;}&#xA;&#x9;resp, err := c.CreateCompletion(ctx, req)&#xA;&#x9;if err != nil {&#xA;&#x9;&#x9;return&#xA;&#x9;}&#xA;&#x9;fmt.Println(resp.Choices[0].Text)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Streaming response example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package main&#xA;&#xA;import (&#xA;&#x9;&#34;errors&#34;&#xA;&#x9;&#34;context&#34;&#xA;&#x9;&#34;fmt&#34;&#xA;&#x9;&#34;io&#34;&#xA;&#x9;gogpt &#34;github.com/sashabaranov/go-gpt3&#34;&#xA;)&#xA;&#xA;func main() {&#xA;&#x9;c := gogpt.NewClient(&#34;your token&#34;)&#xA;&#x9;ctx := context.Background()&#xA;&#xA;&#x9;req := gogpt.CompletionRequest{&#xA;&#x9;&#x9;Model:     gogpt.GPT3Ada,&#xA;&#x9;&#x9;MaxTokens: 5,&#xA;&#x9;&#x9;Prompt:    &#34;Lorem ipsum&#34;,&#xA;&#x9;&#x9;Stream:    true,&#xA;&#x9;}&#xA;&#x9;stream, err := c.CreateCompletionStream(ctx, req)&#xA;&#x9;if err != nil {&#xA;&#x9;&#x9;return&#xA;&#x9;}&#xA;&#x9;defer stream.Close()&#xA;&#xA;&#x9;for {&#xA;&#x9;&#x9;response, err := stream.Recv()&#xA;&#x9;&#x9;if errors.Is(err, io.EOF) {&#xA;&#x9;&#x9;&#x9;fmt.Println(&#34;Stream finished&#34;)&#xA;&#x9;&#x9;&#x9;return&#xA;&#x9;&#x9;}&#xA;&#xA;&#x9;&#x9;if err != nil {&#xA;&#x9;&#x9;&#x9;fmt.Printf(&#34;Stream error: %v\n&#34;, err)&#xA;&#x9;&#x9;&#x9;return&#xA;&#x9;&#x9;}&#xA;&#xA;&#xA;&#x9;&#x9;fmt.Printf(&#34;Stream response: %v\n&#34;, response)&#xA;&#x9;}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>