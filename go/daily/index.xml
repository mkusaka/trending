<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Go Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-12-28T01:30:33Z</updated>
  <subtitle>Daily Trending of Go in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>cashapp/cloner</title>
    <updated>2024-12-28T01:30:33Z</updated>
    <id>tag:github.com,2024-12-28:/cashapp/cloner</id>
    <link href="https://github.com/cashapp/cloner" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Cloner&lt;/h1&gt; &#xA;&lt;p&gt;Cloner is a Golang app that clones a database to another database. It supports:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Best effort cloning&lt;/li&gt; &#xA; &lt;li&gt;Replication (including heartbeating and safe parallelism)&lt;/li&gt; &#xA; &lt;li&gt;Checksumming&lt;/li&gt; &#xA; &lt;li&gt;Consistent cloning&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;None of these algorithms require stopping replication.&lt;/p&gt; &#xA;&lt;h2&gt;Best effort cloning&lt;/h2&gt; &#xA;&lt;p&gt;Best effort cloning performs a diffing clone (&#34;repair&#34;) like this:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Chunk up each table in roughly equally sized parts. &lt;a href=&#34;https://github.com/cashapp/cloner/raw/master/pkg/clone/chunker.go&#34;&gt;See code for more detail.&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Diff the table to generate a bunch of diffs. &lt;a href=&#34;https://github.com/cashapp/cloner/raw/master/pkg/clone/differ.go&#34;&gt;See code for more detail.&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Batch up the diffs by type (inserts, updates or deletes)&lt;/li&gt; &#xA; &lt;li&gt;Send off the batches to writers&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Writers and differs run in parallel in a pool so that longer tables are diffed and written in parallel.&lt;/p&gt; &#xA;&lt;h2&gt;Replication&lt;/h2&gt; &#xA;&lt;p&gt;Reads the MySQL binlog and applies to the target.&lt;/p&gt; &#xA;&lt;p&gt;Writes checkpoints to a checkpoint table in the target. Restarts from the checkpoint if present.&lt;/p&gt; &#xA;&lt;p&gt;Does not currently support DDL. If you need to do DDL then stop replication and delete the checkpoint row, run the DDL, then restart replication and run another consistent clone to repair.&lt;/p&gt; &#xA;&lt;h2&gt;Checksumming&lt;/h2&gt; &#xA;&lt;p&gt;We divide each table into chunks as in cloning above. Then we load each chunk from source and target and compare and report any differences.&lt;/p&gt; &#xA;&lt;p&gt;This will only work if both source and target are synchronized and &#34;frozen in time&#34; (i.e. no writes). This tool can be used to verify replication integrity if there is replication running between the source and the target without any freezing in time (stopping replication).&lt;/p&gt; &#xA;&lt;p&gt;If there is a difference between two chunks it can mean two things: 1) There is data corruption in that chunk or, 2) there is replication lag for that chunk&lt;/p&gt; &#xA;&lt;p&gt;To differentiate between these two possibilities we simply re-load the chunk data and compare again after a fixed amount of time. If there is replication lag for that chunk it should generally resolve after a few retries. If not, it&#39;s likely there is data corruption.&lt;/p&gt; &#xA;&lt;h2&gt;End to end replication lag (&#34;heartbeat&#34;)&lt;/h2&gt; &#xA;&lt;p&gt;Writes to a heartbeat table and then read the heartbeat table from the source. This determines real end to end replication lag (with the heartbeat period as resolution). It&#39;s published as a Prometheus metric.&lt;/p&gt; &#xA;&lt;h2&gt;Consistent clone&lt;/h2&gt; &#xA;&lt;p&gt;Consistent cloning uses Netflix DBLog algorithm as presented in this paper: &lt;a href=&#34;https://arxiv.org/pdf/2010.12597.pdf&#34;&gt;https://arxiv.org/pdf/2010.12597.pdf&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;In summary the consistent cloning algorithm works by this:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Start replication&lt;/li&gt; &#xA; &lt;li&gt;Chunk each table as above&lt;/li&gt; &#xA; &lt;li&gt;For each chunk: write a low watermark to a watermark table in the source, then read the chunk rows then write a high watermark.&lt;/li&gt; &#xA; &lt;li&gt;When the replication loop encounters a low watermark it starts reconciling any binlog events inside the chunk with the in memory result set of the chunk. (Replaces rows that are updated or inserted and deletes rows that are deleted.)&lt;/li&gt; &#xA; &lt;li&gt;When the replication loop encounters a high watermark the chunk is now strongly consistent with the source and is diffed and written (as described above).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;It needs write access to the source to be able to write to the watermark table.&lt;/p&gt; &#xA;&lt;h2&gt;Parallel replication&lt;/h2&gt; &#xA;&lt;p&gt;Apples transactions in parallel unless they are causal. Transactions A and B are causal iff 1) A happens before transaction B in the global ordering and 2) the set of primary keys they write to overlap.&lt;/p&gt; &#xA;&lt;p&gt;The primary key set of a chunk &#34;repair&#34; (i.e. diffing/writing of a chunk at the point of the high watermark) is the full primary key set of that chunk, i.e. a chunk repair never runs in parallel with any other transactions inside that chunk.&lt;/p&gt; &#xA;&lt;p&gt;Rough algorithm:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Gather transactions until we have N transactions or a configurable amount of time passes&lt;/li&gt; &#xA; &lt;li&gt;Calculate causal chains of all transactions which produces a set of transaction sequences&lt;/li&gt; &#xA; &lt;li&gt;Apply all transaction sequences in parallel, once all have been applied write the last replication position to the checkpoint table in the target&lt;/li&gt; &#xA; &lt;li&gt;Repeat from 1 in parallel with 3 but don&#39;t start applying transactions until the previous transactions have completed and the checkpoint table has been written to&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Parallel replication can encounter partial failure as in some transactions may be written before the checkpoint table is written to. In this case when replication starts over again some transactions are re-applied. Fortunately the way we apply a transaction is idempotent.&lt;/p&gt; &#xA;&lt;h2&gt;Sharded cloning&lt;/h2&gt; &#xA;&lt;p&gt;Cloner supports merging sharded databases for all the algorithms above. We filter the target side query by shard using a configurable where clause so we can clone/checksum a single shard at the time without deleting a bunch of out-of-shard data.&lt;/p&gt; &#xA;&lt;h2&gt;Tutorial&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/cashapp/cloner/main/docs/tutorial.md&#34;&gt;tutorial&lt;/a&gt; for more details.&lt;/p&gt;</summary>
  </entry>
</feed>