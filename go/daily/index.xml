<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Go Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-02-10T01:36:35Z</updated>
  <subtitle>Daily Trending of Go in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>OffchainLabs/nitro</title>
    <updated>2023-02-10T01:36:35Z</updated>
    <id>tag:github.com,2023-02-10:/OffchainLabs/nitro</id>
    <link href="https://github.com/OffchainLabs/nitro" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Nitro goes vroom and fixes everything&lt;/p&gt;&lt;hr&gt;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://arbitrum.io/&#34;&gt; &lt;img src=&#34;https://arbitrum.io/wp-content/uploads/2021/08/Arbitrum_Symbol-Full-color-White-background-768x840.png&#34; alt=&#34;Logo&#34; width=&#34;80&#34; height=&#34;80&#34;&gt; &lt;/a&gt; &lt;/p&gt;&#xA;&lt;h3 align=&#34;center&#34;&gt;Arbitrum Nitro&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://developer.arbitrum.io/&#34;&gt;&lt;strong&gt;Next Generation Ethereum L2 Technology »&lt;/strong&gt;&lt;/a&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h2&gt;About Arbitrum Nitro&lt;/h2&gt; &#xA;&lt;img src=&#34;https://arbitrum.io/wp-content/uploads/2021/08/Arbitrum_Symbol-Full-color-White-background-768x840.png&#34; alt=&#34;Logo&#34; width=&#34;80&#34; height=&#34;80&#34;&gt; &#xA;&lt;p&gt;Nitro is the latest iteration of the Arbitrum technology. It is a fully integrated, complete layer 2 optimistic rollup system, including fraud proofs, the sequencer, the token bridges, advanced calldata compression, and more.&lt;/p&gt; &#xA;&lt;p&gt;See the live docs-site &lt;a href=&#34;https://developer.arbitrum.io/&#34;&gt;here&lt;/a&gt; (or &lt;a href=&#34;https://github.com/OffchainLabs/arbitrum-docs&#34;&gt;here&lt;/a&gt; for markdown docs source.)&lt;/p&gt; &#xA;&lt;p&gt;The Nitro stack is built on several innovations. At its core is a new prover, which can do Arbitrum’s classic interactive fraud proofs over WASM code. That means the L2 Arbitrum engine can be written and compiled using standard languages and tools, replacing the custom-designed language and compiler used in previous Arbitrum versions. In normal execution, validators and nodes run the Nitro engine compiled to native code, switching to WASM if a fraud proof is needed. We compile the core of Geth, the EVM engine that practically defines the Ethereum standard, right into Arbitrum. So the previous custom-built EVM emulator is replaced by Geth, the most popular and well-supported Ethereum client.&lt;/p&gt; &#xA;&lt;p&gt;The last piece of the stack is a slimmed-down version of our ArbOS component, rewritten in Go, which provides the rest of what’s needed to run an L2 chain: things like cross-chain communication, and a new and improved batching and compression system to minimize L1 costs.&lt;/p&gt; &#xA;&lt;p&gt;Essentially, Nitro runs Geth at layer 2 on top of Ethereum, and can prove fraud over the core engine of Geth compiled to WASM.&lt;/p&gt; &#xA;&lt;p&gt;Arbitrum One successfully migrated from the Classic Arbitrum stack onto Nitro on 8/31/22. (See &lt;a href=&#34;https://developer.arbitrum.io/migration/state-migration&#34;&gt;state migration&lt;/a&gt; and &lt;a href=&#34;https://developer.arbitrum.io/migration/dapp_migration&#34;&gt;dapp migration&lt;/a&gt; for more info).&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;We currently have Nitro &lt;a href=&#34;https://raw.githubusercontent.com/OffchainLabs/nitro/master/LICENSE&#34;&gt;licensed&lt;/a&gt; under a Business Source License, similar to our friends at Uniswap and Aave, with an &#34;Additional Use Grant&#34; to ensure that everyone can have full comfort using and running nodes on all public Arbitrum chains.&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;Discord - &lt;a href=&#34;https://discord.com/invite/5KE54JwyTs&#34;&gt;Arbitrum&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Twitter: &lt;a href=&#34;https://twitter.com/arbitrum&#34;&gt;Arbitrum&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>bytemate/larkgpt</title>
    <updated>2023-02-10T01:36:35Z</updated>
    <id>tag:github.com,2023-02-10:/bytemate/larkgpt</id>
    <link href="https://github.com/bytemate/larkgpt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Lark chatgpt bot&lt;/p&gt;&lt;hr&gt;</summary>
  </entry>
  <entry>
    <title>telepresenceio/telepresence</title>
    <updated>2023-02-10T01:36:35Z</updated>
    <id>tag:github.com,2023-02-10:/telepresenceio/telepresence</id>
    <link href="https://github.com/telepresenceio/telepresence" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Local development against a remote Kubernetes or OpenShift cluster&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Telepresence 2: fast, efficient local development for Kubernetes microservices&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://cncf-branding.netlify.app/img/projects/telepresence/horizontal/color/telepresence-horizontal-color.png&#34;&gt;&lt;img src=&#34;https://cncf-branding.netlify.app/img/projects/telepresence/horizontal/color/telepresence-horizontal-color.png&#34; width=&#34;80&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Telepresence gives developers infinite scale development environments for Kubernetes.&lt;/p&gt; &#xA;&lt;p&gt;Website: &lt;a href=&#34;https://www.getambassador.io/products/telepresence/&#34;&gt;https://www.getambassador.io/products/telepresence/&lt;/a&gt;&lt;br&gt; Slack: &lt;a href=&#34;https://datawire-oss.slack.com/signup#/domain-signup&#34;&gt;Discuss&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;With Telepresence:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You run one service locally, using your favorite IDE and other tools&lt;/li&gt; &#xA; &lt;li&gt;You run the rest of your application in the &lt;a href=&#34;https://www.getambassador.io/products/ambassador-cloud/&#34;&gt;cloud&lt;/a&gt;, where there is unlimited memory and compute&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;This gives developers:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A fast local dev loop, with no waiting for a container build / push / deploy&lt;/li&gt; &#xA; &lt;li&gt;Ability to use their favorite local tools (IDE, debugger, etc.)&lt;/li&gt; &#xA; &lt;li&gt;Ability to run large-scale applications that can&#39;t run locally&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;A few quick ways to start using Telepresence&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Telepresence Quick Start:&lt;/strong&gt; &lt;a href=&#34;https://www.getambassador.io/docs/telepresence/latest/quick-start/&#34;&gt;Quick Start&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Install Telepresence:&lt;/strong&gt; &lt;a href=&#34;https://www.getambassador.io/docs/telepresence/latest/install/&#34;&gt;Install&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Contributor&#39;s Guide:&lt;/strong&gt; &lt;a href=&#34;https://github.com/telepresenceio/telepresence/raw/release/v2/DEVELOPING.md&#34;&gt;Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Meetings:&lt;/strong&gt; Check out our community &lt;a href=&#34;https://github.com/telepresenceio/telepresence/raw/release/v2/MEETING_SCHEDULE.md&#34;&gt;meeting schedule&lt;/a&gt; for opportunities to interact with Telepresence developers&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Telepresence documentation is available on the Ambassador Labs webside:&lt;br&gt; &lt;a href=&#34;https://www.getambassador.io/docs/telepresence/&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Telepresence 2&lt;/h2&gt; &#xA;&lt;p&gt;Telepresence 2 is based on learnings from the original Telepresence architecture. Rewritten in Go, Telepresence 2 provides a simpler and more powerful user experience, improved performance, and better reliability than Telepresence 1. More details on Telepresence 2 are below.&lt;/p&gt; &#xA;&lt;h2&gt;Walkthrough&lt;/h2&gt; &#xA;&lt;h3&gt;Install an interceptable service:&lt;/h3&gt; &#xA;&lt;p&gt;Start with an empty cluster:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ kubectl create deploy hello --image=k8s.gcr.io/echoserver:1.4&#xA;deployment.apps/hello created&#xA;$ kubectl expose deploy hello --port 80 --target-port 8080&#xA;service/hello exposed&#xA;$ kubectl get ns,svc,deploy,po&#xA;NAME                        STATUS   AGE&#xA;namespace/kube-system       Active   53m&#xA;namespace/default           Active   53m&#xA;namespace/kube-public       Active   53m&#xA;namespace/kube-node-lease   Active   53m&#xA;&#xA;NAME                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE&#xA;service/kubernetes   ClusterIP   10.43.0.1      &amp;lt;none&amp;gt;        443/TCP   53m&#xA;service/hello        ClusterIP   10.43.73.112   &amp;lt;none&amp;gt;        80/TCP    2m&#xA;&#xA;NAME                    READY   UP-TO-DATE   AVAILABLE   AGE&#xA;deployment.apps/hello   1/1     1            1           2m&#xA;&#xA;NAME                        READY   STATUS    RESTARTS   AGE&#xA;pod/hello-9954f98bf-6p2k9   1/1     Running   0          2m15s&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Check telepresence version&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ telepresence version&#xA;Client: v2.6.7 (api v3)&#xA;Root Daemon: v2.6.7 (api v3)&#xA;User Daemon: v2.6.7 (api v3)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Establish a connection to the cluster (outbound traffic)&lt;/h3&gt; &#xA;&lt;p&gt;Let telepresence connect:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ telepresence connect&#xA;Launching Telepresence Root Daemon&#xA;Launching Telepresence User Daemon&#xA;Connected to context default (https://35.232.104.64)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A session is now active and outbound connections will be routed to the cluster. I.e. your laptop is &#34;inside&#34; the cluster.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ curl hello.default&#xA;CLIENT VALUES:&#xA;client_address=10.42.0.189&#xA;command=GET&#xA;real path=/&#xA;query=nil&#xA;request_version=1.1&#xA;request_uri=http://hello.default:8080/&#xA;&#xA;SERVER VALUES:&#xA;server_version=nginx: 1.10.0 - lua: 10001&#xA;&#xA;HEADERS RECEIVED:&#xA;accept=*/*&#xA;host=hello.default&#xA;user-agent=curl/7.79.1&#xA;BODY:&#xA;-no body in request-&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Intercept the service. I.e. redirect traffic to it to our laptop (inbound traffic)&lt;/h3&gt; &#xA;&lt;p&gt;Add an intercept for the hello deployment on port 9000. Here, we also start a service listening on that port:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ telepresence intercept hello --port 9000 -- python3 -m http.server 9000&#xA;Using Deployment hello&#xA;intercepted&#xA;    Intercept name         : hello&#xA;    State                  : ACTIVE&#xA;    Workload kind          : Deployment&#xA;    Destination            : 127.0.0.1:9000&#xA;    Service Port Identifier: 80&#xA;    Volume Mount Point     : /tmp/telfs-524630891&#xA;    Intercepting           : all TCP requests&#xA;Serving HTTP on 0.0.0.0 port 9000 (http://0.0.0.0:9000/) ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;python -m httpserver&lt;/code&gt; is now started on port 9000 and will run until terminated by &lt;code&gt;&amp;lt;ctrl&amp;gt;-C&lt;/code&gt;. Access it from a browser using &lt;code&gt;http://hello/&lt;/code&gt; or use curl from another terminal. With curl, it presents a html listing from the directory where the server was started. Something like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ curl hello&#xA;&amp;lt;!DOCTYPE HTML PUBLIC &#34;-//W3C//DTD HTML 4.01//EN&#34; &#34;http://www.w3.org/TR/html4/strict.dtd&#34;&amp;gt;&#xA;&amp;lt;html&amp;gt;&#xA;&amp;lt;head&amp;gt;&#xA;&amp;lt;meta http-equiv=&#34;Content-Type&#34; content=&#34;text/html; charset=utf-8&#34;&amp;gt;&#xA;&amp;lt;title&amp;gt;Directory listing for /&amp;lt;/title&amp;gt;&#xA;&amp;lt;/head&amp;gt;&#xA;&amp;lt;body&amp;gt;&#xA;&amp;lt;h1&amp;gt;Directory listing for /&amp;lt;/h1&amp;gt;&#xA;&amp;lt;hr&amp;gt;&#xA;&amp;lt;ul&amp;gt;&#xA;&amp;lt;li&amp;gt;&amp;lt;a href=&#34;file1.txt&#34;&amp;gt;file1.txt&amp;lt;/a&amp;gt;&amp;lt;/li&amp;gt;&#xA;&amp;lt;li&amp;gt;&amp;lt;a href=&#34;file2.txt&#34;&amp;gt;file2.txt&amp;lt;/a&amp;gt;&amp;lt;/li&amp;gt;&#xA;&amp;lt;/ul&amp;gt;&#xA;&amp;lt;hr&amp;gt;&#xA;&amp;lt;/body&amp;gt;&#xA;&amp;lt;/html&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Observe that the python service reports that it&#39;s being accessed:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;127.0.0.1 - - [16/Jun/2022 11:39:20] &#34;GET / HTTP/1.1&#34; 200 -&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Since telepresence is now intercepting services in the default namespace, all services in that namespace can now be reached directly by their name. You can of course still use the namespaced name too, e.g. &lt;code&gt;curl hello.default&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Clean-up and close daemon processes&lt;/h3&gt; &#xA;&lt;p&gt;End the service with &lt;code&gt;&amp;lt;ctrl&amp;gt;-C&lt;/code&gt; and then try &lt;code&gt;curl hello.default&lt;/code&gt; or &lt;code&gt;http://hello.default&lt;/code&gt; again. The intercept is gone, and the echo service responds as normal. Using just &lt;code&gt;curl hello&lt;/code&gt; will no longer succeed. This is because telepresence stopped mapping the default namespace when there were no more intercepts using it.&lt;/p&gt; &#xA;&lt;p&gt;Now end the session too. Your desktop no longer has access to the cluster internals.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ telepresence quit&#xA;Telepresence Network disconnecting...done&#xA;Telepresence Traffic Manager disconnecting...done&#xA;$ curl hello.default&#xA;curl: (6) Could not resolve host: hello.default&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The telepresence daemons are still running in the background, which is harmless. You&#39;ll need to stop them before you upgrade telepresence. That&#39;s done by passing the options &lt;code&gt;-u&lt;/code&gt; (stop user daemon) and &lt;code&gt;-r&lt;/code&gt; (stop root daemon) to the quit command.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ telepresence quit -ur&#xA;Telepresence Network quitting...done&#xA;Telepresence Traffic Manager quitting...done&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;What got installed in the cluster?&lt;/h3&gt; &#xA;&lt;p&gt;Telepresence installs the Traffic Manager in your cluster if it is not already present. This deployment remains unless you uninstall it.&lt;/p&gt; &#xA;&lt;p&gt;Telepresence injects the Traffic Agent as an additional container into the pods of the workload you intercept, and will optionally install an init-container to route traffic through the agent (the init-container is only injected when the service is headless or uses a numerical &lt;code&gt;targetPort&lt;/code&gt;). The modifications persist unless you uninstall them.&lt;/p&gt; &#xA;&lt;p&gt;At first glance, we can see that the deployment is installed ...&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ kubectl get svc,deploy,pod&#xA;service/kubernetes   ClusterIP   10.43.0.1       &amp;lt;none&amp;gt;        443/TCP                      7d22h&#xA;service/hello        ClusterIP   10.43.145.57    &amp;lt;none&amp;gt;        80/TCP                       13m&#xA;&#xA;NAME                    READY   UP-TO-DATE   AVAILABLE   AGE&#xA;deployment.apps/hello   1/1     1            1           13m&#xA;&#xA;NAME                         READY   STATUS    RESTARTS        AGE&#xA;pod/hello-774455b6f5-6x6vs   2/2     Running   0               10m&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;... and that the traffic-manager is installed in the &#34;ambassador&#34; namespace.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ kubectl -n ambassador get svc,deploy,pod&#xA;NAME                      TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE&#xA;service/traffic-manager   ClusterIP   None           &amp;lt;none&amp;gt;        8081/TCP   17m&#xA;service/agent-injector    ClusterIP   10.43.72.154   &amp;lt;none&amp;gt;        443/TCP    17m&#xA;&#xA;NAME                              READY   UP-TO-DATE   AVAILABLE   AGE&#xA;deployment.apps/traffic-manager   1/1     1            1           17m&#xA;&#xA;NAME                                  READY   STATUS    RESTARTS   AGE&#xA;pod/traffic-manager-dcd4cc64f-6v5bp   1/1     Running   0          17m&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The traffic-agent is installed too, in the hello pod. Here together with an init-container, because the service is using a numerical &lt;code&gt;targetPort&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ kubectl describe pod hello-774455b6f5-6x6vs &#xA;Name:         hello-774455b6f5-6x6vs&#xA;Namespace:    default&#xA;Priority:     0&#xA;Node:         multi/192.168.1.110&#xA;Start Time:   Thu, 16 Jun 2022 11:38:22 +0200&#xA;Labels:       app=hello&#xA;              pod-template-hash=774455b6f5&#xA;Annotations:  telepresence.getambassador.io/inject-traffic-agent: enabled&#xA;              telepresence.getambassador.io/restartedAt: 2022-06-16T09:38:21Z&#xA;Status:       Running&#xA;IP:           10.42.0.191&#xA;IPs:&#xA;  IP:           10.42.0.191&#xA;Controlled By:  ReplicaSet/hello-774455b6f5&#xA;Init Containers:&#xA;  tel-agent-init:&#xA;    Container ID:  containerd://e968352b3d85d6f966ac55f02da2401f93935f6df1f087b06bbe1cfc8854d5fb&#xA;    Image:         docker.io/datawire/ambassador-telepresence-agent:1.12.6&#xA;    Image ID:      docker.io/datawire/ambassador-telepresence-agent@sha256:2652d2767d1e8968be3fb22f365747315e25ac95e12c3d39f1206080a1e66af3&#xA;    Port:          &amp;lt;none&amp;gt;&#xA;    Host Port:     &amp;lt;none&amp;gt;&#xA;    Args:&#xA;      agent-init&#xA;    State:          Terminated&#xA;      Reason:       Completed&#xA;      Exit Code:    0&#xA;      Started:      Thu, 16 Jun 2022 11:38:39 +0200&#xA;      Finished:     Thu, 16 Jun 2022 11:38:39 +0200&#xA;    Ready:          True&#xA;    Restart Count:  0&#xA;    Environment:    &amp;lt;none&amp;gt;&#xA;    Mounts:&#xA;      /etc/traffic-agent from traffic-config (rw)&#xA;      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-wzhhs (ro)&#xA;Containers:&#xA;  echoserver:&#xA;    Container ID:   containerd://80d4645769a06b8671b5a4ce29d28abfa72ce5659ba96916c231bb9629593a29&#xA;    Image:          k8s.gcr.io/echoserver:1.4&#xA;    Image ID:       sha256:523cad1a4df732d41406c9de49f932cd60d56ffd50619158a2977fd1066028f9&#xA;    Port:           &amp;lt;none&amp;gt;&#xA;    Host Port:      &amp;lt;none&amp;gt;&#xA;    State:          Running&#xA;      Started:      Thu, 16 Jun 2022 11:38:40 +0200&#xA;    Ready:          True&#xA;    Restart Count:  0&#xA;    Environment:    &amp;lt;none&amp;gt;&#xA;    Mounts:&#xA;      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-wzhhs (ro)&#xA;  traffic-agent:&#xA;    Container ID:  containerd://ef3605a60f7c02229f156e3dc0e99f9b055fba1037587513871e64180670d0a4&#xA;    Image:         docker.io/datawire/ambassador-telepresence-agent:1.12.6&#xA;    Image ID:      docker.io/datawire/ambassador-telepresence-agent@sha256:2652d2767d1e8968be3fb22f365747315e25ac95e12c3d39f1206080a1e66af3&#xA;    Port:          9900/TCP&#xA;    Host Port:     0/TCP&#xA;    Args:&#xA;      agent&#xA;    State:          Running&#xA;      Started:      Thu, 16 Jun 2022 11:38:41 +0200&#xA;    Ready:          True&#xA;    Restart Count:  0&#xA;    Readiness:      exec [/bin/stat /tmp/agent/ready] delay=0s timeout=1s period=10s #success=1 #failure=3&#xA;    Environment:&#xA;      _TEL_AGENT_POD_IP:   (v1:status.podIP)&#xA;      _TEL_AGENT_NAME:    hello-774455b6f5-6x6vs (v1:metadata.name)&#xA;    Mounts:&#xA;      /etc/traffic-agent from traffic-config (rw)&#xA;      /tel_app_exports from export-volume (rw)&#xA;      /tel_pod_info from traffic-annotations (rw)&#xA;      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-wzhhs (ro)&#xA;Conditions:&#xA;  Type              Status&#xA;  Initialized       True &#xA;  Ready             True &#xA;  ContainersReady   True &#xA;  PodScheduled      True &#xA;Volumes:&#xA;  kube-api-access-wzhhs:&#xA;    Type:                    Projected (a volume that contains injected data from multiple sources)&#xA;    TokenExpirationSeconds:  3607&#xA;    ConfigMapName:           kube-root-ca.crt&#xA;    ConfigMapOptional:       &amp;lt;nil&amp;gt;&#xA;    DownwardAPI:             true&#xA;  traffic-annotations:&#xA;    Type:  DownwardAPI (a volume populated by information about the pod)&#xA;    Items:&#xA;      metadata.annotations -&amp;gt; annotations&#xA;  traffic-config:&#xA;    Type:      ConfigMap (a volume populated by a ConfigMap)&#xA;    Name:      telepresence-agents&#xA;    Optional:  false&#xA;  export-volume:&#xA;    Type:        EmptyDir (a temporary directory that shares a pod&#39;s lifetime)&#xA;    Medium:      &#xA;    SizeLimit:   &amp;lt;unset&amp;gt;&#xA;QoS Class:       BestEffort&#xA;Node-Selectors:  &amp;lt;none&amp;gt;&#xA;Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s&#xA;                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s&#xA;Events:&#xA;  Type    Reason     Age   From               Message&#xA;  ----    ------     ----  ----               -------&#xA;  Normal  Scheduled  13m   default-scheduler  Successfully assigned default/hello-774455b6f5-6x6vs to multi&#xA;  Normal  Pulling    13m   kubelet            Pulling image &#34;docker.io/datawire/ambassador-telepresence-agent:1.12.6&#34;&#xA;  Normal  Pulled     13m   kubelet            Successfully pulled image &#34;docker.io/datawire/ambassador-telepresence-agent:1.12.6&#34; in 17.043659509s&#xA;  Normal  Created    13m   kubelet            Created container tel-agent-init&#xA;  Normal  Started    13m   kubelet            Started container tel-agent-init&#xA;  Normal  Pulled     13m   kubelet            Container image &#34;k8s.gcr.io/echoserver:1.4&#34; already present on machine&#xA;  Normal  Created    13m   kubelet            Created container echoserver&#xA;  Normal  Started    13m   kubelet            Started container echoserver&#xA;  Normal  Pulled     13m   kubelet            Container image &#34;docker.io/datawire/ambassador-telepresence-agent:1.12.6&#34; already present on machine&#xA;  Normal  Created    13m   kubelet            Created container traffic-agent&#xA;  Normal  Started    13m   kubelet            Started container traffic-agent&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Uninstalling&lt;/h3&gt; &#xA;&lt;p&gt;You can uninstall the traffic-agent from specific deployments or from all deployments. Or you can choose to uninstall everything in which case the traffic-manager and all traffic-agents will be uninstalled.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;telepresence uninstall --everything&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;will remove everything that was automatically installed by telepresence from the cluster.&lt;/p&gt; &#xA;&lt;h3&gt;Troubleshooting&lt;/h3&gt; &#xA;&lt;p&gt;The telepresence background processes &lt;code&gt;daemon&lt;/code&gt; and &lt;code&gt;connector&lt;/code&gt; both produces log files that can be very helpful when problems are encountered. The files are named &lt;code&gt;daemon.log&lt;/code&gt; and &lt;code&gt;connector.log&lt;/code&gt;. The location of the logs differ depending on what platform that is used:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;macOS &lt;code&gt;~/Library/Logs/telepresence&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Linux &lt;code&gt;~/.cache/telepresence/logs&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Windows &lt;code&gt;&#34;%USERPROFILE%\AppData\Local\logs&#34;&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Visit the troubleshooting section in the Telepresence documentation for more advice:&lt;br&gt; &lt;a href=&#34;https://www.getambassador.io/docs/telepresence/latest/troubleshooting/&#34;&gt;Troubleshooting&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>