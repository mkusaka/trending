<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Go Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-03-20T01:30:55Z</updated>
  <subtitle>Daily Trending of Go in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>mark3labs/mcphost</title>
    <updated>2025-03-20T01:30:55Z</updated>
    <id>tag:github.com,2025-03-20:/mark3labs/mcphost</id>
    <link href="https://github.com/mark3labs/mcphost" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A CLI host application that enables Large Language Models (LLMs) to interact with external tools through the Model Context Protocol (MCP).&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MCPHost ü§ñ&lt;/h1&gt; &#xA;&lt;p&gt;A CLI host application that enables Large Language Models (LLMs) to interact with external tools through the Model Context Protocol (MCP). Currently supports both Claude 3.5 Sonnet and Ollama models.&lt;/p&gt; &#xA;&lt;h2&gt;Overview üåü&lt;/h2&gt; &#xA;&lt;p&gt;MCPHost acts as a host in the MCP client-server architecture, where:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hosts&lt;/strong&gt; (like MCPHost) are LLM applications that manage connections and interactions&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Clients&lt;/strong&gt; maintain 1:1 connections with MCP servers&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Servers&lt;/strong&gt; provide context, tools, and capabilities to the LLMs&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This architecture allows language models to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Access external tools and data sources üõ†Ô∏è&lt;/li&gt; &#xA; &lt;li&gt;Maintain consistent context across interactions üîÑ&lt;/li&gt; &#xA; &lt;li&gt;Execute commands and retrieve information safely üîí&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Currently supports:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Claude 3.5 Sonnet (claude-3-5-sonnet-20240620)&lt;/li&gt; &#xA; &lt;li&gt;Any Ollama-compatible model with function calling support&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features ‚ú®&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Interactive conversations with either Claude 3.5 Sonnet or Ollama models&lt;/li&gt; &#xA; &lt;li&gt;Support for multiple concurrent MCP servers&lt;/li&gt; &#xA; &lt;li&gt;Dynamic tool discovery and integration&lt;/li&gt; &#xA; &lt;li&gt;Tool calling capabilities for both model types&lt;/li&gt; &#xA; &lt;li&gt;Configurable MCP server locations and arguments&lt;/li&gt; &#xA; &lt;li&gt;Consistent command interface across model types&lt;/li&gt; &#xA; &lt;li&gt;Configurable message history window for context management&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirements üìã&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Go 1.23 or later&lt;/li&gt; &#xA; &lt;li&gt;For Claude: An Anthropic API key&lt;/li&gt; &#xA; &lt;li&gt;For Ollama: Local Ollama installation with desired models&lt;/li&gt; &#xA; &lt;li&gt;One or more MCP-compatible tool servers&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Environment Setup üîß&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Anthropic API Key (for Claude):&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export ANTHROPIC_API_KEY=&#39;your-api-key&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Ollama Setup:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install Ollama from &lt;a href=&#34;https://ollama.ai&#34;&gt;https://ollama.ai&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Pull your desired model:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ollama pull mistral&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ensure Ollama is running:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ollama serve&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Installation üì¶&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;go install github.com/mark3labs/mcphost@latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Configuration ‚öôÔ∏è&lt;/h2&gt; &#xA;&lt;p&gt;MCPHost will automatically create a configuration file at &lt;code&gt;~/.mcp.json&lt;/code&gt; if it doesn&#39;t exist. You can also specify a custom location using the &lt;code&gt;--config&lt;/code&gt; flag:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;mcpServers&#34;: {&#xA;    &#34;sqlite&#34;: {&#xA;      &#34;command&#34;: &#34;uvx&#34;,&#xA;      &#34;args&#34;: [&#xA;        &#34;mcp-server-sqlite&#34;,&#xA;        &#34;--db-path&#34;,&#xA;        &#34;/tmp/foo.db&#34;&#xA;      ]&#xA;    },&#xA;    &#34;filesystem&#34;: {&#xA;      &#34;command&#34;: &#34;npx&#34;,&#xA;      &#34;args&#34;: [&#xA;        &#34;-y&#34;,&#xA;        &#34;@modelcontextprotocol/server-filesystem&#34;,&#xA;        &#34;/tmp&#34;&#xA;      ]&#xA;    }&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Each MCP server entry requires:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;command&lt;/code&gt;: The command to run (e.g., &lt;code&gt;uvx&lt;/code&gt;, &lt;code&gt;npx&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;args&lt;/code&gt;: Array of arguments for the command: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;For SQLite server: &lt;code&gt;mcp-server-sqlite&lt;/code&gt; with database path&lt;/li&gt; &#xA;   &lt;li&gt;For filesystem server: &lt;code&gt;@modelcontextprotocol/server-filesystem&lt;/code&gt; with directory path&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage üöÄ&lt;/h2&gt; &#xA;&lt;p&gt;MCPHost is a CLI tool that allows you to interact with various AI models through a unified interface. It supports various tools through MCP servers.&lt;/p&gt; &#xA;&lt;h3&gt;Available Models&lt;/h3&gt; &#xA;&lt;p&gt;Models can be specified using the &lt;code&gt;--model&lt;/code&gt; (&lt;code&gt;-m&lt;/code&gt;) flag:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Anthropic Claude (default): &lt;code&gt;anthropic:claude-3-5-sonnet-latest&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;OpenAI: &lt;code&gt;openai:gpt-4&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Ollama models: &lt;code&gt;ollama:modelname&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Use Ollama with Qwen model&#xA;mcphost -m ollama:qwen2.5:3b&#xA;&#xA;# Use OpenAI&#39;s GPT-4&#xA;mcphost -m openai:gpt-4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Flags&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--anthropic-url string&lt;/code&gt;: Base URL for Anthropic API (defaults to api.anthropic.com)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--anthropic-api-key string&lt;/code&gt;: Anthropic API key (can also be set via ANTHROPIC_API_KEY environment variable)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--config string&lt;/code&gt;: Config file location (default is $HOME/mcp.json)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--debug&lt;/code&gt;: Enable debug logging&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--message-window int&lt;/code&gt;: Number of messages to keep in context (default: 10)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;-m, --model string&lt;/code&gt;: Model to use (format: provider:model) (default &#34;anthropic:claude-3-5-sonnet-latest&#34;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--openai-url string&lt;/code&gt;: Base URL for OpenAI API (defaults to api.openai.com)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--openai-api-key string&lt;/code&gt;: OpenAI API key (can also be set via OPENAI_API_KEY environment variable)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Interactive Commands&lt;/h3&gt; &#xA;&lt;p&gt;While chatting, you can use:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;/help&lt;/code&gt;: Show available commands&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/tools&lt;/code&gt;: List all available tools&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/servers&lt;/code&gt;: List configured MCP servers&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/history&lt;/code&gt;: Display conversation history&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/quit&lt;/code&gt;: Exit the application&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Ctrl+C&lt;/code&gt;: Exit at any time&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Global Flags&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--config&lt;/code&gt;: Specify custom config file location&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--message-window&lt;/code&gt;: Set number of messages to keep in context (default: 10)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;MCP Server Compatibility üîå&lt;/h2&gt; &#xA;&lt;p&gt;MCPHost can work with any MCP-compliant server. For examples and reference implementations, see the &lt;a href=&#34;https://github.com/modelcontextprotocol/servers&#34;&gt;MCP Servers Repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing ü§ù&lt;/h2&gt; &#xA;&lt;p&gt;Contributions are welcome! Feel free to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Submit bug reports or feature requests through issues&lt;/li&gt; &#xA; &lt;li&gt;Create pull requests for improvements&lt;/li&gt; &#xA; &lt;li&gt;Share your custom MCP servers&lt;/li&gt; &#xA; &lt;li&gt;Improve documentation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please ensure your contributions follow good coding practices and include appropriate tests.&lt;/p&gt; &#xA;&lt;h2&gt;License üìÑ&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the MIT License - see the &lt;a href=&#34;https://raw.githubusercontent.com/mark3labs/mcphost/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgments üôè&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Thanks to the Anthropic team for Claude and the MCP specification&lt;/li&gt; &#xA; &lt;li&gt;Thanks to the Ollama team for their local LLM runtime&lt;/li&gt; &#xA; &lt;li&gt;Thanks to all contributors who have helped improve this tool&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>