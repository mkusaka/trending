<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Go Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-09-30T01:38:31Z</updated>
  <subtitle>Daily Trending of Go in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>segmentio/kafka-go</title>
    <updated>2022-09-30T01:38:31Z</updated>
    <id>tag:github.com,2022-09-30:/segmentio/kafka-go</id>
    <link href="https://github.com/segmentio/kafka-go" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Kafka library in Go&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;kafka-go &lt;a href=&#34;https://circleci.com/gh/segmentio/kafka-go&#34;&gt;&lt;img src=&#34;https://circleci.com/gh/segmentio/kafka-go.svg?style=shield&#34; alt=&#34;CircleCI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://goreportcard.com/report/github.com/segmentio/kafka-go&#34;&gt;&lt;img src=&#34;https://goreportcard.com/badge/github.com/segmentio/kafka-go&#34; alt=&#34;Go Report Card&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://godoc.org/github.com/segmentio/kafka-go&#34;&gt;&lt;img src=&#34;https://godoc.org/github.com/segmentio/kafka-go?status.svg?sanitize=true&#34; alt=&#34;GoDoc&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;h2&gt;Motivations&lt;/h2&gt; &#xA;&lt;p&gt;We rely on both Go and Kafka a lot at Segment. Unfortunately, the state of the Go client libraries for Kafka at the time of this writing was not ideal. The available options were:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/Shopify/sarama&#34;&gt;sarama&lt;/a&gt;, which is by far the most popular but is quite difficult to work with. It is poorly documented, the API exposes low level concepts of the Kafka protocol, and it doesn&#39;t support recent Go features like &lt;a href=&#34;https://golang.org/pkg/context/&#34;&gt;contexts&lt;/a&gt;. It also passes all values as pointers which causes large numbers of dynamic memory allocations, more frequent garbage collections, and higher memory usage.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/confluentinc/confluent-kafka-go&#34;&gt;confluent-kafka-go&lt;/a&gt; is a cgo based wrapper around &lt;a href=&#34;https://github.com/edenhill/librdkafka&#34;&gt;librdkafka&lt;/a&gt;, which means it introduces a dependency to a C library on all Go code that uses the package. It has much better documentation than sarama but still lacks support for Go contexts.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/lovoo/goka&#34;&gt;goka&lt;/a&gt; is a more recent Kafka client for Go which focuses on a specific usage pattern. It provides abstractions for using Kafka as a message passing bus between services rather than an ordered log of events, but this is not the typical use case of Kafka for us at Segment. The package also depends on sarama for all interactions with Kafka.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This is where &lt;code&gt;kafka-go&lt;/code&gt; comes into play. It provides both low and high level APIs for interacting with Kafka, mirroring concepts and implementing interfaces of the Go standard library to make it easy to use and integrate with existing software.&lt;/p&gt; &#xA;&lt;h4&gt;Note:&lt;/h4&gt; &#xA;&lt;p&gt;In order to better align with our newly adopted Code of Conduct, the kafka-go project has renamed our default branch to &lt;code&gt;main&lt;/code&gt;. For the full details of our Code Of Conduct see &lt;a href=&#34;https://raw.githubusercontent.com/segmentio/kafka-go/main/CODE_OF_CONDUCT.md&#34;&gt;this&lt;/a&gt; document.&lt;/p&gt; &#xA;&lt;h2&gt;Kafka versions&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;kafka-go&lt;/code&gt; is currently tested with Kafka versions 0.10.1.0 to 2.7.1. While it should also be compatible with later versions, newer features available in the Kafka API may not yet be implemented in the client.&lt;/p&gt; &#xA;&lt;h2&gt;Go versions&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;kafka-go&lt;/code&gt; requires Go version 1.15 or later.&lt;/p&gt; &#xA;&lt;h2&gt;Connection &lt;a href=&#34;https://godoc.org/github.com/segmentio/kafka-go#Conn&#34;&gt;&lt;img src=&#34;https://godoc.org/github.com/segmentio/kafka-go?status.svg?sanitize=true&#34; alt=&#34;GoDoc&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;Conn&lt;/code&gt; type is the core of the &lt;code&gt;kafka-go&lt;/code&gt; package. It wraps around a raw network connection to expose a low-level API to a Kafka server.&lt;/p&gt; &#xA;&lt;p&gt;Here are some examples showing typical use of a connection object:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// to produce messages&#xA;topic := &#34;my-topic&#34;&#xA;partition := 0&#xA;&#xA;conn, err := kafka.DialLeader(context.Background(), &#34;tcp&#34;, &#34;localhost:9092&#34;, topic, partition)&#xA;if err != nil {&#xA;    log.Fatal(&#34;failed to dial leader:&#34;, err)&#xA;}&#xA;&#xA;conn.SetWriteDeadline(time.Now().Add(10*time.Second))&#xA;_, err = conn.WriteMessages(&#xA;    kafka.Message{Value: []byte(&#34;one!&#34;)},&#xA;    kafka.Message{Value: []byte(&#34;two!&#34;)},&#xA;    kafka.Message{Value: []byte(&#34;three!&#34;)},&#xA;)&#xA;if err != nil {&#xA;    log.Fatal(&#34;failed to write messages:&#34;, err)&#xA;}&#xA;&#xA;if err := conn.Close(); err != nil {&#xA;    log.Fatal(&#34;failed to close writer:&#34;, err)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// to consume messages&#xA;topic := &#34;my-topic&#34;&#xA;partition := 0&#xA;&#xA;conn, err := kafka.DialLeader(context.Background(), &#34;tcp&#34;, &#34;localhost:9092&#34;, topic, partition)&#xA;if err != nil {&#xA;    log.Fatal(&#34;failed to dial leader:&#34;, err)&#xA;}&#xA;&#xA;conn.SetReadDeadline(time.Now().Add(10*time.Second))&#xA;batch := conn.ReadBatch(10e3, 1e6) // fetch 10KB min, 1MB max&#xA;&#xA;b := make([]byte, 10e3) // 10KB max per message&#xA;for {&#xA;    n, err := batch.Read(b)&#xA;    if err != nil {&#xA;        break&#xA;    }&#xA;    fmt.Println(string(b[:n]))&#xA;}&#xA;&#xA;if err := batch.Close(); err != nil {&#xA;    log.Fatal(&#34;failed to close batch:&#34;, err)&#xA;}&#xA;&#xA;if err := conn.Close(); err != nil {&#xA;    log.Fatal(&#34;failed to close connection:&#34;, err)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;To Create Topics&lt;/h3&gt; &#xA;&lt;p&gt;By default kafka has the &lt;code&gt;auto.create.topics.enable=&#39;true&#39;&lt;/code&gt; (&lt;code&gt;KAFKA_AUTO_CREATE_TOPICS_ENABLE=&#39;true&#39;&lt;/code&gt; in the wurstmeister/kafka kafka docker image). If this value is set to &lt;code&gt;&#39;true&#39;&lt;/code&gt; then topics will be created as a side effect of &lt;code&gt;kafka.DialLeader&lt;/code&gt; like so:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// to create topics when auto.create.topics.enable=&#39;true&#39;&#xA;conn, err := kafka.DialLeader(context.Background(), &#34;tcp&#34;, &#34;localhost:9092&#34;, &#34;my-topic&#34;, 0)&#xA;if err != nil {&#xA;    panic(err.Error())&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If &lt;code&gt;auto.create.topics.enable=&#39;false&#39;&lt;/code&gt; then you will need to create topics explicitly like so:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// to create topics when auto.create.topics.enable=&#39;false&#39;&#xA;topic := &#34;my-topic&#34;&#xA;&#xA;conn, err := kafka.Dial(&#34;tcp&#34;, &#34;localhost:9092&#34;)&#xA;if err != nil {&#xA;    panic(err.Error())&#xA;}&#xA;defer conn.Close()&#xA;&#xA;controller, err := conn.Controller()&#xA;if err != nil {&#xA;    panic(err.Error())&#xA;}&#xA;var controllerConn *kafka.Conn&#xA;controllerConn, err = kafka.Dial(&#34;tcp&#34;, net.JoinHostPort(controller.Host, strconv.Itoa(controller.Port)))&#xA;if err != nil {&#xA;    panic(err.Error())&#xA;}&#xA;defer controllerConn.Close()&#xA;&#xA;&#xA;topicConfigs := []kafka.TopicConfig{&#xA;    {&#xA;        Topic:             topic,&#xA;        NumPartitions:     1,&#xA;        ReplicationFactor: 1,&#xA;    },&#xA;}&#xA;&#xA;err = controllerConn.CreateTopics(topicConfigs...)&#xA;if err != nil {&#xA;    panic(err.Error())&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;To Connect To Leader Via a Non-leader Connection&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// to connect to the kafka leader via an existing non-leader connection rather than using DialLeader&#xA;conn, err := kafka.Dial(&#34;tcp&#34;, &#34;localhost:9092&#34;)&#xA;if err != nil {&#xA;    panic(err.Error())&#xA;}&#xA;defer conn.Close()&#xA;controller, err := conn.Controller()&#xA;if err != nil {&#xA;    panic(err.Error())&#xA;}&#xA;var connLeader *kafka.Conn&#xA;connLeader, err = kafka.Dial(&#34;tcp&#34;, net.JoinHostPort(controller.Host, strconv.Itoa(controller.Port)))&#xA;if err != nil {&#xA;    panic(err.Error())&#xA;}&#xA;defer connLeader.Close()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;To list topics&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;conn, err := kafka.Dial(&#34;tcp&#34;, &#34;localhost:9092&#34;)&#xA;if err != nil {&#xA;    panic(err.Error())&#xA;}&#xA;defer conn.Close()&#xA;&#xA;partitions, err := conn.ReadPartitions()&#xA;if err != nil {&#xA;    panic(err.Error())&#xA;}&#xA;&#xA;m := map[string]struct{}{}&#xA;&#xA;for _, p := range partitions {&#xA;    m[p.Topic] = struct{}{}&#xA;}&#xA;for k := range m {&#xA;    fmt.Println(k)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Because it is low level, the &lt;code&gt;Conn&lt;/code&gt; type turns out to be a great building block for higher level abstractions, like the &lt;code&gt;Reader&lt;/code&gt; for example.&lt;/p&gt; &#xA;&lt;h2&gt;Reader &lt;a href=&#34;https://godoc.org/github.com/segmentio/kafka-go#Reader&#34;&gt;&lt;img src=&#34;https://godoc.org/github.com/segmentio/kafka-go?status.svg?sanitize=true&#34; alt=&#34;GoDoc&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;A &lt;code&gt;Reader&lt;/code&gt; is another concept exposed by the &lt;code&gt;kafka-go&lt;/code&gt; package, which intends to make it simpler to implement the typical use case of consuming from a single topic-partition pair. A &lt;code&gt;Reader&lt;/code&gt; also automatically handles reconnections and offset management, and exposes an API that supports asynchronous cancellations and timeouts using Go contexts.&lt;/p&gt; &#xA;&lt;p&gt;Note that it is important to call &lt;code&gt;Close()&lt;/code&gt; on a &lt;code&gt;Reader&lt;/code&gt; when a process exits. The kafka server needs a graceful disconnect to stop it from continuing to attempt to send messages to the connected clients. The given example will not call &lt;code&gt;Close()&lt;/code&gt; if the process is terminated with SIGINT (ctrl-c at the shell) or SIGTERM (as docker stop or a kubernetes restart does). This can result in a delay when a new reader on the same topic connects (e.g. new process started or new container running). Use a &lt;code&gt;signal.Notify&lt;/code&gt; handler to close the reader on process shutdown.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// make a new reader that consumes from topic-A, partition 0, at offset 42&#xA;r := kafka.NewReader(kafka.ReaderConfig{&#xA;    Brokers:   []string{&#34;localhost:9092&#34;,&#34;localhost:9093&#34;, &#34;localhost:9094&#34;},&#xA;    Topic:     &#34;topic-A&#34;,&#xA;    Partition: 0,&#xA;    MinBytes:  10e3, // 10KB&#xA;    MaxBytes:  10e6, // 10MB&#xA;})&#xA;r.SetOffset(42)&#xA;&#xA;for {&#xA;    m, err := r.ReadMessage(context.Background())&#xA;    if err != nil {&#xA;        break&#xA;    }&#xA;    fmt.Printf(&#34;message at offset %d: %s = %s\n&#34;, m.Offset, string(m.Key), string(m.Value))&#xA;}&#xA;&#xA;if err := r.Close(); err != nil {&#xA;    log.Fatal(&#34;failed to close reader:&#34;, err)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Consumer Groups&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;kafka-go&lt;/code&gt; also supports Kafka consumer groups including broker managed offsets. To enable consumer groups, simply specify the GroupID in the ReaderConfig.&lt;/p&gt; &#xA;&lt;p&gt;ReadMessage automatically commits offsets when using consumer groups.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// make a new reader that consumes from topic-A&#xA;r := kafka.NewReader(kafka.ReaderConfig{&#xA;    Brokers:   []string{&#34;localhost:9092&#34;, &#34;localhost:9093&#34;, &#34;localhost:9094&#34;},&#xA;    GroupID:   &#34;consumer-group-id&#34;,&#xA;    Topic:     &#34;topic-A&#34;,&#xA;    MinBytes:  10e3, // 10KB&#xA;    MaxBytes:  10e6, // 10MB&#xA;})&#xA;&#xA;for {&#xA;    m, err := r.ReadMessage(context.Background())&#xA;    if err != nil {&#xA;        break&#xA;    }&#xA;    fmt.Printf(&#34;message at topic/partition/offset %v/%v/%v: %s = %s\n&#34;, m.Topic, m.Partition, m.Offset, string(m.Key), string(m.Value))&#xA;}&#xA;&#xA;if err := r.Close(); err != nil {&#xA;    log.Fatal(&#34;failed to close reader:&#34;, err)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;There are a number of limitations when using consumer groups:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;(*Reader).SetOffset&lt;/code&gt; will return an error when GroupID is set&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;(*Reader).Offset&lt;/code&gt; will always return &lt;code&gt;-1&lt;/code&gt; when GroupID is set&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;(*Reader).Lag&lt;/code&gt; will always return &lt;code&gt;-1&lt;/code&gt; when GroupID is set&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;(*Reader).ReadLag&lt;/code&gt; will return an error when GroupID is set&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;(*Reader).Stats&lt;/code&gt; will return a partition of &lt;code&gt;-1&lt;/code&gt; when GroupID is set&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Explicit Commits&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;kafka-go&lt;/code&gt; also supports explicit commits. Instead of calling &lt;code&gt;ReadMessage&lt;/code&gt;, call &lt;code&gt;FetchMessage&lt;/code&gt; followed by &lt;code&gt;CommitMessages&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;ctx := context.Background()&#xA;for {&#xA;    m, err := r.FetchMessage(ctx)&#xA;    if err != nil {&#xA;        break&#xA;    }&#xA;    fmt.Printf(&#34;message at topic/partition/offset %v/%v/%v: %s = %s\n&#34;, m.Topic, m.Partition, m.Offset, string(m.Key), string(m.Value))&#xA;    if err := r.CommitMessages(ctx, m); err != nil {&#xA;        log.Fatal(&#34;failed to commit messages:&#34;, err)&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When committing messages in consumer groups, the message with the highest offset for a given topic/partition determines the value of the committed offset for that partition. For example, if messages at offset 1, 2, and 3 of a single partition were retrieved by call to &lt;code&gt;FetchMessage&lt;/code&gt;, calling &lt;code&gt;CommitMessages&lt;/code&gt; with message offset 3 will also result in committing the messages at offsets 1 and 2 for that partition.&lt;/p&gt; &#xA;&lt;h3&gt;Managing Commits&lt;/h3&gt; &#xA;&lt;p&gt;By default, CommitMessages will synchronously commit offsets to Kafka. For improved performance, you can instead periodically commit offsets to Kafka by setting CommitInterval on the ReaderConfig.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// make a new reader that consumes from topic-A&#xA;r := kafka.NewReader(kafka.ReaderConfig{&#xA;    Brokers:        []string{&#34;localhost:9092&#34;, &#34;localhost:9093&#34;, &#34;localhost:9094&#34;},&#xA;    GroupID:        &#34;consumer-group-id&#34;,&#xA;    Topic:          &#34;topic-A&#34;,&#xA;    MinBytes:       10e3, // 10KB&#xA;    MaxBytes:       10e6, // 10MB&#xA;    CommitInterval: time.Second, // flushes commits to Kafka every second&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Writer &lt;a href=&#34;https://godoc.org/github.com/segmentio/kafka-go#Writer&#34;&gt;&lt;img src=&#34;https://godoc.org/github.com/segmentio/kafka-go?status.svg?sanitize=true&#34; alt=&#34;GoDoc&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;To produce messages to Kafka, a program may use the low-level &lt;code&gt;Conn&lt;/code&gt; API, but the package also provides a higher level &lt;code&gt;Writer&lt;/code&gt; type which is more appropriate to use in most cases as it provides additional features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Automatic retries and reconnections on errors.&lt;/li&gt; &#xA; &lt;li&gt;Configurable distribution of messages across available partitions.&lt;/li&gt; &#xA; &lt;li&gt;Synchronous or asynchronous writes of messages to Kafka.&lt;/li&gt; &#xA; &lt;li&gt;Asynchronous cancellation using contexts.&lt;/li&gt; &#xA; &lt;li&gt;Flushing of pending messages on close to support graceful shutdowns.&lt;/li&gt; &#xA; &lt;li&gt;Creation of a missing topic before publishing a message. &lt;em&gt;Note!&lt;/em&gt; it was the default behaviour up to the version &lt;code&gt;v0.4.30&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// make a writer that produces to topic-A, using the least-bytes distribution&#xA;w := &amp;amp;kafka.Writer{&#xA;&#x9;Addr:     kafka.TCP(&#34;localhost:9092&#34;, &#34;localhost:9093&#34;, &#34;localhost:9094&#34;),&#xA;&#x9;Topic:   &#34;topic-A&#34;,&#xA;&#x9;Balancer: &amp;amp;kafka.LeastBytes{},&#xA;}&#xA;&#xA;err := w.WriteMessages(context.Background(),&#xA;&#x9;kafka.Message{&#xA;&#x9;&#x9;Key:   []byte(&#34;Key-A&#34;),&#xA;&#x9;&#x9;Value: []byte(&#34;Hello World!&#34;),&#xA;&#x9;},&#xA;&#x9;kafka.Message{&#xA;&#x9;&#x9;Key:   []byte(&#34;Key-B&#34;),&#xA;&#x9;&#x9;Value: []byte(&#34;One!&#34;),&#xA;&#x9;},&#xA;&#x9;kafka.Message{&#xA;&#x9;&#x9;Key:   []byte(&#34;Key-C&#34;),&#xA;&#x9;&#x9;Value: []byte(&#34;Two!&#34;),&#xA;&#x9;},&#xA;)&#xA;if err != nil {&#xA;    log.Fatal(&#34;failed to write messages:&#34;, err)&#xA;}&#xA;&#xA;if err := w.Close(); err != nil {&#xA;    log.Fatal(&#34;failed to close writer:&#34;, err)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Missing topic creation before publication&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// Make a writer that publishes messages to topic-A.&#xA;// The topic will be created if it is missing.&#xA;w := &amp;amp;Writer{&#xA;    Addr:                   kafka.TCP(&#34;localhost:9092&#34;, &#34;localhost:9093&#34;, &#34;localhost:9094&#34;),&#xA;    Topic:                  &#34;topic-A&#34;,&#xA;    AllowAutoTopicCreation: true,&#xA;}&#xA;&#xA;messages := []kafka.Message{&#xA;    {&#xA;        Key:   []byte(&#34;Key-A&#34;),&#xA;        Value: []byte(&#34;Hello World!&#34;),&#xA;    },&#xA;    {&#xA;        Key:   []byte(&#34;Key-B&#34;),&#xA;        Value: []byte(&#34;One!&#34;),&#xA;    },&#xA;    {&#xA;        Key:   []byte(&#34;Key-C&#34;),&#xA;        Value: []byte(&#34;Two!&#34;),&#xA;    },&#xA;}&#xA;&#xA;var err error&#xA;const retries = 3&#xA;for i := 0; i &amp;lt; retries; i++ {&#xA;    ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)&#xA;    defer cancel()&#xA;    &#xA;    // attempt to create topic prior to publishing the message&#xA;    err = w.WriteMessages(ctx, messages...)&#xA;    if errors.Is(err, LeaderNotAvailable) || errors.Is(err, context.DeadlineExceeded) {&#xA;        time.Sleep(time.Millisecond * 250)&#xA;        continue&#xA;    }&#xA;&#xA;    if err != nil {&#xA;        log.Fatalf(&#34;unexpected error %v&#34;, err)&#xA;    }&#xA;}&#xA;&#xA;if err := w.Close(); err != nil {&#xA;    log.Fatal(&#34;failed to close writer:&#34;, err)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Writing to multiple topics&lt;/h3&gt; &#xA;&lt;p&gt;Normally, the &lt;code&gt;WriterConfig.Topic&lt;/code&gt; is used to initialize a single-topic writer. By excluding that particular configuration, you are given the ability to define the topic on a per-message basis by setting &lt;code&gt;Message.Topic&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;w := &amp;amp;kafka.Writer{&#xA;&#x9;Addr:     kafka.TCP(&#34;localhost:9092&#34;, &#34;localhost:9093&#34;, &#34;localhost:9094&#34;),&#xA;    // NOTE: When Topic is not defined here, each Message must define it instead.&#xA;&#x9;Balancer: &amp;amp;kafka.LeastBytes{},&#xA;}&#xA;&#xA;err := w.WriteMessages(context.Background(),&#xA;    // NOTE: Each Message has Topic defined, otherwise an error is returned.&#xA;&#x9;kafka.Message{&#xA;        Topic: &#34;topic-A&#34;,&#xA;&#x9;&#x9;Key:   []byte(&#34;Key-A&#34;),&#xA;&#x9;&#x9;Value: []byte(&#34;Hello World!&#34;),&#xA;&#x9;},&#xA;&#x9;kafka.Message{&#xA;        Topic: &#34;topic-B&#34;,&#xA;&#x9;&#x9;Key:   []byte(&#34;Key-B&#34;),&#xA;&#x9;&#x9;Value: []byte(&#34;One!&#34;),&#xA;&#x9;},&#xA;&#x9;kafka.Message{&#xA;        Topic: &#34;topic-C&#34;,&#xA;&#x9;&#x9;Key:   []byte(&#34;Key-C&#34;),&#xA;&#x9;&#x9;Value: []byte(&#34;Two!&#34;),&#xA;&#x9;},&#xA;)&#xA;if err != nil {&#xA;    log.Fatal(&#34;failed to write messages:&#34;, err)&#xA;}&#xA;&#xA;if err := w.Close(); err != nil {&#xA;    log.Fatal(&#34;failed to close writer:&#34;, err)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; These 2 patterns are mutually exclusive, if you set &lt;code&gt;Writer.Topic&lt;/code&gt;, you must not also explicitly define &lt;code&gt;Message.Topic&lt;/code&gt; on the messages you are writing. The opposite applies when you do not define a topic for the writer. The &lt;code&gt;Writer&lt;/code&gt; will return an error if it detects this ambiguity.&lt;/p&gt; &#xA;&lt;h3&gt;Compatibility with other clients&lt;/h3&gt; &#xA;&lt;h4&gt;Sarama&lt;/h4&gt; &#xA;&lt;p&gt;If you&#39;re switching from Sarama and need/want to use the same algorithm for message partitioning, you can either use the &lt;code&gt;kafka.Hash&lt;/code&gt; balancer or the &lt;code&gt;kafka.ReferenceHash&lt;/code&gt; balancer:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;kafka.Hash&lt;/code&gt; = &lt;code&gt;sarama.NewHashPartitioner&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;kafka.ReferenceHash&lt;/code&gt; = &lt;code&gt;sarama.NewReferenceHashPartitioner&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The &lt;code&gt;kafka.Hash&lt;/code&gt; and &lt;code&gt;kafka.ReferenceHash&lt;/code&gt; balancers would route messages to the same partitions that the two aforementioned Sarama partitioners would route them to.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;w := &amp;amp;kafka.Writer{&#xA;&#x9;Addr:     kafka.TCP(&#34;localhost:9092&#34;, &#34;localhost:9093&#34;, &#34;localhost:9094&#34;),&#xA;&#x9;Topic:    &#34;topic-A&#34;,&#xA;&#x9;Balancer: &amp;amp;kafka.Hash{},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;librdkafka and confluent-kafka-go&lt;/h4&gt; &#xA;&lt;p&gt;Use the &lt;code&gt;kafka.CRC32Balancer&lt;/code&gt; balancer to get the same behaviour as librdkafka&#39;s default &lt;code&gt;consistent_random&lt;/code&gt; partition strategy.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;w := &amp;amp;kafka.Writer{&#xA;&#x9;Addr:     kafka.TCP(&#34;localhost:9092&#34;, &#34;localhost:9093&#34;, &#34;localhost:9094&#34;),&#xA;&#x9;Topic:    &#34;topic-A&#34;,&#xA;&#x9;Balancer: kafka.CRC32Balancer{},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Java&lt;/h4&gt; &#xA;&lt;p&gt;Use the &lt;code&gt;kafka.Murmur2Balancer&lt;/code&gt; balancer to get the same behaviour as the canonical Java client&#39;s default partitioner. Note: the Java class allows you to directly specify the partition which is not permitted.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;w := &amp;amp;kafka.Writer{&#xA;&#x9;Addr:     kafka.TCP(&#34;localhost:9092&#34;, &#34;localhost:9093&#34;, &#34;localhost:9094&#34;),&#xA;&#x9;Topic:    &#34;topic-A&#34;,&#xA;&#x9;Balancer: kafka.Murmur2Balancer{},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Compression&lt;/h3&gt; &#xA;&lt;p&gt;Compression can be enabled on the &lt;code&gt;Writer&lt;/code&gt; by setting the &lt;code&gt;Compression&lt;/code&gt; field:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;w := &amp;amp;kafka.Writer{&#xA;&#x9;Addr:        kafka.TCP(&#34;localhost:9092&#34;, &#34;localhost:9093&#34;, &#34;localhost:9094&#34;),&#xA;&#x9;Topic:       &#34;topic-A&#34;,&#xA;&#x9;Compression: kafka.Snappy,&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;Reader&lt;/code&gt; will by determine if the consumed messages are compressed by examining the message attributes. However, the package(s) for all expected codecs must be imported so that they get loaded correctly.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Note: in versions prior to 0.4 programs had to import compression packages to install codecs and support reading compressed messages from kafka. This is no longer the case and import of the compression packages are now no-ops.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;TLS Support&lt;/h2&gt; &#xA;&lt;p&gt;For a bare bones Conn type or in the Reader/Writer configs you can specify a dialer option for TLS support. If the TLS field is nil, it will not connect with TLS. &lt;em&gt;Note:&lt;/em&gt; Connecting to a Kafka cluster with TLS enabled without configuring TLS on the Conn/Reader/Writer can manifest in opaque io.ErrUnexpectedEOF errors.&lt;/p&gt; &#xA;&lt;h3&gt;Connection&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;dialer := &amp;amp;kafka.Dialer{&#xA;    Timeout:   10 * time.Second,&#xA;    DualStack: true,&#xA;    TLS:       &amp;amp;tls.Config{...tls config...},&#xA;}&#xA;&#xA;conn, err := dialer.DialContext(ctx, &#34;tcp&#34;, &#34;localhost:9093&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Reader&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;dialer := &amp;amp;kafka.Dialer{&#xA;    Timeout:   10 * time.Second,&#xA;    DualStack: true,&#xA;    TLS:       &amp;amp;tls.Config{...tls config...},&#xA;}&#xA;&#xA;r := kafka.NewReader(kafka.ReaderConfig{&#xA;    Brokers:        []string{&#34;localhost:9092&#34;, &#34;localhost:9093&#34;, &#34;localhost:9094&#34;},&#xA;    GroupID:        &#34;consumer-group-id&#34;,&#xA;    Topic:          &#34;topic-A&#34;,&#xA;    Dialer:         dialer,&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Writer&lt;/h3&gt; &#xA;&lt;p&gt;Direct Writer creation&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;w := kafka.Writer{&#xA;    Addr: kafka.TCP(&#34;localhost:9092&#34;, &#34;localhost:9093&#34;, &#34;localhost:9094&#34;), &#xA;    Topic:   &#34;topic-A&#34;,&#xA;    Balancer: &amp;amp;kafka.Hash{},&#xA;    Transport: &amp;amp;kafka.Transport{&#xA;        TLS: &amp;amp;tls.Config{},&#xA;      },&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Using &lt;code&gt;kafka.NewWriter&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;dialer := &amp;amp;kafka.Dialer{&#xA;    Timeout:   10 * time.Second,&#xA;    DualStack: true,&#xA;    TLS:       &amp;amp;tls.Config{...tls config...},&#xA;}&#xA;&#xA;w := kafka.NewWriter(kafka.WriterConfig{&#xA;&#x9;Brokers: []string{&#34;localhost:9092&#34;, &#34;localhost:9093&#34;, &#34;localhost:9094&#34;},&#xA;&#x9;Topic:   &#34;topic-A&#34;,&#xA;&#x9;Balancer: &amp;amp;kafka.Hash{},&#xA;&#x9;Dialer:   dialer,&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that &lt;code&gt;kafka.NewWriter&lt;/code&gt; and &lt;code&gt;kafka.WriterConfig&lt;/code&gt; are deprecated and will be removed in a future release.&lt;/p&gt; &#xA;&lt;h2&gt;SASL Support&lt;/h2&gt; &#xA;&lt;p&gt;You can specify an option on the &lt;code&gt;Dialer&lt;/code&gt; to use SASL authentication. The &lt;code&gt;Dialer&lt;/code&gt; can be used directly to open a &lt;code&gt;Conn&lt;/code&gt; or it can be passed to a &lt;code&gt;Reader&lt;/code&gt; or &lt;code&gt;Writer&lt;/code&gt; via their respective configs. If the &lt;code&gt;SASLMechanism&lt;/code&gt; field is &lt;code&gt;nil&lt;/code&gt;, it will not authenticate with SASL.&lt;/p&gt; &#xA;&lt;h3&gt;SASL Authentication Types&lt;/h3&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://godoc.org/github.com/segmentio/kafka-go/sasl/plain#Mechanism&#34;&gt;Plain&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;mechanism := plain.Mechanism{&#xA;    Username: &#34;username&#34;,&#xA;    Password: &#34;password&#34;,&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://godoc.org/github.com/segmentio/kafka-go/sasl/scram#Mechanism&#34;&gt;SCRAM&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;mechanism, err := scram.Mechanism(scram.SHA512, &#34;username&#34;, &#34;password&#34;)&#xA;if err != nil {&#xA;    panic(err)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Connection&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;mechanism, err := scram.Mechanism(scram.SHA512, &#34;username&#34;, &#34;password&#34;)&#xA;if err != nil {&#xA;    panic(err)&#xA;}&#xA;&#xA;dialer := &amp;amp;kafka.Dialer{&#xA;    Timeout:       10 * time.Second,&#xA;    DualStack:     true,&#xA;    SASLMechanism: mechanism,&#xA;}&#xA;&#xA;conn, err := dialer.DialContext(ctx, &#34;tcp&#34;, &#34;localhost:9093&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Reader&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;mechanism, err := scram.Mechanism(scram.SHA512, &#34;username&#34;, &#34;password&#34;)&#xA;if err != nil {&#xA;    panic(err)&#xA;}&#xA;&#xA;dialer := &amp;amp;kafka.Dialer{&#xA;    Timeout:       10 * time.Second,&#xA;    DualStack:     true,&#xA;    SASLMechanism: mechanism,&#xA;}&#xA;&#xA;r := kafka.NewReader(kafka.ReaderConfig{&#xA;    Brokers:        []string{&#34;localhost:9092&#34;,&#34;localhost:9093&#34;, &#34;localhost:9094&#34;},&#xA;    GroupID:        &#34;consumer-group-id&#34;,&#xA;    Topic:          &#34;topic-A&#34;,&#xA;    Dialer:         dialer,&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Writer&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;mechanism, err := scram.Mechanism(scram.SHA512, &#34;username&#34;, &#34;password&#34;)&#xA;if err != nil {&#xA;    panic(err)&#xA;}&#xA;&#xA;// Transports are responsible for managing connection pools and other resources,&#xA;// it&#39;s generally best to create a few of these and share them across your&#xA;// application.&#xA;sharedTransport := &amp;amp;kafka.Transport{&#xA;    SASL: mechanism,&#xA;}&#xA;&#xA;w := kafka.Writer{&#xA;&#x9;Addr:      kafka.TCP(&#34;localhost:9092&#34;, &#34;localhost:9093&#34;, &#34;localhost:9094&#34;),&#xA;&#x9;Topic:     &#34;topic-A&#34;,&#xA;&#x9;Balancer:  &amp;amp;kafka.Hash{},&#xA;&#x9;Transport: sharedTransport,&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Client&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;mechanism, err := scram.Mechanism(scram.SHA512, &#34;username&#34;, &#34;password&#34;)&#xA;if err != nil {&#xA;    panic(err)&#xA;}&#xA;&#xA;// Transports are responsible for managing connection pools and other resources,&#xA;// it&#39;s generally best to create a few of these and share them across your&#xA;// application.&#xA;sharedTransport := &amp;amp;kafka.Transport{&#xA;    SASL: mechanism,&#xA;}&#xA;&#xA;client := &amp;amp;kafka.Client{&#xA;    Addr:      kafka.TCP(&#34;localhost:9092&#34;, &#34;localhost:9093&#34;, &#34;localhost:9094&#34;),&#xA;    Timeout:   10 * time.Second,&#xA;    Transport: sharedTransport,&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Reading all messages within a time range&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;startTime := time.Now().Add(-time.Hour)&#xA;endTime := time.Now()&#xA;batchSize := int(10e6) // 10MB&#xA;&#xA;r := kafka.NewReader(kafka.ReaderConfig{&#xA;    Brokers:   []string{&#34;localhost:9092&#34;, &#34;localhost:9093&#34;, &#34;localhost:9094&#34;},&#xA;    Topic:     &#34;my-topic1&#34;,&#xA;    Partition: 0,&#xA;    MinBytes:  batchSize,&#xA;    MaxBytes:  batchSize,&#xA;})&#xA;&#xA;r.SetOffsetAt(context.Background(), startTime)&#xA;&#xA;for {&#xA;    m, err := r.ReadMessage(context.Background())&#xA;&#xA;    if err != nil {&#xA;        break&#xA;    }&#xA;    if m.Time.After(endTime) {&#xA;        break&#xA;    }&#xA;    // TODO: process message&#xA;    fmt.Printf(&#34;message at offset %d: %s = %s\n&#34;, m.Offset, string(m.Key), string(m.Value))&#xA;}&#xA;&#xA;if err := r.Close(); err != nil {&#xA;    log.Fatal(&#34;failed to close reader:&#34;, err)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Logging&lt;/h2&gt; &#xA;&lt;p&gt;For visiblity into the operations of the Reader/Writer types, configure a logger on creation.&lt;/p&gt; &#xA;&lt;h3&gt;Reader&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func logf(msg string, a ...interface{}) {&#xA;&#x9;fmt.Printf(msg, a...)&#xA;&#x9;fmt.Println()&#xA;}&#xA;&#xA;r := kafka.NewReader(kafka.ReaderConfig{&#xA;&#x9;Brokers:     []string{&#34;localhost:9092&#34;, &#34;localhost:9093&#34;, &#34;localhost:9094&#34;},&#xA;&#x9;Topic:       &#34;my-topic1&#34;,&#xA;&#x9;Partition:   0,&#xA;&#x9;Logger:      kafka.LoggerFunc(logf),&#xA;&#x9;ErrorLogger: kafka.LoggerFunc(logf),&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Writer&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func logf(msg string, a ...interface{}) {&#xA;&#x9;fmt.Printf(msg, a...)&#xA;&#x9;fmt.Println()&#xA;}&#xA;&#xA;w := &amp;amp;kafka.Writer{&#xA;&#x9;Addr:        kafka.TCP(&#34;localhost:9092&#34;),&#xA;&#x9;Topic:       &#34;topic&#34;,&#xA;&#x9;Logger:      kafka.LoggerFunc(logf),&#xA;&#x9;ErrorLogger: kafka.LoggerFunc(logf),&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Testing&lt;/h2&gt; &#xA;&lt;p&gt;Subtle behavior changes in later Kafka versions have caused some historical tests to break, if you are running against Kafka 2.3.1 or later, exporting the &lt;code&gt;KAFKA_SKIP_NETTEST=1&lt;/code&gt; environment variables will skip those tests.&lt;/p&gt; &#xA;&lt;p&gt;Run Kafka locally in docker&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker-compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run tests&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;KAFKA_VERSION=2.3.1 \&#xA;  KAFKA_SKIP_NETTEST=1 \&#xA;  go test -race ./...&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>optiv/Freeze</title>
    <updated>2022-09-30T01:38:31Z</updated>
    <id>tag:github.com,2022-09-30:/optiv/Freeze</id>
    <link href="https://github.com/optiv/Freeze" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Freeze is a payload toolkit for bypassing EDRs using suspended processes, direct syscalls, and alternative execution methods&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; &lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/optiv/Freeze/main/Screenshots/Freeze.jpg&#34; height=&#34;310&#34; border=&#34;2px solid #555&#34;&gt; &lt;br&gt; Freeze &lt;/h1&gt; &#xA;&lt;h3&gt;More Information&lt;/h3&gt; &#xA;&lt;p&gt;If you want to learn more about the techniques utilized in this framework, please take a look at &lt;a href=&#34;https://www.optiv.com/insights/source-zero/blog/sacrificing-suspended-processes&#34;&gt;SourceZero Blog&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;&lt;/h1&gt; &#xA;&lt;h2&gt;Description&lt;/h2&gt; &#xA;&lt;p&gt;Freeze is a payload creation tool used for circumventing EDR security controls to execute shellcode in a stealthy manner. Freeze utilizes multiple techniques to not only remove Userland EDR hooks, but to also execute shellcode in such a way that it circumvents other endpoint monitoring controls.&lt;/p&gt; &#xA;&lt;h3&gt;Creating A Suspended Process&lt;/h3&gt; &#xA;&lt;p&gt;When a process is created, Ntdll.dll is the first DLL that is loaded. This happens before any EDR DLLs are loaded. This means that there is a bit of a delay before an EDR can be loaded and start hooking and modifying the assembly of system DLLs. In looking at Windows syscalls in Ntdll.dll, we can see that nothing is hooked yet. If we create a process in a suspend state (one that is frozen in time), we can see that no other DLLs are loaded, except for Ntdll.dll. You can also see that no EDR DLLs are loaded, meaning that the syscalls located in Ntdll.dll are unmodified.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/optiv/Freeze/main/Screenshots/Suspended_Process.png&#34; border=&#34;2px solid #555&#34;&gt; &lt;/p&gt;&#xA;&lt;h3&gt;Address Space Layout Randomization&lt;/h3&gt; &#xA;&lt;p&gt;In order to use this clean suspended process to remove hooks from Freeze loader, we need a way to programmatically find and read the clean suspended process&#39; memory. This is where address space layout randomization (ASLR) comes into play. ASLR is a security mechanism to prevent stack memory corruption-based vulnerabilities. ASLR randomizes the address space inside of a process, to ensure that all memory-mapped objects, the stack, the heap, and the executable program itself, are unique. Now, this is where it gets interesting because while ASLR works, it does not work for position-independent code such as DLLs. What happens with DLLs, (specifically known system DLLs) is that the address space is randomized once at boot time. This means that we don&#39;t need to enumerate a remote process information to find the base address of its ntdll.dll because it is the same in all processes including the one that we control. Since the address of every DLL is the same place per boot, we can pull this information from our own process and never have to enumerate the suspended process to find the address.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/optiv/Freeze/main/Screenshots/Base_Address.png&#34; border=&#34;2px solid #555&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt;With this information, we can use the API ReadProcessMemory to read a process&#39; memory. This API call is commonly associated with the reading of LSASS as part of any credential-based attack; however, on its own it is inherently not malicious, especially if we are just reading an arbitrary section of memory. The only time ReadProcessMemory will be flagged as part of something suspicious is if you are reading something you shouldn&#39;t (like the contents of LSASS). EDR products should never flag the fact that ReadProcessMemory was called, as there are legitimate operational uses for this function and would result in many false positives.&lt;/p&gt; &#xA;&lt;p&gt;We can take this a step further by only reading a section of Ntdll.dll where all syscalls are stored - its .text section, rather than reading the entire DLL.&lt;/p&gt; &#xA;&lt;p&gt;Combining these elements, we can programmatically get a copy of the .text section of Ntdll.dll to overwrite our existing hooked .text section prior to executing shellcode.&lt;/p&gt; &#xA;&lt;h3&gt;ETW Patching&lt;/h3&gt; &#xA;&lt;p&gt;ETW utilizes built-in syscalls to generate this telemetry. Since ETW is also a native feature built into Windows, security products do not need to &#34;hook&#34; the ETW syscalls to access the information. As a result, to prevent ETW, Freeze patches numerous ETW syscalls, flushing out the registers and returning the execution flow to the next instruction. Patching ETW is now default in all loaders.&lt;/p&gt; &#xA;&lt;h3&gt;Shellcode&lt;/h3&gt; &#xA;&lt;p&gt;Since only Ntdll.dll is restored, all subsequent calls to execute shellcode need to reside in Ntdll.dll. Using Go (note you can do this in other languages but in Go, its quite easy to implement) we can define and call the NT syscalls needed to allocate, write, and protect the shellcode, effectively skipping the standard calls that are located in kernel32d.dll, and Kernelbase.dll, as these may still be hooked.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/optiv/Freeze/main/Screenshots/Syscalls.png&#34; border=&#34;2px solid #555&#34;&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/optiv/Freeze/main/Screenshots/Userland_EDR.png&#34; border=&#34;2px solid #555&#34;&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/optiv/Freeze/main/Screenshots/Kernel_EDR.png&#34; border=&#34;2px solid #555&#34;&gt; &lt;/p&gt;&#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Freeze was developed in Golang.&lt;/p&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;To install Freeze, run the following commands, or use the compiled binary:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;go build Freeze.go&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Help&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;        ___________                                    &#xA;        \_   _____/______   ____   ____ ________ ____  &#xA;         |    __) \_  __ \_/ __ \_/ __ \\___   // __ \ &#xA;         |     \   |  | \/\  ___/\  ___/ /    /\  ___/ &#xA;         \___  /   |__|    \___  &amp;gt;\___  &amp;gt;_____ \\___  &amp;gt;&#xA;             \/                \/     \/      \/    \/ &#xA;                                        (@Tyl0us)&#xA;        Soon they will learn that revenge is a dish... best served COLD...&#xA;                 &#xA;Usage of ./Freeze:&#xA;  -I string&#xA;        Path to the raw 64-bit shellcode.&#xA;  -O string&#xA;        Name of output file (e.g. loader.exe or loader.dll). Depending on what file extension defined will determine if Freeze makes a dll or exe.&#xA;  -console&#xA;        Only for Binary Payloads - Generates verbose console information when the payload is executed. This will disable the hidden window feature.&#xA;  -encrypt&#xA;        Encrypts the shellcode using AES 256 encryption&#xA;  -export string&#xA;        For DLL Loaders Only - Specify a specific Export function for a loader to have.&#xA;  -process string&#xA;        The name of process to spawn. This process has to exist in C:\Windows\System32\. Example &#39;notepad.exe&#39; (default &#34;notepad.exe&#34;)&#xA;  -sandbox&#xA;        Enables sandbox evasion by checking:&#xA;                Is Endpoint joined to a domain?&#xA;                Does the Endpoint have more than 2 CPUs?&#xA;                Does the Endpoint have more than 4 gigs of RAM?&#xA;  -sha256&#xA;        Provides the SHA256 value of the loaders (This is useful for tracking)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Binary vs DLL&lt;/h2&gt; &#xA;&lt;p&gt;Freeze can generate either a &lt;code&gt;.exe&lt;/code&gt; or &lt;code&gt;.dll&lt;/code&gt; file. In order to specify this, ensure that the &lt;code&gt;-O&lt;/code&gt; command line option ends with either a &lt;code&gt;.exe&lt;/code&gt; for binaries or &lt;code&gt;.dll&lt;/code&gt; for dlls. No other file types are currently supported. In the case of DLL files, Freeze can also add additional export functionality. To do this use the &lt;code&gt;-export&lt;/code&gt; with specific export function name.&lt;/p&gt; &#xA;&lt;h2&gt;Console&lt;/h2&gt; &#xA;&lt;p&gt;Freeze utilizes a technique to first create the process and then move it into the background. This does two things - first it helps keep the process hidden, and second, avoids being detected by any EDR product. Spawning a process right away in the background can be very suspicious and an indicator of maliciousness. Freeze does this by calling the ‘GetConsoleWindow’ and ‘ShowWindow’ Windows function after the process is created and the EDR’s hooks are loaded, and then changes the windows attributes to hidden. Freeze utilizes these APIs rather than using the traditional -ldflags -H=windowsgui, as this is highly signatured and classified in most security products as an Indicator of Compromise.&lt;/p&gt; &#xA;&lt;p&gt;If the &lt;code&gt;-console&lt;/code&gt; command-line option is selected, Freeze will not hide the process in the background. Instead, Freeze will add several debug messages displaying what the loader is doing.&lt;/p&gt; &#xA;&lt;h2&gt;Credit&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Special thanks to aahmad097 for developing &lt;a href=&#34;https://github.com/aahmad097/AlternativeShellcodeExec&#34;&gt;AlternativeShellcodeExec&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Special thanks to mvdan for developing &lt;a href=&#34;https://github.com/burrowers/garble&#34;&gt;Garble&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>aws/aws-sdk-go-v2</title>
    <updated>2022-09-30T01:38:31Z</updated>
    <id>tag:github.com,2022-09-30:/aws/aws-sdk-go-v2</id>
    <link href="https://github.com/aws/aws-sdk-go-v2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;AWS SDK for the Go programming language.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AWS SDK for Go v2&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/aws/aws-sdk-go-v2/actions/workflows/go.yml&#34;&gt;&lt;img src=&#34;https://github.com/aws/aws-sdk-go-v2/actions/workflows/go.yml/badge.svg?branch=main&#34; alt=&#34;Go Build status&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://github.com/aws/aws-sdk-go-v2/actions/workflows/codegen.yml&#34;&gt;&lt;img src=&#34;https://github.com/aws/aws-sdk-go-v2/actions/workflows/codegen.yml/badge.svg?branch=main&#34; alt=&#34;Codegen Build status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aws.github.io/aws-sdk-go-v2/docs/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/SDK-Documentation-blue&#34; alt=&#34;SDK Documentation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aws.github.io/aws-sdk-go-v2/docs/migrating/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Migration-Guide-blue&#34; alt=&#34;Migration Guide&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pkg.go.dev/mod/github.com/aws/aws-sdk-go-v2&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/api-reference-blue.svg?sanitize=true&#34; alt=&#34;API Reference&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/aws/aws-sdk-go/raw/master/LICENSE.txt&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache%20V2-blue.svg?sanitize=true&#34; alt=&#34;Apache V2 License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;aws-sdk-go-v2&lt;/code&gt; is the v2 AWS SDK for the Go programming language.&lt;/p&gt; &#xA;&lt;p&gt;The v2 SDK requires a minimum version of &lt;code&gt;Go 1.15&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Checkout out the &lt;a href=&#34;https://github.com/aws/aws-sdk-go-v2/raw/main/CHANGELOG.md&#34;&gt;release notes&lt;/a&gt; for information about the latest bug fixes, updates, and features added to the SDK.&lt;/p&gt; &#xA;&lt;p&gt;Jump To:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/aws-sdk-go-v2/main/#getting-started&#34;&gt;Getting Started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/aws-sdk-go-v2/main/#getting-help&#34;&gt;Getting Help&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/aws-sdk-go-v2/main/#feedback-and-contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/aws-sdk-go-v2/main/#resources&#34;&gt;More Resources&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Maintenance and support for SDK major versions&lt;/h2&gt; &#xA;&lt;p&gt;For information about maintenance and support for SDK major versions and their underlying dependencies, see the following in the AWS SDKs and Tools Shared Configuration and Credentials Reference Guide:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.aws.amazon.com/credref/latest/refdocs/maint-policy.html&#34;&gt;AWS SDKs and Tools Maintenance Policy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.aws.amazon.com/credref/latest/refdocs/version-support-matrix.html&#34;&gt;AWS SDKs and Tools Version Support Matrix&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;To get started working with the SDK setup your project for Go modules, and retrieve the SDK dependencies with &lt;code&gt;go get&lt;/code&gt;. This example shows how you can use the v2 SDK to make an API request using the SDK&#39;s &lt;a href=&#34;https://aws.amazon.com/dynamodb/&#34;&gt;Amazon DynamoDB&lt;/a&gt; client.&lt;/p&gt; &#xA;&lt;h6&gt;Initialize Project&lt;/h6&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ mkdir ~/helloaws&#xA;$ cd ~/helloaws&#xA;$ go mod init helloaws&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h6&gt;Add SDK Dependencies&lt;/h6&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ go get github.com/aws/aws-sdk-go-v2/aws&#xA;$ go get github.com/aws/aws-sdk-go-v2/config&#xA;$ go get github.com/aws/aws-sdk-go-v2/service/dynamodb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h6&gt;Write Code&lt;/h6&gt; &#xA;&lt;p&gt;In your preferred editor add the following content to &lt;code&gt;main.go&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package main&#xA;&#xA;import (&#xA;    &#34;context&#34;&#xA;    &#34;fmt&#34;&#xA;    &#34;log&#34;&#xA;&#xA;    &#34;github.com/aws/aws-sdk-go-v2/aws&#34;&#xA;    &#34;github.com/aws/aws-sdk-go-v2/config&#34;&#xA;    &#34;github.com/aws/aws-sdk-go-v2/service/dynamodb&#34;&#xA;)&#xA;&#xA;func main() {&#xA;    // Using the SDK&#39;s default configuration, loading additional config&#xA;    // and credentials values from the environment variables, shared&#xA;    // credentials, and shared configuration files&#xA;    cfg, err := config.LoadDefaultConfig(context.TODO(), config.WithRegion(&#34;us-west-2&#34;))&#xA;    if err != nil {&#xA;        log.Fatalf(&#34;unable to load SDK config, %v&#34;, err)&#xA;    }&#xA;&#xA;    // Using the Config value, create the DynamoDB client&#xA;    svc := dynamodb.NewFromConfig(cfg)&#xA;&#xA;    // Build the request with its input parameters&#xA;    resp, err := svc.ListTables(context.TODO(), &amp;amp;dynamodb.ListTablesInput{&#xA;        Limit: aws.Int32(5),&#xA;    })&#xA;    if err != nil {&#xA;        log.Fatalf(&#34;failed to list tables, %v&#34;, err)&#xA;    }&#xA;&#xA;    fmt.Println(&#34;Tables:&#34;)&#xA;    for _, tableName := range resp.TableNames {&#xA;        fmt.Println(tableName)&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h6&gt;Compile and Execute&lt;/h6&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ go run .&#xA;Table:&#xA;tableOne&#xA;tableTwo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Getting Help&lt;/h2&gt; &#xA;&lt;p&gt;Please use these community resources for getting help. We use the GitHub issues for tracking bugs and feature requests.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ask a question on &lt;a href=&#34;http://stackoverflow.com/&#34;&gt;StackOverflow&lt;/a&gt; and tag it with the &lt;a href=&#34;http://stackoverflow.com/questions/tagged/aws-sdk-go&#34;&gt;&lt;code&gt;aws-sdk-go&lt;/code&gt;&lt;/a&gt; tag.&lt;/li&gt; &#xA; &lt;li&gt;Open a support ticket with &lt;a href=&#34;http://docs.aws.amazon.com/awssupport/latest/user/getting-started.html&#34;&gt;AWS Support&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;If you think you may have found a bug, please open an &lt;a href=&#34;https://github.com/aws/aws-sdk-go-v2/issues/new/choose&#34;&gt;issue&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This SDK implements AWS service APIs. For general issues regarding the AWS services and their limitations, you may also take a look at the &lt;a href=&#34;https://forums.aws.amazon.com/&#34;&gt;Amazon Web Services Discussion Forums&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Opening Issues&lt;/h3&gt; &#xA;&lt;p&gt;If you encounter a bug with the AWS SDK for Go we would like to hear about it. Search the &lt;a href=&#34;https://github.com/aws/aws-sdk-go-v2/issues&#34;&gt;existing issues&lt;/a&gt; and see if others are also experiencing the issue before opening a new issue. Please include the version of AWS SDK for Go, Go language, and OS you’re using. Please also include reproduction case when appropriate.&lt;/p&gt; &#xA;&lt;p&gt;The GitHub issues are intended for bug reports and feature requests. For help and questions with using AWS SDK for Go please make use of the resources listed in the &lt;a href=&#34;https://raw.githubusercontent.com/aws/aws-sdk-go-v2/main/#getting-help&#34;&gt;Getting Help&lt;/a&gt; section. Keeping the list of open issues lean will help us respond in a timely manner.&lt;/p&gt; &#xA;&lt;h2&gt;Feedback and contributing&lt;/h2&gt; &#xA;&lt;p&gt;The v2 SDK will use GitHub &lt;a href=&#34;https://github.com/aws/aws-sdk-go-v2/issues&#34;&gt;Issues&lt;/a&gt; to track feature requests and issues with the SDK. In addition, we&#39;ll use GitHub &lt;a href=&#34;https://github.com/aws/aws-sdk-go-v2/projects&#34;&gt;Projects&lt;/a&gt; to track large tasks spanning multiple pull requests, such as refactoring the SDK&#39;s internal request lifecycle. You can provide feedback to us in several ways.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GitHub issues&lt;/strong&gt;. To provide feedback or report bugs, file GitHub &lt;a href=&#34;https://github.com/aws/aws-sdk-go-v2/issues&#34;&gt;Issues&lt;/a&gt; on the SDK. This is the preferred mechanism to give feedback so that other users can engage in the conversation, +1 issues, etc. Issues you open will be evaluated, and included in our roadmap for the GA launch.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Contributing&lt;/strong&gt;. You can open pull requests for fixes or additions to the AWS SDK for Go 2.0. All pull requests must be submitted under the Apache 2.0 license and will be reviewed by an SDK team member before being merged in. Accompanying unit tests, where possible, are appreciated.&lt;/p&gt; &#xA;&lt;h2&gt;Resources&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://aws.github.io/aws-sdk-go-v2/docs/&#34;&gt;SDK Developer Guide&lt;/a&gt; - Use this document to learn how to get started and use the AWS SDK for Go V2.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://aws.github.io/aws-sdk-go-v2/docs/migrating/&#34;&gt;SDK Migration Guide&lt;/a&gt; - Use this document to learn how to migrate to V2 from the AWS SDK for Go.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pkg.go.dev/mod/github.com/aws/aws-sdk-go-v2&#34;&gt;SDK API Reference Documentation&lt;/a&gt; - Use this document to look up all API operation input and output parameters for AWS services supported by the SDK. The API reference also includes documentation of the SDK, and examples how to using the SDK, service client API operations, and API operation require parameters.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://aws.amazon.com/documentation/&#34;&gt;Service Documentation&lt;/a&gt; - Use this documentation to learn how to interface with AWS services. These guides are great for getting started with a service, or when looking for more information about a service. While this document is not required for coding, services may supply helpful samples to look out for.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://forums.aws.amazon.com/forum.jspa?forumID=293&#34;&gt;Forum&lt;/a&gt; - Ask questions, get help, and give feedback&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/aws/aws-sdk-go-v2/issues&#34;&gt;Issues&lt;/a&gt; - Report issues, submit pull requests, and get involved (see &lt;a href=&#34;http://aws.amazon.com/apache2.0/&#34;&gt;Apache 2.0 License&lt;/a&gt;)&lt;/p&gt;</summary>
  </entry>
</feed>