<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Go Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-06-20T01:31:36Z</updated>
  <subtitle>Daily Trending of Go in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Ingenimax/agent-sdk-go</title>
    <updated>2025-06-20T01:31:36Z</updated>
    <id>tag:github.com,2025-06-20:/Ingenimax/agent-sdk-go</id>
    <link href="https://github.com/Ingenimax/agent-sdk-go" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/Ingenimax/agent-sdk-go/main/docs/img/logo-header.png#gh-light-mode-only&#34; alt=&#34;Ingenimax&#34; width=&#34;600&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/Ingenimax/agent-sdk-go/main/docs/img/logo-header-inverted.png#gh-dark-mode-only&#34; alt=&#34;Ingenimax&#34; width=&#34;600&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;Agent Go SDK&lt;/h1&gt; &#xA;&lt;p&gt;A powerful Go framework for building production-ready AI agents that seamlessly integrates memory management, tool execution, multi-LLM support, and enterprise features into a flexible, extensible architecture.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;h3&gt;Core Capabilities&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üß† &lt;strong&gt;Multi-Model Intelligence&lt;/strong&gt;: Seamless integration with OpenAI, Anthropic, and Google Vertex AI (Gemini models).&lt;/li&gt; &#xA; &lt;li&gt;üîß &lt;strong&gt;Modular Tool Ecosystem&lt;/strong&gt;: Expand agent capabilities with plug-and-play tools for web search, data retrieval, and custom operations&lt;/li&gt; &#xA; &lt;li&gt;üìù &lt;strong&gt;Advanced Memory Management&lt;/strong&gt;: Persistent conversation tracking with buffer and vector-based retrieval options&lt;/li&gt; &#xA; &lt;li&gt;üîå &lt;strong&gt;MCP Integration&lt;/strong&gt;: Support for Model Context Protocol (MCP) servers via HTTP and stdio transports&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Enterprise-Ready&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üö¶ &lt;strong&gt;Built-in Guardrails&lt;/strong&gt;: Comprehensive safety mechanisms for responsible AI deployment&lt;/li&gt; &#xA; &lt;li&gt;üìà &lt;strong&gt;Complete Observability&lt;/strong&gt;: Integrated tracing and logging for monitoring and debugging&lt;/li&gt; &#xA; &lt;li&gt;üè¢ &lt;strong&gt;Enterprise Multi-tenancy&lt;/strong&gt;: Securely support multiple organizations with isolated resources&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Development Experience&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üõ†Ô∏è &lt;strong&gt;Structured Task Framework&lt;/strong&gt;: Plan, approve, and execute complex multi-step operations&lt;/li&gt; &#xA; &lt;li&gt;üìÑ &lt;strong&gt;Declarative Configuration&lt;/strong&gt;: Define sophisticated agents and tasks using intuitive YAML definitions&lt;/li&gt; &#xA; &lt;li&gt;üßô &lt;strong&gt;Zero-Effort Bootstrapping&lt;/strong&gt;: Auto-generate complete agent configurations from simple system prompts&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Go 1.23+&lt;/li&gt; &#xA; &lt;li&gt;Redis (optional, for distributed memory)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;Add the SDK to your Go project:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;go get github.com/Ingenimax/agent-sdk-go&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Configuration&lt;/h3&gt; &#xA;&lt;p&gt;The SDK uses environment variables for configuration. Key variables include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;OPENAI_API_KEY&lt;/code&gt;: Your OpenAI API key&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;OPENAI_MODEL&lt;/code&gt;: The model to use (e.g., gpt-4o-mini)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;LOG_LEVEL&lt;/code&gt;: Logging level (debug, info, warn, error)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;REDIS_ADDRESS&lt;/code&gt;: Redis server address (if using Redis for memory)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See &lt;code&gt;.env.example&lt;/code&gt; for a complete list of configuration options.&lt;/p&gt; &#xA;&lt;h2&gt;Usage Examples&lt;/h2&gt; &#xA;&lt;h3&gt;Creating a Simple Agent&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package main&#xA;&#xA;import (&#xA;&#x9;&#34;context&#34;&#xA;&#x9;&#34;fmt&#34;&#xA;&#xA;&#x9;&#34;github.com/Ingenimax/agent-sdk-go/pkg/agent&#34;&#xA;&#x9;&#34;github.com/Ingenimax/agent-sdk-go/pkg/config&#34;&#xA;&#x9;&#34;github.com/Ingenimax/agent-sdk-go/pkg/llm/openai&#34;&#xA;&#x9;&#34;github.com/Ingenimax/agent-sdk-go/pkg/logging&#34;&#xA;&#x9;&#34;github.com/Ingenimax/agent-sdk-go/pkg/memory&#34;&#xA;&#x9;&#34;github.com/Ingenimax/agent-sdk-go/pkg/multitenancy&#34;&#xA;&#x9;&#34;github.com/Ingenimax/agent-sdk-go/pkg/tools&#34;&#xA;&#x9;&#34;github.com/Ingenimax/agent-sdk-go/pkg/tools/websearch&#34;&#xA;)&#xA;&#xA;func main() {&#xA;&#x9;// Create a logger&#xA;&#x9;logger := logging.New()&#xA;&#xA;&#x9;// Get configuration&#xA;&#x9;cfg := config.Get()&#xA;&#xA;&#x9;// Create a new agent with OpenAI&#xA;&#x9;openaiClient := openai.NewClient(cfg.LLM.OpenAI.APIKey,&#xA;&#x9;&#x9;openai.WithLogger(logger))&#xA;&#xA;&#x9;agent, err := agent.NewAgent(&#xA;&#x9;&#x9;agent.WithLLM(openaiClient),&#xA;&#x9;&#x9;agent.WithMemory(memory.NewConversationBuffer()),&#xA;&#x9;&#x9;agent.WithTools(createTools(logger).List()...),&#xA;&#x9;&#x9;agent.WithSystemPrompt(&#34;You are a helpful AI assistant. When you don&#39;t know the answer or need real-time information, use the available tools to find the information.&#34;),&#xA;&#x9;&#x9;agent.WithName(&#34;ResearchAssistant&#34;),&#xA;&#x9;)&#xA;&#x9;if err != nil {&#xA;&#x9;&#x9;logger.Error(context.Background(), &#34;Failed to create agent&#34;, map[string]interface{}{&#34;error&#34;: err.Error()})&#xA;&#x9;&#x9;return&#xA;&#x9;}&#xA;&#xA;&#x9;// Create a context with organization ID and conversation ID&#xA;&#x9;ctx := context.Background()&#xA;&#x9;ctx = multitenancy.WithOrgID(ctx, &#34;default-org&#34;)&#xA;&#x9;ctx = context.WithValue(ctx, memory.ConversationIDKey, &#34;conversation-123&#34;)&#xA;&#xA;&#x9;// Run the agent&#xA;&#x9;response, err := agent.Run(ctx, &#34;What&#39;s the weather in San Francisco?&#34;)&#xA;&#x9;if err != nil {&#xA;&#x9;&#x9;logger.Error(ctx, &#34;Failed to run agent&#34;, map[string]interface{}{&#34;error&#34;: err.Error()})&#xA;&#x9;&#x9;return&#xA;&#x9;}&#xA;&#xA;&#x9;fmt.Println(response)&#xA;}&#xA;&#xA;func createTools(logger logging.Logger) *tools.Registry {&#xA;&#x9;// Get configuration&#xA;&#x9;cfg := config.Get()&#xA;&#xA;&#x9;// Create tools registry&#xA;&#x9;toolRegistry := tools.NewRegistry()&#xA;&#xA;&#x9;// Add web search tool if API keys are available&#xA;&#x9;if cfg.Tools.WebSearch.GoogleAPIKey != &#34;&#34; &amp;amp;&amp;amp; cfg.Tools.WebSearch.GoogleSearchEngineID != &#34;&#34; {&#xA;&#x9;&#x9;searchTool := websearch.New(&#xA;&#x9;&#x9;&#x9;cfg.Tools.WebSearch.GoogleAPIKey,&#xA;&#x9;&#x9;&#x9;cfg.Tools.WebSearch.GoogleSearchEngineID,&#xA;&#x9;&#x9;)&#xA;&#x9;&#x9;toolRegistry.Register(searchTool)&#xA;&#x9;}&#xA;&#xA;&#x9;return toolRegistry&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Creating an Agent with YAML Configuration&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package main&#xA;&#xA;import (&#xA;&#x9;&#34;context&#34;&#xA;&#x9;&#34;fmt&#34;&#xA;&#x9;&#34;log&#34;&#xA;&#x9;&#34;os&#34;&#xA;&#x9;&#34;path/filepath&#34;&#xA;&#x9;&#34;strings&#34;&#xA;&#xA;&#x9;&#34;github.com/Ingenimax/agent-sdk-go/pkg/agent&#34;&#xA;&#x9;&#34;github.com/Ingenimax/agent-sdk-go/pkg/llm/openai&#34;&#xA;)&#xA;&#xA;func main() {&#xA;&#x9;// Get OpenAI API key from environment&#xA;&#x9;apiKey := os.Getenv(&#34;OPENAI_API_KEY&#34;)&#xA;&#x9;if apiKey == &#34;&#34; {&#xA;&#x9;&#x9;log.Fatal(&#34;OpenAI API key not provided. Set OPENAI_API_KEY environment variable.&#34;)&#xA;&#x9;}&#xA;&#xA;&#x9;// Create the LLM client&#xA;&#x9;llm := openai.NewClient(apiKey)&#xA;&#xA;&#x9;// Load agent configurations&#xA;&#x9;agentConfigs, err := agent.LoadAgentConfigsFromFile(&#34;agents.yaml&#34;)&#xA;&#x9;if err != nil {&#xA;&#x9;&#x9;log.Fatalf(&#34;Failed to load agent configurations: %v&#34;, err)&#xA;&#x9;}&#xA;&#xA;&#x9;// Load task configurations&#xA;&#x9;taskConfigs, err := agent.LoadTaskConfigsFromFile(&#34;tasks.yaml&#34;)&#xA;&#x9;if err != nil {&#xA;&#x9;&#x9;log.Fatalf(&#34;Failed to load task configurations: %v&#34;, err)&#xA;&#x9;}&#xA;&#xA;&#x9;// Create variables map for template substitution&#xA;&#x9;variables := map[string]string{&#xA;&#x9;&#x9;&#34;topic&#34;: &#34;Artificial Intelligence&#34;,&#xA;&#x9;}&#xA;&#xA;&#x9;// Create the agent for a specific task&#xA;&#x9;taskName := &#34;research_task&#34;&#xA;&#x9;agent, err := agent.CreateAgentForTask(taskName, agentConfigs, taskConfigs, variables, agent.WithLLM(llm))&#xA;&#x9;if err != nil {&#xA;&#x9;&#x9;log.Fatalf(&#34;Failed to create agent for task: %v&#34;, err)&#xA;&#x9;}&#xA;&#xA;&#x9;// Execute the task&#xA;&#x9;fmt.Printf(&#34;Executing task &#39;%s&#39; with topic &#39;%s&#39;...\n&#34;, taskName, variables[&#34;topic&#34;])&#xA;&#x9;result, err := agent.ExecuteTaskFromConfig(context.Background(), taskName, taskConfigs, variables)&#xA;&#x9;if err != nil {&#xA;&#x9;&#x9;log.Fatalf(&#34;Failed to execute task: %v&#34;, err)&#xA;&#x9;}&#xA;&#xA;&#x9;// Print the result&#xA;&#x9;fmt.Println(&#34;\nTask Result:&#34;)&#xA;&#x9;fmt.Println(result)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Example YAML configurations:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;agents.yaml&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;researcher:&#xA;  role: &amp;gt;&#xA;    {topic} Senior Data Researcher&#xA;  goal: &amp;gt;&#xA;    Uncover cutting-edge developments in {topic}&#xA;  backstory: &amp;gt;&#xA;    You&#39;re a seasoned researcher with a knack for uncovering the latest&#xA;    developments in {topic}. Known for your ability to find the most relevant&#xA;    information and present it in a clear and concise manner.&#xA;&#xA;reporting_analyst:&#xA;  role: &amp;gt;&#xA;    {topic} Reporting Analyst&#xA;  goal: &amp;gt;&#xA;    Create detailed reports based on {topic} data analysis and research findings&#xA;  backstory: &amp;gt;&#xA;    You&#39;re a meticulous analyst with a keen eye for detail. You&#39;re known for&#xA;    your ability to turn complex data into clear and concise reports, making&#xA;    it easy for others to understand and act on the information you provide.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;tasks.yaml&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;research_task:&#xA;  description: &amp;gt;&#xA;    Conduct a thorough research about {topic}&#xA;    Make sure you find any interesting and relevant information given&#xA;    the current year is 2025.&#xA;  expected_output: &amp;gt;&#xA;    A list with 10 bullet points of the most relevant information about {topic}&#xA;  agent: researcher&#xA;&#xA;reporting_task:&#xA;  description: &amp;gt;&#xA;    Review the context you got and expand each topic into a full section for a report.&#xA;    Make sure the report is detailed and contains any and all relevant information.&#xA;  expected_output: &amp;gt;&#xA;    A fully fledged report with the main topics, each with a full section of information.&#xA;    Formatted as markdown without &#39;```&#39;&#xA;  agent: reporting_analyst&#xA;  output_file: &#34;{topic}_report.md&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Auto-Generating Agent Configurations&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package main&#xA;&#xA;import (&#xA;&#x9;&#34;context&#34;&#xA;&#x9;&#34;fmt&#34;&#xA;&#x9;&#34;os&#34;&#xA;&#xA;&#x9;&#34;github.com/Ingenimax/agent-sdk-go/pkg/agent&#34;&#xA;&#x9;&#34;github.com/Ingenimax/agent-sdk-go/pkg/config&#34;&#xA;&#x9;&#34;github.com/Ingenimax/agent-sdk-go/pkg/llm/openai&#34;&#xA;)&#xA;&#xA;func main() {&#xA;&#x9;// Load configuration&#xA;&#x9;cfg := config.Get()&#xA;&#xA;&#x9;// Create LLM client&#xA;&#x9;openaiClient := openai.NewClient(cfg.LLM.OpenAI.APIKey)&#xA;&#xA;&#x9;// Create agent with auto-configuration from system prompt&#xA;&#x9;agent, err := agent.NewAgentWithAutoConfig(&#xA;&#x9;&#x9;context.Background(),&#xA;&#x9;&#x9;agent.WithLLM(openaiClient),&#xA;&#x9;&#x9;agent.WithSystemPrompt(&#34;You are a travel advisor who helps users plan trips and vacations. You specialize in finding hidden gems and creating personalized itineraries based on travelers&#39; preferences.&#34;),&#xA;&#x9;&#x9;agent.WithName(&#34;Travel Assistant&#34;),&#xA;&#x9;)&#xA;&#x9;if err != nil {&#xA;&#x9;&#x9;panic(err)&#xA;&#x9;}&#xA;&#xA;&#x9;// Access the generated configurations&#xA;&#x9;agentConfig := agent.GetGeneratedAgentConfig()&#xA;&#x9;taskConfigs := agent.GetGeneratedTaskConfigs()&#xA;&#xA;&#x9;// Print generated agent details&#xA;&#x9;fmt.Printf(&#34;Generated Agent Role: %s\n&#34;, agentConfig.Role)&#xA;&#x9;fmt.Printf(&#34;Generated Agent Goal: %s\n&#34;, agentConfig.Goal)&#xA;&#x9;fmt.Printf(&#34;Generated Agent Backstory: %s\n&#34;, agentConfig.Backstory)&#xA;&#xA;&#x9;// Print generated tasks&#xA;&#x9;fmt.Println(&#34;\nGenerated Tasks:&#34;)&#xA;&#x9;for taskName, taskConfig := range taskConfigs {&#xA;&#x9;&#x9;fmt.Printf(&#34;- %s: %s\n&#34;, taskName, taskConfig.Description)&#xA;&#x9;}&#xA;&#xA;&#x9;// Save the generated configurations to YAML files&#xA;&#x9;agentConfigMap := map[string]agent.AgentConfig{&#xA;&#x9;&#x9;&#34;Travel Assistant&#34;: *agentConfig,&#xA;&#x9;}&#xA;&#xA;&#x9;// Save agent configs to file&#xA;&#x9;agentYaml, _ := os.Create(&#34;agent_config.yaml&#34;)&#xA;&#x9;defer agentYaml.Close()&#xA;&#x9;agent.SaveAgentConfigsToFile(agentConfigMap, agentYaml)&#xA;&#xA;&#x9;// Save task configs to file&#xA;&#x9;taskYaml, _ := os.Create(&#34;task_config.yaml&#34;)&#xA;&#x9;defer taskYaml.Close()&#xA;&#x9;agent.SaveTaskConfigsToFile(taskConfigs, taskYaml)&#xA;&#xA;&#x9;// Use the auto-configured agent&#xA;&#x9;response, err := agent.Run(context.Background(), &#34;I want to plan a 3-day trip to Tokyo.&#34;)&#xA;&#x9;if err != nil {&#xA;&#x9;&#x9;panic(err)&#xA;&#x9;}&#xA;&#x9;fmt.Println(response)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The auto-configuration feature uses LLM reasoning to derive a complete agent profile and associated tasks from a simple system prompt. The generated configurations include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Agent Profile&lt;/strong&gt;: Role, goal, and backstory that define the agent&#39;s persona&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Task Definitions&lt;/strong&gt;: Specialized tasks the agent can perform, with descriptions and expected outputs&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Reusable YAML&lt;/strong&gt;: Save configurations for reuse in other applications&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This approach dramatically reduces the effort needed to create specialized agents while ensuring consistency and quality.&lt;/p&gt; &#xA;&lt;h3&gt;Using MCP Servers with an Agent&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package main&#xA;&#xA;import (&#xA;&#x9;&#34;context&#34;&#xA;&#x9;&#34;fmt&#34;&#xA;&#x9;&#34;log&#34;&#xA;&#x9;&#34;os&#34;&#xA;&#xA;&#x9;&#34;github.com/Ingenimax/agent-sdk-go/pkg/agent&#34;&#xA;&#x9;&#34;github.com/Ingenimax/agent-sdk-go/pkg/interfaces&#34;&#xA;&#x9;&#34;github.com/Ingenimax/agent-sdk-go/pkg/llm/openai&#34;&#xA;&#x9;&#34;github.com/Ingenimax/agent-sdk-go/pkg/mcp&#34;&#xA;&#x9;&#34;github.com/Ingenimax/agent-sdk-go/pkg/memory&#34;&#xA;&#x9;&#34;github.com/Ingenimax/agent-sdk-go/pkg/multitenancy&#34;&#xA;)&#xA;&#xA;func main() {&#xA;&#x9;logger := log.New(os.Stderr, &#34;AGENT: &#34;, log.LstdFlags)&#xA;&#xA;&#x9;// Create OpenAI LLM client&#xA;&#x9;apiKey := os.Getenv(&#34;OPENAI_API_KEY&#34;)&#xA;&#x9;if apiKey == &#34;&#34; {&#xA;&#x9;&#x9;logger.Fatal(&#34;Please set the OPENAI_API_KEY environment variable.&#34;)&#xA;&#x9;}&#xA;&#x9;llm := openai.NewClient(apiKey, openai.WithModel(&#34;gpt-4o-mini&#34;))&#xA;&#xA;&#x9;// Create MCP servers&#xA;&#x9;var mcpServers []interfaces.MCPServer&#xA;&#xA;&#x9;// Connect to HTTP-based MCP server&#xA;&#x9;httpServer, err := mcp.NewHTTPServer(context.Background(), mcp.HTTPServerConfig{&#xA;&#x9;&#x9;BaseURL: &#34;http://localhost:8083/mcp&#34;,&#xA;&#x9;})&#xA;&#x9;if err != nil {&#xA;&#x9;&#x9;logger.Printf(&#34;Warning: Failed to initialize HTTP MCP server: %v&#34;, err)&#xA;&#x9;} else {&#xA;&#x9;&#x9;mcpServers = append(mcpServers, httpServer)&#xA;&#x9;&#x9;logger.Println(&#34;Successfully initialized HTTP MCP server.&#34;)&#xA;&#x9;}&#xA;&#xA;&#x9;// Connect to stdio-based MCP server&#xA;&#x9;stdioServer, err := mcp.NewStdioServer(context.Background(), mcp.StdioServerConfig{&#xA;&#x9;&#x9;Command: &#34;go&#34;,&#xA;&#x9;&#x9;Args:    []string{&#34;run&#34;, &#34;./server-stdio/main.go&#34;},&#xA;&#x9;})&#xA;&#x9;if err != nil {&#xA;&#x9;&#x9;logger.Printf(&#34;Warning: Failed to initialize STDIO MCP server: %v&#34;, err)&#xA;&#x9;} else {&#xA;&#x9;&#x9;mcpServers = append(mcpServers, stdioServer)&#xA;&#x9;&#x9;logger.Println(&#34;Successfully initialized STDIO MCP server.&#34;)&#xA;&#x9;}&#xA;&#xA;&#x9;// Create agent with MCP server support&#xA;&#x9;myAgent, err := agent.NewAgent(&#xA;&#x9;&#x9;agent.WithLLM(llm),&#xA;&#x9;&#x9;agent.WithMCPServers(mcpServers),&#xA;&#x9;&#x9;agent.WithMemory(memory.NewConversationBuffer()),&#xA;&#x9;&#x9;agent.WithSystemPrompt(&#34;You are an AI assistant that can use tools from MCP servers.&#34;),&#xA;&#x9;&#x9;agent.WithName(&#34;MCPAgent&#34;),&#xA;&#x9;)&#xA;&#x9;if err != nil {&#xA;&#x9;&#x9;logger.Fatalf(&#34;Failed to create agent: %v&#34;, err)&#xA;&#x9;}&#xA;&#xA;&#x9;// Create context with organization and conversation IDs&#xA;&#x9;ctx := context.Background()&#xA;&#x9;ctx = multitenancy.WithOrgID(ctx, &#34;default-org&#34;)&#xA;&#x9;ctx = context.WithValue(ctx, memory.ConversationIDKey, &#34;mcp-demo&#34;)&#xA;&#xA;&#x9;// Run the agent with a query that will use MCP tools&#xA;&#x9;response, err := myAgent.Run(ctx, &#34;What time is it right now?&#34;)&#xA;&#x9;if err != nil {&#xA;&#x9;&#x9;logger.Fatalf(&#34;Agent run failed: %v&#34;, err)&#xA;&#x9;}&#xA;&#xA;&#x9;fmt.Println(&#34;Agent response:&#34;, response)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Architecture&lt;/h2&gt; &#xA;&lt;p&gt;The SDK follows a modular architecture with these key components:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Agent&lt;/strong&gt;: Coordinates the LLM, memory, and tools&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLM&lt;/strong&gt;: Interface to language model providers (OpenAI, Anthropic, Google Vertex AI)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Memory&lt;/strong&gt;: Stores conversation history and context&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Extend the agent&#39;s capabilities&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Vector Store&lt;/strong&gt;: For semantic search and retrieval&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Guardrails&lt;/strong&gt;: Ensures safe and responsible AI usage&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Execution Plan&lt;/strong&gt;: Manages planning, approval, and execution of complex tasks&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Configuration&lt;/strong&gt;: YAML-based agent and task definitions&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Supported LLM Providers&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;OpenAI&lt;/strong&gt;: GPT-4, GPT-3.5, and other OpenAI models&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Anthropic&lt;/strong&gt;: Claude 3.5 Sonnet, Claude 3 Haiku, and other Claude models&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Google Vertex AI&lt;/strong&gt;: Gemini 1.5 Pro, Gemini 1.5 Flash, Gemini 2.0 Flash, and Gemini Pro Vision &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Advanced reasoning modes (none, minimal, comprehensive)&lt;/li&gt; &#xA;   &lt;li&gt;Multimodal capabilities with vision models&lt;/li&gt; &#xA;   &lt;li&gt;Function calling and tool integration&lt;/li&gt; &#xA;   &lt;li&gt;Flexible authentication (ADC or service account files)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;Check out the &lt;code&gt;cmd/examples&lt;/code&gt; directory for complete examples:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Simple Agent&lt;/strong&gt;: Basic agent with system prompt&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;YAML Configuration&lt;/strong&gt;: Defining agents and tasks in YAML&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Auto-Configuration&lt;/strong&gt;: Generating agent configurations from system prompts&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Agent Config Wizard&lt;/strong&gt;: Interactive CLI for creating and using agents&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;MCP Integration&lt;/strong&gt;: Using Model Context Protocol servers with agents&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-LLM Support&lt;/strong&gt;: Examples using OpenAI, Anthropic, and Vertex AI&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Vertex AI Integration&lt;/strong&gt;: Comprehensive examples with Gemini models, reasoning modes, and tools&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;LLM Provider Examples&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;examples/llm/openai/&lt;/code&gt;: OpenAI integration examples&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;examples/llm/anthropic/&lt;/code&gt;: Anthropic Claude integration examples&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;examples/llm/vertex/&lt;/code&gt;: Google Vertex AI integration examples with Gemini models&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the MIT License - see the LICENSE file for details.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;For more detailed information, refer to the following documents:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Ingenimax/agent-sdk-go/main/docs/environment_variables.md&#34;&gt;Environment Variables&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Ingenimax/agent-sdk-go/main/docs/memory.md&#34;&gt;Memory&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Ingenimax/agent-sdk-go/main/docs/tracing.md&#34;&gt;Tracing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Ingenimax/agent-sdk-go/main/docs/vectorstore.md&#34;&gt;Vector Store&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Ingenimax/agent-sdk-go/main/docs/llm.md&#34;&gt;LLM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Ingenimax/agent-sdk-go/main/docs/multitenancy.md&#34;&gt;Multitenancy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Ingenimax/agent-sdk-go/main/docs/task.md&#34;&gt;Task&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Ingenimax/agent-sdk-go/main/docs/tools.md&#34;&gt;Tools&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Ingenimax/agent-sdk-go/main/docs/agent.md&#34;&gt;Agent&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Ingenimax/agent-sdk-go/main/docs/execution_plan.md&#34;&gt;Execution Plan&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Ingenimax/agent-sdk-go/main/docs/guardrails.md&#34;&gt;Guardrails&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Ingenimax/agent-sdk-go/main/docs/mcp.md&#34;&gt;MCP&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>