<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Go Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-23T01:35:37Z</updated>
  <subtitle>Daily Trending of Go in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>XTLS/RealiTLScanner</title>
    <updated>2023-04-23T01:35:37Z</updated>
    <id>tag:github.com,2023-04-23:/XTLS/RealiTLScanner</id>
    <link href="https://github.com/XTLS/RealiTLScanner" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A TLS server scanner for Reality&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Reality - TLS - Scanner&lt;/h1&gt; &#xA;&lt;p&gt;Build&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;go build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Usage&lt;/p&gt; &#xA;&lt;p&gt;Recommend to run this tool locally. It may cause VPS to be flagged if you run scanner in the cloud.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./RealiTLScanner -addr www.microsoft.com&#xA;./RealiTLScanner -addr 20.53.203.50&#xA;./RealiTLScanner -addr 2607:f8b0:4004:c1b::65 -thread 10&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Example&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Reality TLS Scanner running:  20.53.203.50 : 443&#xA; 20.53.203.50:443       -----  Found TLS v 1.3  ALPN h2          CN=*.oneroute.microsoft.com,O=Microsoft Corporation,L=Redmond,ST=WA,C=US&#xA; 20.53.203.48:443       TLS handshake failed:  read tcp 192.168.211.138:37858-&amp;gt;20.53.203.48:443: read: connection reset by peer&#xA; 20.53.203.46:443       -----  Found TLS v 1.3  ALPN h2          CN=apiserver&#xA; 20.53.203.45:443       -----  Found TLS v 1.2  ALPN http/1.1    CN=*.canon.com.au,O=Canon Australia Pty. Ltd.,L=Macquarie Park,ST=New South Wales,C=AU&#xA; 20.53.203.43:443       -----  Found TLS v 1.2  ALPN             CN=bst-c0a0be99-3539-4442-8884-161054d9aba3.bastion.azure.com,O=Microsoft Corporation,L=Redmond,ST=WA,C=US&#xA;Dial failed:  dial tcp 20.53.203.52:443: i/o timeout&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>go-skynet/LocalAI</title>
    <updated>2023-04-23T01:35:37Z</updated>
    <id>tag:github.com,2023-04-23:/go-skynet/LocalAI</id>
    <link href="https://github.com/go-skynet/LocalAI" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ü§ñ Self-hosted, community-driven simple local OpenAI-compatible API written in go. Can be used as a drop-in replacement for OpenAI, running on CPU with consumer-grade hardware. Supports ggml compatible models, for instance: LLaMA, alpaca, gpt4all, vicuna, koala, gpt4all-j, cerebras&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; &lt;br&gt; &lt;img height=&#34;300&#34; src=&#34;https://user-images.githubusercontent.com/2420543/233147843-88697415-6dbf-4368-a862-ab217f9f7342.jpeg&#34;&gt; &lt;br&gt; LocalAI &lt;br&gt; &lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;span&gt;‚ö†&lt;/span&gt; This project has been renamed from &lt;code&gt;llama-cli&lt;/code&gt; to &lt;code&gt;LocalAI&lt;/code&gt; to reflect the fact that we are focusing on a fast drop-in OpenAI API rather on the CLI interface. We think that there are already many projects that can be used as a CLI interface already, for instance &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; and &lt;a href=&#34;https://github.com/nomic-ai/gpt4all&#34;&gt;gpt4all&lt;/a&gt;. If you are were using &lt;code&gt;llama-cli&lt;/code&gt; for CLI interactions and want to keep using it, use older versions or please open up an issue - contributions are welcome!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;LocalAI is a straightforward, drop-in replacement API compatible with OpenAI for local CPU inferencing, based on &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt;, &lt;a href=&#34;https://github.com/nomic-ai/gpt4all&#34;&gt;gpt4all&lt;/a&gt; and &lt;a href=&#34;https://github.com/ggerganov/ggml&#34;&gt;ggml&lt;/a&gt;, including support GPT4ALL-J which is Apache 2.0 Licensed and can be used for commercial purposes.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OpenAI compatible API&lt;/li&gt; &#xA; &lt;li&gt;Supports multiple-models&lt;/li&gt; &#xA; &lt;li&gt;Once loaded the first time, it keep models loaded in memory for faster inference&lt;/li&gt; &#xA; &lt;li&gt;Support for prompt templates&lt;/li&gt; &#xA; &lt;li&gt;Doesn&#39;t shell-out, but uses C bindings for a faster inference and better performance. Uses &lt;a href=&#34;https://github.com/go-skynet/go-llama.cpp&#34;&gt;go-llama.cpp&lt;/a&gt; and &lt;a href=&#34;https://github.com/go-skynet/go-gpt4all-j.cpp&#34;&gt;go-gpt4all-j.cpp&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Discord channel: &lt;a href=&#34;https://discord.gg/uJAeKSAGDy&#34;&gt;Discord&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Model compatibility&lt;/h2&gt; &#xA;&lt;p&gt;It is compatible with the models supported by &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; supports also &lt;a href=&#34;https://github.com/nomic-ai/gpt4all&#34;&gt;GPT4ALL-J&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/lxe/Cerebras-GPT-2.7B-Alpaca-SP-ggml&#34;&gt;cerebras-GPT with ggml&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Tested with:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Vicuna&lt;/li&gt; &#xA; &lt;li&gt;Alpaca&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/nomic-ai/gpt4all&#34;&gt;GPT4ALL&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gpt4all.io/models/ggml-gpt4all-j.bin&#34;&gt;GPT4ALL-J&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Koala&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/lxe/Cerebras-GPT-2.7B-Alpaca-SP-ggml&#34;&gt;cerebras-GPT with ggml&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;It should also be compatible with StableLM and GPTNeoX ggml models (untested)&lt;/p&gt; &#xA;&lt;p&gt;Note: You might need to convert older models to the new format, see &lt;a href=&#34;https://github.com/ggerganov/llama.cpp#using-gpt4all&#34;&gt;here&lt;/a&gt; for instance to run &lt;code&gt;gpt4all&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;code&gt;LocalAI&lt;/code&gt; comes by default as a container image. You can check out all the available images with corresponding tags &lt;a href=&#34;https://quay.io/repository/go-skynet/local-ai?tab=tags&amp;amp;tag=latest&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;The easiest way to run LocalAI is by using &lt;code&gt;docker-compose&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&#xA;git clone https://github.com/go-skynet/LocalAI&#xA;&#xA;cd LocalAI&#xA;&#xA;# copy your models to models/&#xA;cp your-model.bin models/&#xA;&#xA;# (optional) Edit the .env file to set things like context size and threads&#xA;# vim .env&#xA;&#xA;# start with docker-compose&#xA;docker-compose up -d --build&#xA;&#xA;# Now API is accessible at localhost:8080&#xA;curl http://localhost:8080/v1/models&#xA;# {&#34;object&#34;:&#34;list&#34;,&#34;data&#34;:[{&#34;id&#34;:&#34;your-model.bin&#34;,&#34;object&#34;:&#34;model&#34;}]}&#xA;&#xA;curl http://localhost:8080/v1/completions -H &#34;Content-Type: application/json&#34; -d &#39;{&#xA;     &#34;model&#34;: &#34;your-model.bin&#34;,            &#xA;     &#34;prompt&#34;: &#34;A long time ago in a galaxy far, far away&#34;,&#xA;     &#34;temperature&#34;: 0.7&#xA;   }&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Helm Chart Installation (run LocalAI in Kubernetes)&lt;/h2&gt; &#xA;&lt;p&gt;The local-ai Helm chart supports two options for the LocalAI server&#39;s models directory:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Basic deployment with no persistent volume. You must manually update the Deployment to configure your own models directory.&lt;/p&gt; &lt;p&gt;Install the chart with &lt;code&gt;.Values.deployment.volumes.enabled == false&lt;/code&gt; and &lt;code&gt;.Values.dataVolume.enabled == false&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Advanced, two-phase deployment to provision the models directory using a DataVolume. Requires &lt;a href=&#34;https://github.com/kubevirt/containerized-data-importer&#34;&gt;Containerized Data Importer CDI&lt;/a&gt; to be pre-installed in your cluster.&lt;/p&gt; &lt;p&gt;First, install the chart with &lt;code&gt;.Values.deployment.volumes.enabled == false&lt;/code&gt; and &lt;code&gt;.Values.dataVolume.enabled == true&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;helm install local-ai charts/local-ai -n local-ai --create-namespace&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Wait for CDI to create an importer Pod for the DataVolume and for the importer pod to finish provisioning the model archive inside the PV.&lt;/p&gt; &lt;p&gt;Once the PV is provisioned and the importer Pod removed, set &lt;code&gt;.Values.deployment.volumes.enabled == true&lt;/code&gt; and &lt;code&gt;.Values.dataVolume.enabled == false&lt;/code&gt; and upgrade the chart:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;helm upgrade local-ai -n local-ai charts/local-ai&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This will update the local-ai Deployment to mount the PV that was provisioned by the DataVolume.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Prompt templates&lt;/h2&gt; &#xA;&lt;p&gt;The API doesn&#39;t inject a default prompt for talking to the model. You have to use a prompt similar to what&#39;s described in the standford-alpaca docs: &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca#data-release&#34;&gt;https://github.com/tatsu-lab/stanford_alpaca#data-release&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;details&gt;&#xA;  You can use a default template for every model present in your model path, by creating a corresponding file with the `.tmpl` suffix next to your model. For instance, if the model is called `foo.bin`, you can create a sibiling file, `foo.bin.tmpl` which will be used as a default prompt, for instance this can be used with alpaca: &#xA; &lt;pre&gt;&lt;code&gt;Below is an instruction that describes a task. Write a response that appropriately completes the request.&#xA;&#xA;### Instruction:&#xA;{{.Input}}&#xA;&#xA;### Response:&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;See the &lt;a href=&#34;https://github.com/go-skynet/LocalAI/tree/master/prompt-templates&#34;&gt;prompt-templates&lt;/a&gt; directory in this repository for templates for most popular models.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;API&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;LocalAI&lt;/code&gt; provides an API for running text generation as a service, that follows the OpenAI reference and can be used as a drop-in. The models once loaded the first time will be kept in memory.&lt;/p&gt; &#xA;&lt;details&gt;&#xA;  Example of starting the API with `docker`: &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -p 8080:8080 -ti --rm quay.io/go-skynet/local-ai:latest --models-path /path/to/models --context-size 700 --threads 4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;And you&#39;ll see:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê &#xA;‚îÇ                   Fiber v2.42.0                   ‚îÇ &#xA;‚îÇ               http://127.0.0.1:8080               ‚îÇ &#xA;‚îÇ       (bound on host 0.0.0.0 and port 8080)       ‚îÇ &#xA;‚îÇ                                                   ‚îÇ &#xA;‚îÇ Handlers ............. 1  Processes ........... 1 ‚îÇ &#xA;‚îÇ Prefork ....... Disabled  PID ................. 1 ‚îÇ &#xA;‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;You can control the API server options with command line arguments:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;local-api --models-path &amp;lt;model_path&amp;gt; [--address &amp;lt;address&amp;gt;] [--threads &amp;lt;num_threads&amp;gt;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;The API takes takes the following parameters:&lt;/p&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Parameter&lt;/th&gt; &#xA;    &lt;th&gt;Environment Variable&lt;/th&gt; &#xA;    &lt;th&gt;Default Value&lt;/th&gt; &#xA;    &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;models-path&lt;/td&gt; &#xA;    &lt;td&gt;MODELS_PATH&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;The path where you have models (ending with &lt;code&gt;.bin&lt;/code&gt;).&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;threads&lt;/td&gt; &#xA;    &lt;td&gt;THREADS&lt;/td&gt; &#xA;    &lt;td&gt;Number of Physical cores&lt;/td&gt; &#xA;    &lt;td&gt;The number of threads to use for text generation.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;address&lt;/td&gt; &#xA;    &lt;td&gt;ADDRESS&lt;/td&gt; &#xA;    &lt;td&gt;:8080&lt;/td&gt; &#xA;    &lt;td&gt;The address and port to listen on.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;context-size&lt;/td&gt; &#xA;    &lt;td&gt;CONTEXT_SIZE&lt;/td&gt; &#xA;    &lt;td&gt;512&lt;/td&gt; &#xA;    &lt;td&gt;Default token context size.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;p&gt;Once the server is running, you can start making requests to it using HTTP, using the OpenAI API.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Supported OpenAI API endpoints&lt;/h3&gt; &#xA;&lt;p&gt;You can check out the &lt;a href=&#34;https://platform.openai.com/docs/api-reference/chat/create&#34;&gt;OpenAI API reference&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Following the list of endpoints/parameters supported.&lt;/p&gt; &#xA;&lt;h4&gt;Chat completions&lt;/h4&gt; &#xA;&lt;p&gt;For example, to generate a chat completion, you can send a POST request to the &lt;code&gt;/v1/chat/completions&lt;/code&gt; endpoint with the instruction as the request body:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl http://localhost:8080/v1/chat/completions -H &#34;Content-Type: application/json&#34; -d &#39;{&#xA;     &#34;model&#34;: &#34;ggml-koala-7b-model-q4_0-r2.bin&#34;,&#xA;     &#34;messages&#34;: [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Say this is a test!&#34;}],&#xA;     &#34;temperature&#34;: 0.7&#xA;   }&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Available additional parameters: &lt;code&gt;top_p&lt;/code&gt;, &lt;code&gt;top_k&lt;/code&gt;, &lt;code&gt;max_tokens&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Completions&lt;/h4&gt; &#xA;&lt;p&gt;For example, to generate a comletion, you can send a POST request to the &lt;code&gt;/v1/completions&lt;/code&gt; endpoint with the instruction as the request body:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl http://localhost:8080/v1/completions -H &#34;Content-Type: application/json&#34; -d &#39;{&#xA;     &#34;model&#34;: &#34;ggml-koala-7b-model-q4_0-r2.bin&#34;,&#xA;     &#34;prompt&#34;: &#34;A long time ago in a galaxy far, far away&#34;,&#xA;     &#34;temperature&#34;: 0.7&#xA;   }&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Available additional parameters: &lt;code&gt;top_p&lt;/code&gt;, &lt;code&gt;top_k&lt;/code&gt;, &lt;code&gt;max_tokens&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;List models&lt;/h4&gt; &#xA;&lt;p&gt;You can list all the models available with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl http://localhost:8080/v1/models&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Using other models&lt;/h2&gt; &#xA;&lt;p&gt;gpt4all (&lt;a href=&#34;https://github.com/nomic-ai/gpt4all&#34;&gt;https://github.com/nomic-ai/gpt4all&lt;/a&gt;) works as well, however the original model needs to be converted (same applies for old alpaca models, too):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget -O tokenizer.model https://huggingface.co/decapoda-research/llama-30b-hf/resolve/main/tokenizer.model&#xA;mkdir models&#xA;cp gpt4all.. models/&#xA;git clone https://gist.github.com/eiz/828bddec6162a023114ce19146cb2b82&#xA;pip install sentencepiece&#xA;python 828bddec6162a023114ce19146cb2b82/gistfile1.txt models tokenizer.model&#xA;# There will be a new model with the &#34;.tmp&#34; extension, you have to use that one!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Windows compatibility&lt;/h3&gt; &#xA;&lt;p&gt;It should work, however you need to make sure you give enough resources to the container. See &lt;a href=&#34;https://github.com/go-skynet/LocalAI/issues/2&#34;&gt;https://github.com/go-skynet/LocalAI/issues/2&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Build locally&lt;/h3&gt; &#xA;&lt;p&gt;Pre-built images might fit well for most of the modern hardware, however you can and might need to build the images manually.&lt;/p&gt; &#xA;&lt;p&gt;In order to build the &lt;code&gt;LocalAI&lt;/code&gt; container image locally you can use &lt;code&gt;docker&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# build the image&#xA;docker build -t LocalAI .&#xA;docker run LocalAI&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or build the binary with &lt;code&gt;make&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Short-term roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Mimic OpenAI API (&lt;a href=&#34;https://github.com/go-skynet/LocalAI/issues/10&#34;&gt;https://github.com/go-skynet/LocalAI/issues/10&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Binary releases (&lt;a href=&#34;https://github.com/go-skynet/LocalAI/issues/6&#34;&gt;https://github.com/go-skynet/LocalAI/issues/6&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Upstream our golang bindings to llama.cpp (&lt;a href=&#34;https://github.com/ggerganov/llama.cpp/issues/351&#34;&gt;https://github.com/ggerganov/llama.cpp/issues/351&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Multi-model support&lt;/li&gt; &#xA; &lt;li&gt;Have a webUI!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;MIT&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;https://github.com/tatsu-lab/stanford_alpaca&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/cornelk/llama-go&#34;&gt;https://github.com/cornelk/llama-go&lt;/a&gt; for the initial ideas&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/antimatter15/alpaca.cpp&#34;&gt;https://github.com/antimatter15/alpaca.cpp&lt;/a&gt; for the light model version (this is compatible and tested only with that checkpoint model!)&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>coredns/coredns</title>
    <updated>2023-04-23T01:35:37Z</updated>
    <id>tag:github.com,2023-04-23:/coredns/coredns</id>
    <link href="https://github.com/coredns/coredns" rel="alternate"></link>
    <summary type="html">&lt;p&gt;CoreDNS is a DNS server that chains plugins&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://coredns.io&#34;&gt;&lt;img src=&#34;https://coredns.io/images/CoreDNS_Colour_Horizontal.png&#34; alt=&#34;CoreDNS&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://godoc.org/github.com/coredns/coredns&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/godoc-reference-blue.svg?sanitize=true&#34; alt=&#34;Documentation&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://github.com/coredns/coredns/actions/workflows/codeql-analysis.yml/badge.svg?sanitize=true&#34; alt=&#34;CodeQL&#34;&gt; &lt;img src=&#34;https://github.com/coredns/coredns/actions/workflows/go.test.yml/badge.svg?sanitize=true&#34; alt=&#34;Go Tests&#34;&gt; &lt;a href=&#34;https://circleci.com/gh/coredns/coredns&#34;&gt;&lt;img src=&#34;https://circleci.com/gh/coredns/coredns.svg?style=shield&#34; alt=&#34;CircleCI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/github/coredns/coredns?branch=master&#34;&gt;&lt;img src=&#34;https://img.shields.io/codecov/c/github/coredns/coredns/master.svg?sanitize=true&#34; alt=&#34;Code Coverage&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/coredns/coredns&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/pulls/coredns/coredns.svg?sanitize=true&#34; alt=&#34;Docker Pulls&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://goreportcard.com/report/coredns/coredns&#34;&gt;&lt;img src=&#34;https://goreportcard.com/badge/github.com/coredns/coredns&#34; alt=&#34;Go Report Card&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://bestpractices.coreinfrastructure.org/projects/1250&#34;&gt;&lt;img src=&#34;https://bestpractices.coreinfrastructure.org/projects/1250/badge&#34; alt=&#34;CII Best Practices&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;CoreDNS is a DNS server/forwarder, written in Go, that chains &lt;a href=&#34;https://coredns.io/plugins&#34;&gt;plugins&lt;/a&gt;. Each plugin performs a (DNS) function.&lt;/p&gt; &#xA;&lt;p&gt;CoreDNS is a &lt;a href=&#34;https://cncf.io&#34;&gt;Cloud Native Computing Foundation&lt;/a&gt; graduated project.&lt;/p&gt; &#xA;&lt;p&gt;CoreDNS is a fast and flexible DNS server. The key word here is &lt;em&gt;flexible&lt;/em&gt;: with CoreDNS you are able to do what you want with your DNS data by utilizing plugins. If some functionality is not provided out of the box you can add it by &lt;a href=&#34;https://coredns.io/explugins&#34;&gt;writing a plugin&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;CoreDNS can listen for DNS requests coming in over UDP/TCP (go&#39;old DNS), TLS (&lt;a href=&#34;https://tools.ietf.org/html/rfc7858&#34;&gt;RFC 7858&lt;/a&gt;), also called DoT, DNS over HTTP/2 - DoH - (&lt;a href=&#34;https://tools.ietf.org/html/rfc8484&#34;&gt;RFC 8484&lt;/a&gt;) and &lt;a href=&#34;https://grpc.io&#34;&gt;gRPC&lt;/a&gt; (not a standard).&lt;/p&gt; &#xA;&lt;p&gt;Currently CoreDNS is able to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Serve zone data from a file; both DNSSEC (NSEC only) and DNS are supported (&lt;em&gt;file&lt;/em&gt; and &lt;em&gt;auto&lt;/em&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Retrieve zone data from primaries, i.e., act as a secondary server (AXFR only) (&lt;em&gt;secondary&lt;/em&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Sign zone data on-the-fly (&lt;em&gt;dnssec&lt;/em&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Load balancing of responses (&lt;em&gt;loadbalance&lt;/em&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Allow for zone transfers, i.e., act as a primary server (&lt;em&gt;file&lt;/em&gt; + &lt;em&gt;transfer&lt;/em&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Automatically load zone files from disk (&lt;em&gt;auto&lt;/em&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Caching of DNS responses (&lt;em&gt;cache&lt;/em&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Use etcd as a backend (replacing &lt;a href=&#34;https://github.com/skynetservices/skydns&#34;&gt;SkyDNS&lt;/a&gt;) (&lt;em&gt;etcd&lt;/em&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Use k8s (kubernetes) as a backend (&lt;em&gt;kubernetes&lt;/em&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Serve as a proxy to forward queries to some other (recursive) nameserver (&lt;em&gt;forward&lt;/em&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Provide metrics (by using Prometheus) (&lt;em&gt;prometheus&lt;/em&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Provide query (&lt;em&gt;log&lt;/em&gt;) and error (&lt;em&gt;errors&lt;/em&gt;) logging.&lt;/li&gt; &#xA; &lt;li&gt;Integrate with cloud providers (&lt;em&gt;route53&lt;/em&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Support the CH class: &lt;code&gt;version.bind&lt;/code&gt; and friends (&lt;em&gt;chaos&lt;/em&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Support the RFC 5001 DNS name server identifier (NSID) option (&lt;em&gt;nsid&lt;/em&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Profiling support (&lt;em&gt;pprof&lt;/em&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Rewrite queries (qtype, qclass and qname) (&lt;em&gt;rewrite&lt;/em&gt; and &lt;em&gt;template&lt;/em&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Block ANY queries (&lt;em&gt;any&lt;/em&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Provide DNS64 IPv6 Translation (&lt;em&gt;dns64&lt;/em&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;And more. Each of the plugins is documented. See &lt;a href=&#34;https://coredns.io/plugins&#34;&gt;coredns.io/plugins&lt;/a&gt; for all in-tree plugins, and &lt;a href=&#34;https://coredns.io/explugins&#34;&gt;coredns.io/explugins&lt;/a&gt; for all out-of-tree plugins.&lt;/p&gt; &#xA;&lt;h2&gt;Compilation from Source&lt;/h2&gt; &#xA;&lt;p&gt;To compile CoreDNS, we assume you have a working Go setup. See various tutorials if you don‚Äôt have that already configured.&lt;/p&gt; &#xA;&lt;p&gt;First, make sure your golang version is 1.17 or higher as &lt;code&gt;go mod&lt;/code&gt; support and other api is needed. See &lt;a href=&#34;https://github.com/golang/go/wiki/Modules&#34;&gt;here&lt;/a&gt; for &lt;code&gt;go mod&lt;/code&gt; details. Then, check out the project and run &lt;code&gt;make&lt;/code&gt; to compile the binary:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/coredns/coredns&#xA;$ cd coredns&#xA;$ make&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This should yield a &lt;code&gt;coredns&lt;/code&gt; binary.&lt;/p&gt; &#xA;&lt;h2&gt;Compilation with Docker&lt;/h2&gt; &#xA;&lt;p&gt;CoreDNS requires Go to compile. However, if you already have docker installed and prefer not to setup a Go environment, you could build CoreDNS easily:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ docker run --rm -i -t -v $PWD:/v -w /v golang:1.18 make&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The above command alone will have &lt;code&gt;coredns&lt;/code&gt; binary generated.&lt;/p&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;When starting CoreDNS without any configuration, it loads the &lt;a href=&#34;https://coredns.io/plugins/whoami&#34;&gt;&lt;em&gt;whoami&lt;/em&gt;&lt;/a&gt; and &lt;a href=&#34;https://coredns.io/plugins/log&#34;&gt;&lt;em&gt;log&lt;/em&gt;&lt;/a&gt; plugins and starts listening on port 53 (override with &lt;code&gt;-dns.port&lt;/code&gt;), it should show the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;.:53&#xA;CoreDNS-1.6.6&#xA;linux/amd64, go1.16.10, aa8c32&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The following could be used to query the CoreDNS server that is running now:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;dig @127.0.0.1 -p 53 www.example.com&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Any query sent to port 53 should return some information; your sending address, port and protocol used. The query should also be logged to standard output.&lt;/p&gt; &#xA;&lt;p&gt;The configuration of CoreDNS is done through a file named &lt;code&gt;Corefile&lt;/code&gt;. When CoreDNS starts, it will look for the &lt;code&gt;Corefile&lt;/code&gt; from the current working directory. A &lt;code&gt;Corefile&lt;/code&gt; for CoreDNS server that listens on port &lt;code&gt;53&lt;/code&gt; and enables &lt;code&gt;whoami&lt;/code&gt; plugin is:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-corefile&#34;&gt;.:53 {&#xA;    whoami&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Sometimes port number 53 is occupied by system processes. In that case you can start the CoreDNS server while modifying the &lt;code&gt;Corefile&lt;/code&gt; as given below so that the CoreDNS server starts on port 1053.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-corefile&#34;&gt;.:1053 {&#xA;    whoami&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you have a &lt;code&gt;Corefile&lt;/code&gt; without a port number specified it will, by default, use port 53, but you can override the port with the &lt;code&gt;-dns.port&lt;/code&gt; flag: &lt;code&gt;coredns -dns.port 1053&lt;/code&gt;, runs the server on port 1053.&lt;/p&gt; &#xA;&lt;p&gt;You may import other text files into the &lt;code&gt;Corefile&lt;/code&gt; using the &lt;em&gt;import&lt;/em&gt; directive. You can use globs to match multiple files with a single &lt;em&gt;import&lt;/em&gt; directive.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;.:53 {&#xA;    import example1.txt&#xA;}&#xA;import example2.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can use environment variables in the &lt;code&gt;Corefile&lt;/code&gt; with &lt;code&gt;{$VARIABLE}&lt;/code&gt;. Note that each environment variable is inserted into the &lt;code&gt;Corefile&lt;/code&gt; as a single token. For example, an environment variable with a space in it will be treated as a single token, not as two separate tokens.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;.:53 {&#xA;    {$ENV_VAR}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A Corefile for a CoreDNS server that forward any queries to an upstream DNS (e.g., &lt;code&gt;8.8.8.8&lt;/code&gt;) is as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-corefile&#34;&gt;.:53 {&#xA;    forward . 8.8.8.8:53&#xA;    log&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Start CoreDNS and then query on that port (53). The query should be forwarded to 8.8.8.8 and the response will be returned. Each query should also show up in the log which is printed on standard output.&lt;/p&gt; &#xA;&lt;p&gt;To serve the (NSEC) DNSSEC-signed &lt;code&gt;example.org&lt;/code&gt; on port 1053, with errors and logging sent to standard output. Allow zone transfers to everybody, but specifically mention 1 IP address so that CoreDNS can send notifies to it.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;example.org:1053 {&#xA;    file /var/lib/coredns/example.org.signed&#xA;    transfer {&#xA;        to * 2001:500:8f::53&#xA;    }&#xA;    errors&#xA;    log&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Serve &lt;code&gt;example.org&lt;/code&gt; on port 1053, but forward everything that does &lt;em&gt;not&lt;/em&gt; match &lt;code&gt;example.org&lt;/code&gt; to a recursive nameserver &lt;em&gt;and&lt;/em&gt; rewrite ANY queries to HINFO.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;example.org:1053 {&#xA;    file /var/lib/coredns/example.org.signed&#xA;    transfer {&#xA;        to * 2001:500:8f::53&#xA;    }&#xA;    errors&#xA;    log&#xA;}&#xA;&#xA;. {&#xA;    any&#xA;    forward . 8.8.8.8:53&#xA;    errors&#xA;    log&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;IP addresses are also allowed. They are automatically converted to reverse zones:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-corefile&#34;&gt;10.0.0.0/24 {&#xA;    whoami&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Means you are authoritative for &lt;code&gt;0.0.10.in-addr.arpa.&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This also works for IPv6 addresses. If for some reason you want to serve a zone named &lt;code&gt;10.0.0.0/24&lt;/code&gt; add the closing dot: &lt;code&gt;10.0.0.0/24.&lt;/code&gt; as this also stops the conversion.&lt;/p&gt; &#xA;&lt;p&gt;This even works for CIDR (See RFC 1518 and 1519) addressing, i.e. &lt;code&gt;10.0.0.0/25&lt;/code&gt;, CoreDNS will then check if the &lt;code&gt;in-addr&lt;/code&gt; request falls in the correct range.&lt;/p&gt; &#xA;&lt;p&gt;Listening on TLS (DoT) and for gRPC? Use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-corefile&#34;&gt;tls://example.org grpc://example.org {&#xA;    whoami&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And for DNS over HTTP/2 (DoH) use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-corefile&#34;&gt;https://example.org {&#xA;    whoami&#xA;    tls mycert mykey&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;in this setup, the CoreDNS will be responsible for TLS termination&lt;/p&gt; &#xA;&lt;p&gt;you can also start DNS server serving DoH without TLS termination (plain HTTP), but beware that in such scenario there has to be some kind of TLS termination proxy before CoreDNS instance, which forwards DNS requests otherwise clients will not be able to communicate via DoH with the server&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-corefile&#34;&gt;https://example.org {&#xA;    whoami&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Specifying ports works in the same way:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;grpc://example.org:1443 https://example.org:1444 {&#xA;    # ...&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When no transport protocol is specified the default &lt;code&gt;dns://&lt;/code&gt; is assumed.&lt;/p&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;p&gt;We&#39;re most active on Github (and Slack):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Github: &lt;a href=&#34;https://github.com/coredns/coredns&#34;&gt;https://github.com/coredns/coredns&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Slack: #coredns on &lt;a href=&#34;https://slack.cncf.io&#34;&gt;https://slack.cncf.io&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;More resources can be found:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Website: &lt;a href=&#34;https://coredns.io&#34;&gt;https://coredns.io&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Blog: &lt;a href=&#34;https://blog.coredns.io&#34;&gt;https://blog.coredns.io&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Twitter: &lt;a href=&#34;https://twitter.com/corednsio&#34;&gt;@corednsio&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Mailing list/group: &lt;a href=&#34;mailto:coredns-discuss@googlegroups.com&#34;&gt;coredns-discuss@googlegroups.com&lt;/a&gt; (not very active)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contribution guidelines&lt;/h2&gt; &#xA;&lt;p&gt;If you want to contribute to CoreDNS, be sure to review the &lt;a href=&#34;https://raw.githubusercontent.com/coredns/coredns/master/.github/CONTRIBUTING.md&#34;&gt;contribution guidelines&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Deployment&lt;/h2&gt; &#xA;&lt;p&gt;Examples for deployment via systemd and other use cases can be found in the &lt;a href=&#34;https://github.com/coredns/deployment&#34;&gt;deployment repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Deprecation Policy&lt;/h2&gt; &#xA;&lt;p&gt;When there is a backwards incompatible change in CoreDNS the following process is followed:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Release x.y.z: Announce that in the next release we will make backward incompatible changes.&lt;/li&gt; &#xA; &lt;li&gt;Release x.y+1.0: Increase the minor version and set the patch version to 0. Make the changes, but allow the old configuration to be parsed. I.e. CoreDNS will start from an unchanged Corefile.&lt;/li&gt; &#xA; &lt;li&gt;Release x.y+1.1: Increase the patch version to 1. Remove the lenient parsing, so CoreDNS will not start if those features are still used.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;E.g. 1.3.1 announce a change. 1.4.0 a new release with the change but backward compatible config. And finally 1.4.1 that removes the config workarounds.&lt;/p&gt; &#xA;&lt;h2&gt;Security&lt;/h2&gt; &#xA;&lt;h3&gt;Security Audits&lt;/h3&gt; &#xA;&lt;p&gt;Third party security audits have been performed by:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cure53.de&#34;&gt;Cure53&lt;/a&gt; in March 2018. &lt;a href=&#34;https://coredns.io/assets/DNS-01-report.pdf&#34;&gt;Full Report&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.trailofbits.com&#34;&gt;Trail of Bits&lt;/a&gt; in March 2022. &lt;a href=&#34;https://github.com/trailofbits/publications/raw/master/reviews/CoreDNS.pdf&#34;&gt;Full Report&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Reporting security vulnerabilities&lt;/h3&gt; &#xA;&lt;p&gt;If you find a security vulnerability or any security related issues, please DO NOT file a public issue, instead send your report privately to &lt;code&gt;security@coredns.io&lt;/code&gt;. Security reports are greatly appreciated and we will publicly thank you for it.&lt;/p&gt; &#xA;&lt;p&gt;Please consult &lt;a href=&#34;https://github.com/coredns/coredns/raw/master/.github/SECURITY.md&#34;&gt;security vulnerability disclosures and security fix and release process document&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>