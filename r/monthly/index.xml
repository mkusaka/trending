<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub R Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-03-01T02:00:41Z</updated>
  <subtitle>Monthly Trending of R in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>stan-dev/rstan</title>
    <updated>2025-03-01T02:00:41Z</updated>
    <id>tag:github.com,2025-03-01:/stan-dev/rstan</id>
    <link href="https://github.com/stan-dev/rstan" rel="alternate"></link>
    <summary type="html">&lt;p&gt;RStan, the R interface to Stan&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;RStan &lt;img src=&#34;https://raw.githubusercontent.com/stan-dev/rstan/develop/rstan/rstan/man/figures/stanlogo.png&#34; align=&#34;right&#34; width=&#34;120&#34;&gt;&lt;/h1&gt; &#xA;&lt;!-- badges: start --&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=rstan&#34;&gt;&lt;img src=&#34;https://www.r-pkg.org/badges/version/rstan?color=blue&#34; alt=&#34;CRAN_Status_Badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://cran.rstudio.com/package=rstan&#34;&gt;&lt;img src=&#34;https://cranlogs.r-pkg.org/badges/rstan?color=blue&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://depsy.org/package/r/rstan&#34;&gt;&lt;img src=&#34;http://depsy.org/api/package/cran/rstan/badge.svg?sanitize=true&#34; alt=&#34;Research software impact&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/stan-dev/rstan/actions&#34;&gt;&lt;img src=&#34;https://github.com/stan-dev/rstan/workflows/R-CMD-check/badge.svg?sanitize=true&#34; alt=&#34;R-CMD-check&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- badges: end --&gt; &#xA;&lt;p&gt;&lt;strong&gt;RStan&lt;/strong&gt; is the R interface to &lt;a href=&#34;https://mc-stan.org&#34;&gt;Stan&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Quick links&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mc-stan.org/rstan/&#34;&gt;mc-stan.org/rstan&lt;/a&gt; (online RStan documentation, vignettes)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mc-stan.org/users/documentation/&#34;&gt;Stan documentation&lt;/a&gt; (language manual, case studies, and more)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discourse.mc-stan.org&#34;&gt;Ask a question&lt;/a&gt; (Stan Forums on Discourse)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/stan-dev/rstan/issues&#34;&gt;Open an issue&lt;/a&gt; (GitHub issues for bug reports, feature requests)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Getting Started&lt;/h3&gt; &#xA;&lt;p&gt;For installation instructions and other tips on getting started with RStan see&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started&#34;&gt;RStan Getting Started&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Several Stan users have also contributed translations of the &lt;em&gt;RStan Getting Started&lt;/em&gt; page:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started-(Fran%C3%A7ais)&#34;&gt;RStan Getting Started (French)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started-(Japanese)&#34;&gt;RStan Getting Started (Japanese)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started-(%E7%B9%81%E9%AB%94%E4%B8%AD%E6%96%87)&#34;&gt;RStan Getting Started (繁體中文)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started-(Portugu%C3%AAs)&#34;&gt;RStan Getting Started (Portuguese)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Source Repository&lt;/h3&gt; &#xA;&lt;p&gt;RStan&#39;s source code repository is hosted here on GitHub. Stan&#39;s source repository is defined as a submodule. See &lt;a href=&#34;https://github.com/stan-dev/rstan/wiki/How-to-work-with-the-stan-submodule-in-rstan-repo%3F&#34;&gt;how to work with stan submodule in rstan repo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Licensing&lt;/h3&gt; &#xA;&lt;p&gt;RStan is licensed under GPLv3. The Stan code packaged in RStan is licensed under new BSD.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>tidyverse/ellmer</title>
    <updated>2025-03-01T02:00:41Z</updated>
    <id>tag:github.com,2025-03-01:/tidyverse/ellmer</id>
    <link href="https://github.com/tidyverse/ellmer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Call LLM APIs from R&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ellmer &lt;a href=&#34;https://ellmer.tidyverse.org&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tidyverse/ellmer/main/man/figures/logo.png&#34; align=&#34;right&#34; height=&#34;138&#34; alt=&#34;ellmer website&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;!-- badges: start --&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://lifecycle.r-lib.org/articles/stages.html#experimental&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/lifecycle-experimental-orange.svg?sanitize=true&#34; alt=&#34;Lifecycle: experimental&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/tidyverse/ellmer/actions/workflows/R-CMD-check.yaml&#34;&gt;&lt;img src=&#34;https://github.com/tidyverse/ellmer/actions/workflows/R-CMD-check.yaml/badge.svg?sanitize=true&#34; alt=&#34;R-CMD-check&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- badges: end --&gt; &#xA;&lt;p&gt;ellmer makes it easy to use large language models (LLM) from R. It supports a wide variety of LLM providers and implements a rich set of features including streaming outputs, tool/function calling, structured data extraction, and more.&lt;/p&gt; &#xA;&lt;p&gt;(Looking for something similar to ellmer for python? Check out &lt;a href=&#34;https://github.com/posit-dev/chatlas&#34;&gt;chatlas&lt;/a&gt;!)&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;You can install ellmer from CRAN with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&#34;ellmer&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Providers&lt;/h2&gt; &#xA;&lt;p&gt;ellmer supports a wide variety of model providers:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Anthropic’s Claude: &lt;code&gt;chat_claude()&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;AWS Bedrock: &lt;code&gt;chat_bedrock()&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Azure OpenAI: &lt;code&gt;chat_azure()&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Databricks: &lt;code&gt;chat_databricks()&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;DeepSeek: &lt;code&gt;chat_deepseek()&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;GitHub model marketplace: &lt;code&gt;chat_github()&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Google Gemini: &lt;code&gt;chat_gemini()&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Groq: &lt;code&gt;chat_groq()&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Ollama: &lt;code&gt;chat_ollama()&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;OpenAI: &lt;code&gt;chat_openai()&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;OpenRouter: &lt;code&gt;chat_openrouter()&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;perplexity.ai: &lt;code&gt;chat_perplexity()&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Snowflake Cortex: &lt;code&gt;chat_snowflake()&lt;/code&gt; and &lt;code&gt;chat_cortex_analyst()&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;VLLM: &lt;code&gt;chat_vllm()&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Provider/model choice&lt;/h3&gt; &#xA;&lt;p&gt;If you’re using ellmer inside an organisation, you may have internal policies that limit you to models from big cloud providers, e.g.&amp;nbsp;&lt;code&gt;chat_azure()&lt;/code&gt;, &lt;code&gt;chat_bedrock()&lt;/code&gt;, &lt;code&gt;chat_databricks()&lt;/code&gt;, or &lt;code&gt;chat_snowflake()&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you’re using ellmer for your own exploration, you’ll have a lot more freedom, so we have a few recommendations to help you get started:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;chat_openai()&lt;/code&gt; or &lt;code&gt;chat_claude()&lt;/code&gt; are good places to start. &lt;code&gt;chat_openai()&lt;/code&gt; defaults to &lt;strong&gt;GPT-4o&lt;/strong&gt;, but you can use &lt;code&gt;model = &#34;gpt-4o-mini&#34;&lt;/code&gt; for a cheaper, lower-quality model, or &lt;code&gt;model = &#34;o1-mini&#34;&lt;/code&gt; for more complex reasoning. &lt;code&gt;chat_claude()&lt;/code&gt; is also good; it defaults to &lt;strong&gt;Claude 3.5 Sonnet&lt;/strong&gt;, which we have found to be particularly good at writing code.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;chat_gemini()&lt;/code&gt; is great for large prompts because it has a much larger context window than other models. It allows up to 1 million tokens, compared to Claude 3.5 Sonnet’s 200k and GPT-4o’s 128k. It also comes with a generous free tier (with the downside that &lt;a href=&#34;https://ai.google.dev/gemini-api/terms#unpaid-services&#34;&gt;your data is used&lt;/a&gt; to improve the model).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;chat_ollama()&lt;/code&gt;, which uses &lt;a href=&#34;https://ollama.com&#34;&gt;Ollama&lt;/a&gt;, allows you to run models on your own computer. While the biggest models you can run locally aren’t as good as the state of the art hosted models, they don’t share your data and are effectively free.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Authentication&lt;/h3&gt; &#xA;&lt;p&gt;Authentication works a little differently depending on the provider. A few popular ones (including OpenAI and Anthropic) require you to obtain an API key. We recommend you save it in an environment variable rather than using it directly in your code, and if you deploy an app or report that uses ellmer to another system, you’ll need to ensure that this environment variable is available there, too.&lt;/p&gt; &#xA;&lt;p&gt;ellmer also automatically detects many of the OAuth or IAM-based credentials used by the big cloud providers (currently &lt;code&gt;chat_azure()&lt;/code&gt;, &lt;code&gt;chat_bedrock()&lt;/code&gt;, &lt;code&gt;chat_databricks()&lt;/code&gt;, &lt;code&gt;chat_snowflake()&lt;/code&gt;). That includes credentials for these platforms managed by &lt;a href=&#34;https://docs.posit.co/ide/server-pro/user/posit-workbench/managed-credentials/managed-credentials.html&#34;&gt;Posit Workbench&lt;/a&gt; and &lt;a href=&#34;https://docs.posit.co/connect/user/oauth-integrations/#adding-oauth-integrations-to-deployed-content&#34;&gt;Posit Connect&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you find cases where ellmer cannot detect credentials from one of these cloud providers, feel free to open an issue; we’re happy to add more auth mechanisms if needed.&lt;/p&gt; &#xA;&lt;h2&gt;Using ellmer&lt;/h2&gt; &#xA;&lt;p&gt;You can work with ellmer in several different ways, depending on whether you are working interactively or programmatically. They all start with creating a new chat object:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(ellmer)&#xA;&#xA;chat &amp;lt;- chat_openai(&#xA;  model = &#34;gpt-4o-mini&#34;,&#xA;  system_prompt = &#34;You are a friendly but terse assistant.&#34;,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Chat objects are stateful &lt;a href=&#34;https://r6.r-lib.org&#34;&gt;R6 objects&lt;/a&gt;: they retain the context of the conversation, so each new query builds on the previous ones. You call their methods with &lt;code&gt;$&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Interactive chat console&lt;/h3&gt; &#xA;&lt;p&gt;The most interactive and least programmatic way of using ellmer is to chat directly in your R console or browser with &lt;code&gt;live_console(chat)&lt;/code&gt; or &lt;code&gt;live_browser()&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;live_console(chat)&#xA;#&amp;gt; ╔════════════════════════════════════════════════════════╗&#xA;#&amp;gt; ║  Entering chat console. Use &#34;&#34;&#34; for multi-line input.  ║&#xA;#&amp;gt; ║  Press Ctrl+C to quit.                                 ║&#xA;#&amp;gt; ╚════════════════════════════════════════════════════════╝&#xA;#&amp;gt; &amp;gt;&amp;gt;&amp;gt; Who were the original creators of R?&#xA;#&amp;gt; R was originally created by Ross Ihaka and Robert Gentleman at the University of&#xA;#&amp;gt; Auckland, New Zealand.&#xA;#&amp;gt;&#xA;#&amp;gt; &amp;gt;&amp;gt;&amp;gt; When was that?&#xA;#&amp;gt; R was initially released in 1995. Development began a few years prior to that,&#xA;#&amp;gt; in the early 1990s.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Keep in mind that the chat object retains state, so when you enter the chat console, any previous interactions with that chat object are still part of the conversation, and any interactions you have in the chat console will persist after you exit back to the R prompt. This is true regardless of which chat function you use.&lt;/p&gt; &#xA;&lt;h3&gt;Interactive method call&lt;/h3&gt; &#xA;&lt;p&gt;The second most interactive way to chat is to call the &lt;code&gt;chat()&lt;/code&gt; method:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;chat$chat(&#34;What preceding languages most influenced R?&#34;)&#xA;#&amp;gt; R was primarily influenced by the S programming language, particularly S-PLUS.&#xA;#&amp;gt; Other languages that had an impact include Scheme and various data analysis&#xA;#&amp;gt; languages.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you initialize the chat object in the global environment, the &lt;code&gt;chat&lt;/code&gt; method will stream the response to the console. When the entire response is received, it’s also (invisibly) returned as a character vector. This is useful when you want to see the response as it arrives, but you don’t want to enter the chat console.&lt;/p&gt; &#xA;&lt;p&gt;If you want to ask a question about an image, you can pass one or more additional input arguments using &lt;code&gt;content_image_file()&lt;/code&gt; and/or &lt;code&gt;content_image_url()&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;chat$chat(&#xA;  content_image_url(&#34;https://www.r-project.org/Rlogo.png&#34;),&#xA;  &#34;Can you explain this logo?&#34;&#xA;)&#xA;#&amp;gt; The logo of R features a stylized letter &#34;R&#34; in blue, enclosed in an oval&#xA;#&amp;gt; shape that resembles the letter &#34;O,&#34; signifying the programming language&#39;s&#xA;#&amp;gt; name. The design conveys a modern and professional look, reflecting its use&#xA;#&amp;gt; in statistical computing and data analysis. The blue color often represents&#xA;#&amp;gt; trust and reliability, which aligns with R&#39;s role in data science.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Programmatic chat&lt;/h3&gt; &#xA;&lt;p&gt;The most programmatic way to chat is to create the chat object inside a function. By doing so, live streaming is automatically suppressed and &lt;code&gt;$chat()&lt;/code&gt; returns the result as a string:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;my_function &amp;lt;- function() {&#xA;  chat &amp;lt;- chat_openai(&#xA;    model = &#34;gpt-4o-mini&#34;,&#xA;    system_prompt = &#34;You are a friendly but terse assistant.&#34;,&#xA;  )&#xA;  chat$chat(&#34;Is R a functional programming language?&#34;)&#xA;}&#xA;my_function()&#xA;#&amp;gt; [1] &#34;Yes, R supports functional programming concepts. It allows functions to&#xA;#&amp;gt; be first-class objects, supports higher-order functions, and encourages the&#xA;#&amp;gt; use of functions as core components of code. However, it also supports&#xA;#&amp;gt; procedural and object-oriented programming styles.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If needed, you can manually control this behaviour with the &lt;code&gt;echo&lt;/code&gt; argument. This is useful for programming with ellmer when the result is either not intended for human consumption or when you want to process the response before displaying it.&lt;/p&gt; &#xA;&lt;h2&gt;Learning more&lt;/h2&gt; &#xA;&lt;p&gt;ellmer comes with a bunch of vignettes to help you learn more:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Learn key vocabulary and see example use cases in &lt;code&gt;vignette(&#34;ellmer&#34;)&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Learn how to design your prompt in &lt;code&gt;vignette(&#34;prompt-design&#34;)&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Learn about tool/function calling in &lt;code&gt;vignette(&#34;tool-calling&#34;)&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Learn how to extract structured data in &lt;code&gt;vignette(&#34;structured-data&#34;)&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Learn about streaming and async APIs in &lt;code&gt;vignette(&#34;streaming-async&#34;)&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>