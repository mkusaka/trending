<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub R Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-01T02:10:35Z</updated>
  <subtitle>Monthly Trending of R in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>tidyverse/tidyr</title>
    <updated>2024-03-01T02:10:35Z</updated>
    <id>tag:github.com,2024-03-01:/tidyverse/tidyr</id>
    <link href="https://github.com/tidyverse/tidyr" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Tidy Messy Data&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;tidyr &lt;a href=&#34;https://tidyr.tidyverse.org&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tidyverse/tidyr/main/man/figures/logo.png&#34; align=&#34;right&#34; height=&#34;138&#34; alt=&#34;tidyr website&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;!-- badges: start --&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=tidyr&#34;&gt;&lt;img src=&#34;https://www.r-pkg.org/badges/version/tidyr&#34; alt=&#34;CRAN status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/tidyverse/tidyr/actions/workflows/R-CMD-check.yaml&#34;&gt;&lt;img src=&#34;https://github.com/tidyverse/tidyr/actions/workflows/R-CMD-check.yaml/badge.svg?sanitize=true&#34; alt=&#34;R-CMD-check&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://app.codecov.io/gh/tidyverse/tidyr?branch=main&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/tidyverse/tidyr/branch/main/graph/badge.svg?sanitize=true&#34; alt=&#34;Codecov test coverage&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- badges: end --&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;The goal of tidyr is to help you create &lt;strong&gt;tidy data&lt;/strong&gt;. Tidy data is data where:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Each variable is a column; each column is a variable.&lt;/li&gt; &#xA; &lt;li&gt;Each observation is a row; each row is an observation.&lt;/li&gt; &#xA; &lt;li&gt;Each value is a cell; each cell is a single value.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Tidy data describes a standard way of storing data that is used wherever possible throughout the &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;tidyverse&lt;/a&gt;. If you ensure that your data is tidy, you’ll spend less time fighting with the tools and more time working on your analysis. Learn more about tidy data in &lt;code&gt;vignette(&#34;tidy-data&#34;)&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# The easiest way to get tidyr is to install the whole tidyverse:&#xA;install.packages(&#34;tidyverse&#34;)&#xA;&#xA;# Alternatively, install just tidyr:&#xA;install.packages(&#34;tidyr&#34;)&#xA;&#xA;# Or the development version from GitHub:&#xA;# install.packages(&#34;pak&#34;)&#xA;pak::pak(&#34;tidyverse/tidyr&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Cheatsheet&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/rstudio/cheatsheets/raw/master/tidyr.pdf&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rstudio/cheatsheets/master/pngs/thumbnails/tidyr-thumbs.png&#34; width=&#34;630&#34; height=&#34;252&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tidyr)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;tidyr functions fall into five main categories:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;“Pivoting” which converts between long and wide forms. tidyr 1.0.0 introduces &lt;code&gt;pivot_longer()&lt;/code&gt; and &lt;code&gt;pivot_wider()&lt;/code&gt;, replacing the older &lt;code&gt;spread()&lt;/code&gt; and &lt;code&gt;gather()&lt;/code&gt; functions. See &lt;code&gt;vignette(&#34;pivot&#34;)&lt;/code&gt; for more details.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;“Rectangling”, which turns deeply nested lists (as from JSON) into tidy tibbles. See &lt;code&gt;unnest_longer()&lt;/code&gt;, &lt;code&gt;unnest_wider()&lt;/code&gt;, &lt;code&gt;hoist()&lt;/code&gt;, and &lt;code&gt;vignette(&#34;rectangle&#34;)&lt;/code&gt; for more details.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Nesting converts grouped data to a form where each group becomes a single row containing a nested data frame, and unnesting does the opposite. See &lt;code&gt;nest()&lt;/code&gt;, &lt;code&gt;unnest()&lt;/code&gt;, and &lt;code&gt;vignette(&#34;nest&#34;)&lt;/code&gt; for more details.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Splitting and combining character columns. Use &lt;code&gt;separate_wider_delim()&lt;/code&gt;, &lt;code&gt;separate_wider_position()&lt;/code&gt;, and &lt;code&gt;separate_wider_regex()&lt;/code&gt; to pull a single character column into multiple columns; use &lt;code&gt;unite()&lt;/code&gt; to combine multiple columns into a single character column.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Make implicit missing values explicit with &lt;code&gt;complete()&lt;/code&gt;; make explicit missing values implicit with &lt;code&gt;drop_na()&lt;/code&gt;; replace missing values with next/previous value with &lt;code&gt;fill()&lt;/code&gt;, or a known value with &lt;code&gt;replace_na()&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Related work&lt;/h2&gt; &#xA;&lt;p&gt;tidyr &lt;a href=&#34;https://lifecycle.r-lib.org/articles/stages.html#superseded&#34;&gt;supersedes&lt;/a&gt; reshape2 (2010-2014) and reshape (2005-2010). Somewhat counterintuitively, each iteration of the package has done less. tidyr is designed specifically for tidying data, not general reshaping (reshape2), or the general aggregation (reshape).&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://rdatatable.gitlab.io/data.table&#34;&gt;data.table&lt;/a&gt; provides high-performance implementations of &lt;code&gt;melt()&lt;/code&gt; and &lt;code&gt;dcast()&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you’d like to read more about data reshaping from a CS perspective, I’d recommend the following three papers:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://vis.stanford.edu/papers/wrangler&#34;&gt;Wrangler: Interactive visual specification of data transformation scripts&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www2.eecs.berkeley.edu/Pubs/TechRpts/2000/CSD-00-1110.pdf&#34;&gt;An interactive framework for data cleaning&lt;/a&gt; (Potter’s wheel)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.vldb.org/conf/1999/P45.pdf&#34;&gt;On efficiently implementing SchemaSQL on a SQL database system&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To guide your reading, here’s a translation between the terminology used in different places:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;tidyr 1.0.0&lt;/th&gt; &#xA;   &lt;th&gt;pivot longer&lt;/th&gt; &#xA;   &lt;th&gt;pivot wider&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;tidyr &amp;lt; 1.0.0&lt;/td&gt; &#xA;   &lt;td&gt;gather&lt;/td&gt; &#xA;   &lt;td&gt;spread&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;reshape(2)&lt;/td&gt; &#xA;   &lt;td&gt;melt&lt;/td&gt; &#xA;   &lt;td&gt;cast&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;spreadsheets&lt;/td&gt; &#xA;   &lt;td&gt;unpivot&lt;/td&gt; &#xA;   &lt;td&gt;pivot&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;databases&lt;/td&gt; &#xA;   &lt;td&gt;fold&lt;/td&gt; &#xA;   &lt;td&gt;unfold&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Getting help&lt;/h2&gt; &#xA;&lt;p&gt;If you encounter a clear bug, please file a minimal reproducible example on &lt;a href=&#34;https://github.com/tidyverse/tidyr/issues&#34;&gt;github&lt;/a&gt;. For questions and other discussion, please use &lt;a href=&#34;https://community.rstudio.com/&#34;&gt;community.rstudio.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Please note that the tidyr project is released with a &lt;a href=&#34;https://tidyr.tidyverse.org/CODE_OF_CONDUCT.html&#34;&gt;Contributor Code of Conduct&lt;/a&gt;. By contributing to this project, you agree to abide by its terms.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>tidyverse/readr</title>
    <updated>2024-03-01T02:10:35Z</updated>
    <id>tag:github.com,2024-03-01:/tidyverse/readr</id>
    <link href="https://github.com/tidyverse/readr" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Read flat files (csv, tsv, fwf) into R&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;readr &lt;a href=&#34;https://readr.tidyverse.org&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tidyverse/readr/main/man/figures/logo.png&#34; align=&#34;right&#34; height=&#34;138&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;!-- badges: start --&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://CRAN.R-project.org/package=readr&#34;&gt;&lt;img src=&#34;https://www.r-pkg.org/badges/version/readr&#34; alt=&#34;CRAN status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/tidyverse/readr/actions/workflows/R-CMD-check.yaml&#34;&gt;&lt;img src=&#34;https://github.com/tidyverse/readr/actions/workflows/R-CMD-check.yaml/badge.svg?sanitize=true&#34; alt=&#34;R-CMD-check&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://app.codecov.io/gh/tidyverse/readr?branch=main&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/tidyverse/readr/branch/main/graph/badge.svg?sanitize=true&#34; alt=&#34;Codecov test coverage&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- badges: end --&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;The goal of readr is to provide a fast and friendly way to read rectangular data from delimited files, such as comma-separated values (CSV) and tab-separated values (TSV). It is designed to parse many types of data found in the wild, while providing an informative problem report when parsing leads to unexpected results. If you are new to readr, the best place to start is the &lt;a href=&#34;https://r4ds.hadley.nz/data-import&#34;&gt;data import chapter&lt;/a&gt; in R for Data Science.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# The easiest way to get readr is to install the whole tidyverse:&#xA;install.packages(&#34;tidyverse&#34;)&#xA;&#xA;# Alternatively, install just readr:&#xA;install.packages(&#34;readr&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;div class=&#34;.pkgdown-devel&#34;&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Or you can install the development version from GitHub:&#xA;# install.packages(&#34;pak&#34;)&#xA;pak::pak(&#34;tidyverse/readr&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Cheatsheet&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rstudio/cheatsheets/main/data-import.pdf&#34;&gt;&amp;lt;img src=&#34;&lt;/a&gt;&lt;a href=&#34;https://github.com/rstudio/cheatsheets/raw/main/pngs/thumbnails/data-import-cheatsheet-thumbs.png&#34;&gt;https://github.com/rstudio/cheatsheets/raw/main/pngs/thumbnails/data-import-cheatsheet-thumbs.png&lt;/a&gt;&#34; height=&#34;252&#34; alt=&#34;thumbnail of tidyverse data import cheatsheet&#34;//&amp;gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;readr is part of the core tidyverse, so you can load it with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tidyverse)&#xA;#&amp;gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──&#xA;#&amp;gt; ✔ dplyr     1.1.4          ✔ readr     2.1.4.9000&#xA;#&amp;gt; ✔ forcats   1.0.0          ✔ stringr   1.5.1     &#xA;#&amp;gt; ✔ ggplot2   3.4.3          ✔ tibble    3.2.1     &#xA;#&amp;gt; ✔ lubridate 1.9.3          ✔ tidyr     1.3.0     &#xA;#&amp;gt; ✔ purrr     1.0.2          &#xA;#&amp;gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──&#xA;#&amp;gt; ✖ dplyr::filter() masks stats::filter()&#xA;#&amp;gt; ✖ dplyr::lag()    masks stats::lag()&#xA;#&amp;gt; ℹ Use the conflicted package (&amp;lt;http://conflicted.r-lib.org/&amp;gt;) to force all conflicts to become errors&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Of course, you can also load readr as an individual package:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(readr)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To read a rectangular dataset with readr, you combine two pieces: a function that parses the lines of the file into individual fields and a column specification.&lt;/p&gt; &#xA;&lt;p&gt;readr supports the following file formats with these &lt;code&gt;read_*()&lt;/code&gt; functions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;read_csv()&lt;/code&gt;: comma-separated values (CSV)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;read_tsv()&lt;/code&gt;: tab-separated values (TSV)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;read_csv2()&lt;/code&gt;: semicolon-separated values with &lt;code&gt;,&lt;/code&gt; as the decimal mark&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;read_delim()&lt;/code&gt;: delimited files (CSV and TSV are important special cases)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;read_fwf()&lt;/code&gt;: fixed-width files&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;read_table()&lt;/code&gt;: whitespace-separated files&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;read_log()&lt;/code&gt;: web log files&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;A column specification describes how each column should be converted from a character vector to a specific data type (e.g.&amp;nbsp;character, numeric, datetime, etc.). In the absence of a column specification, readr will guess column types from the data. &lt;code&gt;vignette(&#34;column-types&#34;)&lt;/code&gt; gives more detail on how readr guesses the column types. Column type guessing is very handy, especially during data exploration, but it’s important to remember these are &lt;em&gt;just guesses&lt;/em&gt;. As any data analysis project matures past the exploratory phase, the best strategy is to provide explicit column types.&lt;/p&gt; &#xA;&lt;p&gt;The following example loads a sample file bundled with readr and guesses the column types:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;(chickens &amp;lt;- read_csv(readr_example(&#34;chickens.csv&#34;)))&#xA;#&amp;gt; Rows: 5 Columns: 4&#xA;#&amp;gt; ── Column specification ────────────────────────────────────────────────────────&#xA;#&amp;gt; Delimiter: &#34;,&#34;&#xA;#&amp;gt; chr (3): chicken, sex, motto&#xA;#&amp;gt; dbl (1): eggs_laid&#xA;#&amp;gt; &#xA;#&amp;gt; ℹ Use `spec()` to retrieve the full column specification for this data.&#xA;#&amp;gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.&#xA;#&amp;gt; # A tibble: 5 × 4&#xA;#&amp;gt;   chicken                 sex     eggs_laid motto                               &#xA;#&amp;gt;   &amp;lt;chr&amp;gt;                   &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;                               &#xA;#&amp;gt; 1 Foghorn Leghorn         rooster         0 That&#39;s a joke, ah say, that&#39;s a jok…&#xA;#&amp;gt; 2 Chicken Little          hen             3 The sky is falling!                 &#xA;#&amp;gt; 3 Ginger                  hen            12 Listen. We&#39;ll either die free chick…&#xA;#&amp;gt; 4 Camilla the Chicken     hen             7 Bawk, buck, ba-gawk.                &#xA;#&amp;gt; 5 Ernie The Giant Chicken rooster         0 Put Captain Solo in the cargo hold.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that readr prints the column types – the &lt;em&gt;guessed&lt;/em&gt; column types, in this case. This is useful because it allows you to check that the columns have been read in as you expect. If they haven’t, that means you need to provide the column specification. This sounds like a lot of trouble, but luckily readr affords a nice workflow for this. Use &lt;code&gt;spec()&lt;/code&gt; to retrieve the (guessed) column specification from your initial effort.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;spec(chickens)&#xA;#&amp;gt; cols(&#xA;#&amp;gt;   chicken = col_character(),&#xA;#&amp;gt;   sex = col_character(),&#xA;#&amp;gt;   eggs_laid = col_double(),&#xA;#&amp;gt;   motto = col_character()&#xA;#&amp;gt; )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now you can copy, paste, and tweak this, to create a more explicit readr call that expresses the desired column types. Here we express that &lt;code&gt;sex&lt;/code&gt; should be a factor with levels &lt;code&gt;rooster&lt;/code&gt; and &lt;code&gt;hen&lt;/code&gt;, in that order, and that &lt;code&gt;eggs_laid&lt;/code&gt; should be integer.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;chickens &amp;lt;- read_csv(&#xA;  readr_example(&#34;chickens.csv&#34;),&#xA;  col_types = cols(&#xA;    chicken   = col_character(),&#xA;    sex       = col_factor(levels = c(&#34;rooster&#34;, &#34;hen&#34;)),&#xA;    eggs_laid = col_integer(),&#xA;    motto     = col_character()&#xA;  )&#xA;)&#xA;chickens&#xA;#&amp;gt; # A tibble: 5 × 4&#xA;#&amp;gt;   chicken                 sex     eggs_laid motto                               &#xA;#&amp;gt;   &amp;lt;chr&amp;gt;                   &amp;lt;fct&amp;gt;       &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;                               &#xA;#&amp;gt; 1 Foghorn Leghorn         rooster         0 That&#39;s a joke, ah say, that&#39;s a jok…&#xA;#&amp;gt; 2 Chicken Little          hen             3 The sky is falling!                 &#xA;#&amp;gt; 3 Ginger                  hen            12 Listen. We&#39;ll either die free chick…&#xA;#&amp;gt; 4 Camilla the Chicken     hen             7 Bawk, buck, ba-gawk.                &#xA;#&amp;gt; 5 Ernie The Giant Chicken rooster         0 Put Captain Solo in the cargo hold.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;vignette(&#34;readr&#34;)&lt;/code&gt; gives an expanded introduction to readr.&lt;/p&gt; &#xA;&lt;h2&gt;Editions&lt;/h2&gt; &#xA;&lt;p&gt;readr got a new parsing engine in version 2.0.0 (released July 2021). In this so-called second edition, readr calls &lt;code&gt;vroom::vroom()&lt;/code&gt;, by default.&lt;/p&gt; &#xA;&lt;p&gt;The parsing engine in readr versions prior to 2.0.0 is now called the first edition. If you’re using readr &amp;gt;= 2.0.0, you can still access first edition parsing via the functions &lt;code&gt;with_edition(1, ...)&lt;/code&gt; and &lt;code&gt;local_edition(1)&lt;/code&gt;. And, obviously, if you’re using readr &amp;lt; 2.0.0, you will get first edition parsing, by definition, because that’s all there is.&lt;/p&gt; &#xA;&lt;p&gt;We will continue to support the first edition for a number of releases, but the overall goal is to make the second edition uniformly better than the first. Therefore the plan is to eventually deprecate and then remove the first edition code. New code and actively-maintained code should use the second edition. The workarounds &lt;code&gt;with_edition(1, ...)&lt;/code&gt; and &lt;code&gt;local_edition(1)&lt;/code&gt; are offered as a pragmatic way to patch up legacy code or as a temporary solution for infelicities identified as the second edition matures.&lt;/p&gt; &#xA;&lt;h2&gt;Alternatives&lt;/h2&gt; &#xA;&lt;p&gt;There are two main alternatives to readr: base R and data.table’s &lt;code&gt;fread()&lt;/code&gt;. The most important differences are discussed below.&lt;/p&gt; &#xA;&lt;h3&gt;Base R&lt;/h3&gt; &#xA;&lt;p&gt;Compared to the corresponding base functions, readr functions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Use a consistent naming scheme for the parameters (e.g.&amp;nbsp;&lt;code&gt;col_names&lt;/code&gt; and &lt;code&gt;col_types&lt;/code&gt; not &lt;code&gt;header&lt;/code&gt; and &lt;code&gt;colClasses&lt;/code&gt;).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Are generally much faster (up to 10x-100x) depending on the dataset.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Leave strings as is by default, and automatically parse common date/time formats.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Have a helpful progress bar if loading is going to take a while.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;All functions work exactly the same way regardless of the current locale. To override the US-centric defaults, use &lt;code&gt;locale()&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;data.table and &lt;code&gt;fread()&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Rdatatable/data.table&#34;&gt;data.table&lt;/a&gt; has a function similar to &lt;code&gt;read_csv()&lt;/code&gt; called &lt;code&gt;fread()&lt;/code&gt;. Compared to &lt;code&gt;fread()&lt;/code&gt;, readr functions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Are sometimes slower, particularly on numeric heavy data.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Can automatically guess some parameters, but basically encourage explicit specification of, e.g., the delimiter, skipped rows, and the header row.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Follow tidyverse-wide conventions, such as returning a tibble, a standard approach for column name repair, and a common mini-language for column selection.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;Thanks to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/jcheng5&#34;&gt;Joe Cheng&lt;/a&gt; for showing me the beauty of deterministic finite automata for parsing, and for teaching me why I should write a tokenizer.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/jjallaire&#34;&gt;JJ Allaire&lt;/a&gt; for helping me come up with a design that makes very few copies, and is easy to extend.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://dirk.eddelbuettel.com&#34;&gt;Dirk Eddelbuettel&lt;/a&gt; for coming up with the name!&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>sparklyr/sparklyr</title>
    <updated>2024-03-01T02:10:35Z</updated>
    <id>tag:github.com,2024-03-01:/sparklyr/sparklyr</id>
    <link href="https://github.com/sparklyr/sparklyr" rel="alternate"></link>
    <summary type="html">&lt;p&gt;R interface for Apache Spark&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;sparklyr: R interface for Apache Spark&lt;/h1&gt; &#xA;&lt;!-- README.md is generated from README.Rmd. Please edit that file --&gt; &#xA;&lt;!-- badges: start --&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/sparklyr/sparklyr/actions/workflows/R-CMD-check.yaml&#34;&gt;&lt;img src=&#34;https://github.com/sparklyr/sparklyr/actions/workflows/R-CMD-check.yaml/badge.svg?sanitize=true&#34; alt=&#34;R-CMD-check&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/sparklyr/sparklyr/actions/workflows/spark-tests.yaml&#34;&gt;&lt;img src=&#34;https://github.com/sparklyr/sparklyr/actions/workflows/spark-tests.yaml/badge.svg?sanitize=true&#34; alt=&#34;Spark-Tests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://CRAN.R-project.org/package=sparklyr&#34;&gt;&lt;img src=&#34;https://www.r-pkg.org/badges/version/sparklyr&#34; alt=&#34;CRAN status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://app.codecov.io/gh/sparklyr/sparklyr?branch=main&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/sparklyr/sparklyr/branch/main/graph/badge.svg?sanitize=true&#34; alt=&#34;Codecov test coverage&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- badges: end --&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/tools/readme/sparklyr-diagram.png&#34; width=&#34;320&#34; align=&#34;right&#34; style=&#34;margin-left: 20px; margin-right: 20px&#34;&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install and connect to &lt;a href=&#34;https://spark.apache.org/&#34;&gt;Spark&lt;/a&gt; using YARN, Mesos, Livy or Kubernetes.&lt;/li&gt; &#xA; &lt;li&gt;Use &lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#using-dplyr&#34;&gt;dplyr&lt;/a&gt; to filter and aggregate Spark datasets and &lt;a href=&#34;https://spark.rstudio.com/guides/streaming/&#34;&gt;streams&lt;/a&gt; then bring them into R for analysis and visualization.&lt;/li&gt; &#xA; &lt;li&gt;Use &lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#machine-learning&#34;&gt;MLlib&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#using-h2o&#34;&gt;H2O&lt;/a&gt;, &lt;a href=&#34;https://spark.rstudio.com/packages/sparkxgb/latest/&#34;&gt;XGBoost&lt;/a&gt; and &lt;a href=&#34;https://spark.rstudio.com/packages/graphframes/latest/&#34;&gt;GraphFrames&lt;/a&gt; to train models at scale in Spark.&lt;/li&gt; &#xA; &lt;li&gt;Create interoperable machine learning &lt;a href=&#34;https://spark.rstudio.com/guides/pipelines.html&#34;&gt;pipelines&lt;/a&gt; and productionize them with &lt;a href=&#34;https://spark.rstudio.com/packages/mleap/latest/&#34;&gt;MLeap&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Create &lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#extensions&#34;&gt;extensions&lt;/a&gt; that call the full Spark API or run &lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#distributed-r&#34;&gt;distributed R&lt;/a&gt; code to support new functionality.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#connecting-to-spark&#34;&gt;Connecting to Spark&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#using-dplyr&#34;&gt;Using dplyr&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#window-functions&#34;&gt;Window Functions&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#using-sql&#34;&gt;Using SQL&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#machine-learning&#34;&gt;Machine Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#reading-and-writing-data&#34;&gt;Reading and Writing Data&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#distributed-r&#34;&gt;Distributed R&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#extensions&#34;&gt;Extensions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#table-utilities&#34;&gt;Table Utilities&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#connection-utilities&#34;&gt;Connection Utilities&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#rstudio-ide&#34;&gt;RStudio IDE&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#using-h2o&#34;&gt;Using H2O&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#connecting-through-livy&#34;&gt;Connecting through Livy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#connecting-through-databricks-connect-v2&#34;&gt;Connecting through Databricks Connect&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;You can install the &lt;strong&gt;sparklyr&lt;/strong&gt; package from &lt;a href=&#34;https://CRAN.r-project.org&#34;&gt;CRAN&lt;/a&gt; as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&#34;sparklyr&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You should also install a local version of Spark for development purposes:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(sparklyr)&#xA;spark_install()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To upgrade to the latest version of sparklyr, run the following command and restart your r session:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&#34;devtools&#34;)&#xA;devtools::install_github(&#34;sparklyr/sparklyr&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Connecting to Spark&lt;/h2&gt; &#xA;&lt;p&gt;You can connect to both local instances of Spark as well as remote Spark clusters. Here we’ll connect to a local instance of Spark via the &lt;a href=&#34;https://spark.rstudio.com/packages/sparklyr/latest/reference/spark-connections.html&#34;&gt;spark_connect&lt;/a&gt; function:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(sparklyr)&#xA;sc &amp;lt;- spark_connect(master = &#34;local&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The returned Spark connection (&lt;code&gt;sc&lt;/code&gt;) provides a remote dplyr data source to the Spark cluster.&lt;/p&gt; &#xA;&lt;p&gt;For more information on connecting to remote Spark clusters see the &lt;a href=&#34;https://spark.rstudio.com/deployment.html&#34;&gt;Deployment&lt;/a&gt; section of the sparklyr website.&lt;/p&gt; &#xA;&lt;h2&gt;Using dplyr&lt;/h2&gt; &#xA;&lt;p&gt;We can now use all of the available dplyr verbs against the tables within the cluster.&lt;/p&gt; &#xA;&lt;p&gt;We’ll start by copying some datasets from R into the Spark cluster (note that you may need to install the nycflights13 and Lahman packages in order to execute this code):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(c(&#34;nycflights13&#34;, &#34;Lahman&#34;))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(dplyr)&#xA;iris_tbl &amp;lt;- copy_to(sc, iris, overwrite = TRUE)&#xA;flights_tbl &amp;lt;- copy_to(sc, nycflights13::flights, &#34;flights&#34;, overwrite = TRUE)&#xA;batting_tbl &amp;lt;- copy_to(sc, Lahman::Batting, &#34;batting&#34;, overwrite = TRUE)&#xA;src_tbls(sc)&#xA;#&amp;gt; [1] &#34;batting&#34; &#34;flights&#34; &#34;iris&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To start with here’s a simple filtering example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# filter by departure delay and print the first few records&#xA;flights_tbl %&amp;gt;% filter(dep_delay == 2)&#xA;#&amp;gt; # Source: spark&amp;lt;?&amp;gt; [?? x 19]&#xA;#&amp;gt;     year month   day dep_time sched_dep_time dep_delay&#xA;#&amp;gt;    &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;    &amp;lt;int&amp;gt;          &amp;lt;int&amp;gt;     &amp;lt;dbl&amp;gt;&#xA;#&amp;gt;  1  2013     1     1      517            515         2&#xA;#&amp;gt;  2  2013     1     1      542            540         2&#xA;#&amp;gt;  3  2013     1     1      702            700         2&#xA;#&amp;gt;  4  2013     1     1      715            713         2&#xA;#&amp;gt;  5  2013     1     1      752            750         2&#xA;#&amp;gt;  6  2013     1     1      917            915         2&#xA;#&amp;gt;  7  2013     1     1      932            930         2&#xA;#&amp;gt;  8  2013     1     1     1028           1026         2&#xA;#&amp;gt;  9  2013     1     1     1042           1040         2&#xA;#&amp;gt; 10  2013     1     1     1231           1229         2&#xA;#&amp;gt; # ℹ more rows&#xA;#&amp;gt; # ℹ 13 more variables: arr_time &amp;lt;int&amp;gt;,&#xA;#&amp;gt; #   sched_arr_time &amp;lt;int&amp;gt;, arr_delay &amp;lt;dbl&amp;gt;, carrier &amp;lt;chr&amp;gt;,&#xA;#&amp;gt; #   flight &amp;lt;int&amp;gt;, tailnum &amp;lt;chr&amp;gt;, origin &amp;lt;chr&amp;gt;, dest &amp;lt;chr&amp;gt;,&#xA;#&amp;gt; #   air_time &amp;lt;dbl&amp;gt;, distance &amp;lt;dbl&amp;gt;, hour &amp;lt;dbl&amp;gt;,&#xA;#&amp;gt; #   minute &amp;lt;dbl&amp;gt;, time_hour &amp;lt;dttm&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://spark.rstudio.com/guides/dplyr.html&#34;&gt;Introduction to dplyr&lt;/a&gt; provides additional &lt;code&gt;dplyr&lt;/code&gt; examples you can try. For example, consider the last example from the tutorial which plots data on flight delays:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;delay &amp;lt;- flights_tbl %&amp;gt;%&#xA;  group_by(tailnum) %&amp;gt;%&#xA;  summarise(count = n(), dist = mean(distance), delay = mean(arr_delay)) %&amp;gt;%&#xA;  filter(count &amp;gt; 20, dist &amp;lt; 2000, !is.na(delay)) %&amp;gt;%&#xA;  collect()&#xA;&#xA;# plot delays&#xA;library(ggplot2)&#xA;ggplot(delay, aes(dist, delay)) +&#xA;  geom_point(aes(size = count), alpha = 1/2) +&#xA;  geom_smooth() +&#xA;  scale_size_area(max_size = 2)&#xA;#&amp;gt; `geom_smooth()` using method = &#39;gam&#39; and formula = &#39;y ~&#xA;#&amp;gt; s(x, bs = &#34;cs&#34;)&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/tools/readme/dplyr-ggplot2-1.png&#34; width=&#34;100%&#34;&gt; &#xA;&lt;h3&gt;Window Functions&lt;/h3&gt; &#xA;&lt;p&gt;dplyr &lt;a href=&#34;https://spark.rstudio.com/guides/dplyr.html#grouping&#34;&gt;window functions&lt;/a&gt; are also supported, for example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;batting_tbl %&amp;gt;%&#xA;  select(playerID, yearID, teamID, G, AB:H) %&amp;gt;%&#xA;  arrange(playerID, yearID, teamID) %&amp;gt;%&#xA;  group_by(playerID) %&amp;gt;%&#xA;  filter(min_rank(desc(H)) &amp;lt;= 2 &amp;amp; H &amp;gt; 0)&#xA;#&amp;gt; # Source:     spark&amp;lt;?&amp;gt; [?? x 7]&#xA;#&amp;gt; # Groups:     playerID&#xA;#&amp;gt; # Ordered by: playerID, yearID, teamID&#xA;#&amp;gt;    playerID  yearID teamID     G    AB     R     H&#xA;#&amp;gt;    &amp;lt;chr&amp;gt;      &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;&#xA;#&amp;gt;  1 abadijo01   1875 PH3       11    45     3    10&#xA;#&amp;gt;  2 abadijo01   1875 BR2        1     4     1     1&#xA;#&amp;gt;  3 abbeybe01   1896 BRO       25    63     7    12&#xA;#&amp;gt;  4 abbeybe01   1892 WAS       27    75     5     9&#xA;#&amp;gt;  5 abbeych01   1894 WAS      129   523    95   164&#xA;#&amp;gt;  6 abbeych01   1895 WAS      133   516   102   142&#xA;#&amp;gt;  7 abbotfr01   1903 CLE       77   255    25    60&#xA;#&amp;gt;  8 abbotfr01   1905 PHI       42   128     9    25&#xA;#&amp;gt;  9 abbotky01   1992 PHI       31    29     1     2&#xA;#&amp;gt; 10 abbotky01   1995 PHI       18     2     1     1&#xA;#&amp;gt; # ℹ more rows&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For additional documentation on using dplyr with Spark see the &lt;a href=&#34;https://spark.rstudio.com/dplyr.html&#34;&gt;dplyr&lt;/a&gt; section of the sparklyr website.&lt;/p&gt; &#xA;&lt;h2&gt;Using SQL&lt;/h2&gt; &#xA;&lt;p&gt;It’s also possible to execute SQL queries directly against tables within a Spark cluster. The &lt;code&gt;spark_connection&lt;/code&gt; object implements a &lt;a href=&#34;https://github.com/r-dbi/DBI&#34;&gt;DBI&lt;/a&gt; interface for Spark, so you can use &lt;code&gt;dbGetQuery()&lt;/code&gt; to execute SQL and return the result as an R data frame:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(DBI)&#xA;iris_preview &amp;lt;- dbGetQuery(sc, &#34;SELECT * FROM iris LIMIT 10&#34;)&#xA;iris_preview&#xA;#&amp;gt;    Sepal_Length Sepal_Width Petal_Length Petal_Width&#xA;#&amp;gt; 1           5.1         3.5          1.4         0.2&#xA;#&amp;gt; 2           4.9         3.0          1.4         0.2&#xA;#&amp;gt; 3           4.7         3.2          1.3         0.2&#xA;#&amp;gt; 4           4.6         3.1          1.5         0.2&#xA;#&amp;gt; 5           5.0         3.6          1.4         0.2&#xA;#&amp;gt; 6           5.4         3.9          1.7         0.4&#xA;#&amp;gt; 7           4.6         3.4          1.4         0.3&#xA;#&amp;gt; 8           5.0         3.4          1.5         0.2&#xA;#&amp;gt; 9           4.4         2.9          1.4         0.2&#xA;#&amp;gt; 10          4.9         3.1          1.5         0.1&#xA;#&amp;gt;    Species&#xA;#&amp;gt; 1   setosa&#xA;#&amp;gt; 2   setosa&#xA;#&amp;gt; 3   setosa&#xA;#&amp;gt; 4   setosa&#xA;#&amp;gt; 5   setosa&#xA;#&amp;gt; 6   setosa&#xA;#&amp;gt; 7   setosa&#xA;#&amp;gt; 8   setosa&#xA;#&amp;gt; 9   setosa&#xA;#&amp;gt; 10  setosa&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Machine Learning&lt;/h2&gt; &#xA;&lt;p&gt;You can orchestrate machine learning algorithms in a Spark cluster via the &lt;a href=&#34;https://spark.apache.org/docs/latest/mllib-guide.html&#34;&gt;machine learning&lt;/a&gt; functions within &lt;strong&gt;sparklyr&lt;/strong&gt;. These functions connect to a set of high-level APIs built on top of DataFrames that help you create and tune machine learning workflows.&lt;/p&gt; &#xA;&lt;p&gt;Here’s an example where we use &lt;a href=&#34;https://spark.rstudio.com/packages/sparklyr/latest/reference/ml_linear_regression/&#34;&gt;ml_linear_regression&lt;/a&gt; to fit a linear regression model. We’ll use the built-in &lt;code&gt;mtcars&lt;/code&gt; dataset, and see if we can predict a car’s fuel consumption (&lt;code&gt;mpg&lt;/code&gt;) based on its weight (&lt;code&gt;wt&lt;/code&gt;), and the number of cylinders the engine contains (&lt;code&gt;cyl&lt;/code&gt;). We’ll assume in each case that the relationship between &lt;code&gt;mpg&lt;/code&gt; and each of our features is linear.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# copy mtcars into spark&#xA;mtcars_tbl &amp;lt;- copy_to(sc, mtcars, overwrite = TRUE)&#xA;&#xA;# transform our data set, and then partition into &#39;training&#39;, &#39;test&#39;&#xA;partitions &amp;lt;- mtcars_tbl %&amp;gt;%&#xA;  filter(hp &amp;gt;= 100) %&amp;gt;%&#xA;  mutate(cyl8 = cyl == 8) %&amp;gt;%&#xA;  sdf_partition(training = 0.5, test = 0.5, seed = 1099)&#xA;&#xA;# fit a linear model to the training dataset&#xA;fit &amp;lt;- partitions$training %&amp;gt;%&#xA;  ml_linear_regression(response = &#34;mpg&#34;, features = c(&#34;wt&#34;, &#34;cyl&#34;))&#xA;fit&#xA;#&amp;gt; Formula: mpg ~ wt + cyl&#xA;#&amp;gt; &#xA;#&amp;gt; Coefficients:&#xA;#&amp;gt; (Intercept)          wt         cyl &#xA;#&amp;gt;  37.1464554  -4.3408005  -0.5830515&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For linear regression models produced by Spark, we can use &lt;code&gt;summary()&lt;/code&gt; to learn a bit more about the quality of our fit, and the statistical significance of each of our predictors.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(fit)&#xA;#&amp;gt; Deviance Residuals:&#xA;#&amp;gt;     Min      1Q  Median      3Q     Max &#xA;#&amp;gt; -2.5134 -0.9158 -0.1683  1.1503  2.1534 &#xA;#&amp;gt; &#xA;#&amp;gt; Coefficients:&#xA;#&amp;gt; (Intercept)          wt         cyl &#xA;#&amp;gt;  37.1464554  -4.3408005  -0.5830515 &#xA;#&amp;gt; &#xA;#&amp;gt; R-Squared: 0.9428&#xA;#&amp;gt; Root Mean Squared Error: 1.409&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Spark machine learning supports a wide array of algorithms and feature transformations and as illustrated above it’s easy to chain these functions together with dplyr pipelines. To learn more see the &lt;a href=&#34;https://spark.rstudio.com/mlib/&#34;&gt;machine learning&lt;/a&gt; section.&lt;/p&gt; &#xA;&lt;h2&gt;Reading and Writing Data&lt;/h2&gt; &#xA;&lt;p&gt;You can read and write data in CSV, JSON, and Parquet formats. Data can be stored in HDFS, S3, or on the local filesystem of cluster nodes.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;temp_csv &amp;lt;- tempfile(fileext = &#34;.csv&#34;)&#xA;temp_parquet &amp;lt;- tempfile(fileext = &#34;.parquet&#34;)&#xA;temp_json &amp;lt;- tempfile(fileext = &#34;.json&#34;)&#xA;&#xA;spark_write_csv(iris_tbl, temp_csv)&#xA;iris_csv_tbl &amp;lt;- spark_read_csv(sc, &#34;iris_csv&#34;, temp_csv)&#xA;&#xA;spark_write_parquet(iris_tbl, temp_parquet)&#xA;iris_parquet_tbl &amp;lt;- spark_read_parquet(sc, &#34;iris_parquet&#34;, temp_parquet)&#xA;&#xA;spark_write_json(iris_tbl, temp_json)&#xA;iris_json_tbl &amp;lt;- spark_read_json(sc, &#34;iris_json&#34;, temp_json)&#xA;&#xA;src_tbls(sc)&#xA;#&amp;gt; [1] &#34;batting&#34;      &#34;flights&#34;      &#34;iris&#34;        &#xA;#&amp;gt; [4] &#34;iris_csv&#34;     &#34;iris_json&#34;    &#34;iris_parquet&#34;&#xA;#&amp;gt; [7] &#34;mtcars&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Distributed R&lt;/h2&gt; &#xA;&lt;p&gt;You can execute arbitrary r code across your cluster using &lt;code&gt;spark_apply()&lt;/code&gt;. For example, we can apply &lt;code&gt;rgamma&lt;/code&gt; over &lt;code&gt;iris&lt;/code&gt; as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;spark_apply(iris_tbl, function(data) {&#xA;  data[1:4] + rgamma(1,2)&#xA;})&#xA;#&amp;gt; # Source: spark&amp;lt;?&amp;gt; [?? x 4]&#xA;#&amp;gt;    Sepal_Length Sepal_Width Petal_Length Petal_Width&#xA;#&amp;gt;           &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;&#xA;#&amp;gt;  1         5.75        4.15         2.05       0.846&#xA;#&amp;gt;  2         5.55        3.65         2.05       0.846&#xA;#&amp;gt;  3         5.35        3.85         1.95       0.846&#xA;#&amp;gt;  4         5.25        3.75         2.15       0.846&#xA;#&amp;gt;  5         5.65        4.25         2.05       0.846&#xA;#&amp;gt;  6         6.05        4.55         2.35       1.05 &#xA;#&amp;gt;  7         5.25        4.05         2.05       0.946&#xA;#&amp;gt;  8         5.65        4.05         2.15       0.846&#xA;#&amp;gt;  9         5.05        3.55         2.05       0.846&#xA;#&amp;gt; 10         5.55        3.75         2.15       0.746&#xA;#&amp;gt; # ℹ more rows&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also group by columns to perform an operation over each group of rows and make use of any package within the closure:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;spark_apply(&#xA;  iris_tbl,&#xA;  function(e) broom::tidy(lm(Petal_Width ~ Petal_Length, e)),&#xA;  columns = c(&#34;term&#34;, &#34;estimate&#34;, &#34;std.error&#34;, &#34;statistic&#34;, &#34;p.value&#34;),&#xA;  group_by = &#34;Species&#34;&#xA;)&#xA;#&amp;gt; # Source: spark&amp;lt;?&amp;gt; [?? x 6]&#xA;#&amp;gt;   Species    term      estimate std.error statistic  p.value&#xA;#&amp;gt;   &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;        &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;&#xA;#&amp;gt; 1 versicolor (Interce…  -0.0843    0.161     -0.525 6.02e- 1&#xA;#&amp;gt; 2 versicolor Petal_Le…   0.331     0.0375     8.83  1.27e-11&#xA;#&amp;gt; 3 virginica  (Interce…   1.14      0.379      2.99  4.34e- 3&#xA;#&amp;gt; 4 virginica  Petal_Le…   0.160     0.0680     2.36  2.25e- 2&#xA;#&amp;gt; 5 setosa     (Interce…  -0.0482    0.122     -0.396 6.94e- 1&#xA;#&amp;gt; 6 setosa     Petal_Le…   0.201     0.0826     2.44  1.86e- 2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Extensions&lt;/h2&gt; &#xA;&lt;p&gt;The facilities used internally by sparklyr for its &lt;code&gt;dplyr&lt;/code&gt; and machine learning interfaces are available to extension packages. Since Spark is a general purpose cluster computing system there are many potential applications for extensions (e.g.&amp;nbsp; interfaces to custom machine learning pipelines, interfaces to 3rd party Spark packages, etc.).&lt;/p&gt; &#xA;&lt;p&gt;Here’s a simple example that wraps a Spark text file line counting function with an R function:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# write a CSV&#xA;tempfile &amp;lt;- tempfile(fileext = &#34;.csv&#34;)&#xA;write.csv(nycflights13::flights, tempfile, row.names = FALSE, na = &#34;&#34;)&#xA;&#xA;# define an R interface to Spark line counting&#xA;count_lines &amp;lt;- function(sc, path) {&#xA;  spark_context(sc) %&amp;gt;%&#xA;    invoke(&#34;textFile&#34;, path, 1L) %&amp;gt;%&#xA;      invoke(&#34;count&#34;)&#xA;}&#xA;&#xA;# call spark to count the lines of the CSV&#xA;count_lines(sc, tempfile)&#xA;#&amp;gt; [1] 336777&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To learn more about creating extensions see the &lt;a href=&#34;https://spark.rstudio.com/guides/extensions.html&#34;&gt;Extensions&lt;/a&gt; section of the sparklyr website.&lt;/p&gt; &#xA;&lt;h2&gt;Table Utilities&lt;/h2&gt; &#xA;&lt;p&gt;You can cache a table into memory with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;tbl_cache(sc, &#34;batting&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and unload from memory using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;tbl_uncache(sc, &#34;batting&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Connection Utilities&lt;/h2&gt; &#xA;&lt;p&gt;You can view the Spark web console using the &lt;code&gt;spark_web()&lt;/code&gt; function:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;spark_web(sc)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can show the log using the &lt;code&gt;spark_log()&lt;/code&gt; function:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;spark_log(sc, n = 10)&#xA;#&amp;gt; 24/01/08 15:49:41 INFO Executor: Running task 0.0 in stage 87.0 (TID 115)&#xA;#&amp;gt; 24/01/08 15:49:41 INFO HadoopRDD: Input split: file:/var/folders/l8/v1ym1mc10_b0dftql5wrrm8w0000gn/T/Rtmpl48w30/file213f56a22695.csv:0+33313106&#xA;#&amp;gt; 24/01/08 15:49:41 INFO BlockManagerInfo: Removed broadcast_94_piece0 on localhost:56103 in memory (size: 20.0 KiB, free: 1048.6 MiB)&#xA;#&amp;gt; 24/01/08 15:49:41 INFO Executor: Finished task 0.0 in stage 87.0 (TID 115). 969 bytes result sent to driver&#xA;#&amp;gt; 24/01/08 15:49:41 INFO TaskSetManager: Finished task 0.0 in stage 87.0 (TID 115) in 102 ms on localhost (executor driver) (1/1)&#xA;#&amp;gt; 24/01/08 15:49:41 INFO TaskSchedulerImpl: Removed TaskSet 87.0, whose tasks have all completed, from pool &#xA;#&amp;gt; 24/01/08 15:49:41 INFO DAGScheduler: ResultStage 87 (count at DirectMethodHandleAccessor.java:104) finished in 0.105 s&#xA;#&amp;gt; 24/01/08 15:49:41 INFO DAGScheduler: Job 70 is finished. Cancelling potential speculative or zombie tasks for this job&#xA;#&amp;gt; 24/01/08 15:49:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 87: Stage finished&#xA;#&amp;gt; 24/01/08 15:49:41 INFO DAGScheduler: Job 70 finished: count at DirectMethodHandleAccessor.java:104, took 0.106307 s&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, we disconnect from Spark:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;  spark_disconnect(sc)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;RStudio IDE&lt;/h2&gt; &#xA;&lt;p&gt;The RStudio IDE includes integrated support for Spark and the sparklyr package, including tools for:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Creating and managing Spark connections&lt;/li&gt; &#xA; &lt;li&gt;Browsing the tables and columns of Spark DataFrames&lt;/li&gt; &#xA; &lt;li&gt;Previewing the first 1,000 rows of Spark DataFrames&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Once you’ve installed the sparklyr package, you should find a new &lt;strong&gt;Spark&lt;/strong&gt; pane within the IDE. This pane includes a &lt;strong&gt;New Connection&lt;/strong&gt; dialog which can be used to make connections to local or remote Spark instances:&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/tools/readme/spark-connect.png&#34; class=&#34;screenshot&#34; width=&#34;389&#34;&gt; &#xA;&lt;p&gt;Once you’ve connected to Spark you’ll be able to browse the tables contained within the Spark cluster and preview Spark DataFrames using the standard RStudio data viewer:&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/tools/readme/spark-dataview.png&#34; class=&#34;screenshot&#34; width=&#34;639&#34;&gt; &#xA;&lt;p&gt;You can also connect to Spark through &lt;a href=&#34;https://livy.apache.org/&#34;&gt;Livy&lt;/a&gt; through a new connection dialog:&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/tools/readme/spark-connect-livy.png&#34; class=&#34;screenshot&#34; width=&#34;389&#34;&gt; &#xA;&lt;div style=&#34;margin-bottom: 15px;&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Using H2O&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=rsparkling&#34;&gt;rsparkling&lt;/a&gt; is a CRAN package from &lt;a href=&#34;https://h2o.ai/&#34;&gt;H2O&lt;/a&gt; that extends &lt;a href=&#34;https://spark.rstudio.com/&#34;&gt;sparklyr&lt;/a&gt; to provide an interface into &lt;a href=&#34;https://github.com/h2oai/sparkling-water&#34;&gt;Sparkling Water&lt;/a&gt;. For instance, the following example installs, configures and runs &lt;a href=&#34;https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/glm.html&#34;&gt;h2o.glm&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rsparkling)&#xA;library(sparklyr)&#xA;library(dplyr)&#xA;library(h2o)&#xA;&#xA;sc &amp;lt;- spark_connect(master = &#34;local&#34;, version = &#34;2.3.2&#34;)&#xA;mtcars_tbl &amp;lt;- copy_to(sc, mtcars, &#34;mtcars&#34;, overwrite = TRUE)&#xA;&#xA;mtcars_h2o &amp;lt;- as_h2o_frame(sc, mtcars_tbl, strict_version_check = FALSE)&#xA;&#xA;mtcars_glm &amp;lt;- h2o.glm(x = c(&#34;wt&#34;, &#34;cyl&#34;),&#xA;                      y = &#34;mpg&#34;,&#xA;                      training_frame = mtcars_h2o,&#xA;                      lambda_search = TRUE)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mtcars_glm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;#&amp;gt; Model Details:&#xA;#&amp;gt; ==============&#xA;#&amp;gt;&#xA;#&amp;gt; H2ORegressionModel: glm&#xA;#&amp;gt; Model ID:  GLM_model_R_1527265202599_1&#xA;#&amp;gt; GLM Model: summary&#xA;#&amp;gt;     family     link                              regularization&#xA;#&amp;gt; 1 gaussian identity Elastic Net (alpha = 0.5, lambda = 0.1013 )&#xA;#&amp;gt;                                                                lambda_search&#xA;#&amp;gt; 1 nlambda = 100, lambda.max = 10.132, lambda.min = 0.1013, lambda.1se = -1.0&#xA;#&amp;gt;   number_of_predictors_total number_of_active_predictors&#xA;#&amp;gt; 1                          2                           2&#xA;#&amp;gt;   number_of_iterations                                training_frame&#xA;#&amp;gt; 1                  100 frame_rdd_31_ad5c4e88ec97eb8ccedae9475ad34e02&#xA;#&amp;gt;&#xA;#&amp;gt; Coefficients: glm coefficients&#xA;#&amp;gt;       names coefficients standardized_coefficients&#xA;#&amp;gt; 1 Intercept    38.941654                 20.090625&#xA;#&amp;gt; 2       cyl    -1.468783                 -2.623132&#xA;#&amp;gt; 3        wt    -3.034558                 -2.969186&#xA;#&amp;gt;&#xA;#&amp;gt; H2ORegressionMetrics: glm&#xA;#&amp;gt; ** Reported on training data. **&#xA;#&amp;gt;&#xA;#&amp;gt; MSE:  6.017684&#xA;#&amp;gt; RMSE:  2.453097&#xA;#&amp;gt; MAE:  1.940985&#xA;#&amp;gt; RMSLE:  0.1114801&#xA;#&amp;gt; Mean Residual Deviance :  6.017684&#xA;#&amp;gt; R^2 :  0.8289895&#xA;#&amp;gt; Null Deviance :1126.047&#xA;#&amp;gt; Null D.o.F. :31&#xA;#&amp;gt; Residual Deviance :192.5659&#xA;#&amp;gt; Residual D.o.F. :29&#xA;#&amp;gt; AIC :156.2425&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;spark_disconnect(sc)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Connecting through Livy&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/cloudera/livy&#34;&gt;Livy&lt;/a&gt; enables remote connections to Apache Spark clusters. However, please notice that connecting to Spark clusters through Livy is much slower than any other connection method.&lt;/p&gt; &#xA;&lt;p&gt;Before connecting to Livy, you will need the connection information to an existing service running Livy. Otherwise, to test &lt;code&gt;livy&lt;/code&gt; in your local environment, you can install it and run it locally as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;livy_install()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;livy_service_start()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To connect, use the Livy service address as &lt;code&gt;master&lt;/code&gt; and &lt;code&gt;method = &#34;livy&#34;&lt;/code&gt; in &lt;code&gt;spark_connect()&lt;/code&gt;. Once connection completes, use &lt;code&gt;sparklyr&lt;/code&gt; as usual, for instance:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sc &amp;lt;- spark_connect(master = &#34;http://localhost:8998&#34;, method = &#34;livy&#34;, version = &#34;3.0.0&#34;)&#xA;copy_to(sc, iris, overwrite = TRUE)&#xA;&#xA;spark_disconnect(sc)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once you are done using &lt;code&gt;livy&lt;/code&gt; locally, you should stop this service with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;livy_service_stop()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To connect to remote &lt;code&gt;livy&lt;/code&gt; clusters that support basic authentication connect as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;config &amp;lt;- livy_config(username=&#34;&amp;lt;username&amp;gt;&#34;, password=&#34;&amp;lt;password&amp;gt;&#34;)&#xA;sc &amp;lt;- spark_connect(master = &#34;&amp;lt;address&amp;gt;&#34;, method = &#34;livy&#34;, config = config)&#xA;spark_disconnect(sc)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Connecting through Databricks Connect v2&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;sparklyr&lt;/code&gt; is able to interact with &lt;a href=&#34;https://docs.databricks.com/en/dev-tools/databricks-connect/index.html&#34;&gt;Databricks Connect v2&lt;/a&gt; via a new extension called &lt;code&gt;pysparklyr&lt;/code&gt;. To learn how to use, and the latest updates on this integration see &lt;a href=&#34;https://spark.rstudio.com/deployment/databricks-connect.html&#34;&gt;the article in &lt;code&gt;sparklyr&lt;/code&gt;’s official website&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>