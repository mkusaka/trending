<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub R Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-08-18T01:50:44Z</updated>
  <subtitle>Daily Trending of R in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>EqualityAI/responsible_mlops</title>
    <updated>2022-08-18T01:50:44Z</updated>
    <id>tag:github.com,2022-08-18:/EqualityAI/responsible_mlops</id>
    <link href="https://github.com/EqualityAI/responsible_mlops" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Responsible Machine Learning Toolkit&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://github.com/EqualityAI/Checklist/blob/master/img/color logo only.PNG&#34; align=&#34;left&#34; alt=&#34;EqualityAI Logo&#34; width=&#34;65&#34;&gt; &#xA;&lt;h1&gt;Equality AI &lt;code&gt;responsible_mlops&lt;/code&gt;&lt;/h1&gt; &#xA;&lt;h3&gt;Welcome to our GitHub repo!&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://equalityai.com/&#34;&gt;Equality AI (EAI)&lt;/a&gt; is a public benefit corporation dedicated to closing the health disparity gap by assembling a Responsible AI framework into a MLOps Developer Studio (alpha coming soon Autumn 2022) that includes modernized, end-to-end machine learning (ML) operations (Ops) with functions that can be selectively incorporated to create various workflows designed to produce equitable, responsible models. &lt;/p&gt; &#xA;&lt;p&gt;We have released the technology behind our Responsible MLOPs Studio on GitHub as an open source ML software framework and tool, called &lt;code&gt;responsible_mlops&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you like what we&#39;re doing, give us a &lt;span&gt;⭐&lt;/span&gt; and join our &lt;a href=&#34;https://equalityai.com/community/#manifesto&#34;&gt;EAI Manifesto!&lt;/a&gt;!&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/EqualityAI/responsible_mlops/main/img/star.png&#34; align=&#34;center&#34; alt=&#34;&#34; width=&#34;400&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Fairness-based Machine Learning (Pre-processing)&lt;/h2&gt; &#xA;&lt;p&gt;Are you concerned that data and algorithmic biases lead to machine learning models that treat individuals unfavorably on the basis of characteristics such as race, gender or political orientation? Do you want to address fairness in machine learning but do not know where to start?&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/EqualityAI/responsible_mlops/main/img/infographic.png&#34; align=&#34;center&#34; alt=&#34;&#34; width=&#34;200%&#34;&gt; &#xA;&lt;sub&gt;&lt;b&gt;Figure 1:&lt;/b&gt; An infographic showing the long term consequences of bias finding its way into our data in one example, the health sector (image from the British Medical Journal). &lt;/sub&gt; &#xA;&lt;p&gt;&lt;br&gt;Fairness-based ML offers a potential solution by incorporating bias mitigation methods and fairness metrics into the traditional end-to-end MLOps. Fairness is a principle of Responsible AI, an emerging framework for how artificial intelligence systems should be developed, deployed, and governed to comply with ethics and laws.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/EqualityAI/responsible_mlops/main/img/framework.png&#34; align=&#34;center&#34; alt=&#34;&#34; width=&#34;900&#34;&gt; &#xA;&lt;sub&gt;&lt;b&gt;Figure 2:&lt;/b&gt; Full Responsible AI Framework. &lt;/sub&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;b&gt;Bias mitigation methods&lt;/b&gt; are employed to address bias in data and/or machine learning models and &lt;b&gt;fairness metrics&lt;/b&gt; are needed to mathematically represent the fairness or bias levels of a ML model.&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;Many mitigation methods have been proposed in the literature, which can be broadly classified into the application of a mitigation method on the data set (&lt;b&gt;pre-processing&lt;/b&gt;), in the model fitting (&lt;b&gt;in-processing&lt;/b&gt;), and to the model predictions (&lt;b&gt;post-processing&lt;/b&gt;). Since a typical mitigation method is generally designed to improve some specific fairness metric(s), it is crucial to select the appropriate mitigation methods once the fairness metrics are chosen.&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;There are many different interpretations of what it means for an algorithm to be &#34;fair&#34;. For binary classification problems, fairness metrics can be broadly classified into group fairness metrics and individual and counterfactual fairness metrics. Group fairness metrics can be further classified into parity-based metrics, confusion matrix-based metrics, calibration metrics, and prediction score-based metrics. &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;However, Fairness-based ML comes with its own challenges:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Plethora of fairness metics and mitigation methods&lt;/li&gt; &#xA; &lt;li&gt;Little guidance for selecting the suitable fairness metric&lt;/li&gt; &#xA; &lt;li&gt;Navigating trade-offs between different fairness metrics&lt;/li&gt; &#xA; &lt;li&gt;The most appropriate mitigation methods to use depending on the selected fairness metrics, the machine learning model used in the training, and other characteristics of the use case.&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To the best of our knowledge, no available solution exists in the literature attempting to tackle this problem. Thus, a systematic procedure for selecting the most suitable fairness notions and appropriate mitigation strategies for a specific machine learning based decision making system is highly desirable and necessary.&lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;code&gt;responsible_mlops&lt;/code&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/EqualityAI/responsible_mlops/main/img/learn.jpg&#34; align=&#34;left&#34; alt=&#34;&#34; width=&#34;400&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;We have conducted extensive literature review and &lt;a href=&#34;https://github.com/EqualityAI/responsible_mlops/raw/main/Fairness%20Metrics%20User%20Manual%20(1).pdf&#34;&gt;theoretical analysis&lt;/a&gt; on dozens of fairness metrics and mitigation methods. Theoretical properties of those fairness mitigation methods were analyzed to determine their suitability under various conditions to create our &lt;code&gt;responsible_mlops&lt;/code&gt; open source ML software framework for a pre-processing workflow. The main idea is to train the ML model on a &#34;repaired&#34; data set, and evaluate a Fairness metric to understand if mitigation enhances the fairness of the predictions.&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;Using a healthcare application, we illustrate every step &lt;b&gt;(see steps 1-9 below)&lt;/b&gt; of our fair machine learning framework. From choosing appropriate fairness metrics to determining suitable fairness mitigation strategies, from mitigation of fairness in data and models to fairness evaluation.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Define Research Question&lt;/li&gt; &#xA;  &lt;li&gt;Connect to Source Data&lt;/li&gt; &#xA;  &lt;li&gt;Select Fairness and Mitigation Strategy&lt;/li&gt; &#xA;  &lt;li&gt;Data Preparation&lt;/li&gt; &#xA;  &lt;li&gt;Fit Prediction Model&lt;/li&gt; &#xA;  &lt;li&gt;Compute Model Results and Fairness Score&lt;/li&gt; &#xA;  &lt;li&gt;Run Mitigation&lt;/li&gt; &#xA;  &lt;li&gt;Compute Model Results and Fairness Score After Mitigation&lt;/li&gt; &#xA;  &lt;li&gt;Compare Model Results and Fairness Score Before and After Mitigation&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;To make our &lt;code&gt;responsible_mlops&lt;/code&gt; easy to follow, our expert statisticians, academic partners and machine learning experts have likened these various workflows to something everyone can understand—&lt;a href=&#34;https://github.com/EqualityAI/responsible_mlops/raw/main/Equality%20AI%20Fair%20Pre-processing%20Machine%20Learning%20Recipe.pdf&#34;&gt;a recipe&lt;/a&gt;. These recipes outline the “ingredients” you need and the exact steps to take to fit a fairness-based ML model. A recipe will not be showing any of the code but will walk through how we ran our case study, with explanations of conceptually executing each step or series of functions in our &lt;code&gt;responsible_mlops&lt;/code&gt;. To follow along with the code, open the script &lt;a href=&#34;https://github.com/EqualityAI/responsible_mlops/raw/main/examples/example_dataset_NHAMCS.R&#34;&gt;example_dataset_NHAMCS.R&lt;/a&gt;.&lt;br&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Using a healthcare application for hospital admission, our recipe will illustrate how we selected the fairness metric of Statistical Parity. We focus on three pre-processing methods to mitigate bias in the dataset, namely disparate impact remover, reweighing, and resampling. We assess the (un)fairness of a machine learning model trained on this dataset prior to applying bias mitigation. Finally, we assess the (un)fairness of the same machine learning model after bias mitigation is applied. By comparing the predictions before and after mitigation, we will be able to assess whether and to what extent the fairness of hospital admission predictions across different racial groups can be improved. Furthermore, the trade-offs between the accuracy and fairness of the machine learning model will be examined. &lt;br&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Function Highlights&lt;/h2&gt; &#xA;&lt;p&gt;Our plug and play functions are combined into two categories, 1) standardizing the series of discretionary judgment calls data scientists make into a series of best-practice MLOps functions and 2) providing guidance on selection and application of fairness metrics and mitigation methods.&lt;/p&gt; &#xA;&lt;h3&gt;Best-practice MLOps functions&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;data_fetch()&lt;/li&gt; &#xA; &lt;li&gt;data_prepare()&lt;/li&gt; &#xA; &lt;li&gt;ml_method()&lt;/li&gt; &#xA; &lt;li&gt;ml_results()&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Fairness and Bias mitigation functions&lt;/h3&gt; &#xA;&lt;p&gt;&lt;ins&gt;Guidance on selection&lt;/ins&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;fairness_tree_metric()&lt;/li&gt; &#xA; &lt;li&gt;mitigation_mapping_method()&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;ins&gt;Application of metrics and methods&lt;/ins&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;fairness_scores()&lt;/li&gt; &#xA; &lt;li&gt;bias_mitigation()&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For additional resources, check &lt;a href=&#34;&#34;&gt;out our EAI library&lt;/a&gt; of Fairness research compiled by our statisticians, academic partners and machine learning experts.&lt;/p&gt; &#xA;&lt;h3&gt;Disclaimer&lt;/h3&gt; &#xA;&lt;p&gt;The use case is intended for demonstration purposes only to highlight responsible AI methods to address data and model bias and improve fairness in model predictions. The output model is not intended to be adopted without utilizing additional bias assessment and mitigations, further model development, validation, and adherence to institutional governance processes.&lt;/p&gt; &#xA;&lt;h2&gt;Responsible AI Takes a Community&lt;/h2&gt; &#xA;&lt;p&gt;The connections and trade-offs between fairness, explainability, and privacy require a holistic approach to Responsible AI development in the machine learning community. We are starting with the principle of fairness and working towards a solution that incorporates multiple aspects of Responsible AI for data scientists and healthcare professionals. We have much more in the works, and we want to know—what do you need? Do you have a Responsible AI challenge you need to solve? &lt;a href=&#34;https://equalityai.slack.com/join/shared_invite/zt-1claqpebo-MnGnGoqCM9Do~40HqbSaww#/shared-invite/email&#34;&gt;Drop us a line and let’s see how we can help!&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributing to the project&lt;/h2&gt; &#xA;&lt;p&gt;Equality AI uses both GitHib and Slack to manage our open source community. To participate:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Join the Slack community (&lt;a href=&#34;https://equalityai.com/slack&#34;&gt;https://equalityai.com/slack&lt;/a&gt;) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Introduce yourself in the #Introductions channel. We&#39;re all friendly people!&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Check out the &lt;a href=&#34;https://github.com/EqualityAI/responsible_mlops/raw/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING&lt;/a&gt; file to learn how to contribute to our project, report bugs, or make feature requests.&lt;/li&gt; &#xA; &lt;li&gt;Try out the &lt;a href=&#34;https://github.com/EqualityAI/responsible_mlops&#34;&gt;&lt;code&gt;responsible_mlops&lt;/code&gt;&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Hit the top right &#34;star&#34; button on GitHub to show your love!&lt;/li&gt; &#xA;   &lt;li&gt;Follow the recipe above to use the code.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Provide feedback on your experience using the &lt;a href=&#34;https://github.com/EqualityAI/respomsible_mlops/discussions&#34;&gt;GitHub discussions&lt;/a&gt; or the &lt;a href=&#34;https://equalityai.slack.com/archives/C03HF7G4N0Y&#34;&gt;Slack #support&lt;/a&gt; channel &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;For any questions or problems, send a message on Slack, or send an email to &lt;a href=&#34;mailto:support@equalityai.com&#34;&gt;support@equalityai.com&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>rstudio/r-manuals</title>
    <updated>2022-08-18T01:50:44Z</updated>
    <id>tag:github.com,2022-08-18:/rstudio/r-manuals</id>
    <link href="https://github.com/rstudio/r-manuals" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A re-styled version of the R manuals&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;r-manuals&lt;/h1&gt; &#xA;&lt;!-- badges: start --&gt; &#xA;&lt;!-- badges: end --&gt; &#xA;&lt;p&gt;This project translates the &lt;a href=&#34;https://cran.r-project.org/manuals.html&#34;&gt;official R manuals&lt;/a&gt; to &lt;a href=&#34;https://quarto.org/&#34;&gt;quarto&lt;/a&gt; with the aim of lightly re-styling for better readability:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Separate the manual into page for each chapter&lt;/li&gt; &#xA; &lt;li&gt;Code highlighting and formatting&lt;/li&gt; &#xA; &lt;li&gt;Footnotes in sidebar&lt;/li&gt; &#xA; &lt;li&gt;Improved searching&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can view the resulting &lt;a href=&#34;https://rstudio.github.io/r-manuals/r-intro/&#34;&gt;https://rstudio.github.io/r-manuals&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How this works&lt;/h2&gt; &#xA;&lt;p&gt;Background information&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Every hour the R source code (at SVN) gets copied to github, into the repo &lt;a href=&#34;https://github.com/wch/r-source&#34;&gt;https://github.com/wch/r-source&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The manuals are in the folder &lt;a href=&#34;https://github.com/wch/r-source/tree/trunk/doc/manual&#34;&gt;https://github.com/wch/r-source/tree/trunk/doc/manual&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Each manual is in file of format &lt;code&gt;texinfo&lt;/code&gt;, the “GNU documentation format” (&lt;a href=&#34;https://www.gnu.org/software/texinfo/&#34;&gt;https://www.gnu.org/software/texinfo/&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Since it is possible to render a &lt;code&gt;texinfo&lt;/code&gt; document to HTML&lt;/li&gt; &#xA; &lt;li&gt;Once in HTML format, you can use &lt;code&gt;pandoc&lt;/code&gt; to convert the HTML to markdown, manipulate the markdown and then re-convert to HTML&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;In more detail&lt;/h2&gt; &#xA;&lt;p&gt;For each manual:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download the relevant &lt;code&gt;texinfo&lt;/code&gt; manual files and associated images and environment variables&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;makeinfo&lt;/code&gt; to convert from &lt;code&gt;.texi&lt;/code&gt; to HTML&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;pandoc&lt;/code&gt; with additional &lt;a href=&#34;https://pandoc.org/lua-filters.html&#34;&gt;Lua filters&lt;/a&gt; to convert to markdown&lt;/li&gt; &#xA; &lt;li&gt;Perform additional processing and conversion in R&lt;/li&gt; &#xA; &lt;li&gt;Include the markdown files in a quarto book template&lt;/li&gt; &#xA; &lt;li&gt;Render the manual to quarto&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Then combine the various quarto books into a quarto website&lt;/p&gt; &#xA;&lt;h2&gt;Note about installing &lt;code&gt;makeinfo&lt;/code&gt;&lt;/h2&gt; &#xA;&lt;p&gt;On Linux, you can easily install the &lt;code&gt;makeinfo&lt;/code&gt; utility using &lt;code&gt;sudo apt install texinfo&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;On Windows, &lt;code&gt;makeinfo&lt;/code&gt; is installed as part of &lt;a href=&#34;https://cran.r-project.org/bin/windows/Rtools/rtools40.html&#34;&gt;RTools40&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Licensing&lt;/h1&gt; &#xA;&lt;p&gt;We provide the conversion code, i.e.&amp;nbsp;this collection of R scripts and pandoc lua filters, under the MIT license.&lt;/p&gt; &#xA;&lt;p&gt;The original R manuals are not under this MIT license. Instead, these manuals contain the following license:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Copyright © 1999–2021 R Core Team&lt;/p&gt; &#xA; &lt;p&gt;Permission is granted to make and distribute verbatim copies of this manual provided the copyright notice and this permission notice are preserved on all copies.&lt;/p&gt; &#xA; &lt;p&gt;Permission is granted to copy and distribute modified versions of this manual under the conditions for verbatim copying, provided that the entire resulting derived work is distributed under the terms of a permission notice identical to this one.&lt;/p&gt; &#xA; &lt;p&gt;Permission is granted to copy and distribute translations of this manual into another language, under the above conditions for modified versions, except that this permission notice may be stated in a translation approved by the R Core Team.&lt;/p&gt; &#xA;&lt;/blockquote&gt;</summary>
  </entry>
</feed>