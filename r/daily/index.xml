<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub R Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-11T01:33:47Z</updated>
  <subtitle>Daily Trending of R in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ben-domingue/irw</title>
    <updated>2024-03-11T01:33:47Z</updated>
    <id>tag:github.com,2024-03-11:/ben-domingue/irw</id>
    <link href="https://github.com/ben-domingue/irw" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code related to data for the Item Response Warehouse&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;IRW&lt;/h1&gt; &#xA;&lt;h2&gt;Notes about adding to the IRW&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;To add data from the queue to the IRW repository there are three todo items:&lt;/p&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;Create a Github issue for this repository that describes any decisions you had to make and also includes a file with | in IRW format. Note that this may not be appropriate if the data is not publicly sharable. Contact us at the below email if that is the case.&lt;/li&gt; &#xA;   &lt;li&gt;Once we have confirmed that the data is appropriate, submit a pull request so that the code used to format the data gets added &lt;a href=&#34;https://github.com/ben-domingue/irw/tree/main/data&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Finally, ensure the &#39;data index&#39; page &lt;a href=&#34;https://docs.google.com/spreadsheets/d/1nhPyvuAm3JO8c9oa1swPvQZghAvmnf4xlYgbvsFH99s/edit#gid=0&#34;&gt;here&lt;/a&gt; gets updated with the relevant information.&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;We have a queue of data that we aim to add to the IRW available &lt;a href=&#34;https://docs.google.com/spreadsheets/d/13EzVbybU6pIrTq6xiivLvcN9h5OMi3wGM9W-xASpMVI/edit#gid=1076583183&#34;&gt;here&lt;/a&gt;. We have typically tried to do some initial checks to ensure that these data are appropriate, but further investigation often suggests otherwise. Please feel free to each out to us at &lt;a href=&#34;mailto:itemresponsewarehouse@stanford.edu&#34;&gt;itemresponsewarehouse@stanford.edu&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>RamiKrispin/rstudio-docker-template</title>
    <updated>2024-03-11T01:33:47Z</updated>
    <id>tag:github.com,2024-03-11:/RamiKrispin/rstudio-docker-template</id>
    <link href="https://github.com/RamiKrispin/rstudio-docker-template" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A template to launch RStudio inside a container with the local RStudio settings with docker-compose&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;RStudio Docker Template&lt;/h1&gt; &#xA;&lt;p&gt;üößWIP üèóÔ∏è, pre spell checkingüõ†Ô∏è&lt;/p&gt; &#xA;&lt;p&gt;The repository provides a template for launching an RStudio Server in a container with the local RStudio settings (i.e., code snippets, color theme, environment variables, etc.). The template leverages the &lt;a href=&#34;https://rocker-project.org/&#34;&gt;Rocker Project&lt;/a&gt; built-in RStudio image with an additional customization layer.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;figure&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/RamiKrispin/rstudio-docker-template/main/images/rstudio.png&#34; width=&#34;100%&#34; align=&#34;center&#34;&gt; &#xA;&lt;/figure&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Settings&lt;/h2&gt; &#xA;&lt;p&gt;This repository has the following structure:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;.&#xA;‚îú‚îÄ‚îÄ README.md&#xA;‚îú‚îÄ‚îÄ docker&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ Dockerfile&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ build_image.sh&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ install_packages.R&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ packages.json&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îî‚îÄ‚îÄ set_dependencies.sh&#xA;‚îî‚îÄ‚îÄ docker-compose.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;docker&lt;/code&gt; folder contains the docker template for adding new packages and the &lt;code&gt;docker-compose.yml&lt;/code&gt; file has the RStudio container launch settings.&lt;/p&gt; &#xA;&lt;h2&gt;Launching RStudio with Docker Compose&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;docker-compose.yml&lt;/code&gt; file provides a concise form for the below &lt;code&gt;docker run&lt;/code&gt; command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker run --rm -ti \&#xA;-v .:/home/rstudio \&#xA;-v $HOME/.config/rstudio:/home/rstudio/.config/rstudio \&#xA;-v $HOME/.Renviron:/home/rstudio/.Renviron \&#xA;-e PASSWORD=password \&#xA;-p 8787:8787 rocker/rstudio&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This includes the following functionality:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Launching the Rocker container - &lt;code&gt;rocker/rstudio:4.3.2&lt;/code&gt; in an interactive mode&lt;/li&gt; &#xA; &lt;li&gt;Mount the local folder to the container home folder&lt;/li&gt; &#xA; &lt;li&gt;Mount the local RStudio settings and &lt;code&gt;.Renviron&lt;/code&gt; file&lt;/li&gt; &#xA; &lt;li&gt;Set the login password as a &lt;code&gt;password&lt;/code&gt;. By default, the login username is &lt;code&gt;rstudio&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Map the local port &lt;code&gt;8787&lt;/code&gt; with the container exposed port - &lt;code&gt;8787&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;code&gt;docker-compose.yml&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;version: &#34;3.9&#34;&#xA;services:&#xA;  rstudio:&#xA;    image: &#34;rocker/rstudio:4.3.2&#34;&#xA;    ports:&#xA;      - &#34;8787:8787&#34;&#xA;    volumes:&#xA;      - type: &#34;bind&#34;&#xA;        source: &#34;.&#34;&#xA;        target: &#34;/home/rstudio&#34;&#xA;      - type: &#34;bind&#34;&#xA;        source: &#34;$HOME/.config/rstudio&#34;&#xA;        target: &#34;/home/rstudio/.config/rstudio&#34;&#xA;      - type: &#34;bind&#34;&#xA;        source: &#34;$HOME/.Renviron&#34;&#xA;        target: &#34;/home/rstudio/.Renviron&#34;&#xA;    environment:&#xA;      - PASSWORD=yourpassword&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; This docker-compose setting mounts the local folder to the container. You can customize it by modifying the &lt;code&gt;source&lt;/code&gt; argument under the first &lt;code&gt;bind&lt;/code&gt; section to the preferred folder path.&lt;/p&gt; &#xA;&lt;h2&gt;Customize the Image&lt;/h2&gt; &#xA;&lt;p&gt;The docker folder provides the settings for building an image on top of the rocker/rstudio image. This enables you to customize your environment and add additional libraries and other dependencies. This includes the following files:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;build_image.sh&lt;/code&gt; - a Bash script to build and push the image&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;set_dependencies.sh&lt;/code&gt; - a Bash script that installs the image dependencies during the build time. This includes leveraging the following scripts: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;install_packages.R&lt;/code&gt; - an R script to install R packages during the image build time&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;packages.json&lt;/code&gt; - a JSON file with the list of packages and their versions to install by the &lt;code&gt;install_packages.R&lt;/code&gt; file&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To build the image you will need have &lt;a href=&#34;https://jqlang.github.io/jq/&#34;&gt;jq&lt;/a&gt; installed to parse the JSON file.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;build_image.sh&lt;/code&gt; script has one argument - the image name. For example, the below code will build and name the image as &lt;code&gt;rkrispin/rstudio-customize:dev&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;bash build_image.sh rkrispin/rstudio-customize:dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Running RStudio Inside a Container - TBD&lt;/li&gt; &#xA; &lt;li&gt;RStudio - &lt;a href=&#34;https://posit.co/products/open-source/rstudio/&#34;&gt;https://posit.co/products/open-source/rstudio/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;The Rocker Project - &lt;a href=&#34;https://rocker-project.org/&#34;&gt;https://rocker-project.org/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This template is licensed under a &lt;a href=&#34;https://creativecommons.org/licenses/by-nc-sa/4.0/&#34;&gt;Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International&lt;/a&gt; License.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>AlbertRapp/tidychatmodels</title>
    <updated>2024-03-11T01:33:47Z</updated>
    <id>tag:github.com,2024-03-11:/AlbertRapp/tidychatmodels</id>
    <link href="https://github.com/AlbertRapp/tidychatmodels" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Chat With All Kinds of AI Models Through a Common Interface&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;tidychatmodels&lt;/h1&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/AlbertRapp/tidychatmodels/master/tidychatmodels.png&#34; width=&#34;400&#34;&gt; &#xA;&lt;h2&gt;About this package&lt;/h2&gt; &#xA;&lt;p&gt;This package provides a simple interface to chat with your favorite AI chatbot from R. It is inspired by the modular nature of &lt;code&gt;{tidymodels}&lt;/code&gt; where you can easily swap out any ML model for another one but keep the other parts of the workflow the same. In the same vain, this package aims to communicate with different chatbot vendors like &lt;a href=&#34;https://platform.openai.com/docs/api-reference/making-requests&#34;&gt;openAI&lt;/a&gt;, &lt;a href=&#34;https://docs.mistral.ai/api/&#34;&gt;mistral.ai&lt;/a&gt;, etc. using the same interface.&lt;/p&gt; &#xA;&lt;p&gt;Basically, this package is a wrapper around the API of different chatbots and provides a unified interface to communicate with them. The underlying package that handles all the communication is the &lt;a href=&#34;https://httr2.r-lib.org/&#34;&gt;&lt;code&gt;{httr2}&lt;/code&gt;&lt;/a&gt; package. For a deep dive into &lt;code&gt;{httr2}&lt;/code&gt;, you could check out one of my tutorials &lt;a href=&#34;https://youtu.be/hmtE4QGIOuk&#34;&gt;on YouTube&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Currently this package is only available on GitHub. To install it, you will need to use the &lt;code&gt;devtools&lt;/code&gt; package.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# install.packages(&#34;devtools&#34;)&#xA;devtools::install_github(&#34;AlbertRapp/tidychatmodels&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;What you will need to get started is an API key from the chatbot vendor you want to use. For example, to use the openAI chatbot, you will need to sign up for an API key &lt;a href=&#34;https://platform.openai.com/api-keys&#34;&gt;here&lt;/a&gt;. Once you have that key, you can use it to authenticate with the openAI API. I recommend saving the key into a &lt;code&gt;.env&lt;/code&gt; file and loading the key into your R environment using the &lt;code&gt;{dotenv}&lt;/code&gt; package.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dotenv::load_dot_env(&#39;../.env&#39;)&#xA;library(tidychatmodels)&#xA;chat_openai &amp;lt;- create_chat(&#39;openai&#39;, Sys.getenv(&#39;OAI_DEV_KEY&#39;))&#xA;chat_openai&#xA;## Chat Engine: openai &#xA;## Messages: 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Afterwards, you can add a model to the chat object. In this case, we are adding the &lt;code&gt;gpt-3.5-turbo&lt;/code&gt; model. The user is responsible for knowing which models are available at a vendor like OpenAI.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;chat_openai |&amp;gt;&#xA;  add_model(&#39;gpt-3.5-turbo&#39;)&#xA;## Chat Engine: openai &#xA;## Messages: 0 &#xA;## Model: gpt-3.5-turbo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Similarly, you can add parameters to the chat object.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;create_chat(&#39;openai&#39;, Sys.getenv(&#39;OAI_DEV_KEY&#39;))|&amp;gt;&#xA;  add_model(&#39;gpt-3.5-turbo&#39;) |&amp;gt;&#xA;  add_params(&#39;temperature&#39; = 0.5, &#39;max_tokens&#39; = 100)&#xA;## Chat Engine: openai &#xA;## Messages: 0 &#xA;## Model: gpt-3.5-turbo &#xA;## Parameters: &#xA;##    temperature: 0.5 &#xA;##    max_tokens: 100&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Afterwards, you can add messages to your chat object using different roles. Typically, you might first use a system manage to set the stage for what you bot is required to do. Afterwards, you can add a user message.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;chat_openai &amp;lt;- create_chat(&#39;openai&#39;, Sys.getenv(&#39;OAI_DEV_KEY&#39;))|&amp;gt;&#xA;  add_model(&#39;gpt-3.5-turbo&#39;) |&amp;gt;&#xA;  add_params(temperature = 0.5, max_tokens = 100) |&amp;gt;&#xA;  add_message(&#xA;    role = &#39;system&#39;,&#xA;    message = &#39;You are a chatbot that completes texts.&#xA;    You do not return the full text.&#xA;    Just what you think completes the text.&#39;&#xA;  ) |&amp;gt; &#xA;  add_message(&#xA;    # default role = &#39;user&#39;&#xA;    &#39;2 + 2 is 4, minus 1 that\&#39;s 3, &#39;&#xA;  ) &#xA;chat_openai&#xA;## Chat Engine: openai &#xA;## Messages: 2 &#xA;## Model: gpt-3.5-turbo &#xA;## Parameters: &#xA;##    temperature: 0.5 &#xA;##    max_tokens: 100&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;At this stage, you haven‚Äôt actually started any chat with the bot. You can do so by calling the &lt;code&gt;perform_chat&lt;/code&gt; method. Beware that this will consume your API calls and will likely incur costs. Once the chat is performed, you can extract the chat from the chat object.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;chat_openai &amp;lt;- chat_openai |&amp;gt; perform_chat()&#xA;chat_openai |&amp;gt; extract_chat()&#xA;## System: You are a chatbot that completes texts.&#xA;##     You do not return the full text.&#xA;##     Just what you think completes the text. &#xA;## User: 2 + 2 is 4, minus 1 that&#39;s 3,  &#xA;## Assistant: Quick maths!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Excellent! ChatGPT seems to know the next line of this &lt;a href=&#34;https://www.youtube.com/watch?v=M3ujv8xdK2w&amp;amp;ab_channel=musiclyrics&#34;&gt;glorious song&lt;/a&gt;. Also, you can save the chat into a tibble. If you want to surpress the output of the chat, you can use the &lt;code&gt;silent&lt;/code&gt; parameter.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;msgs &amp;lt;- chat_openai |&amp;gt; extract_chat(silent = TRUE)&#xA;msgs&#xA;## # A tibble: 3 √ó 2&#xA;##   role      message                                                             &#xA;##   &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;                                                               &#xA;## 1 system    &#34;You are a chatbot that completes texts.\n    You do not return the‚Ä¶&#xA;## 2 user      &#34;2 + 2 is 4, minus 1 that&#39;s 3, &#34;                                    &#xA;## 3 assistant &#34;Quick maths!&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You could add another message to the chat by adding a user message and then performing the chat again. While we‚Äôre at it, let‚Äôs just modify the &lt;code&gt;temperature&lt;/code&gt; parameter to for this new reply.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;chat_openai &amp;lt;- chat_openai |&amp;gt; &#xA;  add_message(&#xA;    role = &#39;user&#39;,&#xA;    message = &#39;Make it cooler!&#39;&#xA;  ) |&amp;gt; &#xA;  add_params(temperature = 0.9) |&amp;gt; &#xA;  perform_chat()&#xA;&#xA;chat_openai |&amp;gt; extract_chat()&#xA;## System: You are a chatbot that completes texts.&#xA;##     You do not return the full text.&#xA;##     Just what you think completes the text. &#xA;## User: 2 + 2 is 4, minus 1 that&#39;s 3,  &#xA;## Assistant: Quick maths! &#xA;## User: Make it cooler! &#xA;## Assistant: Everyday man&#39;s on the block!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Ah yes, that‚Äôs much cooler. But beware, this sent the whole chat again and consumed another API call.&lt;/p&gt; &#xA;&lt;h3&gt;Switching to another vendor&lt;/h3&gt; &#xA;&lt;p&gt;Let‚Äôs recap our full workflow.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;create_chat(&#39;openai&#39;, Sys.getenv(&#39;OAI_DEV_KEY&#39;)) |&amp;gt;&#xA;  add_model(&#39;gpt-3.5-turbo&#39;) |&amp;gt;&#xA;  add_params(temperature = 0.5, max_tokens = 100) |&amp;gt;&#xA;  add_message(&#xA;    role = &#39;system&#39;,&#xA;    message = &#39;You are a chatbot that completes texts.&#xA;    You do not return the full text.&#xA;    Just what you think completes the text.&#39;&#xA;  ) |&amp;gt; &#xA;  add_message(&#xA;    # default role = &#39;user&#39;&#xA;    &#39;2 + 2 is 4, minus 1 that\&#39;s 3, &#39;&#xA;  ) |&amp;gt; &#xA;  perform_chat()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can easily switch so some other vendor now. For example, let‚Äôs go for the &lt;code&gt;mistral-large-latest&lt;/code&gt; model from &lt;a href=&#34;https://docs.mistral.ai/api/&#34;&gt;Mistral.ai&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mistral_chat &amp;lt;- create_chat(&#39;mistral&#39;, Sys.getenv(&#39;MISTRAL_DEV_KEY&#39;)) |&amp;gt;&#xA;  add_model(&#39;mistral-large-latest&#39;) |&amp;gt;&#xA;  add_params(temperature = 0.5, max_tokens = 100) |&amp;gt;&#xA;  add_message(&#xA;    role = &#39;system&#39;,&#xA;    message = &#39;You are a chatbot that completes texts.&#xA;    You do not return the full text.&#xA;    Just what you think completes the text.&#39;&#xA;  ) |&amp;gt; &#xA;  add_message(&#xA;    # default role = &#39;user&#39;&#xA;    &#39;2 + 2 is 4, minus 1 that\&#39;s 3, &#39;&#xA;  ) |&amp;gt; &#xA;  perform_chat()&#xA;mistral_chat |&amp;gt; extract_chat()&#xA;## System: You are a chatbot that completes texts.&#xA;##     You do not return the full text.&#xA;##     Just what you think completes the text. &#xA;## User: 2 + 2 is 4, minus 1 that&#39;s 3,  &#xA;## Assistant: and if you add 5, that makes 8.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Supported vendors&lt;/h2&gt; &#xA;&lt;p&gt;Currently only supported vendors are openAI, mistral.ai and &lt;a href=&#34;https://ollama.com/&#34;&gt;ollama&lt;/a&gt;. ollama allows you to deploy local LLMs and chat with them through &lt;code&gt;localhost&lt;/code&gt;. The ollama‚Äôs API is a bit different but on our &lt;code&gt;tidychatmodels&lt;/code&gt; interface everything should still works the same or be at least very similar.&lt;/p&gt; &#xA;&lt;p&gt;For example, creating a chat works pretty much the same but doesn‚Äôt require an API key.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;create_chat(&#39;ollama&#39;) &#xA;## Chat Engine: ollama &#xA;## Messages: 0 &#xA;## Parameters: &#xA;##    stream: FALSE&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Notice how there is already a parameter &lt;code&gt;stream&lt;/code&gt; that is set to &lt;code&gt;false&lt;/code&gt;. This is a change in the API of the ollama chat engine. You see, by default ollama will stream the reply token by token. But &lt;code&gt;{httr2}&lt;/code&gt; doesn‚Äôt want that (or rather I didn‚Äôt bother looking into how to do that with &lt;code&gt;{httr2}&lt;/code&gt;). So that‚Äôs why by default we set &lt;code&gt;stream&lt;/code&gt; to &lt;code&gt;false&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Now, you can add a local model to your chat. So, assume that you ran (outside of R)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ollama pull gemma:7b&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you can add the model to your chat object.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;create_chat(&#39;ollama&#39;) |&amp;gt;&#xA;  add_model(&#39;gemma:7b&#39;)&#xA;## Chat Engine: ollama &#xA;## Messages: 0 &#xA;## Model: gemma:7b &#xA;## Parameters: &#xA;##    stream: FALSE&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And just like before, you can add messages and perform the chat. Beware though that for some models system messages are not actually working like for openAI or mistral.ai models. So here I‚Äôve only added a user message.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ollama_chat &amp;lt;- create_chat(&#39;ollama&#39;) |&amp;gt;&#xA;  add_model(&#39;gemma:7b&#39;) |&amp;gt; &#xA;  add_message(&#39;What is love? IN 10 WORDS.&#39;) |&amp;gt; &#xA;  perform_chat() &#xA;&#xA;ollama_chat |&amp;gt; &#xA;  extract_chat()&#xA;## User: What is love? IN 10 WORDS. &#xA;## Assistant: Love is a feeling of intense affection and fondness for a person, often accompanied by a desire to protect and care for that person.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And adding more messages works too.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ollama_chat &amp;lt;- ollama_chat |&amp;gt; &#xA;  add_message(&#39;Now describe hate in 10 words&#39;) |&amp;gt; &#xA;  perform_chat() &#xA;&#xA;ollama_chat |&amp;gt; &#xA;  extract_chat()&#xA;## User: What is love? IN 10 WORDS. &#xA;## Assistant: Love is a feeling of intense affection and fondness for a person, often accompanied by a desire to protect and care for that person. &#xA;## User: Now describe hate in 10 words &#xA;## Assistant: Hate is a feeling of intense dislike or disgust for a person, often accompanied by a desire to harm or mistreat that person.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;msgs &amp;lt;- ollama_chat |&amp;gt; extract_chat()&#xA;## User: What is love? IN 10 WORDS. &#xA;## Assistant: Love is a feeling of intense affection and fondness for a person, often accompanied by a desire to protect and care for that person. &#xA;## User: Now describe hate in 10 words &#xA;## Assistant: Hate is a feeling of intense dislike or disgust for a person, often accompanied by a desire to harm or mistreat that person.&#xA;&#xA;ollama_chat |&amp;gt; &#xA;  add_message(&#xA;    paste(&#xA;      &#39;You said: &#34;&#39;,&#xA;      msgs$message[2], &#xA;      msgs$message[4],&#xA;      &#39;&#34; Is there a relationship between these two?&#39;&#xA;    )&#xA;  ) |&amp;gt; &#xA;  perform_chat() |&amp;gt; &#xA;  extract_chat()&#xA;## User: What is love? IN 10 WORDS. &#xA;## Assistant: Love is a feeling of intense affection and fondness for a person, often accompanied by a desire to protect and care for that person. &#xA;## User: Now describe hate in 10 words &#xA;## Assistant: Hate is a feeling of intense dislike or disgust for a person, often accompanied by a desire to harm or mistreat that person. &#xA;## User: You said: &#34; Love is a feeling of intense affection and fondness for a person, often accompanied by a desire to protect and care for that person. Hate is a feeling of intense dislike or disgust for a person, often accompanied by a desire to harm or mistreat that person. &#34; Is there a relationship between these two? &#xA;## Assistant: Sure, there is a relationship between love and hate. Love and hate are opposite emotions, and they are often interconnected. When we love someone, we often feel less hate towards them. And when we hate someone, we often feel less love for them.&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>