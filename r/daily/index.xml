<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub R Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-08-12T01:51:22Z</updated>
  <subtitle>Daily Trending of R in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>brodieG/r2c</title>
    <updated>2022-08-12T01:51:22Z</updated>
    <id>tag:github.com,2022-08-12:/brodieG/r2c</id>
    <link href="https://github.com/brodieG/r2c" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Experimental and Limited R to C Conversion and Execution&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;r2c - Fast Iterated Statistic Computation in R&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Proof of Concept&lt;/strong&gt;. Experimental, incomplete, with an interface subject to change.&lt;/p&gt; &#xA;&lt;p&gt;Compiles a subset of R into machine code so that expressions composed with that subset can be applied repeatedly on varying data without interpreter overhead.&lt;/p&gt; &#xA;&lt;h2&gt;Background and Motivation&lt;/h2&gt; &#xA;&lt;p&gt;R is nearly as fast as statically compiled languages for many common numerical calculations:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;set.seed(1)&#xA;n &amp;lt;- 1e7&#xA;x &amp;lt;- runif(n) * runif(n)&#xA;y &amp;lt;- runif(n) * runif(n)&#xA;&#xA;system.time(x + y)&#xA;##   user  system elapsed&#xA;##  0.023   0.000   0.023&#xA;&#xA;system.time(sum(x))&#xA;##   user  system elapsed&#xA;##  0.043   0.000   0.043&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On &lt;a href=&#34;https://raw.githubusercontent.com/brodieG/r2c/main/#notes-on-benchmarking&#34;&gt;my system&lt;/a&gt; that&#39;s about 2-5 &lt;strong&gt;CPU cycles&lt;/strong&gt; for each of the 10 million operations and associated loop overhead. Hard to get much faster[^1]. If we maintain a high ratio of native operations to R level calls our programs will be fast.&lt;/p&gt; &#xA;&lt;p&gt;Some tasks require more R-level calls, such as computing group statistics[^7]: &lt;span id=&#34;in-r&#34;&gt;&lt;/span&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;g &amp;lt;- cumsum(sample(c(TRUE, rep(FALSE, 9)), n, replace=TRUE)) # sorted groups&#xA;x.split &amp;lt;- split(x, g)&#xA;y.split &amp;lt;- split(y, g)     # for later&#xA;length(x.split)            # ~1MM groups&#xA;## [1] 1001458&#xA;mean(lengths(x.split))     # ~10 average size&#xA;## [1] 9.99&#xA;&#xA;system.time(g.sum &amp;lt;- vapply(x.split, sum, 0))&#xA;##    user  system elapsed &#xA;##   0.652   0.010   0.664 &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Despite the same number of additions, our program &lt;strong&gt;slowed by ~15x&lt;/strong&gt;. And this is with a primitive R function that does nothing but go directly to C code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sum&#xA;## function (..., na.rm = FALSE)  .Primitive(&#34;sum&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;What If We Could Compile R?&lt;/h2&gt; &#xA;&lt;p&gt;That would be nice, wouldn&#39;t it? Well, we (at least I) can&#39;t compile the entirety of R, but a small set we can manage:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;library(r2c)&#xA;r2c_sum &amp;lt;- r2cq(sum(x))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And now:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;g.r2c &amp;lt;- process_groups(g, sorted=TRUE)   # pre-compute group sizes, labels&#xA;system.time(g.sum.r2c &amp;lt;- group_exec(r2c_sum, g.r2c, x))&#xA;##   user  system elapsed &#xA;##  0.054   0.001   0.056 &#xA;identical(g.sum, g.sum.r2c)&#xA;## [1] TRUE&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Nearly as fast as the simple sum despite the additional overhead of managing the groups and the larger result.&lt;/p&gt; &#xA;&lt;h2&gt;Alternatives&lt;/h2&gt; &#xA;&lt;p&gt;We compare &lt;code&gt;{r2c}&lt;/code&gt; to various alternatives using pre-sorted and pre-grouped (or split) data set as previously. Pre-sorting and pre-grouping allows us to focus timings on the statistic computation[^11]. Notwithstanding, fast group statistic calculation benefits from fast sorting, and this is possible in R thanks to &lt;a href=&#34;https://github.com/Rdatatable&#34;&gt;&lt;code&gt;{data.table}&lt;/code&gt;&lt;/a&gt; contributing their fast radix sort starting with R 3.3.0.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;re satisfied with simple expressions such as &lt;code&gt;sum(x)&lt;/code&gt; then there are several alternatives:&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/brodieG/r2c/main/extra/time_gsum_all-vs.png&#34;&gt; &#xA;&lt;p&gt;For this specific task &lt;a href=&#34;https://github.com/SebKrantz/collapse&#34;&gt;&lt;code&gt;{collapse}&lt;/code&gt;&lt;/a&gt; makes an impressive case for itself[^4]. &lt;a href=&#34;https://github.com/oracle/fastr&#34;&gt;&lt;code&gt;{FastR}&lt;/code&gt;&lt;/a&gt; is an implementation of R that can JIT compile R code to run on the &lt;a href=&#34;https://www.graalvm.org/&#34;&gt;Graal VM&lt;/a&gt;. It requires a different runtime (i.e. you can&#39;t just run your normal R installation) and has other trade-offs, including warm-up cycles and compatibility limitations[^3]. But otherwise you type in what &lt;a href=&#34;https://raw.githubusercontent.com/brodieG/r2c/main/#in-r&#34;&gt;you would have in normal R&lt;/a&gt; and see some impressive speed-ups.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;The test is unfair to &lt;code&gt;{data.table}&lt;/code&gt; as there is no method to pre-group the data for it (only pre-sort), and we disallow multi-threading, thus I would not use the above timings to pick alternatives over &lt;code&gt;{data.table}&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;The key feature that &lt;code&gt;{r2c}&lt;/code&gt; adds is the ability to construct complex expressions from simple ones. For example, the slope of a bivariate regression: &lt;span id=&#34;r2c-slope&#34;&gt;&lt;/span&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;slope &amp;lt;- function(x, y) sum((x - mean(x)) * (y - mean(y))) / sum((x - mean(x))^2)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And for this &lt;code&gt;{r2c}&lt;/code&gt; is fastest[^9]:&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/brodieG/r2c/main/extra/time_glope_all-vs.png&#34;&gt; &#xA;&lt;p&gt;&lt;code&gt;{data.table}&lt;/code&gt; cannot use its Gforce[^2] optimization on the more complex expression so falls back to standard evaluation. &lt;code&gt;{collapse}&lt;/code&gt; remains close to &lt;code&gt;{r2c}&lt;/code&gt;, but it requires enough knowledge of its workings to craft the equivalent expression[^8]:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;library(collapse)&#xA;g.clp &amp;lt;- GRP(g)    # pre-compute group sizes, labels&#xA;fmean2 &amp;lt;- function(x, cg) fmean(x, cg, TRA=&#34;replace_fill&#34;)  # notice TRA=...&#xA;&#xA;fsum((x - fmean2(x, g.clp)) * (y - fmean2(y, g.clp)), g.clp) / fsum((x - fmean2(x, g.clp))^2, g.clp)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Compare to &lt;code&gt;{r2c}&lt;/code&gt; and the other options where you compose the expression as you would in &lt;a href=&#34;https://raw.githubusercontent.com/brodieG/r2c/main/#r2c-slope&#34;&gt;base R&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;r2c_slope &amp;lt;- r2cq(&#xA;  sum((x - mean(x)) * (y - mean(y))) / sum((x - mean(x))^2)&#xA;)&#xA;group_exec(r2c_slope, g.r2c, list(x, y))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We&#39;ll run one last comparison: large groups. Here we try with average size of ~10,000 elements (vs 10 previously):&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/brodieG/r2c/main/extra/time_gslope_all_1e3-vs.png&#34;&gt; &#xA;&lt;p&gt;In this case the per-group interpreter overhead is no longer noticeable and most implementations are competitive. &lt;code&gt;{r2c}&lt;/code&gt; likely performs better because it re-uses the memory required for intermediate results for every group. The more complex expressions are, the more this benefit should manifest.&lt;/p&gt; &#xA;&lt;p&gt;To summarize:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;code&gt;{r2c}&lt;/code&gt; is fastest at calculating complex expressions group-wise, while also retaining base R semantics for numeric inputs.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/brodieG/r2c/main/#notes-on-benchmarking&#34;&gt;Code and notes on benchmarking&lt;/a&gt; available at end of document.&lt;/p&gt; &#xA;&lt;h2&gt;Caveats - Of Course ...&lt;/h2&gt; &#xA;&lt;p&gt;First is that &lt;code&gt;r2c&lt;/code&gt; requires compilation. I have not included that step in timings[^6] under the view that the compilation time will be amortized over many calculations. The facilities for this don&#39;t exist yet, but the plan is to to have &lt;code&gt;{r2c}&lt;/code&gt; maintain a local library of pre-compiled user-defined functions, and for packages to compile &lt;code&gt;{r2c}&lt;/code&gt; functions at install-time.&lt;/p&gt; &#xA;&lt;p&gt;More importantly, we cannot compile and execute arbitrary R expressions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Only &lt;code&gt;{r2c}&lt;/code&gt; implemented counterpart functions may be used (currently: basic arithmetic operators and &lt;code&gt;sum&lt;/code&gt;/&lt;code&gt;mean&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Primary numeric inputs must be attribute-less (e.g. to avoid expectations of S3 method dispatch or attribute manipulation), and any &lt;code&gt;.numeric&lt;/code&gt; methods defined will be ignored[^10].&lt;/li&gt; &#xA; &lt;li&gt;Future &lt;code&gt;{r2c}&lt;/code&gt; counterparts will be limited to functions that return attribute-less numeric vectors of constant size (e.g. &lt;code&gt;mean&lt;/code&gt;), or of the size of one of their inputs (e.g. like &lt;code&gt;+&lt;/code&gt;, or even &lt;code&gt;quantile&lt;/code&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Within these constraints &lt;code&gt;r2c&lt;/code&gt; is flexible. For example, it is possible to have arbitrary R objects for secondary parameters, as well as to reference group-invariant data:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;w &amp;lt;- c(1, NA, 2, 3)&#xA;u &amp;lt;- c(-1, 1, 0)&#xA;h &amp;lt;- rep(1:2, each=2)&#xA;&#xA;r2c_fun &amp;lt;- r2cq(sum(x, na.rm=TRUE) * y)&#xA;group_exec(r2c_fun, groups=h, data=list(x=w), MoreArgs=list(y=u))&#xA;##  1  1  1  2  2  2&#xA;## -1  1  0 -5  5  0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Notice the &lt;code&gt;na.rm&lt;/code&gt;, and that the &lt;code&gt;u&lt;/code&gt; in &lt;code&gt;list(y=u)&lt;/code&gt; is re-used in full for each group setting the output size to 3.&lt;/p&gt; &#xA;&lt;p&gt;The C counterparts to the R functions are intended to produce identical outputs, but have different implementations. As such, it is possible that for a particular set of inputs on a particular platform the results might diverge.&lt;/p&gt; &#xA;&lt;h2&gt;Future - Maybe?&lt;/h2&gt; &#xA;&lt;p&gt;In addition to cleaning up the existing code, there are many extensions that can be built on this proof of concept. Some are listed below. How many I end up working on will depend on some interaction of external and my own interest.&lt;/p&gt; &#xA;&lt;h3&gt;Better Grouping Semantics&lt;/h3&gt; &#xA;&lt;p&gt;Implement multi-column grouping and non-integer grouping columns.&lt;/p&gt; &#xA;&lt;h3&gt;More Functions&lt;/h3&gt; &#xA;&lt;p&gt;Functions that have direct analogues in C or are simple to code in C are the best candidates, subject to the previously described restrictions. Thus the following should be straightforward to implement:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;abs&lt;/code&gt;, unary &lt;code&gt;+&lt;/code&gt; and &lt;code&gt;-&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;min&lt;/code&gt;, &lt;code&gt;max&lt;/code&gt;, &lt;code&gt;first&lt;/code&gt;, &lt;code&gt;last&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cos&lt;/code&gt;, &lt;code&gt;sin&lt;/code&gt;, and other trigonometric functions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;range&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;length&lt;/code&gt;, &lt;code&gt;seq_along&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[[&lt;/code&gt;, &lt;code&gt;[&lt;/code&gt;, and maybe &lt;code&gt;$&lt;/code&gt;, likely with limitations on allowable index types.&lt;/li&gt; &#xA; &lt;li&gt;Many others.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;More challenging due to code complexity, but otherwise compatible with &lt;code&gt;{r2c}&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;quantile&lt;/code&gt;, and others.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Some other useful functions will require more work:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;diff&lt;/code&gt;, because the result size of &lt;code&gt;n - 1&lt;/code&gt; is not currently supported.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Functions that will likely not be implementable:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;seq&lt;/code&gt;, except perhaps for narrow cases where the parameters are constants or perhaps select expressions such as &lt;code&gt;length(n)&lt;/code&gt;, but even this becomes complicated.&lt;/li&gt; &#xA; &lt;li&gt;And many more.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Simple Assignment and Multi-Line Statements&lt;/h3&gt; &#xA;&lt;p&gt;While non-trivial, the existing structure should allow explicit intermediate variables and multi-call expressions. Both of these already exist implicitly as part of the call processing logic.&lt;/p&gt; &#xA;&lt;h3&gt;Other Repetition Structures&lt;/h3&gt; &#xA;&lt;h4&gt;Window Functions&lt;/h4&gt; &#xA;&lt;p&gt;It will be straightforward to implement a runner that invokes the compiled code on a sliding window instead of on groups. The main complication is accounting for incomplete windows at the beginning and end of the data.&lt;/p&gt; &#xA;&lt;h4&gt;Loops&lt;/h4&gt; &#xA;&lt;p&gt;More complex but in theory possible are loops that reference read/write vectors with subsetting and subset assignment, thus allowing results of previous loop iterations to be re-used in later ones.&lt;/p&gt; &#xA;&lt;h4&gt;Solvers&lt;/h4&gt; &#xA;&lt;p&gt;It should be possible to build a solver around &lt;code&gt;r2c&lt;/code&gt; compiled expressions, but there already exist similar implementations. In particular Rich FitzJohn&#39;s &lt;a href=&#34;https://github.com/mrc-ide/odin&#34;&gt;&lt;code&gt;{Odin}&lt;/code&gt;&lt;/a&gt; uses a very similar approach to &lt;code&gt;r2c&lt;/code&gt; to generate C routines for use with &lt;code&gt;deSolve&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;API&lt;/h4&gt; &#xA;&lt;p&gt;There are likely many applications that could benefit from the capabilities provided by &lt;code&gt;{r2c}&lt;/code&gt;. It should be possible to define an interface for use by external code. Conceivably, &lt;code&gt;{data.table}&lt;/code&gt; could be extended to run &lt;code&gt;{r2c}&lt;/code&gt; compiled expressions.&lt;/p&gt; &#xA;&lt;p&gt;Additionally, it should be possible to allow users to define their own C routines that integrated into the &lt;code&gt;{r2c}&lt;/code&gt; framework.&lt;/p&gt; &#xA;&lt;h3&gt;Re-using Compilation / Cleanup&lt;/h3&gt; &#xA;&lt;p&gt;Ideally once an expression is compiled into an &lt;code&gt;{r2c}&lt;/code&gt; function it would be preserved for re-use in future R sessions. Doing so within a package would be relatively straight-forward, but it should also be possible to create a local library to store such objects in.&lt;/p&gt; &#xA;&lt;p&gt;We&#39;ll also need to ensure that the methods we use to access the compiled instructions are legal, as what we do now is slightly questionable. More generally, the C internals have been implemented with the sole priority of producing a proof of concept rather than robust extensibility, and will need cleanup.&lt;/p&gt; &#xA;&lt;h3&gt;Optimizations&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;More aggressive re-use of intermediate memory.&lt;/li&gt; &#xA; &lt;li&gt;Identification of re-used calculations.&lt;/li&gt; &#xA; &lt;li&gt;Reduce per-group/iteration overhead.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;And likely more. So far the focus has been on implementation rather than optimization.&lt;/p&gt; &#xA;&lt;h2&gt;Related Work&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mrc-ide/odin&#34;&gt;&lt;code&gt;{Odin}&lt;/code&gt;&lt;/a&gt;, which implements a very similar R to C translation and compilation, but specialized for differential equation solving problems.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Rdatatable&#34;&gt;&lt;code&gt;{data.table}&lt;/code&gt;&lt;/a&gt;&#39;s Gforce (see `?data.table::datatable.optimize).&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/oracle/fastr&#34;&gt;&lt;code&gt;{FastR}&lt;/code&gt;&lt;/a&gt; an implementation of R that can JIT compile R code to run on the &lt;a href=&#34;https://www.graalvm.org/&#34;&gt;Graal VM&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/SebKrantz/collapse&#34;&gt;&lt;code&gt;{collapse}&lt;/code&gt;&lt;/a&gt;&#39;s specialized group statistic functions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/eddelbuettel/inline&#34;&gt;&lt;code&gt;{inline}&lt;/code&gt;&lt;/a&gt; to allow compilation and access to generated native code directly from R.&lt;/li&gt; &#xA; &lt;li&gt;In theory &lt;a href=&#34;https://dplyr.tidyverse.org/&#34;&gt;&lt;code&gt;{dplyr}&lt;/code&gt;&lt;/a&gt;&#39;s Hybrid Eval is similar to Gforce, but AFAICT it was &lt;a href=&#34;https://github.com/tidyverse/dplyr/issues/5017&#34;&gt;quietly dropped&lt;/a&gt; and despite suggestions it might return for v1.1 I see no trace of it in the most recent 1.1 candidate development versions (as of 2022-07-03).&lt;/li&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://www.brodieg.com/tags/hydra/&#34;&gt;Hydra Chronicles&lt;/a&gt; series of posts on my blog examining group statistics in R.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;R Core for developing and maintaining such a wonderful language.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mattdowle&#34;&gt;Matt Dowle&lt;/a&gt; and &lt;a href=&#34;https://github.com/arunsrinivasan&#34;&gt;Arun Srinivasan&lt;/a&gt; for contributing the &lt;code&gt;{data.table}&lt;/code&gt;&#39;s radix sort to R.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/SebKrantz&#34;&gt;Sebastian Krantz&lt;/a&gt; for the idea of pre-computing group meta data for possible re-use (taken from &lt;code&gt;collapse::GRP&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hadley/&#34;&gt;Hadley Wickham&lt;/a&gt; and &lt;a href=&#34;https://github.com/klutometis&#34;&gt;Peter Danenberg&lt;/a&gt; for &lt;a href=&#34;https://cran.r-project.org/package=roxygen2&#34;&gt;roxygen2&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kalibera&#34;&gt;Tomas Kalibera&lt;/a&gt; for &lt;a href=&#34;https://github.com/kalibera/rchk&#34;&gt;rchk&lt;/a&gt; and the accompanying vagrant image, and rcnst to help detect errors in compiled code.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/wch&#34;&gt;Winston Chang&lt;/a&gt; for the &lt;a href=&#34;https://hub.docker.com/r/wch1/r-debug/&#34;&gt;r-debug&lt;/a&gt; docker container, in particular because of the valgrind level 2 instrumented version of R.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hadley/&#34;&gt;Hadley Wickham&lt;/a&gt; et al. for &lt;a href=&#34;https://ggplot2.tidyverse.org/&#34;&gt;ggplot2&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Notes on Benchmarking&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/brodieG/r2c/raw/logo-old-method/extra/benchmarks-public.Rmd&#34;&gt;Benchmarks&lt;/a&gt; are under:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;R version 4.2.1 (2022-06-23)&#xA;Platform: x86_64-apple-darwin17.0 (64-bit)&#xA;Running under: macOS Big Sur ... 10.16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On an Intel(R) Core(TM) m5-6Y54 CPU @ 1.20GHz (early 2016 Macbook), using the average of 11 iterations run after one &lt;code&gt;gc()&lt;/code&gt; call, and -O2 optimization level. Different systems / compilers / settings may produce different results.&lt;/p&gt; &#xA;&lt;p&gt;[^1]: Depending on your compilation settings and machine, there is room for improvement, but not enough that R stands out as being particularly slow at this task. [^2]: Gforce is available for simple expressions of the form &lt;code&gt;fun(var)&lt;/code&gt; for many of the basic statistic functions (see &lt;code&gt;?data.table::datatable.optimize). [^3]: My limited experience with {&lt;/code&gt;FastR&lt;code&gt;}is that it is astonishing, but also frustrating. What it does is amazing, but the compatibility limitations are real (e.g. with the current version neither {&lt;/code&gt;data.table&lt;code&gt;} nor {&lt;/code&gt;ggplot2&lt;code&gt;} install out of the box, and more), and performance is volatile (e.g. package installation and some other tasks are painfully slow, some expressions will hiccup after the initial warm-up). At this point it does not seem like a viable drop-in replacement to R. It likely excels at running scalar operations in loops and similar, something that R itself struggles at. [^4]: Notice that &lt;/code&gt;fsum&lt;code&gt;with groups is faster than even the straight up&lt;/code&gt;sum&lt;code&gt;without groups, primarily because it handles the&lt;/code&gt;narm&lt;code&gt;as a dedicated branch instead of a conditional in the loop (this is an [oddity with&lt;/code&gt;sum&lt;code&gt;][8] on some platforms). &lt;/code&gt;fsum&lt;code&gt;also uses a plain double accumulator and not the long double used by the other implementations so the results are not identical to the other implementations that use long doubles (on systems that support them). Curiously on my system summing long doubles is faster than summing doubles (absent NAs, inifinities, or denormals, for which long double performance collapses). [^5]: We can make&lt;/code&gt;{collapse}&lt;code&gt;a little faster by computing&lt;/code&gt;mean(x)&lt;code&gt;once and re-using the result, but at that point the comparison is not apples to apples anymore. [^6]: The first compilation can be quite slow as it requires loading the compiler, etc. Subsequent compilations run in tenths of seconds. [^7]: For this very specific task R also provides&lt;/code&gt;rowsum&lt;code&gt;, but as it is limited to sums and we cannot separate the splitting and summing steps for timing we will not discuss it further. [^8]: Alternatives involve using &lt;/code&gt;fwithin(x)&lt;code&gt;as a replacement for&lt;/code&gt;(x - fmean(x, g, TRA=&#34;replace_fill&#34;))&lt;code&gt;and&lt;/code&gt;fgroup_by(g) |&amp;gt; fsummarize(...)&lt;code&gt;to avoid the need to repeatedly specify groups, although timings are similar with these changes. [^9]: In order to make the benchmarks comparable, we use&lt;/code&gt;r2c::mean1&lt;code&gt;instead of&lt;/code&gt;base::mean&lt;code&gt;. This is to ensure that all implementations are using a single pass mean calculation as that is what &lt;/code&gt;fmean&lt;code&gt;does. [^10]: E.g. don&#39;t expect S3 dispatch to work if you define&lt;/code&gt;mean.numeric&lt;code&gt;, although why one would do that for functions covered by &lt;/code&gt;{r2c}&lt;code&gt;is unclear. [^11]: Pre-grouping in this case means primarily computing group-offsets in data already sorted by group. Depending on the data, sorting and grouping can be a significant part of the computational cost, although it is similar for all all implementations tested here. &lt;/code&gt;{r2c}&lt;code&gt;, &lt;/code&gt;{data.table}&lt;code&gt;, &lt;/code&gt;{collapse}&lt;code&gt;all use radix sort, although&lt;/code&gt;{collapse}` also has a hash-based grouping algorithm that is particularly effective for integer groups in which the grouped result fits in CPU cache. Benchmarking the grouping is out of scope of this document for the time being, but it is an important part of group statistic computation.&lt;/p&gt;</summary>
  </entry>
</feed>