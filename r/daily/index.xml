<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub R Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-10-23T01:38:26Z</updated>
  <subtitle>Daily Trending of R in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>maju116/platypus</title>
    <updated>2023-10-23T01:38:26Z</updated>
    <id>tag:github.com,2023-10-23:/maju116/platypus</id>
    <link href="https://github.com/maju116/platypus" rel="alternate"></link>
    <summary type="html">&lt;p&gt;R package for object detection and image segmentation.&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://raw.githubusercontent.com/maju116/platypus/master/man/figures/hexsticker_platypus.png&#34; align=&#34;right&#34; alt=&#34;&#34; width=&#34;130&#34;&gt; &#xA;&lt;h1&gt;platypus&lt;/h1&gt; &#xA;&lt;!-- badges: start --&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://codecov.io/gh/maju116/platypus&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/maju116/platypus/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- badges: end --&gt; &#xA;&lt;p&gt;&lt;strong&gt;R package for object detection and image segmentation&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;With &lt;code&gt;platypus&lt;/code&gt; it is easy create advanced computer vision models like YOLOv3 and U-Net in a few lines of code.&lt;/p&gt; &#xA;&lt;h2&gt;How to install?&lt;/h2&gt; &#xA;&lt;p&gt;You can install the latest version of &lt;code&gt;platypus&lt;/code&gt; with &lt;code&gt;remotes&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;remotes::install_github(&#34;maju116/platypus&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(&lt;code&gt;master&lt;/code&gt; branch contains the stable version. Use &lt;code&gt;develop&lt;/code&gt; branch for latest features)&lt;/p&gt; &#xA;&lt;p&gt;To install &lt;a href=&#34;&#34;&gt;previous versions&lt;/a&gt; you can run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;remotes::install_github(&#34;maju116/platypus&#34;, ref = &#34;0.1.0&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In order to install &lt;code&gt;platypus&lt;/code&gt; you need to install &lt;code&gt;keras&lt;/code&gt; and &lt;code&gt;tensorflow&lt;/code&gt; packages and &lt;code&gt;Tensorflow&lt;/code&gt; version &lt;code&gt;&amp;gt;= 2.0.0&lt;/code&gt; (&lt;code&gt;Tensorflow 1.x&lt;/code&gt; will not be supported!)&lt;/p&gt; &#xA;&lt;h2&gt;YOLOv3 bounding box prediction with pre-trained COCO weights:&lt;/h2&gt; &#xA;&lt;p&gt;To create &lt;code&gt;YOLOv3&lt;/code&gt; architecture use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tidyverse)&#xA;library(platypus)&#xA;library(abind)&#xA;&#xA;test_yolo &amp;lt;- yolo3(&#xA;  net_h = 416, # Input image height. Must be divisible by 32&#xA;  net_w = 416, # Input image width. Must be divisible by 32&#xA;  grayscale = FALSE, # Should images be loaded as grayscale or RGB&#xA;  n_class = 80, # Number of object classes (80 for COCO dataset)&#xA;  anchors = coco_anchors # Anchor boxes&#xA;)&#xA;&#xA;test_yolo&#xA;#&amp;gt; Model&#xA;#&amp;gt; Model: &#34;yolo3&#34;&#xA;#&amp;gt; ________________________________________________________________________________&#xA;#&amp;gt; Layer (type)              Output Shape      Param #  Connected to               &#xA;#&amp;gt; ================================================================================&#xA;#&amp;gt; input_img (InputLayer)    [(None, 416, 416, 0                                   &#xA;#&amp;gt; ________________________________________________________________________________&#xA;#&amp;gt; darknet53 (Model)         multiple          40620640 input_img[0][0]            &#xA;#&amp;gt; ________________________________________________________________________________&#xA;#&amp;gt; yolo3_conv1 (Model)       (None, 13, 13, 51 11024384 darknet53[1][2]            &#xA;#&amp;gt; ________________________________________________________________________________&#xA;#&amp;gt; yolo3_conv2 (Model)       (None, 26, 26, 25 2957312  yolo3_conv1[1][0]          &#xA;#&amp;gt;                                                      darknet53[1][1]            &#xA;#&amp;gt; ________________________________________________________________________________&#xA;#&amp;gt; yolo3_conv3 (Model)       (None, 52, 52, 12 741376   yolo3_conv2[1][0]          &#xA;#&amp;gt;                                                      darknet53[1][0]            &#xA;#&amp;gt; ________________________________________________________________________________&#xA;#&amp;gt; grid1 (Model)             (None, 13, 13, 3, 4984063  yolo3_conv1[1][0]          &#xA;#&amp;gt; ________________________________________________________________________________&#xA;#&amp;gt; grid2 (Model)             (None, 26, 26, 3, 1312511  yolo3_conv2[1][0]          &#xA;#&amp;gt; ________________________________________________________________________________&#xA;#&amp;gt; grid3 (Model)             (None, 52, 52, 3, 361471   yolo3_conv3[1][0]          &#xA;#&amp;gt; ================================================================================&#xA;#&amp;gt; Total params: 62,001,757&#xA;#&amp;gt; Trainable params: 61,949,149&#xA;#&amp;gt; Non-trainable params: 52,608&#xA;#&amp;gt; ________________________________________________________________________________&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can now load &lt;a href=&#34;https://pjreddie.com/darknet/yolo/&#34;&gt;YOLOv3 Darknet&lt;/a&gt; weights trained on &lt;a href=&#34;https://cocodataset.org/#home&#34;&gt;COCO dataset&lt;/a&gt;. Download pre-trained weights from &lt;a href=&#34;https://pjreddie.com/media/files/yolov3.weights&#34;&gt;here&lt;/a&gt; and run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;test_yolo %&amp;gt;% load_darknet_weights(&#34;development/yolov3.weights&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Calculate predictions for new images:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;test_img_paths &amp;lt;- list.files(system.file(&#34;extdata&#34;, &#34;images&#34;, package = &#34;platypus&#34;), full.names = TRUE, pattern = &#34;coco&#34;)&#xA;test_imgs &amp;lt;- test_img_paths %&amp;gt;%&#xA;  map(~ {&#xA;    image_load(., target_size = c(416, 416), grayscale = FALSE) %&amp;gt;%&#xA;      image_to_array() %&amp;gt;%&#xA;      `/`(255)&#xA;  }) %&amp;gt;%&#xA;  abind(along = 4) %&amp;gt;%&#xA;  aperm(c(4, 1:3))&#xA;test_preds &amp;lt;- test_yolo %&amp;gt;% predict(test_imgs)&#xA;&#xA;str(test_preds)&#xA;#&amp;gt; List of 3&#xA;#&amp;gt;  $ : num [1:2, 1:13, 1:13, 1:3, 1:85] 0.294 0.478 0.371 1.459 0.421 ...&#xA;#&amp;gt;  $ : num [1:2, 1:26, 1:26, 1:3, 1:85] -0.214 1.093 -0.092 2.034 -0.286 ...&#xA;#&amp;gt;  $ : num [1:2, 1:52, 1:52, 1:3, 1:85] 0.242 -0.751 0.638 -2.419 -0.282 ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Transform raw predictions into bounding boxes:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;test_boxes &amp;lt;- get_boxes(&#xA;  preds = test_preds, # Raw predictions form YOLOv3 model&#xA;  anchors = coco_anchors, # Anchor boxes&#xA;  labels = coco_labels, # Class labels&#xA;  obj_threshold = 0.6, # Object threshold&#xA;  nms = TRUE, # Should non-max suppression be applied&#xA;  nms_threshold = 0.6, # Non-max suppression threshold&#xA;  correct_hw = FALSE # Should height and width of bounding boxes be corrected to image height and width&#xA;)&#xA;&#xA;test_boxes&#xA;#&amp;gt; [[1]]&#xA;#&amp;gt; # A tibble: 8 x 7&#xA;#&amp;gt;    xmin  ymin  xmax  ymax p_obj label_id label &#xA;#&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;    &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt; &#xA;#&amp;gt; 1 0.207 0.718 0.236 0.865 0.951        1 person&#xA;#&amp;gt; 2 0.812 0.758 0.846 0.868 0.959        1 person&#xA;#&amp;gt; 3 0.349 0.702 0.492 0.884 1.00         3 car   &#xA;#&amp;gt; 4 0.484 0.543 0.498 0.558 0.837        3 car   &#xA;#&amp;gt; 5 0.502 0.543 0.515 0.556 0.821        3 car   &#xA;#&amp;gt; 6 0.439 0.604 0.469 0.643 0.842        3 car   &#xA;#&amp;gt; 7 0.541 0.554 0.667 0.809 0.999        6 bus   &#xA;#&amp;gt; 8 0.534 0.570 0.675 0.819 0.954        7 train &#xA;#&amp;gt; &#xA;#&amp;gt; [[2]]&#xA;#&amp;gt; # A tibble: 3 x 7&#xA;#&amp;gt;     xmin   ymin  xmax  ymax p_obj label_id label&#xA;#&amp;gt;    &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;    &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;&#xA;#&amp;gt; 1 0.0236 0.0705 0.454 0.909 1.00        23 zebra&#xA;#&amp;gt; 2 0.290  0.206  0.729 0.901 0.997       23 zebra&#xA;#&amp;gt; 3 0.486  0.407  0.848 0.928 1.00        23 zebra&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Plot / save images:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot_boxes(&#xA;  images_paths = test_img_paths, # Images paths&#xA;  boxes = test_boxes, # Bounding boxes&#xA;  correct_hw = TRUE, # Should height and width of bounding boxes be corrected to image height and width&#xA;  labels = coco_labels # Class labels&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/maju116/platypus/master/man/figures/README-unnamed-chunk-8-1.png&#34; alt=&#34;&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/maju116/platypus/master/man/figures/README-unnamed-chunk-8-2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;YOLOv3 Object detection with custom dataset:&lt;/h2&gt; &#xA;&lt;p&gt;Download images and annotations: &lt;a href=&#34;https://www.kaggle.com/surajiiitm/bccd-dataset?&#34;&gt;BCCD dataset&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Generate custom anchor boxes:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tidyverse)&#xA;library(platypus)&#xA;library(abind)&#xA;&#xA;BCCD_path &amp;lt;- &#34;development/BCCD/&#34;&#xA;annot_path &amp;lt;- file.path(BCCD_path, &#34;Annotations/&#34;)&#xA;blood_labels &amp;lt;- c(&#34;Platelets&#34;, &#34;RBC&#34;, &#34;WBC&#34;)&#xA;n_class &amp;lt;- length(blood_labels)&#xA;net_h &amp;lt;- 416 # Must be divisible by 32&#xA;net_w &amp;lt;- 416 # Must be divisible by 32&#xA;anchors_per_grid &amp;lt;- 3&#xA;&#xA;blood_anchors &amp;lt;- generate_anchors(&#xA;  anchors_per_grid = anchors_per_grid, # Number of anchors (per one grid) to generate&#xA;  annot_path = annot_path, # Annotations directory&#xA;  labels = blood_labels, # Class labels&#xA;  n_iter = 10, # Number of k-means++ iterations&#xA;  annot_format = &#34;pascal_voc&#34;, # Annotations format&#xA;  seed = 55, # Random seed&#xA;  centroid_fun = mean # Centroid function&#xA;)&#xA;#&amp;gt;       label    n&#xA;#&amp;gt; 1 Platelets  361&#xA;#&amp;gt; 2       RBC 4153&#xA;#&amp;gt; 3       WBC  372&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/maju116/platypus/master/man/figures/README-unnamed-chunk-9-1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;blood_anchors&#xA;#&amp;gt; [[1]]&#xA;#&amp;gt; [[1]][[1]]&#xA;#&amp;gt; [1] 0.3552235 0.4417515&#xA;#&amp;gt; &#xA;#&amp;gt; [[1]][[2]]&#xA;#&amp;gt; [1] 0.2911290 0.3292675&#xA;#&amp;gt; &#xA;#&amp;gt; [[1]][[3]]&#xA;#&amp;gt; [1] 0.1971296 0.2346442&#xA;#&amp;gt; &#xA;#&amp;gt; &#xA;#&amp;gt; [[2]]&#xA;#&amp;gt; [[2]][[1]]&#xA;#&amp;gt; [1] 0.1757463 0.1592062&#xA;#&amp;gt; &#xA;#&amp;gt; [[2]][[2]]&#xA;#&amp;gt; [1] 0.1652637 0.2065506&#xA;#&amp;gt; &#xA;#&amp;gt; [[2]][[3]]&#xA;#&amp;gt; [1] 0.1630269 0.2439239&#xA;#&amp;gt; &#xA;#&amp;gt; &#xA;#&amp;gt; [[3]]&#xA;#&amp;gt; [[3]][[1]]&#xA;#&amp;gt; [1] 0.1391842 0.1769376&#xA;#&amp;gt; &#xA;#&amp;gt; [[3]][[2]]&#xA;#&amp;gt; [1] 0.1245985 0.2258089&#xA;#&amp;gt; &#xA;#&amp;gt; [[3]][[3]]&#xA;#&amp;gt; [1] 0.06237392 0.08062560&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Build &lt;code&gt;YOLOv3&lt;/code&gt; model and compile it with correct loss and metric:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;blood_yolo &amp;lt;- yolo3(&#xA;  net_h = net_h, # Input image height&#xA;  net_w = net_w, # Input image width&#xA;  grayscale = FALSE, # Should images be loaded as grayscale or RGB&#xA;  n_class = n_class, # Number of object classes (80 for COCO dataset)&#xA;  anchors = blood_anchors # Anchor boxes&#xA;)&#xA;blood_yolo %&amp;gt;% load_darknet_weights(&#34;development/yolov3.weights&#34;) # Optional&#xA;&#xA;blood_yolo %&amp;gt;% compile(&#xA;  optimizer = optimizer_adam(lr = 1e-5),&#xA;  loss = yolo3_loss(blood_anchors, n_class = n_class),&#xA;  metrics = yolo3_metrics(blood_anchors, n_class = n_class)&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Create data generators:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;train_blood_yolo_generator &amp;lt;- yolo3_generator(&#xA;  annot_path = file.path(BCCD_path, &#34;train&#34;, &#34;Annotations/&#34;),&#xA;  images_path = file.path(BCCD_path, &#34;train&#34;, &#34;JPEGImages/&#34;),&#xA;  net_h = net_h,&#xA;  net_w = net_w,&#xA;  batch_size = 16,&#xA;  shuffle = FALSE,&#xA;  labels = blood_labels&#xA;)&#xA;#&amp;gt; 291 images with corresponding annotations detected!&#xA;#&amp;gt; Set &#39;steps_per_epoch&#39; to: 19&#xA;&#xA;valid_blood_yolo_generator &amp;lt;- yolo3_generator(&#xA;  annot_path = file.path(BCCD_path, &#34;valid&#34;, &#34;Annotations/&#34;),&#xA;  images_path = file.path(BCCD_path, &#34;valid&#34;, &#34;JPEGImages/&#34;),&#xA;  net_h = net_h,&#xA;  net_w = net_w,&#xA;  batch_size = 16,&#xA;  shuffle = FALSE,&#xA;  labels = blood_labels&#xA;)&#xA;#&amp;gt; 69 images with corresponding annotations detected!&#xA;#&amp;gt; Set &#39;steps_per_epoch&#39; to: 5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Fit the model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;blood_yolo %&amp;gt;%&#xA;  fit_generator(&#xA;    generator = blood_yolo_generator,&#xA;    epochs = 1000,&#xA;    steps_per_epoch = 19,&#xA;    validation_data = valid_blood_yolo_generator,&#xA;    validation_steps = 5,&#xA;    callbacks = list(callback_model_checkpoint(&#34;development/BCCD/blood_w.hdf5&#34;,&#xA;                                               save_best_only = TRUE,&#xA;                                               save_weights_only = TRUE)&#xA;    )&#xA;  )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Predict on new images:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;blood_yolo &amp;lt;- yolo3(&#xA;  net_h = net_h,&#xA;  net_w = net_w,&#xA;  grayscale = FALSE,&#xA;  n_class = n_class,&#xA;  anchors = blood_anchors&#xA;)&#xA;blood_yolo %&amp;gt;% load_model_weights_hdf5(&#34;development/BCCD/blood_w.hdf5&#34;)&#xA;&#xA;test_blood_yolo_generator &amp;lt;- yolo3_generator(&#xA;  annot_path = file.path(BCCD_path, &#34;test&#34;, &#34;Annotations/&#34;),&#xA;  images_path = file.path(BCCD_path, &#34;test&#34;, &#34;JPEGImages/&#34;),&#xA;  net_h = net_h,&#xA;  net_w = net_w,&#xA;  batch_size = 4,&#xA;  shuffle = FALSE,&#xA;  labels = blood_labels&#xA;)&#xA;#&amp;gt; 4 images with corresponding annotations detected!&#xA;#&amp;gt; Set &#39;steps_per_epoch&#39; to: 1&#xA;&#xA;test_preds &amp;lt;- predict_generator(blood_yolo, test_blood_yolo_generator, 1)&#xA;&#xA;test_boxes &amp;lt;- get_boxes(test_preds, blood_anchors, blood_labels,&#xA;                        obj_threshold = 0.6)&#xA;&#xA;plot_boxes(&#xA;  images_paths = list.files(file.path(BCCD_path, &#34;test&#34;, &#34;JPEGImages/&#34;), full.names = TRUE),&#xA;  boxes = test_boxes,&#xA;  labels = blood_labels)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/maju116/platypus/master/man/figures/README-unnamed-chunk-13-1.png&#34; alt=&#34;&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/maju116/platypus/master/man/figures/README-unnamed-chunk-13-2.png&#34; alt=&#34;&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/maju116/platypus/master/man/figures/README-unnamed-chunk-13-3.png&#34; alt=&#34;&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/maju116/platypus/master/man/figures/README-unnamed-chunk-13-4.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;See full example &lt;a href=&#34;https://github.com/maju116/platypus/raw/master/examples/Blood%20Cell%20Detection/Blood-Cell-Detection.md&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;U-Net image segmentation with custom dataset:&lt;/h2&gt; &#xA;&lt;p&gt;Build &lt;code&gt;U-Net&lt;/code&gt; model and compile it with correct loss and metric:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tidyverse)&#xA;library(platypus)&#xA;library(abind)&#xA;&#xA;train_DCB2018_path &amp;lt;- &#34;development/data-science-bowl-2018/stage1_train&#34;&#xA;test_DCB2018_path &amp;lt;- &#34;development/data-science-bowl-2018/stage1_test&#34;&#xA;&#xA;blocks &amp;lt;- 4 # Number of U-Net convolutional blocks&#xA;n_class &amp;lt;- 2 # Number of classes&#xA;net_h &amp;lt;- 256 # Must be in a form of 2^N&#xA;net_w &amp;lt;- 256 # Must be in a form of 2^N&#xA;&#xA;DCB2018_u_net &amp;lt;- u_net(&#xA;  net_h = net_h,&#xA;  net_w = net_w,&#xA;  grayscale = FALSE,&#xA;  blocks = blocks,&#xA;  n_class = n_class,&#xA;  filters = 16,&#xA;  dropout = 0.1,&#xA;  batch_normalization = TRUE,&#xA;  kernel_initializer = &#34;he_normal&#34;&#xA;)&#xA;&#xA;DCB2018_u_net %&amp;gt;%&#xA;  compile(&#xA;    optimizer = optimizer_adam(lr = 1e-3),&#xA;    loss = loss_dice(),&#xA;    metrics = metric_dice_coeff()&#xA;  )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Create data generator:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;train_DCB2018_generator &amp;lt;- segmentation_generator(&#xA;  path = train_DCB2018_path, # directory with images and masks&#xA;  mode = &#34;nested_dirs&#34;, # Each image with masks in separate folder&#xA;  colormap = binary_colormap,&#xA;  only_images = FALSE,&#xA;  net_h = net_h,&#xA;  net_w = net_w,&#xA;  grayscale = FALSE,&#xA;  scale = 1 / 255,&#xA;  batch_size = 32,&#xA;  shuffle = TRUE,&#xA;  subdirs = c(&#34;images&#34;, &#34;masks&#34;) # Names of subdirs with images and masks&#xA;)&#xA;#&amp;gt; 670 images with corresponding masks detected!&#xA;#&amp;gt; Set &#39;steps_per_epoch&#39; to: 21&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Fit the model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;history &amp;lt;- DCB2018_u_net %&amp;gt;%&#xA;  fit_generator(&#xA;    train_DCB2018_generator,&#xA;    epochs = 20,&#xA;    steps_per_epoch = 21,&#xA;    callbacks = list(callback_model_checkpoint(&#xA;      &#34;development/data-science-bowl-2018/DSB2018_w.hdf5&#34;,&#xA;      save_best_only = TRUE,&#xA;      save_weights_only = TRUE,&#xA;      monitor = &#34;dice_coeff&#34;,&#xA;      mode = &#34;max&#34;,&#xA;      verbose = 1)&#xA;    )&#xA;  )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Predict on new images:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;DCB2018_u_net &amp;lt;- u_net(&#xA;  net_h = net_h,&#xA;  net_w = net_w,&#xA;  grayscale = FALSE,&#xA;  blocks = blocks,&#xA;  filters = 16,&#xA;  dropout = 0.1,&#xA;  batch_normalization = TRUE,&#xA;  kernel_initializer = &#34;he_normal&#34;&#xA;)&#xA;DCB2018_u_net %&amp;gt;% load_model_weights_hdf5(&#34;development/data-science-bowl-2018/DSB2018_w.hdf5&#34;)&#xA;&#xA;test_DCB2018_generator &amp;lt;- segmentation_generator(&#xA;  path = test_DCB2018_path,&#xA;  mode = &#34;nested_dirs&#34;,&#xA;  colormap = binary_colormap,&#xA;  only_images = TRUE,&#xA;  net_h = net_h,&#xA;  net_w = net_w,&#xA;  grayscale = FALSE,&#xA;  scale = 1 / 255,&#xA;  batch_size = 32,&#xA;  shuffle = FALSE,&#xA;  subdirs = c(&#34;images&#34;, &#34;masks&#34;)&#xA;)&#xA;#&amp;gt; 65 images detected!&#xA;#&amp;gt; Set &#39;steps_per_epoch&#39; to: 3&#xA;&#xA;test_preds &amp;lt;- predict_generator(DCB2018_u_net, test_DCB2018_generator, 3)&#xA;&#xA;test_masks &amp;lt;- get_masks(test_preds, binary_colormap)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Plot / save images with masks:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;test_imgs_paths &amp;lt;- create_images_masks_paths(test_DCB2018_path, &#34;nested_dirs&#34;, FALSE, c(&#34;images&#34;, &#34;masks&#34;), &#34;;&#34;)$images_paths&#xA;&#xA;plot_masks(&#xA;  images_paths = test_imgs_paths[1:4],&#xA;  masks = test_masks[1:4],&#xA;  labels = c(&#34;background&#34;, &#34;nuclei&#34;),&#xA;  colormap = binary_colormap&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/maju116/platypus/master/man/figures/README-unnamed-chunk-18-1.png&#34; alt=&#34;&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/maju116/platypus/master/man/figures/README-unnamed-chunk-18-2.png&#34; alt=&#34;&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/maju116/platypus/master/man/figures/README-unnamed-chunk-18-3.png&#34; alt=&#34;&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/maju116/platypus/master/man/figures/README-unnamed-chunk-18-4.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;See full example &lt;a href=&#34;https://github.com/maju116/platypus/raw/master/examples/2018%20Data%20Science%20Bowl/2018-Data-Science-Bowl.md&#34;&gt;here&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ohdsi-studies/HowOften</title>
    <updated>2023-10-23T01:38:26Z</updated>
    <id>tag:github.com,2023-10-23:/ohdsi-studies/HowOften</id>
    <link href="https://github.com/ohdsi-studies/HowOften" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[under development] Large-scale incidence characterization&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;How Often: An Incidence Analysis for a series of OHDSI Community Submissions&lt;/h2&gt; &#xA;&lt;img src=&#34;https://img.shields.io/badge/Study%20Status-Design%20Finalized-brightgreen.svg?sanitize=true&#34; alt=&#34;Study Status: Design Finalized&#34;&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Analytics use case(s): &lt;strong&gt;Characterization&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Study type: &lt;strong&gt;Clinical Application&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Tags: &lt;strong&gt;Incidence&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Study lead: &lt;strong&gt;George Hripcsak&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Study lead forums tag: &lt;strong&gt;&lt;a href=&#34;https://forums.ohdsi.org/u/hripcsa&#34;&gt;hripcsa&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Study start date: &lt;strong&gt;August 2023&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Study end date: &lt;strong&gt;-&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Protocol: &lt;a href=&#34;https://github.com/ohdsi-studies/HowOften/raw/master/Documents/HowOften%20protocol%20v1.0.pdf&#34;&gt;HowOften Study Protocol&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Publications: &lt;strong&gt;None&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Results explorer: &lt;strong&gt;&lt;a href=&#34;https://results.ohdsi.org/&#34;&gt;ShinyApp Explorer&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;How Often is a Large-Scale Characterization analysis to compute incidence for a broad collection of target populations and outcomes across the OHDSI network. Incidence analyses can be framed as: &#34;Amongst patients who are &lt;em&gt;insert your favorite target cohort i&lt;/em&gt;, how many patients experienced &lt;em&gt;insert your favorite outcome j&lt;/em&gt; within &lt;em&gt;time horizon relative to target start&lt;/em&gt;?&#34;, and HowOften aims to systematically apply this analysis to range of target cohorts, outcome cohorts, and time horizons, to address an array of clinical questions that incidence evidence can inform.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;There are 3 parts to executing the study: Pre-Configuration (to get base R libraries set up), Keyring Setup and finally Analysis Execution.&lt;/p&gt; &#xA;&lt;h3&gt;Pre-Configuration&lt;/h3&gt; &#xA;&lt;p&gt;HowOften requires:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;R v4.2 (Preferably 4.2.3)&lt;/li&gt; &#xA; &lt;li&gt;DatabaseConnector &amp;gt;= 6.2.3&lt;/li&gt; &#xA; &lt;li&gt;Strategus v0.1.0&lt;/li&gt; &#xA; &lt;li&gt;Cohortgenerator v0.8.1&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;DatabaseConnector may have challenges to updating if already installed as a Package. Either update the package in a R CLI (outside of RStudio) or use &lt;code&gt;unloadNamespace()&lt;/code&gt; to remove DatabaseConnector from memory. Execute the following to install these packages:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#xA;install.packages(&#34;DatabaseConnector&#34;)&#xA;remotes::install_github(&#34;OHDSI/Strategus&#34;, ref=&#34;v0.1.0&#34;)&#xA;remotes::install_github(&#34;OHDSI/CohortGenerator&#34;, ref=&#34;v0.8.1&#34;)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Keyring Setup&lt;/h3&gt; &#xA;&lt;p&gt;This repository provides a &lt;a href=&#34;https://raw.githubusercontent.com/ohdsi-studies/HowOften/master/keyringSetup.R&#34;&gt;keyringSetup.R&lt;/a&gt; script that provides initialization scripts to set up your R environment and register connection details as &lt;code&gt;connection refs&lt;/code&gt; for use in Strategus.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Part 1&lt;/strong&gt; is to ensure your environment has 2 environment variables:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;STRATEGUS_KEYRING_PASSWORD: used to unlock your Strategus keyring.&lt;/li&gt; &#xA; &lt;li&gt;INSTANTIATED_MODULES_FOLDER: a shared folder location that is used to download and cache Strategus Modules.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;install.packages(&#34;keyring&#34;)&#xA;&#xA;if (Sys.getenv(&#34;STRATEGUS_KEYRING_PASSWORD&#34;) == &#34;&#34;) {&#xA;  # set keyring password by adding STRATEGUS_KEYRING_PASSWORD=&#39;sos&#39; to renviron&#xA;  usethis::edit_r_environ()&#xA;  # then add STRATEGUS_KEYRING_PASSWORD=&#39;yourPassword&#39;, save and close&#xA;  # Restart your R Session to confirm it worked&#xA;  stop(&#34;Please add STRATEGUS_KEYRING_PASSWORD=&#39;yourPassword&#39; to your .Renviron file&#xA;       via usethis::edit_r_environ() as instructed, save and then restart R session&#34;)&#xA;}&#xA;&#xA;if (Sys.getenv(&#34;INSTANTIATED_MODULES_FOLDER&#34;) == &#34;&#34;) {&#xA;  # set a env var to a path to cache Strategus modules&#xA;  usethis::edit_r_environ()&#xA;  # then add INSTANTIATED_MODULES_FOLDER=&#39;path/to/module/cache&#39;, save and close&#xA;  # Restart your R Session to confirm it worked&#xA;  stop(&#34;Please add INSTANTIATED_MODULES_FOLDER=&#39;{path to module cache folder}&#39; to your .Renviron file&#xA;       via usethis::edit_r_environ() as instructed, save and then restart R session&#34;)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Part 2&lt;/strong&gt; is where you will instantiate your connection details in memory, and test connectivity:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Provide your environment specific values ------&#xA;connectionDetails &amp;lt;- NULL # fetch/create your own connection details here&#xA;connectionDetailsReference &amp;lt;- &#34;mYDatasourceKey&#34; # short abbreviation that describes these connection details&#xA;&#xA;# test the connection&#xA;conn &amp;lt;- DatabaseConnector::connect(connectionDetails)&#xA;DatabaseConnector::disconnect(conn)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In the above, you will assign connectionDetails in your r environment through your own script.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Please change the value of &lt;code&gt;myDatasourceKey&lt;/code&gt; to a short, meaningful label for this cdm source.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Part 3&lt;/strong&gt; is to create your Keyring if it does not exist:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Create the keyring if it does not exist.&#xA;allKeyrings &amp;lt;- keyring::keyring_list()&#xA;if (!(keyringName %in% allKeyrings$keyring)) {&#xA;  keyring::keyring_create(keyring = keyringName, password = Sys.getenv(&#34;STRATEGUS_KEYRING_PASSWORD&#34;))&#xA;} else {&#xA;  stop(&#34;Keyring already exists. You do not need to create it again.&#34;)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Part 4&lt;/strong&gt; will store your connection details into your keyring:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# excecute this for each connectionDetails/ConnectionDetailsReference you are going to use&#xA;Strategus::storeConnectionDetails(&#xA;  connectionDetails = connectionDetails,&#xA;  connectionDetailsReference = connectionDetailsReference,&#xA;  keyringName = keyringName&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Executing the Analysis&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/ohdsi-studies/HowOften/master/StrategusCodeToRun.R&#34;&gt;StrategusCodeToRun.R&lt;/a&gt; contains the script that will perform the execution of the 7 individual analyes in HowOften:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Part 1&lt;/strong&gt; sets up variables that will be used as input to execution:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;##=========== START OF INPUTS ==========&#xA;keyringName &amp;lt;- &#34;HowOften&#34;&#xA;connectionDetailsReference &amp;lt;- &#34;mYDatasourceKey&#34;&#xA;workDatabaseSchema &amp;lt;- &#39;writable_schema&#39;&#xA;cdmDatabaseSchema &amp;lt;- &#39;cdm_schema&#39;&#xA;outputLocation &amp;lt;- &#39;{path/to/Strategus/Output}&#39;&#xA;resultsLocation &amp;lt;- &#39;{path/to/Strategus/Results}&#39;&#xA;minCellCount &amp;lt;- 5 # set this to a value where you want to censor small cells&#xA;cohortTableName &amp;lt;- &#34;howoften_cohort&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: the outputLocation will be reused between analysis exeuctions to cache cohort generation info. Each analysis execution will copy from the &lt;code&gt;outputLocation&lt;/code&gt; to the &lt;code&gt;resultsLocation&lt;/code&gt; under the directory dedicated to the individual studies. The &lt;code&gt;resultsLocation&lt;/code&gt; folder will be zipped and submitted for inclusion in the ShinyApp viewer.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Please change the value of &lt;code&gt;myDatasourceKey&lt;/code&gt; to a short, meaningful label for this cdm source. This must be the same value that was use dwhen executing SetupKeyring.R&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Part 2&lt;/strong&gt; sets up execution settings and creates the helper function to execute the analysis and copy results to the result folder:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;##################################&#xA;# DO NOT MODIFY BELOW THIS POINT&#xA;##################################&#xA;&#xA;executionSettings &amp;lt;- Strategus::createCdmExecutionSettings(&#xA;  connectionDetailsReference = connectionDetailsReference,&#xA;  workDatabaseSchema = workDatabaseSchema,&#xA;  cdmDatabaseSchema = cdmDatabaseSchema,&#xA;  cohortTableNames = CohortGenerator::getCohortTableNames(cohortTable = cohortTableName),&#xA;  workFolder = file.path(outputLocation, connectionDetailsReference, &#34;strategusWork&#34;),&#xA;  resultsFolder = file.path(outputLocation, connectionDetailsReference, &#34;strategusOutput&#34;),&#xA;  minCellCount = minCellCount&#xA;)&#xA;&#xA;executeAnalysis &amp;lt;- function(analysisFile, executionSettings, analysisName, outputLocation, resultsLocation, keyringName) {&#xA;&#xA;  analysisSpecifications &amp;lt;- ParallelLogger::loadSettingsFromJson(&#xA;    fileName = analysisFile&#xA;  )&#xA;&#xA;  Strategus::execute(&#xA;    analysisSpecifications = analysisSpecifications,&#xA;    executionSettings = executionSettings,&#xA;    executionScriptFolder = file.path(outputLocation, connectionDetailsReference, &#34;strategusExecution&#34;),&#xA;    keyringName = keyringName&#xA;  )&#xA;&#xA;  # copy Results to final location&#xA;  resultsDir &amp;lt;- file.path(resultsLocation, analysisName, connectionDetailsReference)&#xA;&#xA;  if (dir.exists(resultsDir)) {&#xA;    unlink(resultsDir, recursive = TRUE)&#xA;  }&#xA;  dir.create(file.path(resultsDir), recursive = TRUE)&#xA;  file.copy(file.path(outputLocation, connectionDetailsReference, &#34;strategusOutput&#34;),&#xA;            file.path(resultsDir), recursive = TRUE)&#xA;&#xA;  return(NULL)&#xA;&#xA;}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Part 3&lt;/strong&gt; executes the individual HowOften analyses:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Step 1 : Execute Azza Analysis&#xA;executeAnalysis(&#34;howoften_azza.json&#34;, executionSettings, &#34;azza&#34;, outputLocation, resultsLocation, keyringName)&#xA;&#xA;# Step 2 : Execute Andreas Analysis&#xA;executeAnalysis(&#34;howoften_andreas.json&#34;, executionSettings, &#34;andreas&#34;, outputLocation, resultsLocation, keyringName)&#xA;&#xA;# Step 3, Joel Analysis&#xA;executeAnalysis(&#34;howoften_joel.json&#34;, executionSettings, &#34;joel&#34;, outputLocation, resultsLocation, keyringName)&#xA;&#xA;# step 4, Evan analysis&#xA;executeAnalysis(&#34;howoften_evan.json&#34;, executionSettings, &#34;evan&#34;, outputLocation, resultsLocation, keyringName)&#xA;&#xA;# step 5, gowza analysis&#xA;executeAnalysis(&#34;howoften_gowza.json&#34;, executionSettings, &#34;gowza&#34;, outputLocation, resultsLocation, keyringName)&#xA;&#xA;# step 6, overall analysis&#xA;executeAnalysis(&#34;howoften_overall.json&#34;, executionSettings, &#34;overall&#34;, outputLocation, resultsLocation, keyringName)&#xA;&#xA;# step 7, george analysis&#xA;executeAnalysis(&#34;howoften_george.json&#34;, executionSettings, &#34;george&#34;, outputLocation, resultsLocation, keyringName)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Submitting Results&lt;/h3&gt; &#xA;&lt;p&gt;Once the analyses executions are complete (not all are required to be completed, some analyses are very large and may not be completed in time for the OHDSI Symposium), the Results folder is zipped and submitted to an FTP location for processing. Results that are properlly submitted and formatted will be uploaded to OHDSI servers and will be available on &lt;a href=&#34;https://results.ohdsi.org/&#34;&gt;results.ohdsi.org&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>5vxssbdahves/General-R-Scripts</title>
    <updated>2023-10-23T01:38:26Z</updated>
    <id>tag:github.com,2023-10-23:/5vxssbdahves/General-R-Scripts</id>
    <link href="https://github.com/5vxssbdahves/General-R-Scripts" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;</summary>
  </entry>
</feed>