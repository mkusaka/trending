<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub R Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-06-02T02:21:48Z</updated>
  <subtitle>Weekly Trending of R in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>jokergoo/ComplexHeatmap</title>
    <updated>2022-06-02T02:21:48Z</updated>
    <id>tag:github.com,2022-06-02:/jokergoo/ComplexHeatmap</id>
    <link href="https://github.com/jokergoo/ComplexHeatmap" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Make Complex Heatmaps&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Make Complex Heatmaps &lt;a href=&#34;https://jokergoo.github.io/ComplexHeatmap-reference/book/&#34;&gt;&lt;img src=&#34;https://jokergoo.github.io/ComplexHeatmap-reference/book/complexheatmap-cover.jpg&#34; width=&#34;240&#34; align=&#34;right&#34; style=&#34;border:2px solid black;&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/jokergoo/ComplexHeatmap/actions&#34;&gt;&lt;img src=&#34;https://github.com/jokergoo/ComplexHeatmap/workflows/R-CMD-check/badge.svg?sanitize=true&#34; alt=&#34;R-CMD-check&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/github/jokergoo/ComplexHeatmap&#34;&gt;&lt;img src=&#34;https://img.shields.io/codecov/c/github/jokergoo/ComplexHeatmap.svg?sanitize=true&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://bioconductor.org/packages/stats/bioc/ComplexHeatmap/&#34;&gt;&lt;img src=&#34;http://www.bioconductor.org/shields/downloads/devel/ComplexHeatmap.svg?sanitize=true&#34; alt=&#34;bioc&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://bioconductor.org/packages/devel/bioc/html/ComplexHeatmap.html&#34;&gt;&lt;img src=&#34;http://www.bioconductor.org/shields/years-in-bioc/ComplexHeatmap.svg?sanitize=true&#34; alt=&#34;bioc&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;img src=&#34;http://jokergoo.github.io/complexheatmap_logo.svg?sanitize=true&#34; width=&#34;550&#34;&gt; &#xA;&lt;p&gt;Complex heatmaps are efficient to visualize associations between different sources of data sets and reveal potential patterns. Here the &lt;strong&gt;ComplexHeatmap&lt;/strong&gt; package provides a highly flexible way to arrange multiple heatmaps and supports various annotation graphics.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://github.com/jokergoo/InteractiveComplexHeatmap&#34;&gt;&lt;strong&gt;InteractiveComplexHeatmap&lt;/strong&gt;&lt;/a&gt; package can directly export static complex heatmaps into an interactive Shiny app. Have a try!&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Zuguang Gu, et al., &lt;a href=&#34;http://bioinformatics.oxfordjournals.org/content/early/2016/05/20/bioinformatics.btw313.abstract&#34;&gt;Complex heatmaps reveal patterns and correlations in multidimensional genomic data&lt;/a&gt;, Bioinformatics, 2016&lt;/p&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;ComplexHeatmap&lt;/code&gt; is available on &lt;a href=&#34;http://www.bioconductor.org/packages/devel/bioc/html/ComplexHeatmap.html&#34;&gt;Bioconductor&lt;/a&gt;, you can install it by:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;if (!requireNamespace(&#34;BiocManager&#34;, quietly=TRUE))&#xA;    install.packages(&#34;BiocManager&#34;)&#xA;BiocManager::install(&#34;ComplexHeatmap&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want the latest version, install it directly from GitHub:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(devtools)&#xA;install_github(&#34;jokergoo/ComplexHeatmap&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Make a single heatmap:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Heatmap(mat, ...)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A single Heatmap with column annotations:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ha = HeatmapAnnotation(df = anno1, anno_fun = anno2, ...)&#xA;Heatmap(mat, ..., top_annotation = ha)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Make a list of heatmaps:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Heatmap(mat1, ...) + Heatmap(mat2, ...)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Make a list of heatmaps and row annotations:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ha = HeatmapAnnotation(df = anno1, anno_fun = anno2, ..., which = &#34;row&#34;)&#xA;Heatmap(mat1, ...) + Heatmap(mat2, ...) + ha&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;The full documentations are available at &lt;a href=&#34;https://jokergoo.github.io/ComplexHeatmap-reference/book/&#34;&gt;https://jokergoo.github.io/ComplexHeatmap-reference/book/&lt;/a&gt; and the website is at &lt;a href=&#34;https://jokergoo.github.io/ComplexHeatmap&#34;&gt;https://jokergoo.github.io/ComplexHeatmap&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Blog posts&lt;/h2&gt; &#xA;&lt;p&gt;There are following blog posts focusing on specific topics:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jokergoo.github.io/2021/03/24/3d-heatmap/&#34;&gt;Make 3D heatmap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jokergoo.github.io/2020/05/06/translate-from-pheatmap-to-complexheatmap/&#34;&gt;Translate from pheatmap to ComplexHeatmap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jokergoo.github.io/2020/05/11/set-cell-width/height-in-the-heatmap/&#34;&gt;Set cell width/height in the heatmap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jokergoo.github.io/2020/05/15/interactive-complexheatmap/&#34;&gt;Interactive ComplexHeatmap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jokergoo.github.io/2020/05/31/word-cloud-as-heatmap-annotation/&#34;&gt;Word cloud as heatmap annotation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jokergoo.github.io/2020/06/19/which-heatmap-function-is-faster/&#34;&gt;Which heatmap function is faster?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jokergoo.github.io/2020/06/30/rasterization-in-complexheatmap/&#34;&gt;Rasterization in ComplexHeatmap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jokergoo.github.io/2020/07/06/block-annotation-over-several-slices/&#34;&gt;Block annotation over several slices&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jokergoo.github.io/2020/07/14/integrate-complexheatmap-with-cowplot-package/&#34;&gt;Integrate ComplexHeatmap with cowplot package&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;h3&gt;Visualize Methylation Profile with Complex Annotations&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/449218/47718635-2ec22980-dc49-11e8-9f01-37becb19e0d5.png&#34; alt=&#34;complexheatmap_example4&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Correlations between methylation, expression and other genomic features&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/449218/47718636-2ec22980-dc49-11e8-8db0-1659c27dcf40.png&#34; alt=&#34;complexheatmap_example3&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Visualize Cell Heterogeneity from Single Cell RNASeq&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/449218/47718637-2ec22980-dc49-11e8-925e-955c16cfa982.png&#34; alt=&#34;complexheatmap_example2&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Making Enhanced OncoPrint&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/449218/47718638-2ec22980-dc49-11e8-845e-21e51d3b8e73.png&#34; alt=&#34;complexheatmap_example1&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;UpSet plot&lt;/h3&gt; &#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/449218/102615477-48c76a80-4136-11eb-98d9-3c528844fbe8.png&#34; width=&#34;500&#34;&gt; &#xA;&lt;h3&gt;3D heatmap&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/449218/112284448-8c77c600-8c89-11eb-8d38-c5538900df20.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;MIT @ Zuguang Gu&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>rmcelreath/rethinking</title>
    <updated>2022-06-02T02:21:48Z</updated>
    <id>tag:github.com,2022-06-02:/rmcelreath/rethinking</id>
    <link href="https://github.com/rmcelreath/rethinking" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Statistical Rethinking course and book package&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;rethinking&lt;/h1&gt; &#xA;&lt;p&gt;This R package accompanies a course and book on Bayesian data analysis: McElreath 2020. Statistical Rethinking, 2nd edition, CRC Press. If you are using it with the first edition of the book, please see the notes at the bottom of this file.&lt;/p&gt; &#xA;&lt;p&gt;It contains tools for conducting both quick quadratic approximation of the posterior distribution as well as Hamiltonian Monte Carlo (through RStan or cmdstanr - mc-stan.org). Many packages do this. The signature difference of this package is that it forces the user to specify the model as a list of explicit distributional assumptions. This is more tedious than typical formula-based tools, but it is also much more flexible and powerful and---most important---useful for teaching and learning. When students have to write out every detail of the model, they actually learn the model.&lt;/p&gt; &#xA;&lt;p&gt;For example, a simple Gaussian model could be specified with this list of formulas:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;f &amp;lt;- alist(&#xA;    y ~ dnorm( mu , sigma ),&#xA;    mu ~ dnorm( 0 , 10 ),&#xA;    sigma ~ dexp( 1 )&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The first formula in the list is the probability of the outcome (likelihood); the second is the prior for &lt;code&gt;mu&lt;/code&gt;; the third is the prior for &lt;code&gt;sigma&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;p&gt;There are three steps. (1) Install &lt;code&gt;rstan&lt;/code&gt;, (2) install &lt;code&gt;cmdstanr&lt;/code&gt;, (3) install &lt;code&gt;rethinking&lt;/code&gt;. Details follow.&lt;/p&gt; &#xA;&lt;p&gt;First, install the C++ toolchain and install the &lt;code&gt;rstan&lt;/code&gt; package. Go to &lt;code&gt;https://mc-stan.org/users/interfaces/rstan.html&lt;/code&gt; and follow the instructions for your platform. The biggest challenge is getting a C++ compiler configured to work with your installation of R. The instructions are quite thorough. Obey them, and you&#39;ll succeed.&lt;/p&gt; &#xA;&lt;p&gt;Second, install the &lt;code&gt;cmdstanr&lt;/code&gt; package. Visit &lt;code&gt;https://mc-stan.org/cmdstanr/&lt;/code&gt;. The first time you install cmdstanr, you will also need compile the libraries with &lt;code&gt;cmdstanr::install_cmdstan()&lt;/code&gt;. All this of this bother is worth it. You just have to do it once.&lt;/p&gt; &#xA;&lt;p&gt;Third, once rstan and cmdstanr are installed (almost there), then you can install &lt;code&gt;rethinking&lt;/code&gt; from within R using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;install.packages(c(&#34;coda&#34;,&#34;mvtnorm&#34;,&#34;devtools&#34;,&#34;loo&#34;,&#34;dagitty&#34;,&#34;shape&#34;))&#xA;devtools::install_github(&#34;rmcelreath/rethinking&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If there are any problems, they likely arise when trying to install &lt;code&gt;rstan&lt;/code&gt;, so the &lt;code&gt;rethinking&lt;/code&gt; package has little to do with it. See the manual linked above for some hints about getting &lt;code&gt;rstan&lt;/code&gt; installed. But always consult the RStan section of the website at &lt;code&gt;mc-stan.org&lt;/code&gt; for the latest information on RStan.&lt;/p&gt; &#xA;&lt;p&gt;Note that the &lt;code&gt;rethinking&lt;/code&gt; package is not on CRAN, just on github. The &lt;code&gt;rethinking&lt;/code&gt; package is never going to be on CRAN.&lt;/p&gt; &#xA;&lt;h1&gt;rethinking slim - no MCMC&lt;/h1&gt; &#xA;&lt;p&gt;If you just want to work through the first half of the course, without bothering with MCMC and Stan installs, you can install the &#39;slim&#39; version of the rethinking package. Do this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;install.packages(c(&#34;coda&#34;,&#34;mvtnorm&#34;,&#34;devtools&#34;,&#34;loo&#34;,&#34;dagitty&#34;))&#xA;devtools::install_github(&#34;rmcelreath/rethinking@slim&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;quap&lt;/code&gt; function and related helper functions should still work, and you&#39;ll be able to work through Chapter 8 before you need to install the full version with Stan.&lt;/p&gt; &#xA;&lt;h1&gt;Quadratic Approximation with &lt;code&gt;quap&lt;/code&gt;&lt;/h1&gt; &#xA;&lt;p&gt;Almost any ordinary generalized linear model can be specified with &lt;code&gt;quap&lt;/code&gt;. To use quadratic approximation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;library(rethinking)&#xA;&#xA;f &amp;lt;- alist(&#xA;    y ~ dnorm( mu , sigma ),&#xA;    mu ~ dnorm( 0 , 10 ),&#xA;    sigma ~ dexp( 1 )&#xA;)&#xA;&#xA;fit &amp;lt;- quap( &#xA;    f , &#xA;    data=list(y=c(-1,1)) , &#xA;    start=list(mu=0,sigma=1)&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The object &lt;code&gt;fit&lt;/code&gt; holds the result. For a summary of marginal posterior distributions, use &lt;code&gt;summary(fit)&lt;/code&gt; or &lt;code&gt;precis(fit)&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;      mean   sd  5.5% 94.5%&#xA;mu    0.00 0.59 -0.95  0.95&#xA;sigma 0.84 0.33  0.31  1.36&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It also supports vectorized parameters, which is convenient for categories. See examples &lt;code&gt;?quap&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In the first edition of the textbook, this function was called &lt;code&gt;map&lt;/code&gt;. It can still be used with that alias. It was renamed, because the name &lt;code&gt;map&lt;/code&gt; was misleading. This function produces quadratic approximations of the posterior distribution, not just maximum a posteriori (MAP) estimates.&lt;/p&gt; &#xA;&lt;h1&gt;Hamiltonian Monte Carlo with &lt;code&gt;ulam&lt;/code&gt; (and &lt;code&gt;map2stan&lt;/code&gt;)&lt;/h1&gt; &#xA;&lt;p&gt;The same formula list can be compiled into a Stan (mc-stan.org) model using one of two tools: &lt;code&gt;ulam&lt;/code&gt; or &lt;code&gt;map2stan&lt;/code&gt;. For simple models, they are identical. &lt;code&gt;ulam&lt;/code&gt; is the newer tool that allows for much more flexibility, including explicit variable types and custom distributions. &lt;code&gt;map2stan&lt;/code&gt; is the original tool from the first edition of the package and textbook. Going forward, new features will be added to &lt;code&gt;ulam&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;ulam&lt;/code&gt; is named after Stanisław Ulam, who was one of the parents of the Monte Carlo method and is the namesake of the Stan project as well. It is pronounced something like [OO-lahm], not like [YOU-lamm].&lt;/p&gt; &#xA;&lt;p&gt;Both tools take the same kind of input as &lt;code&gt;quap&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;fit_stan &amp;lt;- ulam( f , data=list(y=c(-1,1)) )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The chain runs automatically, provided &lt;code&gt;rstan&lt;/code&gt; is installed. Chain diagnostics are displayed in the &lt;code&gt;precis(fit_stan)&lt;/code&gt; output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;      mean   sd  5.5% 94.5% n_eff Rhat&#xA;sigma 1.45 0.72  0.67  2.84   145    1&#xA;mu    0.12 1.04 -1.46  1.59   163    1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For &lt;code&gt;ulam&lt;/code&gt; models, &lt;code&gt;plot&lt;/code&gt; displays the same information as &lt;code&gt;precis&lt;/code&gt; and &lt;code&gt;traceplot&lt;/code&gt; displays the chains.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;extract.samples&lt;/code&gt; returns samples in a list. &lt;code&gt;extract.prior&lt;/code&gt; samples from the prior and returns the samples in a list as well.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;stanfit&lt;/code&gt; object itself is in the &lt;code&gt;@stanfit&lt;/code&gt; slot. Anything you&#39;d do with a Stan model can be done with that slot directly.&lt;/p&gt; &#xA;&lt;p&gt;The Stan code can be accessed by using &lt;code&gt;stancode(fit_stan)&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;data{&#xA;    real y[2];&#xA;}&#xA;parameters{&#xA;    real&amp;lt;lower=0&amp;gt; sigma;&#xA;    real mu;&#xA;}&#xA;model{&#xA;    sigma ~ exponential( 1 );&#xA;    mu ~ normal( 0 , 10 );&#xA;    y ~ normal( mu , sigma );&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that &lt;code&gt;ulam&lt;/code&gt; doesn&#39;t care about R distribution names. You can instead use Stan-style names:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;fit_stan &amp;lt;- ulam(&#xA;    alist(&#xA;        y ~ normal( mu , sigma ),&#xA;        mu ~ normal( 0 , 10 ),&#xA;        sigma ~ exponential( 1 )&#xA;    ), data=list(y=c(-1,1)) )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Posterior prediction&lt;/h2&gt; &#xA;&lt;p&gt;All &lt;code&gt;quap&lt;/code&gt;, &lt;code&gt;ulam&lt;/code&gt;, and &lt;code&gt;map2stan&lt;/code&gt; objects can be post-processed to produce posterior predictive distributions.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;link&lt;/code&gt; is used to compute values of any linear models over samples from the posterior distribution.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;sim&lt;/code&gt; is used to simulate posterior predictive distributions, simulating outcomes over samples from the posterior distribution of parameters. &lt;code&gt;sim&lt;/code&gt; can also be used to simulate prior predictives.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;code&gt;?link&lt;/code&gt; and &lt;code&gt;?sim&lt;/code&gt; for details.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;postcheck&lt;/code&gt; automatically computes posterior predictive (retrodictive?) checks. It merely uses &lt;code&gt;link&lt;/code&gt; and &lt;code&gt;sim&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Multilevel model formulas&lt;/h2&gt; &#xA;&lt;p&gt;While &lt;code&gt;quap&lt;/code&gt; is limited to fixed effects models for the most part, &lt;code&gt;ulam&lt;/code&gt; can specify multilevel models, even quite complex ones. For example, a simple varying intercepts model looks like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# prep data&#xA;data( UCBadmit )&#xA;UCBadmit$male &amp;lt;- as.integer(UCBadmit$applicant.gender==&#34;male&#34;)&#xA;UCBadmit$dept &amp;lt;- rep( 1:6 , each=2 )&#xA;UCBadmit$applicant.gender &amp;lt;- NULL&#xA;&#xA;# varying intercepts model&#xA;m_glmm1 &amp;lt;- ulam(&#xA;    alist(&#xA;        admit ~ binomial(applications,p),&#xA;        logit(p) &amp;lt;- a[dept] + b*male,&#xA;        a[dept] ~ normal( abar , sigma ),&#xA;        abar ~ normal( 0 , 4 ),&#xA;        sigma ~ half_normal(0,1),&#xA;        b ~ normal(0,1)&#xA;    ), data=UCBadmit )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The analogous varying slopes model is:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;m_glmm2 &amp;lt;- ulam(&#xA;    alist(&#xA;        admit ~ binomial(applications,p),&#xA;        logit(p) &amp;lt;- a[dept] + b[dept]*male,&#xA;        c( a , b )[dept] ~ multi_normal( c(abar,bbar) , Rho , sigma ),&#xA;        abar ~ normal( 0 , 4 ),&#xA;        bbar ~ normal(0,1),&#xA;        sigma ~ half_normal(0,1),&#xA;        Rho ~ lkjcorr(2)&#xA;    ),&#xA;    data=UCBadmit )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Another way to express the varying slopes model is with a vector of varying effects. This is made possible by using an explicit vector declaration inside the formula:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;m_glmm3 &amp;lt;- ulam(&#xA;    alist(&#xA;        admit ~ binomial(applications,p),&#xA;        logit(p) &amp;lt;- v[dept,1] + v[dept,2]*male,&#xA;        vector[2]:v[dept] ~ multi_normal( c(abar,bbar) , Rho , sigma ),&#xA;        abar ~ normal( 0 , 4 ),&#xA;        bbar ~ normal(0,1),&#xA;        sigma ~ half_normal(0,1),&#xA;        Rho ~ lkjcorr(2)&#xA;    ),&#xA;    data=UCBadmit )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;That &lt;code&gt;vector[2]:v[dept]&lt;/code&gt; means &#34;declare a vector of length two for each unique dept&#34;. To access the elements of these vectors, the linear model uses multiple indexes inside the brackets: &lt;code&gt;[dept,1]&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This strategy can be taken one step further and the means can be declared as a vector as well:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;m_glmm4 &amp;lt;- ulam(&#xA;    alist(&#xA;        admit ~ binomial(applications,p),&#xA;        logit(p) &amp;lt;- v[dept,1] + v[dept,2]*male,&#xA;        vector[2]:v[dept] ~ multi_normal( v_mu , Rho , sigma ),&#xA;        vector[2]:v_mu ~ normal(0,1),&#xA;        sigma[1] ~ half_normal(0,1),&#xA;        sigma[2] ~ half_normal(0,2),&#xA;        Rho ~ lkjcorr(2)&#xA;    ),&#xA;    data=UCBadmit )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And a completely non-centered parameterization can be coded directly as well:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;m_glmm5 &amp;lt;- ulam(&#xA;    alist(&#xA;        admit ~ binomial(applications,p),&#xA;        logit(p) &amp;lt;- v_mu[1] + v[dept,1] + (v_mu[2] + v[dept,2])*male,&#xA;        matrix[dept,2]: v &amp;lt;- t(diag_pre_multiply( sigma , L_Rho ) * z),&#xA;        matrix[2,dept]: z ~ normal( 0 , 1 ),&#xA;        vector[2]: v_mu[[1]] ~ normal(0,4),&#xA;        vector[2]: v_mu[[2]] ~ normal(0,1),&#xA;        vector[2]: sigma ~ half_normal(0,1),&#xA;        cholesky_factor_corr[2]: L_Rho ~ lkj_corr_cholesky( 2 )&#xA;    ),&#xA;    data=UCBadmit )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In the above, the varying effects matrix &lt;code&gt;v&lt;/code&gt; is constructed from a matrix of z-scores &lt;code&gt;z&lt;/code&gt; and a covariance structure contained in &lt;code&gt;sigma&lt;/code&gt; and a Cholesky factor &lt;code&gt;L_Rho&lt;/code&gt;. Note the double-bracket notation &lt;code&gt;v_mu[[1]]&lt;/code&gt; allowing distinct priors for each index of a vector.&lt;/p&gt; &#xA;&lt;h2&gt;log-likelihood calculations for WAIC and LOOCV&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;ulam&lt;/code&gt; can optionally return pointwise log-likelihood values. These are needed for computing WAIC and PSIS-LOO. The &lt;code&gt;log_lik&lt;/code&gt; argument toggles this on:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;m_glmm1 &amp;lt;- ulam(&#xA;    alist(&#xA;        admit ~ binomial(applications,p),&#xA;        logit(p) &amp;lt;- a[dept] + b*male,&#xA;        a[dept] ~ normal( abar , sigma ),&#xA;        abar ~ normal( 0 , 4 ),&#xA;        sigma ~ half_normal(0,1),&#xA;        b ~ normal(0,1)&#xA;    ), data=UCBadmit , log_lik=TRUE )&#xA;WAIC(m_glmm1)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The additional code has been added to the generated quantities block of the Stan model (see this with &lt;code&gt;stancode(m_glmm1)&lt;/code&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;generated quantities{&#xA;    vector[12] log_lik;&#xA;    vector[12] p;&#xA;    for ( i in 1:12 ) {&#xA;        p[i] = a[dept[i]] + b * male[i];&#xA;        p[i] = inv_logit(p[i]);&#xA;    }&#xA;    for ( i in 1:12 ) log_lik[i] = binomial_lpmf( admit[i] | applications[i] , p[i] );&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Conditional statements, custom distributions, and mixture models&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;ulam&lt;/code&gt; also supports if-then statements and custom distribution assignments. These are useful for coding mixture models, such as zero-inflated Poisson and discrete missing value models.&lt;/p&gt; &#xA;&lt;p&gt;Here&#39;s an example zero-inflated Poisson model.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# zero-inflated poisson&#xA;# gen data first - example from text&#xA;prob_drink &amp;lt;- 0.2 # 20% of days&#xA;rate_work &amp;lt;- 1    # average 1 manuscript per day&#xA;N &amp;lt;- 365&#xA;drink &amp;lt;- rbinom( N , 1 , prob_drink )&#xA;y &amp;lt;- as.integer( (1-drink)*rpois( N , rate_work ) )&#xA;x &amp;lt;- rnorm( N ) # dummy covariate&#xA;&#xA;# now ulam code&#xA;m_zip &amp;lt;- ulam(&#xA;    alist(&#xA;        y|y==0 ~ custom( log_mix( p , 0 , poisson_lpmf(0|lambda) ) ),&#xA;        y|y&amp;gt;0 ~ custom( log1m(p) + poisson_lpmf(y|lambda) ),&#xA;        logit(p) &amp;lt;- ap,&#xA;        log(lambda) &amp;lt;- al + bl*x,&#xA;        ap ~ dnorm(0,1),&#xA;        al ~ dnorm(0,10),&#xA;        bl ~ normal(0,1)&#xA;    ) ,&#xA;    data=list(y=y,x=x) )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The Stan code corresponding to the first two lines in the formula above is:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;for ( i in 1:365 ) &#xA;    if ( y[i] &amp;gt; 0 ) target += log1m(p) + poisson_lpmf(y[i] | lambda[i]);&#xA;for ( i in 1:365 ) &#xA;    if ( y[i] == 0 ) target += log_mix(p, 0, poisson_lpmf(0 | lambda[i]));&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;What &lt;code&gt;custom&lt;/code&gt; does is define custom &lt;code&gt;target&lt;/code&gt; updates. And the &lt;code&gt;|&lt;/code&gt; operator makes the line conditional. Note that &lt;code&gt;log1m&lt;/code&gt;, &lt;code&gt;log_mix&lt;/code&gt;, and &lt;code&gt;poisson_lpmf&lt;/code&gt; are Stan functions.&lt;/p&gt; &#xA;&lt;p&gt;The same &lt;code&gt;custom&lt;/code&gt; distribution approach allows for marginalization over discrete missing values. Let&#39;s introduce some missing values in the &lt;code&gt;UCBadmit&lt;/code&gt; data from earlier.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;UCBadmit$male2 &amp;lt;- UCBadmit$male&#xA;UCBadmit$male2[1:2] &amp;lt;- (-1) # missingness code&#xA;UCBadmit$male2 &amp;lt;- as.integer(UCBadmit$male2)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now the model needs to detect when &lt;code&gt;male2&lt;/code&gt; is missing (-1) and then compute a mixture over the unknown state.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;m_mix &amp;lt;- ulam(&#xA;    alist(&#xA;        admit|male2==-1 ~ custom( log_mix( &#xA;            phi_male , &#xA;            binomial_lpmf(admit|applications,p_m1) , &#xA;            binomial_lpmf(admit|applications,p_m0) ) ),&#xA;        admit|male2&amp;gt;-1 ~ binomial( applications , p ),&#xA;        logit(p) &amp;lt;- a[dept] + b*male2,&#xA;        logit(p_m1) &amp;lt;- a[dept] + b*1,&#xA;        logit(p_m0) &amp;lt;- a[dept] + b*0,&#xA;        male2|male2&amp;gt;-1 ~ bernoulli( phi_male ),&#xA;        phi_male ~ beta(2,2),&#xA;        a[dept] ~ normal(0,4),&#xA;        b ~ normal(0,1)&#xA;    ),&#xA;    data=UCBadmit )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note the addition of &lt;code&gt;phi_male&lt;/code&gt; to average over the unknown state.&lt;/p&gt; &#xA;&lt;h2&gt;Continuous missing data imputation&lt;/h2&gt; &#xA;&lt;p&gt;In principle, imputation of missing real-valued data is easy: Just replace each missing value with a parameter. In practice, this involves a bunch of annoying bookkeeping. &lt;code&gt;ulam&lt;/code&gt; has a macro named &lt;code&gt;merge_missing&lt;/code&gt; to simplify this.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;UCBadmit$x &amp;lt;- rnorm(12)&#xA;UCBadmit$x[1:2] &amp;lt;- NA&#xA;m_miss &amp;lt;- ulam(&#xA;    alist(&#xA;        admit ~ binomial(applications,p),&#xA;        logit(p) &amp;lt;- a + b*male + bx*x_merge,&#xA;        x_merge ~ normal( 0 , 1 ),&#xA;        x_merge &amp;lt;- merge_missing( x , x_impute ),&#xA;        a ~ normal(0,4),&#xA;        b ~ normal(0,1),&#xA;        bx ~ normal(0,1)&#xA;    ),&#xA;    data=UCBadmit )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;What &lt;code&gt;merge_missing&lt;/code&gt; does is find the &lt;code&gt;NA&lt;/code&gt; values in &lt;code&gt;x&lt;/code&gt; (whichever symbol is the first argument), build a vector of parameters called &lt;code&gt;x_impute&lt;/code&gt; (whatever you name the second argument) of the right length, and piece together a vector &lt;code&gt;x_merge&lt;/code&gt; that contains both, in the right places. You can then assign a prior to this vector and use it in linear models as usual.&lt;/p&gt; &#xA;&lt;p&gt;The merging is done as the Stan model runs, using a custom function block. See the Stan code &lt;code&gt;stancode(m_miss)&lt;/code&gt; for all the lovely details.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;merge missing&lt;/code&gt; is an example of a macro, which is a way for &lt;code&gt;ulam&lt;/code&gt; to use function names to trigger special compilation. In this case, &lt;code&gt;merge_missing&lt;/code&gt; both inserts a function in the Stan model and builds the necessary index to locate the missing values during run time. Macros will get full documentation later, once the system is finalized.&lt;/p&gt; &#xA;&lt;h2&gt;Gaussian processes&lt;/h2&gt; &#xA;&lt;p&gt;A simple Gaussian process, like the Oceanic islands example in Chapter 13 of the book, is done as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;data(Kline2)&#xA;d &amp;lt;- Kline2&#xA;data(islandsDistMatrix)&#xA;d$society &amp;lt;- 1:10&#xA;dat &amp;lt;- list(&#xA;    y=d$total_tools,&#xA;    society=d$society,&#xA;    log_pop = log(d$population),&#xA;    Dmat=islandsDistMatrix&#xA;)&#xA;&#xA;m_GP1 &amp;lt;- ulam(&#xA;    alist(&#xA;        y ~ poisson( mu ),&#xA;        log(mu) &amp;lt;- a + aj[society] + b*log_pop,&#xA;        a ~ normal(0,10),&#xA;        b ~ normal(0,1),&#xA;        vector[10]: aj ~ multi_normal( 0 , SIGMA ),&#xA;        matrix[10,10]: SIGMA &amp;lt;- cov_GPL2( Dmat , etasq , rhosq , 0.01 ),&#xA;        etasq ~ exponential(1),&#xA;        rhosq ~ exponential(1)&#xA;    ),&#xA;    data=dat )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This is just an ordinary varying intercepts model, but all 10 intercepts are drawn from a single Gaussian distribution. The covariance matrix &lt;code&gt;SIGMA&lt;/code&gt; is defined in the usual L2-norm. Again, &lt;code&gt;cov_GPL2&lt;/code&gt; is a macro that inserts a function in the Stan code to compute the covariance matrix as the model runs.&lt;/p&gt; &#xA;&lt;p&gt;Fancier Gaussian processes require a different parameterization. And these can be built as well. Here&#39;s an example using 151 primate species and a phylogenetic distance matrix. First, prepare the data:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;data(Primates301)&#xA;data(Primates301_distance_matrix)&#xA;d &amp;lt;- Primates301&#xA;d$name &amp;lt;- as.character(d$name)&#xA;dstan &amp;lt;- d[ complete.cases( d$social_learning, d$research_effort , d$body , d$brain ) , ]&#xA;# prune distance matrix to spp in dstan&#xA;spp_obs &amp;lt;- dstan$name&#xA;y &amp;lt;- Primates301_distance_matrix&#xA;y2 &amp;lt;- y[ spp_obs , spp_obs ]&#xA;# scale distances&#xA;y3 &amp;lt;- y2/max(y2)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now the model, which is a non-centered L2-norm Gaussian process:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;m_GP2 &amp;lt;- ulam(&#xA;    alist(&#xA;        social_learning ~ poisson( lambda ),&#xA;        log(lambda) &amp;lt;- a + g[spp_id] + b_ef*log_research_effort + b_body*log_body + b_eq*log_brain,&#xA;        a ~ normal(0,1),&#xA;        vector[N_spp]: g &amp;lt;&amp;lt;- L_SIGMA * eta,&#xA;        vector[N_spp]: eta ~ normal( 0 , 1 ),&#xA;        matrix[N_spp,N_spp]: L_SIGMA &amp;lt;&amp;lt;- cholesky_decompose( SIGMA ),&#xA;        matrix[N_spp,N_spp]: SIGMA &amp;lt;- cov_GPL2( Dmat , etasq , rhosq , 0.01 ),&#xA;        b_body ~ normal(0,1),&#xA;        b_eq ~ normal(0,1),&#xA;        b_ef ~ normal(1,1),&#xA;        etasq ~ exponential(1),&#xA;        rhosq ~ exponential(1)&#xA;    ),&#xA;    data=list(&#xA;        N_spp = nrow(dstan),&#xA;        social_learning = dstan$social_learning,&#xA;        spp_id = 1:nrow(dstan),&#xA;        log_research_effort = log(dstan$research_effort),&#xA;        log_body = log(dstan$body),&#xA;        log_brain = log(dstan$brain),&#xA;        Dmat = y3&#xA;    ) , &#xA;    control=list(max_treedepth=15,adapt_delta=0.95) ,&#xA;    sample=FALSE )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This model does not sample quickly, so I&#39;ve set &lt;code&gt;sample=FALSE&lt;/code&gt;. You can still inspect the Stan code with &lt;code&gt;stancode(m_GP2)&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Note that the covariance &lt;code&gt;SIGMA&lt;/code&gt; is built the same way as before, but then we immediately decompose it to a Cholesky factor and build the varying intercepts &lt;code&gt;g&lt;/code&gt; by matrix multiplication. The &lt;code&gt;&amp;lt;&amp;lt;-&lt;/code&gt; operator tells &lt;code&gt;ulam&lt;/code&gt; not to loop, but to do a direct assignment. So &lt;code&gt;g &amp;lt;&amp;lt;- L_SIGMA * eta&lt;/code&gt; does the right linear algebra.&lt;/p&gt; &#xA;&lt;h2&gt;Within-chain multithreading&lt;/h2&gt; &#xA;&lt;p&gt;Using &lt;code&gt;cmdstanr&lt;/code&gt; instead of &lt;code&gt;rstan&lt;/code&gt; is currently the only way to use within-chain multithreading with &lt;code&gt;rethinking&lt;/code&gt;. It also tends to compile models faster and is more intelligent about when models need to be re-compiled, so using &lt;code&gt;cmdstanr&lt;/code&gt; is recommended, even if you don&#39;t want multithreading.&lt;/p&gt; &#xA;&lt;p&gt;If you want &lt;code&gt;ulam&lt;/code&gt; to access Stan using the &lt;code&gt;cmdstanr&lt;/code&gt; package, then you may install that as well with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;devtools::install_github(&#34;stan-dev/cmdstanr&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you haven&#39;t installed cmdstan previously, you will also need to do that with &lt;code&gt;install_cmdstan()&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Then you need to add &lt;code&gt;cmdstan=TRUE&lt;/code&gt; to the &lt;code&gt;ulam&lt;/code&gt; code. The &lt;code&gt;threads&lt;/code&gt; argument controls the number of threads per chain. Example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;N &amp;lt;- 1e4&#xA;x &amp;lt;- rnorm(N)&#xA;m &amp;lt;- 1 + rpois(N,2)&#xA;y &amp;lt;- rbinom( N , size=m , prob=inv_logit(-3+x) )&#xA;dat &amp;lt;- list( y=y , x=x , m=m )&#xA;# two threads&#xA;m1 &amp;lt;- ulam(&#xA;    alist(&#xA;        y ~ binomial_logit( m , logit_p ),&#xA;        logit_p &amp;lt;- a + b*x,&#xA;        a ~ normal(0,1.5),&#xA;        b ~ normal(0,0.5)&#xA;    ) , data=dat , &#xA;    cmdstan=TRUE , threads=2 , refresh=1000 )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;There are models that cannot be automaticaly multithreaded this way, because of the complexity of the code. In those cases, you can write the code directly in Stan. See &lt;a href=&#34;https://mc-stan.org/users/documentation/case-studies/reduce_sum_tutorial.html&#34;&gt;this guide&lt;/a&gt;. Writing multithreaded models direct in Stan can also be more efficient, since you can make detailed choices about which variables to pass and which pieces of the model to multithread.&lt;/p&gt; &#xA;&lt;h2&gt;Work in progress&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;ulam&lt;/code&gt; is still in development, but mostly feature complete. It will remain primarily a teaching tool, exposing the statistical details of the model while hiding some of the programming details necessary in Stan.&lt;/p&gt; &#xA;&lt;h1&gt;&lt;code&gt;map2stan&lt;/code&gt; syntax and features&lt;/h1&gt; &#xA;&lt;p&gt;The older &lt;code&gt;map2stan&lt;/code&gt; function makes stronger assumtions about the formulas it will see. This allows is to provide some additional automation and it has some special syntax as a result. &lt;code&gt;ulam&lt;/code&gt; in contrast supports such features through its macros library.&lt;/p&gt; &#xA;&lt;h2&gt;Non-centered parameterization&lt;/h2&gt; &#xA;&lt;p&gt;Here is a non-centered parameterization that moves the scale parameters in the varying effects prior to the linear model, which is often more efficient for sampling:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;f4u &amp;lt;- alist(&#xA;    y ~ dnorm( mu , sigma ),&#xA;    mu &amp;lt;- a + zaj[group]*sigma_group[1] + &#xA;         (b + zbj[group]*sigma_group[2])*x,&#xA;    c(zaj,zbj)[group] ~ dmvnorm( 0 , Rho_group ),&#xA;    a ~ dnorm( 0 , 10 ),&#xA;    b ~ dnorm( 0 , 1 ),&#xA;    sigma ~ dcauchy( 0 , 1 ),&#xA;    sigma_group ~ dcauchy( 0 , 1 ),&#xA;    Rho_group ~ dlkjcorr(2)&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Chapter 13 of the book provides a lot more detail on this issue.&lt;/p&gt; &#xA;&lt;p&gt;We can take this strategy one step further and remove the correlation matrix, &lt;code&gt;Rho_group&lt;/code&gt;, from the prior as well. &lt;code&gt;map2stan&lt;/code&gt; facilitates this form via the &lt;code&gt;dmvnormNC&lt;/code&gt; density, which uses an internal Cholesky decomposition of the correlation matrix to build the varying effects. Here is the previous varying slopes model, now with the non-centered notation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;f4nc &amp;lt;- alist(&#xA;    y ~ dnorm( mu , sigma ),&#xA;    mu &amp;lt;- a + aj[group] + (b + bj[group])*x,&#xA;    c(aj,bj)[group] ~ dmvnormNC( sigma_group , Rho_group ),&#xA;    a ~ dnorm( 0 , 10 ),&#xA;    b ~ dnorm( 0 , 1 ),&#xA;    sigma ~ dcauchy( 0 , 1 ),&#xA;    sigma_group ~ dcauchy( 0 , 1 ),&#xA;    Rho_group ~ dlkjcorr(2)&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Internally, a Cholesky factor &lt;code&gt;L_Rho_group&lt;/code&gt; is used to perform sampling. It will appear in the returned samples, in addition to &lt;code&gt;Rho_group&lt;/code&gt;, which is constructed from it.&lt;/p&gt; &#xA;&lt;h2&gt;Semi-automated Bayesian imputation&lt;/h2&gt; &#xA;&lt;p&gt;It is possible to code simple Bayesian imputations. For example, let&#39;s simulate a simple regression with missing predictor values:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;N &amp;lt;- 100&#xA;N_miss &amp;lt;- 10&#xA;x &amp;lt;- rnorm( N )&#xA;y &amp;lt;- rnorm( N , 2*x , 1 )&#xA;x[ sample(1:N,size=N_miss) ] &amp;lt;- NA&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;That removes 10 &lt;code&gt;x&lt;/code&gt; values. Then the &lt;code&gt;map2stan&lt;/code&gt; formula list just defines a distribution for &lt;code&gt;x&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;f5 &amp;lt;- alist(&#xA;    y ~ dnorm( mu , sigma ),&#xA;    mu &amp;lt;- a + b*x,&#xA;    x ~ dnorm( mu_x, sigma_x ),&#xA;    a ~ dnorm( 0 , 100 ),&#xA;    b ~ dnorm( 0  , 10 ),&#xA;    mu_x ~ dnorm( 0 , 100 ),&#xA;    sigma_x ~ dcauchy(0,2),&#xA;    sigma ~ dcauchy(0,2)&#xA;)&#xA;m5 &amp;lt;- map2stan( f5 , data=list(y=y,x=x) )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;What &lt;code&gt;map2stan&lt;/code&gt; does is notice the missing values, see the distribution assigned to the variable with the missing values, build the Stan code that uses a mix of observed and estimated &lt;code&gt;x&lt;/code&gt; values in the regression. See the &lt;code&gt;stancode(m5)&lt;/code&gt; for details of the implementation.&lt;/p&gt; &#xA;&lt;h2&gt;Semi-automated marginalization for binary discrete missing values&lt;/h2&gt; &#xA;&lt;p&gt;Binary (0/1) variables with missing values present a special obstacle, because Stan cannot sample discrete parameters. So instead of imputing binary missing values, &lt;code&gt;map2stan&lt;/code&gt; can average (marginalize) over them. As in the above case, when &lt;code&gt;map2stan&lt;/code&gt; detects missing values in a predictor variable, it will try to find a distribution for the variable containing them. If this variable is binary (0/1), then it will construct a mixture model in which each term is the log-likelihood conditional on the variables taking a particular combination of 0/1 values.&lt;/p&gt; &#xA;&lt;p&gt;Following the example in the previous section, we can simulate missingness in a binary predictor:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;N &amp;lt;- 100&#xA;N_miss &amp;lt;- 10&#xA;x &amp;lt;- rbinom( N , size=1 , prob=0.5 )&#xA;y &amp;lt;- rnorm( N , 2*x , 1 )&#xA;x[ sample(1:N,size=N_miss) ] &amp;lt;- NA&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The model definition is analogous to the previous, but also requires some care in specifying constraints for the hyperparameters that define the distribution for &lt;code&gt;x&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;f6 &amp;lt;- alist(&#xA;    y ~ dnorm( mu , sigma ),&#xA;    mu &amp;lt;- a + b*x,&#xA;    x ~ bernoulli( phi ),&#xA;    a ~ dnorm( 0 , 100 ),&#xA;    b ~ dnorm( 0  , 10 ),&#xA;    phi ~ beta( 1 , 1 ),&#xA;    sigma ~ dcauchy(0,2)&#xA;)&#xA;m6 &amp;lt;- map2stan( f6 , data=list(y=y,x=x) , constraints=list(phi=&#34;lower=0,upper=1&#34;) )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The algorithm works, in theory, for any number of binary predictors with missing values. For example, with two predictors, each with missingness:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;N &amp;lt;- 100&#xA;N_miss &amp;lt;- 10&#xA;x1 &amp;lt;- rbinom( N , size=1 , prob=0.5 )&#xA;x2 &amp;lt;- rbinom( N , size=1 , prob=0.1 )&#xA;y &amp;lt;- rnorm( N , 2*x1 - x2  , 1 )&#xA;x1[ sample(1:N,size=N_miss) ] &amp;lt;- NA&#xA;x2[ sample(1:N,size=N_miss) ] &amp;lt;- NA&#xA;f7 &amp;lt;- alist(&#xA;    y ~ dnorm( mu , sigma ),&#xA;    mu &amp;lt;- a + b1*x1 + b2*x2,&#xA;    x1 ~ bernoulli( phi1 ),&#xA;    x2 ~ bernoulli( phi2 ),&#xA;    a ~ dnorm( 0 , 100 ),&#xA;    c(b1,b2) ~ dnorm( 0  , 10 ),&#xA;    phi1 ~ beta( 1 , 1 ),&#xA;    phi2 ~ beta( 1 , 1 ),&#xA;    sigma ~ dcauchy(0,2)&#xA;)&#xA;m7 &amp;lt;- map2stan( f7 , data=list(y=y,x1=x1,x2=x2) , &#xA;      constraints=list(phi1=&#34;lower=0,upper=1&#34;,phi2=&#34;lower=0,upper=1&#34;) )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;While the unobserved values for the binary predictors are usually not of interest, they can be computed from the posterior distribution. Adding the argument &lt;code&gt;do_discrete_imputation=TRUE&lt;/code&gt; instructs &lt;code&gt;map2stan&lt;/code&gt; to perform these calculations automatically. Example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;m6 &amp;lt;- map2stan( f6 , data=list(y=y,x=x) , constraints=list(phi=&#34;lower=0,upper=1&#34;) ,&#xA;      do_discrete_imputation=TRUE )&#xA;precis( m6 , depth=2 )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The output contains samples for each case with imputed probilities that &lt;code&gt;x&lt;/code&gt; takes the value 1.&lt;/p&gt; &#xA;&lt;p&gt;The algorithm works by constructing a list of mixture terms that are needed to to compute the probability of each observed &lt;code&gt;y&lt;/code&gt; value. In the simplest case, with only one predictor with missing values, the implied mixture likelihood contains two terms:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Pr(y[i]) = Pr(x[i]=1)Pr(y[i]|x[i]=1) + Pr(x[i]=0)Pr(y[i]|x[i]=0)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In the parameters of our example model &lt;code&gt;m6&lt;/code&gt; above, this is:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Pr(y[i]) = phi*N(y[i]|a+b,sigma) + (1-phi)*N(y[i]|a,sigma)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It is now a simple matter to loop over cases &lt;code&gt;i&lt;/code&gt; and compute the above for each. Similarly the posterior probability of that &lt;code&gt;x[i]==1&lt;/code&gt; is given as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Pr(x[i]==1|y[i]) = phi*N(y[i]|a+b,sigma) / Pr(y[i])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When only one predictor has missingness, then this is simple. What about when there are two or more? In that case, all the possible combinations of missingness have to be accounted for. For example, suppose there are two predictors, &lt;code&gt;x1&lt;/code&gt; and &lt;code&gt;x2&lt;/code&gt;, both with missingness on case &lt;code&gt;i&lt;/code&gt;. Now the implied mixture likelihood is:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Pr(y[i]) = Pr(x1=1)Pr(x2=1)*Pr(y[i]|x1=1,x2=1) + Pr(x1=1)Pr(x2=0)Pr(y[i]|x1=1,x2=0) + Pr(x1=0)Pr(x2=1)Pr(y[i]|x1=0,x2=1) + Pr(x1=0)Pr(x2=0)Pr(y[i]|x1=0,x2=0)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;There are four combinations of unobserved values, and so four terms in the mixture likelihood. When &lt;code&gt;x2&lt;/code&gt; is instead observed, we can substitute the observed value into the above, and then the mixture simplifies readily to our previous two-term likelihood:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Pr(y[i]|x2[i]==1) = Pr(x1=1)Pr(x2=1)Pr(y[i]|x1=1,x2=1) + Pr(x1=1)Pr(x2=0)Pr(y[i]|x1=1,x2=1) + Pr(x1=0)Pr(x2=1)Pr(y[i]|x1=0,x2=1) + Pr(x1=0)Pr(x2=0)Pr(y[i]|x1=0,x2=1)&#xA;                  = [Pr(x1=1)Pr(x2=1)+Pr(x1=1)Pr(x2=0)]Pr(y[i]|x1=1,x2=1) &#xA;                    + [Pr(x1=0)Pr(x2=1)+Pr(x1=0)Pr(x2=0)]Pr(y[i]|x1=0,x2=1)&#xA;                  = Pr(x1=1)Pr(y[i]|x1=1,x2=1) + Pr(x1=0)Pr(y[i]|x1=0,x2=1)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This implies that if we loop over cases &lt;code&gt;i&lt;/code&gt; and insert any observed values into the general mixture likelihood, we can compute the relevant mixture for the specific combination of missingness on each case &lt;code&gt;i&lt;/code&gt;. That is what &lt;code&gt;map2stan&lt;/code&gt; does. The general mixture terms can be generated algorithmically. The code below generates a matrix of terms for &lt;code&gt;n&lt;/code&gt; binary variables with missingness.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ncombinations &amp;lt;- 2^n&#xA;d &amp;lt;- matrix(NA,nrow=ncombinations,ncol=n)&#xA;for ( col_var in 1:n ) &#xA;    d[,col_var] &amp;lt;- rep( 0:1 , each=2^(col_var-1) , length.out=ncombinations )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Rows of &lt;code&gt;d&lt;/code&gt; contain terms, columns contain variables, and the values in each column are the corresponding values of each variable. The algorithm builds a linear model for each row in this matrix, composes the mixture likelihood as the sum of these rows, and performs proper substitutions of observed values. All calculations are done on the log scale, for precision.&lt;/p&gt; &#xA;&lt;h2&gt;Gaussian process&lt;/h2&gt; &#xA;&lt;p&gt;A basic Gaussian process can be specified with the &lt;code&gt;GPL2&lt;/code&gt; distribution label. This implies a multivariate Gaussian with a covariance matrix defined by the ordinary L2 norm distance function:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;k(i,j) = eta^2 * exp( -rho^2 * D(i,j)^2 ) + ifelse(i==j,sigma^2,0)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where &lt;code&gt;D&lt;/code&gt; is a matrix of pairwise distances. To use this convention in, for example, a spatial autocorrelation model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;library(rethinking)&#xA;data(Kline2)&#xA;d &amp;lt;- Kline2&#xA;data(islandsDistMatrix)&#xA;d$society &amp;lt;- 1:10&#xA;mGP &amp;lt;- map2stan(&#xA;    alist(&#xA;        total_tools ~ dpois( mu ),&#xA;        log(mu) &amp;lt;- a + aj[society],&#xA;        a ~ dnorm(0,10),&#xA;        aj[society] ~ GPL2( Dmat , etasq , rhosq , 0.01 ),&#xA;        etasq ~ dcauchy(0,1),&#xA;        rhosq ~ dcauchy(0,1)&#xA;    ),&#xA;    data=list(&#xA;        total_tools=d$total_tools,&#xA;        society=d$society,&#xA;        Dmat=islandsDistMatrix),&#xA;    constraints=list(&#xA;        etasq=&#34;lower=0&#34;,&#xA;        rhosq=&#34;lower=0&#34;&#xA;    ),&#xA;    warmup=1000 , iter=5000 , chains=4 )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note the use of the &lt;code&gt;constraints&lt;/code&gt; list to pass custom parameter constraints to Stan. This example is explored in more detail in the book.&lt;/p&gt; &#xA;&lt;h2&gt;Information criteria&lt;/h2&gt; &#xA;&lt;p&gt;Both &lt;code&gt;map&lt;/code&gt; and &lt;code&gt;map2stan&lt;/code&gt; provide DIC and WAIC. Well, in most cases they do. In truth, both tools are flexible enough that you can specify models for which neither DIC nor WAIC can be correctly calculated. But for ordinary GLMs and GLMMs, it works. See the R help &lt;code&gt;?WAIC&lt;/code&gt;. A convenience function &lt;code&gt;compare&lt;/code&gt; summarizes information criteria comparisons, including standard errors for WAIC.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;ulam&lt;/code&gt; supports WAIC calculation with the optional &lt;code&gt;log_lik=TRUE&lt;/code&gt; argument, which returns the kind of log-likelihood vector needed by the &lt;code&gt;loo&lt;/code&gt; package.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;ensemble&lt;/code&gt; computes &lt;code&gt;link&lt;/code&gt; and &lt;code&gt;sim&lt;/code&gt; output for an ensemble of models, each weighted by its Akaike weight, as computed from WAIC.&lt;/p&gt; &#xA;&lt;h1&gt;Code issues with 1st edition of Statistical Rethinking&lt;/h1&gt; &#xA;&lt;p&gt;A small change to &lt;code&gt;link&lt;/code&gt; has broken two examples in the first edition of the book, in Chapter 7.&lt;/p&gt; &#xA;&lt;h2&gt;R code 7.10&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;mu.Africa.mean &amp;lt;- apply( mu.Africa , 2 , mean ) Error in apply(mu.Africa, 2, mean) : dim(X) must have a positive length&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;This occurs because link() now returns all linear models. So mu.Africa is a list containing mu and gamma. To fix, use:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;mu.Africa.mean &amp;lt;- apply( mu.Africa$mu , 2 , mean )&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Use a similar fix in the other apply() calls in the same section.&lt;/p&gt; &#xA;&lt;h2&gt;R code 7.17&lt;/h2&gt; &#xA;&lt;p&gt;Similar problem as for R code 7.10. Use mu.ruggedlo$mu in place of mu.ruggedlo.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>rdpeng/ProgrammingAssignment2</title>
    <updated>2022-06-02T02:21:48Z</updated>
    <id>tag:github.com,2022-06-02:/rdpeng/ProgrammingAssignment2</id>
    <link href="https://github.com/rdpeng/ProgrammingAssignment2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Repository for Programming Assignment 2 for R Programming on Coursera&lt;/p&gt;&lt;hr&gt;&lt;h3&gt;Introduction&lt;/h3&gt; &#xA;&lt;p&gt;This second programming assignment will require you to write an R function that is able to cache potentially time-consuming computations. For example, taking the mean of a numeric vector is typically a fast operation. However, for a very long vector, it may take too long to compute the mean, especially if it has to be computed repeatedly (e.g. in a loop). If the contents of a vector are not changing, it may make sense to cache the value of the mean so that when we need it again, it can be looked up in the cache rather than recomputed. In this Programming Assignment you will take advantage of the scoping rules of the R language and how they can be manipulated to preserve state inside of an R object.&lt;/p&gt; &#xA;&lt;h3&gt;Example: Caching the Mean of a Vector&lt;/h3&gt; &#xA;&lt;p&gt;In this example we introduce the &lt;code&gt;&amp;lt;&amp;lt;-&lt;/code&gt; operator which can be used to assign a value to an object in an environment that is different from the current environment. Below are two functions that are used to create a special object that stores a numeric vector and caches its mean.&lt;/p&gt; &#xA;&lt;p&gt;The first function, &lt;code&gt;makeVector&lt;/code&gt; creates a special &#34;vector&#34;, which is really a list containing a function to&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;set the value of the vector&lt;/li&gt; &#xA; &lt;li&gt;get the value of the vector&lt;/li&gt; &#xA; &lt;li&gt;set the value of the mean&lt;/li&gt; &#xA; &lt;li&gt;get the value of the mean&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;!-- --&gt; &#xA;&lt;pre&gt;&lt;code&gt;makeVector &amp;lt;- function(x = numeric()) {&#xA;        m &amp;lt;- NULL&#xA;        set &amp;lt;- function(y) {&#xA;                x &amp;lt;&amp;lt;- y&#xA;                m &amp;lt;&amp;lt;- NULL&#xA;        }&#xA;        get &amp;lt;- function() x&#xA;        setmean &amp;lt;- function(mean) m &amp;lt;&amp;lt;- mean&#xA;        getmean &amp;lt;- function() m&#xA;        list(set = set, get = get,&#xA;             setmean = setmean,&#xA;             getmean = getmean)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The following function calculates the mean of the special &#34;vector&#34; created with the above function. However, it first checks to see if the mean has already been calculated. If so, it &lt;code&gt;get&lt;/code&gt;s the mean from the cache and skips the computation. Otherwise, it calculates the mean of the data and sets the value of the mean in the cache via the &lt;code&gt;setmean&lt;/code&gt; function.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cachemean &amp;lt;- function(x, ...) {&#xA;        m &amp;lt;- x$getmean()&#xA;        if(!is.null(m)) {&#xA;                message(&#34;getting cached data&#34;)&#xA;                return(m)&#xA;        }&#xA;        data &amp;lt;- x$get()&#xA;        m &amp;lt;- mean(data, ...)&#xA;        x$setmean(m)&#xA;        m&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Assignment: Caching the Inverse of a Matrix&lt;/h3&gt; &#xA;&lt;p&gt;Matrix inversion is usually a costly computation and there may be some benefit to caching the inverse of a matrix rather than computing it repeatedly (there are also alternatives to matrix inversion that we will not discuss here). Your assignment is to write a pair of functions that cache the inverse of a matrix.&lt;/p&gt; &#xA;&lt;p&gt;Write the following functions:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;makeCacheMatrix&lt;/code&gt;: This function creates a special &#34;matrix&#34; object that can cache its inverse.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cacheSolve&lt;/code&gt;: This function computes the inverse of the special &#34;matrix&#34; returned by &lt;code&gt;makeCacheMatrix&lt;/code&gt; above. If the inverse has already been calculated (and the matrix has not changed), then &lt;code&gt;cacheSolve&lt;/code&gt; should retrieve the inverse from the cache.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Computing the inverse of a square matrix can be done with the &lt;code&gt;solve&lt;/code&gt; function in R. For example, if &lt;code&gt;X&lt;/code&gt; is a square invertible matrix, then &lt;code&gt;solve(X)&lt;/code&gt; returns its inverse.&lt;/p&gt; &#xA;&lt;p&gt;For this assignment, assume that the matrix supplied is always invertible.&lt;/p&gt; &#xA;&lt;p&gt;In order to complete this assignment, you must do the following:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Fork the GitHub repository containing the stub R files at &lt;a href=&#34;https://github.com/rdpeng/ProgrammingAssignment2&#34;&gt;https://github.com/rdpeng/ProgrammingAssignment2&lt;/a&gt; to create a copy under your own account.&lt;/li&gt; &#xA; &lt;li&gt;Clone your forked GitHub repository to your computer so that you can edit the files locally on your own machine.&lt;/li&gt; &#xA; &lt;li&gt;Edit the R file contained in the git repository and place your solution in that file (please do not rename the file).&lt;/li&gt; &#xA; &lt;li&gt;Commit your completed R file into YOUR git repository and push your git branch to the GitHub repository under your account.&lt;/li&gt; &#xA; &lt;li&gt;Submit to Coursera the URL to your GitHub repository that contains the completed R code for the assignment.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Grading&lt;/h3&gt; &#xA;&lt;p&gt;This assignment will be graded via peer assessment.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>rstudio/keras</title>
    <updated>2022-06-02T02:21:48Z</updated>
    <id>tag:github.com,2022-06-02:/rstudio/keras</id>
    <link href="https://github.com/rstudio/keras" rel="alternate"></link>
    <summary type="html">&lt;p&gt;R Interface to Keras&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;R interface to Keras&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://s3.amazonaws.com/keras.io/img/keras-logo-2018-large-1200.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/rstudio/keras/actions?workflow=R-CMD-check&#34;&gt;&lt;img src=&#34;https://github.com/rstudio/keras/workflows/R-CMD-check/badge.svg?sanitize=true&#34; alt=&#34;R build status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://cran.r-project.org/package=keras&#34;&gt;&lt;img src=&#34;https://www.r-pkg.org/badges/version/keras&#34; alt=&#34;CRAN_Status_Badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/keras-team/keras/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/mashape/apistatus.svg?maxAge=2592000&#34; alt=&#34;license&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://keras.io/&#34;&gt;Keras&lt;/a&gt; is a high-level neural networks API developed with a focus on enabling fast experimentation. &lt;em&gt;Being able to go from idea to result with the least possible delay is key to doing good research.&lt;/em&gt; Keras has the following key features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Allows the same code to run on CPU or on GPU, seamlessly.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;User-friendly API which makes it easy to quickly prototype deep learning models.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Built-in support for convolutional networks (for computer vision), recurrent networks (for sequence processing), and any combination of both.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Supports arbitrary network architectures: multi-input or multi-output models, layer sharing, model sharing, etc. This means that Keras is appropriate for building essentially any deep learning model, from a memory network to a neural Turing machine.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See the package website at &lt;a href=&#34;https://tensorflow.rstudio.com&#34;&gt;https://tensorflow.rstudio.com&lt;/a&gt; for complete documentation.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>sparklyr/sparklyr</title>
    <updated>2022-06-02T02:21:48Z</updated>
    <id>tag:github.com,2022-06-02:/sparklyr/sparklyr</id>
    <link href="https://github.com/sparklyr/sparklyr" rel="alternate"></link>
    <summary type="html">&lt;p&gt;R interface for Apache Spark&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;sparklyr: R interface for Apache Spark&lt;/h1&gt; &#xA;&lt;!-- README.md is generated from README.Rmd. Please edit that file --&gt; &#xA;&lt;!-- badges: start --&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/sparklyr/sparklyr/actions/workflows/R-CMD-check.yaml&#34;&gt;&lt;img src=&#34;https://github.com/sparklyr/sparklyr/actions/workflows/R-CMD-check.yaml/badge.svg?sanitize=true&#34; alt=&#34;R-CMD-check&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/sparklyr/sparklyr/actions/workflows/spark-tests.yaml&#34;&gt;&lt;img src=&#34;https://github.com/sparklyr/sparklyr/actions/workflows/spark-tests.yaml/badge.svg?sanitize=true&#34; alt=&#34;Spark-Tests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://CRAN.R-project.org/package=sparklyr&#34;&gt;&lt;img src=&#34;https://www.r-pkg.org/badges/version/sparklyr&#34; alt=&#34;CRAN status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://app.codecov.io/gh/sparklyr/sparklyr?branch=main&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/sparklyr/sparklyr/branch/main/graph/badge.svg?sanitize=true&#34; alt=&#34;Codecov test coverage&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- badges: end --&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/tools/readme/sparklyr-diagram.png&#34; width=&#34;320&#34; align=&#34;right&#34; style=&#34;margin-left: 20px; margin-right: 20px&#34;&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install and connect to &lt;a href=&#34;https://spark.apache.org/&#34;&gt;Spark&lt;/a&gt; using YARN, Mesos, Livy or Kubernetes.&lt;/li&gt; &#xA; &lt;li&gt;Use &lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#using-dplyr&#34;&gt;dplyr&lt;/a&gt; to filter and aggregate Spark datasets and &lt;a href=&#34;https://spark.rstudio.com/guides/streaming/&#34;&gt;streams&lt;/a&gt; then bring them into R for analysis and visualization.&lt;/li&gt; &#xA; &lt;li&gt;Use &lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#machine-learning&#34;&gt;MLlib&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#using-h2o&#34;&gt;H2O&lt;/a&gt;, &lt;a href=&#34;https://spark.rstudio.com/packages/sparkxgb/latest/&#34;&gt;XGBoost&lt;/a&gt; and &lt;a href=&#34;https://spark.rstudio.com/packages/graphframes/latest/&#34;&gt;GraphFrames&lt;/a&gt; to train models at scale in Spark.&lt;/li&gt; &#xA; &lt;li&gt;Create interoperable machine learning &lt;a href=&#34;https://spark.rstudio.com/guides/pipelines.html&#34;&gt;pipelines&lt;/a&gt; and productionize them with &lt;a href=&#34;https://spark.rstudio.com/packages/mleap/latest/&#34;&gt;MLeap&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Create &lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#extensions&#34;&gt;extensions&lt;/a&gt; that call the full Spark API or run &lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#distributed-r&#34;&gt;distributed R&lt;/a&gt; code to support new functionality.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#connecting-to-spark&#34;&gt;Connecting to Spark&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#using-dplyr&#34;&gt;Using dplyr&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#window-functions&#34;&gt;Window Functions&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#using-sql&#34;&gt;Using SQL&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#machine-learning&#34;&gt;Machine Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#reading-and-writing-data&#34;&gt;Reading and Writing Data&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#distributed-r&#34;&gt;Distributed R&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#extensions&#34;&gt;Extensions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#table-utilities&#34;&gt;Table Utilities&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#connection-utilities&#34;&gt;Connection Utilities&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#rstudio-ide&#34;&gt;RStudio IDE&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#using-h2o&#34;&gt;Using H2O&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#connecting-through-livy&#34;&gt;Connecting through Livy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#connecting-through-databricks-connect&#34;&gt;Connecting through Databricks Connect&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;You can install the &lt;strong&gt;sparklyr&lt;/strong&gt; package from &lt;a href=&#34;https://CRAN.r-project.org&#34;&gt;CRAN&lt;/a&gt; as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&#34;sparklyr&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You should also install a local version of Spark for development purposes:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(sparklyr)&#xA;spark_install()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To upgrade to the latest version of sparklyr, run the following command and restart your r session:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&#34;devtools&#34;)&#xA;devtools::install_github(&#34;sparklyr/sparklyr&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Connecting to Spark&lt;/h2&gt; &#xA;&lt;p&gt;You can connect to both local instances of Spark as well as remote Spark clusters. Here we’ll connect to a local instance of Spark via the &lt;a href=&#34;https://spark.rstudio.com/packages/sparklyr/latest/reference/spark-connections.html&#34;&gt;spark_connect&lt;/a&gt; function:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(sparklyr)&#xA;sc &amp;lt;- spark_connect(master = &#34;local&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The returned Spark connection (&lt;code&gt;sc&lt;/code&gt;) provides a remote dplyr data source to the Spark cluster.&lt;/p&gt; &#xA;&lt;p&gt;For more information on connecting to remote Spark clusters see the &lt;a href=&#34;https://spark.rstudio.com/deployment.html&#34;&gt;Deployment&lt;/a&gt; section of the sparklyr website.&lt;/p&gt; &#xA;&lt;h2&gt;Using dplyr&lt;/h2&gt; &#xA;&lt;p&gt;We can now use all of the available dplyr verbs against the tables within the cluster.&lt;/p&gt; &#xA;&lt;p&gt;We’ll start by copying some datasets from R into the Spark cluster (note that you may need to install the nycflights13 and Lahman packages in order to execute this code):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(c(&#34;nycflights13&#34;, &#34;Lahman&#34;))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(dplyr)&#xA;iris_tbl &amp;lt;- copy_to(sc, iris, overwrite = TRUE)&#xA;flights_tbl &amp;lt;- copy_to(sc, nycflights13::flights, &#34;flights&#34;, overwrite = TRUE)&#xA;batting_tbl &amp;lt;- copy_to(sc, Lahman::Batting, &#34;batting&#34;, overwrite = TRUE)&#xA;src_tbls(sc)&#xA;#&amp;gt; [1] &#34;batting&#34; &#34;flights&#34; &#34;iris&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To start with here’s a simple filtering example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# filter by departure delay and print the first few records&#xA;flights_tbl %&amp;gt;% filter(dep_delay == 2)&#xA;#&amp;gt; # Source: spark&amp;lt;?&amp;gt; [?? x 19]&#xA;#&amp;gt;     year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time&#xA;#&amp;gt;    &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;    &amp;lt;int&amp;gt;          &amp;lt;int&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;int&amp;gt;          &amp;lt;int&amp;gt;&#xA;#&amp;gt;  1  2013     1     1      517            515         2      830            819&#xA;#&amp;gt;  2  2013     1     1      542            540         2      923            850&#xA;#&amp;gt;  3  2013     1     1      702            700         2     1058           1014&#xA;#&amp;gt;  4  2013     1     1      715            713         2      911            850&#xA;#&amp;gt;  5  2013     1     1      752            750         2     1025           1029&#xA;#&amp;gt;  6  2013     1     1      917            915         2     1206           1211&#xA;#&amp;gt;  7  2013     1     1      932            930         2     1219           1225&#xA;#&amp;gt;  8  2013     1     1     1028           1026         2     1350           1339&#xA;#&amp;gt;  9  2013     1     1     1042           1040         2     1325           1326&#xA;#&amp;gt; 10  2013     1     1     1231           1229         2     1523           1529&#xA;#&amp;gt; # … with more rows, and 11 more variables: arr_delay &amp;lt;dbl&amp;gt;, carrier &amp;lt;chr&amp;gt;,&#xA;#&amp;gt; #   flight &amp;lt;int&amp;gt;, tailnum &amp;lt;chr&amp;gt;, origin &amp;lt;chr&amp;gt;, dest &amp;lt;chr&amp;gt;, air_time &amp;lt;dbl&amp;gt;,&#xA;#&amp;gt; #   distance &amp;lt;dbl&amp;gt;, hour &amp;lt;dbl&amp;gt;, minute &amp;lt;dbl&amp;gt;, time_hour &amp;lt;dttm&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://spark.rstudio.com/guides/dplyr.html&#34;&gt;Introduction to dplyr&lt;/a&gt; provides additional &lt;code&gt;dplyr&lt;/code&gt; examples you can try. For example, consider the last example from the tutorial which plots data on flight delays:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;delay &amp;lt;- flights_tbl %&amp;gt;%&#xA;  group_by(tailnum) %&amp;gt;%&#xA;  summarise(count = n(), dist = mean(distance), delay = mean(arr_delay)) %&amp;gt;%&#xA;  filter(count &amp;gt; 20, dist &amp;lt; 2000, !is.na(delay)) %&amp;gt;%&#xA;  collect()&#xA;&#xA;# plot delays&#xA;library(ggplot2)&#xA;ggplot(delay, aes(dist, delay)) +&#xA;  geom_point(aes(size = count), alpha = 1/2) +&#xA;  geom_smooth() +&#xA;  scale_size_area(max_size = 2)&#xA;#&amp;gt; `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &#34;cs&#34;)&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/tools/readme/dplyr-ggplot2-1.png&#34; width=&#34;100%&#34;&gt; &#xA;&lt;h3&gt;Window Functions&lt;/h3&gt; &#xA;&lt;p&gt;dplyr &lt;a href=&#34;https://spark.rstudio.com/guides/dplyr.html#grouping&#34;&gt;window functions&lt;/a&gt; are also supported, for example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;batting_tbl %&amp;gt;%&#xA;  select(playerID, yearID, teamID, G, AB:H) %&amp;gt;%&#xA;  arrange(playerID, yearID, teamID) %&amp;gt;%&#xA;  group_by(playerID) %&amp;gt;%&#xA;  filter(min_rank(desc(H)) &amp;lt;= 2 &amp;amp; H &amp;gt; 0)&#xA;#&amp;gt; # Source:     spark&amp;lt;?&amp;gt; [?? x 7]&#xA;#&amp;gt; # Groups:     playerID&#xA;#&amp;gt; # Ordered by: playerID, yearID, teamID&#xA;#&amp;gt;    playerID  yearID teamID     G    AB     R     H&#xA;#&amp;gt;    &amp;lt;chr&amp;gt;      &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;&#xA;#&amp;gt;  1 aaronha01   1959 ML1      154   629   116   223&#xA;#&amp;gt;  2 aaronha01   1963 ML1      161   631   121   201&#xA;#&amp;gt;  3 abbotji01   1999 MIL       20    21     0     2&#xA;#&amp;gt;  4 abnersh01   1992 CHA       97   208    21    58&#xA;#&amp;gt;  5 abnersh01   1990 SDN       91   184    17    45&#xA;#&amp;gt;  6 acklefr01   1963 CHA        2     5     0     1&#xA;#&amp;gt;  7 acklefr01   1964 CHA        3     1     0     1&#xA;#&amp;gt;  8 acunaro01   2019 ATL      156   626   127   175&#xA;#&amp;gt;  9 acunaro01   2018 ATL      111   433    78   127&#xA;#&amp;gt; 10 adamecr01   2016 COL      121   225    25    49&#xA;#&amp;gt; # … with more rows&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For additional documentation on using dplyr with Spark see the &lt;a href=&#34;https://spark.rstudio.com/dplyr.html&#34;&gt;dplyr&lt;/a&gt; section of the sparklyr website.&lt;/p&gt; &#xA;&lt;h2&gt;Using SQL&lt;/h2&gt; &#xA;&lt;p&gt;It’s also possible to execute SQL queries directly against tables within a Spark cluster. The &lt;code&gt;spark_connection&lt;/code&gt; object implements a &lt;a href=&#34;https://github.com/r-dbi/DBI&#34;&gt;DBI&lt;/a&gt; interface for Spark, so you can use &lt;code&gt;dbGetQuery()&lt;/code&gt; to execute SQL and return the result as an R data frame:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(DBI)&#xA;iris_preview &amp;lt;- dbGetQuery(sc, &#34;SELECT * FROM iris LIMIT 10&#34;)&#xA;iris_preview&#xA;#&amp;gt;    Sepal_Length Sepal_Width Petal_Length Petal_Width Species&#xA;#&amp;gt; 1           5.1         3.5          1.4         0.2  setosa&#xA;#&amp;gt; 2           4.9         3.0          1.4         0.2  setosa&#xA;#&amp;gt; 3           4.7         3.2          1.3         0.2  setosa&#xA;#&amp;gt; 4           4.6         3.1          1.5         0.2  setosa&#xA;#&amp;gt; 5           5.0         3.6          1.4         0.2  setosa&#xA;#&amp;gt; 6           5.4         3.9          1.7         0.4  setosa&#xA;#&amp;gt; 7           4.6         3.4          1.4         0.3  setosa&#xA;#&amp;gt; 8           5.0         3.4          1.5         0.2  setosa&#xA;#&amp;gt; 9           4.4         2.9          1.4         0.2  setosa&#xA;#&amp;gt; 10          4.9         3.1          1.5         0.1  setosa&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Machine Learning&lt;/h2&gt; &#xA;&lt;p&gt;You can orchestrate machine learning algorithms in a Spark cluster via the &lt;a href=&#34;https://spark.apache.org/docs/latest/mllib-guide.html&#34;&gt;machine learning&lt;/a&gt; functions within &lt;strong&gt;sparklyr&lt;/strong&gt;. These functions connect to a set of high-level APIs built on top of DataFrames that help you create and tune machine learning workflows.&lt;/p&gt; &#xA;&lt;p&gt;Here’s an example where we use &lt;a href=&#34;https://spark.rstudio.com/packages/sparklyr/latest/reference/ml_linear_regression/&#34;&gt;ml_linear_regression&lt;/a&gt; to fit a linear regression model. We’ll use the built-in &lt;code&gt;mtcars&lt;/code&gt; dataset, and see if we can predict a car’s fuel consumption (&lt;code&gt;mpg&lt;/code&gt;) based on its weight (&lt;code&gt;wt&lt;/code&gt;), and the number of cylinders the engine contains (&lt;code&gt;cyl&lt;/code&gt;). We’ll assume in each case that the relationship between &lt;code&gt;mpg&lt;/code&gt; and each of our features is linear.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# copy mtcars into spark&#xA;mtcars_tbl &amp;lt;- copy_to(sc, mtcars, overwrite = TRUE)&#xA;&#xA;# transform our data set, and then partition into &#39;training&#39;, &#39;test&#39;&#xA;partitions &amp;lt;- mtcars_tbl %&amp;gt;%&#xA;  filter(hp &amp;gt;= 100) %&amp;gt;%&#xA;  mutate(cyl8 = cyl == 8) %&amp;gt;%&#xA;  sdf_partition(training = 0.5, test = 0.5, seed = 1099)&#xA;&#xA;# fit a linear model to the training dataset&#xA;fit &amp;lt;- partitions$training %&amp;gt;%&#xA;  ml_linear_regression(response = &#34;mpg&#34;, features = c(&#34;wt&#34;, &#34;cyl&#34;))&#xA;fit&#xA;#&amp;gt; Formula: mpg ~ wt + cyl&#xA;#&amp;gt; &#xA;#&amp;gt; Coefficients:&#xA;#&amp;gt; (Intercept)          wt         cyl &#xA;#&amp;gt;  37.1464554  -4.3408005  -0.5830515&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For linear regression models produced by Spark, we can use &lt;code&gt;summary()&lt;/code&gt; to learn a bit more about the quality of our fit, and the statistical significance of each of our predictors.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(fit)&#xA;#&amp;gt; Deviance Residuals:&#xA;#&amp;gt;     Min      1Q  Median      3Q     Max &#xA;#&amp;gt; -2.5134 -0.9158 -0.1683  1.1503  2.1534 &#xA;#&amp;gt; &#xA;#&amp;gt; Coefficients:&#xA;#&amp;gt; (Intercept)          wt         cyl &#xA;#&amp;gt;  37.1464554  -4.3408005  -0.5830515 &#xA;#&amp;gt; &#xA;#&amp;gt; R-Squared: 0.9428&#xA;#&amp;gt; Root Mean Squared Error: 1.409&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Spark machine learning supports a wide array of algorithms and feature transformations and as illustrated above it’s easy to chain these functions together with dplyr pipelines. To learn more see the &lt;a href=&#34;https://spark.rstudio.com/mlib/&#34;&gt;machine learning&lt;/a&gt; section.&lt;/p&gt; &#xA;&lt;h2&gt;Reading and Writing Data&lt;/h2&gt; &#xA;&lt;p&gt;You can read and write data in CSV, JSON, and Parquet formats. Data can be stored in HDFS, S3, or on the local filesystem of cluster nodes.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;temp_csv &amp;lt;- tempfile(fileext = &#34;.csv&#34;)&#xA;temp_parquet &amp;lt;- tempfile(fileext = &#34;.parquet&#34;)&#xA;temp_json &amp;lt;- tempfile(fileext = &#34;.json&#34;)&#xA;&#xA;spark_write_csv(iris_tbl, temp_csv)&#xA;iris_csv_tbl &amp;lt;- spark_read_csv(sc, &#34;iris_csv&#34;, temp_csv)&#xA;&#xA;spark_write_parquet(iris_tbl, temp_parquet)&#xA;iris_parquet_tbl &amp;lt;- spark_read_parquet(sc, &#34;iris_parquet&#34;, temp_parquet)&#xA;&#xA;spark_write_json(iris_tbl, temp_json)&#xA;iris_json_tbl &amp;lt;- spark_read_json(sc, &#34;iris_json&#34;, temp_json)&#xA;&#xA;src_tbls(sc)&#xA;#&amp;gt; [1] &#34;batting&#34;      &#34;flights&#34;      &#34;iris&#34;         &#34;iris_csv&#34;     &#34;iris_json&#34;   &#xA;#&amp;gt; [6] &#34;iris_parquet&#34; &#34;mtcars&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Distributed R&lt;/h2&gt; &#xA;&lt;p&gt;You can execute arbitrary r code across your cluster using &lt;code&gt;spark_apply()&lt;/code&gt;. For example, we can apply &lt;code&gt;rgamma&lt;/code&gt; over &lt;code&gt;iris&lt;/code&gt; as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;spark_apply(iris_tbl, function(data) {&#xA;  data[1:4] + rgamma(1,2)&#xA;})&#xA;#&amp;gt; # Source: spark&amp;lt;?&amp;gt; [?? x 4]&#xA;#&amp;gt;    Sepal_Length Sepal_Width Petal_Length Petal_Width&#xA;#&amp;gt;           &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;&#xA;#&amp;gt;  1         6.45        4.85         2.75        1.55&#xA;#&amp;gt;  2         6.25        4.35         2.75        1.55&#xA;#&amp;gt;  3         6.05        4.55         2.65        1.55&#xA;#&amp;gt;  4         5.95        4.45         2.85        1.55&#xA;#&amp;gt;  5         6.35        4.95         2.75        1.55&#xA;#&amp;gt;  6         6.75        5.25         3.05        1.75&#xA;#&amp;gt;  7         5.95        4.75         2.75        1.65&#xA;#&amp;gt;  8         6.35        4.75         2.85        1.55&#xA;#&amp;gt;  9         5.75        4.25         2.75        1.55&#xA;#&amp;gt; 10         6.25        4.45         2.85        1.45&#xA;#&amp;gt; # … with more rows&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also group by columns to perform an operation over each group of rows and make use of any package within the closure:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;spark_apply(&#xA;  iris_tbl,&#xA;  function(e) broom::tidy(lm(Petal_Width ~ Petal_Length, e)),&#xA;  columns = c(&#34;term&#34;, &#34;estimate&#34;, &#34;std.error&#34;, &#34;statistic&#34;, &#34;p.value&#34;),&#xA;  group_by = &#34;Species&#34;&#xA;)&#xA;#&amp;gt; # Source: spark&amp;lt;?&amp;gt; [?? x 6]&#xA;#&amp;gt;   Species    term         estimate std.error statistic  p.value&#xA;#&amp;gt;   &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;           &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;&#xA;#&amp;gt; 1 versicolor (Intercept)   -0.0843    0.161     -0.525 6.02e- 1&#xA;#&amp;gt; 2 versicolor Petal_Length   0.331     0.0375     8.83  1.27e-11&#xA;#&amp;gt; 3 virginica  (Intercept)    1.14      0.379      2.99  4.34e- 3&#xA;#&amp;gt; 4 virginica  Petal_Length   0.160     0.0680     2.36  2.25e- 2&#xA;#&amp;gt; 5 setosa     (Intercept)   -0.0482    0.122     -0.396 6.94e- 1&#xA;#&amp;gt; 6 setosa     Petal_Length   0.201     0.0826     2.44  1.86e- 2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Extensions&lt;/h2&gt; &#xA;&lt;p&gt;The facilities used internally by sparklyr for its &lt;code&gt;dplyr&lt;/code&gt; and machine learning interfaces are available to extension packages. Since Spark is a general purpose cluster computing system there are many potential applications for extensions (e.g.&amp;nbsp;interfaces to custom machine learning pipelines, interfaces to 3rd party Spark packages, etc.).&lt;/p&gt; &#xA;&lt;p&gt;Here’s a simple example that wraps a Spark text file line counting function with an R function:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# write a CSV&#xA;tempfile &amp;lt;- tempfile(fileext = &#34;.csv&#34;)&#xA;write.csv(nycflights13::flights, tempfile, row.names = FALSE, na = &#34;&#34;)&#xA;&#xA;# define an R interface to Spark line counting&#xA;count_lines &amp;lt;- function(sc, path) {&#xA;  spark_context(sc) %&amp;gt;%&#xA;    invoke(&#34;textFile&#34;, path, 1L) %&amp;gt;%&#xA;      invoke(&#34;count&#34;)&#xA;}&#xA;&#xA;# call spark to count the lines of the CSV&#xA;count_lines(sc, tempfile)&#xA;#&amp;gt; [1] 336777&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To learn more about creating extensions see the &lt;a href=&#34;https://spark.rstudio.com/guides/extensions.html&#34;&gt;Extensions&lt;/a&gt; section of the sparklyr website.&lt;/p&gt; &#xA;&lt;h2&gt;Table Utilities&lt;/h2&gt; &#xA;&lt;p&gt;You can cache a table into memory with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;tbl_cache(sc, &#34;batting&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and unload from memory using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;tbl_uncache(sc, &#34;batting&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Connection Utilities&lt;/h2&gt; &#xA;&lt;p&gt;You can view the Spark web console using the &lt;code&gt;spark_web()&lt;/code&gt; function:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;spark_web(sc)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can show the log using the &lt;code&gt;spark_log()&lt;/code&gt; function:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;spark_log(sc, n = 10)&#xA;#&amp;gt; 22/05/25 15:05:25 INFO BlockManagerInfo: Removed broadcast_84_piece0 on localhost:58163 in memory (size: 9.2 KiB, free: 912.1 MiB)&#xA;#&amp;gt; 22/05/25 15:05:25 INFO BlockManagerInfo: Removed broadcast_87_piece0 on localhost:58163 in memory (size: 18.4 KiB, free: 912.1 MiB)&#xA;#&amp;gt; 22/05/25 15:05:25 INFO BlockManagerInfo: Removed broadcast_77_piece0 on localhost:58163 in memory (size: 16.7 KiB, free: 912.1 MiB)&#xA;#&amp;gt; 22/05/25 15:05:25 INFO Executor: Finished task 0.0 in stage 67.0 (TID 83). 1004 bytes result sent to driver&#xA;#&amp;gt; 22/05/25 15:05:25 INFO TaskSetManager: Finished task 0.0 in stage 67.0 (TID 83) in 244 ms on localhost (executor driver) (1/1)&#xA;#&amp;gt; 22/05/25 15:05:25 INFO TaskSchedulerImpl: Removed TaskSet 67.0, whose tasks have all completed, from pool &#xA;#&amp;gt; 22/05/25 15:05:25 INFO DAGScheduler: ResultStage 67 (count at NativeMethodAccessorImpl.java:0) finished in 0.259 s&#xA;#&amp;gt; 22/05/25 15:05:25 INFO DAGScheduler: Job 49 is finished. Cancelling potential speculative or zombie tasks for this job&#xA;#&amp;gt; 22/05/25 15:05:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 67: Stage finished&#xA;#&amp;gt; 22/05/25 15:05:25 INFO DAGScheduler: Job 49 finished: count at NativeMethodAccessorImpl.java:0, took 0.268655 s&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, we disconnect from Spark:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;  spark_disconnect(sc)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;RStudio IDE&lt;/h2&gt; &#xA;&lt;p&gt;The latest RStudio &lt;a href=&#34;https://www.rstudio.com/products/rstudio/download/preview/&#34;&gt;Preview Release&lt;/a&gt; of the RStudio IDE includes integrated support for Spark and the sparklyr package, including tools for:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Creating and managing Spark connections&lt;/li&gt; &#xA; &lt;li&gt;Browsing the tables and columns of Spark DataFrames&lt;/li&gt; &#xA; &lt;li&gt;Previewing the first 1,000 rows of Spark DataFrames&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Once you’ve installed the sparklyr package, you should find a new &lt;strong&gt;Spark&lt;/strong&gt; pane within the IDE. This pane includes a &lt;strong&gt;New Connection&lt;/strong&gt; dialog which can be used to make connections to local or remote Spark instances:&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/tools/readme/spark-connect.png&#34; class=&#34;screenshot&#34; width=&#34;389&#34;&gt; &#xA;&lt;p&gt;Once you’ve connected to Spark you’ll be able to browse the tables contained within the Spark cluster and preview Spark DataFrames using the standard RStudio data viewer:&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/tools/readme/spark-dataview.png&#34; class=&#34;screenshot&#34; width=&#34;639&#34;&gt; &#xA;&lt;p&gt;You can also connect to Spark through &lt;a href=&#34;https://livy.apache.org/&#34;&gt;Livy&lt;/a&gt; through a new connection dialog:&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/tools/readme/spark-connect-livy.png&#34; class=&#34;screenshot&#34; width=&#34;389&#34;&gt; &#xA;&lt;div style=&#34;margin-bottom: 15px;&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;The RStudio IDE features for sparklyr are available now as part of the &lt;a href=&#34;https://www.rstudio.com/products/rstudio/download/preview/&#34;&gt;RStudio Preview Release&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Using H2O&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=rsparkling&#34;&gt;rsparkling&lt;/a&gt; is a CRAN package from &lt;a href=&#34;https://h2o.ai/&#34;&gt;H2O&lt;/a&gt; that extends &lt;a href=&#34;https://spark.rstudio.com/&#34;&gt;sparklyr&lt;/a&gt; to provide an interface into &lt;a href=&#34;https://github.com/h2oai/sparkling-water&#34;&gt;Sparkling Water&lt;/a&gt;. For instance, the following example installs, configures and runs &lt;a href=&#34;https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/glm.html&#34;&gt;h2o.glm&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rsparkling)&#xA;library(sparklyr)&#xA;library(dplyr)&#xA;library(h2o)&#xA;&#xA;sc &amp;lt;- spark_connect(master = &#34;local&#34;, version = &#34;2.3.2&#34;)&#xA;mtcars_tbl &amp;lt;- copy_to(sc, mtcars, &#34;mtcars&#34;, overwrite = TRUE)&#xA;&#xA;mtcars_h2o &amp;lt;- as_h2o_frame(sc, mtcars_tbl, strict_version_check = FALSE)&#xA;&#xA;mtcars_glm &amp;lt;- h2o.glm(x = c(&#34;wt&#34;, &#34;cyl&#34;),&#xA;                      y = &#34;mpg&#34;,&#xA;                      training_frame = mtcars_h2o,&#xA;                      lambda_search = TRUE)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mtcars_glm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;#&amp;gt; Model Details:&#xA;#&amp;gt; ==============&#xA;#&amp;gt;&#xA;#&amp;gt; H2ORegressionModel: glm&#xA;#&amp;gt; Model ID:  GLM_model_R_1527265202599_1&#xA;#&amp;gt; GLM Model: summary&#xA;#&amp;gt;     family     link                              regularization&#xA;#&amp;gt; 1 gaussian identity Elastic Net (alpha = 0.5, lambda = 0.1013 )&#xA;#&amp;gt;                                                                lambda_search&#xA;#&amp;gt; 1 nlambda = 100, lambda.max = 10.132, lambda.min = 0.1013, lambda.1se = -1.0&#xA;#&amp;gt;   number_of_predictors_total number_of_active_predictors&#xA;#&amp;gt; 1                          2                           2&#xA;#&amp;gt;   number_of_iterations                                training_frame&#xA;#&amp;gt; 1                  100 frame_rdd_31_ad5c4e88ec97eb8ccedae9475ad34e02&#xA;#&amp;gt;&#xA;#&amp;gt; Coefficients: glm coefficients&#xA;#&amp;gt;       names coefficients standardized_coefficients&#xA;#&amp;gt; 1 Intercept    38.941654                 20.090625&#xA;#&amp;gt; 2       cyl    -1.468783                 -2.623132&#xA;#&amp;gt; 3        wt    -3.034558                 -2.969186&#xA;#&amp;gt;&#xA;#&amp;gt; H2ORegressionMetrics: glm&#xA;#&amp;gt; ** Reported on training data. **&#xA;#&amp;gt;&#xA;#&amp;gt; MSE:  6.017684&#xA;#&amp;gt; RMSE:  2.453097&#xA;#&amp;gt; MAE:  1.940985&#xA;#&amp;gt; RMSLE:  0.1114801&#xA;#&amp;gt; Mean Residual Deviance :  6.017684&#xA;#&amp;gt; R^2 :  0.8289895&#xA;#&amp;gt; Null Deviance :1126.047&#xA;#&amp;gt; Null D.o.F. :31&#xA;#&amp;gt; Residual Deviance :192.5659&#xA;#&amp;gt; Residual D.o.F. :29&#xA;#&amp;gt; AIC :156.2425&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;spark_disconnect(sc)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Connecting through Livy&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/cloudera/livy&#34;&gt;Livy&lt;/a&gt; enables remote connections to Apache Spark clusters. However, please notice that connecting to Spark clusters through Livy is much slower than any other connection method.&lt;/p&gt; &#xA;&lt;p&gt;Before connecting to Livy, you will need the connection information to an existing service running Livy. Otherwise, to test &lt;code&gt;livy&lt;/code&gt; in your local environment, you can install it and run it locally as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;livy_install()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;livy_service_start()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To connect, use the Livy service address as &lt;code&gt;master&lt;/code&gt; and &lt;code&gt;method = &#34;livy&#34;&lt;/code&gt; in &lt;code&gt;spark_connect()&lt;/code&gt;. Once connection completes, use &lt;code&gt;sparklyr&lt;/code&gt; as usual, for instance:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sc &amp;lt;- spark_connect(master = &#34;http://localhost:8998&#34;, method = &#34;livy&#34;, version = &#34;3.0.0&#34;)&#xA;copy_to(sc, iris, overwrite = TRUE)&#xA;&#xA;spark_disconnect(sc)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once you are done using &lt;code&gt;livy&lt;/code&gt; locally, you should stop this service with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;livy_service_stop()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To connect to remote &lt;code&gt;livy&lt;/code&gt; clusters that support basic authentication connect as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;config &amp;lt;- livy_config(username=&#34;&amp;lt;username&amp;gt;&#34;, password=&#34;&amp;lt;password&amp;gt;&#34;)&#xA;sc &amp;lt;- spark_connect(master = &#34;&amp;lt;address&amp;gt;&#34;, method = &#34;livy&#34;, config = config)&#xA;spark_disconnect(sc)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Connecting through Databricks Connect&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.databricks.com/dev-tools/databricks-connect.html#databricks-connect&#34;&gt;Databricks Connect&lt;/a&gt; allows you to connect sparklyr to a remote Databricks Cluster. You can install &lt;a href=&#34;https://pypi.org/project/databricks-connect/&#34;&gt;Databricks Connect python package&lt;/a&gt; and use it to submit Spark jobs written in sparklyr APIs and have them execute remotely on a Databricks cluster instead of in the local Spark session.&lt;/p&gt; &#xA;&lt;p&gt;To use sparklyr with Databricks Connect first launch a Cluster on Databricks. Then follow &lt;a href=&#34;https://docs.databricks.com/dev-tools/databricks-connect.html#client-setup&#34;&gt;these instructions&lt;/a&gt; to setup the client:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Make sure pyspark is not installed&lt;/li&gt; &#xA; &lt;li&gt;Install the Databricks Connect python package. The latest supported version is 6.4.1.&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;databricks-connect configure&lt;/code&gt; and provide the configuration information &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Databricks account URL of the form &lt;code&gt;https://&amp;lt;account&amp;gt;.cloud.databricks.com&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://docs.databricks.com/dev-tools/api/latest/authentication.html#token-management&#34;&gt;User token&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Cluster ID&lt;/li&gt; &#xA;   &lt;li&gt;Port (default port number is &lt;code&gt;15001&lt;/code&gt;)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;To configure &lt;code&gt;sparklyr&lt;/code&gt; with Databricks Connect, set the following environment variables:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export SPARK_VERSION=2.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now simply create a spark connection as follows&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;spark_home &amp;lt;- system(&#34;databricks-connect get-spark-home&#34;)&#xA;sc &amp;lt;- spark_connect(method = &#34;databricks&#34;,&#xA;                    spark_home = spark_home)&#xA;copy_to(sc, iris, overwrite = TRUE)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/tools/readme/databricks-connect.png&#34; class=&#34;screenshot&#34; width=&#34;750&#34;&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;spark_disconnect(sc)&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>Rdatatable/data.table</title>
    <updated>2022-06-02T02:21:48Z</updated>
    <id>tag:github.com,2022-06-02:/Rdatatable/data.table</id>
    <link href="https://github.com/Rdatatable/data.table" rel="alternate"></link>
    <summary type="html">&lt;p&gt;R&#39;s data.table package extends data.frame:&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;data.table &lt;a href=&#34;https://r-datatable.com&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Rdatatable/data.table/master/.graphics/logo.png&#34; align=&#34;right&#34; height=&#34;140&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;!-- badges: start --&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/web/checks/check_results_data.table.html&#34;&gt;&lt;img src=&#34;https://cranchecks.info/badges/flavor/release/data.table&#34; alt=&#34;CRAN status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Rdatatable/data.table/actions&#34;&gt;&lt;img src=&#34;https://github.com/Rdatatable/data.table/workflows/R-CMD-check/badge.svg?sanitize=true&#34; alt=&#34;R-CMD-check&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ci.appveyor.com/project/Rdatatable/data-table&#34;&gt;&lt;img src=&#34;https://ci.appveyor.com/api/projects/status/kayjdh5qtgymhoxr/branch/master?svg=true&#34; alt=&#34;AppVeyor build status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/github/Rdatatable/data.table?branch=master&#34;&gt;&lt;img src=&#34;https://codecov.io/github/Rdatatable/data.table/coverage.svg?branch=master&#34; alt=&#34;Codecov test coverage&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitlab.com/Rdatatable/data.table/-/pipelines&#34;&gt;&lt;img src=&#34;https://gitlab.com/Rdatatable/data.table/badges/master/pipeline.svg?sanitize=true&#34; alt=&#34;GitLab CI build status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.rdocumentation.org/trends&#34;&gt;&lt;img src=&#34;https://cranlogs.r-pkg.org/badges/data.table&#34; alt=&#34;downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitlab.com/jangorecki/rdeps&#34;&gt;&lt;img src=&#34;https://jangorecki.gitlab.io/rdeps/data.table/CRAN_usage.svg?sanitize=true&#34; alt=&#34;CRAN usage&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitlab.com/jangorecki/rdeps&#34;&gt;&lt;img src=&#34;https://jangorecki.gitlab.io/rdeps/data.table/BioC_usage.svg?sanitize=true&#34; alt=&#34;BioC usage&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitlab.com/jangorecki/rdeps&#34;&gt;&lt;img src=&#34;https://jangorecki.gitlab.io/rdeps/data.table/indirect_usage.svg?sanitize=true&#34; alt=&#34;indirect usage&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- badges: end --&gt; &#xA;&lt;p&gt;&lt;code&gt;data.table&lt;/code&gt; provides a high-performance version of &lt;a href=&#34;https://www.r-project.org/about.html&#34;&gt;base R&lt;/a&gt;&#39;s &lt;code&gt;data.frame&lt;/code&gt; with syntax and feature enhancements for ease of use, convenience and programming speed.&lt;/p&gt; &#xA;&lt;h2&gt;Why &lt;code&gt;data.table&lt;/code&gt;?&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;concise syntax: fast to type, fast to read&lt;/li&gt; &#xA; &lt;li&gt;fast speed&lt;/li&gt; &#xA; &lt;li&gt;memory efficient&lt;/li&gt; &#xA; &lt;li&gt;careful API lifecycle management&lt;/li&gt; &#xA; &lt;li&gt;community&lt;/li&gt; &#xA; &lt;li&gt;feature rich&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;fast and friendly delimited &lt;strong&gt;file reader&lt;/strong&gt;: &lt;strong&gt;&lt;a href=&#34;https://rdatatable.gitlab.io/data.table/reference/fread.html&#34;&gt;&lt;code&gt;?fread&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;, see also &lt;a href=&#34;https://github.com/Rdatatable/data.table/wiki/Convenience-features-of-fread&#34;&gt;convenience features for &lt;em&gt;small&lt;/em&gt; data&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;fast and feature rich delimited &lt;strong&gt;file writer&lt;/strong&gt;: &lt;strong&gt;&lt;a href=&#34;https://rdatatable.gitlab.io/data.table/reference/fwrite.html&#34;&gt;&lt;code&gt;?fwrite&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;low-level &lt;strong&gt;parallelism&lt;/strong&gt;: many common operations are internally parallelized to use multiple CPU threads&lt;/li&gt; &#xA; &lt;li&gt;fast and scalable aggregations; e.g. 100GB in RAM (see &lt;a href=&#34;https://h2oai.github.io/db-benchmark/&#34;&gt;benchmarks&lt;/a&gt; on up to &lt;strong&gt;two billion rows&lt;/strong&gt;)&lt;/li&gt; &#xA; &lt;li&gt;fast and feature rich joins: &lt;strong&gt;ordered joins&lt;/strong&gt; (e.g. rolling forwards, backwards, nearest and limited staleness), &lt;strong&gt;&lt;a href=&#34;https://github.com/Rdatatable/data.table/wiki/talks/EARL2014_OverlapRangeJoin_Arun.pdf&#34;&gt;overlapping range joins&lt;/a&gt;&lt;/strong&gt; (similar to &lt;code&gt;IRanges::findOverlaps&lt;/code&gt;), &lt;strong&gt;&lt;a href=&#34;https://github.com/Rdatatable/data.table/wiki/talks/ArunSrinivasanUseR2016.pdf&#34;&gt;non-equi joins&lt;/a&gt;&lt;/strong&gt; (i.e. joins using operators &lt;code&gt;&amp;gt;, &amp;gt;=, &amp;lt;, &amp;lt;=&lt;/code&gt;), &lt;strong&gt;aggregate on join&lt;/strong&gt; (&lt;code&gt;by=.EACHI&lt;/code&gt;), &lt;strong&gt;update on join&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;fast add/update/delete columns &lt;strong&gt;by reference&lt;/strong&gt; by group using no copies at all&lt;/li&gt; &#xA; &lt;li&gt;fast and feature rich &lt;strong&gt;reshaping&lt;/strong&gt; data: &lt;strong&gt;&lt;a href=&#34;https://rdatatable.gitlab.io/data.table/reference/dcast.data.table.html&#34;&gt;&lt;code&gt;?dcast&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; (&lt;em&gt;pivot/wider/spread&lt;/em&gt;) and &lt;strong&gt;&lt;a href=&#34;https://rdatatable.gitlab.io/data.table/reference/melt.data.table.html&#34;&gt;&lt;code&gt;?melt&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; (&lt;em&gt;unpivot/longer/gather&lt;/em&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;any R function from any R package&lt;/strong&gt; can be used in queries not just the subset of functions made available by a database backend, also columns of type &lt;code&gt;list&lt;/code&gt; are supported&lt;/li&gt; &#xA; &lt;li&gt;has &lt;strong&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Dependency_hell&#34;&gt;no dependencies&lt;/a&gt;&lt;/strong&gt; at all other than base R itself, for simpler production/maintenance&lt;/li&gt; &#xA; &lt;li&gt;the R dependency is &lt;strong&gt;as old as possible for as long as possible&lt;/strong&gt;, dated April 2014, and we continuously test against that version; e.g. v1.11.0 released on 5 May 2018 bumped the dependency up from 5 year old R 3.0.0 to 4 year old R 3.1.0&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&#34;data.table&#34;)&#xA;&#xA;# latest development version:&#xA;data.table::update.dev.pkg()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/Rdatatable/data.table/wiki/Installation&#34;&gt;the Installation wiki&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Use &lt;code&gt;data.table&lt;/code&gt; subset &lt;code&gt;[&lt;/code&gt; operator the same way you would use &lt;code&gt;data.frame&lt;/code&gt; one, but...&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;no need to prefix each column with &lt;code&gt;DT$&lt;/code&gt; (like &lt;code&gt;subset()&lt;/code&gt; and &lt;code&gt;with()&lt;/code&gt; but built-in)&lt;/li&gt; &#xA; &lt;li&gt;any R expression using any package is allowed in &lt;code&gt;j&lt;/code&gt; argument, not just list of columns&lt;/li&gt; &#xA; &lt;li&gt;extra argument &lt;code&gt;by&lt;/code&gt; to compute &lt;code&gt;j&lt;/code&gt; expression by group&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(data.table)&#xA;DT = as.data.table(iris)&#xA;&#xA;# FROM[WHERE, SELECT, GROUP BY]&#xA;# DT  [i,     j,      by]&#xA;&#xA;DT[Petal.Width &amp;gt; 1.0, mean(Petal.Length), by = Species]&#xA;#      Species       V1&#xA;#1: versicolor 4.362791&#xA;#2:  virginica 5.552000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Getting started&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cran.r-project.org/package=data.table/vignettes/datatable-intro.html&#34;&gt;Introduction to data.table&lt;/a&gt; vignette&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Rdatatable/data.table/wiki/Getting-started&#34;&gt;Getting started&lt;/a&gt; wiki page&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://rdatatable.gitlab.io/data.table/reference/data.table.html#examples&#34;&gt;Examples&lt;/a&gt; produced by &lt;code&gt;example(data.table)&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Cheatsheets&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rstudio/cheatsheets/master/datatable.pdf&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rstudio/cheatsheets/master/pngs/datatable.png&#34; width=&#34;615&#34; height=&#34;242&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;data.table&lt;/code&gt; is widely used by the R community. It is being directly used by hundreds of CRAN and Bioconductor packages, and indirectly by thousands. It is one of the &lt;a href=&#34;https://www.r-pkg.org/starred&#34;&gt;top most starred&lt;/a&gt; R packages on GitHub, and was highly rated by the &lt;a href=&#34;http://depsy.org/package/r/data.table&#34;&gt;Depsy project&lt;/a&gt;. If you need help, the &lt;code&gt;data.table&lt;/code&gt; community is active on &lt;a href=&#34;https://stackoverflow.com/questions/tagged/data.table&#34;&gt;StackOverflow&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Stay up-to-date&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;click the &lt;strong&gt;Watch&lt;/strong&gt; button at the top and right of GitHub project page&lt;/li&gt; &#xA; &lt;li&gt;read &lt;a href=&#34;https://github.com/Rdatatable/data.table/raw/master/NEWS.md&#34;&gt;NEWS file&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;follow &lt;a href=&#34;https://twitter.com/hashtag/rdatatable&#34;&gt;#rdatatable&lt;/a&gt; on twitter&lt;/li&gt; &#xA; &lt;li&gt;watch recent &lt;a href=&#34;https://github.com/Rdatatable/data.table/wiki/Presentations&#34;&gt;Presentations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;read recent &lt;a href=&#34;https://github.com/Rdatatable/data.table/wiki/Articles&#34;&gt;Articles&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Contributing&lt;/h3&gt; &#xA;&lt;p&gt;Guidelines for filing issues / pull requests: &lt;a href=&#34;https://github.com/Rdatatable/data.table/wiki/Contributing&#34;&gt;Contribution Guidelines&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>