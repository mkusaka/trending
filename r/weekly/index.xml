<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub R Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-12T01:58:38Z</updated>
  <subtitle>Weekly Trending of R in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>tidyverse/tibble</title>
    <updated>2023-03-12T01:58:38Z</updated>
    <id>tag:github.com,2023-03-12:/tidyverse/tibble</id>
    <link href="https://github.com/tidyverse/tibble" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A modern re-imagining of the data frame&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;tibble &lt;img src=&#34;https://raw.githubusercontent.com/tidyverse/tibble/main/man/figures/logo.png&#34; align=&#34;right&#34;&gt;&lt;/h1&gt; &#xA;&lt;!-- badges: start --&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/tidyverse/tibble/actions&#34;&gt;&lt;img src=&#34;https://github.com/tidyverse/tibble/workflows/R-CMD-check/badge.svg?sanitize=true&#34; alt=&#34;R-CMD-check&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://app.codecov.io/gh/tidyverse/tibble?branch=main&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/tidyverse/tibble/branch/main/graph/badge.svg?sanitize=true&#34; alt=&#34;Codecov test coverage&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://cran.r-project.org/package=tibble&#34;&gt;&lt;img src=&#34;https://www.r-pkg.org/badges/version/tibble&#34; alt=&#34;CRAN_Status_Badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://lifecycle.r-lib.org/articles/stages.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/lifecycle-stable-brightgreen.svg?sanitize=true&#34; alt=&#34;Life cycle&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- badges: end --&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;A &lt;strong&gt;tibble&lt;/strong&gt;, or &lt;code&gt;tbl_df&lt;/code&gt;, is a modern reimagining of the data.frame, keeping what time has proven to be effective, and throwing out what is not. Tibbles are data.frames that are lazy and surly: they do less (i.e.&amp;nbsp;they don’t change variable names or types, and don’t do partial matching) and complain more (e.g.&amp;nbsp;when a variable does not exist). This forces you to confront problems earlier, typically leading to cleaner, more expressive code. Tibbles also have an enhanced &lt;a href=&#34;https://rdrr.io/r/base/print.html&#34;&gt;&lt;code&gt;print()&lt;/code&gt;&lt;/a&gt; method which makes them easier to use with large datasets containing complex objects.&lt;/p&gt; &#xA;&lt;p&gt;If you are new to tibbles, the best place to start is the &lt;a href=&#34;https://r4ds.had.co.nz/tibbles.html&#34;&gt;tibbles chapter&lt;/a&gt; in &lt;em&gt;R for data science&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre class=&#34;chroma&#34;&gt;&#xA;&lt;span&gt;&lt;span class=&#34;c&#34;&gt;# The easiest way to get tibble is to install the whole tidyverse:&lt;/span&gt;&lt;/span&gt;&#xA;&lt;span&gt;&lt;span class=&#34;nf&#34;&gt;&lt;a href=&#34;https://rdrr.io/r/utils/install.packages.html&#34;&gt;install.packages&lt;/a&gt;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&#34;tidyverse&#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&#xA;&lt;span&gt;&lt;/span&gt;&#xA;&lt;span&gt;&lt;span class=&#34;c&#34;&gt;# Alternatively, install just tibble:&lt;/span&gt;&lt;/span&gt;&#xA;&lt;span&gt;&lt;span class=&#34;nf&#34;&gt;&lt;a href=&#34;https://rdrr.io/r/utils/install.packages.html&#34;&gt;install.packages&lt;/a&gt;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&#34;tibble&#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&#xA;&lt;span&gt;&lt;/span&gt;&#xA;&lt;span&gt;&lt;span class=&#34;c&#34;&gt;# Or the the development version from GitHub:&lt;/span&gt;&lt;/span&gt;&#xA;&lt;span&gt;&lt;span class=&#34;c&#34;&gt;# install.packages(&#34;devtools&#34;)&lt;/span&gt;&lt;/span&gt;&#xA;&lt;span&gt;&lt;span class=&#34;nf&#34;&gt;devtools&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;&lt;a href=&#34;https://remotes.r-lib.org/reference/install_github.html&#34;&gt;install_github&lt;/a&gt;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&#34;tidyverse/tibble&#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;pre class=&#34;chroma&#34;&gt;&#xA;&lt;span&gt;&lt;span class=&#34;kr&#34;&gt;&lt;a href=&#34;https://rdrr.io/r/base/library.html&#34;&gt;library&lt;/a&gt;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;&lt;a href=&#34;https://tibble.tidyverse.org/&#34;&gt;tibble&lt;/a&gt;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Create a tibble from an existing object with &lt;a href=&#34;https://tibble.tidyverse.org/reference/as_tibble.html&#34;&gt;&lt;code&gt;as_tibble()&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre class=&#34;chroma&#34;&gt;&#xA;&lt;span&gt;&lt;span class=&#34;nv&#34;&gt;data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;&lt;a href=&#34;https://rdrr.io/r/base/data.frame.html&#34;&gt;data.frame&lt;/a&gt;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;a &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;3&lt;/span&gt;, b &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;letters&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;, c &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;&lt;a href=&#34;https://rdrr.io/r/base/Sys.time.html&#34;&gt;Sys.Date&lt;/a&gt;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&#xA;&lt;span&gt;&lt;span class=&#34;nv&#34;&gt;data&lt;/span&gt;&lt;/span&gt;&#xA;&lt;span&gt;&lt;span class=&#34;c&#34;&gt;#&amp;gt; a b c&lt;/span&gt;&lt;/span&gt;&#xA;&lt;span&gt;&lt;span class=&#34;c&#34;&gt;#&amp;gt; 1 1 a 2023-02-21&lt;/span&gt;&lt;/span&gt;&#xA;&lt;span&gt;&lt;span class=&#34;c&#34;&gt;#&amp;gt; 2 2 b 2023-02-20&lt;/span&gt;&lt;/span&gt;&#xA;&lt;span&gt;&lt;span class=&#34;c&#34;&gt;#&amp;gt; 3 3 c 2023-02-19&lt;/span&gt;&lt;/span&gt;&#xA;&lt;span&gt;&lt;/span&gt;&#xA;&lt;span&gt;&lt;span class=&#34;nf&#34;&gt;&lt;a href=&#34;https://tibble.tidyverse.org/reference/as_tibble.html&#34;&gt;as_tibble&lt;/a&gt;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&#xA;&lt;span&gt;&lt;span class=&#34;c&#34;&gt;#&amp;gt; &lt;span style=&#34;color: #555555;&#34;&gt;# A tibble: 3 × 3&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&#xA;&lt;span&gt;&lt;span class=&#34;c&#34;&gt;#&amp;gt; &lt;span style=&#34;font-weight: bold;&#34;&gt;a&lt;/span&gt; &lt;span style=&#34;font-weight: bold;&#34;&gt;b&lt;/span&gt; &lt;span style=&#34;font-weight: bold;&#34;&gt;c&lt;/span&gt; &lt;/span&gt;&lt;/span&gt;&#xA;&lt;span&gt;&lt;span class=&#34;c&#34;&gt;#&amp;gt; &lt;span style=&#34;color: #555555; font-style: italic;&#34;&gt;&amp;lt;int&amp;gt;&lt;/span&gt; &lt;span style=&#34;color: #555555; font-style: italic;&#34;&gt;&amp;lt;chr&amp;gt;&lt;/span&gt; &lt;span style=&#34;color: #555555; font-style: italic;&#34;&gt;&amp;lt;date&amp;gt;&lt;/span&gt; &lt;/span&gt;&lt;/span&gt;&#xA;&lt;span&gt;&lt;span class=&#34;c&#34;&gt;#&amp;gt; &lt;span style=&#34;color: #555555;&#34;&gt;1&lt;/span&gt; 1 a 2023-02-21&lt;/span&gt;&lt;/span&gt;&#xA;&lt;span&gt;&lt;span class=&#34;c&#34;&gt;#&amp;gt; &lt;span style=&#34;color: #555555;&#34;&gt;2&lt;/span&gt; 2 b 2023-02-20&lt;/span&gt;&lt;/span&gt;&#xA;&lt;span&gt;&lt;span class=&#34;c&#34;&gt;#&amp;gt; &lt;span style=&#34;color: #555555;&#34;&gt;3&lt;/span&gt; 3 c 2023-02-19&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will work for reasonable inputs that are already data.frames, lists, matrices, or tables.&lt;/p&gt; &#xA;&lt;p&gt;You can also create a new tibble from column vectors with &lt;a href=&#34;https://tibble.tidyverse.org/reference/tibble.html&#34;&gt;&lt;code&gt;tibble()&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre class=&#34;chroma&#34;&gt;&#xA;&lt;span&gt;&lt;span class=&#34;nf&#34;&gt;&lt;a href=&#34;https://tibble.tidyverse.org/reference/tibble.html&#34;&gt;tibble&lt;/a&gt;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;x &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;5&lt;/span&gt;, y &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;, z &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;^&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&#xA;&lt;span&gt;&lt;span class=&#34;c&#34;&gt;#&amp;gt; &lt;span style=&#34;color: #555555;&#34;&gt;# A tibble: 5 × 3&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&#xA;&lt;span&gt;&lt;span class=&#34;c&#34;&gt;#&amp;gt; &lt;span style=&#34;font-weight: bold;&#34;&gt;x&lt;/span&gt; &lt;span style=&#34;font-weight: bold;&#34;&gt;y&lt;/span&gt; &lt;span style=&#34;font-weight: bold;&#34;&gt;z&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&#xA;&lt;span&gt;&lt;span class=&#34;c&#34;&gt;#&amp;gt; &lt;span style=&#34;color: #555555; font-style: italic;&#34;&gt;&amp;lt;int&amp;gt;&lt;/span&gt; &lt;span style=&#34;color: #555555; font-style: italic;&#34;&gt;&amp;lt;dbl&amp;gt;&lt;/span&gt; &lt;span style=&#34;color: #555555; font-style: italic;&#34;&gt;&amp;lt;dbl&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&#xA;&lt;span&gt;&lt;span class=&#34;c&#34;&gt;#&amp;gt; &lt;span style=&#34;color: #555555;&#34;&gt;1&lt;/span&gt; 1 1 2&lt;/span&gt;&lt;/span&gt;&#xA;&lt;span&gt;&lt;span class=&#34;c&#34;&gt;#&amp;gt; &lt;span style=&#34;color: #555555;&#34;&gt;2&lt;/span&gt; 2 1 5&lt;/span&gt;&lt;/span&gt;&#xA;&lt;span&gt;&lt;span class=&#34;c&#34;&gt;#&amp;gt; &lt;span style=&#34;color: #555555;&#34;&gt;3&lt;/span&gt; 3 1 10&lt;/span&gt;&lt;/span&gt;&#xA;&lt;span&gt;&lt;span class=&#34;c&#34;&gt;#&amp;gt; &lt;span style=&#34;color: #555555;&#34;&gt;4&lt;/span&gt; 4 1 17&lt;/span&gt;&lt;/span&gt;&#xA;&lt;span&gt;&lt;span class=&#34;c&#34;&gt;#&amp;gt; &lt;span style=&#34;color: #555555;&#34;&gt;5&lt;/span&gt; 5 1 26&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://tibble.tidyverse.org/reference/tibble.html&#34;&gt;&lt;code&gt;tibble()&lt;/code&gt;&lt;/a&gt; does much less than &lt;a href=&#34;https://rdrr.io/r/base/data.frame.html&#34;&gt;&lt;code&gt;data.frame()&lt;/code&gt;&lt;/a&gt;: it never changes the type of the inputs (e.g.&amp;nbsp;it keeps list columns unchanged, and never converts strings to factors!), it never changes the names of variables, it only recycles inputs of length 1, and it never creates &lt;a href=&#34;https://rdrr.io/r/base/row.names.html&#34;&gt;&lt;code&gt;row.names()&lt;/code&gt;&lt;/a&gt;. You can read more about these features in &lt;code&gt;vignette(&#34;tibble&#34;)&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can define a tibble row-by-row with &lt;a href=&#34;https://tibble.tidyverse.org/reference/tribble.html&#34;&gt;&lt;code&gt;tribble()&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre class=&#34;chroma&#34;&gt;&#xA;&lt;span&gt;&lt;span class=&#34;nf&#34;&gt;&lt;a href=&#34;https://tibble.tidyverse.org/reference/tribble.html&#34;&gt;tribble&lt;/a&gt;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;/span&gt;&#xA;&lt;span&gt;  &lt;span class=&#34;o&#34;&gt;~&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;x&lt;/span&gt;, &lt;span class=&#34;o&#34;&gt;~&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;y&lt;/span&gt;,  &lt;span class=&#34;o&#34;&gt;~&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;z&lt;/span&gt;,&lt;/span&gt;&#xA;&lt;span&gt;  &lt;span class=&#34;s&#34;&gt;&#34;a&#34;&lt;/span&gt;, &lt;span class=&#34;m&#34;&gt;2&lt;/span&gt;,  &lt;span class=&#34;m&#34;&gt;3.6&lt;/span&gt;,&lt;/span&gt;&#xA;&lt;span&gt;  &lt;span class=&#34;s&#34;&gt;&#34;b&#34;&lt;/span&gt;, &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;,  &lt;span class=&#34;m&#34;&gt;8.5&lt;/span&gt;&lt;/span&gt;&#xA;&lt;span&gt;&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&#xA;&lt;span&gt;&lt;span class=&#34;c&#34;&gt;#&amp;gt; &lt;span style=&#34;color: #555555;&#34;&gt;# A tibble: 2 × 3&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&#xA;&lt;span&gt;&lt;span class=&#34;c&#34;&gt;#&amp;gt; &lt;span style=&#34;font-weight: bold;&#34;&gt;x&lt;/span&gt; &lt;span style=&#34;font-weight: bold;&#34;&gt;y&lt;/span&gt; &lt;span style=&#34;font-weight: bold;&#34;&gt;z&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&#xA;&lt;span&gt;&lt;span class=&#34;c&#34;&gt;#&amp;gt; &lt;span style=&#34;color: #555555; font-style: italic;&#34;&gt;&amp;lt;chr&amp;gt;&lt;/span&gt; &lt;span style=&#34;color: #555555; font-style: italic;&#34;&gt;&amp;lt;dbl&amp;gt;&lt;/span&gt; &lt;span style=&#34;color: #555555; font-style: italic;&#34;&gt;&amp;lt;dbl&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&#xA;&lt;span&gt;&lt;span class=&#34;c&#34;&gt;#&amp;gt; &lt;span style=&#34;color: #555555;&#34;&gt;1&lt;/span&gt; a 2 3.6&lt;/span&gt;&lt;/span&gt;&#xA;&lt;span&gt;&lt;span class=&#34;c&#34;&gt;#&amp;gt; &lt;span style=&#34;color: #555555;&#34;&gt;2&lt;/span&gt; b 1 8.5&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Related work&lt;/h2&gt; &#xA;&lt;p&gt;The tibble print method draws inspiration from &lt;a href=&#34;https://rdatatable.gitlab.io/data.table&#34;&gt;data.table&lt;/a&gt;, and &lt;a href=&#34;https://github.com/patperry/r-frame&#34;&gt;frame&lt;/a&gt;. Like &lt;a href=&#34;https://Rdatatable.gitlab.io/data.table/reference/data.table.html&#34;&gt;&lt;code&gt;data.table::data.table()&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://tibble.tidyverse.org/reference/tibble.html&#34;&gt;&lt;code&gt;tibble()&lt;/code&gt;&lt;/a&gt; doesn’t change column names and doesn’t use rownames.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Code of Conduct&lt;/h2&gt; &#xA;&lt;p&gt;Please note that the tibble project is released with a &lt;a href=&#34;https://tibble.tidyverse.org/CODE_OF_CONDUCT.html&#34;&gt;Contributor Code of Conduct&lt;/a&gt;. By contributing to this project, you agree to abide by its terms.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>sparklyr/sparklyr</title>
    <updated>2023-03-12T01:58:38Z</updated>
    <id>tag:github.com,2023-03-12:/sparklyr/sparklyr</id>
    <link href="https://github.com/sparklyr/sparklyr" rel="alternate"></link>
    <summary type="html">&lt;p&gt;R interface for Apache Spark&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;sparklyr: R interface for Apache Spark&lt;/h1&gt; &#xA;&lt;!-- README.md is generated from README.Rmd. Please edit that file --&gt; &#xA;&lt;!-- badges: start --&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/sparklyr/sparklyr/actions/workflows/R-CMD-check.yaml&#34;&gt;&lt;img src=&#34;https://github.com/sparklyr/sparklyr/actions/workflows/R-CMD-check.yaml/badge.svg?sanitize=true&#34; alt=&#34;R-CMD-check&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/sparklyr/sparklyr/actions/workflows/spark-tests.yaml&#34;&gt;&lt;img src=&#34;https://github.com/sparklyr/sparklyr/actions/workflows/spark-tests.yaml/badge.svg?sanitize=true&#34; alt=&#34;Spark-Tests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://CRAN.R-project.org/package=sparklyr&#34;&gt;&lt;img src=&#34;https://www.r-pkg.org/badges/version/sparklyr&#34; alt=&#34;CRAN status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://app.codecov.io/gh/sparklyr/sparklyr?branch=main&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/sparklyr/sparklyr/branch/main/graph/badge.svg?sanitize=true&#34; alt=&#34;Codecov test coverage&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- badges: end --&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/tools/readme/sparklyr-diagram.png&#34; width=&#34;320&#34; align=&#34;right&#34; style=&#34;margin-left: 20px; margin-right: 20px&#34;&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install and connect to &lt;a href=&#34;https://spark.apache.org/&#34;&gt;Spark&lt;/a&gt; using YARN, Mesos, Livy or Kubernetes.&lt;/li&gt; &#xA; &lt;li&gt;Use &lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#using-dplyr&#34;&gt;dplyr&lt;/a&gt; to filter and aggregate Spark datasets and &lt;a href=&#34;https://spark.rstudio.com/guides/streaming/&#34;&gt;streams&lt;/a&gt; then bring them into R for analysis and visualization.&lt;/li&gt; &#xA; &lt;li&gt;Use &lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#machine-learning&#34;&gt;MLlib&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#using-h2o&#34;&gt;H2O&lt;/a&gt;, &lt;a href=&#34;https://spark.rstudio.com/packages/sparkxgb/latest/&#34;&gt;XGBoost&lt;/a&gt; and &lt;a href=&#34;https://spark.rstudio.com/packages/graphframes/latest/&#34;&gt;GraphFrames&lt;/a&gt; to train models at scale in Spark.&lt;/li&gt; &#xA; &lt;li&gt;Create interoperable machine learning &lt;a href=&#34;https://spark.rstudio.com/guides/pipelines.html&#34;&gt;pipelines&lt;/a&gt; and productionize them with &lt;a href=&#34;https://spark.rstudio.com/packages/mleap/latest/&#34;&gt;MLeap&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Create &lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#extensions&#34;&gt;extensions&lt;/a&gt; that call the full Spark API or run &lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#distributed-r&#34;&gt;distributed R&lt;/a&gt; code to support new functionality.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#connecting-to-spark&#34;&gt;Connecting to Spark&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#using-dplyr&#34;&gt;Using dplyr&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#window-functions&#34;&gt;Window Functions&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#using-sql&#34;&gt;Using SQL&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#machine-learning&#34;&gt;Machine Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#reading-and-writing-data&#34;&gt;Reading and Writing Data&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#distributed-r&#34;&gt;Distributed R&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#extensions&#34;&gt;Extensions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#table-utilities&#34;&gt;Table Utilities&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#connection-utilities&#34;&gt;Connection Utilities&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#rstudio-ide&#34;&gt;RStudio IDE&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#using-h2o&#34;&gt;Using H2O&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#connecting-through-livy&#34;&gt;Connecting through Livy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/#connecting-through-databricks-connect&#34;&gt;Connecting through Databricks Connect&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;You can install the &lt;strong&gt;sparklyr&lt;/strong&gt; package from &lt;a href=&#34;https://CRAN.r-project.org&#34;&gt;CRAN&lt;/a&gt; as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&#34;sparklyr&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You should also install a local version of Spark for development purposes:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(sparklyr)&#xA;spark_install()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To upgrade to the latest version of sparklyr, run the following command and restart your r session:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&#34;devtools&#34;)&#xA;devtools::install_github(&#34;sparklyr/sparklyr&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Connecting to Spark&lt;/h2&gt; &#xA;&lt;p&gt;You can connect to both local instances of Spark as well as remote Spark clusters. Here we’ll connect to a local instance of Spark via the &lt;a href=&#34;https://spark.rstudio.com/packages/sparklyr/latest/reference/spark-connections.html&#34;&gt;spark_connect&lt;/a&gt; function:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(sparklyr)&#xA;sc &amp;lt;- spark_connect(master = &#34;local&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The returned Spark connection (&lt;code&gt;sc&lt;/code&gt;) provides a remote dplyr data source to the Spark cluster.&lt;/p&gt; &#xA;&lt;p&gt;For more information on connecting to remote Spark clusters see the &lt;a href=&#34;https://spark.rstudio.com/deployment.html&#34;&gt;Deployment&lt;/a&gt; section of the sparklyr website.&lt;/p&gt; &#xA;&lt;h2&gt;Using dplyr&lt;/h2&gt; &#xA;&lt;p&gt;We can now use all of the available dplyr verbs against the tables within the cluster.&lt;/p&gt; &#xA;&lt;p&gt;We’ll start by copying some datasets from R into the Spark cluster (note that you may need to install the nycflights13 and Lahman packages in order to execute this code):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(c(&#34;nycflights13&#34;, &#34;Lahman&#34;))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(dplyr)&#xA;iris_tbl &amp;lt;- copy_to(sc, iris, overwrite = TRUE)&#xA;flights_tbl &amp;lt;- copy_to(sc, nycflights13::flights, &#34;flights&#34;, overwrite = TRUE)&#xA;batting_tbl &amp;lt;- copy_to(sc, Lahman::Batting, &#34;batting&#34;, overwrite = TRUE)&#xA;src_tbls(sc)&#xA;#&amp;gt; [1] &#34;batting&#34; &#34;flights&#34; &#34;iris&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To start with here’s a simple filtering example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# filter by departure delay and print the first few records&#xA;flights_tbl %&amp;gt;% filter(dep_delay == 2)&#xA;#&amp;gt; # Source: spark&amp;lt;?&amp;gt; [?? x 19]&#xA;#&amp;gt;     year month   day dep_t…¹ sched…² dep_d…³ arr_t…⁴ sched…⁵&#xA;#&amp;gt;    &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;int&amp;gt;   &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;int&amp;gt;   &amp;lt;int&amp;gt;&#xA;#&amp;gt;  1  2013     1     1     517     515       2     830     819&#xA;#&amp;gt;  2  2013     1     1     542     540       2     923     850&#xA;#&amp;gt;  3  2013     1     1     702     700       2    1058    1014&#xA;#&amp;gt;  4  2013     1     1     715     713       2     911     850&#xA;#&amp;gt;  5  2013     1     1     752     750       2    1025    1029&#xA;#&amp;gt;  6  2013     1     1     917     915       2    1206    1211&#xA;#&amp;gt;  7  2013     1     1     932     930       2    1219    1225&#xA;#&amp;gt;  8  2013     1     1    1028    1026       2    1350    1339&#xA;#&amp;gt;  9  2013     1     1    1042    1040       2    1325    1326&#xA;#&amp;gt; 10  2013     1     1    1231    1229       2    1523    1529&#xA;#&amp;gt; # … with more rows, 11 more variables: arr_delay &amp;lt;dbl&amp;gt;,&#xA;#&amp;gt; #   carrier &amp;lt;chr&amp;gt;, flight &amp;lt;int&amp;gt;, tailnum &amp;lt;chr&amp;gt;,&#xA;#&amp;gt; #   origin &amp;lt;chr&amp;gt;, dest &amp;lt;chr&amp;gt;, air_time &amp;lt;dbl&amp;gt;,&#xA;#&amp;gt; #   distance &amp;lt;dbl&amp;gt;, hour &amp;lt;dbl&amp;gt;, minute &amp;lt;dbl&amp;gt;,&#xA;#&amp;gt; #   time_hour &amp;lt;dttm&amp;gt;, and abbreviated variable names&#xA;#&amp;gt; #   ¹​dep_time, ²​sched_dep_time, ³​dep_delay, ⁴​arr_time,&#xA;#&amp;gt; #   ⁵​sched_arr_time&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://spark.rstudio.com/guides/dplyr.html&#34;&gt;Introduction to dplyr&lt;/a&gt; provides additional &lt;code&gt;dplyr&lt;/code&gt; examples you can try. For example, consider the last example from the tutorial which plots data on flight delays:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;delay &amp;lt;- flights_tbl %&amp;gt;%&#xA;  group_by(tailnum) %&amp;gt;%&#xA;  summarise(count = n(), dist = mean(distance), delay = mean(arr_delay)) %&amp;gt;%&#xA;  filter(count &amp;gt; 20, dist &amp;lt; 2000, !is.na(delay)) %&amp;gt;%&#xA;  collect()&#xA;&#xA;# plot delays&#xA;library(ggplot2)&#xA;ggplot(delay, aes(dist, delay)) +&#xA;  geom_point(aes(size = count), alpha = 1/2) +&#xA;  geom_smooth() +&#xA;  scale_size_area(max_size = 2)&#xA;#&amp;gt; `geom_smooth()` using method = &#39;gam&#39; and formula = &#39;y ~&#xA;#&amp;gt; s(x, bs = &#34;cs&#34;)&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/tools/readme/dplyr-ggplot2-1.png&#34; width=&#34;100%&#34;&gt; &#xA;&lt;h3&gt;Window Functions&lt;/h3&gt; &#xA;&lt;p&gt;dplyr &lt;a href=&#34;https://spark.rstudio.com/guides/dplyr.html#grouping&#34;&gt;window functions&lt;/a&gt; are also supported, for example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;batting_tbl %&amp;gt;%&#xA;  select(playerID, yearID, teamID, G, AB:H) %&amp;gt;%&#xA;  arrange(playerID, yearID, teamID) %&amp;gt;%&#xA;  group_by(playerID) %&amp;gt;%&#xA;  filter(min_rank(desc(H)) &amp;lt;= 2 &amp;amp; H &amp;gt; 0)&#xA;#&amp;gt; # Source:     spark&amp;lt;?&amp;gt; [?? x 7]&#xA;#&amp;gt; # Groups:     playerID&#xA;#&amp;gt; # Ordered by: playerID, yearID, teamID&#xA;#&amp;gt;    playerID  yearID teamID     G    AB     R     H&#xA;#&amp;gt;    &amp;lt;chr&amp;gt;      &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;&#xA;#&amp;gt;  1 aaronha01   1959 ML1      154   629   116   223&#xA;#&amp;gt;  2 aaronha01   1963 ML1      161   631   121   201&#xA;#&amp;gt;  3 abbotji01   1999 MIL       20    21     0     2&#xA;#&amp;gt;  4 abnersh01   1992 CHA       97   208    21    58&#xA;#&amp;gt;  5 abnersh01   1990 SDN       91   184    17    45&#xA;#&amp;gt;  6 acklefr01   1963 CHA        2     5     0     1&#xA;#&amp;gt;  7 acklefr01   1964 CHA        3     1     0     1&#xA;#&amp;gt;  8 acunaro01   2019 ATL      156   626   127   175&#xA;#&amp;gt;  9 acunaro01   2018 ATL      111   433    78   127&#xA;#&amp;gt; 10 adamecr01   2016 COL      121   225    25    49&#xA;#&amp;gt; # … with more rows&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For additional documentation on using dplyr with Spark see the &lt;a href=&#34;https://spark.rstudio.com/dplyr.html&#34;&gt;dplyr&lt;/a&gt; section of the sparklyr website.&lt;/p&gt; &#xA;&lt;h2&gt;Using SQL&lt;/h2&gt; &#xA;&lt;p&gt;It’s also possible to execute SQL queries directly against tables within a Spark cluster. The &lt;code&gt;spark_connection&lt;/code&gt; object implements a &lt;a href=&#34;https://github.com/r-dbi/DBI&#34;&gt;DBI&lt;/a&gt; interface for Spark, so you can use &lt;code&gt;dbGetQuery()&lt;/code&gt; to execute SQL and return the result as an R data frame:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(DBI)&#xA;iris_preview &amp;lt;- dbGetQuery(sc, &#34;SELECT * FROM iris LIMIT 10&#34;)&#xA;iris_preview&#xA;#&amp;gt;    Sepal_Length Sepal_Width Petal_Length Petal_Width&#xA;#&amp;gt; 1           5.1         3.5          1.4         0.2&#xA;#&amp;gt; 2           4.9         3.0          1.4         0.2&#xA;#&amp;gt; 3           4.7         3.2          1.3         0.2&#xA;#&amp;gt; 4           4.6         3.1          1.5         0.2&#xA;#&amp;gt; 5           5.0         3.6          1.4         0.2&#xA;#&amp;gt; 6           5.4         3.9          1.7         0.4&#xA;#&amp;gt; 7           4.6         3.4          1.4         0.3&#xA;#&amp;gt; 8           5.0         3.4          1.5         0.2&#xA;#&amp;gt; 9           4.4         2.9          1.4         0.2&#xA;#&amp;gt; 10          4.9         3.1          1.5         0.1&#xA;#&amp;gt;    Species&#xA;#&amp;gt; 1   setosa&#xA;#&amp;gt; 2   setosa&#xA;#&amp;gt; 3   setosa&#xA;#&amp;gt; 4   setosa&#xA;#&amp;gt; 5   setosa&#xA;#&amp;gt; 6   setosa&#xA;#&amp;gt; 7   setosa&#xA;#&amp;gt; 8   setosa&#xA;#&amp;gt; 9   setosa&#xA;#&amp;gt; 10  setosa&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Machine Learning&lt;/h2&gt; &#xA;&lt;p&gt;You can orchestrate machine learning algorithms in a Spark cluster via the &lt;a href=&#34;https://spark.apache.org/docs/latest/mllib-guide.html&#34;&gt;machine learning&lt;/a&gt; functions within &lt;strong&gt;sparklyr&lt;/strong&gt;. These functions connect to a set of high-level APIs built on top of DataFrames that help you create and tune machine learning workflows.&lt;/p&gt; &#xA;&lt;p&gt;Here’s an example where we use &lt;a href=&#34;https://spark.rstudio.com/packages/sparklyr/latest/reference/ml_linear_regression/&#34;&gt;ml_linear_regression&lt;/a&gt; to fit a linear regression model. We’ll use the built-in &lt;code&gt;mtcars&lt;/code&gt; dataset, and see if we can predict a car’s fuel consumption (&lt;code&gt;mpg&lt;/code&gt;) based on its weight (&lt;code&gt;wt&lt;/code&gt;), and the number of cylinders the engine contains (&lt;code&gt;cyl&lt;/code&gt;). We’ll assume in each case that the relationship between &lt;code&gt;mpg&lt;/code&gt; and each of our features is linear.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# copy mtcars into spark&#xA;mtcars_tbl &amp;lt;- copy_to(sc, mtcars, overwrite = TRUE)&#xA;&#xA;# transform our data set, and then partition into &#39;training&#39;, &#39;test&#39;&#xA;partitions &amp;lt;- mtcars_tbl %&amp;gt;%&#xA;  filter(hp &amp;gt;= 100) %&amp;gt;%&#xA;  mutate(cyl8 = cyl == 8) %&amp;gt;%&#xA;  sdf_partition(training = 0.5, test = 0.5, seed = 1099)&#xA;&#xA;# fit a linear model to the training dataset&#xA;fit &amp;lt;- partitions$training %&amp;gt;%&#xA;  ml_linear_regression(response = &#34;mpg&#34;, features = c(&#34;wt&#34;, &#34;cyl&#34;))&#xA;fit&#xA;#&amp;gt; Formula: mpg ~ wt + cyl&#xA;#&amp;gt; &#xA;#&amp;gt; Coefficients:&#xA;#&amp;gt; (Intercept)          wt         cyl &#xA;#&amp;gt;  37.1464554  -4.3408005  -0.5830515&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For linear regression models produced by Spark, we can use &lt;code&gt;summary()&lt;/code&gt; to learn a bit more about the quality of our fit, and the statistical significance of each of our predictors.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(fit)&#xA;#&amp;gt; Deviance Residuals:&#xA;#&amp;gt;     Min      1Q  Median      3Q     Max &#xA;#&amp;gt; -2.5134 -0.9158 -0.1683  1.1503  2.1534 &#xA;#&amp;gt; &#xA;#&amp;gt; Coefficients:&#xA;#&amp;gt; (Intercept)          wt         cyl &#xA;#&amp;gt;  37.1464554  -4.3408005  -0.5830515 &#xA;#&amp;gt; &#xA;#&amp;gt; R-Squared: 0.9428&#xA;#&amp;gt; Root Mean Squared Error: 1.409&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Spark machine learning supports a wide array of algorithms and feature transformations and as illustrated above it’s easy to chain these functions together with dplyr pipelines. To learn more see the &lt;a href=&#34;https://spark.rstudio.com/mlib/&#34;&gt;machine learning&lt;/a&gt; section.&lt;/p&gt; &#xA;&lt;h2&gt;Reading and Writing Data&lt;/h2&gt; &#xA;&lt;p&gt;You can read and write data in CSV, JSON, and Parquet formats. Data can be stored in HDFS, S3, or on the local filesystem of cluster nodes.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;temp_csv &amp;lt;- tempfile(fileext = &#34;.csv&#34;)&#xA;temp_parquet &amp;lt;- tempfile(fileext = &#34;.parquet&#34;)&#xA;temp_json &amp;lt;- tempfile(fileext = &#34;.json&#34;)&#xA;&#xA;spark_write_csv(iris_tbl, temp_csv)&#xA;iris_csv_tbl &amp;lt;- spark_read_csv(sc, &#34;iris_csv&#34;, temp_csv)&#xA;&#xA;spark_write_parquet(iris_tbl, temp_parquet)&#xA;iris_parquet_tbl &amp;lt;- spark_read_parquet(sc, &#34;iris_parquet&#34;, temp_parquet)&#xA;&#xA;spark_write_json(iris_tbl, temp_json)&#xA;iris_json_tbl &amp;lt;- spark_read_json(sc, &#34;iris_json&#34;, temp_json)&#xA;&#xA;src_tbls(sc)&#xA;#&amp;gt; [1] &#34;batting&#34;      &#34;flights&#34;      &#34;iris&#34;        &#xA;#&amp;gt; [4] &#34;iris_csv&#34;     &#34;iris_json&#34;    &#34;iris_parquet&#34;&#xA;#&amp;gt; [7] &#34;mtcars&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Distributed R&lt;/h2&gt; &#xA;&lt;p&gt;You can execute arbitrary r code across your cluster using &lt;code&gt;spark_apply()&lt;/code&gt;. For example, we can apply &lt;code&gt;rgamma&lt;/code&gt; over &lt;code&gt;iris&lt;/code&gt; as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;spark_apply(iris_tbl, function(data) {&#xA;  data[1:4] + rgamma(1,2)&#xA;})&#xA;#&amp;gt; # Source: spark&amp;lt;?&amp;gt; [?? x 4]&#xA;#&amp;gt;    Sepal_Length Sepal_Width Petal_Length Petal_Width&#xA;#&amp;gt;           &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;&#xA;#&amp;gt;  1         5.51        3.91         1.81       0.610&#xA;#&amp;gt;  2         5.31        3.41         1.81       0.610&#xA;#&amp;gt;  3         5.11        3.61         1.71       0.610&#xA;#&amp;gt;  4         5.01        3.51         1.91       0.610&#xA;#&amp;gt;  5         5.41        4.01         1.81       0.610&#xA;#&amp;gt;  6         5.81        4.31         2.11       0.810&#xA;#&amp;gt;  7         5.01        3.81         1.81       0.710&#xA;#&amp;gt;  8         5.41        3.81         1.91       0.610&#xA;#&amp;gt;  9         4.81        3.31         1.81       0.610&#xA;#&amp;gt; 10         5.31        3.51         1.91       0.510&#xA;#&amp;gt; # … with more rows&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also group by columns to perform an operation over each group of rows and make use of any package within the closure:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;spark_apply(&#xA;  iris_tbl,&#xA;  function(e) broom::tidy(lm(Petal_Width ~ Petal_Length, e)),&#xA;  columns = c(&#34;term&#34;, &#34;estimate&#34;, &#34;std.error&#34;, &#34;statistic&#34;, &#34;p.value&#34;),&#xA;  group_by = &#34;Species&#34;&#xA;)&#xA;#&amp;gt; # Source: spark&amp;lt;?&amp;gt; [?? x 6]&#xA;#&amp;gt;   Species    term         estimate std.er…¹ stati…²  p.value&#xA;#&amp;gt;   &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;           &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;&#xA;#&amp;gt; 1 versicolor (Intercept)   -0.0843   0.161   -0.525 6.02e- 1&#xA;#&amp;gt; 2 versicolor Petal_Length   0.331    0.0375   8.83  1.27e-11&#xA;#&amp;gt; 3 virginica  (Intercept)    1.14     0.379    2.99  4.34e- 3&#xA;#&amp;gt; 4 virginica  Petal_Length   0.160    0.0680   2.36  2.25e- 2&#xA;#&amp;gt; 5 setosa     (Intercept)   -0.0482   0.122   -0.396 6.94e- 1&#xA;#&amp;gt; 6 setosa     Petal_Length   0.201    0.0826   2.44  1.86e- 2&#xA;#&amp;gt; # … with abbreviated variable names ¹​std.error, ²​statistic&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Extensions&lt;/h2&gt; &#xA;&lt;p&gt;The facilities used internally by sparklyr for its &lt;code&gt;dplyr&lt;/code&gt; and machine learning interfaces are available to extension packages. Since Spark is a general purpose cluster computing system there are many potential applications for extensions (e.g.&amp;nbsp;interfaces to custom machine learning pipelines, interfaces to 3rd party Spark packages, etc.).&lt;/p&gt; &#xA;&lt;p&gt;Here’s a simple example that wraps a Spark text file line counting function with an R function:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# write a CSV&#xA;tempfile &amp;lt;- tempfile(fileext = &#34;.csv&#34;)&#xA;write.csv(nycflights13::flights, tempfile, row.names = FALSE, na = &#34;&#34;)&#xA;&#xA;# define an R interface to Spark line counting&#xA;count_lines &amp;lt;- function(sc, path) {&#xA;  spark_context(sc) %&amp;gt;%&#xA;    invoke(&#34;textFile&#34;, path, 1L) %&amp;gt;%&#xA;      invoke(&#34;count&#34;)&#xA;}&#xA;&#xA;# call spark to count the lines of the CSV&#xA;count_lines(sc, tempfile)&#xA;#&amp;gt; [1] 336777&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To learn more about creating extensions see the &lt;a href=&#34;https://spark.rstudio.com/guides/extensions.html&#34;&gt;Extensions&lt;/a&gt; section of the sparklyr website.&lt;/p&gt; &#xA;&lt;h2&gt;Table Utilities&lt;/h2&gt; &#xA;&lt;p&gt;You can cache a table into memory with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;tbl_cache(sc, &#34;batting&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and unload from memory using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;tbl_uncache(sc, &#34;batting&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Connection Utilities&lt;/h2&gt; &#xA;&lt;p&gt;You can view the Spark web console using the &lt;code&gt;spark_web()&lt;/code&gt; function:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;spark_web(sc)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can show the log using the &lt;code&gt;spark_log()&lt;/code&gt; function:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;spark_log(sc, n = 10)&#xA;#&amp;gt; 22/12/08 10:13:49 INFO BlockManagerInfo: Removed broadcast_84_piece0 on localhost:54296 in memory (size: 9.2 KiB, free: 912.1 MiB)&#xA;#&amp;gt; 22/12/08 10:13:49 INFO BlockManagerInfo: Removed broadcast_86_piece0 on localhost:54296 in memory (size: 5.0 KiB, free: 912.1 MiB)&#xA;#&amp;gt; 22/12/08 10:13:49 INFO BlockManagerInfo: Removed broadcast_76_piece0 on localhost:54296 in memory (size: 8.7 KiB, free: 912.1 MiB)&#xA;#&amp;gt; 22/12/08 10:13:49 INFO Executor: Finished task 0.0 in stage 67.0 (TID 83). 1004 bytes result sent to driver&#xA;#&amp;gt; 22/12/08 10:13:49 INFO TaskSetManager: Finished task 0.0 in stage 67.0 (TID 83) in 187 ms on localhost (executor driver) (1/1)&#xA;#&amp;gt; 22/12/08 10:13:49 INFO TaskSchedulerImpl: Removed TaskSet 67.0, whose tasks have all completed, from pool &#xA;#&amp;gt; 22/12/08 10:13:49 INFO DAGScheduler: ResultStage 67 (count at NativeMethodAccessorImpl.java:0) finished in 0.199 s&#xA;#&amp;gt; 22/12/08 10:13:49 INFO DAGScheduler: Job 49 is finished. Cancelling potential speculative or zombie tasks for this job&#xA;#&amp;gt; 22/12/08 10:13:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 67: Stage finished&#xA;#&amp;gt; 22/12/08 10:13:49 INFO DAGScheduler: Job 49 finished: count at NativeMethodAccessorImpl.java:0, took 0.204972 s&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, we disconnect from Spark:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;  spark_disconnect(sc)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;RStudio IDE&lt;/h2&gt; &#xA;&lt;p&gt;The RStudio IDE includes integrated support for Spark and the sparklyr package, including tools for:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Creating and managing Spark connections&lt;/li&gt; &#xA; &lt;li&gt;Browsing the tables and columns of Spark DataFrames&lt;/li&gt; &#xA; &lt;li&gt;Previewing the first 1,000 rows of Spark DataFrames&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Once you’ve installed the sparklyr package, you should find a new &lt;strong&gt;Spark&lt;/strong&gt; pane within the IDE. This pane includes a &lt;strong&gt;New Connection&lt;/strong&gt; dialog which can be used to make connections to local or remote Spark instances:&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/tools/readme/spark-connect.png&#34; class=&#34;screenshot&#34; width=&#34;389&#34;&gt; &#xA;&lt;p&gt;Once you’ve connected to Spark you’ll be able to browse the tables contained within the Spark cluster and preview Spark DataFrames using the standard RStudio data viewer:&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/tools/readme/spark-dataview.png&#34; class=&#34;screenshot&#34; width=&#34;639&#34;&gt; &#xA;&lt;p&gt;You can also connect to Spark through &lt;a href=&#34;https://livy.apache.org/&#34;&gt;Livy&lt;/a&gt; through a new connection dialog:&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/tools/readme/spark-connect-livy.png&#34; class=&#34;screenshot&#34; width=&#34;389&#34;&gt; &#xA;&lt;div style=&#34;margin-bottom: 15px;&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Using H2O&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=rsparkling&#34;&gt;rsparkling&lt;/a&gt; is a CRAN package from &lt;a href=&#34;https://h2o.ai/&#34;&gt;H2O&lt;/a&gt; that extends &lt;a href=&#34;https://spark.rstudio.com/&#34;&gt;sparklyr&lt;/a&gt; to provide an interface into &lt;a href=&#34;https://github.com/h2oai/sparkling-water&#34;&gt;Sparkling Water&lt;/a&gt;. For instance, the following example installs, configures and runs &lt;a href=&#34;https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/glm.html&#34;&gt;h2o.glm&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rsparkling)&#xA;library(sparklyr)&#xA;library(dplyr)&#xA;library(h2o)&#xA;&#xA;sc &amp;lt;- spark_connect(master = &#34;local&#34;, version = &#34;2.3.2&#34;)&#xA;mtcars_tbl &amp;lt;- copy_to(sc, mtcars, &#34;mtcars&#34;, overwrite = TRUE)&#xA;&#xA;mtcars_h2o &amp;lt;- as_h2o_frame(sc, mtcars_tbl, strict_version_check = FALSE)&#xA;&#xA;mtcars_glm &amp;lt;- h2o.glm(x = c(&#34;wt&#34;, &#34;cyl&#34;),&#xA;                      y = &#34;mpg&#34;,&#xA;                      training_frame = mtcars_h2o,&#xA;                      lambda_search = TRUE)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mtcars_glm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;#&amp;gt; Model Details:&#xA;#&amp;gt; ==============&#xA;#&amp;gt;&#xA;#&amp;gt; H2ORegressionModel: glm&#xA;#&amp;gt; Model ID:  GLM_model_R_1527265202599_1&#xA;#&amp;gt; GLM Model: summary&#xA;#&amp;gt;     family     link                              regularization&#xA;#&amp;gt; 1 gaussian identity Elastic Net (alpha = 0.5, lambda = 0.1013 )&#xA;#&amp;gt;                                                                lambda_search&#xA;#&amp;gt; 1 nlambda = 100, lambda.max = 10.132, lambda.min = 0.1013, lambda.1se = -1.0&#xA;#&amp;gt;   number_of_predictors_total number_of_active_predictors&#xA;#&amp;gt; 1                          2                           2&#xA;#&amp;gt;   number_of_iterations                                training_frame&#xA;#&amp;gt; 1                  100 frame_rdd_31_ad5c4e88ec97eb8ccedae9475ad34e02&#xA;#&amp;gt;&#xA;#&amp;gt; Coefficients: glm coefficients&#xA;#&amp;gt;       names coefficients standardized_coefficients&#xA;#&amp;gt; 1 Intercept    38.941654                 20.090625&#xA;#&amp;gt; 2       cyl    -1.468783                 -2.623132&#xA;#&amp;gt; 3        wt    -3.034558                 -2.969186&#xA;#&amp;gt;&#xA;#&amp;gt; H2ORegressionMetrics: glm&#xA;#&amp;gt; ** Reported on training data. **&#xA;#&amp;gt;&#xA;#&amp;gt; MSE:  6.017684&#xA;#&amp;gt; RMSE:  2.453097&#xA;#&amp;gt; MAE:  1.940985&#xA;#&amp;gt; RMSLE:  0.1114801&#xA;#&amp;gt; Mean Residual Deviance :  6.017684&#xA;#&amp;gt; R^2 :  0.8289895&#xA;#&amp;gt; Null Deviance :1126.047&#xA;#&amp;gt; Null D.o.F. :31&#xA;#&amp;gt; Residual Deviance :192.5659&#xA;#&amp;gt; Residual D.o.F. :29&#xA;#&amp;gt; AIC :156.2425&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;spark_disconnect(sc)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Connecting through Livy&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/cloudera/livy&#34;&gt;Livy&lt;/a&gt; enables remote connections to Apache Spark clusters. However, please notice that connecting to Spark clusters through Livy is much slower than any other connection method.&lt;/p&gt; &#xA;&lt;p&gt;Before connecting to Livy, you will need the connection information to an existing service running Livy. Otherwise, to test &lt;code&gt;livy&lt;/code&gt; in your local environment, you can install it and run it locally as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;livy_install()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;livy_service_start()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To connect, use the Livy service address as &lt;code&gt;master&lt;/code&gt; and &lt;code&gt;method = &#34;livy&#34;&lt;/code&gt; in &lt;code&gt;spark_connect()&lt;/code&gt;. Once connection completes, use &lt;code&gt;sparklyr&lt;/code&gt; as usual, for instance:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sc &amp;lt;- spark_connect(master = &#34;http://localhost:8998&#34;, method = &#34;livy&#34;, version = &#34;3.0.0&#34;)&#xA;copy_to(sc, iris, overwrite = TRUE)&#xA;&#xA;spark_disconnect(sc)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once you are done using &lt;code&gt;livy&lt;/code&gt; locally, you should stop this service with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;livy_service_stop()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To connect to remote &lt;code&gt;livy&lt;/code&gt; clusters that support basic authentication connect as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;config &amp;lt;- livy_config(username=&#34;&amp;lt;username&amp;gt;&#34;, password=&#34;&amp;lt;password&amp;gt;&#34;)&#xA;sc &amp;lt;- spark_connect(master = &#34;&amp;lt;address&amp;gt;&#34;, method = &#34;livy&#34;, config = config)&#xA;spark_disconnect(sc)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Connecting through Databricks Connect&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.databricks.com/dev-tools/databricks-connect.html#databricks-connect&#34;&gt;Databricks Connect&lt;/a&gt; allows you to connect sparklyr to a remote Databricks Cluster. You can install &lt;a href=&#34;https://pypi.org/project/databricks-connect/&#34;&gt;Databricks Connect python package&lt;/a&gt; and use it to submit Spark jobs written in sparklyr APIs and have them execute remotely on a Databricks cluster instead of in the local Spark session.&lt;/p&gt; &#xA;&lt;p&gt;To use sparklyr with Databricks Connect first launch a Cluster on Databricks. Then follow &lt;a href=&#34;https://docs.databricks.com/dev-tools/databricks-connect.html#client-setup&#34;&gt;these instructions&lt;/a&gt; to setup the client:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Make sure pyspark is not installed&lt;/li&gt; &#xA; &lt;li&gt;Install the Databricks Connect python package. The latest supported version is 6.4.1.&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;databricks-connect configure&lt;/code&gt; and provide the configuration information &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Databricks account URL of the form &lt;code&gt;https://&amp;lt;account&amp;gt;.cloud.databricks.com&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://docs.databricks.com/dev-tools/api/latest/authentication.html#token-management&#34;&gt;User token&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Cluster ID&lt;/li&gt; &#xA;   &lt;li&gt;Port (default port number is &lt;code&gt;15001&lt;/code&gt;)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;To configure &lt;code&gt;sparklyr&lt;/code&gt; with Databricks Connect, set the following environment variables:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export SPARK_VERSION=2.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now simply create a spark connection as follows&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;spark_home &amp;lt;- system(&#34;databricks-connect get-spark-home&#34;)&#xA;sc &amp;lt;- spark_connect(method = &#34;databricks&#34;,&#xA;                    spark_home = spark_home)&#xA;copy_to(sc, iris, overwrite = TRUE)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/sparklyr/sparklyr/main/tools/readme/databricks-connect.png&#34; class=&#34;screenshot&#34; width=&#34;750&#34;&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;spark_disconnect(sc)&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>BiKC/RamplotR</title>
    <updated>2023-03-12T01:58:38Z</updated>
    <id>tag:github.com,2023-03-12:/BiKC/RamplotR</id>
    <link href="https://github.com/BiKC/RamplotR" rel="alternate"></link>
    <summary type="html">&lt;p&gt;R shiny app for making Ramachandran plots&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;RamplotR&lt;/h1&gt; &#xA;&lt;p&gt;R shiny app for making Ramachandran plots.&lt;/p&gt; &#xA;&lt;p&gt;The calculation of the density backgrounds for the ramachadran plots is done based on the list of proteins proposed by &lt;a href=&#34;https://pubmed.ncbi.nlm.nih.gov/12557186/&#34;&gt;Lovell et al. (2003)&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Published version&lt;/h3&gt; &#xA;&lt;p&gt;This app is published via shinyapps.io and can be used via &lt;a href=&#34;https://bioit.shinyapps.io/RamplotR/&#34;&gt;https://bioit.shinyapps.io/RamplotR/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Install it yourself&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install R from the &lt;a href=&#34;https://www.r-project.org/&#34;&gt;website&lt;/a&gt; (tested using v4.03)&lt;/li&gt; &#xA; &lt;li&gt;Run the following code:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;# Install the packages above if they are not already installed&#xA;installed_packages &amp;lt;- rownames(installed.packages())&#xA;required_packages &amp;lt;- c(&#34;shiny&#34;, &#34;shinycssloaders&#34;, &#34;shinyWidgets&#34;, &#34;colourpicker&#34;, &#34;bio3d&#34;, &#34;plyr&#34;)&#xA;for (p in required_packages) {&#xA;  if (!(p %in% installed_packages)) {&#xA;    install.packages(p)&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Run the app.R script&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Example image of the app&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/BiKC/RamplotR/main/images/app.png&#34; alt=&#34;Example image of the app&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;To be done&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Custom PDB upload support&lt;/li&gt; &#xA; &lt;li&gt;Add CIF support&lt;/li&gt; &#xA; &lt;li&gt;Output a statistic output of the number of residues in each region&lt;/li&gt; &#xA; &lt;li&gt;Output a list of residues in each region&lt;/li&gt; &#xA; &lt;li&gt;Recalculate the density backgrounds with newer best proteins?&lt;/li&gt; &#xA; &lt;li&gt;Optimize the calculation of the torsion angles (now with bio3d but can be quite slow)&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>