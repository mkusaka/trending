<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub R Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-23T02:02:16Z</updated>
  <subtitle>Weekly Trending of R in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>paulhurleyuk/AnalysisWorkflowSkeleton</title>
    <updated>2023-07-23T02:02:16Z</updated>
    <id>tag:github.com,2023-07-23:/paulhurleyuk/AnalysisWorkflowSkeleton</id>
    <link href="https://github.com/paulhurleyuk/AnalysisWorkflowSkeleton" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Skeleton for an R data analysis workflow&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AnalysisWorkflowSkeleton&lt;/h1&gt; &#xA;&lt;p&gt;A Skeleton for an R data analysis workflow&lt;/p&gt; &#xA;&lt;p&gt;This is designed to be an automated analysis that leverages the power of make to deal with dependencies. Key to this seems to be keeping the analysis modular. Hence this worklfow has the following R files, all in ./R/&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;1_GetData.R - Gets data and produces ./data/srcdata.csv&lt;/li&gt; &#xA; &lt;li&gt;2_CleanData.R - Takes the sourcedata.csv file and produces cleandata.csv&lt;/li&gt; &#xA; &lt;li&gt;3_LoadData.R - Takes cleandata.csv and produces processeddata.csv&lt;/li&gt; &#xA; &lt;li&gt;4_TablesPlots.R - Takes processeddata.csv and produces tables and plots, saves them in dataimage&lt;/li&gt; &#xA; &lt;li&gt;5_Report.R - Takes processeddata.csv and dataimage and produces a report&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;exampleReport.Rmd - An example report in R markdown&lt;/p&gt; &#xA;&lt;p&gt;The makefile then specifies this order along with the dependencies. Running make should update any steps that have been updated (i.e new data will run everything, change the Tables and Plots script will only run step 4 and 5)&lt;/p&gt; &#xA;&lt;p&gt;ToDo: For more complicated analysis (like ongoing analysis where we want to analyse two months of data) need to work out how to store the data and keep the dependencies accurate, ie store data in months, need to know to update the analysis for Feb when the Feb dataset is incomplete but not once Feb is done.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>jordandouglas/LowESS</title>
    <updated>2023-07-23T02:02:16Z</updated>
    <id>tag:github.com,2023-07-23:/jordandouglas/LowESS</id>
    <link href="https://github.com/jordandouglas/LowESS" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Identifying the conditions that make a Bayesian phylogenetic analysis slow to converge during MCMC&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Searching for pathological phylogenetic configurations&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;Monday 3 July, 2023&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;The aim is to identify the conditions that make a Bayesian phylogenetic configuration inept at convergence during MCMC. This may arise from conflicting signals in the data, an over-specified or under-specified model, or it could simply be due to the existing MCMC proposal kernels being ill-equipped to traverse the posterior space efficiently.&lt;/p&gt; &#xA;&lt;p&gt;The two challenges in this competition share the same objective: find the conditions which give the &lt;strong&gt;smallest&lt;/strong&gt; effective sample size (ESS) per million states.&lt;/p&gt; &#xA;&lt;h2&gt;The benchmark&lt;/h2&gt; &#xA;&lt;p&gt;The benchmark dataset and model attained an ESS of 775 (standard error: 25) on the posterior density, and 762 (44) on the likelihood. These numbers were calculated from running the full MCMC chain across 10 replicates with a 10% burn-in. The maximum possible ESS on a chain of this length is 901.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Can you do worse than the benchmark?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Challenge 1: Dataset&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Objective:&lt;/strong&gt; Create a nucleotide multiple sequence alignment, with 100 sites and 20 taxa, which gives the smallest ESS when run on &lt;code&gt;C1.xml&lt;/code&gt; in BEAST 2. This model consists of the Jukes-Cantor substitution model, a strict clock, and a Yule tree prior. Do not edit the XML file. &lt;code&gt;benchmark.fasta&lt;/code&gt; can be used as a starting point to create a new alignment.&lt;/p&gt; &#xA;&lt;p&gt;Strategies that may give a pathological dataset:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Non-tree-like?&lt;/li&gt; &#xA; &lt;li&gt;Variable mutation rates?&lt;/li&gt; &#xA; &lt;li&gt;Variable speciation rates?&lt;/li&gt; &#xA; &lt;li&gt;Gaps?&lt;/li&gt; &#xA; &lt;li&gt;Anything else?&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Challenge 2: Model&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Objective:&lt;/strong&gt; Create a BEAST 2 XML file using any of the models available in any released BEAST 2 packages, such that the ESS is as small as possible when run on &lt;code&gt;benchmark.fasta&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Rules:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Set the chain length to 10 million states and log every 10,000 states.&lt;/li&gt; &#xA; &lt;li&gt;The dataset in &lt;code&gt;benchmark.fasta&lt;/code&gt;, and only this dataset, may be used. It must be used as a nucleotide alignment (not as amino acid), however tip date or location data may be included. Do not upload the dataset many times and use it in different partitions.&lt;/li&gt; &#xA; &lt;li&gt;Do not change any of the operators or operator weights from the beauti/LPhy defaults, and do not edit the XML file by hand (except for adding new loggables, like ESS).&lt;/li&gt; &#xA; &lt;li&gt;The MCMC chain must finish in under 20 minutes, so the time per million samples should be less than 2 minutes (2min/Msamples printed on the screen output).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Let&#39;s see which model is the slowest!&lt;/p&gt; &#xA;&lt;h2&gt;Procedure&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone the repository&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/jordandouglas/LowESS&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create a folder with your name and use that as your working directory. Make a README file.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cd LowESS&#xA;mkdir MY_NAME&#xA;cd MY_NAME&#xA;touch README.md&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;For challenge 1, the XML file is already provided. All you need is to make a fasta file, called &lt;code&gt;C1.fasta&lt;/code&gt;. To test your fasta file, first run the Rscript to put the fasta contents into a json file, and then run BEAST 2 using the json file:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Rscript ../fasta2json.R&#xA;~/beast/bin/beast -overwrite -df C1.json ../C1.xml&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Also generate a figure of the multiple sequence alignment &lt;code&gt;C1.png&lt;/code&gt;. This can be done using many software applications, including the programs below&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ormbunkar.se/aliview/&#34;&gt;https://ormbunkar.se/aliview/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/projects/msaviewer/&#34;&gt;https://www.ncbi.nlm.nih.gov/projects/msaviewer/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.ebi.ac.uk/Tools/msa/mview/&#34;&gt;https://www.ebi.ac.uk/Tools/msa/mview/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;For challenge 2, create a new XML file from the &lt;code&gt;benchmark.fasta&lt;/code&gt; dataset. The XML file should be called &lt;code&gt;C2.xml&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In the README file, please include the Challenge 1 alignment figure, describe the model used for Challenge 2 as well as any BEAST 2 packages being used (see &lt;code&gt;benchmark&lt;/code&gt; directory for an example)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;To submit the final results, please upload four files: &lt;code&gt;MY_NAME/C1.fasta&lt;/code&gt;, &lt;code&gt;MY_NAME/C1.png&lt;/code&gt;, &lt;code&gt;MY_NAME/C2.xml&lt;/code&gt;, and &lt;code&gt;MY_NAME/README.md&lt;/code&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git add C1.fasta&#xA;git add C1.png&#xA;git add C2.xml&#xA;git add README.md&#xA;git commit -m &#34;competition submission&#34;&#xA;git push&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;After the competition ends, organiser will run 10 replicates of each challenge per person, and the results will be announced afterwards. The mean ESS/state of the posterior and likelihood densities (which ever is smaller across the 10 independent chains, or the 10 chains combined) will be used (10% burn-in).&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Calculating ESS&lt;/h2&gt; &#xA;&lt;p&gt;The ESS can be caluclated using Tracer.&lt;/p&gt; &#xA;&lt;p&gt;For Challenge 1, the ESS of the posterior and likelihoods are printed in the output (screen logger), for convenience,&lt;/p&gt; &#xA;&lt;p&gt;For Challenge 2, you may want to edit the XML file to give the following screen logger so that the ESSes will be printed to the terminal output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;logger id=&#34;screenlog&#34; spec=&#34;Logger&#34; logEvery=&#34;10000&#34;&amp;gt;&#xA;    &amp;lt;ESS spec=&#39;ESS&#39; name=&#39;log&#39; arg=&#34;@posterior&#34;/&amp;gt;&#xA;    &amp;lt;ESS spec=&#39;ESS&#39; name=&#39;log&#39; arg=&#34;@likelihood&#34;/&amp;gt;&#xA;&amp;lt;/logger&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Dependencies&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;BEAST 2.7&lt;/li&gt; &#xA; &lt;li&gt;Tracer&lt;/li&gt; &#xA; &lt;li&gt;R&lt;/li&gt; &#xA; &lt;li&gt;Editing alignment can be done using a text editor, but it is very easy when using AliView. It is recommended that you download AliView, or a similar program, beforehand &lt;a href=&#34;https://ormbunkar.se/aliview/&#34;&gt;https://ormbunkar.se/aliview/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>