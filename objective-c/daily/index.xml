<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Objective-C Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-04T01:38:38Z</updated>
  <subtitle>Daily Trending of Objective-C in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>grero/RawDataViewer</title>
    <updated>2023-12-04T01:38:38Z</updated>
    <id>tag:github.com,2023-12-04:/grero/RawDataViewer</id>
    <link href="https://github.com/grero/RawDataViewer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A tool to quickly view large sets of time series data&lt;/p&gt;&lt;hr&gt;&lt;p&gt;The RawDataViewer tool arose from the need to quickly view large files consistent of multiple channels of neuro-physiological data. It should work on systems ≥ Mac OS 10.6. The data needs to be formatted in a way that the program understands. Currently, this means that the data file needs to have the following structure:&lt;/p&gt; &#xA;&lt;p&gt;A 90 byte header consisting of the following fields the size of the header, encoded as a single unsigned 32 bit integer the sampling rate of the signal, encoded as a single 32 bit integer the number of recording channels, encoded as a single unsigned 8 bit integer the data itself, encoded as 16 bit integers. This needs to ordered by time, i.e. the first timepoint for all channels, followed by the second time points for each channel, etc. In order words, when after reading in all the data, the resulting matrix would have dimensions of [number of time points] X [number of channels]&lt;/p&gt; &#xA;&lt;p&gt;Once the data have been organized in this way, the data files can be opened by simple dragging and dropping them onto the Application Icon using Finder. Alternatively, from the command line, you can type the following:&lt;/p&gt; &#xA;&lt;p&gt;open data.bin -a /Applications/RawDataViewer.app&lt;/p&gt; &#xA;&lt;p&gt;assuming that your data file is named data.bin and the viewer is located under /Applications. For most data files, it is not feasible to try and load the entire file onto the GPU in one go. There is a setting under Preferences deciding how much data to draw at a time. The default is set at 20 MB, meaning that when you open the file, a data 20 MB data buffer is read from the fie and displayed. To advance to the next 20 MB chunk, press the &#39;up&#39; key. To display the previous chunk, press the &#39;down&#39; key.&lt;/p&gt; &#xA;&lt;p&gt;The interface works as follows: Zooming is done by left-draggin the mouse around the region of interest. The application mantains a zoom stack, which can be traversed using the &#39;right&#39; (go down the stack) and &#39;left&#39; (go up the stack) keys. To get info for a given data point, left click it while holding down the &#39;Options&#39; key. This will display the time in milliseconds, the ampltide and the channel number. To select a particular channel (useful for tracking a channel across data chunks), left click its trace while holding down the &#39;Shift&#39; key.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>robovm/apple-ios-samples</title>
    <updated>2023-12-04T01:38:38Z</updated>
    <id>tag:github.com,2023-12-04:/robovm/apple-ios-samples</id>
    <link href="https://github.com/robovm/apple-ios-samples" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Mirror of Apple&#39;s iOS samples&lt;/h1&gt; &#xA;&lt;p&gt;This repository mirrors &lt;a href=&#34;https://developer.apple.com/library/ios/navigation/#section=Resource%20Types&amp;amp;topic=Sample%20Code&#34;&gt;Apple&#39;s iOS samples&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Topic&lt;/th&gt; &#xA;   &lt;th&gt;Framework&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/ABUIGroups/Introduction/Intro.html&#34;&gt;&lt;sup&gt;ABUIGroups&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Data Management&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Contact Data)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AddressBook&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;ABUIGroups shows how to check and request access to a user’s address book database. It also demonstrates how to retrieve, add, and remove group records using AddressBook APIs. It displays groups organized by their source in the address book.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/AccelerometerGraph/Introduction/Intro.html&#34;&gt;&lt;sup&gt;AccelerometerGraph&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Data Management&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Device Information)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AccelerometerGraph sample application graphs the motion of the device. It demonstrates how to use the UIAccelerometer class and how to use Quartz2D and Core Animation to provide a high performance graph view. It also demonstrates a low-pass filter that you can use to isolate the effects of gravity, and a high-pass filter that you can use to remove the effects of gravity.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/ActivityRings/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Activity Rings: Contributing to Activity Rings on Apple Watch&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;HealthKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample demonstrates the proper way to work with HealthKit on Apple Watch to contribute to the Activity Rings and have your app associated and listed with workouts and within the Move graph in the Activity app on iPhone.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/AdaptivePhotos/Introduction/Intro.html&#34;&gt;&lt;sup&gt;AdaptivePhotos: Using UIKit Traits and Size Classes&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample demonstrates how to use UIKit APIs to make your app work great on all devices and any orientation. You&#39;ll see how to use size classes, traits, and view controller additions to create an app that displays properly at any size (including iPhone 6 Plus) and and configuration (including iPad Multitasking).&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/AddMusic/Introduction/Intro.html&#34;&gt;&lt;sup&gt;AddMusic&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Audio &amp;amp; Video&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Audio)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;MediaPlayer&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AddMusic demonstrates basic use of iPod library access, part of the Media Player framework. You use iPod library access to play songs, audio books, and audio podcasts that are synced from a user&#39;s desktop iTunes library. This sample uses the Media Player framework&#39;s built-in user interface for choosing music.&lt;br&gt; &lt;br&gt; AddMusic also demonstrates how to mix application audio with iPod library audio. The sample includes code for configuring application audio behavior using the AVAudioSession class and Audio Session Services.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/AdvancedURLConnections/Introduction/Intro.html&#34;&gt;&lt;sup&gt;AdvancedURLConnections&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Networking &amp;amp; Internet&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Protocol Streams)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Foundation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample demonstrates various advanced networking techniques with NSURLConnection. Specifically, it demonstrates how to respond to authentication challenges, how to modify the default server trust evaluation (for example, to support a server with a self-signed certificate), and how to provide client identities.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/AgentsCatalog/Introduction/Intro.html&#34;&gt;&lt;sup&gt;AgentsCatalog: Using the Agents System in GameplayKit&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;General&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;GameplayKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Uses the Agents system in GameplayKit to let the characters in a game move themselves according to high-level goals. This sample demonstrates several of the individual goals that an agent can perform, such as moving to a target, avoiding obstacles, and following a predefined path. AgentsCatalog also shows how to tie multiple goals together to create more complex behaviors, such as making a group of agents seek a common target while moving together as a flock.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/sc2273/Introduction/Intro.html&#34;&gt;&lt;sup&gt;AirDrop Examples&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Networking &amp;amp; Internet&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&#34;AirDropSample&#34; demonstrates three use cases for incorporating AirDrop into an app.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/AirLocate/Introduction/Intro.html&#34;&gt;&lt;sup&gt;AirLocate: Using CoreLocation to monitor, range, and configure your device as an iBeacon&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;CoreLocation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&#34;AirLocate&#34; demonstrates CoreLocation fencing and ranging of iBeacons, BTLE devices enabled to aide iOS devices in determining a users proximity to a location rather than their position. Obtaining a users proximity with iBeacons is ideal in more intimate locations such as indoors where other positioning methods either do not work, or do not give the desired level of accuracy an iBeacon can provide. In addition to demonstrating how to use CoreLocation&#39;s CLLocationManager APIs to monitor and range for these CLBeaconRegions, AirLocate also provides an example of how to calibrate and configure an iOS device as a beacon.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/AlternateViews/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Alternate Views&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Windows &amp;amp; Views)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample demonstrates how to implement alternate or distinguishing views for particular device orientations. Doing so can be useful if your app displays different content between orientations or if your app uses vastly different layouts between orientations which cannot be reconciled by auto layout or programatic layout alone.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/Icons/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Application Icons and Launch Images for iOS&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;General&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Every app is required to include an app icon. It is recommended that apps also provide icons for: Spotlight, the Settings app, and when creating an Ad Hoc build and adding it to iTunes. See QA1686: App Icons on iPad and iPhone, for a complete listing of icons required for iPhone, iPad, and Universal apps &lt;a href=&#34;https://developer.apple.com/library/ios/qa/qa1686/_index.html&#34;&gt;https://developer.apple.com/library/ios/qa/qa1686/_index.html&lt;/a&gt;.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/ApplicationShortcuts/Introduction/Intro.html&#34;&gt;&lt;sup&gt;ApplicationShortcuts: Using UIApplicationShortcutItem&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Demonstrates how to use the UIApplicationShortcutItem class to provide quick access to parts of your application directly from the device&#39;s home screen. The sample shows two static shortcuts (defined in the app&#39;s Info.plist), and two dynamic shortcuts (defined in code with the UIMutableApplicationShortcutItem class). The dynamic shortcuts can be edited to change the title, subtitle and icon.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/AppPrefs/Introduction/Intro.html&#34;&gt;&lt;sup&gt;AppPrefs: Storing and Retrieving User Preferences&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Data Management&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Preference Settings)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Demonstrates how to display your app&#39;s user configurable options (preferences) in the &#34;Settings&#34; system application. A settings bundle, included in your application’s bundle directory, contains the information needed by the Settings application to display your preferences and make it possible for the user to modify them. The Settings application saves any configured values in the defaults database so that your application can retrieve them at runtime.&lt;br&gt;&lt;br&gt;This sample also shows how to launch the Settings app from your application and how to dynamically update your application&#39;s UI when its settings are changed while the app is in the background.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/AQOfflineRenderTest/Introduction/Intro.html&#34;&gt;&lt;sup&gt;AQOfflineRenderTest&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Audio &amp;amp; Video&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Audio)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AudioToolbox&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Demonstrates using Audio Queue offline render functionality and the AudioQueueOfflineRender API. The sample produces LPCM output buffers from an ALAC encoded source which are then written to a .caf file. The output.caf file is then played back confirming the offline functionality worked as expected. All the code demonstrating the Audio Queue is in a single file called aqofflinerender.cpp.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/AstroLayout/Introduction/Intro.html&#34;&gt;&lt;sup&gt;AstroLayout: Building Adaptive UI with Auto Layout&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AstroLayout demonstrates how to properly activate and deactivate groups of constraints in response to a size class change. It also shows how to animate layout changes using UIView animations. You&#39;ll see how to use layout guides and anchors to reduce code overhead and allow for more complex layouts.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/iPhoneACFileConvertTest/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Audio Converter File Convert Test&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Audio &amp;amp; Video&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Audio)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AudioToolbox&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Demonstrates using the Audio Converter APIs to convert from a PCM audio format to a compressed format including AAC.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/MixerHost/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Audio Mixer (MixerHost)&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Audio &amp;amp; Video&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Audio)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AudioUnit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;MixerHost demonstrates how to use the Multichannel Mixer audio unit in an iOS application. It also demonstrates how to use a render callback function to provide audio to an audio unit input bus. In this sample, the audio delivered by the callback comes from two short loops read from disk. You could use a similar callback, however, to synthesize sounds to feed into a mixer unit. This sample is described in Audio Unit Hosting Guide for iOS.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/SysSound/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Audio UI Sounds (SysSound)&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Audio &amp;amp; Video&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Audio)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AudioToolbox&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Demonstrates use of System Sound Services (AudioToolbox/AudioServices.h) to play alerts and user-interface sound effects, and to invoke vibration.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/AudioUnitV3Example/Introduction/Intro.html&#34;&gt;&lt;sup&gt;AudioUnitV3Example: A Basic AudioUnit Extension and Host Implementation&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Audio &amp;amp; Video&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Audio)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Demonstrates how to build a fully-functioning example of an Audio Unit extension and Audio Unit host using the version 3 of the Audio Unit APIs. The Audio Unit Extensions API introduces a mechanism for developers to deliver AudioUnits to users on iOS. The same API is available on both iOS and OS X, and provides a bridging mechanism for existing version 2 AudioUnits to coexist in existing AudioUnit host applications, and in future version 3 hosts.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/aurioTouch/Introduction/Intro.html&#34;&gt;&lt;sup&gt;aurioTouch&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Audio &amp;amp; Video&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Audio)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AudioUnit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;aurioTouch demonstrates use of the remote i/o audio unit for handling audio input and output. The application can display the input audio in one of the forms, a regular time domain waveform, a frequency domain waveform (computed by performing a fast fourier transform on the incoming signal), and a sonogram view (a view displaying the frequency content of a signal over time, with the color signaling relative power, the y axis being frequency and the x as time). Tap the sonogram button to switch to a sonogram view, tap anywhere on the screen to return to the oscilloscope. Tap the FFT button to perform and display the input data after an FFT transform. Pinch in the oscilloscope view to expand and contract the scale for the x axis.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/sc1791/Introduction/Intro.html&#34;&gt;&lt;sup&gt;AVARLDelegateDemo&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Audio &amp;amp; Video&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AVFoundation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;The sample code depicts three different use cases of AVAssetResourceLoaderDelegate (for Identity encryption use case scenarios) for HLS (HTTP Live streaming): - Redirect handler (redirection for the HTTP live streaming media files) - Fetching Encryption keys for the HTTP live streaming media (segments) - Custom play list generation (index file) for the HTTP live streaming.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/AVCam/Introduction/Intro.html&#34;&gt;&lt;sup&gt;AVCam-iOS: Using AVFoundation to Capture Images and Movies&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Audio &amp;amp; Video&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AVFoundation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AVCam demonstrates how to use the AVFoundation capture API to record movies and capture still images. The sample has a record button for recording movies, a camera button for switching between front and back cameras (on supported devices), and a still button for capturing still images. AVCam runs only on an actual device, either an iPad or iPhone, and cannot be run in Simulator.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/AVCamManual/Introduction/Intro.html&#34;&gt;&lt;sup&gt;AVCamManual: Extending AVCam to Use Manual Capture API&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Audio &amp;amp; Video&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AVFoundation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AVCamManual adds manual controls for focus, exposure, and white balance to the AVCam sample application.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/AVCaptureToAudioUnit/Introduction/Intro.html&#34;&gt;&lt;sup&gt;AVCaptureAudioDataOutput To AudioUnit iOS&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Audio &amp;amp; Video&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Audio)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AVFoundation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AVCaptureToAudioUnit for iOS demonstrates how to use the CMSampleBufferRefs vended by AVFoundation&#39;s capture AVCaptureAudioDataOutput object with various CoreAudio APIs. The application uses a AVCaptureSession with a AVCaptureAudioDataOutput to capture audio from the default input, applies an effect to that audio using a simple delay effect AudioUnit and writes the modified audio to a file using the CoreAudio ExtAudioFile API. It also demonstrates using and AUGraph containing an AUConverter to convert the AVCaptureAudioDataOutput provided data format into a suitable format for the delay effect.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/AVCompositionDebugVieweriOS/Introduction/Intro.html&#34;&gt;&lt;sup&gt;AVCompositionDebugVieweriOS&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Audio &amp;amp; Video&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AVFoundation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample application has an AVCompositionDebugView which presents a visual description of the underlying AVComposition, AVVideoComposition and AVAudioMix objects which form the composition made using two clips, adding a cross fade transition in between and audio ramps to the two audio tracks. The visualization provided by the sample can be used as a debugging tool to discover issues with an incorrect composition/video composition. For example: a break in video composition would render black frames to screen, which can easily be detected using the visualization in the sample.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/AVCustomEdit/Introduction/Intro.html&#34;&gt;&lt;sup&gt;AVCustomEdit&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Audio &amp;amp; Video&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AVFoundation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;The sample demonstrates the use of custom compositors to add transitions to an AVMutableComposition. It implements the AVVideoCompositing and AVVideoCompositionInstruction protocols to have access to individual source frames, which are then be rendered using OpenGL off screen rendering.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/AVFoundationPiPPlayer/Introduction/Intro.html&#34;&gt;&lt;sup&gt;AVFoundationPiPPlayer: Picture-in-Picture Playback with AVKit&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AVFoundation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Demonstrates how to use the AVPictureInPictureController class to implement picture-in-picture video playback. It shows the steps required to start and stop picture-in-picture mode and to setup a delegate to receive event callbacks. Clients of AVFoundation using the AVPlayerLayer class for media playback must use the AVPictureInPictureController class, whereas clients of AVKit who use the AVPlayerViewController class get picture-in-picture mode without any additional setup.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/AVFoundationQueuePlayer-iOS/Introduction/Intro.html&#34;&gt;&lt;sup&gt;AVFoundationQueuePlayer-iOS: Using a Mixture of Local File Based Assets and HTTP Live Streaming Assets with AVFoundation&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Audio &amp;amp; Video&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AVFoundation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Demonstrates how to create a movie queue playback app using only the AVQueuePlayer and AVPlayerLayer classes of AVFoundation (not AVKit). You’ll find out how to manage a queue compromised of local, HTTP-live-streamed, and progressive-download movies. You’ll also see how to implement play, pause, skip, volume adjustment, time slider updating, and scrubbing.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/AVFoundationSimplePlayer-iOS/Introduction/Intro.html&#34;&gt;&lt;sup&gt;AVFoundationSimplePlayer-iOS: Using AVFoundation to Play Media&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Audio &amp;amp; Video&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AVFoundation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Demonstrates how to create a simple movie playback app using only the AVPlayer and AVPlayerLayer classes from AVFoundation (not AVKit). You&#39;ll see how to open a movie file and then how to implement various functionality including play, pause, fast forward, rewind, volume adjustment, time slider updating, and scrubbing.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/AVLoupe/Introduction/Intro.html&#34;&gt;&lt;sup&gt;AVLoupe&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Audio &amp;amp; Video&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Video)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AVFoundation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample demonstrates how to use multiple synchronized AVPlayerLayer instances, associated with a single AVPlayer, to efficiently produce non-trivial presentation of timed visual media. Using just one AVPlayer this sample demonstrates how you can display the same video in multiple AVPlayerLayers simultaneously. With minimal code you can create very customized and creative forms of video display. As an example, this sample demonstrates an interactive loupe, or magnifying glass, for video playback. This is similar to features that you might have used in iPhoto and Aperture.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/AVMetadataRecordPlay/Introduction/Intro.html&#34;&gt;&lt;sup&gt;AVMetadataRecordPlay: Timed Metadata Capture Recording and Playback&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Audio &amp;amp; Video&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AVFoundation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;The AVMetadataRecordPlay sample demonstrates how to use AVFoundation capture APIs to record and play movies with timed metadata content. The sample also shows how to use timed-metadata tracks to record detected-face, video orientation, and GPS metadata. When playing back content, the AVMetadataRecordPlay class reads the detected-face and GPS timed metadata tracks and uses it to render augmentations on the video layer to indicate their values. AVMetadataRecordPlay also reads the video orientation metadata and dynamically adjusts the video layer to properly render the content. The sample runs only on an actual device (iPad or iPhone); you can’t run it in the Simulator.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/AVMovieExporter/Introduction/Intro.html&#34;&gt;&lt;sup&gt;AVMovieExporter&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Audio &amp;amp; Video&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Video)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AVFoundation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This universal sample application reads movie files from the Asset Library and Media Library then exports them to a new media file using user defined settings. The user can adjust the exported file in the following ways:&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/AVPlayerDemo/Introduction/Intro.html&#34;&gt;&lt;sup&gt;AVPlayerDemo&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Audio &amp;amp; Video&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Video)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AVFoundation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Uses AVPlayer to play videos from the iPod Library, Camera Roll, or via iTunes File Sharing. Also displays metadata.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/ReaderWriter/Introduction/Intro.html&#34;&gt;&lt;sup&gt;AVReaderWriter: Offline Audio / Video Processing&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AVFoundation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample demonstrates how to use AVAssetReader and AVAssetWriter to perform offline (i.e. non-real-time) processing of video and audio.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/AVSimpleEditoriOS/Introduction/Intro.html&#34;&gt;&lt;sup&gt;AVSimpleEditoriOS&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Audio &amp;amp; Video&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AVFoundation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AVSimpleEditor is a simple AVFoundation based movie editing application which exercises the APIs of AVVideoComposition, AVAudioMix and demonstrates how they can be used for simple video editing tasks. It also demonstrates how they interact with playback (AVPlayerItem) and export (AVAssetExportSession). The application performs trim, rotate, crop, add music, add watermark and export. This sample is ARC-enabled.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/AVTimedAnnotationWriter/Introduction/Intro.html&#34;&gt;&lt;sup&gt;AVTimedAnnotationWriter: Using Custom Annotation Metadata for Movie Writing and Playback&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AVFoundation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Demonstrates how to use the AVAssetWriterInputMetadataAdaptor API to write circle annotation metadata during video playback. The captured movie file has video, audio and metadata track. The metadata track contains circle annotation which is vended during playback using AVPlayerItemMetadataOutput.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/avTouch/Introduction/Intro.html&#34;&gt;&lt;sup&gt;avTouch&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Audio &amp;amp; Video&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Audio)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AVFoundation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;The avTouch sample demonstrates use of the AVAudioPlayer class for basic audio playback.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/Bananas/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Bananas: Building a Game with SceneKit&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Graphics &amp;amp; Animation&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(3D Drawing)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;SceneKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample shows how to build a basic game using Scene Kit, demonstrating physics, rendering techniques, lighting, actions and animation.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/UIImageEffects/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Blurring and Tinting an Image&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Graphics &amp;amp; Animation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIImageEffects demonstrates how to create and apply blur and tint effects to an image using the vImage, Quartz, and UIKit frameworks. The vImage framework is suited for high-performance image processing. Using vImage, your app gets all the benefits of vector processing without the need for you to write vectorized code.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/BonjourWeb/Introduction/Intro.html&#34;&gt;&lt;sup&gt;BonjourWeb&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Networking &amp;amp; Internet&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Services &amp;amp; Discovery)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Foundation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This application illustrates the fundamentals of browsing for network services using Bonjour. The BonjourBrowser hierarchically displays Bonjour domains and services as table views in a navigation controller. The contents of the table views are discovered and updated dynamically using NSNetServiceBrowser objects. Tapping an item in the services table causes the corresponding NSNetService object to be resolved asynchronously. When that resolution completes, a delegate method is called which constructs a URL and opens it in Safari.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/BracketStripes/Introduction/Intro.html&#34;&gt;&lt;sup&gt;BracketStripes: Using the Bracketed Capture API&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AVFoundation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;BracketStripes&lt;br&gt;&lt;br&gt;This sample illustrates the use of still image bracketing APIs available in AVFoundation.&lt;br&gt;&lt;br&gt;Two types of brackets are demonstrated:&lt;br&gt;&lt;br&gt;1. Auto-exposure brackets with exposure target bias, and&lt;br&gt;&lt;br&gt;2. Manual exposure with control over ISO and exposure duration.&lt;br&gt;&lt;br&gt;As each of the bracketed frames are captured in real-time, they are &#34;striped&#34; into a destination image buffer and later shown in a modal image viewer so each of the captured frames can be compared side-by-side.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/Breadcrumb/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Breadcrumb&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;MapKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Demonstrates how to draw a path using the Map Kit overlay, MKOverlayView, that follows and tracks the user&#39;s current location. The included CrumbPath and CrumbPathView overlay and overlay view classes can be used for any path of points that are expected to change over time. It also demonstrates what is needed to track the user&#39;s location as a background process.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/BTLE_Transfer/Introduction/Intro.html&#34;&gt;&lt;sup&gt;BTLE Central Peripheral Transfer&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;CoreBluetooth&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample shows how to transfer data from an iOS device in CoreBluetooth Peripheral Mode to another in Central Mode, by using a CBCharacteristic on the Peripheral side that changes its value. The value change is automatically picked up on the Central side.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/PrivacyPrompts/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Checking and Requesting Access to Data Classes in Privacy Settings&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Security&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;PrivacyPrompts shows how to check and request access to data classes such as Location, Contacts, and social media in Privacy Settings on iOS.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/CircleLayout/Introduction/Intro.html&#34;&gt;&lt;sup&gt;CircleLayout&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Windows &amp;amp; Views)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Shows how to use a collection view to arrange views on a circle.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/CloudCaptions/Introduction/Intro.html&#34;&gt;&lt;sup&gt;CloudCaptions: How integrate CloudKit into your application&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;CloudKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample shows how to use CloudKit to upload and retrieve CKRecords and associated assets. In this example, there are two record types, an image record type and a post record type. Users are able to upload their own photos or select an image already found in an image record type. This example also uses an NSPredicate in its CKQueries to filter results based on tags.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/CloudAtlas/Introduction/Intro.html&#34;&gt;&lt;sup&gt;CloudKit Catalog: An Introduction to CloudKit (Cocoa and JavaScript) &lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;CloudKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;CloudKit Catalog will get you up to speed quickly on CloudKit using either the Cloudkit API for JavaScript or the CloudKit API for iOS. The JavaScript portion of this sample code project is also available as a hosted version at &lt;a href=&#34;https://cdn.apple-cloudkit.com/cloudkit-catalog/&#34;&gt;https://cdn.apple-cloudkit.com/cloudkit-catalog/&lt;/a&gt; .&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/CloudPhotos/Introduction/Intro.html&#34;&gt;&lt;sup&gt;CloudPhotos : Using CloudKit with iOS&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;CloudPhotos is a clear and concise CloudKit sample showing how to share photos among other users. Photos are represented as CKRecords, holding the photo (CKAsset), its title (NSString), creation date (NSDate) and the location (CLLocation) it was created. The sample will display all photos found in the public CloudKit container. Users can view photo records, but only the owner of photo records can change or delete them. The attributes that can be edited are the photo itself and it’s title, the creation date and location data are read-only. Users add photo records from their photo library or camera role.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/CloudSearch/Introduction/Intro.html&#34;&gt;&lt;sup&gt;CloudSearch&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Data Management&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(File Management)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Foundation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Demonstrates how to find documents in iCloud, using NSMetaDataQuery. Included as part of this sample is a class called &#34;CloudDocumentsController&#34; which runs Spotlight queries, using NSMetaDataQuery, to discoved files found in iCloud. You can use this class to quickly gain access to those available files.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/CollectionViewTransition/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Collection View Transition&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Windows &amp;amp; Views)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample illustrates how to create a custom transition when navigating between two collection views in a navigation hierarchy managed by a navigation controller. The transition can be interrupted and reversed. It uses a subclass of UICollectionViewTransitionLayout to help in the transition of the cell positions based on gesture position.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/CollectionView-Simple/Introduction/Intro.html&#34;&gt;&lt;sup&gt;CollectionView-Simple&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Windows &amp;amp; Views)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Demonstrates how to use UICollectionView, a way to present ordered data to users in a grid-like fashion. With a collection view object, you are able to define the presentation and arrangement of embedded views. The collection view class works closely with an accompanying layout object to define the placement of individual data items. In this example UIKit provides a standard flow-based layout object that you can use to implement multi-column grids containing items of a standard size.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/CoreAudioUtilityClasses/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Core Audio Utility Classes&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Audio &amp;amp; Video&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Audio)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;CoreAudio&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;The &#34;CoreAudio&#34; folder contains the Public Utility sources (PublicUtility folder) as well as base classes required for codec and audio unit development. These utility classes are used by various Apple Core Audio sample project and extend or wrap Core Audio API&#39;s.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/PhotoLocations/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Core Data Transformable Attributes&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Data Management&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;CoreData&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample illustrates a Core Data application that uses more than one entity and uses transformable attributes. It also shows inferred migration of the persistent store.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/CIFunHouse/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Core Image Filters with Photos and Video for iOS&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Graphics &amp;amp; Animation&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(2D Drawing)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;CoreImage&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;The CIFunHouse project shows how to apply Core Image built in and custom CIFilters to photos and video. The application presents view controllers for adding photo and video sources, choosing CIFilters from a list, and making live adjustments to filter parameters. The project also contains code for custom CIFilter subclasses for effect such as Sobel edge detection, old-style-film, and fake-depth-of-field looks. The code also demonstrates how to save a filtered video stream to the ALAssetsLibrary while simultaneously previewing the video on the display.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/TemperatureSensor/Introduction/Intro.html&#34;&gt;&lt;sup&gt;CoreBluetooth Temperature Sensor&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;CoreBluetooth&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;A simple iOS iPhone application that demonstrates how to use the CoreBluetooth Framework to connect to a Bluetooth LE peripheral and read, write and be notified of changes to the characteristics of the peripheral.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/CoreDataBooks/Introduction/Intro.html&#34;&gt;&lt;sup&gt;CoreDataBooks&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Data Management&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;CoreData&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample illustrates a number of aspects of working with the Core Data framework with an iOS application:&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/CoreTextPageViewer/Introduction/Intro.html&#34;&gt;&lt;sup&gt;CoreTextPageViewer&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Windows &amp;amp; Views)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;CoreText&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample shows how to use Core Text to display large bodies of text, text with mixed styles, and text with special style or layout requirements, such as use of custom fonts. A version of this sample was used in the &#34;Advanced Text Handling for iPhone OS&#34; WWDC 2010 Session.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/CryptoExercise/Introduction/Intro.html&#34;&gt;&lt;sup&gt;CryptoExercise&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Security&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Security&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample demonstrates the use of the two main Cryptographic API sets on the iPhone OS SDK. Asymmetric Key Encryption and random nonce generation is handled through the Security framework API set, whereas, Symmetric Key Encryption and Digest generation is handled by the CommonCrypto API set. The CryptoExercise sample brings both of these APIs together through a network service, discoverable via Bonjour, that performs a &#34;dummy&#34; cryptographic protocol between devices found on the same subnet.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/CurrentAddress/Introduction/Intro.html&#34;&gt;&lt;sup&gt;CurrentAddress&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;MapKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample makes use of the CLGeocoder class that provides services for converting your map coordinate (specified as a latitude/longitude pair) into information about that coordinate, such as the country, city, or street. A reverse geocoder object is a single-shot object that works with a network-based map service to look up placemark information for its specified coordinate value. To use placemark information is leverages the MKPlacemark class to store this information.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/sc2284/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Custom Animatable Property&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Graphics &amp;amp; Animation&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Animation)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;CoreGraphics&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Shows how to leverage Core Animation’s timing and rendering callbacks to implement custom animatable properties for CALayer subclasses. This technique is supported whether your CALayer subclass belongs to a UIView or is standalone. Both explicit and implicit animation triggers are demonstrated, as well as basic and keyframe animation types.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/DateSectionTitles/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Custom Section Titles with NSFetchedResultsController&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Data Management&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;CoreData&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&#34;DateSectionTitles&#34; shows how to create section information for NSFetchedResultsController using dates.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/CustomTransitions/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Custom View Controller Presentations and Transitions&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Windows &amp;amp; Views)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Custom View Controller Presentations and Transitions demonstrates using the view controller transitioning APIs to implement your own view controller presentations and transitions. Learn from a collection of easy to understand examples how to use UIViewControllerAnimatedTransitioning, UIViewControllerInteractiveTransitioning, and UIPresentationController to create unique presentation styles that adapt to the available screen space.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/sc2216/Introduction/Intro.html&#34;&gt;&lt;sup&gt;CustomContentAccessibility&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample, previously known as WWDCMaps, shows you how to support accessibility in a custom drawing UIView and UIControl, demonstrates how to create an accessibility element for each map item, and implement UIAccessibilityContainer protocol in the container view to interact with iOS accessibility system. The Guided Access Restriction API, which is newly introduced in iOS 7 for restricting functions when Guided Access enabled, is also demonstrated in this sample.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/CustomHTTPProtocol/Introduction/Intro.html&#34;&gt;&lt;sup&gt;CustomHTTPProtocol&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Networking &amp;amp; Internet&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Foundation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;CustomHTTPProtocol shows how to use an NSURLProtocol subclass to intercept the NSURLConnections made by a high-level subsystem that does not otherwise expose its network connections. In this specific case, it intercepts the HTTPS requests made by a web view and overrides server trust evaluation, allowing you to browse a site whose certificate is not trusted by default.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/NavBar/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Customizing UINavigationBar&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Controls)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;NavBar demonstrates using UINavigationController and UIViewController classes together as building blocks to your application&#39;s user interface. Use it as a reference when starting the development of your new application. The various pages in this sample exhibit different ways of how to modify the navigation bar directly, using the appearance proxy, and by modifying the view controller&#39;s UINavigationItem. Among the levels of customization are varying appearance styles, and applying custom left and right buttons known as UIBarButtonItems.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/DateCell/Introduction/Intro.html&#34;&gt;&lt;sup&gt;DateCell&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Tables)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Demonstrates formatted display of date objects in table cells and use of UIDatePicker to edit those values.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/DemoBots/Introduction/Intro.html&#34;&gt;&lt;sup&gt;DemoBots: Building a Cross Platform Game with SpriteKit and GameplayKit&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Graphics &amp;amp; Animation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;SpriteKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;DemoBots is a fully-featured 2D game built with SpriteKit and GameplayKit, and written in Swift 2.0. It demonstrates how to use agents, goals, and behaviors to drive the movement of characters in your game, and how to use rule systems and state machines to provide those characters with intelligent behavior. You&#39;ll see how to integrate on-demand resources into a game to optimize resource usage and reduce the time needed to download additional levels. DemoBots takes advantage of the Xcode 7 scene and actions editor to create detailed levels and animations. The sample also contains assets tailored to ensure the best experience on every supported device.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/DocInteraction/Introduction/Intro.html&#34;&gt;&lt;sup&gt;DocInteraction&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Data Management&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(File Management)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Demonstrates how to use UIDocumentInteractionController to obtain information about documents and how to preview them. There are two ways to preview documents: one is to use UIDocumentInteractionController&#39;s preview API, the other is directly use QLPreviewController. This sample also demonstrates the use of UIFileSharingEnabled feature so you can upload documents to the application using iTunes and then to preview them. With the help of &#34;kqueue&#34; kernel event notifications, the sample monitors the contents of the Documents folder.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/DownloadFont/Introduction/Intro.html&#34;&gt;&lt;sup&gt;DownloadFont&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;CoreText&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Demonstrates how to download fonts on demand on iOS 6 and later.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/EADemo/Introduction/Intro.html&#34;&gt;&lt;sup&gt;EADemo&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Data Management&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Device Information)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;ExternalAccessory&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;The sample can be used with any Made For iPod (MFI) device designed for use with the External Accessory Framework. The application will display an External Accessory attached device in the Accessories window, provide information registered by the MFI device, and provides methods to send and receive data to the device.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/EKReminderSuite/Introduction/Intro.html&#34;&gt;&lt;sup&gt;EKReminderSuite&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;EKReminderSuite is a set of sample code that demonstrates how to implement reminders using the EventKit Framework.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/Emporium/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Emporium: A Simple Shopping Experience with Apple Pay&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;PassKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample shows how to integrate Apple Pay into a simple shopping experience. You&#39;ll learn how to make payment requests, collect shipping and contact information, apply discounts for debit/credit cards, and use the Apple Pay button. This project also contains an Apple Watch WatchKit extension that shows you how to start make Apple Pay transactions using Handoﬀ with the NSUserActivity class.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/FastEnumerationSample/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Enumeration Sample&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Foundation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;EnumerationSample is a command line project that demonstrates how to implement a class that supports block-based enumeration, fast enumeration, enumeration using NSEnumerator, and subscripting. While provided as a OS X application, the techniques demonstrated by this sample are fully applicable to iOS development.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/UsingPhotosFramework/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Example app using Photos framework&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Photos&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;A basic Photos-like app which introduces the Photos framework. &lt;br&gt;&lt;br&gt;- List albums, folders, and moments.&lt;br&gt;&lt;br&gt;- Display the contents of moments or albums.&lt;br&gt;&lt;br&gt;- Display the content of:&lt;br&gt;&lt;br&gt; * A single photo.&lt;br&gt;&lt;br&gt; * A video; allowing playback.&lt;br&gt;&lt;br&gt; * A Live Photo; allowing playback via 3D touch gestures or manually via a button.&lt;br&gt;&lt;br&gt;- Allow the following actions:&lt;br&gt;&lt;br&gt; * Single-click editing of a photo.&lt;br&gt;&lt;br&gt; * Creating an album and adding assets to it.&lt;br&gt;&lt;br&gt; * Re-ordering assets in an album.&lt;br&gt;&lt;br&gt; * Removing assets from an album.&lt;br&gt;&lt;br&gt; * Deleting assets and albums.&lt;br&gt;&lt;br&gt; * Hiding and unhiding assets from moments.&lt;br&gt;&lt;br&gt; * Favoriting assets.&lt;br&gt;&lt;br&gt; * Initiating playback of a Live Photo.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/iPhoneExtAudioFileConvertTest/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Extended Audio File Conversion Test&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Audio &amp;amp; Video&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Audio)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AudioToolbox&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Demonstrates using ExtAudioFile API to convert from one audio format and file type to another.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/Fit/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Fit: Store and Retrieve HealthKit Data&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;HealthKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Fit is a sample intended as a quick introduction to HealthKit. It teaches you everything from writing data into HealthKit to reading data from HealthKit. This information may have been entered into the store by some other app; e.g. a user&#39;s birthday may have been entered into Health, and a user&#39;s weight by some popular weight tracker app. Fit shows examples of using queries to retrieve information from HealthKit using sample queries and statistics queries. Fit gives you a quick introduction into using the new Foundation classes NSLengthFormatter, NSMassFormatter, and NSEnergyFormatter.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/footprint/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Footprint: Indoor Positioning with Core Location&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;CoreLocation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Use Core Location to take a Latitude/Longitude position and project it onto a flat floorplan. Demonstrates how to do the conversions between a Geographic coordinate system (Latitude/Longitude), a floorplan PDF coordinate system (x, y), and MapKit.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/FourInARow/Introduction/Intro.html&#34;&gt;&lt;sup&gt;FourInARow: Using the GameplayKit Minmax Strategist for Opponent AI&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;General&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;GameplayKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample demonstrates how to use the GPMinmaxStrategist class to implement a computer-controlled opponent for a simple board game. You&#39;ll see how to structure gameplay model code for use with the minmax strategist using the GPGameModel protocol and related APIs.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/Fox/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Fox: Building a SceneKit Game with the Xcode Scene Editor&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;General&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;SceneKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample demonstrates how to use the Xcode Scene Editor to build a level in a SceneKit-based game. You’ll see how to choose between the Metal and OpenGL ES renderer, add positional audio triggers, and set up light maps using material properties. This sample supports tvOS and Game Controllers.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/GenericKeychain/Introduction/Intro.html&#34;&gt;&lt;sup&gt;GenericKeychain&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Security&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Security&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample shows how to add, query for, remove, and update a keychain item of generic class type. Also demonstrates the use of shared keychain items. All classes exhibit very similar behavior so the included examples will scale to the other classes of Keychain Item: Internet Password, Certificate, Key, and Identity.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/GeocoderDemo/Introduction/Intro.html&#34;&gt;&lt;sup&gt;GeocoderDemo: Uses CLGeocoder for forward and reverse geocoding&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Data Management&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;CoreLocation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample application demonstrates using a CLGeocoder instance to perform forward and reverse geocoding on strings and dictionaries. The application also includes an example distance calculator that will display the distance between two placemarks.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/BatteryStatus/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Get Battery Status&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Data Management&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Device Information)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Demonstrates the use of the battery status properties and notifications provided via the iOS SDK.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/GKAchievements/Introduction/Intro.html&#34;&gt;&lt;sup&gt;GKAchievements&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;General&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;GameCenter&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Abstract: Provide an example of how to successfully submit achievements and store them when submission fails.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/GKAuthentication/Introduction/Intro.html&#34;&gt;&lt;sup&gt;GKAuthentication&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;General&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;GameCenter&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;An example of how to successfully authenticate using GameKit.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/GKLeaderboards/Introduction/Intro.html&#34;&gt;&lt;sup&gt;GKLeaderboards&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;General&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;GameCenter&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;GKLeaderboard is a sample application that shows how to correctly submit a score and view them using GKLeaderboardViewController.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/GKTapper/Introduction/Intro.html&#34;&gt;&lt;sup&gt;GKTapper&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;GameCenter&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;GKTapper is a sample application that shows how to support GameCenter Leaderboards and Achievements. It also demonstrates using GKLeaderboardViewController and GKAchievementViewController to display this data.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/GLAirplay/Introduction/Intro.html&#34;&gt;&lt;sup&gt;GLAirplay&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;OpenGLES&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Demonstrates how to provide a richer experience to your users when they are using Airplay by displaying your UI on the iPhone/iPad and your app/game contents on the second display.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/GLCameraRipple/Introduction/Intro.html&#34;&gt;&lt;sup&gt;GLCameraRipple&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Audio &amp;amp; Video&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Video)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AVFoundation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample demonstrates how to use the AVFoundation framework to capture YUV frames from the camera and process them using shaders in OpenGL ES 2.0. CVOpenGLESTextureCache, which is new to iOS 5.0, is used to provide optimal performance when using the AVCaptureOutput as an OpenGL texture. In addition, a ripple effect is applied by modifying the texture coordinates of a densely tessellated quad.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/GLEssentials/Introduction/Intro.html&#34;&gt;&lt;sup&gt;GLEssentials&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Graphics &amp;amp; Animation&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(3D Drawing)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;OpenGLES&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample provides an example of some essential OpenGL functionality. There are usages of Vertex Buffer Objects (VBOs), Vertex Array Objects (VAOs), Framebuffer Objects (FBO), and GLSL Program Objects. It creates a VAO and VBOs from model data loaded in. It creates a texture for the model from image data&lt;br&gt;&lt;br&gt;and GLSL shaders from source also loaded in. It also creates an FBO and texture to render a reflection of the model. It uses an environment mapping GLSL program to apply the reflection texture to a plane. This sample also demonstrates sharing of OpenGL source code between iOS and OS X. Additionally, it implements fullscreen rendering, retina display support, and demonstrates how to obtain and use an OpenGL Core Profile rendering context on OS X.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/GLGravity/Introduction/Intro.html&#34;&gt;&lt;sup&gt;GLGravity&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Graphics &amp;amp; Animation&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(3D Drawing)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;OpenGLES&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;The GLGravity sample application demonstrates how to use the UIAccelerometer class in combination with OpenGL rendering. It shows how to extract the gravity vector from the accelerometer values using a basic low-pass filter, and how to build an OpenGL transformation matrix from it.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/GLImageProcessing/Introduction/Intro.html&#34;&gt;&lt;sup&gt;GLImageProcessing&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Graphics &amp;amp; Animation&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(3D Drawing)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;OpenGLES&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;The GLImageProcessing sample application demonstrates how to implement simple image processing filters (Brightness, Contrast, Saturation, Hue rotation, Sharpness) using OpenGL ES1.1. The sample also shows how to create simple procedural button icons using CoreGraphics.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/GLPaint/Introduction/Intro.html&#34;&gt;&lt;sup&gt;GLPaint&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Graphics &amp;amp; Animation&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(3D Drawing)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;OpenGLES&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;The GLPaint sample application demonstrates how to support single finger painting using OpenGL ES. This sample also shows how to detect a &#34;shake&#34; motion of the device. By looking at the code you&#39;ll see how to set up an OpenGL ES view and use it for rendering painting strokes. The application creates a brush texture from an image by first drawing the image into a Core Graphics bitmap context. It then uses the bitmap data for the texture.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/GLTextureAtlas/Introduction/Intro.html&#34;&gt;&lt;sup&gt;GLTextureAtlas&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Graphics &amp;amp; Animation&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(3D Drawing)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;OpenGLES&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample demonstrates how to use a texture atlas to draw multiple objects with different textures simultaneously using OpenGL ES. The application uses a texture atlas in the PVR format. By adding in degenerated triangles, and compute 3D transformations ourselves using matrices, we are able to collapse all the draw calls into one.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/Touches/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Handling Touches Using Responder Methods and Gesture Recognizers&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Data Management&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Event Handling)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample contains two applications that demonstrate how to handle touches, including multiple touches that move multiple objects: &#34;Touches_Responder&#34; demonstrates how to handle touches using UIResponder&#39;s: touches began, touches moved, and touches ended methods. &#34;Touches_GestureRecognizers&#34; demonstrates how to use UIGestureRecognizer objects to handle touch events.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/HazardMap/Introduction/Intro.html&#34;&gt;&lt;sup&gt;HazardMap&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;MapKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Demonstrates how to create a custom Map Kit overlay to display USGS earthquake hazard data. It shows how to create a custom Map Kit overlay and corresponding view to display USGS earthquake hazard data on top of an MKMapView.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/HeadsUpUI/Introduction/Intro.html&#34;&gt;&lt;sup&gt;HeadsUpUI&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Windows &amp;amp; Views)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Demonstrates how to implement a Heads Up or HUD-like user interface over the app&#39;s primary view controller. This essentially mimics the behavior of the MPMoviePlayerController&#39;s hovering controls for controlling movie playback. Developers can refer to this sample for best practices in how to implement this translucent kind of interface complete with animation and timer support.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/HelloGoodbye/Introduction/Intro.html&#34;&gt;&lt;sup&gt;HelloGoodbye: Using the Accessibility API to Widen Your User Base&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This project shows you how to use the Accessibility API to widen your user base. It demonstrates how you can adjust your user interface when a user has Bold Text, Reduce Transparency, Darken Colors, or Reduce Motion enabled. It also contains examples of API you can implement to allow a VoiceOver or Switch Control user to interact with your app.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/HomeKitCatalog/Introduction/Intro.html&#34;&gt;&lt;sup&gt;HomeKit Catalog: Creating Homes, Pairing and Controlling Accessories, and Setting Up Triggers&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;General&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;HomeKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;HomeKit Catalog demonstrates how to use the HomeKit framework. Use this project as a reference for interacting with objects and performing common tasks such as creating homes, pairing with and controlling accessories, and setting up triggers to automate actions.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/iAdInterstitialSuite/Introduction/Intro.html&#34;&gt;&lt;sup&gt;iAdInterstitialSuite&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;iAd&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;iAdInterstitialSuite contains two applications that demonstrate the usage of the ADInterstitialAd introduced in iOS 4.3.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/iAdSuite_Storyboard/Introduction/Intro.html&#34;&gt;&lt;sup&gt;iAdSuite with Storyboards&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;iAd&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;iAdSuite is a set of samples demonstrating how to manage an ADBannerView in many common scenarios, each scenario demonstrated in a particular sample application.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/InterAppAudioSuite/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Inter-App Audio Examples&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Audio &amp;amp; Video&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Audio)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AudioUnit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This suite of samples includes three projects that together illustrate Inter-App Audio feature.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/InternationalMountains/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Internationalization and Localization for iOS&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Data Management&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Strings, Text, &amp;amp; Fonts)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Drawing from the existing Cocoa Internationalization Mountains sample, this sample shows how to integrate, design and programmatically access localized resources and data in an iOS application. This sample uses multiple localized views, localized formatted strings, localized application data, localized info.plist strings, and a localized application preferences settings bundle. The sample is localized in three languages: English, French, and Traditional Chinese.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/iPhoneCoreDataRecipes/Introduction/Intro.html&#34;&gt;&lt;sup&gt;iPhoneCoreDataRecipes&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Data Management&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;CoreData&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample shows how you can use view controllers, table views, and Core Data in an iPhone application.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/KeyboardAccessory/Introduction/Intro.html&#34;&gt;&lt;sup&gt;KeyboardAccessory&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Shows how to use a keyboard accessory view.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/KeychainTouchID/Introduction/Intro.html&#34;&gt;&lt;sup&gt;KeychainTouchID: Using Touch ID with Keychain and LocalAuthentication&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;LocalAuthentication&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;The KeychainTouchID sample shows how to store Touch ID protected items to the keychain and how to query for those items with custom message prompts. You’ll see how to use the keychain item accessibility class, which invalidates items when the passcode is removed. You’ll also find out how to use the LocalAuthentication class to invoke Touch ID verification without involving the keychain.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/KMLViewer/Introduction/Intro.html&#34;&gt;&lt;sup&gt;KMLViewer&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;MapKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Demonstrates how to display KML files on top of a MKMapView. It shows how to use MapKit&#39;s Annotations and Overlays to display KML files on top of an MKMapView.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/LargeImageDownsizing/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Large Image Downsizing&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Graphics &amp;amp; Animation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;CoreGraphics&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This code sample demonstrates a way to support displaying very large images in limited memory environments by turning a large image on disk into a smaller image in memory. This is useful in situations where the original image is too large to fit into memory as required for it to be displayed.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/LaunchMe/Introduction/Intro.html&#34;&gt;&lt;sup&gt;LaunchMe&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Data Management&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Event Handling)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;The LaunchMe sample application demonstrates how to implement a custom URL scheme to allow other applications to interact with your application. It registers the &#34;launchme&#34; URL scheme, of which URLs contain an HTML color code (e.g. #FF0000 or #F00). The sample shows how to handle an incoming URL request by overriding -application:openURL:sourceApplication:annotation: to properly parse and extract information from the requested URL before updating the user interface.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/LazyTableImages/Introduction/Intro.html&#34;&gt;&lt;sup&gt;LazyTableImages: Populating UITableView content asynchronously&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Tables)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample demonstrates a multi-stage approach to loading and displaying a UITableView. It displays the top paid iOS apps on Apple&#39;s App Store.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/ListAdder/Introduction/Intro.html&#34;&gt;&lt;sup&gt;ListAdder&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Data Management&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Foundation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample demonstrates the technique of thread confinement using NSOperation. It was written to support TN2109 &#34;Simple and Reliable Threading with NSOperation&#34;.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/Lister/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Lister (for watchOS, iOS, and OS X)&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Lister is a document-based productivity app for iOS 9, OS X 10.11, and watchOS 2. It demonstrates Apple Watch integration; using iCloud to share content between multiple app targets and platforms; using WatchConnectivity to communicate data between an iOS app and its counterpart watchOS 2 app; creating App Extensions to provide Today Widgets and Watch Apps prior to watchOS 2; and defining a framework to share common code between multiple targets. The Lister sample app is provided in both Swift and Objective-C.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/LocateMe/Introduction/Intro.html&#34;&gt;&lt;sup&gt;LocateMe&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Data Management&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Device Information)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;CoreLocation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This demonstrates the two primary use cases for the Core Location Framework: getting the user&#39;s location and tracking changes to the user&#39;s location.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/LookInside/Introduction/Intro.html&#34;&gt;&lt;sup&gt;LookInside: Presentation Controllers, Adaptivity, and Custom Animator Objects&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This example shows how to use a custom presentation controller to create a custom view controller presentation. It provides a transitioning delegate to the view controller, which vends a presentation controller and animator object.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/sc2279/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Managed App Configuration&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;General&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&#34;ManagedAppConfig&#34; demonstrates how to implement managed app configuration and feedback support in an iOS application. This functionality allows a Mobile Device Management (MDM) server to push down a dictionary into the managed app&#39;s NSUserDefaults for the purposes of remotely configuring settings. Also, feedback (such as critical errors) can be written by the app into NSUserDefaults which can then be queried by an MDM server. This is a powerful mechanism enterprise and educational institutions can use to remotely configure managed applications from a centralized MDM server.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/MapCallouts/Introduction/Intro.html&#34;&gt;&lt;sup&gt;MapCallouts: Using MapKit Annotations&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;MapKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Demonstrates the use of the MapKit framework, displaying a map view with custom MKAnnotations each with custom callouts or custom MKAnnotationViews. An annotation object on a map is any object that conforms to the MKAnnotation protocol and is displayed on the screen as a MKAnnotationView. Through the use of the MKAnnotation protocol and MKAnnotationView, this application shows how you can extend annotations with custom strings and left, right and detail calloutAccessoryViews.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/MapSearch/Introduction/Intro.html&#34;&gt;&lt;sup&gt;MapSearch&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Data Management&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;MapKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Demonstrates how to programmatically search for map-based addresses and points of interest using the MKLocalSearch class. It initiates a search for map-based content using a natural language string. A user can type &#34;coffee&#34;, press search and it will find all the coffee places nearby. The places found are centered around the user&#39;s current location. Once the search results have been found, the sample shows various ways to display the results. It demonstrates how to use MKLocalSearchCompletionHandler and populate the UITableView with the search results.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/MetalPartialSumsCompute/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Matrix Multiplication Performance Test&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Mathematical Computation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Metal&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Metal compute example of matrix multiplication using partial sums.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/MessageComposer/Introduction/Intro.html&#34;&gt;&lt;sup&gt;MessageComposer&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;MessageUI&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This application shows how to use the MessageUI framework to compose and send email and SMS messages from within your application.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/Metal_NBody_Simulation/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Metal N-Body Simulation&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;General&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Metal&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Performs an N-body simulation which calculates a gravity field and corresponding velocity and acceleration contributions accumulated by each body in the system from every other body. This further demonstrates performant techniques for compute kernel generating visualization data for the fragment and vertex stages in the Metal render pipeline.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/MetalBasic3D/Introduction/Intro.html&#34;&gt;&lt;sup&gt;MetalBasic3D&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Graphics &amp;amp; Animation&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(3D Drawing)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Metal&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This Metal example has been ported from the Xcode OpenGL ES template and includes a reusable view class and a geometric math library built on top of the simd.h library. The math library has been created for performing geometric graphics operations within Metal&#39;s left hand coordinate system and is used in this sample to set up a basic perspective, look at and model view projection matrix for objects rendered in the scene. Each cube is renderered individually using a basic 3D lighting shader with diffuse and ambient components. Additional effects have been added to one of the spinning cubes from the template to demonstrate altering a single object&#39;s uniform values per frame while keeping the other constant, all the while sharing the same buffer in memory. The cube geometry is defined with interlaced vertex and normal data and demonstrates how stride can be read within a shader.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/MetalDeferredLighting/Introduction/Intro.html&#34;&gt;&lt;sup&gt;MetalDeferredLighting&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Graphics &amp;amp; Animation&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(3D Drawing)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Metal&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;MetalDeferredLighting is designed as an example of rendering of a g-buffer and light accumulation in a single render encoder in one pass using programmable blending. In this sample, we render in 2 passes. As such it is also an example of a multipass renderer in Metal. A &#34;pass&#34; in this case is defined as all draws to a texture before swapping it out for a new texture. The first pass renders a shadow map based on the calculated position of a sun. The second pass performs a deferred lighting algorithm by writing to and reading from a framebuffer containing 4 color attachments. Three of the attachments are seeded with the g-buffer values (albedo, linear depth, normal). Next, light primitives are rendered to accumulate light into the light accumulation attachment, reading the g-buffer values directly from the other attachments. A full screen quad combines the light accumulation buffer with the albedo texture, samples the shadow map as a texture, and applies the light contribution from the sun. The results of this composition pass overwrites the albedo attachment with the final composited output. Lastly, particles representing each point light (lovingly called fairies) are rendered on top. What began as the albedo texture in the g-buffer now contains the final value, which can be presented to the display as its texture is the CAMetalLayer&#39;s drawable texture.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/MetalImageProcessing/Introduction/Intro.html&#34;&gt;&lt;sup&gt;MetalImageProcessing&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Graphics &amp;amp; Animation&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(3D Drawing)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Metal&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample shows how to load a texture into a 2D Quad and perform a simple grayscale conversion compute post processing pass using Metal.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/MetalInstancedHelix/Introduction/Intro.html&#34;&gt;&lt;sup&gt;MetalInstancedHelix&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Metal&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This example renders a set of cubes using Metal and alternates their colors by modifying each cube&#39;s uniforms directly in the shared CPU/GPU memory buffer. Several parameters can be modified directly in the AAPLRenderer.mm file including the number of cubes and their size. The cubes are rendered into a helix path using spherical coordinate system to get x,y,z for the translation matrix. Each cube is renderered individually using a basic 3D phong lighting shader, but drawn in only a single draw call using Metal&#39;s instancing API. Note, for each frame, each cube&#39;s transformation matrix is update along with its color, therefore in each frame the sample must traverese through 2n cubes.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/MetalKitEssentials/Introduction/Intro.html&#34;&gt;&lt;sup&gt;MetalKitEssentials: Using the MetalKit View, Texture Loader, and Model I/O&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Graphics &amp;amp; Animation&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(3D Drawing)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;MetalKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample demonstrates how to use key functionality provided by MetalKit on both iOS and OS X, including usage of the MetalKit view, texture loader, and the Model I/O integration. The sample loads uses the MetalKit view to get Metal rendering, The texture loader to load 2D and cube map assets, and the Model I/O framework to load an OBJ file with help from MetalKit to render the mesh object.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/MetalPerformanceShadersHelloWorld/Introduction/Intro.html&#34;&gt;&lt;sup&gt;MetalPerformanceShadersHelloWorld: An Intro to MetalPerformanceShaders&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Graphics &amp;amp; Animation&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(3D Drawing)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;MetalPerformanceShaders&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample demonstrates how easy it is to use the MetalPerformanceShaders framework. To convince you of its ease of use, the sample shows how to process an image using the Gaussian blur kernel provided by the framework. You’ll also see how MetalPerformanceShaders integrates nicely with MetalKit.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/MetalShaderShowcase/Introduction/Intro.html&#34;&gt;&lt;sup&gt;MetalShaderShowcase&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Metal&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Metal Shader Showcase demonstrates a variety of visual techniques optimized with Metal. It contains 7 unique shaders: a Phong shader, a wood shader, a fog shader, a cel shader, a normal map shader, and a particle system shader. Using the Metal reflection API, the renderer queries the shaders for what arguments are needed and presents them to the render encoder.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/MetalTexturedQuad/Introduction/Intro.html&#34;&gt;&lt;sup&gt;MetalTexturedQuad&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Graphics &amp;amp; Animation&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(3D Drawing)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Metal&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Demonstrates how to create a basic textured quad in metal. It includes all transformations needed to ensure correct rendering orientation of the textured quad.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/MetalUniformStreaming/Introduction/Intro.html&#34;&gt;&lt;sup&gt;MetalUniformStreaming&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Metal creates data buffer resources that can be read and written to on the CPU and GPU asynchronously. This example demonstrates using a data buffer to set uniforms for the vertex and fragment shaders.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/MetalVertexStreaming/Introduction/Intro.html&#34;&gt;&lt;sup&gt;MetalVertexStreaming&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Graphics &amp;amp; Animation&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(3D Drawing)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Metal&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample shows how to stream vertex data between 3 command buffers using one block of memory shared by both the CPU and GPU.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/MetalVideoCapture/Introduction/Intro.html&#34;&gt;&lt;sup&gt;MetalVideoCapture&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Metal&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample demonstrates how to stream captured video textures (from the front facing camera on an iOS device) into a 3D scene rendered with Metal. The video texture is combined with an environment map reflection from a cubemap (which is also rendered seperatly as the starfield skybox) and a 2D mipmap PVRTC texture (copper metal texture). This sample replaces the earlier MetalEnvironmentMapping sample.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/iPhoneMixerEQGraphTest/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Mixer iPodEQ AUGraph Test&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Audio &amp;amp; Video&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;CoreAudio&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Demonstrates how to build an Audio Unit Graph connecting a MultiChannel Mixer to the iPodEQ unit then to the RemoteIO unit.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/sc1249/Introduction/Intro.html&#34;&gt;&lt;sup&gt;MotionEffects&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;MotionEffects demonstrates applying motion effects to views in order to enhance the illusion of depth by creating parallxing effects. This sample is broken down into three different parts, each demonstrating a unique way to use motion effects.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/MotionGraphs/Introduction/Intro.html&#34;&gt;&lt;sup&gt;MotionGraphs&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;CoreMotion&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&#34;MotionGraphs&#34; demonstrates a how to use the push method to receive data from Core Motion. It displays graphs of accelerometer, gyroscope, and device motion data.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/MoveMe/Introduction/Intro.html&#34;&gt;&lt;sup&gt;MoveMe&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This application illustrates simple drawing, touch handling, and animation using UIKit and Core Animation.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/MoviePlayer_iPhone/Introduction/Intro.html&#34;&gt;&lt;sup&gt;MoviePlayer&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Audio &amp;amp; Video&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Video)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;MediaPlayer&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Demonstrates how to use the Media Player framework to play a movie from a file or network stream, and configure the movie background color, playback controls, background color and image, scaling and repeat modes. It also shows how to draw custom overlay controls on top of the movie during playback.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/AudioTapProcessor/Introduction/Intro.html&#34;&gt;&lt;sup&gt;MTAudioProcessingTap Audio Processor&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Audio &amp;amp; Video&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AVFoundation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Sample application that uses the MTAudioProcessingTap in combination with AV Foundation to visualize audio samples as well as applying a Core Audio audio unit effect (Bandpass Filter) to the audio data.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/MultipeerGroupChat/Introduction/Intro.html&#34;&gt;&lt;sup&gt;MultipeerGroupChat&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Networking &amp;amp; Internet&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;MultipeerConnectivity&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Multipeer Group Chat is an example application which builds on the Multipeer Connectivity framework for discovering, connecting, and sharing data between &#34;nearby&#34; peers. This application uses framework based UI for connecting to local peers and the framework convenience API for exhanging text messages and image resources between devices.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/TableMultiSelect/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Multiple Selection with UITableView&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Tables)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&#34;TableMultiSelect&#34; demonstrates the use of multiple selection of table cells in UITableView, in particular using multiple selection to delete one or more items.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/MultipleDetailViews/Introduction/Intro.html&#34;&gt;&lt;sup&gt;MultipleDetailViews&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Windows &amp;amp; Views)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample shows how you can use UISplitViewController to manage the presentation of multiple detail views in conjunction with a navigation hierarchy.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/MusicCube/Introduction/Intro.html&#34;&gt;&lt;sup&gt;MusicCube&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Graphics &amp;amp; Animation&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(3D Drawing)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;GLKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;MusicCube demonstrates basic use of OpenGL ES, OpenAL, and Audio File Services on the iPhone for manipulating sound in a 3D environment. The four modes in the application illustrate how the sound volume and balance will change based on the position of the omnidirectional sound source and the position and rotation of the listener. Tap on the screen to switch between modes.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/MusicMotion/Introduction/Intro.html&#34;&gt;&lt;sup&gt;MusicMotion: Adding Motion Awareness to a Music App&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;CoreMotion&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;CoreMotion provides contextual awareness that you can use to make a smart app even smarter. This sample demonstrates best practices for using the CoreMotion API. You&#39;ll also see how to fuse different types of motion and fitness data to enable context-aware app behavior.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/MVCNetworking/Introduction/Intro.html&#34;&gt;&lt;sup&gt;MVCNetworking&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Networking &amp;amp; Internet&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;MVCNetworking is a sample that shows how to create a network application using the Model-View-Controller design pattern. Specifically, it displays a photo gallery by getting the gallery&#39;s XML description, thumbnails and photos from a web server, and uses Core Data to cache this information locally.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/MyImagePicker/Introduction/Intro.html&#34;&gt;&lt;sup&gt;MyImagePicker&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;General&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AssetsLibrary&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Demonstrates how to create an image picker user interface using the AssetsLibrary framework to display all albums, events and faces synced from iTunes.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/oalTouch/Introduction/Intro.html&#34;&gt;&lt;sup&gt;OpenAL Spatial Environment Demo&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Audio &amp;amp; Video&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Audio)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AudioToolbox&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;oalTouch uses OpenAL to play an audio file containing uncompressed (PCM) audio data. The application uses Audio File Services to manage audio file data reading. The application also uses Audio Session Services to manage interruptions (as described in Core Audio Overview).&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/sc2281/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Packaged Document for iOS&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Data Management&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(File Management)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Foundation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&#34;PackagedDocument_iOS&#34; is a sample application for opening, editing and saving packaged documents using UIDocument and NSFileWrapper. This sample is ARC-enabled (Automatic Reference Counting).&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/PageControl/Introduction/Intro.html&#34;&gt;&lt;sup&gt;PageControl&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This application primarily demonstrates use of UIScrollView&#39;s paging functionality to use horizontal scrolling as a mechanism for navigating between different pages of content. With the iPad, this type of user interface is not really necessary since the screen is larger allowing for more content and detailed information.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/pARk/Introduction/Intro.html&#34;&gt;&lt;sup&gt;pARk&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Data Management&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Event Handling)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;CoreMotion&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;pARk is an application project that demonstrates a how to use Core Motion&#39;s true north-referenced attitude API. It contains a UIView subclass, ARView, that displays a live camera feed with places-of-interest overlaid at the appropriate coordinates. The places-of-interest used are some famous parks around the world.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/PhotoHandoff/Introduction/Intro.html&#34;&gt;&lt;sup&gt;PhotoHandoff: Implementing NSUserActivity to hand off user actions&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Demonstrates how to use NSUserActivity, based on “CollectionView-Simple” sample. It is a universal sample that runs native both on iPhone and iPad.&lt;br&gt;&lt;br&gt;- Enable Handoff for both devices (Settings -&amp;gt; General -&amp;gt; Enable Handoff)&lt;br&gt;&lt;br&gt;- Both devices must be logged into the same iCloud account&lt;br&gt;&lt;br&gt;- Bluetooth must be turned on (Settings -&amp;gt; Bluetooth)&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/PhotoMap/Introduction/Intro.html&#34;&gt;&lt;sup&gt;PhotoMap&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;MapKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;The PhotoMap sample demonstrates how to load and display geotagged photos as Map Kit annotations. It further demonstrates how to cluster multiple annotations together to reduce on-screen clutter.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/PhotoProgress/Introduction/Intro.html&#34;&gt;&lt;sup&gt;PhotoProgress: Using NSProgress&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Foundation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample demonstrates how to create and compose NSProgress objects, and show their progress in your app.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/PhotosByLocation/Introduction/Intro.html&#34;&gt;&lt;sup&gt;PhotosByLocation&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;General&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AssetsLibrary&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Demonstrates how to use the AssetsLibrary APIs to provide a custom image picking UI. The user experience is centered around the idea of using the assets location and time metadata as a basis for certain features.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/PhotoScroller/Introduction/Intro.html&#34;&gt;&lt;sup&gt;PhotoScroller&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&#34;PhotoScroller&#34; demonstrates the use of embedded UIScrollViews and CATiledLayer to create a rich user experience for displaying and paginating photos that can be individually panned and zoomed. CATiledLayer is used to increase the performance of paging, panning, and zooming with high-resolution images or large sets of photos.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/PocketCoreImage/Introduction/Intro.html&#34;&gt;&lt;sup&gt;PocketCoreImage&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Graphics &amp;amp; Animation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;CoreImage&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample demonstrates applying Core Image filters to a still image. The filter configuration is done automatically (using random numbers) and multiple filters may be applied at the same time. While this sample uses a preset list of filters that the user may select from, code is provided in the next section which demonstrates asking the system for a list of filters.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/Popovers/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Popover Controllers in iOS&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Windows &amp;amp; Views)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&#34;Popovers&#34; shows how to use UIPopoverController in iOS, including presentation, dismissal, and rotation of popovers. The sample uses a UISplitViewController to show how to present popovers from bar button items. It also demonstrates how you can ensure that multiple UIPopoverControllers are never presented at the same time.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/PotLoc/Introduction/Intro.html&#34;&gt;&lt;sup&gt;PotLoc: CoreLocation with iPhone and Apple Watch&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;CoreLocation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;The PotLoc sample demonstrates what you can do with CoreLocation, Apple Watch, and iPhone in iOS 9. WatchKit 2.0 apps can make use of standalone location functionality, or they can delegate location use to their parent app on iPhone. This sample provides a working demonstration of each kind of use, and a handy reference for developing more complicated interactions.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/PrefsInCloud/Introduction/Intro.html&#34;&gt;&lt;sup&gt;PrefsInCloud : Using NSUbiquitousKeyValueStore with iCloud&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Data Management&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;A set of simple iOS and OS X applications that&amp;nbsp;demonstrates how to use iCloud key-value store to share a single piece of data, its background color, with the same app on other iOS devices. It uses NSUbiquitousKeyValueStore to achieve this by storing a simple NSInteger representing a chosen color index. The user changes the background color of the app by choosing between 4 known colors.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/PrintPhoto/Introduction/Intro.html&#34;&gt;&lt;sup&gt;PrintPhoto: Using the Printing API with Photos&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Graphics &amp;amp; Animation&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(2D Drawing)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;PrintPhoto demonstrates how to print photos in iOS 9.0 or later.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/PVRTextureLoader/Introduction/Intro.html&#34;&gt;&lt;sup&gt;PVRTextureLoader&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Graphics &amp;amp; Animation&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(3D Drawing)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;OpenGLES&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This application illustrates how to load PVR texture files using the included PVRTexture class and then display them using OpenGL.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/QuartzDemo/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Quartz2D for iOS&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Graphics &amp;amp; Animation&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(2D Drawing)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;CoreGraphics&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;QuartzDemo is an iOS application that demonstrates many of the Quartz2D APIs made available by the CoreGraphics framework. Quartz2D forms the foundation of all drawing on iPhone OS and provides facilities for drawing lines, polygons, curves, images, gradients, PDF and many other graphical facilities.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/QuickContacts/Introduction/Intro.html&#34;&gt;&lt;sup&gt;QuickContacts&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Data Management&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Contact Data)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AddressBook&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;QuickContacts demonstrates how to use the Address Book UI controllers and various properties such as displayedProperties, allowsAddingToAddressBook, and displayPerson. It shows how to browse a list of Address Book contacts, display and edit a contact record, create a new contact record, and update a partial contact record.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/Reachability/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Reachability&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Networking &amp;amp; Internet&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Services &amp;amp; Discovery)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;SystemConfiguration&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;The Reachability sample application demonstrates how to use the System Configuration framework to monitor the network state of an iOS device. In particular, it demonstrates how to know when IP can be routed and when traffic will be routed through a Wireless Wide Area Network (WWAN) interface such as EDGE or 3G.&lt;br&gt;&lt;br&gt;Note: Reachability cannot tell your application if you can connect to a particular host, only that an interface is available that might allow a connection, and whether that interface is the WWAN. To understand when and how to use Reachability, read &#34;Networking Overview&#34;.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/AVBasicVideoOutput/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Real-time Video Processing Using AVPlayerItemVideoOutput&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Audio &amp;amp; Video&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Video)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AVFoundation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AVBasicVideoOutput demonstrates how to perform real-time video processing using AVPlayerItemVideoOutput and how to display processed video frames on screen using CAEAGLLayer and CADisplayLink. AVPlayerItemVideoOutput provides sample buffers (CVPixelBufferRef) which are then adjusted for their luma (Y) and chroma (UV) values based on the input from a user via UISliders. These processed pixel buffers are then rendered to a CAEAGLLayer.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/Reflection/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Reflection&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Graphics &amp;amp; Animation&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(2D Drawing)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample demonstrates how to implement a &#34;reflection&#34; special effect on a given UIImageView most commonly seen in iTunes and iPod player apps.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/Regions/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Regions: region monitoring, significant location changes, background location service, location service authorization&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;MapKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample demonstrates proper use of region monitoring, significant location changes, and handling location events in the background on iOS.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/RosyWriter/Introduction/Intro.html&#34;&gt;&lt;sup&gt;RosyWriter&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Audio &amp;amp; Video&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Video)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AVFoundation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;RosyWriter&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/SamplePhotoEditingExtension/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Sample Photo Editing Extension&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Languages &amp;amp; Utilities&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Graphics Tools)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample shows how to implement a Photo Editing extension. This extension allows the user to select a filter effect to apply to the photo or video selected in Photos or Camera.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/Recipes_+_Printing/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Sample Print Page Renderer&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Graphics &amp;amp; Animation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample demonstrates taking full control over the printed page using a UIPrintPageRenderer subclass. In addition to drawing full-page custom content, custom headers, and footers, it also shows how UIPrintFormatters can be utilized to do some heavy lifting.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/LoadPresetDemo/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Sampler Unit Presets (LoadPresetDemo)&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Audio &amp;amp; Video&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Audio)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AudioUnit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample code project shows how to create an an iOS audio processing graph containing a Sampler audio unit and how to configure the sampler by loading an AUPreset file that was created in Mac OS X. The project also shows how to start the graph and trigger note-on and note-off events to audition the presets.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/SceneKitReel/Introduction/Intro.html&#34;&gt;&lt;sup&gt;SceneKit State of the Union Demo&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;SceneKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample shows how the demo for the state the of union was realized. It includes examples of physics simulation, particles, collisions, physics field, 3D text, the integration with SpriteKit and custom GLSL shaders.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/SceneKitVehicle/Introduction/Intro.html&#34;&gt;&lt;sup&gt;SceneKit Vehicle Demo&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;SceneKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample code shows how to simulate a vehicle using the SCNPhysicsVehicle behaviour. The vehicle can be controller with either the accelerometer or a game controller. It also illustrate basic physics interaction and game overlays done with SpriteKit.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/SegueCatalog/Introduction/Intro.html&#34;&gt;&lt;sup&gt;SegueCatalog: Customizing and Unwinding with View Controller Containment&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample demonstrates how to use External References to factor your storyboards, how to combine UIStoryboardSegue subclasses with Transition Delegates and Adaptivity, and how to use unwind segues with custom container view controllers.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/SeismicXML/Introduction/Intro.html&#34;&gt;&lt;sup&gt;SeismicXML: Using NSXMLParser to parse XML documents&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Data Management&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&#34;SeismicXML&#34; demonstrates how to use NSXMLParser to parse XML data. The XML parsing occurs on a background thread using NSOperation and updates the earthquakes table view with batches of parsed objects.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/ShapeEdit/Introduction/Intro.html&#34;&gt;&lt;sup&gt;ShapeEdit: Building a Simple iCloud Document App&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Data Management&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(File Management)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample demonstrates the best practices for building a modern document-based app that integrates with iCloud Drive APIs. You’ll see how to discover cloud documents and then use a thumbnail-based UI to show them to your users. You’ll find out how to use the UIDocument class to read documents from the cloud using proper file coordination. You’ll also learn how to open cloud documents in place from the iCloud drive app.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/SimpleBackgroundTransfer/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Simple Background Transfer&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Networking &amp;amp; Internet&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&#34;SimpleBackgroundTransfer&#34; illustrates how to support Background Transfer using NSURLSession to manage a download task.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/TaggedLocations/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Simple Core Data Relationships&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Data Management&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;CoreData&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;TaggedLocations illustrates how you can change Core Data attributes and relationships in an iOS application.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/SimpleGestureRecognizers/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Simple Gesture Recognizers&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Data Management&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Event Handling)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample shows how you can easily use UITapGestureRecognizer, UISwipeGestureRecognizer, and UIRotationGestureRecognizer to detect user gestures. It illustrates the use of basic target-action messaging using recognizers, and also shows: how you can toggle the behavior of a recognizer at runtime using delegation; options for maintaining references to recognizers; and interaction with the exclusive touch setting for views.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/SimpleEKDemo/Introduction/Intro.html&#34;&gt;&lt;sup&gt;SimpleEKDemo&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;EventKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;The application demonstrates how to check and request access to a user&#39;s Calendar. It uses table views to display EKCalendar object and EKEvent objects retrieved from an EKEventStore object. It implements EKEventViewController for viewing and editing existing EKEvents, and uses EKEventEditViewController for creating new EKEvents.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/SimpleFTPSample/Introduction/Intro.html&#34;&gt;&lt;sup&gt;SimpleFTPSample&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Networking &amp;amp; Internet&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Protocol Streams)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Foundation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;SimpleFTPSample shows how to do simple FTP operations using the NSURLConnection and CFFTPStream APIs. It can download a file using both NSURLConnection and CFFTPStream. Also, it can upload a file, list a directory, and create a directory using CFFTPStream.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/SimpleNetworkStreams/Introduction/Intro.html&#34;&gt;&lt;sup&gt;SimpleNetworkStreams&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Networking &amp;amp; Internet&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Sockets &amp;amp; TCP)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Shows how to do simple networking using the NSStream API. The goal of this sample is very limited: it does not demonstrate everything you need to implement a fully fledged networking product, rather, it focuses on using the NSStream API to move a realistic amount of data across the network.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/SimpleTunnel/Introduction/Intro.html&#34;&gt;&lt;sup&gt;SimpleTunnel: Customized Networking Using the NetworkExtension Framework&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Networking &amp;amp; Internet&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;NetworkExtension&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;The Network Extension framework exposes APIs that give you the ability to customize the networking features of iOS and OS X. This sample project demonstrates how to:&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/SimpleUndo/Introduction/Intro.html&#34;&gt;&lt;sup&gt;SimpleUndo&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Foundation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;The root view controller displays information (title, author, and copyright date) about a book. The user can edit this information by tapping Edit in the navigation bar. When editing starts, the root view controller creates an undo manager to record changes. The undo manager supports up to three levels of undo and redo. When the user taps Done, changes are considered to be committed and the undo manager is disposed of.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/SimpleURLConnections/Introduction/Intro.html&#34;&gt;&lt;sup&gt;SimpleURLConnections&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Networking &amp;amp; Internet&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Protocol Streams)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Foundation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;SimpleURLConnections shows how to do simple networking using the NSURLConnection API. The goal of this sample is very limited: it does not demonstrate everything you need to implement a fully fledged networking product, rather, its goal is to demonstrate simple HTTP GET, PUT and POST using the NSURLConnection API.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/SpeakHere/Introduction/Intro.html&#34;&gt;&lt;sup&gt;SpeakHere&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Audio &amp;amp; Video&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Audio)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AudioToolbox&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;SpeakHere demonstrates basic use of Audio Queue Services, Audio File Services, and Audio Session Services on the iPhone for recording and playing back audio.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/SquareCam/Introduction/Intro.html&#34;&gt;&lt;sup&gt;SquareCam&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Audio &amp;amp; Video&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Video)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AVFoundation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;SquareCam demonstrates improvements to the AVCaptureStillImageOutput class in iOS 5, highlighting the following features:&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/StateRestore/Introduction/Intro.html&#34;&gt;&lt;sup&gt;State Restoration&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Demonstrates how to implement and debug the APIs for &#34;State Preservation and Restoration&#34;.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/StateRestoreChildViews/Introduction/Intro.html&#34;&gt;&lt;sup&gt;State Restoration of Child View Controllers&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Demonstrates how to implement &#34;State Preservation and Restoration&#34; in an app with child view controllers. The sample contains one parent view controller, which can host two different child view controllers. The user taps the segmented control to toggle between the two different children.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/StickyCorners/Introduction/Intro.html&#34;&gt;&lt;sup&gt;StickyCorners: Using UIFieldBehavior and other UIDynamicBehaviors&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;StickyCorners showcases the usage of UIFieldBehavior using UIKit Dynamics.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/StitchedStreamPlayer/Introduction/Intro.html&#34;&gt;&lt;sup&gt;StitchedStreamPlayer&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Audio &amp;amp; Video&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AVFoundation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;A simple AVFoundation demonstration of how timed metadata can be used to identify different content in a stream, supporting a custom seek UI.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/StopNGo/Introduction/Intro.html&#34;&gt;&lt;sup&gt;StopNGo for iOS&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Audio &amp;amp; Video&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Video)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AVFoundation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;StopNGo is a simple stop-motion animation QuickTime movie recorder that uses AVFoundation.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/sc1991/Introduction/Intro.html&#34;&gt;&lt;sup&gt;StoreKitSuite&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;StoreKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;StoreKitSuite is a suite of sample app that demonstrate how to implement three different aspects of StoreKit:&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/StreetScroller/Introduction/Intro.html&#34;&gt;&lt;sup&gt;StreetScroller&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Controls)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Demonstrates how a UIScrollView subclass can scroll infinitely in the horizontal direction.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/TableSearch_UISearchController/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Table Search with UISearchController&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;“Table Search with UISearchController” is an iOS sample application that demonstrates how to use UISearchController. A search controller manages the presentation of a search bar (in concert with the results view controller’s content).&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/TableViewUpdates/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Table View Animations and Gestures&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Tables)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&#34;TableViewUpdates&#34; demonstrates how you can use animated updates to open and close sections of a table view for viewing, where each section represents a play, and each row contains a quotation from the play. It also uses gesture recognizers to respond to user input: * A UITapGestureRecognizer to allow tapping on the section headers to expand the section; * A UIPinchGestureRecognizer to allow dynamic changes to the height of table view rows; and * A UILongPressGestureRecognizer to allow press-and-hold on table view cells to initiate an email of the quotation.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/TableViewSuite/Introduction/Intro.html&#34;&gt;&lt;sup&gt;TableView Fundamentals for iOS&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Tables)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample shows how to use UITableView and UITableViewController through a progression of increasingly advanced applications that display information about time zones.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/Accessory/Introduction/Intro.html&#34;&gt;&lt;sup&gt;TableViewCell Accessory&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Tables)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample demonstrates two methods that can be used to implement a custom accessory view in your UITableViewCell&#39;s. In both examples, a custom control that implements a toggle-able checkbox is used.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/Tabster/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Tabster&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Windows &amp;amp; Views)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;An eclectic-style application designed to show how to build a tab-bar based iOS application.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/Teslameter/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Teslameter&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;CoreLocation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This application implements a Teslameter, a magnetic field detector. It displays the raw x, y, and z magnetometer values, a plotted history of those values, and a computed magnitude (size or strength) of the magnetic field.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/TheElements/Introduction/Intro.html&#34;&gt;&lt;sup&gt;TheElements&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Data Management&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;TheElements is a sample application that provides access to the data contained in the Periodic Table of the Elements. The Periodic Table of the Elements catalogs all the known atomic elements in the universe.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/ThreadedCoreData/Introduction/Intro.html&#34;&gt;&lt;sup&gt;ThreadedCoreData: Concurrent operations using Core Data&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Tables)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;CoreData&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Demonstrates how to use Core Data in a multi-threaded environment, following the first recommended pattern mentioned in the Core Data Programming Guide.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/TopSongs/Introduction/Intro.html&#34;&gt;&lt;sup&gt;TopSongs&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Performance&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;CoreData&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample code demonstrates efficiently parsing and importing data from an XML RSS feed into Core Data. The feed is from iTunes Top Songs and contains data about songs, artists, and categories. The application&#39;s data model has an entity for Song and for Category. Managed objects are inserted into a managed object context on a background thread, so the application remains responsive to the user while the import is taking place.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/TouchCanvas/Introduction/Intro.html&#34;&gt;&lt;sup&gt;TouchCanvas: Using UITouch efficiently and effectively&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;TouchCanvas illustrates responsive touch handling using coalesced and predictive touches (when available) via a simple drawing app. The sample uses force information (when available) to change line thickness. Apple Pencil and finger touches are distinguished via different colors. In addition, Apple Pencil only data is demonstrated through the use of estimated properties and updates providing the actual property data including the azimuth and altitude of the Apple Pencil while in use.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/VideoRecorder/Introduction/Intro.html&#34;&gt;&lt;sup&gt;UIImagePicker Video Recorder&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Audio &amp;amp; Video&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Video)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Demonstrates how to create a custom UI for the camera variant of the UIImagePickerController and how to programmatically control video recording.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/UICatalog/Introduction/Intro.html&#34;&gt;&lt;sup&gt;UIKit Catalog (iOS): Creating and Customizing UIKit Controls&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Demonstrates how to use many views and controls in the UIKit framework along with their assorted functionalities. Refer to this sample if you are looking for specific controls or views that are provided by the system. You can also find out how to make nonstandard views (images or custom views) accessible. By using the iOS Accessibility API, you&#39;ll enhance the user experience for those who use VoiceOver.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/DynamicsCatalog/Introduction/Intro.html&#34;&gt;&lt;sup&gt;UIKit Dynamics Catalog&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Graphics &amp;amp; Animation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit Dynamics Catalog illustrates a number of uses of UIKit Dynamics, the iOS API that provides physics-related capabilities and animations to views and other dynamic items. Each of the 10 view controllers in this project shows a different way to use UIKit Dynamics—-in many cases, combining dynamic behaviors for interesting results.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/PrintWebView/Introduction/Intro.html&#34;&gt;&lt;sup&gt;UIKit Printing with UIPrintInteractionController and UIViewPrintFormatter&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Graphics &amp;amp; Animation&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(2D Drawing)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;PrintWebView demonstrates how to print the content displayed by a UIWebView object using the UIViewPrintFormatter class. This sample application is a primitive web browser with printing capability.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/UnitTests/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Unit Testing Apps and Frameworks&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Xcode&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(IDEs)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Shows how to build a static library for an iOS app and a Mac app, how to implement and run logic unit tests on the library source code on each platform, and how to implement and run application unit tests for the apps.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/UnwindSegue/Introduction/Intro.html&#34;&gt;&lt;sup&gt;UnwindSegue&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample demonstrates how to use segues and unwind segues to drive navigation between the various screens of content in your app. The project contains two build targets: UnwindSegue and CustomUnwindSegue. UnwindSegue demonstrates using unwind segues with modally presented view controllers as well as view controllers in a UINavigationController. CustomUnwindSegue demonstrates implementing a custom container view controller that can be used with unwind segues.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/URLCache/Introduction/Intro.html&#34;&gt;&lt;sup&gt;URLCache&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Performance&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;URLCache is a sample iPhone application that demonstrates how to download a resource off the web, store it in the application&#39;s data directory, and use the local copy of the resource. URLCache also demonstrates how to implement a couple of caching policies:&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/ToolbarSearch/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Using a Search Bar in a Toolbar&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Controls)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&#34;ToolbarSearch&#34; shows how to use a search bar in a toolbar and present recent searches in a popover.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/PrintBanner/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Using AirPrint to Print a Banner&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;General&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;PrintBanner shows how to print a banner of arbitrary length to a roll printer. The code formats the job for landscape orientation and then calculates a font size that fills the width of the paper. It then determines the length of the text and pads the length so the final cut length results in a pleasing layout.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/iOSMultichannelMixerTest/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Using an AUGraph with the Multi-Channel Mixer and Remote I/O Audio Unit &lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Audio &amp;amp; Video&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Audio)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;CoreAudio&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Demonstrates how to build an Audio Unit Graph connecting a Multichannel Mixer instance to the RemoteIO unit. Two input busses are created each with input volume controls. An overall mixer output volume control is also provided and each bus may be enabled or disabled.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/AVAEMixerSample/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Using AVAudioEngine for Playback, Mixing and Recording&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Audio &amp;amp; Video&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Audio)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AVFoundation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Demonstrates using AVAudioEngine with two AVAudioPlayerNodes and AVAudioPCMBuffer objects along with an AVAudioUnitDelay and AVAudioUnitReverb to playback two loops which can then be mixed, processed and recorded.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/AVCaptureLocation/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Using AVFoundation APIs to record a movie with location metadata&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AVFoundation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample shows how to use AVAssetWriterInputMetadataAdaptor API to write timed location metadata, obtained from CoreLocation, during live video capture. The captured movie file has video, audio and metadata track. The metadata track contains location corresponding to where the video was recorded.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/ExternalDisplay/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Using External Displays&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Windows &amp;amp; Views)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;How to detect the presence of an external display, determine the available display resolutions, select a resolution, and show content on the display.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/PhotoPicker/Introduction/Intro.html&#34;&gt;&lt;sup&gt;Using UIImagePickerController to Select Pictures and Take Photos&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;PhotoPicker demonstrates how to choose images from the photo library, take a picture with the device&#39;s camera, and how to customize the look of the camera&#39;s user interface. This is done by using UIImagePickerController. The chosen image or camera photo is displayed in a UIImageView. &amp;nbsp;To customize the camera&#39;s interface, this sample shows how to use an overlay view. With this overlay view it gives you the ability to customize the UI as you take a picture.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/VideoSnake/Introduction/Intro.html&#34;&gt;&lt;sup&gt;VideoSnake&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Audio &amp;amp; Video&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;AVFoundation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;VideoSnake demonstrates temporal synchronization of video with motion data.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/ViewControllerPreviews/Introduction/Intro.html&#34;&gt;&lt;sup&gt;ViewControllerPreviews: Using the UIViewController previewing APIs&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;User Experience&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample displays both a static and dynamic UITableViewController triggering view controller previews, using preferredContentSize to show previews of different sizes. The sample also includes single-item and grouped action items.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/ViewTransitions/Introduction/Intro.html&#34;&gt;&lt;sup&gt;ViewTransitions&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Graphics &amp;amp; Animation&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Animation)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;UIKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Demonstrates how to perform transitions between two views using UIView&#39;s transitionFromView and animateWithDuration API.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/WKInterfaceCatalog/Introduction/Intro.html&#34;&gt;&lt;sup&gt;WatchKit Catalog: Using WatchKit Interface Elements&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;General&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;WatchKit&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample demonstrates how to use UI elements available in the WatchKit framework. Refer to this sample to see how to use, configure, and interact with each UI element. This project also shows you how to use new capabilities in WatchKit for watchOS 2.0 including media support, the Digital Crown, and ClockKit.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/WiTap/Introduction/Intro.html&#34;&gt;&lt;sup&gt;WiTap&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Networking &amp;amp; Internet&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(Services &amp;amp; Discovery)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Foundation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;The WiTap sample demonstrates peer-to-peer networking over Wi-Fi and Bluetooth. Using Bonjour, the application both advertises itself on the local network and displays a list of other instances on the network. Supports infrastructure networks, peer-to-peer Bluetooth, and peer-to-peer Wi-Fi (on compatible hardware).&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/XMLPerformance/Introduction/Intro.html&#34;&gt;&lt;sup&gt;XMLPerformance&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Performance&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Foundation&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;This sample explores two approaches to parsing XML, focusing on performance with respect to speed, memory footprint, and user experience. The XML data used is the current &#34;Top 300&#34; songs from the iTunes store. The data itself is not particularly important to the sample - it was chosen because of its simplicity, availability, and because the size (approximately 850KB) is sufficient to demonstrate the performance issues central to the sample.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developer.apple.com/library/ios/samplecode/ZoomingPDFViewer/Introduction/Intro.html&#34;&gt;&lt;sup&gt;ZoomingPDFViewer&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Graphics &amp;amp; Animation&lt;/sup&gt;&lt;br&gt;&lt;sup&gt;(2D Drawing)&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;CoreGraphics&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sup&gt;Multi-paged PDF viewing with UIPageViewController demonstrates two-page spline viewing in landscape orientation, which looks like a book within iBooks. The sample also uses UIScrollView and CATiledLayer to support zooming within a single-page view used in portrait orientations. This app is universal and only supports the two-page spline view in landscape orientation on iPad.&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt;</summary>
  </entry>
</feed>