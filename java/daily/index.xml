<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Java Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-21T01:29:44Z</updated>
  <subtitle>Daily Trending of Java in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>mukel/llama3.java</title>
    <updated>2024-05-21T01:29:44Z</updated>
    <id>tag:github.com,2024-05-21:/mukel/llama3.java</id>
    <link href="https://github.com/mukel/llama3.java" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Practical Llama 3 inference in Java&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Llama3.java&lt;/h1&gt; &#xA;&lt;p&gt;Practical &lt;a href=&#34;https://github.com/meta-llama/llama3&#34;&gt;Llama 3&lt;/a&gt; inference implemented in a single Java file.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;700&#34; src=&#34;https://github.com/mukel/llama3.java/assets/1896283/7939588c-c0ff-4261-b67f-8a54bad59ab5&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;This project is the successor of &lt;a href=&#34;https://github.com/mukel/llama2.java&#34;&gt;llama2.java&lt;/a&gt; based on &lt;a href=&#34;https://github.com/karpathy/llama2.c&#34;&gt;llama2.c&lt;/a&gt; by &lt;a href=&#34;https://twitter.com/karpathy&#34;&gt;Andrej Karpathy&lt;/a&gt; and his &lt;a href=&#34;https://www.youtube.com/c/AndrejKarpathy&#34;&gt;excellent educational videos&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Besides the educational value, this project will be used to test and tune compiler optimizations and features on the JVM, particularly for the &lt;a href=&#34;https://www.graalvm.org/latest/reference-manual/java/compiler&#34;&gt;Graal compiler&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Single file, no dependencies&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ggerganov/ggml/raw/master/docs/gguf.md&#34;&gt;GGUF format&lt;/a&gt; parser&lt;/li&gt; &#xA; &lt;li&gt;Llama 3 tokenizer based on &lt;a href=&#34;https://github.com/karpathy/minbpe&#34;&gt;minbpe&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Llama 3 inference with Grouped-Query Attention&lt;/li&gt; &#xA; &lt;li&gt;Support for Q8_0 and Q4_0 quantizations&lt;/li&gt; &#xA; &lt;li&gt;Fast matrix-vector multiplication routines for quantized tensors using Java&#39;s &lt;a href=&#34;https://openjdk.org/jeps/469&#34;&gt;Vector API&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Simple CLI with &lt;code&gt;--chat&lt;/code&gt; and &lt;code&gt;--instruct&lt;/code&gt; modes.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Here&#39;s the interactive &lt;code&gt;--chat&lt;/code&gt; mode in action:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;700&#34; src=&#34;https://github.com/mukel/llama3.java/assets/1896283/2245f59d-6c86-49c3-87d3-8b1a2cb83a91&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;Download pure &lt;code&gt;Q4_0&lt;/code&gt; and (optionally) &lt;code&gt;Q8_0&lt;/code&gt; quantized .gguf files from:&lt;br&gt; &lt;a href=&#34;https://huggingface.co/mukel/Meta-Llama-3-8B-Instruct-GGUF&#34;&gt;https://huggingface.co/mukel/Meta-Llama-3-8B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;~4.3GB&lt;/code&gt; pure &lt;code&gt;Q4_0&lt;/code&gt; quantized model is recommended, please be gentle with &lt;a href=&#34;https://huggingface.co&#34;&gt;huggingface.co&lt;/a&gt; servers:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -L -O https://huggingface.co/mukel/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q4_0.gguf&#xA;&#xA;# Optionally download the Q8_0 quantized model ~8GB&#xA;# curl -L -O https://huggingface.co/mukel/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q8_0.gguf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Optional: quantize to pure &lt;code&gt;Q4_0&lt;/code&gt; manually&lt;/h4&gt; &#xA;&lt;p&gt;In the wild, &lt;code&gt;Q8_0&lt;/code&gt; quantizations are fine, but &lt;code&gt;Q4_0&lt;/code&gt; quantizations are rarely pure e.g. the &lt;code&gt;output.weights&lt;/code&gt; tensor is quantized with &lt;code&gt;Q6_K&lt;/code&gt;, instead of &lt;code&gt;Q4_0&lt;/code&gt;.&lt;br&gt; A &lt;strong&gt;pure&lt;/strong&gt; &lt;code&gt;Q4_0&lt;/code&gt; quantization can be generated from a high precision (F32, F16, BFLOAT16) .gguf source with the &lt;code&gt;quantize&lt;/code&gt; utility from &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./quantize --pure ./Meta-Llama-3-8B-Instruct-F32.gguf ./Meta-Llama-3-8B-Instruct-Q4_0.gguf Q4_0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Build and run&lt;/h2&gt; &#xA;&lt;p&gt;Java 21+ is required, in particular the &lt;a href=&#34;https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/nio/channels/FileChannel.html#map(java.nio.channels.FileChannel.MapMode,long,long,java.lang.foreign.Arena)&#34;&gt;&lt;code&gt;MemorySegment&lt;/code&gt; mmap-ing feature&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.jbang.dev/&#34;&gt;&lt;code&gt;jbang&lt;/code&gt;&lt;/a&gt; is a perfect fit for this use case, just:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;jbang Llama3.java --help&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or execute directly, also via &lt;a href=&#34;https://www.jbang.dev/&#34;&gt;&lt;code&gt;jbang&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;chmod +x Llama3.java&#xA;./Llama3.java --help&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Optional: Makefile + manually build and run&lt;/h4&gt; &#xA;&lt;p&gt;A simple &lt;a href=&#34;https://raw.githubusercontent.com/mukel/llama3.java/main/Makefile&#34;&gt;Makefile&lt;/a&gt; is provided, run &lt;code&gt;make&lt;/code&gt; to produce &lt;code&gt;llama3.jar&lt;/code&gt; or manually:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;javac -g --enable-preview -source 21 --add-modules jdk.incubator.vector -d target/classes Llama3.java&#xA;jar -cvfe llama3.jar com.llama4j.Llama3 LICENSE -C target/classes .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run the resulting &lt;code&gt;llama3.jar&lt;/code&gt; as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;java --enable-preview --add-modules jdk.incubator.vector -jar llama3.jar --help&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Important Note&lt;/strong&gt;&lt;br&gt; On GraalVM, please note that the Graal compiler doesn&#39;t support the Vector API yet, run with &lt;code&gt;-Dllama.VectorAPI=false&lt;/code&gt;, but expect sub-optimal performance.&lt;br&gt; Vanilla OpenJDK 21+ is recommended for now, which supports the Vector API.&lt;/p&gt; &#xA;&lt;h3&gt;llama.cpp&lt;/h3&gt; &#xA;&lt;p&gt;Vanilla &lt;code&gt;llama.cpp&lt;/code&gt; built with &lt;code&gt;make -j 20&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./main --version&#xA;version: 2879 (4f026363)&#xA;built with cc (GCC) 13.2.1 20230801 for x86_64-pc-linux-gnu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Executed as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./main -m ../Meta-Llama-3-8B-Instruct-Q4_0.gguf \&#xA;  -n 512 \&#xA;  -s 42 \&#xA;  -p &#34;&amp;lt;|start_of_header_id|&amp;gt;user&amp;lt;|end_of_header_id|&amp;gt;Why is the sky blue?&amp;lt;|eot_id|&amp;gt;&amp;lt;|start_of_header_id|&amp;gt;assistant&amp;lt;|end_of_header_id|&amp;gt;\n\n&#34; \&#xA;  --interactive-specials&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Collected the &lt;strong&gt;&#34;eval time&#34;&lt;/strong&gt; metric in tokens\s.&lt;/p&gt; &#xA;&lt;h3&gt;Llama3.java&lt;/h3&gt; &#xA;&lt;p&gt;Running on OpenJDK 21.0.2.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;jbang Llama3.java \&#xA;  --model ./Meta-Llama-3-8B-Instruct-Q4_0.gguf \&#xA;  --max-tokens 512 \&#xA;  --seed 42 \&#xA;  --stream false \&#xA;  --prompt &#34;Why is the sky blue?&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Results&lt;/h3&gt; &#xA;&lt;h4&gt;Notebook Intel 13900H 6pC+8eC/20T 64GB (5200) Linux 6.6.26&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;tokens/s&lt;/th&gt; &#xA;   &lt;th&gt;Implementation&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-3-8B-Instruct-Q4_0.gguf&lt;/td&gt; &#xA;   &lt;td&gt;7.53&lt;/td&gt; &#xA;   &lt;td&gt;llama.cpp&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-3-8B-Instruct-Q4_0.gguf&lt;/td&gt; &#xA;   &lt;td&gt;6.95&lt;/td&gt; &#xA;   &lt;td&gt;llama3.java&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-3-8B-Instruct-Q8_0.gguf&lt;/td&gt; &#xA;   &lt;td&gt;5.16&lt;/td&gt; &#xA;   &lt;td&gt;llama.cpp&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-3-8B-Instruct-Q8_0.gguf&lt;/td&gt; &#xA;   &lt;td&gt;4.02&lt;/td&gt; &#xA;   &lt;td&gt;llama3.java&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;Workstation AMD 3950X 16C/32T 64GB (3200) Linux 6.6.25&lt;/h4&gt; &#xA;&lt;p&gt;**&lt;strong&gt;Notes&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Running on a single CCD e.g. &lt;code&gt;taskset -c 0-15 jbang Llama3.java ...&lt;/code&gt; since inference is constrained by memory bandwidth.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;tokens/s&lt;/th&gt; &#xA;   &lt;th&gt;Implementation&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-3-8B-Instruct-Q4_0.gguf&lt;/td&gt; &#xA;   &lt;td&gt;9.26&lt;/td&gt; &#xA;   &lt;td&gt;llama.cpp&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-3-8B-Instruct-Q4_0.gguf&lt;/td&gt; &#xA;   &lt;td&gt;8.03&lt;/td&gt; &#xA;   &lt;td&gt;llama3.java&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-3-8B-Instruct-Q8_0.gguf&lt;/td&gt; &#xA;   &lt;td&gt;5.79&lt;/td&gt; &#xA;   &lt;td&gt;llama.cpp&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-3-8B-Instruct-Q8_0.gguf&lt;/td&gt; &#xA;   &lt;td&gt;4.92&lt;/td&gt; &#xA;   &lt;td&gt;llama3.java&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;MIT&lt;/p&gt;</summary>
  </entry>
</feed>