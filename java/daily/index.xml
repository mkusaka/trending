<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Java Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-10-29T01:35:44Z</updated>
  <subtitle>Daily Trending of Java in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>woowacourse-precourse/java-onboarding</title>
    <updated>2022-10-29T01:35:44Z</updated>
    <id>tag:github.com,2022-10-29:/woowacourse-precourse/java-onboarding</id>
    <link href="https://github.com/woowacourse-precourse/java-onboarding" rel="alternate"></link>
    <summary type="html">&lt;p&gt;온보딩 미션을 진행하는 저장소&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;미션 - 온보딩&lt;/h1&gt; &#xA;&lt;h2&gt;🔍 진행 방식&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;미션은 &lt;strong&gt;기능 요구 사항, 프로그래밍 요구 사항, 과제 진행 요구 사항&lt;/strong&gt; 세 가지로 구성되어 있다.&lt;/li&gt; &#xA; &lt;li&gt;세 개의 요구 사항을 만족하기 위해 노력한다. 특히 기능을 구현하기 전에 기능 목록을 만들고, 기능 단위로 커밋 하는 방식으로 진행한다.&lt;/li&gt; &#xA; &lt;li&gt;기능 요구 사항에 기재되지 않은 내용은 스스로 판단하여 구현한다.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;📮 미션 제출 방법&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;미션 구현을 완료한 후 GitHub을 통해 제출해야 한다. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;GitHub을 활용한 제출 방법은 &lt;a href=&#34;https://github.com/woowacourse/woowacourse-docs/tree/master/precourse&#34;&gt;프리코스 과제 제출&lt;/a&gt; 문서를 참고해 제출한다.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;GitHub에 미션을 제출한 후 &lt;a href=&#34;https://apply.techcourse.co.kr&#34;&gt;우아한테크코스 지원&lt;/a&gt; 사이트에 접속하여 프리코스 과제를 제출한다. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;자세한 방법은 &lt;a href=&#34;https://github.com/woowacourse/woowacourse-docs/tree/master/precourse#%EC%A0%9C%EC%B6%9C-%EA%B0%80%EC%9D%B4%EB%93%9C&#34;&gt;제출 가이드&lt;/a&gt; 참고&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Pull Request만 보내고 지원 플랫폼에서 과제를 제출하지 않으면 최종 제출하지 않은 것으로 처리되니 주의한다.&lt;/strong&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🚨 과제 제출 전 체크 리스트 - 0점 방지&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;기능 구현을 모두 정상적으로 했더라도 &lt;strong&gt;요구 사항에 명시된 출력값 형식을 지키지 않을 경우 0점으로 처리&lt;/strong&gt;한다.&lt;/li&gt; &#xA; &lt;li&gt;기능 구현을 완료한 뒤 아래 가이드에 따라 테스트를 실행했을 때 모든 테스트가 성공하는지 확인한다.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;테스트가 실패할 경우 0점으로 처리&lt;/strong&gt;되므로, 반드시 확인 후 제출한다.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;테스트 실행 가이드&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;터미널에서 &lt;code&gt;java -version&lt;/code&gt;을 실행하여 Java 버전이 11인지 확인한다. 또는 Eclipse 또는 IntelliJ IDEA와 같은 IDE에서 Java 11로 실행되는지 확인한다.&lt;/li&gt; &#xA; &lt;li&gt;터미널에서 Mac 또는 Linux 사용자의 경우 &lt;code&gt;./gradlew clean test&lt;/code&gt; 명령을 실행하고,&lt;br&gt; Windows 사용자의 경우 &lt;code&gt;gradlew.bat clean test&lt;/code&gt; 명령을 실행할 때 모든 테스트가 아래와 같이 통과하는지 확인한다.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;BUILD SUCCESSFUL in 0s&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;🚀 기능 요구 사항&lt;/h2&gt; &#xA;&lt;p&gt;아래의 7가지 기능 요구 사항을 모두 해결해야 한다.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/woowacourse-precourse/java-onboarding/main/docs/PROBLEM1.md&#34;&gt;문제 1&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/woowacourse-precourse/java-onboarding/main/docs/PROBLEM2.md&#34;&gt;문제 2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/woowacourse-precourse/java-onboarding/main/docs/PROBLEM3.md&#34;&gt;문제 3&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/woowacourse-precourse/java-onboarding/main/docs/PROBLEM4.md&#34;&gt;문제 4&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/woowacourse-precourse/java-onboarding/main/docs/PROBLEM5.md&#34;&gt;문제 5&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/woowacourse-precourse/java-onboarding/main/docs/PROBLEM6.md&#34;&gt;문제 6&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/woowacourse-precourse/java-onboarding/main/docs/PROBLEM7.md&#34;&gt;문제 7&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;🎯 프로그래밍 요구 사항&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;JDK 11 버전에서 실행 가능해야 한다. &lt;strong&gt;JDK 11에서 정상적으로 동작하지 않을 경우 0점 처리한다.&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;build.gradle&lt;/code&gt;을 변경할 수 없고, 외부 라이브러리를 사용하지 않는다.&lt;/li&gt; &#xA; &lt;li&gt;프로그램 종료 시 &lt;code&gt;System.exit()&lt;/code&gt;를 호출하지 않는다.&lt;/li&gt; &#xA; &lt;li&gt;프로그램 구현이 완료되면 &lt;code&gt;ApplicationTest&lt;/code&gt;의 모든 테스트가 성공해야 한다. &lt;strong&gt;테스트가 실패할 경우 0점 처리한다.&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;프로그래밍 요구 사항에서 달리 명시하지 않는 한 파일, 패키지 이름을 수정하거나 이동하지 않는다.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;✏️ 과제 진행 요구 사항&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;미션은 &lt;a href=&#34;https://github.com/woowacourse-precourse/java-onboarding&#34;&gt;java-onboarding&lt;/a&gt; 저장소를 Fork &amp;amp; Clone해 시작한다.&lt;/li&gt; &#xA; &lt;li&gt;과제 진행 및 제출 방법은 &lt;a href=&#34;https://github.com/woowacourse/woowacourse-docs/tree/master/precourse&#34;&gt;프리코스 과제 제출&lt;/a&gt; 문서를 참고한다.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>bytedance/bitsail</title>
    <updated>2022-10-29T01:35:44Z</updated>
    <id>tag:github.com,2022-10-29:/bytedance/bitsail</id>
    <link href="https://github.com/bytedance/bitsail" rel="alternate"></link>
    <summary type="html">&lt;p&gt;BitSail is a distributed high-performance data integration engine which supports batch, streaming and incremental scenarios. BitSail is widely used to synchronize hundreds of trillions of data every day.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;BitSail&lt;/h1&gt; &#xA;&lt;p&gt;English | &lt;a href=&#34;https://raw.githubusercontent.com/bytedance/bitsail/master/README_zh.md&#34;&gt;简体中文&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/bytedance/bitsail/actions/workflows/cicd.yml&#34;&gt;&lt;img src=&#34;https://github.com/bytedance/bitsail/actions/workflows/cicd.yml/badge.svg?sanitize=true&#34; alt=&#34;Build&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache%202-4EB1BA.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://join.slack.com/t/slack-ted3816/shared_invite/zt-1inff2sip-u7Ej_o73sUgdpJAvqwlEwQ&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/slack-%23BitSail-72eff8?logo=slack&amp;amp;color=5DADE2&amp;amp;label=Join%20Slack&#34; alt=&#34;Join Slack&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;BitSail is ByteDance&#39;s open source data integration engine which is based on distributed architecture and provides high performance. It supports data synchronization between multiple heterogeneous data sources, and provides global data integration solutions in batch, streaming, and incremental scenarios. At present, it serves almost all business lines in ByteDance, such as Douyin, Toutiao, etc., and synchronizes hundreds of trillions of data every day.&lt;/p&gt; &#xA;&lt;h2&gt;Why Do We Use BitSail&lt;/h2&gt; &#xA;&lt;p&gt;BitSail has been widely used and supports hundreds of trillions of large traffic. At the same time, it has been verified in various scenarios such as the cloud native environment of the volcano engine and the on-premises private cloud environment.&lt;/p&gt; &#xA;&lt;p&gt;We have accumulated a lot of experience and made a number of optimizations to improve the function of data integration&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Global Data Integration, covering batch, streaming and incremental scenarios&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Distributed and cloud-native architecture, supporting horizontal scaling&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;High maturity in terms of accuracy, stability and performance&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Rich basic functions, such as type conversion, dirty data processing, flow control, data lake integration, automatic parallelism calculation , etc.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Task running status monitoring, such as traffic, QPS, dirty data, latency, etc.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;BitSail Use Scenarios&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Mass data synchronization in heterogeneous data sources&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Streaming and batch integration data processing capability&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Data lake and warehouse integration data processing capability&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;High performance, high reliability data synchronization&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Distributed, cloud-native architecture data integration engine&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features of BitSail&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Low start-up cost and high flexibility&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Stream-batch integration and Data lake-warehouse integration architecture, one framework covers almost all data synchronization scenarios&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;High-performance, massive data processing capabilities&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;DDL automatic synchronization&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Type system, conversion between different data source types&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Engine independent reading and writing interface, low development cost&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Real-time display of task progress, under development&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Real-time monitoring of task status&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Architecture of BitSail&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bytedance/bitsail/master/docs/images/bitsail_arch.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Source[Input Sources] -&amp;gt; Framework[Data Transmission] -&amp;gt; Sink[Output Sinks]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The data processing pipeline is as follows. First, pull the source data through Input Sources, then process it through the intermediate framework layer, and finally write the data to the target through Output Sinks&lt;/p&gt; &#xA;&lt;p&gt;At the framework layer, we provide rich functions and take effect for all synchronization scenarios, such as dirty data collection, auto parallelism calculation, task monitoring, etc.&lt;/p&gt; &#xA;&lt;p&gt;In data synchronization scenarios, it covers batch, streaming, and incremental data synchronization&lt;/p&gt; &#xA;&lt;p&gt;In the Runtime layer, it supports multiple execution modes, such as yarn, local, and k8s is under development&lt;/p&gt; &#xA;&lt;h2&gt;Supported Connectors&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;DataSource&lt;/th&gt; &#xA;   &lt;th&gt;Sub Modules&lt;/th&gt; &#xA;   &lt;th&gt;Reader&lt;/th&gt; &#xA;   &lt;th&gt;Writer&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hive&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hadoop&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hbase&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hudi&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Kafka&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RocketMQ&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt; &lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Redis&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt; &lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Doris&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt; &lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MongoDB&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;4&#34;&gt;JDBC&lt;/td&gt; &#xA;   &lt;td&gt;MySQL&lt;/td&gt; &#xA;   &lt;td rowspan=&#34;4&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td rowspan=&#34;4&#34;&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Oracle&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PostgreSQL&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SqlServer&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Fake&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Print&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt; &lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;Documentation for &lt;a href=&#34;https://raw.githubusercontent.com/bytedance/bitsail/master/docs/connectors/introduction.md&#34;&gt;Connectors&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Community Support&lt;/h2&gt; &#xA;&lt;h3&gt;Slack&lt;/h3&gt; &#xA;&lt;p&gt;Join BitSail Slack channel via this &lt;a href=&#34;https://join.slack.com/t/slack-ted3816/shared_invite/zt-1inff2sip-u7Ej_o73sUgdpJAvqwlEwQ&#34;&gt;link&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Mailing List&lt;/h3&gt; &#xA;&lt;p&gt;Currently, BitSail community use Google Group as the mailing list provider. You need to subscribe to the mailing list before starting a conversation&lt;/p&gt; &#xA;&lt;p&gt;Subscribe: Email to this address &lt;code&gt;bitsail+subscribe@googlegroups.com&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Start a conversation: Email to this address &lt;code&gt;bitsail@googlegroups.com&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Unsubscribe: Email to this address &lt;code&gt;bitsail+unsubscribe@googlegroups.com&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;WeChat Group&lt;/h3&gt; &#xA;&lt;p&gt;Welcome to scan this QR code and to join the WeChat group chat.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/bytedance/bitsail/master/docs/images/wechat_QR.png&#34; alt=&#34;qr&#34; width=&#34;100&#34;&gt; &#xA;&lt;h2&gt;Environment Setup&lt;/h2&gt; &#xA;&lt;p&gt;Link to &lt;a href=&#34;https://raw.githubusercontent.com/bytedance/bitsail/master/docs/env_setup.md&#34;&gt;Environment Setup&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Deployment Guide&lt;/h2&gt; &#xA;&lt;p&gt;Link to &lt;a href=&#34;https://raw.githubusercontent.com/bytedance/bitsail/master/docs/deployment.md&#34;&gt;Deployment Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;BitSail Configuration&lt;/h2&gt; &#xA;&lt;p&gt;Link to &lt;a href=&#34;https://raw.githubusercontent.com/bytedance/bitsail/master/docs/config.md&#34;&gt;Configuration Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing Guide&lt;/h2&gt; &#xA;&lt;p&gt;Link to &lt;a href=&#34;https://raw.githubusercontent.com/bytedance/bitsail/master/docs/contributing.md&#34;&gt;Contributing Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Thanks all contributors&lt;/strong&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;a href=&#34;https://github.com/bytedance/bitsail/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=bytedance/bitsail&#34;&gt; &lt;/a&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bytedance/bitsail/master/LICENSE&#34;&gt;Apache 2.0 License&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>apple/batch-processing-gateway</title>
    <updated>2022-10-29T01:35:44Z</updated>
    <id>tag:github.com,2022-10-29:/apple/batch-processing-gateway</id>
    <link href="https://github.com/apple/batch-processing-gateway" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The gateway component to make Spark on K8s much easier for Spark users.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Batch Processing Gateway&lt;/h1&gt; &#xA;&lt;p&gt;Batch Processing Gateway makes running Spark service on Kubernetes easy. It allows users to submit, examine and delete Spark apps on Kubernetes with intuitive API calls, without worrying much about what goes on behind the scene. It can also be configured with many Spark clusters to scale the service horizontally.&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/apple/batch-processing-gateway/main/docs/images/arch_high_level.png&#34; alt=&#34;Architecture&#34; title=&#34;Architecture&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Batch Processing Gateway (BPG) is the frontend of the entire stack of Spark service, which typically includes one gateway instance and multiple Spark K8s clusters.&lt;/p&gt; &#xA;&lt;p&gt;A typical flow of Spark application submission:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Spark users publish the app artifacts (.jar, .py, .zip, etc) to S3 artifacts bucket&lt;/li&gt; &#xA; &lt;li&gt;Users compose job spec which includes key information such as job path, driver core, executor memory, etc, and submit it to a REST endpoint.&lt;/li&gt; &#xA; &lt;li&gt;BPG parses the request, translates it to a custom resource definition (CRD) supported by &lt;a href=&#34;https://github.com/GoogleCloudPlatform/spark-on-k8s-operator&#34;&gt;Spark on K8s Operator&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Using queue and weight based configuration, BPG chooses a Spark K8s Cluster and submits the CRD to it.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://github.com/GoogleCloudPlatform/spark-on-k8s-operator&#34;&gt;Spark on K8s Operator&lt;/a&gt; handles the CRD and submits the Spark app with &lt;code&gt;spark-submit&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Artifacts Bucket&lt;/h3&gt; &#xA;&lt;p&gt;The S3 bucket to hold all the application artifacts, including main app files, dependencies, etc. BPG exposes the upload API for users to upload the artifacts before launching a Spark app.&lt;/p&gt; &#xA;&lt;h3&gt;App Submission DB&lt;/h3&gt; &#xA;&lt;p&gt;BPG generates a &lt;code&gt;Submission ID&lt;/code&gt; as a unique identifier for a submitted app. When the app gets submitted to Spark K8s cluster, Spark will generate an &lt;code&gt;Application ID&lt;/code&gt;, which is also a unique identifier. The App Submission DB maintains the ID mapping, so that users can use both &lt;code&gt;Submission ID&lt;/code&gt; and &lt;code&gt;Application ID&lt;/code&gt; to find the app. A few other metadata fields of the apps are maintained in DB too to enable certain features.&lt;/p&gt; &#xA;&lt;p&gt;To understand how the App Submission DB is populated, refer to the &lt;a href=&#34;https://raw.githubusercontent.com/apple/batch-processing-gateway/main/docs/KEY_COMPONENTS.md#application-monitor&#34;&gt;Application Monitor&lt;/a&gt; section.&lt;/p&gt; &#xA;&lt;h4&gt;application_submission schema (partial)&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Field&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Populated by&lt;/th&gt; &#xA;   &lt;th&gt;Doc&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;submission_id&lt;/td&gt; &#xA;   &lt;td&gt;varchar(255)&lt;/td&gt; &#xA;   &lt;td&gt;Submission&lt;/td&gt; &#xA;   &lt;td&gt;The unique ID generated by BPG&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;user&lt;/td&gt; &#xA;   &lt;td&gt;varchar(255)&lt;/td&gt; &#xA;   &lt;td&gt;Submission&lt;/td&gt; &#xA;   &lt;td&gt;The user who submitted the app&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;app_name&lt;/td&gt; &#xA;   &lt;td&gt;varchar(255)&lt;/td&gt; &#xA;   &lt;td&gt;Monitor&lt;/td&gt; &#xA;   &lt;td&gt;The app name specified in app spec&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;spark_version&lt;/td&gt; &#xA;   &lt;td&gt;varchar(255)&lt;/td&gt; &#xA;   &lt;td&gt;Submission&lt;/td&gt; &#xA;   &lt;td&gt;The Spark version specified in app spec&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;queue&lt;/td&gt; &#xA;   &lt;td&gt;varchar(255)&lt;/td&gt; &#xA;   &lt;td&gt;Submission&lt;/td&gt; &#xA;   &lt;td&gt;The queue specified in app spec&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;status&lt;/td&gt; &#xA;   &lt;td&gt;varchar(255)&lt;/td&gt; &#xA;   &lt;td&gt;Monitor&lt;/td&gt; &#xA;   &lt;td&gt;The latest status of the app&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;app_id&lt;/td&gt; &#xA;   &lt;td&gt;varchar(255)&lt;/td&gt; &#xA;   &lt;td&gt;Monitor&lt;/td&gt; &#xA;   &lt;td&gt;The unique ID generated by Spark K8s cluster&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;request_body&lt;/td&gt; &#xA;   &lt;td&gt;text&lt;/td&gt; &#xA;   &lt;td&gt;Submission&lt;/td&gt; &#xA;   &lt;td&gt;The original request body specified by user&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;created_time&lt;/td&gt; &#xA;   &lt;td&gt;timestamp&lt;/td&gt; &#xA;   &lt;td&gt;Submission&lt;/td&gt; &#xA;   &lt;td&gt;Using system current timestamp (GMT) as default&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;start_time&lt;/td&gt; &#xA;   &lt;td&gt;timestamp&lt;/td&gt; &#xA;   &lt;td&gt;Monitor&lt;/td&gt; &#xA;   &lt;td&gt;The time the app started running (GMT)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Refer to &lt;a href=&#34;https://raw.githubusercontent.com/apple/batch-processing-gateway/main/docs/KEY_COMPONENTS.md&#34;&gt;KEY COMPONENTS&lt;/a&gt; for more details of the key components in BPG.&lt;/p&gt; &#xA;&lt;h2&gt;REST Endpoints&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/apple/batch-processing-gateway/main/docs/images/rest_endpoints.png&#34; alt=&#34;REST Endpoints&#34; title=&#34;REST Endpoints&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;BPG exposes REST endpoints to end users / clients for Spark apps, e.g. &lt;code&gt;POST /apiv2/spark&lt;/code&gt; to submit a Spark app. The REST components receive the user requests, manipulate the requests when necessary, and interact with Spark clusters via &lt;a href=&#34;https://github.com/fabric8io/kubernetes-client&#34;&gt;fabric8&lt;/a&gt; Kubernetes client.&lt;/p&gt; &#xA;&lt;h2&gt;Auth&lt;/h2&gt; &#xA;&lt;p&gt;BPG doesn&#39;t have authentication out of the box. It does have a simple config based &lt;a href=&#34;https://raw.githubusercontent.com/apple/batch-processing-gateway/main/docs/KEY_COMPONENTS.md#user-list-authorizer&#34;&gt;User List Authorizer&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you need authentication or more sophisticated authorization, consider building a sidecar container running in parallel with the BPG container, and pass the username to it after successful auth. This can keep the auth logics decoupled for better maintainability.&lt;/p&gt; &#xA;&lt;p&gt;BPG supports two ways to pass in the user:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Basic authentication: the common header &lt;code&gt;Authorization: Basic &amp;lt;base64-encoded string username:password&amp;gt;&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;A header &lt;code&gt;USER_HEADER_KEY&lt;/code&gt;: this provides more flexibility when auth is done by other processes&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Spark Cluster Routing&lt;/h2&gt; &#xA;&lt;p&gt;BPG essentially takes requests, and routes them as CRDs to the Spark K8s clusters. To utilize the Spark clusters according to business needs, it offers the flexibility to route the requests to a particular namespace based on queues and weights.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/apple/batch-processing-gateway/main/docs/images/spark_cluster_routing.png&#34; alt=&#34;Spark Cluster Routing&#34; title=&#34;Spark Cluster Routing&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Namespace Based Cluster Config&lt;/h3&gt; &#xA;&lt;p&gt;Each Spark cluster configured in BPG maps to a namespace from the actual Spark K8s cluster. In other words, you are able to configure multiple Spark cluster entries, each mapping to a namespace in a single Spark K8s cluster. The Spark jobs will be submitted as CRDs to the particular namespaces. This provides more flexibility to resource allocation.&lt;/p&gt; &#xA;&lt;h3&gt;Queue Config&lt;/h3&gt; &#xA;&lt;p&gt;Each Spark cluster configured has a list of queues to which the Spark apps can be submitted. When there&#39;s no queue specified, BPG will by default try to submit to a &lt;code&gt;poc&lt;/code&gt; queue.&lt;/p&gt; &#xA;&lt;p&gt;When there are multiple Spark clusters supporting a queue, it will choose one cluster based on &lt;a href=&#34;https://raw.githubusercontent.com/apple/batch-processing-gateway/main/#weight-based-cluster-selection&#34;&gt;weight calculation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Weight Based Cluster Selection&lt;/h3&gt; &#xA;&lt;p&gt;Say when a Spark app is submitted to the queue &lt;code&gt;q1&lt;/code&gt;, and all the cluster &lt;code&gt;c01&lt;/code&gt;, &lt;code&gt;c02&lt;/code&gt; and &lt;code&gt;c03&lt;/code&gt; support &lt;code&gt;q1&lt;/code&gt;. How a cluster gets chosen depends on both the cluster weights and some randomness:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;The probability of c01 being selected =&#xA;weight(c01) / (c01.weight + c02.weight + c03.weight)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;So if you want one cluster to be selected more often than the others for the same queue, simply increase the weight of that cluster.&lt;/p&gt; &#xA;&lt;h2&gt;Application Logs&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/apple/batch-processing-gateway/main/docs/images/app_logs_flow.png&#34; alt=&#34;App Log Endpoint&#34; title=&#34;App Log Endpoint&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;When Spark apps run on a Spark K8s cluster, the application logs from driver and executors are written to the pod local storage. However, when the pods are gone after the app completes, the logs will be gone as well. One general way of preserving the logs is to move them to a S3 bucket.&lt;/p&gt; &#xA;&lt;p&gt;When a user requests driver/executor logs via the log endpoint, BPG will first try to load logs from the driver/executor pods. If the pods are gone or the logs are not available, it will then read from a pre-configured S3 bucket.&lt;/p&gt; &#xA;&lt;p&gt;In order for the S3 log storage to work, two things need to be in place:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A &lt;code&gt;log mover&lt;/code&gt; to keep moving the Spark app logs from pods to S3&lt;/li&gt; &#xA; &lt;li&gt;A &lt;code&gt;log index&lt;/code&gt; in DB and an &lt;code&gt;indexer&lt;/code&gt; process to keep track of the S3 prefixes of the log files&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Currently, the &lt;code&gt;log mover&lt;/code&gt; and &lt;code&gt;indexer&lt;/code&gt; are not part of the scope. Service maintainers would need to launch their own processes to utilize the S3 log feature. For &lt;code&gt;log mover&lt;/code&gt;, one solution is to adopt &lt;a href=&#34;https://fluentbit.io&#34;&gt;fluentbit&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;logindex schema (partial)&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Field&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Doc&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;logs3key&lt;/td&gt; &#xA;   &lt;td&gt;varchar(500)&lt;/td&gt; &#xA;   &lt;td&gt;The full path to the log file on S3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;date&lt;/td&gt; &#xA;   &lt;td&gt;date&lt;/td&gt; &#xA;   &lt;td&gt;The date on which the job was created&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;hour&lt;/td&gt; &#xA;   &lt;td&gt;char(2)&lt;/td&gt; &#xA;   &lt;td&gt;The hour on which the job was created&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;containerId&lt;/td&gt; &#xA;   &lt;td&gt;varchar(60)&lt;/td&gt; &#xA;   &lt;td&gt;In the format of &lt;code&gt;&amp;lt;Submission ID&amp;gt;-&amp;lt;driver/exec-index&amp;gt;&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://raw.githubusercontent.com/apple/batch-processing-gateway/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING&lt;/a&gt; for details on how to contribute. To get started on development, refer to the &lt;a href=&#34;https://raw.githubusercontent.com/apple/batch-processing-gateway/main/docs/GETTING_STARTED.md&#34;&gt;GETTING STARTED&lt;/a&gt; guide.&lt;/p&gt; &#xA;&lt;h2&gt;Deployment&lt;/h2&gt; &#xA;&lt;p&gt;In production, typically the Spark apps are run on different Spark K8s clusters, as the Spark apps can be resource demanding. The deployment of BPG on Kubernetes can be managed by a &lt;a href=&#34;https://raw.githubusercontent.com/apple/batch-processing-gateway/main/helm/batch-processing-gateway&#34;&gt;Helm chart&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Built With&lt;/h2&gt; &#xA;&lt;p&gt;Batch Processing Gateway was built with (not limit to):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.dropwizard.io&#34;&gt;Dropwizard&lt;/a&gt; - The web framework used for REST API&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://maven.apache.org/&#34;&gt;Maven&lt;/a&gt; - Dependency Management&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/fabric8io/kubernetes-client&#34;&gt;fabric8 K8s Client&lt;/a&gt; - Kubernetes Client&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://micrometer.io&#34;&gt;Micrometer&lt;/a&gt; - Metrics Registry&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://swagger.io&#34;&gt;Swagger&lt;/a&gt; - OpenAPI and Swagger UI Support&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aws.amazon.com/sdk-for-java/&#34;&gt;AWS SDK for Java&lt;/a&gt; - S3 Upload Support&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://raw.githubusercontent.com/apple/batch-processing-gateway/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; for more information.&lt;/p&gt;</summary>
  </entry>
</feed>