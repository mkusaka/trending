<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub PowerShell Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-10-22T01:58:53Z</updated>
  <subtitle>Weekly Trending of PowerShell in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>AleksaMCode/WiFi-password-stealer</title>
    <updated>2023-10-22T01:58:53Z</updated>
    <id>tag:github.com,2023-10-22:/AleksaMCode/WiFi-password-stealer</id>
    <link href="https://github.com/AleksaMCode/WiFi-password-stealer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Simple Windows and Linux keystroke injection tool that exfiltrates stored WiFi data (SSID and password).&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img width=&#34;150&#34; align=&#34;right&#34; src=&#34;https://raw.githubusercontent.com/AleksaMCode/WiFi-password-stealer/main/resources/wifi-stealer_logo.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;WiFi password stealer&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-GPL_v2-blue.svg?sanitize=true&#34; alt=&#34;License: GPL v2&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/v/release/AleksaMCode/WiFi-password-stealer&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;justify&#34;&gt;Have you ever watched a film where a hacker would plug-in, seemingly ordinary, USB drive into a victim&#39;s computer and steal data from it? - A proper wet dream for some. &lt;/p&gt;&#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;span&gt;‚ö†&lt;/span&gt; &lt;strong&gt;Disclaimer&lt;/strong&gt;: All content in this project is intended for security research purpose only.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Table of contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AleksaMCode/WiFi-password-stealer/main/#wifi-password-stealer&#34;&gt;WiFi password stealer&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AleksaMCode/WiFi-password-stealer/main/#table-of-contents&#34;&gt;Table of contents&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AleksaMCode/WiFi-password-stealer/main/#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AleksaMCode/WiFi-password-stealer/main/#setup&#34;&gt;Setup&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AleksaMCode/WiFi-password-stealer/main/#prerequisites&#34;&gt;Prerequisites&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AleksaMCode/WiFi-password-stealer/main/#requirements---what-youll-need&#34;&gt;Requirements - What you&#39;ll need&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AleksaMCode/WiFi-password-stealer/main/#keystroke-injection-tool&#34;&gt;Keystroke injection tool&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AleksaMCode/WiFi-password-stealer/main/#keystroke-injection&#34;&gt;Keystroke injection&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AleksaMCode/WiFi-password-stealer/main/#delays&#34;&gt;Delays&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AleksaMCode/WiFi-password-stealer/main/#exfiltration&#34;&gt;Exfiltration&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AleksaMCode/WiFi-password-stealer/main/#windows-exploit&#34;&gt;Windows exploit&lt;/a&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AleksaMCode/WiFi-password-stealer/main/#sending-stolen-data-over-email&#34;&gt;Sending stolen data over email&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AleksaMCode/WiFi-password-stealer/main/#linux-exploit&#34;&gt;Linux exploit&lt;/a&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AleksaMCode/WiFi-password-stealer/main/#storing-stolen-data-to-usb-flash-drive&#34;&gt;Storing stolen data to USB flash drive&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AleksaMCode/WiFi-password-stealer/main/#bash-script&#34;&gt;Bash script&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AleksaMCode/WiFi-password-stealer/main/#quick-overview-of-the-payload&#34;&gt;Quick overview of the payload&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AleksaMCode/WiFi-password-stealer/main/#exfiltrated-data-formatting&#34;&gt;Exfiltrated data formatting&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AleksaMCode/WiFi-password-stealer/main/#usb-mass-storage-device-problem&#34;&gt;USB Mass Storage Device Problem&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AleksaMCode/WiFi-password-stealer/main/#payload-writer&#34;&gt;Payload Writer&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AleksaMCode/WiFi-password-stealer/main/#limitationsdrawbacks&#34;&gt;Limitations/Drawbacks&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AleksaMCode/WiFi-password-stealer/main/#to-do-list&#34;&gt;To-Do List&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p align=&#34;justify&#34;&gt;During the summer of 2022, I decided to do exactly that, to build a device that will allow me to steal data from a victim&#39;s computer. So, how does one deploy malware and exfiltrate data? In the following text I will explain all of the necessary steps, theory and nuances when it comes to building your own keystroke injection tool. While this project/tutorial focuses on WiFi passwords, payload code could easily be altered to do something more nefarious. You are only limited by your imagination (and your technical skills).&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p align=&#34;justify&#34;&gt;After creating pico-ducky, you only need to copy the modified payload (adjusted for your SMTP details for Windows exploit and/or adjusted for the Linux password and a USB drive name) to the RPi Pico.&lt;/p&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;p align=&#34;justify&#34;&gt;Physical access to victim&#39;s computer.&lt;/p&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;p align=&#34;justify&#34;&gt;Unlocked victim&#39;s computer.&lt;/p&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;p align=&#34;justify&#34;&gt;Victim&#39;s computer has to have an internet access in order to send the stolen data using &lt;a href=&#34;https://en.wikipedia.org/wiki/Simple_Mail_Transfer_Protocol&#34;&gt;SMTP&lt;/a&gt; for the exfiltration over a network medium.&lt;/p&gt;&lt;/li&gt;&#xA; &lt;p&gt;&lt;/p&gt; &#xA; &lt;li&gt;&lt;p align=&#34;justify&#34;&gt;Knowledge of victim&#39;s computer password for the Linux exploit.&lt;/p&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirements - What you&#39;ll need&lt;/h2&gt; &#xA;&lt;p align=&#34;justify&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AleksaMCode/WiFi-password-stealer/main/resources/RPi-pico.png?raw=true&#34; width=&#34;150&#34; title=&#34;RPi pico illustration&#34; align=&#34;left&#34; hspace=&#34;5&#34; vspace=&#34;5&#34;&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;ul&gt; &#xA; &lt;li&gt;Raspberry Pi Pico (RPi Pico)&lt;/li&gt; &#xA; &lt;li&gt;Micro USB to USB Cable&lt;/li&gt; &#xA; &lt;li&gt;Jumper Wire (optional)&lt;/li&gt; &#xA; &lt;li&gt;pico-ducky - Transformed RPi Pico into a USB Rubber Ducky&lt;/li&gt; &#xA; &lt;li&gt;USB flash drive (for the exploit over physical medium only)&lt;/li&gt; &#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;/p&gt;&#xA;&lt;br&gt;&#xA;&lt;br&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;span&gt;üìù&lt;/span&gt; &lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; &#xA; &lt;ul&gt;&#xA;  &lt;li&gt;&lt;p align=&#34;justify&#34;&gt;It is possible to build this tool using Rubber Ducky, but keep in mind that &lt;a href=&#34;https://www.raspberrypi.com/products/raspberry-pi-pico/&#34;&gt;RPi Pico&lt;/a&gt; costs about $4.00 and the &lt;a href=&#34;https://shop.hak5.org/products/usb-rubber-ducky&#34;&gt;Rubber Ducky&lt;/a&gt; costs $80.00.&lt;/p&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;p align=&#34;justify&#34;&gt;However, while pico-ducky is a good and budget-friedly solution, Rubber Ducky does offer things like stealthiness and usage of the lastest DuckyScript version.&lt;/p&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;p align=&#34;justify&#34;&gt;In order to use Ducky Script to write the payload on your RPi Pico you first need to convert it to a pico-ducky. Follow these &lt;a href=&#34;https://github.com/dbisu/pico-ducky&#34;&gt;simple steps&lt;/a&gt; in order to create pico-ducky.&lt;/p&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Keystroke injection tool&lt;/h2&gt; &#xA;&lt;p align=&#34;justify&#34;&gt;Keystroke injection tool, once connected to a host machine, executes malicious commands by running code that mimics keystrokes entered by a user. While it looks like a USB drive, it acts like a keyboard that types in a preprogrammed payload. Tools like Rubber Ducky can type over 1,000 words per minute. Once created, anyone with physical access can deploy this payload with ease.&lt;/p&gt; &#xA;&lt;h3&gt;Keystroke injection&lt;/h3&gt; &#xA;&lt;p align=&#34;justify&#34;&gt;The payload uses &lt;code&gt;STRING&lt;/code&gt; command processes keystroke for injection. It accepts one or more alphanumeric/punctuation characters and will type the remainder of the line exactly as-is into the target machine. The &lt;code&gt;ENTER&lt;/code&gt;/&lt;code&gt;SPACE&lt;/code&gt; will simulate a press of keyboard keys.&lt;/p&gt; &#xA;&lt;h3&gt;Delays&lt;/h3&gt; &#xA;&lt;p align=&#34;justify&#34;&gt;We use &lt;code&gt;DELAY&lt;/code&gt; command to temporarily pause execution of the payload. This is useful when a payload needs to wait for an element such as a Command Line to load. Delay is useful when used at the very beginning when a new USB device is connected to a targeted computer. Initially, the computer must complete a set of actions before it can begin accepting input commands. In the case of &lt;a href=&#34;https://en.wikipedia.org/wiki/Human_interface_device&#34;&gt;HIDs&lt;/a&gt; setup time is very short. In most cases, it takes a fraction of a second, because the drivers are built-in. However, in some instances, a slower PC may take longer to recognize the pico-ducky. The general advice is to adjust the delay time according to your target.&lt;/p&gt; &#xA;&lt;h2&gt;Exfiltration&lt;/h2&gt; &#xA;&lt;p align=&#34;justify&#34;&gt;Data exfiltration is an unauthorized transfer of data from a computer/device. Once the data is collected, adversary can package it to avoid detection while sending data over the network, using encryption or compression. Two most common way of exfiltration are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Exfiltration over the network medium.&lt;/li&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;p align=&#34;justify&#34;&gt;This approach was used for the Windows exploit. The whole payload can be seen &lt;a href=&#34;https://github.com/AleksaMCode/WiFi-password-stealer/raw/main/payload/payload_windows.template.dd&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;li&gt;Exfiltration over a physical medium.&lt;/li&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;p align=&#34;justify&#34;&gt;This approach was used for the Linux exploit. The whole payload can be seen &lt;a href=&#34;https://github.com/AleksaMCode/WiFi-password-stealer/raw/main/payload/payload_linux.template.dd&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Windows exploit&lt;/h3&gt; &#xA;&lt;p align=&#34;justify&#34;&gt;In order to use the Windows payload (&lt;code&gt;payload1.dd&lt;/code&gt;), you don&#39;t need to connect any jumper wire between pins.&lt;/p&gt; &#xA;&lt;h4&gt;Sending stolen data over email&lt;/h4&gt; &#xA;&lt;p align=&#34;justify&#34;&gt;Once passwords have been exported to the &lt;code&gt;.txt&lt;/code&gt; file, payload will send the data to the appointed email using Yahoo SMTP. For more detailed instructions visit a following &lt;a href=&#34;https://github.com/AleksaMCode/university-notices-email-notifier#yahoo-smtp&#34;&gt;link&lt;/a&gt;. Also, the payload template needs to be updated with your SMTP information, meaning that you need to update &lt;code&gt;RECEIVER_EMAIL&lt;/code&gt;, &lt;code&gt;SENDER_EMAIL&lt;/code&gt; and yours email &lt;code&gt;PASSWORD&lt;/code&gt;. In addition, you could also update the body and the subject of the email.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/AleksaMCode/WiFi-password-stealer/raw/25cf7c56a7df4a9811b4d3eab8d6e6dad4282055/payload/payload_windows.template.dd#L31&#34;&gt;https://github.com/AleksaMCode/WiFi-password-stealer/blob/25cf7c56a7df4a9811b4d3eab8d6e6dad4282055/payload/payload_windows.template.dd#L31&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;span&gt;üìù&lt;/span&gt; &lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;p align=&#34;justify&#34;&gt;After sending data over the email, the &lt;code&gt;.txt&lt;/code&gt; file is deleted.&lt;/p&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;p align=&#34;justify&#34;&gt;You can also use some an SMTP from another email provider, but you should be mindful of SMTP server and port number you will write in the payload.&lt;/p&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;p align=&#34;justify&#34;&gt;Keep in mind that some networks could be blocking usage of an unknown SMTP at the firewall.&lt;/p&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Linux exploit&lt;/h3&gt; &#xA;&lt;p align=&#34;justify&#34;&gt;In order to use the Linux payload (&lt;code&gt;payload2.dd&lt;/code&gt;) you need to connect a jumper wire between &lt;code&gt;GND&lt;/code&gt; and &lt;code&gt;GPIO5&lt;/code&gt; in order to comply with the code in &lt;a href=&#34;https://github.com/dbisu/pico-ducky/raw/main/duckyinpython.py&#34;&gt;&lt;code&gt;code.py&lt;/code&gt;&lt;/a&gt; on your RPi Pico. For more information about how to setup multiple payloads on your RPi Pico visit this &lt;a href=&#34;https://github.com/dbisu/pico-ducky#multiple-payloads&#34;&gt;link&lt;/a&gt;. &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AleksaMCode/WiFi-password-stealer/main/resources/linux-mint_exploit.gif&#34; title=&#34;Linux exploit&#34; width=&#34;600&#34; hspace=&#34;5&#34; vspace=&#34;5&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Storing stolen data to USB flash drive&lt;/h4&gt; &#xA;&lt;p align=&#34;justify&#34;&gt;Once passwords have been exported from the computer, data will be saved to the appointed USB flash drive. In order for this payload to function properly, it needs to be updated with the correct name of your USB drive, meaning you will need to replace &lt;code&gt;USBSTICK&lt;/code&gt; with the name of your USB drive in two places.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/AleksaMCode/WiFi-password-stealer/raw/25cf7c56a7df4a9811b4d3eab8d6e6dad4282055/payload/payload_linux.template.dd#L3&#34;&gt;https://github.com/AleksaMCode/WiFi-password-stealer/blob/25cf7c56a7df4a9811b4d3eab8d6e6dad4282055/payload/payload_linux.template.dd#L3&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/AleksaMCode/WiFi-password-stealer/raw/25cf7c56a7df4a9811b4d3eab8d6e6dad4282055/payload/payload_linux.template.dd#L11&#34;&gt;https://github.com/AleksaMCode/WiFi-password-stealer/blob/25cf7c56a7df4a9811b4d3eab8d6e6dad4282055/payload/payload_linux.template.dd#L11&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;justify&#34;&gt;In addition, you will also need to update the Linux &lt;code&gt;PASSWORD&lt;/code&gt; in the payload in three places. As stated above, in order for this exploit to be successful, you will need to know the victim&#39;s Linux machine password, which makes this attack less plausible.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/AleksaMCode/WiFi-password-stealer/raw/a90ffb208e6a09d1b0ae44d1afe81d82248ba3fe/payload/payload_linux.template.dd#L7&#34;&gt;https://github.com/AleksaMCode/WiFi-password-stealer/blob/a90ffb208e6a09d1b0ae44d1afe81d82248ba3fe/payload/payload_linux.template.dd#L7&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/AleksaMCode/WiFi-password-stealer/raw/a90ffb208e6a09d1b0ae44d1afe81d82248ba3fe/payload/payload_linux.template.dd#L9&#34;&gt;https://github.com/AleksaMCode/WiFi-password-stealer/blob/a90ffb208e6a09d1b0ae44d1afe81d82248ba3fe/payload/payload_linux.template.dd#L9&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Bash script&lt;/h4&gt; &#xA;&lt;p align=&#34;justify&#34;&gt;In order to run the &lt;a href=&#34;https://github.com/AleksaMCode/WiFi-password-stealer/raw/main/scripts/wifi_passwords_print.sh&#34;&gt;&lt;code&gt;wifi_passwords_print.sh&lt;/code&gt;&lt;/a&gt; script you will need to update the script with the correct name of your USB stick after which you can type in the following command in your terminal:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;echo PASSWORD | sudo -S sh wifi_passwords_print.sh USBSTICK&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where &lt;code&gt;PASSWORD&lt;/code&gt; is your account&#39;s password and &lt;code&gt;USBSTICK&lt;/code&gt; is the name for your USB device.&lt;/p&gt; &#xA;&lt;h4&gt;Quick overview of the payload&lt;/h4&gt; &#xA;&lt;p align=&#34;justify&#34;&gt;&lt;b&gt;NetworkManager&lt;/b&gt; is based on the concept of connection profiles, and it uses plugins for reading/writing data. It uses &lt;code&gt;.ini-style&lt;/code&gt; keyfile format and stores network configuration profiles. The &lt;b&gt;keyfile&lt;/b&gt; is a plugin that supports all the connection types and capabilities that &lt;b&gt;NetworkManager&lt;/b&gt; has. The files are located in &lt;i&gt;/etc/NetworkManager/system-connections/&lt;/i&gt;. Based on the &lt;b&gt;keyfile&lt;/b&gt; format, the payload uses the &lt;code&gt;grep&lt;/code&gt; command with regex in order to extract data of interest. For file filtering, a modified positive lookbehind assertion was used (&lt;code&gt;(?&amp;lt;=keyword)&lt;/code&gt;). While the positive lookbehind assertion will match at a certain position in the string, &lt;a href=&#34;https://en.wikipedia.org/wiki/Viz.&#34;&gt;sc.&lt;/a&gt; at a position right after the &lt;i&gt;keyword&lt;/i&gt; without making that text itself part of the match, the regex &lt;code&gt;(?&amp;lt;=keyword).*&lt;/code&gt; will match any text after the &lt;i&gt;keyword&lt;/i&gt;. This allows the payload to match the values after &lt;b&gt;SSID&lt;/b&gt; and &lt;b&gt;psk&lt;/b&gt; (&lt;a href=&#34;https://en.wikipedia.org/wiki/Pre-shared_key&#34;&gt;pre-shared key&lt;/a&gt;) keywords.&lt;/p&gt; &#xA;&lt;p align=&#34;justify&#34;&gt;For more information about &lt;b&gt;NetworkManager&lt;/b&gt; here is some useful links:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_networking/assembly_networkmanager-connection-profiles-in-keyfile-format_configuring-and-managing-networking#proc_manually-creating-a-networkmanager-profile-in-keyfile-format_assembly_networkmanager-connection-profiles-in-keyfile-format&#34;&gt;Manually creating NetworkManager profiles in keyfile format&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://developer-old.gnome.org/NetworkManager/stable/nm-settings-keyfile.html&#34;&gt;Description of keyfile settings plugin&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Exfiltrated data formatting&lt;/h3&gt; &#xA;&lt;p align=&#34;justify&#34;&gt;Below is an example of the exfiltrated and formatted data from a victim&#39;s machine in a &lt;code&gt;.txt&lt;/code&gt; file.&lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/AleksaMCode/WiFi-password-stealer/raw/f5b3b11328764eb07d765a210fbf25db3a828455/resources/wifi_pass.txt#L1-L5&#34;&gt;https://github.com/AleksaMCode/WiFi-password-stealer/blob/f5b3b11328764eb07d765a210fbf25db3a828455/resources/wifi_pass.txt#L1-L5&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;USB Mass Storage Device Problem&lt;/h2&gt; &#xA;&lt;p align=&#34;justify&#34;&gt;One of the advantages of Rubber Ducky over RPi Pico is that it doesn&#39;t show up as a USB mass storage device once plugged in. Once plugged into the computer, all the machine sees it as a USB keyboard. This isn&#39;t a default behavior for the RPi Pico. If you want to prevent your RPi Pico from showing up as a USB mass storage device when plugged in, you need to connect a jumper wire between pin 18 (&lt;code&gt;GND&lt;/code&gt;) and pin 20 (&lt;code&gt;GPIO15&lt;/code&gt;). For more details visit this &lt;a href=&#34;https://github.com/dbisu/pico-ducky#usb-enabledisable-mode&#34;&gt;link&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;span&gt;üí°&lt;/span&gt; &lt;strong&gt;Tip&lt;/strong&gt;:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Upload your payload to RPi Pico before you connect the pins.&lt;/li&gt; &#xA;  &lt;li&gt;Don&#39;t solder the pins because you will probably want to change/update the payload at some point.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Payload Writer&lt;/h2&gt; &#xA;&lt;p align=&#34;justify&#34;&gt;When creating a functioning payload file, you can use the &lt;a href=&#34;https://github.com/AleksaMCode/WiFi-password-stealer/raw/main/payload/writer.py&#34;&gt;&lt;code&gt;writer.py&lt;/code&gt;&lt;/a&gt; script, or you can manually change the template file. In order to run the script successfully you will need to pass, in addition to the script file name, a name of the OS (&lt;i&gt;windows&lt;/i&gt; or &lt;i&gt;linux&lt;/i&gt;) and the name of the payload file (e.q. &lt;i&gt;&lt;a href=&#34;https://github.com/AleksaMCode/WiFi-password-stealer/releases/latest/download/payload1.dd&#34;&gt;payload1.dd&lt;/a&gt;&lt;/i&gt;). Below you can find an example how to run the &lt;i&gt;writer&lt;/i&gt; script when creating a Windows payload.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 writer.py windows payload1.dd&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Limitations/Drawbacks&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;p align=&#34;justify&#34;&gt;&lt;s&gt;This pico-ducky currently works only on Windows OS.&lt;/s&gt;&lt;/p&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;p align=&#34;justify&#34;&gt;This attack requires physical access to an unlocked device in order to be successfully deployed.&lt;/p&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;p align=&#34;justify&#34;&gt;The Linux exploit is far less likely to be successful, because in order to succeed, you not only need physical access to an unlocked device, you also need to know the admins password for the Linux machine.&lt;/p&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;p align=&#34;justify&#34;&gt;Machine&#39;s firewall or network&#39;s firewall may prevent stolen data from being sent over the network medium.&lt;/p&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;p align=&#34;justify&#34;&gt;Payload delays could be inadequate due to varying speeds of different computers used to deploy an attack.&lt;/p&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;p align=&#34;justify&#34;&gt;The pico-ducky device isn&#39;t really stealthy, actually it&#39;s quite the opposite, it&#39;s really bulky especially if you solder the pins.&lt;/p&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;p align=&#34;justify&#34;&gt;Also, the pico-ducky device is noticeably slower compared to the Rubber Ducky running the same script.&lt;/p&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;p align=&#34;justify&#34;&gt;&lt;s&gt;If the &lt;code&gt;Caps Lock&lt;/code&gt; is ON, some of the payload code will not be executed and the exploit will fail.&lt;/s&gt;&lt;/p&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;p align=&#34;justify&#34;&gt;If the computer has a non-English Environment set, this exploit won&#39;t be successful.&lt;/p&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;p align=&#34;justify&#34;&gt;Currently, pico-ducky doesn&#39;t support DuckyScript 3.0, only DuckyScript 1.0 can be used. If you need the 3.0 version you will have to use the Rubber Ducky.&lt;/p&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;To-Do List&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Fix &lt;code&gt;Caps Lock&lt;/code&gt; bug.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Fix non-English Environment bug.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Obfuscate the command prompt.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Implement exfiltration over a physical medium.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Create a payload for Linux.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Encode/Encrypt exfiltrated data before sending it over email.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Implement indicator of successfully completed exploit.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Implement command history clean-up for Linux exploit.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Enhance the Linux exploit in order to avoid usage of &lt;code&gt;sudo&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>FalsePhilosopher/BadUSB-Playground</title>
    <updated>2023-10-22T01:58:53Z</updated>
    <id>tag:github.com,2023-10-22:/FalsePhilosopher/BadUSB-Playground</id>
    <link href="https://github.com/FalsePhilosopher/BadUSB-Playground" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Flipper Zero geared badusb playground&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;BadUSB-Playground&lt;/h1&gt; &#xA;&lt;p&gt;Welcome to the BadUSB Playground, let&#39;s build some cool stuff! Most of what you find in here is either a WIP or resources for a project. If you are just looking for ducky scripts head over to my &lt;a href=&#34;https://github.com/FalsePhilosopher/badusb&#34;&gt;BadUSB repo&lt;/a&gt;.&lt;br&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;h2&gt;Please adhere to the following best practices and style guides when submitting a payload.&lt;/h2&gt; &#xA;&lt;h2&gt;Naming Conventions&lt;/h2&gt; &#xA;&lt;p&gt;Please give your payload a unique, descriptive and appropriate name. Do not use spaces in payload, directory or file names. Each payload should be submit into its own directory, with &lt;code&gt;-&lt;/code&gt; or &lt;code&gt;_&lt;/code&gt; used in place of spaces, to one of the categories such as exfiltration, phishing, remote_access or recon. Do not create your own category.&lt;/p&gt; &#xA;&lt;h2&gt;Payload Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Payloads should begin with &lt;code&gt;REM&lt;/code&gt; comments specifying the title of the payload, the author, the target, and a brief description.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;REM Title: Canary Duck&#xA;REM Author: Jessie Crimson Hart&#xA;REM Description: Opens hidden powershell and connects to canary webserver using Invoke-WebRequest alerting you to spies and snoops.&#xA;REM Target: Windows 10 (Powershell)&#xA;REM Props: Hak5, Thinkst&#xA;REM Version: 1.0&#xA;REM Category: General&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Payloads from this repository are provided for educational purposes only. Hak5/FZ gear is intended for authorized auditing and security analysis purposes only where permitted subject to local and international laws where applicable. Users are solely responsible for compliance with all laws of their locality. FalsePhilosopher and affiliates claim no responsibility for unauthorized or unlawful use.&lt;/p&gt; &#xA;&lt;h1&gt;Disclaimer&lt;/h1&gt; &#xA;&lt;h3&gt;&lt;b&gt;As with any script, you are advised to proceed with caution.&lt;/b&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;&lt;b&gt;Generally, payloads may execute commands on your device. As such, it is possible for a payload to damage your device. Payloads from this repository are provided AS-IS without support. While I make a best effort to review payloads, there are no guarantees as to their effectiveness.&lt;/b&gt;&lt;/h3&gt;</summary>
  </entry>
  <entry>
    <title>omerbsezer/Fast-Kubernetes</title>
    <updated>2023-10-22T01:58:53Z</updated>
    <id>tag:github.com,2023-10-22:/omerbsezer/Fast-Kubernetes</id>
    <link href="https://github.com/omerbsezer/Fast-Kubernetes" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This repo covers Kubernetes with LABs: Kubectl, Pod, Deployment, Service, PV, PVC, Rollout, Multicontainer, Daemonset, Taint-Toleration, Job, Ingress, Kubeadm, Helm, etc.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Fast-Kubernetes&lt;/h1&gt; &#xA;&lt;p&gt;This repo covers Kubernetes objects&#39; and components&#39; details (Kubectl, Pod, Deployment, Service, ConfigMap, Volume, PV, PVC, Daemonset, Secret, Affinity, Taint-Toleration, Helm, etc.) fastly, and possible example usage scenarios (HowTo: Hands-on LAB) in a nutshell. Possible usage scenarios are aimed to update over time.&lt;/p&gt; &#xA;&lt;h2&gt;Prerequisite&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Have a knowledge of Container Technology (Docker). You can learn it from here =&amp;gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Docker&#34;&gt;Fast-Docker&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Keywords:&lt;/strong&gt; Containerization, Kubernetes, Kubectl, Pod, Deployment, Service, ConfigMap, ReplicaSet, Volume, Cheatsheet.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; K8s objects and objects feature can be updated/changed in time. While creating this repo, the version of K8s was v1.22.3. Some sections are trying to be kept up to date. Especially &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Kubeadm-Cluster-Setup.md&#34;&gt;Creating K8s Cluster with Kubeadm and Containerd&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Quick Look (HowTo): Scenarios - Hands-on LAB&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-CreatingPod-Imperative.md&#34;&gt;LAB: K8s Creating Pod - Imperative Way&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8-CreatingPod-Declerative.md&#34;&gt;LAB: K8s Creating Pod - Declarative Way (With File) - Environment Variable&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Multicontainer-Sidecar.md&#34;&gt;LAB: K8s Multicontainer - Sidecar - Emptydir Volume - Port-Forwarding&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Deployment.md&#34;&gt;LAB: K8s Deployment - Scale Up/Down - Bash Connection - Port Forwarding&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Rollout-Rollback.md&#34;&gt;LAB: K8s Rollout - Rollback&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Service-App.md&#34;&gt;LAB: K8s Service Implementations (ClusterIp, NodePort and LoadBalancer)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Liveness-App.md&#34;&gt;LAB: K8s Liveness Probe&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Secret.md&#34;&gt;LAB: K8s Secret (Declarative and Imperative Way)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Configmap.md&#34;&gt;LAB: K8s Config Map&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Node-Affinity.md&#34;&gt;LAB: K8s Node Affinity&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Taint-Toleration.md&#34;&gt;LAB: K8s Taint-Toleration&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Daemon-Sets.md&#34;&gt;LAB: K8s Daemonset - Creating 3 nodes on Minikube&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-PersistantVolume.md&#34;&gt;LAB: K8s Persistent Volume and Persistent Volume Claim&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Statefulset.md&#34;&gt;LAB: K8s Stateful Sets - Nginx&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Job.md&#34;&gt;LAB: K8s Job&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-CronJob.md&#34;&gt;LAB: K8s Cron Job&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Ingress.md&#34;&gt;LAB: K8s Ingress&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/Helm.md&#34;&gt;LAB: Helm Install &amp;amp; Usage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Kubeadm-Cluster-Setup.md&#34;&gt;LAB: K8s Cluster Setup with Kubeadm and Containerd&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Kubeadm-Cluster-Docker.md&#34;&gt;LAB: K8s Cluster Setup with Kubeadm and Docker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Helm-Jenkins.md&#34;&gt;LAB: Helm-Jenkins on running K8s Cluster (2 Node Multipass VM)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Enable-Dashboard-On-Cluster.md&#34;&gt;LAB: Enable Dashboard on Real K8s Cluster&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Monitoring-Prometheus-Grafana.md&#34;&gt;LAB: K8s Monitoring - Prometheus and Grafana&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/KubernetesCommandCheatSheet.md&#34;&gt;Kubectl Commands Cheatsheet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/HelmCheatsheet.md&#34;&gt;Helm Commands Cheatsheet&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Table of Contents&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#motivation&#34;&gt;Motivation&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#containerization&#34;&gt;What is Containerization? What is Container Orchestration?&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#features&#34;&gt;Features&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#whatIsKubernetes&#34;&gt;What is Kubernetes?&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#architecture&#34;&gt;Kubernetes Architecture&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#components&#34;&gt;Kubernetes Components&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#kubectl&#34;&gt;Kubectl Config ‚Äì Usage&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#pod&#34;&gt;Pod: Creating, Yaml, LifeCycle&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#multicontainerpod&#34;&gt;MultiContainer Pod, Init Container&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#labelselector&#34;&gt;Label and Selector, Annotation, Namespace&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#deployment&#34;&gt;Deployment&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#replicaset&#34;&gt;Replicaset&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#rollout-rollback&#34;&gt;Rollout and Rollback&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#network-service&#34;&gt;Network, Service&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#liveness-readiness&#34;&gt;Liveness and Readiness Probe&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#environmentvariable&#34;&gt;Resource Limit, Environment Variable&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#volume&#34;&gt;Volume&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#secret&#34;&gt;Secret&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#configmap&#34;&gt;ConfigMap&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#node-pod-affinity&#34;&gt;Node ‚Äì Pod Affinity&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#taint-tolereation&#34;&gt;Taint and Toleration&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#daemon-set&#34;&gt;Deamon Set&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#pvc&#34;&gt;Persistent Volume and Persistent Volume Claim&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#storageclass&#34;&gt;Storage Class&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#statefulset&#34;&gt;Stateful Set&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#job&#34;&gt;Job, CronJob&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#authentication&#34;&gt;Authentication, Role Based Access Control, Service Account&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#ingress&#34;&gt;Ingress&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#dashboard&#34;&gt;Dashboard&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#playwithkubernetes&#34;&gt;Play With Kubernetes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#helm&#34;&gt;Helm: Kuberbetes Package Manager&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#cheatsheet&#34;&gt;Kubernetes Commands Cheatsheet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#helm_cheatsheet&#34;&gt;Helm Commands Cheatsheet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#cluster_setup&#34;&gt;Kubernetes Cluster Setup: Kubeadm, Containerd, Multipass&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#prometheus_grafana&#34;&gt;Monitoring Kubernetes Cluster with SSH, Prometheus and Grafana&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#resource&#34;&gt;Other Useful Resources Related Kubernetes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Motivation &lt;a name=&#34;motivation&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Why should we use Kubernetes? &#34;Kubernetes is a portable, extensible, open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.&#34; (Ref: Kubernetes.io)&lt;/p&gt; &#xA;&lt;h3&gt;What is Containerization? What is Container Orchestration? &lt;a name=&#34;containerization&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&#34;Containerization is an operating system-level virtualization or application-level virtualization over multiple network resources so that software applications can run in isolated user spaces called containers in any cloud or non-cloud environment&#34; (wikipedia)&lt;/li&gt; &#xA; &lt;li&gt;With Docker Environment, we can create containers.&lt;/li&gt; &#xA; &lt;li&gt;Kubernetes and Docker Swarm are the container orchestration and management tools that automate and schedule the deployment, management, scaling, and networking of containers.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/146249579-b4221dc1-bad7-4da5-831a-849a71fa849e.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Features &lt;a name=&#34;features&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Service discovery and load balancing:&lt;/strong&gt; Kubernetes can expose a container using the DNS name or using their own IP address. If traffic to a container is high, Kubernetes is able to load balance and distribute the network traffic so that the deployment is stable.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Storage orchestration:&lt;/strong&gt; Kubernetes allows you to automatically mount a storage system of your choice, such as local storages, public cloud providers, and more.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Automated rollouts and rollbacks:&lt;/strong&gt;&amp;nbsp; You can describe the desired state for your deployed containers using Kubernetes, and it can change the actual state to the desired state at a controlled rate.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Automatic bin packing:&lt;/strong&gt; You tell Kubernetes how much CPU and memory (RAM) each container needs. Kubernetes can fit containers onto your nodes to make the best use of your resources.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Self-monitoring:&lt;/strong&gt;&amp;nbsp;Kubernetes checks constantly the health of nodes and containers&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Self-healing:&lt;/strong&gt; Kubernetes restarts containers that fail, replaces containers, kills containers that don&#39;t respond to your user-defined health check&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Automates various manual processes:&lt;/strong&gt;&amp;nbsp;for instance, Kubernetes will control for you which server will host the container, how it will be launched etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Interacts with several groups of containers:&lt;/strong&gt;&amp;nbsp;Kubernetes is able to manage more cluster at the same time&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Provides additional services:&lt;/strong&gt;&amp;nbsp;as well as the management of containers, Kubernetes offers security, networking and storage services&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Horizontal scaling:&lt;/strong&gt;&amp;nbsp;Kubernetes allows you scaling resources not only vertically but also horizontally, easily and quickly&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Container balancing:&lt;/strong&gt;&amp;nbsp;Kubernetes always knows where to place containers, by calculating the ‚Äúbest location‚Äù for them&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Run everywhere:&lt;/strong&gt;&amp;nbsp;Kubernetes is an open source tool and gives you the freedom to take advantage of on-premises, hybrid, or public cloud infrastructure, letting you move workloads to anywhere you want&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Secret and configuration management:&lt;/strong&gt; Kubernetes lets you store and manage sensitive information&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What is Kubernetes? &lt;a name=&#34;whatIsKubernetes&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&#34;Kubernetes is a portable, extensible, open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.&#34; (Ref: Kubernetes.io)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/146247396-5bc3bbf9-41fa-47ff-b10d-cac305379e21.png&#34; alt=&#34;image&#34;&gt; (Ref: Kubernetes.io)&lt;/p&gt; &#xA;&lt;h3&gt;Kubernetes Architecture &lt;a name=&#34;architecture&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/146250114-18759a06-e6a6-4554-bc7f-b23a13534f77.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Kubernetes Components &lt;a name=&#34;components&#34;&gt;&lt;/a&gt; (Ref: Kubernetes.io)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Control Plane:&lt;/strong&gt; User enters commands and configuration files from control plane. It controls all cluster. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;API Server:&lt;/strong&gt; &#34;It exposes the Kubernetes API. The API server is the front end for the Kubernetes control plane.&#34;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Etcd:&lt;/strong&gt; &#34;Consistent and highly-available key value store used as Kubernetes&#39; backing store for all cluster data (meta data, objects, etc.).&#34;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Scheduler:&lt;/strong&gt; &#34;It watches for newly created Pods with no assigned node, and selects a node for them to run on. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Factors taken into account for scheduling decisions include: &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;individual and collective resource requirements,&lt;/li&gt; &#xA;       &lt;li&gt;hardware/software/policy constraints,&lt;/li&gt; &#xA;       &lt;li&gt;affinity and anti-affinity specifications,&lt;/li&gt; &#xA;       &lt;li&gt;data locality,&lt;/li&gt; &#xA;       &lt;li&gt;inter-workload interference,&lt;/li&gt; &#xA;       &lt;li&gt;deadlines.&#34;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Controller Manager:&lt;/strong&gt; &#34;It runs controller processes. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Logically, each controller is a separate process, but to reduce complexity, they are all compiled into a single binary and run in a single process.&lt;/li&gt; &#xA;     &lt;li&gt;Some types of these controllers are: &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;Node controller: Responsible for noticing and responding when nodes go down.&lt;/li&gt; &#xA;       &lt;li&gt;Job controller: Watches for Job objects that represent one-off tasks, then creates Pods to run those tasks to completion.&lt;/li&gt; &#xA;       &lt;li&gt;Endpoints controller: Populates the Endpoints object (that is, joins Services &amp;amp; Pods).&lt;/li&gt; &#xA;       &lt;li&gt;Service Account &amp;amp; Token controllers: Create default accounts and API access tokens for new namespaces&#34;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Cloud Controller Manager:&lt;/strong&gt; &#34;It embeds cloud-specific control logic. The cloud controller manager lets you link your cluster into your cloud provider&#39;s API, and separates out the components that interact with that cloud platform from components that only interact with your cluster. The cloud-controller-manager only runs controllers that are specific to your cloud provider &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;The following controllers can have cloud provider dependencies: &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;Node controller: For checking the cloud provider to determine if a node has been deleted in the cloud after it stops responding&lt;/li&gt; &#xA;       &lt;li&gt;Route controller: For setting up routes in the underlying cloud infrastructure&lt;/li&gt; &#xA;       &lt;li&gt;Service controller: For creating, updating and deleting cloud provider load balancers.&#34;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Node:&lt;/strong&gt; &#34;Node components run on every node, maintaining running pods and providing the Kubernetes runtime environment.&#34; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Kubelet:&lt;/strong&gt; &#34;An agent that runs on each node in the cluster. It makes sure that containers are running in a Pod. The kubelet takes a set of PodSpecs that are provided through various mechanisms and ensures that the containers described in those PodSpecs are running and healthy.&#34;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Kube-proxy:&lt;/strong&gt; &#34;It is a network proxy that runs on each node in your cluster, implementing part of the Kubernetes Service concept. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;It maintains network rules on nodes. These network rules allow network communication to your Pods from network sessions inside or outside of your cluster.&lt;/li&gt; &#xA;     &lt;li&gt;It uses the operating system packet filtering layer if there is one and it&#39;s available. Otherwise, kube-proxy forwards the traffic itself.&#34;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Container Runtime:&lt;/strong&gt; &#34;The container runtime is the software that is responsible for running containers. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Kubernetes supports several container runtimes: &lt;strong&gt;Docker, containerd, CRI-O,&lt;/strong&gt; and any implementation of the Kubernetes CRI (Container Runtime Interface)&#34;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/146250916-a9298521-526b-451a-9810-6813e4165db5.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Installation &lt;a name=&#34;installation&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Download:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Kubectl:&lt;/strong&gt; The Kubernetes command-line tool, kubectl, allows you to run commands against Kubernetes clusters.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Minikube:&lt;/strong&gt; It is a tool that lets you run Kubernetes locally. It runs a single-node Kubernetes cluster on your personal computer (&lt;a href=&#34;https://minikube.sigs.k8s.io/docs/start/&#34;&gt;https://minikube.sigs.k8s.io/docs/start/&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;KubeAdm:&lt;/strong&gt; You can use the kubeadm tool to create and manage Kubernetes clusters. This is for creating cluster with computers (Goto: &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Kubeadm-Cluster-Setup.md&#34;&gt;LAB: K8s Kubeadm Cluster Setup&lt;/a&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;from here=&amp;gt; &lt;a href=&#34;https://kubernetes.io/docs/tasks/tools/&#34;&gt;https://kubernetes.io/docs/tasks/tools/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;For learning K8s and running on a computer, &lt;strong&gt;Kubectl and Minikube&lt;/strong&gt; are enough to install.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PS:&lt;/strong&gt; Cloud providers (Azure, Google Cloud, AWS) offer managed K8s (control plane is managed by cloud provides). You can easily create your cluster (number of computer and details) and make connection with Kubectl (using CLI get-credentials of cluster on the cloud)&lt;/p&gt; &#xA;&lt;h3&gt;Kubectl Config ‚Äì Usage &lt;a name=&#34;kubectl&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h4&gt;Config File&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can communicate with K8s cluster in different ways: REST API, Command Line Tool (CLI-Kubectl), GUI (kube-dashboard, etc.)&lt;/li&gt; &#xA; &lt;li&gt;After installation, you can find the kubernetes config file (C:\Users\User.kube\config) that is YAML file.&lt;/li&gt; &#xA; &lt;li&gt;Config file contains 3 main parts: Clusters (cluster certificate data, server, name), Context (cluster and user, namespace), Users (name, config features, certificates, etc.)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Usage&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Kubectl is our main command line tool that connects minikube. There are many combination of commands. So it is not possible to list all commands.&lt;/li&gt; &#xA; &lt;li&gt;When run &#34;kubectl&#34; on the terminal, it can be seen some simple commands. Also &#34;kubectl &lt;command&gt; --help&#34; gives more information.&lt;/li&gt; &#xA; &lt;li&gt;Pattern: kubectl [get|delete|edit|apply] [pods|deployment|services] [podName|serviceName|deploymentName]&lt;/li&gt; &#xA; &lt;li&gt;Example: &#34;kubectl get pods podName&#34;, &#34;kubectl delete pods test_pod&#34;, &#34;kubectl describe pods firstpod&#34;, etc.&lt;/li&gt; &#xA; &lt;li&gt;All necessary/most usable commands are listed in the &#34;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/KubernetesCommandCheatSheet.md&#34;&gt;Kubernetes Commands Cheatsheet&lt;/a&gt;&#34;. Please have a look to get more information and usage.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Pod: Creating, Yaml, LifeCycle &lt;a name=&#34;pod&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Pod is the smallest unit that is created and managed in K8s.&lt;/li&gt; &#xA; &lt;li&gt;Pods may contain more than 1 container, but mostly pods contain only 1 container.&lt;/li&gt; &#xA; &lt;li&gt;Each pod has unique id (uid).&lt;/li&gt; &#xA; &lt;li&gt;Each pod has unique IP address.&lt;/li&gt; &#xA; &lt;li&gt;Containers in the same Pod run on the same Node (computer), and these containers can communicate with each other on the localhost.&lt;/li&gt; &#xA; &lt;li&gt;Creation of the first pod, IMPERATIVE WAY (with command):&lt;/li&gt; &#xA; &lt;li&gt;Please have a look Scenario (&lt;strong&gt;Creating Pod - Imperative way&lt;/strong&gt;, below link) to learn more information about the pod&#39;s kubectl commands. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;how to create basic K8s pod using imperative commands,&lt;/li&gt; &#xA;   &lt;li&gt;how to get more information about pod (to solve troubleshooting),&lt;/li&gt; &#xA;   &lt;li&gt;how to run commands in pod,&lt;/li&gt; &#xA;   &lt;li&gt;how to delete pod.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto the Scenario:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-CreatingPod-Imperative.md&#34;&gt;LAB: K8s Creating Pod - Imperative Way&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Pod: YAML File&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Imperative way could be difficult to store and manage process. Every time we have to enter commands. To prevent this, we can use YAML file to define pods and pods&#39; feature. This way is called &#34;Declarative Way&#34;.&lt;/li&gt; &#xA; &lt;li&gt;Declarative way (with file), Imperative way (with command)&lt;/li&gt; &#xA; &lt;li&gt;Sample Yaml File:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/153674712-426a262d-d13e-489d-9c86-63ac22114d75.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Please have a look Scenario (&lt;strong&gt;Creating Pod - Declarative way&lt;/strong&gt;, below link) to learn more information.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto the Scenario:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8-CreatingPod-Declerative.md&#34;&gt;LAB: K8s Creating Pod - Declarative Way (With File) - Environment Variable&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Pod: Life Cycle&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Pending:&lt;/strong&gt; API-&amp;gt;etcd, pod created, pod id created, but not running on the node.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Creating:&lt;/strong&gt; Scheduler take pod from etcd, assing on node. Kubelet on the Node pull images from docker registry or repository.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ImagePullBackOff:&lt;/strong&gt; Kubelet can not pull image from registry. E.g. Image name is fault (typo error), Authorization Failure, Username/Pass error.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Running:&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Container closes in 3 ways: &#xA;    &lt;ol&gt; &#xA;     &lt;li&gt;App completes the mission and closes automatically without giving error,&lt;/li&gt; &#xA;     &lt;li&gt;Use or System sends close signal and closes automatically without giving error,&lt;/li&gt; &#xA;     &lt;li&gt;Giving error, collapsed and closes with giving error code.&lt;/li&gt; &#xA;    &lt;/ol&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Restart Policies (it can defined in the pod definition): &#xA;    &lt;ol&gt; &#xA;     &lt;li&gt;Always: Default value, kubelet starts always when closing with or without error,&lt;/li&gt; &#xA;     &lt;li&gt;On-failure: It starts again when it gets only error,&lt;/li&gt; &#xA;     &lt;li&gt;Never: It never restarts in any case.&lt;/li&gt; &#xA;    &lt;/ol&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Successed (completed)&lt;/strong&gt;: If the container closes successfully without error and restart policy is configured as on-failure/never, it converts to succeed.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Failed&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;CrashLoopBackOff:&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;If restart policy is configured as always and container closes again and again, container restarts again and again (Restart waiting duration before restarting again: 10 sec -&amp;gt; 20 sec -&amp;gt; 40 sec -&amp;gt; .. -&amp;gt; 5mins), It runs every 5 mins if the pod is crashed.&lt;/li&gt; &#xA;   &lt;li&gt;If container runs more than 10 mins, status converted from &#39;CrashLoopBackOff&#39; to &#39;Running&#39;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;MultiContainer Pod, Init Container &lt;a name=&#34;multicontainerpod&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Best Practice: 1 Container runs in 1 Pod normally, because the smallest element in K8s is Pod (Pod can be scaled up/down).&lt;/li&gt; &#xA; &lt;li&gt;Multicontainers run in the same Pod when containers are dependent of each other.&lt;/li&gt; &#xA; &lt;li&gt;Multicontainers in one Pod have following features: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Multi containers that run on the same Pod run on the same Node.&lt;/li&gt; &#xA;   &lt;li&gt;Containers in the same Pod run/pause/deleted at the same time.&lt;/li&gt; &#xA;   &lt;li&gt;Containers in the same Pod communicate with each other on localhost, there is not any network isolation.&lt;/li&gt; &#xA;   &lt;li&gt;Containers in the same Pod use one volume commonly and they can reach same files in the volume.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Init Containers&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Init containers are used for configuration of apps before running app container.&lt;/li&gt; &#xA; &lt;li&gt;Init containers handle what it should run, then it closes successfully, after init containers close, app containers start.&lt;/li&gt; &#xA; &lt;li&gt;Example below shows how to define init containers in one Pod. There are 2 containers: appcontainer and initcontainer. Initcontainer is polling the service (myservice). When it finds, it closes and app container starts.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1&#xA;kind: Pod&#xA;metadata:&#xA;  name: initcontainerpod&#xA;spec:&#xA;  containers:&#xA;  - name: appcontainer            # after initcontainer closed successfully, appcontainer starts.&#xA;    image: busybox&#xA;    command: [&#39;sh&#39;, &#39;-c&#39;, &#39;echo The app is running! &amp;amp;&amp;amp; sleep 3600&#39;]&#xA;  initContainers:&#xA;  - name: initcontainer&#xA;    image: busybox                # init container starts firstly and look up myservice is up or not in every 2 seconds, if there is myservice available, initcontainer closes. &#xA;    command: [&#39;sh&#39;, &#39;-c&#39;, &#34;until nslookup myservice; do echo waiting for myservice; sleep 2; done&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;# save as service.yaml and run after pod creation&#xA;apiVersion: v1&#xA;kind: Service&#xA;metadata:&#xA;  name: myservice&#xA;spec:&#xA;  ports:&#xA;  - protocol: TCP&#xA;    port: 80&#xA;    targetPort: 9376&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Please have a look Scenario (below link) to learn more information.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto the Scenario:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Multicontainer-Sidecar.md&#34;&gt;LAB: K8s Multicontainer - Sidecar - Emptydir Volume - Port-Forwarding&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Label and Selector, Annotation, Namespace &lt;a name=&#34;labelselector&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h4&gt;Label&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Label is important to reach the K8s objects with key:value pairs.&lt;/li&gt; &#xA; &lt;li&gt;key:value is used for labels. E.g. tier:frontend, stage:test, name:app1, team:development&lt;/li&gt; &#xA; &lt;li&gt;prefix may also be used for optional with key:value. E.g. example.com/tier:front-end, kubernetes.io/ , k8s.io/&lt;/li&gt; &#xA; &lt;li&gt;In the file (declerative way), labels are added under metadata. It is possible to add multiple labels.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/153675164-62265978-60c3-4167-ad0c-4bfbbf1f704b.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;In the command (imperative way), we can also add label to the pods.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;kubectl label pods pod1 team=development  #adding label team=development on pod1&#xA;kubectl get pods --show-labels&#xA;kubectl label pods pod1 team-  #remove team (key:value) from pod1&#xA;kubectl label --overwrite pods pod1 team=test #overwrite/change label on pod1&#xA;kubectl label pods --all foo=bar  # add label foo=bar for all pods&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Selector&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We can select/filter pods with kubectl.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;kubectl get pods -l &#34;app=firstapp&#34; --show-labels&#xA;kubectl get pods -l &#34;app=firstapp,tier=frontend&#34; --show-labels&#xA;kubectl get pods -l &#34;app=firstapp,tier!=frontend&#34; --show-labels&#xA;kubectl get pods -l &#34;app,tier=frontend&#34; --show-labels #equality-based selector&#xA;kubectl get pods -l &#34;app in (firstapp)&#34; --show-labels  #set-based selector&#xA;kubectl get pods -l &#34;app not in (firstapp)&#34; --show-labels  #set-based selector&#xA;kubectl get pods -l &#34;app=firstapp,app=secondapp&#34; --show-labels # comma means and =&amp;gt; firstapp and secondapp&#xA;kubectl get pods -l &#34;app in (firstapp,secondapp)&#34; --show-labels # it means or =&amp;gt; firstapp or secondapp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Node Selector&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;With Node Selector, we can specify which pod run on which Node.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/153676102-03b2137b-ecc8-4802-9a9f-41694e1ce6fa.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;It is also possible to label nodes with imperative way.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;kubectl apply -f podnode.yaml&#xA;kubectl get pods -w #always watch&#xA;kubectl label nodes minikube hddtype=ssd #after labelling node, pod11 configuration can run, because node is labelled with hddtype:ssd &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Annotation&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;It is similar to label, but it is used for the detailed information (e.g. owner, notification-email, releasedate, etc.) that are not used for linking objects.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/153675516-4b71b55a-f7ec-40a4-9e32-0b794208e6ae.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;kubectl apply -f podannotation.yaml&#xA;kubectl describe pod annotationpod&#xA;kubectl annotate pods annotationpod foo=bar #imperative way&#xA;kubectl delete -f podannotation.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Namespaces&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Namespaces provides a mechanism for isolating groups of resources within a single cluster. They provide a scope for names.&lt;/li&gt; &#xA; &lt;li&gt;Namespaces cannot be nested inside one another and each Kubernetes resource can only be in one namespace.&lt;/li&gt; &#xA; &lt;li&gt;Kubectl commands run in default namespaces if it is not determined in the command.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/148784384-96681287-e4c4-46e8-b63f-5953270a5b28.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;kubectl get pods --namespaces kube-system  #get all pods in the kube-system namespaces&#xA;kubectl get pods --all-namespaces  # get pods from all namespaces&#xA;kubectl create namespace development  #create new development namespace in imperative way&#xA;kubectl get pods -n development  # get pods from all namespace&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;In declerative way, it is possible to create namespaces and run pod on the related namespace.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/153675331-ee6ccfb6-b186-4e29-8e85-55adee465a53.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;kubectl apply -f namespace.yaml&#xA;kubectl get pods -n development  #get pods in the development namespace&#xA;kubectl exec -it namespacedpod -n development -- /bin/sh  #run namespacepod in development namespace&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We can avoid to use -n &#xA;  &lt;namespacename&gt;&#xA;    for all command with changing of default namespace (because, if we don&#39;t use -n namespace, kubectl commands run on the default namespace).&#xA;  &lt;/namespacename&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;kubectl config set-context --current  --namespace=development  #now default namespace is development&#xA;kubectl get pods     #returns pods in the development namespace  &#xA;kubectl config set-context --current  --namespace=default  #now namespace is default &#xA;kubectl delete namespaces development  #delete development namespace&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Deployment &lt;a name=&#34;deployment&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A Deployment provides declarative updates for Pods and ReplicaSets.&lt;/li&gt; &#xA; &lt;li&gt;We define states in the deployment, deployment controller compares desired state and take necessary actions to keep desire state.&lt;/li&gt; &#xA; &lt;li&gt;Deployment object is the higher level K8s object that controls and keeps state of single or multiple pods automatically.&lt;/li&gt; &#xA; &lt;li&gt;Imperative way:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;kubectl create deployment firstdeployment --image=nginx:latest --replicas=2 &#xA;kubectl get deployments&#xA;kubectl get pods -w    #on another terminal&#xA;kubectl delete pods &amp;lt;oneofthepodname&amp;gt; #we can see another terminal, new pod will be created (to keep 2 replicas)  &#xA;kubectl scale deployments firstdeployment --replicas=5&#xA;kubectl delete deployments firstdeployment&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Please have a look Scenario (below link) to learn more about the deployment and declarative way of creating deployment.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto the Scenario:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Deployment.md&#34;&gt;LAB: K8s Deployment - Scale Up/Down - Bash Connection - Port Forwarding&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Replicaset &lt;a name=&#34;replicaset&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Deployment object create Replicaset object. Deployment provides the transition of the different replicaset automatically.&lt;/li&gt; &#xA; &lt;li&gt;Replicaset is responsible for the management of replica creation and remove. But, when the pods are updated (e.g. image changed), it can not update replicaset pods. However, deployment can update for all change. So, best practice is to use deployment, not to use replicaset directly.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Important:&lt;/strong&gt; It can be possible to create replicaset directly, but we could not use rollout/rollback, undo features with replicaset. Deployment provide to use rollout/rollback, undo features.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/148804992-8ad27155-1c1e-436f-949e-4aec9a1a9d05.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Rollout and Rollback &lt;a name=&#34;rollout-rollback&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Rollout and Rollback enable to update and return back containers that run under the deployment.&lt;/li&gt; &#xA; &lt;li&gt;2 strategy for rollout: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Recreate Strategy:&lt;/strong&gt; Delete all pods first and create Pods from scratch. If two different versions of SW affect each other negatively, this strategy could be used.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;RollingUpdate Strategy (default)&lt;/strong&gt;: It updates pods step by step. Pods are updated step by step, all pods are not deleted at the same time. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;strong&gt;maxUnavailable:&lt;/strong&gt; At the update duration, it shows the max number of deleted containers (total:10 containers; if maxUn:2, min:8 containers run in that time period)&lt;/li&gt; &#xA;     &lt;li&gt;&lt;strong&gt;maxSurge:&lt;/strong&gt; At the update duration, it shows that the max number of containers run on the cluster (total:10 containers; if maxSurge:2, max:12 containers run in a time)&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;kubectl set image deployment rolldeployment nginx=httpd:alpine --record     # change image of deployment&#xA;kubectl rollout history deployment rolldeployment                           #shows record/history revisions &#xA;kubectl rollout history deployment rolldeployment --revision=2              #select the details of the one of the revisions&#xA;kubectl rollout undo deployment rolldeployment                              #returns back to previous deployment revision&#xA;kubectl rollout undo deployment rolldeployment --to-revision=1              #returns back to the selected revision=1&#xA;kubectl rollout status deployment rolldeployment -w                         #show live status of the rollout deployment&#xA;kubectl rollout pause deployment rolldeployment                             #pause the rollout while updating pods &#xA;kubectl rollout resume deployment rolldeployment                            #resume the rollout if rollout paused&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto the Scenario:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Rollout-Rollback.md&#34;&gt;LAB: K8s Rollout - Rollback&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Network, Service &lt;a name=&#34;network-service&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h4&gt;K8s Networking Requirements&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Each pod has unique and own IP address (Containers within a pod share network namespaces).&lt;/li&gt; &#xA; &lt;li&gt;All PODs can communicate with all other pods without NAT (Network Address Translation)&lt;/li&gt; &#xA; &lt;li&gt;All NODEs can communicate with all pods without NAT.&lt;/li&gt; &#xA; &lt;li&gt;The IP of the POD is same throughout the cluster.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;CNI (Container Network Interface)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Networking of containers and nodes with different vendors and devices is difficult to handle. So K8s give this responsibility to CNI plugins to handle networking requirements.&lt;/li&gt; &#xA; &lt;li&gt;&#34;CNI (Container Network Interface), a Cloud Native Computing Foundation project, consists of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins.&#34; =&amp;gt; &lt;a href=&#34;https://github.com/containernetworking/cni&#34;&gt;https://github.com/containernetworking/cni&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;K8s has CNI plugins that are selected by the users. Some of the CNI methods are: Flannel, calico, weave, and canal.&lt;/li&gt; &#xA; &lt;li&gt;Calico (&lt;a href=&#34;https://github.com/projectcalico/calico&#34;&gt;https://github.com/projectcalico/calico&lt;/a&gt;) is the one of the popular and open source CNI method/plugin in K8s. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Network Management in the cluster: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;IP assignments to Pods&lt;/li&gt; &#xA;     &lt;li&gt;IP Table Management&lt;/li&gt; &#xA;     &lt;li&gt;Overlay definition between Nodes without using NAT (e.g. --pod-network-cidr management)&lt;/li&gt; &#xA;     &lt;li&gt;Vxlan Interface implementation and etc.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Service&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&#34;An abstract way to expose an application running on a set of Pods as a network service.&lt;/li&gt; &#xA; &lt;li&gt;Kubernetes ServiceTypes allow you to specify what kind of Service you want. The default is ClusterIP.&lt;/li&gt; &#xA; &lt;li&gt;Type values and their behaviors are: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;ClusterIP:&lt;/strong&gt; Exposes the Service on a cluster-internal IP. Choosing this value makes the Service only reachable from within the cluster. This is the default ServiceType.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;NodePort:&lt;/strong&gt; Exposes the Service on each Node&#39;s IP at a static port (the NodePort). A ClusterIP Service, to which the NodePort Service routes, is automatically created. You&#39;ll be able to contact the NodePort Service, from outside the cluster, by requesting &#xA;    &lt;nodeip&gt;&#xA;     :&#xA;     &lt;nodeport&gt;&#xA;      .&#xA;     &lt;/nodeport&gt;&#xA;    &lt;/nodeip&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;LoadBalancer:&lt;/strong&gt; Exposes the Service externally using a cloud provider&#39;s load balancer. NodePort and ClusterIP Services, to which the external load balancer routes, are automatically created.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;ExternalName:&lt;/strong&gt; Maps the Service to the contents of the externalName field (e.g. foo.bar.example.com), by returning a CNAME record with its value. No proxying of any kind is set up.&#34; (Ref: Kubernetes.io)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Example of Service Object Definition: (Selector binds service to the related pods, get traffic from port 80 to port 9376)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1&#xA;kind: Service&#xA;metadata:&#xA;  name: my-service&#xA;spec:&#xA;  selector:&#xA;    app: MyApp&#xA;  ports:&#xA;    - protocol: TCP&#xA;      port: 80&#xA;      targetPort: 9376&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto the Scenario:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Service-App.md&#34;&gt;LAB: K8s Service Implementations (ClusterIp, NodePort and LoadBalancer)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Liveness and Readiness Probe &lt;a name=&#34;liveness-readiness&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h4&gt;Liveness Probe&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&#34;The kubelet uses liveness probes to know when to restart a container. For example, liveness probes could catch a deadlock, where an application is running, but unable to make progress.&#34; (Ref: Kubernetes.io)&lt;/li&gt; &#xA; &lt;li&gt;There are different ways of controlling Pods: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;httpGet,&lt;/li&gt; &#xA;   &lt;li&gt;exec command,&lt;/li&gt; &#xA;   &lt;li&gt;tcpSocket,&lt;/li&gt; &#xA;   &lt;li&gt;grpc, etc.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;initialDelaySeconds: waiting some period of time after starting. e.g. 5sec, after 5 sec start to run command&lt;/li&gt; &#xA; &lt;li&gt;periodSeconds: in a period of time, run command.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto the Scenario:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Liveness-App.md&#34;&gt;LAB: K8s Liveness Probe&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Readiness Probe&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&#34;Sometimes, applications are temporarily unable to serve traffic. For example, an application might need to load large data or configuration files during startup, or depend on external services after startup. In such cases, you don&#39;t want to kill the application, but you don&#39;t want to send it requests either. Kubernetes provides readiness probes to detect and mitigate these situations. A pod with containers reporting that they are not ready does not receive traffic through Kubernetes Services.&#34; (Ref: Kubernetes.io)&lt;/li&gt; &#xA; &lt;li&gt;Readiness probe is similar to liveness pod. Only difference is to define &#34;readinessProbe&#34; instead of &#34;livenessProbe&#34;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Resource Limit, Environment Variable &lt;a name=&#34;environmentvariable&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h4&gt;Resource Limit&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Pods can consume resources (cpu, memory) up to physical resource limits, if there was not any limitation.&lt;/li&gt; &#xA; &lt;li&gt;Pods&#39; used resources can be limited. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;use 1 cpu core =&amp;gt; cpu = &#34;1&#34; = &#34;1000&#34; = &#34;1000m&#34;&lt;/li&gt; &#xA;   &lt;li&gt;use 10% of 1 cpu core =&amp;gt; cpu = &#34;0.1&#34; = &#34;100&#34; = &#34;100m&#34;&lt;/li&gt; &#xA;   &lt;li&gt;use 64 MB =&amp;gt; memory: &#34;64M&#34;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;CPU resources are exactly limited when it defines.&lt;/li&gt; &#xA; &lt;li&gt;When pod requests memory resource more than limitation, pod changes its status to &#34;OOMKilled&#34; and restarts itself to limit memory usage.&lt;/li&gt; &#xA; &lt;li&gt;Example (below), pod requests 64MB memory and 0.25 CPU core, uses maximum 256MB memory and 0.5 CPU core.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/153676383-eb783491-79da-4886-9728-55977b6bbd88.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Environment Variable&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Environment Variables can be defined for each pods in the YAML file.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/153676628-d103de1d-e223-451b-8337-cdfe1cebee66.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto the Scenario:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8-CreatingPod-Declerative.md&#34;&gt;LAB: K8s Creating Pod - Declarative Way - Environment Variable&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Volume &lt;a name=&#34;volume&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ephemeral volume (Temporary volume): Multiple containers reach ephemeral volume in the pod. When the pod is deleted/killed, volume is also deleted. But when container is restarted, volume is still available because pod still runs.&lt;/li&gt; &#xA; &lt;li&gt;There are 2 types of ephemeral volumes: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Emptydir&lt;/li&gt; &#xA;   &lt;li&gt;Hostpath &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Directory&lt;/li&gt; &#xA;     &lt;li&gt;DirectoryOrCreate&lt;/li&gt; &#xA;     &lt;li&gt;FileOrCreate&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Emptydir Volume&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Emptydir (empty directory on the node) is created on which node the pod is created on and it is mounted on the container using &#34;volumeMounts&#34;. Multiple containers in the pod can reach this volume (read/write).&lt;/li&gt; &#xA; &lt;li&gt;Emptydir volume is dependent of Pod Lifecycle. If the pod is deleted, emptydir is also deleted.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;spec: &#xA;  containers:&#xA;  - name: sidecar&#xA;    image: busybox&#xA;    command: [&#34;/bin/sh&#34;]&#xA;    args: [&#34;-c&#34;, &#34;sleep 3600&#34;]&#xA;    volumeMounts:                # volume is mounted under &#34;volumeMounts&#34; &#xA;    - name: cache-vol            # &#34;name&#34; of the volume type&#xA;      mountPath: /tmp/log        # &#34;mountPath&#34; is the path in the container.&#xA;  volumes:&#xA;  - name: cache-vol              &#xA;    emptyDir: {}                 # &#34;volume&#34; type &#34;emptydir&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto the Scenario:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Multicontainer-Sidecar.md&#34;&gt;LAB: K8s Multicontainer - Sidecar - Emptydir Volume - Port-Forwarding&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Hostpath Volume&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;It is similar to emtpydir, hostpath is also created on which node the pod is created on. In addition, the hostpath is specifically defined path on the node.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1&#xA;kind: Pod&#xA;metadata:&#xA;  name: hostpath&#xA;spec:&#xA;  containers:&#xA;  - name: hostpathcontainer&#xA;    image: ImageName                  # e.g. nginx&#xA;    volumeMounts:&#xA;    - name: directory-vol             # container connects &#34;volume&#34; name    &#xA;      mountPath: /dir1                # on the container which path this volume is mounted&#xA;    - name: dircreate-vol&#xA;      mountPath: /cache               # on the container which path this volume is mounted&#xA;    - name: file-vol&#xA;      mountPath: /cache/config.json   # on the container which file this volume is mounted     &#xA;  volumes:&#xA;  - name: directory-vol               # &#34;volume&#34; name&#xA;    hostPath:                         # &#34;volume&#34; type &#34;hostpath&#34;&#xA;      path: /tmp                      # &#34;path&#34; on the node, &#34;/tmp&#34; is defined volume&#xA;      type: Directory                 # &#34;hostpath&#34; type &#34;Directory&#34;, existed directory&#xA;  - name: dircreate-vol&#xA;    hostPath:                         # &#34;volume&#34; type &#34;hostpath&#34;&#xA;      path: /cache                    # &#34;path&#34; on the node&#xA;      type: DirectoryOrCreate         # &#34;hostpath&#34; type &#34;DirectoryOrCreate&#34;, if it is not existed, create directory&#xA;  - name: file-vol&#xA;    hostPath:                         # &#34;volume&#34; type &#34;hostpath&#34;&#xA;      path: /cache/config.json        # &#34;path&#34; on the node&#xA;      type: FileOrCreate              # &#34;hostpath&#34; type &#34;FileOrCreate&#34;,  if it is not existed, create file&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/154715083-f5972de0-d95e-47f2-bc6d-92cf7b8a182a.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Secret &lt;a name=&#34;secret&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Secret objects store the sensitive and secure information like username, password, ssh-tokens, certificates.&lt;/li&gt; &#xA; &lt;li&gt;Secrets (that you defined) and pods (that you defined) should be in the same namespace (e.g. if defined secret is in the &#34;default&#34; namespace, pod should be also in the &#34;default&#34; namepace).&lt;/li&gt; &#xA; &lt;li&gt;There are 8 different secret types (basic-auth, tls, ssh-auth, token, service-account-token, dockercfg, dockerconfigjson, opaque). Opaque type is the default one and mostly used.&lt;/li&gt; &#xA; &lt;li&gt;Secrets are called by the pod in 2 different ways: volume and environment variable&lt;/li&gt; &#xA; &lt;li&gt;Imperative way, run on the terminal (geneneric in the command = opaque):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;kubectl create secret generic mysecret2 --from-literal=db_server=db.example.com --from-literal=db_username=admin --from-literal=db_password=P@ssw0rd!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Imperative way with file to hide pass in the command history&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;kubectl create secret generic mysecret3 --from-file=db_server=server.txt --from-file=db_username=username.txt --from-file=db_password=password.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Imperative way with json file to hide pass in the command history&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;kubectl create secret generic mysecret4 --from-file=config.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto the Scenario:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Secret.md&#34;&gt;LAB: K8s Secret (Declarative and Imperative Way)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;ConfigMap &lt;a name=&#34;configmap&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;It is same as &#34;secrets&#34;. The difference is that configmap does not save sensitive information. It stores config variables.&lt;/li&gt; &#xA; &lt;li&gt;Configmap stores data with key-value pairs.&lt;/li&gt; &#xA; &lt;li&gt;Configmaps are called by the pod in 2 different ways: volume and environment variable&lt;/li&gt; &#xA; &lt;li&gt;Scenario shows the usage of configmaps.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto the Scenario:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Configmap.md&#34;&gt;LAB: K8s Config Map&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Node ‚Äì Pod Affinity &lt;a name=&#34;node-pod-affinity&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Affinity means closeness, proximity, familarity.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Node Affinity&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;With node affinity, specific pods can enable to run on the desired node (Node selector also supports that feature, but node affinity is more flexible).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If node is labelled with key-value, we can run some of the pods on that specific node.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Terms for Node Affinity:&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;requiredDuringSchedulingIgnoredDuringExecution:&lt;/strong&gt; Find a node during scheduling according to &#34;matchExpression&#34; and run pod on that node. If it is not found, do not run this pod until finding specific node &#34;matchExpression&#34;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;IgnoredDuringExecution:&lt;/strong&gt; After scheduling, if the node label is removed/deleted from node, ignore it while executing.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;preferredDuringSchedulingIgnoredDuringExecution:&lt;/strong&gt; Find a node during scheduling according to &#34;matchExpression&#34; and run pod on that node. If it is not found, run this pod wherever it finds. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;strong&gt;weight:&lt;/strong&gt; Preference weight. If weight is more than other weights, this weight is higher priority than others.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;For a better understanding, please have a look &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Node-Affinity.md&#34;&gt;LAB: K8s Node Affinity&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Go to the Scenario:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Node-Affinity.md&#34;&gt;LAB: K8s Node Affinity&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Pod Affinity&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Some of the pods should run with other pods on same node or same availability zone (e.g. frontend pods run with cache pod on the same availability zone)&lt;/li&gt; &#xA; &lt;li&gt;If pod affinity is defined for one pod, that pod runs with the related pod on same node or same availability zone.&lt;/li&gt; &#xA; &lt;li&gt;Each node in the cluster is labelled with default labels. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&#34;kubernetes.io/hostname&#34;: e.g &#34;kubernetes.io/hostname=minikube&#34;&lt;/li&gt; &#xA;   &lt;li&gt;&#34;kubernetes.io/arch&#34;: e.g &#34;kubernetes.io/arch=amd64&#34;&lt;/li&gt; &#xA;   &lt;li&gt;&#34;kubernetes.io/os&#34;: e.g &#34;kubernetes.io/os=linux&#34;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Each node in the cluster that runs on the Cloud is labelled with following labels. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&#34;topology.kubernetes.io/region&#34;: e.g. &#34;topology.kubernetes.io/region=northeurope&#34;&lt;/li&gt; &#xA;   &lt;li&gt;&#34;topology.kubernetes.io/zone&#34;: e.g. &#34;topology.kubernetes.io/zone=northeurope-1&#34;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1&#xA;kind: Pod&#xA;metadata:&#xA;  name: frontendpod&#xA;  labels:&#xA;    app: frontend                                     # defined labels&#xA;    deployment: test                      &#xA;spec:&#xA;  containers:&#xA;  - name: nginx&#xA;    image: nginx:latest&#xA;    ports:&#xA;    - containerPort: 80&#xA;---&#xA;apiVersion: v1&#xA;kind: Pod&#xA;metadata:&#xA;  name: cachepod&#xA;spec:&#xA;  affinity:&#xA;    podAffinity:&#xA;      requiredDuringSchedulingIgnoredDuringExecution:    # required: if not found, not run this pod on any node&#xA;      - labelSelector:&#xA;          matchExpressions:&#xA;          - key: app&#xA;            operator: In&#xA;            values:&#xA;            - frontend&#xA;        topologyKey: kubernetes.io/hostname               # run this pod with the POD which includes &#34;app=frontend&#34; on the same worker NODE  &#xA;      preferredDuringSchedulingIgnoredDuringExecution:    # preferred: if not found, run this pod on any node&#xA;      - weight: 1&#xA;        podAffinityTerm:&#xA;          labelSelector:&#xA;            matchExpressions:&#xA;            - key: branch&#xA;              operator: In&#xA;              values:&#xA;              - develop&#xA;          topologyKey: topology.kubernetes.io/zone         # run this pod with the POD which includes &#34;branch=develop&#34; on the any NODE in the same ZONE &#xA;    podAntiAffinity:                                       # anti-affinity: NOT run this pod with the following match &#34;&#34;&#xA;      preferredDuringSchedulingIgnoredDuringExecution:&#xA;      - weight: 100&#xA;        podAffinityTerm:&#xA;          labelSelector:&#xA;            matchExpressions:&#xA;            - key: deployment&#xA;              operator: In&#xA;              values:&#xA;              - prod&#xA;          topologyKey: topology.kubernetes.io/zone         # NOT run this pod with the POD which includes &#34;deployment=prod&#34; on the any NODE in the same ZONE   &#xA;  containers:&#xA;  - name: cachecontainer                                   # cache image and container name&#xA;    image: redis:6-alpine&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/154729871-1294d423-1429-4a00-9d2b-78cfcdace18a.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/154730052-19e96985-1452-4d93-9fc3-d70ea06ceb8a.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Taint and Toleration &lt;a name=&#34;taint-tolereation&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Node affinity is a property of Pods that attracts/accepts them to a set of nodes. Taints are the opposite, they allow a node to repel/reject a set of pods.&lt;/li&gt; &#xA; &lt;li&gt;TAINTs are assigned to the NODEs. TOLERATIONs assigned to the PODs &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&#34;kubectl describe nodes minikube&#34;, at taints section, it can be seen taints.&lt;/li&gt; &#xA;   &lt;li&gt;To add taint to the node with commmand: &#34;kubectl taint node minikube app=production:NoSchedule&#34;&lt;/li&gt; &#xA;   &lt;li&gt;To delete taint to the node with commmand: &#34;kubectl taint node minikube app-&#34;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;If pod has not any toleration for related taint, it can not be started on the tainted node (status of pod remains pending)&lt;/li&gt; &#xA; &lt;li&gt;Taint Types: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;key1=value1:effect&lt;/strong&gt;: (e.g.&#34;kubectl taint node minikube app=production:NoSchedule&#34;)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Taint &#34;effect&#34; types: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;NoSchedule:&lt;/strong&gt; If pod is not tolerated with this effect, it can not run on the related node (status will be pending, until toleration/untaint)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;PreferNoSchedule:&lt;/strong&gt; If pod is not tolerated with this effect and if there is not any untainted node, it can run on the related node.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;NoExecute:&lt;/strong&gt; If pod is not tolerated with this effect, it can not run on the related node. If there are pods running on the node before assigning &#34;NoExecute&#34; taint, after tainting &#34;NoExecute&#34;, untolerated pods stopped on this node.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;For clarification, please have a look &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Taint-Toleration.md&#34;&gt;LAB: K8s Taint Toleration&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto the Scenario:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Taint-Toleration.md&#34;&gt;LAB: K8s Taint-Toleration&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Deamon Set &lt;a name=&#34;daemon-set&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;It provides to run pods on EACH nodes. It can be configured to run only specific nodes.&lt;/li&gt; &#xA; &lt;li&gt;For example, you can run log application that runs on each node in the cluster and app sends these logs to the main log server. Manual configuration of each nodes could be headache in this sceneario, so using deamon sets would be beneficial to save time and effort.&lt;/li&gt; &#xA; &lt;li&gt;If the new nodes are added on the cluster and running deamon sets on the cluster at that time period, default pods which are defined on deamon sets also run on the new nodes without any action.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto the scenario:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Daemon-Sets.md&#34;&gt;LAB: K8s Daemonset - Creating 3 nodes on Minikube&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Persistent Volume and Persistent Volume Claim &lt;a name=&#34;pvc&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Volumes are ephemeral/temporary area that stores data. Emptydir and hostpath create volume on node which runs related pod.&lt;/li&gt; &#xA; &lt;li&gt;In the scenario of creating Mysql pod on cluster, we can not use emptydir and hostpath for long term. Because they don&#39;t provide the long term/persistent volume.&lt;/li&gt; &#xA; &lt;li&gt;Persistent volume provides long term storage area that runs out of the cluster.&lt;/li&gt; &#xA; &lt;li&gt;There are many storage solutions that can be enabled on the cluster: nfs, iscsi, azure disk, aws ebs, google pd, cephfs.&lt;/li&gt; &#xA; &lt;li&gt;Container Storage Interface (CSI) provides the connection of K8s cluster and different storage solution.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Persistent Volume&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&#34;accessModes&#34; types: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&#34;ReadWriteOnce&#34;: read/write for only 1 node.&lt;/li&gt; &#xA;   &lt;li&gt;&#34;ReadOnlyMany&#34; : only read for many nodes.&lt;/li&gt; &#xA;   &lt;li&gt;&#34;ReadWriteMany&#34;: read/write for many nodes.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&#34;persistentVolumeReclaimPolicy&#34; types: it defines the behaviour of volume after the end of using volume. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&#34;Retain&#34; : volume remains with all data after using it.&lt;/li&gt; &#xA;   &lt;li&gt;&#34;Recycle&#34;: volume is not deleted but all data in the volume is deleted. We get empty volume if it is chosen.&lt;/li&gt; &#xA;   &lt;li&gt;&#34;Delete&#34; : volume is deleted after using it.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;# Creating Persistent Volume on NFS Server on the network    &#xA;apiVersion: v1                               &#xA;kind: PersistentVolume&#xA;metadata:&#xA;   name: mysqlpv&#xA;   labels:&#xA;     app: mysql                                # labelled PV with &#34;mysql&#34;&#xA;spec:&#xA;  capacity:&#xA;    storage: 5Gi                               # 5Gibibyte = power of 2; 5GB= power of 10&#xA;  accessModes:&#xA;    - ReadWriteOnce&#xA;  persistentVolumeReclaimPolicy: Recycle       # volume is not deleted, all data in the volume will be deleted.&#xA;  nfs:&#xA;    path: /tmp                                 # binds the path on the NFS Server&#xA;    server: 10.255.255.10                      # IP of NFS Server&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/154734368-323af0cc-e745-4aa0-b844-65b4a410426d.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Persistent Volume Claim (PVC)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We should create PVCs to use volume. With PVCs, existed PVs can be chosen.&lt;/li&gt; &#xA; &lt;li&gt;The reason why K8s manage volume with 2 files (PVC and PV) is to seperate the management of K8s Cluster (PV) and using of volume (PVC).&lt;/li&gt; &#xA; &lt;li&gt;If there is seperate role of system management of K8s cluster, system manager creates PV (to connect different storage vendors), developers only use existed PVs with PVCs.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1&#xA;kind: PersistentVolumeClaim&#xA;metadata:&#xA;  name: mysqlclaim&#xA;spec:&#xA;  accessModes:&#xA;    - ReadWriteOnce&#xA;  volumeMode: Filesystem                    # VolumeMode&#xA;  resources:&#xA;    requests:&#xA;      storage: 5Gi&#xA;  storageClassName: &#34;&#34;&#xA;  selector:&#xA;    matchLabels:                          &#xA;      app: mysql                            # choose/select &#34;mysql&#34; PV that is defined above.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/154735404-80221355-1493-4043-ba7a-8c7a4ddc8df0.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto the scenario:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-PersistantVolume.md&#34;&gt;LAB: K8s Persistant Volume and Persistant Volume Claim&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Storage Class &lt;a name=&#34;storageclass&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Creating volume with PV is manual way of creating volume. With storage classes, it can be automated.&lt;/li&gt; &#xA; &lt;li&gt;Cloud providers provide storage classes on their infrastructure.&lt;/li&gt; &#xA; &lt;li&gt;When pod/deployment is created, storage class is triggered to create PV automatically (Trigger order: Pod -&amp;gt; PVC -&amp;gt; Storage Class -&amp;gt; PV).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;# Storage Class Creation on Azure&#xA;apiVersion: storage.k8s.io/v1&#xA;kind: StorageClass&#xA;metadata:&#xA;  name: standarddisk&#xA;parameters:&#xA;  cachingmode: ReadOnly&#xA;  kind: Managed&#xA;  storageaccounttype: StandardSSD_LRS&#xA;provisioner: kubernetes.io/azure-disk&#xA;reclaimPolicy: Delete&#xA;volumeBindingMode: WaitForFirstConsumer    &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&#34;storageClassName&#34; is added into PVC file.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1&#xA;kind: PersistentVolumeClaim&#xA;metadata:&#xA;  name: mysqlclaim&#xA;spec:&#xA;  accessModes:&#xA;    - ReadWriteOnce&#xA;  volumeMode: Filesystem&#xA;  resources:&#xA;    requests:&#xA;      storage: 5Gi&#xA;  storageClassName: &#34;standarddisk&#34;               # selects/binds to storage class (defined above)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;When deployment/pod request PVC (claim), storage class provides volume on the infrastructure automatically.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Stateful Set &lt;a name=&#34;statefulset&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Pods/Deployments are stateless objects. Stateful set provides to run stateful apps.&lt;/li&gt; &#xA; &lt;li&gt;Differences between Deployment and Statefulset: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Name of the pods in the statefulset are not assigned randomly. It gives name statefulsetName_0,1,2,3.&lt;/li&gt; &#xA;   &lt;li&gt;Pods in the statefulset are not created at the same time. Pods are created in order (new pod creation waits until previous pod&#39;s running status).&lt;/li&gt; &#xA;   &lt;li&gt;When scaling down of statefulset, pods are deleted in random. Pods are deleted in order.&lt;/li&gt; &#xA;   &lt;li&gt;If PVC is defined in the statefulset, each pod in the statefulset has own PV&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto the scenario:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Statefulset.md&#34;&gt;LAB: K8s Stateful Sets - Nginx&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Job, CronJob &lt;a name=&#34;job&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h4&gt;Job Object&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&#34;A Job creates one or more Pods and will continue to retry execution of the Pods until a specified number of them successfully terminate&#34;. If the container is not successfully completed, it will recreated again.&lt;/li&gt; &#xA; &lt;li&gt;&#34;When a specified number of successful completions is reached, the task (ie, Job) is complete.&#34;&lt;/li&gt; &#xA; &lt;li&gt;After finishing a job, pods are not deleted. Logs in the pods can be viewed.&lt;/li&gt; &#xA; &lt;li&gt;Job is used for the task that runs once (e.g. maintanence scripts, scripts that are used for creating DB)&lt;/li&gt; &#xA; &lt;li&gt;Job is also used for processing tasks that are stored in queue or bucket.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;spec:&#xA;  parallelism: 2               # each step how many pods start in parallel at a time&#xA;  completions: 10              # number of pods that run and complete job at the end of the time&#xA;  backoffLimit: 5              # to tolerate fail number of job, after 5 times of failure, not try to continue job, fail the job&#xA;  activeDeadlineSeconds: 100   # if this job is not completed in 100 seconds, fail the job&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/154946885-80e87f3c-5120-4c09-bde2-a35cd09a7383.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto the scenario:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Job.md&#34;&gt;LAB: K8s Job&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Cron Job Object&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Cron job is a scheduled job that can be started in scheduled time.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;# ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ minute (0 - 59)&#xA;# ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ hour (0 - 23)&#xA;# ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ day of the month (1 - 31)&#xA;# ‚îÇ ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ month (1 - 12)&#xA;# ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ day of the week (0 - 6) (Sunday to Saturday;&#xA;# ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ                                   7 is also Sunday on some systems)&#xA;# ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ&#xA;# ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ&#xA;# * * * * *&#xA;#&#xA;# https://crontab.guru/ &#xA;# Examples: &#xA;#   5 * * * *   : (means) For every day start at minute 5: 00:05 - Second day 00:05 ....&#xA;#   */5 * * * * : (means) At every 5th minute: 00:05 - 00:10 - 00:15 ... &#xA;#   0 */2 * * * : (means) At minute 0 pass every 2d hour: 00:00 - 02:00 - 04:00 ... &#xA;#  &#34;*&#34; means &#34;every&#34;&#xA;#  &#34;/&#34; means &#34;repetitive&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;spec:&#xA;  schedule: &#34;*/1 * * * *&#34;                        # At every 1st minute: 00:01 - 00:02 ...&#xA;  jobTemplate:&#xA;    spec:&#xA;      template:&#xA;        spec:&#xA;          containers:&#xA;          - name: hello&#xA;            image: busybox&#xA;            imagePullPolicy: IfNotPresent&#xA;            command:                             # start shell and echo  &#xA;            - /bin/sh&#xA;            - -c&#xA;            - date; echo Hello from the Kubernetes cluster &#xA;          restartPolicy: OnFailure&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/154948618-8b71bf38-62a7-44de-bdd2-ac40a1709eb4.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Go to the scenario:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-CronJob.md&#34;&gt;LAB: K8s Cron Job&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Authentication, Role Based Access Control, Service Account &lt;a name=&#34;authentication&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h4&gt;Authentication&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;It is related to authenticate user to use specific cluster.&lt;/li&gt; &#xA; &lt;li&gt;Theory of the creating authentication is explained in short: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;user creates .key (key file) and .csr (certificate signing request file includes username and roles) with openssl application&lt;/li&gt; &#xA;   &lt;li&gt;user sends .csr file to the K8s admin&lt;/li&gt; &#xA;   &lt;li&gt;K8s admin creates a K8s object with this .csr file and creates .crt file (certification file) to give user&lt;/li&gt; &#xA;   &lt;li&gt;user gets this .crt file (certification file) and creates credential (set-credentials) in user&#39;s pc with certification.&lt;/li&gt; &#xA;   &lt;li&gt;user creates context (set-context) with cluster and credential, and uses this context.&lt;/li&gt; &#xA;   &lt;li&gt;now it requires to get/create authorization for the user.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Role Based Access Control (RBAC, Authorization)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;It provides to give authorization (role) to the specific user.&lt;/li&gt; &#xA; &lt;li&gt;&#34;Role&#34;, &#34;RoleBinding&#34; K8s objects are used to bind users for specific &#34;namespace&#34;.&lt;/li&gt; &#xA; &lt;li&gt;&#34;ClusterRole&#34;, &#34;ClusterRoleBinding&#34; K8s objects are used to bind users for specific &#34;namespace&#34;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: rbac.authorization.k8s.io/v1&#xA;kind: Role&#xA;metadata:&#xA;  namespace: default&#xA;  name: pod-reader&#xA;rules:&#xA;- apiGroups: [&#34;&#34;]                            # &#34;&#34; indicates the core API group&#xA;  resources: [&#34;pods&#34;]                        # &#34;services&#34;, &#34;endpoints&#34;, &#34;pods&#34;, &#34;pods/log&#34; etc.&#xA;  verbs: [&#34;get&#34;, &#34;watch&#34;, &#34;list&#34;]            # &#34;get&#34;, &#34;list&#34;, &#34;watch&#34;, &#34;create&#34;, &#34;update&#34;, &#34;patch&#34;, &#34;delete&#34;  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/154953311-84f616cf-3a25-486f-beb9-e2d6a3a2e01a.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: rbac.authorization.k8s.io/v1&#xA;kind: RoleBinding&#xA;metadata:&#xA;  name: read-pods&#xA;  namespace: default&#xA;subjects:&#xA;- kind: User&#xA;  name: username@hostname.net                 # &#34;name&#34; is case sensitive, this name was defined while creating .csr file&#xA;  apiGroup: rbac.authorization.k8s.io&#xA;roleRef:&#xA;  kind: Role #this must be Role or ClusterRole&#xA;  name: pod-reader                            # this must match the name of the Role or ClusterRole you wish to bind to&#xA;  apiGroup: rbac.authorization.k8s.io    &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/154953439-1dd52309-611b-48bf-8f7b-51433b678f8c.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: rbac.authorization.k8s.io/v1&#xA;kind: ClusterRole&#xA;metadata:&#xA;  name: secret-reader&#xA;rules:&#xA;- apiGroups: [&#34;&#34;]&#xA;  resources: [&#34;secrets&#34;]&#xA;  verbs: [&#34;get&#34;, &#34;watch&#34;, &#34;list&#34;]    &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/154953542-3723d691-632e-41d6-908f-5b15080ffa7b.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: rbac.authorization.k8s.io/v1&#xA;kind: ClusterRoleBinding&#xA;metadata:&#xA;  name: read-secrets-global&#xA;subjects:&#xA;- kind: Group&#xA;  name: DevTeam                              # Name is case sensitive&#xA;  apiGroup: rbac.authorization.k8s.io&#xA;roleRef:&#xA;  kind: ClusterRole&#xA;  name: secret-reader&#xA;  apiGroup: rbac.authorization.k8s.io &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/154953630-dcd71073-6de6-4194-955e-9b50a0f9c978.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Service Account&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;RBACs are used for real people.&lt;/li&gt; &#xA; &lt;li&gt;Service accounts are used for pods/apps that can connect K8s API to create K8s objects.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Ingress &lt;a name=&#34;ingress&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&#34;An API object that manages external access to the services in a cluster, typically HTTP.&#34; (ref: Kubernetes.io)&lt;/li&gt; &#xA; &lt;li&gt;&#34;Ingress may provide load balancing, SSL termination and name-based virtual hosting&#34; (ref: Kubernetes.io)&lt;/li&gt; &#xA; &lt;li&gt;Ingress is not a Service type, but it acts as the entry point for your cluster.&lt;/li&gt; &#xA; &lt;li&gt;Ingress resource only supports rules for directing HTTP(S) (L7) traffic.&lt;/li&gt; &#xA; &lt;li&gt;&#34;Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. Traffic routing is controlled by rules defined on the Ingress resource.&#34; (ref: Kubernetes.io)&lt;/li&gt; &#xA; &lt;li&gt;Ingress controller is a L7 Application Loadbalancer that works in K8s according to K8s specification. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Ingress Controllers: Nginx, HAproxy, Traefik&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/152972977-5cfb148f-4ac7-4fb6-b68b-9a576e199e68.png&#34; alt=&#34;image&#34;&gt; (ref: Kubernetes.io)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;# Simple Ingress Object Definition    &#xA;apiVersion: networking.k8s.io/v1&#xA;kind: Ingress&#xA;metadata:&#xA;  name: minimal-ingress&#xA;  annotations:&#xA;    nginx.ingress.kubernetes.io/rewrite-target: /&#xA;spec:&#xA;  ingressClassName: nginx-example&#xA;  rules:&#xA;  - http:&#xA;      paths:&#xA;      - path: /testpath&#xA;        pathType: Prefix&#xA;        backend:&#xA;          service:&#xA;            name: test&#xA;            port:&#xA;              number: 80&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto the scenario:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Ingress.md&#34;&gt;LAB: K8s Ingress&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Dashboard &lt;a name=&#34;dashboard&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can view followings using default K8s dashboard: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;All Workloads on Cluster: Memory and CPU usages, update time, image name, node name, status&lt;/li&gt; &#xA;   &lt;li&gt;Cron Jobs and Jobs&lt;/li&gt; &#xA;   &lt;li&gt;Daeamon Sets&lt;/li&gt; &#xA;   &lt;li&gt;Deployments, Replicasets&lt;/li&gt; &#xA;   &lt;li&gt;Pods, Stateful Sets&lt;/li&gt; &#xA;   &lt;li&gt;Services, Endpoints, IPs, Ports,&lt;/li&gt; &#xA;   &lt;li&gt;Persistent Volume Claims, Persisten Volumes&lt;/li&gt; &#xA;   &lt;li&gt;Config Maps,&lt;/li&gt; &#xA;   &lt;li&gt;Secrets, Storage Classes&lt;/li&gt; &#xA;   &lt;li&gt;Cluster Roles and Role Binding&lt;/li&gt; &#xA;   &lt;li&gt;Namespaces&lt;/li&gt; &#xA;   &lt;li&gt;Network Policies&lt;/li&gt; &#xA;   &lt;li&gt;Nodes&lt;/li&gt; &#xA;   &lt;li&gt;Roles and Role Bindings&lt;/li&gt; &#xA;   &lt;li&gt;Service Accounts&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;# if working on minikube&#xA;minikube addons enable dashboard&#xA;minikube addons enable metrics-server&#xA;minikube dashboard&#xA;# if running on WSL/WSL2 to open browser&#xA;sensible-browser http://127.0.0.1:45771/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;to see better resolution, click on it&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/152148024-6ec65b33-9fd0-42eb-89c3-927e453553a2.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/152147845-017c6c10-a687-4ee3-b868-a08d96f6d884.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto the scenario:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Enable-Dashboard-On-Cluster.md&#34;&gt;LAB: Enable Dashboard on Real Cluster&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Play With Kubernetes &lt;a name=&#34;playwithkubernetes&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://labs.play-with-k8s.com/&#34;&gt;https://labs.play-with-k8s.com/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Helm &lt;a name=&#34;helm&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Helm is the package manager of K8s (&lt;a href=&#34;https://helm.sh/&#34;&gt;https://helm.sh/&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&#34;Helm installs charts into Kubernetes, creating a new release for each installation. And to find new charts, you can search Helm chart repositories.&#34; (Ref: Helm.sh)&lt;/li&gt; &#xA; &lt;li&gt;With Helm, it is easy to install best-practice K8s designs and products. Search K8s packages =&amp;gt; &lt;a href=&#34;https://artifacthub.io/&#34;&gt;https://artifacthub.io/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Detailed Tutorial =&amp;gt; &lt;a href=&#34;https://helm.sh/docs/intro/quickstart/&#34;&gt;https://helm.sh/docs/intro/quickstart/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Important Terms:&lt;/strong&gt; (Ref: Helm.sh) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Chart:&lt;/strong&gt; &#34;A Chart is a Helm package. It contains all of the resource definitions necessary to run an application, tool, or service inside of a Kubernetes cluster. Think of it like the Kubernetes equivalent of a Homebrew formula, an Apt dpkg, or a Yum RPM file.&#34;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Repository:&lt;/strong&gt; &#34;A Repository is the place where charts can be collected and shared&#34;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Release:&lt;/strong&gt; &#34;A Release is an instance of a chart running in a Kubernetes cluster. One chart can often be installed many times into the same cluster. And each time it is installed, a new release is created. Consider a MySQL chart. If you want two databases running in your cluster, you can install that chart twice. Each one will have its own release, which will in turn have its own release name.&#34;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto the scenario:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/Helm.md&#34;&gt;LAB: HELM Install &amp;amp; Usage&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto the scenario:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Helm-Jenkins.md&#34;&gt;LAB: Helm-Jenkins on running K8s Cluster (2 Node Multipass VM)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/HelmCheatsheet.md&#34;&gt;Helm Commands Cheatsheet&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Kubernetes Commands Cheatsheet &lt;a name=&#34;cheatsheet&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/KubernetesCommandCheatSheet.md&#34;&gt;Kubernetes Commands Cheatsheet&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Helm Commands Cheatsheet &lt;a name=&#34;helm_cheatsheet&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/HelmCheatsheet.md&#34;&gt;Helm Commands Cheatsheet&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Kubernetes Cluster Setup: Kubeadm, Containerd, Multipass &lt;a name=&#34;cluster_setup&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Kubeadm-Cluster-Setup.md&#34;&gt;LAB: K8s Kubeadm Cluster Setup&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Monitoring Kubernetes Cluster with SSH, Prometheus and Grafana &lt;a name=&#34;prometheus_grafana&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Monitoring-Prometheus-Grafana.md&#34;&gt;LAB: K8s Monitoring - Prometheus and Grafana&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Enable-Dashboard-On-Cluster.md&#34;&gt;LAB: Enable Dashboard on Real K8s Cluster&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Other Useful Resources Related Docker &lt;a name=&#34;resource&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/tutorials/&#34;&gt;KubernetesTutorial&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Docker and Kubernetes Tutorial - Youtube: &lt;a href=&#34;https://www.youtube.com/watch?v=bhBSlnQcq2k&amp;amp;t=3088s&#34;&gt;https://www.youtube.com/watch?v=bhBSlnQcq2k&amp;amp;t=3088s&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;References &lt;a name=&#34;references&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/overview/&#34;&gt;Kubernetes.io&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/tutorials/&#34;&gt;KubernetesTutorial&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.udemy.com/course/kubernetes-temelleri/&#34;&gt;udemy-course:Kubernetes-Temelleri&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://helm.sh/&#34;&gt;Helm.sh&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>