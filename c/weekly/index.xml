<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-30T01:51:44Z</updated>
  <subtitle>Weekly Trending of C in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>FreeRDP/FreeRDP</title>
    <updated>2023-07-30T01:51:44Z</updated>
    <id>tag:github.com,2023-07-30:/FreeRDP/FreeRDP</id>
    <link href="https://github.com/FreeRDP/FreeRDP" rel="alternate"></link>
    <summary type="html">&lt;p&gt;FreeRDP is a free remote desktop protocol library and clients&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;FreeRDP: A Remote Desktop Protocol Implementation&lt;/h1&gt; &#xA;&lt;p&gt;FreeRDP is a free implementation of the Remote Desktop Protocol (RDP), released under the Apache license. Enjoy the freedom of using your software wherever you want, the way you want it, in a world where interoperability can finally liberate your computing experience.&lt;/p&gt; &#xA;&lt;h2&gt;Resources&lt;/h2&gt; &#xA;&lt;p&gt;Project website: &lt;a href=&#34;https://www.freerdp.com/&#34;&gt;https://www.freerdp.com/&lt;/a&gt;&lt;br&gt; Issue tracker: &lt;a href=&#34;https://github.com/FreeRDP/FreeRDP/issues&#34;&gt;https://github.com/FreeRDP/FreeRDP/issues&lt;/a&gt;&lt;br&gt; Sources: &lt;a href=&#34;https://github.com/FreeRDP/FreeRDP/&#34;&gt;https://github.com/FreeRDP/FreeRDP/&lt;/a&gt;&lt;br&gt; Downloads: &lt;a href=&#34;https://pub.freerdp.com/releases/&#34;&gt;https://pub.freerdp.com/releases/&lt;/a&gt;&lt;br&gt; Wiki: &lt;a href=&#34;https://github.com/FreeRDP/FreeRDP/wiki&#34;&gt;https://github.com/FreeRDP/FreeRDP/wiki&lt;/a&gt;&lt;br&gt; API documentation: &lt;a href=&#34;https://pub.freerdp.com/api/&#34;&gt;https://pub.freerdp.com/api/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Security policy: &lt;a href=&#34;https://github.com/FreeRDP/FreeRDP/security/policy&#34;&gt;https://github.com/FreeRDP/FreeRDP/security/policy&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Matrix room : #FreeRDP:matrix.org (main) XMPP channel: #FreeRDP#&lt;a href=&#34;mailto:matrix.org@matrix.org&#34;&gt;matrix.org@matrix.org&lt;/a&gt; (bridged) IRC channel : #freerdp @ irc.oftc.net (bridged) Mailing list: &lt;a href=&#34;https://lists.sourceforge.net/lists/listinfo/freerdp-devel&#34;&gt;https://lists.sourceforge.net/lists/listinfo/freerdp-devel&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Microsoft Open Specifications&lt;/h2&gt; &#xA;&lt;p&gt;Information regarding the Microsoft Open Specifications can be found at: &lt;a href=&#34;https://www.microsoft.com/openspecifications/&#34;&gt;https://www.microsoft.com/openspecifications/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A list of reference documentation is maintained here: &lt;a href=&#34;https://github.com/FreeRDP/FreeRDP/wiki/Reference-Documentation&#34;&gt;https://github.com/FreeRDP/FreeRDP/wiki/Reference-Documentation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Compilation&lt;/h2&gt; &#xA;&lt;p&gt;Instructions on how to get started compiling FreeRDP can be found on the wiki: &lt;a href=&#34;https://github.com/FreeRDP/FreeRDP/wiki/Compilation&#34;&gt;https://github.com/FreeRDP/FreeRDP/wiki/Compilation&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>termux/termux-x11</title>
    <updated>2023-07-30T01:51:44Z</updated>
    <id>tag:github.com,2023-07-30:/termux/termux-x11</id>
    <link href="https://github.com/termux/termux-x11" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Termux X11 add-on application. Still in early development.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Termux:X11&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/termux/termux-x11/actions/workflows/debug_build.yml&#34;&gt;&lt;img src=&#34;https://github.com/termux/termux-x11/actions/workflows/debug_build.yml/badge.svg?branch=master&#34; alt=&#34;Nightly build&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitter.im/termux/termux&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/termux/termux.svg?sanitize=true&#34; alt=&#34;Join the chat at https://gitter.im/termux/termux&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/HXpF69X&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/641256914684084234?label=&amp;amp;logo=discord&amp;amp;logoColor=ffffff&amp;amp;color=5865F2&#34; alt=&#34;Join the Termux discord server&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A &lt;a href=&#34;https://termux.com&#34;&gt;Termux&lt;/a&gt; X11 server add-on app.&lt;/p&gt; &#xA;&lt;h2&gt;About&lt;/h2&gt; &#xA;&lt;p&gt;Termux:X11 is a fully fledged X server. It is built with Android NDK and optimized to be used with Termux.&lt;/p&gt; &#xA;&lt;h2&gt;Submodules caveat&lt;/h2&gt; &#xA;&lt;p&gt;This repo uses submodules. Use&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;~ $ git clone --recurse-submodules https://github.com/termux/termux-x11 &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;~ $ git clone https://github.com/termux/termux-x11&#xA;~ $ cd termux-x11&#xA;~ $ git submodule update --init --recursive&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;How does it work?&lt;/h2&gt; &#xA;&lt;p&gt;Just like any other X server.&lt;/p&gt; &#xA;&lt;h2&gt;Setup Instructions&lt;/h2&gt; &#xA;&lt;p&gt;For this one you must enable the &lt;code&gt;x11-repo&lt;/code&gt; repository can be done by executing &lt;code&gt;pkg install x11-repo&lt;/code&gt; command&lt;/p&gt; &#xA;&lt;p&gt;For X applications to work, you must install Termux-x11 companion package. You can do that by downloading an artifact from &lt;a href=&#34;https://github.com/termux/termux-x11/actions/workflows/debug_build.yml&#34;&gt;last successful build&lt;/a&gt; and installing &lt;code&gt;termux-x11-*-debug.apk&lt;/code&gt; (according to device &lt;code&gt;architecture&lt;/code&gt;, universal if you are doubting) and &lt;code&gt;*.deb&lt;/code&gt; (if you use termux with &lt;code&gt;pkg&lt;/code&gt;) or &lt;code&gt;*.tar.xz&lt;/code&gt; (if you use termux with &lt;code&gt;pacman&lt;/code&gt;) files from &lt;code&gt;termux-companion packages&lt;/code&gt; artifact (do not try to install &lt;code&gt;shell-loader-nightly.apk&lt;/code&gt; as Android application, it is not intended to be installed, it is only for chroot users). Or you can install nightly companion package from repositories with &lt;code&gt;pkg in x11-repo &amp;amp;&amp;amp; pkg in termux-x11-nightly&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Running Graphical Applications&lt;/h2&gt; &#xA;&lt;p&gt;You can start your desired graphical application by doing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;~ $ termux-x11 :1 -xstartup &#34;dbus-launch --exit-with-session xfce4-session&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;~ $ termux-x11 :1 &amp;amp;&#xA;~ $ env DISPLAY=:1 dbus-launch --exit-with-session xfce4-session&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You may replace &lt;code&gt;xfce4-session&lt;/code&gt; if you use other than Xfce&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;dbus-launch&lt;/code&gt; does not work for some users so you can start session with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;~ $ termux-x11 :1 -xstartup &#34;xfce4-session&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you&#39;re done using Termux:X11 just simply exit it through it&#39;s notification drawer by expanding the Termux:X11 notification then &#34;Exit&#34; But you should pay attention that &lt;code&gt;termux-x11&lt;/code&gt; command is still running and can not be killed this way.&lt;/p&gt; &#xA;&lt;h2&gt;Using with proot environment&lt;/h2&gt; &#xA;&lt;p&gt;If you plan to use the program with proot, keep in mind that you need to launch proot/proot-distro with the --shared-tmp option. If passing this option is not possible, set the TMPDIR environment variable to point to the directory that corresponds to /tmp in the target container. If you are using proot-distro you should know that it is possible to start &lt;code&gt;termux-x11&lt;/code&gt; command from inside proot container.&lt;/p&gt; &#xA;&lt;h2&gt;Using with chroot environment&lt;/h2&gt; &#xA;&lt;p&gt;If you plan to use the program with chroot or unshare, you must to run it as root and set the TMPDIR environment variable to point to the directory that corresponds to /tmp in the target container. This directory must be accessible from the shell from which you launch termux-x11, i.e. it must be in the same SELinux context, same mount namespace, and so on. Also you must set &lt;code&gt;XKB_CONFIG_ROOT&lt;/code&gt; environment variable pointing to container&#39;s &lt;code&gt;/usr/share/X11/xkb&lt;/code&gt; directory, otherwise you will have &lt;code&gt;xkbcomp&lt;/code&gt;-related errors. You can get loader for nightly build from an artifact of &lt;a href=&#34;https://github.com/termux/termux-x11/actions/workflows/debug_build.yml&#34;&gt;last successful build&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export TMPDIR=/path/to/chroot/container/tmp&#xA;export CLASSPATH=$(/system/bin/pm path com.termux.x11 | cut -d: -f2)&#xA;/system/bin/app_process / com.termux.x11.CmdEntryPoint :0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Logs&lt;/h3&gt; &#xA;&lt;p&gt;If you need to obtain logs from the &lt;code&gt;com.termux.x11&lt;/code&gt; application, set the &lt;code&gt;TERMUX_X11_DEBUG&lt;/code&gt; environment variable to 1, like this: &lt;code&gt;TERMUX_X11_DEBUG=1 termux-x11 :0&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;The log obtained in this way can be quite long. It&#39;s better to redirect the output of the command to a file right away.&lt;/p&gt; &#xA;&lt;h3&gt;Notification&lt;/h3&gt; &#xA;&lt;p&gt;In Android 13 post notifications was restricted so you should explicitly let Termux:X11 show you notifications.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Video&lt;/summary&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/9674930/227760411-11d440eb-90b8-451e-9024-d5a194d10b16.webm&#34;&gt;img_enable-notifications.webm&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;Preferences: You can access preferences menu three ways:&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;By clicking &#34;PREFERENCES&#34; button on main screen when no client connected.&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/termux/termux-x11/master/img/1.jpg&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;By clicking &#34;Preferences&#34; button in notification, if available.&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/termux/termux-x11/master/img/2.jpg&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;By clicking &#34;Preferences&#34; application shortcut (long tap `Termux:X11` icon in launcher). &lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/termux/termux-x11/master/img/3.jpg&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Touch gestures&lt;/h2&gt; &#xA;&lt;h3&gt;Touchpad emulation mode.&lt;/h3&gt; &#xA;&lt;p&gt;In touchpad emulation mode you can use the following gestures:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Tap for click&lt;/li&gt; &#xA; &lt;li&gt;Double tap for double click&lt;/li&gt; &#xA; &lt;li&gt;Two-finger tap for right click&lt;/li&gt; &#xA; &lt;li&gt;Three-finger tap for middle click&lt;/li&gt; &#xA; &lt;li&gt;Two-finger vertical swipe for vertical scroll&lt;/li&gt; &#xA; &lt;li&gt;Two-finger horizontal swipe for horizontal scroll&lt;/li&gt; &#xA; &lt;li&gt;Three-finger swipe down to show-hide additional keys bar.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Mouse emulation mode.&lt;/h3&gt; &#xA;&lt;p&gt;In touchpad emulation mode you can use the following gestures:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Mouse is in click mode as long as you hold finger on a screen.&lt;/li&gt; &#xA; &lt;li&gt;Double tap for double click&lt;/li&gt; &#xA; &lt;li&gt;Two-finger tap for right click&lt;/li&gt; &#xA; &lt;li&gt;Three-finger tap for middle click&lt;/li&gt; &#xA; &lt;li&gt;Two-finger vertical swipe for vertical scroll&lt;/li&gt; &#xA; &lt;li&gt;Two-finger horizontal swipe for horizontal scroll&lt;/li&gt; &#xA; &lt;li&gt;Three-finger swipe down to show-hide additional keys bar.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Font or scaling is too big!&lt;/h2&gt; &#xA;&lt;p&gt;Some apps may have issues with X server regarding DPI. please see &lt;a href=&#34;https://wiki.archlinux.org/title/HiDPI&#34;&gt;https://wiki.archlinux.org/title/HiDPI&lt;/a&gt; on how to override application-specific DPI or scaling.&lt;/p&gt; &#xA;&lt;p&gt;You can fix this in your window manager settings (in the case of xfce4 and lxqt via Applications Menu &amp;gt; Settings &amp;gt; Appearance). Look for the DPI value, if it is disabled enable it and adjust its value until the fonts are the appropriate size.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Screenshot &lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/termux/termux-x11/master/img/dpi-scale.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;Also you can start &lt;code&gt;termux-x11&lt;/code&gt; with &lt;code&gt;-dpi&lt;/code&gt; option.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;~ $ termux-x11 :1 -xstartup &#34;xfce4-session&#34; -dpi 120&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Using with 3rd party apps&lt;/h2&gt; &#xA;&lt;p&gt;It is possible to use Termux:X11 with 3rd party apps. Check how &lt;code&gt;shell-loader/src/main/java/com/termux/x11/Loader.java&lt;/code&gt; works.&lt;/p&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;Released under the &lt;a href=&#34;https://www.gnu.org/licenses/gpl-3.0.html&#34;&gt;GPLv3 license&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>jmorganca/ollama</title>
    <updated>2023-07-30T01:51:44Z</updated>
    <id>tag:github.com,2023-07-30:/jmorganca/ollama</id>
    <link href="https://github.com/jmorganca/ollama" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Get up and running with large language models locally&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; height=&#34;200px&#34; srcset=&#34;https://github.com/jmorganca/ollama/assets/3325447/56ea1849-1284-4645-8970-956de6e51c3c&#34;&gt; &#xA;  &lt;img alt=&#34;logo&#34; height=&#34;200px&#34; src=&#34;https://github.com/jmorganca/ollama/assets/3325447/0d0b44e2-8f4a-4e99-9b52-a5c1c741c8f7&#34;&gt; &#xA; &lt;/picture&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;Ollama&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/ollama&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/ollama?style=flat&amp;amp;compact=true&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: Ollama is in early preview. Please report any issues you find.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Run, create, and share large language models (LLMs).&lt;/p&gt; &#xA;&lt;h2&gt;Download&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ollama.ai/download&#34;&gt;Download&lt;/a&gt; for macOS on Apple Silicon (Intel coming soon)&lt;/li&gt; &#xA; &lt;li&gt;Download for Windows and Linux (coming soon)&lt;/li&gt; &#xA; &lt;li&gt;Build &lt;a href=&#34;https://raw.githubusercontent.com/jmorganca/ollama/main/#building&#34;&gt;from source&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;To run and chat with &lt;a href=&#34;https://ai.meta.com/llama&#34;&gt;Llama 2&lt;/a&gt;, the new model by Meta:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ollama run llama2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Model library&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;ollama&lt;/code&gt; includes a library of open-source models:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Parameters&lt;/th&gt; &#xA;   &lt;th&gt;Size&lt;/th&gt; &#xA;   &lt;th&gt;Download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama2&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;3.8GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama pull llama2&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama2 13B&lt;/td&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;7.3GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama pull llama2:13b&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Orca Mini&lt;/td&gt; &#xA;   &lt;td&gt;3B&lt;/td&gt; &#xA;   &lt;td&gt;1.9GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama pull orca&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Vicuna&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;3.8GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama pull vicuna&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Nous-Hermes&lt;/td&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;7.3GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama pull nous-hermes&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Wizard Vicuna Uncensored&lt;/td&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;7.3GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama pull wizard-vicuna&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: You should have at least 8 GB of RAM to run the 3B models, 16 GB to run the 7B models, and 32 GB to run the 13B models.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;h3&gt;Run a model&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;ollama run llama2&#xA;&amp;gt;&amp;gt;&amp;gt; hi&#xA;Hello! How can I help you today?&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Create a custom model&lt;/h3&gt; &#xA;&lt;p&gt;Pull a base model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ollama pull llama2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Create a &lt;code&gt;Modelfile&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;FROM llama2&#xA;&#xA;# set the temperature to 1 [higher is more creative, lower is more coherent]&#xA;PARAMETER temperature 1&#xA;&#xA;# set the system prompt&#xA;SYSTEM &#34;&#34;&#34;&#xA;You are Mario from Super Mario Bros. Answer as Mario, the assistant, only.&#xA;&#34;&#34;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, create and run the model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ollama create mario -f ./Modelfile&#xA;ollama run mario&#xA;&amp;gt;&amp;gt;&amp;gt; hi&#xA;Hello! It&#39;s your friend Mario.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more examples, see the &lt;a href=&#34;https://raw.githubusercontent.com/jmorganca/ollama/main/examples&#34;&gt;examples&lt;/a&gt; directory.&lt;/p&gt; &#xA;&lt;p&gt;For more information on creating a Modelfile, see the &lt;a href=&#34;https://raw.githubusercontent.com/jmorganca/ollama/main/docs/modelfile.md&#34;&gt;Modelfile&lt;/a&gt; documentation.&lt;/p&gt; &#xA;&lt;h3&gt;Pull a model from the registry&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;ollama pull orca&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Listing local models&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;ollama list&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Model packages&lt;/h2&gt; &#xA;&lt;h3&gt;Overview&lt;/h3&gt; &#xA;&lt;p&gt;Ollama bundles model weights, configuration, and data into a single package, defined by a &lt;a href=&#34;https://raw.githubusercontent.com/jmorganca/ollama/main/docs/modelfile.md&#34;&gt;Modelfile&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;picture&gt; &#xA; &lt;source media=&#34;(prefers-color-scheme: dark)&#34; height=&#34;480&#34; srcset=&#34;https://github.com/jmorganca/ollama/assets/251292/2fd96b5f-191b-45c1-9668-941cfad4eb70&#34;&gt; &#xA; &lt;img alt=&#34;logo&#34; height=&#34;480&#34; src=&#34;https://github.com/jmorganca/ollama/assets/251292/2fd96b5f-191b-45c1-9668-941cfad4eb70&#34;&gt; &#xA;&lt;/picture&gt; &#xA;&lt;h2&gt;Building&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;go build .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run it start the server:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./ollama serve &amp;amp;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, run a model!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./ollama run llama2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;REST API&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;code&gt;POST /api/generate&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Generate text from a model.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -X POST http://localhost:11434/api/generate -d &#39;{&#34;model&#34;: &#34;llama2&#34;, &#34;prompt&#34;:&#34;Why is the sky blue?&#34;}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;code&gt;POST /api/create&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Create a model from a &lt;code&gt;Modelfile&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -X POST http://localhost:11434/api/create -d &#39;{&#34;name&#34;: &#34;my-model&#34;, &#34;path&#34;: &#34;/path/to/modelfile&#34;}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>