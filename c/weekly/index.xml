<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-10-16T01:41:06Z</updated>
  <subtitle>Weekly Trending of C in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>libsql/libsql</title>
    <updated>2022-10-16T01:41:06Z</updated>
    <id>tag:github.com,2022-10-16:/libsql/libsql</id>
    <link href="https://github.com/libsql/libsql" rel="alternate"></link>
    <summary type="html">&lt;p&gt;libSQL is a fork of SQLite that is both Open Source, and Open Contributions.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/libsql/libsql/raw/master/LICENSE.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-MIT-blue&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/TxwbQTWHSr&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1026540227218640906?color=5865F2&amp;amp;label=discord&amp;amp;logo=discord&amp;amp;logoColor=8a9095&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;What is libSQL?&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://libsql.org&#34;&gt;libSQL&lt;/a&gt; is an open source, open contribution fork of SQLite. We aim to evolve it to suit many more use cases than SQLite was originally designed for.&lt;/p&gt; &#xA;&lt;h2&gt;We like SQLite a lot, and with modifications.&lt;/h2&gt; &#xA;&lt;p&gt;Wildly successful, and broadly useful, SQLite has solidified its place in modern technology stacks, embedded in nearly any computing device you can think of. Its open source nature and public domain availability make it a popular choice for modification to meet specific use cases.&lt;/p&gt; &#xA;&lt;h2&gt;Hack SQLite internally or externally?&lt;/h2&gt; &#xA;&lt;p&gt;It seems to us that there are two obvious avenues to modify SQLite: forking the code to add features directly to it, or running it on top of a modified OS. History suggests that neither of these work well. The way we see it, this is a result of one major limitation of the software: SQLite is open source but does not accept contributions, so community improvements cannot be widely enjoyed.&lt;/p&gt; &#xA;&lt;h2&gt;SQLite needs to open contributions.&lt;/h2&gt; &#xA;&lt;p&gt;We want to see a world where everyone can benefit from all of the great ideas and hard work that the SQLite community contributes back to the codebase. Community contributions work well, because &lt;a href=&#34;https://glaubercosta-11125.medium.com/sqlite-qemu-all-over-again-aedad19c9a1c&#34;&gt;we’ve done it before&lt;/a&gt;. If this was possible, what do you think SQLite could become?&lt;/p&gt; &#xA;&lt;h2&gt;Could SQLite become a distributed database?&lt;/h2&gt; &#xA;&lt;p&gt;SQLite is gaining ground in edge use cases, since it is fast, embeddable, and matches well the read-mostly, low-to-medium volume of data use cases that often arise at the edge. But there is still the problem of how to make the data available in all nodes.&lt;/p&gt; &#xA;&lt;p&gt;That is challenging to do without support from the core database. Without proper hooks, existing solutions either build a different database around SQLite (like dqlite, rqlite, ChiselStore), or have to replicate at the filesystem layer (LiteFS).&lt;/p&gt; &#xA;&lt;p&gt;What if we could do it natively?&lt;/p&gt; &#xA;&lt;h2&gt;Could SQLite be optimized with an asynchronous API?&lt;/h2&gt; &#xA;&lt;p&gt;Recently, Linux has gained a new supposedly magical interface called &lt;a href=&#34;https://www.theregister.com/2022/09/16/column/&#34;&gt;&lt;code&gt;io_uring&lt;/code&gt;&lt;/a&gt;. It leverages Asynchronous I/O and has been slowly but surely gaining adoption everywhere. Other databases like Postgres have already adopted it for asynchronous I/O, but this hasn’t made its way to SQLite. Supposedly because SQLite interfaces are synchronous. But with asynchronous runtimes and interfaces gaining popularity in both languages and the Kernel, is it time to add a new, async interface to SQLite, that plays well with &lt;code&gt;io_uring&lt;/code&gt; ?&lt;/p&gt; &#xA;&lt;h2&gt;Could SQLite be embedded in the Linux kernel?&lt;/h2&gt; &#xA;&lt;p&gt;Another innovation in the Linux Kernel is &lt;a href=&#34;https://www.scylladb.com/2020/05/05/how-io_uring-and-ebpf-will-revolutionize-programming-in-linux/&#34;&gt;eBPF&lt;/a&gt;. That is a domain-specific VM that allows programs to execute in the kernel. Although still mainly used for tracing, there’s &lt;a href=&#34;https://www.asafcidon.com/uploads/5/9/7/0/59701649/xrp.pdf&#34;&gt;research&lt;/a&gt; about pushing complex data functions like B-Tree lookups entirely in the kernel, as close as possible to NVMe devices. What if SQLite could take advantage of that, and also be unbeatable in workloads that don’t fit in memory?&lt;/p&gt; &#xA;&lt;h2&gt;Could SQLite support WASM user-defined functions?&lt;/h2&gt; &#xA;&lt;p&gt;SQLite does support &lt;a href=&#34;http://www.sqlite.org/c3ref/create_function.html&#34;&gt;user-defined functions&lt;/a&gt;. But there are two big problems with how they are approached: first, functions are written in C, which is increasingly becoming a tall ask for most developers with safety in mind. WASM is growing in popularity, allowing developers to write functions in their preferred language and be safely executed.&lt;/p&gt; &#xA;&lt;p&gt;Second, once a function proves itself generally useful, the “not Open Contribution” policy makes it difficult for it to be included in the standard distribution of SQLite.&lt;/p&gt; &#xA;&lt;h2&gt;Could you be a part of the team that makes these possible?&lt;/h2&gt; &#xA;&lt;p&gt;We’ve decided that now is the time to take SQLite to the places where the community wants to be. Are you interested in building the future of SQLite? We’re kicking off our efforts on GitHub, and you can find us on Discord.&lt;/p&gt; &#xA;&lt;h1&gt;Our plan&lt;/h1&gt; &#xA;&lt;h2&gt;Start with a fork of SQLite&lt;/h2&gt; &#xA;&lt;p&gt;There is nothing to be gained in reinventing greatness, so we will simply build upon that with a fork of core SQLite. Unlike SQLite, this fork will be both fully open source (MIT) and open to community contributions.&lt;/p&gt; &#xA;&lt;h2&gt;Preserve compatibility&lt;/h2&gt; &#xA;&lt;p&gt;We are committed to preserving compatibility with everything that was previously written for SQLite. All of your favorite tools and libraries will continue to work as-is. Preserve stability SQLite is a very well-tested piece of software. We admire that, and commit to preserving the existing test suite while expanding it for the new code we add.&lt;/p&gt; &#xA;&lt;h2&gt;Use Rust for new features&lt;/h2&gt; &#xA;&lt;p&gt;We intend to add Rust to implement new capabilities, but not exclusively so. The existing C codebase already serves as a great foundation for interoperability with other languages and systems, and we don’t intend to change that.&lt;/p&gt; &#xA;&lt;h2&gt;Rejoin core SQLite if its policy changes&lt;/h2&gt; &#xA;&lt;p&gt;We are strong believers in open source that is also open to community contributions. If and when SQLite changes its policy to accept contributions, we will gladly merge our work back into the core product and continue in that space.&lt;/p&gt; &#xA;&lt;h2&gt;Adhere to a code of conduct&lt;/h2&gt; &#xA;&lt;p&gt;We take our code of conduct seriously, and unlike SQLite, we do not substitute it with an &lt;a href=&#34;https://sqlite.org/codeofethics.html&#34;&gt;unclear alternative&lt;/a&gt;. We strive to foster a community that values diversity, equity, and inclusion. We encourage others to speak up if they feel uncomfortable.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ggerganov/whisper.cpp</title>
    <updated>2022-10-16T01:41:06Z</updated>
    <id>tag:github.com,2022-10-16:/ggerganov/whisper.cpp</id>
    <link href="https://github.com/ggerganov/whisper.cpp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Port of OpenAI&#39;s Whisper model in C/C++&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;whisper.cpp&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ggerganov/whisper.cpp/actions&#34;&gt;&lt;img src=&#34;https://github.com/ggerganov/whisper.cpp/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;Actions Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;High-performance inference of &lt;a href=&#34;https://github.com/openai/whisper&#34;&gt;OpenAI&#39;s Whisper&lt;/a&gt; automatic speech recognition (ASR) model:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Plain C/C++ implementation without dependencies&lt;/li&gt; &#xA; &lt;li&gt;ARM_NEON and AVX intrinsics support&lt;/li&gt; &#xA; &lt;li&gt;Mixed F16 / F32 precision&lt;/li&gt; &#xA; &lt;li&gt;Low memory usage (Flash Attention + Flash Forward)&lt;/li&gt; &#xA; &lt;li&gt;Zero memory allocations at runtime&lt;/li&gt; &#xA; &lt;li&gt;Runs on the CPU&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ggerganov/whisper.cpp/raw/master/whisper.h&#34;&gt;C-style API&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Supported platforms: Linux, Mac OS (Intel and Arm), Windows (MSVC and MinGW), Raspberry Pi, Android&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;To build the main program, run &lt;code&gt;make&lt;/code&gt;. You can then transcribe a &lt;code&gt;.wav&lt;/code&gt; file like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./main -f input.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Before running the program, make sure to download one of the ggml Whisper models. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash ./download-ggml-model.sh base.en&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;For a quick demo, simply run &lt;code&gt;make base.en&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;$ make base.en&#xA;cc  -O3 -std=c11   -Wall -Wextra -Wno-unused-parameter -Wno-unused-function -pthread -c ggml.c&#xA;c++ -O3 -std=c++11 -Wall -Wextra -Wno-unused-parameter -Wno-unused-function -pthread -c whisper.cpp&#xA;c++ -O3 -std=c++11 -Wall -Wextra -Wno-unused-parameter -Wno-unused-function -pthread main.cpp whisper.o ggml.o -o main&#xA;./main -h&#xA;&#xA;usage: ./main [options] file0.wav file1.wav ...&#xA;&#xA;options:&#xA;  -h,       --help           show this help message and exit&#xA;  -s SEED,  --seed SEED      RNG seed (default: -1)&#xA;  -t N,     --threads N      number of threads to use during computation (default: 4)&#xA;  -o N,     --offset N       offset in milliseconds (default: 0)&#xA;  -v,       --verbose        verbose output&#xA;            --translate      translate from source language to english&#xA;  -otxt,    --output-txt     output result in a text file&#xA;  -ovtt,    --output-vtt     output result in a vtt file&#xA;  -osrt,    --output-srt     output result in a srt file&#xA;  -ps,      --print_special  print special tokens&#xA;  -nt,      --no_timestamps  do not print timestamps&#xA;  -l LANG,  --language LANG  spoken language (default: en)&#xA;  -m FNAME, --model FNAME    model path (default: models/ggml-base.en.bin)&#xA;  -f FNAME, --file FNAME     input WAV file path&#xA;&#xA;bash ./download-ggml-model.sh base.en&#xA;Downloading ggml model base.en ...&#xA;models/ggml-base.en.bin            100%[===================================&amp;gt;] 141.11M  6.49MB/s    in 23s&#xA;Done! Model &#39;base.en&#39; saved in &#39;models/ggml-base.en.bin&#39;&#xA;You can now use it like this:&#xA;&#xA;  $ ./main -m models/ggml-base.en.bin -f samples/jfk.wav&#xA;&#xA;&#xA;===============================================&#xA;Running base.en on all samples in ./samples ...&#xA;===============================================&#xA;&#xA;----------------------------------------------&#xA;[+] Running base.en on samples/jfk.wav ... (run &#39;ffplay samples/jfk.wav&#39; to listen)&#xA;----------------------------------------------&#xA;&#xA;whisper_model_load: loading model from &#39;models/ggml-base.en.bin&#39;&#xA;whisper_model_load: n_vocab       = 51864&#xA;whisper_model_load: n_audio_ctx   = 1500&#xA;whisper_model_load: n_audio_state = 512&#xA;whisper_model_load: n_audio_head  = 8&#xA;whisper_model_load: n_audio_layer = 6&#xA;whisper_model_load: n_text_ctx    = 448&#xA;whisper_model_load: n_text_state  = 512&#xA;whisper_model_load: n_text_head   = 8&#xA;whisper_model_load: n_text_layer  = 6&#xA;whisper_model_load: n_mels        = 80&#xA;whisper_model_load: f16           = 1&#xA;whisper_model_load: type          = 2&#xA;whisper_model_load: mem_required  = 377.00 MB&#xA;whisper_model_load: adding 1607 extra tokens&#xA;whisper_model_load: ggml ctx size = 163.43 MB&#xA;whisper_model_load: memory size =    22.83 MB&#xA;whisper_model_load: model size  =   140.54 MB&#xA;&#xA;main: processing &#39;samples/jfk.wav&#39; (176000 samples, 11.0 sec), 4 threads, lang = en, task = transcribe, timestamps = 1 ...&#xA;&#xA;[00:00.000 --&amp;gt; 00:11.000]   And so my fellow Americans, ask not what your country can do for you, ask what you can do for your country.&#xA;&#xA;&#xA;whisper_print_timings:     load time =    77.48 ms&#xA;whisper_print_timings:      mel time =    26.10 ms&#xA;whisper_print_timings:   sample time =     2.19 ms&#xA;whisper_print_timings:   encode time =   632.95 ms / 105.49 ms per layer&#xA;whisper_print_timings:   decode time =    85.11 ms / 14.18 ms per layer&#xA;whisper_print_timings:    total time =   824.14 ms&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The command downloads the &lt;code&gt;base.en&lt;/code&gt; model converted to custom &lt;code&gt;ggml&lt;/code&gt; format and runs the inference on all &lt;code&gt;.wav&lt;/code&gt; samples in the folder &lt;code&gt;samples&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For detailed usage instructions, run: &lt;code&gt;./main -h&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Note that &lt;code&gt;whisper.cpp&lt;/code&gt; currently runs only with 16-bit WAV files, so make sure to convert your input before running the tool. For example, you can use &lt;code&gt;ffmpeg&lt;/code&gt; like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;ffmpeg -i input.mp3 -ar 16000 -ac 1 -c:a pcm_s16le output.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;More audio samples&lt;/h2&gt; &#xA;&lt;p&gt;If you want some extra audio samples to play with, simply run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make samples&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will download a few more audio files from Wikipedia and convert them to 16-bit WAV format via &lt;code&gt;ffmpeg&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can download and run the other models as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make tiny.en&#xA;make tiny&#xA;make base.en&#xA;make base&#xA;make small.en&#xA;make small&#xA;make medium.en&#xA;make medium&#xA;make large&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Another example&lt;/h2&gt; &#xA;&lt;p&gt;Here is another example of transcribing a &lt;a href=&#34;https://upload.wikimedia.org/wikipedia/commons/1/1f/George_W_Bush_Columbia_FINAL.ogg&#34;&gt;3:24 min speech&lt;/a&gt; in less than a minute on a MacBook M1 Pro, using &lt;code&gt;medium.en&lt;/code&gt; model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;$ ./main -m models/ggml-medium.en.bin -f samples/gb1.wav -t 8&#xA;&#xA;whisper_model_load: loading model from &#39;models/ggml-medium.en.bin&#39;&#xA;whisper_model_load: n_vocab       = 51864&#xA;whisper_model_load: n_audio_ctx   = 1500&#xA;whisper_model_load: n_audio_state = 1024&#xA;whisper_model_load: n_audio_head  = 16&#xA;whisper_model_load: n_audio_layer = 24&#xA;whisper_model_load: n_text_ctx    = 448&#xA;whisper_model_load: n_text_state  = 1024&#xA;whisper_model_load: n_text_head   = 16&#xA;whisper_model_load: n_text_layer  = 24&#xA;whisper_model_load: n_mels        = 80&#xA;whisper_model_load: f16           = 1&#xA;whisper_model_load: type          = 4&#xA;whisper_model_load: mem_required  = 2502.00 MB&#xA;whisper_model_load: adding 1607 extra tokens&#xA;whisper_model_load: ggml ctx size = 1644.97 MB&#xA;whisper_model_load: memory size =   182.62 MB&#xA;whisper_model_load: model size  =  1462.12 MB&#xA;log_mel_spectrogram: n_sample = 3179750, n_len = 19873&#xA;log_mel_spectrogram: recording length: 198.734375 s&#xA;&#xA;main: processing 3179750 samples (198.7 sec), 8 threads, lang = english, task = transcribe, timestamps = 1 ...&#xA;&#xA;[00:00.000 --&amp;gt; 00:08.000]   My fellow Americans, this day has brought terrible news and great sadness to our country.&#xA;[00:08.000 --&amp;gt; 00:17.000]   At 9 o&#39;clock this morning, Mission Control in Houston lost contact with our Space Shuttle Columbia.&#xA;[00:17.000 --&amp;gt; 00:24.000]   A short time later, debris was seen falling from the skies above Texas.&#xA;[00:24.000 --&amp;gt; 00:29.000]   The Columbia&#39;s lost. There are no survivors.&#xA;[00:29.000 --&amp;gt; 00:32.000]   On board was a crew of seven.&#xA;[00:32.000 --&amp;gt; 00:43.000]   Colonel Rick Husband, Lieutenant Colonel Michael Anderson, Commander Laurel Clark, Captain David Brown, Commander William McCool,&#xA;[00:43.000 --&amp;gt; 00:52.000]   Dr. Kultner Aschavla, and Elon Ramon, a Colonel in the Israeli Air Force.&#xA;[00:52.000 --&amp;gt; 00:58.000]   These men and women assumed great risk in the service to all humanity.&#xA;[00:58.000 --&amp;gt; 01:06.000]   In an age when space flight has come to seem almost routine, it is easy to overlook the dangers of travel by rocket&#xA;[01:06.000 --&amp;gt; 01:12.000]   and the difficulties of navigating the fierce outer atmosphere of the Earth.&#xA;[01:12.000 --&amp;gt; 01:22.000]   These astronauts knew the dangers, and they faced them willingly, knowing they had a high and noble purpose in life.&#xA;[01:22.000 --&amp;gt; 01:30.000]   Because of their courage, endearing, and idealism, we will miss them all the more.&#xA;[01:30.000 --&amp;gt; 01:40.000]   All Americans today are thinking as well of the families of these men and women who have been given this sudden shock and grief.&#xA;[01:40.000 --&amp;gt; 01:45.000]   You&#39;re not alone. Our entire nation agrees with you.&#xA;[01:45.000 --&amp;gt; 01:52.000]   And those you love will always have the respect and gratitude of this country.&#xA;[01:52.000 --&amp;gt; 01:56.000]   The cause in which they died will continue.&#xA;[01:56.000 --&amp;gt; 02:07.000]   Mankind is led into the darkness beyond our world by the inspiration of discovery and the longing to understand.&#xA;[02:07.000 --&amp;gt; 02:11.000]   Our journey into space will go on.&#xA;[02:11.000 --&amp;gt; 02:16.000]   In the skies today, we saw destruction and tragedy.&#xA;[02:16.000 --&amp;gt; 02:22.000]   Yet farther than we can see, there is comfort and hope.&#xA;[02:22.000 --&amp;gt; 02:31.000]   In the words of the prophet Isaiah, &#34;Lift your eyes and look to the heavens who created all these.&#xA;[02:31.000 --&amp;gt; 02:39.000]   He who brings out the starry hosts one by one and calls them each by name.&#34;&#xA;[02:39.000 --&amp;gt; 02:46.000]   Because of his great power and mighty strength, not one of them is missing.&#xA;[02:46.000 --&amp;gt; 02:55.000]   The same creator who names the stars also knows the names of the seven souls we mourn today.&#xA;[02:55.000 --&amp;gt; 03:05.000]   The crew of the shuttle Columbia did not return safely to Earth, yet we can pray that all are safely home.&#xA;[03:05.000 --&amp;gt; 03:14.000]   May God bless the grieving families and may God continue to bless America.&#xA;[03:14.000 --&amp;gt; 03:24.000]   [Music]&#xA;&#xA;&#xA;main:     load time =   522.18 ms&#xA;main:      mel time =   423.43 ms&#xA;main:   sample time =    31.42 ms&#xA;main:   encode time = 41518.51 ms / 1729.94 ms per layer&#xA;main:   decode time = 14907.22 ms&#xA;main:    total time = 57416.63 ms&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Real-time audio input example&lt;/h2&gt; &#xA;&lt;p&gt;This is a naive example of performing real-time inference on audio from your microphone. The &lt;code&gt;stream&lt;/code&gt; tool samples the audio every half a second and runs the transcription continously. More info is available in &lt;a href=&#34;https://github.com/ggerganov/whisper.cpp/issues/10&#34;&gt;issue #10&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;./stream -m ./models/ggml-base.en.bin -t 8 --step 500 --length 5000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/1991296/194935793-76afede7-cfa8-48d8-a80f-28ba83be7d09.mp4&#34;&gt;https://user-images.githubusercontent.com/1991296/194935793-76afede7-cfa8-48d8-a80f-28ba83be7d09.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;stream&lt;/code&gt; tool depends on SDL2 library to capture audio from the microphone. You can build it like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Install SDL2 on Linux &#xA;sudo apt-get install libsdl2-dev&#xA;&#xA;# Install SDL2 on Mac OS&#xA;brew install sdl2&#xA;&#xA;make stream&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Implementation details&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The core tensor operations are implemented in C (&lt;a href=&#34;https://raw.githubusercontent.com/ggerganov/whisper.cpp/master/ggml.h&#34;&gt;ggml.h&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/ggerganov/whisper.cpp/master/ggml.c&#34;&gt;ggml.c&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;The high-level C-style API is implemented in C++ (&lt;a href=&#34;https://raw.githubusercontent.com/ggerganov/whisper.cpp/master/whisper.h&#34;&gt;whisper.h&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/ggerganov/whisper.cpp/master/whisper.cpp&#34;&gt;whisper.cpp&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Simple usage is demonstrated in &lt;a href=&#34;https://raw.githubusercontent.com/ggerganov/whisper.cpp/master/main.cpp&#34;&gt;main.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Sample real-time audio transcription from the microphone is demonstrated in &lt;a href=&#34;https://raw.githubusercontent.com/ggerganov/whisper.cpp/master/stream.cpp&#34;&gt;stream.cpp&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Limitations&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Very basic greedy sampling scheme - always pick up the top token. You can implement your own strategy&lt;/li&gt; &#xA; &lt;li&gt;Inference only&lt;/li&gt; &#xA; &lt;li&gt;No GPU support&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Memory usage&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Disk&lt;/th&gt; &#xA;   &lt;th&gt;Mem&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;tiny&lt;/td&gt; &#xA;   &lt;td&gt;75 MB&lt;/td&gt; &#xA;   &lt;td&gt;~240 MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;base&lt;/td&gt; &#xA;   &lt;td&gt;142 MB&lt;/td&gt; &#xA;   &lt;td&gt;~380 MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;small&lt;/td&gt; &#xA;   &lt;td&gt;466 MB&lt;/td&gt; &#xA;   &lt;td&gt;~970 MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;medium&lt;/td&gt; &#xA;   &lt;td&gt;1.5 GB&lt;/td&gt; &#xA;   &lt;td&gt;~2.5 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;large&lt;/td&gt; &#xA;   &lt;td&gt;2.9 GB&lt;/td&gt; &#xA;   &lt;td&gt;~4.6 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;ggml format&lt;/h2&gt; &#xA;&lt;p&gt;The original models are converted to a custom binary format. This allows to pack everything needed into a single file:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;model parameters&lt;/li&gt; &#xA; &lt;li&gt;mel filters&lt;/li&gt; &#xA; &lt;li&gt;vocabulary&lt;/li&gt; &#xA; &lt;li&gt;weights&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can download the converted models using the &lt;a href=&#34;https://raw.githubusercontent.com/ggerganov/whisper.cpp/master/download-ggml-model.sh&#34;&gt;download-ggml-model.sh&lt;/a&gt; script or from here:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ggml.ggerganov.com&#34;&gt;https://ggml.ggerganov.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;For more details, see the conversion script &lt;a href=&#34;https://raw.githubusercontent.com/ggerganov/whisper.cpp/master/convert-pt-to-ggml.py&#34;&gt;convert-pt-to-ggml.py&lt;/a&gt; or the README in &lt;a href=&#34;https://raw.githubusercontent.com/ggerganov/whisper.cpp/master/models&#34;&gt;models&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Bindings&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Rust: &lt;a href=&#34;https://github.com/tazz4843/whisper-rs&#34;&gt;tazz4843/whisper-rs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Python:&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Obj-C:&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Java:&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>TelegramMessenger/Telegram-iOS</title>
    <updated>2022-10-16T01:41:06Z</updated>
    <id>tag:github.com,2022-10-16:/TelegramMessenger/Telegram-iOS</id>
    <link href="https://github.com/TelegramMessenger/Telegram-iOS" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Telegram-iOS&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Telegram iOS Source Code Compilation Guide&lt;/h1&gt; &#xA;&lt;p&gt;We welcome all developers to use our API and source code to create applications on our platform. There are several things we require from &lt;strong&gt;all developers&lt;/strong&gt; for the moment.&lt;/p&gt; &#xA;&lt;h1&gt;Creating your Telegram Application&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://core.telegram.org/api/obtaining_api_id&#34;&gt;&lt;strong&gt;Obtain your own api_id&lt;/strong&gt;&lt;/a&gt; for your application.&lt;/li&gt; &#xA; &lt;li&gt;Please &lt;strong&gt;do not&lt;/strong&gt; use the name Telegram for your app — or make sure your users understand that it is unofficial.&lt;/li&gt; &#xA; &lt;li&gt;Kindly &lt;strong&gt;do not&lt;/strong&gt; use our standard logo (white paper plane in a blue circle) as your app&#39;s logo.&lt;/li&gt; &#xA; &lt;li&gt;Please study our &lt;a href=&#34;https://core.telegram.org/mtproto/security_guidelines&#34;&gt;&lt;strong&gt;security guidelines&lt;/strong&gt;&lt;/a&gt; and take good care of your users&#39; data and privacy.&lt;/li&gt; &#xA; &lt;li&gt;Please remember to publish &lt;strong&gt;your&lt;/strong&gt; code too in order to comply with the licences.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Compilation Guide&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install Xcode (directly from &lt;a href=&#34;https://developer.apple.com/download/more&#34;&gt;https://developer.apple.com/download/more&lt;/a&gt; or using the App Store).&lt;/li&gt; &#xA; &lt;li&gt;Clone the project from GitHub:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone --recursive -j8 https://github.com/TelegramMessenger/Telegram-iOS.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Adjust configuration parameters&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;mkdir -p $HOME/telegram-configuration&#xA;cp -R build-system/example-configuration/* $HOME/telegram-configuration/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Modify the values in &lt;code&gt;variables.bzl&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Replace the provisioning profiles in &lt;code&gt;provisioning&lt;/code&gt; with valid files&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;(Optional) Create a build cache directory to speed up rebuilds&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;mkdir -p &#34;$HOME/telegram-bazel-cache&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;Build the app&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 build-system/Make/Make.py \&#xA;    --cacheDir=&#34;$HOME/telegram-bazel-cache&#34; \&#xA;    build \&#xA;    --configurationPath=&#34;$HOME/telegram-configuration&#34; \&#xA;    --buildNumber=100001 \&#xA;    --configuration=release_universal&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;6&#34;&gt; &#xA; &lt;li&gt;(Optional) Generate an Xcode project&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 build-system/Make/Make.py \&#xA;    --cacheDir=&#34;$HOME/telegram-bazel-cache&#34; \&#xA;    generateProject \&#xA;    --configurationPath=&#34;$HOME/telegram-configuration&#34; \&#xA;    --disableExtensions&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It is possible to generate a project that does not require any codesigning certificates to be installed: add &lt;code&gt;--disableProvisioningProfiles&lt;/code&gt; flag:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 build-system/Make/Make.py \&#xA;    --cacheDir=&#34;$HOME/telegram-bazel-cache&#34; \&#xA;    generateProject \&#xA;    --configurationPath=&#34;$HOME/telegram-configuration&#34; \&#xA;    --disableExtensions \&#xA;    --disableProvisioningProfiles&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Tip: use &lt;code&gt;--disableExtensions&lt;/code&gt; when developing to speed up development by not building application extensions and the WatchOS app.&lt;/p&gt; &#xA;&lt;h1&gt;Tips&lt;/h1&gt; &#xA;&lt;p&gt;Bazel is used to build the app. To simplify the development setup a helper script is provided (&lt;code&gt;build-system/Make/Make.py&lt;/code&gt;). See help:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 build-system/Make/Make.py --help&#xA;python3 build-system/Make/Make.py build --help&#xA;python3 build-system/Make/Make.py generateProject --help&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Bazel is automatically downloaded when running Make.py for the first time. If you wish to use your own build of Bazel, pass &lt;code&gt;--bazel=path-to-bazel&lt;/code&gt;. If your Bazel version differs from that in &lt;code&gt;versions.json&lt;/code&gt;, you may use &lt;code&gt;--overrideBazelVersion&lt;/code&gt; to skip the version check.&lt;/p&gt; &#xA;&lt;p&gt;Each release is built using specific Xcode and Bazel versions (see &lt;code&gt;versions.json&lt;/code&gt;). The helper script checks the versions of installed software and reports an error if they don&#39;t match the ones specified in &lt;code&gt;versions.json&lt;/code&gt;. There are flags that allow to bypass these checks:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 build-system/Make/Make.py --overrideBazelVersion build ... # Don&#39;t check the version of Bazel&#xA;python3 build-system/Make/Make.py --overrideXcodeVersion build ... # Don&#39;t check the version of Xcode&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>