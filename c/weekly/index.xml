<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-10T01:50:58Z</updated>
  <subtitle>Weekly Trending of C in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>RIOT-OS/RIOT</title>
    <updated>2023-12-10T01:50:58Z</updated>
    <id>tag:github.com,2023-12-10:/RIOT-OS/RIOT</id>
    <link href="https://github.com/RIOT-OS/RIOT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;RIOT - The friendly OS for IoT&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://ci.riot-os.org/details/branch/master&#34;&gt;&lt;img src=&#34;https://ci.riot-os.org/job/branch/master/badge&#34; alt=&#34;Nightly CI status master&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/RIOT-OS/RIOT/releases/latest&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/release/RIOT-OS/RIOT.svg?sanitize=true&#34; alt=&#34;GitHub release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/RIOT-OS/RIOT/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/RIOT-OS/RIOT&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doc.riot-os.org/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/docs-API-informational.svg?sanitize=true&#34; alt=&#34;API docs&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/RIOT-OS/RIOT/wiki&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/docs-Wiki-informational.svg?sanitize=true&#34; alt=&#34;Wiki&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://stackoverflow.com/questions/tagged/riot-os&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/stackoverflow-%5Briot--os%5D-yellow&#34; alt=&#34;Stack Overflow questions&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/RIOT_OS&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/social-Twitter-informational.svg?sanitize=true&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://matrix.to/#/#riot-os:matrix.org&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/chat-Matrix-brightgreen.svg?sanitize=true&#34; alt=&#34;Matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RIOT-OS/RIOT/master/doc/doxygen/src/riot-logo.svg?sanitize=true&#34; width=&#34;66%&#34;&gt;&#xA; &lt;!--&#xA;                          ZZZZZZ&#xA;                        ZZZZZZZZZZZZ&#xA;                      ZZZZZZZZZZZZZZZZ&#xA;                     ZZZZZZZ     ZZZZZZ&#xA;                    ZZZZZZ        ZZZZZ&#xA;                    ZZZZZ          ZZZZ&#xA;                    ZZZZ           ZZZZZ&#xA;                    ZZZZ           ZZZZ&#xA;                    ZZZZ          ZZZZZ&#xA;                    ZZZZ        ZZZZZZ&#xA;                    ZZZZ     ZZZZZZZZ       777        7777       7777777777&#xA;              ZZ    ZZZZ   ZZZZZZZZ         777      77777777    77777777777&#xA;          ZZZZZZZ   ZZZZ  ZZZZZZZ           777     7777  7777       777&#xA;        ZZZZZZZZZ   ZZZZ    Z               777     777    777       777&#xA;       ZZZZZZ       ZZZZ                    777     777    777       777&#xA;      ZZZZZ         ZZZZ                    777     777    777       777&#xA;     ZZZZZ          ZZZZZ    ZZZZ           777     777    777       777&#xA;     ZZZZ           ZZZZZ    ZZZZZ          777     777    777       777&#xA;     ZZZZ           ZZZZZ     ZZZZZ         777     777    777       777&#xA;     ZZZZ           ZZZZ       ZZZZZ        777     777    777       777&#xA;     ZZZZZ         ZZZZZ        ZZZZZ       777     777    777       777&#xA;      ZZZZZZ     ZZZZZZ          ZZZZZ      777     7777777777       777&#xA;       ZZZZZZZZZZZZZZZ            ZZZZ      777      77777777        777&#xA;         ZZZZZZZZZZZ               Z&#xA;            ZZZZZ                                                           --&gt;&lt;/p&gt; &#xA;&lt;p&gt;The friendly Operating System for IoT!&lt;/p&gt; &#xA;&lt;p&gt;RIOT is a real-time multi-threading operating system that supports a range of devices that are typically found in the Internet of Things (IoT): 8-bit, 16-bit and 32-bit microcontrollers.&lt;/p&gt; &#xA;&lt;p&gt;RIOT is based on the following design principles: energy-efficiency, real-time capabilities, small memory footprint, modularity, and uniform API access, independent of the underlying hardware (this API offers partial POSIX compliance).&lt;/p&gt; &#xA;&lt;p&gt;RIOT is developed by an international open source community which is independent of specific vendors (e.g. similarly to the Linux community). RIOT is licensed with LGPLv2.1, a copyleft license which fosters indirect business models around the free open-source software platform provided by RIOT, e.g. it is possible to link closed-source code with the LGPL code.&lt;/p&gt; &#xA;&lt;h2&gt;FEATURES&lt;/h2&gt; &#xA;&lt;p&gt;RIOT provides features including, but not limited to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;a preemptive, tickless scheduler with priorities&lt;/li&gt; &#xA; &lt;li&gt;flexible memory management&lt;/li&gt; &#xA; &lt;li&gt;high resolution, long-term timers&lt;/li&gt; &#xA; &lt;li&gt;MTD abstraction layer&lt;/li&gt; &#xA; &lt;li&gt;File System integration&lt;/li&gt; &#xA; &lt;li&gt;support 200+ boards based on AVR, MSP430, ESP8266, ESP32, RISC-V, ARM7 and ARM Cortex-M&lt;/li&gt; &#xA; &lt;li&gt;the native port allows to run RIOT as-is on Linux and BSD. Multiple instances of RIOT running on a single machine can also be interconnected via a simple virtual Ethernet bridge or via a simulated IEEE 802.15.4 network (ZEP)&lt;/li&gt; &#xA; &lt;li&gt;IPv6&lt;/li&gt; &#xA; &lt;li&gt;6LoWPAN (RFC4944, RFC6282, and RFC6775)&lt;/li&gt; &#xA; &lt;li&gt;UDP&lt;/li&gt; &#xA; &lt;li&gt;RPL (storing mode, P2P mode)&lt;/li&gt; &#xA; &lt;li&gt;CoAP&lt;/li&gt; &#xA; &lt;li&gt;OTA updates via SUIT&lt;/li&gt; &#xA; &lt;li&gt;MQTT&lt;/li&gt; &#xA; &lt;li&gt;USB (device mode)&lt;/li&gt; &#xA; &lt;li&gt;Display / Touchscreen support&lt;/li&gt; &#xA; &lt;li&gt;CCN-Lite&lt;/li&gt; &#xA; &lt;li&gt;LoRaWAN&lt;/li&gt; &#xA; &lt;li&gt;UWB&lt;/li&gt; &#xA; &lt;li&gt;Bluetooth (BLE) via &lt;a href=&#34;https://github.com/apache/mynewt-nimble&#34;&gt;NimBLE&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;GETTING RIOT&lt;/h2&gt; &#xA;&lt;p&gt;The most convenient way to get RIOT is to clone it via Git&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ git clone https://github.com/RIOT-OS/RIOT&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;this will ensure that you get all the newest features and bug fixes with the caveat of an ever changing work environment.&lt;/p&gt; &#xA;&lt;p&gt;If you prefer things more stable, you can download the source code of one of our quarter annual releases &lt;a href=&#34;https://github.com/RIOT-OS/RIOT/releases&#34;&gt;via Github&lt;/a&gt; as ZIP file or tarball. You can also checkout a release in a cloned Git repository using&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ git pull --tags&#xA;$ git checkout &amp;lt;YYYY.MM&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more details on our release cycle, check our &lt;a href=&#34;https://doc.riot-os.org/release-cycle.html&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;GETTING STARTED&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You want to start the RIOT? Just follow our &lt;a href=&#34;https://doc.riot-os.org/index.html#the-quickest-start&#34;&gt;quickstart guide&lt;/a&gt; or try this &lt;a href=&#34;https://github.com/RIOT-OS/Tutorials/raw/master/README.md&#34;&gt;tutorial&lt;/a&gt;. For specific toolchain installation, follow instructions in the &lt;a href=&#34;https://doc.riot-os.org/getting-started.html&#34;&gt;getting started&lt;/a&gt; page.&lt;/li&gt; &#xA; &lt;li&gt;The RIOT API itself can be built from the code using doxygen. The latest version of the documentation is uploaded daily to &lt;a href=&#34;https://doc.riot-os.org&#34;&gt;doc.riot-os.org&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;FORUM&lt;/h2&gt; &#xA;&lt;p&gt;Do you have a question, want to discuss a new feature, or just want to present your latest project using RIOT? Come over to our &lt;a href=&#34;https://forum.riot-os.org&#34;&gt;forum&lt;/a&gt; and post to your hearts content.&lt;/p&gt; &#xA;&lt;h2&gt;CONTRIBUTE&lt;/h2&gt; &#xA;&lt;p&gt;To contribute something to RIOT, please refer to our &lt;a href=&#34;https://raw.githubusercontent.com/RIOT-OS/RIOT/master/CONTRIBUTING.md&#34;&gt;contributing document&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;MAILING LISTS&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;RIOT commits: &lt;a href=&#34;https://lists.riot-os.org/mailman/listinfo/commits&#34;&gt;commits@riot-os.org&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Github notifications: &lt;a href=&#34;https://lists.riot-os.org/mailman/listinfo/notifications&#34;&gt;notifications@riot-os.org&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;LICENSE&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Most of the code developed by the RIOT community is licensed under the GNU Lesser General Public License (LGPL) version 2.1 as published by the Free Software Foundation.&lt;/li&gt; &#xA; &lt;li&gt;Some external sources, especially files developed by SICS are published under a separate license.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;All code files contain licensing information.&lt;/p&gt; &#xA;&lt;p&gt;For more information, see the RIOT website:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.riot-os.org&#34;&gt;https://www.riot-os.org&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/seamless_communication</title>
    <updated>2023-12-10T01:50:58Z</updated>
    <id>tag:github.com,2023-12-10:/facebookresearch/seamless_communication</id>
    <link href="https://github.com/facebookresearch/seamless_communication" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Foundational Models for State-of-the-Art Speech and Text Translation&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/23-11_SEAMLESS_BlogHero_11.17.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Seamless Intro&lt;/h1&gt; &#xA;&lt;h2&gt;SeamlessM4T&lt;/h2&gt; &#xA;&lt;p&gt;SeamlessM4T is our foundational all-in-one &lt;strong&gt;M&lt;/strong&gt;assively &lt;strong&gt;M&lt;/strong&gt;ultilingual and &lt;strong&gt;M&lt;/strong&gt;ultimodal &lt;strong&gt;M&lt;/strong&gt;achine &lt;strong&gt;T&lt;/strong&gt;ranslation model delivering high-quality translation for speech and text in nearly 100 languages.&lt;/p&gt; &#xA;&lt;p&gt;SeamlessM4T models support the tasks of:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speech-to-speech translation (S2ST)&lt;/li&gt; &#xA; &lt;li&gt;Speech-to-text translation (S2TT)&lt;/li&gt; &#xA; &lt;li&gt;Text-to-speech translation (T2ST)&lt;/li&gt; &#xA; &lt;li&gt;Text-to-text translation (T2TT)&lt;/li&gt; &#xA; &lt;li&gt;Automatic speech recognition (ASR)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;span&gt;🌟&lt;/span&gt; We are releasing SeamlessM4T v2, an updated version with our novel &lt;em&gt;UnitY2&lt;/em&gt; architecture. This new model improves over SeamlessM4T v1 in quality as well as inference latency in speech generation tasks.&lt;/p&gt; &#xA;&lt;p&gt;To learn more about the collection of SeamlessM4T models, the approach used in each, their language coverage and their performance, visit the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/docs/m4t/README.md&#34;&gt;SeamlessM4T README&lt;/a&gt; or &lt;a href=&#34;https://huggingface.co/facebook/seamless-m4t-v2-large&#34;&gt;🤗 Model Card&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;SeamlessExpressive&lt;/h2&gt; &#xA;&lt;p&gt;SeamlessExpressive is a speech-to-speech translation model that captures certain underexplored aspects of prosody such as speech rate and pauses, while preserving the style of one&#39;s voice and high content translation quality.&lt;/p&gt; &#xA;&lt;p&gt;To learn more about SeamlessExpressive models, visit the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/docs/expressive/README.md&#34;&gt;SeamlessExpressive README&lt;/a&gt; or &lt;a href=&#34;https://huggingface.co/facebook/seamless-expressive&#34;&gt;🤗 Model Card&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;SeamlessStreaming&lt;/h2&gt; &#xA;&lt;p&gt;SeamlessStreaming is a streaming translation model. The model supports speech as input modality and speech/text as output modalities.&lt;/p&gt; &#xA;&lt;p&gt;The SeamlessStreaming model supports the following tasks:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speech-to-speech translation (S2ST)&lt;/li&gt; &#xA; &lt;li&gt;Speech-to-text translation (S2TT)&lt;/li&gt; &#xA; &lt;li&gt;Automatic speech recognition (ASR)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To learn more about SeamlessStreaming models, visit the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/docs/streaming/README.md&#34;&gt;SeamlessStreaming README&lt;/a&gt; or &lt;a href=&#34;https://huggingface.co/facebook/seamless-streaming&#34;&gt;🤗 Model Card&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Seamless&lt;/h2&gt; &#xA;&lt;p&gt;The Seamless model is the unified model for expressive streaming speech-to-speech translations.&lt;/p&gt; &#xA;&lt;h2&gt;Links&lt;/h2&gt; &#xA;&lt;h3&gt;Blog&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ai.meta.com/research/seamless-communication/&#34;&gt;AI at Meta Blog&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Papers&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ai.facebook.com/research/publications/seamless-multilingual-expressive-and-streaming-speech-translation/&#34;&gt;Seamless&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ai.meta.com/research/publications/efficient-monotonic-multihead-attention/&#34;&gt;EMMA&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ai.meta.com/research/publications/sonar-expressive-zero-shot-expressive-speech-to-speech-translation/&#34;&gt;SONAR&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Demos&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;SeamlessM4T v2&lt;/th&gt; &#xA;   &lt;th&gt;SeamlessExpressive&lt;/th&gt; &#xA;   &lt;th&gt;SeamlessStreaming&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Demo&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://seamless.metademolab.com/m4t?utm_source=github&amp;amp;utm_medium=web&amp;amp;utm_campaign=seamless&amp;amp;utm_content=readme&#34;&gt;SeamlessM4T v2 Demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://seamless.metademolab.com/expressive?utm_source=github&amp;amp;utm_medium=web&amp;amp;utm_campaign=seamless&amp;amp;utm_content=readme&#34;&gt;SeamlessExpressive Demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HuggingFace Space Demo&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/facebook/seamless-m4t-v2-large&#34;&gt;🤗 SeamlessM4T v2 Space&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/facebook/seamless-expressive&#34;&gt;🤗 SeamlessExpressive Space&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/facebook/seamless-streaming&#34;&gt;🤗 SeamlessStreaming Space&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;What&#39;s new&lt;/h2&gt; &#xA;&lt;h1&gt;Quick Start&lt;/h1&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] One of the prerequisites is &lt;a href=&#34;https://github.com/facebookresearch/fairseq2&#34;&gt;fairseq2&lt;/a&gt; which has pre-built packages available only for Linux x86-64 and Apple-silicon Mac computers. In addition it has a dependency on &lt;a href=&#34;https://github.com/libsndfile/libsndfile&#34;&gt;libsndfile&lt;/a&gt; which might not be installed on your machine. If you experience any installation issues, please refer to its &lt;a href=&#34;https://github.com/facebookresearch/fairseq2&#34;&gt;README&lt;/a&gt; for further instructions.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] Transcribing inference audio for computing metric uses &lt;a href=&#34;https://github.com/openai/whisper#setup&#34;&gt;Whisper&lt;/a&gt;, which is automatically installed. Whisper in turn requires the command-line tool &lt;a href=&#34;https://ffmpeg.org/&#34;&gt;&lt;code&gt;ffmpeg&lt;/code&gt;&lt;/a&gt; to be installed on your system, which is available from most package managers.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Running inference&lt;/h2&gt; &#xA;&lt;h3&gt;SeamlessM4T Inference&lt;/h3&gt; &#xA;&lt;p&gt;Here’s an example of using the CLI from the root directory to run inference.&lt;/p&gt; &#xA;&lt;p&gt;S2ST task:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;m4t_predict &amp;lt;path_to_input_audio&amp;gt; --task s2st --tgt_lang &amp;lt;tgt_lang&amp;gt; --output_path &amp;lt;path_to_save_audio&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;T2TT task:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;m4t_predict &amp;lt;input_text&amp;gt; --task t2tt --tgt_lang &amp;lt;tgt_lang&amp;gt; --src_lang &amp;lt;src_lang&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/src/seamless_communication/cli/m4t/predict&#34;&gt;inference README&lt;/a&gt; for detailed instruction on how to run inference and the list of supported languages on the source, target sides for speech, text modalities.&lt;/p&gt; &#xA;&lt;p&gt;For running S2TT/ASR natively (without Python) using GGML, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/#unitycpp&#34;&gt;the unity.cpp section&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;SeamlessExpressive Inference&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] Please check the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/#seamlessexpressive-models&#34;&gt;section&lt;/a&gt; on how to download the model.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Below is the script for efficient batched inference.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export MODEL_DIR=&#34;/path/to/SeamlessExpressive/model&#34;&#xA;export TEST_SET_TSV=&#34;input.tsv&#34; # Your dataset in a TSV file, with headers &#34;id&#34;, &#34;audio&#34;&#xA;export TGT_LANG=&#34;spa&#34; # Target language to translate into, options including &#34;fra&#34;, &#34;deu&#34;, &#34;eng&#34; (&#34;cmn&#34; and &#34;ita&#34; are experimental)&#xA;export OUTPUT_DIR=&#34;tmp/&#34; # Output directory for generated text/unit/waveform&#xA;export TGT_TEXT_COL=&#34;tgt_text&#34; # The column in your ${TEST_SET_TSV} for reference target text to calcuate BLEU score. You can skip this argument.&#xA;export DFACTOR=&#34;1.0&#34; # Duration factor for model inference to tune predicted duration (preddur=DFACTOR*preddur) per each position which affects output speech rate. Greater value means slower speech rate (default to 1.0). See expressive evaluation README for details on duration factor we used.&#xA;python src/seamless_communication/cli/expressivity/evaluate/pretssel_inference.py \&#xA;  ${TEST_SET_TSV} --gated-model-dir ${MODEL_DIR} --task s2st --tgt_lang ${TGT_LANG}\&#xA;  --audio_root_dir &#34;&#34; --output_path ${OUTPUT_DIR} --ref_field ${TGT_TEXT_COL} \&#xA;  --model_name seamless_expressivity --vocoder_name vocoder_pretssel \&#xA;  --text_unk_blocking True --duration_factor ${DFACTOR}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;SeamlessStreaming and Seamless Inference&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/src/seamless_communication/cli/streaming&#34;&gt;Streaming Evaluation README&lt;/a&gt; has detailed instructions for running evaluations for the SeamlessStreaming and Seamless models. The CLI has an &lt;code&gt;--no-scoring&lt;/code&gt; option that can be used to skip the scoring part and just run inference.&lt;/p&gt; &#xA;&lt;h2&gt;Running SeamlessStreaming Demo&lt;/h2&gt; &#xA;&lt;p&gt;You can duplicate the &lt;a href=&#34;https://huggingface.co/spaces/facebook/seamless-streaming?duplicate=true&#34;&gt;SeamlessStreaming HF space&lt;/a&gt; to run the streaming demo.&lt;/p&gt; &#xA;&lt;p&gt;You can also run the demo locally, by cloning the space from &lt;a href=&#34;https://huggingface.co/spaces/facebook/seamless-streaming/tree/main&#34;&gt;here&lt;/a&gt;. See the &lt;a href=&#34;https://huggingface.co/spaces/facebook/seamless-streaming/blob/main/README.md&#34;&gt;README&lt;/a&gt; of the SeamlessStreaming HF repo for more details on installation.&lt;/p&gt; &#xA;&lt;h2&gt;Running SeamlessM4T &amp;amp; SeamlessExpressive &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt; demos locally&lt;/h2&gt; &#xA;&lt;p&gt;To launch the same demo Space we host on Hugging Face locally:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd demo&#xA;pip install -r requirements.txt&#xA;python app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Seamless M4T is also available in the 🤗 Transformers library. For more details, refer to the &lt;a href=&#34;https://huggingface.co/docs/transformers/main/en/model_doc/seamless_m4t_v2&#34;&gt;SeamlessM4T docs&lt;/a&gt; or this hands-on &lt;a href=&#34;https://colab.research.google.com/github/ylacombe/explanatory_notebooks/blob/main/seamless_m4t_hugging_face.ipynb&#34;&gt;Google Colab&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Resources and usage&lt;/h1&gt; &#xA;&lt;h2&gt;Model&lt;/h2&gt; &#xA;&lt;h3&gt;SeamlessM4T models&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model Name&lt;/th&gt; &#xA;   &lt;th&gt;#params&lt;/th&gt; &#xA;   &lt;th&gt;checkpoint&lt;/th&gt; &#xA;   &lt;th&gt;metrics&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SeamlessM4T-Large v2&lt;/td&gt; &#xA;   &lt;td&gt;2.3B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/facebook/seamless-m4t-v2-large&#34;&gt;🤗 Model card&lt;/a&gt; - &lt;a href=&#34;https://huggingface.co/facebook/seamless-m4t-v2-large/resolve/main/seamlessM4T_v2_large.pt&#34;&gt;checkpoint&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/seamless/metrics/seamlessM4T_large_v2.zip&#34;&gt;metrics&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SeamlessM4T-Large (v1)&lt;/td&gt; &#xA;   &lt;td&gt;2.3B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/facebook/seamless-m4t-large&#34;&gt;🤗 Model card&lt;/a&gt; - &lt;a href=&#34;https://huggingface.co/facebook/seamless-m4t-large/resolve/main/multitask_unity_large.pt&#34;&gt;checkpoint&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/seamless/metrics/seamlessM4T_large.zip&#34;&gt;metrics&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SeamlessM4T-Medium (v1)&lt;/td&gt; &#xA;   &lt;td&gt;1.2B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/facebook/seamless-m4t-medium&#34;&gt;🤗 Model card&lt;/a&gt; - &lt;a href=&#34;https://huggingface.co/facebook/seamless-m4t-medium/resolve/main/multitask_unity_medium.pt&#34;&gt;checkpoint&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/seamless/metrics/seamlessM4T_medium.zip&#34;&gt;metrics&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;SeamlessExpressive models&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/facebook/seamless-expressive&#34;&gt;🤗 Model card&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;To access and download SeamlessExpressive, please request the model artifacts through &lt;a href=&#34;https://ai.meta.com/resources/models-and-libraries/seamless-downloads/&#34;&gt;this request form&lt;/a&gt;. Upon approval, you will then receive an email with download links to each model artifact.&lt;/p&gt; &#xA;&lt;p&gt;Please note that SeamlessExpressive is made available under its own &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/SEAMLESS_LICENSE&#34;&gt;License&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/ACCEPTABLE_USE_POLICY&#34;&gt;Acceptable Use Policy&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;SeamlessStreaming models&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model Name&lt;/th&gt; &#xA;   &lt;th&gt;#params&lt;/th&gt; &#xA;   &lt;th&gt;checkpoint&lt;/th&gt; &#xA;   &lt;th&gt;metrics&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SeamlessStreaming&lt;/td&gt; &#xA;   &lt;td&gt;2.5B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/facebook/seamless-streaming&#34;&gt;🤗 Model card&lt;/a&gt; - &lt;a href=&#34;https://huggingface.co/facebook/seamless-streaming/resolve/main/seamless_streaming_monotonic_decoder.pt&#34;&gt;monotonic decoder checkpoint&lt;/a&gt; - &lt;a href=&#34;https://huggingface.co/facebook/seamless-streaming/resolve/main/seamless_streaming_unity.pt&#34;&gt;streaming UnitY2 checkpoint&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/seamless/metrics/streaming/seamless_streaming.zip&#34;&gt;metrics&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Seamless models&lt;/h3&gt; &#xA;&lt;p&gt;Seamless model is simply the SeamlessStreaming model with the non-expressive &lt;code&gt;vocoder_v2&lt;/code&gt; swapped out with the expressive &lt;code&gt;vocoder_pretssel&lt;/code&gt;. Please check out above &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/#seamlessexpressive-models&#34;&gt;section&lt;/a&gt; on how to acquire &lt;code&gt;vocoder_pretssel&lt;/code&gt; checkpoint.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;h3&gt;SeamlessM4T Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;To reproduce our results, or to evaluate using the same metrics over your own test sets, please check out the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/src/seamless_communication/cli/m4t/evaluate&#34;&gt;README here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;SeamlessExpressive Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;Please check out this &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/docs/expressive/README.md#automatic-evaluation&#34;&gt;README section&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;SeamlessStreaming and Seamless Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/src/seamless_communication/cli/streaming&#34;&gt;Streaming Evaluation README&lt;/a&gt; has detailed instructions for running evaluations on the SeamlessStreaming and Seamless models.&lt;/p&gt; &#xA;&lt;h2&gt;Unity.cpp&lt;/h2&gt; &#xA;&lt;p&gt;To enable Seamless Communication Everywhere, we implemented unity.cpp so users could run SeamlessM4T models in GGML - a C tensor library allowing easier integration on verbose platforms.&lt;/p&gt; &#xA;&lt;p&gt;To transcribe/translte a given audio,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./ggml/bin/unity --model seamlessM4T_medium.ggml input.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For details of build and more usage please check out &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/ggml&#34;&gt;unity.cpp&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Expressive Datasets&lt;/h2&gt; &#xA;&lt;p&gt;We created two expressive speech-to-speech translation datasets, mExpresso and mDRAL, between English and five other languages -- French, German, Italian, Mandarin and Spanish. We currently open source the speech-to-text of mExpresso for out-of-English directions, and we will open source the remaining part of the datasets soon. For details, please check out &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/docs/expressive/README.md#benchmark-datasets&#34;&gt;README&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;SeamlessAlignExpressive&lt;/h3&gt; &#xA;&lt;p&gt;We’re introducing the first expressive speech alignment procedure. Starting with raw data, the expressive alignment procedure automatically discovers pairs of audio segments sharing not only the same meaning, but the same overall expressivity. To showcase this procedure, we are making metadata available to create a benchmarking dataset called SeamlessAlignExpressive, that can be used to validate the quality of our alignment method. SeamlessAlignExpressive is the first large-scale (11k+ hours) collection of multilingual audio alignments for expressive translation. More details can be found on the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/docs/expressive/seamless_align_expressive_README.md&#34;&gt;SeamlessAlignExpressive README&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Converting raw audio to units&lt;/h2&gt; &#xA;&lt;p&gt;Please check out the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/src/seamless_communication/cli/m4t/audio_to_units&#34;&gt;README here&lt;/a&gt;. Note that SeamlessM4T v1 model uses reduced units and other models use non-reduced units.&lt;/p&gt; &#xA;&lt;h1&gt;Libraries&lt;/h1&gt; &#xA;&lt;p&gt;Seamless Communication depends on 4 libraries developed by Meta.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/facebookresearch/fairseq2&#34;&gt;fairseq2&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;fairseq2 is our next-generation open-source library of sequence modeling components that provides researchers and developers with building blocks for machine translation, language modeling, and other sequence generation tasks. All SeamlessM4T models in this repository are powered by fairseq2.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/facebookresearch/SONAR&#34;&gt;SONAR and BLASER 2.0&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;SONAR, Sentence-level multimOdal and laNguage-Agnostic Representations is a new multilingual and -modal sentence embedding space which outperforms existing sentence embeddings such as LASER3 and LabSE on the xsim and xsim++ multilingual similarity search tasks. SONAR provides text and speech encoders for many languages. SeamlessAlign was mined based on SONAR embeddings.&lt;/p&gt; &#xA;&lt;p&gt;BLASER 2.0 is our latest model-based evaluation metric for multimodal translation. It is an extension of BLASER, supporting both speech and text. It operates directly on the source signal, and as such, does not require any intermediate ASR system like ASR-BLEU. As in the first version, BLASER 2.0 leverages the similarity between input and output sentence embeddings. SONAR is the underlying embedding space for BLASER 2.0. Scripts to run evaluation with BLASER 2.0 can be found in the &lt;a href=&#34;https://github.com/facebookresearch/SONAR&#34;&gt;SONAR repo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/facebookresearch/stopes&#34;&gt;stopes&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;As part of the seamless communication project, we&#39;ve extended the stopes library. Version 1 provided a text-to-text mining tool to build training dataset for translation models. Version 2 has been extended thanks to SONAR, to support tasks around training large speech translation models. In particular, we provide tools to read/write the fairseq audiozip datasets and a new mining pipeline that can do speech-to-speech, text-to-speech, speech-to-text and text-to-text mining, all based on the new SONAR embedding space.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/facebookresearch/SimulEval&#34;&gt;SimulEval&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;SimulEval is a library used for evaluating simulaneous translation models. SimulEval also provides a backend for generation using partial/incremental inputs with flexible/extensible states, which is used to implement streaming inference. Users define agents which implement SimulEval&#39;s interface, which can be connected together in a pipeline. You can find agents implemented for SeamlessStreaming &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/src/seamless_communication/streaming/agents&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;[Legacy] SeamlessM4T v1 instructions&lt;/h2&gt; &#xA;&lt;h4&gt;Finetuning SeamlessM4T v1 models&lt;/h4&gt; &#xA;&lt;p&gt;Please check out the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/src/seamless_communication/cli/m4t/finetune&#34;&gt;README here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;On-device models&lt;/h4&gt; &#xA;&lt;p&gt;Apart from Seamless-M4T large (2.3B) and medium (1.2B) models, we are also releasing a small model (281M) targeted for on-device inference. To learn more about the usage and model details check out the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/docs/m4t/on_device_README.md&#34;&gt;README here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;SeamlessAlign mined dataset&lt;/h4&gt; &#xA;&lt;p&gt;We open-source the metadata to SeamlessAlign, the largest open dataset for multimodal translation, totaling 270k+ hours of aligned Speech and Text data. The dataset can be rebuilt by the community based on the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/docs/m4t/seamless_align_README.md&#34;&gt;SeamlessAlign readme&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Citation&lt;/h1&gt; &#xA;&lt;p&gt;If you use Seamless in your work or any models/datasets/artifacts published in Seamless, please cite :&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{seamless2023,&#xA;   title=&#34;Seamless: Multilingual Expressive and Streaming Speech Translation&#34;,&#xA;   author=&#34;{Seamless Communication}, Lo{\&#34;i}c Barrault, Yu-An Chung, Mariano Coria Meglioli, David Dale, Ning Dong, Mark Duppenthaler, Paul-Ambroise Duquenne, Brian Ellis, Hady Elsahar, Justin Haaheim, John Hoffman, Min-Jae Hwang, Hirofumi Inaguma, Christopher Klaiber, Ilia Kulikov, Pengwei Li, Daniel Licht, Jean Maillard, Ruslan Mavlyutov, Alice Rakotoarison, Kaushik Ram Sadagopan, Abinesh Ramakrishnan, Tuan Tran, Guillaume Wenzek, Yilin Yang, Ethan Ye, Ivan Evtimov, Pierre Fernandez, Cynthia Gao, Prangthip Hansanti, Elahe Kalbassi, Amanda Kallet, Artyom Kozhevnikov, Gabriel Mejia, Robin San Roman, Christophe Touret, Corinne Wong, Carleigh Wood, Bokai Yu, Pierre Andrews, Can Balioglu, Peng-Jen Chen, Marta R. Costa-juss{\`a}, Maha Elbayad, Hongyu Gong, Francisco Guzm{\&#39;a}n, Kevin Heffernan, Somya Jain, Justine Kao, Ann Lee, Xutai Ma, Alex Mourachko, Benjamin Peloquin, Juan Pino, Sravya Popuri, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, Anna Sun, Paden Tomasello, Changhan Wang, Jeff Wang, Skyler Wang, Mary Williamson&#34;,&#xA;  journal={ArXiv},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;We have three license categories.&lt;/p&gt; &#xA;&lt;p&gt;The following non-generative components are MIT licensed as found in &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/MIT_LICENSE&#34;&gt;MIT_LICENSE&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Code&lt;/li&gt; &#xA; &lt;li&gt;Text only part of the mExpresso dataset found in the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/docs/expressive/README.md&#34;&gt;SeamlessExpressive README&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;UnitY2 forced alignment extractor found in the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/docs/m4t/unity2_aligner_README.md&#34;&gt;UnitY2 Aligner README&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Speech toxicity tool with the etox dataset found in the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/src/seamless_communication/cli/toxicity&#34;&gt;Toxicity README&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The following models are CC-BY-NC 4.0 licensed as found in the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;SeamlessM4T models (v1 and v2).&lt;/li&gt; &#xA; &lt;li&gt;SeamlessStreaming models.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The following models are Seamless licensed as found in &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/SEAMLESS_LICENSE&#34;&gt;SEAMLESS_LICENSE&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Seamless models.&lt;/li&gt; &#xA; &lt;li&gt;SeamlessExpressive models.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>earlephilhower/arduino-pico</title>
    <updated>2023-12-10T01:50:58Z</updated>
    <id>tag:github.com,2023-12-10:/earlephilhower/arduino-pico</id>
    <link href="https://github.com/earlephilhower/arduino-pico" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Raspberry Pi Pico Arduino core, for all RP2040 boards&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Arduino-Pico&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/earlephilhower/arduino-pico/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/earlephilhower/arduino-pico?style=plastic&#34; alt=&#34;Release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitter.im/arduino-pico/community&#34;&gt;&lt;img src=&#34;https://img.shields.io/gitter/room/earlephilhower/arduino-pico?style=plastic&#34; alt=&#34;Gitter&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Raspberry Pi Pico Arduino core, for all RP2040 boards&lt;/p&gt; &#xA;&lt;p&gt;This is a port of the RP2040 (Raspberry Pi Pico processor) to the Arduino ecosystem. It uses the bare Raspberry Pi Pico SDK and a custom GCC 12.3/Newlib 4.0 toolchain.&lt;/p&gt; &#xA;&lt;h1&gt;Documentation&lt;/h1&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://arduino-pico.readthedocs.io/en/latest/&#34;&gt;https://arduino-pico.readthedocs.io/en/latest/&lt;/a&gt; along with the examples for more detailed usage information.&lt;/p&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;Read the &lt;a href=&#34;https://github.com/earlephilhower/arduino-pico/raw/master/docs/contrib.rst&#34;&gt;Contributing Guide&lt;/a&gt; for more information on submitting pull requests and porting libraries or sketches to this core.&lt;/p&gt; &#xA;&lt;h1&gt;Supported Boards&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Raspberry Pi Pico&lt;/li&gt; &#xA; &lt;li&gt;Raspberry Pi Pico W&lt;/li&gt; &#xA; &lt;li&gt;0xCB Helios&lt;/li&gt; &#xA; &lt;li&gt;Adafruit Feather RP2040&lt;/li&gt; &#xA; &lt;li&gt;Adafruit Feather RP2040 SCORPIO&lt;/li&gt; &#xA; &lt;li&gt;Adafruit ItsyBitsy RP2040&lt;/li&gt; &#xA; &lt;li&gt;Adafruit KB2040&lt;/li&gt; &#xA; &lt;li&gt;Adafruit Macropad RP2040&lt;/li&gt; &#xA; &lt;li&gt;Adafruit Metro RP2040&lt;/li&gt; &#xA; &lt;li&gt;Adafruit QTPy RP2040&lt;/li&gt; &#xA; &lt;li&gt;Adafruit STEMMA Friend RP2040&lt;/li&gt; &#xA; &lt;li&gt;Adafruit Trinkey RP2040 QT&lt;/li&gt; &#xA; &lt;li&gt;Arduino Nano RP2040 Connect&lt;/li&gt; &#xA; &lt;li&gt;ArtronShop RP2 Nano&lt;/li&gt; &#xA; &lt;li&gt;BridgeTek IDM2040-7A&lt;/li&gt; &#xA; &lt;li&gt;Cytron Maker Pi RP2040&lt;/li&gt; &#xA; &lt;li&gt;Cytron Maker Nano RP2040&lt;/li&gt; &#xA; &lt;li&gt;DatanoiseTV PicoADK+&lt;/li&gt; &#xA; &lt;li&gt;Degz Suibo RP2040&lt;/li&gt; &#xA; &lt;li&gt;DeRuiLab FlyBoard2040 Core&lt;/li&gt; &#xA; &lt;li&gt;DFRobot Beetle RP2040&lt;/li&gt; &#xA; &lt;li&gt;ElectronicCats Hunter Cat NFC&lt;/li&gt; &#xA; &lt;li&gt;ExtremeElectronics RC2040&lt;/li&gt; &#xA; &lt;li&gt;Invector Labs Challenger RP2040 WiFi&lt;/li&gt; &#xA; &lt;li&gt;Invector Labs Challenger RP2040 WiFi/BLE&lt;/li&gt; &#xA; &lt;li&gt;Invector Labs Challenger RP2040 WiFi6/BLE&lt;/li&gt; &#xA; &lt;li&gt;Invector Labs Challenger NB RP2040 WiFi&lt;/li&gt; &#xA; &lt;li&gt;Invector Labs Challenger RP2040 LTE&lt;/li&gt; &#xA; &lt;li&gt;Invector Labs Challenger RP2040 LoRa&lt;/li&gt; &#xA; &lt;li&gt;Invector Labs Challenger RP2040 SubGHz&lt;/li&gt; &#xA; &lt;li&gt;Invector Labs Challenger RP2040 SD/RTC&lt;/li&gt; &#xA; &lt;li&gt;Invector Labs Challenger RP2040 UWB&lt;/li&gt; &#xA; &lt;li&gt;Invector Labs RPICO32&lt;/li&gt; &#xA; &lt;li&gt;Melopero Cookie RP2040&lt;/li&gt; &#xA; &lt;li&gt;Melopero Shake RP2040&lt;/li&gt; &#xA; &lt;li&gt;Neko Systems BL2040 Mini&lt;/li&gt; &#xA; &lt;li&gt;nullbits Bit-C PRO&lt;/li&gt; &#xA; &lt;li&gt;Pimoroni PGA2040&lt;/li&gt; &#xA; &lt;li&gt;Pimoroni Plasma2040&lt;/li&gt; &#xA; &lt;li&gt;Pimoroni Tiny2040&lt;/li&gt; &#xA; &lt;li&gt;RAKwireless RAK11300&lt;/li&gt; &#xA; &lt;li&gt;Redscorp RP2040-Eins&lt;/li&gt; &#xA; &lt;li&gt;Redscorp RP2040-ProMini&lt;/li&gt; &#xA; &lt;li&gt;Sea-Picro&lt;/li&gt; &#xA; &lt;li&gt;Seeed Indicator RP2040&lt;/li&gt; &#xA; &lt;li&gt;Seeed XIAO RP2040&lt;/li&gt; &#xA; &lt;li&gt;Silicognition RP2040-Shim&lt;/li&gt; &#xA; &lt;li&gt;Solder Party RP2040 Stamp&lt;/li&gt; &#xA; &lt;li&gt;SparkFun ProMicro RP2040&lt;/li&gt; &#xA; &lt;li&gt;SparkFun Thing Plus RP2040&lt;/li&gt; &#xA; &lt;li&gt;uPesy RP2040 DevKit&lt;/li&gt; &#xA; &lt;li&gt;VCC-GND YD-RP2040&lt;/li&gt; &#xA; &lt;li&gt;Viyalab Mizu RP2040&lt;/li&gt; &#xA; &lt;li&gt;Waveshare RP2040 Zero&lt;/li&gt; &#xA; &lt;li&gt;Waveshare RP2040 One&lt;/li&gt; &#xA; &lt;li&gt;Waveshare RP2040 Plus&lt;/li&gt; &#xA; &lt;li&gt;Waveshare RP2040 LCD 0.96&lt;/li&gt; &#xA; &lt;li&gt;Waveshare RP2040 LCD 1.28&lt;/li&gt; &#xA; &lt;li&gt;WIZnet W5100S-EVB-Pico&lt;/li&gt; &#xA; &lt;li&gt;WIZnet W5500-EVB-Pico&lt;/li&gt; &#xA; &lt;li&gt;WIZnet WizFi360-EVB-Pico&lt;/li&gt; &#xA; &lt;li&gt;Generic (configurable flash, I/O pins)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Installing via Arduino Boards Manager&lt;/h1&gt; &#xA;&lt;h2&gt;Windows-specific Notes&lt;/h2&gt; &#xA;&lt;p&gt;Please do not use the Windows Store version of the actual Arduino application because it has issues detecting attached Pico boards. Use the &#34;Windows ZIP&#34; or plain &#34;Windows&#34; executable (EXE) download direct from &lt;a href=&#34;https://arduino.cc&#34;&gt;https://arduino.cc&lt;/a&gt;. and allow it to install any device drivers it suggests. Otherwise the Pico board may not be detected. Also, if trying out the 2.0 beta Arduino please install the release 1.8 version beforehand to ensure needed device drivers are present. (See #20 for more details.)&lt;/p&gt; &#xA;&lt;h2&gt;Linux-specific Notes&lt;/h2&gt; &#xA;&lt;p&gt;Installing Arduino using flatpak (often used by &#34;App Stores&#34; in various Linux distributions) will mean it has restricted access to the host. This might cause uploads to fail with error messages such as the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Scanning for RP2040 devices&#xA;...&#xA;No drive to deploy.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you encounter this, you will need to either install Arduino in a different manner, or override the flatpak sandboxing feature using the following command, then restarting Arduino.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;flatpak override --user --filesystem=host:ro cc.arduino.IDE2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Open up the Arduino IDE and go to File-&amp;gt;Preferences.&lt;/p&gt; &#xA;&lt;p&gt;In the dialog that pops up, enter the following URL in the &#34;Additional Boards Manager URLs&#34; field:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/earlephilhower/arduino-pico/releases/download/global/package_rp2040_index.json&#34;&gt;https://github.com/earlephilhower/arduino-pico/releases/download/global/package_rp2040_index.json&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/11875/111917251-3c57f400-8a3c-11eb-8120-810a8328ab3f.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Hit OK to close the dialog.&lt;/p&gt; &#xA;&lt;p&gt;Go to Tools-&amp;gt;Boards-&amp;gt;Board Manager in the IDE&lt;/p&gt; &#xA;&lt;p&gt;Type &#34;pico&#34; in the search box and select &#34;Add&#34;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/11875/111917223-12063680-8a3c-11eb-8884-4f32b8f0feb1.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Installing via GIT&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Windows Users:&lt;/strong&gt; Before installing via &lt;code&gt;git&lt;/code&gt; on Windows, please read and follow the directions in &lt;a href=&#34;https://arduino-pico.readthedocs.io/en/latest/platformio.html#important-steps-for-windows-users-before-installing&#34;&gt;this link&lt;/a&gt;. If Win32 long paths are not enabled, and &lt;code&gt;git&lt;/code&gt; not configured to use them then there may be errors when attempting to clone the submodules.&lt;/p&gt; &#xA;&lt;p&gt;To install via GIT (for latest and greatest versions):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mkdir -p ~/Arduino/hardware/pico&#xA;git clone https://github.com/earlephilhower/arduino-pico.git ~/Arduino/hardware/pico/rp2040&#xA;cd ~/Arduino/hardware/pico/rp2040&#xA;git submodule update --init&#xA;cd pico-sdk&#xA;git submodule update --init&#xA;cd ../tools&#xA;python3 ./get.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Installing both Arduino and CMake&lt;/h1&gt; &#xA;&lt;p&gt;Tom&#39;s Hardware presented a very nice writeup on installing &lt;code&gt;arduino-pico&lt;/code&gt; on both Windows and Linux, available at &lt;a href=&#34;https://www.tomshardware.com/how-to/program-raspberry-pi-pico-with-arduino-ide&#34;&gt;https://www.tomshardware.com/how-to/program-raspberry-pi-pico-with-arduino-ide&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you follow Les&#39; step-by-step you will also have a fully functional &lt;code&gt;CMake&lt;/code&gt;-based environment to build Pico apps on if you outgrow the Arduino ecosystem.&lt;/p&gt; &#xA;&lt;h1&gt;Uploading Sketches&lt;/h1&gt; &#xA;&lt;p&gt;To upload your first sketch, you will need to hold the BOOTSEL button down while plugging in the Pico to your computer. Then hit the upload button and the sketch should be transferred and start to run.&lt;/p&gt; &#xA;&lt;p&gt;After the first upload, this should not be necessary as the &lt;code&gt;arduino-pico&lt;/code&gt; core has auto-reset support. Select the appropriate serial port shown in the Arduino Tools-&amp;gt;Port-&amp;gt;Serial Port menu once (this setting will stick and does not need to be touched for multiple uploads). This selection allows the auto-reset tool to identify the proper device to reset. Them hit the upload button and your sketch should upload and run.&lt;/p&gt; &#xA;&lt;p&gt;In some cases the Pico will encounter a hard hang and its USB port will not respond to the auto-reset request. Should this happen, just follow the initial procedure of holding the BOOTSEL button down while plugging in the Pico to enter the ROM bootloader.&lt;/p&gt; &#xA;&lt;h1&gt;Uploading Filesystem Images&lt;/h1&gt; &#xA;&lt;p&gt;The onboard flash filesystem for the Pico, LittleFS, lets you upload a filesystem image from the sketch directory for your sketch to use. Download the needed plugin from&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/earlephilhower/arduino-pico-littlefs-plugin/releases&#34;&gt;https://github.com/earlephilhower/arduino-pico-littlefs-plugin/releases&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To install, follow the directions in&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/earlephilhower/arduino-pico-littlefs-plugin/raw/master/README.md&#34;&gt;https://github.com/earlephilhower/arduino-pico-littlefs-plugin/blob/master/README.md&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For detailed usage information, please check the ESP8266 repo documentation (ignore SPIFFS related notes) available at&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arduino-esp8266.readthedocs.io/en/latest/filesystem.html&#34;&gt;https://arduino-esp8266.readthedocs.io/en/latest/filesystem.html&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Uploading Sketches with Picoprobe&lt;/h1&gt; &#xA;&lt;p&gt;If you have built a Raspberry Pi Picoprobe, you can use OpenOCD to handle your sketch uploads and for debugging with GDB.&lt;/p&gt; &#xA;&lt;p&gt;Under Windows a local admin user should be able to access the Picoprobe port automatically, but under Linux &lt;code&gt;udev&lt;/code&gt; must be told about the device and to allow normal users access.&lt;/p&gt; &#xA;&lt;p&gt;To set up user-level access to Picoprobes on Ubuntu (and other OSes which use &lt;code&gt;udev&lt;/code&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;echo &#39;SUBSYSTEMS==&#34;usb&#34;, ATTRS{idVendor}==&#34;2e8a&#34;, ATTRS{idProduct}==&#34;0004&#34;, GROUP=&#34;users&#34;, MODE=&#34;0666&#34;&#39; | sudo tee -a /etc/udev/rules.d/98-PicoProbe.rules&#xA;sudo udevadm control --reload&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The first line creates a file with the USB vendor and ID of the Picoprobe and tells UDEV to give users full access to it. The second causes &lt;code&gt;udev&lt;/code&gt; to load this new rule. Note that you will need to unplug and re-plug in your device the first time you create this file, to allow udev to make the device node properly.&lt;/p&gt; &#xA;&lt;p&gt;Once Picoprobe permissions are set up properly, then select the board &#34;Raspberry Pi Pico (Picoprobe)&#34; in the Tools menu and upload as normal.&lt;/p&gt; &#xA;&lt;h1&gt;Uploading Sketches with pico-debug&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/majbthrd/pico-debug/&#34;&gt;pico-debug&lt;/a&gt; differs from Picoprobe in that pico-debug is a virtual debug pod that runs side-by-side on the same RP2040 that you run your code on; so, you only need one RP2040 board instead of two. pico-debug also differs from Picoprobe in that pico-debug is standards-based; it uses the CMSIS-DAP protocol, which means even software not specially written for the Raspberry Pi Pico can support it. pico-debug uses OpenOCD to handle your sketch uploads, and debugging can be accomplished with CMSIS-DAP capable debuggers including GDB.&lt;/p&gt; &#xA;&lt;p&gt;Under Windows and macOS, any user should be able to access pico-debug automatically, but under Linux &lt;code&gt;udev&lt;/code&gt; must be told about the device and to allow normal users access.&lt;/p&gt; &#xA;&lt;p&gt;To set up user-level access to all CMSIS-DAP adapters on Ubuntu (and other OSes which use &lt;code&gt;udev&lt;/code&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;echo &#39;ATTRS{product}==&#34;*CMSIS-DAP*&#34;, MODE=&#34;664&#34;, GROUP=&#34;plugdev&#34;&#39; | sudo tee -a /etc/udev/rules.d/98-CMSIS-DAP.rules&#xA;sudo udevadm control --reload&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The first line creates a file that recognizes all CMSIS-DAP adapters and tells UDEV to give users full access to it. The second causes &lt;code&gt;udev&lt;/code&gt; to load this new rule. Note that you will need to unplug and re-plug in your device the first time you create this file, to allow udev to make the device node properly.&lt;/p&gt; &#xA;&lt;p&gt;Once CMSIS-DAP permissions are set up properly, then select the board &#34;Raspberry Pi Pico (pico-debug)&#34; in the Tools menu.&lt;/p&gt; &#xA;&lt;p&gt;When first connecting the USB port to your PC, you must copy &lt;a href=&#34;https://github.com/majbthrd/pico-debug/releases/&#34;&gt;pico-debug-gimmecache.uf2&lt;/a&gt; to the Pi Pico to load pico-debug into RAM; after this, upload as normal.&lt;/p&gt; &#xA;&lt;h1&gt;Debugging with Picoprobe/pico-debug, OpenOCD, and GDB&lt;/h1&gt; &#xA;&lt;p&gt;The installed tools include a version of OpenOCD (in the pqt-openocd directory) and GDB (in the pqt-gcc directory). These may be used to run GDB in an interactive window as documented in the Pico Getting Started manuals from the Raspberry Pi Foundation. For &lt;a href=&#34;https://github.com/majbthrd/pico-debug/&#34;&gt;pico-debug&lt;/a&gt;, replace the raspberrypi-swd and picoprobe example OpenOCD arguments of &#34;-f interface/raspberrypi-swd.cfg -f target/rp2040.cfg&#34; or &#34;-f interface/picoprobe.cfg -f target/rp2040.cfg&#34; respectively in the Pico Getting Started manual with &#34;-f board/pico-debug.cfg&#34;.&lt;/p&gt; &#xA;&lt;h1&gt;Features&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Adafruit TinyUSB Arduino (USB mouse, keyboard, flash drive, generic HID, CDC Serial, MIDI, WebUSB, others)&lt;/li&gt; &#xA; &lt;li&gt;Bluetooth on the PicoW (Classic and BLE) with Keyboard, Mouse, Joystick, and Virtual Serial&lt;/li&gt; &#xA; &lt;li&gt;Generic Arduino USB Serial, Keyboard, Joystick, and Mouse emulation&lt;/li&gt; &#xA; &lt;li&gt;WiFi (Pico W)&lt;/li&gt; &#xA; &lt;li&gt;Ethernet (Wired W5500, W5100, ENC28J60)&lt;/li&gt; &#xA; &lt;li&gt;HTTP client and server (WebServer)&lt;/li&gt; &#xA; &lt;li&gt;SSL/TLS/HTTPS&lt;/li&gt; &#xA; &lt;li&gt;Over-the-Air (OTA) upgrades&lt;/li&gt; &#xA; &lt;li&gt;Filesystems (LittleFS and SD/SDFS)&lt;/li&gt; &#xA; &lt;li&gt;Multicore support (setup1() and loop1())&lt;/li&gt; &#xA; &lt;li&gt;FreeRTOS SMP support&lt;/li&gt; &#xA; &lt;li&gt;Overclocking and underclocking from the menus&lt;/li&gt; &#xA; &lt;li&gt;digitalWrite/Read, shiftIn/Out, tone, analogWrite(PWM)/Read, temperature&lt;/li&gt; &#xA; &lt;li&gt;Analog stereo audio in using DMA and the built-in ADC&lt;/li&gt; &#xA; &lt;li&gt;Analog stereo audio out using PWM hardware&lt;/li&gt; &#xA; &lt;li&gt;USB drive mode for data loggers (SingleFileDrive)&lt;/li&gt; &#xA; &lt;li&gt;Peripherals: SPI master/slave, Wire(I2C) master/slave, dual UART, emulated EEPROM, I2S audio input/output, Servo&lt;/li&gt; &#xA; &lt;li&gt;printf (i.e. debug) output over USB serial&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The RP2040 PIO state machines (SMs) are used to generate jitter-free:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Servos&lt;/li&gt; &#xA; &lt;li&gt;Tones&lt;/li&gt; &#xA; &lt;li&gt;I2S Input&lt;/li&gt; &#xA; &lt;li&gt;I2S Output&lt;/li&gt; &#xA; &lt;li&gt;Software UARTs (Serial ports)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Tutorials from Across the Web&lt;/h1&gt; &#xA;&lt;p&gt;Here are some links to coverage and additional tutorials for using &lt;code&gt;arduino-pico&lt;/code&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The File:: class is taken from the ESP8266. See &lt;a href=&#34;https://arduino-esp8266.readthedocs.io/en/latest/filesystem.html&#34;&gt;https://arduino-esp8266.readthedocs.io/en/latest/filesystem.html&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Arduino Support for the Pi Pico available! And how fast is the Pico? - &lt;a href=&#34;https://youtu.be/-XHh17cuH5E&#34;&gt;https://youtu.be/-XHh17cuH5E&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Pre-release Adafruit QT Py RP2040 - &lt;a href=&#34;https://www.youtube.com/watch?v=sfC1msqXX0I&#34;&gt;https://www.youtube.com/watch?v=sfC1msqXX0I&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Adafruit Feather RP2040 running LCD + TMP117 - &lt;a href=&#34;https://www.youtube.com/watch?v=fKDeqZiIwHg&#34;&gt;https://www.youtube.com/watch?v=fKDeqZiIwHg&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Demonstration of Servos and I2C in Korean - &lt;a href=&#34;https://cafe.naver.com/arduinoshield/1201&#34;&gt;https://cafe.naver.com/arduinoshield/1201&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Home Assistant Pico W integration starter project using Arduino - &lt;a href=&#34;https://github.com/daniloc/PicoW_HomeAssistant_Starter&#34;&gt;https://github.com/daniloc/PicoW_HomeAssistant_Starter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Tutorials for the Raspberry Pi Pico / uPesy RP2040 DevKit board &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;English version: &lt;a href=&#34;https://www.upesy.com/blogs/tutorials/best-tutorials-for-rpi-pi-pico-with-arduino-code&#34;&gt;https://www.upesy.com/blogs/tutorials/best-tutorials-for-rpi-pi-pico-with-arduino-code&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;French version: &lt;a href=&#34;https://www.upesy.fr/blogs/tutorials/best-tutorials-for-rpi-pi-pico-with-arduino-code&#34;&gt;https://www.upesy.fr/blogs/tutorials/best-tutorials-for-rpi-pi-pico-with-arduino-code&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;If you want to contribute or have bugfixes, drop me a note at &lt;a href=&#34;mailto:earlephilhower@yahoo.com&#34;&gt;earlephilhower@yahoo.com&lt;/a&gt; or open an issue/PR here.&lt;/p&gt; &#xA;&lt;h1&gt;Licensing and Credits&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://arduino.cc&#34;&gt;Arduino IDE and ArduinoCore-API&lt;/a&gt; are developed and maintained by the Arduino team. The IDE is licensed under GPL.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://github.com/earlephilhower/pico-quick-toolchain&#34;&gt;RP2040 GCC-based toolchain&lt;/a&gt; is licensed under under the GPL.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://github.com/raspberrypi/pico-sdk&#34;&gt;Pico-SDK&lt;/a&gt; is by Raspberry Pi (Trading) Ltd and licensed under the BSD 3-Clause license.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/earlephilhower/arduino-pico&#34;&gt;Arduino-Pico&lt;/a&gt; core files are licensed under the LGPL.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ARMmbed/littlefs&#34;&gt;LittleFS&lt;/a&gt; library written by ARM Limited and released under the &lt;a href=&#34;https://github.com/ARMmbed/littlefs/raw/master/LICENSE.md&#34;&gt;BSD 3-clause license&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/uf2&#34;&gt;UF2CONV.PY&lt;/a&gt; is by Microsoft Corporation and licensed under the MIT license.&lt;/li&gt; &#xA; &lt;li&gt;Networking and filesystem code taken from the &lt;a href=&#34;https://github.com/esp8266/Arduino&#34;&gt;ESP8266 Arduino Core&lt;/a&gt; and licensed under the LGPL.&lt;/li&gt; &#xA; &lt;li&gt;DHCP server for AP host mode from the &lt;a href=&#34;https://micropython.org&#34;&gt;Micropython Project&lt;/a&gt;, distributed under the MIT License.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://freertos.org&#34;&gt;FreeRTOS&lt;/a&gt; is copyright Amazon.com, Inc. or its affiliates, and distributed under the MIT license.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://savannah.nongnu.org/projects/lwip/&#34;&gt;lwIP&lt;/a&gt; is (c) the Swedish Institute of Computer Science and licenced under the BSD license.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://bearssl.org&#34;&gt;BearSSL&lt;/a&gt; library written by Thomas Pornin, is distributed under the &lt;a href=&#34;https://bearssl.org/#legal-details&#34;&gt;MIT License&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pfalcon/uzlib&#34;&gt;UZLib&lt;/a&gt; is copyright (c) 2003 Joergen Ibsen and distributed under the zlib license.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/LaborEtArs/ESP8266mDNS&#34;&gt;LEAmDNS&lt;/a&gt; is copyright multiple authors and distributed under the MIT license.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/nodejs/http-parser&#34;&gt;http-parser&lt;/a&gt; is copyright Joyent, Inc. and other Node contributors.&lt;/li&gt; &#xA; &lt;li&gt;WebServer code modified from the &lt;a href=&#34;https://github.com/espressif/arduino-esp32/tree/master/libraries/WebServer&#34;&gt;ESP32 WebServer&lt;/a&gt; and is copyright (c) 2015 Ivan Grokhotkov and others.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Reputeless/Xoshiro-cpp&#34;&gt;Xoshiro-cpp&lt;/a&gt; is copyright (c) 2020 Ryo Suzuki and distributed under the MIT license.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;-Earle F. Philhower, III&lt;br&gt; &lt;a href=&#34;mailto:earlephilhower@yahoo.com&#34;&gt;earlephilhower@yahoo.com&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>