<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-12-31T01:46:07Z</updated>
  <subtitle>Weekly Trending of C in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>WayneD/rsync</title>
    <updated>2024-12-31T01:46:07Z</updated>
    <id>tag:github.com,2024-12-31:/WayneD/rsync</id>
    <link href="https://github.com/WayneD/rsync" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An open source utility that provides fast incremental file transfer. It also has useful features for backup and restore operations among many other use cases.&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;WHAT IS RSYNC?&lt;/h2&gt; &#xA;&lt;p&gt;Rsync is a fast and extraordinarily versatile file copying tool for both remote and local files.&lt;/p&gt; &#xA;&lt;p&gt;Rsync uses a delta-transfer algorithm which provides a very fast method for bringing remote files into sync. It does this by sending just the differences in the files across the link, without requiring that both sets of files are present at one of the ends of the link beforehand. At first glance this may seem impossible because the calculation of diffs between two files normally requires local access to both files.&lt;/p&gt; &#xA;&lt;p&gt;A technical report describing the rsync algorithm is included with this package.&lt;/p&gt; &#xA;&lt;h2&gt;USAGE&lt;/h2&gt; &#xA;&lt;p&gt;Basically you use rsync just like scp, but rsync has many additional options. To get a complete list of supported options type:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;rsync --help&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://download.samba.org/pub/rsync/rsync.1&#34;&gt;manpage&lt;/a&gt; for more detailed information.&lt;/p&gt; &#xA;&lt;h2&gt;BUILDING AND INSTALLING&lt;/h2&gt; &#xA;&lt;p&gt;If you need to build rsync yourself, check out the &lt;a href=&#34;https://github.com/WayneD/rsync/raw/master/INSTALL.md&#34;&gt;INSTALL&lt;/a&gt; page for information on what libraries and packages you can use to get the maximum features in your build.&lt;/p&gt; &#xA;&lt;h2&gt;SETUP&lt;/h2&gt; &#xA;&lt;p&gt;Rsync normally uses ssh or rsh for communication with remote systems. It does not need to be setuid and requires no special privileges for installation. You must, however, have a working ssh or rsh system. Using ssh is recommended for its security features.&lt;/p&gt; &#xA;&lt;p&gt;Alternatively, rsync can run in `daemon&#39; mode, listening on a socket. This is generally used for public file distribution, although authentication and access control are available.&lt;/p&gt; &#xA;&lt;p&gt;To install rsync, first run the &#34;configure&#34; script. This will create a Makefile and config.h appropriate for your system. Then type &#34;make&#34;.&lt;/p&gt; &#xA;&lt;p&gt;Note that on some systems you will have to force configure not to use gcc because gcc may not support some features (such as 64 bit file offsets) that your system may support. Set the environment variable CC to the name of your native compiler before running configure in this case.&lt;/p&gt; &#xA;&lt;p&gt;Once built put a copy of rsync in your search path on the local and remote systems (or use &#34;make install&#34;). That&#39;s it!&lt;/p&gt; &#xA;&lt;h2&gt;RSYNC DAEMONS&lt;/h2&gt; &#xA;&lt;p&gt;Rsync can also talk to &#34;rsync daemons&#34; which can provide anonymous or authenticated rsync. See the rsyncd.conf(5) manpage for details on how to setup an rsync daemon. See the rsync(1) manpage for info on how to connect to an rsync daemon.&lt;/p&gt; &#xA;&lt;h2&gt;WEB SITE&lt;/h2&gt; &#xA;&lt;p&gt;For more information, visit the &lt;a href=&#34;https://rsync.samba.org/&#34;&gt;main rsync web site&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You&#39;ll find a FAQ list, downloads, resources, HTML versions of the manpages, etc.&lt;/p&gt; &#xA;&lt;h2&gt;MAILING LISTS&lt;/h2&gt; &#xA;&lt;p&gt;There is a mailing list for the discussion of rsync and its applications that is open to anyone to join. New releases are announced on this list, and there is also an announcement-only mailing list for those that want official announcements. See the &lt;a href=&#34;https://rsync.samba.org/lists.html&#34;&gt;mailing-list page&lt;/a&gt; for full details.&lt;/p&gt; &#xA;&lt;h2&gt;BUG REPORTS&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://rsync.samba.org/bug-tracking.html&#34;&gt;bug-tracking web page&lt;/a&gt; has full details on bug reporting.&lt;/p&gt; &#xA;&lt;p&gt;That page contains links to the current bug list, and information on how to do a good job when reporting a bug. You might also like to try searching the Internet for the error message you&#39;ve received, or looking in the &lt;a href=&#34;https://mail-archive.com/rsync@lists.samba.org/&#34;&gt;mailing list archives&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To send a bug report, follow the instructions on the bug-tracking page of the web site.&lt;/p&gt; &#xA;&lt;p&gt;Alternately, email your bug report to &lt;a href=&#34;mailto:rsync@lists.samba.org&#34;&gt;rsync@lists.samba.org&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;GIT REPOSITORY&lt;/h2&gt; &#xA;&lt;p&gt;If you want to get the very latest version of rsync direct from the source code repository, then you will need to use git. The git repo is hosted &lt;a href=&#34;https://github.com/WayneD/rsync&#34;&gt;on GitHub&lt;/a&gt; and &lt;a href=&#34;https://git.samba.org/?p=rsync.git;a=summary&#34;&gt;on Samba&#39;s site&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://rsync.samba.org/download.html&#34;&gt;the download page&lt;/a&gt; for full details on all the ways to grab the source.&lt;/p&gt; &#xA;&lt;h2&gt;COPYRIGHT&lt;/h2&gt; &#xA;&lt;p&gt;Rsync was originally written by Andrew Tridgell and is currently maintained by Wayne Davison. It has been improved by many developers from around the world.&lt;/p&gt; &#xA;&lt;p&gt;Rsync may be used, modified and redistributed only under the terms of the GNU General Public License, found in the file &lt;a href=&#34;https://github.com/WayneD/rsync/raw/master/COPYING&#34;&gt;COPYING&lt;/a&gt; in this distribution, or at &lt;a href=&#34;https://www.fsf.org/licenses/gpl.html&#34;&gt;the Free Software Foundation&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>SJTU-IPADS/PowerInfer</title>
    <updated>2024-12-31T01:46:07Z</updated>
    <id>tag:github.com,2024-12-31:/SJTU-IPADS/PowerInfer</id>
    <link href="https://github.com/SJTU-IPADS/PowerInfer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;High-speed Large Language Model Serving on PCs with Consumer-grade GPUs&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU&lt;/h1&gt; &#xA;&lt;h2&gt;TL;DR&lt;/h2&gt; &#xA;&lt;p&gt;PowerInfer is a CPU/GPU LLM inference engine leveraging &lt;strong&gt;activation locality&lt;/strong&gt; for your device.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/orgs/SJTU-IPADS/projects/2/views/2&#34;&gt;Project Kanban&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Latest News üî•&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2023/12/24] We released an online &lt;a href=&#34;https://powerinfer-gradio.vercel.app/&#34;&gt;gradio demo&lt;/a&gt; for Falcon(ReLU)-40B-FP16!&lt;/li&gt; &#xA; &lt;li&gt;[2023/12/19] We officially released PowerInfer!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Demo üî•&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/SJTU-IPADS/PowerInfer/assets/34213478/fe441a42-5fce-448b-a3e5-ea4abb43ba23&#34;&gt;https://github.com/SJTU-IPADS/PowerInfer/assets/34213478/fe441a42-5fce-448b-a3e5-ea4abb43ba23&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;PowerInfer v.s. llama.cpp on a single RTX 4090(24G) running Falcon(ReLU)-40B-FP16 with a 11x speedup!&lt;/p&gt; &#xA;&lt;p&gt;&lt;sub&gt;Both PowerInfer and llama.cpp were running on the same hardware and fully utilized VRAM on RTX 4090.&lt;/sub&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] &lt;strong&gt;Live Demo Online‚ö°Ô∏è&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;Try out our &lt;a href=&#34;https://powerinfer-gradio.vercel.app/&#34;&gt;Gradio server&lt;/a&gt; hosting Falcon(ReLU)-40B-FP16 on a RTX 4090!&lt;/p&gt; &#xA; &lt;p&gt;&lt;sub&gt;Experimental and without warranties üöß&lt;/sub&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Abstract&lt;/h2&gt; &#xA;&lt;p&gt;We introduce PowerInfer, a high-speed Large Language Model (LLM) inference engine on a personal computer (PC) equipped with a single consumer-grade GPU. The key underlying the design of PowerInfer is exploiting the high &lt;strong&gt;locality&lt;/strong&gt; inherent in LLM inference, characterized by a power-law distribution in neuron activation.&lt;/p&gt; &#xA;&lt;p&gt;This distribution indicates that a small subset of neurons, termed hot neurons, are consistently activated across inputs, while the majority, cold neurons, vary based on specific inputs. PowerInfer exploits such an insight to design a GPU-CPU hybrid inference engine: hot-activated neurons are preloaded onto the GPU for fast access, while cold-activated neurons are computed on the CPU, thus significantly reducing GPU memory demands and CPU-GPU data transfers. PowerInfer further integrates adaptive predictors and neuron-aware sparse operators, optimizing the efficiency of neuron activation and computational sparsity.&lt;/p&gt; &#xA;&lt;p&gt;Evaluation shows that PowerInfer attains an average token generation rate of 13.20 tokens/s, with a peak of 29.08 tokens/s, across various LLMs (including OPT-175B) on a single NVIDIA RTX 4090 GPU, only 18% lower than that achieved by a top-tier server-grade A100 GPU. This significantly outperforms llama.cpp by up to 11.69x while retaining model accuracy.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;PowerInfer is a high-speed and easy-to-use inference engine for deploying LLMs locally.&lt;/p&gt; &#xA;&lt;p&gt;PowerInfer is fast with:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Locality-centric design&lt;/strong&gt;: Utilizes sparse activation and &#39;hot&#39;/&#39;cold&#39; neuron concept for efficient LLM inference, ensuring high speed with lower resource demands.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hybrid CPU/GPU Utilization&lt;/strong&gt;: Seamlessly integrates memory/computation capabilities of CPU and GPU for a balanced workload and faster processing.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;PowerInfer is flexible and easy to use with:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Easy Integration&lt;/strong&gt;: Compatible with popular &lt;a href=&#34;https://huggingface.co/SparseLLM&#34;&gt;ReLU-sparse models&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Local Deployment Ease&lt;/strong&gt;: Designed and deeply optimized for local deployment on consumer-grade hardware, enabling low-latency LLM inference and serving on a single GPU.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Backward Compatibility&lt;/strong&gt;: While distinct from llama.cpp, you can make use of most of &lt;code&gt;examples/&lt;/code&gt; the same way as llama.cpp such as server and batched generation. PowerInfer also supports inference with llama.cpp&#39;s model weights for compatibility purposes, but there will be no performance gain.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can use these models with PowerInfer today:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Falcon-40B&lt;/li&gt; &#xA; &lt;li&gt;Llama2 family&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We have tested PowerInfer on the following platforms:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;x86-64 CPU (with AVX2 instructions) on Linux&lt;/li&gt; &#xA; &lt;li&gt;x86-64 CPU and NVIDIA GPU on Linux&lt;/li&gt; &#xA; &lt;li&gt;Apple M Chips on macOS (As we do not optimize for Mac, the performance improvement is not significant now.)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;And new features coming soon:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Mistral-7B model&lt;/li&gt; &#xA; &lt;li&gt;Metal backend for sparse inference on macOS&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please kindly refer to our &lt;a href=&#34;https://github.com/orgs/SJTU-IPADS/projects/2/views/2&#34;&gt;Project Kanban&lt;/a&gt; for our current focus of development.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SJTU-IPADS/PowerInfer/main/#setup-and-installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SJTU-IPADS/PowerInfer/main/#model-weights&#34;&gt;Model Weights&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup and Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Get the Code&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/SJTU-IPADS/PowerInfer&#xA;cd PowerInfer&#xA;pip install -r requirements.txt # install Python helpers&#39; dependencies&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Build&lt;/h3&gt; &#xA;&lt;p&gt;In order to build PowerInfer you have two different options. These commands are supposed to be run from the root directory of the project.&lt;/p&gt; &#xA;&lt;p&gt;Using &lt;code&gt;CMake&lt;/code&gt;(3.13+) on Linux or macOS:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you have an NVIDIA GPU:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cmake -S . -B build -DLLAMA_CUBLAS=ON&#xA;cmake --build build --config Release&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you just CPU:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cmake -S . -B build&#xA;cmake --build build --config Release&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Model Weights&lt;/h2&gt; &#xA;&lt;p&gt;PowerInfer models are stored in a special format called &lt;em&gt;PowerInfer GGUF&lt;/em&gt; based on GGUF format, consisting of both LLM weights and predictor weights.&lt;/p&gt; &#xA;&lt;h3&gt;Download PowerInfer GGUF via Hugging Face&lt;/h3&gt; &#xA;&lt;p&gt;You can obtain PowerInfer GGUF weights at &lt;code&gt;*.powerinfer.gguf&lt;/code&gt; as well as profiled model activation statistics for &#39;hot&#39;-neuron offloading from each Hugging Face repo below.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Base Model&lt;/th&gt; &#xA;   &lt;th&gt;PowerInfer GGUF&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA(ReLU)-2-7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/PowerInfer/ReluLLaMA-7B-PowerInfer-GGUF&#34;&gt;PowerInfer/ReluLLaMA-7B-PowerInfer-GGUF&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA(ReLU)-2-13B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/PowerInfer/ReluLLaMA-13B-PowerInfer-GGUF&#34;&gt;PowerInfer/ReluLLaMA-13B-PowerInfer-GGUF&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Falcon(ReLU)-40B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/PowerInfer/ReluFalcon-40B-PowerInfer-GGUF&#34;&gt;PowerInfer/ReluFalcon-40B-PowerInfer-GGUF&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA(ReLU)-2-70B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/PowerInfer/ReluLLaMA-70B-PowerInfer-GGUF&#34;&gt;PowerInfer/ReluLLaMA-70B-PowerInfer-GGUF&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;We suggest downloading/cloning the whole repo so PowerInfer can automatically make use of such directory structure for feature-complete model offloading:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;.&#xA;‚îú‚îÄ‚îÄ *.powerinfer.gguf (Unquantized PowerInfer model)&#xA;‚îú‚îÄ‚îÄ *.q4.powerinfer.gguf (INT4 quantized PowerInfer model, if available)&#xA;‚îú‚îÄ‚îÄ activation (Profiled activation statistics for fine-grained FFN offloading)&#xA;‚îÇ   ‚îú‚îÄ‚îÄ activation_x.pt (Profiled activation statistics for layer x)&#xA;‚îÇ   ‚îî‚îÄ‚îÄ ...&#xA;‚îú‚îÄ‚îÄ *.[q4].powerinfer.gguf.generated.gpuidx (Generated GPU index at runtime for corresponding model)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Convert from Original Model Weights + Predictor Weights&lt;/h3&gt; &#xA;&lt;p&gt;Hugging Face limits single model weight to 50GiB. For unquantized models &amp;gt;= 40B, you can convert PowerInfer GGUF from the original model weights and predictor weights obtained from Hugging Face.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Base Model&lt;/th&gt; &#xA;   &lt;th&gt;Original Model&lt;/th&gt; &#xA;   &lt;th&gt;Predictor&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA(ReLU)-2-7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/SparseLLM/ReluLLaMA-7B&#34;&gt;SparseLLM/ReluLLaMA-7B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/PowerInfer/ReluLLaMA-7B-Predictor&#34;&gt;PowerInfer/ReluLLaMA-7B-Predictor&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA(ReLU)-2-13B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/SparseLLM/ReluLLaMA-13B&#34;&gt;SparseLLM/ReluLLaMA-13B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/PowerInfer/ReluLLaMA-13B-Predictor&#34;&gt;PowerInfer/ReluLLaMA-13B-Predictor&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Falcon(ReLU)-40B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/SparseLLM/ReluFalcon-40B&#34;&gt;SparseLLM/ReluFalcon-40B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/PowerInfer/ReluFalcon-40B-Predictor&#34;&gt;PowerInfer/ReluFalcon-40B-Predictor&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA(ReLU)-2-70B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/SparseLLM/ReluLLaMA-70B&#34;&gt;SparseLLM/ReluLLaMA-70B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/PowerInfer/ReluLLaMA-70B-Predictor&#34;&gt;PowerInfer/ReluLLaMA-70B-Predictor&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;You can use the following command to convert the original model weights and predictor weights to PowerInfer GGUF:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# make sure that you have done `pip install -r requirements.txt`&#xA;python convert.py --outfile /PATH/TO/POWERINFER/GGUF/REPO/MODELNAME.powerinfer.gguf /PATH/TO/ORIGINAL/MODEL /PATH/TO/PREDICTOR&#xA;# python convert.py --outfile ./ReluLLaMA-70B-PowerInfer-GGUF/llama-70b-relu.powerinfer.gguf ./SparseLLM/ReluLLaMA-70B ./PowerInfer/ReluLLaMA-70B-Predictor&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For the same reason, we suggest keeping the same directory structure as PowerInfer GGUF repos after conversion.&lt;/p&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;For CPU-only and CPU-GPU hybrid inference with all available VRAM, you can use the following instructions to run PowerInfer:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./build/bin/main -m /PATH/TO/MODEL -n $output_token_count -t $thread_num -p $prompt&#xA;# ./build/bin/main -m ./ReluFalcon-40B-PowerInfer-GGUF/falcon-40b-relu.q4.powerinfer.gguf -n 128 -t 8 -p &#34;Once upon a time&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to limit the VRAM usage of GPU:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./build/bin/main -m /PATH/TO/MODEL -n $output_token_count -t $thread_num -p $prompt --vram-budget $vram_gb&#xA;# ./build/bin/main -m ./ReluLLaMA-7B-PowerInfer-GGUF/llama-7b-relu.powerinfer.gguf -n 128 -t 8 -p &#34;Once upon a time&#34; --vram-budget 8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Under CPU-GPU hybrid inference, PowerInfer will automatically offload all dense activation blocks to GPU, then split FFN and offload to GPU if possible.&lt;/p&gt; &#xA;&lt;h2&gt;Quantization&lt;/h2&gt; &#xA;&lt;p&gt;PowerInfer has optimized quantization support for INT4(&lt;code&gt;Q4_0&lt;/code&gt;) models. You can use the following instructions to quantize PowerInfer GGUF model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./build/bin/quantize /PATH/TO/MODEL /PATH/TO/OUTPUT/QUANTIZED/MODEL Q4_0&#xA;# ./build/bin/quantize ./ReluFalcon-40B-PowerInfer-GGUF/falcon-40b-relu.powerinfer.gguf ./ReluFalcon-40B-PowerInfer-GGUF/falcon-40b-relu.q4.powerinfer.gguf Q4_0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you can use the quantized model for inference with PowerInfer with the same instructions as above.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;We evaluated PowerInfer vs. llama.cpp on a single RTX 4090(24G) with a series of FP16 ReLU models under inputs of length 64, and the results are shown below. PowerInfer achieves up to 11x speedup on Falcon 40B and up to 3x speedup on Llama 2 70B.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/SJTU-IPADS/PowerInfer/assets/34213478/d700fa6c-77ba-462f-a2fc-3fd21c898f33&#34; alt=&#34;github-eval-4090&#34;&gt; &lt;sub&gt;The X axis indicates the output length, and the Y axis represents the speedup compared with llama.cpp. The number above each bar indicates the end-to-end generation speed (total prompting + generation time / total tokens generated, in tokens/s).&lt;/sub&gt;&lt;/p&gt; &#xA;&lt;p&gt;We also evaluated PowerInfer on a single RTX 2080Ti(11G) with INT4 ReLU models under inputs of length 8, and the results are illustrated in the same way as above. PowerInfer achieves up to 8x speedup on Falcon 40B and up to 3x speedup on Llama 2 70B.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/SJTU-IPADS/PowerInfer/assets/34213478/0fc1bfc4-aafc-4e82-a865-bec0143aff1a&#34; alt=&#34;github-eval-2080ti-q4&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Please refer to our &lt;a href=&#34;https://ipads.se.sjtu.edu.cn/_media/publications/powerinfer-20231219.pdf&#34;&gt;paper&lt;/a&gt; for more evaluation details.&lt;/p&gt; &#xA;&lt;h2&gt;FAQs&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;What if I encountered &lt;code&gt;CUDA_ERROR_OUT_OF_MEMORY&lt;/code&gt;?&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;You can try to run with &lt;code&gt;--reset-gpu-index&lt;/code&gt; argument to rebuild the GPU index for this model to avoid any stale cache.&lt;/li&gt; &#xA;   &lt;li&gt;Due to our current implementation, model offloading might not be as accurate as expected. You can try with &lt;code&gt;--vram-budget&lt;/code&gt; with a slightly lower value or &lt;code&gt;--disable-gpu-index&lt;/code&gt; to disable FFN offloading.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Does PowerInfer support mistral, original llama, Qwen, ...?&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Now we only support models with ReLU/ReGLU/Squared ReLU activation function. So we do not support these models now. It&#39;s worth mentioning that a &lt;a href=&#34;https://arxiv.org/pdf/2310.04564.pdf&#34;&gt;paper&lt;/a&gt; has demonstrated that using the ReLU/ReGLU activation function has a negligible impact on convergence and performance.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Why is there a noticeable downgrade in the performance metrics of our current ReLU model, particularly the 70B model?&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;In contrast to the typical requirement of around 2T tokens for LLM training, our model&#39;s fine-tuning was conducted with only 5B tokens. This insufficient retraining has resulted in the model&#39;s inability to regain its original performance. We are actively working on updating to a more capable model, so please stay tuned.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;What if...&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Issues are welcomed! Please feel free to open an issue and attach your running environment and running parameters. We will try our best to help you.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;TODOs&lt;/h2&gt; &#xA;&lt;p&gt;We will release the code and data in the following order, please stay tuned!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release core code of PowerInfer, supporting Llama-2, Falcon-40B.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support Mistral-7B&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support Windows&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support text-generation-webui&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release perplexity evaluation code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support Metal for Mac&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release code for OPT models&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release predictor training code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support online split for FFN network&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support Multi-GPU&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Paper and Citation&lt;/h2&gt; &#xA;&lt;p&gt;More technical details can be found in our &lt;a href=&#34;https://ipads.se.sjtu.edu.cn/_media/publications/powerinfer-20231219.pdf&#34;&gt;paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you find PowerInfer useful or relevant to your project and research, please kindly cite our paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{song2023powerinfer,&#xA;      title={PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU}, &#xA;      author={Yixin Song and Zeyu Mi and Haotong Xie and Haibo Chen},&#xA;      year={2023},&#xA;      eprint={2312.12456},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.LG}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;We are thankful for the easily modifiable operator library &lt;a href=&#34;https://github.com/ggerganov/ggml&#34;&gt;ggml&lt;/a&gt; and execution runtime provided by &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt;. We also extend our gratitude to &lt;a href=&#34;https://nlp.csai.tsinghua.edu.cn/&#34;&gt;THUNLP&lt;/a&gt; for their support of ReLU-based sparse models. We also appreciate the research of &lt;a href=&#34;https://proceedings.mlr.press/v202/liu23am.html&#34;&gt;Deja Vu&lt;/a&gt;, which inspires PowerInfer.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>screetsec/TheFatRat</title>
    <updated>2024-12-31T01:46:07Z</updated>
    <id>tag:github.com,2024-12-31:/screetsec/TheFatRat</id>
    <link href="https://github.com/screetsec/TheFatRat" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Thefatrat a massive exploiting tool : Easy tool to generate backdoor and easy tool to post exploitation attack like browser attack and etc . This tool compiles a malware with popular payload and then the compiled malware can be execute on windows, android, mac . The malware that created with this tool also have an ability to bypass most AV softw‚Ä¶&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TheFatRat&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/TheFatRat-1.9.8-brightgreen.svg?maxAge=259200&#34; alt=&#34;Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Codename-Target-red.svg?maxAge=259200&#34; alt=&#34;Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Release-Testing-brightgreen.svg?sanitize=true&#34; alt=&#34;Stage&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Supported_OS-Linux-orange.svg?sanitize=true&#34; alt=&#34;Build&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Available-BlackArch-red.svg?maxAge=259200&#34; alt=&#34;Available&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ManhNho/CEHv10/tree/master/Slides&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CEHv10-eccouncil-blue.svg?maxAge=259200&#34; alt=&#34;Documentation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/contributions-welcome-blue.svg?style=flat&#34; alt=&#34;Contributions Welcome&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;A Massive Exploiting Tool&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/17976841/65820028-6ae17e00-e24e-11e9-894f-35836481cc2c.png&#34; alt=&#34;Banner&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TheFatRat&lt;/strong&gt; is an exploiting tool which compiles a malware with famous payload, and then the compiled maware can be executed on Linux , Windows , Mac and Android. &lt;strong&gt;TheFatRat&lt;/strong&gt; Provides An Easy way to create Backdoors and Payload which can bypass most anti-virus.&lt;/p&gt; &#xA;&lt;h2&gt;Information&lt;/h2&gt; &#xA;&lt;p&gt;This tool is for educational purpose only, usage of TheFatRat for attacking targets without prior mutual consent is illegal. Developers assume no liability and are not responsible for any misuse or damage cause by this program.&lt;/p&gt; &#xA;&lt;h2&gt;Features !&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fully Automating MSFvenom &amp;amp; Metasploit.&lt;/li&gt; &#xA; &lt;li&gt;Local or remote listener Generation.&lt;/li&gt; &#xA; &lt;li&gt;Easily Make Backdoor by category Operating System.&lt;/li&gt; &#xA; &lt;li&gt;Generate payloads in Various formats.&lt;/li&gt; &#xA; &lt;li&gt;Bypass anti-virus backdoors.&lt;/li&gt; &#xA; &lt;li&gt;File pumper that you can use for increasing the size of your files.&lt;/li&gt; &#xA; &lt;li&gt;The ability to detect external IP &amp;amp; Interface address .&lt;/li&gt; &#xA; &lt;li&gt;Automatically creates AutoRun files for USB / CDROM exploitation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;But it&#39;s shit! And your implementation sucks!&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Yes, you&#39;re probably correct. Feel free to &#34;Not use it&#34; and there is a pull button to &#34;Make it better&#34;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;p&gt;Instructions on how to install &lt;em&gt;TheFatRat&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/Screetsec/TheFatRat.git&#xA;cd TheFatRat&#xA;chmod +x setup.sh &amp;amp;&amp;amp; ./setup.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Update&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd TheFatRat&#xA;./update &amp;amp;&amp;amp; chmod +x setup.sh &amp;amp;&amp;amp; ./setup.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Troubleshoot on TheFatRat&lt;/h3&gt; &#xA;&lt;p&gt;chk_tools script to use in case of problems in setup.sh of fatrat this script will check if everything is in the right version to run fatrat and will also provide you a solution for the problem&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd TheFatRat&#xA;chmod +x chk_tools &#xA;./chk_tools&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Tools Overview&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Front View&lt;/th&gt; &#xA;   &lt;th&gt;Sample Feature&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://cloud.githubusercontent.com/assets/17976841/25420100/9ee12cf6-2a80-11e7-8dfa-c2e3cfe71366.png&#34; alt=&#34;Index&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/17976841/65820886-91a4b200-e258-11e9-9a00-1e5905f6be16.jpg&#34; alt=&#34;f&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Documentation Available in Modules CEH v9 and V10 , Download source here &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/khanhnnvn/CEHv10/raw/master/Labs/CEHv10%20Module%2006%20System%20Hacking.pdf&#34;&gt;CEHv10 Module 06 System Hacking.pdf&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/khanhnnvn/CEHv10/raw/master/Labs/CEHv10%20Module%2017%20Hacking%20Mobile%20Platforms.pdf&#34;&gt;CEHv10 Module 17 Hacking Mobile Platforms.pdf&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Published in International Journal of Cyber-Security and Digital Forensics &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.researchgate.net/publication/323574673_MALWARE_ANALYSIS_OF_BACKDOOR_CREATOR_FATRAT&#34;&gt;Malware Analysis Of Backdoor Creator : TheFatRat&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Youtube Videos &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=FsSgJFxyzFQ&#34;&gt;How To Download &amp;amp; Install TheFatRat&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=NCsrcqhUBCc&amp;amp;feature=youtu.be&amp;amp;list=PLbyfDadg3caj6nc3KBk375lKWDOjiCmb8&#34;&gt;TheFatRat 1.9.6 - Trodebi ( Embed Trojan into Debian Package )&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=bFXVAXRXE9Q&#34;&gt;hacking windows 10 with TheFatRat&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=FlXMslSjnGw&#34;&gt;Hacking Windows using TheFatRat + Apache2 Server + Ettercap + Metasploit&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=lglOXojT84M&#34;&gt;Hacking with a Microsoft Office Word Document from TheFatRat&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=pbvg7pgxVjo&#34;&gt;XSS to powershell attack and bypass Antivirus using BeEF + TheFatRat + Metasploit&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=XLNigYZ5-fM&#34;&gt;TheFatRat - Hacking Over WAN - Embedding Payload in Original Android APK - Without Port Forwarding&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=C_Og6LnEZSg&#34;&gt;How To Automatically Embed Payloads In APK&#39;s - Evil-Droid, Thefatrat &amp;amp; Apkinjector&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=VPl1TMCAIy8&#34;&gt;Bind FUD Payload with JPG and Hack over WAN with TheFatRat&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Changelog&lt;/h2&gt; &#xA;&lt;p&gt;All notable changes to this project will be documented in this &lt;a href=&#34;https://github.com/Screetsec/thefatrat/raw/master/CHANGELOG.md&#34;&gt;file&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;About issues&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Read the &lt;a href=&#34;https://github.com/Screetsec/TheFatRat/raw/master/issues.md&#34;&gt;document&lt;/a&gt; before making an issue&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Alternative Best Tool - Generating Backdoor &amp;amp; Bypass&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Veil-Framework/Veil&#34;&gt;Veil-Framework /Veil&lt;/a&gt; - Veil Framework&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.shellterproject.com/download/&#34;&gt;Shellter&lt;/a&gt; - Shellter AV Evasion Artware&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/trustedsec/unicorn&#34;&gt;Unicorn&lt;/a&gt; - Trustedsec&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/g0tmi1k/msfpc&#34;&gt;MSFvenom Payload Creator (MSFPC)&lt;/a&gt; - g0tmi1k&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/r00t-3xp10it/venom&#34;&gt;Venom&lt;/a&gt; - Pedro Ubuntu&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/oddcod3/Phantom-Evasion&#34;&gt;Phantom-Evasion&lt;/a&gt; - Diego Cornacchini&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Credits &amp;amp; Thanks&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.offensive-security.com/&#34;&gt;Offensive Security&lt;/a&gt; - Offensive Security&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dracos-linux.org/&#34;&gt;dracOs Linux&lt;/a&gt; - Penetration Testing OS From Indonesia&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/peterpt&#34;&gt;peterpt&lt;/a&gt; - Maintainer &amp;amp; Contributor&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dana-at-cp/backdoor-apk&#34;&gt;Dana James Traversie&lt;/a&gt; - backdoor_apk&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/z0noxz/powerstager&#34;&gt;z0noxz&lt;/a&gt; - Powerstager&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/trustedsec/unicorn&#34;&gt;TrustedSec&lt;/a&gt; - Unicorn&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rsmudge&#34;&gt;Raphael Mudge&lt;/a&gt; - External Source&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://astr0baby.wordpress.com&#34;&gt;astr0baby&lt;/a&gt; - Reference Source&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ngesec.id/&#34;&gt;NgeSEC&lt;/a&gt; Community&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gauli.net/&#34;&gt;Gauli(dot)Net&lt;/a&gt; - Lab Penetration&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;TheFatRat is made with üñ§ by Edo Maland &amp;amp; All &lt;a href=&#34;https://github.com/Screetsec/TheFatRat/graphs/contributors&#34;&gt;Contributors&lt;/a&gt;. See the &lt;strong&gt;License&lt;/strong&gt; file for more details.&lt;/p&gt;</summary>
  </entry>
</feed>