<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-01-07T01:29:18Z</updated>
  <subtitle>Daily Trending of C in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>openwch/arduino_core_ch32</title>
    <updated>2024-01-07T01:29:18Z</updated>
    <id>tag:github.com,2024-01-07:/openwch/arduino_core_ch32</id>
    <link href="https://github.com/openwch/arduino_core_ch32" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Core library for CH32duino&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Arduino core support for CH32 EVT Boards&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openwch/arduino_core_ch32#Introduction&#34;&gt;Introduction&lt;/a&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openwch/arduino_core_ch32#How-to-use&#34;&gt;How to use&lt;/a&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openwch/arduino_core_ch32#Supported-boards&#34;&gt;Supported boards&lt;/a&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openwch/arduino_core_ch32#OS-support&#34;&gt;OS support&lt;/a&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openwch/arduino_core_ch32#Submit-bugs&#34;&gt;Submit bugs&lt;/a&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;This repo adds the support of CH32 MCU in Arduino IDE.&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;The file includes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openwch/arduino_core_ch32&#34;&gt;Arduino_Core_CH32&lt;/a&gt;:Public library files.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openwch/openocd_wch&#34;&gt;openocd&lt;/a&gt;:can directly use WCH-LINKE to download and debug wch chips.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openwch/risc-none-embed-gcc&#34;&gt;riscv-none-embed-gcc&lt;/a&gt;:A toolchain that supports WCH custom half word and byte compression instruction extensions and hardware stack push/pop functions.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to use&lt;/h2&gt; &#xA;&lt;p&gt;You can add this software package directly on the IDE through the &lt;a href=&#34;https://www.arduino.cc/en/guide/cores&#34;&gt;Arduino Boards Manager&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Add the following link in the &#34;&lt;em&gt;Additional Boards Managers URLs&lt;/em&gt;&#34; field:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/openwch/board_manager_files/raw/main/package_ch32v_index.json&#34;&gt;https://github.com/openwch/board_manager_files/raw/main/package_ch32v_index.json&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Then you can search for &#34;&lt;strong&gt;wch&lt;/strong&gt;&#34; through the &#34;&lt;strong&gt;board manager&lt;/strong&gt;&#34;, find the installation package, and install it.&lt;/p&gt; &#xA;&lt;h2&gt;Supported boards&lt;/h2&gt; &#xA;&lt;p&gt;It will be a long-term support and maintenance project, unless we encounter force majeure factors.The current version supports the following development boards:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openwch/arduino_core_ch32/main/#CH32V00x-EVT-Boards&#34;&gt;CH32V00x EVT Boards&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openwch/arduino_core_ch32/main/#CH32V10x-EVT-Boards&#34;&gt;CH32V10x EVT Boards&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openwch/arduino_core_ch32/main/#CH32V20x-EVT-Boards&#34;&gt;CH32V20x EVT Boards&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openwch/arduino_core_ch32/main/#CH32V30x-EVT-Boards&#34;&gt;CH32V30x EVT Boards&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openwch/arduino_core_ch32/main/#CH32X035-EVT-Boards&#34;&gt;CH32X035 EVT Boards&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;CH32V00x EVT Boards&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Status&lt;/th&gt; &#xA;   &lt;th&gt;Boards name&lt;/th&gt; &#xA;   &lt;th&gt;Peripherals&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Release&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Notes&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;CH32V003F4P&lt;/td&gt; &#xA;   &lt;td&gt;ADC,DAC,USART,GPIO,EXTI,SysTick&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.0.0&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;SPI,I2C_Master since 1.0.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;CH32V20x EVT Boards&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Status&lt;/th&gt; &#xA;   &lt;th&gt;Boards name&lt;/th&gt; &#xA;   &lt;th&gt;Peripherals&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Release&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Notes&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;CH32V203G8U&lt;/td&gt; &#xA;   &lt;td&gt;ADC,DAC,USART,GPIO,EXTI,SysTick&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.0.0&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;SPI,I2C_Master since 1.0.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;CH32X035 EVT Boards&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Status&lt;/th&gt; &#xA;   &lt;th&gt;Boards name&lt;/th&gt; &#xA;   &lt;th&gt;Peripherals&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Release&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Notes&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;CH32X035G8U&lt;/td&gt; &#xA;   &lt;td&gt;ADC,DAC,USART,GPIO,EXTI,SysTick&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.0.1&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;SPI,I2C_Master since 1.0.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;CH32V10x EVT Boards&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Status&lt;/th&gt; &#xA;   &lt;th&gt;Boards name&lt;/th&gt; &#xA;   &lt;th&gt;Peripherals&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Release&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Notes&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;CH32V103R8T6_BLACK&lt;/td&gt; &#xA;   &lt;td&gt;ADC,DAC,USART,GPIO,EXTI,SysTick,SPI,I2C_Master&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.0.3&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;CH32V30x EVT Boards&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Status&lt;/th&gt; &#xA;   &lt;th&gt;Boards name&lt;/th&gt; &#xA;   &lt;th&gt;Peripherals&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Release&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Notes&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;CH32V307VCT6_BLACK&lt;/td&gt; &#xA;   &lt;td&gt;ADC,DAC,USART,GPIO,EXTI,SysTick,SPI,I2C_Master&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.0.3&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;OS support&lt;/h2&gt; &#xA;&lt;p&gt;Adopting toolchain and openocd under &lt;a href=&#34;http://www.mounriver.com/&#34;&gt;MRS&lt;/a&gt;, supporting HPE, custom byte and half-word compression extensions,&#34;upload&#34; via WCH_LINKE.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Most importantly, the version of Arduino IDE is 2.0+.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Win&lt;/h3&gt; &#xA;&lt;p&gt;If you encounter an error during upload, please confirm that the version of your WCH-LINKE is consistent with the latest version under MRS. WCH-LINKE related information can &lt;a href=&#34;https://github.com/openwch/ch32v307/tree/main/WCH-Link&#34;&gt;refer to this link&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Linux&lt;/h3&gt; &#xA;&lt;p&gt;For Linux, after installing the support package for the first time, to ensure the normal upload function, please open the packages installation path of the Arduino IDE, run the script, and automatically configure the environment.&lt;/p&gt; &#xA;&lt;p&gt;Usually, it can be operated as follows:&lt;br&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd ~/.arduino15/packages/WCH/tools/beforeinstall/1.0.0&#xA;./start.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After authorization, it will copy or generate some necessary libraries and rules:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;Copy&amp;nbsp;Libs&#xA;[sudo] password for temperslee: &#xA;Register&amp;nbsp;new&amp;nbsp;Libs&#xA;copy rules&#xA;Reload rules&#xA;DONE&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;MAC&lt;/h3&gt; &#xA;&lt;p&gt;For MAC, please install the &#34;libusb&#34; library before starting to use it.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;brew install libusb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If &#34;libusb&#34; related errors still occur when &#34;uploading&#34; firmware after installing the libusb library, please contact the &lt;strong&gt;MRS team&lt;/strong&gt; for assistance through &#34;&lt;em&gt;&lt;a href=&#34;mailto:support@mounriver.com&#34;&gt;support@mounriver.com&lt;/a&gt;&lt;/em&gt;&#34;.&lt;/p&gt; &#xA;&lt;h2&gt;Submit bugs&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions, you could contact me through the email &#34;&lt;em&gt;&lt;a href=&#34;mailto:yy@wch.cn&#34;&gt;yy@wch.cn&lt;/a&gt;&lt;/em&gt;&#34;. Or you could &lt;a href=&#34;https://github.com/openwch/arduino_core_ch32/issues/new&#34;&gt;file an issue on GitHub&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>spotify/sparkey</title>
    <updated>2024-01-07T01:29:18Z</updated>
    <id>tag:github.com,2024-01-07:/spotify/sparkey</id>
    <link href="https://github.com/spotify/sparkey" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Simple constant key/value storage library, for read-heavy systems with infrequent large bulk inserts.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;Sparkey is a simple constant key/value storage library. It is mostly suited for read heavy systems with infrequent large bulk inserts. It includes both a C library for working with sparkey index and log files (&lt;code&gt;libsparkey&lt;/code&gt;), and a command line utility for getting info about and reading values from a sparkey index/log (&lt;code&gt;sparkey&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;h3&gt;Travis&lt;/h3&gt; &#xA;&lt;p&gt;Continuous integration with &lt;a href=&#34;https://travis-ci.org/spotify/sparkey&#34;&gt;travis&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://travis-ci.org/spotify/sparkey&#34;&gt;&lt;img src=&#34;https://travis-ci.org/spotify/sparkey.svg?branch=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Dependencies&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GNU build system (autoconf, automake, libtool)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://google.github.io/snappy/&#34;&gt;Snappy&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Optional&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.doxygen.org/&#34;&gt;Doxygen&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Building&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;autoreconf --install&#xA;./configure&#xA;make&#xA;make check&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;API documentation can be generated with &lt;code&gt;doxygen&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installing&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo make install &amp;amp;&amp;amp; sudo ldconfig&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Related projects&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/spotify/sparkey-python&#34;&gt;spotify/sparkey-python: Official python bindings&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/spotify/sparkey-java&#34;&gt;spotify/sparkey-java: Official java implementation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/emnl/gnista&#34;&gt;emnl/gnista: Unofficial ruby bindings&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/adamtanner/sparkey&#34;&gt;adamtanner/sparkey: Unofficial ruby bindings&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/stephenmathieson/node-sparkey&#34;&gt;stephenmathieson/node-sparkey: Unofficial node bindings&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tiegz/sparkey-go&#34;&gt;tiegz/sparkey-go: Unofficial go bindings&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dflemstr/sparkey-rs&#34;&gt;dflemstr/sparkey-rs: Unofficial rust bindings&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Description&lt;/h2&gt; &#xA;&lt;p&gt;Sparkey is an extremely simple persistent key-value store. You could think of it as a read-only hashtable on disk and you wouldn&#39;t be far off. It is designed and optimized for some server side usecases at Spotify but it is written to be completely generic and makes no assumptions about what kind of data is stored.&lt;/p&gt; &#xA;&lt;p&gt;Some key characteristics:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Supports data sizes up to 2^63 - 1 bytes.&lt;/li&gt; &#xA; &lt;li&gt;Supports iteration, get, put, delete&lt;/li&gt; &#xA; &lt;li&gt;Optimized for bulk writes.&lt;/li&gt; &#xA; &lt;li&gt;Immutable hash table.&lt;/li&gt; &#xA; &lt;li&gt;Any amount of concurrent independent readers.&lt;/li&gt; &#xA; &lt;li&gt;Only allows one writer at a time per storage unit.&lt;/li&gt; &#xA; &lt;li&gt;Cross platform storage file.&lt;/li&gt; &#xA; &lt;li&gt;Low overhead per entry.&lt;/li&gt; &#xA; &lt;li&gt;Constant read startup cost&lt;/li&gt; &#xA; &lt;li&gt;Low number of disk seeks per read&lt;/li&gt; &#xA; &lt;li&gt;Support for block level compression.&lt;/li&gt; &#xA; &lt;li&gt;Data agnostic, it just maps byte arrays to byte arrays.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;What it&#39;s not:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;It&#39;s not a distributed key value store - it&#39;s just a hash table on disk.&lt;/li&gt; &#xA; &lt;li&gt;It&#39;s not a compacted data store, but that can be implemented on top of it, if needed.&lt;/li&gt; &#xA; &lt;li&gt;It&#39;s not robust against data corruption.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The usecase we have for it at Spotify is serving data that rarely gets updated to users or other services. The fast and efficient bulk writes makes it feasible to periodically rebuild the data, and the fast random access reads makes it suitable for high throughput low latency services. For some services we have been able to saturate network interfaces while keeping cpu usage really low.&lt;/p&gt; &#xA;&lt;h2&gt;Limitations&lt;/h2&gt; &#xA;&lt;p&gt;The hash writing process requires memory allocation of num_entries * 16 * 1.3 bytes. This means that you may run out of memory if trying to write a hash index for too many entries. For instance, with 16 GB available RAM you may write 825 million entries.&lt;/p&gt; &#xA;&lt;p&gt;This limitation has been removed in sparkey-java, but it has not yet been implemented in this version.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Sparkey is meant to be used as a library embedded in other software. Take a look at the API documentation which gives examples on how to use it.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Apache License, Version 2.0&lt;/p&gt; &#xA;&lt;h2&gt;Design&lt;/h2&gt; &#xA;&lt;p&gt;Sparkey uses two files on disk to store its data. The first is the sparkey log file (.spl), which is simply a sequence of key value pairs. This is an append-only file. You&#39;re not allowed to modify it in the middle, and you can&#39;t use more than one writer to append to it.&lt;/p&gt; &#xA;&lt;p&gt;The other file is the sparkey index file (.spi) which is a just a hashtable pointing at entries in the log. This is an immutable file, so you would typically only update it once you&#39;re done with your bulk appends.&lt;/p&gt; &#xA;&lt;p&gt;Doing a random lookup involves first finding the proper entry in the hashtable, and then doing a seek to the right offset in the log file. On average, this means two disk seeks per access for a cold disk cache. If you mlock the index file, it goes down to one seek. For some of our usecases, the total data set is less than the available RAM, so it makes sense to mlock everything.&lt;/p&gt; &#xA;&lt;p&gt;The advantages of having two files instead of just one (another solution would be to append the hash table at the end) is that it&#39;s trivial to mlock one of the files and not the other. It also enables us to append more data to existing log files, even after it&#39;s already in use.&lt;/p&gt; &#xA;&lt;h2&gt;History&lt;/h2&gt; &#xA;&lt;p&gt;Sparkey is the product of hackdays at Spotify, where our developers get to spend some of their time on anything they think is interesting.&lt;/p&gt; &#xA;&lt;p&gt;We have several usecases where we need to serve large amounts of static data with high throughput and low latency. To do this, we&#39;ve built our own services, backed by various storage systems. Our flow consists of first generating large static storage files in our offline-systems, which then gets pushed out to the user facing services to serve the data.&lt;/p&gt; &#xA;&lt;p&gt;The storage solutions we used for that have all served us well for a time, but they had limitations that became problematic.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We used to rely a lot on CDB (which is a really great piece of software). It performed blazingly quick and produces compact files. We only stopped using it when our data started growing close to the 4 GB limit&lt;/li&gt; &#xA; &lt;li&gt;We also used (and still use) Tokyo Cabinet for a bunch of usecases. It performs really well for reading, but the write throughput really suffers when you can no longer keep the entire dataset in memory, and there were issues with opening the same file multiple times from the same process.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We needed a key-value store with the following characteristics:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;random read throughput comparable to tokyo cabinet and cdb.&lt;/li&gt; &#xA; &lt;li&gt;high throughput bulk writes.&lt;/li&gt; &#xA; &lt;li&gt;low overhead.&lt;/li&gt; &#xA; &lt;li&gt;high limit on data size.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For fun, we started hacking on a new key-value store on our internal hackdays, where developers get to work on whatever they&#39;re interested in. The result was this project.&lt;/p&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;p&gt;A very simple benchmark program is included - see src/bench.c. The program is designed to be easily extended to measure other key value stores if anyone wants to. Running it on a production-like server (Intel(R) Xeon(R) CPU L5630 @ 2.13GHz) we get the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Testing bulk insert of 1000 elements and 1000.000 random lookups&#xA;  Candidate: Sparkey&#xA;    creation time (wall):     0.00&#xA;    creation time (cpu):      0.00&#xA;    throughput (puts/cpusec): 1098272.88&#xA;    file size:                28384&#xA;    lookup time (wall):          0.50&#xA;    lookup time (cpu):           0.58&#xA;    throughput (lookups/cpusec): 1724692.62&#xA;&#xA;Testing bulk insert of 1000.000 elements and 1000.000 random lookups&#xA;  Candidate: Sparkey&#xA;    creation time (wall):     0.50&#xA;    creation time (cpu):      0.69&#xA;    throughput (puts/cpusec): 1448618.25&#xA;    file size:                34177984&#xA;    lookup time (wall):          1.00&#xA;    lookup time (cpu):           0.78&#xA;    throughput (lookups/cpusec): 1284477.75&#xA;&#xA;Testing bulk insert of 10.000.000 elements and 1000.000 random lookups&#xA;  Candidate: Sparkey&#xA;    creation time (wall):     7.50&#xA;    creation time (cpu):      7.73&#xA;    throughput (puts/cpusec): 1294209.62&#xA;    file size:                413777988&#xA;    lookup time (wall):          1.00&#xA;    lookup time (cpu):           0.99&#xA;    throughput (lookups/cpusec): 1014608.94&#xA;&#xA;Testing bulk insert of 100.000.000 elements and 1000.000 random lookups&#xA;  Candidate: Sparkey&#xA;    creation time (wall):     82.00&#xA;    creation time (cpu):      81.58&#xA;    throughput (puts/cpusec): 1225726.75&#xA;    file size:                4337777988&#xA;    lookup time (wall):          2.00&#xA;    lookup time (cpu):           1.98&#xA;    throughput (lookups/cpusec): 503818.84&#xA;&#xA;Testing bulk insert of 1000 elements and 1000.000 random lookups&#xA;  Candidate: Sparkey compressed(1024)&#xA;    creation time (wall):     0.00&#xA;    creation time (cpu):      0.00&#xA;    throughput (puts/cpusec): 1101445.38&#xA;    file size:                19085&#xA;    lookup time (wall):          3.50&#xA;    lookup time (cpu):           3.30&#xA;    throughput (lookups/cpusec): 303335.78&#xA;&#xA;Testing bulk insert of 1000.000 elements and 1000.000 random lookups&#xA;  Candidate: Sparkey compressed(1024)&#xA;    creation time (wall):     0.50&#xA;    creation time (cpu):      0.75&#xA;    throughput (puts/cpusec): 1333903.25&#xA;    file size:                19168683&#xA;    lookup time (wall):          3.00&#xA;    lookup time (cpu):           2.91&#xA;    throughput (lookups/cpusec): 343833.28&#xA;&#xA;Testing bulk insert of 10.000.000 elements and 1000.000 random lookups&#xA;  Candidate: Sparkey compressed(1024)&#xA;    creation time (wall):     8.50&#xA;    creation time (cpu):      8.50&#xA;    throughput (puts/cpusec): 1176634.25&#xA;    file size:                311872187&#xA;    lookup time (wall):          3.00&#xA;    lookup time (cpu):           2.99&#xA;    throughput (lookups/cpusec): 334490.22&#xA;&#xA;Testing bulk insert of 100.000.000 elements and 1000.000 random lookups&#xA;  Candidate: Sparkey compressed(1024)&#xA;    creation time (wall):     90.50&#xA;    creation time (cpu):      90.46&#xA;    throughput (puts/cpusec): 1105412.00&#xA;    file size:                3162865465&#xA;    lookup time (wall):          3.50&#xA;    lookup time (cpu):           3.60&#xA;    throughput (lookups/cpusec): 277477.41&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;File format details&lt;/h2&gt; &#xA;&lt;h3&gt;Log file format&lt;/h3&gt; &#xA;&lt;p&gt;The contents of the log file starts with a constant size header, describing some metadata about the log file. After that is just a sequence of entries, where each entry consists of a type, key and a value.&lt;/p&gt; &#xA;&lt;p&gt;Each entry begins with two Variable Length Quantity (VLQ) non-negative integers, A and B. The type is determined by the A. If A = 0, it&#39;s a DELETE, and B represents the length of the key to delete. If A &amp;gt; 0, it&#39;s a PUT and the key length is A - 1, and the value length is B.&lt;/p&gt; &#xA;&lt;p&gt;(It gets slightly more complex if block level compression is used, but we&#39;ll ignore that for now.)&lt;/p&gt; &#xA;&lt;h3&gt;Hash file format&lt;/h3&gt; &#xA;&lt;p&gt;The contents of the hash file starts with a constant size header, similarly to the log file. The rest of the file is a hash table, represented as capacity * slotsize bytes. The capacity is simply an upper bound of the number of live entries multiplied by a hash density factor &amp;gt; 1.0. The default implementation uses density factor = 1.3. Each slot consists of two parts, the hash value part and the address. The size of the hash value is either 4 or 8 bytes, depending on the hash algorithm. It currently uses murmurhash32 if the number of entries is small, and a 64 bit truncation of murmurhash128 if the number of entries is large. The address is simply a reference into the log file, either as 4 or 8 bytes, depending on the size of the log file. That means that the slotsize is usually 16 bytes for any reasonably large set of entries. By storing the hash value itself in each slot we&#39;re wasting some space, but in return we can expect to avoid visiting the log file in most cases.&lt;/p&gt; &#xA;&lt;h2&gt;Hash lookup algorithm&lt;/h2&gt; &#xA;&lt;p&gt;One of few non-trivial parts in Sparkey is the way it does hash lookups. With hashtables there is always a risk of collisions. Even if the hash itself may not collide, the assigned slots may.&lt;/p&gt; &#xA;&lt;p&gt;(It recently came to my attention that the method described below is basically the same thing as Robin Hood hashing with backward shift deletion)&lt;/p&gt; &#xA;&lt;p&gt;Let&#39;s define displacement as the distance from the calculated optimal slot for a given hash to the slot it&#39;s actually placed in. Distance in this case is defined as the number of steps you need to move forward from your optimal slot to reach the actual slot.&lt;/p&gt; &#xA;&lt;p&gt;The trivial and naive solution for this is to simply start with an empty hash table, move through the entries and put them in the first available slot, starting from the optimal slot, and this is almost what we do.&lt;/p&gt; &#xA;&lt;p&gt;If we consider the average displacement, we can&#39;t really do better than that. We can however minimize the maximum displacement, which gives us some nice properties:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We can store the maximum displacement in the header, so we have an upper bound on traversals. We could possibly even use this information to binary search for the entry.&lt;/li&gt; &#xA; &lt;li&gt;As soon as we reach an entry with higher displacement than the thing we&#39;re looking for, we can abort the lookup.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;It&#39;s very easy to set up the hash table like this, we just need to do insertions into slots instead of appends. As soon as we reach a slot with a smaller displacement than our own, we shift the following slots up until the first empty slot one step and insert our own element.&lt;/p&gt; &#xA;&lt;p&gt;Let&#39;s illustrate it with an example - let&#39;s start off with an empty hash table with a capacity of 7:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;         hash value   log offset    optimal slot    displacement&#xA;       +------------+------------+&#xA;slot 0 |            |            |&#xA;       +------------+------------+&#xA;slot 1 |            |            |&#xA;       +------------+------------+&#xA;slot 2 |            |            |&#xA;       +------------+------------+&#xA;slot 3 |            |            |&#xA;       +------------+------------+&#xA;slot 4 |            |            |&#xA;       +------------+------------+&#xA;slot 5 |            |            |&#xA;       +------------+------------+&#xA;slot 6 |            |            |&#xA;       +------------+------------+&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We add the key &#34;key0&#34; which happens to end up in slot 3, h(&#34;key0&#34;) % 7 == 3. The slot is empty, so this is trivial:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;         hash value   log offset    optimal slot    displacement&#xA;       +------------+------------+&#xA;slot 0 |            |            |&#xA;       +------------+------------+&#xA;slot 1 |            |            |&#xA;       +------------+------------+&#xA;slot 2 |            |            |&#xA;       +------------+------------+&#xA;slot 3 | h(key0)    | 1          |  3               0&#xA;       +------------+------------+&#xA;slot 4 |            |            |&#xA;       +------------+------------+&#xA;slot 5 |            |            |&#xA;       +------------+------------+&#xA;slot 6 |            |            |&#xA;       +------------+------------+&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now we add &#34;key1&#34; which happens to end up in slot 4:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;         hash value   log offset    optimal slot    displacement&#xA;       +------------+------------+&#xA;slot 0 |            |            |&#xA;       +------------+------------+&#xA;slot 1 |            |            |&#xA;       +------------+------------+&#xA;slot 2 |            |            |&#xA;       +------------+------------+&#xA;slot 3 | h(key0)    | 1          |  3               0&#xA;       +------------+------------+&#xA;slot 4 | h(key1)    | 11         |  4               0&#xA;       +------------+------------+&#xA;slot 5 |            |            |&#xA;       +------------+------------+&#xA;slot 6 |            |            |&#xA;       +------------+------------+&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now we add &#34;key2&#34; which also wants to be in slot 3. This is a conflict, so we skip forward until we found a slot which has a lower displacement than our current displacement. When we find that slot, all following entries until the next empty slot move down one step:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;         hash value   log offset    optimal slot    displacement&#xA;       +------------+------------+&#xA;slot 0 |            |            |&#xA;       +------------+------------+&#xA;slot 1 |            |            |&#xA;       +------------+------------+&#xA;slot 2 |            |            |&#xA;       +------------+------------+&#xA;slot 3 | h(key0)    | 1          |  3               0&#xA;       +------------+------------+&#xA;slot 4 | h(key2)    | 21         |  3               1&#xA;       +------------+------------+&#xA;slot 5 | h(key1)    | 11         |  4               1&#xA;       +------------+------------+&#xA;slot 6 |            |            |&#xA;       +------------+------------+&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Let&#39;s add &#34;key3&#34; which maps to slot 5. We can&#39;t push down key1, because it already has displacement 1 and our current displacement for key3 is 0, so we have to move forward:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;         hash value   log offset    optimal slot    displacement&#xA;       +------------+------------+&#xA;slot 0 |            |            |&#xA;       +------------+------------+&#xA;slot 1 |            |            |&#xA;       +------------+------------+&#xA;slot 2 |            |            |&#xA;       +------------+------------+&#xA;slot 3 | h(key0)    | 1          |  3               0&#xA;       +------------+------------+&#xA;slot 4 | h(key2)    | 21         |  3               1&#xA;       +------------+------------+&#xA;slot 5 | h(key1)    | 11         |  4               1&#xA;       +------------+------------+&#xA;slot 6 | h(key3)    | 31         |  5               1&#xA;       +------------+------------+&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Adding &#34;key4&#34; for slot 3. It ends up in slot 5 with displacement 2 and key3 loops around to slot 0:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;         hash value   log offset    optimal slot    displacement&#xA;       +------------+------------+&#xA;slot 0 | key(key3)  | 31         |  5               2&#xA;       +------------+------------+&#xA;slot 1 |            |            |&#xA;       +------------+------------+&#xA;slot 2 |            |            |&#xA;       +------------+------------+&#xA;slot 3 | h(key0)    | 1          |  3               0&#xA;       +------------+------------+&#xA;slot 4 | h(key2)    | 21         |  3               1&#xA;       +------------+------------+&#xA;slot 5 | h(key4)    | 41         |  3               2&#xA;       +------------+------------+&#xA;slot 6 | h(key1)    | 11         |  4               2&#xA;       +------------+------------+&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now, if we search for key123 which maps to slot 3 (but doesn&#39;t exist!), we can stop scanning as soon as we reach slot 6, because then the current displacement (3) is higher than the displacement of the entry at the current slot (2).&lt;/p&gt; &#xA;&lt;h2&gt;Compression&lt;/h2&gt; &#xA;&lt;p&gt;Sparkey also supports block level compression using google snappy. You select a block size which is then used to split the contents of the log into blocks. Each block is compressed independently with snappy. This can be useful if your bottleneck is file size and there is a lot of redundant data across adjacent entries. The downside of using this is that during lookups, at least one block needs to be decompressed. The larger blocks you choose, the better compression you may get, but you will also have higher lookup cost. This is a tradeoff that needs to be empirically evaluated for each use case.&lt;/p&gt;</summary>
  </entry>
</feed>