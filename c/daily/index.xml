<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-10-30T01:33:16Z</updated>
  <subtitle>Daily Trending of C in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>stacksmashing/tamarin-firmware</title>
    <updated>2022-10-30T01:33:16Z</updated>
    <id>tag:github.com,2022-10-30:/stacksmashing/tamarin-firmware</id>
    <link href="https://github.com/stacksmashing/tamarin-firmware" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Tamarin Firmware&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/stacksmashing/tamarin-firmware/raw/main/media/tamarin-logo-300.png?raw=true&#34; alt=&#34;Tamarin Logo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Build&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;mkdir build&#xA;cd build&#xA;cmake ..&#xA;make&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Hooking it up&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/stacksmashing/tamarin-firmware/raw/main/media/pinout.png?raw=true&#34; alt=&#34;Pinout diagram&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;With this cable, connect:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Purple to GPIO1&lt;/li&gt; &#xA; &lt;li&gt;Orange to GPIO2&lt;/li&gt; &#xA; &lt;li&gt;Black to any GND pin&lt;/li&gt; &#xA; &lt;li&gt;Blue to GPIO3&lt;/li&gt; &#xA; &lt;li&gt;Yellow to GPIO4&lt;/li&gt; &#xA; &lt;li&gt;Red to 5V&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Tamarin Cable provides three USB endpoints, of which two are serial ports.&lt;/p&gt; &#xA;&lt;p&gt;Serial port 1 is the control serial port, use it to configure DCSD/JTAG mode.&lt;/p&gt; &#xA;&lt;p&gt;Serial port 2 is the DCSD port, when Tamarin Cable is in DCSD mode the serial output will be provided here.&lt;/p&gt; &#xA;&lt;h3&gt;OpenOCD&lt;/h3&gt; &#xA;&lt;p&gt;To use Tamarin as a JTAG adapter you need to use our &lt;a href=&#34;https://github.com/stacksmashing/openocd&#34;&gt;OpenOCD fork&lt;/a&gt; that includes support for the Tamarin probe.&lt;/p&gt; &#xA;&lt;p&gt;To enable JTAG on production iPhones they need to be demoted. For checkm8 vulnerable iPhones this can be done using &lt;a href=&#34;https://github.com/axi0mX/ipwndfu&#34;&gt;ipwndfu&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Once the phone is successfully demoted the &lt;a href=&#34;https://github.com/lambdaconcept/bonobo-configs/raw/master/t8015.cfg&#34;&gt;bonobo configs&lt;/a&gt; can be used to connect to the iPhone like so:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;openocd -f interface/tamarin.cfg -f t8015.cfg&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>leandromoreira/ffmpeg-libav-tutorial</title>
    <updated>2022-10-30T01:33:16Z</updated>
    <id>tag:github.com,2022-10-30:/leandromoreira/ffmpeg-libav-tutorial</id>
    <link href="https://github.com/leandromoreira/ffmpeg-libav-tutorial" rel="alternate"></link>
    <summary type="html">&lt;p&gt;FFmpeg libav tutorial - learn how media works from basic to transmuxing, transcoding and more&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/README-cn.md&#34; title=&#34;Simplified Chinese&#34;&gt;ðŸ‡¨ðŸ‡³&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/README-ko.md&#34; title=&#34;Korean&#34;&gt;ðŸ‡°ðŸ‡·&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/README-es.md&#34; title=&#34;Spanish&#34;&gt;ðŸ‡ªðŸ‡¸&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/README-vn.md&#34; title=&#34;Vietnamese&#34;&gt;ðŸ‡»ðŸ‡³&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://img.shields.io/badge/license-BSD--3--Clause-blue.svg&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-BSD--3--Clause-blue.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;I was looking for a tutorial/book that would teach me how to start to use &lt;a href=&#34;https://www.ffmpeg.org/&#34;&gt;FFmpeg&lt;/a&gt; as a library (a.k.a. libav) and then I found the &lt;a href=&#34;http://dranger.com/ffmpeg/&#34;&gt;&#34;How to write a video player in less than 1k lines&#34;&lt;/a&gt; tutorial. Unfortunately it was deprecated, so I decided to write this one.&lt;/p&gt; &#xA;&lt;p&gt;Most of the code in here will be in C &lt;strong&gt;but don&#39;t worry&lt;/strong&gt;: you can easily understand and apply it to your preferred language. FFmpeg libav has lots of bindings for many languages like &lt;a href=&#34;https://pyav.org/&#34;&gt;python&lt;/a&gt;, &lt;a href=&#34;https://github.com/imkira/go-libav&#34;&gt;go&lt;/a&gt; and even if your language doesn&#39;t have it, you can still support it through the &lt;code&gt;ffi&lt;/code&gt; (here&#39;s an example with &lt;a href=&#34;https://github.com/daurnimator/ffmpeg-lua-ffi/raw/master/init.lua&#34;&gt;Lua&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;We&#39;ll start with a quick lesson about what is video, audio, codec and container and then we&#39;ll go to a crash course on how to use &lt;code&gt;FFmpeg&lt;/code&gt; command line and finally we&#39;ll write code, feel free to skip directly to&lt;a href=&#34;http://newmediarockstars.com/wp-content/uploads/2015/11/nintendo-direct-iwata.jpg&#34;&gt; &lt;/a&gt;the section &lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/#learn-ffmpeg-libav-the-hard-way&#34;&gt;Learn FFmpeg libav the Hard Way.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Some people used to say that the Internet video streaming is the future of the traditional TV, in any case, the FFmpeg is something that is worth studying.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/#intro&#34;&gt;Intro&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/#video---what-you-see&#34;&gt;video - what you see!&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/#audio---what-you-listen&#34;&gt;audio - what you listen!&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/#codec---shrinking-data&#34;&gt;codec - shrinking data&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/#container---a-comfy-place-for-audio-and-video&#34;&gt;container - a comfy place for audio and video&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/#ffmpeg---command-line&#34;&gt;FFmpeg - command line&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/#ffmpeg-command-line-tool-101&#34;&gt;FFmpeg command line tool 101&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/#common-video-operations&#34;&gt;Common video operations&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/#transcoding&#34;&gt;Transcoding&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/#transmuxing&#34;&gt;Transmuxing&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/#transrating&#34;&gt;Transrating&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/#transsizing&#34;&gt;Transsizing&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/#bonus-round-adaptive-streaming&#34;&gt;Bonus Round: Adaptive Streaming&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/#going-beyond&#34;&gt;Going beyond&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/#learn-ffmpeg-libav-the-hard-way&#34;&gt;Learn FFmpeg libav the Hard Way&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/#chapter-0---the-infamous-hello-world&#34;&gt;Chapter 0 - The infamous hello world&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/#ffmpeg-libav-architecture&#34;&gt;FFmpeg libav architecture&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/#chapter-1---syncing-audio-and-video&#34;&gt;Chapter 1 - timing&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/#chapter-2---remuxing&#34;&gt;Chapter 2 - remuxing&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/#chapter-3---transcoding&#34;&gt;Chapter 3 - transcoding&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Intro&lt;/h1&gt; &#xA;&lt;h2&gt;video - what you see!&lt;/h2&gt; &#xA;&lt;p&gt;If you have a sequence series of images and change them at a given frequency (let&#39;s say &lt;a href=&#34;https://www.filmindependent.org/blog/hacking-film-24-frames-per-second/&#34;&gt;24 images per second&lt;/a&gt;), you will create an &lt;a href=&#34;https://en.wikipedia.org/wiki/Persistence_of_vision&#34;&gt;illusion of movement&lt;/a&gt;. In summary this is the very basic idea behind a video: &lt;strong&gt;a series of pictures / frames running at a given rate&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/1/1f/Linnet_kineograph_1886.jpg&#34; title=&#34;flip book&#34; height=&#34;280&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;ZeitgenÃ¶ssische Illustration (1886)&lt;/p&gt; &#xA;&lt;h2&gt;audio - what you listen!&lt;/h2&gt; &#xA;&lt;p&gt;Although a muted video can express a variety of feelings, adding sound to it brings more pleasure to the experience.&lt;/p&gt; &#xA;&lt;p&gt;Sound is the vibration that propagates as a wave of pressure, through the air or any other transmission medium, such as a gas, liquid or solid.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;In a digital audio system, a microphone converts sound to an analog electrical signal, then an analog-to-digital converter (ADC) â€” typically using &lt;a href=&#34;https://en.wikipedia.org/wiki/Pulse-code_modulation&#34;&gt;pulse-code modulation (PCM)&lt;/a&gt; - converts the analog signal into a digital signal.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/c/c7/CPT-Sound-ADC-DAC.svg/640px-CPT-Sound-ADC-DAC.svg.png&#34; alt=&#34;audio analog to digital&#34; title=&#34;audio analog to digital&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://commons.wikimedia.org/wiki/File:CPT-Sound-ADC-DAC.svg&#34;&gt;Source&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;codec - shrinking data&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;CODEC is an electronic circuit or software that &lt;strong&gt;compresses or decompresses digital audio/video.&lt;/strong&gt; It converts raw (uncompressed) digital audio/video to a compressed format or vice versa. &lt;a href=&#34;https://en.wikipedia.org/wiki/Video_codec&#34;&gt;https://en.wikipedia.org/wiki/Video_codec&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;But if we chose to pack millions of images in a single file and called it a movie, we might end up with a huge file. Let&#39;s do the math:&lt;/p&gt; &#xA;&lt;p&gt;Suppose we are creating a video with a resolution of &lt;code&gt;1080 x 1920&lt;/code&gt; (height x width) and that we&#39;ll spend &lt;code&gt;3 bytes&lt;/code&gt; per pixel (the minimal point at a screen) to encode the color (or &lt;a href=&#34;https://en.wikipedia.org/wiki/Color_depth#True_color_.2824-bit.29&#34;&gt;24 bit color&lt;/a&gt;, what gives us 16,777,216 different colors) and this video runs at &lt;code&gt;24 frames per second&lt;/code&gt; and it is &lt;code&gt;30 minutes&lt;/code&gt; long.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;toppf = 1080 * 1920 //total_of_pixels_per_frame&#xA;cpp = 3 //cost_per_pixel&#xA;tis = 30 * 60 //time_in_seconds&#xA;fps = 24 //frames_per_second&#xA;&#xA;required_storage = tis * fps * toppf * cpp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This video would require approximately &lt;code&gt;250.28GB&lt;/code&gt; of storage or &lt;code&gt;1.19 Gbps&lt;/code&gt; of bandwidth! That&#39;s why we need to use a &lt;a href=&#34;https://github.com/leandromoreira/digital_video_introduction#how-does-a-video-codec-work&#34;&gt;CODEC&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;container - a comfy place for audio and video&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;A container or wrapper format is a metafile format whose specification describes how different elements of data and metadata coexist in a computer file. &lt;a href=&#34;https://en.wikipedia.org/wiki/Digital_container_format&#34;&gt;https://en.wikipedia.org/wiki/Digital_container_format&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;A &lt;strong&gt;single file that contains all the streams&lt;/strong&gt; (mostly the audio and video) and it also provides &lt;strong&gt;synchronization and general metadata&lt;/strong&gt;, such as title, resolution and etc.&lt;/p&gt; &#xA;&lt;p&gt;Usually we can infer the format of a file by looking at its extension: for instance a &lt;code&gt;video.webm&lt;/code&gt; is probably a video using the container &lt;a href=&#34;https://www.webmproject.org/&#34;&gt;&lt;code&gt;webm&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/img/container.png&#34; alt=&#34;container&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;FFmpeg - command line&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;A complete, cross-platform solution to record, convert and stream audio and video.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;To work with multimedia we can use the AMAZING tool/library called &lt;a href=&#34;https://www.ffmpeg.org/&#34;&gt;FFmpeg&lt;/a&gt;. Chances are you already know/use it directly or indirectly (do you use &lt;a href=&#34;https://www.chromium.org/developers/design-documents/video&#34;&gt;Chrome?&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;It has a command line program called &lt;code&gt;ffmpeg&lt;/code&gt;, a very simple yet powerful binary. For instance, you can convert from &lt;code&gt;mp4&lt;/code&gt; to the container &lt;code&gt;avi&lt;/code&gt; just by typing the follow command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ ffmpeg -i input.mp4 output.avi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We just made a &lt;strong&gt;remuxing&lt;/strong&gt; here, which is converting from one container to another one. Technically FFmpeg could also be doing a transcoding but we&#39;ll talk about that later.&lt;/p&gt; &#xA;&lt;h2&gt;FFmpeg command line tool 101&lt;/h2&gt; &#xA;&lt;p&gt;FFmpeg does have a &lt;a href=&#34;https://www.ffmpeg.org/ffmpeg.html&#34;&gt;documentation&lt;/a&gt; that does a great job of explaining how it works.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# you can also look for the documentation using the command line&#xA;&#xA;ffmpeg -h full | grep -A 10 -B 10 avoid_negative_ts&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To make things short, the FFmpeg command line program expects the following argument format to perform its actions &lt;code&gt;ffmpeg {1} {2} -i {3} {4} {5}&lt;/code&gt;, where:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;global options&lt;/li&gt; &#xA; &lt;li&gt;input file options&lt;/li&gt; &#xA; &lt;li&gt;input url&lt;/li&gt; &#xA; &lt;li&gt;output file options&lt;/li&gt; &#xA; &lt;li&gt;output url&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The parts 2, 3, 4 and 5 can be as many as you need. It&#39;s easier to understand this argument format in action:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# WARNING: this file is around 300MB&#xA;$ wget -O bunny_1080p_60fps.mp4 http://distribution.bbb3d.renderfarming.net/video/mp4/bbb_sunflower_1080p_60fps_normal.mp4&#xA;&#xA;$ ffmpeg \&#xA;-y \ # global options&#xA;-c:a libfdk_aac \ # input options&#xA;-i bunny_1080p_60fps.mp4 \ # input url&#xA;-c:v libvpx-vp9 -c:a libvorbis \ # output options&#xA;bunny_1080p_60fps_vp9.webm # output url&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This command takes an input file &lt;code&gt;mp4&lt;/code&gt; containing two streams (an audio encoded with &lt;code&gt;aac&lt;/code&gt; CODEC and a video encoded using &lt;code&gt;h264&lt;/code&gt; CODEC) and convert it to &lt;code&gt;webm&lt;/code&gt;, changing its audio and video CODECs too.&lt;/p&gt; &#xA;&lt;p&gt;We could simplify the command above but then be aware that FFmpeg will adopt or guess the default values for you. For instance when you just type &lt;code&gt;ffmpeg -i input.avi output.mp4&lt;/code&gt; what audio/video CODEC does it use to produce the &lt;code&gt;output.mp4&lt;/code&gt;?&lt;/p&gt; &#xA;&lt;p&gt;Werner Robitza wrote a must read/execute &lt;a href=&#34;http://slhck.info/ffmpeg-encoding-course/#/&#34;&gt;tutorial about encoding and editing with FFmpeg&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Common video operations&lt;/h1&gt; &#xA;&lt;p&gt;While working with audio/video we usually do a set of tasks with the media.&lt;/p&gt; &#xA;&lt;h2&gt;Transcoding&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/img/transcoding.png&#34; alt=&#34;transcoding&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;What?&lt;/strong&gt; the act of converting one of the streams (audio or video) from one CODEC to another one.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Why?&lt;/strong&gt; sometimes some devices (TVs, smartphones, console and etc) doesn&#39;t support X but Y and newer CODECs provide better compression rate.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;How?&lt;/strong&gt; converting an &lt;code&gt;H264&lt;/code&gt; (AVC) video to an &lt;code&gt;H265&lt;/code&gt; (HEVC).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ ffmpeg \&#xA;-i bunny_1080p_60fps.mp4 \&#xA;-c:v libx265 \&#xA;bunny_1080p_60fps_h265.mp4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Transmuxing&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/img/transmuxing.png&#34; alt=&#34;transmuxing&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;What?&lt;/strong&gt; the act of converting from one format (container) to another one.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Why?&lt;/strong&gt; sometimes some devices (TVs, smartphones, console and etc) doesn&#39;t support X but Y and sometimes newer containers provide modern required features.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;How?&lt;/strong&gt; converting a &lt;code&gt;mp4&lt;/code&gt; to a &lt;code&gt;ts&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ ffmpeg \&#xA;-i bunny_1080p_60fps.mp4 \&#xA;-c copy \ # just saying to ffmpeg to skip encoding&#xA;bunny_1080p_60fps.ts&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Transrating&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/img/transrating.png&#34; alt=&#34;transrating&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;What?&lt;/strong&gt; the act of changing the bit rate, or producing other renditions.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Why?&lt;/strong&gt; people will try to watch your video in a &lt;code&gt;2G&lt;/code&gt; (edge) connection using a less powerful smartphone or in a &lt;code&gt;fiber&lt;/code&gt; Internet connection on their 4K TVs therefore you should offer more than one rendition of the same video with different bit rate.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;How?&lt;/strong&gt; producing a rendition with bit rate between 3856K and 2000K.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ ffmpeg \&#xA;-i bunny_1080p_60fps.mp4 \&#xA;-minrate 964K -maxrate 3856K -bufsize 2000K \&#xA;bunny_1080p_60fps_transrating_964_3856.mp4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Usually we&#39;ll be using transrating with transsizing. Werner Robitza wrote another must read/execute &lt;a href=&#34;http://slhck.info/posts/&#34;&gt;series of posts about FFmpeg rate control&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Transsizing&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/img/transsizing.png&#34; alt=&#34;transsizing&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;What?&lt;/strong&gt; the act of converting from one resolution to another one. As said before transsizing is often used with transrating.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Why?&lt;/strong&gt; reasons are about the same as for the transrating.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;How?&lt;/strong&gt; converting a &lt;code&gt;1080p&lt;/code&gt; to a &lt;code&gt;480p&lt;/code&gt; resolution.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ ffmpeg \&#xA;-i bunny_1080p_60fps.mp4 \&#xA;-vf scale=480:-1 \&#xA;bunny_1080p_60fps_transsizing_480.mp4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Bonus Round: Adaptive Streaming&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/img/adaptive-streaming.png&#34; alt=&#34;adaptive streaming&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;What?&lt;/strong&gt; the act of producing many resolutions (bit rates) and split the media into chunks and serve them via http.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Why?&lt;/strong&gt; to provide a flexible media that can be watched on a low end smartphone or on a 4K TV, it&#39;s also easy to scale and deploy but it can add latency.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;How?&lt;/strong&gt; creating an adaptive WebM using DASH.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# video streams&#xA;$ ffmpeg -i bunny_1080p_60fps.mp4 -c:v libvpx-vp9 -s 160x90 -b:v 250k -keyint_min 150 -g 150 -an -f webm -dash 1 video_160x90_250k.webm&#xA;&#xA;$ ffmpeg -i bunny_1080p_60fps.mp4 -c:v libvpx-vp9 -s 320x180 -b:v 500k -keyint_min 150 -g 150 -an -f webm -dash 1 video_320x180_500k.webm&#xA;&#xA;$ ffmpeg -i bunny_1080p_60fps.mp4 -c:v libvpx-vp9 -s 640x360 -b:v 750k -keyint_min 150 -g 150 -an -f webm -dash 1 video_640x360_750k.webm&#xA;&#xA;$ ffmpeg -i bunny_1080p_60fps.mp4 -c:v libvpx-vp9 -s 640x360 -b:v 1000k -keyint_min 150 -g 150 -an -f webm -dash 1 video_640x360_1000k.webm&#xA;&#xA;$ ffmpeg -i bunny_1080p_60fps.mp4 -c:v libvpx-vp9 -s 1280x720 -b:v 1500k -keyint_min 150 -g 150 -an -f webm -dash 1 video_1280x720_1500k.webm&#xA;&#xA;# audio streams&#xA;$ ffmpeg -i bunny_1080p_60fps.mp4 -c:a libvorbis -b:a 128k -vn -f webm -dash 1 audio_128k.webm&#xA;&#xA;# the DASH manifest&#xA;$ ffmpeg \&#xA; -f webm_dash_manifest -i video_160x90_250k.webm \&#xA; -f webm_dash_manifest -i video_320x180_500k.webm \&#xA; -f webm_dash_manifest -i video_640x360_750k.webm \&#xA; -f webm_dash_manifest -i video_640x360_1000k.webm \&#xA; -f webm_dash_manifest -i video_1280x720_500k.webm \&#xA; -f webm_dash_manifest -i audio_128k.webm \&#xA; -c copy -map 0 -map 1 -map 2 -map 3 -map 4 -map 5 \&#xA; -f webm_dash_manifest \&#xA; -adaptation_sets &#34;id=0,streams=0,1,2,3,4 id=1,streams=5&#34; \&#xA; manifest.mpd&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;PS: I stole this example from the &lt;a href=&#34;http://wiki.webmproject.org/adaptive-streaming/instructions-to-playback-adaptive-webm-using-dash&#34;&gt;Instructions to playback Adaptive WebM using DASH&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Going beyond&lt;/h2&gt; &#xA;&lt;p&gt;There are &lt;a href=&#34;https://github.com/leandromoreira/digital_video_introduction/raw/master/encoding_pratical_examples.md#split-and-merge-smoothly&#34;&gt;many and many other usages for FFmpeg&lt;/a&gt;. I use it in conjunction with &lt;em&gt;iMovie&lt;/em&gt; to produce/edit some videos for YouTube and you can certainly use it professionally.&lt;/p&gt; &#xA;&lt;h1&gt;Learn FFmpeg libav the Hard Way&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Don&#39;t you wonder sometimes &#39;bout sound and vision? &lt;strong&gt;David Robert Jones&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Since the &lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/#ffmpeg---command-line&#34;&gt;FFmpeg&lt;/a&gt; is so useful as a command line tool to do essential tasks over the media files, how can we use it in our programs?&lt;/p&gt; &#xA;&lt;p&gt;FFmpeg is &lt;a href=&#34;https://www.ffmpeg.org/doxygen/trunk/index.html&#34;&gt;composed by several libraries&lt;/a&gt; that can be integrated into our own programs. Usually, when you install FFmpeg, it installs automatically all these libraries. I&#39;ll be referring to the set of these libraries as &lt;strong&gt;FFmpeg libav&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;This title is a homage to Zed Shaw&#39;s series &lt;a href=&#34;https://learncodethehardway.org/&#34;&gt;Learn X the Hard Way&lt;/a&gt;, particularly his book Learn C the Hard Way.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Chapter 0 - The infamous hello world&lt;/h2&gt; &#xA;&lt;p&gt;This hello world actually won&#39;t show the message &lt;code&gt;&#34;hello world&#34;&lt;/code&gt; in the terminal &lt;span&gt;ðŸ‘…&lt;/span&gt; Instead we&#39;re going to &lt;strong&gt;print out information about the video&lt;/strong&gt;, things like its format (container), duration, resolution, audio channels and, in the end, we&#39;ll &lt;strong&gt;decode some frames and save them as image files&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;FFmpeg libav architecture&lt;/h3&gt; &#xA;&lt;p&gt;But before we start to code, let&#39;s learn how &lt;strong&gt;FFmpeg libav architecture&lt;/strong&gt; works and how its components communicate with others.&lt;/p&gt; &#xA;&lt;p&gt;Here&#39;s a diagram of the process of decoding a video:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/img/decoding.png&#34; alt=&#34;ffmpeg libav architecture - decoding process&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;You&#39;ll first need to load your media file into a component called &lt;a href=&#34;https://ffmpeg.org/doxygen/trunk/structAVFormatContext.html&#34;&gt;&lt;code&gt;AVFormatContext&lt;/code&gt;&lt;/a&gt; (the video container is also known as format). It actually doesn&#39;t fully load the whole file: it often only reads the header.&lt;/p&gt; &#xA;&lt;p&gt;Once we loaded the minimal &lt;strong&gt;header of our container&lt;/strong&gt;, we can access its streams (think of them as a rudimentary audio and video data). Each stream will be available in a component called &lt;a href=&#34;https://ffmpeg.org/doxygen/trunk/structAVStream.html&#34;&gt;&lt;code&gt;AVStream&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Stream is a fancy name for a continuous flow of data.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Suppose our video has two streams: an audio encoded with &lt;a href=&#34;https://en.wikipedia.org/wiki/Advanced_Audio_Coding&#34;&gt;AAC CODEC&lt;/a&gt; and a video encoded with &lt;a href=&#34;https://en.wikipedia.org/wiki/H.264/MPEG-4_AVC&#34;&gt;H264 (AVC) CODEC&lt;/a&gt;. From each stream we can extract &lt;strong&gt;pieces (slices) of data&lt;/strong&gt; called packets that will be loaded into components named &lt;a href=&#34;https://ffmpeg.org/doxygen/trunk/structAVPacket.html&#34;&gt;&lt;code&gt;AVPacket&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;strong&gt;data inside the packets are still coded&lt;/strong&gt; (compressed) and in order to decode the packets, we need to pass them to a specific &lt;a href=&#34;https://ffmpeg.org/doxygen/trunk/structAVCodec.html&#34;&gt;&lt;code&gt;AVCodec&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;AVCodec&lt;/code&gt; will decode them into &lt;a href=&#34;https://ffmpeg.org/doxygen/trunk/structAVFrame.html&#34;&gt;&lt;code&gt;AVFrame&lt;/code&gt;&lt;/a&gt; and finally, this component gives us &lt;strong&gt;the uncompressed frame&lt;/strong&gt;. Noticed that the same terminology/process is used either by audio and video stream.&lt;/p&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;p&gt;Since some people were &lt;a href=&#34;https://github.com/leandromoreira/ffmpeg-libav-tutorial/issues?utf8=%E2%9C%93&amp;amp;q=is%3Aissue+is%3Aopen+compiling&#34;&gt;facing issues while compiling or running the examples&lt;/a&gt; &lt;strong&gt;we&#39;re going to use &lt;a href=&#34;https://docs.docker.com/install/&#34;&gt;&lt;code&gt;Docker&lt;/code&gt;&lt;/a&gt; as our development/runner environment,&lt;/strong&gt; we&#39;ll also use the big buck bunny video so if you don&#39;t have it locally just run the command &lt;code&gt;make fetch_small_bunny_video&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Chapter 0 - code walkthrough&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;TLDR; show me the &lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/0_hello_world.c&#34;&gt;code&lt;/a&gt; and execution.&lt;/h4&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ make run_hello&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;We&#39;ll skip some details, but don&#39;t worry: the &lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/0_hello_world.c&#34;&gt;source code is available at github&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We&#39;re going to allocate memory to the component &lt;a href=&#34;http://ffmpeg.org/doxygen/trunk/structAVFormatContext.html&#34;&gt;&lt;code&gt;AVFormatContext&lt;/code&gt;&lt;/a&gt; that will hold information about the format (container).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;AVFormatContext *pFormatContext = avformat_alloc_context();&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now we&#39;re going to open the file and read its header and fill the &lt;code&gt;AVFormatContext&lt;/code&gt; with minimal information about the format (notice that usually the codecs are not opened). The function used to do this is &lt;a href=&#34;http://ffmpeg.org/doxygen/trunk/group__lavf__decoding.html#ga31d601155e9035d5b0e7efedc894ee49&#34;&gt;&lt;code&gt;avformat_open_input&lt;/code&gt;&lt;/a&gt;. It expects an &lt;code&gt;AVFormatContext&lt;/code&gt;, a &lt;code&gt;filename&lt;/code&gt; and two optional arguments: the &lt;a href=&#34;https://ffmpeg.org/doxygen/trunk/structAVInputFormat.html&#34;&gt;&lt;code&gt;AVInputFormat&lt;/code&gt;&lt;/a&gt; (if you pass &lt;code&gt;NULL&lt;/code&gt;, FFmpeg will guess the format) and the &lt;a href=&#34;https://ffmpeg.org/doxygen/trunk/structAVDictionary.html&#34;&gt;&lt;code&gt;AVDictionary&lt;/code&gt;&lt;/a&gt; (which are the options to the demuxer).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;avformat_open_input(&amp;amp;pFormatContext, filename, NULL, NULL);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We can print the format name and the media duration:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;printf(&#34;Format %s, duration %lld us&#34;, pFormatContext-&amp;gt;iformat-&amp;gt;long_name, pFormatContext-&amp;gt;duration);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To access the &lt;code&gt;streams&lt;/code&gt;, we need to read data from the media. The function &lt;a href=&#34;https://ffmpeg.org/doxygen/trunk/group__lavf__decoding.html#gad42172e27cddafb81096939783b157bb&#34;&gt;&lt;code&gt;avformat_find_stream_info&lt;/code&gt;&lt;/a&gt; does that. Now, the &lt;code&gt;pFormatContext-&amp;gt;nb_streams&lt;/code&gt; will hold the amount of streams and the &lt;code&gt;pFormatContext-&amp;gt;streams[i]&lt;/code&gt; will give us the &lt;code&gt;i&lt;/code&gt; stream (an &lt;a href=&#34;https://ffmpeg.org/doxygen/trunk/structAVStream.html&#34;&gt;&lt;code&gt;AVStream&lt;/code&gt;&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;avformat_find_stream_info(pFormatContext,  NULL);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now we&#39;ll loop through all the streams.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;for (int i = 0; i &amp;lt; pFormatContext-&amp;gt;nb_streams; i++)&#xA;{&#xA;  //&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For each stream, we&#39;re going to keep the &lt;a href=&#34;https://ffmpeg.org/doxygen/trunk/structAVCodecParameters.html&#34;&gt;&lt;code&gt;AVCodecParameters&lt;/code&gt;&lt;/a&gt;, which describes the properties of a codec used by the stream &lt;code&gt;i&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;AVCodecParameters *pLocalCodecParameters = pFormatContext-&amp;gt;streams[i]-&amp;gt;codecpar;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;With the codec properties we can look up the proper CODEC querying the function &lt;a href=&#34;https://ffmpeg.org/doxygen/trunk/group__lavc__decoding.html#ga19a0ca553277f019dd5b0fec6e1f9dca&#34;&gt;&lt;code&gt;avcodec_find_decoder&lt;/code&gt;&lt;/a&gt; and find the registered decoder for the codec id and return an &lt;a href=&#34;http://ffmpeg.org/doxygen/trunk/structAVCodec.html&#34;&gt;&lt;code&gt;AVCodec&lt;/code&gt;&lt;/a&gt;, the component that knows how to en&lt;strong&gt;CO&lt;/strong&gt;de and &lt;strong&gt;DEC&lt;/strong&gt;ode the stream.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;AVCodec *pLocalCodec = avcodec_find_decoder(pLocalCodecParameters-&amp;gt;codec_id);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now we can print information about the codecs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;// specific for video and audio&#xA;if (pLocalCodecParameters-&amp;gt;codec_type == AVMEDIA_TYPE_VIDEO) {&#xA;  printf(&#34;Video Codec: resolution %d x %d&#34;, pLocalCodecParameters-&amp;gt;width, pLocalCodecParameters-&amp;gt;height);&#xA;} else if (pLocalCodecParameters-&amp;gt;codec_type == AVMEDIA_TYPE_AUDIO) {&#xA;  printf(&#34;Audio Codec: %d channels, sample rate %d&#34;, pLocalCodecParameters-&amp;gt;channels, pLocalCodecParameters-&amp;gt;sample_rate);&#xA;}&#xA;// general&#xA;printf(&#34;\tCodec %s ID %d bit_rate %lld&#34;, pLocalCodec-&amp;gt;long_name, pLocalCodec-&amp;gt;id, pLocalCodecParameters-&amp;gt;bit_rate);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;With the codec, we can allocate memory for the &lt;a href=&#34;https://ffmpeg.org/doxygen/trunk/structAVCodecContext.html&#34;&gt;&lt;code&gt;AVCodecContext&lt;/code&gt;&lt;/a&gt;, which will hold the context for our decode/encode process, but then we need to fill this codec context with CODEC parameters; we do that with &lt;a href=&#34;https://ffmpeg.org/doxygen/trunk/group__lavc__core.html#gac7b282f51540ca7a99416a3ba6ee0d16&#34;&gt;&lt;code&gt;avcodec_parameters_to_context&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Once we filled the codec context, we need to open the codec. We call the function &lt;a href=&#34;https://ffmpeg.org/doxygen/trunk/group__lavc__core.html#ga11f785a188d7d9df71621001465b0f1d&#34;&gt;&lt;code&gt;avcodec_open2&lt;/code&gt;&lt;/a&gt; and then we can use it.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;AVCodecContext *pCodecContext = avcodec_alloc_context3(pCodec);&#xA;avcodec_parameters_to_context(pCodecContext, pCodecParameters);&#xA;avcodec_open2(pCodecContext, pCodec, NULL);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now we&#39;re going to read the packets from the stream and decode them into frames but first, we need to allocate memory for both components, the &lt;a href=&#34;https://ffmpeg.org/doxygen/trunk/structAVPacket.html&#34;&gt;&lt;code&gt;AVPacket&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://ffmpeg.org/doxygen/trunk/structAVFrame.html&#34;&gt;&lt;code&gt;AVFrame&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;AVPacket *pPacket = av_packet_alloc();&#xA;AVFrame *pFrame = av_frame_alloc();&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Let&#39;s feed our packets from the streams with the function &lt;a href=&#34;https://ffmpeg.org/doxygen/trunk/group__lavf__decoding.html#ga4fdb3084415a82e3810de6ee60e46a61&#34;&gt;&lt;code&gt;av_read_frame&lt;/code&gt;&lt;/a&gt; while it has packets.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;while (av_read_frame(pFormatContext, pPacket) &amp;gt;= 0) {&#xA;  //...&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Let&#39;s &lt;strong&gt;send the raw data packet&lt;/strong&gt; (compressed frame) to the decoder, through the codec context, using the function &lt;a href=&#34;https://ffmpeg.org/doxygen/trunk/group__lavc__decoding.html#ga58bc4bf1e0ac59e27362597e467efff3&#34;&gt;&lt;code&gt;avcodec_send_packet&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;avcodec_send_packet(pCodecContext, pPacket);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And let&#39;s &lt;strong&gt;receive the raw data frame&lt;/strong&gt; (uncompressed frame) from the decoder, through the same codec context, using the function &lt;a href=&#34;https://ffmpeg.org/doxygen/trunk/group__lavc__decoding.html#ga11e6542c4e66d3028668788a1a74217c&#34;&gt;&lt;code&gt;avcodec_receive_frame&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;avcodec_receive_frame(pCodecContext, pFrame);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We can print the frame number, the &lt;a href=&#34;https://en.wikipedia.org/wiki/Presentation_timestamp&#34;&gt;PTS&lt;/a&gt;, DTS, &lt;a href=&#34;https://en.wikipedia.org/wiki/Video_compression_picture_types&#34;&gt;frame type&lt;/a&gt; and etc.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;printf(&#xA;    &#34;Frame %c (%d) pts %d dts %d key_frame %d [coded_picture_number %d, display_picture_number %d]&#34;,&#xA;    av_get_picture_type_char(pFrame-&amp;gt;pict_type),&#xA;    pCodecContext-&amp;gt;frame_number,&#xA;    pFrame-&amp;gt;pts,&#xA;    pFrame-&amp;gt;pkt_dts,&#xA;    pFrame-&amp;gt;key_frame,&#xA;    pFrame-&amp;gt;coded_picture_number,&#xA;    pFrame-&amp;gt;display_picture_number&#xA;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally we can save our decoded frame into a &lt;a href=&#34;https://en.wikipedia.org/wiki/Netpbm_format#PGM_example&#34;&gt;simple gray image&lt;/a&gt;. The process is very simple, we&#39;ll use the &lt;code&gt;pFrame-&amp;gt;data&lt;/code&gt; where the index is related to the &lt;a href=&#34;https://en.wikipedia.org/wiki/YCbCr&#34;&gt;planes Y, Cb and Cr&lt;/a&gt;, we just picked &lt;code&gt;0&lt;/code&gt; (Y) to save our gray image.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;save_gray_frame(pFrame-&amp;gt;data[0], pFrame-&amp;gt;linesize[0], pFrame-&amp;gt;width, pFrame-&amp;gt;height, frame_filename);&#xA;&#xA;static void save_gray_frame(unsigned char *buf, int wrap, int xsize, int ysize, char *filename)&#xA;{&#xA;    FILE *f;&#xA;    int i;&#xA;    f = fopen(filename,&#34;w&#34;);&#xA;    // writing the minimal required header for a pgm file format&#xA;    // portable graymap format -&amp;gt; https://en.wikipedia.org/wiki/Netpbm_format#PGM_example&#xA;    fprintf(f, &#34;P5\n%d %d\n%d\n&#34;, xsize, ysize, 255);&#xA;&#xA;    // writing line by line&#xA;    for (i = 0; i &amp;lt; ysize; i++)&#xA;        fwrite(buf + i * wrap, 1, xsize, f);&#xA;    fclose(f);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And voilÃ ! Now we have a gray scale image with 2MB:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/img/generated_frame.png&#34; alt=&#34;saved frame&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Chapter 1 - syncing audio and video&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Be the player&lt;/strong&gt; - a young JS developer writing a new MSE video player.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Before we move to &lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/#chapter-2---transcoding&#34;&gt;code a transcoding example&lt;/a&gt; let&#39;s talk about &lt;strong&gt;timing&lt;/strong&gt;, or how a video player knows the right time to play a frame.&lt;/p&gt; &#xA;&lt;p&gt;In the last example, we saved some frames that can be seen here:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/img/hello_world_frames/frame0.png&#34; alt=&#34;frame 0&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/img/hello_world_frames/frame1.png&#34; alt=&#34;frame 1&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/img/hello_world_frames/frame2.png&#34; alt=&#34;frame 2&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/img/hello_world_frames/frame3.png&#34; alt=&#34;frame 3&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/img/hello_world_frames/frame4.png&#34; alt=&#34;frame 4&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/img/hello_world_frames/frame5.png&#34; alt=&#34;frame 5&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;When we&#39;re designing a video player we need to &lt;strong&gt;play each frame at a given pace&lt;/strong&gt;, otherwise it would be hard to pleasantly see the video either because it&#39;s playing so fast or so slow.&lt;/p&gt; &#xA;&lt;p&gt;Therefore we need to introduce some logic to play each frame smoothly. For that matter, each frame has a &lt;strong&gt;presentation timestamp&lt;/strong&gt; (PTS) which is an increasing number factored in a &lt;strong&gt;timebase&lt;/strong&gt; that is a rational number (where the denominator is known as &lt;strong&gt;timescale&lt;/strong&gt;) divisible by the &lt;strong&gt;frame rate (fps)&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;It&#39;s easier to understand when we look at some examples, let&#39;s simulate some scenarios.&lt;/p&gt; &#xA;&lt;p&gt;For a &lt;code&gt;fps=60/1&lt;/code&gt; and &lt;code&gt;timebase=1/60000&lt;/code&gt; each PTS will increase &lt;code&gt;timescale / fps = 1000&lt;/code&gt; therefore the &lt;strong&gt;PTS real time&lt;/strong&gt; for each frame could be (supposing it started at 0):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;frame=0, PTS = 0, PTS_TIME = 0&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;frame=1, PTS = 1000, PTS_TIME = PTS * timebase = 0.016&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;frame=2, PTS = 2000, PTS_TIME = PTS * timebase = 0.033&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For almost the same scenario but with a timebase equal to &lt;code&gt;1/60&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;frame=0, PTS = 0, PTS_TIME = 0&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;frame=1, PTS = 1, PTS_TIME = PTS * timebase = 0.016&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;frame=2, PTS = 2, PTS_TIME = PTS * timebase = 0.033&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;frame=3, PTS = 3, PTS_TIME = PTS * timebase = 0.050&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For a &lt;code&gt;fps=25/1&lt;/code&gt; and &lt;code&gt;timebase=1/75&lt;/code&gt; each PTS will increase &lt;code&gt;timescale / fps = 3&lt;/code&gt; and the PTS time could be:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;frame=0, PTS = 0, PTS_TIME = 0&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;frame=1, PTS = 3, PTS_TIME = PTS * timebase = 0.04&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;frame=2, PTS = 6, PTS_TIME = PTS * timebase = 0.08&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;frame=3, PTS = 9, PTS_TIME = PTS * timebase = 0.12&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;...&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;frame=24, PTS = 72, PTS_TIME = PTS * timebase = 0.96&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;...&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;frame=4064, PTS = 12192, PTS_TIME = PTS * timebase = 162.56&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Now with the &lt;code&gt;pts_time&lt;/code&gt; we can find a way to render this synched with audio &lt;code&gt;pts_time&lt;/code&gt; or with a system clock. The FFmpeg libav provides these info through its API:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;fps = &lt;a href=&#34;https://ffmpeg.org/doxygen/trunk/structAVStream.html#a946e1e9b89eeeae4cab8a833b482c1ad&#34;&gt;&lt;code&gt;AVStream-&amp;gt;avg_frame_rate&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;tbr = &lt;a href=&#34;https://ffmpeg.org/doxygen/trunk/structAVStream.html#ad63fb11cc1415e278e09ddc676e8a1ad&#34;&gt;&lt;code&gt;AVStream-&amp;gt;r_frame_rate&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;tbn = &lt;a href=&#34;https://ffmpeg.org/doxygen/trunk/structAVStream.html#a9db755451f14e2bf590d4b85d82b32e6&#34;&gt;&lt;code&gt;AVStream-&amp;gt;time_base&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Just out of curiosity, the frames we saved were sent in a DTS order (frames: 1,6,4,2,3,5) but played at a PTS order (frames: 1,2,3,4,5). Also, notice how cheap are B-Frames in comparison to P or I-Frames.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;LOG: AVStream-&amp;gt;r_frame_rate 60/1&#xA;LOG: AVStream-&amp;gt;time_base 1/60000&#xA;...&#xA;LOG: Frame 1 (type=I, size=153797 bytes) pts 6000 key_frame 1 [DTS 0]&#xA;LOG: Frame 2 (type=B, size=8117 bytes) pts 7000 key_frame 0 [DTS 3]&#xA;LOG: Frame 3 (type=B, size=8226 bytes) pts 8000 key_frame 0 [DTS 4]&#xA;LOG: Frame 4 (type=B, size=17699 bytes) pts 9000 key_frame 0 [DTS 2]&#xA;LOG: Frame 5 (type=B, size=6253 bytes) pts 10000 key_frame 0 [DTS 5]&#xA;LOG: Frame 6 (type=P, size=34992 bytes) pts 11000 key_frame 0 [DTS 1]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Chapter 2 - remuxing&lt;/h2&gt; &#xA;&lt;p&gt;Remuxing is the act of changing from one format (container) to another, for instance, we can change a &lt;a href=&#34;https://en.wikipedia.org/wiki/MPEG-4_Part_14&#34;&gt;MPEG-4&lt;/a&gt; video to a &lt;a href=&#34;https://en.wikipedia.org/wiki/MPEG_transport_stream&#34;&gt;MPEG-TS&lt;/a&gt; one without much pain using FFmpeg:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ffmpeg input.mp4 -c copy output.ts&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It&#39;ll demux the mp4 but it won&#39;t decode or encode it (&lt;code&gt;-c copy&lt;/code&gt;) and in the end, it&#39;ll mux it into a &lt;code&gt;mpegts&lt;/code&gt; file. If you don&#39;t provide the format &lt;code&gt;-f&lt;/code&gt; the ffmpeg will try to guess it based on the file&#39;s extension.&lt;/p&gt; &#xA;&lt;p&gt;The general usage of FFmpeg or the libav follows a pattern/architecture or workflow:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://ffmpeg.org/doxygen/trunk/protocols_8c.html&#34;&gt;protocol layer&lt;/a&gt;&lt;/strong&gt; - it accepts an &lt;code&gt;input&lt;/code&gt; (a &lt;code&gt;file&lt;/code&gt; for instance but it could be a &lt;code&gt;rtmp&lt;/code&gt; or &lt;code&gt;HTTP&lt;/code&gt; input as well)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://ffmpeg.org/doxygen/trunk/group__libavf.html&#34;&gt;format layer&lt;/a&gt;&lt;/strong&gt; - it &lt;code&gt;demuxes&lt;/code&gt; its content, revealing mostly metadata and its streams&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://ffmpeg.org/doxygen/trunk/group__libavc.html&#34;&gt;codec layer&lt;/a&gt;&lt;/strong&gt; - it &lt;code&gt;decodes&lt;/code&gt; its compressed streams data &lt;sup&gt;&lt;em&gt;optional&lt;/em&gt;&lt;/sup&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://ffmpeg.org/doxygen/trunk/group__lavfi.html&#34;&gt;pixel layer&lt;/a&gt;&lt;/strong&gt; - it can also apply some &lt;code&gt;filters&lt;/code&gt; to the raw frames (like resizing)&lt;sup&gt;&lt;em&gt;optional&lt;/em&gt;&lt;/sup&gt;&lt;/li&gt; &#xA; &lt;li&gt;and then it does the reverse path&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://ffmpeg.org/doxygen/trunk/group__libavc.html&#34;&gt;codec layer&lt;/a&gt;&lt;/strong&gt; - it &lt;code&gt;encodes&lt;/code&gt; (or &lt;code&gt;re-encodes&lt;/code&gt; or even &lt;code&gt;transcodes&lt;/code&gt;) the raw frames&lt;sup&gt;&lt;em&gt;optional&lt;/em&gt;&lt;/sup&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://ffmpeg.org/doxygen/trunk/group__libavf.html&#34;&gt;format layer&lt;/a&gt;&lt;/strong&gt; - it &lt;code&gt;muxes&lt;/code&gt; (or &lt;code&gt;remuxes&lt;/code&gt;) the raw streams (the compressed data)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://ffmpeg.org/doxygen/trunk/protocols_8c.html&#34;&gt;protocol layer&lt;/a&gt;&lt;/strong&gt; - and finally the muxed data is sent to an &lt;code&gt;output&lt;/code&gt; (another file or maybe a network remote server)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/img/ffmpeg_libav_workflow.jpeg&#34; alt=&#34;ffmpeg libav workflow&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;This graph is strongly inspired by &lt;a href=&#34;http://leixiaohua1020.github.io/#ffmpeg-development-examples&#34;&gt;Leixiaohua&#39;s&lt;/a&gt; and &lt;a href=&#34;https://slhck.info/ffmpeg-encoding-course/#/9&#34;&gt;Slhck&#39;s&lt;/a&gt; works.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Now let&#39;s code an example using libav to provide the same effect as in &lt;code&gt;ffmpeg input.mp4 -c copy output.ts&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We&#39;re going to read from an input (&lt;code&gt;input_format_context&lt;/code&gt;) and change it to another output (&lt;code&gt;output_format_context&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;AVFormatContext *input_format_context = NULL;&#xA;AVFormatContext *output_format_context = NULL;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We start doing the usually allocate memory and open the input format. For this specific case, we&#39;re going to open an input file and allocate memory for an output file.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;if ((ret = avformat_open_input(&amp;amp;input_format_context, in_filename, NULL, NULL)) &amp;lt; 0) {&#xA;  fprintf(stderr, &#34;Could not open input file &#39;%s&#39;&#34;, in_filename);&#xA;  goto end;&#xA;}&#xA;if ((ret = avformat_find_stream_info(input_format_context, NULL)) &amp;lt; 0) {&#xA;  fprintf(stderr, &#34;Failed to retrieve input stream information&#34;);&#xA;  goto end;&#xA;}&#xA;&#xA;avformat_alloc_output_context2(&amp;amp;output_format_context, NULL, NULL, out_filename);&#xA;if (!output_format_context) {&#xA;  fprintf(stderr, &#34;Could not create output context\n&#34;);&#xA;  ret = AVERROR_UNKNOWN;&#xA;  goto end;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We&#39;re going to remux only the video, audio and subtitle types of streams so we&#39;re holding what streams we&#39;ll be using into an array of indexes.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;number_of_streams = input_format_context-&amp;gt;nb_streams;&#xA;streams_list = av_mallocz_array(number_of_streams, sizeof(*streams_list));&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Just after we allocated the required memory, we&#39;re going to loop throughout all the streams and for each one we need to create new out stream into our output format context, using the &lt;a href=&#34;https://ffmpeg.org/doxygen/trunk/group__lavf__core.html#gadcb0fd3e507d9b58fe78f61f8ad39827&#34;&gt;avformat_new_stream&lt;/a&gt; function. Notice that we&#39;re marking all the streams that aren&#39;t video, audio or subtitle so we can skip them after.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;for (i = 0; i &amp;lt; input_format_context-&amp;gt;nb_streams; i++) {&#xA;  AVStream *out_stream;&#xA;  AVStream *in_stream = input_format_context-&amp;gt;streams[i];&#xA;  AVCodecParameters *in_codecpar = in_stream-&amp;gt;codecpar;&#xA;  if (in_codecpar-&amp;gt;codec_type != AVMEDIA_TYPE_AUDIO &amp;amp;&amp;amp;&#xA;      in_codecpar-&amp;gt;codec_type != AVMEDIA_TYPE_VIDEO &amp;amp;&amp;amp;&#xA;      in_codecpar-&amp;gt;codec_type != AVMEDIA_TYPE_SUBTITLE) {&#xA;    streams_list[i] = -1;&#xA;    continue;&#xA;  }&#xA;  streams_list[i] = stream_index++;&#xA;  out_stream = avformat_new_stream(output_format_context, NULL);&#xA;  if (!out_stream) {&#xA;    fprintf(stderr, &#34;Failed allocating output stream\n&#34;);&#xA;    ret = AVERROR_UNKNOWN;&#xA;    goto end;&#xA;  }&#xA;  ret = avcodec_parameters_copy(out_stream-&amp;gt;codecpar, in_codecpar);&#xA;  if (ret &amp;lt; 0) {&#xA;    fprintf(stderr, &#34;Failed to copy codec parameters\n&#34;);&#xA;    goto end;&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now we can create the output file.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;if (!(output_format_context-&amp;gt;oformat-&amp;gt;flags &amp;amp; AVFMT_NOFILE)) {&#xA;  ret = avio_open(&amp;amp;output_format_context-&amp;gt;pb, out_filename, AVIO_FLAG_WRITE);&#xA;  if (ret &amp;lt; 0) {&#xA;    fprintf(stderr, &#34;Could not open output file &#39;%s&#39;&#34;, out_filename);&#xA;    goto end;&#xA;  }&#xA;}&#xA;&#xA;ret = avformat_write_header(output_format_context, NULL);&#xA;if (ret &amp;lt; 0) {&#xA;  fprintf(stderr, &#34;Error occurred when opening output file\n&#34;);&#xA;  goto end;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After that, we can copy the streams, packet by packet, from our input to our output streams. We&#39;ll loop while it has packets (&lt;code&gt;av_read_frame&lt;/code&gt;), for each packet we need to re-calculate the PTS and DTS to finally write it (&lt;code&gt;av_interleaved_write_frame&lt;/code&gt;) to our output format context.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;while (1) {&#xA;  AVStream *in_stream, *out_stream;&#xA;  ret = av_read_frame(input_format_context, &amp;amp;packet);&#xA;  if (ret &amp;lt; 0)&#xA;    break;&#xA;  in_stream  = input_format_context-&amp;gt;streams[packet.stream_index];&#xA;  if (packet.stream_index &amp;gt;= number_of_streams || streams_list[packet.stream_index] &amp;lt; 0) {&#xA;    av_packet_unref(&amp;amp;packet);&#xA;    continue;&#xA;  }&#xA;  packet.stream_index = streams_list[packet.stream_index];&#xA;  out_stream = output_format_context-&amp;gt;streams[packet.stream_index];&#xA;  /* copy packet */&#xA;  packet.pts = av_rescale_q_rnd(packet.pts, in_stream-&amp;gt;time_base, out_stream-&amp;gt;time_base, AV_ROUND_NEAR_INF|AV_ROUND_PASS_MINMAX);&#xA;  packet.dts = av_rescale_q_rnd(packet.dts, in_stream-&amp;gt;time_base, out_stream-&amp;gt;time_base, AV_ROUND_NEAR_INF|AV_ROUND_PASS_MINMAX);&#xA;  packet.duration = av_rescale_q(packet.duration, in_stream-&amp;gt;time_base, out_stream-&amp;gt;time_base);&#xA;  // https://ffmpeg.org/doxygen/trunk/structAVPacket.html#ab5793d8195cf4789dfb3913b7a693903&#xA;  packet.pos = -1;&#xA;&#xA;  //https://ffmpeg.org/doxygen/trunk/group__lavf__encoding.html#ga37352ed2c63493c38219d935e71db6c1&#xA;  ret = av_interleaved_write_frame(output_format_context, &amp;amp;packet);&#xA;  if (ret &amp;lt; 0) {&#xA;    fprintf(stderr, &#34;Error muxing packet\n&#34;);&#xA;    break;&#xA;  }&#xA;  av_packet_unref(&amp;amp;packet);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To finalize we need to write the stream trailer to an output media file with &lt;a href=&#34;https://ffmpeg.org/doxygen/trunk/group__lavf__encoding.html#ga7f14007e7dc8f481f054b21614dfec13&#34;&gt;av_write_trailer&lt;/a&gt; function.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;av_write_trailer(output_format_context);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now we&#39;re ready to test it and the first test will be a format (video container) conversion from a MP4 to a MPEG-TS video file. We&#39;re basically making the command line &lt;code&gt;ffmpeg input.mp4 -c copy output.ts&lt;/code&gt; with libav.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make run_remuxing_ts&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It&#39;s working!!! don&#39;t you trust me?! you shouldn&#39;t, we can check it with &lt;code&gt;ffprobe&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ffprobe -i remuxed_small_bunny_1080p_60fps.ts&#xA;&#xA;Input #0, mpegts, from &#39;remuxed_small_bunny_1080p_60fps.ts&#39;:&#xA;  Duration: 00:00:10.03, start: 0.000000, bitrate: 2751 kb/s&#xA;  Program 1&#xA;    Metadata:&#xA;      service_name    : Service01&#xA;      service_provider: FFmpeg&#xA;    Stream #0:0[0x100]: Video: h264 (High) ([27][0][0][0] / 0x001B), yuv420p(progressive), 1920x1080 [SAR 1:1 DAR 16:9], 60 fps, 60 tbr, 90k tbn, 120 tbc&#xA;    Stream #0:1[0x101]: Audio: ac3 ([129][0][0][0] / 0x0081), 48000 Hz, 5.1(side), fltp, 320 kb/s&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To sum up what we did here in a graph, we can revisit our initial &lt;a href=&#34;https://github.com/leandromoreira/ffmpeg-libav-tutorial#ffmpeg-libav-architecture&#34;&gt;idea about how libav works&lt;/a&gt; but showing that we skipped the codec part.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/img/remuxing_libav_components.png&#34; alt=&#34;remuxing libav components&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Before we end this chapter I&#39;d like to show an important part of the remuxing process, &lt;strong&gt;you can pass options to the muxer&lt;/strong&gt;. Let&#39;s say we want to delivery &lt;a href=&#34;https://developer.mozilla.org/en-US/docs/Web/Apps/Fundamentals/Audio_and_video_delivery/Setting_up_adaptive_streaming_media_sources#MPEG-DASH_Encoding&#34;&gt;MPEG-DASH&lt;/a&gt; format for that matter we need to use &lt;a href=&#34;https://stackoverflow.com/a/35180327&#34;&gt;fragmented mp4&lt;/a&gt; (sometimes referred as &lt;code&gt;fmp4&lt;/code&gt;) instead of MPEG-TS or plain MPEG-4.&lt;/p&gt; &#xA;&lt;p&gt;With the &lt;a href=&#34;https://developer.mozilla.org/en-US/docs/Web/API/Media_Source_Extensions_API/Transcoding_assets_for_MSE#Fragmenting&#34;&gt;command line we can do that easily&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ffmpeg -i non_fragmented.mp4 -movflags frag_keyframe+empty_moov+default_base_moof fragmented.mp4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Almost equally easy as the command line is the libav version of it, we just need to pass the options when write the output header, just before the packets copy.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;AVDictionary* opts = NULL;&#xA;av_dict_set(&amp;amp;opts, &#34;movflags&#34;, &#34;frag_keyframe+empty_moov+default_base_moof&#34;, 0);&#xA;ret = avformat_write_header(output_format_context, &amp;amp;opts);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We now can generate this fragmented mp4 file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make run_remuxing_fragmented_mp4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;But to make sure that I&#39;m not lying to you. You can use the amazing site/tool &lt;a href=&#34;http://download.tsi.telecom-paristech.fr/gpac/mp4box.js/filereader.html&#34;&gt;gpac/mp4box.js&lt;/a&gt; or the site &lt;a href=&#34;http://mp4parser.com/&#34;&gt;http://mp4parser.com/&lt;/a&gt; to see the differences, first load up the &#34;common&#34; mp4.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/img/boxes_normal_mp4.png&#34; alt=&#34;mp4 boxes&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;As you can see it has a single &lt;code&gt;mdat&lt;/code&gt; atom/box, &lt;strong&gt;this is place where the video and audio frames are&lt;/strong&gt;. Now load the fragmented mp4 to see which how it spreads the &lt;code&gt;mdat&lt;/code&gt; boxes.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/img/boxes_fragmente_mp4.png&#34; alt=&#34;fragmented mp4 boxes&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Chapter 3 - transcoding&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;TLDR; show me the &lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/3_transcoding.c&#34;&gt;code&lt;/a&gt; and execution.&lt;/h4&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ make run_transcoding&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;We&#39;ll skip some details, but don&#39;t worry: the &lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/3_transcoding.c&#34;&gt;source code is available at github&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;In this chapter, we&#39;re going to create a minimalist transcoder, written in C, that can convert videos coded in H264 to H265 using &lt;strong&gt;FFmpeg/libav&lt;/strong&gt; library specifically &lt;a href=&#34;https://ffmpeg.org/libavcodec.html&#34;&gt;libavcodec&lt;/a&gt;, libavformat, and libavutil.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/img/transcoding_flow.png&#34; alt=&#34;media transcoding flow&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;em&gt;Just a quick recap:&lt;/em&gt; The &lt;a href=&#34;https://www.ffmpeg.org/doxygen/trunk/structAVFormatContext.html&#34;&gt;&lt;strong&gt;AVFormatContext&lt;/strong&gt;&lt;/a&gt; is the abstraction for the format of the media file, aka container (ex: MKV, MP4, Webm, TS). The &lt;a href=&#34;https://www.ffmpeg.org/doxygen/trunk/structAVStream.html&#34;&gt;&lt;strong&gt;AVStream&lt;/strong&gt;&lt;/a&gt; represents each type of data for a given format (ex: audio, video, subtitle, metadata). The &lt;a href=&#34;https://www.ffmpeg.org/doxygen/trunk/structAVPacket.html&#34;&gt;&lt;strong&gt;AVPacket&lt;/strong&gt;&lt;/a&gt; is a slice of compressed data obtained from the &lt;code&gt;AVStream&lt;/code&gt; that can be decoded by an &lt;a href=&#34;https://www.ffmpeg.org/doxygen/trunk/structAVCodec.html&#34;&gt;&lt;strong&gt;AVCodec&lt;/strong&gt;&lt;/a&gt; (ex: av1, h264, vp9, hevc) generating a raw data called &lt;a href=&#34;https://www.ffmpeg.org/doxygen/trunk/structAVFrame.html&#34;&gt;&lt;strong&gt;AVFrame&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Transmuxing&lt;/h3&gt; &#xA;&lt;p&gt;Let&#39;s start with the simple transmuxing operation and then we can build upon this code, the first step is to &lt;strong&gt;load the input file&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;// Allocate an AVFormatContext&#xA;avfc = avformat_alloc_context();&#xA;// Open an input stream and read the header.&#xA;avformat_open_input(avfc, in_filename, NULL, NULL);&#xA;// Read packets of a media file to get stream information.&#xA;avformat_find_stream_info(avfc, NULL);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now we&#39;re going to set up the decoder, the &lt;code&gt;AVFormatContext&lt;/code&gt; will give us access to all the &lt;code&gt;AVStream&lt;/code&gt; components and for each one of them, we can get their &lt;code&gt;AVCodec&lt;/code&gt; and create the particular &lt;code&gt;AVCodecContext&lt;/code&gt; and finally we can open the given codec so we can proceed to the decoding process.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;The &lt;a href=&#34;https://www.ffmpeg.org/doxygen/trunk/structAVCodecContext.html&#34;&gt;&lt;strong&gt;AVCodecContext&lt;/strong&gt;&lt;/a&gt; holds data about media configuration such as bit rate, frame rate, sample rate, channels, height, and many others.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;for (int i = 0; i &amp;lt; avfc-&amp;gt;nb_streams; i++)&#xA;{&#xA;  AVStream *avs = avfc-&amp;gt;streams[i];&#xA;  AVCodec *avc = avcodec_find_decoder(avs-&amp;gt;codecpar-&amp;gt;codec_id);&#xA;  AVCodecContext *avcc = avcodec_alloc_context3(*avc);&#xA;  avcodec_parameters_to_context(*avcc, avs-&amp;gt;codecpar);&#xA;  avcodec_open2(*avcc, *avc, NULL);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We need to prepare the output media file for transmuxing as well, we first &lt;strong&gt;allocate memory&lt;/strong&gt; for the output &lt;code&gt;AVFormatContext&lt;/code&gt;. We create &lt;strong&gt;each stream&lt;/strong&gt; in the output format. In order to pack the stream properly, we &lt;strong&gt;copy the codec parameters&lt;/strong&gt; from the decoder.&lt;/p&gt; &#xA;&lt;p&gt;We &lt;strong&gt;set the flag&lt;/strong&gt; &lt;code&gt;AV_CODEC_FLAG_GLOBAL_HEADER&lt;/code&gt; which tells the encoder that it can use the global headers and finally we open the output &lt;strong&gt;file for write&lt;/strong&gt; and persist the headers.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;avformat_alloc_output_context2(&amp;amp;encoder_avfc, NULL, NULL, out_filename);&#xA;&#xA;AVStream *avs = avformat_new_stream(encoder_avfc, NULL);&#xA;avcodec_parameters_copy(avs-&amp;gt;codecpar, decoder_avs-&amp;gt;codecpar);&#xA;&#xA;if (encoder_avfc-&amp;gt;oformat-&amp;gt;flags &amp;amp; AVFMT_GLOBALHEADER)&#xA;  encoder_avfc-&amp;gt;flags |= AV_CODEC_FLAG_GLOBAL_HEADER;&#xA;&#xA;avio_open(&amp;amp;encoder_avfc-&amp;gt;pb, encoder-&amp;gt;filename, AVIO_FLAG_WRITE);&#xA;avformat_write_header(encoder-&amp;gt;avfc, &amp;amp;muxer_opts);&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We&#39;re getting the &lt;code&gt;AVPacket&lt;/code&gt;&#39;s from the decoder, adjusting the timestamps, and write the packet properly to the output file. Even though the function &lt;code&gt;av_interleaved_write_frame&lt;/code&gt; says &#34;write frame&#34; we are storing the packet. We finish the transmuxing process by writing the stream trailer to the file.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;AVFrame *input_frame = av_frame_alloc();&#xA;AVPacket *input_packet = av_packet_alloc();&#xA;&#xA;while (av_read_frame(decoder_avfc, input_packet) &amp;gt;= 0)&#xA;{&#xA;  av_packet_rescale_ts(input_packet, decoder_video_avs-&amp;gt;time_base, encoder_video_avs-&amp;gt;time_base);&#xA;  av_interleaved_write_frame(*avfc, input_packet) &amp;lt; 0));&#xA;}&#xA;&#xA;av_write_trailer(encoder_avfc);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Transcoding&lt;/h3&gt; &#xA;&lt;p&gt;The previous section showed a simple transmuxer program, now we&#39;re going to add the capability to encode files, specifically we&#39;re going to enable it to transcode videos from &lt;code&gt;h264&lt;/code&gt; to &lt;code&gt;h265&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;After we prepared the decoder but before we arrange the output media file we&#39;re going to set up the encoder.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create the video &lt;code&gt;AVStream&lt;/code&gt; in the encoder, &lt;a href=&#34;https://www.ffmpeg.org/doxygen/trunk/group__lavf__core.html#gadcb0fd3e507d9b58fe78f61f8ad39827&#34;&gt;&lt;code&gt;avformat_new_stream&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Use the &lt;code&gt;AVCodec&lt;/code&gt; called &lt;code&gt;libx265&lt;/code&gt;, &lt;a href=&#34;https://www.ffmpeg.org/doxygen/trunk/group__lavc__encoding.html#gaa614ffc38511c104bdff4a3afa086d37&#34;&gt;&lt;code&gt;avcodec_find_encoder_by_name&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Create the &lt;code&gt;AVCodecContext&lt;/code&gt; based in the created codec, &lt;a href=&#34;https://www.ffmpeg.org/doxygen/trunk/group__lavc__core.html#gae80afec6f26df6607eaacf39b561c315&#34;&gt;&lt;code&gt;avcodec_alloc_context3&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Set up basic attributes for the transcoding session, and&lt;/li&gt; &#xA; &lt;li&gt;Open the codec and copy parameters from the context to the stream. &lt;a href=&#34;https://www.ffmpeg.org/doxygen/trunk/group__lavc__core.html#ga11f785a188d7d9df71621001465b0f1d&#34;&gt;&lt;code&gt;avcodec_open2&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://www.ffmpeg.org/doxygen/trunk/group__lavc__core.html#ga0c7058f764778615e7978a1821ab3cfe&#34;&gt;&lt;code&gt;avcodec_parameters_from_context&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;AVRational input_framerate = av_guess_frame_rate(decoder_avfc, decoder_video_avs, NULL);&#xA;AVStream *video_avs = avformat_new_stream(encoder_avfc, NULL);&#xA;&#xA;char *codec_name = &#34;libx265&#34;;&#xA;char *codec_priv_key = &#34;x265-params&#34;;&#xA;// we&#39;re going to use internal options for the x265&#xA;// it disables the scene change detection and fix then&#xA;// GOP on 60 frames.&#xA;char *codec_priv_value = &#34;keyint=60:min-keyint=60:scenecut=0&#34;;&#xA;&#xA;AVCodec *video_avc = avcodec_find_encoder_by_name(codec_name);&#xA;AVCodecContext *video_avcc = avcodec_alloc_context3(video_avc);&#xA;// encoder codec params&#xA;av_opt_set(sc-&amp;gt;video_avcc-&amp;gt;priv_data, codec_priv_key, codec_priv_value, 0);&#xA;video_avcc-&amp;gt;height = decoder_ctx-&amp;gt;height;&#xA;video_avcc-&amp;gt;width = decoder_ctx-&amp;gt;width;&#xA;video_avcc-&amp;gt;pix_fmt = video_avc-&amp;gt;pix_fmts[0];&#xA;// control rate&#xA;video_avcc-&amp;gt;bit_rate = 2 * 1000 * 1000;&#xA;video_avcc-&amp;gt;rc_buffer_size = 4 * 1000 * 1000;&#xA;video_avcc-&amp;gt;rc_max_rate = 2 * 1000 * 1000;&#xA;video_avcc-&amp;gt;rc_min_rate = 2.5 * 1000 * 1000;&#xA;// time base&#xA;video_avcc-&amp;gt;time_base = av_inv_q(input_framerate);&#xA;video_avs-&amp;gt;time_base = sc-&amp;gt;video_avcc-&amp;gt;time_base;&#xA;&#xA;avcodec_open2(sc-&amp;gt;video_avcc, sc-&amp;gt;video_avc, NULL);&#xA;avcodec_parameters_from_context(sc-&amp;gt;video_avs-&amp;gt;codecpar, sc-&amp;gt;video_avcc);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We need to expand our decoding loop for the video stream transcoding:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Send the empty &lt;code&gt;AVPacket&lt;/code&gt; to the decoder, &lt;a href=&#34;https://www.ffmpeg.org/doxygen/trunk/group__lavc__decoding.html#ga58bc4bf1e0ac59e27362597e467efff3&#34;&gt;&lt;code&gt;avcodec_send_packet&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Receive the uncompressed &lt;code&gt;AVFrame&lt;/code&gt;, &lt;a href=&#34;https://www.ffmpeg.org/doxygen/trunk/group__lavc__decoding.html#ga11e6542c4e66d3028668788a1a74217c&#34;&gt;&lt;code&gt;avcodec_receive_frame&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Start to transcode this raw frame,&lt;/li&gt; &#xA; &lt;li&gt;Send the raw frame, &lt;a href=&#34;https://www.ffmpeg.org/doxygen/trunk/group__lavc__decoding.html#ga9395cb802a5febf1f00df31497779169&#34;&gt;&lt;code&gt;avcodec_send_frame&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Receive the compressed, based on our codec, &lt;code&gt;AVPacket&lt;/code&gt;, &lt;a href=&#34;https://www.ffmpeg.org/doxygen/trunk/group__lavc__decoding.html#ga5b8eff59cf259747cf0b31563e38ded6&#34;&gt;&lt;code&gt;avcodec_receive_packet&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Set up the timestamp, and &lt;a href=&#34;https://www.ffmpeg.org/doxygen/trunk/group__lavc__packet.html#gae5c86e4d93f6e7aa62ef2c60763ea67e&#34;&gt;&lt;code&gt;av_packet_rescale_ts&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Write it to the output file. &lt;a href=&#34;https://www.ffmpeg.org/doxygen/trunk/group__lavf__encoding.html#ga37352ed2c63493c38219d935e71db6c1&#34;&gt;&lt;code&gt;av_interleaved_write_frame&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;AVFrame *input_frame = av_frame_alloc();&#xA;AVPacket *input_packet = av_packet_alloc();&#xA;&#xA;while (av_read_frame(decoder_avfc, input_packet) &amp;gt;= 0)&#xA;{&#xA;  int response = avcodec_send_packet(decoder_video_avcc, input_packet);&#xA;  while (response &amp;gt;= 0) {&#xA;    response = avcodec_receive_frame(decoder_video_avcc, input_frame);&#xA;    if (response == AVERROR(EAGAIN) || response == AVERROR_EOF) {&#xA;      break;&#xA;    } else if (response &amp;lt; 0) {&#xA;      return response;&#xA;    }&#xA;    if (response &amp;gt;= 0) {&#xA;      encode(encoder_avfc, decoder_video_avs, encoder_video_avs, decoder_video_avcc, input_packet-&amp;gt;stream_index);&#xA;    }&#xA;    av_frame_unref(input_frame);&#xA;  }&#xA;  av_packet_unref(input_packet);&#xA;}&#xA;av_write_trailer(encoder_avfc);&#xA;&#xA;// used function&#xA;int encode(AVFormatContext *avfc, AVStream *dec_video_avs, AVStream *enc_video_avs, AVCodecContext video_avcc int index) {&#xA;  AVPacket *output_packet = av_packet_alloc();&#xA;  int response = avcodec_send_frame(video_avcc, input_frame);&#xA;&#xA;  while (response &amp;gt;= 0) {&#xA;    response = avcodec_receive_packet(video_avcc, output_packet);&#xA;    if (response == AVERROR(EAGAIN) || response == AVERROR_EOF) {&#xA;      break;&#xA;    } else if (response &amp;lt; 0) {&#xA;      return -1;&#xA;    }&#xA;&#xA;    output_packet-&amp;gt;stream_index = index;&#xA;    output_packet-&amp;gt;duration = enc_video_avs-&amp;gt;time_base.den / enc_video_avs-&amp;gt;time_base.num / dec_video_avs-&amp;gt;avg_frame_rate.num * dec_video_avs-&amp;gt;avg_frame_rate.den;&#xA;&#xA;    av_packet_rescale_ts(output_packet, dec_video_avs-&amp;gt;time_base, enc_video_avs-&amp;gt;time_base);&#xA;    response = av_interleaved_write_frame(avfc, output_packet);&#xA;  }&#xA;  av_packet_unref(output_packet);&#xA;  av_packet_free(&amp;amp;output_packet);&#xA;  return 0;&#xA;}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We converted the media stream from &lt;code&gt;h264&lt;/code&gt; to &lt;code&gt;h265&lt;/code&gt;, as expected the &lt;code&gt;h265&lt;/code&gt; version of the media file is smaller than the &lt;code&gt;h264&lt;/code&gt; however the &lt;a href=&#34;https://raw.githubusercontent.com/leandromoreira/ffmpeg-libav-tutorial/master/3_transcoding.c&#34;&gt;created program&lt;/a&gt; is capable of:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;&#xA;  /*&#xA;   * H264 -&amp;gt; H265&#xA;   * Audio -&amp;gt; remuxed (untouched)&#xA;   * MP4 - MP4&#xA;   */&#xA;  StreamingParams sp = {0};&#xA;  sp.copy_audio = 1;&#xA;  sp.copy_video = 0;&#xA;  sp.video_codec = &#34;libx265&#34;;&#xA;  sp.codec_priv_key = &#34;x265-params&#34;;&#xA;  sp.codec_priv_value = &#34;keyint=60:min-keyint=60:scenecut=0&#34;;&#xA;&#xA;  /*&#xA;   * H264 -&amp;gt; H264 (fixed gop)&#xA;   * Audio -&amp;gt; remuxed (untouched)&#xA;   * MP4 - MP4&#xA;   */&#xA;  StreamingParams sp = {0};&#xA;  sp.copy_audio = 1;&#xA;  sp.copy_video = 0;&#xA;  sp.video_codec = &#34;libx264&#34;;&#xA;  sp.codec_priv_key = &#34;x264-params&#34;;&#xA;  sp.codec_priv_value = &#34;keyint=60:min-keyint=60:scenecut=0:force-cfr=1&#34;;&#xA;&#xA;  /*&#xA;   * H264 -&amp;gt; H264 (fixed gop)&#xA;   * Audio -&amp;gt; remuxed (untouched)&#xA;   * MP4 - fragmented MP4&#xA;   */&#xA;  StreamingParams sp = {0};&#xA;  sp.copy_audio = 1;&#xA;  sp.copy_video = 0;&#xA;  sp.video_codec = &#34;libx264&#34;;&#xA;  sp.codec_priv_key = &#34;x264-params&#34;;&#xA;  sp.codec_priv_value = &#34;keyint=60:min-keyint=60:scenecut=0:force-cfr=1&#34;;&#xA;  sp.muxer_opt_key = &#34;movflags&#34;;&#xA;  sp.muxer_opt_value = &#34;frag_keyframe+empty_moov+delay_moov+default_base_moof&#34;;&#xA;&#xA;  /*&#xA;   * H264 -&amp;gt; H264 (fixed gop)&#xA;   * Audio -&amp;gt; AAC&#xA;   * MP4 - MPEG-TS&#xA;   */&#xA;  StreamingParams sp = {0};&#xA;  sp.copy_audio = 0;&#xA;  sp.copy_video = 0;&#xA;  sp.video_codec = &#34;libx264&#34;;&#xA;  sp.codec_priv_key = &#34;x264-params&#34;;&#xA;  sp.codec_priv_value = &#34;keyint=60:min-keyint=60:scenecut=0:force-cfr=1&#34;;&#xA;  sp.audio_codec = &#34;aac&#34;;&#xA;  sp.output_extension = &#34;.ts&#34;;&#xA;&#xA;  /* WIP :P  -&amp;gt; it&#39;s not playing on VLC, the final bit rate is huge&#xA;   * H264 -&amp;gt; VP9&#xA;   * Audio -&amp;gt; Vorbis&#xA;   * MP4 - WebM&#xA;   */&#xA;  //StreamingParams sp = {0};&#xA;  //sp.copy_audio = 0;&#xA;  //sp.copy_video = 0;&#xA;  //sp.video_codec = &#34;libvpx-vp9&#34;;&#xA;  //sp.audio_codec = &#34;libvorbis&#34;;&#xA;  //sp.output_extension = &#34;.webm&#34;;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Now, to be honest, this was &lt;a href=&#34;https://github.com/leandromoreira/ffmpeg-libav-tutorial/pull/54&#34;&gt;harder than I thought&lt;/a&gt; it&#39;d be and I had to dig into the &lt;a href=&#34;https://github.com/leandromoreira/ffmpeg-libav-tutorial/pull/54#issuecomment-570746749&#34;&gt;FFmpeg command line source code&lt;/a&gt; and test it a lot and I think I&#39;m missing something because I had to enforce &lt;code&gt;force-cfr&lt;/code&gt; for the &lt;code&gt;h264&lt;/code&gt; to work and I&#39;m still seeing some warning messages like &lt;code&gt;warning messages (forced frame type (5) at 80 was changed to frame type (3))&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt;</summary>
  </entry>
</feed>