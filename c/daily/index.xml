<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-24T01:30:49Z</updated>
  <subtitle>Daily Trending of C in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Totoro97/f2-nerf</title>
    <updated>2023-04-24T01:30:49Z</updated>
    <id>tag:github.com,2023-04-24:/Totoro97/f2-nerf</id>
    <link href="https://github.com/Totoro97/f2-nerf" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Fast neural radiance field training with free camera trajectories&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;F2-NeRF&lt;/h1&gt; &#xA;&lt;p&gt;This is the repo for the implementation of &lt;strong&gt;F2-NeRF: Fast Neural Radiance Field Training with Free Camera Trajectories&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Totoro97/f2-nerf/main/static/intro_1.gif&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Totoro97/f2-nerf/main/static/intro_2.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://totoro97.github.io/projects/f2-nerf&#34;&gt;Project page&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2303.15951&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;https://www.dropbox.com/sh/jmfao2c4dp9usji/AAC7Ydj6rrrhy1-VvlAVjyE_a?dl=0&#34;&gt;Data&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;The development of this project is primarily based on LibTorch.&lt;/p&gt; &#xA;&lt;h3&gt;Step 1. Install dependencies&lt;/h3&gt; &#xA;&lt;p&gt;For Debian based Linux distributions:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt install zlib1g-dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For Arch based Linux distributions:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo pacman -S zlib&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step 2. Clone this repository:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone --recursive https://github.com/Totoro97/f2-nerf.git&#xA;cd f2-nerf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step 3. Download pre-compiled LibTorch&lt;/h3&gt; &#xA;&lt;p&gt;We take &lt;code&gt;torch-1.13.1+cu117&lt;/code&gt; for example.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd External&#xA;https://download.pytorch.org/libtorch/cu117/libtorch-cxx11-abi-shared-with-deps-1.13.1%2Bcu117.zip&#xA;unzip ./libtorch-cxx11-abi-shared-with-deps-1.13.1%2Bcu117.zip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step 4. Compile&lt;/h3&gt; &#xA;&lt;p&gt;The lowest G++ version I have tested is 7.5.0.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd ..&#xA;cmake . -B build&#xA;cmake --build build --target main --config RelWithDebInfo -j&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Run&lt;/h2&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;p&gt;Here is an example command to train F2-NeRF:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python scripts/run.py --config-name=wanjinyou dataset_name=example case_name=ngp_fox mode=train&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Render test images&lt;/h3&gt; &#xA;&lt;p&gt;Simply run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python scripts/run.py --config-name=wanjinyou dataset_name=example case_name=ngp_fox mode=test is_continue=true&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Render path&lt;/h3&gt; &#xA;&lt;p&gt;We provide a script to generate render path (by interpolating the input camera poses). For example, for the fox data, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python scripts/inter_poses.py --data_dir ./data/example/ngp_fox --key_poses 5,10,15,20,25,30,35,40,45,49 --n_out_poses 200&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The file &lt;code&gt;poses_render.npy&lt;/code&gt; in the data directory would be generated. Then run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python scripts/run.py --config-name=wanjiyou dataset_name=example case_name=ngp_fox mode=render_path is_continue=true&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The synthesized images can be found in &lt;code&gt;./exp/ngp_fox/novel_images&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Train F2-NeRF on your custom data&lt;/h2&gt; &#xA;&lt;p&gt;Make sure COLMAP has been installed.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Run COLMAP SfM:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash scripts/local_colmap_and_resize.sh &amp;lt;your-data-dir&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Make sure the images are at &lt;code&gt;&amp;lt;your-data-dir&amp;gt;/images&lt;/code&gt;&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Generate cameras file:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/colmap2poses.py --data_dir &amp;lt;your-data-dir&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Run F2-NeRF!&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Train F2-NeRF on LLFF/NeRF-360-V2 dataset&lt;/h2&gt; &#xA;&lt;p&gt;We provide a script to convert the LLFF camera format to our camera format. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/llff2poses.py --data_dir=xxx/nerf_llff_data/horns &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;TODO/Future work&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add anti-aliasing&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Acknowledgment&lt;/h2&gt; &#xA;&lt;p&gt;Besides LibTorch, this project is also built upon the following awesome libraries:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/nmwsharp/happly&#34;&gt;happly&lt;/a&gt; for I/O of PLY files&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/nothings/stb&#34;&gt;stb_image&lt;/a&gt; for I/O of image files&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NVlabs/tiny-cuda-nn&#34;&gt;tiny-cuda-nn&lt;/a&gt; for fast MLP training/inference&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://eigen.tuxfamily.org/index.php?title=Main_Page&#34;&gt;eigen&lt;/a&gt; for linear algebra computing&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jbeder/yaml-cpp&#34;&gt;yaml-cpp&lt;/a&gt; for I/O of YAML files&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/hydra&#34;&gt;hydra&lt;/a&gt; for configuration&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rogersce/cnpy&#34;&gt;cnpy&lt;/a&gt; for I/O of npy files&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Some of the code snippets are inspired from &lt;a href=&#34;https://github.com/NVlabs/instant-ngp&#34;&gt;instant-ngp&lt;/a&gt;, &lt;a href=&#34;https://github.com/ashawkey/torch-ngp&#34;&gt;torch-ngp&lt;/a&gt; and &lt;a href=&#34;https://github.com/kwea123/ngp_pl&#34;&gt;ngp-pl&lt;/a&gt;. The COLMAP processing scripts are from &lt;a href=&#34;https://github.com/google-research/multinerf&#34;&gt;multinerf&lt;/a&gt;. The example data &lt;code&gt;ngp_fox&lt;/code&gt; is from &lt;a href=&#34;https://github.com/NVlabs/instant-ngp&#34;&gt;instant-ngp&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Cite as below if you find this repository is helpful to your project:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{wang2023f2nerf,&#xA;  title={F2-NeRF: Fast Neural Radiance Field Training with Free Camera Trajectories},&#xA;  author={Wang, Peng and Liu, Yuan and Chen, Zhaoxi and Liu, Lingjie and Liu, Ziwei and Komura, Taku and Theobalt, Christian and Wang, Wenping},&#xA;  journal={CVPR},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>