<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-09-02T01:31:52Z</updated>
  <subtitle>Daily Trending of C in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ImageMagick/ImageMagick</title>
    <updated>2022-09-02T01:31:52Z</updated>
    <id>tag:github.com,2022-09-02:/ImageMagick/ImageMagick</id>
    <link href="https://github.com/ImageMagick/ImageMagick" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üßô‚Äç‚ôÇÔ∏è ImageMagick 7&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ImageMagick&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ImageMagick/ImageMagick/actions&#34;&gt;&lt;img src=&#34;https://github.com/ImageMagick/ImageMagick/workflows/main/badge.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&amp;amp;can=1&amp;amp;q=proj:imagemagick&#34;&gt;&lt;img src=&#34;https://oss-fuzz-build-logs.storage.googleapis.com/badges/imagemagick.svg?sanitize=true&#34; alt=&#34;Fuzzing Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/sponsors/ImageMagick&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%24-donate-ff00ff.svg?sanitize=true&#34; alt=&#34;Donate&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img align=&#34;center&#34; src=&#34;https://imagemagick.org/image/wizard.png&#34; alt=&#34;ImageMagick logo&#34; width=&#34;265&#34; height=&#34;353&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Use &lt;a href=&#34;https://imagemagick.org/&#34;&gt;ImageMagick¬Æ&lt;/a&gt; to create, edit, compose, or convert digital images. It can read and write images in a variety of formats (over 200) including PNG, JPEG, GIF, WebP, HEIC, SVG, PDF, DPX, EXR, and TIFF. ImageMagick can resize, flip, mirror, rotate, distort, shear and transform images, adjust image colors, apply various special effects, or draw text, lines, polygons, ellipses, and B√©zier curves.&lt;/p&gt; &#xA;&lt;h4&gt;What is ImageMagick?&lt;/h4&gt; &#xA;&lt;p&gt;ImageMagick is free software delivered as a ready-to-run binary distribution or as source code that you may use, copy, modify, and distribute in both open and proprietary applications. It is distributed under a derived Apache 2.0 &lt;a href=&#34;https://imagemagick.org/script/license.php&#34;&gt;license&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;ImageMagick utilizes multiple computational threads to increase performance and can read, process, or write mega-, giga-, or tera-pixel image sizes. The current release is the ImageMagick 7.1.0 series. It runs on Linux, Windows, macOS X, iOS, Android OS, and others.&lt;/p&gt; &#xA;&lt;p&gt;The authoritative ImageMagick web site is &lt;a href=&#34;https://imagemagick.org&#34;&gt;https://imagemagick.org&lt;/a&gt;. The authoritative source code repository is &lt;a href=&#34;https://github.com/ImageMagick/ImageMagick&#34;&gt;https://github.com/ImageMagick/ImageMagick&lt;/a&gt;. We continue to maintain the legacy release of ImageMagick, version 6, at &lt;a href=&#34;https://legacy.imagemagick.org&#34;&gt;https://legacy.imagemagick.org&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Features and Capabilities&lt;/h4&gt; &#xA;&lt;p&gt;Here are just a few &lt;a href=&#34;https://imagemagick.org/script/examples.php&#34;&gt;examples&lt;/a&gt; of what ImageMagick can do:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://imagemagick.org/script/command-line-options.php#bilateral-blur&#34;&gt;Animation&lt;/a&gt;: non-linear, edge-preserving, and noise-reducing smoothing filter.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://imagemagick.org/script/command-line-options.php#bilateral-blur&#34;&gt;Bilateral Blur&lt;/a&gt;: non-linear, edge-preserving, and noise-reducing smoothing filter.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://imagemagick.org/script/color-management.php&#34;&gt;Color management&lt;/a&gt;: accurate color management with color profiles or in lieu of-- built-in gamma compression or expansion as demanded by the colorspace.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://imagemagick.org/script/color-management.php&#34;&gt;Color thresholding&lt;/a&gt; force all pixels in the color range to white otherwise black.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://imagemagick.org/script/command-line-processing.php&#34;&gt;Command-line processing&lt;/a&gt; utilize ImageMagick from the command-line.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Complex_text_layout&#34;&gt;Complex text layout&lt;/a&gt; bidirectional text support and shaping.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://imagemagick.org/script/composite.php&#34;&gt;Composite&lt;/a&gt;: overlap one image over another.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://imagemagick.org/script/connected-components.php&#34;&gt;Connected component labeling&lt;/a&gt;: uniquely label connected regions in an image.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://imagemagick.org/script/convex-hull.php&#34;&gt;Convex hull&lt;/a&gt; smallest area convex polygon containing the image foreground objects. In addition, the minimum bounding box and unrotate angle are also generated.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://imagemagick.org/Usage/crop/&#34;&gt;Decorate&lt;/a&gt;: add a border or frame to an image.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://imagemagick.org/Usage/transform/#vision&#34;&gt;Delineate image features&lt;/a&gt;: Canny edge detection, mean-shift, Hough lines.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://imagemagick.org/Usage/fourier/&#34;&gt;Discrete Fourier transform&lt;/a&gt;: implements the forward and inverse &lt;a href=&#34;http://en.wikipedia.org/wiki/Discrete_Fourier_transform&#34;&gt;DFT&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://imagemagick.org/script/distribute-pixel-cache.php&#34;&gt;Distributed pixel cache&lt;/a&gt;: offload intermediate pixel storage to one or more remote servers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://imagemagick.org/Usage/draw/&#34;&gt;Draw&lt;/a&gt;: add shapes or text to an image.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://imagemagick.org/script/cipher.php&#34;&gt;Encipher or decipher an image&lt;/a&gt;: convert ordinary images into unintelligible gibberish and back again.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://imagemagick.org/script/convert.php&#34;&gt;Format conversion&lt;/a&gt;: convert an image from one &lt;a href=&#34;https://imagemagick.org/script/formats.php&#34;&gt;format&lt;/a&gt; to another (e.g. PNG to JPEG).&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://imagemagick.org/Usage/distorts/&#34;&gt;Generalized pixel distortion&lt;/a&gt;: correct for, or induce image distortions including perspective.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://imagemagick.org/script/architecture.php#distributed&#34;&gt;Heterogeneous distributed processing&lt;/a&gt;: certain algorithms are OpenCL-enabled to take advantage of speed-ups offered by executing in concert across heterogeneous platforms consisting of CPUs, GPUs, and other processors.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://imagemagick.org/script/high-dynamic-range.php&#34;&gt;High dynamic-range images&lt;/a&gt;: accurately represent the wide range of intensity levels found in real scenes ranging from the brightest direct sunlight to the deepest darkest shadows.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://imagemagick.org/script/clahe.php&#34;&gt;Histogram equalization&lt;/a&gt; use adaptive histogram equalization to improve contrast in images.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://imagemagick.org/script/magick-cache.php&#34;&gt;Image cache&lt;/a&gt;: secure methods and tools to cache images, image sequences, video, audio or metadata in a local folder.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://imagemagick.org/script/fx.php&#34;&gt;Image calculator&lt;/a&gt;: apply a mathematical expression to an image or image channels.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://imagemagick.org/script/gradient.php&#34;&gt;Image gradients&lt;/a&gt;: create a gradual blend of one color whose shape is horizontal, vertical, circular, or elliptical.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://imagemagick.org/script/identify.php&#34;&gt;Image identification&lt;/a&gt;: describe the format and attributes of an image.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://imagemagick.org/script/download.php#iOS&#34;&gt;ImageMagick on the iPhone&lt;/a&gt;: convert, edit, or compose images on your iPhone.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://imagemagick.org/script/architecture.php#tera-pixel&#34;&gt;Large image support&lt;/a&gt;: read, process, or write mega-, giga-, or tera-pixel image sizes.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://imagemagick.org/script/montage.php&#34;&gt;Montage&lt;/a&gt;: juxtapose image thumbnails on an image canvas.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://imagemagick.org/Usage/morphology/&#34;&gt;Morphology of shapes&lt;/a&gt;: extract features, describe shapes and recognize patterns in images.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://imagemagick.org/script/motion-picture.php&#34;&gt;Motion picture support&lt;/a&gt;: read and write the common image formats used in digital film work.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://imagemagick.org/script/architecture.php#multispectral&#34;&gt;Multispectral imagery&lt;/a&gt;: support multispectral imagery up to 64 bands.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://imagemagick.org/Usage/transform/#vision&#34;&gt;Noise and color reduction&lt;/a&gt; Kuwahara Filter, mean-shift.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.fmwconcepts.com/misc_tests/perceptual_hash_test_results_510/index.html&#34;&gt;Perceptual hash&lt;/a&gt;: maps visually identical images to the same or similar hash-- useful in image retrieval, authentication, indexing, or copy detection as well as digital watermarking.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://imagemagick.org/Usage/blur/&#34;&gt;Special effects&lt;/a&gt;: blur, sharpen, threshold, or tint an image.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://imagemagick.org/Usage/text/&#34;&gt;Text &amp;amp; comments&lt;/a&gt;: insert descriptive or artistic text in an image.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://imagemagick.org/script/architecture.php#threads&#34;&gt;Threads of execution support&lt;/a&gt;: ImageMagick is thread safe and most internal algorithms are OpenMP-enabled to take advantage of speed-ups offered by multicore processor chips.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://imagemagick.org/Usage/resize/&#34;&gt;Transform&lt;/a&gt;: resize, rotate, deskew, crop, flip or trim an image.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://imagemagick.org/Usage/masking/&#34;&gt;Transparency&lt;/a&gt;: render portions of an image invisible.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://imagemagick.org/script/architecture.php#virtual-pixels&#34;&gt;Virtual pixel support&lt;/a&gt;: convenient access to pixels outside the image region.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://imagemagick.org/Usage/&#34;&gt;Examples of ImageMagick Usage&lt;/a&gt;, shows how to use ImageMagick from the command-line to accomplish any of these tasks and much more. Also, see &lt;a href=&#34;http://www.fmwconcepts.com/imagemagick/&#34;&gt;Fred&#39;s ImageMagick Scripts&lt;/a&gt;: a plethora of command-line scripts that perform geometric transforms, blurs, sharpens, edging, noise removal, and color manipulations. With &lt;a href=&#34;https://github.com/dlemstra/Magick.NET&#34;&gt;Magick.NET&lt;/a&gt;, use ImageMagick without having to install ImageMagick on your server or desktop.&lt;/p&gt; &#xA;&lt;h4&gt;News&lt;/h4&gt; &#xA;&lt;p&gt;ImageMagick best practices &lt;strong&gt;strongly&lt;/strong&gt; encourages you to configure a &lt;a href=&#34;https://imagemagick.org/script/security-policy.php&#34;&gt;security policy&lt;/a&gt; that suits your local environment.&lt;/p&gt; &#xA;&lt;p&gt;Now that ImageMagick version 7 is released, we continue to maintain the legacy release of ImageMagick, version 6, at &lt;a href=&#34;https://legacy.imagemagick.org&#34;&gt;https://legacy.imagemagick.org&lt;/a&gt;. Learn how ImageMagick version 7 differs from previous versions with our &lt;a href=&#34;https://imagemagick.org/script/porting.php&#34;&gt;porting guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Want more performance from ImageMagick? Try these options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;add more memory to your system, see the &lt;a href=&#34;https://imagemagick.org/script/architecture.php#cache&#34;&gt;pixel cache&lt;/a&gt;;&lt;/li&gt; &#xA; &lt;li&gt;add more cores to your system, see &lt;a href=&#34;https://imagemagick.org/script/architecture.php#threads&#34;&gt;threads of execution support&lt;/a&gt;;&lt;/li&gt; &#xA; &lt;li&gt;reduce lock contention with the &lt;a href=&#34;http://goog-perftools.sourceforge.net/doc/tcmalloc.html&#34;&gt;tcmalloc&lt;/a&gt; memory allocation library;&lt;/li&gt; &#xA; &lt;li&gt;push large images to a solid-state drive, see &lt;a href=&#34;https://imagemagick.org/script/architecture.php#tera-pixel&#34;&gt;large image support&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If these options are prohibitive, you can reduce the quality of the image results. The default build is Q16 HDRI. If you disable &lt;a href=&#34;https://imagemagick.org/script/high-dynamic-range.php&#34;&gt;HDRI&lt;/a&gt;, you use half the memory and instead of predominantly floating point operations, you use the typically more efficient integer operations. The tradeoff is reduced precision and you cannot process out of range pixel values (e.g. negative). If you build the Q8 non-HDRI version of ImageMagick, you again reduce the memory requirements in half-- and once again there is a tradeoff, even less precision and no out of range pixel values. For a Q8 non-HDRI build of ImageMagick, use these configure script options: --with-quantum-depth=8 --disable-hdri.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>arut/nginx-rtmp-module</title>
    <updated>2022-09-02T01:31:52Z</updated>
    <id>tag:github.com,2022-09-02:/arut/nginx-rtmp-module</id>
    <link href="https://github.com/arut/nginx-rtmp-module" rel="alternate"></link>
    <summary type="html">&lt;p&gt;NGINX-based Media Streaming Server&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;NGINX-based Media Streaming Server&lt;/h1&gt; &#xA;&lt;h2&gt;nginx-rtmp-module&lt;/h2&gt; &#xA;&lt;h3&gt;Project blog&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://nginx-rtmp.blogspot.com&#34;&gt;http://nginx-rtmp.blogspot.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Wiki manual&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/arut/nginx-rtmp-module/wiki/Directives&#34;&gt;https://github.com/arut/nginx-rtmp-module/wiki/Directives&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Google group&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://groups.google.com/group/nginx-rtmp&#34;&gt;https://groups.google.com/group/nginx-rtmp&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://groups.google.com/group/nginx-rtmp-ru&#34;&gt;https://groups.google.com/group/nginx-rtmp-ru&lt;/a&gt; (Russian)&lt;/p&gt; &#xA;&lt;h3&gt;Donation page (Paypal etc)&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://arut.github.com/nginx-rtmp-module/&#34;&gt;http://arut.github.com/nginx-rtmp-module/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Features&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;RTMP/HLS/MPEG-DASH live streaming&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;RTMP Video on demand FLV/MP4, playing from local filesystem or HTTP&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Stream relay support for distributed streaming: push &amp;amp; pull models&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Recording streams in multiple FLVs&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;H264/AAC support&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Online transcoding with FFmpeg&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;HTTP callbacks (publish/play/record/update etc)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Running external programs on certain events (exec)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;HTTP control module for recording audio/video and dropping clients&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Advanced buffering techniques to keep memory allocations at a minimum level for faster streaming and low memory footprint&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Proved to work with Wirecast, FMS, Wowza, JWPlayer, FlowPlayer, StrobeMediaPlayback, ffmpeg, avconv, rtmpdump, flvstreamer and many more&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Statistics in XML/XSL in machine- &amp;amp; human- readable form&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Linux/FreeBSD/MacOS/Windows&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Build&lt;/h3&gt; &#xA;&lt;p&gt;cd to NGINX source directory &amp;amp; run this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./configure --add-module=/path/to/nginx-rtmp-module&#xA;make&#xA;make install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Several versions of nginx (1.3.14 - 1.5.0) require http_ssl_module to be added as well:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./configure --add-module=/path/to/nginx-rtmp-module --with-http_ssl_module&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For building debug version of nginx add &lt;code&gt;--with-debug&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./configure --add-module=/path/to-nginx/rtmp-module --with-debug&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/arut/nginx-rtmp-module/wiki/Debug-log&#34;&gt;Read more about debug log&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Windows limitations&lt;/h3&gt; &#xA;&lt;p&gt;Windows support is limited. These features are not supported&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;execs&lt;/li&gt; &#xA; &lt;li&gt;static pulls&lt;/li&gt; &#xA; &lt;li&gt;auto_push&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;RTMP URL format&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;rtmp://rtmp.example.com/app[/name]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;app - should match one of application {} blocks in config&lt;/p&gt; &#xA;&lt;p&gt;name - interpreted by each application can be empty&lt;/p&gt; &#xA;&lt;h3&gt;Multi-worker live streaming&lt;/h3&gt; &#xA;&lt;p&gt;Module supports multi-worker live streaming through automatic stream pushing to nginx workers. This option is toggled with rtmp_auto_push directive.&lt;/p&gt; &#xA;&lt;h3&gt;Example nginx.conf&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;rtmp {&#xA;&#xA;    server {&#xA;&#xA;        listen 1935;&#xA;&#xA;        chunk_size 4000;&#xA;&#xA;        # TV mode: one publisher, many subscribers&#xA;        application mytv {&#xA;&#xA;            # enable live streaming&#xA;            live on;&#xA;&#xA;            # record first 1K of stream&#xA;            record all;&#xA;            record_path /tmp/av;&#xA;            record_max_size 1K;&#xA;&#xA;            # append current timestamp to each flv&#xA;            record_unique on;&#xA;&#xA;            # publish only from localhost&#xA;            allow publish 127.0.0.1;&#xA;            deny publish all;&#xA;&#xA;            #allow play all;&#xA;        }&#xA;&#xA;        # Transcoding (ffmpeg needed)&#xA;        application big {&#xA;            live on;&#xA;&#xA;            # On every pusblished stream run this command (ffmpeg)&#xA;            # with substitutions: $app/${app}, $name/${name} for application &amp;amp; stream name.&#xA;            #&#xA;            # This ffmpeg call receives stream from this application &amp;amp;&#xA;            # reduces the resolution down to 32x32. The stream is the published to&#xA;            # &#39;small&#39; application (see below) under the same name.&#xA;            #&#xA;            # ffmpeg can do anything with the stream like video/audio&#xA;            # transcoding, resizing, altering container/codec params etc&#xA;            #&#xA;            # Multiple exec lines can be specified.&#xA;&#xA;            exec ffmpeg -re -i rtmp://localhost:1935/$app/$name -vcodec flv -acodec copy -s 32x32&#xA;                        -f flv rtmp://localhost:1935/small/${name};&#xA;        }&#xA;&#xA;        application small {&#xA;            live on;&#xA;            # Video with reduced resolution comes here from ffmpeg&#xA;        }&#xA;&#xA;        application webcam {&#xA;            live on;&#xA;&#xA;            # Stream from local webcam&#xA;            exec_static ffmpeg -f video4linux2 -i /dev/video0 -c:v libx264 -an&#xA;                               -f flv rtmp://localhost:1935/webcam/mystream;&#xA;        }&#xA;&#xA;        application mypush {&#xA;            live on;&#xA;&#xA;            # Every stream published here&#xA;            # is automatically pushed to&#xA;            # these two machines&#xA;            push rtmp1.example.com;&#xA;            push rtmp2.example.com:1934;&#xA;        }&#xA;&#xA;        application mypull {&#xA;            live on;&#xA;&#xA;            # Pull all streams from remote machine&#xA;            # and play locally&#xA;            pull rtmp://rtmp3.example.com pageUrl=www.example.com/index.html;&#xA;        }&#xA;&#xA;        application mystaticpull {&#xA;            live on;&#xA;&#xA;            # Static pull is started at nginx start&#xA;            pull rtmp://rtmp4.example.com pageUrl=www.example.com/index.html name=mystream static;&#xA;        }&#xA;&#xA;        # video on demand&#xA;        application vod {&#xA;            play /var/flvs;&#xA;        }&#xA;&#xA;        application vod2 {&#xA;            play /var/mp4s;&#xA;        }&#xA;&#xA;        # Many publishers, many subscribers&#xA;        # no checks, no recording&#xA;        application videochat {&#xA;&#xA;            live on;&#xA;&#xA;            # The following notifications receive all&#xA;            # the session variables as well as&#xA;            # particular call arguments in HTTP POST&#xA;            # request&#xA;&#xA;            # Make HTTP request &amp;amp; use HTTP retcode&#xA;            # to decide whether to allow publishing&#xA;            # from this connection or not&#xA;            on_publish http://localhost:8080/publish;&#xA;&#xA;            # Same with playing&#xA;            on_play http://localhost:8080/play;&#xA;&#xA;            # Publish/play end (repeats on disconnect)&#xA;            on_done http://localhost:8080/done;&#xA;&#xA;            # All above mentioned notifications receive&#xA;            # standard connect() arguments as well as&#xA;            # play/publish ones. If any arguments are sent&#xA;            # with GET-style syntax to play &amp;amp; publish&#xA;            # these are also included.&#xA;            # Example URL:&#xA;            #   rtmp://localhost/myapp/mystream?a=b&amp;amp;c=d&#xA;&#xA;            # record 10 video keyframes (no audio) every 2 minutes&#xA;            record keyframes;&#xA;            record_path /tmp/vc;&#xA;            record_max_frames 10;&#xA;            record_interval 2m;&#xA;&#xA;            # Async notify about an flv recorded&#xA;            on_record_done http://localhost:8080/record_done;&#xA;&#xA;        }&#xA;&#xA;&#xA;        # HLS&#xA;&#xA;        # For HLS to work please create a directory in tmpfs (/tmp/hls here)&#xA;        # for the fragments. The directory contents is served via HTTP (see&#xA;        # http{} section in config)&#xA;        #&#xA;        # Incoming stream must be in H264/AAC. For iPhones use baseline H264&#xA;        # profile (see ffmpeg example).&#xA;        # This example creates RTMP stream from movie ready for HLS:&#xA;        #&#xA;        # ffmpeg -loglevel verbose -re -i movie.avi  -vcodec libx264&#xA;        #    -vprofile baseline -acodec libmp3lame -ar 44100 -ac 1&#xA;        #    -f flv rtmp://localhost:1935/hls/movie&#xA;        #&#xA;        # If you need to transcode live stream use &#39;exec&#39; feature.&#xA;        #&#xA;        application hls {&#xA;            live on;&#xA;            hls on;&#xA;            hls_path /tmp/hls;&#xA;        }&#xA;&#xA;        # MPEG-DASH is similar to HLS&#xA;&#xA;        application dash {&#xA;            live on;&#xA;            dash on;&#xA;            dash_path /tmp/dash;&#xA;        }&#xA;    }&#xA;}&#xA;&#xA;# HTTP can be used for accessing RTMP stats&#xA;http {&#xA;&#xA;    server {&#xA;&#xA;        listen      8080;&#xA;&#xA;        # This URL provides RTMP statistics in XML&#xA;        location /stat {&#xA;            rtmp_stat all;&#xA;&#xA;            # Use this stylesheet to view XML as web page&#xA;            # in browser&#xA;            rtmp_stat_stylesheet stat.xsl;&#xA;        }&#xA;&#xA;        location /stat.xsl {&#xA;            # XML stylesheet to view RTMP stats.&#xA;            # Copy stat.xsl wherever you want&#xA;            # and put the full directory path here&#xA;            root /path/to/stat.xsl/;&#xA;        }&#xA;&#xA;        location /hls {&#xA;            # Serve HLS fragments&#xA;            types {&#xA;                application/vnd.apple.mpegurl m3u8;&#xA;                video/mp2t ts;&#xA;            }&#xA;            root /tmp;&#xA;            add_header Cache-Control no-cache;&#xA;        }&#xA;&#xA;        location /dash {&#xA;            # Serve DASH fragments&#xA;            root /tmp;&#xA;            add_header Cache-Control no-cache;&#xA;        }&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Multi-worker streaming example&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;rtmp_auto_push on;&#xA;&#xA;rtmp {&#xA;    server {&#xA;        listen 1935;&#xA;&#xA;        application mytv {&#xA;            live on;&#xA;        }&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>mit-han-lab/tinyengine</title>
    <updated>2022-09-02T01:31:52Z</updated>
    <id>tag:github.com,2022-09-02:/mit-han-lab/tinyengine</id>
    <link href="https://github.com/mit-han-lab/tinyengine" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[NeurIPS 2020] MCUNet: Tiny Deep Learning on IoT Devices; [NeurIPS 2021] MCUNetV2: Memory-Efficient Patch-based Inference for Tiny Deep Learning; MCUNetV3: On-Device Training Under 256KB Memory&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TinyEngine&lt;/h1&gt; &#xA;&lt;p&gt;This is the official implementation of TinyEngine, a memory-efficient and high-performance neural network library for Microcontrollers. TinyEngine is a part of MCUNet, which also consists of TinyNAS. MCUNet is a system-algorithm co-design framework for tiny deep learning on microcontrollers. TinyEngine and TinyNAS are co-designed to fit the tight memory budgets.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The MCUNet and TinyNAS repo is &lt;a href=&#34;https://github.com/mit-han-lab/mcunet&#34;&gt;here&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://mcunet.mit.edu/#mcunetv1&#34;&gt;MCUNetV1&lt;/a&gt; | &lt;a href=&#34;https://mcunet.mit.edu/#mcunetv2&#34;&gt;MCUNetV2&lt;/a&gt; | &lt;a href=&#34;https://mcunet.mit.edu/#mcunetv3&#34;&gt;MCUNetV3&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=YvioBgtec4U&#34;&gt;Demo (Inference)&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mit-han-lab/tinyengine/master/assets/figures/mcunet_demo.gif&#34; alt=&#34;demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=XaDCO8YtmBw&#34;&gt;Demo (Training)&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mit-han-lab/tinyengine/master/assets/figures/mcunetV3_demo_2images.gif&#34; alt=&#34;demo_v3&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;p&gt;We will soon release &lt;strong&gt;Tiny Training Engine&lt;/strong&gt; used in &lt;a href=&#34;https://mcunet.mit.edu/#mcunetv3&#34;&gt;MCUNetV3: On-Device Training Under 256KB Memory&lt;/a&gt;. &lt;strong&gt;If you are interested in getting updates, please sign up &lt;a href=&#34;https://forms.gle/UW1uUmnfk1k6UJPPA&#34;&gt;here&lt;/a&gt; to get notified!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;(2022/08)&lt;/strong&gt; Our &lt;strong&gt;New Course on TinyML and Efficient Deep Learning&lt;/strong&gt; will be released soon in September 2022: &lt;a href=&#34;https://efficientml.ai/&#34;&gt;efficientml.ai&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;(2022/08)&lt;/strong&gt; We include the &lt;a href=&#34;https://raw.githubusercontent.com/mit-han-lab/tinyengine/master/tutorial&#34;&gt;demo tutorial&lt;/a&gt; for deploying a visual wake word (VWW) model onto microcontrollers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;(2022/08)&lt;/strong&gt; We opensource the TinyEngine repo.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;(2022/07)&lt;/strong&gt; We include the person detection model used in the video demo above in the &lt;a href=&#34;https://github.com/mit-han-lab/mcunet&#34;&gt;MCUNet repo&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;(2022/06)&lt;/strong&gt; We refactor the &lt;a href=&#34;https://github.com/mit-han-lab/mcunet&#34;&gt;MCUNet repo&lt;/a&gt; as a standalone repo (previous repo: &lt;a href=&#34;https://github.com/mit-han-lab/tinyml&#34;&gt;https://github.com/mit-han-lab/tinyml&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;(2021/10)&lt;/strong&gt; &lt;strong&gt;MCUNetV2&lt;/strong&gt; is accepted to NeurIPS 2021: &lt;a href=&#34;https://arxiv.org/abs/2110.15352&#34;&gt;https://arxiv.org/abs/2110.15352&lt;/a&gt; !&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;(2020/10)&lt;/strong&gt; &lt;strong&gt;MCUNet&lt;/strong&gt; is accepted to NeurIPS 2020 as &lt;strong&gt;spotlight&lt;/strong&gt;: &lt;a href=&#34;https://arxiv.org/abs/2007.10319&#34;&gt;https://arxiv.org/abs/2007.10319&lt;/a&gt; !&lt;/li&gt; &#xA; &lt;li&gt;Our projects are covered by: &lt;a href=&#34;https://news.mit.edu/2020/iot-deep-learning-1113&#34;&gt;MIT News&lt;/a&gt;, &lt;a href=&#34;https://news.mit.edu/2021/tiny-machine-learning-design-alleviates-bottleneck-memory-usage-iot-devices-1208&#34;&gt;MIT News (v2)&lt;/a&gt;, &lt;a href=&#34;https://www.wired.com/story/ai-algorithms-slimming-fit-fridge/&#34;&gt;WIRED&lt;/a&gt;, &lt;a href=&#34;https://www.morningbrew.com/emerging-tech/stories/2020/12/07/researchers-figured-fit-ai-ever-onto-internet-things-microchips&#34;&gt;Morning Brew&lt;/a&gt;, &lt;a href=&#34;https://staceyoniot.com/researchers-take-a-3-pronged-approach-to-edge-ai/&#34;&gt;Stacey on IoT&lt;/a&gt;, &lt;a href=&#34;https://www.analyticsinsight.net/amalgamating-ml-and-iot-in-smart-home-devices/&#34;&gt;Analytics Insight&lt;/a&gt;, &lt;a href=&#34;https://techable.jp/archives/142462&#34;&gt;Techable&lt;/a&gt;, etc.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;Microcontrollers are low-cost, low-power hardware. They are widely deployed and have wide applications, but the tight memory budget (50,000x smaller than GPUs) makes deep learning deployment difficult.&lt;/p&gt; &#xA;&lt;p&gt;MCUNet is a &lt;strong&gt;system-algorithm co-design&lt;/strong&gt; framework for tiny deep learning on microcontrollers. It consists of &lt;strong&gt;TinyNAS&lt;/strong&gt; and &lt;strong&gt;TinyEngine&lt;/strong&gt;. They are co-designed to fit the tight memory budgets. With system-algorithm co-design, we can significantly improve the deep learning performance on the same tiny memory budget.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mit-han-lab/tinyengine/master/assets/figures/overview.png&#34; alt=&#34;overview&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Specifically, TinyEngine is a memory-efficient inference library. TinyEngine adapts the memory scheduling according to the overall network topology rather than layer-wise optimization, reducing memory usage and accelerating the inference. It outperforms existing inference libraries such as &lt;a href=&#34;https://www.tensorflow.org/lite/microcontrollers&#34;&gt;TF-Lite Micro&lt;/a&gt; from Google, &lt;a href=&#34;https://arxiv.org/abs/1801.06601&#34;&gt;CMSIS-NN&lt;/a&gt; from Arm, and &lt;a href=&#34;https://www.st.com/en/embedded-software/x-cube-ai.html&#34;&gt;X-CUBE-AI&lt;/a&gt; from STMicroelectronics.&lt;/p&gt; &#xA;&lt;p&gt;TinyEngine adopts the following optimization techniques to accelerate inference speed and minimize memory footprint.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mcunet.mit.edu/#mcunetv1&#34;&gt;&lt;strong&gt;In-place depth-wise convolution&lt;/strong&gt;&lt;/a&gt;: A unique data placement technique for depth-wise convolution that overwrites input data by intermediate/output data to reduce peak SRAM memory.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/en-us/windows/ai/directml/dml-fused-activations&#34;&gt;&lt;strong&gt;Operator fusion&lt;/strong&gt;&lt;/a&gt;: A method that improves performance by merging one operator into a different operator so that they are executed together without requiring a roundtrip to memory.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Single_instruction,_multiple_data&#34;&gt;&lt;strong&gt;SIMD (Single instruction, multiple data) programming&lt;/strong&gt;&lt;/a&gt;: A computing method that performs the same operation on multiple data points simultaneously.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://oneapi-src.github.io/oneDNN/dev_guide_understanding_memory_formats.html&#34;&gt;&lt;strong&gt;HWC to CHW weight format transformation&lt;/strong&gt;&lt;/a&gt;: A weight format transformation technique that increases cache hit ratio for in-place depth-wise convolution.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://iq.opengenus.org/im2col/&#34;&gt;&lt;strong&gt;Image to Column (Im2col) convolution&lt;/strong&gt;&lt;/a&gt;: An implementation technique of computing convolution operation using general matrix multiplication (GEMM) operations.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://xilinx.github.io/Vitis_Accel_Examples/2019.2/html/loop_reorder.html&#34;&gt;&lt;strong&gt;Loop reordering&lt;/strong&gt;&lt;/a&gt;: A loop transformation technique that attempts to optimize a program&#39;s execution speed by reordering/interchanging the sequence of loops.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Loop_unrolling&#34;&gt;&lt;strong&gt;Loop unrolling&lt;/strong&gt;&lt;/a&gt;: A loop transformation technique that attempts to optimize a program&#39;s execution speed at the expense of its binary size, which is an approach known as space-time tradeoff.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Loop_nest_optimization&#34;&gt;&lt;strong&gt;Loop tiling&lt;/strong&gt;&lt;/a&gt;: A loop transformation technique that attempts to reduce memory access latency by partitioning a loop&#39;s iteration space into smaller chunks or blocks, so as to help ensure data used in a loop stays in the cache until it is reused.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mit-han-lab/tinyengine/master/assets/figures/inplace_depthwise.png&#34; alt=&#34;inplace_depthwise&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;By adopting the abovementioned optimization techniques, TinyEngine can not only enhance inference speed but also reduce peak memory, as shown in the figures below.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MAC/s improvement breakdown:&lt;/strong&gt; &lt;img src=&#34;https://raw.githubusercontent.com/mit-han-lab/tinyengine/master/assets/figures/mac_result.png&#34; alt=&#34;mac_result&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Peak memory reduction:&lt;/strong&gt; &lt;img src=&#34;https://raw.githubusercontent.com/mit-han-lab/tinyengine/master/assets/figures/peakmem_result.png&#34; alt=&#34;peakmem_result&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;To sum up, our &lt;strong&gt;TinyEngine&lt;/strong&gt; inference engine could be a useful infrastructure for MCU-based AI applications. It significantly &lt;strong&gt;improves the inference speed and reduces the memory usage&lt;/strong&gt; compared to existing libraries like &lt;a href=&#34;https://www.tensorflow.org/lite/microcontrollers&#34;&gt;TF-Lite Micro&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/1801.06601&#34;&gt;CMSIS-NN&lt;/a&gt;, &lt;a href=&#34;https://www.st.com/en/embedded-software/x-cube-ai.html&#34;&gt;X-CUBE-AI&lt;/a&gt;, etc. It improves the inference speed by &lt;strong&gt;1.1-18.6x&lt;/strong&gt;, and reduces the peak memory by &lt;strong&gt;1.3-3.6x&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mit-han-lab/tinyengine/master/assets/figures/measured_result.png&#34; alt=&#34;measured_result&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Code Structure&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;code_generator&lt;/code&gt; contains a python library that is used to compile neural networks into low-level source code (C/C++).&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;TinyEngine&lt;/code&gt; contains a C/C++ library that implements operators and performs inference on Microcontrollers.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;examples&lt;/code&gt; contains the examples of transforming TFLite models into our TinyEngine models.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;tutorial&lt;/code&gt; contains the demo tutorial of deploying a visual wake word (VWW) model onto microcontrollers.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;assets&lt;/code&gt; contains misc assets.&lt;/p&gt; &#xA;&lt;h2&gt;Requirement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.6+&lt;/li&gt; &#xA; &lt;li&gt;STM32CubeIDE 1.5+&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup for Users&lt;/h2&gt; &#xA;&lt;p&gt;First, clone this repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone --recursive https://github.com/mit-han-lab/tinyengine.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(Optional) Using a virtual environment with &lt;code&gt;conda&lt;/code&gt; is recommended.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n tinyengine python=3.6 pip&#xA;conda activate tinyengine&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Setup for Developers&lt;/h2&gt; &#xA;&lt;p&gt;Install pre-commit hooks to automatically format changes in your code.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pre-commit install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Deployment Example&lt;/h2&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://raw.githubusercontent.com/mit-han-lab/tinyengine/master/tutorial&#34;&gt;tutorial&lt;/a&gt; to learn how to deploy a visual wake word (VWW) model onto microcontrollers by using TinyEngine.&lt;/p&gt; &#xA;&lt;h2&gt;Measured Results&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;All the tflite models are from &lt;a href=&#34;https://github.com/mit-han-lab/mcunet#model-zoo&#34;&gt;Model Zoo in MCUNet repo&lt;/a&gt;. Please see MCUNet repo to know how to build the pre-trained int8 quantized models in TF-Lite format.&lt;/li&gt; &#xA; &lt;li&gt;All the &lt;strong&gt;latency&lt;/strong&gt;, &lt;strong&gt;peak memory (SRAM)&lt;/strong&gt; and &lt;strong&gt;Flash memory usage&lt;/strong&gt; results are profiled on STM32F746G-DISCO discovery boards.&lt;/li&gt; &#xA; &lt;li&gt;Note that we measure the newer versions of libraries in this repo, so that the results in this repo might be different from the ones in the MCUNet papers.&lt;/li&gt; &#xA; &lt;li&gt;Since TF-Lite Micro no longer has version numbers anymore, we use the git commit ID to indicate its newer version.&lt;/li&gt; &#xA; &lt;li&gt;All the tflite models are compiled by &lt;code&gt;-Ofast&lt;/code&gt; optimization level in STM32CubeIDE.&lt;/li&gt; &#xA; &lt;li&gt;OOM denotes Out Of Memory.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The &lt;strong&gt;latency&lt;/strong&gt; results:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;net_id&lt;/th&gt; &#xA;   &lt;th&gt;TF-Lite Micro&lt;br&gt;v2.1.0&lt;/th&gt; &#xA;   &lt;th&gt;TF-Lite Micro&lt;br&gt;&lt;a href=&#34;https://github.com/tensorflow/tflite-micro/tree/713b6ed6bd81d8d6906d885e14f444aaf9c154f6&#34;&gt;@ 713b6ed&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;CMSIS-NN&lt;br&gt;v2.0.0&lt;/th&gt; &#xA;   &lt;th&gt;X-CUBE-AI&lt;br&gt;v7.1.0&lt;/th&gt; &#xA;   &lt;th&gt;TinyEngine&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;em&gt;# mcunet models (VWW)&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mcunet-5fps-vww&lt;/td&gt; &#xA;   &lt;td&gt;624ms&lt;/td&gt; &#xA;   &lt;td&gt;2346ms&lt;/td&gt; &#xA;   &lt;td&gt;269ms&lt;/td&gt; &#xA;   &lt;td&gt;137ms&lt;/td&gt; &#xA;   &lt;td&gt;128ms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mcunet-10fps-vww&lt;/td&gt; &#xA;   &lt;td&gt;345ms&lt;/td&gt; &#xA;   &lt;td&gt;1230ms&lt;/td&gt; &#xA;   &lt;td&gt;143ms&lt;/td&gt; &#xA;   &lt;td&gt;76ms&lt;/td&gt; &#xA;   &lt;td&gt;66ms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mcunet-320kB-vww&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;657ms&lt;/td&gt; &#xA;   &lt;td&gt;570ms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;em&gt;# mcunet models (ImageNet)&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mcunet-5fps&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;149ms&lt;/td&gt; &#xA;   &lt;td&gt;135ms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mcunet-10fps&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;84ms&lt;/td&gt; &#xA;   &lt;td&gt;62ms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mcunet-256kB&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;839ms&lt;/td&gt; &#xA;   &lt;td&gt;681ms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mcunet-320kB&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;819ms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;em&gt;# baseline models&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mbv2-320kB&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;292ms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;proxyless-320kB&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;484ms&lt;/td&gt; &#xA;   &lt;td&gt;425ms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The &lt;strong&gt;peak memory (SRAM)&lt;/strong&gt; results:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;net_id&lt;/th&gt; &#xA;   &lt;th&gt;TF-Lite Micro&lt;br&gt;v2.1.0&lt;/th&gt; &#xA;   &lt;th&gt;TF-Lite Micro&lt;br&gt;&lt;a href=&#34;https://github.com/tensorflow/tflite-micro/tree/713b6ed6bd81d8d6906d885e14f444aaf9c154f6&#34;&gt;@ 713b6ed&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;CMSIS-NN&lt;br&gt;v2.0.0&lt;/th&gt; &#xA;   &lt;th&gt;X-CUBE-AI&lt;br&gt;v7.1.0&lt;/th&gt; &#xA;   &lt;th&gt;TinyEngine&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;em&gt;# mcunet models (VWW)&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mcunet-5fps-vww&lt;/td&gt; &#xA;   &lt;td&gt;227kB&lt;/td&gt; &#xA;   &lt;td&gt;220kB&lt;/td&gt; &#xA;   &lt;td&gt;248kB&lt;/td&gt; &#xA;   &lt;td&gt;123kB&lt;/td&gt; &#xA;   &lt;td&gt;88kB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mcunet-10fps-vww&lt;/td&gt; &#xA;   &lt;td&gt;169kB&lt;/td&gt; &#xA;   &lt;td&gt;163kB&lt;/td&gt; &#xA;   &lt;td&gt;199kB&lt;/td&gt; &#xA;   &lt;td&gt;98kB&lt;/td&gt; &#xA;   &lt;td&gt;56kB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mcunet-320kB-vww&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;259kB&lt;/td&gt; &#xA;   &lt;td&gt;162kB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;em&gt;# mcunet models (ImageNet)&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mcunet-5fps&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;126kB&lt;/td&gt; &#xA;   &lt;td&gt;90kB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mcunet-10fps&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;76kB&lt;/td&gt; &#xA;   &lt;td&gt;45kB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mcunet-256kB&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;311kB&lt;/td&gt; &#xA;   &lt;td&gt;200kB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mcunet-320kB&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;242kB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;em&gt;# baseline models&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mbv2-320kB&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;284kB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;proxyless-320kB&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;312kB&lt;/td&gt; &#xA;   &lt;td&gt;242kB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The &lt;strong&gt;Flash memory usage&lt;/strong&gt; results:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;net_id&lt;/th&gt; &#xA;   &lt;th&gt;TF-Lite Micro&lt;br&gt;v2.1.0&lt;/th&gt; &#xA;   &lt;th&gt;TF-Lite Micro&lt;br&gt;&lt;a href=&#34;https://github.com/tensorflow/tflite-micro/tree/713b6ed6bd81d8d6906d885e14f444aaf9c154f6&#34;&gt;@ 713b6ed&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;CMSIS-NN&lt;br&gt;v2.0.0&lt;/th&gt; &#xA;   &lt;th&gt;X-CUBE-AI&lt;br&gt;v7.1.0&lt;/th&gt; &#xA;   &lt;th&gt;TinyEngine&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;em&gt;# mcunet models (VWW)&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mcunet-5fps-vww&lt;/td&gt; &#xA;   &lt;td&gt;782kB&lt;/td&gt; &#xA;   &lt;td&gt;733kB&lt;/td&gt; &#xA;   &lt;td&gt;743kB&lt;/td&gt; &#xA;   &lt;td&gt;534kB&lt;/td&gt; &#xA;   &lt;td&gt;517kB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mcunet-10fps-vww&lt;/td&gt; &#xA;   &lt;td&gt;691kB&lt;/td&gt; &#xA;   &lt;td&gt;643kB&lt;/td&gt; &#xA;   &lt;td&gt;653kB&lt;/td&gt; &#xA;   &lt;td&gt;463kB&lt;/td&gt; &#xA;   &lt;td&gt;447kB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mcunet-320kB-vww&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;773kB&lt;/td&gt; &#xA;   &lt;td&gt;742kB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;em&gt;# mcunet models (ImageNet)&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mcunet-5fps&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;737kB&lt;/td&gt; &#xA;   &lt;td&gt;720kB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mcunet-10fps&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;856kB&lt;/td&gt; &#xA;   &lt;td&gt;837kB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mcunet-256kB&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;850kB&lt;/td&gt; &#xA;   &lt;td&gt;827kB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mcunet-320kB&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;835kB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;em&gt;# baseline models&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mbv2-320kB&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;828kB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;proxyless-320kB&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;866kB&lt;/td&gt; &#xA;   &lt;td&gt;835kB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find the project helpful, please consider citing our paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{&#xA;  lin2020mcunet,&#xA;  title={Mcunet: Tiny deep learning on iot devices},&#xA;  author={Lin, Ji and Chen, Wei-Ming and Lin, Yujun and Gan, Chuang and Han, Song},&#xA;  journal={Advances in Neural Information Processing Systems},&#xA;  volume={33},&#xA;  year={2020}&#xA;}&#xA;&#xA;@inproceedings{&#xA;  lin2021mcunetv2,&#xA;  title={MCUNetV2: Memory-Efficient Patch-based Inference for Tiny Deep Learning},&#xA;  author={Lin, Ji and Chen, Wei-Ming and Cai, Han and Gan, Chuang and Han, Song},&#xA;  booktitle={Annual Conference on Neural Information Processing Systems (NeurIPS)},&#xA;  year={2021}&#xA;}&#xA;&#xA;@inproceedings{&#xA;  lin2022ondevice,&#xA;  title={On-Device Training Under 256KB Memory},&#xA;  author={Lin, Ji and Zhu, Ligeng and Chen, Wei-Ming and Wang, Wei-Chen and Gan, Chuang and Han, Song},&#xA;  booktitle={ArXiv},&#xA;  year={2022}&#xA;} &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Related Projects&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://mcunet.mit.edu/#mcunetv1&#34;&gt;MCUNet: Tiny Deep Learning on IoT Devices&lt;/a&gt; (NeurIPS&#39;20)&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://mcunet.mit.edu/#mcunetv2&#34;&gt;MCUNetV2: Memory-Efficient Patch-based Inference for Tiny Deep Learning&lt;/a&gt; (NeurIPS&#39;21)&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://mcunet.mit.edu/#mcunetv3&#34;&gt;MCUNetV3: On-Device Training Under 256KB Memory&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>