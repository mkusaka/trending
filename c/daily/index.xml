<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-03T02:18:02Z</updated>
  <subtitle>Daily Trending of C in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>libAudioFlux/audioFlux</title>
    <updated>2023-03-03T02:18:02Z</updated>
    <id>tag:github.com,2023-03-03:/libAudioFlux/audioFlux</id>
    <link href="https://github.com/libAudioFlux/audioFlux" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A library for audio and music analysis, feature extraction.&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://raw.githubusercontent.com/libAudioFlux/audioFlux/master/image/logo.png&#34; width=&#34;400&#34; style=&#34;max-width: 100%;&#34;&gt; &#xA;&lt;h1&gt;audioFlux&lt;/h1&gt; &#xA;&lt;!--&lt;p align=&#34;center&#34;&gt;&#xA;&lt;img src=&#39;./image/logo.png&#39;  width=&#34;380&#34;  style=&#34;max-width: 100%;&#34; &gt; &#xA;&lt;/p&gt;--&gt; &#xA;&lt;!-- &#xA;[![Pypi Downloads](https://img.shields.io/pypi/dm/aubio.svg?label=Pypi%20downloads)](https://pypi.org/project/aubio/)&#xA;[![Conda Downloads](https://img.shields.io/conda/dn/conda-forge/aubio.svg?label=Conda%20downloads)](https://anaconda.org/conda-forge/aubio)&#xA;[![Documentation](https://readthedocs.org/projects/aubio/badge/?version=latest)](http://aubio.readthedocs.io/en/latest/?badge=latest &#34;Latest documentation&#34;) --&gt; &#xA;&lt;!--![GitHub code size in bytes](https://img.shields.io/github/languages/code-size/libAudioFlux/audioFlux)--&gt; &#xA;&lt;!-- shields.io --&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/libAudioFlux/audioFlux/build.yml?branch=master&#34; alt=&#34;GitHub Workflow Status (with branch)&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/license/libAudioFlux/audioFlux&#34; alt=&#34;GitHub&#34;&gt; &lt;img src=&#34;https://github.com/libAudioFlux/audioFlux/actions/workflows/build.yml/badge.svg?branch=master&#34; alt=&#34;example branch parameter&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/language-python%20%7C%20c%20-blue.svg?sanitize=true&#34; alt=&#34;language&#34;&gt; &lt;a href=&#34;https://pypi.org/project/audioflux/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/audioflux&#34; alt=&#34;PyPI - Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/audioflux/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-%3E%3D3.6-brightgreen&#34; alt=&#34;PyPI - Python Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/audioflux/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/dm/audioflux.svg?label=Pypi%20downloads&#34; alt=&#34;PyPI Downloads&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://audioflux.top/index.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Docs-passing-brightgreen&#34; alt=&#34;Docs&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.5281/zenodo.7548288&#34;&gt;&lt;img src=&#34;https://zenodo.org/badge/DOI/10.5281/zenodo.7548288.svg?sanitize=true&#34; alt=&#34;DOI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!--[![codebeat badge](https://codebeat.co/badges/0e21a344-0928-4aee-8262-be9a41fa488b)](https://codebeat.co/projects/github-com-libaudioflux-audioflux-master)&#xA;![](https://img.shields.io/badge/pod-v0.1.1-blue.svg)--&gt; &#xA;&lt;p&gt;A library for audio and music analysis, feature extraction.&lt;/p&gt; &#xA;&lt;h1&gt;Table of Contents&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/libAudioFlux/audioFlux/master/#overview&#34;&gt;Overview&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/libAudioFlux/audioFlux/master/#description&#34;&gt;Description&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/libAudioFlux/audioFlux/master/#functionality&#34;&gt;Functionality&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/libAudioFlux/audioFlux/master/#1-transform&#34;&gt;Transform&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/libAudioFlux/audioFlux/master/#2-feature&#34;&gt;Feature&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/libAudioFlux/audioFlux/master/#3-mir&#34;&gt;MIR&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/libAudioFlux/audioFlux/master/#quickstart&#34;&gt;Quickstart&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/libAudioFlux/audioFlux/master/#mel--mfcc&#34;&gt;Mel &amp;amp; MFCC&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/libAudioFlux/audioFlux/master/#cwt--synchrosqueezing&#34;&gt;CWT &amp;amp; Synchrosqueezing&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/libAudioFlux/audioFlux/master/#other-examples&#34;&gt;Other examples&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/libAudioFlux/audioFlux/master/#installation&#34;&gt;Installation&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/libAudioFlux/audioFlux/master/#python-package-intsall&#34;&gt;Python Package Intsall&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/libAudioFlux/audioFlux/master/#ios-build&#34;&gt;iOS build&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/libAudioFlux/audioFlux/master/#android-build&#34;&gt;Android build&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/libAudioFlux/audioFlux/master/#building-from-source&#34;&gt;Building from source&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/libAudioFlux/audioFlux/master/#benchmark&#34;&gt;Benchmark&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/libAudioFlux/audioFlux/master/#server-performance&#34;&gt;Server performance&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/libAudioFlux/audioFlux/master/#mobile-performance&#34;&gt;Mobile performance&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/libAudioFlux/audioFlux/master/#documentation&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/libAudioFlux/audioFlux/master/#contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/libAudioFlux/audioFlux/master/#citing&#34;&gt;Citing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/libAudioFlux/audioFlux/master/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;h3&gt;Description&lt;/h3&gt; &#xA;&lt;p&gt;In audio domain, feature extraction is particularly important for Audio Classification, Speech enhancement, Audio/Music Separation,music-information-retrieval(MIR), ASR and other audio task.&lt;/p&gt; &#xA;&lt;p&gt;In the above tasks, &lt;strong&gt;mel spectrogram&lt;/strong&gt; and &lt;strong&gt;mfcc&lt;/strong&gt; features are commonly used in traditional machine-learning based on statistics and deep-learning based on neural network.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;audioFlux&lt;/code&gt;&lt;/strong&gt; provides systematic, comprehensive and multi-dimensional feature extraction and combination, and combines various deep learning network models to conduct research and development learning in different fields.&lt;/p&gt; &#xA;&lt;p&gt;Can be used for deep learning, pattern recognition, signal processing, bioinformatics, statistics, finance, etc.&lt;/p&gt; &#xA;&lt;h3&gt;Functionality&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;audioFlux&lt;/code&gt;&lt;/strong&gt; is based on the design of data flow. It decouples each algorithm module structurally, and it is convenient, fast and efficient to extract features from large batches.The following are the main feature architecture diagrams, specific and detailed description view the documentation.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/libAudioFlux/audioFlux/master/image/feature_all.png&#34;&gt; &#xA;&lt;!--&lt;img src=&#39;./feature_all.pdf&#39;&gt;--&gt; &#xA;&lt;p&gt;The main functions of &lt;strong&gt;&lt;code&gt;audioFlux&lt;/code&gt;&lt;/strong&gt; include &lt;strong&gt;transform&lt;/strong&gt;, &lt;strong&gt;feature&lt;/strong&gt; and &lt;strong&gt;mir&lt;/strong&gt; modules.&lt;/p&gt; &#xA;&lt;h4&gt;1. Transform&lt;/h4&gt; &#xA;&lt;p&gt;In the timeâ€“frequency representation, main transform algorithm:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;BFT&lt;/code&gt;&lt;/strong&gt;&amp;nbsp;&amp;nbsp; - &amp;nbsp;&amp;nbsp;Based Fourier Transform, similar short-time Fourier transform.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;NSGT&lt;/code&gt;&lt;/strong&gt; - &amp;nbsp; Non-Stationary Gabor Transform.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;CWT&lt;/code&gt;&lt;/strong&gt;&amp;nbsp;&amp;nbsp; - &amp;nbsp;&amp;nbsp;Continuous Wavelet Transform.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;PWT&lt;/code&gt;&lt;/strong&gt;&amp;nbsp;&amp;nbsp; - &amp;nbsp;&amp;nbsp;Pseudo Wavelet Transform.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- &amp;emsp --&gt; &#xA;&lt;p&gt;The above transform supports all the following frequency scale types:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Linear - Short-time Fourier transform spectrogram.&lt;/li&gt; &#xA; &lt;li&gt;Linspace - Linspace-scale spectrogram.&lt;/li&gt; &#xA; &lt;li&gt;Mel - Mel-scale spectrogram.&lt;/li&gt; &#xA; &lt;li&gt;Bark - Bark-scale spectrogram.&lt;/li&gt; &#xA; &lt;li&gt;Erb - Erb-scale spectrogram.&lt;/li&gt; &#xA; &lt;li&gt;Octave - Octave-scale spectrogram.&lt;/li&gt; &#xA; &lt;li&gt;Log - Logarithmic-scale spectrogram.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The following transform are not supports multiple frequency scale types, only used as independent transform:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;CQT&lt;/code&gt;&lt;/strong&gt; - &amp;nbsp;&amp;nbsp;Constant-Q Transform.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;VQT&lt;/code&gt;&lt;/strong&gt; - &amp;nbsp;&amp;nbsp;Variable-Q Transform.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;ST&lt;/code&gt;&lt;/strong&gt;&amp;nbsp;&amp;nbsp; - &amp;nbsp;&amp;nbsp;S-Transform/Stockwell Transform.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;FST&lt;/code&gt;&lt;/strong&gt; - &amp;nbsp;&amp;nbsp;Fast S-Transform.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;DWT&lt;/code&gt;&lt;/strong&gt; - &amp;nbsp;&amp;nbsp;Discrete Wavelet Transform.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;WPT&lt;/code&gt;&lt;/strong&gt; - &amp;nbsp;&amp;nbsp;Wave Packet Transform.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;SWT&lt;/code&gt;&lt;/strong&gt; - &amp;nbsp;&amp;nbsp;Stationary Wavelet Transform.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Detailed transform function, description, and use view the documentation.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;em&gt;&lt;em&gt;synchrosqueezing&lt;/em&gt;&lt;/em&gt; or &lt;em&gt;&lt;em&gt;reassignment&lt;/em&gt;&lt;/em&gt; is a technique for sharpening a time-frequency representation, contains the following algorithms:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;reassign&lt;/code&gt;&lt;/strong&gt; - reassign transform for &lt;code&gt;STFT&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;synsq&lt;/code&gt;&lt;/strong&gt; - reassign data use &lt;code&gt;CWT&lt;/code&gt; data.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;wsst&lt;/code&gt;&lt;/strong&gt; - reassign transform for &lt;code&gt;CWT&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;2. Feature&lt;/h4&gt; &#xA;&lt;p&gt;The feature module contains the following algorithms:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;spectral&lt;/code&gt;&lt;/strong&gt; - Spectrum feature, supports all spectrum types.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;xxcc&lt;/code&gt;&lt;/strong&gt; - Cepstrum coefficients, supports all spectrum types.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;deconv&lt;/code&gt;&lt;/strong&gt; - Deconvolution for spectrum, supports all spectrum types.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;chroma&lt;/code&gt;&lt;/strong&gt; - Chroma feature, only supports &lt;code&gt;CQT&lt;/code&gt; spectrum, Linear/Octave spectrum based on &lt;code&gt;BFT&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- harmonic pitch class profiles(HPCP) --&gt; &#xA;&lt;h4&gt;3. MIR&lt;/h4&gt; &#xA;&lt;p&gt;The mir module contains the following algorithms:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;pitch&lt;/code&gt;&lt;/strong&gt; - YIN, STFT, etc algorithm.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;onset&lt;/code&gt;&lt;/strong&gt; - Spectrum flux, novelty, etc algorithm.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;hpss&lt;/code&gt;&lt;/strong&gt; - Median filtering, NMF algorithm.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;To install the &lt;strong&gt;&lt;code&gt;audioFlux&lt;/code&gt;&lt;/strong&gt; package, Python &amp;gt;=3.6, using the released python package:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install audioflux&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Mel &amp;amp; MFCC&lt;/h3&gt; &#xA;&lt;p&gt;Mel spectrogram and Mel-frequency cepstral coefficients&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np&#xA;import audioflux as af&#xA;&#xA;import matplotlib.pyplot as plt&#xA;from audioflux.display import fill_spec&#xA;&#xA;# Get a 220Hz&#39;s audio file path&#xA;sample_path = af.utils.sample_path(&#39;220&#39;)&#xA;&#xA;# Read audio data and sample rate&#xA;audio_arr, sr = af.read(sample_path)&#xA;&#xA;# Extract mel spectrogram&#xA;spec_arr, mel_fre_band_arr = af.mel_spectrogram(audio_arr, num=128, radix2_exp=12, samplate=sr)&#xA;spec_arr = np.abs(spec_arr)&#xA;&#xA;# Extract mfcc&#xA;mfcc_arr, _ = af.mfcc(audio_arr, cc_num=13, mel_num=128, radix2_exp=12, samplate=sr)&#xA;&#xA;# Display&#xA;audio_len = audio_arr.shape[0]&#xA;# calculate x/y-coords&#xA;x_coords = np.linspace(0, audio_len / sr, spec_arr.shape[1] + 1)&#xA;y_coords = np.insert(mel_fre_band_arr, 0, 0)&#xA;fig, ax = plt.subplots()&#xA;img = fill_spec(spec_arr, axes=ax,&#xA;                x_coords=x_coords, y_coords=y_coords,&#xA;                x_axis=&#39;time&#39;, y_axis=&#39;log&#39;,&#xA;                title=&#39;Mel Spectrogram&#39;)&#xA;fig.colorbar(img, ax=ax)&#xA;&#xA;fig, ax = plt.subplots()&#xA;img = fill_spec(mfcc_arr, axes=ax,&#xA;                x_coords=x_coords, x_axis=&#39;time&#39;,&#xA;                title=&#39;MFCC&#39;)&#xA;fig.colorbar(img, ax=ax)&#xA;&#xA;plt.show()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/libAudioFlux/audioFlux/master/image/demo_mel.png&#34; width=&#34;415&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/libAudioFlux/audioFlux/master/image/demo_mfcc.png&#34; width=&#34;415&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;CWT &amp;amp; Synchrosqueezing&lt;/h3&gt; &#xA;&lt;p&gt;Continuous Wavelet Transform spectrogram and its corresponding synchrosqueezing reassignment spectrogram&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np&#xA;import audioflux as af&#xA;from audioflux.type import SpectralFilterBankScaleType, WaveletContinueType&#xA;from audioflux.utils import note_to_hz&#xA;&#xA;import matplotlib.pyplot as plt&#xA;from audioflux.display import fill_spec&#xA;&#xA;# Get a 220Hz&#39;s audio file path&#xA;sample_path = af.utils.sample_path(&#39;220&#39;)&#xA;&#xA;# Read audio data and sample rate&#xA;audio_arr, sr = af.read(sample_path)&#xA;audio_arr = audio_arr[:4096]&#xA;&#xA;cwt_obj = af.CWT(num=84, radix2_exp=12, samplate=sr, low_fre=note_to_hz(&#39;C1&#39;),&#xA;                 bin_per_octave=12, wavelet_type=WaveletContinueType.MORSE,&#xA;                 scale_type=SpectralFilterBankScaleType.OCTAVE)&#xA;&#xA;cwt_spec_arr = cwt_obj.cwt(audio_arr)&#xA;&#xA;synsq_obj = af.Synsq(num=cwt_obj.num,&#xA;                     radix2_exp=cwt_obj.radix2_exp,&#xA;                     samplate=cwt_obj.samplate)&#xA;&#xA;synsq_arr = synsq_obj.synsq(cwt_spec_arr,&#xA;                            filter_bank_type=cwt_obj.scale_type,&#xA;                            fre_arr=cwt_obj.get_fre_band_arr())&#xA;&#xA;# Show CWT&#xA;fig, ax = plt.subplots(figsize=(7, 4))&#xA;img = fill_spec(np.abs(cwt_spec_arr), axes=ax,&#xA;                x_coords=cwt_obj.x_coords(),&#xA;                y_coords=cwt_obj.y_coords(),&#xA;                x_axis=&#39;time&#39;, y_axis=&#39;log&#39;,&#xA;                title=&#39;CWT&#39;)&#xA;fig.colorbar(img, ax=ax)&#xA;# Show Synsq&#xA;fig, ax = plt.subplots(figsize=(7, 4))&#xA;img = fill_spec(np.abs(synsq_arr), axes=ax,&#xA;                x_coords=cwt_obj.x_coords(),&#xA;                y_coords=cwt_obj.y_coords(),&#xA;                x_axis=&#39;time&#39;, y_axis=&#39;log&#39;,&#xA;                title=&#39;Synsq&#39;)&#xA;fig.colorbar(img, ax=ax)&#xA;&#xA;plt.show()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/libAudioFlux/audioFlux/master/image/demo_cwt.png&#34; width=&#34;415&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/libAudioFlux/audioFlux/master/image/demo_synsq.png&#34; width=&#34;415&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Other examples&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/libAudioFlux/audioFlux/master/docs/examples.md#cqt--chroma&#34;&gt;CQT &amp;amp; Chroma&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/libAudioFlux/audioFlux/master/docs/examples.md#different-wavelet-type&#34;&gt;Different Wavelet Type&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/libAudioFlux/audioFlux/master/docs/examples.md#spectral-features&#34;&gt;Spectral Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/libAudioFlux/audioFlux/master/docs/examples.md#pitch-estimate&#34;&gt;Pitch Estimate&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/libAudioFlux/audioFlux/master/docs/examples.md#onset-detection&#34;&gt;Onset Detection&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/libAudioFlux/audioFlux/master/docs/examples.md#harmonic-percussive-source-separation&#34;&gt;Harmonic Percussive Source Separation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;More example scripts are provided in the &lt;a href=&#34;https://audioflux.top/&#34;&gt;Documentation&lt;/a&gt; section.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/platform-iOS%20%7C%20android%20%7C%20macOS%20%7C%20linux%20%7C%20windows%20-lyellow.svg?sanitize=true&#34; alt=&#34;language&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The library is cross-platform and currently supports Linux, macOS, Windows, iOS and Android systems.&lt;/p&gt; &#xA;&lt;h3&gt;Python Package Intsall&lt;/h3&gt; &#xA;&lt;p&gt;Using PyPI:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ pip install audioflux &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Using Anaconda:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ conda install -c tanky25 -c conda-forge audioflux&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;!--Read installation instructions:&#xA;https://audioflux.top/install--&gt; &#xA;&lt;h3&gt;iOS build&lt;/h3&gt; &#xA;&lt;p&gt;To compile iOS on a Mac, Xcode Command Line Tools must exist in the system:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install the full Xcode package&lt;/li&gt; &#xA; &lt;li&gt;install Xcode Command Line Tools when triggered by a command or run xcode-select command:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ xcode-select --install &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Enter the &lt;strong&gt;&lt;code&gt;audioFlux&lt;/code&gt;&lt;/strong&gt; project &lt;strong&gt;&lt;code&gt;scripts&lt;/code&gt;&lt;/strong&gt; directory and switch to the current directory, run the following script to build and compile:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ./build_iOS.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Build and compile successfully, the project build compilation results are in the &lt;strong&gt;&lt;code&gt;build&lt;/code&gt;&lt;/strong&gt; folder&lt;/p&gt; &#xA;&lt;h3&gt;Android build&lt;/h3&gt; &#xA;&lt;p&gt;The current system development environment needs to be installed &lt;a href=&#34;https://developer.android.com/ndk&#34;&gt;&lt;strong&gt;android NDK&lt;/strong&gt;&lt;/a&gt;, ndk version&amp;gt;=16,after installation, set the environment variable ndk path.&lt;/p&gt; &#xA;&lt;p&gt;For example, ndk installation path is &lt;code&gt;~/Android/android-ndk-r16b&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ export NDK_ROOT=~/Android/android-ndk-r16b&#xA;$ export PATH=$NDK_ROOT:$PATH&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Android &lt;strong&gt;&lt;code&gt;audioFlux&lt;/code&gt;&lt;/strong&gt; build uses &lt;a href=&#34;https://www.fftw.org/&#34;&gt;&lt;strong&gt;fftw&lt;/strong&gt;&lt;/a&gt; library to accelerate performance, compile the single-floating point version for android platform. fftw lib successful compilation, copy to &lt;strong&gt;&lt;code&gt;audioFlux&lt;/code&gt;&lt;/strong&gt; project &lt;strong&gt;&lt;code&gt;scripts/android/fftw3&lt;/code&gt;&lt;/strong&gt; directory.&lt;/p&gt; &#xA;&lt;p&gt;Enter the &lt;strong&gt;&lt;code&gt;audioFlux&lt;/code&gt;&lt;/strong&gt; project &lt;strong&gt;&lt;code&gt;scripts&lt;/code&gt;&lt;/strong&gt; directory and switch to the current directory, run the following script to build and compile:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ./build_android.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Build and compile successfully, the project build compilation results are in the &lt;strong&gt;&lt;code&gt;build&lt;/code&gt;&lt;/strong&gt; folder&lt;/p&gt; &#xA;&lt;h3&gt;Building from source&lt;/h3&gt; &#xA;&lt;p&gt;For Linux, macOS, Windows systems. Read installation instructions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/libAudioFlux/audioFlux/master/docs/installing.md&#34;&gt;docs/installing.md&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Benchmark&lt;/h2&gt; &#xA;&lt;h3&gt;Server performance&lt;/h3&gt; &#xA;&lt;p&gt;server hardware:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;- CPU: AMD Ryzen Threadripper 3970X 32-Core Processor&#xA;- Memory: 128GB&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Each sample data is 128ms(sampling rate: 32000, data length: 4096).&lt;/p&gt; &#xA;&lt;p&gt;The total time spent on extracting features for 1000 sample data.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Package&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://github.com/libAudioFlux/audioFlux&#34;&gt;audioFlux&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://github.com/librosa/librosa&#34;&gt;librosa&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://github.com/tyiannak/pyAudioAnalysis&#34;&gt;pyAudioAnalysis&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://github.com/jameslyons/python_speech_features&#34;&gt;python_speech_features&lt;/a&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mel&lt;/td&gt; &#xA;   &lt;td&gt;0.777s&lt;/td&gt; &#xA;   &lt;td&gt;2.967s&lt;/td&gt; &#xA;   &lt;td&gt;--&lt;/td&gt; &#xA;   &lt;td&gt;--&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MFCC&lt;/td&gt; &#xA;   &lt;td&gt;0.797s&lt;/td&gt; &#xA;   &lt;td&gt;2.963s&lt;/td&gt; &#xA;   &lt;td&gt;0.805s&lt;/td&gt; &#xA;   &lt;td&gt;2.150s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CQT&lt;/td&gt; &#xA;   &lt;td&gt;5.743s&lt;/td&gt; &#xA;   &lt;td&gt;21.477s&lt;/td&gt; &#xA;   &lt;td&gt;--&lt;/td&gt; &#xA;   &lt;td&gt;--&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chroma&lt;/td&gt; &#xA;   &lt;td&gt;0.155s&lt;/td&gt; &#xA;   &lt;td&gt;2.174s&lt;/td&gt; &#xA;   &lt;td&gt;1.287s&lt;/td&gt; &#xA;   &lt;td&gt;--&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Mobile performance&lt;/h3&gt; &#xA;&lt;p&gt;For 128ms audio data per frame(sampling rate: 32000, data length: 4096).&lt;/p&gt; &#xA;&lt;p&gt;The time spent on extracting features for 1 frame data.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Mobile&lt;/th&gt; &#xA;   &lt;th&gt;iPhone 13 Pro&lt;/th&gt; &#xA;   &lt;th&gt;iPhone X&lt;/th&gt; &#xA;   &lt;th&gt;Honor V40&lt;/th&gt; &#xA;   &lt;th&gt;OPPO Reno4 SE 5G&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mel&lt;/td&gt; &#xA;   &lt;td&gt;0.249ms&lt;/td&gt; &#xA;   &lt;td&gt;0.359ms&lt;/td&gt; &#xA;   &lt;td&gt;0.313ms&lt;/td&gt; &#xA;   &lt;td&gt;0.891ms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MFCC&lt;/td&gt; &#xA;   &lt;td&gt;0.249ms&lt;/td&gt; &#xA;   &lt;td&gt;0.361ms&lt;/td&gt; &#xA;   &lt;td&gt;0.315ms&lt;/td&gt; &#xA;   &lt;td&gt;1.116ms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CQT&lt;/td&gt; &#xA;   &lt;td&gt;0.350ms&lt;/td&gt; &#xA;   &lt;td&gt;0.609ms&lt;/td&gt; &#xA;   &lt;td&gt;0.786ms&lt;/td&gt; &#xA;   &lt;td&gt;1.779ms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chroma&lt;/td&gt; &#xA;   &lt;td&gt;0.354ms&lt;/td&gt; &#xA;   &lt;td&gt;0.615ms&lt;/td&gt; &#xA;   &lt;td&gt;0.803ms&lt;/td&gt; &#xA;   &lt;td&gt;1.775ms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Documentation of the package can be found online:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://audioflux.top/&#34;&gt;https://audioflux.top&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We are more than happy to collaborate and receive your contributions to &lt;strong&gt;&lt;code&gt;audioFlux&lt;/code&gt;&lt;/strong&gt;. If you want to contribute, please fork the latest git repository and create a feature branch. Submitted requests should pass all continuous integration tests.&lt;/p&gt; &#xA;&lt;p&gt;You are also more than welcome to suggest any improvements, including proposals for need help, find a bug, have a feature request, ask a general question, new algorithms. &lt;a href=&#34;https://github.com/libAudioFlux/audioFlux/issues/new&#34;&gt;Open an issue&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citing&lt;/h2&gt; &#xA;&lt;p&gt;If you want to cite &lt;strong&gt;&lt;code&gt;audioFlux&lt;/code&gt;&lt;/strong&gt; in a scholarly work, there are two ways to do it.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;If you are using the library for your work, for the sake of reproducibility, please cite the version you used as indexed at Zenodo:&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.5281/zenodo.7548288&#34;&gt;&lt;img src=&#34;https://zenodo.org/badge/DOI/10.5281/zenodo.7548288.svg?sanitize=true&#34; alt=&#34;DOI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;audioFlux project is available MIT License.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>open-meteo/open-meteo</title>
    <updated>2023-03-03T02:18:02Z</updated>
    <id>tag:github.com,2023-03-03:/open-meteo/open-meteo</id>
    <link href="https://github.com/open-meteo/open-meteo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Free Weather Forecast API for non-commercial use&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ðŸŒ¤ Open-Meteo Weather API&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/open-meteo/open-meteo/actions/workflows/test.yml&#34;&gt;&lt;img src=&#34;https://github.com/open-meteo/open-meteo/actions/workflows/test.yml/badge.svg?branch=main&#34; alt=&#34;Test&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codebeat.co/projects/github-com-open-meteo-open-meteo-main&#34;&gt;&lt;img src=&#34;https://codebeat.co/badges/af28fed6-9cbf-41df-96a1-9bba03ae3c53&#34; alt=&#34;codebeat badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-meteo/open-meteo/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/open-meteo/open-meteo&#34; alt=&#34;GitHub license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://creativecommons.org/licenses/by/4.0/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-CC%20BY%204.0-lightgrey.svg?sanitize=true&#34; alt=&#34;license: CC BY 4.0&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/open_meteo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/follow-%40open_meteo-1DA1F2?logo=twitter&amp;amp;style=social&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://fosstodon.org/@openmeteo&#34;&gt;&lt;img src=&#34;https://img.shields.io/mastodon/follow/109320332765909743?domain=https%3A%2F%2Ffosstodon.org&#34; alt=&#34;Mastodon&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Open-Meteo is an open-source weather API and offers free access for non-commercial use. No API key is required. You can use it immediately!&lt;/p&gt; &#xA;&lt;p&gt;Head over to &lt;a href=&#34;https://open-meteo.com&#34;&gt;https://open-meteo.com&lt;/a&gt;! Stay up to date with our blog at &lt;a href=&#34;https://openmeteo.substack.com&#34;&gt;https://openmeteo.substack.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://open-meteo.com/en/docs&#34;&gt;Hourly weather forecast&lt;/a&gt; for 7 days&lt;/li&gt; &#xA; &lt;li&gt;Global weather models 11 km and regional up to 1.5 km resolution&lt;/li&gt; &#xA; &lt;li&gt;60 years &lt;a href=&#34;https://open-meteo.com/en/docs/historical-weather-api&#34;&gt;Historical Weather API&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Based on the best weather models: &lt;a href=&#34;https://open-meteo.com/en/docs/gfs-api&#34;&gt;NOAA GFS with HRRR&lt;/a&gt;, &lt;a href=&#34;https://open-meteo.com/en/docs/dwd-api&#34;&gt;DWD ICON&lt;/a&gt;, &lt;a href=&#34;https://open-meteo.com/en/docs/meteofrance-api&#34;&gt;MeteoFrance Arome&amp;amp;Arpege&lt;/a&gt;, &lt;a href=&#34;https://open-meteo.com/en/docs/ecmwf-api&#34;&gt;ECMWF IFS&lt;/a&gt;, &lt;a href=&#34;https://open-meteo.com/en/docs/jma-api&#34;&gt;JMA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://open-meteo.com/en/docs/marine-weather-api&#34;&gt;Marine Forecast API&lt;/a&gt;, &lt;a href=&#34;https://open-meteo.com/en/docs/air-quality-api&#34;&gt;Air Quality API&lt;/a&gt;, &lt;a href=&#34;https://open-meteo.com/en/docs/geocoding-api&#34;&gt;Geocoding API&lt;/a&gt;, &lt;a href=&#34;https://open-meteo.com/en/docs/elevation-api&#34;&gt;Elevation API&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;No API key required, CORS supported, no ads, no tracking, not even cookies&lt;/li&gt; &#xA; &lt;li&gt;Free for non-commercial use with data under Attribution 4.0 International (CC BY 4.0)&lt;/li&gt; &#xA; &lt;li&gt;Lightning fast APIs with response times below 10 ms&lt;/li&gt; &#xA; &lt;li&gt;Servers located in Germany (POP for North-America in planning. Sponsors welcome!)&lt;/li&gt; &#xA; &lt;li&gt;Source code available under AGPLv3&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How does Open-Meteo work?&lt;/h2&gt; &#xA;&lt;p&gt;Open-Meteo is using open-data weather forecasts from national weather providers (NWP).&lt;/p&gt; &#xA;&lt;p&gt;NWPs offer numerical weather predictions free to download. Unfortunately working with those models is difficult and requires expert knowledge about binary file formats, grid-systems, projections and fundamentals in weather predictions.&lt;/p&gt; &#xA;&lt;p&gt;The gap between downloading weather forecasts from NWPs and using weather forecasts in your home automation system, personal website, widgets for Linux or just tinkering around is huge! Even for small pet projects, you have to sign-up with credit-cards to large API vendors, which honestly do not offer properly engineered APIs.&lt;/p&gt; &#xA;&lt;p&gt;Open-Meteo fills this gap and offers free weather forecast APIs for non-commercial use without any sign-up, credit-card or even an API key required!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Do you want to build an open-source widget for Ubuntu? Sure!&lt;/li&gt; &#xA; &lt;li&gt;Use Open-Meteo for a React/Angular/Flutter App? Go for it!&lt;/li&gt; &#xA; &lt;li&gt;Improve your home automation system? Automate your robot lawn mower? Optimize your garden irrigation system? Open-Meteo is a good place to start!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Who is using Open-Meteo?&lt;/h2&gt; &#xA;&lt;p&gt;Apps:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://weathergraph.app&#34;&gt;WeatherGraph&lt;/a&gt; Apple Watch App&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://slideshow.digital/&#34;&gt;Slideshow&lt;/a&gt; Digital Signage app for Android&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/roe-dl/weewx-DWD&#34;&gt;weewx-DWD&lt;/a&gt; Weather forecasts etc. for WeeWX&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/woheller69/omweather&#34;&gt;omWeather&lt;/a&gt; Android Weather App&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Repositories:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/cburton-godaddy/captain-cold&#34;&gt;Captain Cold&lt;/a&gt; Simple Open-Meteo -&amp;gt; Discord integration&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tobealive/wthrr-the-weathercrab&#34;&gt;wthrr-the-weathercrab&lt;/a&gt; Weather companion for the terminal&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Rayrsn/Weather-Cli&#34;&gt;Weather-Cli&lt;/a&gt; A CLI program written in golang that allows you to get weather information from the terminal&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/benphelps/homepage/&#34;&gt;Homepage&lt;/a&gt; A highly customizable homepage (or startpage / application dashboard) with Docker and service API integrations.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.spots.guru&#34;&gt;Spots Guru&lt;/a&gt; Weather forecast for lazy, the best wind &amp;amp; wave spots around you.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Other:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.raycast.com/koinzhang/menubar-weather&#34;&gt;Menubar Weather&lt;/a&gt; A Raycast extension that displays live weather information in your menu bar&lt;/li&gt; &#xA; &lt;li&gt;Contributions welcome!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Do you use Open-Meteo? Please open a pull request and add your repository or app to the list!&lt;/p&gt; &#xA;&lt;h2&gt;Client SDKs&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Go &lt;a href=&#34;https://github.com/HectorMalot/omgo&#34;&gt;https://github.com/HectorMalot/omgo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Python &lt;a href=&#34;https://github.com/m0rp43us/openmeteopy&#34;&gt;https://github.com/m0rp43us/openmeteopy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Kotlin &lt;a href=&#34;https://github.com/open-meteo/open-meteo-api-kotlin&#34;&gt;https://github.com/open-meteo/open-meteo-api-kotlin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;.Net / C# &lt;a href=&#34;https://github.com/AlienDwarf/open-meteo-dotnet&#34;&gt;https://github.com/AlienDwarf/open-meteo-dotnet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;PHP Laravel &lt;a href=&#34;https://github.com/michaelnabil230/laravel-weather&#34;&gt;https://github.com/michaelnabil230/laravel-weather&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Contributions welcome! Writing a SDK for Open-Meteo is more than welcome and a great way to help users.&lt;/p&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Forecasts in 6-hour intervals for &lt;code&gt;morning&lt;/code&gt;, &lt;code&gt;afternoon&lt;/code&gt;, &lt;code&gt;evening&lt;/code&gt; and &lt;code&gt;night&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;14 day weather forecast based on GFS ensemble and ICON ensemble&lt;/li&gt; &#xA; &lt;li&gt;Wave and current forecasts&lt;/li&gt; &#xA; &lt;li&gt;Air quality forecast with gases and pollen in hourly resolution&lt;/li&gt; &#xA; &lt;li&gt;15 minutes weather forecast for 2 days for temperature, wind and solar radiation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;p&gt;If you encounter bugs while using Open-Meteo APIs, please file a new issue ticket. For general ideas or Q&amp;amp;A please use the &lt;a href=&#34;https://github.com/open-meteo/open-meteo/discussions&#34;&gt;Discussion&lt;/a&gt; section on Github. Thanks!&lt;/p&gt; &#xA;&lt;p&gt;For other enquiries please contact &lt;a href=&#34;mailto:info@open-meteo.com&#34;&gt;info@open-meteo.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Run your own API&lt;/h2&gt; &#xA;&lt;p&gt;Instructions to use Docker to run your own weather API are available in the &lt;a href=&#34;https://raw.githubusercontent.com/open-meteo/open-meteo/main/docs/getting-started.md&#34;&gt;getting started guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Terms &amp;amp; Privacy&lt;/h2&gt; &#xA;&lt;p&gt;Open-Meteo APIs are free for open-source developer and non-commercial use. We do not restrict access, but ask for fair use.&lt;/p&gt; &#xA;&lt;p&gt;If your application exceeds 10&#39;000 requests per day, please contact us. We reserve the right to block applications and IP addresses that misuse our service.&lt;/p&gt; &#xA;&lt;p&gt;For commercial use of Open-Meteo APIs, please contact us.&lt;/p&gt; &#xA;&lt;p&gt;All data is provided as is without any warranty.&lt;/p&gt; &#xA;&lt;p&gt;We do not collect any personal data. We do not share any personal information. We do not integrate any third party analytics, ads, beacons or plugins.&lt;/p&gt; &#xA;&lt;h2&gt;Data License&lt;/h2&gt; &#xA;&lt;p&gt;API data are offered under Attribution 4.0 International (CC BY 4.0)&lt;/p&gt; &#xA;&lt;p&gt;You are free to share: copy and redistribute the material in any medium or format and adapt: remix, transform, and build upon the material.&lt;/p&gt; &#xA;&lt;p&gt;Attribution: You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.&lt;/p&gt; &#xA;&lt;p&gt;You must include a link next to any location, Open-Meteo data are displayed like:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://open-meteo.com/&#34;&gt;Weather data by Open-Meteo.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Source Code License&lt;/h2&gt; &#xA;&lt;p&gt;Open-Meteo is open-source under the GNU Affero General Public License Version 3 (AGPLv3) or any later version. You can &lt;a href=&#34;https://raw.githubusercontent.com/open-meteo/open-meteo/main/LICENSE&#34;&gt;find the license here&lt;/a&gt;. Exceptions are third party source-code with individual licensing in each file.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>tvlad1234/pillScopePlus</title>
    <updated>2023-03-03T02:18:02Z</updated>
    <id>tag:github.com,2023-03-03:/tvlad1234/pillScopePlus</id>
    <link href="https://github.com/tvlad1234/pillScopePlus" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Oscilloscope based around the STM32F401 Black Pill and a color LCD screen&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;pillScope Plus&lt;/h1&gt; &#xA;&lt;p&gt;Oscilloscope based around the STM32F401 Black Pill and a color LCD screen, meant to be used as an educational tool &lt;img src=&#34;https://user-images.githubusercontent.com/60291077/177203708-384191ef-0c0f-4918-a163-46cf7b4721da.jpg&#34; alt=&#34;The pillScope in its case&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Why does it exist?&lt;/h2&gt; &#xA;&lt;p&gt;The goal of this project was to create a simple and easy to build but still very usable oscillsocope. I wanted to learn the basics of how a &lt;strong&gt;digital storage oscilloscope&lt;/strong&gt; functions, while also ending up with something which can be used as an educational tool in the lab. A short description of how oscillscopes (and this one in particular) work can be found &lt;a href=&#34;https://raw.githubusercontent.com/tvlad1234/pillScopePlus/main/HowItWorks.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;-3.3V to 3.3V input range (can be increased if using attenuator probes)&lt;/li&gt; &#xA; &lt;li&gt;1MOhm input impedance&lt;/li&gt; &#xA; &lt;li&gt;10uS/div minimum timebase&lt;/li&gt; &#xA; &lt;li&gt;1.6 MSa/S sampling rate&lt;/li&gt; &#xA; &lt;li&gt;On screen measurements: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;min/max voltage&lt;/li&gt; &#xA;   &lt;li&gt;peak-to-peak voltage&lt;/li&gt; &#xA;   &lt;li&gt;frequency&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Captured waveforms can be sent to a computer over UART and analyzed in the Tektronix TekScope app&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Required parts&lt;/h2&gt; &#xA;&lt;h3&gt;Base parts:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;STM32F401CC Black Pill development board&lt;/li&gt; &#xA; &lt;li&gt;128x160 ST7735-based TFT display&lt;/li&gt; &#xA; &lt;li&gt;3 pushbuttons&lt;/li&gt; &#xA; &lt;li&gt;LM358 dual op-amp (rail-to-rail opamps should work better in this context, but this is what I had on hand)&lt;/li&gt; &#xA; &lt;li&gt;2x 68kOhm resistors (to create a 1.65V offset voltage)&lt;/li&gt; &#xA; &lt;li&gt;2x 500kOhm resistors (to create the input attenuator)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Useful, but not mandatory:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;a 5V power supply&lt;/li&gt; &#xA; &lt;li&gt;an opto-isolated USB UART adapter&lt;/li&gt; &#xA; &lt;li&gt;a BNC connector, for using proper oscilloscope probes&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Schematics&lt;/h2&gt; &#xA;&lt;p&gt;The LCD screen is connected to SPI1 (PB3-SCK, PB5-MOSI, PB12-CS, PB13-RST, PB14-DC). The buttons are connected as follows:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;PB9: Up&lt;/li&gt; &#xA; &lt;li&gt;PB8: Select&lt;/li&gt; &#xA; &lt;li&gt;PB7: Down&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The output of the analog frontend is connected to ADC1_IN0, which corresponds to PA0.&lt;br&gt; The schematic of the frontend can be found in the &lt;a href=&#34;https://raw.githubusercontent.com/tvlad1234/pillScopePlus/main/frontend.pdf&#34;&gt;frontend.pdf&lt;/a&gt; file.&lt;/p&gt; &#xA;&lt;h2&gt;Using the oscilloscope&lt;/h2&gt; &#xA;&lt;h3&gt;The UI&lt;/h3&gt; &#xA;&lt;p&gt;The Select button cycles through the different parameters, which can be adjusted using the Up and Down buttons.&lt;/p&gt; &#xA;&lt;h3&gt;Auto calibration&lt;/h3&gt; &#xA;&lt;p&gt;Pressing Up and Down at the same time triggers the auto-calibration function. The tip and ground clip of the probed should be coupled together while calibrating.&lt;/p&gt; &#xA;&lt;h3&gt;Measuring things&lt;/h3&gt; &#xA;&lt;p&gt;The frontend of the instrument makes use of a virtual ground point which is 1.65V above the real ground. Because of this, the oscilloscope and the device under test must not be sharing the same ground reference. If you need to send data to the computer while measuring a device which shares ground with the scope, you should connect the computer via an opto-isolated adapter, while powering the oscilloscope from an external source.&lt;/p&gt; &#xA;&lt;h3&gt;Saving captured wavevorms&lt;/h3&gt; &#xA;&lt;p&gt;The captured waveforms can be sent to a computer over UART.&lt;/p&gt; &#xA;&lt;h4&gt;CSV Output&lt;/h4&gt; &#xA;&lt;p&gt;Sending &lt;code&gt;s&lt;/code&gt; (lowercase s) to the UART tells the instrument to output the captured waveform in CSV format, which is compatible with the Tektronix TekScope app.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/60291077/177576118-8649bee9-bdd7-459b-9a0d-911d3b135e4e.png&#34; alt=&#34;putty&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Direct TekScope output&lt;/h4&gt; &#xA;&lt;p&gt;Sending &lt;code&gt;S&lt;/code&gt; (capital S) tells the scope to output raw data, which can be read by a &lt;a href=&#34;https://github.com/tvlad1234/tekscopeIngest&#34;&gt;companion app&lt;/a&gt;. This app automatically streams the captured data to TekScope, which allows almost real-time waveform analysis on the computer. &lt;img src=&#34;https://user-images.githubusercontent.com/60291077/177576844-adfbda6b-5129-4aee-bc59-4e8e6be63796.png&#34; alt=&#34;companion&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Code&lt;/h2&gt; &#xA;&lt;p&gt;The code can be compiled with &lt;code&gt;make&lt;/code&gt;. The actual oscilloscope code of this project is located in &lt;code&gt;Core\Src&lt;/code&gt;, the &lt;a href=&#34;https://raw.githubusercontent.com/tvlad1234/pillScopePlus/main/Core/Src/scope.c&#34;&gt;scope.c&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/tvlad1234/pillScopePlus/main/Core/Src/ui.c&#34;&gt;ui.c&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/tvlad1234/pillScopePlus/main/Core/Src/wave.c&#34;&gt;wave.c&lt;/a&gt; files. Feel free to take a look, as they&#39;re commented for ease of understanding.&lt;/p&gt;</summary>
  </entry>
</feed>