<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-13T01:31:10Z</updated>
  <subtitle>Daily Trending of C in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ggerganov/llama.cpp</title>
    <updated>2023-03-13T01:31:10Z</updated>
    <id>tag:github.com,2023-03-13:/ggerganov/llama.cpp</id>
    <link href="https://github.com/ggerganov/llama.cpp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Port of Facebook&#39;s LLaMA model in C/C++&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;llama.cpp&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ggerganov/llama.cpp/actions&#34;&gt;&lt;img src=&#34;https://github.com/ggerganov/llama.cpp/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;Actions Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Inference of &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;Facebook&#39;s LLaMA&lt;/a&gt; model in pure C/C++&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Hot topics&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Running on Windows: &lt;a href=&#34;https://github.com/ggerganov/llama.cpp/issues/22&#34;&gt;https://github.com/ggerganov/llama.cpp/issues/22&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Fix Tokenizer / Unicode support: &lt;a href=&#34;https://github.com/ggerganov/llama.cpp/issues/11&#34;&gt;https://github.com/ggerganov/llama.cpp/issues/11&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Description&lt;/h2&gt; &#xA;&lt;p&gt;The main goal is to run the model using 4-bit quantization on a MacBook&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Plain C/C++ implementation without dependencies&lt;/li&gt; &#xA; &lt;li&gt;Apple silicon first-class citizen - optimized via Arm Neon and Accelerate framework&lt;/li&gt; &#xA; &lt;li&gt;AVX2 support for x86 architectures&lt;/li&gt; &#xA; &lt;li&gt;Mixed F16 / F32 precision&lt;/li&gt; &#xA; &lt;li&gt;4-bit quantization support&lt;/li&gt; &#xA; &lt;li&gt;Runs on the CPU&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This was &lt;a href=&#34;https://github.com/ggerganov/llama.cpp/issues/33#issuecomment-1465108022&#34;&gt;hacked in an evening&lt;/a&gt; - I have no idea if it works correctly. Please do not make conclusions about the models based on the results from this implementation. For all I know, it can be completely wrong. This project is for educational purposes and is not going to be maintained properly. New features will probably be added mostly through community contributions, if any.&lt;/p&gt; &#xA;&lt;p&gt;Supported platforms:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Mac OS&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Linux&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Windows (soon)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Here is a typical run using LLaMA-7B:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;make -j &amp;amp;&amp;amp; ./main -m ./models/7B/ggml-model-q4_0.bin -p &#34;Building a website can be done in 10 simple steps:&#34; -t 8 -n 512&#xA;I llama.cpp build info:&#xA;I UNAME_S:  Darwin&#xA;I UNAME_P:  arm&#xA;I UNAME_M:  arm64&#xA;I CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -DGGML_USE_ACCELERATE&#xA;I CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -pthread&#xA;I LDFLAGS:   -framework Accelerate&#xA;I CC:       Apple clang version 14.0.0 (clang-1400.0.29.202)&#xA;I CXX:      Apple clang version 14.0.0 (clang-1400.0.29.202)&#xA;&#xA;make: Nothing to be done for `default&#39;.&#xA;main: seed = 1678486056&#xA;llama_model_load: loading model from &#39;./models/7B/ggml-model-q4_0.bin&#39; - please wait ...&#xA;llama_model_load: n_vocab = 32000&#xA;llama_model_load: n_ctx   = 512&#xA;llama_model_load: n_embd  = 4096&#xA;llama_model_load: n_mult  = 256&#xA;llama_model_load: n_head  = 32&#xA;llama_model_load: n_layer = 32&#xA;llama_model_load: n_rot   = 128&#xA;llama_model_load: f16     = 2&#xA;llama_model_load: n_ff    = 11008&#xA;llama_model_load: ggml ctx size = 4529.34 MB&#xA;llama_model_load: memory_size =   512.00 MB, n_mem = 16384&#xA;llama_model_load: .................................... done&#xA;llama_model_load: model size =  4017.27 MB / num tensors = 291&#xA;&#xA;main: prompt: &#39;Building a website can be done in 10 simple steps:&#39;&#xA;main: number of tokens in prompt = 15&#xA;     1 -&amp;gt; &#39;&#39;&#xA;  8893 -&amp;gt; &#39;Build&#39;&#xA;   292 -&amp;gt; &#39;ing&#39;&#xA;   263 -&amp;gt; &#39; a&#39;&#xA;  4700 -&amp;gt; &#39; website&#39;&#xA;   508 -&amp;gt; &#39; can&#39;&#xA;   367 -&amp;gt; &#39; be&#39;&#xA;  2309 -&amp;gt; &#39; done&#39;&#xA;   297 -&amp;gt; &#39; in&#39;&#xA; 29871 -&amp;gt; &#39; &#39;&#xA; 29896 -&amp;gt; &#39;1&#39;&#xA; 29900 -&amp;gt; &#39;0&#39;&#xA;  2560 -&amp;gt; &#39; simple&#39;&#xA;  6576 -&amp;gt; &#39; steps&#39;&#xA; 29901 -&amp;gt; &#39;:&#39;&#xA;&#xA;sampling parameters: temp = 0.800000, top_k = 40, top_p = 0.950000&#xA;&#xA;&#xA;Building a website can be done in 10 simple steps:&#xA;1) Select a domain name and web hosting plan&#xA;2) Complete a sitemap&#xA;3) List your products&#xA;4) Write product descriptions&#xA;5) Create a user account&#xA;6) Build the template&#xA;7) Start building the website&#xA;8) Advertise the website&#xA;9) Provide email support&#xA;10) Submit the website to search engines&#xA;A website is a collection of web pages that are formatted with HTML. HTML is the code that defines what the website looks like and how it behaves.&#xA;The HTML code is formatted into a template or a format. Once this is done, it is displayed on the user&#39;s browser.&#xA;The web pages are stored in a web server. The web server is also called a host. When the website is accessed, it is retrieved from the server and displayed on the user&#39;s computer.&#xA;A website is known as a website when it is hosted. This means that it is displayed on a host. The host is usually a web server.&#xA;A website can be displayed on different browsers. The browsers are basically the software that renders the website on the user&#39;s screen.&#xA;A website can also be viewed on different devices such as desktops, tablets and smartphones.&#xA;Hence, to have a website displayed on a browser, the website must be hosted.&#xA;A domain name is an address of a website. It is the name of the website.&#xA;The website is known as a website when it is hosted. This means that it is displayed on a host. The host is usually a web server.&#xA;A website can be displayed on different browsers. The browsers are basically the software that renders the website on the user’s screen.&#xA;A website can also be viewed on different devices such as desktops, tablets and smartphones. Hence, to have a website displayed on a browser, the website must be hosted.&#xA;A domain name is an address of a website. It is the name of the website.&#xA;A website is an address of a website. It is a collection of web pages that are formatted with HTML. HTML is the code that defines what the website looks like and how it behaves.&#xA;The HTML code is formatted into a template or a format. Once this is done, it is displayed on the user’s browser.&#xA;A website is known as a website when it is hosted&#xA;&#xA;main: mem per token = 14434244 bytes&#xA;main:     load time =  1332.48 ms&#xA;main:   sample time =  1081.40 ms&#xA;main:  predict time = 31378.77 ms / 61.41 ms per token&#xA;main:    total time = 34036.74 ms&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And here is another demo of running both LLaMA-7B and &lt;a href=&#34;https://github.com/ggerganov/whisper.cpp&#34;&gt;whisper.cpp&lt;/a&gt; on a single M1 Pro MacBook:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/1991296/224442907-7693d4be-acaa-4e01-8b4f-add84093ffff.mp4&#34;&gt;https://user-images.githubusercontent.com/1991296/224442907-7693d4be-acaa-4e01-8b4f-add84093ffff.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Here are the step for the LLaMA-7B model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# build this repo&#xA;git clone https://github.com/ggerganov/llama.cpp&#xA;cd llama.cpp&#xA;make&#xA;&#xA;# obtain the original LLaMA model weights and place them in ./models&#xA;ls ./models&#xA;65B 30B 13B 7B tokenizer_checklist.chk tokenizer.model&#xA;&#xA;# install Python dependencies&#xA;python3 -m pip install torch numpy sentencepiece&#xA;&#xA;# convert the 7B model to ggml FP16 format&#xA;python3 convert-pth-to-ggml.py models/7B/ 1&#xA;&#xA;# quantize the model to 4-bits&#xA;./quantize ./models/7B/ggml-model-f16.bin ./models/7B/ggml-model-q4_0.bin 2&#xA;&#xA;# run the inference&#xA;./main -m ./models/7B/ggml-model-q4_0.bin -t 8 -n 128&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For the bigger models, there are a few extra quantization steps. For example, for LLaMA-13B, converting to FP16 format will create 2 ggml files, instead of one:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ggml-model-f16.bin&#xA;ggml-model-f16.bin.1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You need to quantize each of them separately like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./quantize ./models/13B/ggml-model-f16.bin   ./models/13B/ggml-model-q4_0.bin 2&#xA;./quantize ./models/13B/ggml-model-f16.bin.1 ./models/13B/ggml-model-q4_0.bin.1 2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Everything else is the same. Simply run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./main -m ./models/13B/ggml-model-q4_0.bin -t 8 -n 128&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The number of files generated for each model is as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;7B  -&amp;gt; 1 file&#xA;13B -&amp;gt; 2 files&#xA;30B -&amp;gt; 4 files&#xA;65B -&amp;gt; 8 files&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When running the larger models, make sure you have enough disk space to store all the intermediate files.&lt;/p&gt; &#xA;&lt;h3&gt;Interactive mode&lt;/h3&gt; &#xA;&lt;p&gt;If you want a more ChatGPT-like experience, you can run in interactive mode by passing &lt;code&gt;-i&lt;/code&gt; as a parameter. In this mode, you can always interrupt generation by pressing Ctrl+C and enter one or more lines of text which will be converted into tokens and appended to the current context. You can also specify a &lt;em&gt;reverse prompt&lt;/em&gt; with the parameter &lt;code&gt;-r &#34;reverse prompt string&#34;&lt;/code&gt;. This will result in user input being prompted whenever the exact tokens of the reverse prompt string are encountered in the generation. A typical use is to use a prompt which makes LLaMa emulate a chat between multiple users, say Alice and Bob, and pass &lt;code&gt;-r &#34;Alice:&#34;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Here is an example few-shot interaction, invoked with the command&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./main -m ./models/13B/ggml-model-q4_0.bin -t 8 -n 256 --repeat_penalty 1.0 --color -i -r &#34;User:&#34; \&#xA;                                           -p \&#xA;&#34;Transcript of a dialog, where the User interacts with an Assistant named Bob. Bob is helpful, kind, honest, good at writing, and never fails to answer the User&#39;s requests immediately and with precision.&#xA;&#xA;User: Hello, Bob.&#xA;Bob: Hello. How may I help you today?&#xA;User: Please tell me the largest city in Europe.&#xA;Bob: Sure. The largest city in Europe is Moscow, the capital of Russia.&#xA;User:&#34;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note the use of &lt;code&gt;--color&lt;/code&gt; to distinguish between user input and generated text.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/1991296/224575029-2af3c7dc-5a65-4f64-a6bb-517a532aea38.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Limitations&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Not sure if my tokenizer is correct. There are a few places where we might have a mistake: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/ggerganov/llama.cpp/raw/26c084662903ddaca19bef982831bfb0856e8257/convert-pth-to-ggml.py#L79-L87&#34;&gt;https://github.com/ggerganov/llama.cpp/blob/26c084662903ddaca19bef982831bfb0856e8257/convert-pth-to-ggml.py#L79-L87&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/ggerganov/llama.cpp/raw/26c084662903ddaca19bef982831bfb0856e8257/utils.h#L65-L69&#34;&gt;https://github.com/ggerganov/llama.cpp/blob/26c084662903ddaca19bef982831bfb0856e8257/utils.h#L65-L69&lt;/a&gt; In general, it seems to work, but I think it fails for unicode character support. Hopefully, someone can help with that&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;I don&#39;t know yet how much the quantization affects the quality of the generated text&lt;/li&gt; &#xA; &lt;li&gt;Probably the token sampling can be improved&lt;/li&gt; &#xA; &lt;li&gt;The Accelerate framework is actually currently unused since I found that for tensor shapes typical for the Decoder, there is no benefit compared to the ARM_NEON intrinsics implementation. Of course, it&#39;s possible that I simlpy don&#39;t know how to utilize it properly. But in any case, you can even disable it with &lt;code&gt;LLAMA_NO_ACCELERATE=1 make&lt;/code&gt; and the performance will be the same, since no BLAS calls are invoked by the current implementation&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>