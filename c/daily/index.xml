<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-08-19T01:34:21Z</updated>
  <subtitle>Daily Trending of C in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>firedancer-io/firedancer</title>
    <updated>2022-08-19T01:34:21Z</updated>
    <id>tag:github.com,2022-08-19:/firedancer-io/firedancer</id>
    <link href="https://github.com/firedancer-io/firedancer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Firedancer is Jump Crypto&#39;s Solana consensus node implementation.&lt;/p&gt;&lt;hr&gt;</summary>
  </entry>
  <entry>
    <title>cboxdoerfer/fsearch</title>
    <updated>2022-08-19T01:34:21Z</updated>
    <id>tag:github.com,2022-08-19:/cboxdoerfer/fsearch</id>
    <link href="https://github.com/cboxdoerfer/fsearch" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A fast file search utility for Unix-like systems based on GTK3&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://github.com/cboxdoerfer/fsearch/actions/workflows/build_test.yml/badge.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt; &lt;a href=&#34;https://hosted.weblate.org/engage/fsearch/?utm_source=widget&#34;&gt;&lt;img src=&#34;https://hosted.weblate.org/widgets/fsearch/-/svg-badge.svg?sanitize=true&#34; alt=&#34;Translation status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;FSearch is a fast file search utility, inspired by Everything Search Engine. It&#39;s written in C and based on GTK3.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For bug reports and feature requests please use the issue tracker: &lt;a href=&#34;https://github.com/cboxdoerfer/fsearch/issues&#34;&gt;https://github.com/cboxdoerfer/fsearch/issues&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;For discussions and questions about FSearch use the discussion forum: &lt;a href=&#34;https://github.com/cboxdoerfer/fsearch/discussions&#34;&gt;https://github.com/cboxdoerfer/fsearch/discussions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;For everything else related to FSearch you can talk to me on Matrix: &lt;a href=&#34;https://matrix.to/#/#fsearch:matrix.org&#34;&gt;https://matrix.to/#/#fsearch:matrix.org&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cboxdoerfer/fsearch/master/data/screenshots/02-main_window_menubar.png&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/cboxdoerfer/fsearch/master/data/screenshots/01-main_window_headerbar.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Instant (as you type) results&lt;/li&gt; &#xA; &lt;li&gt;Wildcard support&lt;/li&gt; &#xA; &lt;li&gt;RegEx support&lt;/li&gt; &#xA; &lt;li&gt;Filter support (only search for files, folders or everything)&lt;/li&gt; &#xA; &lt;li&gt;Include and exclude specific folders to be indexed&lt;/li&gt; &#xA; &lt;li&gt;Ability to exclude certain files/folders from index using wildcard expressions&lt;/li&gt; &#xA; &lt;li&gt;Fast sort by filename, path, size or modification time&lt;/li&gt; &#xA; &lt;li&gt;Customizable interface (e.g., switch between traditional UI with menubar and client-side decorations)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GTK 3.18&lt;/li&gt; &#xA; &lt;li&gt;GLib 2.44&lt;/li&gt; &#xA; &lt;li&gt;glibc 2.19 or musl 1.1.15 (other C standard libraries might work too, those are just the ones I verified)&lt;/li&gt; &#xA; &lt;li&gt;PCRE2 (libpcre2)&lt;/li&gt; &#xA; &lt;li&gt;ICU 3.8&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Download&lt;/h2&gt; &#xA;&lt;h3&gt;Ubuntu&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Release Builds: &lt;a href=&#34;https://launchpad.net/~christian-boxdoerfer/+archive/ubuntu/fsearch-stable&#34;&gt;https://launchpad.net/~christian-boxdoerfer/+archive/ubuntu/fsearch-stable&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Development Builds: &lt;a href=&#34;https://launchpad.net/~christian-boxdoerfer/+archive/ubuntu/fsearch-daily&#34;&gt;https://launchpad.net/~christian-boxdoerfer/+archive/ubuntu/fsearch-daily&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Arch Linux (AUR)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Release Builds: &lt;a href=&#34;https://aur.archlinux.org/packages/fsearch/&#34;&gt;https://aur.archlinux.org/packages/fsearch/&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Development Builds: &lt;a href=&#34;https://aur.archlinux.org/packages/fsearch-git/&#34;&gt;https://aur.archlinux.org/packages/fsearch-git/&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Fedora/RHEL/CentOS&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Release Builds: &lt;a href=&#34;https://copr.fedorainfracloud.org/coprs/cboxdoerfer/fsearch/&#34;&gt;https://copr.fedorainfracloud.org/coprs/cboxdoerfer/fsearch/&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Development Builds: &lt;a href=&#34;https://copr.fedorainfracloud.org/coprs/cboxdoerfer/fsearch_nightly/&#34;&gt;https://copr.fedorainfracloud.org/coprs/cboxdoerfer/fsearch_nightly/&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;FreeBSD&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Release Builds: &lt;a href=&#34;https://www.freshports.org/sysutils/fsearch&#34;&gt;https://www.freshports.org/sysutils/fsearch&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Flatpak (&lt;a href=&#34;https://github.com/cboxdoerfer/fsearch/wiki/Flatpak-version-limitations&#34;&gt;limited in functionality&lt;/a&gt;)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Release Builds: &lt;a href=&#34;https://flathub.org/apps/details/io.github.cboxdoerfer.FSearch&#34;&gt;https://flathub.org/apps/details/io.github.cboxdoerfer.FSearch&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;NixOS (unofficial)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Development Builds: &lt;a href=&#34;https://search.nixos.org/packages?channel=unstable&amp;amp;show=fsearch&amp;amp;query=fsearch&#34;&gt;https://search.nixos.org/packages?channel=unstable&amp;amp;show=fsearch&amp;amp;query=fsearch&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Solus&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Release Builds: &lt;a href=&#34;https://dev.getsol.us/source/fsearch/&#34;&gt;https://dev.getsol.us/source/fsearch/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/cboxdoerfer/fsearch/wiki/Roadmap&#34;&gt;https://github.com/cboxdoerfer/fsearch/wiki/Roadmap&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Build Instructions&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/cboxdoerfer/fsearch/wiki/Build-instructions&#34;&gt;https://github.com/cboxdoerfer/fsearch/wiki/Build-instructions&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Localization&lt;/h2&gt; &#xA;&lt;p&gt;The localization of FSearch is managed with Weblate.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://hosted.weblate.org/projects/fsearch/&#34;&gt;https://hosted.weblate.org/projects/fsearch/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to contribute translations please submit them there, instead of opening pull requests on GitHub. Instructions can be found here: &lt;a href=&#34;https://docs.weblate.org/en/latest/user/basic.html&#34;&gt;https://docs.weblate.org/en/latest/user/basic.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;And of course: Thank you for taking the time to translate FSearch!&lt;/p&gt; &#xA;&lt;h2&gt;Current Limitations&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Sorting lots of results by &lt;em&gt;Type&lt;/em&gt; can be very slow, since gathering that information is expensive, and the data isn&#39;t indexed. This also means that when the view is sorted by &lt;em&gt;Type&lt;/em&gt;, searching will reset the sort order to &lt;em&gt;Name&lt;/em&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Sorting can&#39;t be aborted. This is usually not an issue, because it&#39;s very fast for all columns except the &lt;em&gt;Type&lt;/em&gt; column. (Fixed in &amp;gt;=0.2alpha)&lt;/li&gt; &#xA; &lt;li&gt;Using the &lt;em&gt;Move to Trash&lt;/em&gt; option doesn&#39;t update the database index, so trashed files/folders show up in the result list as if nothing happened to them.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Why yet another search utility?&lt;/h2&gt; &#xA;&lt;p&gt;Performance. On Windows I really like to use Everything Search Engine. It provides instant results as you type for all your files and lots of useful features (regex, filters, bookmarks, ...). On Linux I couldn&#39;t find anything that&#39;s even remotely as fast and powerful.&lt;/p&gt; &#xA;&lt;p&gt;Before I started working on FSearch, I took a look at existing solutions. I tried MATE Search Tool (formerly GNOME Search Tool), Recoll, Krusader (locate based search), SpaceFM File Search, Nautilus, ANGRYsearch and Catfish, to find out whether it makes sense to improve those. However, they&#39;re not exactly what I was looking for:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;standalone application (not part of a file manager)&lt;/li&gt; &#xA; &lt;li&gt;written in a language with C like performance&lt;/li&gt; &#xA; &lt;li&gt;no dependencies to any specific desktop environment&lt;/li&gt; &#xA; &lt;li&gt;Qt5 or GTK3 based&lt;/li&gt; &#xA; &lt;li&gt;small memory usage (both hard drive and RAM)&lt;/li&gt; &#xA; &lt;li&gt;target audience: advanced users&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Looking for a command line interface?&lt;/h2&gt; &#xA;&lt;p&gt;I highly recommend &lt;a href=&#34;https://github.com/junegunn/fzf&#34;&gt;fzf&lt;/a&gt; or the obvious tools: find and (m)locate&lt;/p&gt; &#xA;&lt;h2&gt;Why GTK3 and not Qt5?&lt;/h2&gt; &#xA;&lt;p&gt;I like both of them, and my long term goal is to provide console, GTK3 and Qt5 interfaces, or at least make it easy for others to build those. However, for the time being it&#39;s only GTK3 because I like C more than C++, and I&#39;m more familiar with GTK development.&lt;/p&gt; &#xA;&lt;h2&gt;Questions?&lt;/h2&gt; &#xA;&lt;p&gt;Email: christian.boxdoerfer[AT]posteo.de&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>sipeed/TinyMaix</title>
    <updated>2022-08-19T01:34:21Z</updated>
    <id>tag:github.com,2022-08-19:/sipeed/TinyMaix</id>
    <link href="https://github.com/sipeed/TinyMaix" rel="alternate"></link>
    <summary type="html">&lt;p&gt;TinyMaix is a tiny inference library for microcontrollers (TinyML).&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TinyMaix&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sipeed/TinyMaix/main/README_ZH.md&#34;&gt;中文&lt;/a&gt; | English&lt;/p&gt; &#xA;&lt;p&gt;TinyMaix is a tiny inference Neural Network library specifically for microcontrollers (TinyML).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Highlights&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Core Code less than &lt;strong&gt;400 lines&lt;/strong&gt;(tm_layers.c+tm_model.c+arch_O0.h), code .text section less than &lt;strong&gt;3KB&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Low ram consume, even &lt;strong&gt;Arduino ATmega328&lt;/strong&gt; (32KB Flash, 2KB Ram) can run mnist with TinyMaix~&lt;/li&gt; &#xA; &lt;li&gt;Support &lt;strong&gt;INT8/FP32/FP16&lt;/strong&gt; model, convert from keras h5 or tflite.&lt;/li&gt; &#xA; &lt;li&gt;Supoort multi architecture accelerate: &lt;strong&gt;ARM SIMD/NEON/MVEI，RV32P, RV64V&lt;/strong&gt; ~&lt;/li&gt; &#xA; &lt;li&gt;User-friendly interfaces, just load/run models~&lt;/li&gt; &#xA; &lt;li&gt;Support Full Static Memory config&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://maixhub.com&#34;&gt;MaixHub&lt;/a&gt; &lt;strong&gt;Online Model Training&lt;/strong&gt; support soon~&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Run mnist demo on Arduino ATmega328&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mnist demo&#xA;0000000000000000000000000000&#xA;0000000000000000000000000000&#xA;0000000000000000000000000000&#xA;000000000077AFF9500000000000&#xA;000000000AFFFFFFD10000000000&#xA;00000000AFFFD8BFF70000000000&#xA;00000003FFD2000CF80000000000&#xA;00000004FD10007FF40000000000&#xA;00000000110000DFF40000000000&#xA;00000000000007FFC00000000000&#xA;0000000000004FFE300000000000&#xA;0000000000008FF9000000000000&#xA;00000000000BFF90000000000000&#xA;00000000001EFE20000000000000&#xA;0000000000CFF800000000000000&#xA;0000000004FFB000000000000000&#xA;000000001CFF8000000000000000&#xA;000000008FFA0000000000000000&#xA;00000000FFF10000000000000000&#xA;00000000FFF21111000112999900&#xA;00000000FFFFFFFFA8AFFFFFFF70&#xA;00000000AFFFFFFFFFFFFFFA7730&#xA;0000000007777AFFF97720000000&#xA;0000000000000000000000000000&#xA;0000000000000000000000000000&#xA;0000000000000000000000000000&#xA;0000000000000000000000000000&#xA;0000000000000000000000000000&#xA;===use 49912us&#xA;0: 0&#xA;1: 0&#xA;2: 89&#xA;3: 0&#xA;4: 1&#xA;5: 6&#xA;6: 1&#xA;7: 0&#xA;8: 0&#xA;9: 0&#xA;### Predict output is: Number 2, prob=89&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;TinyMaix Design&lt;/h2&gt; &#xA;&lt;p&gt;TinyMaix is design for running AI Neural Network Mdoels on resources limited MCUs, which usually called &lt;strong&gt;TinyML&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;There are many TinyML infer library now, like TFLite micro, microTVM, NNoM, so why we need TinyMaix?&lt;/p&gt; &#xA;&lt;p&gt;TinyMaix is a weekend hackathons project, so it is simple enough to read though in 30 minutes, and it will help TinyML newbies to understand how is it running.&lt;/p&gt; &#xA;&lt;p&gt;TinyMaix aims to be a simple TinyML infererence library, it abandon many new features and doesn&#39;t use libs like CMSIS-NN.&lt;/p&gt; &#xA;&lt;p&gt;Following this design goal, now TinyMaix is as simple as 5 files to compile~&lt;/p&gt; &#xA;&lt;p&gt;We hope TinyMaix can help any MCU run AI Neural Network Mdoels, every one can port it to theirself hardware platform~&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: Although TinyMaix support multi architecture accelerate, but it still need more effort to balance size and speed.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Features in design&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support up to mobilenet v1, RepVGG backbone &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;they are most common used, efficient structure for MCUs&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Basic Conv2d, dwConv2d, FC, Relu/Relu6/Softmax, GAP, Reshape&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; MaxPool, AvgPool (now use stride instead)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; FP32 model, INT8 quant model, FP16 model(&lt;strong&gt;NEW&lt;/strong&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Convert tmdl from keras h5 or tflite &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;model is simple enough to train with keras/tf&lt;/li&gt; &#xA;   &lt;li&gt;tflite have quant functions already&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Model statistics functions in C &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Optional for reduce code&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Features maybe added&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; INT16 quant model &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Advantages: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;more accuracy&lt;/li&gt; &#xA;     &lt;li&gt;friendly for SIMD/RV32P accelerate&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Disadvantages: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;increase FLASH/RAM consume 2X&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Concat OP &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Advantages: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;support mobilenet v2, more accuracy&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Disadvantages: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;increase RAM consume 2X&lt;/li&gt; &#xA;     &lt;li&gt;concat mat cost many time cause model infer slow&lt;/li&gt; &#xA;     &lt;li&gt;need more work to cvt model into flat structure (in script)&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Winograd Convolution Optimization &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Advantages: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;may speed up Conv computing&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Disadvantages: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;increase RAM consume, and consume more memory bandwidth&lt;/li&gt; &#xA;     &lt;li&gt;increase code (.text) size&lt;/li&gt; &#xA;     &lt;li&gt;need many Transforms, weak MCU may cost many time here&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Features won&#39;t be added&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; BF16 model &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;most MCU don&#39;t have BF16 computing ability&lt;/li&gt; &#xA;   &lt;li&gt;accuracy won&#39;t better than INT16 to much&lt;/li&gt; &#xA;   &lt;li&gt;increase FLASH/RAM consume 2X&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; AVX/vulkan acceleration &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;TinyMaix is for MCUs, not for powerful PC/mobilephones&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; other misc OPs &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;TinyMaix support MCUs to run basic model in minimum resource consumption, if you want more OPs, switch to TFlite-micro/TVM/NCNN...&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Try Demos&lt;/h2&gt; &#xA;&lt;h3&gt;mnist&lt;/h3&gt; &#xA;&lt;p&gt;MNIST is handwritten digit recognition task, it is simple enough for even 8bit MCU like ATmega328.&lt;br&gt; Try it on PC:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd examples/mnist&#xA;mkdir build&#xA;cd build &#xA;cmake ..&#xA;make&#xA;./mnist&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;mbnet&lt;/h3&gt; &#xA;&lt;p&gt;mbnet (mobilenet v1) is simple classification model for mobile devices, but it is still a little heavy for MCUs.&lt;br&gt; The model in demo is mobilenet v1 0.25, it input 128x128x3 RGB image, output 1000 classes predict.&lt;br&gt; It need at least 128KB SRAM and 512KB Flash, STM32F411 is the typical minimum config for this model.&lt;/p&gt; &#xA;&lt;p&gt;Try run mobilenet&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd examples/mbnet&#xA;mkdir build&#xA;cd build &#xA;cmake ..&#xA;make&#xA;./mbnet&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;How to use (API)&lt;/h2&gt; &#xA;&lt;h3&gt;Load Model&lt;/h3&gt; &#xA;&lt;p&gt;tm_err_t tm_load (tm_mdl_t* mdl, const uint8_t* bin, uint8_t&lt;em&gt;buf, tm_cb_t cb, tm_mat_t&lt;/em&gt; in);&lt;/p&gt; &#xA;&lt;p&gt;mdl: model handle;&lt;br&gt; bin: model bin buf;&lt;br&gt; buf: main buf for middle output; if NULL, auto malloc main buf; else, use your static buffer.&lt;br&gt; cb: layer callback;&lt;br&gt; in: return input mat, include buf addr; //you can ignore it if use static buf&lt;/p&gt; &#xA;&lt;h3&gt;Remove Model&lt;/h3&gt; &#xA;&lt;p&gt;void tm_unload(tm_mdl_t* mdl);&lt;/p&gt; &#xA;&lt;h3&gt;Preprocess Input Data&lt;/h3&gt; &#xA;&lt;p&gt;tm_err_t tm_preprocess(tm_mdl_t* mdl, tm_pp_t pp_type, tm_mat_t* in, tm_mat_t* out);&lt;br&gt; TMPP_FP2INT //user own fp buf -&amp;gt; int input buf&lt;br&gt; TMPP_UINT2INT //int8: cvt in place; int16: can&#39;t cvt in place&lt;br&gt; TMPP_UINT2FP01 // u8/255.0&lt;br&gt; TMPP_UINT2FPN11// (u8-128)/128&lt;/p&gt; &#xA;&lt;h3&gt;Run Model&lt;/h3&gt; &#xA;&lt;p&gt;tm_err_t tm_run (tm_mdl_t* mdl, tm_mat_t* in, tm_mat_t* out);&lt;/p&gt; &#xA;&lt;h2&gt;How to port&lt;/h2&gt; &#xA;&lt;p&gt;The core file is those 5 files: tm_model.c, tm_layers.c, tinymaix.h, tm_port.h, arch_xxx.h&lt;/p&gt; &#xA;&lt;p&gt;If you are using normal mcu without any acceleration instructions, choose arch_O0.h, otherwise choose corresponding architecture header.&lt;/p&gt; &#xA;&lt;p&gt;And you should edit tm_port.h to fill your desired configs, all config macro have annotation follow it.&lt;/p&gt; &#xA;&lt;p&gt;Note TM_MAX_CSIZE,TM_MAX_KSIZE,TM_MAX_KCSIZE will occupy static buffers.&lt;/p&gt; &#xA;&lt;p&gt;And now just put them into your project, compile it~&lt;/p&gt; &#xA;&lt;h2&gt;How to train/convert models&lt;/h2&gt; &#xA;&lt;p&gt;There are training scripts in examples/mnist to learn how to train simple mnist models.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: you need install TensorFlow (&amp;gt;=2.7) first.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;After training and save h5 models, you can use scripts in tools to convert to tmdl or c header files.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;h5_to_tflite.py&lt;br&gt; convert h5 model to float or int8 quant tflite files&lt;br&gt; python3 h5_to_tflite.py h5/mnist.h5 tflite/mnist_f.tflite 0&lt;br&gt; python3 h5_to_tflite.py h5/mnist.h5 tflite/mnist_q.tflite 1 quant_img_mnist/ 0to1&lt;/li&gt; &#xA; &lt;li&gt;tflite2tmdl.py&lt;br&gt; convert tflite file to tmdl or c header files.&lt;br&gt; python3 tflite2tmdl.py tflite/mnist_q.tflite tmdl/mnist_q.tmdl int8 1 28,28,1 10&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;================ pack model head ================&#xA;mdl_type   =0&#xA;out_deq    =1&#xA;input_cnt  =1&#xA;output_cnt =1&#xA;layer_cnt  =6&#xA;buf_size   =1464&#xA;sub_size   =0&#xA;in_dims    = [3, 28, 28, 1]&#xA;out_dims   = [1, 1, 1, 10]&#xA;================   pack layers   ================&#xA;CONV_2D&#xA;    [3, 28, 28, 1] [3, 13, 13, 4]&#xA;    in_oft:0, size:784;  out_oft:784, size:680&#xA;    padding valid&#xA;    layer_size=152&#xA;CONV_2D&#xA;    [3, 13, 13, 4] [3, 6, 6, 8]&#xA;    in_oft:784, size:680;  out_oft:0, size:288&#xA;    padding valid&#xA;    layer_size=432&#xA;CONV_2D&#xA;    [3, 6, 6, 8] [3, 2, 2, 16]&#xA;    in_oft:0, size:288;  out_oft:1400, size:64&#xA;    padding valid&#xA;    layer_size=1360&#xA;MEAN&#xA;    [3, 2, 2, 16] [1, 1, 1, 16]&#xA;    in_oft:1400, size:64;  out_oft:0, size:16&#xA;    layer_size=48&#xA;FULLY_CONNECTED&#xA;    [1, 1, 1, 16] [1, 1, 1, 10]&#xA;    in_oft:0, size:16;  out_oft:1448, size:16&#xA;    layer_size=304&#xA;SOFTMAX&#xA;    [1, 1, 1, 10] [1, 1, 1, 10]&#xA;    OUTPUT!&#xA;    in_oft:1448, size:16;  out_oft:0, size:56&#xA;    layer_size=48&#xA;================    pack done!   ================&#xA;    model  size 2.4KB (2408 B) FLASH&#xA;    buffer size 1.4KB (1464 B) RAM&#xA;    single layer mode subbuff size 1.4KB (64+1360=1424 B) RAM&#xA;Saved to tmdl/mnist_q.tmdl, tmdl/mnist_q.h&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now you have tmdl or c header files, put it into your project to use it~&lt;/p&gt; &#xA;&lt;h2&gt;How to train models online with MaixHub&lt;/h2&gt; &#xA;&lt;p&gt;TODO&lt;/p&gt; &#xA;&lt;h2&gt;How to add new platform acceleration code&lt;/h2&gt; &#xA;&lt;p&gt;TinyMaix use basic dot_product function to accelerate Conv computing.&lt;br&gt; You just need add arch_xxx_yyy.h in src dir, and implement your platform&#39;s dot_product function:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;TM_INLINE void tm_dot_prod(mtype_t* sptr, mtype_t* kptr,uint32_t size, sumtype_t* result);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Preprocess with mean/std&lt;/li&gt; &#xA; &lt;li&gt;find good backbone for 64KB/128KB/256KB/512KB ram litmit&lt;/li&gt; &#xA; &lt;li&gt;Add example: KWS,HAR,Gesture,OCR,... ...&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Contribution &amp;amp; Contacts&lt;/h2&gt; &#xA;&lt;p&gt;If you want contribute fucntions to TinyMaix, please read &#34;TinyMaix Design&#34; sections, we only want functions in &#34;Features in design&#34; and &#34;Features maybe added&#34;.&lt;/p&gt; &#xA;&lt;p&gt;If you want commit your port test result, please commit to benchmark.md. You are welcome to port TinyMaix to your chip/boards, it will prove how easy to use TinyMaix run Deeplearning model in MCUs~&lt;/p&gt; &#xA;&lt;p&gt;If you have question with TinyMaix usage/porting, please feedback Issues in this repo.&lt;/p&gt; &#xA;&lt;p&gt;If you have bussiness project consulting or private questions, you can send mail to &lt;a href=&#34;mailto:support@sipeed.com&#34;&gt;support@sipeed.com&lt;/a&gt; or &lt;a href=&#34;mailto:zepan@sipeed.com&#34;&gt;zepan@sipeed.com&lt;/a&gt; (Caesar Wu).&lt;/p&gt;</summary>
  </entry>
</feed>