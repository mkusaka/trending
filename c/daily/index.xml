<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-27T01:30:53Z</updated>
  <subtitle>Daily Trending of C in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>PotatoSpudowski/fastLLaMa</title>
    <updated>2023-03-27T01:30:53Z</updated>
    <id>tag:github.com,2023-03-27:/PotatoSpudowski/fastLLaMa</id>
    <link href="https://github.com/PotatoSpudowski/fastLLaMa" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Python wrapper to run llama.cpp ðŸ‘€&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;fastLLaMa&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Python wrapper to run Inference of &lt;a href=&#34;https://arxiv.org/abs/2302.13971&#34;&gt;LLaMA&lt;/a&gt; models using C++&lt;/p&gt; &#xA;&lt;p&gt;This repo was built on top of &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt;&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/PotatoSpudowski/fastLLaMa/main/assets/fast_llama.jpg&#34; alt=&#34;My Image&#34; width=&#34;500&#34; height=&#34;500&#34;&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Description&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;fastLLaMa&lt;/code&gt; is a Python package that provides a Pythonic interface to a C++ library, llama.cpp. It allows you to use the functionality of the C++ library from within Python, without having to write C++ code or deal with low-level C++ APIs.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Easy-to-use Python interface to the C++ library.&lt;/li&gt; &#xA; &lt;li&gt;High performance compared to the original &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;LLaMA repo&lt;/a&gt;, thanks to the C++ implementation.&lt;/li&gt; &#xA; &lt;li&gt;Ability to save and load the state of the model with system prompts.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Supported Models&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;model&lt;/th&gt; &#xA;   &lt;th&gt;model_id&lt;/th&gt; &#xA;   &lt;th&gt;status&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMa 7B&lt;/td&gt; &#xA;   &lt;td&gt;LLAMA-7B&lt;/td&gt; &#xA;   &lt;td&gt;Done&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMa 13B&lt;/td&gt; &#xA;   &lt;td&gt;LLAMA-13B&lt;/td&gt; &#xA;   &lt;td&gt;Done&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMa 30B&lt;/td&gt; &#xA;   &lt;td&gt;LLAMA-30B&lt;/td&gt; &#xA;   &lt;td&gt;Done&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMa 65B&lt;/td&gt; &#xA;   &lt;td&gt;LLAMA-65B&lt;/td&gt; &#xA;   &lt;td&gt;Done&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Alpaca-LoRA 7B&lt;/td&gt; &#xA;   &lt;td&gt;ALPACA-LORA-7B&lt;/td&gt; &#xA;   &lt;td&gt;Done&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Alpaca-LoRA 13B&lt;/td&gt; &#xA;   &lt;td&gt;ALPACA-LORA-13B&lt;/td&gt; &#xA;   &lt;td&gt;Done&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Alpaca-LoRA 30B&lt;/td&gt; &#xA;   &lt;td&gt;ALPACA-LORA-30B&lt;/td&gt; &#xA;   &lt;td&gt;Done&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Alpaca-LoRA 65B&lt;/td&gt; &#xA;   &lt;td&gt;ALPACA-LORA-65B&lt;/td&gt; &#xA;   &lt;td&gt;Pending&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;CMake&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;For Linux: &lt;br&gt; &lt;code&gt;sudo apt-get -y install cmake&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;For OS X: &lt;br&gt; &lt;code&gt;brew install cmake&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;For Windows &lt;br&gt; Download cmake-*.exe installer from &lt;a href=&#34;https://cmake.org/download/&#34;&gt;Download page&lt;/a&gt; and run it.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Minimum C++ 17&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Python 3.x&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Example&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/PotatoSpudowski/fastLLaMa&#xA;cd fast_llama&#xA;&#xA;chmod +x build.sh&#xA;&#xA;./build.sh&#xA;&#xA;# obtain the original LLaMA model weights and place them in ./models&#xA;ls ./models&#xA;65B 30B 13B 7B tokenizer_checklist.chk tokenizer.model&#xA;&#xA;# install Python dependencies&#xA;pip install -r requirements.txt&#xA;&#xA;# convert the 7B model to ggml FP16 format&#xA;# python [PythonFile] [ModelPath] [Floattype] [SplitType]&#xA;python3 convert-pth-to-ggml.py models/7B/ 1 0&#xA;&#xA;# quantize the model to 4-bits&#xA;python3 quantize.py 7B&#xA;&#xA;# run the inference&#xA;python example.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Importing fastLlama&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import sys&#xA;&#xA;sys.path.append(&#34;./build/&#34;)&#xA;&#xA;import fastLlama&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Initializing the Model&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;MODEL_PATH = &#34;./models/7B/ggml-model-q4_0.bin&#34;&#xA;&#xA;model = fastLlama.Model(&#xA;        id=MODEL_ID&#xA;        path=MODEL_PATH, #path to model&#xA;        num_threads=8, #number of threads to use&#xA;        n_ctx=512, #context size of model&#xA;        last_n_size=64, #size of last n tokens (used for repetition penalty) (Optional)&#xA;        seed=0 #seed for random number generator (Optional)&#xA;    )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Ingesting Prompts&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;prompt = &#34;&#34;&#34;Transcript of a dialog, where the User interacts with an Assistant named Bob. Bob is helpful, kind, honest, good at writing, and never fails to answer the User&#39;s requests immediately and with precision.&#xA;&#xA;User: Hello, Bob.&#xA;Bob: Hello. How may I help you today?&#xA;User: Please tell me the largest city in Europe.&#xA;Bob: Sure. The largest city in Europe is Moscow, the capital of Russia.&#xA;User: &#34;&#34;&#34;&#xA;&#xA;res = model.ingest(prompt) &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Generating Output&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def stream_token(x: str) -&amp;gt; None:&#xA;    &#34;&#34;&#34;&#xA;    This function is called by the library to stream tokens&#xA;    &#34;&#34;&#34;&#xA;    print(x, end=&#39;&#39;, flush=True)&#xA;&#xA;res = model.generate(&#xA;    num_tokens=100, &#xA;    top_p=0.95, #top p sampling (Optional)&#xA;    temp=0.8, #temperature (Optional)&#xA;    repeat_penalty=1.0, #repetition penalty (Optional)&#xA;    streaming_fn=stream_token, #streaming function&#xA;    stop_word=[&#34;User:&#34;, &#34;\n&#34;] #stop generation when this word is encountered (Optional)&#xA;    )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Saving Model State&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;res = model.save_state(&#34;./models/fast_llama.bin&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Loading Model State&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;res = model.load_state(&#34;./models/fast_llama.bin&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running Alpaca-LoRA&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install -r requirements.txt&#xA;&#xA;#Before running this command&#xA;#You need to provide the HF model paths as found in the original script &#xA;python export-alpaca-lora.py&#xA;&#xA;# python [PythonFile] [ModelPath] [Floattype] [SplitType]&#xA;# SplitType should be 1 for Alpaca-Lora models exported from HF&#xA;python3 convert-pth-to-ggml.py models/ALPACA-LORA-7B 1 1&#xA;&#xA;./quantize models/ALPACA-LORA-7B/ggml-model-f16.bin models/ALPACA-LORA-7B/alpaca-lora-q4_0.bin 2&#xA;&#xA;python example-alpaca.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Memory/Disk Requirements&lt;/h3&gt; &#xA;&lt;p&gt;As the models are currently fully loaded into memory, you will need adequate disk space to save them and sufficient RAM to load them. At the moment, memory and disk requirements are the same.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;model size&lt;/th&gt; &#xA;   &lt;th&gt;original size&lt;/th&gt; &#xA;   &lt;th&gt;quantized size (4-bit)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;13 GB&lt;/td&gt; &#xA;   &lt;td&gt;3.9 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;24 GB&lt;/td&gt; &#xA;   &lt;td&gt;7.8 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;30B&lt;/td&gt; &#xA;   &lt;td&gt;60 GB&lt;/td&gt; &#xA;   &lt;td&gt;19.5 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;65B&lt;/td&gt; &#xA;   &lt;td&gt;120 GB&lt;/td&gt; &#xA;   &lt;td&gt;38.5 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Example Dockerfile&lt;/h3&gt; &#xA;&lt;p&gt;This is a Dockerfile to build a minimal working example. Note that it does not download any models for you. It is also compatible with Alpaca models.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-dockerfile&#34;&gt;FROM ubuntu:18.04&#xA;# Add GCC and G++ New&#xA;RUN apt-get update &amp;amp;&amp;amp; apt-get install software-properties-common curl git -qy &#xA;RUN add-apt-repository ppa:ubuntu-toolchain-r/test &amp;amp;&amp;amp; apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 42D5A192B819C5DA&#xA;&#xA;# Add CMAKE New&#xA;RUN wget -O - https://apt.kitware.com/keys/kitware-archive-latest.asc 2&amp;gt;/dev/null | gpg --dearmor - | tee /etc/apt/trusted.gpg.d/kitware.gpg &amp;gt;/dev/null&#xA;RUN apt-add-repository &#34;deb https://apt.kitware.com/ubuntu/ bionic main&#34; &amp;amp;&amp;amp; apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 6AF7F09730B3F0A4&#xA;&#xA;RUN apt-get update&#xA;&#xA;# Install&#xA;RUN apt-get install -qy cmake gcc-10 g++-10&#xA;&#xA;# Configure&#xA;RUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 30 &amp;amp;&amp;amp; update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-10 30&#xA;RUN update-alternatives --install /usr/bin/cc cc /usr/bin/gcc 30 &amp;amp;&amp;amp; update-alternatives --set cc /usr/bin/gcc&#xA;RUN update-alternatives --install /usr/bin/c++ c++ /usr/bin/g++ 30 &amp;amp;&amp;amp; update-alternatives --set c++ /usr/bin/g++&#xA;&#xA;ARG DEBIAN_FRONTEND=noninteractive&#xA;ENV TZ=Etc/UTC&#xA;RUN add-apt-repository -y ppa:deadsnakes/ppa &amp;amp;&amp;amp; apt-get update \&#xA;    &amp;amp;&amp;amp; apt-get install -y python3.9-dev python3.9-distutils python3.9&#xA;&#xA;WORKDIR /app&#xA;RUN git clone https://github.com/PotatoSpudowski/fastLLaMa.git /app&#xA;RUN chmod +x build.sh &amp;amp;&amp;amp; bash ./build.sh&#xA;&#xA;RUN apt-get install -qy python3-pip&#xA;RUN python3.9 -m pip install --upgrade pip &amp;amp;&amp;amp; python3.9 -m pip install setuptools-rust&#xA;RUN python3.9 -m pip install -r requirements.txt&#xA;&#xA;CMD [&#34;python3.9&#34;, &#34;example.py&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Notes&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Tested on &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;M1 Pro Mac&lt;/li&gt; &#xA;   &lt;li&gt;Intel Mac&lt;/li&gt; &#xA;   &lt;li&gt;Ubuntu:18.04 - Python 3.9&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;The whole inspiration behind fastLLaMa is to let the community test the capabilities of LLaMA by creating custom workflows using Python.&lt;/li&gt; &#xA; &lt;li&gt;This project was possible because of &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt;, Do have a look at it as well.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>TsudaKageyu/minhook</title>
    <updated>2023-03-27T01:30:53Z</updated>
    <id>tag:github.com,2023-03-27:/TsudaKageyu/minhook</id>
    <link href="https://github.com/TsudaKageyu/minhook" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The Minimalistic x86/x64 API Hooking Library for Windows&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MinHook&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://opensource.org/licenses/BSD-2-Clause&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-BSD%202--Clause-orange.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The Minimalistic x86/x64 API Hooking Library for Windows&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://www.codeproject.com/KB/winsdk/LibMinHook.aspx&#34;&gt;http://www.codeproject.com/KB/winsdk/LibMinHook.aspx&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Version history&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;v1.3.3 - 8 Jan 2017&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Added a helper function &lt;code&gt;MH_CreateHookApiEx&lt;/code&gt;. (Thanks to asm256)&lt;/li&gt; &#xA;   &lt;li&gt;Support Visual Studio 2017 RC.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;v1.3.2.1 - 9 Nov 2015&lt;/strong&gt; (Nuget package only)&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Fixed an insufficient support for Visual Studio 2015.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;v1.3.2 - 1 Nov 2015&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Support Visual Studio 2015.&lt;/li&gt; &#xA;   &lt;li&gt;Support MinGW.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;v1.3.2-beta3 - 21 Jul 2015&lt;/strong&gt; (Nuget package only)&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Support MinGW. (Experimental)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;v1.3.2-beta2 - 18 May 2015&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Fixed some subtle bugs. (Thanks to RaMMicHaeL)&lt;/li&gt; &#xA;   &lt;li&gt;Added a helper function &lt;code&gt;MH_StatusToString&lt;/code&gt;. (Thanks to Jan Klass)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;v1.3.2-beta - 12 May 2015&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Fixed a possible thread deadlock in x64 mode. (Thanks to Aleh Kazakevich)&lt;/li&gt; &#xA;   &lt;li&gt;Reduced the footprint a little more.&lt;/li&gt; &#xA;   &lt;li&gt;Support Visual Studio 2015 RC. (Experimental)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;v1.3.1.1 - 7 Apr 2015&lt;/strong&gt; (Nuget package only)&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Support for WDK8.0 and 8.1.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;v1.3.1 - 19 Mar 2015&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;No major changes from v1.3.1-beta.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;v1.3.1-beta - 11 Mar 2015&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Added a helper function &lt;code&gt;MH_CreateHookApi&lt;/code&gt;. (Thanks to uniskz).&lt;/li&gt; &#xA;   &lt;li&gt;Fixed a false memory leak reported by some tools.&lt;/li&gt; &#xA;   &lt;li&gt;Fixed a degradated compatibility issue.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;v1.3 - 13 Sep 2014&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;No major changes from v1.3-beta3.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;v1.3-beta3 - 31 Jul 2014&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Fixed some small bugs.&lt;/li&gt; &#xA;   &lt;li&gt;Improved the memory management.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;v1.3-beta2 - 21 Jul 2014&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Changed the parameters to Windows-friendly types. (void* to LPVOID)&lt;/li&gt; &#xA;   &lt;li&gt;Fixed some small bugs.&lt;/li&gt; &#xA;   &lt;li&gt;Reorganized the source files.&lt;/li&gt; &#xA;   &lt;li&gt;Reduced the footprint a little more.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;v1.3-beta - 17 Jul 2014&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Rewrote in plain C to reduce the footprint and memory usage. (suggested by Andrey Unis)&lt;/li&gt; &#xA;   &lt;li&gt;Simplified the overall code base to make it more readable and maintainable.&lt;/li&gt; &#xA;   &lt;li&gt;Changed the license from 3-clause to 2-clause BSD License.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;v1.2 - 28 Sep 2013&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Removed boost dependency (&lt;a href=&#34;https://github.com/jarredholman/minhook&#34;&gt;jarredholman&lt;/a&gt;).&lt;/li&gt; &#xA;   &lt;li&gt;Fixed a small bug in the GetRelativeBranchDestination function (&lt;a href=&#34;http://www.codeproject.com/Messages/4058892/Small-Bug-Found.aspx&#34;&gt;pillbug99&lt;/a&gt;).&lt;/li&gt; &#xA;   &lt;li&gt;Added the &lt;code&gt;MH_RemoveHook&lt;/code&gt; function, which removes a hook created with the &lt;code&gt;MH_CreateHook&lt;/code&gt; function.&lt;/li&gt; &#xA;   &lt;li&gt;Added the following functions to enable or disable multiple hooks in one go: &lt;code&gt;MH_QueueEnableHook&lt;/code&gt;, &lt;code&gt;MH_QueueDisableHook&lt;/code&gt;, &lt;code&gt;MH_ApplyQueued&lt;/code&gt;. This is the preferred way of handling multiple hooks as every call to &lt;code&gt;MH_EnableHook&lt;/code&gt; or &lt;code&gt;MH_DisableHook&lt;/code&gt; suspends and resumes all threads.&lt;/li&gt; &#xA;   &lt;li&gt;Made the functions &lt;code&gt;MH_EnableHook&lt;/code&gt; and &lt;code&gt;MH_DisableHook&lt;/code&gt; enable/disable all created hooks when the &lt;code&gt;MH_ALL_HOOKS&lt;/code&gt; parameter is passed. This, too, is an efficient way of handling multiple hooks.&lt;/li&gt; &#xA;   &lt;li&gt;If the target function is too small to be patched with a jump, MinHook tries to place the jump above the function. If that fails as well, the &lt;code&gt;MH_CreateHook&lt;/code&gt; function returns &lt;code&gt;MH_ERROR_UNSUPPORTED_FUNCTION&lt;/code&gt;. This fixes an issue of hooking the LoadLibraryExW function on Windows 7 x64 (&lt;a href=&#34;http://www.codeproject.com/Messages/4578613/Re-Bug-LoadLibraryExW-hook-fails-on-windows-2008-r.aspx&#34;&gt;reported by Obble&lt;/a&gt;).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;v1.1 - 26 Nov 2009&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Changed the interface to create a hook and a trampoline function in one go to prevent the detour function from being called before the trampoline function is created. (&lt;a href=&#34;http://www.codeproject.com/Messages/3280374/Unsafe.aspx&#34;&gt;reported by xliqz&lt;/a&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;Shortened the function names from &lt;code&gt;MinHook_*&lt;/code&gt; to &lt;code&gt;MH_*&lt;/code&gt; to make them handier.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;v1.0 - 22 Nov 2009&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Initial release.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Building MinHook - Using vcpkg&lt;/h3&gt; &#xA;&lt;p&gt;You can download and install MinHook using the &lt;a href=&#34;https://github.com/Microsoft/vcpkg&#34;&gt;vcpkg&lt;/a&gt; dependency manager:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/microsoft/vcpkg&#xA;.\vcpkg\bootstrap-vcpkg.bat&#xA;.\vcpkg\vcpkg integrate install&#xA;.\vcpkg\vcpkg install minhook&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The MinHook port in vcpkg is kept up to date by Microsoft team members and community contributors. If the version is out of date, please &lt;a href=&#34;https://github.com/Microsoft/vcpkg&#34;&gt;create an issue or pull request&lt;/a&gt; on the vcpkg repository.&lt;/p&gt;</summary>
  </entry>
</feed>