<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-10-11T01:31:54Z</updated>
  <subtitle>Daily Trending of C in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ggerganov/whisper.cpp</title>
    <updated>2022-10-11T01:31:54Z</updated>
    <id>tag:github.com,2022-10-11:/ggerganov/whisper.cpp</id>
    <link href="https://github.com/ggerganov/whisper.cpp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Port of OpenAI&#39;s Whisper model in C/C++&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;whisper.cpp&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ggerganov/whisper.cpp/actions&#34;&gt;&lt;img src=&#34;https://github.com/ggerganov/whisper.cpp/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;Actions Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;High-performance inference of &lt;a href=&#34;https://github.com/openai/whisper&#34;&gt;OpenAI&#39;s Whisper&lt;/a&gt; automatic speech recognition (ASR) model:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Plain C/C++ implementation without dependencies&lt;/li&gt; &#xA; &lt;li&gt;ARM_NEON and AVX intrinsics support&lt;/li&gt; &#xA; &lt;li&gt;Mixed F16 / F32 precision&lt;/li&gt; &#xA; &lt;li&gt;Low memory usage (Flash Attention + Flash Forward)&lt;/li&gt; &#xA; &lt;li&gt;Zero memory allocations at runtime&lt;/li&gt; &#xA; &lt;li&gt;Runs on the CPU&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ggerganov/whisper.cpp/raw/master/whisper.h&#34;&gt;C-style API&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Supported platforms: Linux, Mac OS (Intel and Arm), Windows (MinGW), Raspberry Pi, Android&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;To build the main program, run &lt;code&gt;make&lt;/code&gt;. You can then transcribe a &lt;code&gt;.wav&lt;/code&gt; file like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ ./main -f input.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Before running the program, make sure to download one of the ggml Whisper models. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash ./download-ggml-model.sh base.en&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;For a quick demo, simply run &lt;code&gt;make base.en&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;$ make base.en&#xA;cc  -O3 -std=c11   -Wall -Wextra -Wno-unused-parameter -Wno-unused-function -pthread -c ggml.c&#xA;c++ -O3 -std=c++11 -Wall -Wextra -Wno-unused-parameter -Wno-unused-function -pthread -c whisper.cpp&#xA;c++ -O3 -std=c++11 -Wall -Wextra -Wno-unused-parameter -Wno-unused-function -pthread main.cpp whisper.o ggml.o -o main&#xA;./main -h&#xA;&#xA;usage: ./main [options] file0.wav file1.wav ...&#xA;&#xA;options:&#xA;  -h,       --help           show this help message and exit&#xA;  -s SEED,  --seed SEED      RNG seed (default: -1)&#xA;  -t N,     --threads N      number of threads to use during computation (default: 4)&#xA;  -v,       --verbose        verbose output&#xA;            --translate      translate from source language to english&#xA;  -ps,      --print_special  print special tokens&#xA;  -nt,      --no_timestamps  do not print timestamps&#xA;  -l LANG,  --language LANG  spoken language (default: en)&#xA;  -m FNAME, --model FNAME    model path (default: models/ggml-base.en.bin)&#xA;  -f FNAME, --file FNAME     input WAV file path&#xA;&#xA;bash ./download-ggml-model.sh base.en&#xA;Downloading ggml model base.en ...&#xA;models/ggml-base.en.bin            100%[===================================&amp;gt;] 141.11M  6.49MB/s    in 23s&#xA;Done! Model &#39;base.en&#39; saved in &#39;models/ggml-base.en.bin&#39;&#xA;You can now use it like this:&#xA;&#xA;  $ ./main -m models/ggml-base.en.bin -f samples/jfk.wav&#xA;&#xA;&#xA;===============================================&#xA;Running base.en on all samples in ./samples ...&#xA;===============================================&#xA;&#xA;----------------------------------------------&#xA;[+] Running base.en on samples/jfk.wav ... (run &#39;ffplay samples/jfk.wav&#39; to listen)&#xA;----------------------------------------------&#xA;&#xA;whisper_model_load: loading model from &#39;models/ggml-base.en.bin&#39;&#xA;whisper_model_load: n_vocab       = 51864&#xA;whisper_model_load: n_audio_ctx   = 1500&#xA;whisper_model_load: n_audio_state = 512&#xA;whisper_model_load: n_audio_head  = 8&#xA;whisper_model_load: n_audio_layer = 6&#xA;whisper_model_load: n_text_ctx    = 448&#xA;whisper_model_load: n_text_state  = 512&#xA;whisper_model_load: n_text_head   = 8&#xA;whisper_model_load: n_text_layer  = 6&#xA;whisper_model_load: n_mels        = 80&#xA;whisper_model_load: f16           = 1&#xA;whisper_model_load: type          = 2&#xA;whisper_model_load: mem_required  = 377.00 MB&#xA;whisper_model_load: adding 1607 extra tokens&#xA;whisper_model_load: ggml ctx size = 163.43 MB&#xA;whisper_model_load: memory size =    22.83 MB&#xA;whisper_model_load: model size  =   140.54 MB&#xA;&#xA;main: processing &#39;samples/jfk.wav&#39; (176000 samples, 11.0 sec), 4 threads, lang = en, task = transcribe, timestamps = 1 ...&#xA;&#xA;[00:00.000 --&amp;gt; 00:11.000]   And so my fellow Americans, ask not what your country can do for you, ask what you can do for your country.&#xA;&#xA;&#xA;whisper_print_timings:     load time =    77.48 ms&#xA;whisper_print_timings:      mel time =    26.10 ms&#xA;whisper_print_timings:   sample time =     2.19 ms&#xA;whisper_print_timings:   encode time =   632.95 ms / 105.49 ms per layer&#xA;whisper_print_timings:   decode time =    85.11 ms / 14.18 ms per layer&#xA;whisper_print_timings:    total time =   824.14 ms&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The command downloads the &lt;code&gt;base.en&lt;/code&gt; model converted to custom &lt;code&gt;ggml&lt;/code&gt; format and runs the inference on all &lt;code&gt;.wav&lt;/code&gt; samples in the folder &lt;code&gt;samples&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For detailed usage instructions, run: &lt;code&gt;./main -h&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Note that &lt;code&gt;whisper.cpp&lt;/code&gt; currently runs only with 16-bit WAV files, so make sure to convert your input before running the tool. For example, you can use &lt;code&gt;ffmpeg&lt;/code&gt; like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;ffmpeg -i input.mp3 -ar 16000 -ac 1 -c:a pcm_s16le output.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;More audio samples&lt;/h2&gt; &#xA;&lt;p&gt;If you want some extra audio samples to play with, simply run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make samples&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will download a few more audio files from Wikipedia and convert them to 16-bit WAV format via &lt;code&gt;ffmpeg&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can download and run the other models as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make tiny.en&#xA;make tiny&#xA;make base.en&#xA;make base&#xA;make small.en&#xA;make small&#xA;make medium.en&#xA;make medium&#xA;make large&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Another example&lt;/h2&gt; &#xA;&lt;p&gt;Here is another example of transcribing a &lt;a href=&#34;https://upload.wikimedia.org/wikipedia/commons/1/1f/George_W_Bush_Columbia_FINAL.ogg&#34;&gt;3:24 min speech&lt;/a&gt; in less than a minute on a MacBook M1 Pro, using &lt;code&gt;medium.en&lt;/code&gt; model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;$ ./main -m models/ggml-medium.en.bin -f samples/gb1.wav -t 8&#xA;&#xA;whisper_model_load: loading model from &#39;models/ggml-medium.en.bin&#39;&#xA;whisper_model_load: n_vocab       = 51864&#xA;whisper_model_load: n_audio_ctx   = 1500&#xA;whisper_model_load: n_audio_state = 1024&#xA;whisper_model_load: n_audio_head  = 16&#xA;whisper_model_load: n_audio_layer = 24&#xA;whisper_model_load: n_text_ctx    = 448&#xA;whisper_model_load: n_text_state  = 1024&#xA;whisper_model_load: n_text_head   = 16&#xA;whisper_model_load: n_text_layer  = 24&#xA;whisper_model_load: n_mels        = 80&#xA;whisper_model_load: f16           = 1&#xA;whisper_model_load: type          = 4&#xA;whisper_model_load: mem_required  = 2502.00 MB&#xA;whisper_model_load: adding 1607 extra tokens&#xA;whisper_model_load: ggml ctx size = 1644.97 MB&#xA;whisper_model_load: memory size =   182.62 MB&#xA;whisper_model_load: model size  =  1462.12 MB&#xA;log_mel_spectrogram: n_sample = 3179750, n_len = 19873&#xA;log_mel_spectrogram: recording length: 198.734375 s&#xA;&#xA;main: processing 3179750 samples (198.7 sec), 8 threads, lang = english, task = transcribe, timestamps = 1 ...&#xA;&#xA;[00:00.000 --&amp;gt; 00:08.000]   My fellow Americans, this day has brought terrible news and great sadness to our country.&#xA;[00:08.000 --&amp;gt; 00:17.000]   At 9 o&#39;clock this morning, Mission Control in Houston lost contact with our Space Shuttle Columbia.&#xA;[00:17.000 --&amp;gt; 00:24.000]   A short time later, debris was seen falling from the skies above Texas.&#xA;[00:24.000 --&amp;gt; 00:29.000]   The Columbia&#39;s lost. There are no survivors.&#xA;[00:29.000 --&amp;gt; 00:32.000]   On board was a crew of seven.&#xA;[00:32.000 --&amp;gt; 00:43.000]   Colonel Rick Husband, Lieutenant Colonel Michael Anderson, Commander Laurel Clark, Captain David Brown, Commander William McCool,&#xA;[00:43.000 --&amp;gt; 00:52.000]   Dr. Kultner Aschavla, and Elon Ramon, a Colonel in the Israeli Air Force.&#xA;[00:52.000 --&amp;gt; 00:58.000]   These men and women assumed great risk in the service to all humanity.&#xA;[00:58.000 --&amp;gt; 01:06.000]   In an age when space flight has come to seem almost routine, it is easy to overlook the dangers of travel by rocket&#xA;[01:06.000 --&amp;gt; 01:12.000]   and the difficulties of navigating the fierce outer atmosphere of the Earth.&#xA;[01:12.000 --&amp;gt; 01:22.000]   These astronauts knew the dangers, and they faced them willingly, knowing they had a high and noble purpose in life.&#xA;[01:22.000 --&amp;gt; 01:30.000]   Because of their courage, endearing, and idealism, we will miss them all the more.&#xA;[01:30.000 --&amp;gt; 01:40.000]   All Americans today are thinking as well of the families of these men and women who have been given this sudden shock and grief.&#xA;[01:40.000 --&amp;gt; 01:45.000]   You&#39;re not alone. Our entire nation agrees with you.&#xA;[01:45.000 --&amp;gt; 01:52.000]   And those you love will always have the respect and gratitude of this country.&#xA;[01:52.000 --&amp;gt; 01:56.000]   The cause in which they died will continue.&#xA;[01:56.000 --&amp;gt; 02:07.000]   Mankind is led into the darkness beyond our world by the inspiration of discovery and the longing to understand.&#xA;[02:07.000 --&amp;gt; 02:11.000]   Our journey into space will go on.&#xA;[02:11.000 --&amp;gt; 02:16.000]   In the skies today, we saw destruction and tragedy.&#xA;[02:16.000 --&amp;gt; 02:22.000]   Yet farther than we can see, there is comfort and hope.&#xA;[02:22.000 --&amp;gt; 02:31.000]   In the words of the prophet Isaiah, &#34;Lift your eyes and look to the heavens who created all these.&#xA;[02:31.000 --&amp;gt; 02:39.000]   He who brings out the starry hosts one by one and calls them each by name.&#34;&#xA;[02:39.000 --&amp;gt; 02:46.000]   Because of his great power and mighty strength, not one of them is missing.&#xA;[02:46.000 --&amp;gt; 02:55.000]   The same creator who names the stars also knows the names of the seven souls we mourn today.&#xA;[02:55.000 --&amp;gt; 03:05.000]   The crew of the shuttle Columbia did not return safely to Earth, yet we can pray that all are safely home.&#xA;[03:05.000 --&amp;gt; 03:14.000]   May God bless the grieving families and may God continue to bless America.&#xA;[03:14.000 --&amp;gt; 03:24.000]   [Music]&#xA;&#xA;&#xA;main:     load time =   522.18 ms&#xA;main:      mel time =   423.43 ms&#xA;main:   sample time =    31.42 ms&#xA;main:   encode time = 41518.51 ms / 1729.94 ms per layer&#xA;main:   decode time = 14907.22 ms&#xA;main:    total time = 57416.63 ms&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Real-time audio input example&lt;/h2&gt; &#xA;&lt;p&gt;This is a naive example of performing real-time inference on audio from your microphone. The &lt;code&gt;stream&lt;/code&gt; tool samples the audio every half a second and runs the transcription continously. More info is available in &lt;a href=&#34;https://github.com/ggerganov/whisper.cpp/issues/10&#34;&gt;issue #10&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;$ ./stream -m ./models/ggml-base.en.bin -t 8 --step 500 --length 5000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/1991296/194935793-76afede7-cfa8-48d8-a80f-28ba83be7d09.mp4&#34;&gt;https://user-images.githubusercontent.com/1991296/194935793-76afede7-cfa8-48d8-a80f-28ba83be7d09.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Implementation details&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The core tensor operations are implemented in C (&lt;a href=&#34;https://raw.githubusercontent.com/ggerganov/whisper.cpp/master/ggml.h&#34;&gt;ggml.h&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/ggerganov/whisper.cpp/master/ggml.c&#34;&gt;ggml.c&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;The high-level C-style API is implemented in C++ (&lt;a href=&#34;https://raw.githubusercontent.com/ggerganov/whisper.cpp/master/whisper.h&#34;&gt;whisper.h&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/ggerganov/whisper.cpp/master/whisper.cpp&#34;&gt;whisper.cpp&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Simple usage is demonstrated in &lt;a href=&#34;https://raw.githubusercontent.com/ggerganov/whisper.cpp/master/main.cpp&#34;&gt;main.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Sample real-time audio transcription from the microphone is demonstrated in &lt;a href=&#34;https://raw.githubusercontent.com/ggerganov/whisper.cpp/master/stream.cpp&#34;&gt;stream.cpp&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Limitations&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Very basic greedy sampling scheme - always pick up the top token. You can implement your own strategy&lt;/li&gt; &#xA; &lt;li&gt;Inference only&lt;/li&gt; &#xA; &lt;li&gt;No GPU support&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Memory usage&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Disk&lt;/th&gt; &#xA;   &lt;th&gt;Mem&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;tiny&lt;/td&gt; &#xA;   &lt;td&gt;75 MB&lt;/td&gt; &#xA;   &lt;td&gt;~240 MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;base&lt;/td&gt; &#xA;   &lt;td&gt;142 MB&lt;/td&gt; &#xA;   &lt;td&gt;~380 MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;small&lt;/td&gt; &#xA;   &lt;td&gt;466 MB&lt;/td&gt; &#xA;   &lt;td&gt;~970 MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;medium&lt;/td&gt; &#xA;   &lt;td&gt;1.5 GB&lt;/td&gt; &#xA;   &lt;td&gt;~2.5 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;large&lt;/td&gt; &#xA;   &lt;td&gt;2.9 GB&lt;/td&gt; &#xA;   &lt;td&gt;~4.6 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;ggml format&lt;/h2&gt; &#xA;&lt;p&gt;The original models are converted to a custom binary format. This allows to pack everything needed into a single file:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;model parameters&lt;/li&gt; &#xA; &lt;li&gt;mel filters&lt;/li&gt; &#xA; &lt;li&gt;vocabulary&lt;/li&gt; &#xA; &lt;li&gt;weights&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can download the converted models using the &lt;a href=&#34;https://raw.githubusercontent.com/ggerganov/whisper.cpp/master/download-ggml-model.sh&#34;&gt;download-ggml-model.sh&lt;/a&gt; script or from here:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ggml.ggerganov.com&#34;&gt;https://ggml.ggerganov.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;For more details, see the conversion script &lt;a href=&#34;https://raw.githubusercontent.com/ggerganov/whisper.cpp/master/convert-pt-to-ggml.py&#34;&gt;convert-pt-to-ggml.py&lt;/a&gt; or the README in &lt;a href=&#34;https://raw.githubusercontent.com/ggerganov/whisper.cpp/master/models&#34;&gt;models&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Bindings&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Rust: &lt;a href=&#34;https://github.com/tazz4843/whisper-rs&#34;&gt;tazz4843/whisper-rs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Python:&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Obj-C:&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Java:&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>