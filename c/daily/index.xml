<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-03T01:30:40Z</updated>
  <subtitle>Daily Trending of C in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>RahulSChand/llama2.c-for-dummies</title>
    <updated>2023-08-03T01:30:40Z</updated>
    <id>tag:github.com,2023-08-03:/RahulSChand/llama2.c-for-dummies</id>
    <link href="https://github.com/RahulSChand/llama2.c-for-dummies" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Step by step explanation/tutorial of llama2.c&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;llama2.c for Dummies&lt;/h1&gt; &#xA;&lt;h3&gt;Purpose&lt;/h3&gt; &#xA;&lt;p&gt;This repo is line by line walk through of the inference file in &lt;a href=&#34;http://github.com/karpathy/llama2.c&#34;&gt;llama2.c&lt;/a&gt;. Its very verbose &amp;amp; intended for beginners.&lt;/p&gt; &#xA;&lt;p&gt;You will need some familiarity with transformers architecture. If you are a complete novice refer to this excellent &lt;a href=&#34;https://jalammar.github.io/illustrated-transformer/&#34;&gt;blog&lt;/a&gt; first.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Transformer architecture: 3 components &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;Embedding (1 matmul)&lt;/li&gt; &#xA;   &lt;li&gt;Layers: matmul with Q, K , V, O and feed foward weights: W1, W2 &amp;amp; W3. (7 matmul)&lt;/li&gt; &#xA;   &lt;li&gt;Classifier: In our case the classifier is just matmul of &lt;code&gt;(vocab,768) x (768,1)&lt;/code&gt; . Basically giving us what is the probability of each next token. (1 matmul)&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/RahulSChand/llama2.c-for-dummies/main/imgs/arch.png&#34; width=&#34;400&#34; height=&#34;400&#34;&gt; &#xA;&lt;h2&gt;Code walkthrough&lt;/h2&gt; &#xA;&lt;p&gt;Code has 3 parts, structs, functions &amp;amp; read logic in &lt;code&gt;main()&lt;/code&gt; we will take a look at structs first, then go to main() and then cover the important functions.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PS: The code was taken from commit 4e23ad83. The original repo might be different as it gets newer commits.&lt;/strong&gt; But 99% of the logic should remain the same :)&lt;/p&gt; &#xA;&lt;h3&gt;Part 1: Structs&lt;/h3&gt; &#xA;&lt;p&gt;We define 3 structs for storing model config, model weights &amp;amp; to store intermediate values (run state) during forward pass&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Config struct&lt;/strong&gt;: Defines the transformer model. &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;code&gt;n_layers&lt;/code&gt; , &lt;code&gt;vocab_size&lt;/code&gt; : Self explanatory&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;dim&lt;/code&gt; and &lt;code&gt;hidden_dim&lt;/code&gt; : Define shape of Q, K, V &amp;amp; O and W1, W2 &amp;amp; W3 params.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;n_heads&lt;/code&gt; : Number of heads for query(Q). If &lt;code&gt;n_heads=12&lt;/code&gt; then matrix &lt;code&gt;Q=(768,768)&lt;/code&gt; behaves/viewed as &lt;code&gt;(12,768/12,768)&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;n_kv_heads&lt;/code&gt; : Number of heads for K &amp;amp; V. &lt;strong&gt;Why are these different from above?&lt;/strong&gt; : Read &lt;a href=&#34;https://arxiv.org/pdf/1911.02150.pdf&#34;&gt;multi query paper&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;seq_len&lt;/code&gt; : No. of tokens we will generate&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;typedef struct {&#xA;    int dim; // transformer dimension&#xA;    int hidden_dim; // for ffn layers&#xA;    int n_layers; // number of layers&#xA;    int n_heads; // number of query heads&#xA;    int n_kv_heads; // number of key/value heads (can be &amp;lt; query heads because of multiquery)&#xA;    int vocab_size; // vocabulary size, usually 256 (byte-level)&#xA;    int seq_len; // max sequence length&#xA;} Config;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Weight struct&lt;/strong&gt; for llama. This is our pytorch &lt;code&gt;ffn=nn.Linear(...)&lt;/code&gt; counterpart. &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;Why are they &lt;code&gt;float*&lt;/code&gt; ? Because all matrices are just 1d flattened array. See below diagram&lt;/li&gt; &#xA;   &lt;li&gt;code is self explanatory with shapes commented. &lt;code&gt;rms_&lt;/code&gt; are weights used for normalization &amp;amp; &lt;code&gt;freq_cis_&lt;/code&gt; are for &lt;a href=&#34;https://arxiv.org/pdf/2104.09864.pdf&#34;&gt;RoPE embedding&lt;/a&gt;. We will look at &lt;code&gt;RoPE&lt;/code&gt; in detail ahead.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;wcls&lt;/code&gt; is the final classifier. Matrix of size &lt;code&gt;(vocab, dim)&lt;/code&gt; that maps final embedding from a vector to probability for each token in vocab.&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;typedef struct {&#xA;    // token embedding table&#xA;    float* token_embedding_table;    // (vocab_size, dim)&#xA;    // weights for rmsnorms&#xA;    float* rms_att_weight; // (layer, dim) rmsnorm weights&#xA;    float* rms_ffn_weight; // (layer, dim)&#xA;    // weights for matmuls&#xA;    float* wq; // (layer, dim, dim)&#xA;    float* wk; // (layer, dim, dim)&#xA;    float* wv; // (layer, dim, dim)&#xA;    float* wo; // (layer, dim, dim)&#xA;    // weights for ffn&#xA;    float* w1; // (layer, hidden_dim, dim)&#xA;    float* w2; // (layer, dim, hidden_dim)&#xA;    float* w3; // (layer, hidden_dim, dim)&#xA;    // final rmsnorm&#xA;    float* rms_final_weight; // (dim,)&#xA;    // freq_cis for RoPE relatively positional embeddings&#xA;    float* freq_cis_real; // (seq_len, dim/2)&#xA;    float* freq_cis_imag; // (seq_len, dim/2)&#xA;    // (optional) classifier weights for the logits, on the last layer&#xA;    float* wcls;&#xA;} TransformerWeights;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/RahulSChand/llama2.c-for-dummies/main/imgs/arr.png&#34; width=&#34;700&#34; height=&#34;200&#34;&gt; &#xA;&lt;hr&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Intermediate activations (Run state) &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;During forward pass we need to store intermediate values, e.g. output of matmul or output after norm. Will take a look at all variables later&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;key_cahce&lt;/code&gt; and &lt;code&gt;value_cache&lt;/code&gt; store the key, value outputs of previous tokens. e.g. during inference if the 5th token is being generated, this will store &lt;code&gt;key&lt;/code&gt;, &lt;code&gt;value&lt;/code&gt; of previous 4.&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;typedef struct {&#xA;    // current wave of activations&#xA;    float *x; // activation at current time stamp (dim,)&#xA;    float *xb; // same, but inside a residual branch (dim,)&#xA;    float *xb2; // an additional buffer just for convenience (dim,)&#xA;    float *hb; // buffer for hidden dimension in the ffn (hidden_dim,)&#xA;    float *hb2; // buffer for hidden dimension in the ffn (hidden_dim,)&#xA;    float *q; // query (dim,)&#xA;    float *k; // key (dim,)&#xA;    float *v; // value (dim,)&#xA;    float *att; // buffer for scores/attention values (n_heads, seq_len)&#xA;    float *logits; // output logits&#xA;    // kv cache&#xA;    float* key_cache;   // (layer, seq_len, dim)&#xA;    float* value_cache; // (layer, seq_len, dim)&#xA;} RunState;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;We will take a look at functions as we encounter them. For now lets see the logic inside &lt;code&gt;main()&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Part 2: Main (Can skip this part if you are only interested in &lt;a href=&#34;https://raw.githubusercontent.com/RahulSChand/llama2.c-for-dummies/main/#actual-forward-pass&#34;&gt;forward logic&lt;/a&gt; )&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Get command line arguments. Nothing interesting. Currently you can call &lt;code&gt;run.c&lt;/code&gt; with&lt;/p&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;code&gt;./run llama2_7b.bin&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;./run llama2_7b.bin 0.1&lt;/code&gt; -&amp;gt; with temperature&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;./run llama2_7b.bin 0.1 100&lt;/code&gt; -&amp;gt; with temperature &amp;amp; steps (no. of output tokens generated)&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Declare &lt;code&gt;config&lt;/code&gt; &amp;amp; &lt;code&gt;weights&lt;/code&gt; in the end&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;int main(int argc, char *argv[]) {&#xA;    // poor man&#39;s C argparse&#xA;    char *checkpoint = NULL;  // e.g. out/model.bin&#xA;    float temperature = 0.9f; // e.g. 1.0, or 0.0&#xA;    int steps = 256;          // max number of steps to run for, 0: use seq_len&#xA;    // &#39;checkpoint&#39; is necessary arg&#xA;    if (argc &amp;lt; 2) {&#xA;        printf(&#34;Usage: %s &amp;lt;checkpoint_file&amp;gt; [temperature] [steps]\n&#34;, argv[0]);&#xA;        return 1;&#xA;    }&#xA;    if (argc &amp;gt;= 2) {&#xA;        checkpoint = argv[1];&#xA;    }&#xA;    if (argc &amp;gt;= 3) {&#xA;        // optional temperature. 0.0 = (deterministic) argmax sampling. 1.0 = baseline&#xA;        temperature = atof(argv[2]);&#xA;    }&#xA;    if (argc &amp;gt;= 4) {&#xA;        steps = atoi(argv[3]);&#xA;    }&#xA;&#x9;// seed rng with time. if you want deterministic behavior use temperature 0.0&#xA;    srand((unsigned int)time(NULL)); &#xA;    // read in the model.bin file&#xA;    Config config;&#xA;    TransformerWeights weights;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;Reading &lt;code&gt;checkpoint&lt;/code&gt; file.&lt;/p&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;If you are familiar with PyTorch. Usually &lt;code&gt;config.json&lt;/code&gt; &amp;amp; &lt;code&gt;model.bin&lt;/code&gt; are separate (we load weights like a dictionary). But here &lt;code&gt;train.py&lt;/code&gt; saves everything in one &lt;code&gt;.bin&lt;/code&gt; file in a specific format. This specific format allows us to easily read config &amp;amp; then each weight one by one.&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;p&gt;Details&lt;/p&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;code&gt;shared_weights&lt;/code&gt; : Should input embedding matrix &amp;amp; output classifier matrix be same?&lt;/li&gt; &#xA;   &lt;li&gt;Next load into &lt;code&gt;weights&lt;/code&gt;. Get file size via &lt;code&gt;file_size = ftell(file);&lt;/code&gt; Unlike vanilla PyTorch inference we &lt;strong&gt;don&#39;t&lt;/strong&gt; load all weights into RAM. Instead we call &lt;code&gt;mmap(..)&lt;/code&gt; to allocate RAM memory when we want lazily. For more detail &lt;a href=&#34;https://stackoverflow.com/questions/5877797/how-does-mmap-work&#34;&gt;read&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Finally call &lt;code&gt;checkpoint_init_weights&lt;/code&gt; (snippet of function below). Here we map our weight pointers to correct address returned by &lt;code&gt;mmap&lt;/code&gt;. Since we already read config we offset for it in line &lt;code&gt;float* weights_ptr = data + sizeof(Config)/sizeof(float);&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;void checkpoint_init_weights(TransformerWeights *w, Config* p, float* f, int shared_weights){&#xA;float* ptr = f;&#xA;w-&amp;gt;token_embedding_table = ptr;&#xA;ptr += p-&amp;gt;vocab_size * p-&amp;gt;dim;&#xA;w-&amp;gt;rms_att_weight = ptr;&#xA;.......&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Original code we are talking about in above section&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;    int fd = 0;&#xA;    float* data = NULL;&#xA;    long file_size;&#xA;    {&#xA;        FILE *file = fopen(checkpoint, &#34;rb&#34;);&#xA;        if (!file) {&#xA;            printf(&#34;Unable to open the checkpoint file %s!\n&#34;, checkpoint);&#xA;            return 1;&#xA;        } &#xA;&#x9;    // read in the config header&#xA;        if(fread(&amp;amp;config, sizeof(Config), 1, file) != 1) { return 1; }&#xA;        // negative vocab size is hacky way of signaling unshared weights. bit yikes.&#xA;        int shared_weights = config.vocab_size &amp;gt; 0 ? 1 : 0;&#xA;        config.vocab_size = abs(config.vocab_size);&#xA;        // figure out the file size&#xA;        fseek(file, 0, SEEK_END); // move file pointer to end of file&#xA;        file_size = ftell(file); // get the file size, in bytes&#xA;        fclose(file);&#xA;        &#xA;        // memory map the Transformer weights into the data pointer&#xA;        fd = open(checkpoint, O_RDONLY); // open in read only mode&#xA;        if (fd == -1) { printf(&#34;open failed!\n&#34;); return 1; }&#xA;        data = mmap(NULL, file_size, PROT_READ, MAP_PRIVATE, fd, 0);&#xA;        if (data == MAP_FAILED) { printf(&#34;mmap failed!\n&#34;); return 1; }&#xA;        float* weights_ptr = data + sizeof(Config)/sizeof(float);&#xA;        checkpoint_init_weights(&amp;amp;weights, &amp;amp;config, weights_ptr, shared_weights);&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Reading vocab file -&amp;gt; Mostly straightforward, only few details &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;code&gt;vocab&lt;/code&gt; is &lt;code&gt;char**&lt;/code&gt; since each token is a string &amp;amp; &lt;code&gt;vocab&lt;/code&gt; is a list of tokens.&lt;/li&gt; &#xA;   &lt;li&gt;For loop over &lt;code&gt;vocab_size&lt;/code&gt; &amp;amp; read each token&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;// right now we cannot run for more than config.seq_len steps&#xA;    if (steps &amp;lt;= 0 || steps &amp;gt; config.seq_len) { steps = config.seq_len; }&#xA;    // read in the tokenizer.bin file&#xA;    char** vocab = (char**)malloc(config.vocab_size * sizeof(char*));&#xA;    {&#xA;        FILE *file = fopen(&#34;tokenizer.bin&#34;, &#34;rb&#34;);&#xA;        if (!file) {&#xA;            printf(&#34;Unable to open the tokenizer file tokenizer.bin! Run &#34;&#xA;            &#34;python tokenizer.py to convert tokenizer.model -&amp;gt; tokenizer.bin\n&#34;);&#xA;            return 1;&#xA;        }&#xA;        int len;&#xA;        for (int i = 0; i &amp;lt; config.vocab_size; i++) {&#xA;            if(fread(&amp;amp;len, sizeof(int), 1, file) != 1) { return 1; }&#xA;            vocab[i] = (char *)malloc(len + 1);&#xA;            if(fread(vocab[i], len, 1, file) != 1) { return 1; }&#xA;            vocab[i][len] = &#39;\0&#39;; // add the string terminating token&#xA;        }&#xA;        fclose(file);&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;Forward Loop &amp;amp; sampling in main (Go to &lt;a href=&#34;https://raw.githubusercontent.com/RahulSChand/llama2.c-for-dummies/main/#actual-forward-pass&#34;&gt;important part&lt;/a&gt;)&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Allocate memory for run state/intermediate values. The first &lt;code&gt;token&lt;/code&gt; we pass into our model is BOS token (&#34;Beginning of Statement&#34;) who&#39;s vocab index is &lt;code&gt;1&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;    RunState state;&#xA;    malloc_run_state(&amp;amp;state, &amp;amp;config);&#xA;    &#xA;    // the current position we are in&#xA;    long start = time_in_ms();&#xA;    int next;&#xA;    int token = 1; // 1 = BOS token in Llama-2 sentencepiece&#xA;    int pos = 0;&#xA;    printf(&#34;&amp;lt;s&amp;gt;\n&#34;); // explicit print the initial BOS token (=1), stylistically symmetric&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Forward loop: &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;code&gt;transformer(token, pos, &amp;amp;config, &amp;amp;state, &amp;amp;weights);&lt;/code&gt; stores classifier score of each token as being the next token in sequence inside &lt;code&gt;state.logits&lt;/code&gt;.(contents of &lt;code&gt;transformer&lt;/code&gt; function convered in next section).&lt;/li&gt; &#xA;   &lt;li&gt;Next we sample. Greedy sample is trivial, get max in &lt;code&gt;state.logits&lt;/code&gt; array. For &lt;code&gt;temperate&amp;gt;0&lt;/code&gt; convert &lt;code&gt;state.logits&lt;/code&gt; into probabilities using softmax &amp;amp; store back in &lt;code&gt;state.logits&lt;/code&gt;. The &lt;code&gt;sample(..)&lt;/code&gt; function returns a token sampled from the &lt;code&gt;state.logits&lt;/code&gt; probability distribution. Read more &lt;a href=&#34;https://web.mit.edu/urban_or_book/www/book/chapter7/7.1.3.html&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;The token generated &lt;code&gt;next&lt;/code&gt; becomes the next input token in line &lt;code&gt;token=next&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;while (pos &amp;lt; steps) {&#xA;        // forward the transformer to get logits for the next token&#xA;        transformer(token, pos, &amp;amp;config, &amp;amp;state, &amp;amp;weights);&#xA;        // sample the next token&#xA;        if(temperature == 0.0f) {&#xA;            // greedy argmax sampling&#xA;            next = argmax(state.logits, config.vocab_size);&#xA;        } else {&#xA;            // apply the temperature to the logits&#xA;            for (int q=0; q&amp;lt;config.vocab_size; q++) { state.logits[q] /= temperature; }&#xA;            // apply softmax to the logits to get the probabilities for next token&#xA;            softmax(state.logits, config.vocab_size);&#xA;            // we now want to sample from this distribution to get the next token&#xA;            next = sample(state.logits, config.vocab_size);&#xA;        }&#xA;        printf(&#34;%s&#34;, vocab[next]);&#xA;        fflush(stdout);&#xA;&#xA;        // advance forward&#xA;        token = next;&#xA;        pos++;&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Actual Forward pass&lt;/h3&gt; &#xA;&lt;p&gt;Details of &lt;code&gt;transformer(token, pos, &amp;amp;config, &amp;amp;state, &amp;amp;weights);&lt;/code&gt; called from &lt;code&gt;main()&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Section below uses 2d/3d array indexing extensively. We cover it briefly here to make life easier&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;If matrix &lt;code&gt;float* mat&lt;/code&gt; is of size &lt;code&gt;(dim1, dim2, dim3)&lt;/code&gt; then pointer to access &lt;code&gt;mat[l][i][j]&lt;/code&gt; is &lt;code&gt;dim2*dim3*l + dim3*i + j;&lt;/code&gt; - This is &lt;code&gt;formula-1&lt;/code&gt; we will refer to this often later. Read &lt;a href=&#34;https://www.learncpp.com/cpp-tutorial/pointer-arithmetic-and-array-indexing/&#34;&gt;link&lt;/a&gt; if you are confused&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;How to view matrices in terms of head?&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;K (key) &lt;code&gt;float* wk&lt;/code&gt; is a matrix defined as shape &lt;code&gt;(layer, dim, dim)&lt;/code&gt; when viewed in terms of heads is &lt;code&gt;(layer, dim, n_heads, head_dim)&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Convenience variables. Nothing interesting apart from copying the embedding of &lt;code&gt;token&lt;/code&gt; into &lt;code&gt;s-&amp;gt;xb&lt;/code&gt; using &lt;code&gt;memcpy&lt;/code&gt;. Why not use &lt;code&gt;float* content_row&lt;/code&gt; itself? Because &lt;code&gt;s-&amp;gt;xb&lt;/code&gt; is going to change &amp;amp; using &lt;code&gt;content_row&lt;/code&gt; will change model weights.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;void transformer(int token, int pos, Config* p, RunState* s, TransformerWeights* w) {&#xA;    // a few convenience variables&#xA;    float *x = s-&amp;gt;x;&#xA;    int dim = p-&amp;gt;dim;                  &#xA;    int hidden_dim =  p-&amp;gt;hidden_dim;  &#xA;    int head_size = dim / p-&amp;gt;n_heads; &#xA;    float* content_row = &amp;amp;(w-&amp;gt;token_embedding_table[token * dim]);&#xA;    // copy the token embedding into x&#xA;    memcpy(x, content_row, dim*sizeof(*x)); &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;RoPE&lt;/strong&gt; : Rotary Positional Embeddings&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Formulation: Transforms feature pairs by rotating it in 2D plane. e.g. If your vector is &lt;code&gt;[0.8, 0.5, -0.1, 0.3]&lt;/code&gt; we group them into pairs: &lt;code&gt;[[0.8,-0.1], [0.5, 0.3]&lt;/code&gt; and rotate by some angle $\theta$. This $\theta$ is &lt;del&gt;part of the weights &amp;amp; is learned during training&lt;/del&gt; $\theta$ is fixed from the start (its not learnable). In the paper the value of $\theta_{i}$ is $10000^{2(i-1)/d}$&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;RoPE Formula (For 2 features grouped into a pair) is below. $m$ is the index of the pair. $\theta$ is a learned parameter that we load from &lt;code&gt;.bin&lt;/code&gt; file&lt;/p&gt; &#xA;&lt;p&gt;$$ \left[ {\begin{array}{ccccc} x_{m}^{i} &amp;amp; x_{m}^{j} \ \end{array} } \right] * \left[ {\begin{array}{ccccc} cos(m\theta_{m}) &amp;amp; -sin(m\theta_{m}) \ sin(m\theta_{m}) &amp;amp; cos(m\theta_{m}) \ \end{array} } \right] $$&lt;/p&gt; &#xA;&lt;p&gt;Our example pair &lt;code&gt;[[0.8,-0.1], [0.5, 0.3]&lt;/code&gt; will be transformed like below. Keep in mind for the first pair &lt;code&gt;[0.8, 0.1]&lt;/code&gt; $m=0$ since (therefore $sin(0)=0$). And for 2nd pair &lt;code&gt;m=1&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;$$ \left[ {\begin{array}{ccccc} 0.8 &amp;amp; -0.1 \ \end{array} } \right] * \left[ {\begin{array}{ccccc} 1 * 1 &amp;amp; -0.0 * 1 \ 0.0 * 1 &amp;amp; 1.0 * 1 \ \end{array} } \right] = \left[ {\begin{array}{ccccc} 0.8 &amp;amp; -0.1 \ \end{array} } \right] $$&lt;/p&gt; &#xA;&lt;p&gt;$$ \left[ {\begin{array}{ccccc} 0.5 &amp;amp; 0.3 \ \end{array} } \right] * \left[ {\begin{array}{ccccc} 0.86 * 1 &amp;amp; -0.5 * 1 \ 0.5 * 1 &amp;amp; 0.86 * 1 \ \end{array} } \right] = \left[ {\begin{array}{ccccc} 0.58 &amp;amp; 0.08 \ \end{array} } \right] $$&lt;/p&gt; &#xA;&lt;p&gt;Combining both, the output is &lt;code&gt;[[0.8, 0.1], [0.58, 0.08]]&lt;/code&gt; now &lt;strong&gt;un-pairing&lt;/strong&gt; them will give us &lt;code&gt;[0.8, 0.58, 0.1, 0.08]&lt;/code&gt; So &lt;code&gt;RoPE&lt;/code&gt; transformed &lt;code&gt;[0.8, 0.5, -0.1, 0.3]&lt;/code&gt; into &lt;code&gt;[0.8, 0.58, 0.1, 0.08]&lt;/code&gt;. Keep in mind if a feature is of &lt;code&gt;dim=768&lt;/code&gt; then there are half of it &lt;strong&gt;384&lt;/strong&gt; learnable $\theta$&#39;s.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Back to code&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;We get $\theta$ for current position (&lt;code&gt;pos&lt;/code&gt; is our $m$). &lt;code&gt;freq_cis_real_row&lt;/code&gt; is $cos(m\theta)$ and &lt;code&gt;freq_cis_imag_row&lt;/code&gt; is $sin(m\theta)$.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;    // pluck out the &#34;pos&#34; row of freq_cis_real and freq_cis_imag66&#xA;    float* freq_cis_real_row = w-&amp;gt;freq_cis_real + pos * head_size / 2;&#xA;    float* freq_cis_imag_row = w-&amp;gt;freq_cis_imag + pos * head_size / 2;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Iterate over layers. Apply &lt;code&gt;rmsnorm&lt;/code&gt; to input of the layer. &lt;code&gt;rmsnorm&lt;/code&gt; function calculates the below&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-math&#34;&gt;out\; = \;  (x*g*n)/\sum_{i} \sqrt{x_{i}^{2}} &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where $x$ is input, $g$ is learnable parameter (&lt;code&gt;w-&amp;gt;rms_attn_weight&lt;/code&gt; below) &amp;amp; $n$ is &lt;code&gt;dim&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;matmul&lt;/code&gt; does matrix mult of a 2d matrix with a 1d matrix. &lt;code&gt;(A, B) x (A,)&lt;/code&gt;. The implementation is trivial (we cover this at very end). We multiply Q,K,V with &lt;code&gt;s-&amp;gt;xb&lt;/code&gt; (output of &lt;code&gt;rmsnorm&lt;/code&gt;) and store output in &lt;code&gt;s-&amp;gt;q&lt;/code&gt;, &lt;code&gt;s-&amp;gt;k&lt;/code&gt; ..&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;for(int l = 0; l &amp;lt; p-&amp;gt;n_layers; l++) {&#xA;// attention rmsnorm&#xA;&#x9;rmsnorm(s-&amp;gt;xb, x, w-&amp;gt;rms_att_weight + l*dim, dim);&#xA;&#x9;&#xA;&#x9;// qkv matmuls for this position&#xA;&#x9;matmul(s-&amp;gt;q, s-&amp;gt;xb, w-&amp;gt;wq + l*dim*dim, dim, dim);&#xA;&#x9;matmul(s-&amp;gt;k, s-&amp;gt;xb, w-&amp;gt;wk + l*dim*dim, dim, dim);&#xA;&#x9;matmul(s-&amp;gt;v, s-&amp;gt;xb, w-&amp;gt;wv + l*dim*dim, dim, dim);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Go over each head &amp;amp; apply the 2-d $cos$/$sin$ transformation we discussed above to &lt;code&gt;s-&amp;gt;q&lt;/code&gt; and &lt;code&gt;s-&amp;gt;k&lt;/code&gt;. We do it separately for each head, therefore we take offset of &lt;code&gt;h*head_size&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;// apply RoPE rotation to the q and k vectors for each head&#xA;        for (int h = 0; h &amp;lt; p-&amp;gt;n_heads; h++) {&#xA;            // get the q and k vectors for this head&#xA;            float* q = s-&amp;gt;q + h * head_size;&#xA;            float* k = s-&amp;gt;k + h * head_size;&#xA;            // rotate q and k by the freq_cis_real and freq_cis_imag&#xA;            for (int i = 0; i &amp;lt; head_size; i+=2) {&#xA;                float q0 = q[i];&#xA;                float q1 = q[i+1];&#xA;                float k0 = k[i];&#xA;                float k1 = k[i+1];&#xA;                float fcr = freq_cis_real_row[i/2];&#xA;                float fci = freq_cis_imag_row[i/2];&#xA;                q[i]   = q0 * fcr - q1 * fci;&#xA;                q[i+1] = q0 * fci + q1 * fcr;&#xA;                k[i]   = k0 * fcr - k1 * fci;&#xA;                k[i+1] = k0 * fci + k1 * fcr;&#xA;            }&#xA;        }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Once we get &lt;code&gt;q, k, v&lt;/code&gt; for current token, we need to calculate self-attention. Where we multiply query into key. &lt;code&gt;k &amp;amp; v&lt;/code&gt; are only for the current token. We store the &lt;code&gt;k, v&lt;/code&gt; for all past tokens in &lt;code&gt;key_cache_row&lt;/code&gt; &amp;amp; &lt;code&gt;value_cache_row&lt;/code&gt;. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;For example, if we have generated the tokens (&#34;fox&#34;, &#34;jumps&#34;, &#34;over&#34;) until now then we already have Q &amp;amp; V for &#34;fox&#34; &amp;amp; &#34;jumps&#34; from previous forward passes stored in our cache. We need not recalculate.&lt;/li&gt; &#xA;   &lt;li&gt;Since caches store key, query for all layers &amp;amp; for all tokens (max no.of tokens is &lt;code&gt;seq_length&lt;/code&gt;) its dimensions are &lt;code&gt;(layer, seq_length, dim)&lt;/code&gt;. &lt;code&gt;seq_length&lt;/code&gt; is usually called &lt;code&gt;context&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Consider below code in terms of above example. Lets say &lt;code&gt;seq_length=32&lt;/code&gt; (which means we generate at-most 32 tokens). &lt;code&gt;pos=2&lt;/code&gt; since &#34;fox&#34; is the 3rd token (2nd since python is 0-indexed). &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;We already have &lt;code&gt;layer*(pos-1)*dim&lt;/code&gt; values filled in &lt;code&gt;s-&amp;gt;key_cache&lt;/code&gt; We need to fill the key, value of current token &#34;fox&#34; into &lt;code&gt;s-&amp;gt;key_cache&lt;/code&gt; too before doing self-attention. This is what &lt;code&gt;memcpy(key_cache_row, s-&amp;gt;k, dim*sizeof(*key_cache_row));&lt;/code&gt; does&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;// save key,value at this time step (pos) to our kv cache&#xA;int loff = l * p-&amp;gt;seq_len * dim; // kv cache layer offset for convenience&#xA;float* key_cache_row = s-&amp;gt;key_cache + loff + pos * dim;&#xA;float* value_cache_row = s-&amp;gt;value_cache + loff + pos * dim;&#xA;memcpy(key_cache_row, s-&amp;gt;k, dim*sizeof(*key_cache_row));&#xA;memcpy(value_cache_row, s-&amp;gt;v, dim*sizeof(*value_cache_row));&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Doing self-attention&lt;/h3&gt; &#xA;&lt;p&gt;Formula&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-math&#34;&gt;\begin{align} &#xA;out = (QK^{T})\;V/\sqrt{d} \\&#xA;where\;\;\; Q=(1,dim) \;\; K=(dim,N) \;\; V=(dim,N)&#xA;\end{align}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In above $N$ is &lt;code&gt;pos&lt;/code&gt; (current length of the generated text)&lt;/p&gt; &#xA;&lt;p&gt;This part of the code becomes easy if you remember that &lt;code&gt;s-&amp;gt;q&lt;/code&gt;, &lt;code&gt;s-&amp;gt;k&lt;/code&gt; when viewed in terms of heads are of shape &lt;code&gt;(dim, n_heads, head_dim)&lt;/code&gt; &amp;amp; &lt;code&gt;key_cache&lt;/code&gt;&#39;s are &lt;code&gt;(seq_length, n_heads, head_dim)&lt;/code&gt;. Lets go over the code&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;int h&lt;/code&gt; is the current head count. Lets look at each line one by one &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;code&gt;q = s-&amp;gt;q + h*head_size&lt;/code&gt; : Gets pointer to start of $h^{th}$ head. Remember &lt;code&gt;formula-1&lt;/code&gt;. Matrix is of size &lt;code&gt;(dim, n_heads, head_dim)&lt;/code&gt; we need &lt;code&gt;s-&amp;gt;q[0][h][0]&lt;/code&gt; which is &lt;code&gt;0*n_heads*head_dim + h*head_dim + 0&lt;/code&gt; which is &lt;code&gt;h*head_size&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;att = s-&amp;gt;att + h * p-&amp;gt;seq_len&lt;/code&gt;: We will store attention in &lt;code&gt;s-&amp;gt;attn&lt;/code&gt; run state variable.&lt;/li&gt; &#xA;   &lt;li&gt;For each position (&lt;code&gt;pos&lt;/code&gt; is 2 currently if you go back to &#34;fox&#34;, &#34;jumps&#34;, &#34;over&#34; example) 1.To get $l^{th}$ layer, $t^{th}$ position &amp;amp; $h^{th}$ head we do &lt;code&gt;s-&amp;gt;key_cache + l*seq_length*dim + t*n_heads*head_dim + h*head_dim&lt;/code&gt; . Since &lt;code&gt;loff&lt;/code&gt; defined before is already &lt;code&gt;l*seq_length*dim&lt;/code&gt;. Final offset is &lt;code&gt;loff + t*n_heads*head_dim + h*head_size&lt;/code&gt; since &lt;code&gt;n_heads*head_dim=dim&lt;/code&gt; we get offset as &lt;code&gt;loff + t*dim + h*head_size&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;We now have &lt;code&gt;q&lt;/code&gt; &lt;code&gt;(head_size,)&lt;/code&gt;, &lt;code&gt;k&lt;/code&gt; &lt;code&gt;(head_size,)&lt;/code&gt; &amp;amp; &lt;code&gt;att&lt;/code&gt; &lt;code&gt;(seq_length,)&lt;/code&gt;. We can calculate self-attention score for $h^{th}$ head at position $t$. We sum this over all the heads &amp;amp; positions till now.&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;&#x9;int h;        &#xA;&#x9;#pragma omp parallel for private(h)&#xA;&#x9;for (h = 0; h &amp;lt; p-&amp;gt;n_heads; h++) {&#xA;&#x9;// get the query vector for this head&#xA;&#x9;float* q = s-&amp;gt;q + h * head_size;&#xA;&#x9;// attention scores for this head&#xA;&#x9;float* att = s-&amp;gt;att + h * p-&amp;gt;seq_len;&#xA;&#x9;// iterate over all timesteps, including the current one&#xA;&#x9;for (int t = 0; t &amp;lt;= pos; t++) {&#xA;&#x9;&#x9;// get the key vector for this head and at this timestep&#xA;&#x9;&#x9;float* k = s-&amp;gt;key_cache + loff + t * dim + h * head_size;&#xA;&#x9;&#x9;// calculate the attention score as the dot product of q and k&#xA;&#x9;&#x9;float score = 0.0f;&#xA;&#x9;&#x9;for (int i = 0; i &amp;lt; head_size; i++) {&#xA;&#x9;&#x9;&#x9;score += q[i] * k[i];&#xA;&#x9;&#x9;}&#xA;&#x9;&#x9;score /= sqrtf(head_size);&#xA;&#x9;&#x9;// save the score to the attention buffer&#xA;&#x9;&#x9;att[t] = score;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;&lt;code&gt;attn&lt;/code&gt; obtained above is of shape &lt;code&gt;(seq_length, )&lt;/code&gt;. Next we multiply it with &lt;code&gt;v&lt;/code&gt; which is &lt;code&gt;(seq_length, dim)&lt;/code&gt;. Remember the below loop is inside the &lt;code&gt;for (h = 0; h &amp;lt; p-&amp;gt;n_heads; h++)&lt;/code&gt; that started in previous section.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;// softmax the scores to get attention weights, from 0..pos inclusively&#xA;softmax(att, pos + 1);&#xA;&#xA;// weighted sum of the values, store back into xb&#xA;float* xb = s-&amp;gt;xb + h * head_size;&#xA;memset(xb, 0, head_size * sizeof(float));&#xA;for (int t = 0; t &amp;lt;= pos; t++) {&#xA;&#x9;// get the value vector for this head and at this timestep&#xA;&#x9;float* v = s-&amp;gt;value_cache + loff + t * dim + h * head_size;&#xA;&#x9;// get the attention weight for this timestep&#xA;&#x9;float a = att[t];&#xA;&#x9;// accumulate the weighted value into xb&#xA;&#x9;for (int i = 0; i &amp;lt; head_size; i++) {&#xA;&#x9;&#x9;xb[i] += a * v[i];&#xA;&#x9;}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Feed Forward &amp;amp; Classifier&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;To complete attention module, we need to multiply with $O$ which we do in first line. Next line &lt;code&gt;accum&lt;/code&gt; adds input which comes from skip layer (red arrow) &amp;amp; output of attention. Followed by normalization.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;// final matmul to get the output of the attention&#xA;matmul(s-&amp;gt;xb2, s-&amp;gt;xb, w-&amp;gt;wo + l*dim*dim, dim, dim);&#xA;// residual connection back into x&#xA;accum(x, s-&amp;gt;xb2, dim);&#xA;// ffn rmsnorm&#xA;rmsnorm(s-&amp;gt;xb, x, w-&amp;gt;rms_ffn_weight + l*dim, dim);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/RahulSChand/llama2.c-for-dummies/main/imgs/accum.png&#34; width=&#34;400&#34; height=&#34;400&#34;&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Next we calculate the FFN output which is&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-math&#34;&gt;out = W_{3}\;\sigma (W_{1}X*W_{2}X)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;$\sigma$ is &lt;code&gt;silu&lt;/code&gt; &lt;a href=&#34;https://pytorch.org/docs/stable/generated/torch.nn.SiLU.html&#34;&gt;activation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;img src=&#34;https://pytorch.org/docs/stable/_images/SiLU.png&#34; width=&#34;400&#34; height=&#34;400&#34;&gt; &#xA;&lt;p&gt;This portion is self explanatory&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;// Now for FFN in PyTorch we have: self.w2(F.silu(self.w1(x)) * self.w3(x))&#xA;// first calculate self.w1(x) and self.w3(x)&#xA;matmul(s-&amp;gt;hb, s-&amp;gt;xb, w-&amp;gt;w1 + l*dim*hidden_dim, dim, hidden_dim);&#xA;matmul(s-&amp;gt;hb2, s-&amp;gt;xb, w-&amp;gt;w3 + l*dim*hidden_dim, dim, hidden_dim);&#xA;// F.silu; silu(x)=x*σ(x),where σ(x) is the logistic sigmoid&#xA;for (int i = 0; i &amp;lt; hidden_dim; i++) {&#xA;&#x9;s-&amp;gt;hb[i] = s-&amp;gt;hb[i] * (1.0f / (1.0f + expf(-s-&amp;gt;hb[i])));&#xA;}&#xA;// elementwise multiply with w3(x)&#xA;for (int i = 0; i &amp;lt; hidden_dim; i++) {&#xA;&#x9;s-&amp;gt;hb[i] = s-&amp;gt;hb[i] * s-&amp;gt;hb2[i];&#xA;}&#xA;// final matmul to get the output of the ffn&#xA;//memcpy(tmp_w_hid, w-&amp;gt;w2 + l*dim*hidden_dim, hidden_dim*dim*sizeof(float));&#xA;matmul(s-&amp;gt;xb, s-&amp;gt;hb, w-&amp;gt;w2 + l*dim*hidden_dim, hidden_dim, dim);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;The last line is another accum (2nd skip layer in above diagram)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;accum(x, s-&amp;gt;xb, dim);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Final Classifier&lt;/h3&gt; &#xA;&lt;p&gt;After running above module for all layers, we get an embedding of shape &lt;code&gt;(dim,)&lt;/code&gt;. We need to convert this into a vector of shape &lt;code&gt;(vocab,)&lt;/code&gt; whose each entry tells us what is the score for that word to be next token.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Before multiplying with classifier matrix (&lt;code&gt;w-&amp;gt;wcls&lt;/code&gt;) we normalize our embedding. The scores our saved in &lt;code&gt;s-&amp;gt;logits&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;// final rmsnorm&#xA;rmsnorm(x, x, w-&amp;gt;rms_final_weight, dim);&#xA;// classifier into logits&#xA;matmul(s-&amp;gt;logits, x, w-&amp;gt;wcls, p-&amp;gt;dim, p-&amp;gt;vocab_size);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;The end&lt;/h3&gt; &#xA;&lt;p&gt;Once we get &lt;code&gt;s-&amp;gt;logits&lt;/code&gt; we sample next token (do this until we get &lt;code&gt;seq_length&lt;/code&gt; tokens). This has already been covered in &#34;Forward Loop &amp;amp; sampling in main&#34; section. Congratulations! now you know how LLMs work &amp;amp; how to code them in C. If you now want to know how to code them in Python know, refer to &lt;a href=&#34;https://github.com/huggingface/transformers/raw/main/src/transformers/models/llama/modeling_llama.py&#34;&gt;modelling_llama.py&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Here is a picture of a cat :)&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/RahulSChand/llama2.c-for-dummies/main/imgs/cat.jpg&#34; width=&#34;700&#34; height=&#34;500&#34;&gt;</summary>
  </entry>
  <entry>
    <title>prusa3d/Prusa-Firmware-Buddy</title>
    <updated>2023-08-03T01:30:40Z</updated>
    <id>tag:github.com,2023-08-03:/prusa3d/Prusa-Firmware-Buddy</id>
    <link href="https://github.com/prusa3d/Prusa-Firmware-Buddy" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Firmware for the Original Prusa MINI, Original Prusa XL and the Original Prusa XL 3D printers by Prusa Research.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Buddy&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/prusa3d/Prusa-Firmware-Buddy/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/release/prusa3d/Prusa-Firmware-Buddy.svg?sanitize=true&#34; alt=&#34;GitHub release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://holly.prusa3d.com/job/Prusa-Firmware-Buddy/job/Multibranch/job/master/&#34;&gt;&lt;img src=&#34;https://holly.prusa3d.com/buildStatus/icon?job=Prusa-Firmware-Buddy%2FMultibranch%2Fmaster&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository includes source code and firmware releases for the Original Prusa 3D printers based on the 32-bit ARM microcontrollers.&lt;/p&gt; &#xA;&lt;p&gt;The currently supported models are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Original Prusa MINI&lt;/li&gt; &#xA; &lt;li&gt;Original Prusa MK4&lt;/li&gt; &#xA; &lt;li&gt;Original Prusa XL&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.6 or newer (with pip)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Cloning this repository&lt;/h3&gt; &#xA;&lt;p&gt;Run &lt;code&gt;git clone https://github.com/prusa3d/Prusa-Firmware-Buddy.git&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Building (on all platforms, without an IDE)&lt;/h3&gt; &#xA;&lt;p&gt;Run &lt;code&gt;python utils/build.py&lt;/code&gt;. The binaries are then going to be stored under &lt;code&gt;./build/products&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Without any arguments, it will build a release version of the firmware for all supported printers and bootloader settings.&lt;/li&gt; &#xA; &lt;li&gt;To generate &lt;code&gt;.bbf&lt;/code&gt; versions of the firmware, use: &lt;code&gt;./utils/build.py --generate-bbf&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Use &lt;code&gt;--build-type&lt;/code&gt; to select build configurations to be built (&lt;code&gt;debug&lt;/code&gt;, &lt;code&gt;release&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Use &lt;code&gt;--preset&lt;/code&gt; to select for which printers the firmware should be built.&lt;/li&gt; &#xA; &lt;li&gt;By default, it will build the firmware in &#34;prerelease mode&#34; set to &lt;code&gt;beta&lt;/code&gt;. You can change the prerelease using &lt;code&gt;--prerelease alpha&lt;/code&gt;, or use &lt;code&gt;--final&lt;/code&gt; to build a final version of the firmware.&lt;/li&gt; &#xA; &lt;li&gt;Use &lt;code&gt;--host-tools&lt;/code&gt; to include host tools in the build (&lt;code&gt;bin2cc&lt;/code&gt;, &lt;code&gt;png2font&lt;/code&gt;, ...)&lt;/li&gt; &#xA; &lt;li&gt;Find more options using the &lt;code&gt;--help&lt;/code&gt; flag!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Examples:&lt;/h4&gt; &#xA;&lt;p&gt;Build the firmware for MINI and XL in &lt;code&gt;debug&lt;/code&gt; mode:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python utils/build.py --preset mini,xl --build-type debug&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Build the firmware for MINI using a custom version of gcc-arm-none-eabi (available in &lt;code&gt;$PATH&lt;/code&gt;) and use &lt;code&gt;Make&lt;/code&gt; instead of &lt;code&gt;Ninja&lt;/code&gt; (not recommended):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python utils/build.py --preset mini --toolchain cmake/AnyGccArmNoneEabi.cmake --generator &#39;Unix Makefiles&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Windows 10 troubleshooting&lt;/h4&gt; &#xA;&lt;p&gt;If you have python installed and in your PATH but still getting cmake error &lt;code&gt;Python3 not found.&lt;/code&gt; Try running python and python3 from cmd. If one of it opens Microsoft Store instead of either opening python interpreter or complaining &lt;code&gt;&#39;python3&#39; is not recognized as an internal or external command, operable program or batch file.&lt;/code&gt; Open &lt;code&gt;manage app execution aliases&lt;/code&gt; and disable &lt;code&gt;App Installer&lt;/code&gt; association with &lt;code&gt;python.exe&lt;/code&gt; and &lt;code&gt;python3.exe&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Python environment&lt;/h4&gt; &#xA;&lt;p&gt;The &lt;code&gt;build.py&lt;/code&gt; script will install some Python packages. If you prefer not to have your system modified, we recommend to use &lt;code&gt;virtualenv&lt;/code&gt; or a similar tool.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;virtualenv venv&#xA;. venv/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Development&lt;/h3&gt; &#xA;&lt;p&gt;The build process of this project is driven by CMake and &lt;code&gt;build.py&lt;/code&gt; is just a high-level wrapper around it. As most modern IDEs support some kind of CMake integration, it should be possible to use almost any editor for development. Below are some documents describing how to setup some popular text editors.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/prusa3d/Prusa-Firmware-Buddy/master/doc/editor/vscode.md&#34;&gt;Visual Studio Code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/prusa3d/Prusa-Firmware-Buddy/master/doc/editor/vim.md&#34;&gt;Vim&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/prusa3d/Prusa-Firmware-Buddy/master/doc/editor/stm32cubeide.md&#34;&gt;Eclipse, STM32CubeIDE&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/prusa3d/Prusa-Firmware-Buddy/master/doc/editor/lsp-based-ides.md&#34;&gt;Other LSP-based IDEs (Atom, Sublime Text, ...)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Formatting&lt;/h4&gt; &#xA;&lt;p&gt;All the source code in this repository is automatically formatted:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;C/C++ files using &lt;a href=&#34;https://clang.llvm.org/docs/ClangFormat.html&#34;&gt;clang-format&lt;/a&gt;,&lt;/li&gt; &#xA; &lt;li&gt;Python files using &lt;a href=&#34;https://github.com/google/yapf&#34;&gt;yapf&lt;/a&gt;,&lt;/li&gt; &#xA; &lt;li&gt;and CMake files using &lt;a href=&#34;https://github.com/cheshirekow/cmake_format&#34;&gt;cmake-format&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you want to contribute, make sure to install &lt;a href=&#34;https://pre-commit.com&#34;&gt;pre-commit&lt;/a&gt; and then run &lt;code&gt;pre-commit install&lt;/code&gt; within the repository. This makes sure that all your future commits will be formatted appropriately. Our build server automatically rejects improperly formatted pull requests.&lt;/p&gt; &#xA;&lt;h4&gt;XL and Puppies&lt;/h4&gt; &#xA;&lt;p&gt;With the XL, the situation gets a bit more complex. The firmware of XLBuddy contains firmwares for the puppies (Dwarf and Modularbed) to flash them when necessary. We support several ways of dealing with those firmwares when developing:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Build Dwarf/Modularbed firmware automatically and flash it on startup by XLBuddy (the default)&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The Dwarf &amp;amp; ModularBed firmware will be built from this repo.&lt;/li&gt; &#xA;   &lt;li&gt;The puppies are going to be flashed on startup by the XLBuddy. The puppies have to be running the &lt;a href=&#34;http://github.com/prusa3d/Prusa-Bootloader-Puppy&#34;&gt;Puppy Bootloader&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Build Dwarf/Modularbed from a given source directory and flash it on startup by XLBuddy.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Specify &lt;code&gt;DWARF_SOURCE_DIR&lt;/code&gt;/&lt;code&gt;MODULARBED_SOURCE_DIR&lt;/code&gt; CMake cache variable with the local repo you want to use.&lt;/li&gt; &#xA;   &lt;li&gt;Example below would build modularbed&#39;s firmware from /Projects/Prusa-Firmware-Buddy-ModularBed and include it in the xlBuddy firmware.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;pre&gt;&lt;code&gt;cmake .. --preset xl_release_boot -DMODULARBED_SOURCE_DIR=/Projects/Prusa-Firmware-Buddy-ModularBed&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;You can also specify the build directory you want to use:&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;pre&gt;&lt;code&gt;cmake .. --preset xl_release_boot \&#xA;    -DMODULARBED_SOURCE_DIR=/Projects/Prusa-Firmware-Buddy-ModularBed  \&#xA;    -DMODULARBED_BINARY_DIR=/Projects/Prusa-Firmware-Buddy-ModularBed/build&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Use pre-built Dwarf/Modularbed firmware and flash it on startup by xlBuddy&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Specify the location of the .bin file with &lt;code&gt;DWARF_BINARY_PATH&lt;/code&gt;/&lt;code&gt;MODULARBED_BINARY_PATH&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;For example&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;pre&gt;&lt;code&gt;cmake .. --preset xl_release_boot -DDWARF_BINARY_PATH=/Downloads/dwarf-4.4.0-boot.bin&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Do not include any puppy firmware, and do not flash the puppies by XLBuddy.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;-DENABLE_PUPPY_BOOTLOAD=NO&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;With the &lt;code&gt;ENABLE_PUPPY_BOOTLOAD&lt;/code&gt; set to false, the project will disable Puppy flashing &amp;amp; interaction with Puppy bootloaders.&lt;/li&gt; &#xA;   &lt;li&gt;It is up to you to flash the correct firmware to the puppies (noboot variant).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;See /ProjectOptions.cmake for more information about those cache variables.&lt;/p&gt; &#xA;&lt;h4&gt;Running tests&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir build-tests&#xA;cd build-tests&#xA;cmake ..&#xA;make tests&#xA;ctest .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Flashing Custom Firmware&lt;/h2&gt; &#xA;&lt;p&gt;To install custom firmware, you have to break the appendix on the board. Learn how to in the following article &lt;a href=&#34;https://help.prusa3d.com/article/zoiw36imrs-flashing-custom-firmware&#34;&gt;https://help.prusa3d.com/article/zoiw36imrs-flashing-custom-firmware&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Feedback&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/prusa3d/Prusa-Firmware-Buddy/labels/feature%20request&#34;&gt;Feature Requests from Community&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The firmware source code is licensed under the GNU General Public License v3.0 and the graphics and design are licensed under Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0). Fonts are licensed under different license (see &lt;a href=&#34;https://raw.githubusercontent.com/prusa3d/Prusa-Firmware-Buddy/master/LICENSE.md&#34;&gt;LICENSE&lt;/a&gt;).&lt;/p&gt;</summary>
  </entry>
</feed>