<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-06-23T01:29:25Z</updated>
  <subtitle>Daily Trending of C in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>robertdavidgraham/wc2</title>
    <updated>2024-06-23T01:29:25Z</updated>
    <id>tag:github.com,2024-06-23:/robertdavidgraham/wc2</id>
    <link href="https://github.com/robertdavidgraham/wc2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Investigates optimizing &#39;wc&#39;, the Unix word count program&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;wc2 - asynchronous state machine parsing&lt;/h1&gt; &#xA;&lt;p&gt;There have been multiple articles lately implementing the classic &lt;code&gt;wc&lt;/code&gt; program in various programming &lt;em&gt;languages&lt;/em&gt;, to &#34;prove&#34; their favorite language can be &#34;just as fast&#34; as C.&lt;/p&gt; &#xA;&lt;p&gt;This project does something different. Instead of a different &lt;em&gt;language&lt;/em&gt; it uses a different &lt;em&gt;algorithm&lt;/em&gt;. The new algorithm is significantly faster -- implementing in a slow language like JavaScript is still faster than the original &lt;code&gt;wc&lt;/code&gt; program written in C.&lt;/p&gt; &#xA;&lt;p&gt;The algorithm is known as an &#34;asynchronous state-machine parser&#34;. It&#39;s a technique for &lt;em&gt;parsing&lt;/em&gt; that you don&#39;t learn in college. It&#39;s more &lt;em&gt;efficient&lt;/em&gt;, but more importantly, it&#39;s more &lt;em&gt;scalable&lt;/em&gt;. That&#39;s why your browser uses a state-machine to parse GIFs, and most web servers use state-machiens to parse incoming HTTP requests.&lt;/p&gt; &#xA;&lt;p&gt;This projects contains three versions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;wc2o.c&lt;/code&gt; is a simplified 25 line version highlighting the idea&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;wc2.c&lt;/code&gt; is the full version in C, supporting Unicode&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;wc2.js&lt;/code&gt; is the version in JavaScript&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;The basic algorithm&lt;/h2&gt; &#xA;&lt;p&gt;The algorithm reads input and passes each byte one at a time to a state-machine. It looks something like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;    length = fread(buf, 1, sizeof(buf), fp);&#xA;    for (i=0; i&amp;lt;length; i++) {&#xA;        c = buf[i];&#xA;        state = table[state][c];&#xA;        counts[state]++;&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;No, you aren&#39;t suppose to be able to see how the word-count works by looking at this code. The complexity happens elsewhere, setting up the state-machine.&lt;/p&gt; &#xA;&lt;p&gt;The state-machine table is the difference between the simple version (&lt;code&gt;wc2o.c&lt;/code&gt;) and complex version (&lt;code&gt;wc2.c&lt;/code&gt;) of the program. The algorithm is the same, the one shown above, the difference is in how they setup the table. The simple program creates a table for ASCII, the complex program creates a much larger table supporting UTF-8.&lt;/p&gt; &#xA;&lt;h2&gt;How &lt;code&gt;wc&lt;/code&gt; works&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;wc&lt;/code&gt; word-count program counts the number of words in a file. A &#34;word&#34; is some non-space characters separate by space.&lt;/p&gt; &#xA;&lt;p&gt;Those who re-implement &lt;code&gt;wc&lt;/code&gt; simplify the problem by only doing ASCII instead of the full UTF-8 Unicode. This is cheating, because much of the speed of &lt;code&gt;wc&lt;/code&gt; comes from its need to handle character-sets like UTF-8. The real programs spend most of their time in functions like &lt;code&gt;mbrtowc()&lt;/code&gt; to parse multi-byte characters and &lt;code&gt;iswspace()&lt;/code&gt; to test if they are spaces -- which re-implementations of &lt;code&gt;wc&lt;/code&gt; skip.&lt;/p&gt; &#xA;&lt;p&gt;For this reason, we&#39;ve implemented a full UTF-8 version in this project, to prove that it works without cheating. Now the real &lt;code&gt;wc&lt;/code&gt; works with a lot more character-sets, and we don&#39;t do that. But by implementing UTF-8, we&#39;ve shown that it&#39;s possible, and that the speed for any character-set is the same.&lt;/p&gt; &#xA;&lt;p&gt;Another simplification is how invalid input is handled. The original &lt;code&gt;wc&lt;/code&gt; program largley ignores errors, but it&#39;s still an important factor in making sure you are doing things correctly.&lt;/p&gt; &#xA;&lt;h2&gt;Benchmark input files&lt;/h2&gt; &#xA;&lt;p&gt;This project uses a number of large input files for benchmarking. The traditional &lt;code&gt;wc&lt;/code&gt; program has wildly different performance depending upon input, such as whether the file is full of illegal characters, or whether UTF-8 is being handled. The first test file is downloaded from the Internet as &#34;real-world data&#34;, while the others are generated using a program built with this project (&lt;code&gt;wctool&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;pocorgtfo18.pdf&lt;/code&gt; a large 92-million byte PDF file that contains binary/illegal characters&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ascii.txt&lt;/code&gt; a file the same size containing random words, ASCII-only&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;utf8.txt&lt;/code&gt; a file containing random UTF-8 sequences of 1, 2, 3, and 4 bytes&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;word.txt&lt;/code&gt; a file containing 92-million &#39;x&#39; characters&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;space.txt&lt;/code&gt; a file containing 92-million &#39; &#39; (space) characters&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Before benchmarking the old &lt;code&gt;wc&lt;/code&gt;, set the character-set to UTF-8. It&#39;s probably already set to this on new systems, but do this to make sure:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ export LC_CTYPE=en_US.UTF-8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When running &lt;code&gt;wc&lt;/code&gt;, the &lt;code&gt;-lwc&lt;/code&gt; is the default for counting words in ASCII text. To convert it into UTF-8 &#34;multi-byte&#34; mode, change &lt;code&gt;c&lt;/code&gt; o &lt;code&gt;m&lt;/code&gt;, as in &lt;code&gt;-lwm&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The numbers are reported come from the Unix &lt;code&gt;time&lt;/code&gt; command, the number of seconds for &lt;code&gt;user&lt;/code&gt; time. In other words, &lt;code&gt;elapsed&lt;/code&gt; time or &lt;code&gt;system&lt;/code&gt; time aren&#39;t reported.&lt;/p&gt; &#xA;&lt;p&gt;The following table shows benchmarking a 2019 x86 MacBook Air of the old &lt;code&gt;wc&lt;/code&gt; program. As you can see, it has a wide variety of speeds depending on input.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;wc&lt;/code&gt; program included with macOS and Linux are completely different. Therefore, the following table shows them benchmarked against each other on the same hardware.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Command&lt;/th&gt; &#xA;   &lt;th&gt;Input File&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;macOS&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Linux&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;wc -lwc&lt;/td&gt; &#xA;   &lt;td&gt;pocorgtfo18.pdf&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.709&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;5.591&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;wc -lwm&lt;/td&gt; &#xA;   &lt;td&gt;pocorgtfo18.pdf&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.693&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;5.419&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;wc -lwc&lt;/td&gt; &#xA;   &lt;td&gt;ascii.txt&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.296&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2.509&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;wc -lwm&lt;/td&gt; &#xA;   &lt;td&gt;utf8.txt&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.532&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1.840&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;wc -lwc&lt;/td&gt; &#xA;   &lt;td&gt;space.txt&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.296&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.284&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;wc -lwm&lt;/td&gt; &#xA;   &lt;td&gt;space.txt&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.295&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.298&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;wc -lwc&lt;/td&gt; &#xA;   &lt;td&gt;word.txt&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.302&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1.268&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;wc -lwm&lt;/td&gt; &#xA;   &lt;td&gt;word.txt&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.294&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1.337&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;These results tell us:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Illegal characters (in &lt;code&gt;pocorgtfo18.pdf&lt;/code&gt;) slow things down a lot, twice as slow on macOS, 10x slower on Linux.&lt;/li&gt; &#xA; &lt;li&gt;Text that randomly switches between spaces and words is much slower than text containing all the same character.&lt;/li&gt; &#xA; &lt;li&gt;On Linux, the code path that reads all spaces is significantly faster.&lt;/li&gt; &#xA; &lt;li&gt;The macOS program is in general much faster than the Linux version.&lt;/li&gt; &#xA; &lt;li&gt;Processing Unicode (the file &lt;code&gt;utf8.txt&lt;/code&gt; with the &lt;code&gt;-m&lt;/code&gt; option) is slower than processing ASCII (the file &lt;code&gt;ascii.txt&lt;/code&gt; with the &lt;code&gt;-c&lt;/code&gt; option).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Our benchmarks&lt;/h2&gt; &#xA;&lt;p&gt;The time for our algorithm, in C and JavaScript, are the following. The state-machine parser is immune to input type, all the input files show the same results.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Program&lt;/th&gt; &#xA;   &lt;th&gt;Input File&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;macOS&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Linux&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;wc2.c&lt;/td&gt; &#xA;   &lt;td&gt;(all)&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.206&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.278&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;wc2.js&lt;/td&gt; &#xA;   &lt;td&gt;(all)&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.281&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.488&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;These results tell us:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This state machine approach always results in the same speed, regardless of input.&lt;/li&gt; &#xA; &lt;li&gt;This state machine approach is faster than the built-in programs.&lt;/li&gt; &#xA; &lt;li&gt;Even written in JavaScript, the state machine approach is competitive in speed.&lt;/li&gt; &#xA; &lt;li&gt;The difference in macOS and Linux speed is actually the difference in &lt;code&gt;clang&lt;/code&gt; and &lt;code&gt;gcc&lt;/code&gt; speed. The LLVM &lt;code&gt;clang&lt;/code&gt; compiler is doing better optimizations for x86 processors here.&lt;/li&gt; &#xA; &lt;li&gt;I don&#39;t know why Node.js behaves differently on macOS and Linux, it&#39;s probably just due to different versions.&lt;/li&gt; &#xA; &lt;li&gt;A JIT (like NodeJS) works well with simple compute algorithms. This tells us little about it&#39;s relative performance in larger programs. All languages that have a JIT should compile this sort of algorithm to roughly the same speed.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Asynchronous scalability&lt;/h2&gt; &#xA;&lt;p&gt;The algorithm is &lt;em&gt;faster&lt;/em&gt;, but more importantly, it&#39;s more &lt;em&gt;scalable&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Such scalability isn&#39;t usefull for &lt;code&gt;wc&lt;/code&gt;, but is incredibly important for network programs. Consider an HTTP web-server. The traditional way that the Apache web-server worked was by reading the entire header in and buffering it, before then parsing the header. This need to buffer the entire header caused an enormous scalability problem. In contrast, asynchronous web-servers like Nginx use a state-machine parser. They parse the bytes as they arrive, and discard them.&lt;/p&gt; &#xA;&lt;p&gt;This is analogous to NFA and DFA regular-expressions. If you use the NFA approach, you need to buffer the entire chunk of data, so that the regex can backtrack. Using the DFA approach, input can be provided as a stream, one byte at a time, without needing buffering. DFAs are more scalable than NFAs.&lt;/p&gt; &#xA;&lt;h2&gt;State machine parsers&lt;/h2&gt; &#xA;&lt;p&gt;This project contains a minimalistic &lt;code&gt;wc2o.c&lt;/code&gt; program to highlight the algorithm, without all the fuss of building UTF-8 tables, supporting only ASCII.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;#include &amp;lt;stdio.h&amp;gt;&#xA;int main(void)&#xA;{&#xA;    static const unsigned char table[4][4] = {&#xA;        {2,0,1,0,}, {2,0,1,0,}, {3,0,1,0,},  {3,0,1,0,}&#xA;    };&#xA;    static const unsigned char column[256] = {&#xA;        0,0,0,0,0,0,0,0,0,1,2,1,1,1,0,0,0,&#xA;        0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,&#xA;    };&#xA;    unsigned long counts[4] = {0,0,0,0};&#xA;    int state = 0;&#xA;    int c;&#xA;&#xA;    while ((c = getchar()) != EOF) {&#xA;        state = table[state][column[c]];&#xA;        counts[state]++;&#xA;    }&#xA;&#xA;    printf(&#34;%lu %lu %lu\n&#34;, counts[1], counts[2], &#xA;                counts[0] + counts[1] + counts[2] + counts[3]);&#xA;    return 0;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The key part that does all the word counting is in the two lines inside:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;    while ((c = getchar()) != EOF) {&#xA;        state = table[state][column[c]];&#xA;        counts[state]++;&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This is only defined for ASCII, so you can see the state-machine on a single-line in the code (&lt;code&gt;table&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;h2&gt;Additional tools&lt;/h2&gt; &#xA;&lt;p&gt;This project includes additional tools:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;wctool&lt;/code&gt; to generate large test files&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;wcdiff&lt;/code&gt; to find difference between two implementatins of &lt;code&gt;wc&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;wcstream&lt;/code&gt; to fragment input files (demonstrates a bug in macOS&#39;s &lt;code&gt;wc&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The program &lt;code&gt;wc2.c&lt;/code&gt; has the same logic, the difference being that it generates a larger state-machine for parsing UTF-8.&lt;/p&gt; &#xA;&lt;h2&gt;Pointer arithmetic&lt;/h2&gt; &#xA;&lt;p&gt;C has a peculiar idiom called &#34;pointer arithmetic&#34;, where pointers can be incremented. Looping through a buffer is done with an expression like &lt;code&gt;*buf++&lt;/code&gt; instead of &lt;code&gt;buf[i++]&lt;/code&gt;. Many programmers think pointer-arithmetic is faster.&lt;/p&gt; &#xA;&lt;p&gt;To test this, the &lt;code&gt;wc2.c&lt;/code&gt; program has an option &lt;code&gt;-P&lt;/code&gt; that makes this small change, to test the difference in speed.&lt;/p&gt;</summary>
  </entry>
</feed>