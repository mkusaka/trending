<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-03T01:27:36Z</updated>
  <subtitle>Daily Trending of C in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>cpldcpu/BitNetMCU</title>
    <updated>2024-05-03T01:27:36Z</updated>
    <id>tag:github.com,2024-05-03:/cpldcpu/BitNetMCU</id>
    <link href="https://github.com/cpldcpu/BitNetMCU" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Neural Networks with low bit weights on a CH32V003 RISC-V Microcontroller without multiplication&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;BitNetMCU: High Accuracy Low-Bit Quantized Neural Networks on a low-end Microcontroller&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;BitNetMCU&lt;/strong&gt; is a project focused on the training and inference of low-bit quantized neural networks, specifically designed to run efficiently on low-end RISC-V microcontrollers like the CH32V003. Quantization aware training (QAT) and finetuning of model structure and inference code allowed &lt;em&gt;surpassing 99% Test accuracy on a 16x16 MNIST dataset without using multiplication instructions and in only 2kb of RAM and 16kb of Flash&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The training pipeline is based on PyTorch and should run anywhere. The inference engine is implemented in Ansi-C and can be easily ported to any Microcontroller.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;You can find a detailed report on the project in the &lt;code&gt;docs/&lt;/code&gt; directory &lt;a href=&#34;https://raw.githubusercontent.com/cpldcpu/BitNetMCU/main/docs/documentation.md&#34;&gt;here&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/cpldcpu/BitNetMCU/main/docs/header.png&#34; width=&#34;95%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Project Structure&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;BitNetMCU/&#xA;│&#xA;├── docs/                      # Report&#xA;├── mcu/                       # MCU specific code for CH32V003&#xA;├── modeldata/                 # Pre-trained models&#xA;│&#xA;├── BitNetMCU.py               # Pytorch model and QAT classes&#xA;├── BitNetMCU_inference.c      # C code for inference &#xA;├── BitNetMCU_inference.h      # Header file for C inference code&#xA;├── BitNetMCU_MNIST_test.c     # Test script for MNIST dataset&#xA;├── BitNetMCU_MNIST_test_data.h# MNIST test data in header format (generated)&#xA;├── BitNetMCU_model.h          # Model data in C header format (generated)&#xA;├── exportquant.py             # Script to convert trained model to quantized format&#xA;├── test_inference.py          # Script to test C implementation of inference&#xA;├── training.py                # Training script for the neural network&#xA;└── trainingparameters.yaml    # Configuration file for training parameters&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training Pipeline&lt;/h2&gt; &#xA;&lt;p&gt;The data pipeline is split into several Python scripts for flexibility:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Configuration&lt;/strong&gt;: Modify &lt;code&gt;trainingparameters.yaml&lt;/code&gt; to set all hyperparameters for training the model.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Training the Model&lt;/strong&gt;: The &lt;code&gt;training.py&lt;/code&gt; script is used to train the model and store it as a &lt;code&gt;.pth&lt;/code&gt; file in the &lt;code&gt;modeldata/&lt;/code&gt; folder. The model weights are still in float format at this stage, as they are quantized on-the-fly during training.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Exporting the Quantized Model&lt;/strong&gt;: The &lt;code&gt;exportquant.py&lt;/code&gt; script is used to convert the model into a quantized format. The quantized model is exported to the C header file &lt;code&gt;BitNetMCU_model.h&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Optional: Testing the C-Model&lt;/strong&gt;: Compile and execute &lt;code&gt;BitNetMCU_MNIST_test.c&lt;/code&gt; to test inference of ten digits. The model data is included from &lt;code&gt;BitNetMCU_MNIST_test_data.h&lt;/code&gt;, and the test data is included from the &lt;code&gt;BitNetMCU_MNIST_test_data.h&lt;/code&gt; file.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Optional: Verification C vs Python Model on full dataset&lt;/strong&gt;: The inference code, along with the model data, is compiled into a DLL. The &lt;code&gt;test-inference.py&lt;/code&gt; script calls the DLL and compares the results with the original Python model. This allows for an accurate comparison to the entire MNIST test data set of 10,000 images.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Optional: Testing inference on the MCU&lt;/strong&gt;: follow the instructions in &lt;code&gt;mcu/readme.md&lt;/code&gt;. Porting to architectures other than CH32V003 is straighfoward and the files in the &lt;code&gt;mcu&lt;/code&gt; directory can serve as a reference&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
</feed>