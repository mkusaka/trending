<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-07T01:31:46Z</updated>
  <subtitle>Daily Trending of C in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>marella/ctransformers</title>
    <updated>2023-06-07T01:31:46Z</updated>
    <id>tag:github.com,2023-06-07:/marella/ctransformers</id>
    <link href="https://github.com/marella/ctransformers" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Python bindings for the Transformer models implemented in C/C++ using GGML library.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;a href=&#34;https://github.com/marella/ctransformers&#34;&gt;C Transformers&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/ctransformers/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/ctransformers&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/marella/ctransformers/actions/workflows/tests.yml&#34;&gt;&lt;img src=&#34;https://github.com/marella/ctransformers/actions/workflows/tests.yml/badge.svg?sanitize=true&#34; alt=&#34;tests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/marella/ctransformers/actions/workflows/build.yml&#34;&gt;&lt;img src=&#34;https://github.com/marella/ctransformers/actions/workflows/build.yml/badge.svg?sanitize=true&#34; alt=&#34;build&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;Python bindings for the Transformer models implemented in C/C++ using &lt;a href=&#34;https://github.com/ggerganov/ggml&#34;&gt;GGML&lt;/a&gt; library.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Also see &lt;a href=&#34;https://github.com/marella/chatdocs&#34;&gt;ChatDocs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/marella/ctransformers/main/#supported-models&#34;&gt;Supported Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/marella/ctransformers/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/marella/ctransformers/main/#usage&#34;&gt;Usage&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/marella/ctransformers/main/#hugging-face-hub&#34;&gt;Hugging Face Hub&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/marella/ctransformers/main/#langchain&#34;&gt;LangChain&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/marella/ctransformers/main/#gpu&#34;&gt;GPU&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/marella/ctransformers/main/#documentation&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/marella/ctransformers/main/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Supported Models&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Models&lt;/th&gt; &#xA;   &lt;th&gt;Model Type&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;GPT-2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;gpt2&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;GPT-J, GPT4All-J&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;gptj&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;GPT-NeoX, StableLM&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;gpt_neox&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;LLaMA&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;llama&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;MPT&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;mpt&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Dolly V2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;dolly-v2&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;StarCoder&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;starcoder&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;More models coming soon.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install ctransformers&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For GPU (CUDA) support, set environment variable &lt;code&gt;CT_CUBLAS=1&lt;/code&gt; and install from source using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;CT_CUBLAS=1 pip install ctransformers --no-binary ctransformers&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;strong&gt;Show commands for Windows&lt;/strong&gt;&lt;/summary&gt;&#xA; &lt;br&gt; &#xA; &lt;p&gt;On Windows PowerShell run:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$env:CT_CUBLAS=1&#xA;pip install ctransformers --no-binary ctransformers&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;On Windows Command Prompt run:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;set CT_CUBLAS=1&#xA;pip install ctransformers --no-binary ctransformers&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;It provides a unified interface for all models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;from ctransformers import AutoModelForCausalLM&#xA;&#xA;llm = AutoModelForCausalLM.from_pretrained(&#39;/path/to/ggml-gpt-2.bin&#39;, model_type=&#39;gpt2&#39;)&#xA;&#xA;print(llm(&#39;AI is going to&#39;))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1GMhYMUAv_TyZkpfvUI1NirM8-9mCXQyL&#34;&gt;Run in Google Colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you are getting &lt;code&gt;illegal instruction&lt;/code&gt; error, try using &lt;code&gt;lib=&#39;avx&#39;&lt;/code&gt; or &lt;code&gt;lib=&#39;basic&#39;&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;llm = AutoModelForCausalLM.from_pretrained(&#39;/path/to/ggml-gpt-2.bin&#39;, model_type=&#39;gpt2&#39;, lib=&#39;avx&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It provides a generator interface for more control:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;tokens = llm.tokenize(&#39;AI is going to&#39;)&#xA;&#xA;for token in llm.generate(tokens):&#xA;    print(llm.detokenize(token))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It can be used with a custom or Hugging Face tokenizer:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;from transformers import AutoTokenizer&#xA;&#xA;tokenizer = AutoTokenizer.from_pretrained(&#39;gpt2&#39;)&#xA;&#xA;tokens = tokenizer.encode(&#39;AI is going to&#39;)&#xA;&#xA;for token in llm.generate(tokens):&#xA;    print(tokenizer.decode(token))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It also provides access to the low-level C API. See &lt;a href=&#34;https://raw.githubusercontent.com/marella/ctransformers/main/#documentation&#34;&gt;Documentation&lt;/a&gt; section below.&lt;/p&gt; &#xA;&lt;h3&gt;Hugging Face Hub&lt;/h3&gt; &#xA;&lt;p&gt;It can be used with models hosted on the Hub:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;llm = AutoModelForCausalLM.from_pretrained(&#39;marella/gpt-2-ggml&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If a model repo has multiple model files (&lt;code&gt;.bin&lt;/code&gt; files), specify a model file using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;llm = AutoModelForCausalLM.from_pretrained(&#39;marella/gpt-2-ggml&#39;, model_file=&#39;ggml-model.bin&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It can be used with your own models uploaded on the Hub. For better user experience, upload only one model per repo.&lt;/p&gt; &#xA;&lt;p&gt;To use it with your own model, add &lt;code&gt;config.json&lt;/code&gt; file to your model repo specifying the &lt;code&gt;model_type&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;model_type&#34;: &#34;gpt2&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also specify additional parameters under &lt;code&gt;task_specific_params.text-generation&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://huggingface.co/marella/gpt-2-ggml/blob/main/config.json&#34;&gt;marella/gpt-2-ggml&lt;/a&gt; for a minimal example and &lt;a href=&#34;https://huggingface.co/marella/gpt-2-ggml-example/blob/main/config.json&#34;&gt;marella/gpt-2-ggml-example&lt;/a&gt; for a full example.&lt;/p&gt; &#xA;&lt;h3&gt;LangChain&lt;/h3&gt; &#xA;&lt;p&gt;It is integrated into LangChain. See &lt;a href=&#34;https://python.langchain.com/en/latest/integrations/ctransformers.html&#34;&gt;LangChain docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;GPU&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Currently only LLaMA models have GPU support.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;To run some of the model layers on GPU, set the &lt;code&gt;gpu_layers&lt;/code&gt; parameter:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;llm = AutoModelForCausalLM.from_pretrained(&#39;/path/to/ggml-llama.bin&#39;, model_type=&#39;llama&#39;, gpu_layers=50)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1Ihn7iPCYiqlTotpkqa1tOhUIpJBrJ1Tp&#34;&gt;Run in Google Colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;!-- API_DOCS --&gt; &#xA;&lt;h3&gt;Config&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Parameter&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Type&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Default&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;top_k&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;The top-k value to use for sampling.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;40&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;top_p&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;float&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;The top-p value to use for sampling.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;0.95&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;temperature&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;float&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;The temperature to use for sampling.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;0.8&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;repetition_penalty&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;float&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;The repetition penalty to use for sampling.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;1.1&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;last_n_tokens&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;The number of last tokens to use for repetition penalty.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;64&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;seed&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;The seed value to use for sampling tokens.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;-1&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;max_new_tokens&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;The maximum number of new tokens to generate.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;256&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;stop&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;List[str]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;A list of sequences to stop generation when encountered.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;stream&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;bool&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Whether to stream the generated text.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;reset&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;bool&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Whether to reset the model state before generating text.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;True&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;batch_size&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;The batch size to use for evaluating tokens.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;8&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;threads&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;The number of threads to use for evaluating tokens.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;-1&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;context_length&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;The maximum context length to use.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;-1&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;gpu_layers&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;The number of layers to run on GPU.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;0&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Currently only LLaMA models support the &lt;code&gt;context_length&lt;/code&gt; and &lt;code&gt;gpu_layers&lt;/code&gt; parameters.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;&lt;kbd&gt;class&lt;/kbd&gt; &lt;code&gt;AutoModelForCausalLM&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;&lt;kbd&gt;classmethod&lt;/kbd&gt; &lt;code&gt;AutoModelForCausalLM.from_pretrained&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from_pretrained(&#xA;    model_path_or_repo_id: str,&#xA;    model_type: Optional[str] = None,&#xA;    model_file: Optional[str] = None,&#xA;    config: Optional[ctransformers.hub.AutoConfig] = None,&#xA;    lib: Optional[str] = None,&#xA;    local_files_only: bool = False,&#xA;    **kwargs&#xA;) → LLM&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Loads the language model from a local file or remote repo.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Args:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;model_path_or_repo_id&lt;/code&gt;&lt;/b&gt;: The path to a model file or directory or the name of a Hugging Face Hub model repo.&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;model_type&lt;/code&gt;&lt;/b&gt;: The model type.&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;model_file&lt;/code&gt;&lt;/b&gt;: The name of the model file in repo or directory.&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;config&lt;/code&gt;&lt;/b&gt;: &lt;code&gt;AutoConfig&lt;/code&gt; object.&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;lib&lt;/code&gt;&lt;/b&gt;: The path to a shared library or one of &lt;code&gt;avx2&lt;/code&gt;, &lt;code&gt;avx&lt;/code&gt;, &lt;code&gt;basic&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;local_files_only&lt;/code&gt;&lt;/b&gt;: Whether or not to only look at local files (i.e., do not try to download the model).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Returns:&lt;/strong&gt; &lt;code&gt;LLM&lt;/code&gt; object.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;kbd&gt;class&lt;/kbd&gt; &lt;code&gt;LLM&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;&lt;kbd&gt;method&lt;/kbd&gt; &lt;code&gt;LLM.__init__&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;__init__(&#xA;    model_path: str,&#xA;    model_type: str,&#xA;    config: Optional[ctransformers.llm.Config] = None,&#xA;    lib: Optional[str] = None&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Loads the language model from a local file.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Args:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;model_path&lt;/code&gt;&lt;/b&gt;: The path to a model file.&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;model_type&lt;/code&gt;&lt;/b&gt;: The model type.&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;config&lt;/code&gt;&lt;/b&gt;: &lt;code&gt;Config&lt;/code&gt; object.&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;lib&lt;/code&gt;&lt;/b&gt;: The path to a shared library or one of &lt;code&gt;avx2&lt;/code&gt;, &lt;code&gt;avx&lt;/code&gt;, &lt;code&gt;basic&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;&lt;kbd&gt;property&lt;/kbd&gt; LLM.config&lt;/h5&gt; &#xA;&lt;p&gt;The config object.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;&lt;kbd&gt;property&lt;/kbd&gt; LLM.embeddings&lt;/h5&gt; &#xA;&lt;p&gt;The input embeddings.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;&lt;kbd&gt;property&lt;/kbd&gt; LLM.logits&lt;/h5&gt; &#xA;&lt;p&gt;The unnormalized log probabilities.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;&lt;kbd&gt;property&lt;/kbd&gt; LLM.model_path&lt;/h5&gt; &#xA;&lt;p&gt;The path to the model file.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;&lt;kbd&gt;property&lt;/kbd&gt; LLM.model_type&lt;/h5&gt; &#xA;&lt;p&gt;The model type.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;&lt;kbd&gt;method&lt;/kbd&gt; &lt;code&gt;LLM.detokenize&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;detokenize(tokens: Sequence[int]) → str&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Converts a list of tokens to text.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Args:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;tokens&lt;/code&gt;&lt;/b&gt;: The list of tokens.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Returns:&lt;/strong&gt; The combined text of all tokens.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;&lt;kbd&gt;method&lt;/kbd&gt; &lt;code&gt;LLM.embed&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;embed(&#xA;    input: Union[str, Sequence[int]],&#xA;    batch_size: Optional[int] = None,&#xA;    threads: Optional[int] = None&#xA;) → List[float]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Computes embeddings for a text or list of tokens.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Currently only LLaMA models support embeddings.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;Args:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;input&lt;/code&gt;&lt;/b&gt;: The input text or list of tokens to get embeddings for.&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;batch_size&lt;/code&gt;&lt;/b&gt;: The batch size to use for evaluating tokens. Default: &lt;code&gt;8&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;threads&lt;/code&gt;&lt;/b&gt;: The number of threads to use for evaluating tokens. Default: &lt;code&gt;-1&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Returns:&lt;/strong&gt; The input embeddings.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;&lt;kbd&gt;method&lt;/kbd&gt; &lt;code&gt;LLM.eval&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;eval(&#xA;    tokens: Sequence[int],&#xA;    batch_size: Optional[int] = None,&#xA;    threads: Optional[int] = None&#xA;) → None&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Evaluates a list of tokens.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Args:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;tokens&lt;/code&gt;&lt;/b&gt;: The list of tokens to evaluate.&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;batch_size&lt;/code&gt;&lt;/b&gt;: The batch size to use for evaluating tokens. Default: &lt;code&gt;8&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;threads&lt;/code&gt;&lt;/b&gt;: The number of threads to use for evaluating tokens. Default: &lt;code&gt;-1&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;&lt;kbd&gt;method&lt;/kbd&gt; &lt;code&gt;LLM.generate&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;generate(&#xA;    tokens: Sequence[int],&#xA;    top_k: Optional[int] = None,&#xA;    top_p: Optional[float] = None,&#xA;    temperature: Optional[float] = None,&#xA;    repetition_penalty: Optional[float] = None,&#xA;    last_n_tokens: Optional[int] = None,&#xA;    seed: Optional[int] = None,&#xA;    batch_size: Optional[int] = None,&#xA;    threads: Optional[int] = None,&#xA;    reset: Optional[bool] = None&#xA;) → Generator[int, NoneType, NoneType]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Generates new tokens from a list of tokens.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Args:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;tokens&lt;/code&gt;&lt;/b&gt;: The list of tokens to generate tokens from.&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;top_k&lt;/code&gt;&lt;/b&gt;: The top-k value to use for sampling. Default: &lt;code&gt;40&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;top_p&lt;/code&gt;&lt;/b&gt;: The top-p value to use for sampling. Default: &lt;code&gt;0.95&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;temperature&lt;/code&gt;&lt;/b&gt;: The temperature to use for sampling. Default: &lt;code&gt;0.8&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;repetition_penalty&lt;/code&gt;&lt;/b&gt;: The repetition penalty to use for sampling. Default: &lt;code&gt;1.1&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;last_n_tokens&lt;/code&gt;&lt;/b&gt;: The number of last tokens to use for repetition penalty. Default: &lt;code&gt;64&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;seed&lt;/code&gt;&lt;/b&gt;: The seed value to use for sampling tokens. Default: &lt;code&gt;-1&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;batch_size&lt;/code&gt;&lt;/b&gt;: The batch size to use for evaluating tokens. Default: &lt;code&gt;8&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;threads&lt;/code&gt;&lt;/b&gt;: The number of threads to use for evaluating tokens. Default: &lt;code&gt;-1&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;reset&lt;/code&gt;&lt;/b&gt;: Whether to reset the model state before generating text. Default: &lt;code&gt;True&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Returns:&lt;/strong&gt; The generated tokens.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;&lt;kbd&gt;method&lt;/kbd&gt; &lt;code&gt;LLM.is_eos_token&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;is_eos_token(token: int) → bool&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Checks if a token is an end-of-sequence token.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Args:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;token&lt;/code&gt;&lt;/b&gt;: The token to check.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Returns:&lt;/strong&gt; &lt;code&gt;True&lt;/code&gt; if the token is an end-of-sequence token else &lt;code&gt;False&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;&lt;kbd&gt;method&lt;/kbd&gt; &lt;code&gt;LLM.reset&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;reset() → None&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Resets the model state.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;&lt;kbd&gt;method&lt;/kbd&gt; &lt;code&gt;LLM.sample&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sample(&#xA;    top_k: Optional[int] = None,&#xA;    top_p: Optional[float] = None,&#xA;    temperature: Optional[float] = None,&#xA;    repetition_penalty: Optional[float] = None,&#xA;    last_n_tokens: Optional[int] = None,&#xA;    seed: Optional[int] = None&#xA;) → int&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Samples a token from the model.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Args:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;top_k&lt;/code&gt;&lt;/b&gt;: The top-k value to use for sampling. Default: &lt;code&gt;40&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;top_p&lt;/code&gt;&lt;/b&gt;: The top-p value to use for sampling. Default: &lt;code&gt;0.95&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;temperature&lt;/code&gt;&lt;/b&gt;: The temperature to use for sampling. Default: &lt;code&gt;0.8&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;repetition_penalty&lt;/code&gt;&lt;/b&gt;: The repetition penalty to use for sampling. Default: &lt;code&gt;1.1&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;last_n_tokens&lt;/code&gt;&lt;/b&gt;: The number of last tokens to use for repetition penalty. Default: &lt;code&gt;64&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;seed&lt;/code&gt;&lt;/b&gt;: The seed value to use for sampling tokens. Default: &lt;code&gt;-1&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Returns:&lt;/strong&gt; The sampled token.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;&lt;kbd&gt;method&lt;/kbd&gt; &lt;code&gt;LLM.tokenize&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tokenize(text: str) → List[int]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Converts a text into list of tokens.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Args:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;text&lt;/code&gt;&lt;/b&gt;: The text to tokenize.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Returns:&lt;/strong&gt; The list of tokens.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;&lt;kbd&gt;method&lt;/kbd&gt; &lt;code&gt;LLM.__call__&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;__call__(&#xA;    prompt: str,&#xA;    max_new_tokens: Optional[int] = None,&#xA;    top_k: Optional[int] = None,&#xA;    top_p: Optional[float] = None,&#xA;    temperature: Optional[float] = None,&#xA;    repetition_penalty: Optional[float] = None,&#xA;    last_n_tokens: Optional[int] = None,&#xA;    seed: Optional[int] = None,&#xA;    batch_size: Optional[int] = None,&#xA;    threads: Optional[int] = None,&#xA;    stop: Optional[Sequence[str]] = None,&#xA;    stream: Optional[bool] = None,&#xA;    reset: Optional[bool] = None&#xA;) → Union[str, Generator[str, NoneType, NoneType]]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Generates text from a prompt.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Args:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;prompt&lt;/code&gt;&lt;/b&gt;: The prompt to generate text from.&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;max_new_tokens&lt;/code&gt;&lt;/b&gt;: The maximum number of new tokens to generate. Default: &lt;code&gt;256&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;top_k&lt;/code&gt;&lt;/b&gt;: The top-k value to use for sampling. Default: &lt;code&gt;40&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;top_p&lt;/code&gt;&lt;/b&gt;: The top-p value to use for sampling. Default: &lt;code&gt;0.95&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;temperature&lt;/code&gt;&lt;/b&gt;: The temperature to use for sampling. Default: &lt;code&gt;0.8&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;repetition_penalty&lt;/code&gt;&lt;/b&gt;: The repetition penalty to use for sampling. Default: &lt;code&gt;1.1&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;last_n_tokens&lt;/code&gt;&lt;/b&gt;: The number of last tokens to use for repetition penalty. Default: &lt;code&gt;64&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;seed&lt;/code&gt;&lt;/b&gt;: The seed value to use for sampling tokens. Default: &lt;code&gt;-1&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;batch_size&lt;/code&gt;&lt;/b&gt;: The batch size to use for evaluating tokens. Default: &lt;code&gt;8&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;threads&lt;/code&gt;&lt;/b&gt;: The number of threads to use for evaluating tokens. Default: &lt;code&gt;-1&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;stop&lt;/code&gt;&lt;/b&gt;: A list of sequences to stop generation when encountered. Default: &lt;code&gt;None&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;stream&lt;/code&gt;&lt;/b&gt;: Whether to stream the generated text. Default: &lt;code&gt;False&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;reset&lt;/code&gt;&lt;/b&gt;: Whether to reset the model state before generating text. Default: &lt;code&gt;True&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Returns:&lt;/strong&gt; The generated text.&lt;/p&gt; &#xA;&lt;!-- API_DOCS --&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/marella/ctransformers/raw/main/LICENSE&#34;&gt;MIT&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>OpenDriver2/REDRIVER2</title>
    <updated>2023-06-07T01:31:46Z</updated>
    <id>tag:github.com,2023-06-07:/OpenDriver2/REDRIVER2</id>
    <link href="https://github.com/OpenDriver2/REDRIVER2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Driver 2 Playstation game reverse engineering effort&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;REDRIVER2 (Reverse-Engineered Driver 2)&lt;/h1&gt; &#xA;&lt;p&gt;Game running on Windows:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.ibb.co/2q1pp06/red2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Game running in Firefox Web Browser:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.ibb.co/JxfC5xX/aaa.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Game running on Playstation (emulated)&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.ibb.co/ydLsK9z/aaa.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Build status (Windows and Linux): &lt;a href=&#34;https://ci.appveyor.com/project/SoapyMan/redriver2-10jm8/branch/master&#34;&gt;&lt;img src=&#34;https://ci.appveyor.com/api/projects/status/9abepvls6jexapqy/branch/master?svg=true&#34; alt=&#34;Build status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Information&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This is an original game - it was carefully and completely disassembled and translated from MIPS back to C (except Memory Card menu) - &lt;em&gt;&lt;strong&gt;no emulation involved&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;Game has been &lt;em&gt;significantly improved over the original Playstation version&lt;/em&gt;, original bugs were fixed and completed some unfinished gameplay aspects&lt;/li&gt; &#xA; &lt;li&gt;It now runs on Windows, Linux and soon to be on the other platforms (including backport to Playstation) and utilizes &lt;a href=&#34;https://github.com/OpenDriver2/REDRIVER2/tree/master/src_rebuild/PsyX&#34;&gt;Psy-X (Psy-Cross) (formely extended TOMB5 emulator)&lt;/a&gt; as a layer for porting from Playstation&lt;/li&gt; &#xA; &lt;li&gt;Basic modding support - with textures and models replacement provided with &lt;a href=&#34;https://github.com/OpenDriver2/OpenDriver2Tools&#34;&gt;OpenDriver2Tools&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;How is it done?&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;All information (variables, types and function names) have been obtained from debugging symbols (.SYM), driver_psx_level and DLE&lt;/li&gt; &#xA; &lt;li&gt;Ghidra project (based on Italian SYM) to deal with overlays, simple code complex branching - semi-auto decompilation&lt;/li&gt; &#xA; &lt;li&gt;IDB based on Spanish SYM when dealing with things Ghidra can&#39;t handle properly - manual decompilation (mostly GTE code)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;History:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Feb 19, 2019 - created skeleton, one year long preparations&lt;/li&gt; &#xA; &lt;li&gt;Mar 28, 2020 - started work on reimplementing the game, game intro running&lt;/li&gt; &#xA; &lt;li&gt;Sep 19, 2020 - reimplementing game complete - game is fully playable&lt;/li&gt; &#xA; &lt;li&gt;Jan 15, 2021 - refactoring complete&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;How to use&lt;/h3&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/OpenDriver2/REDRIVER2/wiki/Installation-instructions&#34;&gt;Wiki/Installation Instructions&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;How can I contribute?&lt;/h3&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/OpenDriver2/REDRIVER2/wiki/Contributing-to-project&#34;&gt;Contributing to project&lt;/a&gt; page&lt;/p&gt; &#xA;&lt;h3&gt;What&#39;s next&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;strong&gt;OpenDriver2&lt;/strong&gt;&lt;/em&gt; project - complete rewrite with lots of new features such as &lt;em&gt;new renderer and sound engine, Lua scripting, Driver 1 content support&lt;/em&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Credits&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;SoapyMan&lt;/strong&gt; - lead reverse engineer and programmer&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fireboyd78&lt;/strong&gt; - code refactoring and improvements&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Krishty, someone972&lt;/strong&gt; - early formats decoding&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Gh0stBlade&lt;/strong&gt; - HLE Emulator code used as a base for Psy-Cross &lt;a href=&#34;https://github.com/TOMB5/TOMB5/tree/master/EMULATOR&#34;&gt;(link)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Ben Lincoln&lt;/strong&gt; - &lt;a href=&#34;https://www.beneaththewaves.net/Software/This_Dust_Remembers_What_It_Once_Was.html&#34;&gt;This Dust Remembers What It Once Was&lt;/a&gt; (&lt;em&gt;TDR&lt;/em&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Stohrendorf&lt;/strong&gt; - &lt;a href=&#34;https://github.com/stohrendorf/symdump&#34;&gt;Symdump&lt;/a&gt; utility&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>Forairaaaaa/monica</title>
    <updated>2023-06-07T01:31:46Z</updated>
    <id>tag:github.com,2023-06-07:/Forairaaaaa/monica</id>
    <link href="https://github.com/Forairaaaaa/monica" rel="alternate"></link>
    <summary type="html">&lt;p&gt;DIY Watch based on ESP32-S3 and Amoled screen&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Monica&lt;/h1&gt; &#xA;&lt;p&gt;视频介绍：&lt;a href=&#34;https://www.bilibili.com/video/BV1AP411Q7dk&#34;&gt;https://www.bilibili.com/video/BV1AP411Q7dk&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;硬件开源：&lt;a href=&#34;https://oshwhub.com/eedadada/monica&#34;&gt;https://oshwhub.com/eedadada/monica&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/Forairaaaaa/monica/raw/main/Pics/cover.JPG?raw=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;原理图&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/Forairaaaaa/monica/raw/main/Pics/hardware1.png?raw=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;主控&lt;/strong&gt;：ESP32-S3，32M 外置 Flash&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;IMU&lt;/strong&gt;：BMI270 + BM150 （抬手唤醒、计步器、指南针）&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;电源管理&lt;/strong&gt;：AXP2101&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;其他&lt;/strong&gt;：SD卡槽、RTC、蜂鸣器、MIC、气压传感、按键&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://item.taobao.com/item.htm?spm=a1z09.2.0.0.a41c2e8dNZ5GOq&amp;amp;id=688638221390&#34;&gt;&lt;strong&gt;屏幕&lt;/strong&gt;&lt;/a&gt;：1.8寸 Amoled 368*448&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://detail.tmall.com/item.htm?_u=42bdtj0f19f8&amp;amp;id=632947661144&#34;&gt;电池&lt;/a&gt;&lt;/strong&gt;：402728 400毫安&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;PCB&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/Forairaaaaa/monica/raw/main/Pics/hardware2.png?raw=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;单面四层0.8MM，元件比较密集&lt;/li&gt; &#xA; &lt;li&gt;1.27MM 母座用于下载和 USB 连接&lt;/li&gt; &#xA; &lt;li&gt;引出串口和 I2C 接口等，预留用于底板拓展&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;程序&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/Forairaaaaa/monica/raw/main/Pics/firmware.png?raw=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;应用、驱动层解耦&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Forairaaaaa/simplekv.git&#34;&gt;&lt;strong&gt;SimpleKV&lt;/strong&gt;&lt;/a&gt;：简单的 &lt;code&gt;Key-Value (键-值)&lt;/code&gt; 内存数据库框架，实现两层的数据交互。使用 &lt;code&gt;键&lt;/code&gt; (字符串) 索引数据，切断应用层和驱动层的数据耦合。&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lvgl/lvgl.git&#34;&gt;&lt;strong&gt;Lvgl&lt;/strong&gt;&lt;/a&gt;： 使用 Lvgl 作上层应用默认图形库，及其文件系统接口&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;因此，只要设备适配 Lvgl ，就可以直接使用这套 &lt;a href=&#34;https://github.com/Forairaaaaa/mooncake.git&#34;&gt;Mooncake&lt;/a&gt; UI框架，如 Linux ：&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/Forairaaaaa/monica/raw/main/Pics/firmware2.png?raw=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;驱动层&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;HAL&lt;/strong&gt;：外设驱动的对象抽象&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hardware Manager&lt;/strong&gt;：硬件管理层。负责底层的 &lt;code&gt;硬件初始化&lt;/code&gt;、运行时的 &lt;code&gt;状态维护更新&lt;/code&gt;，例如自动睡眠、抬手唤醒、计步器更新等&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;USB MSC 模式&lt;/strong&gt;：为了读写 SD 卡方便，同时按 &lt;code&gt;电源&lt;/code&gt; 和 &lt;code&gt;下&lt;/code&gt; 按键启动时，会进入 &lt;code&gt;USB MSC 模式&lt;/code&gt;，SD 卡会被挂载成 U盘，接上 USB 即可直接读写&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;应用层&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;App 基类、安装卸载&lt;/strong&gt;框架：所有 App 的对象基类，描述了 App 的基础框架。安装卸载框架实现简单的 App 列表管理&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;App 生命周期管理&lt;/strong&gt;：支持前后台的 App 运行生命周期 &lt;code&gt;调度器&lt;/code&gt; (单线程，单前台，多后台) ，使用 &lt;code&gt;状态机&lt;/code&gt; (FSM)实现&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://github.com/Forairaaaaa/monica/raw/main/Pics/firmware3.png?raw=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;系统内置 APP&lt;/strong&gt;：都是在 App 层之上实现的应用，但 &lt;code&gt;启动动画&lt;/code&gt; 和 &lt;code&gt;启动器&lt;/code&gt; 会被 &lt;code&gt;Framwaork&lt;/code&gt; 框架特殊调用&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;用户自定义表盘&lt;/h4&gt; &#xA;&lt;p&gt;&lt;strong&gt;watch_faces&lt;/strong&gt; 目录下的每一个文件夹都视作一个&lt;code&gt;表盘&lt;/code&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;background&lt;/strong&gt; 目录：存放表盘背景资源 (png、gif)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;number&lt;/strong&gt; 目录：存放从0~9的时间数字资源 (png)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;face.json&lt;/strong&gt; ：资源描述文件，如时间数字坐标等&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;watch_faces/&#xA;|&#xA;├── AyanamiRei&#xA;│&amp;nbsp;&amp;nbsp; ├── background&#xA;│&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; ├── background.png&#xA;│&amp;nbsp;&amp;nbsp; ├── face.json&#xA;│&amp;nbsp;&amp;nbsp; └── number&#xA;│&amp;nbsp;&amp;nbsp;     ├── 0.png&#xA;│&amp;nbsp;&amp;nbsp;     ├── ~&#xA;│&amp;nbsp;&amp;nbsp;     └── 9.png&#xA;|&#xA;├── FloatingNum&#xA;│&amp;nbsp;&amp;nbsp; ├── background&#xA;│&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; └── background.png&#xA;│&amp;nbsp;&amp;nbsp; ├── face.json&#xA;│&amp;nbsp;&amp;nbsp; └── number&#xA;│&amp;nbsp;&amp;nbsp;     ├── 0.png&#xA;│&amp;nbsp;&amp;nbsp;     ├── ~&#xA;│&amp;nbsp;&amp;nbsp;     └── 9.png&#xA;|&#xA;└── PixelThinking&#xA;  ├── background&#xA;  │&amp;nbsp;&amp;nbsp; ├── background.png&#xA;  ├── face.json&#xA;  └── number&#xA;      ├── 0.png&#xA;      ├── ~&#xA;      └── 9.png&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;模型&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/Forairaaaaa/monica/raw/main/Pics/model.png?raw=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/Forairaaaaa/monica/raw/main/Pics/model2.png?raw=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;存在问题：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;目前模型只能选激光烧结，树脂打印会有缝&lt;/li&gt; &#xA; &lt;li&gt;按键位置有偏移，需要按靠上部位&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/Forairaaaaa/monica/raw/main/Pics/monica2.mp4_20230604_001454.009.png?raw=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/Forairaaaaa/monica/raw/main/Pics/monica2.mp4_20230604_001528.106.png?raw=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/Forairaaaaa/monica/raw/main/Pics/monica2.mp4_20230604_001548.448.png?raw=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/Forairaaaaa/monica/raw/main/Pics/monica2.mp4_20230604_001608.565.png?raw=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/Forairaaaaa/monica/raw/main/Pics/monica2.mp4_20230604_001620.656.png?raw=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/Forairaaaaa/monica/raw/main/Pics/monica2.mp4_20230604_001711.494.png?raw=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt;</summary>
  </entry>
</feed>