<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-21T01:30:56Z</updated>
  <subtitle>Daily Trending of C in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>jmorganca/ollama</title>
    <updated>2023-07-21T01:30:56Z</updated>
    <id>tag:github.com,2023-07-21:/jmorganca/ollama</id>
    <link href="https://github.com/jmorganca/ollama" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Run and package large language models on macOS&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; height=&#34;200px&#34; srcset=&#34;https://github.com/jmorganca/ollama/assets/3325447/56ea1849-1284-4645-8970-956de6e51c3c&#34;&gt; &#xA;  &lt;img alt=&#34;logo&#34; height=&#34;200px&#34; src=&#34;https://github.com/jmorganca/ollama/assets/3325447/0d0b44e2-8f4a-4e99-9b52-a5c1c741c8f7&#34;&gt; &#xA; &lt;/picture&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;Ollama&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/ollama&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/ollama?style=flat&amp;amp;compact=true&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: Ollama is in early preview. Please report any issues you find.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Run, create, and share large language models (LLMs).&lt;/p&gt; &#xA;&lt;h2&gt;Download&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ollama.ai/download&#34;&gt;Download&lt;/a&gt; for macOS on Apple Silicon (Intel coming soon)&lt;/li&gt; &#xA; &lt;li&gt;Download for Windows and Linux (coming soon)&lt;/li&gt; &#xA; &lt;li&gt;Build &lt;a href=&#34;https://raw.githubusercontent.com/jmorganca/ollama/main/#building&#34;&gt;from source&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;To run and chat with &lt;a href=&#34;https://ai.meta.com/llama&#34;&gt;Llama 2&lt;/a&gt;, the new model by Meta:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ollama run llama2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Model library&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;ollama&lt;/code&gt; includes a library of open-source models:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Parameters&lt;/th&gt; &#xA;   &lt;th&gt;Size&lt;/th&gt; &#xA;   &lt;th&gt;Download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama2&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;3.8GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama pull llama2&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama2 13B&lt;/td&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;7.3GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama pull llama2:13b&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Orca Mini&lt;/td&gt; &#xA;   &lt;td&gt;3B&lt;/td&gt; &#xA;   &lt;td&gt;1.9GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama pull orca&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Vicuna&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;3.8GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama pull vicuna&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Nous-Hermes&lt;/td&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;7.3GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama pull nous-hermes&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Wizard Vicuna Uncensored&lt;/td&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;7.3GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama pull wizard-vicuna&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: You should have at least 8 GB of RAM to run the 3B models, 16 GB to run the 7B models, and 32 GB to run the 13B models.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;h3&gt;Run a model&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;ollama run llama2&#xA;&amp;gt;&amp;gt;&amp;gt; hi&#xA;Hello! How can I help you today?&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Create a custom model&lt;/h3&gt; &#xA;&lt;p&gt;Pull a base model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ollama pull llama2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Create a &lt;code&gt;Modelfile&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;FROM llama2&#xA;&#xA;# set the temperature to 1 [higher is more creative, lower is more coherent]&#xA;PARAMETER temperature 1&#xA;&#xA;# set the system prompt&#xA;SYSTEM &#34;&#34;&#34;&#xA;You are Mario from Super Mario Bros. Answer as Mario, the assistant, only.&#xA;&#34;&#34;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, create and run the model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ollama create mario -f ./Modelfile&#xA;ollama run mario&#xA;&amp;gt;&amp;gt;&amp;gt; hi&#xA;Hello! It&#39;s your friend Mario.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more examples, see the &lt;a href=&#34;https://raw.githubusercontent.com/jmorganca/ollama/main/examples&#34;&gt;examples&lt;/a&gt; directory.&lt;/p&gt; &#xA;&lt;h3&gt;Pull a model from the registry&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;ollama pull orca&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Listing local models&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;ollama list&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Model packages&lt;/h2&gt; &#xA;&lt;h3&gt;Overview&lt;/h3&gt; &#xA;&lt;p&gt;Ollama bundles model weights, configuration, and data into a single package, defined by a &lt;a href=&#34;https://raw.githubusercontent.com/jmorganca/ollama/main/docs/modelfile.md&#34;&gt;Modelfile&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;picture&gt; &#xA; &lt;source media=&#34;(prefers-color-scheme: dark)&#34; height=&#34;480&#34; srcset=&#34;https://github.com/jmorganca/ollama/assets/251292/2fd96b5f-191b-45c1-9668-941cfad4eb70&#34;&gt; &#xA; &lt;img alt=&#34;logo&#34; height=&#34;480&#34; src=&#34;https://github.com/jmorganca/ollama/assets/251292/2fd96b5f-191b-45c1-9668-941cfad4eb70&#34;&gt; &#xA;&lt;/picture&gt; &#xA;&lt;h2&gt;Building&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;go build .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run it start the server:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./ollama serve &amp;amp;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, run a model!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./ollama run llama2&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>