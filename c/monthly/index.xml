<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-01T02:11:34Z</updated>
  <subtitle>Monthly Trending of C in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>RT-Thread/rt-thread</title>
    <updated>2023-05-01T02:11:34Z</updated>
    <id>tag:github.com,2023-05-01:/RT-Thread/rt-thread</id>
    <link href="https://github.com/RT-Thread/rt-thread" rel="alternate"></link>
    <summary type="html">&lt;p&gt;RT-Thread is an open source IoT real-time operating system (RTOS).&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/RT-Thread/rt-thread/master/documentation/figures/logo.png&#34; width=&#34;60%&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;English&lt;/strong&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/RT-Thread/rt-thread/master/README_zh.md&#34;&gt;中文&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/RT-Thread/rt-thread/master/README_es.md&#34;&gt;Español&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/RT-Thread/rt-thread/master/README_de.md&#34;&gt;Deutsch&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/RT-Thread/rt-thread/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/RT-Thread/rt-thread?style=flat-square&amp;amp;logo=GitHub&#34; alt=&#34;GitHubStars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitee.com/rtthread/rt-thread/stargazers&#34;&gt;&lt;img src=&#34;https://gitee.com/rtthread/rt-thread/badge/star.svg?theme=gvp&#34; alt=&#34;GiteeStars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/RT-Thread/rt-thread/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/RT-Thread/rt-thread.svg?sanitize=true&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/RT-Thread/rt-thread/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/release/RT-Thread/rt-thread.svg?sanitize=true&#34; alt=&#34;GitHub release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitter.im/RT-Thread/rt-thread?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/Join%20Chat.svg?sanitize=true&#34; alt=&#34;Gitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/RT-Thread/rt-thread/pulls&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-pr/RT-Thread/rt-thread.svg?sanitize=true&#34; alt=&#34;GitHub pull-requests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/RT-Thread/rt-thread/pulls&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat&#34; alt=&#34;PRs Welcome&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;RT-Thread&lt;/h1&gt; &#xA;&lt;p&gt;RT-Thread was born in 2006, it is an open source, neutral, and community-based real-time operating system (RTOS).&lt;/p&gt; &#xA;&lt;p&gt;RT-Thread is mainly written in C language, easy to understand and easy to port(can be quickly port to a wide range of mainstream MCUs and module chips). It applies object-oriented programming methods to real-time system design, making the code elegant, structured, modular, and very tailorable.&lt;/p&gt; &#xA;&lt;p&gt;RT-Thread has Standard version and Nano version. For resource-constrained microcontroller (MCU) systems, the Nano version that requires only 3KB Flash and 1.2KB RAM memory resources can be tailored with easy-to-use tools. For resource-rich IoT devices, RT-Thread can use the on-line software package management tool, together with system configuration tools, to achieve intuitive and rapid modular cutting, seamlessly import rich software packages; thus, achieving complex functions like Android&#39;s graphical interface and touch sliding effects, smart voice interaction effects, and so on.&lt;/p&gt; &#xA;&lt;h2&gt;RT-Thread Architecture&lt;/h2&gt; &#xA;&lt;p&gt;RT-Thread has not only a real-time kernel, but also rich components. Its architecture is as follows:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RT-Thread/rt-thread/master/documentation/figures/architecture.png&#34; alt=&#34;architecture&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;It includes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Kernel layer: RT-Thread kernel, the core part of RT-Thread, includes the implementation of objects in the kernel system, such as multi-threading and its scheduling, semaphore, mailbox, message queue, memory management, timer, etc.; libcpu/BSP (Chip Migration Related Files/Board Support Package) is closely related to hardware and consists of peripheral drivers and CPU porting.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Components and Service Layer: Components are based on upper-level software on top of the RT-Thread kernel, such as virtual file systems, FinSH command-line interfaces, network frameworks, device frameworks, and more. Its modular design allows for high internal cohesion inside the components and low coupling between components.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://packages.rt-thread.org/en/index.html&#34;&gt;RT-Thread software package&lt;/a&gt;: A general-purpose software component running on the RT-Thread IoT operating system platform for different application areas, consisting of description information, source code or library files. RT-Thread provides an open package platform with officially available or developer-supplied packages that provide developers with a choice of reusable packages that are an important part of the RT-Thread ecosystem. The package ecosystem is critical to the choice of an operating system because these packages are highly reusable and modular, making it easy for application developers to build the system they want in the shortest amount of time. RT-Thread supports 450+ software packages.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;RT-Thread Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Designed for resource-constrained devices, the minimum kernel requires only 1.2KB of RAM and 3 KB of Flash.&lt;/li&gt; &#xA; &lt;li&gt;A variety of standard interfaces, such as POSIX, CMSIS, C++ application environment.&lt;/li&gt; &#xA; &lt;li&gt;Has rich components and a prosperous and fast growing package ecosystem.&lt;/li&gt; &#xA; &lt;li&gt;Elegant code style, easy to use, read and master.&lt;/li&gt; &#xA; &lt;li&gt;High Scalability. RT-Thread has high-quality scalable software architecture, loose coupling, modularity, is easy to tailor and expand.&lt;/li&gt; &#xA; &lt;li&gt;Supports high-performance applications.&lt;/li&gt; &#xA; &lt;li&gt;Supports all mainstream compiling tools such as GCC, Keil and IAR.&lt;/li&gt; &#xA; &lt;li&gt;Supports a wide range of &lt;a href=&#34;https://www.rt-thread.io/board.html&#34;&gt;architectures and chips&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Code Catalogue&lt;/h2&gt; &#xA;&lt;p&gt;RT-Thread source code catalog is shown as follow:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BSP&lt;/td&gt; &#xA;   &lt;td&gt;Board Support Package based on the porting of various development boards&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;components&lt;/td&gt; &#xA;   &lt;td&gt;Components, such as finsh shell, file system, protocol stack etc.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;documentation&lt;/td&gt; &#xA;   &lt;td&gt;Related documents, like coding style, doxygen etc.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;examples&lt;/td&gt; &#xA;   &lt;td&gt;Related sample code&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;include&lt;/td&gt; &#xA;   &lt;td&gt;Head files of RT-Thread kernel&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;libcpu&lt;/td&gt; &#xA;   &lt;td&gt;CPU porting code such as ARM/MIPS/RISC-V etc.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;src&lt;/td&gt; &#xA;   &lt;td&gt;The source files for the RT-Thread kernel.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;tools&lt;/td&gt; &#xA;   &lt;td&gt;The script files for the RT-Thread command build tool.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;RT-Thread has now been ported for nearly 200 development boards, most BSPs support MDK, IAR development environment and GCC compiler, and have provided default MDK and IAR project, which allows users to add their own application code directly based on the project. Each BSP has a similar directory structure, and most BSPs provide a README.md file, which is a markdown-format file that contains the basic introduction of BSP, and introduces how to simply start using BSP.&lt;/p&gt; &#xA;&lt;h1&gt;Resources&lt;/h1&gt; &#xA;&lt;h2&gt;Supported Architectures&lt;/h2&gt; &#xA;&lt;p&gt;RT-Thread supports many architectures, and has covered the major architectures in current applications. Architecture and chip manufacturer involved:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;ARM Cortex-M0/M0+&lt;/strong&gt;：manufacturers like ST&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ARM Cortex-M3&lt;/strong&gt;：manufacturers like ST、Winner Micro、MindMotion, ect.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ARM Cortex-M4&lt;/strong&gt;：manufacturers like ST、Infineon、Nuvoton、NXP、&lt;a href=&#34;https://github.com/RT-Thread/rt-thread/tree/master/bsp/nrf5x&#34;&gt;Nordic&lt;/a&gt;、GigaDevice、Realtek、Ambiq Micro, ect.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ARM Cortex-M7&lt;/strong&gt;：manufacturers like ST、NXP&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ARM Cortex-M23&lt;/strong&gt;：manufacturers like GigaDevice&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ARM Cortex-M33&lt;/strong&gt;：manufacturers like ST&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ARM Cortex-R4&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ARM Cortex-A8/A9&lt;/strong&gt;：manufacturers like NXP&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ARM7&lt;/strong&gt;：manufacturers like Samsung&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ARM9&lt;/strong&gt;：manufacturers like Allwinner、Xilinx 、GOKE&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ARM11&lt;/strong&gt;：manufacturers like Fullhan&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;MIPS32&lt;/strong&gt;：manufacturers like loongson、Ingenic&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;RISC-V RV32E/RV32I[F]/RV64[D]&lt;/strong&gt;：manufacturers like sifive、&lt;a href=&#34;https://github.com/RT-Thread/rt-thread/tree/master/bsp/k210&#34;&gt;Canaan Kendryte&lt;/a&gt;、&lt;a href=&#34;https://github.com/RT-Thread/rt-thread/tree/master/bsp/bouffalo_lab&#34;&gt;bouffalo_lab&lt;/a&gt;、&lt;a href=&#34;https://nucleisys.com/&#34;&gt;Nuclei&lt;/a&gt;、&lt;a href=&#34;https://www.t-head.cn/&#34;&gt;T-Head&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ARC&lt;/strong&gt;：manufacturers like SYNOPSYS&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;DSP&lt;/strong&gt;：manufacturers like TI&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;C-Sky&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;x86&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Supported IDE and Compiler&lt;/h2&gt; &#xA;&lt;p&gt;The main IDE/compilers supported by RT-Thread are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;RT-Thread Studio IDE&lt;/li&gt; &#xA; &lt;li&gt;MDK KEIL&lt;/li&gt; &#xA; &lt;li&gt;IAR&lt;/li&gt; &#xA; &lt;li&gt;GCC&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;RT-Thread Studio IDE&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.rt-thread.io/document/site/rtthread-studio/um/studio-user-manual/&#34;&gt;User Manual&lt;/a&gt; | &lt;a href=&#34;https://youtu.be/ucq5eJgZIQg&#34;&gt;Tutorial Videos&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;RT-Thread Studio IDE (a.k.a. RT-Studio) is a one-stop intergrated development environment built by RT-Thread team. It has a easy-to-use graphical configuration system and a wealth of software packages and components resources. RT-Studio has the features of project creation, configuration and management,as well as code editing, SDK management, build configuration, debugging configuration, program download and debug. We&#39;re looking to make the use of RT-Studio as intuitive as possible, reducing the duplication of work and improving the development efficiency.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RT-Thread/rt-thread/master/documentation/figures/studio.gif&#34; alt=&#34;studio&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Env Tool&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RT-Thread/rt-thread/master/documentation/env/env.md&#34;&gt;User Manual&lt;/a&gt; | &lt;a href=&#34;https://www.youtube.com/watch?v=dEK94o_YoSo&#34;&gt;Tutorial Videos&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;In the early stage, RT-Thread team also created an auxiliary tool called Env. It is an auxiliary tool with a TUI (Text-based user interface). Developers can use Env tool to configure and generate the GCC, Keil MDK, and IAR projects.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RT-Thread/rt-thread/master/documentation/figures/env.png&#34; alt=&#34;env&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Getting Started&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.rt-thread.io/document/site/tutorial/quick-start/introduction/introduction/&#34;&gt;RT-Thread Programming Guide&lt;/a&gt; | &lt;a href=&#34;https://www.rt-thread.io/studio.html&#34;&gt;RT-Thread Studio IDE&lt;/a&gt; | &lt;a href=&#34;https://github.com/RT-Thread-packages/kernel-sample&#34;&gt;Kernel Sample&lt;/a&gt; | &lt;a href=&#34;https://www.youtube.com/watch?v=ZMi1O-Rr7yc&amp;amp;list=PLXUV89C_M3G5KVw2IerI-pqApdSM_IaZo&#34;&gt;RT-Thread Beginners Guide&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Based on &lt;a href=&#34;https://github.com/RT-Thread/rt-thread/tree/master/bsp/stm32/stm32f103-blue-pill&#34;&gt;STM32F103 BluePill&lt;/a&gt; | &lt;a href=&#34;https://github.com/RT-Thread/rt-thread/tree/master/bsp/raspberry-pico&#34;&gt;Raspberry Pi Pico&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Simulator&lt;/h2&gt; &#xA;&lt;p&gt;RT-Thread BSP can be compiled directly and downloaded to the corresponding development board for use. In addition, RT-Thread also provides qemu-vexpress-a9 BSP, which can be used without hardware platform. See the getting started guide below for details. Getting Started of QEMU with Env: &lt;a href=&#34;https://raw.githubusercontent.com/RT-Thread/rt-thread/master/documentation/quick-start/quick_start_qemu/quick_start_qemu.md&#34;&gt;Windows&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/RT-Thread/rt-thread/master/documentation/quick-start/quick_start_qemu/quick_start_qemu_linux.md&#34;&gt;Linux Ubuntu&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/RT-Thread/rt-thread/master/documentation/quick-start/quick_start_qemu/quick_start_qemu_macos.md&#34;&gt;Mac OS&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;RT-Thread follows the Apache License 2.0 free software license. It&#39;s completely open-source, can be used in commercial applications for free, does not require the disclosure of code, and has no potential commercial risk. License information and copyright information can generally be seen at the beginning of the code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;/* Copyright (c) 2006-2018, RT-Thread Development Team&#xA; *&#xA; * SPDX-License-Identifier: Apache-2.0&#xA; * ...&#xA; */&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Community&lt;/h1&gt; &#xA;&lt;p&gt;RT-Thread is very grateful for the support from all community developers, and if you have any ideas, suggestions or questions in the process of using RT-Thread, RT-Thread can be reached by the following means, and we are also updating RT-Thread in real time on these channels. At the same time, any questions can be asked in the &lt;a href=&#34;https://github.com/RT-Thread/rt-thread/issues&#34;&gt;issue section of RT-Thread repository&lt;/a&gt; or &lt;a href=&#34;https://club.rt-thread.io/&#34;&gt;RT-Thread forum&lt;/a&gt;, and community members will answer them.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.rt-thread.io&#34;&gt;Website&lt;/a&gt; | &lt;a href=&#34;https://github.com/RT-Thread/rt-thread&#34;&gt;Github&lt;/a&gt; | &lt;a href=&#34;https://twitter.com/rt_thread&#34;&gt;Twitter&lt;/a&gt; | &lt;a href=&#34;https://www.linkedin.com/company/rt-thread-iot-os/posts/?feedView=all&#34;&gt;LinkedIn&lt;/a&gt; | &lt;a href=&#34;https://www.youtube.com/channel/UCdDHtIfSYPq4002r27ffqPw&#34;&gt;Youtube&lt;/a&gt; | &lt;a href=&#34;https://www.facebook.com/RT-Thread-IoT-OS-110395723808463/?modal=admin_todo_tour&#34;&gt;Facebook&lt;/a&gt; | &lt;a href=&#34;https://rt-thread.medium.com/&#34;&gt;Medium&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Contribution&lt;/h1&gt; &#xA;&lt;p&gt;If you are interested in RT-Thread and want to join in the development of RT-Thread and become a code contributor,please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/RT-Thread/rt-thread/master/documentation/contribution_guide/contribution_guide.md&#34;&gt;Code Contribution Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Thanks for the following contributors!&lt;/h2&gt; &#xA;&lt;a href=&#34;https://github.com/RT-Thread/rt-thread/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=RT-Thread/rt-thread&#34;&gt; &lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>tiann/KernelSU</title>
    <updated>2023-05-01T02:11:34Z</updated>
    <id>tag:github.com,2023-05-01:/tiann/KernelSU</id>
    <link href="https://github.com/tiann/KernelSU" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Kernel based root solution for Android&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;strong&gt;English&lt;/strong&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/tiann/KernelSU/main/README_CN.md&#34;&gt;简体中文&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/tiann/KernelSU/main/README_TW.md&#34;&gt;繁體中文&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;KernelSU&lt;/h1&gt; &#xA;&lt;p&gt;A Kernel based root solution for Android devices.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Kernel-based &lt;code&gt;su&lt;/code&gt; and root access management.&lt;/li&gt; &#xA; &lt;li&gt;Module system based on overlayfs.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Compatibility State&lt;/h2&gt; &#xA;&lt;p&gt;KernelSU officially supports Android GKI 2.0 devices(with kernel 5.10+), old kernels(4.14+) is also compatiable, but you need to build kernel yourself.&lt;/p&gt; &#xA;&lt;p&gt;WSA and containter-based Android should also work with KernelSU integrated.&lt;/p&gt; &#xA;&lt;p&gt;And the current supported ABIs are : &lt;code&gt;arm64-v8a&lt;/code&gt; and &lt;code&gt;x86_64&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://kernelsu.org/guide/installation.html&#34;&gt;Installation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Build&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://kernelsu.org/guide/how-to-build.html&#34;&gt;How to build?&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Discussion&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Telegram: &lt;a href=&#34;https://t.me/KernelSU&#34;&gt;@KernelSU&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Files under &lt;code&gt;kernel&lt;/code&gt; directory are &lt;a href=&#34;https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html&#34;&gt;GPL-2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;All other parts except &lt;code&gt;kernel&lt;/code&gt; directory are &lt;a href=&#34;https://www.gnu.org/licenses/gpl-3.0.html&#34;&gt;GPL-3&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://git.zx2c4.com/kernel-assisted-superuser/about/&#34;&gt;kernel-assisted-superuser&lt;/a&gt;: the KernelSU idea.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/brevent/genuine/&#34;&gt;genuine&lt;/a&gt;: apk v2 signature validation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/m0nad/Diamorphine&#34;&gt;Diamorphine&lt;/a&gt;: some rootkit skills.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/topjohnwu/Magisk&#34;&gt;Magisk&lt;/a&gt;: the sepolicy implementation.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>ggerganov/llama.cpp</title>
    <updated>2023-05-01T02:11:34Z</updated>
    <id>tag:github.com,2023-05-01:/ggerganov/llama.cpp</id>
    <link href="https://github.com/ggerganov/llama.cpp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Port of Facebook&#39;s LLaMA model in C/C++&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;llama.cpp&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/1991296/230134379-7181e485-c521-4d23-a0d6-f7b3b61ba524.png&#34; alt=&#34;llama&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ggerganov/llama.cpp/actions&#34;&gt;&lt;img src=&#34;https://github.com/ggerganov/llama.cpp/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;Actions Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Inference of &lt;a href=&#34;https://arxiv.org/abs/2302.13971&#34;&gt;LLaMA&lt;/a&gt; model in pure C/C++&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Hot topics:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ggerganov/llama.cpp/discussions/1220&#34;&gt;Roadmap May 2023&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ggerganov/llama.cpp#quantization&#34;&gt;New quantization methods&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Description&lt;/h2&gt; &#xA;&lt;p&gt;The main goal of &lt;code&gt;llama.cpp&lt;/code&gt; is to run the LLaMA model using 4-bit integer quantization on a MacBook&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Plain C/C++ implementation without dependencies&lt;/li&gt; &#xA; &lt;li&gt;Apple silicon first-class citizen - optimized via ARM NEON and Accelerate framework&lt;/li&gt; &#xA; &lt;li&gt;AVX2 support for x86 architectures&lt;/li&gt; &#xA; &lt;li&gt;Mixed F16 / F32 precision&lt;/li&gt; &#xA; &lt;li&gt;4-bit integer quantization support&lt;/li&gt; &#xA; &lt;li&gt;Runs on the CPU&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The original implementation of &lt;code&gt;llama.cpp&lt;/code&gt; was &lt;a href=&#34;https://github.com/ggerganov/llama.cpp/issues/33#issuecomment-1465108022&#34;&gt;hacked in an evening&lt;/a&gt;. Since then, the project has improved significantly thanks to many contributions. This project is for educational purposes and serves as the main playground for developing new features for the &lt;a href=&#34;https://github.com/ggerganov/ggml&#34;&gt;ggml&lt;/a&gt; library.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Supported platforms:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Mac OS&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Linux&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Windows (via CMake)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Docker&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Supported models:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; LLaMA 🦙&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/ggerganov/llama.cpp#instruction-mode-with-alpaca&#34;&gt;Alpaca&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/ggerganov/llama.cpp#using-gpt4all&#34;&gt;GPT4All&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca&#34;&gt;Chinese LLaMA / Alpaca&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/bofenghuang/vigogne&#34;&gt;Vigogne (French)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/ggerganov/llama.cpp/discussions/643#discussioncomment-5533894&#34;&gt;Vicuna&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://bair.berkeley.edu/blog/2023/04/03/koala/&#34;&gt;Koala&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Bindings:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python: &lt;a href=&#34;https://github.com/abetlen/llama-cpp-python&#34;&gt;abetlen/llama-cpp-python&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Go: &lt;a href=&#34;https://github.com/go-skynet/go-llama.cpp&#34;&gt;go-skynet/go-llama.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Node.js: &lt;a href=&#34;https://github.com/hlhr202/llama-node&#34;&gt;hlhr202/llama-node&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Ruby: &lt;a href=&#34;https://github.com/yoshoku/llama_cpp.rb&#34;&gt;yoshoku/llama_cpp.rb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;UI:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/nat/openplayground&#34;&gt;nat/openplayground&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui&#34;&gt;oobabooga/text-generation-webui&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Here is a typical run using LLaMA-7B:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;make -j &amp;amp;&amp;amp; ./main -m ./models/7B/ggml-model-q4_0.bin -p &#34;Building a website can be done in 10 simple steps:&#34; -n 512&#xA;I llama.cpp build info:&#xA;I UNAME_S:  Darwin&#xA;I UNAME_P:  arm&#xA;I UNAME_M:  arm64&#xA;I CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -DGGML_USE_ACCELERATE&#xA;I CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -pthread&#xA;I LDFLAGS:   -framework Accelerate&#xA;I CC:       Apple clang version 14.0.0 (clang-1400.0.29.202)&#xA;I CXX:      Apple clang version 14.0.0 (clang-1400.0.29.202)&#xA;&#xA;make: Nothing to be done for `default&#39;.&#xA;main: seed = 1678486056&#xA;llama_model_load: loading model from &#39;./models/7B/ggml-model-q4_0.bin&#39; - please wait ...&#xA;llama_model_load: n_vocab = 32000&#xA;llama_model_load: n_ctx   = 512&#xA;llama_model_load: n_embd  = 4096&#xA;llama_model_load: n_mult  = 256&#xA;llama_model_load: n_head  = 32&#xA;llama_model_load: n_layer = 32&#xA;llama_model_load: n_rot   = 128&#xA;llama_model_load: f16     = 2&#xA;llama_model_load: n_ff    = 11008&#xA;llama_model_load: ggml ctx size = 4529.34 MB&#xA;llama_model_load: memory_size =   512.00 MB, n_mem = 16384&#xA;llama_model_load: .................................... done&#xA;llama_model_load: model size =  4017.27 MB / num tensors = 291&#xA;&#xA;main: prompt: &#39;Building a website can be done in 10 simple steps:&#39;&#xA;main: number of tokens in prompt = 15&#xA;     1 -&amp;gt; &#39;&#39;&#xA;  8893 -&amp;gt; &#39;Build&#39;&#xA;   292 -&amp;gt; &#39;ing&#39;&#xA;   263 -&amp;gt; &#39; a&#39;&#xA;  4700 -&amp;gt; &#39; website&#39;&#xA;   508 -&amp;gt; &#39; can&#39;&#xA;   367 -&amp;gt; &#39; be&#39;&#xA;  2309 -&amp;gt; &#39; done&#39;&#xA;   297 -&amp;gt; &#39; in&#39;&#xA; 29871 -&amp;gt; &#39; &#39;&#xA; 29896 -&amp;gt; &#39;1&#39;&#xA; 29900 -&amp;gt; &#39;0&#39;&#xA;  2560 -&amp;gt; &#39; simple&#39;&#xA;  6576 -&amp;gt; &#39; steps&#39;&#xA; 29901 -&amp;gt; &#39;:&#39;&#xA;&#xA;sampling parameters: temp = 0.800000, top_k = 40, top_p = 0.950000&#xA;&#xA;&#xA;Building a website can be done in 10 simple steps:&#xA;1) Select a domain name and web hosting plan&#xA;2) Complete a sitemap&#xA;3) List your products&#xA;4) Write product descriptions&#xA;5) Create a user account&#xA;6) Build the template&#xA;7) Start building the website&#xA;8) Advertise the website&#xA;9) Provide email support&#xA;10) Submit the website to search engines&#xA;A website is a collection of web pages that are formatted with HTML. HTML is the code that defines what the website looks like and how it behaves.&#xA;The HTML code is formatted into a template or a format. Once this is done, it is displayed on the user&#39;s browser.&#xA;The web pages are stored in a web server. The web server is also called a host. When the website is accessed, it is retrieved from the server and displayed on the user&#39;s computer.&#xA;A website is known as a website when it is hosted. This means that it is displayed on a host. The host is usually a web server.&#xA;A website can be displayed on different browsers. The browsers are basically the software that renders the website on the user&#39;s screen.&#xA;A website can also be viewed on different devices such as desktops, tablets and smartphones.&#xA;Hence, to have a website displayed on a browser, the website must be hosted.&#xA;A domain name is an address of a website. It is the name of the website.&#xA;The website is known as a website when it is hosted. This means that it is displayed on a host. The host is usually a web server.&#xA;A website can be displayed on different browsers. The browsers are basically the software that renders the website on the user’s screen.&#xA;A website can also be viewed on different devices such as desktops, tablets and smartphones. Hence, to have a website displayed on a browser, the website must be hosted.&#xA;A domain name is an address of a website. It is the name of the website.&#xA;A website is an address of a website. It is a collection of web pages that are formatted with HTML. HTML is the code that defines what the website looks like and how it behaves.&#xA;The HTML code is formatted into a template or a format. Once this is done, it is displayed on the user’s browser.&#xA;A website is known as a website when it is hosted&#xA;&#xA;main: mem per token = 14434244 bytes&#xA;main:     load time =  1332.48 ms&#xA;main:   sample time =  1081.40 ms&#xA;main:  predict time = 31378.77 ms / 61.41 ms per token&#xA;main:    total time = 34036.74 ms&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And here is another demo of running both LLaMA-7B and &lt;a href=&#34;https://github.com/ggerganov/whisper.cpp&#34;&gt;whisper.cpp&lt;/a&gt; on a single M1 Pro MacBook:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/1991296/224442907-7693d4be-acaa-4e01-8b4f-add84093ffff.mp4&#34;&gt;https://user-images.githubusercontent.com/1991296/224442907-7693d4be-acaa-4e01-8b4f-add84093ffff.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Here are the steps for the LLaMA-7B model.&lt;/p&gt; &#xA;&lt;h3&gt;Get the Code&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/ggerganov/llama.cpp&#xA;cd llama.cpp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Build&lt;/h3&gt; &#xA;&lt;p&gt;In order to build llama.cpp you have three different options.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Using &lt;code&gt;make&lt;/code&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;On Linux or MacOS:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;On Windows:&lt;/p&gt; &#xA;    &lt;ol&gt; &#xA;     &lt;li&gt;Download the latest fortran version of &lt;a href=&#34;https://github.com/skeeto/w64devkit/releases&#34;&gt;w64devkit&lt;/a&gt;.&lt;/li&gt; &#xA;     &lt;li&gt;Extract &lt;code&gt;w64devkit&lt;/code&gt; on your pc.&lt;/li&gt; &#xA;     &lt;li&gt;Run &lt;code&gt;w64devkit.exe&lt;/code&gt;.&lt;/li&gt; &#xA;     &lt;li&gt;Use the &lt;code&gt;cd&lt;/code&gt; command to reach the &lt;code&gt;llama.cpp&lt;/code&gt; folder.&lt;/li&gt; &#xA;     &lt;li&gt;From here you can run: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;    &lt;/ol&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Using &lt;code&gt;CMake&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir build&#xA;cd build&#xA;cmake ..&#xA;cmake --build . --config Release&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Using &lt;code&gt;Zig&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;zig build -Drelease-fast&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;BLAS Build&lt;/h3&gt; &#xA;&lt;p&gt;Building the program with BLAS support may lead to some performance improvements in prompt processing using batch sizes higher than 32 (the default is 512). BLAS doesn&#39;t affect the normal generation performance. There are currently three different implementations of it:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Accelerate Framework:&lt;/p&gt; &lt;p&gt;This is only available on Mac PCs and it&#39;s enabled by default. You can just build using the normal instructions.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;OpenBLAS:&lt;/p&gt; &lt;p&gt;This provides BLAS acceleration using only the CPU. Make sure to have OpenBLAS installed on your machine.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Using &lt;code&gt;make&lt;/code&gt;:&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt; &lt;p&gt;On Linux:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make LLAMA_OPENBLAS=1&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note: In order to build on Arch Linux with OpenBLAS support enabled you must edit the Makefile adding at the end of the line 105: &lt;code&gt;-lcblas&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;     &lt;li&gt; &lt;p&gt;On Windows:&lt;/p&gt; &#xA;      &lt;ol&gt; &#xA;       &lt;li&gt; &lt;p&gt;Download the latest fortran version of &lt;a href=&#34;https://github.com/skeeto/w64devkit/releases&#34;&gt;w64devkit&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;       &lt;li&gt; &lt;p&gt;Download the latest version of &lt;a href=&#34;https://github.com/xianyi/OpenBLAS/releases&#34;&gt;OpenBLAS for Windows&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;       &lt;li&gt; &lt;p&gt;Extract &lt;code&gt;w64devkit&lt;/code&gt; on your pc.&lt;/p&gt; &lt;/li&gt; &#xA;       &lt;li&gt; &lt;p&gt;From the OpenBLAS zip that you just downloaded copy &lt;code&gt;libopenblas.a&lt;/code&gt;, located inside the &lt;code&gt;lib&lt;/code&gt; folder, inside &lt;code&gt;w64devkit\x86_64-w64-mingw32\lib&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;       &lt;li&gt; &lt;p&gt;From the same OpenBLAS zip copy the content of the &lt;code&gt;include&lt;/code&gt; folder inside &lt;code&gt;w64devkit\x86_64-w64-mingw32\include&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;       &lt;li&gt; &lt;p&gt;Run &lt;code&gt;w64devkit.exe&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;       &lt;li&gt; &lt;p&gt;Use the &lt;code&gt;cd&lt;/code&gt; command to reach the &lt;code&gt;llama.cpp&lt;/code&gt; folder.&lt;/p&gt; &lt;/li&gt; &#xA;       &lt;li&gt; &lt;p&gt;From here you can run:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make LLAMA_OPENBLAS=1&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;      &lt;/ol&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Using &lt;code&gt;CMake&lt;/code&gt; on Linux:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir build&#xA;cd build&#xA;cmake .. -DLLAMA_OPENBLAS=ON&#xA;cmake --build . --config Release&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;cuBLAS&lt;/p&gt; &lt;p&gt;This provides BLAS acceleration using the CUDA cores of your Nvidia GPU. Make sure to have the CUDA toolkit installed. You can download it from your Linux distro&#39;s package manager or from here: &lt;a href=&#34;https://developer.nvidia.com/cuda-downloads&#34;&gt;CUDA Toolkit&lt;/a&gt;.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Using &lt;code&gt;make&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make LLAMA_CUBLAS=1&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Using &lt;code&gt;CMake&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir build&#xA;cd build&#xA;cmake .. -DLLAMA_CUBLAS=ON&#xA;cmake --build . --config Release&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Prepare Data &amp;amp; Run&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# obtain the original LLaMA model weights and place them in ./models&#xA;ls ./models&#xA;65B 30B 13B 7B tokenizer_checklist.chk tokenizer.model&#xA;&#xA;# install Python dependencies&#xA;python3 -m pip install -r requirements.txt&#xA;&#xA;# convert the 7B model to ggml FP16 format&#xA;python3 convert.py models/7B/&#xA;&#xA;# quantize the model to 4-bits (using q4_0 method)&#xA;./quantize ./models/7B/ggml-model-f16.bin ./models/7B/ggml-model-q4_0.bin q4_0&#xA;&#xA;# run the inference&#xA;./main -m ./models/7B/ggml-model-q4_0.bin -n 128&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When running the larger models, make sure you have enough disk space to store all the intermediate files.&lt;/p&gt; &#xA;&lt;h3&gt;Memory/Disk Requirements&lt;/h3&gt; &#xA;&lt;p&gt;As the models are currently fully loaded into memory, you will need adequate disk space to save them and sufficient RAM to load them. At the moment, memory and disk requirements are the same.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Original size&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Quantized size (4-bit)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;7B&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;13 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;3.9 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;13B&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;24 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;7.8 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;30B&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;60 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;19.5 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;65B&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;120 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;38.5 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Quantization&lt;/h3&gt; &#xA;&lt;p&gt;Several quantization methods are supported. They differ in the resulting model disk size and inference speed.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Measure&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;F16&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Q4_0&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Q4_1&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Q4_2&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Q5_0&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Q5_1&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Q8_0&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;perplexity&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;5.9565&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;6.2103&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;6.1286&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;6.1698&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;6.0139&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;5.9934&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;5.9571&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;file size&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;13.0G&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;4.0G&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;4.8G&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;4.0G&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;4.4G&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;4.8G&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;7.1G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;ms/tok @ 4th&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;128&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;56&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;61&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;84&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;91&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;95&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;75&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;ms/tok @ 8th&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;128&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;47&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;55&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;48&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;53&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;59&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;75&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;bits/weight&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;16.0&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;5.0&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;6.0&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;5.0&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;5.5&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;6.0&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;9.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;perplexity&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;5.2455&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;5.3748&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;5.3471&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;5.3433&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;5.2768&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;5.2582&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;5.2458&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;file size&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;25.0G&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;7.6G&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;9.1G&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;7.6G&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;8.4G&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;9.1G&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;14G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;ms/tok @ 4th&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;239&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;104&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;113&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;160&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;176&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;185&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;141&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;ms/tok @ 8th&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;240&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;85&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;99&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;97&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;108&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;117&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;147&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;bits/weight&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;16.0&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;5.0&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;6.0&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;5.0&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;5.5&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;6.0&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;9.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Interactive mode&lt;/h3&gt; &#xA;&lt;p&gt;If you want a more ChatGPT-like experience, you can run in interactive mode by passing &lt;code&gt;-i&lt;/code&gt; as a parameter. In this mode, you can always interrupt generation by pressing Ctrl+C and entering one or more lines of text, which will be converted into tokens and appended to the current context. You can also specify a &lt;em&gt;reverse prompt&lt;/em&gt; with the parameter &lt;code&gt;-r &#34;reverse prompt string&#34;&lt;/code&gt;. This will result in user input being prompted whenever the exact tokens of the reverse prompt string are encountered in the generation. A typical use is to use a prompt that makes LLaMa emulate a chat between multiple users, say Alice and Bob, and pass &lt;code&gt;-r &#34;Alice:&#34;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Here is an example of a few-shot interaction, invoked with the command&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# default arguments using a 7B model&#xA;./examples/chat.sh&#xA;&#xA;# advanced chat with a 13B model&#xA;./examples/chat-13B.sh&#xA;&#xA;# custom arguments using a 13B model&#xA;./main -m ./models/13B/ggml-model-q4_0.bin -n 256 --repeat_penalty 1.0 --color -i -r &#34;User:&#34; -f prompts/chat-with-bob.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note the use of &lt;code&gt;--color&lt;/code&gt; to distinguish between user input and generated text. Other parameters are explained in more detail in the &lt;a href=&#34;https://raw.githubusercontent.com/ggerganov/llama.cpp/master/examples/main/README.md&#34;&gt;README&lt;/a&gt; for the &lt;code&gt;main&lt;/code&gt; example program.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/1991296/224575029-2af3c7dc-5a65-4f64-a6bb-517a532aea38.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Instruction mode with Alpaca&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;First, download the &lt;code&gt;ggml&lt;/code&gt; Alpaca model into the &lt;code&gt;./models&lt;/code&gt; folder&lt;/li&gt; &#xA; &lt;li&gt;Run the &lt;code&gt;main&lt;/code&gt; tool like this:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;./examples/alpaca.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Sample run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;== Running in interactive mode. ==&#xA; - Press Ctrl+C to interject at any time.&#xA; - Press Return to return control to LLaMa.&#xA; - If you want to submit another line, end your input in &#39;\&#39;.&#xA;&#xA; Below is an instruction that describes a task. Write a response that appropriately completes the request.&#xA;&#xA;&amp;gt; How many letters are there in the English alphabet?&#xA;There 26 letters in the English Alphabet&#xA;&amp;gt; What is the most common way of transportation in Amsterdam?&#xA;The majority (54%) are using public transit. This includes buses, trams and metros with over 100 lines throughout the city which make it very accessible for tourists to navigate around town as well as locals who commute by tram or metro on a daily basis&#xA;&amp;gt; List 5 words that start with &#34;ca&#34;.&#xA;cadaver, cauliflower, cabbage (vegetable), catalpa (tree) and Cailleach.&#xA;&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using &lt;a href=&#34;https://github.com/nomic-ai/gpt4all&#34;&gt;GPT4All&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Obtain the &lt;code&gt;tokenizer.model&lt;/code&gt; file from LLaMA model and put it to &lt;code&gt;models&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Obtain the &lt;code&gt;added_tokens.json&lt;/code&gt; file from Alpaca model and put it to &lt;code&gt;models&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Obtain the &lt;code&gt;gpt4all-lora-quantized.bin&lt;/code&gt; file from GPT4All model and put it to &lt;code&gt;models/gpt4all-7B&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;It is distributed in the old &lt;code&gt;ggml&lt;/code&gt; format which is now obsoleted&lt;/li&gt; &#xA; &lt;li&gt;You have to convert it to the new format using &lt;code&gt;convert.py&lt;/code&gt;:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 convert.py models/gpt4all-7B/gpt4all-lora-quantized.bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;You can now use the newly generated &lt;code&gt;models/gpt4all-7B/ggml-model-q4_0.bin&lt;/code&gt; model in exactly the same way as all other models&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The newer GPT4All-J model is not yet supported!&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Obtaining and verifying the Facebook LLaMA original model and Stanford Alpaca model data&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Under no circumstances should IPFS, magnet links, or any other links to model downloads be shared anywhere in this repository, including in issues, discussions, or pull requests. They will be immediately deleted.&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The LLaMA models are officially distributed by Facebook and will &lt;strong&gt;never&lt;/strong&gt; be provided through this repository.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Refer to &lt;a href=&#34;https://github.com/facebookresearch/llama/pull/73/files&#34;&gt;Facebook&#39;s LLaMA repository&lt;/a&gt; if you need to request access to the model data.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Please verify the &lt;a href=&#34;https://raw.githubusercontent.com/ggerganov/llama.cpp/master/SHA256SUMS&#34;&gt;sha256 checksums&lt;/a&gt; of all downloaded model files to confirm that you have the correct model data files before creating an issue relating to your model files.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The following command will verify if you have all possible latest files in your self-installed &lt;code&gt;./models&lt;/code&gt; subdirectory:&lt;/p&gt; &lt;p&gt;&lt;code&gt;sha256sum --ignore-missing -c SHA256SUMS&lt;/code&gt; on Linux&lt;/p&gt; &lt;p&gt;or&lt;/p&gt; &lt;p&gt;&lt;code&gt;shasum -a 256 --ignore-missing -c SHA256SUMS&lt;/code&gt; on macOS&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If your issue is with model generation quality, then please at least scan the following links and papers to understand the limitations of LLaMA models. This is especially important when choosing an appropriate model size and appreciating both the significant and subtle differences between LLaMA models and ChatGPT:&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;LLaMA:&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://ai.facebook.com/blog/large-language-model-llama-meta-ai/&#34;&gt;Introducing LLaMA: A foundational, 65-billion-parameter large language model&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.13971&#34;&gt;LLaMA: Open and Efficient Foundation Language Models&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;GPT-3&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2005.14165&#34;&gt;Language Models are Few-Shot Learners&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;GPT-3.5 / InstructGPT / ChatGPT:&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://openai.com/research/instruction-following&#34;&gt;Aligning language models to follow instructions&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2203.02155&#34;&gt;Training language models to follow instructions with human feedback&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Perplexity (measuring model quality)&lt;/h3&gt; &#xA;&lt;p&gt;You can use the &lt;code&gt;perplexity&lt;/code&gt; example to measure perplexity over the given prompt. For more background, see &lt;a href=&#34;https://huggingface.co/docs/transformers/perplexity&#34;&gt;https://huggingface.co/docs/transformers/perplexity&lt;/a&gt;. However, in general, lower perplexity is better for LLMs.&lt;/p&gt; &#xA;&lt;h4&gt;Latest measurements&lt;/h4&gt; &#xA;&lt;p&gt;The latest perplexity scores for the various model sizes and quantizations are being tracked in &lt;a href=&#34;https://github.com/ggerganov/llama.cpp/discussions/406&#34;&gt;discussion #406&lt;/a&gt;. &lt;code&gt;llama.cpp&lt;/code&gt; is measuring very well compared to the baseline implementations. Quantization has a small negative impact on quality, but, as you can see, running 13B at q4_0 beats the 7B f16 model by a significant amount.&lt;/p&gt; &#xA;&lt;p&gt;All measurements are done against the wikitext2 test dataset (&lt;a href=&#34;https://paperswithcode.com/dataset/wikitext-2&#34;&gt;https://paperswithcode.com/dataset/wikitext-2&lt;/a&gt;), with default options (512 length context). Note that changing the context length will have a significant impact on perplexity (longer context = better perplexity).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Perplexity - model options&#xA;5.5985 - 13B, q4_0&#xA;5.9565 - 7B, f16&#xA;6.3001 - 7B, q4_1&#xA;6.5949 - 7B, q4_0&#xA;6.5995 - 7B, q4_0, --memory_f16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;How to run&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download/extract: &lt;a href=&#34;https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip?ref=salesforce-research&#34;&gt;https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip?ref=salesforce-research&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;./perplexity -m models/7B/ggml-model-q4_0.bin -f wiki.test.raw&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Output:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;perplexity : calculating perplexity over 655 chunks&#xA;24.43 seconds per pass - ETA 4.45 hours&#xA;[1]4.5970,[2]5.1807,[3]6.0382,...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And after 4.45 hours, you will have the final perplexity.&lt;/p&gt; &#xA;&lt;h3&gt;Android&lt;/h3&gt; &#xA;&lt;p&gt;You can easily run &lt;code&gt;llama.cpp&lt;/code&gt; on Android device with &lt;a href=&#34;https://termux.dev/&#34;&gt;termux&lt;/a&gt;. First, obtain the &lt;a href=&#34;https://developer.android.com/ndk&#34;&gt;Android NDK&lt;/a&gt; and then build with CMake:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ mkdir build-android&#xA;$ cd build-android&#xA;$ export NDK=&amp;lt;your_ndk_directory&amp;gt;&#xA;$ cmake -DCMAKE_TOOLCHAIN_FILE=$NDK/build/cmake/android.toolchain.cmake -DANDROID_ABI=arm64-v8a -DANDROID_PLATFORM=android-23 -DCMAKE_C_FLAGS=-march=armv8.4a+dotprod ..&#xA;$ make&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install &lt;a href=&#34;https://termux.dev/&#34;&gt;termux&lt;/a&gt; on your device and run &lt;code&gt;termux-setup-storage&lt;/code&gt; to get access to your SD card. Finally, copy the &lt;code&gt;llama&lt;/code&gt; binary and the model files to your device storage. Here is a demo of an interactive session running on Pixel 5 phone:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/271616/225014776-1d567049-ad71-4ef2-b050-55b0b3b9274c.mp4&#34;&gt;https://user-images.githubusercontent.com/271616/225014776-1d567049-ad71-4ef2-b050-55b0b3b9274c.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Docker&lt;/h3&gt; &#xA;&lt;h4&gt;Prerequisites&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Docker must be installed and running on your system.&lt;/li&gt; &#xA; &lt;li&gt;Create a folder to store big models &amp;amp; intermediate files (ex. /llama/models)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Images&lt;/h4&gt; &#xA;&lt;p&gt;We have two Docker images available for this project:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;ghcr.io/ggerganov/llama.cpp:full&lt;/code&gt;: This image includes both the main executable file and the tools to convert LLaMA models into ggml and convert into 4-bit quantization.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ghcr.io/ggerganov/llama.cpp:light&lt;/code&gt;: This image only includes the main executable file.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;Usage&lt;/h4&gt; &#xA;&lt;p&gt;The easiest way to download the models, convert them to ggml and optimize them is with the --all-in-one command which includes the full docker image.&lt;/p&gt; &#xA;&lt;p&gt;Replace &lt;code&gt;/path/to/models&lt;/code&gt; below with the actual path where you downloaded the models.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -v /path/to/models:/models ghcr.io/ggerganov/llama.cpp:full --all-in-one &#34;/models/&#34; 7B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On completion, you are ready to play!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -v /path/to/models:/models ghcr.io/ggerganov/llama.cpp:full --run -m /models/7B/ggml-model-q4_0.bin -p &#34;Building a website can be done in 10 simple steps:&#34; -n 512&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or with a light image:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -v /path/to/models:/models ghcr.io/ggerganov/llama.cpp:light -m /models/7B/ggml-model-q4_0.bin -p &#34;Building a website can be done in 10 simple steps:&#34; -n 512&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Contributing&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Contributors can open PRs&lt;/li&gt; &#xA; &lt;li&gt;Collaborators can push to branches in the &lt;code&gt;llama.cpp&lt;/code&gt; repo and merge PRs into the &lt;code&gt;master&lt;/code&gt; branch&lt;/li&gt; &#xA; &lt;li&gt;Collaborators will be invited based on contributions&lt;/li&gt; &#xA; &lt;li&gt;Any help with managing issues and PRs is very appreciated!&lt;/li&gt; &#xA; &lt;li&gt;Make sure to read this: &lt;a href=&#34;https://github.com/ggerganov/llama.cpp/discussions/205&#34;&gt;Inference at the edge&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;A bit of backstory for those who are interested: &lt;a href=&#34;https://changelog.com/podcast/532&#34;&gt;Changelog podcast&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Coding guidelines&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Avoid adding third-party dependencies, extra files, extra headers, etc.&lt;/li&gt; &#xA; &lt;li&gt;Always consider cross-compatibility with other operating systems and architectures&lt;/li&gt; &#xA; &lt;li&gt;Avoid fancy looking modern STL constructs, use basic &lt;code&gt;for&lt;/code&gt; loops, avoid templates, keep it simple&lt;/li&gt; &#xA; &lt;li&gt;There are no strict rules for the code style, but try to follow the patterns in the code (indentation, spaces, etc.). Vertical alignment makes things more readable and easier to batch edit&lt;/li&gt; &#xA; &lt;li&gt;Clean-up any trailing whitespaces, use 4 spaces for indentation, brackets on the same line, &lt;code&gt;void * ptr&lt;/code&gt;, &lt;code&gt;int &amp;amp; a&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;See &lt;a href=&#34;https://github.com/ggerganov/llama.cpp/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22&#34;&gt;good first issues&lt;/a&gt; for tasks suitable for first contributions&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Docs&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ggerganov/llama.cpp/wiki/GGML-Tips-&amp;amp;-Tricks&#34;&gt;GGML tips &amp;amp; tricks&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>