<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-01T02:08:43Z</updated>
  <subtitle>Monthly Trending of C in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>FFmpeg/FFmpeg</title>
    <updated>2023-04-01T02:08:43Z</updated>
    <id>tag:github.com,2023-04-01:/FFmpeg/FFmpeg</id>
    <link href="https://github.com/FFmpeg/FFmpeg" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Mirror of https://git.ffmpeg.org/ffmpeg.git&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;FFmpeg README&lt;/h1&gt; &#xA;&lt;p&gt;FFmpeg is a collection of libraries and tools to process multimedia content such as audio, video, subtitles and related metadata.&lt;/p&gt; &#xA;&lt;h2&gt;Libraries&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;libavcodec&lt;/code&gt; provides implementation of a wider range of codecs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;libavformat&lt;/code&gt; implements streaming protocols, container formats and basic I/O access.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;libavutil&lt;/code&gt; includes hashers, decompressors and miscellaneous utility functions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;libavfilter&lt;/code&gt; provides means to alter decoded audio and video through a directed graph of connected filters.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;libavdevice&lt;/code&gt; provides an abstraction to access capture and playback devices.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;libswresample&lt;/code&gt; implements audio mixing and resampling routines.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;libswscale&lt;/code&gt; implements color conversion and scaling routines.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Tools&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ffmpeg.org/ffmpeg.html&#34;&gt;ffmpeg&lt;/a&gt; is a command line toolbox to manipulate, convert and stream multimedia content.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ffmpeg.org/ffplay.html&#34;&gt;ffplay&lt;/a&gt; is a minimalistic multimedia player.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ffmpeg.org/ffprobe.html&#34;&gt;ffprobe&lt;/a&gt; is a simple analysis tool to inspect multimedia content.&lt;/li&gt; &#xA; &lt;li&gt;Additional small tools such as &lt;code&gt;aviocat&lt;/code&gt;, &lt;code&gt;ismindex&lt;/code&gt; and &lt;code&gt;qt-faststart&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;The offline documentation is available in the &lt;strong&gt;doc/&lt;/strong&gt; directory.&lt;/p&gt; &#xA;&lt;p&gt;The online documentation is available in the main &lt;a href=&#34;https://ffmpeg.org&#34;&gt;website&lt;/a&gt; and in the &lt;a href=&#34;https://trac.ffmpeg.org&#34;&gt;wiki&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;p&gt;Coding examples are available in the &lt;strong&gt;doc/examples&lt;/strong&gt; directory.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;FFmpeg codebase is mainly LGPL-licensed with optional components licensed under GPL. Please refer to the LICENSE file for detailed information.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Patches should be submitted to the ffmpeg-devel mailing list using &lt;code&gt;git format-patch&lt;/code&gt; or &lt;code&gt;git send-email&lt;/code&gt;. Github pull requests should be avoided because they are not part of our review process and will be ignored.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Genymobile/scrcpy</title>
    <updated>2023-04-01T02:08:43Z</updated>
    <id>tag:github.com,2023-04-01:/Genymobile/scrcpy</id>
    <link href="https://github.com/Genymobile/scrcpy" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Display and control your Android device&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;scrcpy (v2.0)&lt;/h1&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/Genymobile/scrcpy/master/app/data/icon.svg?sanitize=true&#34; width=&#34;128&#34; height=&#34;128&#34; alt=&#34;scrcpy&#34; align=&#34;right&#34;&gt; &#xA;&lt;p&gt;&lt;em&gt;pronounced &#34;&lt;strong&gt;scr&lt;/strong&gt;een &lt;strong&gt;c&lt;/strong&gt;o&lt;strong&gt;py&lt;/strong&gt;&#34;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;This application mirrors Android devices (video and audio) connected via USB or &lt;a href=&#34;https://raw.githubusercontent.com/Genymobile/scrcpy/master/doc/device.md#tcpip-wireless&#34;&gt;over TCP/IP&lt;/a&gt;, and allows to control the device with the keyboard and the mouse of the computer. It does not require any &lt;em&gt;root&lt;/em&gt; access. It works on &lt;em&gt;Linux&lt;/em&gt;, &lt;em&gt;Windows&lt;/em&gt; and &lt;em&gt;macOS&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Genymobile/scrcpy/master/assets/screenshot-debian-600.jpg&#34; alt=&#34;screenshot&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;It focuses on:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;lightness&lt;/strong&gt;: native, displays only the device screen&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;performance&lt;/strong&gt;: 30~120fps, depending on the device&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;quality&lt;/strong&gt;: 1920×1080 or above&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;low latency&lt;/strong&gt;: &lt;a href=&#34;https://github.com/Genymobile/scrcpy/pull/646&#34;&gt;35~70ms&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;low startup time&lt;/strong&gt;: ~1 second to display the first image&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;non-intrusiveness&lt;/strong&gt;: nothing is left installed on the Android device&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;user benefits&lt;/strong&gt;: no account, no ads, no internet required&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;freedom&lt;/strong&gt;: free and open source software&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Its features include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Genymobile/scrcpy/master/doc/audio.md&#34;&gt;audio forwarding&lt;/a&gt; (Android &amp;gt;= 11)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Genymobile/scrcpy/master/doc/recording.md&#34;&gt;recording&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;mirroring with &lt;a href=&#34;https://raw.githubusercontent.com/Genymobile/scrcpy/master/doc/device.md#turn-screen-off&#34;&gt;Android device screen off&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Genymobile/scrcpy/master/doc/control.md#copy-paste&#34;&gt;copy-paste&lt;/a&gt; in both directions&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Genymobile/scrcpy/master/doc/video.md&#34;&gt;configurable quality&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Android device screen &lt;a href=&#34;https://raw.githubusercontent.com/Genymobile/scrcpy/master/doc/v4l2.md&#34;&gt;as a webcam (V4L2)&lt;/a&gt; (Linux-only)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Genymobile/scrcpy/master/doc/hid-otg.md&#34;&gt;physical keyboard/mouse simulation (HID)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Genymobile/scrcpy/master/doc/hid-otg.md#otg&#34;&gt;OTG mode&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;and more…&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;p&gt;The Android device requires at least API 21 (Android 5.0).&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Genymobile/scrcpy/master/doc/audio.md&#34;&gt;Audio forwarding&lt;/a&gt; is supported from API 30 (Android 11).&lt;/p&gt; &#xA;&lt;p&gt;Make sure you &lt;a href=&#34;https://developer.android.com/studio/debug/dev-options#enable&#34;&gt;enabled USB debugging&lt;/a&gt; on your device(s).&lt;/p&gt; &#xA;&lt;p&gt;On some devices, you also need to enable &lt;a href=&#34;https://github.com/Genymobile/scrcpy/issues/70#issuecomment-373286323&#34;&gt;an additional option&lt;/a&gt; &lt;code&gt;USB debugging (Security Settings)&lt;/code&gt; (this is an item different from &lt;code&gt;USB debugging&lt;/code&gt;) to control it using a keyboard and mouse. Rebooting the device is necessary once this option is set.&lt;/p&gt; &#xA;&lt;p&gt;Note that USB debugging is not required to run scrcpy in &lt;a href=&#34;https://raw.githubusercontent.com/Genymobile/scrcpy/master/doc/hid-otg.md#otg&#34;&gt;OTG mode&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Get the app&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Genymobile/scrcpy/master/doc/linux.md&#34;&gt;Linux&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Genymobile/scrcpy/master/doc/windows.md&#34;&gt;Windows&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Genymobile/scrcpy/master/doc/macos.md&#34;&gt;macOS&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;User documentation&lt;/h2&gt; &#xA;&lt;p&gt;The application provides a lot of features and configuration options. They are documented in the following pages:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Genymobile/scrcpy/master/doc/device.md&#34;&gt;Device&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Genymobile/scrcpy/master/doc/video.md&#34;&gt;Video&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Genymobile/scrcpy/master/doc/audio.md&#34;&gt;Audio&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Genymobile/scrcpy/master/doc/control.md&#34;&gt;Control&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Genymobile/scrcpy/master/doc/window.md&#34;&gt;Window&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Genymobile/scrcpy/master/doc/recording.md&#34;&gt;Recording&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Genymobile/scrcpy/master/doc/tunnels.md&#34;&gt;Tunnels&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Genymobile/scrcpy/master/doc/hid-otg.md&#34;&gt;HID/OTG&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Genymobile/scrcpy/master/doc/v4l2.md&#34;&gt;Video4Linux&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Genymobile/scrcpy/master/doc/shortcuts.md&#34;&gt;Shortcuts&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Genymobile/scrcpy/master/FAQ.md&#34;&gt;FAQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Genymobile/scrcpy/wiki&#34;&gt;Translations&lt;/a&gt; (not necessarily up to date)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Genymobile/scrcpy/master/doc/build.md&#34;&gt;Build instructions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Genymobile/scrcpy/master/doc/develop.md&#34;&gt;Developers&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Articles&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.rom1v.com/2018/03/introducing-scrcpy/&#34;&gt;Introducing scrcpy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.genymotion.com/blog/open-source-project-scrcpy-now-works-wirelessly/&#34;&gt;Scrcpy now works wirelessly&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.rom1v.com/2023/03/scrcpy-2-0-with-audio/&#34;&gt;Scrcpy 2.0, with audio&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you encounter a bug, please read the &lt;a href=&#34;https://raw.githubusercontent.com/Genymobile/scrcpy/master/FAQ.md&#34;&gt;FAQ&lt;/a&gt; first, then open an &lt;a href=&#34;https://github.com/Genymobile/scrcpy/issues&#34;&gt;issue&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For general questions or discussions, you can also use:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Reddit: &lt;a href=&#34;https://www.reddit.com/r/scrcpy&#34;&gt;&lt;code&gt;r/scrcpy&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Twitter: &lt;a href=&#34;https://twitter.com/scrcpy_app&#34;&gt;&lt;code&gt;@scrcpy_app&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Donate&lt;/h2&gt; &#xA;&lt;p&gt;I&#39;m &lt;a href=&#34;https://github.com/rom1v&#34;&gt;@rom1v&lt;/a&gt;, the author and maintainer of &lt;em&gt;scrcpy&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you appreciate this application, you can &lt;a href=&#34;https://blog.rom1v.com/about/#support-my-open-source-work&#34;&gt;support my open source work&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Licence&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;Copyright (C) 2018 Genymobile&#xA;Copyright (C) 2018-2023 Romain Vimont&#xA;&#xA;Licensed under the Apache License, Version 2.0 (the &#34;License&#34;);&#xA;you may not use this file except in compliance with the License.&#xA;You may obtain a copy of the License at&#xA;&#xA;    http://www.apache.org/licenses/LICENSE-2.0&#xA;&#xA;Unless required by applicable law or agreed to in writing, software&#xA;distributed under the License is distributed on an &#34;AS IS&#34; BASIS,&#xA;WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&#xA;See the License for the specific language governing permissions and&#xA;limitations under the License.&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>pjreddie/darknet</title>
    <updated>2023-04-01T02:08:43Z</updated>
    <id>tag:github.com,2023-04-01:/pjreddie/darknet</id>
    <link href="https://github.com/pjreddie/darknet" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Convolutional Neural Networks&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;http://pjreddie.com/media/files/darknet-black-small.png&#34; alt=&#34;Darknet Logo&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Darknet&lt;/h1&gt; &#xA;&lt;p&gt;Darknet is an open source neural network framework written in C and CUDA. It is fast, easy to install, and supports CPU and GPU computation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Discord&lt;/strong&gt; invite link for for communication and questions: &lt;a href=&#34;https://discord.gg/zSq8rtW&#34;&gt;https://discord.gg/zSq8rtW&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;YOLOv7:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;paper&lt;/strong&gt; - YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors: &lt;a href=&#34;https://arxiv.org/abs/2207.02696&#34;&gt;https://arxiv.org/abs/2207.02696&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;source code - Pytorch (use to reproduce results):&lt;/strong&gt; &lt;a href=&#34;https://github.com/WongKinYiu/yolov7&#34;&gt;https://github.com/WongKinYiu/yolov7&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Official YOLOv7 is more accurate and faster than YOLOv5 by &lt;strong&gt;120%&lt;/strong&gt; FPS, than YOLOX by &lt;strong&gt;180%&lt;/strong&gt; FPS, than Dual-Swin-T by &lt;strong&gt;1200%&lt;/strong&gt; FPS, than ConvNext by &lt;strong&gt;550%&lt;/strong&gt; FPS, than SWIN-L by &lt;strong&gt;500%&lt;/strong&gt; FPS.&lt;/p&gt; &#xA;&lt;p&gt;YOLOv7 surpasses all known object detectors in both speed and accuracy in the range from 5 FPS to 160 FPS and has the highest accuracy 56.8% AP among all known real-time object detectors with 30 FPS or higher on GPU V100, batch=1.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;YOLOv7-e6 (55.9% AP, 56 FPS V100 b=1) by &lt;code&gt;+500%&lt;/code&gt; FPS faster than SWIN-L Cascade-Mask R-CNN (53.9% AP, 9.2 FPS A100 b=1)&lt;/li&gt; &#xA; &lt;li&gt;YOLOv7-e6 (55.9% AP, 56 FPS V100 b=1) by &lt;code&gt;+550%&lt;/code&gt; FPS faster than ConvNeXt-XL C-M-RCNN (55.2% AP, 8.6 FPS A100 b=1)&lt;/li&gt; &#xA; &lt;li&gt;YOLOv7-w6 (54.6% AP, 84 FPS V100 b=1) by &lt;code&gt;+120%&lt;/code&gt; FPS faster than YOLOv5-X6-r6.1 (55.0% AP, 38 FPS V100 b=1)&lt;/li&gt; &#xA; &lt;li&gt;YOLOv7-w6 (54.6% AP, 84 FPS V100 b=1) by &lt;code&gt;+1200%&lt;/code&gt; FPS faster than Dual-Swin-T C-M-RCNN (53.6% AP, 6.5 FPS V100 b=1)&lt;/li&gt; &#xA; &lt;li&gt;YOLOv7x (52.9% AP, 114 FPS V100 b=1) by &lt;code&gt;+150%&lt;/code&gt; FPS faster than PPYOLOE-X (51.9% AP, 45 FPS V100 b=1)&lt;/li&gt; &#xA; &lt;li&gt;YOLOv7 (51.2% AP, 161 FPS V100 b=1) by &lt;code&gt;+180%&lt;/code&gt; FPS faster than YOLOX-X (51.1% AP, 58 FPS V100 b=1)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/4096485/179425274-f55a36d4-8450-4471-816b-8c105841effd.jpg&#34; alt=&#34;more5&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/4096485/177675030-a929ee00-0eba-4d93-95c2-225231d0fd61.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/4096485/177688869-d75e0c36-63af-46ec-bdbd-81dbb281f257.png&#34; alt=&#34;yolov7_640_1280&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Scaled-YOLOv4:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;paper (CVPR 2021)&lt;/strong&gt;: &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Scaled-YOLOv4_Scaling_Cross_Stage_Partial_Network_CVPR_2021_paper.html&#34;&gt;https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Scaled-YOLOv4_Scaling_Cross_Stage_Partial_Network_CVPR_2021_paper.html&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;source code - Pytorch (use to reproduce results):&lt;/strong&gt; &lt;a href=&#34;https://github.com/WongKinYiu/ScaledYOLOv4&#34;&gt;https://github.com/WongKinYiu/ScaledYOLOv4&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;source code - Darknet:&lt;/strong&gt; &lt;a href=&#34;https://github.com/AlexeyAB/darknet&#34;&gt;https://github.com/AlexeyAB/darknet&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Medium:&lt;/strong&gt; &lt;a href=&#34;https://alexeyab84.medium.com/scaled-yolo-v4-is-the-best-neural-network-for-object-detection-on-ms-coco-dataset-39dfa22fa982?source=friends_link&amp;amp;sk=c8553bfed861b1a7932f739d26f487c8&#34;&gt;https://alexeyab84.medium.com/scaled-yolo-v4-is-the-best-neural-network-for-object-detection-on-ms-coco-dataset-39dfa22fa982?source=friends_link&amp;amp;sk=c8553bfed861b1a7932f739d26f487c8&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;YOLOv4:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;paper:&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2004.10934&#34;&gt;https://arxiv.org/abs/2004.10934&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;source code:&lt;/strong&gt; &lt;a href=&#34;https://github.com/AlexeyAB/darknet&#34;&gt;https://github.com/AlexeyAB/darknet&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Wiki:&lt;/strong&gt; &lt;a href=&#34;https://github.com/AlexeyAB/darknet/wiki&#34;&gt;https://github.com/AlexeyAB/darknet/wiki&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;useful links:&lt;/strong&gt; &lt;a href=&#34;https://medium.com/@alexeyab84/yolov4-the-most-accurate-real-time-neural-network-on-ms-coco-dataset-73adfd3602fe?source=friends_link&amp;amp;sk=6039748846bbcf1d960c3061542591d7&#34;&gt;https://medium.com/@alexeyab84/yolov4-the-most-accurate-real-time-neural-network-on-ms-coco-dataset-73adfd3602fe?source=friends_link&amp;amp;sk=6039748846bbcf1d960c3061542591d7&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For more information see the &lt;a href=&#34;http://pjreddie.com/darknet&#34;&gt;Darknet project website&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt; &lt;b&gt;Expand&lt;/b&gt; &lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/4096485/146988929-1ed0cbec-1e01-4ad0-b42c-808dcef32994.png&#34; alt=&#34;yolo_progress&#34;&gt; &lt;a href=&#34;https://paperswithcode.com/sota/object-detection-on-coco&#34;&gt;https://paperswithcode.com/sota/object-detection-on-coco&lt;/a&gt;&lt;/p&gt; &#xA; &lt;hr&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/4096485/112776361-281d8380-9048-11eb-8083-8728b12dcd55.png&#34; alt=&#34;scaled_yolov4&#34;&gt; AP50:95 - FPS (Tesla V100) Paper: &lt;a href=&#34;https://arxiv.org/abs/2011.08036&#34;&gt;https://arxiv.org/abs/2011.08036&lt;/a&gt;&lt;/p&gt; &#xA; &lt;hr&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/4096485/101363015-e5c21200-38b1-11eb-986f-b3e516e05977.png&#34; alt=&#34;YOLOv4Tiny&#34;&gt;&lt;/p&gt; &#xA; &lt;hr&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/4096485/90338826-06114c80-dff5-11ea-9ba2-8eb63a7409b3.png&#34; alt=&#34;YOLOv4&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/4096485/90338805-e5e18d80-dff4-11ea-8a68-5710956256ff.png&#34; alt=&#34;OpenCV_TRT&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{https://doi.org/10.48550/arxiv.2207.02696,&#xA;  doi = {10.48550/ARXIV.2207.02696},&#xA;  url = {https://arxiv.org/abs/2207.02696},&#xA;  author = {Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark},&#xA;  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},&#xA;  title = {YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors},&#xA;  publisher = {arXiv},&#xA;  year = {2022}, &#xA;  copyright = {arXiv.org perpetual, non-exclusive license}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{bochkovskiy2020yolov4,&#xA;      title={YOLOv4: Optimal Speed and Accuracy of Object Detection}, &#xA;      author={Alexey Bochkovskiy and Chien-Yao Wang and Hong-Yuan Mark Liao},&#xA;      year={2020},&#xA;      eprint={2004.10934},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;@InProceedings{Wang_2021_CVPR,&#xA;    author    = {Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark},&#xA;    title     = {{Scaled-YOLOv4}: Scaling Cross Stage Partial Network},&#xA;    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},&#xA;    month     = {June},&#xA;    year      = {2021},&#xA;    pages     = {13029-13038}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>