<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-09-01T02:05:49Z</updated>
  <subtitle>Monthly Trending of C in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>RfidResearchGroup/ChameleonUltra</title>
    <updated>2023-09-01T02:05:49Z</updated>
    <id>tag:github.com,2023-09-01:/RfidResearchGroup/ChameleonUltra</id>
    <link href="https://github.com/RfidResearchGroup/ChameleonUltra" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The new generation chameleon based on NRF52840 makes the performance of card emulation more stable. And gave the chameleon the ability to read, write, and decrypt cards.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RfidResearchGroup/ChameleonUltra/main/docs/images/ultra-logo.png&#34; alt=&#34;logo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RfidResearchGroup/ChameleonUltra/main/docs/images/ultra-overview.png&#34; alt=&#34;ultra picture&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;ChameleonUltra Authorized Distributors&lt;/h1&gt; &#xA;&lt;p&gt;Europe: &lt;a href=&#34;https://lab401.com/&#34;&gt;Lab401&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;United States: &lt;a href=&#34;https://hackerwarehouse.com/&#34;&gt;Hackerwarehouse&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Anywhere else: &lt;a href=&#34;https://sneaktechnology.com&#34;&gt;Sneaktechnology&lt;/a&gt; / &lt;a href=&#34;https://proxgrind.aliexpress.com/store/1101312023&#34;&gt;Aliexpress by RRG&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;What is it and how to use ?&lt;/h1&gt; &#xA;&lt;p&gt;Read the &lt;a href=&#34;https://raw.githubusercontent.com/RfidResearchGroup/ChameleonUltra/main/docs/README.md&#34;&gt;available documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Compatible applications&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/GameTec-live/ChameleonUltraGUI&#34;&gt;ChameleonUltraGUI&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Videos&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;Beware some of the instructions might have changed since recording, theck the current documentation in doubt!&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=VGpAeitNXH0&#34;&gt;Downloading and compiling the official CLI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=rHH7iqbX3nY&#34;&gt;Downloading ChameleonUltraGUI&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Official channels&lt;/h1&gt; &#xA;&lt;p&gt;Where do you find the community?&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://t.ly/d4_C&#34;&gt;RFID Hacking community discord server&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Software/chameleon-dev for firmware and clients development discussions&lt;/li&gt; &#xA;   &lt;li&gt;Devices/chameleon-ultra for usage discussions&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>brunodev85/winlator</title>
    <updated>2023-09-01T02:05:49Z</updated>
    <id>tag:github.com,2023-09-01:/brunodev85/winlator</id>
    <link href="https://github.com/brunodev85/winlator" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/brunodev85/winlator/main/logo.png&#34; width=&#34;376&#34; height=&#34;128&#34; alt=&#34;Winlator Logo&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Winlator&lt;/h1&gt; &#xA;&lt;p&gt;Winlator is an Android application that lets you to run Windows (x86_64) applications with Wine and Box86/Box64.&lt;/p&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download and install the APK from &lt;a href=&#34;https://github.com/brunodev85/Winlator/releases&#34;&gt;GitHub Releases&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Download the OBB file (main.1.com.winlator.obb) and put it into the directory &lt;code&gt;/storage/emulated/0/Android/obb/com.winlator&lt;/code&gt; (create it if it doesn&#39;t exist)&lt;/li&gt; &#xA; &lt;li&gt;Launch the app and wait for the installation process to finish&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=9E4wnKf2OsI&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/9E4wnKf2OsI/2.jpg&#34; alt=&#34;Play on Youtube&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=czEn4uT3Ja8&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/czEn4uT3Ja8/2.jpg&#34; alt=&#34;Play on Youtube&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=eD36nxfT_Z0&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/eD36nxfT_Z0/2.jpg&#34; alt=&#34;Play on Youtube&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Credits and Third-party apps&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ubuntu RootFs (&lt;a href=&#34;https://releases.ubuntu.com/focal&#34;&gt;Focal Fossa&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Wine (&lt;a href=&#34;https://www.winehq.org/&#34;&gt;winehq.org&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Box86/Box64 by &lt;a href=&#34;https://github.com/ptitSeb&#34;&gt;ptitseb&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;PRoot (&lt;a href=&#34;https://proot-me.github.io&#34;&gt;proot-me.github.io&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Mesa3D (&lt;a href=&#34;https://www.mesa3d.org&#34;&gt;mesa3d.org&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;DXVK (&lt;a href=&#34;https://github.com/doitsujin/dxvk&#34;&gt;github.com/doitsujin/dxvk&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;D8VK (&lt;a href=&#34;https://github.com/AlpyneDreams/d8vk&#34;&gt;github.com/AlpyneDreams/d8vk&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Thanks to &lt;a href=&#34;https://github.com/alexvorxx&#34;&gt;alexvorxx&lt;/a&gt; for the Mesa mods and build instructions&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>marella/ctransformers</title>
    <updated>2023-09-01T02:05:49Z</updated>
    <id>tag:github.com,2023-09-01:/marella/ctransformers</id>
    <link href="https://github.com/marella/ctransformers" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Python bindings for the Transformer models implemented in C/C++ using GGML library.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;a href=&#34;https://github.com/marella/ctransformers&#34;&gt;CTransformers&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/ctransformers/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/ctransformers&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/marella/ctransformers/actions/workflows/tests.yml&#34;&gt;&lt;img src=&#34;https://github.com/marella/ctransformers/actions/workflows/tests.yml/badge.svg?sanitize=true&#34; alt=&#34;tests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/marella/ctransformers/actions/workflows/build.yml&#34;&gt;&lt;img src=&#34;https://github.com/marella/ctransformers/actions/workflows/build.yml/badge.svg?sanitize=true&#34; alt=&#34;build&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;Python bindings for the Transformer models implemented in C/C++ using &lt;a href=&#34;https://github.com/ggerganov/ggml&#34;&gt;GGML&lt;/a&gt; library.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Also see &lt;a href=&#34;https://github.com/marella/chatdocs&#34;&gt;ChatDocs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/marella/ctransformers/main/#supported-models&#34;&gt;Supported Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/marella/ctransformers/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/marella/ctransformers/main/#usage&#34;&gt;Usage&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/marella/ctransformers/main/#transformers&#34;&gt;ðŸ¤— Transformers&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/marella/ctransformers/main/#langchain&#34;&gt;LangChain&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/marella/ctransformers/main/#gpu&#34;&gt;GPU&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/marella/ctransformers/main/#gptq&#34;&gt;GPTQ&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/marella/ctransformers/main/#documentation&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/marella/ctransformers/main/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Supported Models&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Models&lt;/th&gt; &#xA;   &lt;th&gt;Model Type&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;CUDA&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Metal&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;GPT-2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;gpt2&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;GPT-J, GPT4All-J&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;gptj&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;GPT-NeoX, StableLM&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;gpt_neox&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Falcon&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;falcon&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;âœ…&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;LLaMA, LLaMA 2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;llama&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;âœ…&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;MPT&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;mpt&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;âœ…&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;StarCoder, StarChat&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;gpt_bigcode&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;âœ…&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Dolly V2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;dolly-v2&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Replit&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;replit&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install ctransformers&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;It provides a unified interface for all models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;from ctransformers import AutoModelForCausalLM&#xA;&#xA;llm = AutoModelForCausalLM.from_pretrained(&#34;/path/to/ggml-model.bin&#34;, model_type=&#34;gpt2&#34;)&#xA;&#xA;print(llm(&#34;AI is going to&#34;))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1GMhYMUAv_TyZkpfvUI1NirM8-9mCXQyL&#34;&gt;Run in Google Colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;To stream the output, set &lt;code&gt;stream=True&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;for text in llm(&#34;AI is going to&#34;, stream=True):&#xA;    print(text, end=&#34;&#34;, flush=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can load models from Hugging Face Hub directly:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;llm = AutoModelForCausalLM.from_pretrained(&#34;marella/gpt-2-ggml&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If a model repo has multiple model files (&lt;code&gt;.bin&lt;/code&gt; or &lt;code&gt;.gguf&lt;/code&gt; files), specify a model file using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;llm = AutoModelForCausalLM.from_pretrained(&#34;marella/gpt-2-ggml&#34;, model_file=&#34;ggml-model.bin&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a id=&#34;transformers&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;ðŸ¤— Transformers&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; This is an experimental feature and may change in the future.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;To use it with ðŸ¤— Transformers, create model and tokenizer using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;from ctransformers import AutoModelForCausalLM, AutoTokenizer&#xA;&#xA;model = AutoModelForCausalLM.from_pretrained(&#34;marella/gpt-2-ggml&#34;, hf=True)&#xA;tokenizer = AutoTokenizer.from_pretrained(model)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1FVSLfTJ2iBbQ1oU2Rqz0MkpJbaB_5Got&#34;&gt;Run in Google Colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can use ðŸ¤— Transformers text generation pipeline:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;from transformers import pipeline&#xA;&#xA;pipe = pipeline(&#34;text-generation&#34;, model=model, tokenizer=tokenizer)&#xA;print(pipe(&#34;AI is going to&#34;, max_new_tokens=256))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can use ðŸ¤— Transformers generation &lt;a href=&#34;https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig&#34;&gt;parameters&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;pipe(&#34;AI is going to&#34;, max_new_tokens=256, do_sample=True, temperature=0.8, repetition_penalty=1.1)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can use ðŸ¤— Transformers tokenizers:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;from ctransformers import AutoModelForCausalLM&#xA;from transformers import AutoTokenizer&#xA;&#xA;model = AutoModelForCausalLM.from_pretrained(&#34;marella/gpt-2-ggml&#34;, hf=True)  # Load model from GGML model repo.&#xA;tokenizer = AutoTokenizer.from_pretrained(&#34;gpt2&#34;)  # Load tokenizer from original model repo.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;LangChain&lt;/h3&gt; &#xA;&lt;p&gt;It is integrated into LangChain. See &lt;a href=&#34;https://python.langchain.com/docs/ecosystem/integrations/ctransformers&#34;&gt;LangChain docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;GPU&lt;/h3&gt; &#xA;&lt;p&gt;To run some of the model layers on GPU, set the &lt;code&gt;gpu_layers&lt;/code&gt; parameter:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;llm = AutoModelForCausalLM.from_pretrained(&#34;TheBloke/Llama-2-7B-GGML&#34;, gpu_layers=50)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1Ihn7iPCYiqlTotpkqa1tOhUIpJBrJ1Tp&#34;&gt;Run in Google Colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;CUDA&lt;/h4&gt; &#xA;&lt;p&gt;Install CUDA libraries using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install ctransformers[cuda]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;ROCm&lt;/h4&gt; &#xA;&lt;p&gt;To enable ROCm support, install the &lt;code&gt;ctransformers&lt;/code&gt; package using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;CT_HIPBLAS=1 pip install ctransformers --no-binary ctransformers&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Metal&lt;/h4&gt; &#xA;&lt;p&gt;To enable Metal support, install the &lt;code&gt;ctransformers&lt;/code&gt; package using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;CT_METAL=1 pip install ctransformers --no-binary ctransformers&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;GPTQ&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; This is an experimental feature and only LLaMA models are supported using &lt;a href=&#34;https://github.com/turboderp/exllama&#34;&gt;ExLlama&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Install additional dependencies using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install ctransformers[gptq]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Load a GPTQ model using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;llm = AutoModelForCausalLM.from_pretrained(&#34;TheBloke/Llama-2-7B-GPTQ&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1SzHslJ4CiycMOgrppqecj4VYCWFnyrN0&#34;&gt;Run in Google Colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;If model name or path doesn&#39;t contain the word &lt;code&gt;gptq&lt;/code&gt; then specify &lt;code&gt;model_type=&#34;gptq&#34;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;It can also be used with LangChain. Low-level APIs are not fully supported.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;!-- API_DOCS --&gt; &#xA;&lt;h3&gt;Config&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Parameter&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Type&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Default&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;top_k&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;The top-k value to use for sampling.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;40&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;top_p&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;float&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;The top-p value to use for sampling.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;0.95&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;temperature&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;float&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;The temperature to use for sampling.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;0.8&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;repetition_penalty&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;float&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;The repetition penalty to use for sampling.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;1.1&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;last_n_tokens&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;The number of last tokens to use for repetition penalty.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;64&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;seed&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;The seed value to use for sampling tokens.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;-1&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;max_new_tokens&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;The maximum number of new tokens to generate.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;256&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;stop&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;List[str]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;A list of sequences to stop generation when encountered.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;stream&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;bool&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Whether to stream the generated text.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;reset&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;bool&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Whether to reset the model state before generating text.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;True&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;batch_size&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;The batch size to use for evaluating tokens in a single prompt.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;8&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;threads&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;The number of threads to use for evaluating tokens.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;-1&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;context_length&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;The maximum context length to use.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;-1&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;gpu_layers&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;The number of layers to run on GPU.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;0&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Currently only LLaMA, MPT and Falcon models support the &lt;code&gt;context_length&lt;/code&gt; parameter.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;&lt;kbd&gt;class&lt;/kbd&gt; &lt;code&gt;AutoModelForCausalLM&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;&lt;kbd&gt;classmethod&lt;/kbd&gt; &lt;code&gt;AutoModelForCausalLM.from_pretrained&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from_pretrained(&#xA;    model_path_or_repo_id: str,&#xA;    model_type: Optional[str] = None,&#xA;    model_file: Optional[str] = None,&#xA;    config: Optional[ctransformers.hub.AutoConfig] = None,&#xA;    lib: Optional[str] = None,&#xA;    local_files_only: bool = False,&#xA;    revision: Optional[str] = None,&#xA;    hf: bool = False,&#xA;    **kwargs&#xA;) â†’ LLM&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Loads the language model from a local file or remote repo.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Args:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;model_path_or_repo_id&lt;/code&gt;&lt;/b&gt;: The path to a model file or directory or the name of a Hugging Face Hub model repo.&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;model_type&lt;/code&gt;&lt;/b&gt;: The model type.&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;model_file&lt;/code&gt;&lt;/b&gt;: The name of the model file in repo or directory.&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;config&lt;/code&gt;&lt;/b&gt;: &lt;code&gt;AutoConfig&lt;/code&gt; object.&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;lib&lt;/code&gt;&lt;/b&gt;: The path to a shared library or one of &lt;code&gt;avx2&lt;/code&gt;, &lt;code&gt;avx&lt;/code&gt;, &lt;code&gt;basic&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;local_files_only&lt;/code&gt;&lt;/b&gt;: Whether or not to only look at local files (i.e., do not try to download the model).&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;revision&lt;/code&gt;&lt;/b&gt;: The specific model version to use. It can be a branch name, a tag name, or a commit id.&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;hf&lt;/code&gt;&lt;/b&gt;: Whether to create a Hugging Face Transformers model.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Returns:&lt;/strong&gt; &lt;code&gt;LLM&lt;/code&gt; object.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;kbd&gt;class&lt;/kbd&gt; &lt;code&gt;LLM&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;&lt;kbd&gt;method&lt;/kbd&gt; &lt;code&gt;LLM.__init__&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;__init__(&#xA;    model_path: str,&#xA;    model_type: Optional[str] = None,&#xA;    config: Optional[ctransformers.llm.Config] = None,&#xA;    lib: Optional[str] = None&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Loads the language model from a local file.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Args:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;model_path&lt;/code&gt;&lt;/b&gt;: The path to a model file.&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;model_type&lt;/code&gt;&lt;/b&gt;: The model type.&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;config&lt;/code&gt;&lt;/b&gt;: &lt;code&gt;Config&lt;/code&gt; object.&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;lib&lt;/code&gt;&lt;/b&gt;: The path to a shared library or one of &lt;code&gt;avx2&lt;/code&gt;, &lt;code&gt;avx&lt;/code&gt;, &lt;code&gt;basic&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;&lt;kbd&gt;property&lt;/kbd&gt; LLM.bos_token_id&lt;/h5&gt; &#xA;&lt;p&gt;The beginning-of-sequence token.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;&lt;kbd&gt;property&lt;/kbd&gt; LLM.config&lt;/h5&gt; &#xA;&lt;p&gt;The config object.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;&lt;kbd&gt;property&lt;/kbd&gt; LLM.context_length&lt;/h5&gt; &#xA;&lt;p&gt;The context length of model.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;&lt;kbd&gt;property&lt;/kbd&gt; LLM.embeddings&lt;/h5&gt; &#xA;&lt;p&gt;The input embeddings.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;&lt;kbd&gt;property&lt;/kbd&gt; LLM.eos_token_id&lt;/h5&gt; &#xA;&lt;p&gt;The end-of-sequence token.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;&lt;kbd&gt;property&lt;/kbd&gt; LLM.logits&lt;/h5&gt; &#xA;&lt;p&gt;The unnormalized log probabilities.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;&lt;kbd&gt;property&lt;/kbd&gt; LLM.model_path&lt;/h5&gt; &#xA;&lt;p&gt;The path to the model file.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;&lt;kbd&gt;property&lt;/kbd&gt; LLM.model_type&lt;/h5&gt; &#xA;&lt;p&gt;The model type.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;&lt;kbd&gt;property&lt;/kbd&gt; LLM.pad_token_id&lt;/h5&gt; &#xA;&lt;p&gt;The padding token.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;&lt;kbd&gt;property&lt;/kbd&gt; LLM.vocab_size&lt;/h5&gt; &#xA;&lt;p&gt;The number of tokens in vocabulary.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;&lt;kbd&gt;method&lt;/kbd&gt; &lt;code&gt;LLM.detokenize&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;detokenize(tokens: Sequence[int], decode: bool = True) â†’ Union[str, bytes]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Converts a list of tokens to text.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Args:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;tokens&lt;/code&gt;&lt;/b&gt;: The list of tokens.&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;decode&lt;/code&gt;&lt;/b&gt;: Whether to decode the text as UTF-8 string.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Returns:&lt;/strong&gt; The combined text of all tokens.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;&lt;kbd&gt;method&lt;/kbd&gt; &lt;code&gt;LLM.embed&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;embed(&#xA;    input: Union[str, Sequence[int]],&#xA;    batch_size: Optional[int] = None,&#xA;    threads: Optional[int] = None&#xA;) â†’ List[float]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Computes embeddings for a text or list of tokens.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Currently only LLaMA and Falcon models support embeddings.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;Args:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;input&lt;/code&gt;&lt;/b&gt;: The input text or list of tokens to get embeddings for.&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;batch_size&lt;/code&gt;&lt;/b&gt;: The batch size to use for evaluating tokens in a single prompt. Default: &lt;code&gt;8&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;threads&lt;/code&gt;&lt;/b&gt;: The number of threads to use for evaluating tokens. Default: &lt;code&gt;-1&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Returns:&lt;/strong&gt; The input embeddings.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;&lt;kbd&gt;method&lt;/kbd&gt; &lt;code&gt;LLM.eval&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;eval(&#xA;    tokens: Sequence[int],&#xA;    batch_size: Optional[int] = None,&#xA;    threads: Optional[int] = None&#xA;) â†’ None&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Evaluates a list of tokens.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Args:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;tokens&lt;/code&gt;&lt;/b&gt;: The list of tokens to evaluate.&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;batch_size&lt;/code&gt;&lt;/b&gt;: The batch size to use for evaluating tokens in a single prompt. Default: &lt;code&gt;8&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;threads&lt;/code&gt;&lt;/b&gt;: The number of threads to use for evaluating tokens. Default: &lt;code&gt;-1&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;&lt;kbd&gt;method&lt;/kbd&gt; &lt;code&gt;LLM.generate&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;generate(&#xA;    tokens: Sequence[int],&#xA;    top_k: Optional[int] = None,&#xA;    top_p: Optional[float] = None,&#xA;    temperature: Optional[float] = None,&#xA;    repetition_penalty: Optional[float] = None,&#xA;    last_n_tokens: Optional[int] = None,&#xA;    seed: Optional[int] = None,&#xA;    batch_size: Optional[int] = None,&#xA;    threads: Optional[int] = None,&#xA;    reset: Optional[bool] = None&#xA;) â†’ Generator[int, NoneType, NoneType]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Generates new tokens from a list of tokens.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Args:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;tokens&lt;/code&gt;&lt;/b&gt;: The list of tokens to generate tokens from.&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;top_k&lt;/code&gt;&lt;/b&gt;: The top-k value to use for sampling. Default: &lt;code&gt;40&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;top_p&lt;/code&gt;&lt;/b&gt;: The top-p value to use for sampling. Default: &lt;code&gt;0.95&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;temperature&lt;/code&gt;&lt;/b&gt;: The temperature to use for sampling. Default: &lt;code&gt;0.8&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;repetition_penalty&lt;/code&gt;&lt;/b&gt;: The repetition penalty to use for sampling. Default: &lt;code&gt;1.1&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;last_n_tokens&lt;/code&gt;&lt;/b&gt;: The number of last tokens to use for repetition penalty. Default: &lt;code&gt;64&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;seed&lt;/code&gt;&lt;/b&gt;: The seed value to use for sampling tokens. Default: &lt;code&gt;-1&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;batch_size&lt;/code&gt;&lt;/b&gt;: The batch size to use for evaluating tokens in a single prompt. Default: &lt;code&gt;8&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;threads&lt;/code&gt;&lt;/b&gt;: The number of threads to use for evaluating tokens. Default: &lt;code&gt;-1&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;reset&lt;/code&gt;&lt;/b&gt;: Whether to reset the model state before generating text. Default: &lt;code&gt;True&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Returns:&lt;/strong&gt; The generated tokens.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;&lt;kbd&gt;method&lt;/kbd&gt; &lt;code&gt;LLM.is_eos_token&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;is_eos_token(token: int) â†’ bool&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Checks if a token is an end-of-sequence token.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Args:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;token&lt;/code&gt;&lt;/b&gt;: The token to check.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Returns:&lt;/strong&gt; &lt;code&gt;True&lt;/code&gt; if the token is an end-of-sequence token else &lt;code&gt;False&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;&lt;kbd&gt;method&lt;/kbd&gt; &lt;code&gt;LLM.reset&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;reset() â†’ None&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Resets the model state.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;&lt;kbd&gt;method&lt;/kbd&gt; &lt;code&gt;LLM.sample&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sample(&#xA;    top_k: Optional[int] = None,&#xA;    top_p: Optional[float] = None,&#xA;    temperature: Optional[float] = None,&#xA;    repetition_penalty: Optional[float] = None,&#xA;    last_n_tokens: Optional[int] = None,&#xA;    seed: Optional[int] = None&#xA;) â†’ int&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Samples a token from the model.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Args:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;top_k&lt;/code&gt;&lt;/b&gt;: The top-k value to use for sampling. Default: &lt;code&gt;40&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;top_p&lt;/code&gt;&lt;/b&gt;: The top-p value to use for sampling. Default: &lt;code&gt;0.95&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;temperature&lt;/code&gt;&lt;/b&gt;: The temperature to use for sampling. Default: &lt;code&gt;0.8&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;repetition_penalty&lt;/code&gt;&lt;/b&gt;: The repetition penalty to use for sampling. Default: &lt;code&gt;1.1&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;last_n_tokens&lt;/code&gt;&lt;/b&gt;: The number of last tokens to use for repetition penalty. Default: &lt;code&gt;64&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;seed&lt;/code&gt;&lt;/b&gt;: The seed value to use for sampling tokens. Default: &lt;code&gt;-1&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Returns:&lt;/strong&gt; The sampled token.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;&lt;kbd&gt;method&lt;/kbd&gt; &lt;code&gt;LLM.tokenize&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tokenize(text: str, add_bos_token: Optional[bool] = None) â†’ List[int]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Converts a text into list of tokens.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Args:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;text&lt;/code&gt;&lt;/b&gt;: The text to tokenize.&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;add_bos_token&lt;/code&gt;&lt;/b&gt;: Whether to add the beginning-of-sequence token.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Returns:&lt;/strong&gt; The list of tokens.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;&lt;kbd&gt;method&lt;/kbd&gt; &lt;code&gt;LLM.__call__&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;__call__(&#xA;    prompt: str,&#xA;    max_new_tokens: Optional[int] = None,&#xA;    top_k: Optional[int] = None,&#xA;    top_p: Optional[float] = None,&#xA;    temperature: Optional[float] = None,&#xA;    repetition_penalty: Optional[float] = None,&#xA;    last_n_tokens: Optional[int] = None,&#xA;    seed: Optional[int] = None,&#xA;    batch_size: Optional[int] = None,&#xA;    threads: Optional[int] = None,&#xA;    stop: Optional[Sequence[str]] = None,&#xA;    stream: Optional[bool] = None,&#xA;    reset: Optional[bool] = None&#xA;) â†’ Union[str, Generator[str, NoneType, NoneType]]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Generates text from a prompt.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Args:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;prompt&lt;/code&gt;&lt;/b&gt;: The prompt to generate text from.&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;max_new_tokens&lt;/code&gt;&lt;/b&gt;: The maximum number of new tokens to generate. Default: &lt;code&gt;256&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;top_k&lt;/code&gt;&lt;/b&gt;: The top-k value to use for sampling. Default: &lt;code&gt;40&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;top_p&lt;/code&gt;&lt;/b&gt;: The top-p value to use for sampling. Default: &lt;code&gt;0.95&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;temperature&lt;/code&gt;&lt;/b&gt;: The temperature to use for sampling. Default: &lt;code&gt;0.8&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;repetition_penalty&lt;/code&gt;&lt;/b&gt;: The repetition penalty to use for sampling. Default: &lt;code&gt;1.1&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;last_n_tokens&lt;/code&gt;&lt;/b&gt;: The number of last tokens to use for repetition penalty. Default: &lt;code&gt;64&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;seed&lt;/code&gt;&lt;/b&gt;: The seed value to use for sampling tokens. Default: &lt;code&gt;-1&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;batch_size&lt;/code&gt;&lt;/b&gt;: The batch size to use for evaluating tokens in a single prompt. Default: &lt;code&gt;8&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;threads&lt;/code&gt;&lt;/b&gt;: The number of threads to use for evaluating tokens. Default: &lt;code&gt;-1&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;stop&lt;/code&gt;&lt;/b&gt;: A list of sequences to stop generation when encountered. Default: &lt;code&gt;None&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;stream&lt;/code&gt;&lt;/b&gt;: Whether to stream the generated text. Default: &lt;code&gt;False&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;&lt;code&gt;reset&lt;/code&gt;&lt;/b&gt;: Whether to reset the model state before generating text. Default: &lt;code&gt;True&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Returns:&lt;/strong&gt; The generated text.&lt;/p&gt; &#xA;&lt;!-- API_DOCS --&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/marella/ctransformers/raw/main/LICENSE&#34;&gt;MIT&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>