<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Haskell Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-16T01:30:47Z</updated>
  <subtitle>Daily Trending of Haskell in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>haskell-works/tasty-testmanager</title>
    <updated>2024-04-16T01:30:47Z</updated>
    <id>tag:github.com,2024-04-16:/haskell-works/tasty-testmanager</id>
    <link href="https://github.com/haskell-works/tasty-testmanager" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;</summary>
  </entry>
  <entry>
    <title>haskell/tokenize</title>
    <updated>2024-04-16T01:30:47Z</updated>
    <id>tag:github.com,2024-04-16:/haskell/tokenize</id>
    <link href="https://github.com/haskell/tokenize" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Simple tokenizer for English text&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;http://hackage.haskell.org/package/tokenize&#34;&gt;&lt;img src=&#34;https://img.shields.io/hackage/v/tokenize.svg?label=Hackage&amp;amp;color=informational&#34; alt=&#34;Hackage version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://stackage.org/nightly/package/tokenize&#34;&gt;&lt;img src=&#34;https://stackage.org/package/tokenize/badge/nightly&#34; alt=&#34;tokenize on Stackage Nightly&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.stackage.org/package/tokenize&#34;&gt;&lt;img src=&#34;https://www.stackage.org/package/tokenize/badge/lts?label=Stackage&#34; alt=&#34;Stackage LTS version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/haskell/tokenize/actions&#34;&gt;&lt;img src=&#34;https://github.com/haskell/tokenize/workflows/Haskell-CI/badge.svg?sanitize=true&#34; alt=&#34;Cabal build&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;The &lt;code&gt;tokenize&lt;/code&gt; Package&lt;/h1&gt; &#xA;&lt;p&gt;Simple tokenizer for English text. Used by &lt;code&gt;hackage-server&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;http://hackage.haskell.org/package/tokenize&#34;&gt;&lt;code&gt;tokenize&lt;/code&gt; on Hackage&lt;/a&gt; for more information.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>neurallambda/automata</title>
    <updated>2024-04-16T01:30:47Z</updated>
    <id>tag:github.com,2024-04-16:/neurallambda/automata</id>
    <link href="https://github.com/neurallambda/automata" rel="alternate"></link>
    <summary type="html">&lt;p&gt;generate synthetic data for training finite state machines/pushdown automata/turing machines&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Automata&lt;/h1&gt; &#xA;&lt;p&gt;A tool for generating synthetic data that results from grammars writen for various automata (eg Finite State Machines, Pushdown Automata and Turing Machines). Grammars are saved as &lt;code&gt;json&lt;/code&gt;, and you can build 1000 valid strings that match this grammar with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ automata -i my_grammar.json -o output.json -n 1000&#xA;aXb&#xA;aaXbb&#xA;aaaXbbb&#xA;aaaaXbbbb&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This is intended to help train the &lt;a href=&#34;https://github.com/neurallambda/neurallambda&#34;&gt;&lt;code&gt;neurallambda&lt;/code&gt;&lt;/a&gt; project and confer reasoning ability to LLMs.&lt;/p&gt; &#xA;&lt;h2&gt;Layout&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;rules/  # example grammars&#xA;data/   # data generated from those rules&#xA;app/    # the CLI tool&#xA;src/    # the supporting library&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;PDA example&lt;/h2&gt; &#xA;&lt;p&gt;There&#39;s a grammar that looks like &lt;code&gt;a^nXb^n&lt;/code&gt;, which is any string like &lt;code&gt;aaXbb&lt;/code&gt; and &lt;code&gt;aaaaaXbbbbb&lt;/code&gt;, where the same number of &lt;code&gt;b&lt;/code&gt;s follow the same number of &lt;code&gt;a&lt;/code&gt;s (and in the middle is &lt;code&gt;X&lt;/code&gt;). You cannot write a regex for this (try defining that &lt;code&gt;n&lt;/code&gt;), but you can write a pushdown automata that recognizes it. A transition function involves the following mappings between states (slightly simplified, see &lt;a href=&#34;https://raw.githubusercontent.com/neurallambda/automata/master/rules/anbn.json&#34;&gt;&lt;code&gt;rules/anbn.json&lt;/code&gt;&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;{&#xA;    &#34;machine&#34;: &#34;PDA&#34;,&#xA;    &#34;symbols&#34;: [&#34;a&#34;, &#34;b&#34;, &#34;X&#34;],&#xA;    &#34;rules&#34;: [&#xA;         # input symbol&#xA;           |  # current state         # resulting state&#xA;           |    |  # top of stack     |    # stack operation&#xA;           |    |          |          |      |&#xA;        [[&#34;a&#34;, &#34;INITIAL&#34;, &#34;A&#34;],     [&#34;Q1&#34;, [&#34;push&#34;, &#34;A&#34;]]],&#xA;        [[&#34;X&#34;, &#34;Q1&#34;, &#34;A&#34;],          [&#34;Q2&#34;, &#34;nullop&#34;]],&#xA;        [[&#34;b&#34;, &#34;Q2&#34;, &#34;A&#34;],          [&#34;Q2&#34;, &#34;pop&#34;]],&#xA;    ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Notice there are exactly 5 items in each transition rule for a PDA, and they come as 3 on the Left and 2 on the Right. These represent the left and right hand side of the relation that defines a PDA. In english, the rules say:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;given an input symbol &#39;a&#39;, if I&#39;m in the state Q1, and the top of the stack is &#34;A&#34;, let&#39;s transition by staying in state Q1, and push an &#34;A&#34; on the stack.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;if the input symbol is &#39;X&#39;, we know we need to transition to &#39;Q2&#39;, which implies we&#39;re gonna start looking for &#39;b&#39;s.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;each time we see a &#39;b&#39;, pop the stack. This is how we can ensure that the same number of pushes and pops happen, ie, same number of &#39;b&#39;s and &#39;a&#39;s.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Again, see &lt;a href=&#34;https://raw.githubusercontent.com/neurallambda/automata/master/rules/anbn.json&#34;&gt;&lt;code&gt;rules/anbn.json&lt;/code&gt;&lt;/a&gt;) for the full implementation.&lt;/p&gt; &#xA;&lt;h2&gt;About the tool&lt;/h2&gt; &#xA;&lt;p&gt;Haskell was chosen because it produces trustworthy code, especially in this department of computer languages. When I next face a funky loss while training some NN, I want to minimize the risk that the data was improperly generated.&lt;/p&gt; &#xA;&lt;p&gt;A simple &lt;code&gt;typeclass&lt;/code&gt; is offered to allow the easy (if you speak haskell) specification of new Automata:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-haskell&#34;&gt;class Machine m a (s :: Type) where&#xA;  data L m a s -- ^ the Left side of a delta function/relation&#xA;  data R m a s -- ^ the Right side of a delta function/relation&#xA;  data S m a s -- ^ the State of the Machine&#xA;  -- | update the state (ex apply stack ops)&#xA;  action :: R m a s -&amp;gt; S m a s -&amp;gt; S m a s&#xA;  -- | build an input (ex add a peek at the top of a stack)&#xA;  mkL :: a -&amp;gt; S m a s -&amp;gt; L m a s&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;There are probably infinite things that are Turing Complete formulations of Turing Machines, and I hope to represent some in this library:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The classic tape model&lt;/li&gt; &#xA; &lt;li&gt;An FSM with 1 queue&lt;/li&gt; &#xA; &lt;li&gt;An FSM with 2 stacks&lt;/li&gt; &#xA; &lt;li&gt;An FSM with multiple queues&lt;/li&gt; &#xA; &lt;li&gt;An FSM with multiple tapes&lt;/li&gt; &#xA; &lt;li&gt;Something (i&#39;m not sure what) with addressable memory&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&#34;I just got here and have no clue what you&#39;re doing&#34;&lt;/h2&gt; &#xA;&lt;p&gt;That&#39;s because I&#39;m bad at explaining things.&lt;/p&gt; &#xA;&lt;p&gt;I have a project called &lt;a href=&#34;https://github.com/neurallambda/neurallambda&#34;&gt;&lt;code&gt;neurallambda&lt;/code&gt;&lt;/a&gt; where I hope to build architectures that augment traditional architectures with reasoning ability. Checkout the readme if you&#39;re skeptical, it explains why I don&#39;t think the current batch of AI has reasoning.&lt;/p&gt; &#xA;&lt;p&gt;So for experimenting on these architectures I need data. &lt;a href=&#34;https://github.com/neurallambda/awesome-reasoning&#34;&gt;Current datasets&lt;/a&gt; are largely around Natural Language, and probably GPT generated (therefore their reasoning is circumspect). It&#39;ll be easier to train on super simple perfectly trustworthy datasets, therefore why not generate them ourselves? That&#39;s where this lib comes in.&lt;/p&gt;</summary>
  </entry>
</feed>