<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Haskell Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-09-05T01:28:52Z</updated>
  <subtitle>Daily Trending of Haskell in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Bodigrim/tasty-bench</title>
    <updated>2023-09-05T01:28:52Z</updated>
    <id>tag:github.com,2023-09-05:/Bodigrim/tasty-bench</id>
    <link href="https://github.com/Bodigrim/tasty-bench" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Featherlight benchmark framework, drop-in replacement for criterion and gauge.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;tasty-bench &lt;a href=&#34;https://hackage.haskell.org/package/tasty-bench&#34;&gt;&lt;img src=&#34;http://img.shields.io/hackage/v/tasty-bench.svg?sanitize=true&#34; alt=&#34;Hackage&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://stackage.org/lts/package/tasty-bench&#34;&gt;&lt;img src=&#34;http://stackage.org/package/tasty-bench/badge/lts&#34; alt=&#34;Stackage LTS&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://stackage.org/nightly/package/tasty-bench&#34;&gt;&lt;img src=&#34;http://stackage.org/package/tasty-bench/badge/nightly&#34; alt=&#34;Stackage Nightly&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;Featherlight benchmark framework (only one file!) for performance measurement with API mimicking &lt;a href=&#34;http://hackage.haskell.org/package/criterion&#34;&gt;&lt;code&gt;criterion&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;http://hackage.haskell.org/package/gauge&#34;&gt;&lt;code&gt;gauge&lt;/code&gt;&lt;/a&gt;. A prominent feature is built-in comparison against previous runs and between benchmarks.&lt;/p&gt; &#xA;&lt;!-- MarkdownTOC autolink=&#34;true&#34; --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Bodigrim/tasty-bench/master/#how-lightweight-is-it&#34;&gt;How lightweight is it?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Bodigrim/tasty-bench/master/#how-is-it-possible&#34;&gt;How is it possible?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Bodigrim/tasty-bench/master/#how-to-switch&#34;&gt;How to switch?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Bodigrim/tasty-bench/master/#how-to-write-a-benchmark&#34;&gt;How to write a benchmark?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Bodigrim/tasty-bench/master/#how-to-read-results&#34;&gt;How to read results?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Bodigrim/tasty-bench/master/#wall-clock-time-vs-cpu-time&#34;&gt;Wall-clock time vs. CPU time&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Bodigrim/tasty-bench/master/#statistical-model&#34;&gt;Statistical model&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Bodigrim/tasty-bench/master/#memory-usage&#34;&gt;Memory usage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Bodigrim/tasty-bench/master/#combining-tests-and-benchmarks&#34;&gt;Combining tests and benchmarks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Bodigrim/tasty-bench/master/#troubleshooting&#34;&gt;Troubleshooting&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Bodigrim/tasty-bench/master/#isolating-interfering-benchmarks&#34;&gt;Isolating interfering benchmarks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Bodigrim/tasty-bench/master/#comparison-against-baseline&#34;&gt;Comparison against baseline&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Bodigrim/tasty-bench/master/#comparison-between-benchmarks&#34;&gt;Comparison between benchmarks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Bodigrim/tasty-bench/master/#plotting-results&#34;&gt;Plotting results&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Bodigrim/tasty-bench/master/#build-flags&#34;&gt;Build flags&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Bodigrim/tasty-bench/master/#command-line-options&#34;&gt;Command-line options&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Bodigrim/tasty-bench/master/#custom-command-line-options&#34;&gt;Custom command-line options&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- /MarkdownTOC --&gt; &#xA;&lt;h2&gt;How lightweight is it?&lt;/h2&gt; &#xA;&lt;p&gt;There is only one source file &lt;code&gt;Test.Tasty.Bench&lt;/code&gt; and no non-boot dependencies except &lt;a href=&#34;http://hackage.haskell.org/package/tasty&#34;&gt;&lt;code&gt;tasty&lt;/code&gt;&lt;/a&gt;. So if you already depend on &lt;code&gt;tasty&lt;/code&gt; for a test suite, there is nothing else to install.&lt;/p&gt; &#xA;&lt;p&gt;Compare this to &lt;code&gt;criterion&lt;/code&gt; (10+ modules, 50+ dependencies) and &lt;code&gt;gauge&lt;/code&gt; (40+ modules, depends on &lt;code&gt;basement&lt;/code&gt; and &lt;code&gt;vector&lt;/code&gt;). A build on a clean machine is up to 16x faster than &lt;code&gt;criterion&lt;/code&gt; and up to 4x faster than &lt;code&gt;gauge&lt;/code&gt;. A build without dependencies is up to 6x faster than &lt;code&gt;criterion&lt;/code&gt; and up to 8x faster than &lt;code&gt;gauge&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;tasty-bench&lt;/code&gt; is a native Haskell library and works everywhere, where GHC does, including WASM. We support a full range of architectures (&lt;code&gt;i386&lt;/code&gt;, &lt;code&gt;amd64&lt;/code&gt;, &lt;code&gt;armhf&lt;/code&gt;, &lt;code&gt;arm64&lt;/code&gt;, &lt;code&gt;ppc64le&lt;/code&gt;, &lt;code&gt;s390x&lt;/code&gt;) and operating systems (Linux, Windows, macOS, FreeBSD, OpenBSD, NetBSD), plus any GHC from 7.0 to 9.6.&lt;/p&gt; &#xA;&lt;h2&gt;How is it possible?&lt;/h2&gt; &#xA;&lt;p&gt;Our benchmarks are literally regular &lt;code&gt;tasty&lt;/code&gt; tests, so we can leverage all existing machinery for command-line options, resource management, structuring, listing and filtering benchmarks, running and reporting results. It also means that &lt;code&gt;tasty-bench&lt;/code&gt; can be used in conjunction with other &lt;code&gt;tasty&lt;/code&gt; ingredients.&lt;/p&gt; &#xA;&lt;p&gt;Unlike &lt;code&gt;criterion&lt;/code&gt; and &lt;code&gt;gauge&lt;/code&gt; we use a very simple statistical model described below. This is arguably a questionable choice, but it works pretty well in practice. A rare developer is sufficiently well-versed in probability theory to make sense and use of all numbers generated by &lt;code&gt;criterion&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;How to switch?&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://cabal.readthedocs.io/en/3.4/cabal-package.html#pkg-field-mixins&#34;&gt;Cabal mixins&lt;/a&gt; allow to taste &lt;code&gt;tasty-bench&lt;/code&gt; instead of &lt;code&gt;criterion&lt;/code&gt; or &lt;code&gt;gauge&lt;/code&gt; without changing a single line of code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cabal&#34;&gt;cabal-version: 2.0&#xA;&#xA;benchmark foo&#xA;  ...&#xA;  build-depends:&#xA;    tasty-bench&#xA;  mixins:&#xA;    tasty-bench (Test.Tasty.Bench as Criterion, Test.Tasty.Bench as Criterion.Main, Test.Tasty.Bench as Gauge, Test.Tasty.Bench as Gauge.Main)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This works vice versa as well: if you use &lt;code&gt;tasty-bench&lt;/code&gt;, but at some point need a more comprehensive statistical analysis, it is easy to switch temporarily back to &lt;code&gt;criterion&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;How to write a benchmark?&lt;/h2&gt; &#xA;&lt;p&gt;Benchmarks are declared in a separate section of &lt;code&gt;cabal&lt;/code&gt; file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cabal&#34;&gt;cabal-version:   2.0&#xA;name:            bench-fibo&#xA;version:         0.0&#xA;build-type:      Simple&#xA;synopsis:        Example of a benchmark&#xA;&#xA;benchmark bench-fibo&#xA;  main-is:       BenchFibo.hs&#xA;  type:          exitcode-stdio-1.0&#xA;  build-depends: base, tasty-bench&#xA;  ghc-options:   &#34;-with-rtsopts=-A32m&#34;&#xA;  if impl(ghc &amp;gt;= 8.6)&#xA;    ghc-options: -fproc-alignment=64&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And here is &lt;code&gt;BenchFibo.hs&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-haskell&#34;&gt;import Test.Tasty.Bench&#xA;&#xA;fibo :: Int -&amp;gt; Integer&#xA;fibo n = if n &amp;lt; 2 then toInteger n else fibo (n - 1) + fibo (n - 2)&#xA;&#xA;main :: IO ()&#xA;main = defaultMain&#xA;  [ bgroup &#34;Fibonacci numbers&#34;&#xA;    [ bench &#34;fifth&#34;     $ nf fibo  5&#xA;    , bench &#34;tenth&#34;     $ nf fibo 10&#xA;    , bench &#34;twentieth&#34; $ nf fibo 20&#xA;    ]&#xA;  ]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Since &lt;code&gt;tasty-bench&lt;/code&gt; provides an API compatible with &lt;code&gt;criterion&lt;/code&gt;, one can refer to &lt;a href=&#34;http://www.serpentine.com/criterion/tutorial.html#how-to-write-a-benchmark-suite&#34;&gt;its documentation&lt;/a&gt; for more examples.&lt;/p&gt; &#xA;&lt;h2&gt;How to read results?&lt;/h2&gt; &#xA;&lt;p&gt;Running the example above (&lt;code&gt;cabal bench&lt;/code&gt; or &lt;code&gt;stack bench&lt;/code&gt;) results in the following output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;All&#xA;  Fibonacci numbers&#xA;    fifth:     OK (2.13s)&#xA;       63 ns ± 3.4 ns&#xA;    tenth:     OK (1.71s)&#xA;      809 ns ±  73 ns&#xA;    twentieth: OK (3.39s)&#xA;      104 μs ± 4.9 μs&#xA;&#xA;All 3 tests passed (7.25s)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The output says that, for instance, the first benchmark was repeatedly executed for 2.13 seconds (wall-clock time), its predicted mean CPU time was 63 nanoseconds and means of individual samples do not often diverge from it further than ±3.4 nanoseconds (double standard deviation). Take standard deviation numbers with a grain of salt; there are lies, damned lies, and statistics.&lt;/p&gt; &#xA;&lt;h2&gt;Wall-clock time vs. CPU time&lt;/h2&gt; &#xA;&lt;p&gt;What time are we talking about? Both &lt;code&gt;criterion&lt;/code&gt; and &lt;code&gt;gauge&lt;/code&gt; by default report wall-clock time, which is affected by any other application which runs concurrently. Ideally benchmarks are executed on a dedicated server without any other load, but — let&#39;s face the truth — most of developers run benchmarks on a laptop with a hundred other services and a window manager, and watch videos while waiting for benchmarks to finish. That&#39;s the cause of a notorious &#34;variance introduced by outliers: 88% (severely inflated)&#34; warning.&lt;/p&gt; &#xA;&lt;p&gt;To alleviate this issue &lt;code&gt;tasty-bench&lt;/code&gt; measures CPU time by &lt;code&gt;getCPUTime&lt;/code&gt; instead of wall-clock time by default. It does not provide a perfect isolation from other processes (e. g., if CPU cache is spoiled by others, populating data back from RAM is your burden), but is a bit more stable.&lt;/p&gt; &#xA;&lt;p&gt;Caveat: this means that for multithreaded algorithms &lt;code&gt;tasty-bench&lt;/code&gt; reports total elapsed CPU time across all cores, while &lt;code&gt;criterion&lt;/code&gt; and &lt;code&gt;gauge&lt;/code&gt; print maximum of core&#39;s wall-clock time. It also means that by default &lt;code&gt;tasty-bench&lt;/code&gt; does not measure time spent out of process, e. g., calls to other executables. To work around this limitation use &lt;code&gt;--time-mode&lt;/code&gt; command-line option or set it locally via &lt;code&gt;TimeMode&lt;/code&gt; option.&lt;/p&gt; &#xA;&lt;h2&gt;Statistical model&lt;/h2&gt; &#xA;&lt;p&gt;Here is a procedure used by &lt;code&gt;tasty-bench&lt;/code&gt; to measure execution time:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Set $n \leftarrow 1$.&lt;/li&gt; &#xA; &lt;li&gt;Measure execution time $t_n$ of $n$ iterations and execution time $t_{2n}$ of $2n$ iterations.&lt;/li&gt; &#xA; &lt;li&gt;Find $t$ which minimizes deviation of $(nt,2nt)$ from $(t_n,t_{2n})$, namely $t \leftarrow (t_n + 2t_{2n}) / 5n$.&lt;/li&gt; &#xA; &lt;li&gt;If deviation is small enough (see &lt;code&gt;--stdev&lt;/code&gt; below) or time is running out soon (see &lt;code&gt;--timeout&lt;/code&gt; below), return $t$ as a mean execution time.&lt;/li&gt; &#xA; &lt;li&gt;Otherwise set $n \leftarrow 2n$ and jump back to Step 2.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;This is roughly similar to the linear regression approach which &lt;code&gt;criterion&lt;/code&gt; takes, but we fit only two last points. This allows us to simplify away all heavy-weight statistical analysis. More importantly, earlier measurements, which are presumably shorter and noisier, do not affect overall result. This is in contrast to &lt;code&gt;criterion&lt;/code&gt;, which fits all measurements and is biased to use more data points corresponding to shorter runs (it employs $n \leftarrow 1.05n$ progression).&lt;/p&gt; &#xA;&lt;p&gt;Mean time and its deviation does not say much about the distribution of individual timings. E. g., imagine a computation which (according to a coarse system timer) takes either 0 ms or 1 ms with equal probability. While one would be able to establish that its mean time is 0.5 ms with a very small deviation, this does not imply that individual measurements are anywhere near 0.5 ms. Even assuming an infinite precision of a system timer, the distribution of individual times is not known to be &lt;a href=&#34;https://en.wikipedia.org/wiki/Normal_distribution&#34;&gt;normal&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Obligatory disclaimer: statistics is a tricky matter, there is no one-size-fits-all approach. In the absence of a good theory simplistic approaches are as (un)sound as obscure ones. Those who seek statistical soundness should rather collect raw data and process it themselves using a proper statistical toolbox. Data reported by &lt;code&gt;tasty-bench&lt;/code&gt; is only of indicative and comparative significance.&lt;/p&gt; &#xA;&lt;h2&gt;Memory usage&lt;/h2&gt; &#xA;&lt;p&gt;Configuring RTS to collect GC statistics (e. g., via &lt;code&gt;cabal bench --benchmark-options &#39;+RTS -T&#39;&lt;/code&gt; or &lt;code&gt;stack bench --ba &#39;+RTS -T&#39;&lt;/code&gt;) enables &lt;code&gt;tasty-bench&lt;/code&gt; to estimate and report memory usage:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;All&#xA;  Fibonacci numbers&#xA;    fifth:     OK (2.13s)&#xA;       63 ns ± 3.4 ns, 223 B  allocated,   0 B  copied, 2.0 MB peak memory&#xA;    tenth:     OK (1.71s)&#xA;      809 ns ±  73 ns, 2.3 KB allocated,   0 B  copied, 4.0 MB peak memory&#xA;    twentieth: OK (3.39s)&#xA;      104 μs ± 4.9 μs, 277 KB allocated,  59 B  copied, 5.0 MB peak memory&#xA;&#xA;All 3 tests passed (7.25s)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This data is reported as per &lt;code&gt;RTSStats&lt;/code&gt; fields: &lt;code&gt;allocated_bytes&lt;/code&gt;, &lt;code&gt;copied_bytes&lt;/code&gt; and &lt;code&gt;max_mem_in_use_bytes&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Combining tests and benchmarks&lt;/h2&gt; &#xA;&lt;p&gt;When optimizing an existing function, it is important to check that its observable behavior remains unchanged. One can rebuild both tests and benchmarks after each change, but it would be more convenient to run sanity checks within benchmark itself. Since our benchmarks are compatible with &lt;code&gt;tasty&lt;/code&gt; tests, we can easily do so.&lt;/p&gt; &#xA;&lt;p&gt;Imagine you come up with a faster function &lt;code&gt;myFibo&lt;/code&gt; to generate Fibonacci numbers:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-haskell&#34;&gt;import Test.Tasty.Bench&#xA;import Test.Tasty.QuickCheck -- from tasty-quickcheck package&#xA;&#xA;fibo :: Int -&amp;gt; Integer&#xA;fibo n = if n &amp;lt; 2 then toInteger n else fibo (n - 1) + fibo (n - 2)&#xA;&#xA;myFibo :: Int -&amp;gt; Integer&#xA;myFibo n = if n &amp;lt; 3 then toInteger n else myFibo (n - 1) + myFibo (n - 2)&#xA;&#xA;main :: IO ()&#xA;main = Test.Tasty.Bench.defaultMain -- not Test.Tasty.defaultMain&#xA;  [ bench &#34;fibo   20&#34; $ nf fibo   20&#xA;  , bench &#34;myFibo 20&#34; $ nf myFibo 20&#xA;  , testProperty &#34;myFibo = fibo&#34; $ \n -&amp;gt; fibo n === myFibo n&#xA;  ]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This outputs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;All&#xA;  fibo   20:     OK (3.02s)&#xA;    104 μs ± 4.9 μs&#xA;  myFibo 20:     OK (1.99s)&#xA;     71 μs ± 5.3 μs&#xA;  myFibo = fibo: FAIL&#xA;    *** Failed! Falsified (after 5 tests and 1 shrink):&#xA;    2&#xA;    1 /= 2&#xA;    Use --quickcheck-replay=927711 to reproduce.&#xA;&#xA;1 out of 3 tests failed (5.03s)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We see that &lt;code&gt;myFibo&lt;/code&gt; is indeed significantly faster than &lt;code&gt;fibo&lt;/code&gt;, but unfortunately does not do the same thing. One should probably look for another way to speed up generation of Fibonacci numbers.&lt;/p&gt; &#xA;&lt;h2&gt;Troubleshooting&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;If benchmarks take too long, set &lt;code&gt;--timeout&lt;/code&gt; to limit execution time of individual benchmarks, and &lt;code&gt;tasty-bench&lt;/code&gt; will do its best to fit into a given time frame. Without &lt;code&gt;--timeout&lt;/code&gt; we rerun benchmarks until achieving a target precision set by &lt;code&gt;--stdev&lt;/code&gt;, which in a noisy environment of a modern laptop with GUI may take a lot of time.&lt;/p&gt; &lt;p&gt;While &lt;code&gt;criterion&lt;/code&gt; runs each benchmark at least for 5 seconds, &lt;code&gt;tasty-bench&lt;/code&gt; is happy to conclude earlier, if it does not compromise the quality of results. In our experiments &lt;code&gt;tasty-bench&lt;/code&gt; suites tend to finish earlier, even if some individual benchmarks take longer than with &lt;code&gt;criterion&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;A common source of noisiness is garbage collection. Setting a larger allocation area (&lt;em&gt;nursery&lt;/em&gt;) is often a good idea, either via &lt;code&gt;cabal bench --benchmark-options &#39;+RTS -A32m&#39;&lt;/code&gt; or &lt;code&gt;stack bench --ba &#39;+RTS -A32m&#39;&lt;/code&gt;. Alternatively bake it into &lt;code&gt;cabal&lt;/code&gt; file as &lt;code&gt;ghc-options: &#34;-with-rtsopts=-A32m&#34;&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Never compile benchmarks with &lt;code&gt;-fstatic-argument-transformation&lt;/code&gt;, because it breaks a trick we use to force GHC into reevaluation of the same function application over and over again.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If benchmark results look malformed like below, make sure that you are invoking &lt;code&gt;Test.Tasty.Bench.defaultMain&lt;/code&gt; and not &lt;code&gt;Test.Tasty.defaultMain&lt;/code&gt; (the difference is &lt;code&gt;consoleBenchReporter&lt;/code&gt; vs. &lt;code&gt;consoleTestReporter&lt;/code&gt;):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;All&#xA;  fibo 20:       OK (1.46s)&#xA;    Response {respEstimate = Estimate {estMean = Measurement {measTime = 87496728, measAllocs = 0, measCopied = 0}, estStdev = 694487}, respIfSlower = FailIfSlower Infinity, respIfFaster = FailIfFaster Infinity}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If benchmarks fail with an error message&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Unhandled resource. Probably a bug in the runner you&#39;re using.&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;or&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Unexpected state of the resource (NotCreated) in getResource. Report as a tasty bug.&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;this is likely caused by &lt;code&gt;env&lt;/code&gt; or &lt;code&gt;envWithCleanup&lt;/code&gt; affecting benchmarks structure. You can use &lt;code&gt;env&lt;/code&gt; to read test data from &lt;code&gt;IO&lt;/code&gt;, but not to read benchmark names or affect their hierarchy in other way. This is a fundamental restriction of &lt;code&gt;tasty&lt;/code&gt; to list and filter benchmarks without launching missiles.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If benchmarks fail with &lt;code&gt;Test dependencies form a loop&lt;/code&gt; or &lt;code&gt;Test dependencies have cycles&lt;/code&gt;, this is likely because of &lt;code&gt;bcompare&lt;/code&gt;, which compares a benchmark with itself. Locating a benchmark in a global environment may be tricky, please refer to &lt;a href=&#34;https://github.com/UnkindPartition/tasty#patterns&#34;&gt;&lt;code&gt;tasty&lt;/code&gt; documentation&lt;/a&gt; for details and consider using &lt;code&gt;locateBenchmark&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;When seeing&lt;/p&gt; &lt;pre&gt;&lt;code&gt;This benchmark takes more than 100 seconds. Consider setting --timeout, if this is unexpected (or to silence this warning).&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;do follow the advice: abort benchmarks and pass &lt;code&gt;-t100&lt;/code&gt; or similar. Unless you are benchmarking a very computationally expensive function, a single benchmark should stabilize after a couple of seconds. This warning is a sign that your environment is too noisy, in which case &lt;code&gt;tasty-bench&lt;/code&gt; will continue trying with exponentially longer intervals, often unproductively.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The following error can be thrown when benchmarks are built with &lt;code&gt;ghc-options: -threaded&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Benchmarks must not be run concurrently. Please pass -j1 and/or avoid +RTS -N.&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The underlying cause is that &lt;code&gt;tasty&lt;/code&gt; runs tests concurrently, which is harmful for reliable performance measurements. Make sure to use &lt;code&gt;tasty-bench &amp;gt;= 0.3.4&lt;/code&gt; and invoke &lt;code&gt;Test.Tasty.Bench.defaultMain&lt;/code&gt; and not &lt;code&gt;Test.Tasty.defaultMain&lt;/code&gt;. Note that &lt;code&gt;localOption (NumThreads 1)&lt;/code&gt; quashes the warning, but does not eliminate the cause.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If benchmarks using GHC 9.4.4+ segfault on Windows, check that you are not using non-moving garbage collector &lt;code&gt;--nonmoving-gc&lt;/code&gt;. This is likely caused by &lt;a href=&#34;https://gitlab.haskell.org/ghc/ghc/-/issues/23003&#34;&gt;GHC issue&lt;/a&gt;. Previous releases of &lt;code&gt;tasty-bench&lt;/code&gt; recommended enabling &lt;code&gt;--nonmoving-gc&lt;/code&gt; to stabilise benchmarks, but it&#39;s discouraged now.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you see&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;lt;stdout&amp;gt;: commitBuffer: invalid argument (cannot encode character &#39;\177&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;it means that your locale does not support UTF-8. &lt;code&gt;tasty-bench&lt;/code&gt; makes an effort to force locale to UTF-8, but sometimes, when benchmarks are a part of a larger application, it&#39;s &lt;a href=&#34;https://gitlab.haskell.org/ghc/ghc/-/issues/23606&#34;&gt;impossible&lt;/a&gt; to do so. In such case run &lt;code&gt;locale -a&lt;/code&gt; to list available locales and set a UTF-8-capable one (e. g., &lt;code&gt;export LANG=C.UTF-8&lt;/code&gt;) before starting benchmarks.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Isolating interfering benchmarks&lt;/h2&gt; &#xA;&lt;p&gt;One difficulty of benchmarking in Haskell is that it is hard to isolate benchmarks so that they do not interfere. Changing the order of benchmarks or skipping some of them has an effect on heap&#39;s layout and thus affects garbage collection. This issue is well attested in &lt;a href=&#34;https://github.com/haskell/criterion/issues/166&#34;&gt;both&lt;/a&gt; &lt;a href=&#34;https://github.com/haskell/criterion/issues/60&#34;&gt;&lt;code&gt;criterion&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://github.com/vincenthz/hs-gauge/issues/2&#34;&gt;&lt;code&gt;gauge&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Usually (but not always) skipping some benchmarks speeds up remaining ones. That&#39;s because once a benchmark allocated heap which for some reason was not promptly released afterwards (e. g., it forced a top-level thunk in an underlying library), all further benchmarks are slowed down by garbage collector processing this additional amount of live data over and over again.&lt;/p&gt; &#xA;&lt;p&gt;There are several mitigation strategies. First of all, giving garbage collector more breathing space by &lt;code&gt;+RTS -A32m&lt;/code&gt; (or more) is often good enough.&lt;/p&gt; &#xA;&lt;p&gt;Further, avoid using top-level bindings to store large test data. Once such thunks are forced, they remain allocated forever, which affects detrimentally subsequent unrelated benchmarks. Treat them as external data, supplied via &lt;code&gt;env&lt;/code&gt;: instead of&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-haskell&#34;&gt;largeData :: String&#xA;largeData = replicate 1000000 &#39;a&#39;&#xA;&#xA;main :: IO ()&#xA;main = defaultMain&#xA;  [ bench &#34;large&#34; $ nf length largeData, ... ]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;use&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-haskell&#34;&gt;import Control.DeepSeq (force)&#xA;import Control.Exception (evaluate)&#xA;&#xA;main :: IO ()&#xA;main = defaultMain&#xA;  [ env (evaluate (force (replicate 1000000 &#39;a&#39;))) $ \largeData -&amp;gt;&#xA;    bench &#34;large&#34; $ nf length largeData, ... ]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, as an ultimate measure to reduce interference between benchmarks, one can run each of them in a separate process. We do not quite recommend this approach, but if you are desperate, here is how:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cabal run -v0 all:benches -- -l | sed -e &#39;s/[\&#34;]/\\\\\\&amp;amp;/g&#39; | while read -r name; do cabal run -v0 all:benches -- -p &#39;$0 == &#34;&#39;&#34;$name&#34;&#39;&#34;&#39;; done&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This assumes that there is a single benchmark suite in the project and that benchmark names do not contain newlines.&lt;/p&gt; &#xA;&lt;h2&gt;Comparison against baseline&lt;/h2&gt; &#xA;&lt;p&gt;One can compare benchmark results against an earlier run in an automatic way.&lt;/p&gt; &#xA;&lt;p&gt;When using this feature, it&#39;s especially important to compile benchmarks with &lt;code&gt;ghc-options: &lt;/code&gt;&lt;a href=&#34;https://downloads.haskell.org/ghc/latest/docs/users_guide/debugging.html#ghc-flag--fproc-alignment&#34;&gt;&lt;code&gt;-fproc-alignment&lt;/code&gt;&lt;/a&gt;&lt;code&gt;=64&lt;/code&gt;, otherwise results could be skewed by intermittent changes in cache-line alignment.&lt;/p&gt; &#xA;&lt;p&gt;Firstly, run &lt;code&gt;tasty-bench&lt;/code&gt; with &lt;code&gt;--csv FILE&lt;/code&gt; key to dump results to &lt;code&gt;FILE&lt;/code&gt; in CSV format (it could be a good idea to set smaller &lt;code&gt;--stdev&lt;/code&gt;, if possible):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Name,Mean (ps),2*Stdev (ps)&#xA;All.Fibonacci numbers.fifth,48453,4060&#xA;All.Fibonacci numbers.tenth,637152,46744&#xA;All.Fibonacci numbers.twentieth,81369531,3342646&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now modify implementation and rerun benchmarks with &lt;code&gt;--baseline FILE&lt;/code&gt; key. This produces a report as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;All&#xA;  Fibonacci numbers&#xA;    fifth:     OK (0.44s)&#xA;       53 ns ± 2.7 ns,  8% more than baseline&#xA;    tenth:     OK (0.33s)&#xA;      641 ns ±  59 ns,       same as baseline&#xA;    twentieth: OK (0.36s)&#xA;       77 μs ± 6.4 μs,  5% less than baseline&#xA;&#xA;All 3 tests passed (1.50s)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also fail benchmarks, which deviate too far from baseline, using &lt;code&gt;--fail-if-slower&lt;/code&gt; and &lt;code&gt;--fail-if-faster&lt;/code&gt; options. For example, setting both of them to 6 will fail the first benchmark above (because it is more than 6% slower), but the last one still succeeds (even while it is measurably faster than baseline, deviation is less than 6%). Consider also using &lt;code&gt;--hide-successes&lt;/code&gt; to show only problematic benchmarks, or even &lt;a href=&#34;http://hackage.haskell.org/package/tasty-rerun&#34;&gt;&lt;code&gt;tasty-rerun&lt;/code&gt;&lt;/a&gt; package to focus on rerunning failing items only.&lt;/p&gt; &#xA;&lt;p&gt;If you wish to compare two CSV reports non-interactively, here is a handy &lt;code&gt;awk&lt;/code&gt; incantation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;awk &#39;BEGIN{FS=&#34;,&#34;;OFS=&#34;,&#34;;print &#34;Name,Old,New,Ratio&#34;}FNR==1{trueNF=NF;next}NF&amp;lt;trueNF{print &#34;Benchmark names should not contain newlines&#34;;exit 1}FNR==NR{oldTime=$(NF-trueNF+2);NF-=trueNF-1;a[$0]=oldTime;next}{newTime=$(NF-trueNF+2);NF-=trueNF-1;print $0,a[$0],newTime,newTime/a[$0];gs+=log(newTime/a[$0]);gc++}END{if(gc&amp;gt;0)print &#34;Geometric mean,,&#34;,exp(gs/gc)}&#39; old.csv new.csv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A larger shell snippet to compare two &lt;code&gt;git&lt;/code&gt; commits can be found in &lt;code&gt;compare_benches.sh&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Note that columns in CSV report are different from what &lt;code&gt;criterion&lt;/code&gt; or &lt;code&gt;gauge&lt;/code&gt; would produce. If names do not contain commas, missing columns can be faked this way:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;awk &#39;BEGIN{FS=&#34;,&#34;;OFS=&#34;,&#34;;print &#34;Name,Mean,MeanLB,MeanUB,Stddev,StddevLB,StddevUB&#34;}NR==1{trueNF=NF;next}NF&amp;lt;trueNF{print $0;next}{mean=$(NF-trueNF+2);stddev=$(NF-trueNF+3);NF-=trueNF-1;print $0,mean/1e12,mean/1e12,mean/1e12,stddev/2e12,stddev/2e12,stddev/2e12}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To fake &lt;code&gt;gauge&lt;/code&gt; in &lt;code&gt;--csvraw&lt;/code&gt; mode use&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;awk &#39;BEGIN{FS=&#34;,&#34;;OFS=&#34;,&#34;;print &#34;name,iters,time,cycles,cpuTime,utime,stime,maxrss,minflt,majflt,nvcsw,nivcsw,allocated,numGcs,bytesCopied,mutatorWallSeconds,mutatorCpuSeconds,gcWallSeconds,gcCpuSeconds&#34;}NR==1{trueNF=NF;next}NF&amp;lt;trueNF{print $0;next}{mean=$(NF-trueNF+2);fourth=$(NF-trueNF+4);fifth=$(NF-trueNF+5);sixth=$(NF-trueNF+6);NF-=trueNF-1;print $0,1,mean/1e12,0,mean/1e12,mean/1e12,0,sixth+0,0,0,0,0,fourth+0,0,fifth+0,0,0,0,0}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Comparison between benchmarks&lt;/h2&gt; &#xA;&lt;p&gt;You can also compare benchmarks to each other without any external tools, all in the comfort of your terminal.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-haskell&#34;&gt;import Test.Tasty.Bench&#xA;&#xA;fibo :: Int -&amp;gt; Integer&#xA;fibo n = if n &amp;lt; 2 then toInteger n else fibo (n - 1) + fibo (n - 2)&#xA;&#xA;main :: IO ()&#xA;main = defaultMain&#xA;  [ bgroup &#34;Fibonacci numbers&#34;&#xA;    [ bcompare &#34;tenth&#34;  $ bench &#34;fifth&#34;     $ nf fibo  5&#xA;    ,                     bench &#34;tenth&#34;     $ nf fibo 10&#xA;    , bcompare &#34;tenth&#34;  $ bench &#34;twentieth&#34; $ nf fibo 20&#xA;    ]&#xA;  ]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This produces a report, comparing mean times of &lt;code&gt;fifth&lt;/code&gt; and &lt;code&gt;twentieth&lt;/code&gt; to &lt;code&gt;tenth&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;All&#xA;  Fibonacci numbers&#xA;    fifth:     OK (16.56s)&#xA;      121 ns ± 2.6 ns, 0.08x&#xA;    tenth:     OK (6.84s)&#xA;      1.6 μs ±  31 ns&#xA;    twentieth: OK (6.96s)&#xA;      203 μs ± 4.1 μs, 128.36x&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To locate a baseline benchmark in a larger suite use &lt;code&gt;locateBenchmark&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;One can leverage comparisons between benchmarks to implement portable performance tests, expressing properties like &#34;this algorithm must be at least twice faster than that one&#34; or &#34;this operation should not be more than thrice slower than that&#34;. This can be achieved with &lt;code&gt;bcompareWithin&lt;/code&gt;, which takes an acceptable interval of performance as an argument.&lt;/p&gt; &#xA;&lt;h2&gt;Plotting results&lt;/h2&gt; &#xA;&lt;p&gt;Users can dump results into CSV with &lt;code&gt;--csv FILE&lt;/code&gt; and plot them using &lt;code&gt;gnuplot&lt;/code&gt; or other software. But for convenience there is also a built-in quick-and-dirty SVG plotting feature, which can be invoked by passing &lt;code&gt;--svg FILE&lt;/code&gt;. Here is a sample of its output:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://hackage.haskell.org/package/tasty-bench/src/example.svg?sanitize=true&#34; alt=&#34;Plotting&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Build flags&lt;/h2&gt; &#xA;&lt;p&gt;Build flags are a brittle subject and users do not normally need to touch them.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;If you find yourself in an environment, where &lt;code&gt;tasty&lt;/code&gt; is not available and you have access to boot packages only, you can still use &lt;code&gt;tasty-bench&lt;/code&gt;! Just copy &lt;code&gt;Test/Tasty/Bench.hs&lt;/code&gt; to your project (imagine it like a header-only C library). It will provide you with functions to build &lt;code&gt;Benchmarkable&lt;/code&gt; and run them manually via &lt;code&gt;measureCpuTime&lt;/code&gt;. This mode of operation can be also configured by disabling Cabal flag &lt;code&gt;tasty&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If results are amiss or oscillate wildly and adjusting &lt;code&gt;--timeout&lt;/code&gt; and &lt;code&gt;--stdev&lt;/code&gt; does not help, you may be interested to investigate individual timings of successive runs by enabling Cabal flag &lt;code&gt;debug&lt;/code&gt;. This will pipe raw data into &lt;code&gt;stderr&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Command-line options&lt;/h2&gt; &#xA;&lt;p&gt;Use &lt;code&gt;--help&lt;/code&gt; to list all command-line options.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;-p&lt;/code&gt;, &lt;code&gt;--pattern&lt;/code&gt;&lt;/p&gt; &lt;p&gt;This is a standard &lt;code&gt;tasty&lt;/code&gt; option, which allows filtering benchmarks by a pattern or &lt;code&gt;awk&lt;/code&gt; expression. Please refer to &lt;a href=&#34;https://github.com/UnkindPartition/tasty#patterns&#34;&gt;&lt;code&gt;tasty&lt;/code&gt; documentation&lt;/a&gt; for details.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;-t&lt;/code&gt;, &lt;code&gt;--timeout&lt;/code&gt;&lt;/p&gt; &lt;p&gt;This is a standard &lt;code&gt;tasty&lt;/code&gt; option, setting timeout for individual benchmarks in seconds. Use it when benchmarks tend to take too long: &lt;code&gt;tasty-bench&lt;/code&gt; will make an effort to report results (even if of subpar quality) before timeout. Setting timeout too tight (insufficient for at least three iterations) will result in a benchmark failure. One can adjust it locally for a group of benchmarks, e. g., &lt;code&gt;localOption (mkTimeout 100000000)&lt;/code&gt; for 100 seconds.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;--stdev&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Target relative standard deviation of measurements in percents (5% by default). Large values correspond to fast and loose benchmarks, and small ones to long and precise. It can also be adjusted locally for a group of benchmarks, e. g., &lt;code&gt;localOption (RelStDev 0.02)&lt;/code&gt;. If benchmarking takes far too long, consider setting &lt;code&gt;--timeout&lt;/code&gt;, which will interrupt benchmarks, potentially before reaching the target deviation.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;--csv&lt;/code&gt;&lt;/p&gt; &lt;p&gt;File to write results in CSV format.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;--baseline&lt;/code&gt;&lt;/p&gt; &lt;p&gt;File to read baseline results in CSV format (as produced by &lt;code&gt;--csv&lt;/code&gt;).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;--fail-if-slower&lt;/code&gt;, &lt;code&gt;--fail-if-faster&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Upper bounds of acceptable slow down / speed up in percents. If a benchmark is unacceptably slower / faster than baseline (see &lt;code&gt;--baseline&lt;/code&gt;), it will be reported as failed. Can be used in conjunction with a standard &lt;code&gt;tasty&lt;/code&gt; option &lt;code&gt;--hide-successes&lt;/code&gt; to show only problematic benchmarks. Both options can be adjusted locally for a group of benchmarks, e. g., &lt;code&gt;localOption (FailIfSlower 0.10)&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;--svg&lt;/code&gt;&lt;/p&gt; &lt;p&gt;File to plot results in SVG format.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;--time-mode&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Whether to measure CPU time (&lt;code&gt;cpu&lt;/code&gt;, default) or wall-clock time (&lt;code&gt;wall&lt;/code&gt;).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;+RTS -T&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Estimate and report memory usage.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Custom command-line options&lt;/h2&gt; &#xA;&lt;p&gt;As usual with &lt;code&gt;tasty&lt;/code&gt;, it is easy to extend benchmarks with custom command-line options. Here is an example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-haskell&#34;&gt;import Data.Proxy&#xA;import Test.Tasty.Bench&#xA;import Test.Tasty.Ingredients.Basic&#xA;import Test.Tasty.Options&#xA;import Test.Tasty.Runners&#xA;&#xA;newtype RandomSeed = RandomSeed Int&#xA;&#xA;instance IsOption RandomSeed where&#xA;  defaultValue = RandomSeed 42&#xA;  parseValue = fmap RandomSeed . safeRead&#xA;  optionName = pure &#34;seed&#34;&#xA;  optionHelp = pure &#34;Random seed used in benchmarks&#34;&#xA;&#xA;main :: IO ()&#xA;main = do&#xA;  let customOpts  = [Option (Proxy :: Proxy RandomSeed)]&#xA;      ingredients = includingOptions customOpts : benchIngredients&#xA;  opts &amp;lt;- parseOptions ingredients benchmarks&#xA;  let RandomSeed seed = lookupOption opts&#xA;  defaultMainWithIngredients ingredients benchmarks&#xA;&#xA;benchmarks :: Benchmark&#xA;benchmarks = bgroup &#34;All&#34; []&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>