<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Cuda Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-01-01T02:07:02Z</updated>
  <subtitle>Monthly Trending of Cuda in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Liu-xiandong/How_to_optimize_in_GPU</title>
    <updated>2024-01-01T02:07:02Z</updated>
    <id>tag:github.com,2024-01-01:/Liu-xiandong/How_to_optimize_in_GPU</id>
    <link href="https://github.com/Liu-xiandong/How_to_optimize_in_GPU" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This is a series of GPU optimization topics. Here we will introduce how to optimize the CUDA kernel in detail. I will introduce several basic kernel optimizations, including: elementwise, reduce, sgemv, sgemm, etc. The performance of these kernels is basically at or near the theoretical limit.&lt;/p&gt;&lt;hr&gt;</summary>
  </entry>
  <entry>
    <title>ashawkey/diff-gaussian-rasterization</title>
    <updated>2024-01-01T02:07:02Z</updated>
    <id>tag:github.com,2024-01-01:/ashawkey/diff-gaussian-rasterization</id>
    <link href="https://github.com/ashawkey/diff-gaussian-rasterization" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;</summary>
  </entry>
  <entry>
    <title>flashinfer-ai/flashinfer</title>
    <updated>2024-01-01T02:07:02Z</updated>
    <id>tag:github.com,2024-01-01:/flashinfer-ai/flashinfer</id>
    <link href="https://github.com/flashinfer-ai/flashinfer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;FlashInfer: Kernel Library for LLM Serving&lt;/p&gt;&lt;hr&gt;</summary>
  </entry>
</feed>