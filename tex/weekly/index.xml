<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TeX Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-05T01:41:42Z</updated>
  <subtitle>Weekly Trending of TeX in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ybayle/awesome-deep-learning-music</title>
    <updated>2024-05-05T01:41:42Z</updated>
    <id>tag:github.com,2024-05-05:/ybayle/awesome-deep-learning-music</id>
    <link href="https://github.com/ybayle/awesome-deep-learning-music" rel="alternate"></link>
    <summary type="html">&lt;p&gt;List of articles related to deep learning applied to music&lt;/p&gt;&lt;hr&gt;&lt;p&gt;⚠️ This repo is unmaintained. While the info are still relevant, contributions to keep it up to date are welcome! A good starting point are the articles referenced here: &lt;a href=&#34;https://github.com/ybayle/awesome-deep-learning-music/issues/5&#34;&gt;https://github.com/ybayle/awesome-deep-learning-music/issues/5&lt;/a&gt;&lt;/p&gt; &#xA;&lt;img align=&#34;right&#34; src=&#34;https://raw.githubusercontent.com/ybayle/awesome-deep-learning-music/master/fig/logo.png&#34;&gt; &#xA;&lt;h1&gt;Deep Learning for Music (DL4M) &lt;a href=&#34;https://github.com/sindresorhus/awesome&#34;&gt;&lt;img src=&#34;https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg?sanitize=true&#34; alt=&#34;Awesome&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;By &lt;a href=&#34;http://yannbayle.fr/english/index.php&#34;&gt;Yann Bayle&lt;/a&gt; (&lt;a href=&#34;http://yannbayle.fr/english/index.php&#34;&gt;Website&lt;/a&gt;, &lt;a href=&#34;https://github.com/ybayle&#34;&gt;GitHub&lt;/a&gt;) from LaBRI (&lt;a href=&#34;http://www.labri.fr/&#34;&gt;Website&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/labriOfficial/&#34;&gt;Twitter&lt;/a&gt;), Univ. Bordeaux (&lt;a href=&#34;https://www.u-bordeaux.fr/&#34;&gt;Website&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/univbordeaux&#34;&gt;Twitter&lt;/a&gt;), CNRS (&lt;a href=&#34;http://www.cnrs.fr/&#34;&gt;Website&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/CNRS&#34;&gt;Twitter&lt;/a&gt;) and SCRIME (&lt;a href=&#34;https://scrime.u-bordeaux.fr/&#34;&gt;Website&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; Non-exhaustive list of scientific articles on deep learning for music: &lt;a href=&#34;https://raw.githubusercontent.com/ybayle/awesome-deep-learning-music/master/#dl4m-summary&#34;&gt;summary&lt;/a&gt; (Article title, pdf link and code), &lt;a href=&#34;https://raw.githubusercontent.com/ybayle/awesome-deep-learning-music/master/dl4m.tsv&#34;&gt;details&lt;/a&gt; (table - more info), &lt;a href=&#34;https://raw.githubusercontent.com/ybayle/awesome-deep-learning-music/master/dl4m.bib&#34;&gt;details&lt;/a&gt; (bib - all info)&lt;/p&gt; &#xA;&lt;p&gt;The role of this curated list is to gather scientific articles, thesis and reports that use deep learning approaches applied to music. The list is currently under construction but feel free to contribute to the missing fields and to add other resources! To do so, please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/ybayle/awesome-deep-learning-music/master/#how-to-contribute&#34;&gt;How To Contribute&lt;/a&gt; section. The resources provided here come from my review of the state-of-the-art for my PhD Thesis for which an article is being written. There are already surveys on deep learning for &lt;a href=&#34;https://arxiv.org/pdf/1709.01620.pdf&#34;&gt;music generation&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/ftp/arxiv/papers/1708/1708.07524.pdf&#34;&gt;speech separation&lt;/a&gt; and &lt;a href=&#34;https://www.researchgate.net/profile/Seyed_Reza_Shahamiri/publication/319158024_Speaker_Identification_Features_Extraction_Methods_A_Systematic_Review/links/599e2816aca272dff12fdef1/Speaker-Identification-Features-Extraction-Methods-A-Systematic-Review.pdf&#34;&gt;speaker identification&lt;/a&gt;. However, these surveys do not cover music information retrieval tasks that are included in this repository.&lt;/p&gt; &#xA;&lt;h2&gt;Table of contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ybayle/awesome-deep-learning-music/master/#dl4m-summary&#34;&gt;DL4M summary&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ybayle/awesome-deep-learning-music/master/#dl4m-details&#34;&gt;DL4M details&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ybayle/awesome-deep-learning-music/master/#code-without-articles&#34;&gt;Code without articles&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ybayle/awesome-deep-learning-music/master/#statistics-and-visualisations&#34;&gt;Statistics and visualisations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ybayle/awesome-deep-learning-music/master/#advices-for-reviewers-of-dl4m-articles&#34;&gt;Advices for reviewers of dl4m articles&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ybayle/awesome-deep-learning-music/master/#how-to-contribute&#34;&gt;How To Contribute&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ybayle/awesome-deep-learning-music/master/#faq&#34;&gt;FAQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ybayle/awesome-deep-learning-music/master/#acronyms-used&#34;&gt;Acronyms used&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ybayle/awesome-deep-learning-music/master/#sources&#34;&gt;Sources&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ybayle/awesome-deep-learning-music/master/#contributors&#34;&gt;Contributors&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ybayle/awesome-deep-learning-music/master/#other-useful-related-lists-and-resources&#34;&gt;Other useful related lists&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ybayle/awesome-deep-learning-music/master/#cited-by&#34;&gt;Cited by&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ybayle/awesome-deep-learning-music/master/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;DL4M summary&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&amp;nbsp;Year&lt;/th&gt; &#xA;   &lt;th&gt;Articles, Thesis and Reports&lt;/th&gt; &#xA;   &lt;th&gt;Code&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1988&lt;/td&gt; &#xA;   &lt;td&gt;Neural net modeling of music&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1988&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=23933&#34;&gt;Creation by refinement: A creativity paradigm for gradient descent learning networks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1988&lt;/td&gt; &#xA;   &lt;td&gt;A sequential network design for musical applications&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1989&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.jstor.org/stable/3679550&#34;&gt;The representation of pitch in a neural net model of chord classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1989&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://quod.lib.umich.edu/cgi/p/pod/dod-idx/algorithms-for-music-composition.pdf?c=icmc;idno=bbp2372.1989.044;format=pdf&#34;&gt;Algorithms for music composition by neural nets: Improved CBR paradigms&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1989&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.jstor.org/stable/3679551&#34;&gt;A connectionist approach to algorithmic composition&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1994&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www-labs.iro.umontreal.ca/~pift6080/H09/documents/papers/mozer-music.pdf&#34;&gt;Neural network music composition by prediction: Exploring the benefits of psychoacoustic constraints and multi-scale processing&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1995&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.researchgate.net/publication/3622871_Automatic_source_identification_of_monophonic_musical_instrument_sounds&#34;&gt;Automatic source identification of monophonic musical instrument sounds&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1995&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://ieeexplore.ieee.org/abstract/document/514161/&#34;&gt;Neural network based model for classification of music type&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1997&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://repository.cmu.edu/cgi/viewcontent.cgi?article=1496&amp;amp;context=compsci&#34;&gt;A machine learning approach to musical style recognition&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1998&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.ri.cmu.edu/pub_files/pub1/soltau_hagen_1998_2/soltau_hagen_1998_2.pdf&#34;&gt;Recognition of music types&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1999&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://s3.amazonaws.com/academia.edu.documents/3551783/10.1.1.39.6248.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&amp;amp;Expires=1507055806&amp;amp;Signature=5mGzQc7bvJgUZYfXOmCX8eeNQOs%3D&amp;amp;response-content-disposition=inline%3B%20filename%3DMusical_networks_Parallel_distributed_pe.pdf&#34;&gt;Musical networks: Parallel distributed perception and performance&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2001&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.cs.smith.edu/~jfrankli/papers/CtColl01.pdf&#34;&gt;Multi-phase learning for jazz improvisation and interaction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2002&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.researchgate.net/profile/Giuseppe_Buzzanca/publication/228588086_A_supervised_learning_approach_to_musical_style_recognition/links/54b43ee90cf26833efd0109f.pdf&#34;&gt;A supervised learning approach to musical style recognition&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2002&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www-perso.iro.umontreal.ca/~eckdoug/papers/2002_ieee.pdf&#34;&gt;Finding temporal structure in music: Blues improvisation with LSTM recurrent networks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2002&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.researchgate.net/profile/Matija_Marolt/publication/2473938_Neural_Networks_for_Note_Onset_Detection_in_Piano_Music/links/00b49525efccc79fed000000.pdf&#34;&gt;Neural networks for note onset detection in piano music&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2004&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.murase.nuie.nagoya-u.ac.jp/~ide/res/paper/E04-conference-pablo-1.pdf&#34;&gt;A convolutional-kernel based approach for note onset detection in piano-solo audio signals&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2009&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://papers.nips.cc/paper/3674-unsupervised-feature-learning-for-audio-classification-using-convolutional-deep-belief-networks.pdf&#34;&gt;Unsupervised feature learning for audio classification using convolutional deep belief networks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2010&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://lbms03.cityu.edu.hk/theses/c_ftt/mphil-cs-b39478026f.pdf&#34;&gt;Audio musical genre classification using convolutional neural networks and pitch and tempo transformations&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2010&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.researchgate.net/profile/Antoni_Chan2/publication/44260643_Automatic_Musical_Pattern_Feature_Extraction_Using_Convolutional_Neural_Network/links/02e7e523dac6bb86b0000000.pdf&#34;&gt;Automatic musical pattern feature extraction using convolutional neural network&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2011&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.ismir2011.ismir.net/papers/PS6-3.pdf&#34;&gt;Audio-based music classification with a pretrained convolutional network&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2012&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://ieeexplore.ieee.org/abstract/document/6406762/&#34;&gt;Rethinking automatic chord recognition with convolutional neural networks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2012&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.294.2304&amp;amp;rep=rep1&amp;amp;type=pdf&#34;&gt;Moving beyond feature design: Deep architectures and automatic feature learning in music informatics&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2012&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://liris.cnrs.fr/Documents/Liris-5602.pdf&#34;&gt;Local-feature-map integration using convolutional neural networks for music genre classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2012&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pdfs.semanticscholar.org/099d/85f25e9336f48ff64287a4b53ee5fb64ab51.pdf&#34;&gt;Learning sparse feature representations for music annotation and retrieval&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2012&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.ismir2012.ismir.net/event/papers/139_ISMIR_2012.pdf&#34;&gt;Unsupervised learning of local features for music classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2013&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://ismir2013.ismir.net/wp-content/uploads/2013/09/69_Paper.pdf&#34;&gt;Multiscale approaches to music audio feature learning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2013&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://phenicx.upf.edu/system/files/publications/Schlueter_MML13.pdf&#34;&gt;Musical onset detection with convolutional neural networks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2013&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://papers.nips.cc/paper/5004-deep-content-based-music-recommendation.pdf&#34;&gt;Deep content-based music recommendation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2014&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pdfs.semanticscholar.org/8a24/c5131d5a28165f719697028c34b00e6d3f60.pdf&#34;&gt;The munich LSTM-RNN approach to the MediaEval 2014 Emotion In Music task&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2014&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://ieeexplore.ieee.org/abstract/document/6854950/&#34;&gt;End-to-end learning for music audio&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2014&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://courses.engr.illinois.edu/ece544na/fa2014/Tao_Feng.pdf&#34;&gt;Deep learning for music genre classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2014&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.cs.tut.fi/sgn/arg/music/tuomasv/dnn_eusipco2014.pdf&#34;&gt;Recognition of acoustic events using deep neural networks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2014&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.degruyter.com/downloadpdf/j/eletel.2014.60.issue-4/eletel-2014-0042/eletel-2014-0042.pdf&#34;&gt;Deep image features in music information retrieval&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2014&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ejhumphrey.com/assets/pdf/humphrey2014music.pdf&#34;&gt;From music audio to chord tablature: Teaching deep convolutional networks to play guitar&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2014&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.mirlab.org/conference_papers/International_Conference/ICASSP%202014/papers/p7029-schluter.pdf&#34;&gt;Improved musical onset detection with convolutional neural networks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2014&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dav.grrrr.org/public/pub/ullrich_schlueter_grill-2014-ismir.pdf&#34;&gt;Boundary detection in music structure analysis using convolutional neural networks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2014&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.smcnus.org/wp-content/uploads/2014/08/reco_MM14.pdf&#34;&gt;Improving content-based and hybrid music recommendation using deep learning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2014&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.mirlab.org/conference_papers/International_Conference/ICASSP%202014/papers/p7034-zhang.pdf&#34;&gt;A deep representation for invariance and music classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2015&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://ismir2015.uma.es/LBD/LBD24.pdf&#34;&gt;Auralisation of deep convolutional neural networks: Listening to learned features&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/keunwoochoi/Auralisation&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2015&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://perso.telecom-paristech.fr/~grichard/Publications/2015-durand-icassp.pdf&#34;&gt;Downbeat tracking with multiple features and deep neural networks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2015&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.ofai.at/~jan.schlueter/pubs/2015_eusipco.pdf&#34;&gt;Music boundary detection using neural networks on spectrograms and self-similarity lag matrices&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2015&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.researchgate.net/profile/Toni_Hirvonen/publication/276061831_Classification_of_Spatial_Audio_Location_and_Content_Using_Convolutional_Neural_Networks/links/5550665908ae12808b37fe5a/Classification-of-Spatial-Audio-Location-and-Content-Using-Convolutional-Neural-Networks.pdf&#34;&gt;Classification of spatial audio location and content using convolutional neural networks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2015&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/6905/pdf/imm6905.pdf&#34;&gt;Deep learning, audio adversaries, and music content analysis&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2015&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1507.04761.pdf&#34;&gt;Deep learning and music adversaries&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/coreyker/dnn-mgr&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2015&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://hal-imt.archives-ouvertes.fr/hal-01110035/&#34;&gt;Singing voice detection with deep recurrent neural networks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2015&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1511.05520.pdf&#34;&gt;Automatic instrument recognition in polyphonic music using convolutional neural networks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2015&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://bmcfee.github.io/papers/ismir2015_augmentation.pdf&#34;&gt;A software framework for musical data augmentation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2015&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1508.04999v1.pdf&#34;&gt;A deep bag-of-features model for music auto-tagging&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2015&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://ismir2015.uma.es/LBD/LBD27.pdf&#34;&gt;Music-noise segmentation in spectrotemporal domain using convolutional neural networks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2015&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/ftp/arxiv/papers/1512/1512.07370.pdf&#34;&gt;Musical instrument sound classification with deep convolutional neural network using feature fusion approach&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2015&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://karol.piczak.com/papers/Piczak2015-ESC-ConvNet.pdf&#34;&gt;Environmental sound classification with convolutional neural networks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2015&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://grrrr.org/pub/schlueter-2015-ismir.pdf&#34;&gt;Exploring data augmentation for improved singing voice detection with neural networks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/f0k/ismir2015&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2015&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://cs224d.stanford.edu/reports/SkiZhengshan.pdf&#34;&gt;Singer traits identification using deep neural network&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2015&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1411.1623.pdf&#34;&gt;A hybrid recurrent neural network for music transcription&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2015&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1508.01774.pdf&#34;&gt;An end-to-end neural network for polyphonic music transcription&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2015&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-319-22482-4_50&#34;&gt;Deep karaoke: Extracting vocals from musical mixtures using a convolutional deep neural network&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2015&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://ismir2015.uma.es/LBD/LBD13.pdf&#34;&gt;Folk music style modelling by recurrent neural networks with long short term memory units&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/IraKorshunova/folk-rnn&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2015&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.researchgate.net/profile/Stefan_Uhlich/publication/282001406_Deep_neural_network_based_instrument_extraction_from_music/links/5600eeda08ae07629e52b397/Deep-neural-network-based-instrument-extraction-from-music.pdf&#34;&gt;Deep neural network based instrument extraction from music&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2015&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.researchgate.net/profile/Xiaoqing_Zheng3/publication/275347034_A_Deep_Neural_Network_for_Modeling_Music/links/5539d2060cf2239f4e7dad0d/A-Deep-Neural-Network-for-Modeling-Music.pdf&#34;&gt;A deep neural network for modeling music&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2016&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://file.scirp.org/pdf/CS_2016042615054817.pdf&#34;&gt;An efficient approach for segmentation, feature extraction and classification of audio signals&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2016&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/file/d/0B1OooSxEtl0FcG9MYnY2Ylh5c0U/view&#34;&gt;Text-based LSTM networks for automatic music composition&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2016&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1606.02096.pdf&#34;&gt;Towards playlist generation algorithms using RNNs trained on within-track transitions&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2016&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1606.00298.pdf&#34;&gt;Automatic tagging using deep convolutional neural networks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2016&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://ieeexplore.ieee.org/abstract/document/7471677/&#34;&gt;Automatic chord estimation on seventhsbass chord vocabulary using deep neural network&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2016&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1612.01010.pdf&#34;&gt;DeepBach: A steerable model for Bach chorales generation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Ghadjeres/DeepBach&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2016&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.rhythmos.org/MMILab-Andre_files/ISMIR2016_CNNDBNbeats_camready.pdf&#34;&gt;Bayesian meter tracking on learned signal representations&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2016&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1606.04930.pdf&#34;&gt;Deep learning for music&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2016&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.researchgate.net/profile/Il_Young_Jeong/publication/305683876_Learning_temporal_features_using_a_deep_neural_network_and_its_application_to_music_genre_classification/links/5799a27c08aec89db7bb9f92.pdf&#34;&gt;Learning temporal features using a deep neural network and its application to music genre classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2016&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1612.05153.pdf&#34;&gt;On the potential of simple framewise approaches to piano transcription&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2016&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1612.05065.pdf&#34;&gt;Feature learning for chord recognition: The deep chroma extractor&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/fdlm/chordrec/tree/master/experiments/ismir2016&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2016&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.researchgate.net/profile/Filip_Korzeniowski/publication/305590295_A_Fully_Convolutional_Deep_Auditory_Model_for_Musical_Chord_Recognition/links/579486ba08aed51475cc6958/A-Fully-Convolutional-Deep-Auditory-Model-for-Musical-Chord-Recognition.pdf?_iepl%5BhomeFeedViewId%5D=HTzFFmKPia2YminQ4psHT5at&amp;amp;_iepl%5Bcontexts%5D%5B0%5D=pcfhf&amp;amp;_iepl%5BinteractionType%5D=publicationDownload&amp;amp;origin=publication_detail&amp;amp;ev=pub_int_prw_xdl&amp;amp;msrp=Dz_6LKHzYcPyP-LmgZPF-m63ayZ6k0entFEntooiu_e32zfETNQXKPQSTFOI87NONIIQuUQdnUtwORdomTXfteTrb09KiAIdDtBJnw_02P6JeRr5zu2eyaCG.2Uxsi_eENxtbYL39lvorIK8LofRYhkgpUHzpzmVzkIEiyHc0wUY87rEa4PH1qbXi4k4RyagHUsA2IsZtewnprglORjx2v9Cwbk9ZfQ.cd67BaqtHul_hE6SX6vUFKuldz81aH6dWq-cYMkq5vQKCHcvB8l9zgeM694Efb_r2wBB5GT9idt3OLeME0UxVHI6ROxamgK3LMNlSw.JtZXAo9HhR9t-8Wl3gxJgnoM4--rtmDEUDbXSWezbFyU-CoB_nyfxbRQ4kdoN4-5aJ3Tgx4YHdikicqAhc_cezB2ZntjxkB4rEDx1A&#34;&gt;A fully convolutional deep auditory model for musical chord recognition&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2016&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://ieeexplore.ieee.org/document/7471734/&#34;&gt;A deep bidirectional long short-term memory based multi-scale approach for music dynamic emotion prediction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2016&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://mac.citi.sinica.edu.tw/~yang/pub/liu16mm.pdf&#34;&gt;Event localization in music auto-tagging&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ciaua/clip2frame&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2016&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/lostanlen/ismir2016/raw/master/paper/lostanlen_ismir2016.pdf&#34;&gt;Deep convolutional networks on the pitch spiral for musical instrument recognition&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/lostanlen/ismir2016&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2016&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://openreview.net/pdf?id=SkxKPDv5xl&#34;&gt;SampleRNN: An unconditional end-to-end neural audio generation model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/soroushmehr/sampleRNN_ICLR2017&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2016&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1604.06338.pdf&#34;&gt;Robust audio event recognition with 1-max pooling convolutional neural networks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2016&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://jordipons.me/media/CBMI16.pdf&#34;&gt;Experimenting with musically motivated convolutional neural networks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/jordipons/&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2016&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/163_Paper.pdf&#34;&gt;Singing voice melody transcription using deep neural networks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2016&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.music-ir.org/mirex/abstracts/2016/RSGP1.pdf&#34;&gt;Singing voice separation using deep neural networks and F0 estimation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://cvssp.org/projects/maruss/mirex2016/&#34;&gt;Website&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2016&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.ofai.at/~jan.schlueter/pubs/2016_ismir.pdf&#34;&gt;Learning to pinpoint singing voice from weakly labeled examples&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2016&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://ieeexplore.ieee.org/abstract/document/7733228/&#34;&gt;Analysis of time-frequency representations for musical onset detection with convolutional neural network&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2016&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.degruyter.com/downloadpdf/j/amcs.2016.26.issue-1/amcs-2016-0014/amcs-2016-0014.pdf&#34;&gt;Note onset detection in musical signals via neural-network-based multi-ODF fusion&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2016&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/file/d/0B1OooSxEtl0FcTBiOGdvSTBmWnc/view&#34;&gt;Music transcription modelling and composition using deep learning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/IraKorshunova/folk-rnn&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2016&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.mirlab.org/conference_papers/International_Conference/ICASSP%202016/pdfs/0000579.pdf&#34;&gt;Convolutional neural network for robust pitch determination&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2016&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1604.07160.pdf&#34;&gt;Deep convolutional neural networks and data augmentation for acoustic event detection&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://bitbucket.org/naoya1/aenet_release&#34;&gt;Website&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1706.08818.pdf&#34;&gt;Gabor frames and deep scattering networks in audio processing&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://dorienherremans.com/dlm2017/papers/bazzica2017clarinet.pdf&#34;&gt;Vision-based detection of acoustic timed events: A case study on clarinet note onsets&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1709.01620.pdf&#34;&gt;Deep learning techniques for music generation - A survey&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1711.07682.pdf&#34;&gt;JamBot: Music theory aware chord based generation of polyphonic music with LSTMs&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/brunnergino/JamBot&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1709.00572.pdf&#34;&gt;XFlow: 1D &amp;lt;-&amp;gt; 2D cross-modal deep neural networks for audiovisual classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://dorienherremans.com/dlm2017/papers/cella2017mli.pdf&#34;&gt;Machine listening intelligence&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://mtg.upf.edu/system/files/publications/monoaural-audio-source_0.pdf&#34;&gt;Monoaural audio source separation using deep convolutional neural networks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/MTG/DeepConvSep&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://ieeexplore.ieee.org/abstract/document/8019322/&#34;&gt;Deep multimodal network for multi-label classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1709.04396.pdf&#34;&gt;A tutorial on deep learning for music information retrieval&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/keunwoochoi/dl4mir&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1709.01922.pdf&#34;&gt;A comparison on audio signal preprocessing methods for deep neural networks on music tagging&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/keunwoochoi/transfer_learning_music&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1703.09179v3.pdf&#34;&gt;Transfer learning for music classification and regression tasks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/keunwoochoi/transfer_learning_music&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://ieeexplore.ieee.org/abstract/document/7952585/&#34;&gt;Convolutional recurrent neural networks for music classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/keunwoochoi/icassp_2017&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.inf.ufpr.br/lesoliveira/download/ASOC2017.pdf&#34;&gt;An evaluation of convolutional neural networks for music classification using spectrograms&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1709.07153.pdf&#34;&gt;Large vocabulary automatic chord estimation using deep neural nets: Design framework, system variations and limitations&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1709.02291.pdf&#34;&gt;Basic filters for convolutional neural networks: Training or design?&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1708.05826.pdf&#34;&gt;Ensemble Of Deep Neural Networks For Acoustic Scene Classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://ieeexplore.ieee.org/abstract/document/7728057/&#34;&gt;Robust downbeat tracking using an ensemble of convolutional networks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://dorienherremans.com/dlm2017/papers/fan2017vector.pdf&#34;&gt;Music signal processing using vector product neural networks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://dorienherremans.com/dlm2017/papers/geng2017genre.pdf&#34;&gt;Transforming musical signals through a genre classifying convolutional neural network&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1707.03547.pdf&#34;&gt;Audio to score matching by combining phonetic and duration information&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ronggong/jingjuSingingPhraseMatching/tree/v0.1.0&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1709.06404.pdf&#34;&gt;Interactive music generation with positional constraints using anticipation-RNNs&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1709.00740.pdf&#34;&gt;Deep rank-based transposition-invariant distances on musical sequences&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1707.04588.pdf&#34;&gt;GLSR-VAE: Geodesic latent space regularization for variational autoencoder architectures&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://dl.acm.org/citation.cfm?id=3068697&#34;&gt;Deep convolutional neural networks for predominant instrument recognition in polyphonic music&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1609.09430v2.pdf&#34;&gt;CNN architectures for large-scale audio classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://ieeexplore.ieee.org/abstract/document/8026272/&#34;&gt;DeepSheet: A sheet music generator based on deep learning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://dorienherremans.com/dlm2017/papers/hutchings2017drums.pdf&#34;&gt;Talking Drums: Generating drum grooves with neural networks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ismir2017.smcnus.org/wp-content/uploads/2017/10/171_Paper.pdf&#34;&gt;Singing voice separation with deep U-Net convolutional networks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Xiao-Ming/UNet-VocalSeparation-Chainer&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://ceur-ws.org/Vol-1905/recsys2017_poster18.pdf&#34;&gt;Music emotion recognition via end-to-end multimodal neural networks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://dorienherremans.com/dlm2017/papers/koops2017pers.pdf&#34;&gt;Chord label personalization through deep learning of integrated harmonic interval-based representations&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1706.02921.pdf&#34;&gt;End-to-end musical key estimation using a convolutional neural network&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.cp.jku.at/research/papers/Koutini_2017_mediaeval-acousticbrainz.pdf&#34;&gt;MediaEval 2017 AcousticBrainz genre task: Multilayer perceptron approach&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.preprints.org/manuscript/201711.0027/v1&#34;&gt;Classification-based singing melody extraction using deep convolutional neural networks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1703.01793v2.pdf&#34;&gt;Multi-level and multi-scale feature aggregation using pre-trained convolutional neural networks for music auto-tagging&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1706.06810.pdf&#34;&gt;Multi-level and multi-scale feature aggregation using sample-level deep convolutional neural networks for music classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/jongpillee/musicTagging_MSD&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1703.01789v2.pdf&#34;&gt;Sample-level deep convolutional neural networks for music auto-tagging using raw waveforms&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1710.11418.pdf&#34;&gt;A SeqGAN for Polyphonic Music Generation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/L0SG/seqgan-music&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.eurasip.org/Proceedings/Eusipco/Eusipco2017/papers/1570346835.pdf&#34;&gt;Harmonic and percussive source separation using a convolutional auto encoder&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1706.02292.pdf&#34;&gt;Stacked convolutional and recurrent neural networks for music emotion recognition&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://repositori.upf.edu/bitstream/handle/10230/32919/Martel_2017.pdf?sequence=1&amp;amp;isAllowed=y&#34;&gt;A deep learning approach to source separation and remixing of hiphop music&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://link.springer.com/chapter/10.1007%2F978-3-319-70096-0_49&#34;&gt;Music Genre Classification Using Masked Conditional Neural Networks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1711.01437.pdf&#34;&gt;Monaural Singing Voice Separation with Skip-Filtering Connections and Recurrent Inference of Time-Frequency Mask&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Js-Mim/mss_pytorch&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.researchgate.net/profile/Marius_Miron/publication/318322107_Generating_data_to_train_convolutional_neural_networks_for_classical_music_source_separation/links/59637cc3458515a3575b93c6/Generating-data-to-train-convolutional-neural-networks-for-classical-music-source-separation.pdf?_iepl%5BhomeFeedViewId%5D=WchoMnlUL1Hk9hBLVTeR8Amh&amp;amp;_iepl%5Bcontexts%5D%5B0%5D=pcfhf&amp;amp;_iepl%5BinteractionType%5D=publicationDownload&amp;amp;origin=publication_detail&amp;amp;ev=pub_int_prw_xdl&amp;amp;msrp=p3lQ8M4uZlb4TF5Hv9a2U3P2y4wW7ant5KWj4E5-OcD1Mg53p1ykTKHMG9_zVTB9n6mI8fvZOCL2Xhpru186pCEY-2ZxiYR-CB8_QvwHc1kUG-QE4SHdProR.LoJb2BDOiiQth3iR9xgZUxxCWEJgtTBF4whFrFa01OD49-3YYRxA0WQVN--zhtQU_7C2Pt0rKdwoFxT1pfxFvnKXSXmy2eT1Jpz-pw.U1QLoFO_Uc6aQVr2Nm2FcAi6BqAUfngH2Or5__6wegbCgVvTYoIGt22tmCkYbGTOQ_4PxBgt1LrvsFQiL0oMyogP8Yk8myTj0gs9jw.fGpkufGqAI4R2v8Hfe0ThcXL7M7yN2PuAlx974BGVn50SdUWvNhhIPWBD-zWTn8NKtVJx3XrjKXFrMgi9Cx7qGrNP8tBWpha6Srf6g&#34;&gt;Generating data to train convolutional neural networks for classical music source separation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/MTG/DeepConvSep&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.researchgate.net/profile/Marius_Miron/publication/318637038_Monaural_score-informed_source_separation_for_classical_music_using_convolutional_neural_networks/links/597327c6458515e26dfdb007/Monaural-score-informed-source-separation-for-classical-music-using-convolutional-neural-networks.pdf?_iepl%5BhomeFeedViewId%5D=WchoMnlUL1Hk9hBLVTeR8Amh&amp;amp;_iepl%5Bcontexts%5D%5B0%5D=pcfhf&amp;amp;_iepl%5BinteractionType%5D=publicationDownload&amp;amp;origin=publication_detail&amp;amp;ev=pub_int_prw_xdl&amp;amp;msrp=Hp6dDqMepEiRZ5E6WkreaqyjFkFkwMxPFoJvr14etVJsoKZBc5qb99fBnJjVUZrRHLFRhaXvNY9k1sMvYPOouuGbQP0YhEGm28zLw_55Zewu86WGnHck1Tqi.93HH2WqXfTedn6IaZRjjhQGYZVDHBz1X6nr4ABBgMAVv584gvGN3sW5IyBAY-4MBWf5DJFPBGm8zsaC2dKz8G-odZPfosWoXY0afAQ.KoCP2mO9l31lCER0oMZMZBrbuRGvb6ZzeBwHb88pL8AhMfJk03Hj1eLrohQIjPDETBj4hhqb0gniDGJgtZ9GnW64ZNjh9GbQDrIl5A.egNQTyC7t8P26zCQWrbEhf51Pxy2JRBZoTkH6SpRHHhRhFl1_AT_AT481lMcFI34-JbeRq-5oTQR7DpvAuw7iUIivd78ltuxpI9syg&#34;&gt;Monaural score-informed source separation for classical music using convolutional neural networks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/MTG/DeepConvSep&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ismir2017.smcnus.org/wp-content/uploads/2017/10/126_Paper.pdf&#34;&gt;Multi-label music genre classification from audio, text, and images using deep features&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/sergiooramas/tartarus&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1706.09739.pdf&#34;&gt;A deep multimodal approach for cold-start music recommendation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/sergiooramas/tartarus&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://ieeexplore.ieee.org/abstract/document/7952660/&#34;&gt;Melody extraction and detection through LSTM-RNN with harmonic sum loss&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1710.06648.pdf&#34;&gt;Representation learning of music using artist labels&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://dorienherremans.com/dlm2017/papers/pfalz2017synthesis.pdf&#34;&gt;Toward inverse control of physics-based sound synthesis&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.cct.lsu.edu/~apfalz/inverse_control.html&#34;&gt;Website&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1708.03211.pdf&#34;&gt;DNN and CNN with weighted and multi-task loss functions for audio event detection&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ismir2017.smcnus.org/wp-content/uploads/2017/10/46_Paper.pdf&#34;&gt;Score-informed syllable segmentation for a cappella singing voice with convolutional neural networks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ronggong/jingjuSyllabicSegmentaion/tree/v0.1.0&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1711.02520.pdf&#34;&gt;End-to-end learning for music audio tagging at scale&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/jordipons/music-audio-tagging-at-scale-models&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://ieeexplore.ieee.org/document/7952601/&#34;&gt;Designing efficient architectures for modeling temporal features with convolutional neural networks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/jordipons/ICASSP2017&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ronggong/EUSIPCO2017&#34;&gt;Timbre analysis of music audio signals with convolutional neural networks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/jordipons/EUSIPCO2017&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://doi.org/10.5281/zenodo.1117372&#34;&gt;The MUSDB18 corpus for music separation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/sigsep/website&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.semanticaudio.co.uk/wp-content/uploads/2017/09/WIMP2017_Martinez-RamirezReiss.pdf&#34;&gt;Deep learning and intelligent audio mixing&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://ofai.at/~jan.schlueter/pubs/phd/phd.pdf&#34;&gt;Deep learning for event detection, sequence labelling and similarity estimation in music signals&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.researchgate.net/profile/Thomas_Pellegrini/publication/319326354_Music_Feature_Maps_with_Convolutional_Neural_Networks_for_Music_Genre_Classification/links/59ba5ae3458515bb9c4c6724/Music-Feature-Maps-with-Convolutional-Neural-Networks-for-Music-Genre-Classification.pdf?origin=publication_detail&amp;amp;ev=pub_int_prw_xdl&amp;amp;msrp=wzXuHZAa5zAnqEmErYyZwIRr2H0q01LnNEd4Wd7A15CQfdVLwdy98pmE-AdnrDvoc3-bVENSFrHt0yhaOiE2mQrYllVS9CJZOk-c9R0j_R1rbgcZugS6RtQ_.AUjPuJSF5P_DMngf-woH7W-7jdnQlbNQziR4_h6NnCHfR_zGcEa8vOyyOz5gx5nc4azqKTPQ5ZgGGLUxkLj1qCQLEQ5ThkhGlWHLyA.s6MBZE20-EO_RjRGCOCV4wk0WSFdN56Aloiraxz9hKCbJwRM2Et27RHVUA8jj9H8qvXIB6f7zSIrQgjXGrL2yCpyQlLffuf57rzSwg.KMMXbZrHsihV8DJM53xkHAWf3VebCJESi4KU4btNv9nQsyK2KnkhSQaTILKv0DSZY3c70a61LzywCBuoHtIhVOFhW5hVZN2n5O9uKQ&#34;&gt;Music feature maps with convolutional neural networks for music genre classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://carlsouthall.files.wordpress.com/2017/12/ismir2017adt.pdf&#34;&gt;Automatic drum transcription for polyphonic recordings using soft attention mechanisms and convolutional neural networks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/CarlSouthall/ADTLib&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1711.00048.pdf&#34;&gt;Adversarial semi-supervised audio source separation applied to singing voice extraction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://jcms.org.uk/issues/Vol2Issue1/taking-models-back-to-music-practice/Taking%20the%20Models%20back%20to%20Music%20Practice:%20Evaluating%20Generative%20Transcription%20Models%20built%20using%20Deep%20Learning.pdf&#34;&gt;Taking the models back to music practice: Evaluating generative transcription models built using deep learning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/IraKorshunova/folk-rnn&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ismir2017.smcnus.org/wp-content/uploads/2017/10/178_Paper.pdf&#34;&gt;Generating nontrivial melodies for music as a service&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1711.04845.pdf&#34;&gt;Invariances and data augmentation for supervised music transcription&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/jthickstun/thickstun2018invariances/&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ismir2017.smcnus.org/wp-content/uploads/2017/10/43_Paper.pdf&#34;&gt;Lyrics-based music genre classification using a hierarchical attention network&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/alexTsaptsinos/lyricsHAN&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1709.08243.pdf&#34;&gt;A hybrid DSP/deep learning approach to real-time full-band speech enhancement&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/xiph/rnnoise/&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://vbn.aau.dk/files/260308151/PHD_Gissel_Velarde_E_pdf.pdf&#34;&gt;Convolutional methods for music analysis&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.aes.org/e-lib/browse.cfm?elib=18682&#34;&gt;Extending temporal feature integration for semantic audio analysis&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://ieeexplore.ieee.org/abstract/document/8019552/&#34;&gt;Recognition and retrieval of sound events using sparse coding convolutional neural network&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.mdpi.com/2076-3417/7/9/901/htm&#34;&gt;A two-stage approach to note-level transcription of a specific piano&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1711.00229.pdf&#34;&gt;Reducing model complexity for DNN based large-scale audio classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://dorienherremans.com/dlm2017/papers/wyse2017spect.pdf&#34;&gt;Audio spectrogram representations for processing with convolutional neural networks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://lonce.org/research/audioST/&#34;&gt;Website&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1607.03681.pdf&#34;&gt;Unsupervised feature learning based on deep models for environmental audio tagging&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1703.06052.pdf&#34;&gt;Attention and localization based on a deep convolutional recurrent model for weakly supervised audio tagging&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/yongxuUSTC/att_loc_cgrnn&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.cs.tut.fi/sgn/arg/dcase2017/documents/challenge_technical_reports/DCASE2017_Xu_146.pdf&#34;&gt;Surrey-CVSSP system for DCASE2017 challenge task4&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/yongxuUSTC/dcase2017_task4_cvssp&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://qmro.qmul.ac.uk/xmlui/handle/123456789/24946&#34;&gt;A study on LSTM networks for polyphonic music sequence modelling&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.eecs.qmul.ac.uk/~ay304/code/ismir17&#34;&gt;Website&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2018&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1709.06298.pdf&#34;&gt;MuseGAN: Multi-track sequential generative adversarial networks for symbolic music generation and accompaniment&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/salu133445/musegan&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2018&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1809.04281.pdf&#34;&gt;Music transformer: Generating music with long-term structure&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2018&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://nips2018creativity.github.io/doc/music_theory_inspired_policy_gradient.pdf&#34;&gt;Music theory inspired policy gradient method for piano music transcription&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2019&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1810.12247&#34;&gt;Enabling factorized piano music modeling and generation with the MAESTRO dataset&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/magenta/magenta/tree/master/magenta/models/onsets_frames_transcription&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2019&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1904.10509.pdf&#34;&gt;Generating Long Sequences with Sparse Transformers&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/openai/sparse_attention&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2021&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://archives.ismir.net/ismir2021/paper/000076.pdf&#34;&gt;DadaGP: a Dataset of Tokenized GuitarPro Songs for Sequence Models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/dada-bots/dadaGP&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ybayle/awesome-deep-learning-music#deep-learning-for-music-dl4m-&#34;&gt;Go back to top&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;DL4M details&lt;/h2&gt; &#xA;&lt;p&gt;A human-readable table summarized version if displayed in the file &lt;a href=&#34;https://raw.githubusercontent.com/ybayle/awesome-deep-learning-music/master/dl4m.tsv&#34;&gt;dl4m.tsv&lt;/a&gt;. All details for each article are stored in the corresponding bib entry in &lt;a href=&#34;https://raw.githubusercontent.com/ybayle/awesome-deep-learning-music/master/dl4m.bib&#34;&gt;dl4m.bib&lt;/a&gt;. Each entry has the regular bib field:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;author&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;year&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;title&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;journal&lt;/code&gt; or &lt;code&gt;booktitle&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Each entry in &lt;a href=&#34;https://raw.githubusercontent.com/ybayle/awesome-deep-learning-music/master/dl4m.bib&#34;&gt;dl4m.bib&lt;/a&gt; also displays additional information:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;link&lt;/code&gt; - HTML link to the PDF file&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;code&lt;/code&gt; - Link to the source code if available&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;archi&lt;/code&gt; - Neural network architecture&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;layer&lt;/code&gt; - Number of layers&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;task&lt;/code&gt; - The proposed tasks studied in the article&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;dataset&lt;/code&gt; - The names of the dataset used&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;dataaugmentation&lt;/code&gt; - The type of data augmentation technique used&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;time&lt;/code&gt; - The computation time&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;hardware&lt;/code&gt; - The hardware used&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;note&lt;/code&gt; - Additional notes and information&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;repro&lt;/code&gt; - Indication to what extent the experiments are reproducible&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ybayle/awesome-deep-learning-music#deep-learning-for-music-dl4m-&#34;&gt;Go back to top&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Code without articles&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/drscotthawley/audio-classifier-keras-cnn&#34;&gt;Audio Classifier in Keras using Convolutional Neural Network&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jisungk/deepjazz&#34;&gt;Deep learning driven jazz generation using Keras &amp;amp; Theano&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jordipons/music-audio-tagging-at-scale-models&#34;&gt;End-to-end learning for music audio tagging at scale&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Hguimaraes/gtzan.keras&#34;&gt;Music Genre classification on GTZAN dataset using CNNs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/helenacuesta/choir-pitch-estimation&#34;&gt;Pitch Estimation of Choir Music using Deep Learning Strategies: from Solo to Unison Recordings&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ruohoruotsi/LSTM-Music-Genre-Classification&#34;&gt;Music Genre Classification with LSTMs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rickiepark/cnn_mer&#34;&gt;CNN based Music Emotion Classification using TensorFlow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/andabi/music-source-separation&#34;&gt;Separating singing voice from music based on deep neural networks in Tensorflow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kristijanbartol/Deep-Music-Tagger&#34;&gt;Music tag classification model using CRNN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/despoisj/DeepAudioClassification&#34;&gt;Finding the genre of a song with Deep Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/fephsun/neuralnetmusic&#34;&gt;Composing music using neural nets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/djosix/Performance-RNN-PyTorch&#34;&gt;Performance-RNN-PyTorch&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ybayle/awesome-deep-learning-music#deep-learning-for-music-dl4m-&#34;&gt;Go back to top&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Statistics and visualisations&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;167 papers referenced. See the details in &lt;a href=&#34;https://raw.githubusercontent.com/ybayle/awesome-deep-learning-music/master/dl4m.bib&#34;&gt;dl4m.bib&lt;/a&gt;. There are more papers from 2017 than any other years combined. Number of articles per year: &lt;img src=&#34;https://raw.githubusercontent.com/ybayle/awesome-deep-learning-music/master/fig/articles_per_year.png&#34; alt=&#34;Number of articles per year&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;If you are applying DL to music, there are &lt;a href=&#34;https://raw.githubusercontent.com/ybayle/awesome-deep-learning-music/master/authors.md&#34;&gt;364 other researchers&lt;/a&gt; in your field.&lt;/li&gt; &#xA; &lt;li&gt;34 tasks investigated. See the list of &lt;a href=&#34;https://raw.githubusercontent.com/ybayle/awesome-deep-learning-music/master/tasks.md&#34;&gt;tasks&lt;/a&gt;. Tasks pie chart: &lt;img src=&#34;https://raw.githubusercontent.com/ybayle/awesome-deep-learning-music/master/fig/pie_chart_task.png&#34; alt=&#34;Tasks pie chart&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;55 datasets used. See the list of &lt;a href=&#34;https://raw.githubusercontent.com/ybayle/awesome-deep-learning-music/master/datasets.md&#34;&gt;datasets&lt;/a&gt;. Datasets pie chart: &lt;img src=&#34;https://raw.githubusercontent.com/ybayle/awesome-deep-learning-music/master/fig/pie_chart_dataset.png&#34; alt=&#34;Datasets pie chart&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;30 architectures used. See the list of &lt;a href=&#34;https://raw.githubusercontent.com/ybayle/awesome-deep-learning-music/master/architectures.md&#34;&gt;architectures&lt;/a&gt;. Architectures pie chart: &lt;img src=&#34;https://raw.githubusercontent.com/ybayle/awesome-deep-learning-music/master/fig/pie_chart_architecture.png&#34; alt=&#34;Architectures pie chart&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;9 frameworks used. See the list of &lt;a href=&#34;https://raw.githubusercontent.com/ybayle/awesome-deep-learning-music/master/frameworks.md&#34;&gt;frameworks&lt;/a&gt;. Frameworks pie chart: &lt;img src=&#34;https://raw.githubusercontent.com/ybayle/awesome-deep-learning-music/master/fig/pie_chart_framework.png&#34; alt=&#34;Frameworks pie chart&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;Only 47 articles (28%) provide their source code. Repeatability is the key to good science, so check out the &lt;a href=&#34;https://raw.githubusercontent.com/ybayle/awesome-deep-learning-music/master/reproducibility.md&#34;&gt;list of useful resources on reproducibility for MIR and ML&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ybayle/awesome-deep-learning-music#deep-learning-for-music-dl4m-&#34;&gt;Go back to top&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Advices for reviewers of dl4m articles&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/ybayle/awesome-deep-learning-music/master/advice_review.md&#34;&gt;advice_review.md&lt;/a&gt; file.&lt;/p&gt; &#xA;&lt;h2&gt;How To Contribute&lt;/h2&gt; &#xA;&lt;p&gt;Contributions are welcome! Please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/ybayle/awesome-deep-learning-music/master/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; file.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ybayle/awesome-deep-learning-music#deep-learning-for-music-dl4m-&#34;&gt;Go back to top&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;How are the articles sorted?&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;The articles are first sorted by decreasing year (to keep up with the latest news) and then alphabetically by the main author&#39;s family name.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Why are preprint from arXiv included in the list?&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;I want to have exhaustive research and the latest news on DL4M. However, one should take care of the information provided in the articles currently in review. If possible you should wait for the final accepted and peer-reviewed version before citing an arXiv paper. I regularly update the arXiv links to the corresponding published papers when available.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;How much can I trust the results published in an article?&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;The list provided here does not guarantee the quality of the articles. You should either try to reproduce the experiments described or submit a request to &lt;a href=&#34;https://github.com/ReScience/ReScience&#34;&gt;ReScience&lt;/a&gt;. Use one article&#39;s conclusion at your own risks.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ybayle/awesome-deep-learning-music#deep-learning-for-music-dl4m-&#34;&gt;Go back to top&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Acronyms used&lt;/h2&gt; &#xA;&lt;p&gt;A list of useful acronyms used in deep learning and music is stored in &lt;a href=&#34;https://raw.githubusercontent.com/ybayle/awesome-deep-learning-music/master/acronyms.md&#34;&gt;acronyms.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ybayle/awesome-deep-learning-music#deep-learning-for-music-dl4m-&#34;&gt;Go back to top&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Sources&lt;/h2&gt; &#xA;&lt;p&gt;The list of conferences, journals and aggregators used to gather the proposed materials is stored in &lt;a href=&#34;https://raw.githubusercontent.com/ybayle/awesome-deep-learning-music/master/sources.md&#34;&gt;sources.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ybayle/awesome-deep-learning-music#deep-learning-for-music-dl4m-&#34;&gt;Go back to top&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://yannbayle.fr/english/index.php&#34;&gt;Yann Bayle&lt;/a&gt; (&lt;a href=&#34;https://github.com/ybayle&#34;&gt;GitHub&lt;/a&gt;) - Instigator and principal maintainer&lt;/li&gt; &#xA; &lt;li&gt;Vincent Lostanlen (&lt;a href=&#34;https://github.com/lostanlen&#34;&gt;GitHub&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://keunwoochoi.wordpress.com/&#34;&gt;Keunwoo Choi&lt;/a&gt; (&lt;a href=&#34;https://github.com/keunwoochoi&#34;&gt;GitHub&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.eecs.qmul.ac.uk/~sturm/&#34;&gt;Bob L. Sturm&lt;/a&gt; (&lt;a href=&#34;https://github.com/boblsturm&#34;&gt;GitHub&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.audiolabs-erlangen.de/fau/assistant/balke&#34;&gt;Stefan Balke&lt;/a&gt; (&lt;a href=&#34;https://github.com/stefan-balke&#34;&gt;GitHub&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.jordipons.me/&#34;&gt;Jordi Pons&lt;/a&gt; (&lt;a href=&#34;https://github.com/jordipons&#34;&gt;GitHub&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Mirza Zulfan (&lt;a href=&#34;https://github.com/mirzazulfan&#34;&gt;GitHub&lt;/a&gt;) for the logo&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/devn&#34;&gt;Devin Walters&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/LegendJ&#34;&gt;https://github.com/LegendJ&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ybayle/awesome-deep-learning-music#deep-learning-for-music-dl4m-&#34;&gt;Go back to top&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Other useful related lists and resources&lt;/h2&gt; &#xA;&lt;h4&gt;Audio&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tuwien-musicir/DL_MIR_Tutorial&#34;&gt;DL4MIR tutorial with keras&lt;/a&gt; - Tutorial for Deep Learning on Music Information Retrieval by &lt;a href=&#34;http://ifs.tuwien.ac.at/~lidy/&#34;&gt;Thomas Lidy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=sI_8EA0_ha8&#34;&gt;Video talk from Ron Weiss&lt;/a&gt; - Ron Weiss (Google) Talk on Training neural network acoustic models on waveforms&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.jordipons.me/media/DL4Music_Pons.pdf&#34;&gt;Slides on DL4M&lt;/a&gt; - A personal (re)view of the state-of-the-art by &lt;a href=&#34;http://www.jordipons.me/&#34;&gt;Jordi Pons&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/marl/dl4mir-tutorial&#34;&gt;DL4MIR tutorial&lt;/a&gt; - Python tutorials for learning to solve MIR tasks with DL&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/faroit/awesome-python-scientific-audio&#34;&gt;Awesome Python Scientific Audio&lt;/a&gt; - Python resources for Audio and Machine Learning&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://ismir.net/resources.php&#34;&gt;ISMIR resources&lt;/a&gt; - Community maintained list&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://groups.google.com/a/ismir.net/forum/#!forum/community&#34;&gt;ISMIR Google group&lt;/a&gt; - Daily dose of general MIR&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vinta/awesome-python#audio&#34;&gt;Awesome Python&lt;/a&gt; - Audio section of Python resources&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/notthetup/awesome-webaudio&#34;&gt;Awesome Web Audio&lt;/a&gt; - WebAudio packages and resources&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ciconia/awesome-music&#34;&gt;Awesome Music&lt;/a&gt; - Music softwares&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/adius/awesome-music-production&#34;&gt;Awesome Music Production&lt;/a&gt; - Music creation&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.asimovinstitute.org/analyzing-deep-learning-tools-music/&#34;&gt;The Asimov Institute&lt;/a&gt; - 6 deep learning tools for music generation&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://groups.google.com/forum/#!forum/icdlm&#34;&gt;DLM Google group&lt;/a&gt; - Deep Learning in Music group&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://slackpass.io/mircommunity&#34;&gt;MIR community on Slack&lt;/a&gt; - Link to subscribe to the MIR community&#39;s Slack&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.music.mcgill.ca/~cmckay/links_academic.html&#34;&gt;Unclassified list of MIR-related links&lt;/a&gt; - &lt;a href=&#34;http://www.music.mcgill.ca/~cmckay/&#34;&gt;Cory McKay&lt;/a&gt;&#39;s list of various links on DL, MIR, ...&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://jordipons.me/wiki/index.php/MIRDL&#34;&gt;MIRDL&lt;/a&gt; - Unmaintained list of DL articles for MIR from &lt;a href=&#34;http://www.jordipons.me/&#34;&gt;Jordi Pons&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.crowdai.org/challenges/www-2018-challenge-learning-to-recognize-musical-genre&#34;&gt;WWW 2018 Challenge&lt;/a&gt; - Learning to Recognize Musical Genre on the &lt;a href=&#34;https://github.com/mdeff/fma&#34;&gt;FMA&lt;/a&gt; dataset&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/umbrellabeach/music-generation-with-DL&#34;&gt;Music generation with DL&lt;/a&gt; - List of resources on music generation with deep learning&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mitpress.mit.edu/books/auditory-scene-analysis&#34;&gt;Auditory Scene Analysis&lt;/a&gt; - Book about the perceptual organization of sound by &lt;a href=&#34;https://en.wikipedia.org/wiki/Albert_Bregman&#34;&gt;Albert Bregman&lt;/a&gt;, the &#34;father of &lt;a href=&#34;https://en.wikipedia.org/wiki/Auditory_scene_analysis&#34;&gt;Auditory Scene Analysis&lt;/a&gt;&#34;. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;http://webpages.mcgill.ca/staff/Group2/abregm1/web/downloadstoc.htm&#34;&gt;Demonstrations of Auditory Scene Analysis&lt;/a&gt; - Audio demonstrations, which illustrate examples of auditory perceptual organization.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ybayle/awesome-deep-learning-music#deep-learning-for-music-dl4m-&#34;&gt;Go back to top&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Music datasets&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.audiocontentanalysis.org/data-sets/&#34;&gt;AudioContentAnalysis nearly exhaustive list of music-related datasets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://teachingmir.wikispaces.com/Datasets&#34;&gt;Teaching MIR&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research#cite_ref-215&#34;&gt;Wikipedia&#39;s list of datasets for machine learning research&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://deeplearning.net/datasets/&#34;&gt;Datasets for deep learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/caesar0301/awesome-public-datasets&#34;&gt;Awesome public datasets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ybayle/awesome-music-listening&#34;&gt;Awesome music listening&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ybayle/awesome-deep-learning-music#deep-learning-for-music-dl4m-&#34;&gt;Go back to top&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Deep learning&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1711.03543&#34;&gt;DLPaper2Code: Auto-generation of Code from Deep Learning Research Papers&lt;/a&gt; -&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ysh329/deep-learning-model-convertor&#34;&gt;Model Convertors&lt;/a&gt; - Convertors for DL frameworks and backend&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hunkim/deep_architecture_genealogy&#34;&gt;Deep architecture genealogy&lt;/a&gt; - Genealogy of DL architectures&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.univie.ac.at/nuhag-php/dateien/talks/3358_schlueter.pdf&#34;&gt;Deep Learning as an Engineer&lt;/a&gt; - Slides from Jan Schlüter&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ChristosChristofidis/awesome-deep-learning&#34;&gt;Awesome Deep Learning&lt;/a&gt; - General deep learning resources&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/endymecy/awesome-deeplearning-resources&#34;&gt;Awesome Deep Learning Resources&lt;/a&gt; - Papers regarding deep learning and deep reinforcement learning&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kjw0612/awesome-rnn&#34;&gt;Awesome RNNs&lt;/a&gt; - RNNs code, theory and applications&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kailashahirwar/cheatsheets-ai&#34;&gt;Cheatsheets AI&lt;/a&gt; - Cheat Sheets for Keras, neural networks, scikit-learn,...&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dennybritz/deeplearning-papernotes&#34;&gt;DL PaperNotes&lt;/a&gt; - Summaries and notes on general deep learning research papers&lt;/li&gt; &#xA; &lt;li&gt;General &lt;a href=&#34;https://github.com/sindresorhus/awesome&#34;&gt;&lt;img src=&#34;https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg?sanitize=true&#34; alt=&#34;Awesome&#34;&gt;&lt;/a&gt; lists&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://minds.jacobs-university.de/sites/default/files/uploads/papers/PracticalESN.pdf&#34;&gt;Echo State Network&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://ruder.io/deep-learning-nlp-best-practices/index.html#introduction&#34;&gt;DL in NLP&lt;/a&gt; - Best practices for using neural networks by &lt;a href=&#34;http://ruder.io/&#34;&gt;Sebastian Ruder&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://cs231n.github.io/convolutional-networks/&#34;&gt;CNN overview&lt;/a&gt; - Stanford Course&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1710.02224.pdf&#34;&gt;Dilated Recurrent Neural Networks&lt;/a&gt; - How to improve RNNs?&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://machinelearningmastery.com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/?utm_content=buffer0d2a7&amp;amp;utm_medium=social&amp;amp;utm_source=twitter.com&amp;amp;utm_campaign=bufferhttps://blog.recast.ai/ml-spotlight-rnn/?utm_content=bufferf19d3&amp;amp;utm_medium=social&amp;amp;utm_source=twitter.com&amp;amp;utm_campaign=buffer&#34;&gt;Encoder-Decoder in RNNs&lt;/a&gt; - How Does Attention Work in Encoder-Decoder Recurrent Neural Networks&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/randal_olson/status/927157485240311808/photo/1&#34;&gt;On the use of DL&lt;/a&gt; - Misc fun around DL&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/eriklindernoren/ML-From-Scratch&#34;&gt;ML from scratch&lt;/a&gt; - Python implementations of ML models and algorithms from scratch from Data Mining to DL&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://project.inria.fr/deeplearning/files/2016/05/DLFrameworks.pdf&#34;&gt;Comparison of DL frameworks&lt;/a&gt; - Presentation describing the different existing frameworks for DL&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1511.07289.pdf&#34;&gt;ELU &amp;gt; ReLU&lt;/a&gt; - Article describing the differences between ELU and ReLU&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://incompleteideas.net/sutton/book/bookdraft2017nov5.pdf&#34;&gt;Reinforcement Learning: An Introduction&lt;/a&gt; - Book about reinforcement learning&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0&#34;&gt;Estimating Optimal Learning Rate&lt;/a&gt; - Blog post on the learning rate optimisation&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/scikit-learn-contrib/imbalanced-learn&#34;&gt;GitHub repo for sklearn add-on for imbalanced learning&lt;/a&gt; - ML in uneven datasets&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=YJnddoa8sHk&#34;&gt;Video on DL from Nando de Freitas, Scott Reed and Oriol Vinyals&lt;/a&gt; - Deep Learning: Practice and Trends (NIPS 2017 Tutorial, parts I &amp;amp; II)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1711.10337&#34;&gt;Article &#34;Are GANs Created Equal? A Large-Scale Study&#34;&lt;/a&gt; - Actually comparing DL algorithms&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/battle-of-the-deep-learning-frameworks-part-i-cff0e3841750&#34;&gt;Battle of the Deep Learning frameworks&lt;/a&gt; - DL frameworks comparison and evolution&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://timvieira.github.io/blog/post/2018/03/16/black-box-optimization/&#34;&gt;Black-box optimization&lt;/a&gt; - There are other optimization algorithms than just gradient descent&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ybayle/awesome-deep-learning-music#deep-learning-for-music-dl4m-&#34;&gt;Go back to top&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Cited by&lt;/h2&gt; &#xA;&lt;p&gt;If you use the information contained in this repository, please let us know! This repository is cited by:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/Slychief/status/915218386421997568&#34;&gt;Alexander Schindler&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.audiolabs-erlangen.de/resources/MIR/2017-GI-Tutorial-Musik/2017_MuellerWeissBalke_GI_DeepLearningMIR.pdf&#34;&gt;Meinard Müller, Christof Weiss, Stefan Balke&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.crowdai.org/challenges/www-2018-challenge-learning-to-recognize-musical-genre&#34;&gt;WWW 2018 Challenge: Learning to Recognize Musical Genre&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ChristosChristofidis/awesome-deep-learning&#34;&gt;Awesome Deep Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/AINewsFeed/status/897832912351105025&#34;&gt;AINewsFeed&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ybayle/awesome-deep-learning-music#deep-learning-for-music-dl4m-&#34;&gt;Go back to top&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;You are free to copy, modify, and distribute &lt;em&gt;&lt;strong&gt;Deep Learning for Music (DL4M)&lt;/strong&gt;&lt;/em&gt; with attribution under the terms of the MIT license. See the LICENSE file for details. This project use another projects and you may refer to them for appropriate license information :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ddbeck/readme-checklist&#34;&gt;Readme checklist&lt;/a&gt; - To build an universal Readme.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.pylint.org/&#34;&gt;Pylint&lt;/a&gt; - To clean the python code.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.numpy.org/&#34;&gt;Numpy&lt;/a&gt; - To manage python structure.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://matplotlib.org/&#34;&gt;Matplotlib&lt;/a&gt; - To plot nice figures.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sciunto-org/python-bibtexparser&#34;&gt;Bibtexparser&lt;/a&gt; - To deal with the bib entries.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ybayle/awesome-deep-learning-music#deep-learning-for-music-dl4m-&#34;&gt;Go back to top&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>kyahnu/latex-docker-kotex</title>
    <updated>2024-05-05T01:41:42Z</updated>
    <id>tag:github.com,2024-05-05:/kyahnu/latex-docker-kotex</id>
    <link href="https://github.com/kyahnu/latex-docker-kotex" rel="alternate"></link>
    <summary type="html">&lt;p&gt;latex-docker + kotex&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;latex-docker-kotex&lt;/h1&gt; &#xA;&lt;p&gt;latex-docker + kotex&lt;/p&gt; &#xA;&lt;p&gt;A lab-exercise template to build a docker image from &lt;code&gt;blang/latex:ctanbasic&lt;/code&gt;, adding &lt;code&gt;kotex&lt;/code&gt; packages.&lt;/p&gt; &#xA;&lt;h2&gt;Commands&lt;/h2&gt; &#xA;&lt;p&gt;To build docker image&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker build -t mylatex .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To build pdf&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./latexdockercmd.sh pdflatex main.tex&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Links&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/blang/latex-docker&#34;&gt;https://github.com/blang/latex-docker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://wiki.ktug.org/wiki/wiki.php/ko.TeX&#34;&gt;http://wiki.ktug.org/wiki/wiki.php/ko.TeX&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>