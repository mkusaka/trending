<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TeX Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-24T01:54:59Z</updated>
  <subtitle>Weekly Trending of TeX in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>AlonzoLeeeooo/awesome-text-to-image-studies</title>
    <updated>2024-03-24T01:54:59Z</updated>
    <id>tag:github.com,2024-03-24:/AlonzoLeeeooo/awesome-text-to-image-studies</id>
    <link href="https://github.com/AlonzoLeeeooo/awesome-text-to-image-studies" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A collection of awesome text-to-image generation studies.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;h1 align=&#34;center&#34;&gt;A Collection of Text-to-Image Generation Studies&lt;/h1&gt; &#xA;&lt;p&gt;This GitHub repository summarizes papers and resources related to the text-to-image (T2I) generation task.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] This document serves as the &lt;code&gt;homepage&lt;/code&gt; of the whole GitHub repo. Papers are summarized according to &lt;strong&gt;different research directions, published years, and conferences.&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/topics/topics.md&#34;&gt;The &lt;code&gt;topics&lt;/code&gt; section&lt;/a&gt; summarizes papers that are highly related to T2I generation according to different properties, e.g., prerequisites of T2I generation, diffusion models with other techniques (e.g., Diffusion Transformer, LLMs, Mamba, etc.), and diffusion models for other tasks.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;If you have any suggestions about this repository, please feel free to &lt;a href=&#34;https://github.com/AlonzoLeeeooo/awesome-text-to-image-generation-studies/issues/new&#34;&gt;start a new issue&lt;/a&gt; or &lt;a href=&#34;https://github.com/AlonzoLeeeooo/awesome-text-to-image-generation-studies/pulls&#34;&gt;pull requests&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;!-- omit in toc --&gt; &#xA;&lt;h1&gt;üî• News&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[Mar. 21th] &lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/topics/topics.md&#34;&gt;The &lt;code&gt;topics&lt;/code&gt; section&lt;/a&gt; has been updated. This section aims to offer &lt;strong&gt;paper lists that are summarized according to other properties of diffusion models&lt;/strong&gt;, e.g., Diffusion Transformer-based methods, diffusion models for NLP, diffusion models integrated with LLMs, etc. The corresponding references of these papers are also concluded in &lt;code&gt;reference.bib&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[Mar. 7th] All available &lt;strong&gt;CVPR, ICLR, and AAAI 2024 papers and references&lt;/strong&gt; are updated. Papers highlighted with ‚ö†Ô∏è will be updated as soon as their metadata is avaiable.&lt;/li&gt; &#xA; &lt;li&gt;[Mar. 1st] Websites of &lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#available-products&#34;&gt;&lt;strong&gt;the off-the-shelf text-to-image generation products&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#toolkits&#34;&gt;&lt;strong&gt;toolkits&lt;/strong&gt;&lt;/a&gt; are summarized.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- omit in toc --&gt; &#xA;&lt;h1&gt;&lt;span id=&#34;contents&#34;&gt;Contents&lt;/span&gt;&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#available-products&#34;&gt;Products&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#to-do-lists&#34;&gt;To-Do Lists&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#papers&#34;&gt;Papers&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#survey-papers&#34;&gt;Survey Papers&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#text-to-image-generation&#34;&gt;Text-to-Image Generation&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#text-year-2024&#34;&gt;Year 2024&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#text-year-2023&#34;&gt;Year 2023&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#text-year-2022&#34;&gt;Year 2022&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#text-year-2021&#34;&gt;Year 2021&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#text-year-2020&#34;&gt;Year 2020&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#conditional-text-to-image-generation&#34;&gt;Conditional Text-to-Image Generation&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#conditional-year-2024&#34;&gt;Year 2024&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#conditional-year-2023&#34;&gt;Year 2023&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#conditional-year-2022&#34;&gt;Year 2022&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#personalized-text-to-image-generation&#34;&gt;Personalized Text-to-Image Generation&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#personalized-year-2024&#34;&gt;Year 2024&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#personalized-year-2023&#34;&gt;Year 2023&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#text-guided-image-editing&#34;&gt;Text-Guided Image Editing&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#editing-year-2024&#34;&gt;Year 2024&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#editing-year-2023&#34;&gt;Year 2023&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#editing-year-2022&#34;&gt;Year 2022&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#text-image-generation&#34;&gt;Text Image Generation&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#gentext-year-2024&#34;&gt;Year 2024&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#datasets&#34;&gt;Datasets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#toolkits&#34;&gt;Toolkits&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#qa&#34;&gt;Q&amp;amp;A&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#star-history&#34;&gt;Star History&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- omit in toc --&gt; &#xA;&lt;h1&gt;To-Do Lists&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Published Papers on Conferences &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Update CVPR 2024 Papers&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Update AAAI 2024 Papers &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Update ‚ö†Ô∏è Papers and References&lt;/li&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Update arXiv References into CVPR and AAAI Versions&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Update ICLR 2024 Papers&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Update NeurIPS 2024 Papers&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Create A List with only Diffusion Model-based Papers&lt;/li&gt; &#xA; &lt;li&gt;Regular Maintenance of Preprint arXiv Papers and Missed Papers&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#contents&#34;&gt;&lt;u&gt;&lt;small&gt;&amp;lt;üéØBack to Top&amp;gt;&lt;/small&gt;&lt;/u&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- omit in toc --&gt; &#xA;&lt;h1&gt;Products&lt;/h1&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Year&lt;/th&gt; &#xA;   &lt;th&gt;Website&lt;/th&gt; &#xA;   &lt;th&gt;Specialties&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Stable Diffusion 3&lt;/td&gt; &#xA;   &lt;td&gt;2024&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://stability.ai/news/stable-diffusion-3&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Diffusion Transformer-based Stable Diffusion&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Stable Video&lt;/td&gt; &#xA;   &lt;td&gt;2024&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.stablevideo.com/&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;High-quality high-resolution images&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DALL-E 3&lt;/td&gt; &#xA;   &lt;td&gt;2023&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://openai.com/dall-e-3&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Collaborate with &lt;a href=&#34;https://chat.openai.com/&#34;&gt;ChatGPT&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ideogram&lt;/td&gt; &#xA;   &lt;td&gt;2023&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ideogram.ai/login&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Text images&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Playground&lt;/td&gt; &#xA;   &lt;td&gt;2023&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://playground.com/&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Athestic images&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HiDream.ai&lt;/td&gt; &#xA;   &lt;td&gt;2023&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://hidreamai.com/#/&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Dashtoon&lt;/td&gt; &#xA;   &lt;td&gt;2023&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dashtoon.com/&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Text-to-Comic Generation&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Midjourney&lt;/td&gt; &#xA;   &lt;td&gt;2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.midjourney.com/home&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Powerful close-sourced generation tool&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#contents&#34;&gt;&lt;u&gt;&lt;small&gt;&amp;lt;üéØBack to Top&amp;gt;&lt;/small&gt;&lt;/u&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- omit in toc --&gt; &#xA;&lt;h1&gt;Papers&lt;/h1&gt; &#xA;&lt;!-- omit in toc --&gt; &#xA;&lt;h2&gt;Survey Papers&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Text-to-Image Generation&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Year 2024 &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;strong&gt;ACM Computing Surveys&lt;/strong&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;Diffusion Models: A Comprehensive Survey of Methods and Applications &lt;a href=&#34;https://arxiv.org/pdf/2209.00796.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Year 2023 &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;strong&gt;TPAMI&lt;/strong&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;Diffusion Models in Vision: A Survey &lt;a href=&#34;https://arxiv.org/pdf/2209.04747v2&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/CroitoruAlin/Diffusion-Models-in-Vision-A-Survey&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;&lt;strong&gt;arXiv&lt;/strong&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;Text-to-image Diffusion Models in Generative AI: A Survey &lt;a href=&#34;https://arxiv.org/pdf/2303.07909&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;State of the Art on Diffusion Models for Visual Computing &lt;a href=&#34;https://arxiv.org/pdf/2310.07204.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Year 2022 &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;strong&gt;arXiv&lt;/strong&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;Efficient Diffusion Models for Vision: A Survey &lt;a href=&#34;https://arxiv.org/pdf/2210.09292&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Conditional Text-to-Image Generation&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Year 2024 &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;strong&gt;arXiv&lt;/strong&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;Controllable Generation with Text-to-Image Diffusion Models: A Survey &lt;a href=&#34;https://arxiv.org/pdf/2403.04279&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Text-Guided Image Editing&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Year 2024 &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;strong&gt;arXiv&lt;/strong&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;Diffusion Model-Based Image Editing: A Survey &lt;a href=&#34;https://arxiv.org/pdf/2402.17525.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/SiatMMLab/Awesome-Diffusion-Model-Based-Image-Editing-Methods&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#contents&#34;&gt;&lt;u&gt;&lt;small&gt;&amp;lt;üéØBack to Top&amp;gt;&lt;/small&gt;&lt;/u&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- omit in toc --&gt; &#xA;&lt;h2&gt;Text-to-Image Generation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;span id=&#34;text-year-2024&#34;&gt;&lt;strong&gt;Year 2024&lt;/strong&gt;&lt;/span&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;CVPR&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;DistriFusion:&lt;/strong&gt;&lt;/em&gt; Distributed Parallel Inference for High-Resolution Diffusion Models &lt;a href=&#34;https://arxiv.org/pdf/2402.19481.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/mit-han-lab/distrifuser&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;InstanceDiffusion:&lt;/strong&gt;&lt;/em&gt; Instance-level Control for Image Generation &lt;a href=&#34;https://arxiv.org/pdf/2402.03290.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/frank-xwang/InstanceDiffusion&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://people.eecs.berkeley.edu/~xdwang/projects/InstDiff/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;ECLIPSE:&lt;/strong&gt;&lt;/em&gt; A Resource-Efficient Text-to-Image Prior for Image Generations &lt;a href=&#34;https://arxiv.org/pdf/2312.04655.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://eclipse-t2i.vercel.app/&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://github.com/eclipse-t2i/eclipse-inference&#34;&gt;[Project]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/ECLIPSE-Community/ECLIPSE-Kandinsky-v2.2&#34;&gt;[Demo]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Instruct-Imagen:&lt;/strong&gt;&lt;/em&gt; Image Generation with Multi-modal Instruction &lt;a href=&#34;https://arxiv.org/pdf/2401.01952.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Learning Continuous 3D Words for Text-to-Image Generation &lt;a href=&#34;https://arxiv.org/pdf/2402.08654.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/ttchengab/continuous_3d_words_code/&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;HanDiffuser:&lt;/strong&gt;&lt;/em&gt; Text-to-Image Generation With Realistic Hand Appearances &lt;a href=&#34;https://arxiv.org/pdf/2403.01693.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Rich Human Feedback for Text-to-Image Generation &lt;a href=&#34;https://arxiv.org/pdf/2312.10240.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;MarkovGen:&lt;/strong&gt;&lt;/em&gt; Structured Prediction for Efficient Text-to-Image Generation &lt;a href=&#34;https://arxiv.org/pdf/2308.10997.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Customization Assistant for Text-to-image Generation &lt;a href=&#34;https://arxiv.org/pdf/2312.03045.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;ADI:&lt;/strong&gt;&lt;/em&gt; Learning Disentangled Identifiers for Action-Customized Text-to-Image Generation &lt;a href=&#34;https://arxiv.org/pdf/2311.15841.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://adi-t2i.github.io/ADI/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;UFOGen:&lt;/strong&gt;&lt;/em&gt; You Forward Once Large Scale Text-to-Image Generation via Diffusion GANs &lt;a href=&#34;https://arxiv.org/pdf/2311.09257.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Self-Discovering Interpretable Diffusion Latent Directions for Responsible Text-to-Image Generation &lt;a href=&#34;https://arxiv.org/pdf/2311.17216.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Tailored Visions:&lt;/strong&gt;&lt;/em&gt; Enhancing Text-to-Image Generation with Personalized Prompt Rewriting &lt;a href=&#34;https://arxiv.org/pdf/2310.08129.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/zzjchen/Tailored-Visions&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;CoDi:&lt;/strong&gt;&lt;/em&gt; Conditional Diffusion Distillation for Higher-Fidelity and Faster Image Generation &lt;a href=&#34;https://arxiv.org/pdf/2310.01407.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/fast-codi/CoDi&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://fast-codi.github.io/&#34;&gt;[Project]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/MKFMIKU/CoDi&#34;&gt;[Demo]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;‚ö†Ô∏è Arbitrary-Scale Image Generation and Upsampling using Latent Diffusion Model and Implicit Neural Decoder [Paper]&lt;/li&gt; &#xA;     &lt;li&gt;‚ö†Ô∏è On the Scalability of Diffusion-based Text-to-Image Generation [Paper]&lt;/li&gt; &#xA;     &lt;li&gt;‚ö†Ô∏è &lt;em&gt;&lt;strong&gt;MULAN:&lt;/strong&gt;&lt;/em&gt; A Multi Layer Annotated Dataset for Controllable Text-to-Image Generation [Paper]&lt;/li&gt; &#xA;     &lt;li&gt;‚ö†Ô∏è Discriminative Probing and Tuning for Text-to-Image Generation [Paper]&lt;/li&gt; &#xA;     &lt;li&gt;‚ö†Ô∏è Learning Multi-dimensional Human Preference for Text-to-Image Generation [Paper]&lt;/li&gt; &#xA;     &lt;li&gt;‚ö†Ô∏è Towards Effective Usage of Human-Centric Priors in Diffusion Models for Text-based Human Image Generation [Paper]&lt;/li&gt; &#xA;     &lt;li&gt;‚ö†Ô∏è Training Diffusion Models Towards Diverse Image Generation with Reinforcement Learning [Paper]&lt;/li&gt; &#xA;     &lt;li&gt;‚ö†Ô∏è Adversarial Text to Continuous Image Generation [Paper]&lt;/li&gt; &#xA;     &lt;li&gt;‚ö†Ô∏è Dynamic Prompt Optimizing for Text-to-Image Generation [Paper]&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;ICLR&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Patched Denoising Diffusion Models For High-Resolution Image Synthesis &lt;a href=&#34;https://arxiv.org/pdf/2308.01316.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/mlpc-ucsd/patch-dm&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Relay Diffusion:&lt;/strong&gt;&lt;/em&gt; Unifying diffusion process across resolutions for image synthesis &lt;a href=&#34;https://arxiv.org/pdf/2309.03350.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/THUDM/RelayDiffusion&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;SDXL:&lt;/strong&gt;&lt;/em&gt; Improving Latent Diffusion Models for High-Resolution Image Synthesis &lt;a href=&#34;https://arxiv.org/pdf/2307.01952.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/Stability-AI/generative-models&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Compose and Conquer: Diffusion-Based 3D Depth Aware Composable Image Synthesis &lt;a href=&#34;https://arxiv.org/pdf/2401.09048.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/tomtom1103/compose-and-conquer&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;PixArt-Œ±:&lt;/strong&gt;&lt;/em&gt; Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis &lt;a href=&#34;https://arxiv.org/pdf/2310.00426.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/PixArt-alpha/PixArt-alpha&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://pixart-alpha.github.io/&#34;&gt;[Project]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/PixArt-alpha/PixArt-alpha&#34;&gt;[Demo]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;AAAI&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Semantic-aware Data Augmentation for Text-to-image Synthesis &lt;a href=&#34;https://arxiv.org/pdf/2312.07951.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;‚ö†Ô∏è Text-to-Image Generation for Abstract Concepts [Paper]&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;arXiv&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation &lt;a href=&#34;https://arxiv.org/pdf/2402.10210.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;RPG:&lt;/strong&gt;&lt;/em&gt; Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs &lt;a href=&#34;https://arxiv.org/pdf/2401.11708.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/YangLing0818/RPG-DiffusionMaster&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Playground v2.5:&lt;/strong&gt;&lt;/em&gt; Three Insights towards Enhancing Aesthetic Quality in Text-to-Image Generation &lt;a href=&#34;https://arxiv.org/pdf/2402.17245.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/playgroundai/playground-v2.5-1024px-aesthetic&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;ResAdapter:&lt;/strong&gt;&lt;/em&gt; Domain Consistent Resolution Adapter for Diffusion Models &lt;a href=&#34;https://arxiv.org/pdf/2403.02084.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/bytedance/res-adapter&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://res-adapter.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;InstantID:&lt;/strong&gt;&lt;/em&gt; Zero-shot Identity-Preserving Generation in Seconds &lt;a href=&#34;https://arxiv.org/pdf/2401.07519.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/InstantID/InstantID&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://instantid.github.io/&#34;&gt;[Project]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/InstantX/InstantID&#34;&gt;[Demo]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;PIXART-Œ¥:&lt;/strong&gt;&lt;/em&gt; Fast and Controllable Image Generation with Latent Consistency Models &lt;a href=&#34;https://arxiv.org/pdf/2401.05252&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/b.com/PixArt-alpha/PixArt-alpha?tab=readme-ov-file&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;PixArt-Œ£:&lt;/strong&gt;&lt;/em&gt; Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation &lt;a href=&#34;https://arxiv.org/pdf/2403.04692&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/PixArt-alpha/PixArt-sigma&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://pixart-alpha.github.io/PixArt-sigma-project/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;CogView3:&lt;/strong&gt;&lt;/em&gt; Finer and Faster Text-to-Image Generation via Relay Diffusion &lt;a href=&#34;https://arxiv.org/pdf/2403.05121&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/THUDM/CogView&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;ELLA:&lt;/strong&gt;&lt;/em&gt; Equip Diffusion Models with LLM for Enhanced Semantic Alignment &lt;a href=&#34;https://arxiv.org/pdf/2403.05135&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/ELLA-Diffusion/ELLA&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://ella-diffusion.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Text2Street:&lt;/strong&gt;&lt;/em&gt; Controllable Text-to-image Generation for Street Views &lt;a href=&#34;https://arxiv.org/pdf/2402.04504.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;LayerDiffuse:&lt;/strong&gt;&lt;/em&gt; Transparent Image Layer Diffusion using Latent Transparency &lt;a href=&#34;https://arxiv.org/pdf/2402.17113&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/layerdiffusion/LayerDiffuse&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;SD3-Turbo:&lt;/strong&gt;&lt;/em&gt; Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation &lt;a href=&#34;https://arxiv.org/pdf/2403.12015.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;StreamMultiDiffusion:&lt;/strong&gt;&lt;/em&gt; Real-Time Interactive Generation with Region-Based Semantic Control &lt;a href=&#34;https://arxiv.org/pdf/2403.09055&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/ironjr/StreamMultiDiffusion&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;SVGDreamer:&lt;/strong&gt;&lt;/em&gt; Text Guided SVG Generation with Diffusion Model &lt;a href=&#34;https://arxiv.org/pdf/2312.16476&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/ximinng/SVGDreamer&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://ximinng.github.io/SVGDreamer-project/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;PromptCharm:&lt;/strong&gt;&lt;/em&gt; Text-to-Image Generation through Multi-modal Prompting and Refinement &lt;a href=&#34;https://arxiv.org/pdf/2403.04014.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Others&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Stable Cascade&lt;/strong&gt;&lt;/em&gt; &lt;a href=&#34;https://stability.ai/news/introducing-stable-cascade&#34;&gt;[Blog]&lt;/a&gt; &lt;a href=&#34;https://github.com/Stability-AI/StableCascade&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#contents&#34;&gt;&lt;u&gt;&lt;small&gt;&amp;lt;üéØBack to Top&amp;gt;&lt;/small&gt;&lt;/u&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;span id=&#34;text-year-2023&#34;&gt;&lt;strong&gt;Year 2023&lt;/strong&gt;&lt;/span&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;CVPR&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;GigaGAN:&lt;/strong&gt;&lt;/em&gt; Scaling Up GANs for Text-to-Image Synthesis &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Kang_Scaling_Up_GANs_for_Text-to-Image_Synthesis_CVPR_2023_paper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/lucidrains/gigagan-pytorch&#34;&gt;[Reproduced Code]&lt;/a&gt; &lt;a href=&#34;https://mingukkang.github.io/GigaGAN/&#34;&gt;[Project]&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=ZjxtuDQkOPY&amp;amp;feature=youtu.be&#34;&gt;[Video]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;ERNIE-ViLG 2.0:&lt;/strong&gt;&lt;/em&gt; Improving Text-to-Image Diffusion Model With Knowledge-Enhanced Mixture-of-Denoising-Experts &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_ERNIE-ViLG_2.0_Improving_Text-to-Image_Diffusion_Model_With_Knowledge-Enhanced_Mixture-of-Denoising-Experts_CVPR_2023_paper.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Shifted Diffusion for Text-to-image Generation &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Shifted_Diffusion_for_Text-to-Image_Generation_CVPR_2023_paper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/drboog/Shifted_Diffusion&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;GALIP:&lt;/strong&gt;&lt;/em&gt; Generative Adversarial CLIPs for Text-to-Image Synthesis &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Tao_GALIP_Generative_Adversarial_CLIPs_for_Text-to-Image_Synthesis_CVPR_2023_paper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/tobran/GALIP&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Specialist Diffusion:&lt;/strong&gt;&lt;/em&gt; Plug-and-Play Sample-Efficient Fine-Tuning of Text-to-Image Diffusion Models to Learn Any Unseen Style &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Specialist_Diffusion_Plug-and-Play_Sample-Efficient_Fine-Tuning_of_Text-to-Image_Diffusion_Models_To_CVPR_2023_paper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/Picsart-AI-Research/Specialist-Diffusion&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Toward Verifiable and Reproducible Human Evaluation for Text-to-Image Generation &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Otani_Toward_Verifiable_and_Reproducible_Human_Evaluation_for_Text-to-Image_Generation_CVPR_2023_paper.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;RIATIG:&lt;/strong&gt;&lt;/em&gt; Reliable and Imperceptible Adversarial Text-to-Image Generation with Natural Prompts &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_RIATIG_Reliable_and_Imperceptible_Adversarial_Text-to-Image_Generation_With_Natural_Prompts_CVPR_2023_paper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/WUSTL-CSPL/RIATIG&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;NeurIPS&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;ImageReward:&lt;/strong&gt;&lt;/em&gt; Learning and Evaluating Human Preferences for Text-to-Image Generation &lt;a href=&#34;https://openreview.net/pdf?id=JVzeOYEx6d&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/THUDM/ImageReward&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;RAPHAEL&lt;/strong&gt;&lt;/em&gt;: Text-to-Image Generation via Large Mixture of Diffusion Paths &lt;a href=&#34;https://arxiv.org/pdf/2305.18295&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://raphael-painter.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;ICLR&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Training-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis &lt;a href=&#34;https://openreview.net/pdf?id=PUIqjT4rzq7&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/weixi-feng/Structured-Diffusion-Guidance&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;ICML&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;StyleGAN-T:&lt;/strong&gt;&lt;/em&gt; Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis &lt;a href=&#34;https://proceedings.mlr.press/v202/sauer23a/sauer23a.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/autonomousvision/stylegan-t&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://sites.google.com/view/stylegan-t/&#34;&gt;[Project]&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=MMj8OTOUIok&#34;&gt;[Video]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Muse:&lt;/strong&gt;&lt;/em&gt; Text-To-Image Generation via Masked Generative Transformers &lt;a href=&#34;https://proceedings.mlr.press/v202/chang23b/chang23b.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/lucidrains/muse-maskgit-pytorch&#34;&gt;[Reproduced Code]&lt;/a&gt; &lt;a href=&#34;https://muse-icml.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;UniDiffusers:&lt;/strong&gt;&lt;/em&gt; One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale &lt;a href=&#34;https://arxiv.org/pdf/2303.06555&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/thu-ml/unidiffuser&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;ACM MM&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;SUR-adapter:&lt;/strong&gt;&lt;/em&gt; Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models &lt;a href=&#34;https://arxiv.org/pdf/2305.05189.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/Qrange-group/SUR-adapter&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;ControlStyle:&lt;/strong&gt;&lt;/em&gt; Text-Driven Stylized Image Generation Using Diffusion Priors &lt;a href=&#34;https://arxiv.org/pdf/2311.05463.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;SIGGRAPH&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Attend-and-Excite:&lt;/strong&gt;&lt;/em&gt; Attention-Based Semantic Guidance for Text-to-Image Diffusion Models &lt;a href=&#34;https://arxiv.org/pdf/2301.13826.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/yuval-alaluf/Attend-and-Excite&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://yuval-alaluf.github.io/Attend-and-Excite/&#34;&gt;[Project]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/AttendAndExcite/Attend-and-Excite&#34;&gt;[Demo]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;arXiv&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;P+:&lt;/strong&gt;&lt;/em&gt; Extended Textual Conditioning in Text-to-Image Generation &lt;a href=&#34;https://prompt-plus.github.io/files/PromptPlus.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;SDXL-Turbo:&lt;/strong&gt;&lt;/em&gt; Adversarial Diffusion Distillation &lt;a href=&#34;https://arxiv.org/pdf/2311.17042.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/Stability-AI/generative-models&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Wuerstchen:&lt;/strong&gt;&lt;/em&gt; An Efficient Architecture for Large-Scale Text-to-Image Diffusion Models &lt;a href=&#34;https://arxiv.org/pdf/2306.00637.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/dome272/Wuerstchen&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;StreamDiffusion:&lt;/strong&gt;&lt;/em&gt; A Pipeline-level Solution for Real-time Interactive Generation &lt;a href=&#34;https://arxiv.org/pdf/2312.12491.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/cumulo-autumn/StreamDiffusion&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;ParaDiffusion:&lt;/strong&gt;&lt;/em&gt; Paragraph-to-Image Generation with Information-Enriched Diffusion Model &lt;a href=&#34;https://arxiv.org/pdf/2311.14284&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/weijiawu/ParaDiffusion&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Others&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;DALL-E 3:&lt;/strong&gt;&lt;/em&gt; Improving Image Generation with Better Captions &lt;a href=&#34;https://cdn.openai.com/papers/dall-e-3.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#contents&#34;&gt;&lt;u&gt;&lt;small&gt;&amp;lt;üéØBack to Top&amp;gt;&lt;/small&gt;&lt;/u&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;span id=&#34;text-year-2022&#34;&gt;&lt;strong&gt;Year 2022&lt;/strong&gt;&lt;/span&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;CVPR&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;üî• &lt;em&gt;&lt;strong&gt;Stable Diffusion:&lt;/strong&gt;&lt;/em&gt; High-Resolution Image Synthesis With Latent Diffusion Models &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/CompVis/latent-diffusion&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://ommer-lab.com/research/latent-diffusion-models/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Vector Quantized Diffusion Model for Text-to-Image Synthesis &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/papers/Gu_Vector_Quantized_Diffusion_Model_for_Text-to-Image_Synthesis_CVPR_2022_paper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/cientgu/VQ-Diffusion&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;DF-GAN:&lt;/strong&gt;&lt;/em&gt; A Simple and Effective Baseline for Text-to-Image Synthesis &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/papers/Tao_DF-GAN_A_Simple_and_Effective_Baseline_for_Text-to-Image_Synthesis_CVPR_2022_paper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/tobran/DF-GAN&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;LAFITE:&lt;/strong&gt;&lt;/em&gt; Towards Language-Free Training for Text-to-Image Generation &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Towards_Language-Free_Training_for_Text-to-Image_Generation_CVPR_2022_paper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/drboog/Lafite&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Text-to-Image Synthesis based on Object-Guided Joint-Decoding Transformer &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_Text-to-Image_Synthesis_Based_on_Object-Guided_Joint-Decoding_Transformer_CVPR_2022_paper.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;StyleT2I:&lt;/strong&gt;&lt;/em&gt; Toward Compositional and High-Fidelity Text-to-Image Synthesis &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/papers/Li_StyleT2I_Toward_Compositional_and_High-Fidelity_Text-to-Image_Synthesis_CVPR_2022_paper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/zhihengli-UR/StyleT2I&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;ECCV&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Make-A-Scene:&lt;/strong&gt;&lt;/em&gt; Scene-Based Text-to-Image Generation with Human Priors &lt;a href=&#34;https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136750087.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/CasualGANPapers/Make-A-Scene&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1SPyQ-epTsAOAu8BEohUokN4-b5RM_TnE?usp=sharing&#34;&gt;[Demo]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Trace Controlled Text to Image Generation &lt;a href=&#34;https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136960058.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Improved Masked Image Generation with Token-Critic &lt;a href=&#34;https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136830070.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;VQGAN-CLIP:&lt;/strong&gt;&lt;/em&gt; Open Domain Image Generation and Manipulation Using Natural Language &lt;a href=&#34;https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136970088.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/EleutherAI/vqgan-clip&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;TISE:&lt;/strong&gt;&lt;/em&gt; Bag of Metrics for Text-to-Image Synthesis Evaluation &lt;a href=&#34;https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136960585.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/VinAIResearch/tise-toolbox&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;StoryDALL-E:&lt;/strong&gt;&lt;/em&gt; Adapting Pretrained Text-to-image Transformers for Story Continuation &lt;a href=&#34;https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136970070.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/adymaharana/storydalle&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/ECCV2022/storydalle&#34;&gt;[Demo]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;NeurIPS&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;CogView2:&lt;/strong&gt;&lt;/em&gt; Faster and Better Text-to-Image Generation via Hierarchical Transformers &lt;a href=&#34;https://openreview.net/pdf?id=GkDbQb6qu_r&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://openreview.net/pdf?id=GkDbQb6qu_r&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Imagen:&lt;/strong&gt;&lt;/em&gt; Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding &lt;a href=&#34;https://papers.nips.cc/paper_files/paper/2022/file/ec795aeadae0b7d230fa35cbaf04c041-Paper-Conference.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/lucidrains/imagen-pytorch&#34;&gt;[Reproduced Code]&lt;/a&gt; &lt;a href=&#34;https://imagen.research.google/&#34;&gt;[Project]&lt;/a&gt; &lt;a href=&#34;https://deepmind.google/technologies/imagen-2/&#34;&gt;[&lt;em&gt;&lt;strong&gt;Imagen 2&lt;/strong&gt;&lt;/em&gt;]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;ACM MM&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Adma-GAN:&lt;/strong&gt;&lt;/em&gt; Attribute-Driven Memory Augmented GANs for Text-to-Image Generation &lt;a href=&#34;https://arxiv.org/pdf/2209.14046.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/Hsintien-Ng/Adma-GAN&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Background Layout Generation and Object Knowledge Transfer for Text-to-Image Generation &lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3503161.3548154&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;DSE-GAN:&lt;/strong&gt;&lt;/em&gt; Dynamic Semantic Evolution Generative Adversarial Network for Text-to-Image Generation &lt;a href=&#34;https://arxiv.org/pdf/2209.01339.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;AtHom:&lt;/strong&gt;&lt;/em&gt; Two Divergent Attentions Stimulated By Homomorphic Training in Text-to-Image Synthesis &lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3503161.3548159&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;arXiv&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;DALLE-2:&lt;/strong&gt;&lt;/em&gt; Hierarchical Text-Conditional Image Generation with CLIP Latents &lt;a href=&#34;https://cdn.openai.com/papers/dall-e-2.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;PITI:&lt;/strong&gt;&lt;/em&gt; Pretraining is All You Need for Image-to-Image Translation &lt;a href=&#34;https://arxiv.org/pdf/2205.12952.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/PITI-Synthesis/PITI&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#contents&#34;&gt;&lt;u&gt;&lt;small&gt;&amp;lt;üéØBack to Top&amp;gt;&lt;/small&gt;&lt;/u&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;span id=&#34;text-year-2021&#34;&gt;&lt;strong&gt;Year 2021&lt;/strong&gt;&lt;/span&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;ICCV&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;DAE-GAN:&lt;/strong&gt;&lt;/em&gt; Dynamic Aspect-aware GAN for Text-to-Image Synthesis &lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2021/papers/Ruan_DAE-GAN_Dynamic_Aspect-Aware_GAN_for_Text-to-Image_Synthesis_ICCV_2021_paper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/hiarsal/DAE-GAN&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;NeurIPS&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;CogView:&lt;/strong&gt;&lt;/em&gt; Mastering Text-to-Image Generation via Transformers &lt;a href=&#34;https://proceedings.neurips.cc/paper/2021/file/a4d92e2cd541fca87e4620aba658316d-Paper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/THUDM/CogView&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://thudm.github.io/CogView/index.html&#34;&gt;[Demo]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;UFC-BERT:&lt;/strong&gt;&lt;/em&gt; Unifying Multi-Modal Controls for Conditional Image Synthesis &lt;a href=&#34;https://proceedings.neurips.cc/paper/2021/file/e46bc064f8e92ac2c404b9871b2a4ef2-Paper.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;ICML&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;DALLE-1:&lt;/strong&gt;&lt;/em&gt; Zero-Shot Text-to-Image Generation &lt;a href=&#34;https://proceedings.mlr.press/v139/ramesh21a/ramesh21a.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/lucidrains/DALLE-pytorch&#34;&gt;[Reproduced Code]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;ACM MM&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Cycle-Consistent Inverse GAN for Text-to-Image Synthesis &lt;a href=&#34;https://arxiv.org/pdf/2108.01361.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;R-GAN:&lt;/strong&gt;&lt;/em&gt; Exploring Human-like Way for Reasonable Text-to-Image Synthesis via Generative Adversarial Networks &lt;a href=&#34;https://dl.acm.org/doi/10.1145/3474085.3475363&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#contents&#34;&gt;&lt;u&gt;&lt;small&gt;&amp;lt;üéØBack to Top&amp;gt;&lt;/small&gt;&lt;/u&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;span id=&#34;text-year-2020&#34;&gt;&lt;strong&gt;Year 2020&lt;/strong&gt;&lt;/span&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;ACM MM&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Text-to-Image Synthesis via Aesthetic Layout &lt;a href=&#34;https://dl.acm.org/doi/10.1145/3394171.3414357&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#contents&#34;&gt;&lt;u&gt;&lt;small&gt;&amp;lt;üéØBack to Top&amp;gt;&lt;/small&gt;&lt;/u&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- omit in toc --&gt; &#xA;&lt;h2&gt;Conditional Text-to-Image Generation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;span id=&#34;conditional-year-2024&#34;&gt;&lt;strong&gt;Year 2024&lt;/strong&gt;&lt;/span&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;CVPR&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;PLACE:&lt;/strong&gt;&lt;/em&gt; Adaptive Layout-Semantic Fusion for Semantic Image Synthesis &lt;a href=&#34;https://arxiv.org/pdf/2403.01852.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;One-Shot Structure-Aware Stylized Image Synthesis &lt;a href=&#34;https://arxiv.org/pdf/2402.17275.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Grounded Text-to-Image Synthesis with Attention Refocusing [Paper] [[Project]]&lt;/li&gt; &#xA;     &lt;li&gt;Coarse-to-Fine Latent Diffusion for Pose-Guided Person Image Synthesis &lt;a href=&#34;https://arxiv.org/pdf/2402.18078.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/YanzuoLu/CFLD&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;‚ö†Ô∏è &lt;em&gt;&lt;strong&gt;Zero-Painter:&lt;/strong&gt;&lt;/em&gt; Training-Free Layout Control for Text-to-Image Synthesis [Paper]&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;ICLR&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Advancing Pose-Guided Image Synthesis with Progressive Conditional Diffusion Models &lt;a href=&#34;https://arxiv.org/pdf/2310.06313.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/muzishen/PCDMs&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;WACV&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Training-Free Layout Control with Cross-Attention Guidance &lt;a href=&#34;https://openaccess.thecvf.com/content/WACV2024/papers/Chen_Training-Free_Layout_Control_With_Cross-Attention_Guidance_WACV_2024_paper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/silent-chen/layout-guidance&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://silent-chen.github.io/layout-guidance/&#34;&gt;[Project]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/silentchen/layout-guidance&#34;&gt;[Demo]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;AAAI&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;SSMG:&lt;/strong&gt;&lt;/em&gt; Spatial-Semantic Map Guided Diffusion Model for Free-form Layout-to-image Generation &lt;a href=&#34;https://arxiv.org/pdf/2308.10156.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Compositional Text-to-Image Synthesis with Attention Map Control of Diffusion Models &lt;a href=&#34;https://arxiv.org/pdf/2305.13921.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/OPPO-Mente-Lab/attention-mask-control&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;arXiv&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;em&gt;&lt;strong&gt;DEADiff:&lt;/strong&gt;&lt;/em&gt; An Efficient Stylization Diffusion Model with Disentangled Representations &lt;a href=&#34;https://arxiv.org/pdf/2403.06951&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#contents&#34;&gt;&lt;u&gt;&lt;small&gt;&amp;lt;üéØBack to Top&amp;gt;&lt;/small&gt;&lt;/u&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;span id=&#34;conditional-year-2023&#34;&gt;&lt;strong&gt;Year 2023&lt;/strong&gt;&lt;/span&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;CVPR&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;GLIGEN:&lt;/strong&gt;&lt;/em&gt; Open-Set Grounded Text-to-Image Generation &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Li_GLIGEN_Open-Set_Grounded_Text-to-Image_Generation_CVPR_2023_paper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/gligen/GLIGEN&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://gligen.github.io/&#34;&gt;[Project]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/gligen/demo&#34;&gt;[Demo]&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=-MCkU7IAGKs&amp;amp;feature=youtu.be&#34;&gt;[Video]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Autoregressive Image Generation using Residual Quantization &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/papers/Lee_Autoregressive_Image_Generation_Using_Residual_Quantization_CVPR_2022_paper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/kakaobrain/rq-vae-transformer&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;SpaText:&lt;/strong&gt;&lt;/em&gt; Spatio-Textual Representation for Controllable Image Generation &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Avrahami_SpaText_Spatio-Textual_Representation_for_Controllable_Image_Generation_CVPR_2023_paper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://omriavrahami.com/spatext/&#34;&gt;[Project]&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=VlieNoCwHO4&#34;&gt;[Video]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Text to Image Generation with Semantic-Spatial Aware GAN &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/papers/Liao_Text_to_Image_Generation_With_Semantic-Spatial_Aware_GAN_CVPR_2022_paper.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;ReCo:&lt;/strong&gt;&lt;/em&gt; Region-Controlled Text-to-Image Generation &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_ReCo_Region-Controlled_Text-to-Image_Generation_CVPR_2023_paper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/ReCo&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;LayoutDiffusion:&lt;/strong&gt;&lt;/em&gt; Controllable Diffusion Model for Layout-to-image Generation &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_LayoutDiffusion_Controllable_Diffusion_Model_for_Layout-to-Image_Generation_CVPR_2023_paper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/ZGCTroy/LayoutDiffusion&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;ICCV&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;ControlNet:&lt;/strong&gt;&lt;/em&gt; Adding Conditional Control to Text-to-Image Diffusion Models &lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Adding_Conditional_Control_to_Text-to-Image_Diffusion_Models_ICCV_2023_paper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/lllyasviel/ControlNet&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;SceneGenie:&lt;/strong&gt;&lt;/em&gt; Scene Graph Guided Diffusion Models for Image Synthesis &lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2023W/SG2RL/papers/Farshad_SceneGenie_Scene_Graph_Guided_Diffusion_Models_for_Image_Synthesis_ICCVW_2023_paper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2023W/SG2RL/papers/Farshad_SceneGenie_Scene_Graph_Guided_Diffusion_Models_for_Image_Synthesis_ICCVW_2023_paper.pdf&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;ICML&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Composer:&lt;/strong&gt;&lt;/em&gt; Creative and Controllable Image Synthesis with Composable Conditions &lt;a href=&#34;https://proceedings.mlr.press/v202/huang23b/huang23b.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/ali-vilab/composer&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://ali-vilab.github.io/composer-page/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;MultiDiffusion:&lt;/strong&gt;&lt;/em&gt; Fusing Diffusion Paths for Controlled Image Generation &lt;a href=&#34;https://proceedings.mlr.press/v202/bar-tal23a/bar-tal23a.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/omerbt/MultiDiffusion&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=D2Q0D1gIeqg&#34;&gt;[Video]&lt;/a&gt; &lt;a href=&#34;https://multidiffusion.github.io/&#34;&gt;[Project]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/weizmannscience/MultiDiffusion&#34;&gt;[Demo]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;SIGGRAPH&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Sketch-Guided Text-to-Image Diffusion Models &lt;a href=&#34;https://dl.acm.org/doi/pdf/10.1145/3588432.3591560&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/ogkalu2/Sketch-Guided-Stable-Diffusion&#34;&gt;[Reproduced Code]&lt;/a&gt; &lt;a href=&#34;https://sketch-guided-diffusion.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;NeurIPS&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Uni-ControlNet:&lt;/strong&gt;&lt;/em&gt; All-in-One Control to Text-to-Image Diffusion Models &lt;a href=&#34;https://arxiv.org/pdf/2305.16322.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/ShihaoZhaoZSH/Uni-ControlNet&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://shihaozhaozsh.github.io/unicontrolnet/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Prompt Diffusion:&lt;/strong&gt;&lt;/em&gt; In-Context Learning Unlocked for Diffusion Models &lt;a href=&#34;https://openreview.net/pdf?id=6BZS2EAkns&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/Zhendong-Wang/Prompt-Diffusion&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://zhendong-wang.github.io/prompt-diffusion.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;WACV&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;More Control for Free! Image Synthesis with Semantic Diffusion Guidance &lt;a href=&#34;https://openaccess.thecvf.com/content/WACV2023/papers/Liu_More_Control_for_Free_Image_Synthesis_With_Semantic_Diffusion_Guidance_WACV_2023_paper.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;ACM MM&lt;/strong&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;&lt;strong&gt;LayoutLLM-T2I:&lt;/strong&gt;&lt;/em&gt; Eliciting Layout Guidance from LLM for Text-to-Image Generation &lt;a href=&#34;https://arxiv.org/pdf/2308.05095.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;arXiv&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;T2I-Adapter:&lt;/strong&gt;&lt;/em&gt; Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models &lt;a href=&#34;https://arxiv.org/pdf/2302.08453.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/TencentARC/T2I-Adapter&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/TencentARC/T2I-Adapter-SDXL&#34;&gt;[Demo]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;BLIP-Diffusion:&lt;/strong&gt;&lt;/em&gt; Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing &lt;a href=&#34;https://arxiv.org/pdf/2305.14720.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#contents&#34;&gt;&lt;u&gt;&lt;small&gt;&amp;lt;üéØBack to Top&amp;gt;&lt;/small&gt;&lt;/u&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- omit in toc --&gt; &#xA;&lt;h2&gt;Personalized Text-to-Image Generation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;span id=&#34;personalized-year-2024&#34;&gt;&lt;strong&gt;Year 2024&lt;/strong&gt;&lt;/span&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;CVPR&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Cross Initialization for Personalized Text-to-Image Generation &lt;a href=&#34;https://arxiv.org/pdf/2312.15905.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;When StyleGAN Meets Stable Diffusion: a W+ Adapter for Personalized Image Generation &lt;a href=&#34;https://arxiv.org/pdf/2311.17461.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/csxmli2016/w-plus-adapter&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://csxmli2016.github.io/projects/w-plus-adapter/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Style Aligned Image Generation via Shared Attention &lt;a href=&#34;https://arxiv.org/pdf/2312.02133.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/google/style-aligned&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://style-aligned-gen.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;InstantBooth:&lt;/strong&gt;&lt;/em&gt; Personalized Text-to-Image Generation without Test-Time Finetuning &lt;a href=&#34;https://arxiv.org/pdf/2304.03411.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://jshi31.github.io/InstantBooth/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;High Fidelity Person-centric Subject-to-Image Synthesis &lt;a href=&#34;https://arxiv.org/pdf/2311.10329.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;RealCustom:&lt;/strong&gt;&lt;/em&gt; Narrowing Real Text Word for Real-Time Open-Domain Text-to-Image Customization &lt;a href=&#34;https://arxiv.org/pdf/2403.00483.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://corleone-huang.github.io/realcustom/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;‚ö†Ô∏è FreeCustom: Tuning-Free Customized Image Generation for Multi-Concept Composition [Paper]&lt;/li&gt; &#xA;     &lt;li&gt;‚ö†Ô∏è &lt;em&gt;&lt;strong&gt;JeDi:&lt;/strong&gt;&lt;/em&gt; Joint-Image Diffusion Models for Finetuning-Free Personalized Text-to-Image Generation [Paper]&lt;/li&gt; &#xA;     &lt;li&gt;‚ö†Ô∏è Countering Personalized Text-to-Image Generation with Influence Watermarks [Paper]&lt;/li&gt; &#xA;     &lt;li&gt;‚ö†Ô∏è Personalized Residuals for Concept-Driven Text-to-Image Generation [Paper]&lt;/li&gt; &#xA;     &lt;li&gt;‚ö†Ô∏è Improving Subject-Driven Image Synthesis with Context-Agnostic Guidance [Paper]&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;AAAI&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Decoupled Textual Embeddings for Customized Image Generation &lt;a href=&#34;https://arxiv.org/pdf/2312.11826.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;span id=&#34;personalized-year-2023&#34;&gt;&lt;strong&gt;Year 2023&lt;/strong&gt;&lt;/span&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;CVPR&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Custom Diffusion:&lt;/strong&gt;&lt;/em&gt; Multi-Concept Customization of Text-to-Image Diffusion &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Kumari_Multi-Concept_Customization_of_Text-to-Image_Diffusion_CVPR_2023_paper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/adobe-research/custom-diffusion&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://www.cs.cmu.edu/~custom-diffusion/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;DreamBooth:&lt;/strong&gt;&lt;/em&gt; Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Ruiz_DreamBooth_Fine_Tuning_Text-to-Image_Diffusion_Models_for_Subject-Driven_Generation_CVPR_2023_paper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/google/dreambooth&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://dreambooth.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;ICCV&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;ELITE:&lt;/strong&gt;&lt;/em&gt; Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation &lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2023/papers/Wei_ELITE_Encoding_Visual_Concepts_into_Textual_Embeddings_for_Customized_Text-to-Image_ICCV_2023_paper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/csyxwei/ELITE&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;ICLR&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Textual Inversion:&lt;/strong&gt;&lt;/em&gt; An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion &lt;a href=&#34;https://openreview.net/pdf?id=NAQvF08TcyG&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/rinongal/textual_inversion&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://textual-inversion.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;SIGGRAPH&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Break-A-Scene:&lt;/strong&gt;&lt;/em&gt; Extracting Multiple Concepts from a Single Image &lt;a href=&#34;https://arxiv.org/pdf/2305.16311.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/google/break-a-scene&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Encoder-based Domain Tuning for Fast Personalization of Text-to-Image Models &lt;a href=&#34;https://arxiv.org/pdf/2302.12228.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://tuning-encoder.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;LayerDiffusion:&lt;/strong&gt;&lt;/em&gt; Layered Controlled Image Editing with Diffusion Models &lt;a href=&#34;https://dl.acm.org/doi/pdf/10.1145/3610543.3626172&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;arXiv&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;DreamTuner:&lt;/strong&gt;&lt;/em&gt; Single Image is Enough for Subject-Driven Generation &lt;a href=&#34;https://arxiv.org/pdf/2312.13691.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://dreamtuner-diffusion.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;PhotoMaker:&lt;/strong&gt;&lt;/em&gt; Customizing Realistic Human Photos via Stacked ID Embedding &lt;a href=&#34;https://arxiv.org/pdf/2312.04461.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/TencentARC/PhotoMaker&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#contents&#34;&gt;&lt;u&gt;&lt;small&gt;&amp;lt;üéØBack to Top&amp;gt;&lt;/small&gt;&lt;/u&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- omit in toc --&gt; &#xA;&lt;h2&gt;Text-Guided Image Editing&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;span id=&#34;editing-year-2024&#34;&gt;&lt;strong&gt;Year 2024&lt;/strong&gt;&lt;/span&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;CVPR&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;InfEdit:&lt;/strong&gt;&lt;/em&gt; Inversion-Free Image Editing with Natural Language &lt;a href=&#34;https://arxiv.org/pdf/2312.04965.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/sled-group/InfEdit&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://sled-group.github.io/InfEdit/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Towards Understanding Cross and Self-Attention in Stable Diffusion for Text-Guided Image Editing &lt;a href=&#34;https://arxiv.org/pdf/2403.03431.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Doubly Abductive Counterfactual Inference for Text-based Image Editing &lt;a href=&#34;https://arxiv.org/pdf/2403.02981.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/xuesong39/DAC&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Focus on Your Instruction: Fine-grained and Multi-instruction Image Editing by Attention Modulation &lt;a href=&#34;https://arxiv.org/pdf/2312.10113.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/guoqincode/Focus-on-Your-Instruction&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Contrastive Denoising Score for Text-guided Latent Diffusion Image Editing &lt;a href=&#34;https://arxiv.org/pdf/2311.18608.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;DragDiffusion:&lt;/strong&gt;&lt;/em&gt; Harnessing Diffusion Models for Interactive Point-based Image Editing &lt;a href=&#34;https://arxiv.org/pdf/2306.14435.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/Yujun-Shi/DragDiffusion&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;DiffEditor:&lt;/strong&gt;&lt;/em&gt; Boosting Accuracy and Flexibility on Diffusion-based Image Editing &lt;a href=&#34;https://arxiv.org/pdf/2402.02583.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;FreeDrag:&lt;/strong&gt;&lt;/em&gt; Feature Dragging for Reliable Point-based Image Editing &lt;a href=&#34;https://arxiv.org/pdf/2307.04684.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/LPengYang/FreeDrag&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Text-Driven Image Editing via Learnable Regions &lt;a href=&#34;https://arxiv.org/pdf/2311.16432.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/yuanze-lin/Learnable_Regions&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://yuanze-lin.me/LearnableRegions_page/&#34;&gt;[Project]&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=FpMWRXFraK8&amp;amp;feature=youtu.be&#34;&gt;[Video]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;LEDITS++:&lt;/strong&gt;&lt;/em&gt; Limitless Image Editing using Text-to-Image Models &lt;a href=&#34;https://arxiv.org/pdf/2311.16711.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/editing-images/ledtisplusplus/tree/main&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://leditsplusplus-project.static.hf.space/index.html&#34;&gt;[Project]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/editing-images/leditsplusplus&#34;&gt;[Demo]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;SmartEdit:&lt;/strong&gt;&lt;/em&gt; Exploring Complex Instruction-based Image Editing with Large Language Models &lt;a href=&#34;https://arxiv.org/pdf/2312.06739.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/TencentARC/SmartEdit&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://yuzhou914.github.io/SmartEdit/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Edit One for All: Interactive Batch Image Editing &lt;a href=&#34;https://arxiv.org/pdf/2401.10219.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/thaoshibe/edit-one-for-all&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://thaoshibe.github.io/edit-one-for-all/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;‚ö†Ô∏è &lt;em&gt;&lt;strong&gt;TiNO-Edit:&lt;/strong&gt;&lt;/em&gt; Timestep and Noise Optimization for Robust Diffusion-Based Image Editing [Paper]&lt;/li&gt; &#xA;     &lt;li&gt;‚ö†Ô∏è Person in Place: Generating Associative Skeleton-Guidance Maps for Human-Object Interaction Image Editing [Paper]&lt;/li&gt; &#xA;     &lt;li&gt;‚ö†Ô∏è Referring Image Editing: Object-level Image Editing via Referring Expressions [Paper]&lt;/li&gt; &#xA;     &lt;li&gt;‚ö†Ô∏è The Devil is in the Details: StyleFeatureEditor for Detail-Rich StyleGAN Inversion and High Quality Image Editing [Paper]&lt;/li&gt; &#xA;     &lt;li&gt;‚ö†Ô∏è Prompt Augmentation for Self-supervised Text-guided Image Manipulation [Paper]&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;ICLR&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Guiding Instruction-based Image Editing via Multimodal Large Language Models &lt;a href=&#34;https://arxiv.org/pdf/2309.17102.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/apple/ml-mgie&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://mllm-ie.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;The Blessing of Randomness: SDE Beats ODE in General Diffusion-based Image Editing &lt;a href=&#34;https://arxiv.org/pdf/2311.01410.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/ML-GSAI/SDE-Drag&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://ml-gsai.github.io/SDE-Drag-demo/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Motion Guidance:&lt;/strong&gt;&lt;/em&gt; Diffusion-Based Image Editing with Differentiable Motion Estimators &lt;a href=&#34;https://arxiv.org/pdf/2401.18085.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/dangeng/motion_guidance&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://dangeng.github.io/motion_guidance/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Object-Aware Inversion and Reassembly for Image Editing &lt;a href=&#34;https://arxiv.org/pdf/2310.12149.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/aim-uofa/OIR&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://aim-uofa.github.io/OIR-Diffusion/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Noise Map Guidance:&lt;/strong&gt;&lt;/em&gt; Inversion with Spatial Context for Real Image Editing &lt;a href=&#34;https://arxiv.org/pdf/2402.04625.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;AAAI&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Tuning-Free Inversion-Enhanced Control for Consistent Image Editing &lt;a href=&#34;https://arxiv.org/pdf/2312.14611&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;BARET:&lt;/strong&gt;&lt;/em&gt; Balanced Attention based Real image Editing driven by Target-text Inversion &lt;a href=&#34;https://arxiv.org/pdf/2312.05482&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Accelerating Text-to-Image Editing via Cache-Enabled Sparse Diffusion Inference &lt;a href=&#34;https://arxiv.org/pdf/2305.17423&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;High-Fidelity Diffusion-based Image Editing &lt;a href=&#34;https://arxiv.org/pdf/2312.15707&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;AdapEdit:&lt;/strong&gt;&lt;/em&gt; Spatio-Temporal Guided Adaptive Editing Algorithm for Text-Based Continuity-Sensitive Image Editing &lt;a href=&#34;https://arxiv.org/pdf/2312.08019&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;‚ö†Ô∏è TexFit: Text-Driven Fashion Image Editing with Diffusion Models [Paper]&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;arXiv&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;An Item is Worth a Prompt:&lt;/strong&gt;&lt;/em&gt; Versatile Image Editing with Disentangled Control &lt;a href=&#34;https://arxiv.org/pdf/2403.04880.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/asFeng/d-edit&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;StableDrag:&lt;/strong&gt;&lt;/em&gt; Stable Dragging for Point-based Image Editing &lt;a href=&#34;https://arxiv.org/pdf/2403.04437&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;One-Dimensional Adapter to Rule Them All: Concepts, Diffusion Models and Erasing Applications &lt;a href=&#34;https://arxiv.org/pdf/2312.16145&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/Con6924/SPM&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://lyumengyao.github.io/projects/spm&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;span id=&#34;editing-year-2023&#34;&gt;&lt;strong&gt;Year 2023&lt;/strong&gt;&lt;/span&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;CVPR&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Uncovering the Disentanglement Capability in Text-to-Image Diffusion Models &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Uncovering_the_Disentanglement_Capability_in_Text-to-Image_Diffusion_Models_CVPR_2023_paper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/UCSB-NLP-Chang/DiffusionDisentanglement&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;SINE:&lt;/strong&gt;&lt;/em&gt; SINgle Image Editing with Text-to-Image Diffusion Models &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_SINE_SINgle_Image_Editing_With_Text-to-Image_Diffusion_Models_CVPR_2023_paper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/zhang-zx/SINE&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Imagic:&lt;/strong&gt;&lt;/em&gt; Text-Based Real Image Editing with Diffusion Models &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Kawar_Imagic_Text-Based_Real_Image_Editing_With_Diffusion_Models_CVPR_2023_paper.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;InstructPix2Pix:&lt;/strong&gt;&lt;/em&gt; Learning to Follow Image Editing Instructions &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Brooks_InstructPix2Pix_Learning_To_Follow_Image_Editing_Instructions_CVPR_2023_paper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/timothybrooks/instruct-pix2pix&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://instruct-pix2pix.eecs.berkeley.edu/&#34;&gt;[Dataset]&lt;/a&gt; &lt;a href=&#34;https://www.timothybrooks.com/instruct-pix2pix/&#34;&gt;[Project]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/timbrooks/instruct-pix2pix&#34;&gt;[Demo]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Null-text Inversion&lt;/strong&gt;&lt;/em&gt; for Editing Real Images using Guided Diffusion Models &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Mokady_NULL-Text_Inversion_for_Editing_Real_Images_Using_Guided_Diffusion_Models_CVPR_2023_paper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/google/prompt-to-prompt/#null-text-inversion-for-editing-real-images&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;ICCV&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;MasaCtrl:&lt;/strong&gt;&lt;/em&gt; Tuning-Free Mutual Self-Attention Control for Consistent Image Synthesis and Editing &lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2023/papers/Cao_MasaCtrl_Tuning-Free_Mutual_Self-Attention_Control_for_Consistent_Image_Synthesis_and_ICCV_2023_paper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/TencentARC/MasaCtrl&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://ljzycmd.github.io/projects/MasaCtrl/&#34;&gt;[Project]&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1DZeQn2WvRBsNg4feS1bJrwWnIzw1zLJq?usp=sharing&#34;&gt;[Demo]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Localizing Object-level Shape Variations with Text-to-Image Diffusion Models &lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2023/papers/Patashnik_Localizing_Object-Level_Shape_Variations_with_Text-to-Image_Diffusion_Models_ICCV_2023_paper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/orpatashnik/local-prompt-mixing&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://orpatashnik.github.io/local-prompt-mixing/&#34;&gt;[Project]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/orpatashnik/local-prompt-mixing&#34;&gt;[Demo]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;ICLR&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;SDEdit:&lt;/strong&gt;&lt;/em&gt; Guided Image Synthesis and Editing with Stochastic Differential Equations &lt;a href=&#34;https://arxiv.org/pdf/2108.01073.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/ermongroup/SDEdit&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://sde-image-editing.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;span id=&#34;editing-year-2022&#34;&gt;&lt;strong&gt;Year 2022&lt;/strong&gt;&lt;/span&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;CVPR&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;DiffusionCLIP:&lt;/strong&gt;&lt;/em&gt; Text-Guided Diffusion Models for Robust Image Manipulation &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/papers/Kim_DiffusionCLIP_Text-Guided_Diffusion_Models_for_Robust_Image_Manipulation_CVPR_2022_paper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/gwang-kim/DiffusionCLIP&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#contents&#34;&gt;&lt;u&gt;&lt;small&gt;&amp;lt;üéØBack to Top&amp;gt;&lt;/small&gt;&lt;/u&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- omit in toc --&gt; &#xA;&lt;h2&gt;Text Image Generation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;span id=&#34;gentext-year-2024&#34;&gt;&lt;strong&gt;Year 2024&lt;/strong&gt;&lt;/span&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;arXiv&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;AnyText:&lt;/strong&gt;&lt;/em&gt; Multilingual Visual Text Generation And Editing &lt;a href=&#34;https://arxiv.org/pdf/2311.03054.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/tyxsspa/AnyText&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://anytext.pics/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;CVPR&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;‚ö†Ô∏è &lt;em&gt;&lt;strong&gt;SceneTextGen:&lt;/strong&gt;&lt;/em&gt; Layout-Agnostic Scene Text Image Synthesis with Integrated Character-Level Diffusion and Contextual Consistency [Paper]&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#contents&#34;&gt;&lt;u&gt;&lt;small&gt;&amp;lt;üéØBack to Top&amp;gt;&lt;/small&gt;&lt;/u&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- omit in toc --&gt; &#xA;&lt;h1&gt;Datasets&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;strong&gt;Microsoft COCO:&lt;/strong&gt;&lt;/em&gt; Common Objects in Context &lt;a href=&#34;https://arxiv.org/pdf/1405.0312.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://cocodataset.org/#home&#34;&gt;[Dataset]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;strong&gt;Conceptual Captions:&lt;/strong&gt;&lt;/em&gt; A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning &lt;a href=&#34;https://aclanthology.org/P18-1238.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://ai.google.com/research/ConceptualCaptions/&#34;&gt;[Dataset]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;strong&gt;LAION-5B:&lt;/strong&gt;&lt;/em&gt; An Open Large-Scale Dataset for Training Next Generation Image-Text Models &lt;a href=&#34;https://openreview.net/pdf?id=M3Y74vmsMcY&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://laion.ai/&#34;&gt;[Dataset]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#contents&#34;&gt;&lt;u&gt;&lt;small&gt;&amp;lt;üéØBack to Top&amp;gt;&lt;/small&gt;&lt;/u&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- omit in toc --&gt; &#xA;&lt;h1&gt;Toolkits&lt;/h1&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Website&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Stable Diffusion WebUI&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Built based on Gradio, deployed locally to run Stable Diffusion checkpoints, LoRA weights, ControlNet weights, etc.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Stable Diffusion WebUI-forge&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Built based on Gradio, deployed locally to run Stable Diffusion checkpoints, LoRA weights, ControlNet weights, etc.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Fooocus&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/lllyasviel/Fooocus&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Built based on Gradio, offline, open source, and free. &lt;br&gt;The manual tweaking is not needed, and users only need to focus on the prompts and images.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ComfyUI&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/comfyanonymous/ComfyUI&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Deployed locally to enable customized workflows with Stable Diffusion&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Civitai&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://civitai.com/&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Websites for community Stable Diffusion and LoRA checkpoints&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#contents&#34;&gt;&lt;u&gt;&lt;small&gt;&amp;lt;üéØBack to Top&amp;gt;&lt;/small&gt;&lt;/u&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- omit in toc --&gt; &#xA;&lt;h1&gt;Q&amp;amp;A&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Q: The conference sequence of this paper list?&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;This paper list is organized according to the following sequence: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;CVPR&lt;/li&gt; &#xA;     &lt;li&gt;ICCV&lt;/li&gt; &#xA;     &lt;li&gt;ECCV&lt;/li&gt; &#xA;     &lt;li&gt;WACV&lt;/li&gt; &#xA;     &lt;li&gt;NeurIPS&lt;/li&gt; &#xA;     &lt;li&gt;ICLR&lt;/li&gt; &#xA;     &lt;li&gt;ICML&lt;/li&gt; &#xA;     &lt;li&gt;ACM MM&lt;/li&gt; &#xA;     &lt;li&gt;SIGGRAPH&lt;/li&gt; &#xA;     &lt;li&gt;AAAI&lt;/li&gt; &#xA;     &lt;li&gt;arXiv&lt;/li&gt; &#xA;     &lt;li&gt;Others&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Q: What does &lt;code&gt;Others&lt;/code&gt; refers to?&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Some of the following studies (e.g., &lt;code&gt;Stable Casacade&lt;/code&gt;) does not publish their technical report on arXiv. Instead, they tend to write a blog in their official websites. The &lt;code&gt;Others&lt;/code&gt; category refers to such kind of studies.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#contents&#34;&gt;&lt;u&gt;&lt;small&gt;&amp;lt;üéØBack to Top&amp;gt;&lt;/small&gt;&lt;/u&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- omit in toc --&gt; &#xA;&lt;h1&gt;References&lt;/h1&gt; &#xA;&lt;p&gt;The &lt;code&gt;reference.bib&lt;/code&gt; file summarizes bibtex references of up-to-date image inpainting papers, widely used datasets, and toolkits. Based on the original references, I have made the following modifications to make their results look nice in the &lt;code&gt;LaTeX&lt;/code&gt; manuscripts:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Refereces are normally constructed in the form of &lt;code&gt;author-etal-year-nickname&lt;/code&gt;. Particularly, references of datasets and toolkits are directly constructed as &lt;code&gt;nickname&lt;/code&gt;, e.g., &lt;code&gt;imagenet&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;In each reference, all names of conferences/journals are converted into abbreviations, e.g., &lt;code&gt;Computer Vision and Pattern Recognition -&amp;gt; CVPR&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;url&lt;/code&gt;, &lt;code&gt;doi&lt;/code&gt;, &lt;code&gt;publisher&lt;/code&gt;, &lt;code&gt;organization&lt;/code&gt;, &lt;code&gt;editor&lt;/code&gt;, &lt;code&gt;series&lt;/code&gt; in all references are removed.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;pages&lt;/code&gt; of all references are added if they are missing.&lt;/li&gt; &#xA; &lt;li&gt;All paper names are in title case. Besides, I have added an additional &lt;code&gt;{}&lt;/code&gt; to make sure that the title case would also work well in some particular templates.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you have other demands of reference formats, you may refer to the original references of papers by searching their names in &lt;a href=&#34;https://dblp.org/&#34;&gt;DBLP&lt;/a&gt; or &lt;a href=&#34;https://scholar.google.com/&#34;&gt;Google Scholar&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] Note that references in the &lt;code&gt;homepage&lt;/code&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/topics/topics.md&#34;&gt;the &lt;code&gt;topic&lt;/code&gt; section&lt;/a&gt; can be repeated in &lt;code&gt;reference.bib&lt;/code&gt;. Personally, I recommend using &lt;code&gt;&#34;Ctrl+F&#34; / &#34;Command+F&#34;&lt;/code&gt; to search your desired &lt;code&gt;BibTeX&lt;/code&gt; reference.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#contents&#34;&gt;&lt;u&gt;&lt;small&gt;&amp;lt;üéØBack to Top&amp;gt;&lt;/small&gt;&lt;/u&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- omit in toc --&gt; &#xA;&lt;h1&gt;Star History&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://api.star-history.com/svg?repos=AlonzoLeeeooo/awesome-text-to-image-studies&amp;amp;type=Date&#34; target=&#34;_blank&#34;&gt; &lt;img width=&#34;500&#34; src=&#34;https://api.star-history.com/svg?repos=AlonzoLeeeooo/awesome-text-to-image-studies&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt; &lt;/a&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-text-to-image-studies/main/#contents&#34;&gt;&lt;u&gt;&lt;small&gt;&amp;lt;üéØBack to Top&amp;gt;&lt;/small&gt;&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Project-DARC/DARC-whitepaper</title>
    <updated>2024-03-24T01:54:59Z</updated>
    <id>tag:github.com,2024-03-24:/Project-DARC/DARC-whitepaper</id>
    <link href="https://github.com/Project-DARC/DARC-whitepaper" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;The DARC Whitepaper&lt;/h1&gt; &#xA;&lt;h3&gt;Download&lt;/h3&gt; &#xA;&lt;p&gt;English version: &lt;a href=&#34;https://raw.githubusercontent.com/Project-DARC/DARC-whitepaper/release/darc-whitepaper.pdf&#34;&gt;darc-whitepaper.pdf&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Chinese version(‰∏≠ÊñáÁâàÊú¨): &lt;a href=&#34;https://raw.githubusercontent.com/Project-DARC/DARC-whitepaper/release/darc-whitepaper-cn.pdf&#34;&gt;darc-whitepaper-cn.pdf&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;(Thanks for &lt;a href=&#34;https://github.com/yuntianlong2002&#34;&gt;@yuntianlong2002&lt;/a&gt; translation) ÔºàÊÑüË∞¢ &lt;a href=&#34;https://github.com/yuntianlong2002&#34;&gt;@yuntianlong2002&lt;/a&gt; Êèê‰æõÁöÑÁøªËØëÔºâ&lt;/p&gt; &#xA;&lt;h3&gt;Compile&lt;/h3&gt; &#xA;&lt;p&gt;Type below command in the terminal to compile the LaTeX file.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd ./source&#xA;latexmk -pdf main&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>OCEANOFANYTHINGOFFICIAL/Adobe-Photoshop-7.0-With-Serial-key</title>
    <updated>2024-03-24T01:54:59Z</updated>
    <id>tag:github.com,2024-03-24:/OCEANOFANYTHINGOFFICIAL/Adobe-Photoshop-7.0-With-Serial-key</id>
    <link href="https://github.com/OCEANOFANYTHINGOFFICIAL/Adobe-Photoshop-7.0-With-Serial-key" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Adobe Photoshop 7.0 With Serial key&lt;/h1&gt;</summary>
  </entry>
</feed>