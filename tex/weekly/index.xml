<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TeX Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-06-03T02:25:02Z</updated>
  <subtitle>Weekly Trending of TeX in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>latex3/latex3</title>
    <updated>2022-06-03T02:25:02Z</updated>
    <id>tag:github.com,2022-06-03:/latex3/latex3</id>
    <link href="https://github.com/latex3/latex3" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The LaTeX3 Development Repository&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;The &lt;code&gt;expl3&lt;/code&gt; (LaTeX3) Development Repository&lt;/h1&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;The repository contains development material for &lt;code&gt;expl3&lt;/code&gt;. This includes not only code to be developed into the &lt;code&gt;expl3&lt;/code&gt; kernel, but also a variety of test, documentation and more experimental material. All of this code works on top of LaTeX2e.&lt;/p&gt; &#xA;&lt;p&gt;The following directories are present in the repository:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;l3kernel&lt;/code&gt;: code forms the &lt;code&gt;expl3&lt;/code&gt; kernel and with the exception of the contents of &lt;code&gt;l3candidates&lt;/code&gt;, all stable code. With a modern LaTeX2e kernel, this code is loaded during format creation; when using an older LaTeX2e kernel, this material is accessible using the &lt;code&gt;expl3&lt;/code&gt; package.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;l3backend&lt;/code&gt;: code for backend (driver) level interfaces across the &lt;code&gt;expl3&lt;/code&gt; codebase; none of this code has public interfaces, and so no distinction is made between stable and experimental code.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;l3packages&lt;/code&gt;: code which is written to be used on top of LaTeX2e to explore interfaces; these higher-level packages are &#39;stable&#39;. It is unlikely that any new packages will be added to this area.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;l3experimental&lt;/code&gt;: code which is written to be used on top of LaTeX2e to experiment with code and interface concepts. The interfaces for these packages are still under active discussion. Parts of this code may eventually be migrated to &lt;code&gt;l3kernel&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;l3trial&lt;/code&gt;: material which is under very active development, for potential addition to &lt;code&gt;l3kernel&lt;/code&gt; or &lt;code&gt;l3experimental&lt;/code&gt;. Material in this directory may include potential replacements for existing modules, where large-scale changes are under-way. This code is &lt;em&gt;not&lt;/em&gt; released to CTAN.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;l3leftovers&lt;/code&gt;: code which has been developed in the past by The LaTeX Project but is not suitable for use in its current form. Parts of this code may be used as the basis for new developments in &lt;code&gt;l3kernel&lt;/code&gt; or &lt;code&gt;l3experimental&lt;/code&gt; over time.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Support material for development is found in:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;support&lt;/code&gt;, which contains files for the automated test suite which are &#39;local&#39; to the repository.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Documentation is found in:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;articles&lt;/code&gt;: discussion of concepts by team members for publication in &lt;a href=&#34;http://www.tug.org/tugboat&#34;&gt;&lt;em&gt;TUGBoat&lt;/em&gt;&lt;/a&gt; or elsewhere.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The repository also contains the directory &lt;code&gt;xpackages&lt;/code&gt;. This contain code which is being moved (broadly) &lt;code&gt;l3experimental&lt;/code&gt;. Over time, &lt;code&gt;xpackages&lt;/code&gt; is expected to be removed from the repository.&lt;/p&gt; &#xA;&lt;h2&gt;Discussion&lt;/h2&gt; &#xA;&lt;p&gt;Discussion concerning the approach, suggestions for improvements, changes, additions, &lt;em&gt;etc.&lt;/em&gt; should be addressed to the list &lt;a href=&#34;https://listserv.uni-heidelberg.de/cgi-bin/wa?A0=LATEX-L&#34;&gt;LaTeX-L&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can subscribe to this list by sending mail to&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;listserv@urz.uni-heidelberg.de&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;with the body containing&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;subscribe LATEX-L  &amp;lt;Your-First-Name&amp;gt; &amp;lt;Your-Second-Name&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Issues&lt;/h2&gt; &#xA;&lt;p&gt;The issue tracker for &lt;code&gt;expl3&lt;/code&gt; is currently located &lt;a href=&#34;https://github.com/latex3/latex3/issues&#34;&gt;on GitHub&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Please report specific issues with &lt;code&gt;expl3&lt;/code&gt; code there; more general discussion should be directed to the &lt;a href=&#34;https://raw.githubusercontent.com/latex3/latex3/main/#Discussion&#34;&gt;LaTeX-L list&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Build status&lt;/h2&gt; &#xA;&lt;p&gt;We use &lt;a href=&#34;https://github.com/features/actions&#34;&gt;GitHub Actions&lt;/a&gt; as a hosted continuous integration service. For each commit, the build status is tested using the current release of TeX Live.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Current build status:&lt;/em&gt; &lt;img src=&#34;https://github.com/latex3/latex3/actions/workflows/main.yaml/badge.svg?branch=main&#34; alt=&#34;build status&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Development team&lt;/h2&gt; &#xA;&lt;p&gt;This code is developed by &lt;a href=&#34;https://latex-project.org&#34;&gt;The LaTeX Project&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Copyright&lt;/h2&gt; &#xA;&lt;p&gt;This README file is copyright 2021 The LaTeX Project.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mohuangrui/ucasthesis</title>
    <updated>2022-06-03T02:25:02Z</updated>
    <id>tag:github.com,2022-06-03:/mohuangrui/ucasthesis</id>
    <link href="https://github.com/mohuangrui/ucasthesis" rel="alternate"></link>
    <summary type="html">&lt;p&gt;LaTeX Thesis Template for the University of Chinese Academy of Sciences&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;code&gt;ucasthesis&lt;/code&gt; 国科大学位论文 LaTeX 模板 [最新样式]&lt;/h1&gt; &#xA;&lt;h2&gt;模板下载&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;页面右边点击：&lt;strong&gt;Clone or download -&amp;gt; Download Zip&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/wiki/%E5%AD%97%E4%BD%93%E9%85%8D%E7%BD%AE#linuxoverleaf-%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%AD%97%E4%BD%93%E9%85%8D%E7%BD%AE&#34;&gt;Overleaf&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;重要建议&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;关于 ucasthesis 编译和设计的问题，请先读 &lt;strong&gt;模板使用说明.pdf&lt;/strong&gt;，如发问需遵从&lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/wiki/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98&#34;&gt;提问流程&lt;/a&gt;。&lt;/li&gt; &#xA; &lt;li&gt;开题报告见：&lt;a href=&#34;https://github.com/mohuangrui/ucasproposal&#34;&gt;ucasproposal: 中国科学院大学开题报告 LaTeX 模板&lt;/a&gt;。&lt;/li&gt; &#xA; &lt;li&gt;书脊制作见：&lt;a href=&#34;https://github.com/mohuangrui/latexspine&#34;&gt;latexspine: LaTeX 书脊模板&lt;/a&gt;。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt; &lt;img width=&#34;50%&#34; src=&#34;https://github.com/mohuangrui/mohuangrui/raw/main/gallery/ucasthesis.gif&#34; alt=&#34;ucasthesis&#34;&gt; &lt;/h1&gt; &#xA;&lt;h2&gt;模板简介&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;ucasthesis 为撰写中国科学院大学&lt;strong&gt;本&lt;/strong&gt;、&lt;strong&gt;硕&lt;/strong&gt;、&lt;strong&gt;博&lt;/strong&gt;学位论文和&lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/wiki/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98#%E5%A6%82%E4%BD%95%E5%A1%AB%E5%86%99%E5%8D%9A%E5%A3%AB%E5%90%8E%E7%9A%84-frontinfotex-&#34;&gt;&lt;strong&gt;任意高校博后&lt;/strong&gt;&lt;/a&gt;报告的 LaTeX 模版。ucasthesis 提供了简单明了的&lt;strong&gt;模板使用说明.pdf&lt;/strong&gt;。无论你是否具有 LaTeX 使用经验，都可较为轻松地使用以完成学位论文的撰写和排版。谢谢大家的测试、反馈和支持，我们一起的努力让 ucasthesis 非常荣幸地得到了国科大本科部陆晴老师、本科部学位办丁云云老师和中科院数学与系统科学研究院吴凌云研究员的支持，并得到吴凌云学长在 &lt;a href=&#34;http://www.ctex.org/HomePage&#34;&gt;CTEX&lt;/a&gt; 的发布。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;考虑到许多同学可能缺乏 LaTeX 使用经验，ucasthesis 将 LaTeX 的复杂性高度封装，开放出简单的接口，以便轻易使用。同时，对用 LaTeX 撰写论文的一些主要难题，如制图、制表、文献索引等，进行了详细说明，并提供了相应的代码样本，理解了上述问题后，对于初学者而言，使用此模板撰写学位论文将不存在实质性的困难。所以，如果你是初学者，请不要直接放弃，因为同样为初学者的我，十分明白让 LaTeX 简单易用的重要性，而这正是 ucasthesis 所追求和体现的。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;此中国科学院大学学位论文模板 ucasthesis 基于中科院数学与系统科学研究院吴凌云研究员的 CASthesis 模板发展而来。当前 ucasthesis 模板满足最新的中国科学院大学学位论文撰写要求和封面设定。兼顾操作系统：Windows、Linux、MacOS；LaTeX 编译引擎：pdflatex、xelatex、lualatex；文献编译引擎：bibtex、biber (biblatex)；文献样式：著者-出版年制（authoryear）、顺序编码制（numbers）、上标顺序编码制（super）、字符编码制（alpha）。支持中文书签、中文渲染、中文粗体显示、拷贝 PDF 中的文本到其他文本编辑器等特性（&lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/wiki/%E5%AD%97%E4%BD%93%E9%85%8D%E7%BD%AE&#34;&gt;Windows 系统 PDF 拷贝乱码的解决方案需见：字体配置&lt;/a&gt;）。此外，对模板的文档结构进行了精心设计，撰写了编译脚本提高模板的易用性和使用效率。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ucasthesis 的目标在于简化学位论文的撰写，利用 LaTeX 格式与内容分离的特征，模板将格式设计好后，作者可只需关注论文内容。 同时，ucasthesis 有着整洁一致的代码结构和扼要的注解，对文档的仔细阅读可为初学者提供一个学习 LaTeX 的窗口。此外，模板的架构十分注重通用性，事实上，ucasthesis 不仅是国科大学位论文模板，同时，通过少量修改即可成为使用 LaTeX 撰写中英文文章或书籍的通用模板，并为使用者的个性化设定提供了接口。&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;重要通知&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;2021-09-27&lt;/code&gt; 模板样式进行了修改，请查看下面的修改描述，以决定是否需要更新。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;更新记录&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2021-09-27&lt;/code&gt; &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/359&#34;&gt;benkwoook, issue #359&lt;/a&gt;，增强 artratex.sty，提供去掉“引言”类章节的章节编号的功能。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2021-03-30&lt;/code&gt; 更新原创性声明和使用声明页。移除英文封面声明中的 &#34;the&#34;。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2020-07-28&lt;/code&gt; &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/299&#34;&gt;Tony, issue #299&lt;/a&gt;，更新 bibtex 样式。文献样式更多讨论可见：&lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/wiki/%E6%96%87%E7%8C%AE%E6%A0%B7%E5%BC%8F&#34;&gt;文献样式&lt;/a&gt;。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2020-07-22&lt;/code&gt; &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/296&#34;&gt;hushidong, zepinglee, issue #296&lt;/a&gt;，完善 biblatex 和 bibtex 样式。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2020-07-17&lt;/code&gt; &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/296&#34;&gt;GitatHub, hushidong, issue #296&lt;/a&gt;，更新 bibtex 国标样式 &lt;a href=&#34;https://github.com/CTeX-org/gbt7714-bibtex-style&#34;&gt;gbt7714-bibtex-style&lt;/a&gt; ，增加 biblatex 国标样式 &lt;a href=&#34;https://github.com/hushidong/biblatex-gb7714-2015&#34;&gt;biblatex-gb7714-2015&lt;/a&gt;。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2020-05-22&lt;/code&gt; &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/281&#34;&gt;lipcaty, issue #281&lt;/a&gt; 修复 ctex 移除 xeCJKfntef 后对 ulem 的加载。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2020-03-20&lt;/code&gt; &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/250&#34;&gt;zepinglee, issue #250&lt;/a&gt; 增加 LaTeX 和依赖宏包版本检测功能。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2020-02-11&lt;/code&gt; &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/182&#34;&gt;ck2019ML, issue #182&lt;/a&gt;、&lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/229&#34;&gt;univeryinli, issue #229&lt;/a&gt; 将 ucasthesis 在 &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/wiki/%E5%AD%97%E4%BD%93%E9%85%8D%E7%BD%AE#linuxoverleaf-%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%AD%97%E4%BD%93%E9%85%8D%E7%BD%AE&#34;&gt;Overleaf&lt;/a&gt; 发布并支持调用外部字体，详见&lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/wiki/%E5%AD%97%E4%BD%93%E9%85%8D%E7%BD%AE&#34;&gt;字体配置&lt;/a&gt;。&lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/231&#34;&gt;xiaokongkong, issue #231&lt;/a&gt;修正几个书写。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2020-01-09&lt;/code&gt; &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/223&#34;&gt;NineSH, issue #223&lt;/a&gt; 修复&lt;code&gt;bicaption&lt;/code&gt;错误。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2019-12-06&lt;/code&gt; 移除 commit 中的二进制文件，以极大减少 Fork 后的文件大小。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2019-10-12&lt;/code&gt; &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/198&#34;&gt;huiwenzhang, issue #198&lt;/a&gt; 修复&lt;code&gt;mainmatter&lt;/code&gt;下&lt;code&gt;\chapter*&lt;/code&gt;的页眉错误。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2019-10-12&lt;/code&gt; &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/195&#34;&gt;Fancy0609, muzimuzhi, issue #195&lt;/a&gt; 调整由&lt;code&gt;AutoFakeBold&lt;/code&gt;控制的伪粗体加粗程度。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2019-10-11&lt;/code&gt; &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/190&#34;&gt;Pantrick, issue #190&lt;/a&gt; 采用 &lt;a href=&#34;https://github.com/muzimuzhi&#34;&gt;muzimuzhi&lt;/a&gt; 提供的方法实现&lt;code&gt;\advisor{}&lt;/code&gt;和&lt;code&gt;\institute{}&lt;/code&gt;的自动换行功能。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2019-08-01&lt;/code&gt; &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/183&#34;&gt;vectorliu, issue #183&lt;/a&gt; 修改英文模式下的&lt;code&gt;plain&lt;/code&gt;选项为&lt;code&gt;scheme=plain&lt;/code&gt;以消除对&lt;code&gt;Algorithm&lt;/code&gt;样式的修改。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2019-06-15&lt;/code&gt; &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/177&#34;&gt;HaorenWang, issue #177&lt;/a&gt; 调整矢量、矩阵、张量字体样式。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2019-06-09&lt;/code&gt; &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/170&#34;&gt;DRjy, issue #170&lt;/a&gt; 轻微缩减目录中编号与标题的间距；&lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/174&#34;&gt;e71828, issue #174&lt;/a&gt; 轻微增加页眉中编号与标题的间距。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2019-05-25&lt;/code&gt; &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/169&#34;&gt;CDMA2019, issue #169&lt;/a&gt; 提供横排图表环境下页眉页脚的横排，具体使用见 &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/wiki/%E6%A8%AA%E6%8E%92%E5%9B%BE%E8%A1%A8&#34;&gt;横排图表&lt;/a&gt;。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2019-04-24&lt;/code&gt; 拓展模版兼容 &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/wiki/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98#%E5%A6%82%E4%BD%95%E5%A1%AB%E5%86%99%E5%8D%9A%E5%A3%AB%E5%90%8E%E7%9A%84-frontinfotex-&#34;&gt;博后报告&lt;/a&gt;。修复 &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/156&#34;&gt;gsp2014, issue #156&lt;/a&gt; 文献引用中的连字符的间断显示和上标引用中逗号下沉。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2019-04-19&lt;/code&gt; 修复 &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/117&#34;&gt;nihaomiao, issue #117&lt;/a&gt;&lt;code&gt;\mathbf&lt;/code&gt;失效问题。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2019-04-16&lt;/code&gt; 修复国际生需要的&lt;code&gt;plain&lt;/code&gt;模式下无法改变英文章标题字体大小的问题。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2019-04-09&lt;/code&gt; 对部分宏命令进行调整，无功能及样式上的修改。若需更新，建议参考 &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/wiki/%E6%9B%B4%E6%96%B0%E6%8C%87%E5%8D%97&#34;&gt;更新指南&lt;/a&gt;。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2019-04-04&lt;/code&gt; &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/134&#34;&gt;liuy334, songchunlin, issue #134&lt;/a&gt; ，调整行距使&lt;code&gt;LaTeX&lt;/code&gt;版与&lt;code&gt;Word&lt;/code&gt;版的行数和每行字数相一致。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2019-03-28&lt;/code&gt; &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/49&#34;&gt;zssasa, allenwoods, issue #49&lt;/a&gt; ，修复&lt;code&gt;bicaption&lt;/code&gt;对&lt;code&gt;longtable&lt;/code&gt;的兼容性。&lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/133&#34;&gt;BowenHou, issue #133&lt;/a&gt; ，使下划线能对长标题自动换行。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2019-03-25&lt;/code&gt; &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/127&#34;&gt;DRjy, muzimuzhi, issue #127&lt;/a&gt; ，为&lt;code&gt;摘要&lt;/code&gt;等无需在目录中显示的结构元素建立书签。&lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/130&#34;&gt;muzimuzhi, issue #130&lt;/a&gt; ，修正对&lt;code&gt;\voffset&lt;/code&gt;的使用。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2019-03-14&lt;/code&gt; &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/121&#34;&gt;opt-gaobin, issue #121&lt;/a&gt; ，修正中文标点使下划线断掉的问题。&lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/120&#34;&gt;Guoqiang Zhang, email; weili-ict, issue #120&lt;/a&gt; ，修复&lt;code&gt;\proofname&lt;/code&gt;命令对2015年及更早&lt;code&gt;LaTeX&lt;/code&gt;编译器的兼容性问题。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2019-02-20&lt;/code&gt; &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/100&#34;&gt;opt-gaobin, issue #100&lt;/a&gt; ，增加定理、定义、证明等数学环境。&lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/102&#34;&gt;DRjy, issue #102&lt;/a&gt; ，调整&lt;code&gt;\mathcal&lt;/code&gt;字体样式。[zike Liu, email] ，适当缩减目录列表的缩进。&lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/105&#34;&gt;xiaoyaoE, issue #105&lt;/a&gt; ，使数字字体和英文字体一致。完善中文版和国际版之间的中英格式切换。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2019-01-10&lt;/code&gt; &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/57&#34;&gt;mnpengjk, issue #57&lt;/a&gt; ，将公式编号前加点纳入模版默认，更多讨论可见：&lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/wiki/%E7%90%90%E5%B1%91%E7%BB%86%E8%8A%82&#34;&gt;琐屑细节&lt;/a&gt; 。&lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/95&#34;&gt;yunyun2019, issue #95&lt;/a&gt;，更新文献样式。[邵岳林, email] ，将附录复原为常规的排版设置，若需将附录置于参考文献后，请见：&lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/wiki/%E7%90%90%E5%B1%91%E7%BB%86%E8%8A%82&#34;&gt;琐屑细节&lt;/a&gt;。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2018-04-03&lt;/code&gt; 根据国科大本科部陆晴老师和本科部学位办丁云云老师的复审审核建议再次修复一些样式细节问题。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2018-04-02&lt;/code&gt; 模板进行了重大更新，修复了样式、字体、格式等许多问题。&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;根据国科大本科部陆晴老师的建议对模版样式进行了诸多拓展和修正，并完善对本科生论文元素的兼容性。&lt;/li&gt; &#xA;   &lt;li&gt;在 &lt;a href=&#34;https://github.com/CTeX-org/ctex-kit&#34;&gt;ctex&lt;/a&gt; 开发者的帮助下解决了如何多次调用&lt;code&gt;Times New Roman&lt;/code&gt;而不导致黑体调用错误的问题。[twn1993, email]，修复默认黑体为微软雅黑而不是&lt;code&gt;SimHei&lt;/code&gt;的问题。&lt;/li&gt; &#xA;   &lt;li&gt;繁复折腾测试后终于找出一个在&lt;code&gt;ctex&lt;/code&gt;默认黑体替换粗宋体设定环境内全局&lt;code&gt;AutoFakeBold&lt;/code&gt;失效状态下折衷特定字体库不全条件下生僻字显示和系统默认字重不全条件下粗宋体显示以及不同操作系统下如何平衡上述字库自重矛盾还有根据操作系统自动调用所带有的&lt;code&gt;Times&lt;/code&gt;字体的方案。&lt;/li&gt; &#xA;   &lt;li&gt;设定论文封面据英文学位名如自动切换。密级据是否填写自动显示。&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2018-03-22&lt;/code&gt; 演示表标题居表上，加粗图表标注，设置长图表标题悬挂缩进（由于&lt;code&gt;bicaption&lt;/code&gt;宏包无法正确接受&lt;code&gt;caption&lt;/code&gt;宏包的&lt;code&gt;margin&lt;/code&gt;选项，图表中英标题第一行无法正确同步缩进，从而放弃第一行的缩进），强调多图中子图标题的规范使用，通过摘要和符号列表演示标题不在目录中显示却仍在页眉中显示。[赵永明, email]，设置双语图表标题和&lt;code&gt;bicaption&lt;/code&gt;不在图形列表和表格列表中显示英文标题。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2018-03-21&lt;/code&gt; &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/42&#34;&gt;zhanglinbo, issue #42&lt;/a&gt; ，使用 &lt;a href=&#34;https://github.com/xiaoyao9933/UCASthesis&#34;&gt;xiaoyao9933&lt;/a&gt; 制作的&lt;code&gt;ucas_logo.pdf&lt;/code&gt;使学校&lt;code&gt;logo&lt;/code&gt;放大不失真。&lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/41&#34;&gt;Starsky Wong, issue #41&lt;/a&gt; ，设置标题英文设为&lt;code&gt;Times New Roman&lt;/code&gt;。&lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/29&#34;&gt;will0n, issue #29&lt;/a&gt; ，&lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/26&#34;&gt;Man-Ting-Fang, issue #26&lt;/a&gt; ，&lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/12&#34;&gt;diyiliaoya, issue #12&lt;/a&gt; ，和 [赵永明, email] ，矫正一些格式细节问题。&lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/30&#34;&gt;tangjie1992, issue #30&lt;/a&gt; ，配置算法环境。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2018-02-04&lt;/code&gt; 在 &lt;a href=&#34;https://github.com/CTeX-org/ctex-kit&#34;&gt;ctex&lt;/a&gt; 开发者的帮助下修复误用字体命令导致的粗宋体异常。然后，将模板兼容性进一步扩展为兼容操作系统&lt;code&gt;Windows&lt;/code&gt;，&lt;code&gt;Linux&lt;/code&gt;，&lt;code&gt;MacOS&lt;/code&gt;和&lt;code&gt;LaTeX &lt;/code&gt;编译引擎&lt;code&gt;pdflatex&lt;/code&gt;，&lt;code&gt;xelatex&lt;/code&gt;，&lt;code&gt;lualatex&lt;/code&gt;。移除&lt;code&gt;microtype&lt;/code&gt;宏包以提高编译效率。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2018-01-28&lt;/code&gt; 基于国科大&lt;code&gt;2018&lt;/code&gt;新版论文规范进行了重大修改，采用新的封面、声明、页眉页脚样式。展示标题中使用数学公式。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2017-05-14&lt;/code&gt; [赵永明, email] ，增加&lt;code&gt;\citepns{}&lt;/code&gt;和&lt;code&gt;\citetns{}&lt;/code&gt;命令提供上标引用下混合非上标引用的需求。[臧光明, email] ，添加设定论文为&lt;code&gt;thesis&lt;/code&gt;或&lt;code&gt;dissertation&lt;/code&gt;的命令。&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>weihaox/awesome-gan-inversion</title>
    <updated>2022-06-03T02:25:02Z</updated>
    <id>tag:github.com,2022-06-03:/weihaox/awesome-gan-inversion</id>
    <link href="https://github.com/weihaox/awesome-gan-inversion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A collection of resources on GAN inversion.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;p align=&#34;center&#34;&gt;&lt;code&gt;awesome gan-inversion&lt;/code&gt;&lt;/p&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/sindresorhus/awesome&#34;&gt;&lt;img src=&#34;https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg?sanitize=true&#34; alt=&#34;Awesome&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/Naereen/StrapDown.js/graphs/commit-activity&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Maintained%3F-yes-green.svg?sanitize=true&#34; alt=&#34;Maintenance&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://makeapullrequest.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat&#34; alt=&#34;PR&#39;s Welcome&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repo is a collection of resources on GAN inversion, as a supplement for our &lt;a href=&#34;https://arxiv.org/abs/2101.05278&#34;&gt;survey&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;survey&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;GAN Inversion: A Survey.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Weihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei Zhou, Ming-Hsuan Yang.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2101.05278&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;citation&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;@article{xia2021survey,&#xA;    author  = {Xia, Weihao and Zhang, Yulun and Yang, Yujiu and Xue, Jing-Hao and Zhou, Bolei and Yang, Ming-Hsuan},&#xA;    title   = {GAN Inversion: A Survey},&#xA;    journal = {arXiv preprint arXiv: 2101.05278},&#xA;    year={2021}&#xA;  }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;inverted pretrained model&lt;/h2&gt; &#xA;&lt;h3&gt;2D GANs&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleGAN-XL: Scaling StyleGAN to Large Diverse Datasets.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://axelsauer.com/&#34;&gt;Axel Sauer&lt;/a&gt;, &lt;a href=&#34;https://katjaschwarz.github.io/&#34;&gt;Katja Schwarz&lt;/a&gt;, &lt;a href=&#34;http://www.cvlibs.net/&#34;&gt;Andreas Geiger&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; SIGGRAPH 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.00273&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://sites.google.com/view/stylegan-xl/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/autonomousvision/stylegan_xl&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Self-Distilled StyleGAN: Towards Generation from Internet Photos.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://rmokady.github.io/&#34;&gt;Ron Mokady&lt;/a&gt;, Michal Yarom, Omer Tov, Oran Lang, Daniel Cohen-Or, Tali Dekel, Michal Irani, Inbar Mosseri.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.12211&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://self-distilled-stylegan.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/self-distilled-stylegan/self-distilled-internet-photos&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Ensembling Off-the-shelf Models for GAN Training.&lt;/strong&gt;&lt;br&gt; &lt;a href=&#34;https://nupurkmr9.github.io/&#34;&gt;Nupur Kumari&lt;/a&gt;, &lt;a href=&#34;https://richzhang.github.io/&#34;&gt;Richard Zhang&lt;/a&gt;, &lt;a href=&#34;https://research.adobe.com/person/eli-shechtman/&#34;&gt;Eli Shechtman&lt;/a&gt;, &lt;a href=&#34;https://www.cs.cmu.edu/~junyanz/&#34;&gt;Jun-Yan Zhu&lt;/a&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/pdf/2112.09130.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://www.cs.cmu.edu/~vision-aided-gan/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/nupurkmr9/vision-aided-gan&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleGAN3: Alias-Free Generative Adversarial Networks.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tero Karras, Miika Aittala, Samuli Laine, Erik Härkönen, Janne Hellsten, Jaakko Lehtinen, Timo Aila.&lt;/em&gt;&lt;br&gt; NeurIPS 2021. [&lt;a href=&#34;https://arxiv.org/abs/2106.12423&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://nvlabs.github.io/alias-free-gan&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/NVlabs/stylegan3&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://github.com/rosinality/alias-free-gan-pytorch&#34;&gt;Rosinality&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleGAN2-Ada: Training Generative Adversarial Networks with Limited Data.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, Timo Aila.&lt;/em&gt;&lt;br&gt; NeurIPS 2020. [&lt;a href=&#34;https://arxiv.org/abs/2006.06676&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/NVlabs/stylegan2-ada&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://github.com/woctezuma/steam-stylegan2-ada&#34;&gt;Steam StyleGAN2-ADA&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleGAN2: Analyzing and Improving the Image Quality of StyleGAN.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://research.nvidia.com/person/tero-karras&#34;&gt;Tero Karras&lt;/a&gt;, &lt;a href=&#34;https://research.nvidia.com/person/samuli-laine&#34;&gt;Samuli Laine&lt;/a&gt;, &lt;a href=&#34;https://research.nvidia.com/person/miika-aittala&#34;&gt;Miika Aittala&lt;/a&gt;, Janne Hellsten, Jaakko Lehtinen, &lt;a href=&#34;https://research.nvidia.com/person/timo-aila&#34;&gt;Timo Aila&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2020. [&lt;a href=&#34;https://arxiv.org/abs/1912.04958&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/rosinality/stylegan2-pytorch&#34;&gt;PyTorch&lt;/a&gt;] [&lt;a href=&#34;https://github.com/NVlabs/stylegan2&#34;&gt;Offical TF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/manicman1999/StyleGAN2-Tensorflow-2.0&#34;&gt;Unoffical Tensorflow 2.0&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleGAN: A Style-Based Generator Architecture for Generative Adversarial Networks.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tero Karras, Samuli Laine, Timo Aila.&lt;/em&gt;&lt;br&gt; CVPR 2019. [&lt;a href=&#34;https://arxiv.org/abs/1812.04948&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/NVlabs/stylegan&#34;&gt;Offical TF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ProGAN: Progressive Growing of GANs for Improved Quality, Stability, and Variation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen.&lt;/em&gt;&lt;br&gt; ICLR 2018. [&lt;a href=&#34;https://arxiv.org/abs/1710.10196&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/tkarras/progressive_growing_of_gans&#34;&gt;Offical TF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h3&gt;3D-aware GANs&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;EG3D: Efficient Geometry-aware 3D Generative Adversarial Networks.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://ericryanchan.github.io/&#34;&gt;Eric R. Chan&lt;/a&gt;, &lt;a href=&#34;https://connorzlin.com/&#34;&gt;Connor Z. Lin&lt;/a&gt;, &lt;a href=&#34;https://matthew-a-chan.github.io/&#34;&gt;Matthew A. Chan&lt;/a&gt;, &lt;a href=&#34;https://luminohope.org/&#34;&gt;Koki Nagano&lt;/a&gt;, &lt;a href=&#34;https://cs.stanford.edu/~bxpan/&#34;&gt;Boxiao Pan&lt;/a&gt;, &lt;a href=&#34;https://research.nvidia.com/person/shalini-gupta&#34;&gt;Shalini De Mello&lt;/a&gt;, &lt;a href=&#34;https://oraziogallo.github.io/&#34;&gt;Orazio Gallo&lt;/a&gt;, &lt;a href=&#34;https://geometry.stanford.edu/member/guibas/&#34;&gt;Leonidas Guibas&lt;/a&gt;, &lt;a href=&#34;https://research.nvidia.com/person/jonathan-tremblay&#34;&gt;Jonathan Tremblay&lt;/a&gt;, &lt;a href=&#34;https://www.samehkhamis.com/&#34;&gt;Sameh Khamis&lt;/a&gt;, &lt;a href=&#34;https://research.nvidia.com/person/tero-karras&#34;&gt;Tero Karras&lt;/a&gt;, &lt;a href=&#34;https://stanford.edu/~gordonwz/&#34;&gt;Gordon Wetzstein&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2112.07945&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://matthew-a-chan.github.io/EG3D&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleNeRF: A Style-based 3D-Aware Generator for High-resolution Image Synthesis.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;http://jiataogu.me/&#34;&gt;Jiatao Gu&lt;/a&gt;, &lt;a href=&#34;https://lingjie0206.github.io/&#34;&gt;Lingjie Liu&lt;/a&gt;, &lt;a href=&#34;https://totoro97.github.io/about.html&#34;&gt;Peng Wang&lt;/a&gt;, &lt;a href=&#34;http://people.mpi-inf.mpg.de/~theobalt/&#34;&gt;Christian Theobalt&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2110.08985&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;http://jiataogu.me/style_nerf/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleSDF: High-Resolution 3D-Consistent Image and Geometry Generation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://homes.cs.washington.edu/~royorel/&#34;&gt;Roy Or-El&lt;/a&gt;, &lt;a href=&#34;https://roxanneluo.github.io/&#34;&gt;Xuan Luo&lt;/a&gt;, Mengyi Shan, Eli Shechtman, Jeong Joon Park, Ira Kemelmacher-Shlizerman.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2112.11427&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://stylesdf.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/royorel/StyleSDF&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://ericryanchan.github.io/&#34;&gt;Eric R. Chan&lt;/a&gt;, &lt;a href=&#34;https://marcoamonteiro.github.io/pi-GAN-website/&#34;&gt;Marco Monteiro&lt;/a&gt;, &lt;a href=&#34;https://kellnhofer.xyz/&#34;&gt;Petr Kellnhofer&lt;/a&gt;, &lt;a href=&#34;https://jiajunwu.com/&#34;&gt;Jiajun Wu&lt;/a&gt;, &lt;a href=&#34;https://stanford.edu/~gordonwz/&#34;&gt;Gordon Wetzstein&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2012.00926&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://marcoamonteiro.github.io/pi-GAN-website/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/lucidrains/pi-GAN-pytorch&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h2&gt;inversion method&lt;/h2&gt; &#xA;&lt;p&gt;This part contatins generatal inversion methods, while methods in the next &lt;em&gt;application&lt;/em&gt; part are mainly designed for specific tasks.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Style Transformer for Image Inversion and Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Xueqi Hu, Qiusheng Huang, Zhengyi Shi, Siyuan Li, Changxin Gao, Li Sun, Qingli Li.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.07932&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/sapphire497/style-transformer&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High-Fidelity GAN Inversion for Image Attribute Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://tengfei-wang.github.io&#34;&gt;Tengfei Wang&lt;/a&gt;, Yong Zhang, Yanbo Fan, Jue Wang, Qifeng Chen.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2109.06590&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://tengfei-wang.github.io/HFGI/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Tengfei-Wang/HFGI&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HyperInverter: Improving StyleGAN Inversion via Hypernetwork.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://di-mi-ta.github.io/&#34;&gt;Tan M. Dinh&lt;/a&gt;, &lt;a href=&#34;https://sites.google.com/site/anhttranusc/&#34;&gt;Anh Tuan Tran&lt;/a&gt;, &lt;a href=&#34;https://sites.google.com/site/rangmanhonguyen/&#34;&gt;Rang Nguyen&lt;/a&gt;, &lt;a href=&#34;https://sonhua.github.io/&#34;&gt;Binh-Son Hua&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2112.00719&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://di-mi-ta.github.io/HyperInverter/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HyperStyle: StyleGAN Inversion with HyperNetworks for Real Image Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuval Alaluf, Omer Tov, Ron Mokady, Rinon Gal, Amit H. Bermano.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2111.15666&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;http://yuval-alaluf.github.io/hyperstyle/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yuval-alaluf/hyperstyle&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Overparameterization Improves StyleGAN Inversion.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yohan Poirier-Ginter, Alexandre Lessard, Ryan Smith, Jean-François Lalonde.&lt;/em&gt;&lt;br&gt; CVPR 2022 Workshop on AI for Content Creation. [&lt;a href=&#34;https://arxiv.org/abs/2205.06304&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://lvsn.github.io/OverparamStyleGAN/&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleAlign: Analysis and Applications of Aligned StyleGAN Models.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Zongze Wu, Yotam Nitzan, Eli Shechtman, Dani Lischinski.&lt;/em&gt;&lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2110.11323&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Expanding the Latent Space of StyleGAN for Real Face Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yin Yu, Ghasedi Kamran, Wu HsiangTao, Yang Jiaolong, Tong Xi, Fu Yun.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2204.12530&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Encode-in-Style: Latent-based Video Encoding using StyleGAN2.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://trevineoorloff.github.io/&#34;&gt;Trevine Oorloff&lt;/a&gt;, &lt;a href=&#34;https://www.umiacs.umd.edu/people/yaser&#34;&gt;Yaser Yacoob&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.14512&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://trevineoorloff.github.io/Encode-in-Style.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/trevineoorloff/Encode-in-Style&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High-fidelity GAN Inversion with Padding Space.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://ezioby.github.io/padinv/&#34;&gt;Qingyan Bai&lt;/a&gt;, Yinghao Xu, Jiapeng Zhu, Weihao Xia, Yujiu Yang, Yujun Shen.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.11105&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/EzioBy/padinv&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/EzioBy/padinv&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Solving Inverse Problems with NerfGANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Giannis Daras, Wen-Sheng Chu, Abhishek Kumar, Dmitry Lagun, Alexandros G. Dimakis.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2112.09061&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Third Time&#39;s the Charm? Image and Video Editing with StyleGAN3.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuval Alaluf, Or Patashnik, Zongze Wu, Asif Zamir, Eli Shechtman, Dani Lischinski, Daniel Cohen-Or.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2201.13433&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://yuval-alaluf.github.io/stylegan3-editing/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yuval-alaluf/stylegan3-editing&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Feature-Style Encoder for Style-Based GAN Inversion.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Xu Yao, Alasdair Newson, Yann Gousseau, Pierre Hellier.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.02183&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleGAN of All Trades: Image Manipulation with Only Pretrained StyleGAN.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Min Jin Chong, Hsin-Ying Lee, David Forsyth.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2111.01619&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mchong6/SOAT&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Real Image Inversion via Segments.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;David Futschik, Michal Lukáč, Eli Shechtman, Daniel Sýkora.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2110.06269&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Semantic and Geometric Unfolding of StyleGAN Latent Space.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Mustafa Shukor, Xu Yao, Bharath Bhushan Damodaran, Pierre Hellier.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2107.04481&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pivotal Tuning for Latent-based Editing of Real Images.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Daniel Roich, Ron Mokady, Amit H. Bermano, Daniel Cohen-Or.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/pdf/2106.05744.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/danielroich/PTI&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Transforming the Latent Space of StyleGAN for Real Face Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Heyi Li, Jinlong Liu, Yunzhi Bai, Huayan Wang, Klaus Mueller.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2105.14230&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/AnonSubm2021/TransStyleGAN&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;E2Style: Improve the Efficiency and Effectiveness of StyleGAN Inversion.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tianyi Wei, Dongdong Chen, Wenbo Zhou, Jing Liao, Weiming Zhang, Lu Yuan, Gang Hua, Nenghai Yu.&lt;/em&gt;&lt;br&gt; TIP 2022. [&lt;a href=&#34;https://arxiv.org/abs/2104.07661&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://wty-ustc.github.io/inversion/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/wty-ustc/StyleGAN-Inversion-Baseline&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LatentCLR: A Contrastive Learning Approach for Unsupervised Discovery of Interpretable Directions.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Oğuz Kaan Yüksel, &lt;a href=&#34;https://enis.dev&#34;&gt;Enis Simsar&lt;/a&gt;, Ezgi Gülperi Er, Pinar Yanardag.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://arxiv.org/abs/2104.00820&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/catlab-team/latentclr&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GAN-Control: Explicitly Controllable GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Alon Shoshan, Nadav Bhonker, Igor Kviatkovsky, Gerard Medioni.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2101.02477&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improved StyleGAN Embedding: Where are the Good Latents?&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://github.com/ZPdesu&#34;&gt;Peihao Zhu&lt;/a&gt;, &lt;a href=&#34;https://github.com/RameenAbdal&#34;&gt;Rameen Abdal&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=ojgWPpgAAAAJ&amp;amp;hl=en&#34;&gt;Yipeng Qin&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=rS1xJIIAAAAJ&amp;amp;hl=en&#34;&gt;John Femiani&lt;/a&gt;, &lt;a href=&#34;http://peterwonka.net/&#34;&gt;Peter Wonka&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; arxiv 2020. [&lt;a href=&#34;https://arxiv.org/abs/2012.09036&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ZPdesu/II2S&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning a Deep Reinforcement Learning Policy Over the Latent Space of a Pre-trained GAN for Semantic Age Manipulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Kumar Shubham, Gopalakrishnan Venkatesh, Reijul Sachdev, Akshi, Dinesh Babu Jayagopi, G. Srinivasaraghavan.&lt;/em&gt;&lt;br&gt; arxiv 2020. [&lt;a href=&#34;https://arxiv.org/abs/2011.00954&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Lifting 2D StyleGAN for 3D-Aware Face Generation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://seasonsh.github.io/&#34;&gt;Yichun Shi&lt;/a&gt;, Divyansh Aggarwal, &lt;a href=&#34;http://www.cse.msu.edu/~jain/&#34;&gt;Anil K. Jain&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; arxiv 2020. [&lt;a href=&#34;https://arxiv.org/abs/2011.13126&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Augmentation-Interpolative AutoEncoders for Unsupervised Few-Shot Image Generation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Davis Wertheimer, Omid Poursaeed, Bharath Hariharan.&lt;/em&gt;&lt;br&gt; arxiv 2020. [&lt;a href=&#34;https://arxiv.org/abs/2011.13026&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised Discovery of Disentangled Manifolds in GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yu-Ding Lu, Hsin-Ying Lee, Hung-Yu Tseng, Ming-Hsuan Yang.&lt;/em&gt;&lt;br&gt; arxiv 2020. &lt;a href=&#34;https://arxiv.org/abs/2011.11842&#34;&gt;[PDF]&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Collaborative Learning for Faster StyleGAN Embedding.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Shanyan Guan, &lt;a href=&#34;https://tyshiwo.github.io/&#34;&gt;Ying Tai&lt;/a&gt;, Bingbing Ni, Feida Zhu, Feiyue Huang, Xiaokang Yang.&lt;/em&gt;&lt;br&gt; arxiv 2020. [&lt;a href=&#34;https://arxiv.org/abs/2007.01758&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improving Inversion and Generation Diversity in StyleGAN using a Gaussianized Latent Space.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;http://people.csail.mit.edu/jwulff/&#34;&gt;Jonas Wulff&lt;/a&gt;, Antonio Torralba.&lt;/em&gt;&lt;br&gt; arxiv 2020. [&lt;a href=&#34;https://arxiv.org/abs/2009.06529&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Explaining in Style: Training a GAN to explain a classifier in StyleSpace.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Oran Lang, Yossi Gandelsman, Michal Yarom, Yoav Wald, Gal Elidan, Avinatan Hassidim, William T. Freeman, Phillip Isola, Amir Globerson, Michal Irani, Inbar Mosseri.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://arxiv.org/abs/2104.13369&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://explaining-in-style.github.io/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;BDInvert: GAN Inversion for Out-of-Range Images with Geometric Transformations.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://kkang831.github.io/&#34;&gt;Kyoungkook Kang&lt;/a&gt;, Seongtae Kim, &lt;a href=&#34;https://www.scho.pe.kr/&#34;&gt;Sunghyun Cho&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://arxiv.org/abs/2108.08998&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://kkang831.github.io/publication/ICCV_2021_BDInvert/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;From Continuity to Editability: Inverting GANs with Consecutive Images.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://qingyang-xu.github.io/&#34;&gt;Yangyang Xu&lt;/a&gt;, &lt;a href=&#34;https://www.csyongdu.com/&#34;&gt;Yong Du&lt;/a&gt;, Wenpeng Xiao, Xuemiao Xu and &lt;a href=&#34;https://raw.githubusercontent.com/weihaox/awesome-gan-inversion/main/hengfenghe.com&#34;&gt;Shengfeng He&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://arxiv.org/abs/2107.13812&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Qingyang-Xu/InvertingGANs_with_ConsecutiveImgs&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ReStyle: A Residual-Based StyleGAN Encoder via Iterative Refinement.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://yuval-alaluf.github.io/&#34;&gt;Yuval Alaluf&lt;/a&gt;, &lt;a href=&#34;https://orpatashnik.github.io/&#34;&gt;Or Patashnik&lt;/a&gt;, &lt;a href=&#34;https://www.cs.tau.ac.il/~dcor/&#34;&gt;Daniel Cohen-Or&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://arxiv.org/abs/2104.02699&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://yuval-alaluf.github.io/restyle-encoder/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yuval-alaluf/restyle-encoder&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Disentangled Face Attribute Editing via Instance-Aware Latent Space Search.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuxuan Han, &lt;a href=&#34;http://jlyang.org/&#34;&gt;Jiaolong Yang&lt;/a&gt;, Ying Fu.&lt;/em&gt;&lt;br&gt; IJCAI 2021. [&lt;a href=&#34;https://arxiv.org/abs/2105.12660&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yxuhan/IALS&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prior Image-Constrained Reconstruction using Style-Based Generative Models.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Varun A Kelkar, Mark Anastasio.&lt;/em&gt;&lt;br&gt; ICML 2021. [&lt;a href=&#34;https://arxiv.org/pdf/2102.12525.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Intermediate Layer Optimization for Inverse Problems using Deep Generative Models.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Giannis Daras, Joseph Dean, Ajil Jalal, Alexandros G. Dimakis.&lt;/em&gt;&lt;br&gt; ICML 2021. [&lt;a href=&#34;https://arxiv.org/abs/2102.07364&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/giannisdaras/ilo&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Using Latent Space Regression to Analyze and Leverage Compositionality in GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;http://people.csail.mit.edu/lrchai/&#34;&gt;Lucy Chai&lt;/a&gt;, &lt;a href=&#34;http://people.csail.mit.edu/jwulff/&#34;&gt;Jonas Wulff&lt;/a&gt;, &lt;a href=&#34;http://web.mit.edu/phillipi/&#34;&gt;Phillip Isola&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; ICLR 2021. [&lt;a href=&#34;https://openreview.net/pdf?id=sjuuTm4vj0&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/chail/latent-composition&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://chail.github.io/latent-composition/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://colab.research.google.com/drive/1p-L2dPMaqMyr56TYoYmBJhoyIyBJ7lzH?usp=sharing&#34;&gt;Colab&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Enjoy Your Editing: Controllable GANs for Image Editing via Latent Space Navigation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Peiye Zhuang, Oluwasanmi Koyejo, Alexander G. Schwing.&lt;/em&gt;&lt;br&gt; ICLR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2102.01187&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High Fidelity GAN Inversion via Prior Multi-Subspace Feature Composition.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Guanyue Li, Qianfen Jiao, Sheng Qian, Si Wu, Hau-San Wong.&lt;/em&gt;&lt;br&gt; AAAI 2021. [&lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/view/17017&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Ensembling with Deep Generative Views.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Lucy Chai, Jun-Yan Zhu, Eli Shechtman, Phillip Isola, Richard Zhang.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2104.14551&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/chail/gan-ensembling&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://chail.github.io/gan-ensembling/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Navigating the GAN Parameter Space for Semantic Image Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Anton Cherepkov, Andrey Voynov, Artem Babenko.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2011.13786&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yandex-research/navigan&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleSpace Analysis: Disentangled Controls for StyleGAN Image Generation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Zongze Wu, Dani Lischinski, Eli Shechtman.&lt;/em&gt;&lt;br&gt; CVPR 2021 (oral). &lt;a href=&#34;https://arxiv.org/abs/2011.12799&#34;&gt;[PDF]&lt;/a&gt; [&lt;a href=&#34;https://github.com/betterze/StyleSpace&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, Daniel Cohen-Or.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2008.00951&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/eladrich/pixel2style2pixel&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/weihaox/awesome-gan-inversion/main/eladrich.github.io/pixel2style2pixel/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GHFeat: Generative Hierarchical Features from Synthesizing Images.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yinghao Xu, Yujun Shen, Jiapeng Zhu, Ceyuan Yang, Bolei Zhou.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/pdf/2007.10379.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/genforce/ghfeat&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://genforce.github.io/ghfeat/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Hijack-GAN: Unintended-Use of Pretrained, Black-Box GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Hui-Po Wang, Ning Yu, Mario Fritz.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2011.14107&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;e4e: Designing an Encoder for StyleGAN Image Manipulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://yotamnitzan.github.io/&#34;&gt;Omer Tov&lt;/a&gt;, Yuval Alaluf, Yotam Nitzan, Or Patashnik, Daniel Cohen-Or.&lt;/em&gt;&lt;br&gt; TOG 2021. [&lt;a href=&#34;https://arxiv.org/abs/2102.02766&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/omertov/encoder4editing&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleFlow: Attribute-conditioned Exploration of StyleGAN-Generated Images using Conditional Continuous Normalizing Flows.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Rameen Abdal, Peihao Zhu, Niloy Mitra, Peter Wonka.&lt;/em&gt;&lt;br&gt; TOG 2021. [&lt;a href=&#34;https://arxiv.org/abs/2008.02401&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://rameenabdal.github.io/StyleFlow&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Mask-Guided Discovery of Semantic Manifolds in Generative Models.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://mengyu.page/&#34;&gt;Mengyu Yang&lt;/a&gt;, &lt;a href=&#34;https://www.cdtps.utoronto.ca/people/directories/all-faculty/david-rokeby&#34;&gt;David Rokeby&lt;/a&gt;, &lt;a href=&#34;https://wxs.ca/&#34;&gt;Xavier Snelgrove&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; NeurIPS 2020 Workshop on Machine Learning for Creativity and Design. [&lt;a href=&#34;https://github.com/bmolab/masked-gan-manifold/raw/main/masked-gan-manifold.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/bmolab/masked-gan-manifold&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Face Identity Disentanglement via Latent Space Mapping.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://yotamnitzan.github.io/&#34;&gt;Yotam Nitzan&lt;/a&gt;, &lt;a href=&#34;https://www.cs.tau.ac.il/~amberman/&#34;&gt;Amit Bermano&lt;/a&gt;, &lt;a href=&#34;https://yangyan.li/&#34;&gt;Yangyan Li&lt;/a&gt;, Daniel Cohen-Or.&lt;/em&gt;&lt;br&gt; SIGGRAPH ASIA 2020. [&lt;a href=&#34;https://arxiv.org/abs/2005.07728&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://yotamnitzan.github.io/ID-disentanglement/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/YotamNitzan/ID-disentanglement&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PIE: Portrait Image Embedding for Semantic Control.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;http://people.mpi-inf.mpg.de/~atewari/&#34;&gt;A. Tewari&lt;/a&gt;, M. Elgharib, M. BR, F. Bernard, H-P. Seidel, P. P‌érez, M. Zollhöfer, C.Theobalt.&lt;/em&gt;&lt;br&gt; TOG 2020. [&lt;a href=&#34;http://gvv.mpi-inf.mpg.de/projects/PIE/data/paper.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;http://gvv.mpi-inf.mpg.de/projects/PIE/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Understanding the Role of Individual Units in a Deep Neural Network.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;David Bau, Jun-Yan Zhu, Hendrik Strobelt, Agata Lapedriza, Bolei Zhou, Antonio Torralba.&lt;/em&gt;&lt;br&gt; National Academy of Sciences 2020. [&lt;a href=&#34;https://arxiv.org/abs/2009.05041&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/davidbau/dissect/&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://dissect.csail.mit.edu/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Face Identity Disentanglement via Latent Space Mapping.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yotam Nitzan, Amit Bermano, Yangyan Li, Daniel Cohen-Or.&lt;/em&gt;&lt;br&gt; TOG 2020. [&lt;a href=&#34;https://arxiv.org/abs/2005.07728&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/YotamNitzan/ID-disentanglement&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Transforming and Projecting Images into Class-conditional Generative Networks.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;http://minyounghuh.com/&#34;&gt;Minyoung Huh&lt;/a&gt;, &lt;a href=&#34;https://richzhang.github.io/&#34;&gt;Richard Zhang&lt;/a&gt;, &lt;a href=&#34;https://people.csail.mit.edu/junyanz/&#34;&gt;Jun-Yan Zhu&lt;/a&gt;, &lt;a href=&#34;http://people.csail.mit.edu/sparis/&#34;&gt;Sylvain Paris&lt;/a&gt;, &lt;a href=&#34;https://www.dgp.toronto.edu/~hertzman/&#34;&gt;Aaron Hertzmann&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; ECCV 2020. [&lt;a href=&#34;http://arxiv.org/abs/2005.01703&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/minyoungg/GAN-Transform-and-Project&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://minyoungg.github.io/GAN-Transform-and-Project/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MimicGAN: Robust Projection onto Image Manifolds with Corruption Mimicking.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://rushila.com/&#34;&gt;Rushil Anirudh&lt;/a&gt;, &lt;a href=&#34;https://jjthiagarajan.com/&#34;&gt;Jayaraman J. Thiagarajan&lt;/a&gt;, &lt;a href=&#34;https://people.llnl.gov/kailkhura1&#34;&gt;Bhavya Kailkhura&lt;/a&gt;, &lt;a href=&#34;https://people.llnl.gov/bremer5&#34;&gt;Timo Bremer&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; IJCV 2020. [&lt;a href=&#34;https://link.springer.com/article/10.1007/s11263-020-01310-5&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Rewriting a Deep Generative Model.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;David Bau, Steven Liu, Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba.&lt;/em&gt;&lt;br&gt; ECCV 2020. [&lt;a href=&#34;https://arxiv.org/abs/2007.15646&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/davidbau/rewriting&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleGAN2 Distillation for Feed-forward Image Manipulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuri Viazovetskyi, Vladimir Ivashkin, Evgeny Kashin.&lt;/em&gt;&lt;br&gt; ECCV 2020. [&lt;a href=&#34;https://arxiv.org/abs/2003.03581&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/EvgenyKashin/stylegan2-distillation&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;In-Domain GAN Inversion for Real Image Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jiapeng Zhu, Yujun Shen, Deli Zhao, Bolei Zhou.&lt;/em&gt;&lt;br&gt; ECCV 2020. [&lt;a href=&#34;https://arxiv.org/abs/2004.00049&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://genforce.github.io/idinvert/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/genforce/idinvert&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Exploiting Deep Generative Prior for Versatile Image Restoration and Manipulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, Ping Luo.&lt;/em&gt;&lt;br&gt; ECCV 2020. [&lt;a href=&#34;https://arxiv.org/abs/2003.13659&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/XingangPan/deep-generative-prior&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Your Local GAN: Designing Two Dimensional Local Attention Mechanisms for Generative Models.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Giannis Daras, Augustus Odena, Han Zhang, Alexandros G. Dimakis.&lt;/em&gt;&lt;br&gt; CVPR 2020. [&lt;a href=&#34;https://arxiv.org/abs/1911.12287&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Disentangling Invertible Interpretation Network for Explaining Latent Representations.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Patrick Esser, Robin Rombach, Björn Ommer.&lt;/em&gt;&lt;br&gt; CVPR 2020. [&lt;a href=&#34;https://arxiv.org/abs/2004.13166&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://compvis.github.io/iin/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/CompVis/iin&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Editing in Style: Uncovering the Local Semantics of GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Edo Collins, Raja Bala, Bob Price, Sabine Süsstrunk.&lt;/em&gt;&lt;br&gt; CVPR 2020. [&lt;a href=&#34;https://arxiv.org/abs/2004.14367&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/IVRL/GANLocalEditing&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Image Processing Using Multi-Code GAN Prior.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;http://www.jasongt.com/&#34;&gt;Jinjin Gu&lt;/a&gt;, Yujun Shen, Bolei Zhou.&lt;/em&gt;&lt;br&gt; CVPR 2020. [&lt;a href=&#34;https://arxiv.org/abs/1912.07116&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://genforce.github.io/mganprior/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/genforce/mganprior&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Image2StyleGAN++: How to Edit the Embedded Images?&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Rameen Abdal, &lt;a href=&#34;http://yipengqin.github.io/&#34;&gt;Yipeng Qin&lt;/a&gt;, Peter Wonka.&lt;/em&gt;&lt;br&gt; CVPR 2020. [&lt;a href=&#34;https://arxiv.org/abs/1911.11544&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Semantic Photo Manipulation with a Generative Image Prior.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;David Bau, Hendrik Strobelt, William Peebles, Jonas, Bolei Zhou, Jun-Yan Zhu, Antonio Torralba.&lt;/em&gt;&lt;br&gt; TOG 2019. [&lt;a href=&#34;https://arxiv.org/abs/2005.07727&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Image2StyleGAN: How to Embed Images Into the StyleGAN Latent Space?&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Rameen Abdal, &lt;a href=&#34;http://yipengqin.github.io/&#34;&gt;Yipeng Qin&lt;/a&gt;, Peter Wonka.&lt;/em&gt;&lt;br&gt; ICCV 2019. [&lt;a href=&#34;https://arxiv.org/abs/1904.03189&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/RameenAbdal/image2styleganv1-v2&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GAN-based Projector for Faster Recovery with Convergence Guarantees in Linear Inverse Problems.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ankit Raj, Yuqi Li, Yoram Bresler.&lt;/em&gt;&lt;br&gt; ICCV 2019. [&lt;a href=&#34;https://arxiv.org/abs/1902.09698&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Inverting Layers of a Large Generator.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;David Bau, Jun-Yan Zhu, Jonas Wulff, William Peebles, Hendrik Strobelt, Bolei Zhou, Antonio Torralba.&lt;/em&gt;&lt;br&gt; ICCV 2019. [&lt;a href=&#34;https://debug-ml-iclr2019.github.io/cameraready/DebugML-19_paper_18.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Detecting Overfitting in Deep Generators via Latent Recovery.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ryan Webster, Julien Rabin, Loic Simon, Frederic Jurie.&lt;/em&gt;&lt;br&gt; CVPR 2019. [&lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2019/papers/Webster_Detecting_Overfitting_of_Deep_Generative_Networks_via_Latent_Recovery_CVPR_2019_paper.pdf&#34;&gt;PDF&lt;/a&gt;][&lt;a href=&#34;https://colab.research.google.com/drive/1N6zP4xlPunWOkmakcl0mamfhq946nMLB?usp=sharing&#34;&gt;Colab&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Inverting The Generator Of A Generative Adversarial Network (II).&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Antonia Creswell, Anil A Bharath.&lt;/em&gt;&lt;br&gt; TNNLS 2018. [&lt;a href=&#34;https://arxiv.org/abs/1802.05701&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ToniCreswell/InvertingGAN&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Invertibility of Convolutional Generative Networks from Partial Measurements.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Fangchang Ma, Ulas Ayaz, Sertac Karaman.&lt;/em&gt;&lt;br&gt; NeurIPS 2018. [&lt;a href=&#34;https://papers.nips.cc/paper/8171-invertibility-of-convolutional-generative-networks-from-partial-measurements&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/fangchangma/invert-generative-networks&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Metrics for Deep Generative Models.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Nutan Chen, Alexej Klushyn, Richard Kurle, Xueyan Jiang, Justin Bayer, Patrick van der Smagt.&lt;/em&gt;&lt;br&gt; AISTATS 2018. [&lt;a href=&#34;https://arxiv.org/abs/1711.01204&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards Understanding the Invertibility of Convolutional Neural Networks.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Anna C. Gilbert, Yi Zhang, Kibok Lee, Yuting Zhang, Honglak Lee.&lt;/em&gt;&lt;br&gt; IJCAI 2017. [&lt;a href=&#34;https://arxiv.org/abs/1705.08664&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;One Network to Solve Them All - Solving Linear Inverse Problems using Deep Projection Models.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;J. H. Rick Chang, Chun-Liang Li, Barnabas Poczos, B. V. K. Vijaya Kumar, Aswin C. Sankaranarayanan.&lt;/em&gt;&lt;br&gt; ICCV 2017. [&lt;a href=&#34;https://arxiv.org/abs/1703.09912&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Precise Recovery of Latent Vectors from Generative Adversarial Networks.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Zachary C. Lipton, Subarna Tripathi.&lt;/em&gt;&lt;br&gt; ICLR 2017 workshop. [&lt;a href=&#34;https://arxiv.org/abs/1702.04782&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/SubarnaTripathi/ReverseGAN&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Inverting The Generator Of A Generative Adversarial Network.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Antonia Creswell, Anil Anthony Bharath.&lt;/em&gt;&lt;br&gt; NeurIPS 2016 Workshop. [&lt;a href=&#34;https://arxiv.org/abs/1611.05644&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generative Visual Manipulation on the Natural Image Manifold.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jun-Yan Zhu, Philipp Krähenbühl, Eli Shechtman, Alexei A. Efros.&lt;/em&gt;&lt;br&gt; ECCV 2016. [&lt;a href=&#34;https://arxiv.org/abs/1609.03552v2&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h2&gt;3D GANs inverson&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;3D GAN Inversion for Controllable Portrait Image Animation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://connorzlin.com/&#34;&gt;Connor Z. Lin&lt;/a&gt;, David B. Lindell, Eric R. Chan, &lt;a href=&#34;https://stanford.edu/~gordonwz/&#34;&gt;Gordon Wetzstein&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.13441&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://www.computationalimaging.org/publications/3dganinversion/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pix2NeRF: Unsupervised Conditional π-GAN for Single Image to Neural Radiance Fields Translation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Shengqu Cai, Anton Obukhov, Dengxin Dai, Luc Van Gool.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.13162&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised 3D Shape Completion through GAN-Inversion.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://junzhezhang.github.io/&#34;&gt;Junzhe Zhang&lt;/a&gt;, Xinyi Chen, Zhongang Cai, Liang Pan, Haiyu Zhao, Shuai Yi, Chai Kiat Yeo, Bo Dai, Chen Change Loy.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/pdf/2104.13366&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://junzhezhang.github.io/projects/ShapeInversion/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h2&gt;latent space navigation&lt;/h2&gt; &#xA;&lt;p&gt;Inversion is not the ultimate goal. The reason that we invert a real image into the latent space of a trained GAN model is that we can manipulate the inverted image in the latent space by discovering the desired code with certain attributes. This technique is usually known as latent space navigation, GAN steerability, latent code manipulation, or other names in the literature. Although often regarded as an independent research field, it acts as an indispensable component of GAN inversion for manipulation. Many inversion methods also involve efficient discovery of a desired latent code.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fantastic Style Channels and Where to Find Them: A Submodular Framework for Discovering Diverse Directions in GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://enis.dev/&#34;&gt;Enis Simsar&lt;/a&gt;, &lt;a href=&#34;https://catlab-team.github.io/&#34;&gt;Umut Kocasari&lt;/a&gt;, &lt;a href=&#34;https://catlab-team.github.io/&#34;&gt;Ezgi Gülperi Er&lt;/a&gt;, &lt;a href=&#34;https://pinguar.org/&#34;&gt;Pinar Yanardag&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.08516&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://catlab-team.github.io/fantasticstyles/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://catlab-team.github.io/styleatlas/classes/FFHQ/&#34;&gt;Demo&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Region-Based Semantic Factorization in GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jiapeng Zhu, Yujun Shen, Yinghao Xu, Deli Zhao, Qifeng Chen.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.09649&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multi-level Latent Space Structuring for Generative Control.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://orenkatzir.github.io/&#34;&gt;Oren Katzir&lt;/a&gt;, Vicky Perepelook, Dani Lischinski, Daniel Cohen-Or.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.05910&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Rayleigh EigenDirections (REDs): GAN Latent Space Traversals for Multidimensional Features.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Guha Balakrishnan, Raghudeep Gadde, Aleix Martinez, Pietro Perona.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/pdf/2201.10423.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Optimizing Latent Space Directions For GAN-based Local Image Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ehsan Pajouheshgar, Tong Zhang, Sabine Süsstrunk.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2111.12583&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Interpreting the Latent Space of GANs via Correlation Analysis for Controllable Concept Manipulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ziqiang Li, Rentuo Tao, Hongjing Niu, Bin Li.&lt;/em&gt;&lt;br&gt; arxiv 2020. [&lt;a href=&#34;https://arxiv.org/abs/2006.10132&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tensor Component Analysis for Interpreting the Latent Space of GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://james-oldfield.github.io/&#34;&gt;James Oldfield&lt;/a&gt;, Markos Georgopoulos, Yannis Panagakis, Mihalis A. Nicolaou, Ioannis Patras.&lt;/em&gt;&lt;br&gt; BMVC 2021. [&lt;a href=&#34;https://arxiv.org/abs/2111.11736&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;http://eecs.qmul.ac.uk/~jo001/TCA-latent-space/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/james-oldfield/TCA-latent-space&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tensor-based Subspace Factorization for StyleGAN.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Rene Haas, Stella Graßhof and Sami S. Brandt.&lt;/em&gt;&lt;br&gt; FG 2021. [&lt;a href=&#34;https://arxiv.org/abs/2111.04554&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tensor-based Emotion Editing in the StyleGAN Latent Space.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;René Haas, Stella Graßhof, Sami S. Brandt.&lt;/em&gt;&lt;br&gt; CVPR 2022 Workshop on AI for Content Creation Workshop. [&lt;a href=&#34;https://arxiv.org/pdf/2205.06102.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LARGE: Latent-Based Regression through GAN Semantics.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://yotamnitzan.github.io&#34;&gt;Yotam Nitzan&lt;/a&gt;, &lt;a href=&#34;https://rinongal.github.io/&#34;&gt;Rinon Gal&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=iLLlWr8AAAAJ&#34;&gt;Ofir Brenner&lt;/a&gt;, &lt;a href=&#34;https://danielcohenor.com/&#34;&gt;Daniel Cohen-Or&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2107.11186&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/YotamNitzan/LARGE&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://yotamnitzan.github.io/LARGE&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Latent Image Animator: Learning to Animate Image via Latent Space Navigation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yaohui Wang, Di Yang, Francois Bremond, Antitza Dantcheva.&lt;/em&gt;&lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://openreview.net/forum?id=7r6kDq0mK_&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://wyhsirius.github.io/LIA-project&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/wyhsirius/LIA&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleFusion: Disentangling Spatial Segments in StyleGAN-Generated Images.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Omer Kafri, Or Patashnik, Yuval Alaluf, Daniel Cohen-Or.&lt;/em&gt;&lt;br&gt; TOG 2022. [&lt;a href=&#34;https://arxiv.org/abs/2107.07437&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/OmerKafri/StyleFusion&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Do Not Escape From the Manifold: Discovering the Local Coordinates on the Latent Space of GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jaewoong Choi, Changyeon Yoon, Junho Lee, Jung Ho Park, Geonho Hwang, Myungjoo Kang.&lt;/em&gt;&lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2106.06959&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Exploratory Search of GANs with Contextual Bandits.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ivan Kropotov, Alan Medlar, Dorota Glowacka.&lt;/em&gt;&lt;br&gt; CIKM 2021. [&lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3459637.3482103&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LowRankGAN: Low-Rank Subspaces in GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jiapeng Zhu, Ruili Feng, Yujun Shen, Deli Zhao, Zhengjun Zha, Jingren Zhou, Qifeng Chen.&lt;/em&gt;&lt;br&gt; NeurIPS 2021. [&lt;a href=&#34;https://arxiv.org/abs/2106.04488&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/zhujiapeng/LowRankGAN&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Controllable and Compositional Generation with Latent-Space Energy-Based Models.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Weili Nie, Arash Vahdat, Anima Anandkumar.&lt;/em&gt;&lt;br&gt; NeurIPS 2021. [&lt;a href=&#34;https://arxiv.org/abs/2110.10873&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Latent Transformer for Disentangled Face Editing in Images and Videos.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Xu Yao, Alasdair Newson, Yann Gousseau, Pierre Hellier.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2021/html/Yao_A_Latent_Transformer_for_Disentangled_Face_Editing_in_Images_and_ICCV_2021_paper.html&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/2106.11895&#34;&gt;ArXiV&lt;/a&gt;] [&lt;a href=&#34;https://github.com/InterDigitalInc/latent-transformer&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Toward a Visual Concept Vocabulary for GAN Latent Space.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Sarah Schwettmann, Evan Hernandez, David Bau, Samuel Klein, Jacob Andreas, Antonio Torralba.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2021/html/Schwettmann_Toward_a_Visual_Concept_Vocabulary_for_GAN_Latent_Space_ICCV_2021_paper.html&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://visualvocab.csail.mit.edu/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;WarpedGANSpace: Finding Non-linear RBF Paths in GAN Latent Space.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Christos Tzelepis, Georgios Tzimiropoulos, Ioannis Patras.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://arxiv.org/abs/2109.13357&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/chi0tzp/WarpedGANSpace&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Latent Transformations via NeuralODEs for GAN-based Image Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Valentin Khrulkov, Leyla Mirvakhabova, Ivan Oseledets, Artem Babenko.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2021/papers/Khrulkov_Latent_Transformations_via_NeuralODEs_for_GAN-Based_Image_Editing_ICCV_2021_paper.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/KhrulkovV/nonlinear-image-editing&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;OroJaR: Orthogonal Jacobian Regularization for Unsupervised Disentanglement in Image Generation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuxiang Wei, Yupeng Shi, Xiao Liu, Zhilong Ji, Yuan Gao, Zhongqin Wu, Wangmeng Zuo.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://arxiv.org/abs/2108.07668&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/csyxwei/OroJaR&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;EigenGAN: Layer-Wise Eigen-Learning for GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Zhenliang He, Meina Kan, Shiguang Shan.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://arxiv.org/abs/2104.12476&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/LynnHo/EigenGAN-Tensorflow&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SalS-GAN: Spatially-Adaptive Latent Space in StyleGAN for Real Image Embedding.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Lingyun Zhang, Xiuxiu Bai, Yao Gao.&lt;/em&gt;&lt;br&gt; ACM MM 2021. [&lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3474085.3475633&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Discovering Density-Preserving Latent Space Walks in GANs for Semantic Image Transformations.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Guanyue Li, Yi Liu, Xiwen Wei, Yang Zhang, Si Wu, Yong Xu, Hau San Wong.&lt;/em&gt;&lt;br&gt; ACM MM 2021. [&lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3474085.3475293&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Discovering Interpretable Latent Space Directions of GANs Beyond Binary Attributes.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Huiting Yang, Liangyu Chai, Qiang Wen, Shuang Zhao, Zixun Sun, Shengfeng He.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_Discovering_Interpretable_Latent_Space_Directions_of_GANs_Beyond_Binary_Attributes_CVPR_2021_paper.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Surrogate Gradient Field for Latent Space Manipulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Minjun Li, Yanghua Jin, Huachun Zhu.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;http://arxiv.org/abs/2104.09065&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SeFa: Closed-Form Factorization of Latent Semantics in GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yujun Shen, Bolei Zhou.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2007.06600&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/genforce/sefa&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://genforce.github.io/sefa/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;L2M-GAN: Learning To Manipulate Latent Space Semantics for Facial Attribute Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Guoxing Yang, Nanyi Fei, Mingyu Ding, Guangzhen Liu, Zhiwu Lu, Tao Xiang.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2021/html/Yang_L2M-GAN_Learning_To_Manipulate_Latent_Space_Semantics_for_Facial_Attribute_CVPR_2021_paper.html&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/songquanpeng/L2M-GAN&#34;&gt;Unofficial Pytorch&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MoCoGAN-HD: A Good Image Generator Is What You Need for High-Resolution Video Synthesis.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng, Dimitris N. Metaxas, Sergey Tulyakov.&lt;/em&gt;&lt;br&gt; ICLR 2021. [&lt;a href=&#34;https://openreview.net/pdf?id=6puCSjH3hwA&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/snap-research/MoCoGAN-HD&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GAN Steerability without optimization.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Nurit Spingarn-Eliezer, Ron Banner, Tomer Michaeli.&lt;/em&gt;&lt;br&gt; ICLR 2021. [&lt;a href=&#34;https://openreview.net/forum?id=zDy_nQCXiIj&#34;&gt;OpenReview&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/2012.05328&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;On the &#34;steerability&#34; of generative adversarial networks.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ali Jahanian, Lucy Chai, Phillip Isola.&lt;/em&gt;&lt;br&gt; ICLR 2020. [&lt;a href=&#34;https://arxiv.org/abs/1907.07171&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://ali-design.github.io/gan_steerability/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GANSpace: Discovering Interpretable GAN Controls.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Erik Härkönen, Aaron Hertzmann, Jaakko Lehtinen, Sylvain Paris.&lt;/em&gt;&lt;br&gt; NeurIPS 2020. [&lt;a href=&#34;https://arxiv.org/abs/2004.02546&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/harskish/ganspace&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Interpreting the Latent Space of GANs for Semantic Face Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;http://shenyujun.github.io/&#34;&gt;Yujun Shen&lt;/a&gt;, &lt;a href=&#34;http://www.jasongt.com/&#34;&gt;Jinjin Gu&lt;/a&gt;, &lt;a href=&#34;http://www.ie.cuhk.edu.hk/people/xotang.shtml&#34;&gt;Xiaoou Tang&lt;/a&gt;, &lt;a href=&#34;http://bzhou.ie.cuhk.edu.hk/&#34;&gt;Bolei Zhou&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2020. [&lt;a href=&#34;https://arxiv.org/abs/1907.10786&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://genforce.github.io/interfacegan/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/genforce/interfacegan&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Seeing What a GAN Cannot Generate.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;David Bau, Jun-Yan Zhu, Jonas Wulff, William Peebles, Hendrik Strobelt, Bolei Zhou, Antonio Torralba.&lt;/em&gt;&lt;br&gt; ICCV 2019. [&lt;a href=&#34;https://arxiv.org/abs/1910.11626&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;http://ganseeing.csail.mit.edu/&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised Discovery of Interpretable Directions in the GAN Latent Space.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Andrey Voynov, Artem Babenko.&lt;/em&gt;&lt;br&gt; ICML 2020. [&lt;a href=&#34;https://arxiv.org/abs/2002.03754&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/anvoynov/GANLatentDiscovery&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h2&gt;application&lt;/h2&gt; &#xA;&lt;h3&gt;image and video generation and manipulation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Sound-Guided Semantic Video Generation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Seung Hyun Lee, Gyeongrok Oh, Wonmin Byeon, Jihyun Bae, Chanyoung Kim, Won Jeong Ryoo, Sang Ho Yoon, Jinkyu Kim, Sangpil Kim.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2204.09273&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FEAT: Face Editing with Attention.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Xianxu Hou, Linlin Shen, Or Patashnik, Daniel Cohen-Or, Hui Huang.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2202.02713&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DyStyle: Dynamic Neural Network for Multi-Attribute-Conditioned Style Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Bingchuan Li, Shaofei Cai, Wei Liu, Peng Zhang, Miao Hua, Qian He, Zili Yi.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2109.10737&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/phycvgan/DyStyle&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Few-shot Semantic Image Synthesis Using StyleGAN Prior.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuki Endo, Yoshihiro Kanamori.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2103.14877&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/endo-yuki-t/Fewshot-SMIS&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Heredity-aware Child Face Image Generation with Latent Space Disentanglement.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Xiao Cui, Wengang Zhou, Yang Hu, Weilun Wang, Houqiang Li.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2108.11080&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised Image Transformation Learning via Generative Adversarial Networks.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Kaiwen Zha, Yujun Shen, Bolei Zhou.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2103.07751&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://genforce.github.io/trgan&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised Image-to-Image Translation via Pre-trained StyleGAN2 Network.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jialu Huang, Jing Liao, Sam Kwong.&lt;/em&gt;&lt;br&gt; arxiv 2020. [&lt;a href=&#34;https://arxiv.org/abs/2010.05713&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Identity-Guided Face Generation with Multi-modal Contour Conditions.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Qingyan Bai, Weihao Xia, Fei Yin, Yujiu Yang.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2110.04854&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Sound-Guided Semantic Image Manipulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Seung Hyun Lee, Wonseok Roh, Wonmin Byeon, Sang Ho Yoon, Chan Young Kim, Jinkyu Kim, Sangpil Kim.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2112.00007&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HairCLIP: Design Your Hair by Text and Reference Image.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tianyi Wei, Dongdong Chen, Wenbo Zhou, Jing Liao, Zhentao Tan, Lu Yuan, Weiming Zhang, Nenghai Yu.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2112.05142&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/wty-ustc/HairCLIP&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HairMapper: Removing Hair from Portraits Using GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://onethousandwu.com/&#34;&gt;Yiqian Wu&lt;/a&gt;, &lt;a href=&#34;http://www.yongliangyang.net/&#34;&gt;Yong-Liang Yang&lt;/a&gt;, &lt;a href=&#34;http://www.cad.zju.edu.cn/home/jin&#34;&gt;Xiaogang Jin&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;http://www.cad.zju.edu.cn/home/jin/cvpr2022/HairMapper.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;http://www.cad.zju.edu.cn/home/jin/cvpr2022/cvpr2022.htm&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/oneThousand1000/non-hair-FFHQ/raw/main&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://github.com/oneThousand1000/non-hair-FFHQ&#34;&gt;Non-hair-FFHQ Data&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Attribute Group Editing for Reliable Few-shot Image Generation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Guanqi Ding, Xinzhe Han, Shuhui Wang, Shuzhe Wu, Xin Jin, Dandan Tu, Qingming Huang.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.08422&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/UniBester/AGE&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;InsetGAN for Full-Body Image Generation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://afruehstueck.github.io/&#34;&gt;Anna Frühstück&lt;/a&gt;, &lt;a href=&#34;http://krsingh.cs.ucdavis.edu/&#34;&gt;Krishna Kumar Singh&lt;/a&gt;, &lt;a href=&#34;https://research.adobe.com/person/eli-shechtman/&#34;&gt;Eli Shechtman&lt;/a&gt;, &lt;a href=&#34;https://research.adobe.com/person/niloy-mitra/&#34;&gt;Niloy J. Mitra&lt;/a&gt;, &lt;a href=&#34;http://peterwonka.net/&#34;&gt;Peter Wonka&lt;/a&gt;, &lt;a href=&#34;https://research.adobe.com/person/jingwan-lu/&#34;&gt;Jingwan Lu&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2112.07200&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;http://afruehstueck.github.io/insetgan&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/stylegan-human/StyleGAN-Human/raw/main/insetgan.py&#34;&gt;Unofficial&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SpaceEdit: Learning a Unified Editing Space for Open-Domain Image Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://www.cs.rochester.edu/u/jshi31/&#34;&gt;Jing Shi&lt;/a&gt;, &lt;a href=&#34;https://sites.google.com/view/ningxu/&#34;&gt;Ning Xu&lt;/a&gt;, &lt;a href=&#34;https://www.cs.rochester.edu/u/hzheng15/haitian_homepage/index.html&#34;&gt;Haitian Zheng&lt;/a&gt;, Alex Smith, &lt;a href=&#34;https://www.cs.rochester.edu/u/jluo/&#34;&gt;Jiebo Luo&lt;/a&gt;, &lt;a href=&#34;https://www.cs.rochester.edu/~cxu22/&#34;&gt;Chenliang Xu&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2112.00180&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;In&amp;amp;Out: Diverse Image Outpainting via GAN Inversion.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yen-Chi Cheng, Chieh Hubert Lin, Hsin-Ying Lee, Jian Ren, Sergey Tulyakov, Ming-Hsuan Yang.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2104.00675&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://yccyenchicheng.github.io/InOut/&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;InfinityGAN: Towards Infinite-Resolution Image Synthesis.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Chieh Hubert Lin, Hsin-Ying Lee, Yen-Chi Cheng, Sergey Tulyakov, Ming-Hsuan Yang.&lt;/em&gt;&lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2104.03963&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://hubert0527.github.io/infinityGAN/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Latent to Latent: A Learned Mapper for Identity Preserving Editing of Multiple.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Siavash Khodadadeh, Shabnam Ghadar, Saeid Motiian, Wei-An Lin, Ladislau Bölöni, Ratheesh Kalarot.&lt;/em&gt;&lt;br&gt; WACV 2022. [&lt;a href=&#34;https://openaccess.thecvf.com/content/WACV2022/html/Khodadadeh_Latent_to_Latent_A_Learned_Mapper_for_Identity_Preserving_Editing_WACV_2022_paper.html&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleVideoGAN: A Temporal Generative Model using a Pretrained StyleGAN.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://www.mpi-inf.mpg.de/~gfox/&#34;&gt;Gereon Fox&lt;/a&gt;, &lt;a href=&#34;https://www.mpi-inf.mpg.de/~atewari/&#34;&gt;Ayush Tewari&lt;/a&gt;, Mohamed Elgharib, &lt;a href=&#34;http://gvv.mpi-inf.mpg.de/&#34;&gt;Christian Theobalt&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; BMVC 2021 (Oral). [&lt;a href=&#34;https://arxiv.org/abs/2107.07224&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Constrained Graphic Layout Generation via Latent Optimization.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Kotaro Kikuchi, Edgar Simo-Serra, Mayu Otani, Kota Yamaguchi.&lt;/em&gt;&lt;br&gt; ACM MM 2021. [&lt;a href=&#34;https://arxiv.org/abs/2108.00871&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ktrk115/const_layout&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleCariGAN: Caricature Generation via StyleGAN Feature Map Modulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Wongjong Jang, Gwangjin Ju, &lt;a href=&#34;https://ycjung.info/&#34;&gt;Yucheol Jung&lt;/a&gt;, &lt;a href=&#34;http://jlyang.org/&#34;&gt;Jiaolong Yang&lt;/a&gt;, &lt;a href=&#34;https://www.microsoft.com/en-us/research/people/xtong/&#34;&gt;Xin Tong&lt;/a&gt;, &lt;a href=&#34;http://phome.postech.ac.kr/~leesy/&#34;&gt;Seungyong Lee&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; TOG 2021. [&lt;a href=&#34;https://arxiv.org/pdf/2107.04331.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/PeterZhouSZ/StyleCariGAN&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Coarse-to-Fine: Facial Structure Editing of Portrait Images via Latent Space Classifications.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://onethousandwu.com/&#34;&gt;Yiqian Wu&lt;/a&gt;, &lt;a href=&#34;http://www.yongliangyang.net/&#34;&gt;Yongliang Yang&lt;/a&gt;, Qinjie Xiao, &lt;a href=&#34;http://www.cad.zju.edu.cn/home/jin&#34;&gt;Xiaogang Ji&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; TOG 2021. [&lt;a href=&#34;http://www.cad.zju.edu.cn/home/jin/sig2021/paper46.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;http://www.cad.zju.edu.cn/home/jin/sig2021/sig2021.htm&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SAM: Only a Matter of Style-Age Transformation Using a Style-Based Regression Model.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuval Alaluf, Or Patashnik, &lt;a href=&#34;https://www.cs.tau.ac.il/~dcor/&#34;&gt;Daniel Cohen-Or&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; TOG 2021. [&lt;a href=&#34;https://arxiv.org/abs/2102.02754&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yuval-alaluf/SAM&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Exploring Adversarial Fake Images on Face Manifold.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Dongze Li, Wei Wang, Hongxing Fan, Jing Dong.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2101.03272&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HistoGAN: Controlling Colors of GAN-Generated and Real Images via Color Histograms.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://sites.google.com/view/mafifi&#34;&gt;Mahmoud Afifi&lt;/a&gt;, Marcus A. Brubaker, Michael S. Brown.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2011.11731&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mahmoudnafifi/HistoGAN&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://ln2.sync.com/dl/1891becc0/uhsxtprq-33wfwmyq-dhhqeb3s-mtstuqw7/view/default/11118541390008&#34;&gt;4K Landscape&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;One Shot Face Swapping on Megapixels.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuhao Zhu, Qi Li, Jian Wang, Chengzhong Xu, Zhenan Sun.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/pdf/2105.04932.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/zyainfal/One-Shot-Face-Swapping-on-Megapixels&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LOHO: Latent Optimization of Hairstyles via Orthogonalization.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Rohit Saha, Brendan Duke, Florian Shkurti, Graham W. Taylor, Parham Aarabi.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2103.03891&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/dukebw/LOHO&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleMapGAN: Exploiting Spatial Dimensions of Latent in GAN for Real-time Image Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://github.com/blandocs&#34;&gt;Hyunsu Kim&lt;/a&gt;, &lt;a href=&#34;https://yunjey.github.io/&#34;&gt;Yunjey Choi&lt;/a&gt;, &lt;a href=&#34;https://github.com/taki0112&#34;&gt;Junho Kim&lt;/a&gt;, &lt;a href=&#34;http://cmalab.snu.ac.kr/&#34;&gt;Sungjoo Yoo&lt;/a&gt;, &lt;a href=&#34;https://github.com/youngjung&#34;&gt;Youngjung Uh&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2104.14754&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/naver-ai/StyleMapGAN&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DeepI2I: Enabling Deep Hierarchical Image-to-Image Translation by Transferring from GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;yaxing wang, Lu Yu, Joost van de Weijer.&lt;/em&gt;&lt;br&gt; NeurIPS 2020. [&lt;a href=&#34;https://arxiv.org/abs/2011.05867&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yaxingwang/DeepI2I&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h3&gt;multimodal learning&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image Generators.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://rinongal.github.io/&#34;&gt;Rinon Gal&lt;/a&gt;, &lt;a href=&#34;https://orpatashnik.github.io/&#34;&gt;Or Patashnik&lt;/a&gt;, &lt;a href=&#34;https://haggaim.github.io/&#34;&gt;Haggai Maron&lt;/a&gt;, &lt;a href=&#34;https://research.nvidia.com/person/gal-chechik&#34;&gt;Gal Chechik&lt;/a&gt;, &lt;a href=&#34;https://www.cs.tau.ac.il/~dcor/&#34;&gt;Daniel Cohen-Or&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2108.00946&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://stylegan-nada.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/rinongal/StyleGAN-nada&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Paint by Word.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;David Bau, Alex Andonian, Audrey Cui, YeonHwan Park, Ali Jahanian, Aude Oliva, Antonio Torralba.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/pdf/2103.10951&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CI-GAN: Cycle-Consistent Inverse GAN for Text-to-Image Synthesis.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Hao Wang, Guosheng Lin, Steven C. H. Hoi, Chunyan Miao.&lt;/em&gt;&lt;br&gt; ACM MM 2021. [&lt;a href=&#34;https://arxiv.org/abs/2108.01361&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TediGAN: Text-Guided Diverse Image Generation and Manipulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Weihao Xia, Yujiu Yang, Jing-Hao Xue, Baoyuan Wu.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2012.03308&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/weihaox/Multi-Modal-CelebA-HQ&#34;&gt;Data&lt;/a&gt;] [&lt;a href=&#34;https://github.com/weihaox/TediGAN&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DeepLandscape: Adversarial Modeling of Landscape Videos.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;E. Logacheva, R. Suvorov, O. Khomenko, A. Mashikhin, and V. Lempitsky.&lt;/em&gt;&lt;br&gt; ECCV 2020. [&lt;a href=&#34;https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123680256.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/saic-mdal/deep-landscape&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://saic-mdal.github.io/deep-landscape/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h3&gt;image restoration&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Escaping Data Scarcity for High-Resolution Heterogeneous Face Hallucination.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yiqun Mei, Pengfei Guo, Vishal M. Patel.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/pdf/2203.16669&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards High-Fidelity Face Self-Occlusion Recovery via Multi-View Residual-Based GAN Inversion.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jinsong Chen, Hu Han, Shiguang Shan.&lt;/em&gt;&lt;br&gt; AAAI 2022. [&lt;a href=&#34;https://www.aaai.org/AAAI22Papers/AAAI-2208.ChenJ.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Time-Travel Rephotography.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://time-travel-rephotography.github.io/&#34;&gt;Xuan Luo&lt;/a&gt;, &lt;a href=&#34;https://people.eecs.berkeley.edu/~cecilia77/&#34;&gt;Xuaner Zhang&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/paul-yoo-768a3715b&#34;&gt;Paul Yoo&lt;/a&gt;, &lt;a href=&#34;http://www.ricardomartinbrualla.com/&#34;&gt;Ricardo Martin-Brualla&lt;/a&gt;, &lt;a href=&#34;http://jasonlawrence.info/&#34;&gt;Jason Lawrence&lt;/a&gt;, &lt;a href=&#34;https://homes.cs.washington.edu/~seitz/&#34;&gt;Steven M. Seitz&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; SIGGRAPH Asia 2021 (TOG). [&lt;a href=&#34;https://arxiv.org/abs/2012.12261&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://time-travel-rephotography.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Time-Travel-Rephotography/Time-Travel-Rephotography.github.io&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GPEN: GAN Prior Embedded Network for Blind Face Restoration in the Wild.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tao Yang, Peiran Ren, Xuansong Xie, Lei Zhang.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_GAN_Prior_Embedded_Network_for_Blind_Face_Restoration_in_the_CVPR_2021_paper.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GLEAN: Generative Latent Bank for Large-Factor Image Super-Resolution.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://ckkelvinchan.github.io/&#34;&gt;Kelvin C.K. Chan&lt;/a&gt;, Xintao Wang, Xiangyu Xu, Jinwei Gu, Chen Change Loy.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2012.00739&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://ckkelvinchan.github.io/projects/GLEAN&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ckkelvinchan/GLEAN&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GFP-GAN: Towards Real-World Blind Face Restoration with Generative Facial Prior.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://xinntao.github.io/&#34;&gt;Xintao Wang&lt;/a&gt;, &lt;a href=&#34;https://yu-li.github.io/&#34;&gt;Yu Li&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?hl=en&amp;amp;user=KjQLROoAAAAJ&#34;&gt;Honglun Zhang&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=4oXBp9UAAAAJ&amp;amp;hl=en&#34;&gt;Ying Shan&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2101.04061&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://xinntao.github.io/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, Cynthia Rudin.&lt;/em&gt;&lt;br&gt; CVPR 2020. [&lt;a href=&#34;https://arxiv.org/abs/2003.03808&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/adamian98/pulse&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LTT-GAN: Looking Through Turbulence by Inverting GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://kfmei.page/&#34;&gt;Kangfu Mei&lt;/a&gt;, &lt;a href=&#34;https://engineering.jhu.edu/vpatel36/sciencex_teams/vishalpatel/&#34;&gt;Vishal M. Patel&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2112.02379&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://kfmei.page/LTT-GAN/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Style Generator Inversion for Image Enhancement and Animation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;http://www.cs.huji.ac.il/~avivga&#34;&gt;Aviv Gabbay&lt;/a&gt;, &lt;a href=&#34;http://www.cs.huji.ac.il/~ydidh&#34;&gt;Yedid Hoshen&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; arxiv 2019. [&lt;a href=&#34;https://arxiv.org/abs/1906.11880&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;http://www.vision.huji.ac.il/style-image-prior/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/avivga/style-image-prior&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h3&gt;image understanding&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Segmentation in Style: Unsupervised Semantic Image Segmentation with Stylegan and CLIP.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Daniil Pakhomov, Sanchit Hira, Narayani Wagle, Kemar E. Green, Nassir Navab.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/pdf/2107.12518.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://segmentation-in-style.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/warmspringwinds/segmentation_in_style&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Finding an Unsupervised Image Segmenter in each of your Deep Generative Models.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina, Andrea Vedaldi.&lt;/em&gt;&lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://openreview.net/forum?id=Ug-bgjgSlKV&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Labels4Free: Unsupervised Segmentation using StyleGAN.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=kEQimk0AAAAJ&amp;amp;hl=en&#34;&gt;Rameen Abdal&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=Gn8URq0AAAAJ&amp;amp;hl=en&#34;&gt;Peihao Zhu&lt;/a&gt;, &lt;a href=&#34;http://www0.cs.ucl.ac.uk/staff/n.mitra/&#34;&gt;Niloy Mitra&lt;/a&gt;, &lt;a href=&#34;http://peterwonka.net/&#34;&gt;Peter Wonka&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://arxiv.org/abs/2103.14968&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://rameenabdal.github.io/Labels4Free&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/RameenAbdal/Labels4Free&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DatasetGAN: Efficient Labeled Data Factory with Minimal Human Effort.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://www.alexyuxuanzhang.com/&#34;&gt;Yuxuan Zhang&lt;/a&gt;, &lt;a href=&#34;http://www.cs.toronto.edu/~linghuan/&#34;&gt;Huan Ling&lt;/a&gt;, &lt;a href=&#34;http://www.cs.toronto.edu/~jungao/&#34;&gt;Jun Gao&lt;/a&gt;, &lt;a href=&#34;https://kangxue.org/&#34;&gt;Kangxue Yin&lt;/a&gt;, &lt;a href=&#34;&#34;&gt;Jean-Francois Lafleche&lt;/a&gt;, &lt;a href=&#34;&#34;&gt;Adela Barriuso&lt;/a&gt;, &lt;a href=&#34;https://groups.csail.mit.edu/vision/torralbalab/&#34;&gt;Antonio Torralba&lt;/a&gt;, &lt;a href=&#34;http://www.cs.utoronto.ca/~fidler/&#34;&gt;Sanja Fidler&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2104.06490&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/nv-tlabs/datasetGAN_release&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://nv-tlabs.github.io/datasetGAN/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Repurposing GANs for One-shot Semantic Part Segmentation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Nontawat Tritrong, Pitchaporn Rewatbowornwong, &lt;a href=&#34;https://www.supasorn.com/&#34;&gt;Supasorn Suwajanakorn&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2021 (oral). [&lt;a href=&#34;https://arxiv.org/abs/2103.04379&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://repurposegans.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/bryandlee/repurpose-gan&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h3&gt;3D&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;StylePart: Image-based Shape Part Manipulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;I-Chao Shen, Li-Wen Su, Yu-Ting Wu, Bing-Yu Chen.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2111.10520&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CoordGAN: Self-Supervised Dense Correspondences Emerge from GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://jitengmu.github.io/&#34;&gt;Jiteng Mu&lt;/a&gt;, Shalini De Mello, Zhiding Yu, Nuno Vasconcelos, Xiaolong Wang, Jan Kautz, Sifei Liu.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.16521&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://jitengmu.github.io/CoordGAN/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GAN2Shape: Do 2D GANs Know 3D Shape? Unsupervised 3D shape reconstruction from 2D Image GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://xingangpan.github.io/&#34;&gt;Xingang Pan&lt;/a&gt;, Bo Dai, Ziwei Liu, Chen Change Loy, Ping Luo.&lt;/em&gt;&lt;br&gt; ICLR 2021 (oral). [&lt;a href=&#34;https://arxiv.org/abs/2011.00844&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/XingangPan/GAN2Shape&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://xingangpan.github.io/projects/GAN2Shape.html&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Normalized Avatar Synthesis Using StyleGAN and Perceptual Refinement.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Huiwen Luo, Koki Nagano, Han-Wei Kung, Mclean Goldwhite, &lt;a href=&#34;https://qingguo-xu.com/&#34;&gt;Qingguo Xu&lt;/a&gt;, Zejian Wang, Lingyu Wei, Liwen Hu, Hao Li.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2106.11423&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised 3D Shape Completion through GAN-Inversion.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://junzhezhang.github.io/&#34;&gt;Junzhe Zhang&lt;/a&gt;, Xinyi Chen, Zhongang Cai, Liang Pan, Haiyu Zhao, Shuai Yi, Chai Kiat Yeo, Bo Dai, Chen Change Loy.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/pdf/2104.13366&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://junzhezhang.github.io/projects/ShapeInversion/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;OSTeC: One-Shot Texture Completion.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Baris Gecer, Jiankang Deng, Stefanos Zafeiriou.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2012.15370&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/barisgecer/OSTeC&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h3&gt;compressed sensing&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generator Surgery for Compressed Sensing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Niklas Smedemark-Margulies, Jung Yeon Park, Max Daniels, Rose Yu, Jan-Willem van de Meent, Paul Hand.&lt;/em&gt;&lt;br&gt; NeurIPS 2020 Workshop Deep Inverse. [&lt;a href=&#34;https://arxiv.org/abs/2102.11163&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/nik-sm/generator-surgery&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Task-Aware Compressed Sensing with Generative Adversarial Networks.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Maya Kabkab, Pouya Samangouei, Rama Chellappa.&lt;/em&gt;&lt;br&gt; AAAI 2018. [&lt;a href=&#34;https://arxiv.org/pdf/1802.01284.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h3&gt;medical imaging&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Controllable Medical Image Generation via Generative Adversarial Networks.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Zhihang Ren, Stella X. Yu, David Whitney.&lt;/em&gt;&lt;br&gt; Human Vision and Electronic Imaging 2021. [&lt;a href=&#34;https://whitneylab.berkeley.edu/PDFs/Ren_MedImageGen.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High-resolution Controllable Prostatic Histology Synthesis using StyleGAN.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Gagandeep B. Daroach, Josiah A. Yoder, Kenneth A. Iczkowski, Peter S. LaViolette.&lt;/em&gt;&lt;br&gt; BIOIMAGING 2021. [&lt;a href=&#34;https://www.scitepress.org/Papers/2021/103939/103939.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h3&gt;security&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Differentially Private Imaging via Latent Space Manipulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tao Li, Chris Clifton.&lt;/em&gt;&lt;br&gt; IEEE Symposium on Security &amp;amp; Privacy (S&amp;amp;P) 2021. [&lt;a href=&#34;https://arxiv.org/abs/2103.05472&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h2&gt;acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;Thanks for the constructive comments from anonymous reviewers and feedback from &lt;a href=&#34;https://www.cs.cmu.edu/~junyanz/&#34;&gt;Jun-Yan Zhu&lt;/a&gt;, &lt;a href=&#34;https://github.com/anvoynov&#34;&gt;Andrey Voynov&lt;/a&gt;, and &lt;a href=&#34;https://rushila.com/&#34;&gt;Rushil Anirudh&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>wowchemy/starter-hugo-research-group</title>
    <updated>2022-06-03T02:25:02Z</updated>
    <id>tag:github.com,2022-06-03:/wowchemy/starter-hugo-research-group</id>
    <link href="https://github.com/wowchemy/starter-hugo-research-group" rel="alternate"></link>
    <summary type="html">&lt;p&gt;👥 轻松创建研究组或组织网站 Create a stunning Research Group, Team, or Organization Website with Hugo&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;a href=&#34;https://github.com/wowchemy/starter-hugo-research-group&#34;&gt;Hugo Research Group Theme&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/hugo-themes/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wowchemy/starter-hugo-research-group/main/preview.png&#34; alt=&#34;Screenshot&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The &lt;strong&gt;Research Group Template&lt;/strong&gt; empowers your research group to easily create a beautiful website with a stunning homepage, news, academic publications, events, team profiles, and a contact form.&lt;/p&gt; &#xA;&lt;p&gt;️&lt;strong&gt;Trusted by 250,000+ researchers, educators, and students.&lt;/strong&gt; Highly customizable via the integrated &lt;strong&gt;no-code, widget-based Wowchemy page builder&lt;/strong&gt;, making every site truly personalized ⭐⭐⭐⭐⭐&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/hugo-themes/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-Get%20started-ff4655?style=for-the-badge&#34; alt=&#34;Get Started&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.com/channels/722225264733716590/742892432458252370/742895548159492138&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/722225264733716590?style=for-the-badge&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://twitter.com/wowchemy&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/wowchemy?label=Follow%20on%20Twitter&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Easily write technical content with plain text Markdown, LaTeX math, diagrams, RMarkdown, or Jupyter, and import publications from BibTeX.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://research-group.netlify.app/&#34;&gt;Check out the latest demo&lt;/a&gt; of what you&#39;ll get in less than 60 seconds, or &lt;a href=&#34;https://wowchemy.com/creators/&#34;&gt;view the showcase&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The integrated &lt;a href=&#34;https://wowchemy.com&#34;&gt;&lt;strong&gt;Wowchemy&lt;/strong&gt;&lt;/a&gt; website builder and CMS makes it easy to create a beautiful website for free. Edit your site in the CMS (or your favorite editor), generate it with &lt;a href=&#34;https://github.com/gohugoio/hugo&#34;&gt;Hugo&lt;/a&gt;, and deploy with GitHub or Netlify. Customize anything on your site with widgets, light/dark themes, and language packs.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;👉 &lt;a href=&#34;https://wowchemy.com/hugo-themes/&#34;&gt;&lt;strong&gt;Get Started&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;📚 &lt;a href=&#34;https://wowchemy.com/docs/&#34;&gt;View the &lt;strong&gt;documentation&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;💬 &lt;a href=&#34;https://discord.gg/z8wNYzb&#34;&gt;Chat with the &lt;strong&gt;Wowchemy research community&lt;/strong&gt;&lt;/a&gt; or &lt;a href=&#34;https://discourse.gohugo.io&#34;&gt;&lt;strong&gt;Hugo community&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;⬇️ &lt;strong&gt;Automatically import citations from BibTeX&lt;/strong&gt; with the &lt;a href=&#34;https://github.com/wowchemy/hugo-academic-cli&#34;&gt;Hugo Academic CLI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;🐦 Share your new site with the community: &lt;a href=&#34;https://twitter.com/wowchemy&#34;&gt;@wowchemy&lt;/a&gt; &lt;a href=&#34;https://twitter.com/GeorgeCushen&#34;&gt;@GeorgeCushen&lt;/a&gt; &lt;a href=&#34;https://twitter.com/search?q=%23MadeWithWowchemy&amp;amp;src=typed_query&#34;&gt;#MadeWithWowchemy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;🗳 &lt;a href=&#34;https://forms.gle/NioD9VhUg7PNmdCAA&#34;&gt;Take the survey and help us improve #OpenSource&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;🚀 &lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-themes/raw/main/.github/contributing.md&#34;&gt;Contribute improvements&lt;/a&gt; or &lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-themes/issues&#34;&gt;suggest improvements&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;⬆️ &lt;strong&gt;Updating?&lt;/strong&gt; View the &lt;a href=&#34;https://wowchemy.com/docs/hugo-tutorials/update/&#34;&gt;Update Guide&lt;/a&gt; and &lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-themes/releases&#34;&gt;Release Notes&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;We ask you, humbly, to support this open source movement&lt;/h2&gt; &#xA;&lt;p&gt;Today we ask you to defend the open source independence of the Wowchemy website builder and themes 🐧&lt;/p&gt; &#xA;&lt;p&gt;We&#39;re an open source movement that depends on your support to stay online and thriving, but 99.9% of our creators don&#39;t give; they simply look the other way.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/sponsors/gcushen&#34;&gt;❤️ Click here to become a GitHub Sponsor, unlocking awesome perks such as &lt;em&gt;exclusive academic templates and widgets&lt;/em&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h2&gt;Demo credits&lt;/h2&gt; &#xA;&lt;p&gt;Please replace the demo images with your own.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://unsplash.com/photos/uVnRa6mOLOM&#34;&gt;Female scientist&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://unsplash.com/photos/kwzWjTnDPLk&#34;&gt;2 Coders&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://unsplash.com/photos/RnDGGnMEOao&#34;&gt;Cafe&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Blog posts &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://unsplash.com/photos/AndE50aaHn4&#34;&gt;https://unsplash.com/photos/AndE50aaHn4&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://unsplash.com/photos/OYzbqk2y26c&#34;&gt;https://unsplash.com/photos/OYzbqk2y26c&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Avatars &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://unsplash.com/photos/5yENNRbbat4&#34;&gt;https://unsplash.com/photos/5yENNRbbat4&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://unsplash.com/photos/WNoLnJo7tS8&#34;&gt;https://unsplash.com/photos/WNoLnJo7tS8&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>hmemcpy/milewski-ctfp-pdf</title>
    <updated>2022-06-03T02:25:02Z</updated>
    <id>tag:github.com,2022-06-03:/hmemcpy/milewski-ctfp-pdf</id>
    <link href="https://github.com/hmemcpy/milewski-ctfp-pdf" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Bartosz Milewski&#39;s &#39;Category Theory for Programmers&#39; unofficial PDF and LaTeX source&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Category Theory for Programmers&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/601206/43392303-f770d7be-93fb-11e8-8db8-b7e915b435ba.png&#34; alt=&#34;image&#34;&gt; &lt;b&gt;Direct link: &lt;a href=&#34;https://github.com/hmemcpy/milewski-ctfp-pdf/releases/download/v1.3.0/category-theory-for-programmers.pdf&#34;&gt;category-theory-for-programmers.pdf&lt;/a&gt;&lt;/b&gt;&lt;br&gt; (Latest release: v1.3.0, August 2019. See &lt;a href=&#34;https://github.com/hmemcpy/milewski-ctfp-pdf/releases&#34;&gt;releases&lt;/a&gt; for additional formats and languages.)&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://travis-ci.org/hmemcpy/milewski-ctfp-pdf&#34;&gt;&lt;img src=&#34;https://travis-ci.org/hmemcpy/milewski-ctfp-pdf.svg?branch=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://s3.amazonaws.com/milewski-ctfp-pdf/category-theory-for-programmers.pdf&#34;&gt;(latest CI build)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/601206/47271389-8eea0900-d581-11e8-8e81-5b932e336336.png&#34; alt=&#34;Buy Category Theory for Programmers&#34; width=&#34;410&#34;&gt;&lt;br&gt; &lt;strong&gt;&lt;a href=&#34;https://www.blurb.com/b/9621951-category-theory-for-programmers-new-edition-hardco&#34;&gt;Available in full-color hardcover print&lt;/a&gt;&lt;/strong&gt;&lt;br&gt; Publish date: 12 August, 2019. Based off release tag &lt;a href=&#34;https://github.com/hmemcpy/milewski-ctfp-pdf/releases/tag/v1.3.0&#34;&gt;v1.3.0&lt;/a&gt;. See &lt;a href=&#34;https://raw.githubusercontent.com/hmemcpy/milewski-ctfp-pdf/master/errata-1.3.0.md&#34;&gt;errata-1.3.0&lt;/a&gt; for changes and fixes since print.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://www.blurb.com/b/9603882-category-theory-for-programmers-scala-edition-pape&#34;&gt;Scala Edition is now available in paperback&lt;/a&gt;&lt;/strong&gt;&lt;br&gt; Publish date: 12 August, 2019. Based off release tag &lt;a href=&#34;https://github.com/hmemcpy/milewski-ctfp-pdf/releases/tag/v1.3.0&#34;&gt;v1.3.0&lt;/a&gt;. See &lt;a href=&#34;https://raw.githubusercontent.com/hmemcpy/milewski-ctfp-pdf/master/errata-scala.md&#34;&gt;errata-scala&lt;/a&gt; for changes and fixes since print.&lt;/p&gt; &#xA;&lt;p&gt;This is an &lt;em&gt;unofficial&lt;/em&gt; PDF version of &#34;Category Theory for Programmers&#34; by Bartosz Milewski, converted from his &lt;a href=&#34;https://bartoszmilewski.com/2014/10/28/category-theory-for-programmers-the-preface/&#34;&gt;blogpost series&lt;/a&gt; (with permission!)&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Building&lt;/h2&gt; &#xA;&lt;p&gt;The best way to build the book is using the &lt;a href=&#34;https://nixos.org/nix/&#34;&gt;Nix&lt;/a&gt; package manager. After &lt;a href=&#34;https://nixos.org/download.html&#34;&gt;installing Nix&lt;/a&gt;, if you&#39;re using a non-NixOS operating system, you need to install &lt;code&gt;nixFlakes&lt;/code&gt; in your environment following the steps below (&lt;a href=&#34;https://nixos.wiki/wiki/Flakes#Non-NixOS&#34;&gt;source&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ nix-env -iA nixpkgs.nixFlakes&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Edit either &lt;code&gt;~/.config/nix/nix.conf&lt;/code&gt; or &lt;code&gt;/etc/nix/nix.conf&lt;/code&gt; and add:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;experimental-features = nix-command flakes&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This is needed to expose the Nix 2.0 CLI and flakes support that are hidden behind feature-flags.&lt;/p&gt; &#xA;&lt;p&gt;Also, if the Nix installation is in multi-user mode, don’t forget to restart the nix-daemon.&lt;/p&gt; &#xA;&lt;p&gt;Afterwards, type &lt;code&gt;nix flake show&lt;/code&gt; in the root directory of the project to see all the available versions of this book. Then type &lt;code&gt;nix build .#&amp;lt;edition&amp;gt;&lt;/code&gt; to build the edition you want (Haskell, Scala, OCaml, Reason and their printed versions). For example, to build the Scala edition you&#39;ll have to type &lt;code&gt;nix build .#ctfp-scala&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Upon successful compilation, the PDF file will be placed in the &lt;code&gt;result&lt;/code&gt; directory inside the root directory &lt;code&gt;milewski-ctfp-pdf&lt;/code&gt; of the repository.&lt;/p&gt; &#xA;&lt;p&gt;The file &lt;code&gt;preamble.tex&lt;/code&gt; contains all the configuration and style declarations.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;PDF LaTeX source and the tools to create it are based on the work by Andres Raba et al., available here: &lt;a href=&#34;https://github.com/sarabander/sicp-pdf&#34;&gt;https://github.com/sarabander/sicp-pdf&lt;/a&gt;.&lt;br&gt; The book content is taken, with permission, from Bartosz Milewski&#39;s blogpost series, and adapted to the LaTeX format.&lt;/p&gt; &#xA;&lt;p&gt;Thanks to the following people for contributing corrections/conversions and misc:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Oleg Rakitskiy&lt;/li&gt; &#xA; &lt;li&gt;Jared Weakly&lt;/li&gt; &#xA; &lt;li&gt;Paolo G. Giarrusso&lt;/li&gt; &#xA; &lt;li&gt;Adi Shavit&lt;/li&gt; &#xA; &lt;li&gt;Mico Loretan&lt;/li&gt; &#xA; &lt;li&gt;Marcello Seri&lt;/li&gt; &#xA; &lt;li&gt;Erwin Maruli Tua Pakpahan&lt;/li&gt; &#xA; &lt;li&gt;Markus Hauck&lt;/li&gt; &#xA; &lt;li&gt;Yevheniy Zelenskyy&lt;/li&gt; &#xA; &lt;li&gt;Ross Kirsling&lt;/li&gt; &#xA; &lt;li&gt;...and many others!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The original blog post acknowledgments by Bartosz are consolidated in the &lt;em&gt;Acknowledgments&lt;/em&gt; page at the end of the book.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note from Bartosz&lt;/strong&gt;: I really appreciate all your contributions. You made this book much better than I could have imagined. Thank you!&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The PDF book, &lt;code&gt;.tex&lt;/code&gt; files, and associated images and figures in directories &lt;code&gt;src/fig&lt;/code&gt; and &lt;code&gt;src/content&lt;/code&gt; are licensed under Creative Commons Attribution-ShareAlike 4.0 International License (&lt;a href=&#34;http://creativecommons.org/licenses/by-sa/4.0/&#34;&gt;cc by-sa&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;The script files &lt;code&gt;scraper.py&lt;/code&gt; and others are licensed under GNU General Public License version 3 (for details, see &lt;a href=&#34;https://github.com/hmemcpy/milewski-ctfp-pdf/raw/master/LICENSE&#34;&gt;LICENSE&lt;/a&gt;).&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>minsk-hackerspace/OO</title>
    <updated>2022-06-03T02:25:02Z</updated>
    <id>tag:github.com,2022-06-03:/minsk-hackerspace/OO</id>
    <link href="https://github.com/minsk-hackerspace/OO" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Общественне объединение и зачем это надо&lt;/p&gt;&lt;hr&gt;&lt;p&gt;По неподтверждённым данным (reference needed), чтобы что-то организовывать, снимать или делать вскладчину в РБ, нужно регистрировать Общественное Объединение. Зачем и почему - неизвестно (reference needed), но нужно (reference needed).&lt;/p&gt; &#xA;&lt;p&gt;Поэтому инициативная группа ХС, которым не безразлична это проблема, решила на всякий случай это ОО зарегистрировать, и вот что пока что получается.&lt;/p&gt; &#xA;&lt;h3&gt;Что нужно для регистрации ОО?&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;документ, устав, не обязательно понятный всемучастникам, но понятный регистрирующему органу&lt;/li&gt; &#xA; &lt;li&gt;другие документы (reference needed)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Что есть?&lt;/h3&gt; &#xA;&lt;p&gt;constitution - Устав Просветительского Общественного Объединения &#34;Открытая лаборатория технического творчества&#34;&lt;/p&gt; &#xA;&lt;h3&gt;Как с этим работать?&lt;/h3&gt; &#xA;&lt;p&gt;To get PDF run something like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo pip install commonmark&#xA;sudo aptitude install texlive-fonts-recommended texlive-lang-cyrillic&#xA;&#xA;git clone https://github.com/minsk-hackerspace/OO.git&#xA;cd OO&#xA;&#xA;make&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Дополнительные замечания юриста:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;В списках учредителей и органов предлагаю исправить все телефоны на установленный правилами делопроизводства формат Х(ХХХ)ХХХХХХХ&lt;/li&gt; &#xA; &lt;li&gt;К вашему названию можно придраться с той точки зрения, что оно не в полной мере содержит указание на характер деятельности, можно добавить Просветительское общественное объединение&lt;/li&gt; &#xA; &lt;li&gt;Пошлину платить сюда: &lt;a href=&#34;http://www.just-minsk.gov.by/ru/society/OO_stavki_gp&#34;&gt;http://www.just-minsk.gov.by/ru/society/OO_stavki_gp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Я бы вам советовала убрать учредителей из Гомеля и Бреста, а потом включить их в члены. На практике Управление юстиции не приветствует включение в список учредителей граждан, проживающих не на территории деятельности объединения&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Остальные комментарии продублированы в файле исходников устава в виде комментов.&lt;/p&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;line 237 - прелогают добавить такой пункт: защита прав и законных интересов, а также представление законных интересов своих членов в государственных органах и иных организациях;&lt;/li&gt; &#xA; &lt;li&gt;line 245 - убрать следующие пункты: в установленном законодательством порядке вступление в местные и международные общественные (неправительственные) объединения и союзы, установление и поддержка прямых международных контактов и связей, заключение для этих целей соответствующих соглашений; получение финансовой и иной поддержки из различных не запрещенных законодательством Республики Беларусь источников; иные методы, не противоречащие законодательству Республики Беларусь.&lt;/li&gt; &#xA; &lt;li&gt;line 271 Вам (т.е. нам) надо поработать над задачами, вернее даже над связкой цели-задачи-методы. К вашей организации могут придраться в том контексте, что из устава не понятно, чем вы собираетесь занимать и для чего создаетесь (это достаточно распространенное явление). Например, стоит добавить что-то типа привлечение молодежи к занятию техническим творчеством, организация их досуга.&lt;/li&gt; &#xA; &lt;li&gt;line 296 В соответствии с Налоговым кодексом Республики Беларусь членские взносы не являются внереализационным доходом (не облагаются налогом на прибыль) в соответствии с размером, установленным в Уставе общественного объединения. Поэтому размер членских и вступительных взносов лучше прописывать в Уставе объединения, хотя бы «от-до»&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Остальные изменения только в виде изменений.&lt;/p&gt;</summary>
  </entry>
</feed>