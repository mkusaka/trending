<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TeX Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-09-04T01:48:32Z</updated>
  <subtitle>Weekly Trending of TeX in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>jacobeisenstein/gt-nlp-class</title>
    <updated>2022-09-04T01:48:32Z</updated>
    <id>tag:github.com,2022-09-04:/jacobeisenstein/gt-nlp-class</id>
    <link href="https://github.com/jacobeisenstein/gt-nlp-class" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Course materials for Georgia Tech CS 4650 and 7650, &#34;Natural Language&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CS 4650 and 7650&lt;/h1&gt; &#xA;&lt;p&gt;(&lt;strong&gt;Note about registration&lt;/strong&gt;: registration is currently restricted to students pursuing CS degrees for which this course is an essential requirement. Unfortunately, the enrollment is already at the limit of the classroom space, so this restriction is unlikely to be lifted.)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Course&lt;/strong&gt;: Natural Language Understanding&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Instructor&lt;/strong&gt;: Jacob Eisenstein&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Semester&lt;/strong&gt;: Spring 2018&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Time&lt;/strong&gt;: Mondays and Wednesdays, 3:00-4:15pm&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;TAs&lt;/strong&gt;: Murali Raghu Babu, James Mullenbach, Yuval Pinter, Zhewei Sun&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1BuvRjPhfHmy7XAfpc5KoygdfqI3Cue3bbmiO6yYuX_E/edit?usp=sharing&#34;&gt;Schedule&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.google.com/document/d/1loefqZhmOaF2mP8yQPEx91jZ7BHylWixVtYlFhpIlGM/edit?usp=sharing&#34;&gt;Recaps&lt;/a&gt; from previous classes&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This course gives an overview of modern data-driven techniques for natural language processing. The course moves from shallow bag-of-words models to richer structural representations of how words interact to create meaning. At each level, we will discuss the salient linguistic phemonena and most successful computational models. Along the way we will cover machine learning techniques which are especially relevant to natural language processing.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jacobeisenstein/gt-nlp-class/master/#readings&#34;&gt;Readings&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jacobeisenstein/gt-nlp-class/master/#grading&#34;&gt;Grading&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jacobeisenstein/gt-nlp-class/master/#help&#34;&gt;Help&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jacobeisenstein/gt-nlp-class/master/#policies&#34;&gt;Policies&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Learning goals&lt;/h1&gt; &#xA;&lt;a name=&#34;learning&#34;&gt;&lt;/a&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Acquire the fundamental linguistic concepts that are relevant to language technology. This goal will be assessed in the short homework assignments and the exams.&lt;/li&gt; &#xA; &lt;li&gt;Analyze and understand state-of-the-art algorithms and statistical techniques for reasoning about linguistic data. This goal will be assessed in the exams and the assigned projects.&lt;/li&gt; &#xA; &lt;li&gt;Implement state-of-the-art algorithms and statistical techniques for reasoning about linguistic data. This goal will be assessed in the assigned projects.&lt;/li&gt; &#xA; &lt;li&gt;Adapt and apply state-of-the-art language technology to new problems and settings. This goal will be assessed in assigned projects.&lt;/li&gt; &#xA; &lt;li&gt;(7650 only) Read and understand current research on natural language processing. This goal will be assessed in assigned projects.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Readings&lt;/h1&gt; &#xA;&lt;a name=&#34;readings&#34;&gt;&lt;/a&gt; &#xA;&lt;p&gt;Readings will be drawn mainly from my &lt;a href=&#34;https://github.com/jacobeisenstein/gt-nlp-class/raw/master/notes&#34;&gt;notes&lt;/a&gt;. Additional readings may be assigned from published papers, blogposts, and tutorials.&lt;/p&gt; &#xA;&lt;h2&gt;Supplemental textbooks&lt;/h2&gt; &#xA;&lt;p&gt;These are completely optional, but might deepen your understanding of the material.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.amazon.com/Speech-Language-Processing-2nd-Edition/dp/0131873210/&#34;&gt;Speech and Language Processing&lt;/a&gt; is the textbook most often used in NLP courses. It&#39;s a great reference for both the linguistics and algorithms we&#39;ll encounter in this course. Several chapters from the upcoming &lt;a href=&#34;https://web.stanford.edu/~jurafsky/slp3/&#34;&gt;third edition&lt;/a&gt; are free online.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.amazon.com/Natural-Language-Processing-Python-Steven/dp/0596516495&#34;&gt;Natural Language Processing with Python&lt;/a&gt; shows how to do hands-on work with Python&#39;s Natural Language Toolkit (NLTK), and also brings a strong linguistic perspective.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.amazon.com/Schaums-Outline-Probability-Statistics-Edition/dp/007179557X/ref=pd_sim_b_1?ie=UTF8&amp;amp;refRID=1R57HWNCW6EEWD1ZRH4C&#34;&gt;Schaum&#39;s Outline of Probability and Statistics&lt;/a&gt; can help you review the probability and statistics that we use in this course.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Grading&lt;/h1&gt; &#xA;&lt;a name=&#34;grading&#34;&gt;&lt;/a&gt; &#xA;&lt;p&gt;The graded material for the course will consist of:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Seven short homework assignments, of which you must do six. Most of these involve performing linguistic annotation on some text of your choice. The purpose is to get a basic understanding of key linguistic concepts. Each assignment should take less than an hour. Each homework is worth 2 points (12 total). (Many of these homeworks are implemented at &lt;strong&gt;quizzes&lt;/strong&gt; on Canvas.)&lt;/li&gt; &#xA; &lt;li&gt;Four assigned problem sets. These involve building and using NLP techniques which are at or near the state-of-the-art. The purpose is to learn how to implement natural language processing software, and to have fun. These assignments must be done individually. Each problem set is worth ten points (48 total). Students enrolled in CS 7650 will have an additional, research-oriented component to the problem sets.&lt;/li&gt; &#xA; &lt;li&gt;An in-class midterm exam, worth 20 points, and a final exam, worth 20 points. The purpose of these exams is to assess understanding of the core theoretical concepts, and to encourage you to review and synthesize your understanding of these concepts.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Barring a personal emergency or an institute-approved absence, you must take each exam on the day indicated in the schedule. Job interviews and travel plans are generally not a reason for an institute-approved absence. See &lt;a href=&#34;https://registrar.gatech.edu/info/institute-approved-absence-form-for-students&#34;&gt;here&lt;/a&gt; for more information on GT policy about absences.&lt;/p&gt; &#xA;&lt;h2&gt;Late policy&lt;/h2&gt; &#xA;&lt;p&gt;Problem sets will be accepted up to 72 hours late, at a penalty of 2 points per 24 hours. (Maximum score after missing the deadline: 10/12; maximum score 24 hours after the deadline: 8/12, etc.) It is usually best just to turn in what you have at the due date. Late homeworks will not be accepted. This late policy is intended to ensure fair and timely evaluation.&lt;/p&gt; &#xA;&lt;h1&gt;Getting help&lt;/h1&gt; &#xA;&lt;a name=&#34;help&#34;&gt;&lt;/a&gt; &#xA;&lt;h2&gt;Office hours&lt;/h2&gt; &#xA;&lt;p&gt;My office hours follow Wednesday classes (4:15-5:15PM) and take place in class when available.&lt;/p&gt; &#xA;&lt;p&gt;TA office hours are in CCB commons (1st floor) unless otherwise announced on Piazza.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Murali: Friday 10AM-11AM&lt;/li&gt; &#xA; &lt;li&gt;James: Thursday 11AM-12PM&lt;/li&gt; &#xA; &lt;li&gt;Yuval: Tuesday 3PM-4PM&lt;/li&gt; &#xA; &lt;li&gt;Zhewei: Monday 1PM-2PM&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Online help&lt;/h2&gt; &#xA;&lt;p&gt;Please use Piazza rather than personal email to ask questions. This helps other students, who may have the same question. Personal emails may not be answered. If you cannot make it to office hours, please use Piazza to make an appointment. It is unlikely that I will be able to chat if you make an unscheduled visit to my office. The same is true for the TAs.&lt;/p&gt; &#xA;&lt;h1&gt;Class policies&lt;/h1&gt; &#xA;&lt;a name=&#34;policies&#34;&gt;&lt;/a&gt; &#xA;&lt;p&gt;Attendance will not be taken, but &lt;strong&gt;you are responsible for knowing what happens in every class&lt;/strong&gt;. If you cannot attend class, make sure you check up with someone who was there.&lt;/p&gt; &#xA;&lt;p&gt;Respect your classmates and your instructor by preventing distractions. This means be on time, turn off your cellphone, and save side conversations for after class. If you can&#39;t read something I wrote on the board, or if you think I made a mistake in a derivation, please raise your hand and tell me!&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Using a laptop in class is likely to reduce your education attainment&lt;/strong&gt;. This has been documented by multiple studies, which are nicely summarized in the following article:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.nytimes.com/2017/11/22/business/laptops-not-during-lecture-or-meeting.html&#34;&gt;https://www.nytimes.com/2017/11/22/business/laptops-not-during-lecture-or-meeting.html&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;I am not going to ban laptops, as long as they are not a distraction to anyone but the user. But I suggest you try pen and paper for a few weeks, and see if it helps.&lt;/p&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;a name=&#34;prerequisites&#34;&gt;&lt;/a&gt; &#xA;&lt;p&gt;The official prerequisite for CS 4650 is CS 3510/3511, &#34;Design and Analysis of Algorithms.&#34; This prerequisite is essential because understanding natural language processing algorithms requires familiarity with dynamic programming, as well as automata and formal language theory: finite-state and context-free languages, NP-completeness, etc. While course prerequisites are not enforced for graduate students, prior exposure to analysis of algorithms is very strongly recommended.&lt;/p&gt; &#xA;&lt;p&gt;Furthermore, this course assumes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Good coding ability, corresponding to at least a third or fourth-year undergraduate CS major. Assignments will be in Python.&lt;/li&gt; &#xA; &lt;li&gt;Background in basic probability, linear algebra, and calculus.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;People sometimes want to take the course without having all of these prerequisites. Frequent cases are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Junior CS students with strong programming skills but limited theoretical and mathematical background,&lt;/li&gt; &#xA; &lt;li&gt;Non-CS students with strong mathematical background but limited programming experience.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Students in the first group suffer in the exam and don&#39;t understand the lectures, and students in the second group suffer in the problem sets. My advice is to get the background material first, and then take this course.&lt;/p&gt; &#xA;&lt;h2&gt;Collaboration policy&lt;/h2&gt; &#xA;&lt;p&gt;One of the goals of the assigned work is to assess your individual progress in meeting the learning objectives of the course. You may discuss the homework and projects with other students, but your work must be your own -- particularly all coding and writing. For example:&lt;/p&gt; &#xA;&lt;h3&gt;Examples of acceptable collaboration&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Alice and Bob discuss alternatives for storing large, sparse vectors of feature counts, as required by a problem set.&lt;/li&gt; &#xA; &lt;li&gt;Bob is confused about how to implement the Viterbi algorithm, and asks Alice for a conceptual description of her strategy.&lt;/li&gt; &#xA; &lt;li&gt;Alice asks Bob if he encountered a failure condition at a &#34;sanity check&#34; in a coding assignment, and Bob explains at a conceptual level how he overcame that failure condition.&lt;/li&gt; &#xA; &lt;li&gt;Alice is having trouble getting adequate performance from her part-of-speech tagger. She finds a blog page or research paper that gives her some new ideas, which she implements.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Examples of unacceptable collaboration&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Alice and Bob work together to write code for storing feature counts.&lt;/li&gt; &#xA; &lt;li&gt;Alice and Bob divide the assignment into parts, and each write the code for their part, and then share their solutions with each other to complete the assignment.&lt;/li&gt; &#xA; &lt;li&gt;Alice or Bob obtain a solution to a previous year&#39;s assignment or to a related assignment in another class, and use it as the starting point for their own solutions.&lt;/li&gt; &#xA; &lt;li&gt;Bob is having trouble getting adequate performance from his part-of-speech tagger. He finds source code online, and copies it into his own submission.&lt;/li&gt; &#xA; &lt;li&gt;Alice wants to win the Kaggle competition for a problem set. She finds the test set online, and customizes her submission to do well on it.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Some assignments will involve written responses. Using other people’s text or figures without attribution is plagiarism, and is never acceptable.&lt;/p&gt; &#xA;&lt;p&gt;Suspected cases of academic misconduct will be (and have been!) referred to the Honor Advisory Council. For any questions involving these or any other Academic Honor Code issues, please consult me, my teaching assistants, or &lt;a href=&#34;http://www.honor.gatech.edu&#34;&gt;http://www.honor.gatech.edu&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>weihaox/awesome-gan-inversion</title>
    <updated>2022-09-04T01:48:32Z</updated>
    <id>tag:github.com,2022-09-04:/weihaox/awesome-gan-inversion</id>
    <link href="https://github.com/weihaox/awesome-gan-inversion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A collection of resources on GAN inversion.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/sindresorhus/awesome&#34;&gt;&lt;img src=&#34;https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg?sanitize=true&#34; alt=&#34;Awesome&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/Naereen/StrapDown.js/graphs/commit-activity&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Maintained%3F-yes-green.svg?sanitize=true&#34; alt=&#34;Maintenance&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://makeapullrequest.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat&#34; alt=&#34;PR&#39;s Welcome&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://visitor-badge.glitch.me/badge?style=flat-square&amp;amp;page_id=weihaox.awesome-gan-inversion&#34; alt=&#34;visitors&#34;&gt;&lt;/p&gt; &#xA;&lt;!-- ![visitors](https://visitor-badge.glitch.me/badge?style=flat-square&amp;page_id=weihaox)  --&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;h1 align=&#34;center&#34;&gt;GAN Inversion: A Survey&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; TPAMI, 2022 &lt;br&gt; &lt;a href=&#34;https://xiaweihao.com/&#34;&gt;&lt;strong&gt;Weihao Xia&lt;/strong&gt;&lt;/a&gt; · &lt;a href=&#34;https://yulunzhang.com/&#34;&gt;&lt;strong&gt;Yulun Zhang&lt;/strong&gt;&lt;/a&gt; · &lt;a href=&#34;https://sites.google.com/view/iigroup-thu/about&#34;&gt;&lt;strong&gt;Yujiu Yang&lt;/strong&gt;&lt;/a&gt; · &lt;a href=&#34;http://www.homepages.ucl.ac.uk/~ucakjxu/&#34;&gt;&lt;strong&gt;Jing-Hao Xue&lt;/strong&gt;&lt;/a&gt; · &lt;a href=&#34;https://boleizhou.github.io/&#34;&gt;&lt;strong&gt;Bolei Zhou&lt;/strong&gt;&lt;/a&gt; · &lt;a href=&#34;https://faculty.ucmerced.edu/mhyang/&#34;&gt;&lt;strong&gt;Ming-Hsuan Yang&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://arxiv.org/abs/2101.05278&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Paper-PDF-green?style=flat&amp;amp;logo=arXiv&amp;amp;logoColor=green&#34; alt=&#34;arXiv PDF&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/weihaox/awesome-gan-inversion&#34; style=&#34;padding-left: 0.5rem;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Project-Page-blue?style=flat&amp;amp;logo=Google%20chrome&amp;amp;logoColor=blue&#34; alt=&#34;Project Page&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://ieeexplore.ieee.org/document/9792208&#34; style=&#34;padding-left: 0.5rem;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/TPAMI-PDF-red?style=flat&amp;amp;logoColor=red&#34; alt=&#34;TPAMI PDF&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;This repo is a collection of resources on GAN inversion, as a supplement for our &lt;a href=&#34;https://arxiv.org/abs/2101.05278&#34;&gt;survey&lt;/a&gt;. If you find any work missing or have any suggestions (papers, implementations and other resources), feel free to &lt;a href=&#34;https://github.com/weihaox/awesome-gan-inversion/pulls&#34;&gt;pull requests&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;details style=&#34;margin-left:3%;&#34;&gt; &#xA; &lt;summary&gt;citation&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bib&#34; style=&#34;font-size: 0.9rem;&#34; id=&#34;citation&#34;&gt;@article{xia2022gan,&#xA;    author  = {Xia, Weihao and Zhang, Yulun and Yang, Yujiu and Xue, Jing-Hao and Zhou, Bolei and Yang, Ming-Hsuan},&#xA;    title   = {GAN Inversion: A Survey},&#xA;    journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},&#xA;    year={2022}&#xA;  }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Table of Contents&lt;/summary&gt;&#xA; &lt;p&gt; &lt;/p&gt;&#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/weihaox/awesome-gan-inversion/main/#inverted-pretrained-model&#34;&gt;inverted pretrained model&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/weihaox/awesome-gan-inversion/main/#inversion-method&#34;&gt;inversion method&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/weihaox/awesome-gan-inversion/main/#latent-space-navigation&#34;&gt;latent space navigation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/weihaox/awesome-gan-inversion/main/#application&#34;&gt;application&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/weihaox/awesome-gan-inversion/main/#acknowledgement&#34;&gt;acknowledgement&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;&lt;/p&gt;&#xA;&lt;/details&gt;&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h2&gt;inverted pretrained model&lt;/h2&gt; &#xA;&lt;h3&gt;2D GANs&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleGAN-XL: Scaling StyleGAN to Large Diverse Datasets.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://axelsauer.com/&#34;&gt;Axel Sauer&lt;/a&gt;, &lt;a href=&#34;https://katjaschwarz.github.io/&#34;&gt;Katja Schwarz&lt;/a&gt;, &lt;a href=&#34;http://www.cvlibs.net/&#34;&gt;Andreas Geiger&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; SIGGRAPH 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.00273&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://sites.google.com/view/stylegan-xl/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/autonomousvision/stylegan_xl&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Self-Distilled StyleGAN: Towards Generation from Internet Photos.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://rmokady.github.io/&#34;&gt;Ron Mokady&lt;/a&gt;, Michal Yarom, Omer Tov, Oran Lang, Daniel Cohen-Or, Tali Dekel, Michal Irani, Inbar Mosseri.&lt;/em&gt;&lt;br&gt; SIGGRAPH 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.12211&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://self-distilled-stylegan.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/self-distilled-stylegan/self-distilled-internet-photos&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Ensembling Off-the-shelf Models for GAN Training.&lt;/strong&gt;&lt;br&gt; &lt;a href=&#34;https://nupurkmr9.github.io/&#34;&gt;Nupur Kumari&lt;/a&gt;, &lt;a href=&#34;https://richzhang.github.io/&#34;&gt;Richard Zhang&lt;/a&gt;, &lt;a href=&#34;https://research.adobe.com/person/eli-shechtman/&#34;&gt;Eli Shechtman&lt;/a&gt;, &lt;a href=&#34;https://www.cs.cmu.edu/~junyanz/&#34;&gt;Jun-Yan Zhu&lt;/a&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/pdf/2112.09130.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://www.cs.cmu.edu/~vision-aided-gan/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/nupurkmr9/vision-aided-gan&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleGAN3: Alias-Free Generative Adversarial Networks.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tero Karras, Miika Aittala, Samuli Laine, Erik Härkönen, Janne Hellsten, Jaakko Lehtinen, Timo Aila.&lt;/em&gt;&lt;br&gt; NeurIPS 2021. [&lt;a href=&#34;https://arxiv.org/abs/2106.12423&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://nvlabs.github.io/alias-free-gan&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/NVlabs/stylegan3&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://github.com/rosinality/alias-free-gan-pytorch&#34;&gt;Rosinality&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleGAN2-Ada: Training Generative Adversarial Networks with Limited Data.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, Timo Aila.&lt;/em&gt;&lt;br&gt; NeurIPS 2020. [&lt;a href=&#34;https://arxiv.org/abs/2006.06676&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/NVlabs/stylegan2-ada&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://github.com/woctezuma/steam-stylegan2-ada&#34;&gt;Steam StyleGAN2-ADA&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleGAN2: Analyzing and Improving the Image Quality of StyleGAN.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://research.nvidia.com/person/tero-karras&#34;&gt;Tero Karras&lt;/a&gt;, &lt;a href=&#34;https://research.nvidia.com/person/samuli-laine&#34;&gt;Samuli Laine&lt;/a&gt;, &lt;a href=&#34;https://research.nvidia.com/person/miika-aittala&#34;&gt;Miika Aittala&lt;/a&gt;, Janne Hellsten, Jaakko Lehtinen, &lt;a href=&#34;https://research.nvidia.com/person/timo-aila&#34;&gt;Timo Aila&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2020. [&lt;a href=&#34;https://arxiv.org/abs/1912.04958&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/rosinality/stylegan2-pytorch&#34;&gt;PyTorch&lt;/a&gt;] [&lt;a href=&#34;https://github.com/NVlabs/stylegan2&#34;&gt;Offical TF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/manicman1999/StyleGAN2-Tensorflow-2.0&#34;&gt;Unoffical Tensorflow 2.0&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleGAN: A Style-Based Generator Architecture for Generative Adversarial Networks.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tero Karras, Samuli Laine, Timo Aila.&lt;/em&gt;&lt;br&gt; CVPR 2019. [&lt;a href=&#34;https://arxiv.org/abs/1812.04948&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/NVlabs/stylegan&#34;&gt;Offical TF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ProGAN: Progressive Growing of GANs for Improved Quality, Stability, and Variation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen.&lt;/em&gt;&lt;br&gt; ICLR 2018. [&lt;a href=&#34;https://arxiv.org/abs/1710.10196&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/tkarras/progressive_growing_of_gans&#34;&gt;Offical TF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h3&gt;3D-aware GANs&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;EG3D: Efficient Geometry-aware 3D Generative Adversarial Networks.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://ericryanchan.github.io/&#34;&gt;Eric R. Chan&lt;/a&gt;, &lt;a href=&#34;https://connorzlin.com/&#34;&gt;Connor Z. Lin&lt;/a&gt;, &lt;a href=&#34;https://matthew-a-chan.github.io/&#34;&gt;Matthew A. Chan&lt;/a&gt;, &lt;a href=&#34;https://luminohope.org/&#34;&gt;Koki Nagano&lt;/a&gt;, &lt;a href=&#34;https://cs.stanford.edu/~bxpan/&#34;&gt;Boxiao Pan&lt;/a&gt;, &lt;a href=&#34;https://research.nvidia.com/person/shalini-gupta&#34;&gt;Shalini De Mello&lt;/a&gt;, &lt;a href=&#34;https://oraziogallo.github.io/&#34;&gt;Orazio Gallo&lt;/a&gt;, &lt;a href=&#34;https://geometry.stanford.edu/member/guibas/&#34;&gt;Leonidas Guibas&lt;/a&gt;, &lt;a href=&#34;https://research.nvidia.com/person/jonathan-tremblay&#34;&gt;Jonathan Tremblay&lt;/a&gt;, &lt;a href=&#34;https://www.samehkhamis.com/&#34;&gt;Sameh Khamis&lt;/a&gt;, &lt;a href=&#34;https://research.nvidia.com/person/tero-karras&#34;&gt;Tero Karras&lt;/a&gt;, &lt;a href=&#34;https://stanford.edu/~gordonwz/&#34;&gt;Gordon Wetzstein&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2112.07945&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://matthew-a-chan.github.io/EG3D&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/NVlabs/eg3d&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleNeRF: A Style-based 3D-Aware Generator for High-resolution Image Synthesis.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;http://jiataogu.me/&#34;&gt;Jiatao Gu&lt;/a&gt;, &lt;a href=&#34;https://lingjie0206.github.io/&#34;&gt;Lingjie Liu&lt;/a&gt;, &lt;a href=&#34;https://totoro97.github.io/about.html&#34;&gt;Peng Wang&lt;/a&gt;, &lt;a href=&#34;http://people.mpi-inf.mpg.de/~theobalt/&#34;&gt;Christian Theobalt&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2110.08985&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;http://jiataogu.me/style_nerf/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleSDF: High-Resolution 3D-Consistent Image and Geometry Generation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://homes.cs.washington.edu/~royorel/&#34;&gt;Roy Or-El&lt;/a&gt;, &lt;a href=&#34;https://roxanneluo.github.io/&#34;&gt;Xuan Luo&lt;/a&gt;, Mengyi Shan, Eli Shechtman, Jeong Joon Park, Ira Kemelmacher-Shlizerman.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2112.11427&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://stylesdf.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/royorel/StyleSDF&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://ericryanchan.github.io/&#34;&gt;Eric R. Chan&lt;/a&gt;, &lt;a href=&#34;https://marcoamonteiro.github.io/pi-GAN-website/&#34;&gt;Marco Monteiro&lt;/a&gt;, &lt;a href=&#34;https://kellnhofer.xyz/&#34;&gt;Petr Kellnhofer&lt;/a&gt;, &lt;a href=&#34;https://jiajunwu.com/&#34;&gt;Jiajun Wu&lt;/a&gt;, &lt;a href=&#34;https://stanford.edu/~gordonwz/&#34;&gt;Gordon Wetzstein&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2012.00926&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://marcoamonteiro.github.io/pi-GAN-website/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/lucidrains/pi-GAN-pytorch&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h2&gt;inversion method&lt;/h2&gt; &#xA;&lt;p&gt;This part contatins generatal inversion methods, while methods in the next &lt;em&gt;application&lt;/em&gt; part are mainly designed for specific tasks.&lt;/p&gt; &#xA;&lt;h3&gt;2D GAN inversion&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Chunkmogrify: Real image inversion via Segments.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://dcgi.fel.cvut.cz/people/futscdav&#34;&gt;David Futschik&lt;/a&gt;, &lt;a href=&#34;https://research.adobe.com/person/michal-lukac/&#34;&gt;Michal Lukáč&lt;/a&gt;, &lt;a href=&#34;https://research.adobe.com/person/eli-shechtman/&#34;&gt;Eli Shechtman&lt;/a&gt;, &lt;a href=&#34;https://dcgi.fel.cvut.cz/home/sykorad/&#34;&gt;Daniel Sýkora&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; ECCV 2022. [&lt;a href=&#34;https://arxiv.org/abs/2110.06269&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/futscdav/Chunkmogrify&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Third Time&#39;s the Charm? Image and Video Editing with StyleGAN3.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuval Alaluf, Or Patashnik, Zongze Wu, Asif Zamir, Eli Shechtman, Dani Lischinski, Daniel Cohen-Or.&lt;/em&gt;&lt;br&gt; Advances in Image Manipulation Workshop, ECCV 2022. [&lt;a href=&#34;https://arxiv.org/abs/2201.13433&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://yuval-alaluf.github.io/stylegan3-editing/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yuval-alaluf/stylegan3-editing&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Cycle Encoding of a StyleGAN Encoder for Improved Reconstruction and Editability.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Xudong Mao, Liujuan Cao, Aurele Tohokantche Gnanha, Zhenguo Yang, Qing Li, Rongrong Ji.&lt;/em&gt;&lt;br&gt; ACM MM 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.09367&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/xudonmao/CycleEncoding&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spatially-Adaptive Multilayer Selection for GAN Inversion and Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://gauravparmar.com/&#34;&gt;Gaurav Parmar&lt;/a&gt;, &lt;a href=&#34;https://yijunmaverick.github.io/&#34;&gt;Yijun Li&lt;/a&gt;, &lt;a href=&#34;https://research.adobe.com/person/jingwan-lu/&#34;&gt;Jingwan Lu&lt;/a&gt;, &lt;a href=&#34;http://richzhang.github.io/&#34;&gt;Richard Zhang&lt;/a&gt;, &lt;a href=&#34;https://www.cs.cmu.edu/~junyanz/&#34;&gt;Jun-Yan Zhu&lt;/a&gt;, &lt;a href=&#34;http://krsingh.cs.ucdavis.edu/&#34;&gt;Krishna Kumar Singh&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/papers/Parmar_Spatially-Adaptive_Multilayer_Selection_for_GAN_Inversion_and_Editing_CVPR_2022_paper.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://www.cs.cmu.edu/~SAMInversion&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/adobe-research/sam_inversion&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Style Transformer for Image Inversion and Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Xueqi Hu, Qiusheng Huang, Zhengyi Shi, Siyuan Li, Changxin Gao, Li Sun, Qingli Li.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.07932&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/sapphire497/style-transformer&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High-Fidelity GAN Inversion for Image Attribute Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://tengfei-wang.github.io&#34;&gt;Tengfei Wang&lt;/a&gt;, Yong Zhang, Yanbo Fan, Jue Wang, Qifeng Chen.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2109.06590&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://tengfei-wang.github.io/HFGI/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Tengfei-Wang/HFGI&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HyperInverter: Improving StyleGAN Inversion via Hypernetwork.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://di-mi-ta.github.io/&#34;&gt;Tan M. Dinh&lt;/a&gt;, &lt;a href=&#34;https://sites.google.com/site/anhttranusc/&#34;&gt;Anh Tuan Tran&lt;/a&gt;, &lt;a href=&#34;https://sites.google.com/site/rangmanhonguyen/&#34;&gt;Rang Nguyen&lt;/a&gt;, &lt;a href=&#34;https://sonhua.github.io/&#34;&gt;Binh-Son Hua&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2112.00719&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://di-mi-ta.github.io/HyperInverter/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HyperStyle: StyleGAN Inversion with HyperNetworks for Real Image Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuval Alaluf, Omer Tov, Ron Mokady, Rinon Gal, Amit H. Bermano.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2111.15666&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;http://yuval-alaluf.github.io/hyperstyle/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yuval-alaluf/hyperstyle&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Overparameterization Improves StyleGAN Inversion.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yohan Poirier-Ginter, Alexandre Lessard, Ryan Smith, Jean-François Lalonde.&lt;/em&gt;&lt;br&gt; CVPR 2022 Workshop on AI for Content Creation. [&lt;a href=&#34;https://arxiv.org/abs/2205.06304&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://lvsn.github.io/OverparamStyleGAN/&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleAlign: Analysis and Applications of Aligned StyleGAN Models.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Zongze Wu, Yotam Nitzan, Eli Shechtman, Dani Lischinski.&lt;/em&gt;&lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2110.11323&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Semantic Unfolding of StyleGAN Latent Space.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Mustafa Shukor, Xu Yao, Bharath Bushan Damodaran, Pierre Hellier.&lt;/em&gt;&lt;br&gt; ICIP 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.14892&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Video2StyleGAN: Encoding Video in Latent Space for Manipulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jiyang Yu, Jingen Liu, Jing Huang, Wei Zhang, Tao Mei.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.13078&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Editing Out-of-domain GAN Inversion via Differential Activations.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Haorui Song, Yong Du, Tianyi Xiang, Junyu Dong, Jing Qin, Shengfeng He&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.08134&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/HaoruiSong622/Editing-Out-of-Domain&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Expanding the Latent Space of StyleGAN for Real Face Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yin Yu, Ghasedi Kamran, Wu HsiangTao, Yang Jiaolong, Tong Xi, Fu Yun.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2204.12530&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Encode-in-Style: Latent-based Video Encoding using StyleGAN2.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://trevineoorloff.github.io/&#34;&gt;Trevine Oorloff&lt;/a&gt;, &lt;a href=&#34;https://www.umiacs.umd.edu/people/yaser&#34;&gt;Yaser Yacoob&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.14512&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://trevineoorloff.github.io/Encode-in-Style.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/trevineoorloff/Encode-in-Style&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://trevineoorloff.github.io/Encode-in-Style.io/&#34;&gt;Data&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High-fidelity GAN Inversion with Padding Space.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://ezioby.github.io/padinv/&#34;&gt;Qingyan Bai&lt;/a&gt;, Yinghao Xu, Jiapeng Zhu, Weihao Xia, Yujiu Yang, Yujun Shen.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.11105&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/EzioBy/padinv&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/EzioBy/padinv&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Solving Inverse Problems with NerfGANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Giannis Daras, Wen-Sheng Chu, Abhishek Kumar, Dmitry Lagun, Alexandros G. Dimakis.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2112.09061&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Feature-Style Encoder for Style-Based GAN Inversion.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Xu Yao, Alasdair Newson, Yann Gousseau, Pierre Hellier.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.02183&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/InterDigitalInc/FeatureStyleEncoder&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleGAN of All Trades: Image Manipulation with Only Pretrained StyleGAN.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Min Jin Chong, Hsin-Ying Lee, David Forsyth.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2111.01619&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mchong6/SOAT&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Semantic and Geometric Unfolding of StyleGAN Latent Space.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Mustafa Shukor, Xu Yao, Bharath Bhushan Damodaran, Pierre Hellier.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2107.04481&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pivotal Tuning for Latent-based Editing of Real Images.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Daniel Roich, Ron Mokady, Amit H. Bermano, Daniel Cohen-Or.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/pdf/2106.05744.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/danielroich/PTI&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Transforming the Latent Space of StyleGAN for Real Face Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Heyi Li, Jinlong Liu, Yunzhi Bai, Huayan Wang, Klaus Mueller.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2105.14230&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/AnonSubm2021/TransStyleGAN&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;E2Style: Improve the Efficiency and Effectiveness of StyleGAN Inversion.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tianyi Wei, Dongdong Chen, Wenbo Zhou, Jing Liao, Weiming Zhang, Lu Yuan, Gang Hua, Nenghai Yu.&lt;/em&gt;&lt;br&gt; TIP 2022. [&lt;a href=&#34;https://arxiv.org/abs/2104.07661&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://wty-ustc.github.io/inversion/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/wty-ustc/StyleGAN-Inversion-Baseline&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GAN-Control: Explicitly Controllable GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Alon Shoshan, Nadav Bhonker, Igor Kviatkovsky, Gerard Medioni.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2101.02477&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improved StyleGAN Embedding: Where are the Good Latents?&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://github.com/ZPdesu&#34;&gt;Peihao Zhu&lt;/a&gt;, &lt;a href=&#34;https://github.com/RameenAbdal&#34;&gt;Rameen Abdal&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=ojgWPpgAAAAJ&amp;amp;hl=en&#34;&gt;Yipeng Qin&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=rS1xJIIAAAAJ&amp;amp;hl=en&#34;&gt;John Femiani&lt;/a&gt;, &lt;a href=&#34;http://peterwonka.net/&#34;&gt;Peter Wonka&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; arxiv 2020. [&lt;a href=&#34;https://arxiv.org/abs/2012.09036&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ZPdesu/II2S&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning a Deep Reinforcement Learning Policy Over the Latent Space of a Pre-trained GAN for Semantic Age Manipulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Kumar Shubham, Gopalakrishnan Venkatesh, Reijul Sachdev, Akshi, Dinesh Babu Jayagopi, G. Srinivasaraghavan.&lt;/em&gt;&lt;br&gt; arxiv 2020. [&lt;a href=&#34;https://arxiv.org/abs/2011.00954&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Augmentation-Interpolative AutoEncoders for Unsupervised Few-Shot Image Generation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Davis Wertheimer, Omid Poursaeed, Bharath Hariharan.&lt;/em&gt;&lt;br&gt; arxiv 2020. [&lt;a href=&#34;https://arxiv.org/abs/2011.13026&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised Discovery of Disentangled Manifolds in GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yu-Ding Lu, Hsin-Ying Lee, Hung-Yu Tseng, Ming-Hsuan Yang.&lt;/em&gt;&lt;br&gt; arxiv 2020. &lt;a href=&#34;https://arxiv.org/abs/2011.11842&#34;&gt;[PDF]&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Collaborative Learning for Faster StyleGAN Embedding.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Shanyan Guan, &lt;a href=&#34;https://tyshiwo.github.io/&#34;&gt;Ying Tai&lt;/a&gt;, Bingbing Ni, Feida Zhu, Feiyue Huang, Xiaokang Yang.&lt;/em&gt;&lt;br&gt; arxiv 2020. [&lt;a href=&#34;https://arxiv.org/abs/2007.01758&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improving Inversion and Generation Diversity in StyleGAN using a Gaussianized Latent Space.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;http://people.csail.mit.edu/jwulff/&#34;&gt;Jonas Wulff&lt;/a&gt;, Antonio Torralba.&lt;/em&gt;&lt;br&gt; arxiv 2020. [&lt;a href=&#34;https://arxiv.org/abs/2009.06529&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;From Continuity to Editability: Inverting GANs with Consecutive Images.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://qingyang-xu.github.io/&#34;&gt;Yangyang Xu&lt;/a&gt;, &lt;a href=&#34;https://www.csyongdu.com/&#34;&gt;Yong Du&lt;/a&gt;, Wenpeng Xiao, Xuemiao Xu and &lt;a href=&#34;https://raw.githubusercontent.com/weihaox/awesome-gan-inversion/main/hengfenghe.com&#34;&gt;Shengfeng He&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://arxiv.org/abs/2107.13812&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Qingyang-Xu/InvertingGANs_with_ConsecutiveImgs&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Explaining in Style: Training a GAN to explain a classifier in StyleSpace.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Oran Lang, Yossi Gandelsman, Michal Yarom, Yoav Wald, Gal Elidan, Avinatan Hassidim, William T. Freeman, Phillip Isola, Amir Globerson, Michal Irani, Inbar Mosseri.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://arxiv.org/abs/2104.13369&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://explaining-in-style.github.io/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;BDInvert: GAN Inversion for Out-of-Range Images with Geometric Transformations.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://kkang831.github.io/&#34;&gt;Kyoungkook Kang&lt;/a&gt;, Seongtae Kim, &lt;a href=&#34;https://www.scho.pe.kr/&#34;&gt;Sunghyun Cho&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://arxiv.org/abs/2108.08998&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://kkang831.github.io/publication/ICCV_2021_BDInvert/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ReStyle: A Residual-Based StyleGAN Encoder via Iterative Refinement.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://yuval-alaluf.github.io/&#34;&gt;Yuval Alaluf&lt;/a&gt;, &lt;a href=&#34;https://orpatashnik.github.io/&#34;&gt;Or Patashnik&lt;/a&gt;, &lt;a href=&#34;https://www.cs.tau.ac.il/~dcor/&#34;&gt;Daniel Cohen-Or&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://arxiv.org/abs/2104.02699&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://yuval-alaluf.github.io/restyle-encoder/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yuval-alaluf/restyle-encoder&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LatentCLR: A Contrastive Learning Approach for Unsupervised Discovery of Interpretable Directions.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Oğuz Kaan Yüksel, &lt;a href=&#34;https://enis.dev&#34;&gt;Enis Simsar&lt;/a&gt;, Ezgi Gülperi Er, Pinar Yanardag.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://arxiv.org/abs/2104.00820&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/catlab-team/latentclr&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Lifting 2D StyleGAN for 3D-Aware Face Generation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://seasonsh.github.io/&#34;&gt;Yichun Shi&lt;/a&gt;, Divyansh Aggarwal, &lt;a href=&#34;http://www.cse.msu.edu/~jain/&#34;&gt;Anil K. Jain&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2011.13126&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Ensembling with Deep Generative Views.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Lucy Chai, Jun-Yan Zhu, Eli Shechtman, Phillip Isola, Richard Zhang.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2104.14551&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/chail/gan-ensembling&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://chail.github.io/gan-ensembling/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Navigating the GAN Parameter Space for Semantic Image Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Anton Cherepkov, Andrey Voynov, Artem Babenko.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2011.13786&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yandex-research/navigan&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleSpace Analysis: Disentangled Controls for StyleGAN Image Generation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Zongze Wu, Dani Lischinski, Eli Shechtman.&lt;/em&gt;&lt;br&gt; CVPR 2021 (oral). &lt;a href=&#34;https://arxiv.org/abs/2011.12799&#34;&gt;[PDF]&lt;/a&gt; [&lt;a href=&#34;https://github.com/betterze/StyleSpace&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, Daniel Cohen-Or.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2008.00951&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/eladrich/pixel2style2pixel&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/weihaox/awesome-gan-inversion/main/eladrich.github.io/pixel2style2pixel/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GHFeat: Generative Hierarchical Features from Synthesizing Images.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yinghao Xu, Yujun Shen, Jiapeng Zhu, Ceyuan Yang, Bolei Zhou.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/pdf/2007.10379.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/genforce/ghfeat&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://genforce.github.io/ghfeat/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Hijack-GAN: Unintended-Use of Pretrained, Black-Box GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Hui-Po Wang, Ning Yu, Mario Fritz.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2011.14107&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prior Image-Constrained Reconstruction using Style-Based Generative Models.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Varun A Kelkar, Mark Anastasio.&lt;/em&gt;&lt;br&gt; ICML 2021. [&lt;a href=&#34;https://arxiv.org/pdf/2102.12525.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Intermediate Layer Optimization for Inverse Problems using Deep Generative Models.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Giannis Daras, Joseph Dean, Ajil Jalal, Alexandros G. Dimakis.&lt;/em&gt;&lt;br&gt; ICML 2021. [&lt;a href=&#34;https://arxiv.org/abs/2102.07364&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/giannisdaras/ilo&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Using Latent Space Regression to Analyze and Leverage Compositionality in GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;http://people.csail.mit.edu/lrchai/&#34;&gt;Lucy Chai&lt;/a&gt;, &lt;a href=&#34;http://people.csail.mit.edu/jwulff/&#34;&gt;Jonas Wulff&lt;/a&gt;, &lt;a href=&#34;http://web.mit.edu/phillipi/&#34;&gt;Phillip Isola&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; ICLR 2021. [&lt;a href=&#34;https://openreview.net/pdf?id=sjuuTm4vj0&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/chail/latent-composition&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://chail.github.io/latent-composition/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://colab.research.google.com/drive/1p-L2dPMaqMyr56TYoYmBJhoyIyBJ7lzH?usp=sharing&#34;&gt;Colab&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Enjoy Your Editing: Controllable GANs for Image Editing via Latent Space Navigation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Peiye Zhuang, Oluwasanmi Koyejo, Alexander G. Schwing.&lt;/em&gt;&lt;br&gt; ICLR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2102.01187&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Disentangled Face Attribute Editing via Instance-Aware Latent Space Search.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuxuan Han, &lt;a href=&#34;http://jlyang.org/&#34;&gt;Jiaolong Yang&lt;/a&gt;, Ying Fu.&lt;/em&gt;&lt;br&gt; IJCAI 2021. [&lt;a href=&#34;https://arxiv.org/abs/2105.12660&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yxuhan/IALS&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High Fidelity GAN Inversion via Prior Multi-Subspace Feature Composition.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Guanyue Li, Qianfen Jiao, Sheng Qian, Si Wu, Hau-San Wong.&lt;/em&gt;&lt;br&gt; AAAI 2021. [&lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/view/17017&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;e4e: Designing an Encoder for StyleGAN Image Manipulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://yotamnitzan.github.io/&#34;&gt;Omer Tov&lt;/a&gt;, Yuval Alaluf, Yotam Nitzan, Or Patashnik, Daniel Cohen-Or.&lt;/em&gt;&lt;br&gt; TOG 2021. [&lt;a href=&#34;https://arxiv.org/abs/2102.02766&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/omertov/encoder4editing&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleFlow: Attribute-conditioned Exploration of StyleGAN-Generated Images using Conditional Continuous Normalizing Flows.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Rameen Abdal, Peihao Zhu, Niloy Mitra, Peter Wonka.&lt;/em&gt;&lt;br&gt; TOG 2021. [&lt;a href=&#34;https://arxiv.org/abs/2008.02401&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://rameenabdal.github.io/StyleFlow&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Mask-Guided Discovery of Semantic Manifolds in Generative Models.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://mengyu.page/&#34;&gt;Mengyu Yang&lt;/a&gt;, &lt;a href=&#34;https://www.cdtps.utoronto.ca/people/directories/all-faculty/david-rokeby&#34;&gt;David Rokeby&lt;/a&gt;, &lt;a href=&#34;https://wxs.ca/&#34;&gt;Xavier Snelgrove&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; NeurIPS 2020 Workshop on Machine Learning for Creativity and Design. [&lt;a href=&#34;https://github.com/bmolab/masked-gan-manifold/raw/main/masked-gan-manifold.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/bmolab/masked-gan-manifold&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Face Identity Disentanglement via Latent Space Mapping.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://yotamnitzan.github.io/&#34;&gt;Yotam Nitzan&lt;/a&gt;, &lt;a href=&#34;https://www.cs.tau.ac.il/~amberman/&#34;&gt;Amit Bermano&lt;/a&gt;, &lt;a href=&#34;https://yangyan.li/&#34;&gt;Yangyan Li&lt;/a&gt;, Daniel Cohen-Or.&lt;/em&gt;&lt;br&gt; SIGGRAPH ASIA 2020. [&lt;a href=&#34;https://arxiv.org/abs/2005.07728&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://yotamnitzan.github.io/ID-disentanglement/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/YotamNitzan/ID-disentanglement&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PIE: Portrait Image Embedding for Semantic Control.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;http://people.mpi-inf.mpg.de/~atewari/&#34;&gt;A. Tewari&lt;/a&gt;, M. Elgharib, M. BR, F. Bernard, H-P. Seidel, P. P‌érez, M. Zollhöfer, C.Theobalt.&lt;/em&gt;&lt;br&gt; TOG 2020. [&lt;a href=&#34;http://gvv.mpi-inf.mpg.de/projects/PIE/data/paper.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;http://gvv.mpi-inf.mpg.de/projects/PIE/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Understanding the Role of Individual Units in a Deep Neural Network.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;David Bau, Jun-Yan Zhu, Hendrik Strobelt, Agata Lapedriza, Bolei Zhou, Antonio Torralba.&lt;/em&gt;&lt;br&gt; National Academy of Sciences 2020. [&lt;a href=&#34;https://arxiv.org/abs/2009.05041&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/davidbau/dissect/&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://dissect.csail.mit.edu/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Face Identity Disentanglement via Latent Space Mapping.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yotam Nitzan, Amit Bermano, Yangyan Li, Daniel Cohen-Or.&lt;/em&gt;&lt;br&gt; TOG 2020. [&lt;a href=&#34;https://arxiv.org/abs/2005.07728&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/YotamNitzan/ID-disentanglement&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Transforming and Projecting Images into Class-conditional Generative Networks.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;http://minyounghuh.com/&#34;&gt;Minyoung Huh&lt;/a&gt;, &lt;a href=&#34;https://richzhang.github.io/&#34;&gt;Richard Zhang&lt;/a&gt;, &lt;a href=&#34;https://people.csail.mit.edu/junyanz/&#34;&gt;Jun-Yan Zhu&lt;/a&gt;, &lt;a href=&#34;http://people.csail.mit.edu/sparis/&#34;&gt;Sylvain Paris&lt;/a&gt;, &lt;a href=&#34;https://www.dgp.toronto.edu/~hertzman/&#34;&gt;Aaron Hertzmann&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; ECCV 2020. [&lt;a href=&#34;http://arxiv.org/abs/2005.01703&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/minyoungg/GAN-Transform-and-Project&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://minyoungg.github.io/GAN-Transform-and-Project/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MimicGAN: Robust Projection onto Image Manifolds with Corruption Mimicking.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://rushila.com/&#34;&gt;Rushil Anirudh&lt;/a&gt;, &lt;a href=&#34;https://jjthiagarajan.com/&#34;&gt;Jayaraman J. Thiagarajan&lt;/a&gt;, &lt;a href=&#34;https://people.llnl.gov/kailkhura1&#34;&gt;Bhavya Kailkhura&lt;/a&gt;, &lt;a href=&#34;https://people.llnl.gov/bremer5&#34;&gt;Timo Bremer&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; IJCV 2020. [&lt;a href=&#34;https://link.springer.com/article/10.1007/s11263-020-01310-5&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Rewriting a Deep Generative Model.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;David Bau, Steven Liu, Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba.&lt;/em&gt;&lt;br&gt; ECCV 2020. [&lt;a href=&#34;https://arxiv.org/abs/2007.15646&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/davidbau/rewriting&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleGAN2 Distillation for Feed-forward Image Manipulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuri Viazovetskyi, Vladimir Ivashkin, Evgeny Kashin.&lt;/em&gt;&lt;br&gt; ECCV 2020. [&lt;a href=&#34;https://arxiv.org/abs/2003.03581&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/EvgenyKashin/stylegan2-distillation&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;In-Domain GAN Inversion for Real Image Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jiapeng Zhu, Yujun Shen, Deli Zhao, Bolei Zhou.&lt;/em&gt;&lt;br&gt; ECCV 2020. [&lt;a href=&#34;https://arxiv.org/abs/2004.00049&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://genforce.github.io/idinvert/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/genforce/idinvert&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Exploiting Deep Generative Prior for Versatile Image Restoration and Manipulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, Ping Luo.&lt;/em&gt;&lt;br&gt; ECCV 2020. [&lt;a href=&#34;https://arxiv.org/abs/2003.13659&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/XingangPan/deep-generative-prior&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Your Local GAN: Designing Two Dimensional Local Attention Mechanisms for Generative Models.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Giannis Daras, Augustus Odena, Han Zhang, Alexandros G. Dimakis.&lt;/em&gt;&lt;br&gt; CVPR 2020. [&lt;a href=&#34;https://arxiv.org/abs/1911.12287&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Disentangling Invertible Interpretation Network for Explaining Latent Representations.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Patrick Esser, Robin Rombach, Björn Ommer.&lt;/em&gt;&lt;br&gt; CVPR 2020. [&lt;a href=&#34;https://arxiv.org/abs/2004.13166&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://compvis.github.io/iin/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/CompVis/iin&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Editing in Style: Uncovering the Local Semantics of GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Edo Collins, Raja Bala, Bob Price, Sabine Süsstrunk.&lt;/em&gt;&lt;br&gt; CVPR 2020. [&lt;a href=&#34;https://arxiv.org/abs/2004.14367&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/IVRL/GANLocalEditing&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Image Processing Using Multi-Code GAN Prior.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;http://www.jasongt.com/&#34;&gt;Jinjin Gu&lt;/a&gt;, Yujun Shen, Bolei Zhou.&lt;/em&gt;&lt;br&gt; CVPR 2020. [&lt;a href=&#34;https://arxiv.org/abs/1912.07116&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://genforce.github.io/mganprior/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/genforce/mganprior&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Image2StyleGAN++: How to Edit the Embedded Images?&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Rameen Abdal, &lt;a href=&#34;http://yipengqin.github.io/&#34;&gt;Yipeng Qin&lt;/a&gt;, Peter Wonka.&lt;/em&gt;&lt;br&gt; CVPR 2020. [&lt;a href=&#34;https://arxiv.org/abs/1911.11544&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Semantic Photo Manipulation with a Generative Image Prior.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;David Bau, Hendrik Strobelt, William Peebles, Jonas, Bolei Zhou, Jun-Yan Zhu, Antonio Torralba.&lt;/em&gt;&lt;br&gt; TOG 2019. [&lt;a href=&#34;https://arxiv.org/abs/2005.07727&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Image2StyleGAN: How to Embed Images Into the StyleGAN Latent Space?&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Rameen Abdal, &lt;a href=&#34;http://yipengqin.github.io/&#34;&gt;Yipeng Qin&lt;/a&gt;, Peter Wonka.&lt;/em&gt;&lt;br&gt; ICCV 2019. [&lt;a href=&#34;https://arxiv.org/abs/1904.03189&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/RameenAbdal/image2styleganv1-v2&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GAN-based Projector for Faster Recovery with Convergence Guarantees in Linear Inverse Problems.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ankit Raj, Yuqi Li, Yoram Bresler.&lt;/em&gt;&lt;br&gt; ICCV 2019. [&lt;a href=&#34;https://arxiv.org/abs/1902.09698&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Inverting Layers of a Large Generator.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;David Bau, Jun-Yan Zhu, Jonas Wulff, William Peebles, Hendrik Strobelt, Bolei Zhou, Antonio Torralba.&lt;/em&gt;&lt;br&gt; ICCV 2019. [&lt;a href=&#34;https://debug-ml-iclr2019.github.io/cameraready/DebugML-19_paper_18.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Detecting Overfitting in Deep Generators via Latent Recovery.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ryan Webster, Julien Rabin, Loic Simon, Frederic Jurie.&lt;/em&gt;&lt;br&gt; CVPR 2019. [&lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2019/papers/Webster_Detecting_Overfitting_of_Deep_Generative_Networks_via_Latent_Recovery_CVPR_2019_paper.pdf&#34;&gt;PDF&lt;/a&gt;][&lt;a href=&#34;https://colab.research.google.com/drive/1N6zP4xlPunWOkmakcl0mamfhq946nMLB?usp=sharing&#34;&gt;Colab&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Inverting The Generator Of A Generative Adversarial Network (II).&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Antonia Creswell, Anil A Bharath.&lt;/em&gt;&lt;br&gt; TNNLS 2018. [&lt;a href=&#34;https://arxiv.org/abs/1802.05701&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ToniCreswell/InvertingGAN&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Invertibility of Convolutional Generative Networks from Partial Measurements.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Fangchang Ma, Ulas Ayaz, Sertac Karaman.&lt;/em&gt;&lt;br&gt; NeurIPS 2018. [&lt;a href=&#34;https://papers.nips.cc/paper/8171-invertibility-of-convolutional-generative-networks-from-partial-measurements&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/fangchangma/invert-generative-networks&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Metrics for Deep Generative Models.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Nutan Chen, Alexej Klushyn, Richard Kurle, Xueyan Jiang, Justin Bayer, Patrick van der Smagt.&lt;/em&gt;&lt;br&gt; AISTATS 2018. [&lt;a href=&#34;https://arxiv.org/abs/1711.01204&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards Understanding the Invertibility of Convolutional Neural Networks.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Anna C. Gilbert, Yi Zhang, Kibok Lee, Yuting Zhang, Honglak Lee.&lt;/em&gt;&lt;br&gt; IJCAI 2017. [&lt;a href=&#34;https://arxiv.org/abs/1705.08664&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;One Network to Solve Them All - Solving Linear Inverse Problems using Deep Projection Models.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;J. H. Rick Chang, Chun-Liang Li, Barnabas Poczos, B. V. K. Vijaya Kumar, Aswin C. Sankaranarayanan.&lt;/em&gt;&lt;br&gt; ICCV 2017. [&lt;a href=&#34;https://arxiv.org/abs/1703.09912&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Precise Recovery of Latent Vectors from Generative Adversarial Networks.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Zachary C. Lipton, Subarna Tripathi.&lt;/em&gt;&lt;br&gt; ICLR 2017 workshop. [&lt;a href=&#34;https://arxiv.org/abs/1702.04782&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/SubarnaTripathi/ReverseGAN&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Inverting The Generator Of A Generative Adversarial Network.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Antonia Creswell, Anil Anthony Bharath.&lt;/em&gt;&lt;br&gt; NeurIPS 2016 Workshop. [&lt;a href=&#34;https://arxiv.org/abs/1611.05644&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generative Visual Manipulation on the Natural Image Manifold.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jun-Yan Zhu, Philipp Krähenbühl, Eli Shechtman, Alexei A. Efros.&lt;/em&gt;&lt;br&gt; ECCV 2016. [&lt;a href=&#34;https://arxiv.org/abs/1609.03552v2&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h3&gt;3D GAN inverson&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;3D GAN Inversion for Controllable Portrait Image Animation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://connorzlin.com/&#34;&gt;Connor Z. Lin&lt;/a&gt;, David B. Lindell, Eric R. Chan, &lt;a href=&#34;https://stanford.edu/~gordonwz/&#34;&gt;Gordon Wetzstein&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.13441&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://www.computationalimaging.org/publications/3dganinversion/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pix2NeRF: Unsupervised Conditional π-GAN for Single Image to Neural Radiance Fields Translation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Shengqu Cai, Anton Obukhov, Dengxin Dai, Luc Van Gool.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.13162&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised 3D Shape Completion through GAN-Inversion.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://junzhezhang.github.io/&#34;&gt;Junzhe Zhang&lt;/a&gt;, Xinyi Chen, Zhongang Cai, Liang Pan, Haiyu Zhao, Shuai Yi, Chai Kiat Yeo, Bo Dai, Chen Change Loy.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/pdf/2104.13366&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://junzhezhang.github.io/projects/ShapeInversion/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p width=&#34;100%&#34; align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/weihaox/awesome-gan-inversion/main/#&#34;&gt;🔝&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;latent space navigation&lt;/h2&gt; &#xA;&lt;p&gt;Inversion is not the ultimate goal. The reason that we invert a real image into the latent space of a trained GAN model is that we can manipulate the inverted image in the latent space by discovering the desired code with certain attributes. This technique is usually known as latent space navigation, GAN steerability, latent code manipulation, or other names in the literature. Although often regarded as an independent research field, it acts as an indispensable component of GAN inversion for manipulation. Many inversion methods also involve efficient discovery of a desired latent code.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Hierarchical Semantic Regularization of Latent Spaces in StyleGANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tejan Karmali, Rishubh Parihar, Susmit Agrawal, Harsh Rangwani, Varun Jampani, Maneesh Singh, R. Venkatesh Babu.&lt;/em&gt;&lt;br&gt; ECCV 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.03764&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://sites.google.com/view/hsr-eccv22/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CLIP2StyleGAN: Unsupervised Extraction of StyleGAN Edit Directions.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Rameen Abdal, Peihao Zhu, John Femiani, Niloy J. Mitra, Peter Wonka.&lt;/em&gt;&lt;br&gt; SIGGRAPH 2022. [&lt;a href=&#34;https://arxiv.org/abs/2112.05219&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/RameenAbdal/CLIP2StyleGAN&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Region-Based Semantic Factorization in GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jiapeng Zhu, Yujun Shen, Yinghao Xu, Deli Zhao, Qifeng Chen.&lt;/em&gt;&lt;br&gt; ICML 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.09649&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/zhujiapeng/resefa&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Latent Image Animator: Learning to Animate Image via Latent Space Navigation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yaohui Wang, Di Yang, Francois Bremond, Antitza Dantcheva.&lt;/em&gt;&lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://openreview.net/forum?id=7r6kDq0mK_&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://wyhsirius.github.io/LIA-project&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/wyhsirius/LIA&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Do Not Escape From the Manifold: Discovering the Local Coordinates on the Latent Space of GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jaewoong Choi, Changyeon Yoon, Junho Lee, Jung Ho Park, Geonho Hwang, Myungjoo Kang.&lt;/em&gt;&lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2106.06959&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tensor-based Emotion Editing in the StyleGAN Latent Space.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;René Haas, Stella Graßhof, Sami S. Brandt.&lt;/em&gt;&lt;br&gt; CVPR 2022 Workshop on AI for Content Creation Workshop. [&lt;a href=&#34;https://arxiv.org/pdf/2205.06102.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PaintInStyle: One-Shot Discovery of Interpretable Directions by Painting.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Berkay Doner, Elif Sema Balcioglu, Merve Rabia Barin, Umut Kocasari, Mert Tiftikci, Pinar Yanardag.&lt;/em&gt;&lt;br&gt; CVPR 2022 Workshops. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022W/CVFAD/html/Doner_PaintInStyle_One-Shot_Discovery_of_Interpretable_Directions_by_Painting_CVPRW_2022_paper.html&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Rank in Style: A Ranking-Based Approach To Find Interpretable Directions.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Umut Kocasari, Kerem Zaman, Mert Tiftikci, Enis Simsar, Pinar Yanardag.&lt;/em&gt;&lt;br&gt; CVPR 2022 Workshops. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022W/CVFAD/html/Kocasari_Rank_in_Style_A_Ranking-Based_Approach_To_Find_Interpretable_Directions_CVPRW_2022_paper.html&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LARGE: Latent-Based Regression through GAN Semantics.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://yotamnitzan.github.io&#34;&gt;Yotam Nitzan&lt;/a&gt;, &lt;a href=&#34;https://rinongal.github.io/&#34;&gt;Rinon Gal&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=iLLlWr8AAAAJ&#34;&gt;Ofir Brenner&lt;/a&gt;, &lt;a href=&#34;https://danielcohenor.com/&#34;&gt;Daniel Cohen-Or&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2107.11186&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/YotamNitzan/LARGE&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://yotamnitzan.github.io/LARGE&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleFusion: Disentangling Spatial Segments in StyleGAN-Generated Images.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Omer Kafri, Or Patashnik, Yuval Alaluf, Daniel Cohen-Or.&lt;/em&gt;&lt;br&gt; TOG 2022. [&lt;a href=&#34;https://arxiv.org/abs/2107.07437&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/OmerKafri/StyleFusion&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fantastic Style Channels and Where to Find Them: A Submodular Framework for Discovering Diverse Directions in GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://enis.dev/&#34;&gt;Enis Simsar&lt;/a&gt;, &lt;a href=&#34;https://catlab-team.github.io/&#34;&gt;Umut Kocasari&lt;/a&gt;, &lt;a href=&#34;https://catlab-team.github.io/&#34;&gt;Ezgi Gülperi Er&lt;/a&gt;, &lt;a href=&#34;https://pinguar.org/&#34;&gt;Pinar Yanardag&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.08516&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://catlab-team.github.io/fantasticstyles/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://catlab-team.github.io/styleatlas/classes/FFHQ/&#34;&gt;Demo&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Analyzing the Latent Space of GAN through Local Dimension Estimation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jaewoong Choi, Geonho Hwang, Hyunsoo Cho, Myungjoo Kang.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2205.13182&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PandA: Unsupervised Learning of Parts and Appearances in the Feature Maps of GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;James Oldfield, Christos Tzelepis, Yannis Panagakis, Mihalis A. Nicolaou, Ioannis Patras.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.00048&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;http://eecs.qmul.ac.uk/~jo001/PandA/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/james-oldfield/PandA&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multi-level Latent Space Structuring for Generative Control.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://orenkatzir.github.io/&#34;&gt;Oren Katzir&lt;/a&gt;, Vicky Perepelook, Dani Lischinski, Daniel Cohen-Or.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.05910&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Rayleigh EigenDirections (REDs): GAN Latent Space Traversals for Multidimensional Features.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Guha Balakrishnan, Raghudeep Gadde, Aleix Martinez, Pietro Perona.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/pdf/2201.10423.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Optimizing Latent Space Directions For GAN-based Local Image Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ehsan Pajouheshgar, Tong Zhang, Sabine Süsstrunk.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2111.12583&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Interpreting the Latent Space of GANs via Correlation Analysis for Controllable Concept Manipulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ziqiang Li, Rentuo Tao, Hongjing Niu, Bin Li.&lt;/em&gt;&lt;br&gt; arxiv 2020. [&lt;a href=&#34;https://arxiv.org/abs/2006.10132&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tensor Component Analysis for Interpreting the Latent Space of GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://james-oldfield.github.io/&#34;&gt;James Oldfield&lt;/a&gt;, Markos Georgopoulos, Yannis Panagakis, Mihalis A. Nicolaou, Ioannis Patras.&lt;/em&gt;&lt;br&gt; BMVC 2021. [&lt;a href=&#34;https://arxiv.org/abs/2111.11736&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;http://eecs.qmul.ac.uk/~jo001/TCA-latent-space/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/james-oldfield/TCA-latent-space&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tensor-based Subspace Factorization for StyleGAN.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Rene Haas, Stella Graßhof and Sami S. Brandt.&lt;/em&gt;&lt;br&gt; FG 2021. [&lt;a href=&#34;https://arxiv.org/abs/2111.04554&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Exploratory Search of GANs with Contextual Bandits.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ivan Kropotov, Alan Medlar, Dorota Glowacka.&lt;/em&gt;&lt;br&gt; CIKM 2021. [&lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3459637.3482103&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LowRankGAN: Low-Rank Subspaces in GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jiapeng Zhu, Ruili Feng, Yujun Shen, Deli Zhao, Zhengjun Zha, Jingren Zhou, Qifeng Chen.&lt;/em&gt;&lt;br&gt; NeurIPS 2021. [&lt;a href=&#34;https://arxiv.org/abs/2106.04488&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/zhujiapeng/LowRankGAN&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Controllable and Compositional Generation with Latent-Space Energy-Based Models.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Weili Nie, Arash Vahdat, Anima Anandkumar.&lt;/em&gt;&lt;br&gt; NeurIPS 2021. [&lt;a href=&#34;https://arxiv.org/abs/2110.10873&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Latent Transformer for Disentangled Face Editing in Images and Videos.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Xu Yao, Alasdair Newson, Yann Gousseau, Pierre Hellier.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2021/html/Yao_A_Latent_Transformer_for_Disentangled_Face_Editing_in_Images_and_ICCV_2021_paper.html&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/2106.11895&#34;&gt;ArXiV&lt;/a&gt;] [&lt;a href=&#34;https://github.com/InterDigitalInc/latent-transformer&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Toward a Visual Concept Vocabulary for GAN Latent Space.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Sarah Schwettmann, Evan Hernandez, David Bau, Samuel Klein, Jacob Andreas, Antonio Torralba.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2021/html/Schwettmann_Toward_a_Visual_Concept_Vocabulary_for_GAN_Latent_Space_ICCV_2021_paper.html&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://visualvocab.csail.mit.edu/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;WarpedGANSpace: Finding Non-linear RBF Paths in GAN Latent Space.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Christos Tzelepis, Georgios Tzimiropoulos, Ioannis Patras.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://arxiv.org/abs/2109.13357&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/chi0tzp/WarpedGANSpace&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Latent Transformations via NeuralODEs for GAN-based Image Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Valentin Khrulkov, Leyla Mirvakhabova, Ivan Oseledets, Artem Babenko.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2021/papers/Khrulkov_Latent_Transformations_via_NeuralODEs_for_GAN-Based_Image_Editing_ICCV_2021_paper.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/KhrulkovV/nonlinear-image-editing&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;OroJaR: Orthogonal Jacobian Regularization for Unsupervised Disentanglement in Image Generation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuxiang Wei, Yupeng Shi, Xiao Liu, Zhilong Ji, Yuan Gao, Zhongqin Wu, Wangmeng Zuo.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://arxiv.org/abs/2108.07668&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/csyxwei/OroJaR&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;EigenGAN: Layer-Wise Eigen-Learning for GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Zhenliang He, Meina Kan, Shiguang Shan.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://arxiv.org/abs/2104.12476&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/LynnHo/EigenGAN-Tensorflow&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SalS-GAN: Spatially-Adaptive Latent Space in StyleGAN for Real Image Embedding.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Lingyun Zhang, Xiuxiu Bai, Yao Gao.&lt;/em&gt;&lt;br&gt; ACM MM 2021. [&lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3474085.3475633&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Discovering Density-Preserving Latent Space Walks in GANs for Semantic Image Transformations.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Guanyue Li, Yi Liu, Xiwen Wei, Yang Zhang, Si Wu, Yong Xu, Hau San Wong.&lt;/em&gt;&lt;br&gt; ACM MM 2021. [&lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3474085.3475293&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Discovering Interpretable Latent Space Directions of GANs Beyond Binary Attributes.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Huiting Yang, Liangyu Chai, Qiang Wen, Shuang Zhao, Zixun Sun, Shengfeng He.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_Discovering_Interpretable_Latent_Space_Directions_of_GANs_Beyond_Binary_Attributes_CVPR_2021_paper.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Surrogate Gradient Field for Latent Space Manipulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Minjun Li, Yanghua Jin, Huachun Zhu.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;http://arxiv.org/abs/2104.09065&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SeFa: Closed-Form Factorization of Latent Semantics in GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yujun Shen, Bolei Zhou.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2007.06600&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/genforce/sefa&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://genforce.github.io/sefa/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;L2M-GAN: Learning To Manipulate Latent Space Semantics for Facial Attribute Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Guoxing Yang, Nanyi Fei, Mingyu Ding, Guangzhen Liu, Zhiwu Lu, Tao Xiang.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2021/html/Yang_L2M-GAN_Learning_To_Manipulate_Latent_Space_Semantics_for_Facial_Attribute_CVPR_2021_paper.html&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/songquanpeng/L2M-GAN&#34;&gt;Unofficial Pytorch&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MoCoGAN-HD: A Good Image Generator Is What You Need for High-Resolution Video Synthesis.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng, Dimitris N. Metaxas, Sergey Tulyakov.&lt;/em&gt;&lt;br&gt; ICLR 2021. [&lt;a href=&#34;https://openreview.net/pdf?id=6puCSjH3hwA&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/snap-research/MoCoGAN-HD&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GAN Steerability without optimization.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Nurit Spingarn-Eliezer, Ron Banner, Tomer Michaeli.&lt;/em&gt;&lt;br&gt; ICLR 2021. [&lt;a href=&#34;https://openreview.net/forum?id=zDy_nQCXiIj&#34;&gt;OpenReview&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/2012.05328&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;On the &#34;steerability&#34; of generative adversarial networks.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ali Jahanian, Lucy Chai, Phillip Isola.&lt;/em&gt;&lt;br&gt; ICLR 2020. [&lt;a href=&#34;https://arxiv.org/abs/1907.07171&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://ali-design.github.io/gan_steerability/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GANSpace: Discovering Interpretable GAN Controls.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Erik Härkönen, Aaron Hertzmann, Jaakko Lehtinen, Sylvain Paris.&lt;/em&gt;&lt;br&gt; NeurIPS 2020. [&lt;a href=&#34;https://arxiv.org/abs/2004.02546&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/harskish/ganspace&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Interpreting the Latent Space of GANs for Semantic Face Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;http://shenyujun.github.io/&#34;&gt;Yujun Shen&lt;/a&gt;, &lt;a href=&#34;http://www.jasongt.com/&#34;&gt;Jinjin Gu&lt;/a&gt;, &lt;a href=&#34;http://www.ie.cuhk.edu.hk/people/xotang.shtml&#34;&gt;Xiaoou Tang&lt;/a&gt;, &lt;a href=&#34;http://bzhou.ie.cuhk.edu.hk/&#34;&gt;Bolei Zhou&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2020. [&lt;a href=&#34;https://arxiv.org/abs/1907.10786&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://genforce.github.io/interfacegan/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/genforce/interfacegan&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Seeing What a GAN Cannot Generate.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;David Bau, Jun-Yan Zhu, Jonas Wulff, William Peebles, Hendrik Strobelt, Bolei Zhou, Antonio Torralba.&lt;/em&gt;&lt;br&gt; ICCV 2019. [&lt;a href=&#34;https://arxiv.org/abs/1910.11626&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;http://ganseeing.csail.mit.edu/&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised Discovery of Interpretable Directions in the GAN Latent Space.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Andrey Voynov, Artem Babenko.&lt;/em&gt;&lt;br&gt; ICML 2020. [&lt;a href=&#34;https://arxiv.org/abs/2002.03754&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/anvoynov/GANLatentDiscovery&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p width=&#34;100%&#34; align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/weihaox/awesome-gan-inversion/main/#&#34;&gt;🔝&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;application&lt;/h2&gt; &#xA;&lt;h3&gt;image and video generation and manipulation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;3D-FM GAN: Towards 3D-Controllable Face Manipulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://lychenyoko.github.io/&#34;&gt;Yuchen Liu&lt;/a&gt;, Zhixin Shu, Yijun Li, Zhe Lin, Richard Zhang, and Sun-Yuan Kung.&lt;/em&gt;&lt;br&gt; ECCV 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.11257&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://lychenyoko.github.io/3D-FM-GAN-Webpage/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generative Multiplane Images: Making a 2D GAN 3D-Aware.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://xiaoming-zhao.com/&#34;&gt;Xiaoming Zhao&lt;/a&gt;, &lt;a href=&#34;https://fangchangma.github.io/&#34;&gt;Fangchang Ma&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=bckYvFkAAAAJ&amp;amp;hl=en&#34;&gt;David Güera&lt;/a&gt;, &lt;a href=&#34;https://jrenzhile.com/&#34;&gt;Zhile Ren&lt;/a&gt;, &lt;a href=&#34;https://www.alexander-schwing.de/&#34;&gt;Alexander G. Schwing&lt;/a&gt;, &lt;a href=&#34;https://www.colburn.org/&#34;&gt;Alex Colburn&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; ECCV 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.10642&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://xiaoming-zhao.github.io/projects/gmpi/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/apple/ml-gmpi&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Injecting 3D Perception of Controllable NeRF-GAN into StyleGAN for Editable Portrait Image Synthesis.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jeong-gi Kwak, Yuanming Li, Dongsik Yoon, Donghyeon Kim, David Han, Hanseok Ko.&lt;/em&gt;&lt;br&gt; ECCV 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.10257&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://jgkwak95.github.io/surfgan/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Temporally Consistent Semantic Video Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://twizwei.github.io/&#34;&gt;Yiran Xu&lt;/a&gt;, &lt;a href=&#34;https://badouralbahar.github.io/&#34;&gt;Badour AlBahar&lt;/a&gt;, &lt;a href=&#34;https://jbhuang0604.github.io/&#34;&gt;Jia-Bin Huang&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; ECCV 2022. [&lt;a href=&#34;https://arxiv.org/pdf/2206.10590.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://video-edit-gan.github.io/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Sound-Guided Semantic Video Generation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Seung Hyun Lee, Gyeongrok Oh, Wonmin Byeon, Jihyun Bae, Chanyoung Kim, Won Jeong Ryoo, Sang Ho Yoon, Jinkyu Kim, Sangpil Kim.&lt;/em&gt;&lt;br&gt; ECCV 2022. [&lt;a href=&#34;https://arxiv.org/abs/2204.09273&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://kuai-lab.github.io/eccv2022sound/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Everything is There in Latent Space: Attribute Editing and Attribute Style Manipulation by StyleGAN Latent Space Exploration.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Rishubh Parihar, Ankit Dhiman, Tejan Karmali, R. Venkatesh Babu.&lt;/em&gt;&lt;br&gt; ACM MM 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.09855&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://sites.google.com/view/flamelatentediting&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SD-GAN: Semantic Decomposition for Face Image Synthesis with Discrete Attribute.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Zhou Kangneng, Zhu Xiaobin, Gao Daiheng, Lee Kai, Li Xinjie, Yin Xu-Cheng.&lt;/em&gt;&lt;br&gt; ACM MM 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.05300&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Identity-Guided Face Generation with Multi-modal Contour Conditions.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Qingyan Bai, Weihao Xia, Fei Yin, Yujiu Yang.&lt;/em&gt;&lt;br&gt; ICIP 2022. [&lt;a href=&#34;https://arxiv.org/abs/2110.04854&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Self-Conditioned Generative Adversarial Networks for Image Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yunzhe Liu, Rinon Gal, Amit H. Bermano, Baoquan Chen, Daniel Cohen-Or.&lt;/em&gt;&lt;br&gt; SIGGRAPH 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.04040&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yzliu567/sc-gan&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SphericGAN: Semi-Supervised Hyper-Spherical Generative Adversarial Networks for Fine-Grained Image Synthesis.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tianyi Chen, Yunfei Zhang, Xiaoyang Huo, Si Wu, Yong Xu, Hau San Wong.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/html/Chen_SphericGAN_Semi-Supervised_Hyper-Spherical_Generative_Adversarial_Networks_for_Fine-Grained_Image_Synthesis_CVPR_2022_paper.html&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Sound-Guided Semantic Image Manipulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Seung Hyun Lee, Wonseok Roh, Wonmin Byeon, Sang Ho Yoon, Chan Young Kim, Jinkyu Kim, Sangpil Kim.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2112.00007&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HairCLIP: Design Your Hair by Text and Reference Image.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tianyi Wei, Dongdong Chen, Wenbo Zhou, Jing Liao, Zhentao Tan, Lu Yuan, Weiming Zhang, Nenghai Yu.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2112.05142&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/wty-ustc/HairCLIP&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HairMapper: Removing Hair from Portraits Using GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://onethousandwu.com/&#34;&gt;Yiqian Wu&lt;/a&gt;, &lt;a href=&#34;http://www.yongliangyang.net/&#34;&gt;Yong-Liang Yang&lt;/a&gt;, &lt;a href=&#34;http://www.cad.zju.edu.cn/home/jin&#34;&gt;Xiaogang Jin&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;http://www.cad.zju.edu.cn/home/jin/cvpr2022/HairMapper.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;http://www.cad.zju.edu.cn/home/jin/cvpr2022/cvpr2022.htm&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/oneThousand1000/non-hair-FFHQ/raw/main&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://github.com/oneThousand1000/non-hair-FFHQ&#34;&gt;Non-hair-FFHQ Data&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Attribute Group Editing for Reliable Few-shot Image Generation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Guanqi Ding, Xinzhe Han, Shuhui Wang, Shuzhe Wu, Xin Jin, Dandan Tu, Qingming Huang.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.08422&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/UniBester/AGE&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;InsetGAN for Full-Body Image Generation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://afruehstueck.github.io/&#34;&gt;Anna Frühstück&lt;/a&gt;, &lt;a href=&#34;http://krsingh.cs.ucdavis.edu/&#34;&gt;Krishna Kumar Singh&lt;/a&gt;, &lt;a href=&#34;https://research.adobe.com/person/eli-shechtman/&#34;&gt;Eli Shechtman&lt;/a&gt;, &lt;a href=&#34;https://research.adobe.com/person/niloy-mitra/&#34;&gt;Niloy J. Mitra&lt;/a&gt;, &lt;a href=&#34;http://peterwonka.net/&#34;&gt;Peter Wonka&lt;/a&gt;, &lt;a href=&#34;https://research.adobe.com/person/jingwan-lu/&#34;&gt;Jingwan Lu&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2112.07200&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;http://afruehstueck.github.io/insetgan&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/stylegan-human/StyleGAN-Human/raw/main/insetgan.py&#34;&gt;Unofficial&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SpaceEdit: Learning a Unified Editing Space for Open-Domain Image Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://www.cs.rochester.edu/u/jshi31/&#34;&gt;Jing Shi&lt;/a&gt;, &lt;a href=&#34;https://sites.google.com/view/ningxu/&#34;&gt;Ning Xu&lt;/a&gt;, &lt;a href=&#34;https://www.cs.rochester.edu/u/hzheng15/haitian_homepage/index.html&#34;&gt;Haitian Zheng&lt;/a&gt;, Alex Smith, &lt;a href=&#34;https://www.cs.rochester.edu/u/jluo/&#34;&gt;Jiebo Luo&lt;/a&gt;, &lt;a href=&#34;https://www.cs.rochester.edu/~cxu22/&#34;&gt;Chenliang Xu&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2112.00180&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;In&amp;amp;Out: Diverse Image Outpainting via GAN Inversion.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yen-Chi Cheng, Chieh Hubert Lin, Hsin-Ying Lee, Jian Ren, Sergey Tulyakov, Ming-Hsuan Yang.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2104.00675&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://yccyenchicheng.github.io/InOut/&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;InfinityGAN: Towards Infinite-Resolution Image Synthesis.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Chieh Hubert Lin, Hsin-Ying Lee, Yen-Chi Cheng, Sergey Tulyakov, Ming-Hsuan Yang.&lt;/em&gt;&lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2104.03963&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://hubert0527.github.io/infinityGAN/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Latent to Latent: A Learned Mapper for Identity Preserving Editing of Multiple.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Siavash Khodadadeh, Shabnam Ghadar, Saeid Motiian, Wei-An Lin, Ladislau Bölöni, Ratheesh Kalarot.&lt;/em&gt;&lt;br&gt; WACV 2022. [&lt;a href=&#34;https://openaccess.thecvf.com/content/WACV2022/html/Khodadadeh_Latent_to_Latent_A_Learned_Mapper_for_Identity_Preserving_Editing_WACV_2022_paper.html&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleVideoGAN: A Temporal Generative Model using a Pretrained StyleGAN.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://www.mpi-inf.mpg.de/~gfox/&#34;&gt;Gereon Fox&lt;/a&gt;, &lt;a href=&#34;https://www.mpi-inf.mpg.de/~atewari/&#34;&gt;Ayush Tewari&lt;/a&gt;, Mohamed Elgharib, &lt;a href=&#34;http://gvv.mpi-inf.mpg.de/&#34;&gt;Christian Theobalt&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; BMVC 2021 (Oral). [&lt;a href=&#34;https://arxiv.org/abs/2107.07224&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Constrained Graphic Layout Generation via Latent Optimization.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Kotaro Kikuchi, Edgar Simo-Serra, Mayu Otani, Kota Yamaguchi.&lt;/em&gt;&lt;br&gt; ACM MM 2021. [&lt;a href=&#34;https://arxiv.org/abs/2108.00871&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ktrk115/const_layout&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Face Image Retrieval With Attribute Manipulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://zaeemzadeh.com/&#34;&gt;Alireza Zaeemzadeh&lt;/a&gt;, Shabnam Ghadar, Baldo Faieta, Zhe Lin, Nazanin Rahnavard, Mubarak Shah, Ratheesh Kalarot.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2021/html/Zaeemzadeh_Face_Image_Retrieval_With_Attribute_Manipulation_ICCV_2021_paper.html&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleCariGAN: Caricature Generation via StyleGAN Feature Map Modulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Wongjong Jang, Gwangjin Ju, &lt;a href=&#34;https://ycjung.info/&#34;&gt;Yucheol Jung&lt;/a&gt;, &lt;a href=&#34;http://jlyang.org/&#34;&gt;Jiaolong Yang&lt;/a&gt;, &lt;a href=&#34;https://www.microsoft.com/en-us/research/people/xtong/&#34;&gt;Xin Tong&lt;/a&gt;, &lt;a href=&#34;http://phome.postech.ac.kr/~leesy/&#34;&gt;Seungyong Lee&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; TOG 2021. [&lt;a href=&#34;https://arxiv.org/pdf/2107.04331.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/PeterZhouSZ/StyleCariGAN&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Coarse-to-Fine: Facial Structure Editing of Portrait Images via Latent Space Classifications.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://onethousandwu.com/&#34;&gt;Yiqian Wu&lt;/a&gt;, &lt;a href=&#34;http://www.yongliangyang.net/&#34;&gt;Yongliang Yang&lt;/a&gt;, Qinjie Xiao, &lt;a href=&#34;http://www.cad.zju.edu.cn/home/jin&#34;&gt;Xiaogang Ji&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; TOG 2021. [&lt;a href=&#34;http://www.cad.zju.edu.cn/home/jin/sig2021/paper46.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;http://www.cad.zju.edu.cn/home/jin/sig2021/sig2021.htm&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SAM: Only a Matter of Style-Age Transformation Using a Style-Based Regression Model.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuval Alaluf, Or Patashnik, &lt;a href=&#34;https://www.cs.tau.ac.il/~dcor/&#34;&gt;Daniel Cohen-Or&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; TOG 2021. [&lt;a href=&#34;https://arxiv.org/abs/2102.02754&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yuval-alaluf/SAM&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Barbershop: GAN-based Image Compositing using Segmentation Masks.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://github.com/ZPdesu&#34;&gt;Peihao Zhu&lt;/a&gt;, &lt;a href=&#34;https://github.com/RameenAbdal&#34;&gt;Rameen Abdal&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=rS1xJIIAAAAJ&amp;amp;hl=en&#34;&gt;John Femiani&lt;/a&gt;, &lt;a href=&#34;http://peterwonka.net/&#34;&gt;Peter Wonka&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; SIGGRAPH Asia 2021. [&lt;a href=&#34;https://arxiv.org/abs/2106.01505&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://zpdesu.github.io/Barbershop/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ZPdesu/Barbershop&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Exploring Adversarial Fake Images on Face Manifold.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Dongze Li, Wei Wang, Hongxing Fan, Jing Dong.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2101.03272&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HistoGAN: Controlling Colors of GAN-Generated and Real Images via Color Histograms.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://sites.google.com/view/mafifi&#34;&gt;Mahmoud Afifi&lt;/a&gt;, Marcus A. Brubaker, Michael S. Brown.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2011.11731&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mahmoudnafifi/HistoGAN&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://ln2.sync.com/dl/1891becc0/uhsxtprq-33wfwmyq-dhhqeb3s-mtstuqw7/view/default/11118541390008&#34;&gt;4K Landscape&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;One Shot Face Swapping on Megapixels.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuhao Zhu, Qi Li, Jian Wang, Chengzhong Xu, Zhenan Sun.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/pdf/2105.04932.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/zyainfal/One-Shot-Face-Swapping-on-Megapixels&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LOHO: Latent Optimization of Hairstyles via Orthogonalization.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Rohit Saha, Brendan Duke, Florian Shkurti, Graham W. Taylor, Parham Aarabi.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2103.03891&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/dukebw/LOHO&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleMapGAN: Exploiting Spatial Dimensions of Latent in GAN for Real-time Image Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://github.com/blandocs&#34;&gt;Hyunsu Kim&lt;/a&gt;, &lt;a href=&#34;https://yunjey.github.io/&#34;&gt;Yunjey Choi&lt;/a&gt;, &lt;a href=&#34;https://github.com/taki0112&#34;&gt;Junho Kim&lt;/a&gt;, &lt;a href=&#34;http://cmalab.snu.ac.kr/&#34;&gt;Sungjoo Yoo&lt;/a&gt;, &lt;a href=&#34;https://github.com/youngjung&#34;&gt;Youngjung Uh&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2104.14754&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/naver-ai/StyleMapGAN&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DeepI2I: Enabling Deep Hierarchical Image-to-Image Translation by Transferring from GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;yaxing wang, Lu Yu, Joost van de Weijer.&lt;/em&gt;&lt;br&gt; NeurIPS 2020. [&lt;a href=&#34;https://arxiv.org/abs/2011.05867&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yaxingwang/DeepI2I&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Robust Sound-Guided Image Manipulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Seung Hyun Lee, Chanyoung Kim, Wonmin Byeon, Gyeongrok Oh, Jooyoung Lee, Sang Ho Yoon, Jinkyu Kim, Sangpil Kim.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.14114&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt-to-Prompt Image Editing with Cross Attention Control.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, Daniel Cohen-Or.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.09446&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, Daniel Cohen-Or.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.09446&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://textual-inversion.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/rinongal/textual_inversion&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stitch it in Time: GAN-Based Facial Editing of Real Videos.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Rotem Tzaban, Ron Mokady, Rinon Gal, Amit H. Bermano, Daniel Cohen-Or.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2201.08361&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://stitch-time.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/rotemtzaban/STIT&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FEAT: Face Editing with Attention.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Xianxu Hou, Linlin Shen, Or Patashnik, Daniel Cohen-Or, Hui Huang.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2202.02713&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DyStyle: Dynamic Neural Network for Multi-Attribute-Conditioned Style Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Bingchuan Li, Shaofei Cai, Wei Liu, Peng Zhang, Miao Hua, Qian He, Zili Yi.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2109.10737&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/phycvgan/DyStyle&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Few-shot Semantic Image Synthesis Using StyleGAN Prior.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuki Endo, Yoshihiro Kanamori.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2103.14877&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/endo-yuki-t/Fewshot-SMIS&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Heredity-aware Child Face Image Generation with Latent Space Disentanglement.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Xiao Cui, Wengang Zhou, Yang Hu, Weilun Wang, Houqiang Li.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2108.11080&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised Image Transformation Learning via Generative Adversarial Networks.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Kaiwen Zha, Yujun Shen, Bolei Zhou.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2103.07751&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://genforce.github.io/trgan&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised Image-to-Image Translation via Pre-trained StyleGAN2 Network.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jialu Huang, Jing Liao, Sam Kwong.&lt;/em&gt;&lt;br&gt; arxiv 2020. [&lt;a href=&#34;https://arxiv.org/abs/2010.05713&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h3&gt;multimodal learning&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image Generators.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://rinongal.github.io/&#34;&gt;Rinon Gal&lt;/a&gt;, &lt;a href=&#34;https://orpatashnik.github.io/&#34;&gt;Or Patashnik&lt;/a&gt;, &lt;a href=&#34;https://haggaim.github.io/&#34;&gt;Haggai Maron&lt;/a&gt;, &lt;a href=&#34;https://research.nvidia.com/person/gal-chechik&#34;&gt;Gal Chechik&lt;/a&gt;, &lt;a href=&#34;https://www.cs.tau.ac.il/~dcor/&#34;&gt;Daniel Cohen-Or&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; SIGGRAPH 2022. [&lt;a href=&#34;https://arxiv.org/abs/2108.00946&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://stylegan-nada.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/rinongal/StyleGAN-nada&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Paint by Word.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;David Bau, Alex Andonian, Audrey Cui, YeonHwan Park, Ali Jahanian, Aude Oliva, Antonio Torralba.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/pdf/2103.10951&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CI-GAN: Cycle-Consistent Inverse GAN for Text-to-Image Synthesis.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Hao Wang, Guosheng Lin, Steven C. H. Hoi, Chunyan Miao.&lt;/em&gt;&lt;br&gt; ACM MM 2021. [&lt;a href=&#34;https://arxiv.org/abs/2108.01361&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TediGAN: Text-Guided Diverse Image Generation and Manipulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Weihao Xia, Yujiu Yang, Jing-Hao Xue, Baoyuan Wu.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2012.03308&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/weihaox/Multi-Modal-CelebA-HQ&#34;&gt;Data&lt;/a&gt;] [&lt;a href=&#34;https://github.com/weihaox/TediGAN&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DeepLandscape: Adversarial Modeling of Landscape Videos.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;E. Logacheva, R. Suvorov, O. Khomenko, A. Mashikhin, and V. Lempitsky.&lt;/em&gt;&lt;br&gt; ECCV 2020. [&lt;a href=&#34;https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123680256.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/saic-mdal/deep-landscape&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://saic-mdal.github.io/deep-landscape/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h3&gt;image restoration&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;High-Fidelity Image Inpainting with GAN Inversion.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yongsheng Yu, Libo Zhang, Heng Fan, Tiejian Luo.&lt;/em&gt;&lt;br&gt; ECCV 2022. [&lt;a href=&#34;https://arxiv.org/pdf/2203.16669&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Escaping Data Scarcity for High-Resolution Heterogeneous Face Hallucination.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yiqun Mei, Pengfei Guo, Vishal M. Patel.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/pdf/2203.16669&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards High-Fidelity Face Self-Occlusion Recovery via Multi-View Residual-Based GAN Inversion.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jinsong Chen, Hu Han, Shiguang Shan.&lt;/em&gt;&lt;br&gt; AAAI 2022. [&lt;a href=&#34;https://www.aaai.org/AAAI22Papers/AAAI-2208.ChenJ.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Time-Travel Rephotography.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://time-travel-rephotography.github.io/&#34;&gt;Xuan Luo&lt;/a&gt;, &lt;a href=&#34;https://people.eecs.berkeley.edu/~cecilia77/&#34;&gt;Xuaner Zhang&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/paul-yoo-768a3715b&#34;&gt;Paul Yoo&lt;/a&gt;, &lt;a href=&#34;http://www.ricardomartinbrualla.com/&#34;&gt;Ricardo Martin-Brualla&lt;/a&gt;, &lt;a href=&#34;http://jasonlawrence.info/&#34;&gt;Jason Lawrence&lt;/a&gt;, &lt;a href=&#34;https://homes.cs.washington.edu/~seitz/&#34;&gt;Steven M. Seitz&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; SIGGRAPH Asia 2021 (TOG). [&lt;a href=&#34;https://arxiv.org/abs/2012.12261&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://time-travel-rephotography.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Time-Travel-Rephotography/Time-Travel-Rephotography.github.io&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GPEN: GAN Prior Embedded Network for Blind Face Restoration in the Wild.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tao Yang, Peiran Ren, Xuansong Xie, Lei Zhang.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_GAN_Prior_Embedded_Network_for_Blind_Face_Restoration_in_the_CVPR_2021_paper.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GLEAN: Generative Latent Bank for Large-Factor Image Super-Resolution.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://ckkelvinchan.github.io/&#34;&gt;Kelvin C.K. Chan&lt;/a&gt;, Xintao Wang, Xiangyu Xu, Jinwei Gu, Chen Change Loy.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2012.00739&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://ckkelvinchan.github.io/projects/GLEAN&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ckkelvinchan/GLEAN&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GFP-GAN: Towards Real-World Blind Face Restoration with Generative Facial Prior.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://xinntao.github.io/&#34;&gt;Xintao Wang&lt;/a&gt;, &lt;a href=&#34;https://yu-li.github.io/&#34;&gt;Yu Li&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?hl=en&amp;amp;user=KjQLROoAAAAJ&#34;&gt;Honglun Zhang&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=4oXBp9UAAAAJ&amp;amp;hl=en&#34;&gt;Ying Shan&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2101.04061&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://xinntao.github.io/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, Cynthia Rudin.&lt;/em&gt;&lt;br&gt; CVPR 2020. [&lt;a href=&#34;https://arxiv.org/abs/2003.03808&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/adamian98/pulse&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Semantic uncertainty intervals for disentangled latent spaces.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Swami Sankaranarayanan, Anastasios N. Angelopoulos, Stephen Bates, Yaniv Romano, Phillip Isola.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2207.10074&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/swamiviv/generative_semantic_uncertainty&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LTT-GAN: Looking Through Turbulence by Inverting GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://kfmei.page/&#34;&gt;Kangfu Mei&lt;/a&gt;, &lt;a href=&#34;https://engineering.jhu.edu/vpatel36/sciencex_teams/vishalpatel/&#34;&gt;Vishal M. Patel&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2112.02379&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://kfmei.page/LTT-GAN/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Style Generator Inversion for Image Enhancement and Animation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;http://www.cs.huji.ac.il/~avivga&#34;&gt;Aviv Gabbay&lt;/a&gt;, &lt;a href=&#34;http://www.cs.huji.ac.il/~ydidh&#34;&gt;Yedid Hoshen&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; arxiv 2019. [&lt;a href=&#34;https://arxiv.org/abs/1906.11880&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;http://www.vision.huji.ac.il/style-image-prior/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/avivga/style-image-prior&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h3&gt;image understanding&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Segmentation in Style: Unsupervised Semantic Image Segmentation with Stylegan and CLIP.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Daniil Pakhomov, Sanchit Hira, Narayani Wagle, Kemar E. Green, Nassir Navab.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/pdf/2107.12518.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://segmentation-in-style.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/warmspringwinds/segmentation_in_style&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Finding an Unsupervised Image Segmenter in each of your Deep Generative Models.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina, Andrea Vedaldi.&lt;/em&gt;&lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://openreview.net/forum?id=Ug-bgjgSlKV&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Labels4Free: Unsupervised Segmentation using StyleGAN.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=kEQimk0AAAAJ&amp;amp;hl=en&#34;&gt;Rameen Abdal&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=Gn8URq0AAAAJ&amp;amp;hl=en&#34;&gt;Peihao Zhu&lt;/a&gt;, &lt;a href=&#34;http://www0.cs.ucl.ac.uk/staff/n.mitra/&#34;&gt;Niloy Mitra&lt;/a&gt;, &lt;a href=&#34;http://peterwonka.net/&#34;&gt;Peter Wonka&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://arxiv.org/abs/2103.14968&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://rameenabdal.github.io/Labels4Free&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/RameenAbdal/Labels4Free&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DatasetGAN: Efficient Labeled Data Factory with Minimal Human Effort.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://www.alexyuxuanzhang.com/&#34;&gt;Yuxuan Zhang&lt;/a&gt;, &lt;a href=&#34;http://www.cs.toronto.edu/~linghuan/&#34;&gt;Huan Ling&lt;/a&gt;, &lt;a href=&#34;http://www.cs.toronto.edu/~jungao/&#34;&gt;Jun Gao&lt;/a&gt;, &lt;a href=&#34;https://kangxue.org/&#34;&gt;Kangxue Yin&lt;/a&gt;, &lt;a href=&#34;&#34;&gt;Jean-Francois Lafleche&lt;/a&gt;, &lt;a href=&#34;&#34;&gt;Adela Barriuso&lt;/a&gt;, &lt;a href=&#34;https://groups.csail.mit.edu/vision/torralbalab/&#34;&gt;Antonio Torralba&lt;/a&gt;, &lt;a href=&#34;http://www.cs.utoronto.ca/~fidler/&#34;&gt;Sanja Fidler&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2104.06490&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/nv-tlabs/datasetGAN_release&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://nv-tlabs.github.io/datasetGAN/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Repurposing GANs for One-shot Semantic Part Segmentation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Nontawat Tritrong, Pitchaporn Rewatbowornwong, &lt;a href=&#34;https://www.supasorn.com/&#34;&gt;Supasorn Suwajanakorn&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2021 (oral). [&lt;a href=&#34;https://arxiv.org/abs/2103.04379&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://repurposegans.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/bryandlee/repurpose-gan&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h3&gt;3D&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Monocular 3D Object Reconstruction with GAN Inversion.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Junzhe Zhang, Daxuan Ren, Zhongang Cai, Chai Kiat Yeo, Bo Dai, Chen Change Loy.&lt;/em&gt;&lt;br&gt; ECCV 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.10061&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://www.mmlab-ntu.com/project/meshinversion/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/junzhezhang/mesh-inversion&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StylePart: Image-based Shape Part Manipulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;I-Chao Shen, Li-Wen Su, Yu-Ting Wu, Bing-Yu Chen.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2111.10520&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CoordGAN: Self-Supervised Dense Correspondences Emerge from GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://jitengmu.github.io/&#34;&gt;Jiteng Mu&lt;/a&gt;, Shalini De Mello, Zhiding Yu, Nuno Vasconcelos, Xiaolong Wang, Jan Kautz, Sifei Liu.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.16521&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://jitengmu.github.io/CoordGAN/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GAN2Shape: Do 2D GANs Know 3D Shape? Unsupervised 3D shape reconstruction from 2D Image GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://xingangpan.github.io/&#34;&gt;Xingang Pan&lt;/a&gt;, Bo Dai, Ziwei Liu, Chen Change Loy, Ping Luo.&lt;/em&gt;&lt;br&gt; ICLR 2021 (oral). [&lt;a href=&#34;https://arxiv.org/abs/2011.00844&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/XingangPan/GAN2Shape&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://xingangpan.github.io/projects/GAN2Shape.html&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Normalized Avatar Synthesis Using StyleGAN and Perceptual Refinement.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Huiwen Luo, Koki Nagano, Han-Wei Kung, Mclean Goldwhite, &lt;a href=&#34;https://qingguo-xu.com/&#34;&gt;Qingguo Xu&lt;/a&gt;, Zejian Wang, Lingyu Wei, Liwen Hu, Hao Li.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2106.11423&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised 3D Shape Completion through GAN-Inversion.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://junzhezhang.github.io/&#34;&gt;Junzhe Zhang&lt;/a&gt;, Xinyi Chen, Zhongang Cai, Liang Pan, Haiyu Zhao, Shuai Yi, Chai Kiat Yeo, Bo Dai, Chen Change Loy.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/pdf/2104.13366&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://junzhezhang.github.io/projects/ShapeInversion/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;OSTeC: One-Shot Texture Completion.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Baris Gecer, Jiankang Deng, Stefanos Zafeiriou.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2012.15370&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/barisgecer/OSTeC&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h3&gt;compressed sensing&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generator Surgery for Compressed Sensing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Niklas Smedemark-Margulies, Jung Yeon Park, Max Daniels, Rose Yu, Jan-Willem van de Meent, Paul Hand.&lt;/em&gt;&lt;br&gt; NeurIPS 2020 Workshop Deep Inverse. [&lt;a href=&#34;https://arxiv.org/abs/2102.11163&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/nik-sm/generator-surgery&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Task-Aware Compressed Sensing with Generative Adversarial Networks.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Maya Kabkab, Pouya Samangouei, Rama Chellappa.&lt;/em&gt;&lt;br&gt; AAAI 2018. [&lt;a href=&#34;https://arxiv.org/pdf/1802.01284.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h3&gt;medical imaging&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Controllable Medical Image Generation via Generative Adversarial Networks.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Zhihang Ren, Stella X. Yu, David Whitney.&lt;/em&gt;&lt;br&gt; Human Vision and Electronic Imaging 2021. [&lt;a href=&#34;https://whitneylab.berkeley.edu/PDFs/Ren_MedImageGen.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High-resolution Controllable Prostatic Histology Synthesis using StyleGAN.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Gagandeep B. Daroach, Josiah A. Yoder, Kenneth A. Iczkowski, Peter S. LaViolette.&lt;/em&gt;&lt;br&gt; BIOIMAGING 2021. [&lt;a href=&#34;https://www.scitepress.org/Papers/2021/103939/103939.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h3&gt;compression and security&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Video Coding Using Learned Latent GAN Compression.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Mustafa Shukor, Bharath Bhushan Damodaran, Xu Yao, Pierre Hellier.&lt;/em&gt;&lt;br&gt; ACM Multimedia 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.04324&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Differentially Private Imaging via Latent Space Manipulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tao Li, Chris Clifton.&lt;/em&gt;&lt;br&gt; IEEE Symposium on Security &amp;amp; Privacy (S&amp;amp;P) 2021. [&lt;a href=&#34;https://arxiv.org/abs/2103.05472&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h2&gt;acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;Thanks for the constructive comments from anonymous reviewers and feedback from &lt;a href=&#34;https://www.cs.cmu.edu/~junyanz/&#34;&gt;Jun-Yan Zhu&lt;/a&gt;, &lt;a href=&#34;https://github.com/anvoynov&#34;&gt;Andrey Voynov&lt;/a&gt;, and &lt;a href=&#34;https://rushila.com/&#34;&gt;Rushil Anirudh&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you find this repo or our paper is helpful for your research, please consider to cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{xia2022gan,&#xA;    author  = {Xia, Weihao and Zhang, Yulun and Yang, Yujiu and Xue, Jing-Hao and Zhou, Bolei and Yang, Ming-Hsuan},&#xA;    title   = {GAN Inversion: A Survey},&#xA;    journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},&#xA;    year={2022}&#xA;  }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p width=&#34;100%&#34; align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/weihaox/awesome-gan-inversion/main/#&#34;&gt;🔝&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>MLNLP-World/Paper-Picture-Writing-Code</title>
    <updated>2022-09-04T01:48:32Z</updated>
    <id>tag:github.com,2022-09-04:/MLNLP-World/Paper-Picture-Writing-Code</id>
    <link href="https://github.com/MLNLP-World/Paper-Picture-Writing-Code" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Paper Picture Writing Code&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;h1 align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/imgs/icon/ai.png&#34; width=&#34;30&#34;&gt;Paper Picture Writing Code&lt;/h1&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://img.shields.io/badge/version-v0.1.0-blue&#34;&gt; &lt;img alt=&#34;version&#34; src=&#34;https://img.shields.io/badge/version-v0.1.0-blue?color=FF8000?color=009922&#34;&gt; &lt;/a&gt; &lt;a&gt; &lt;img alt=&#34;Status-building&#34; src=&#34;https://img.shields.io/badge/Status-building-blue&#34;&gt; &lt;/a&gt; &lt;a&gt; &lt;img alt=&#34;PRs-Welcome&#34; src=&#34;https://img.shields.io/badge/PRs-Welcome-red&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/MLNLP-World/Paper-Picture-Writing-Code/stargazers&#34;&gt; &lt;img alt=&#34;stars&#34; src=&#34;https://img.shields.io/github/stars/MLNLP-World/Paper-Picture-Writing-Code&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/MLNLP-World/Paper-Picture-Writing-Code/network/members&#34;&gt; &lt;img alt=&#34;FORK&#34; src=&#34;https://img.shields.io/github/forks/MLNLP-World/Paper-Picture-Writing-Code?color=FF8000&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/MLNLP-World/Paper-Picture-Writing-Code/issues&#34;&gt; &lt;img alt=&#34;Issues&#34; src=&#34;https://img.shields.io/github/issues/MLNLP-World/Paper-Picture-Writing-Code?color=0088ff&#34;&gt; &lt;/a&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;#项目动机&#34;&gt;项目动机&lt;/a&gt;/ &lt;a href=&#34;#项目简介&#34;&gt;项目简介&lt;/a&gt;/ &lt;a href=&#34;#参考资源&#34;&gt;参考资源&lt;/a&gt;/ &lt;a href=&#34;#目录&#34;&gt;目录&lt;/a&gt;/ &lt;a href=&#34;#文件夹说明&#34;&gt;文件夹说明&lt;/a&gt;/ &lt;a href=&#34;#组织者&#34;&gt;组织者&lt;/a&gt;/ &lt;a href=&#34;#贡献者&#34;&gt;贡献者&lt;/a&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/imgs/icon/motivation.png&#34; width=&#34;25&#34;&gt;项目动机&lt;/h2&gt; &#xA;&lt;p&gt;对于很多科研新手来说论文图片的绘制常常成为论文写作中一大难题，图像失真、图示不合规范等问题会导致论文质量的降低，LaTex原生支持的图包绘制的矢量图具有不会出现失真、便于实时修改和维护等优点，为了帮助一些同学提高论文写作画图的质量和效率、学会更好地利用&lt;strong&gt;LaTex画图&lt;/strong&gt;，本项目基于&lt;strong&gt;LaTex&lt;/strong&gt;开源论文中常用的&lt;strong&gt;画图代码&lt;/strong&gt;，希望能对大家的论文写作图片绘制有所帮助。&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;本项目的特色：&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;&lt;strong&gt;代码开源&lt;/strong&gt;：为初学者提供了可学习和可复用的代码样例，方便大家快速入门。&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;笔记详细&lt;/strong&gt;：帮助此项目基础上进一步上手实践，也便于快速定位自己代码中的错误。&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;类别广繁&lt;/strong&gt;：涵盖AI论文画图中常用的几类画图代码。&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;由于我们的水平有限，如有错误与疏漏，还望谅解，有任何问题欢迎随时指出，我们会进行更正，谢谢大家。&lt;br&gt; 本项目所用徽章来自互联网，如侵犯了您的图片版权请联系我们删除，谢谢。&lt;/p&gt; &#xA;&lt;h2&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/imgs/icon/intro.png&#34; width=&#34;25&#34;&gt;项目简介&lt;/h2&gt; &#xA;&lt;p&gt;本项目目前基于LaTex开源了几种AI论文中常用的画图代码，共包含&lt;strong&gt;折线图&lt;/strong&gt;、&lt;strong&gt;柱状图&lt;/strong&gt;、&lt;strong&gt;散点图&lt;/strong&gt;、&lt;strong&gt;注意力可视化&lt;/strong&gt;以及&lt;strong&gt;结构图&lt;/strong&gt;五种图，供各位同学参考。此外，附有详细的代码笔记讲解，可供同学们进一步学习以及拓展实践。&lt;/p&gt; &#xA;&lt;h2&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/imgs/icon/resource.png&#34; width=&#34;25&#34;&gt;参考资源&lt;/h2&gt; &#xA;&lt;p&gt;该项目部分代码参考了 &lt;a href=&#34;https://github.com/NiuTrans/MTBook&#34;&gt;MT Book&lt;/a&gt;，MT Book是一本高质量的机器翻译书籍。书中所有的latex代码已经开源，也可以作为大家latex画图的一个参考。&lt;/p&gt; &#xA;&lt;h2&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/imgs/icon/catalogue.png&#34; width=&#34;25&#34;&gt;目录&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;图类别&lt;/th&gt; &#xA;   &lt;th&gt;代码&lt;/th&gt; &#xA;   &lt;th&gt;笔记&lt;/th&gt; &#xA;   &lt;th&gt;贡献者&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/imgs/category/intro.png&#34; width=&#34;100&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/README.md&#34;&gt;引言&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/YudiZh&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/imgs/profile/yudi_zhang.jpg&#34; width=&#34;80&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/imgs/category/line.png&#34; width=&#34;100&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/code/line_chart.tex&#34;&gt;折线图&lt;/a&gt;、&lt;a href=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/code/line_chart_with_error_bar.tex&#34;&gt;带误差条的折线图&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/notes/line_chart.pdf&#34;&gt;折线图&lt;/a&gt;、&lt;a href=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/notes/line_chart_with_error_bar.pdf&#34;&gt;带误差条的折线图&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/JoeYing1019&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/imgs/profile/shijue_huang.jpg&#34; width=&#34;80&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/imgs/category/histogram.png&#34; width=&#34;100&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/code/histogram.tex&#34;&gt;柱状图&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/notes/histogram-latex.pdf&#34;&gt;柱状图&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/yizhen20133868&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/imgs/profile/libo_qin.jpg&#34; width=&#34;80&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/imgs/category/scatter.png&#34; width=&#34;100&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/code/scatter_diagram.tex&#34;&gt;散点图&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/notes/scatter_diagram.md&#34;&gt;散点图&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/libeineu&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/imgs/profile/bei_li.jpg&#34; width=&#34;80&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/imgs/category/text_attention.png&#34; width=&#34;100&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/code/text_attention.tex&#34;&gt;注意力序列可视化&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/notes/text_attention.md&#34;&gt;注意力序列可视化&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/SivilTaram&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/imgs/profile/qian_liu.jpg&#34; width=&#34;80&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/imgs/category/attention_distribution.png&#34; width=&#34;100&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/code/attention_distribution.tex&#34;&gt;注意力分布&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/notes/attention_distribution.md&#34;&gt;注意力分布&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/libeineu&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/imgs/profile/bei_li.jpg&#34; width=&#34;80&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/imgs/category/struct.png&#34; width=&#34;100&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/code/transformer.tex&#34;&gt;结构图&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/notes/transformer.md&#34;&gt;结构图&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/libeineu&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/imgs/profile/bei_li.jpg&#34; width=&#34;80&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/imgs/icon/folders.png&#34; width=&#34;25&#34;&gt;文件夹说明&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;code&lt;/strong&gt;：图片绘制代码&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;notes&lt;/strong&gt;: 图片绘制笔记&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;imgs&lt;/strong&gt;：项目中的图片&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/imgs/icon/organizer.png&#34; width=&#34;25&#34;&gt;组织者&lt;/h2&gt; &#xA;&lt;p&gt;感谢以下同学对本项目的组织&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/libeineu&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/imgs/profile/bei_li.jpg&#34; width=&#34;80&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/SivilTaram&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/imgs/profile/qian_liu.jpg&#34; width=&#34;80&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/JoeYing1019&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/imgs/profile/shijue_huang.jpg&#34; width=&#34;80&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/YudiZh&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/imgs/profile/yudi_zhang.jpg&#34; width=&#34;80&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/yizhen20133868&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/imgs/profile/libo_qin.jpg&#34; width=&#34;80&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/imgs/icon/heart.png&#34; width=&#34;25&#34;&gt;贡献者&lt;/h2&gt; &#xA;&lt;p&gt;感谢以下同学对本项目的支持与贡献&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/libeineu&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/imgs/profile/bei_li.jpg&#34; width=&#34;80&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/SivilTaram&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/imgs/profile/qian_liu.jpg&#34; width=&#34;80&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/JoeYing1019&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/imgs/profile/shijue_huang.jpg&#34; width=&#34;80&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/YudiZh&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/imgs/profile/yudi_zhang.jpg&#34; width=&#34;80&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/yizhen20133868&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MLNLP-World/Paper-Picture-Writing-Code/main/imgs/profile/libo_qin.jpg&#34; width=&#34;80&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>