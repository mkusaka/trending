<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TeX Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-26T02:04:07Z</updated>
  <subtitle>Weekly Trending of TeX in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>libretro/docs</title>
    <updated>2023-03-26T02:04:07Z</updated>
    <id>tag:github.com,2023-03-26:/libretro/docs</id>
    <link href="https://github.com/libretro/docs" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This is a repo of the RetroArch official document page.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Contribute to the documentation&lt;/h1&gt; &#xA;&lt;p&gt;The docs are written in &lt;a href=&#34;https://en.wikipedia.org/wiki/Markdown&#34;&gt;Markdown&lt;/a&gt; if you need help with the syntax use &lt;a href=&#34;https://guides.github.com/features/mastering-markdown/&#34;&gt;this guide&lt;/a&gt;. Mkdocs uses some &lt;a href=&#34;http://www.mkdocs.org/user-guide/writing-your-docs/#markdown-extensions&#34;&gt;Markdown extensions&lt;/a&gt; that you may have to familiarize with.&lt;/p&gt; &#xA;&lt;p&gt;The documentation source is maintained via &lt;a href=&#34;https://en.wikipedia.org/wiki/Git&#34;&gt;Git&lt;/a&gt;. For more info on how to use git &lt;a href=&#34;https://help.github.com/&#34;&gt;refer to their help&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;In order to propose improvements to a document:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/libretro/docs&#34;&gt;Clone the repo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Make the changes and update your clone&lt;/li&gt; &#xA; &lt;li&gt;Follow the &#34;Building the docs&#34; section to render the documentation site locally&lt;/li&gt; &#xA; &lt;li&gt;Propose your changes using the button &lt;code&gt;New Pull Request&lt;/code&gt; &lt;a href=&#34;https://github.com/libretro/docs&#34;&gt;in the docs repo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;There is a To-Do list for libretro/docs &lt;em&gt;here&lt;/em&gt; and you can submit suggestions or issues regarding documentation at the &lt;a href=&#34;https://github.com/libretro/docs/issues&#34;&gt;libretro/docs issue tracker&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Building the docs&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Make sure you have &lt;a href=&#34;https://www.python.org/&#34;&gt;Python&lt;/a&gt; and &lt;a href=&#34;https://pip.pypa.io&#34;&gt;pip&lt;/a&gt; installed &lt;pre&gt;&lt;code&gt;python --version&#xA;pip --version&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;!!! Note &#34;Building in Windows/msys2&#34; If you are using the standard RetroArch msys2 environment, you will need to install python with the command &lt;code&gt;pacman -S python&lt;/code&gt;. Next you will need to download &lt;a href=&#34;https://bootstrap.pypa.io/get-pip.py&#34;&gt;the &lt;code&gt;get-pip.py&lt;/code&gt; script&lt;/a&gt; from the &lt;code&gt;pip&lt;/code&gt; bootstrap site. Finally, execute the script with the command &lt;code&gt;python get-pip.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;Install MkDocs&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install mkdocs&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install MkDocs-Material&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install mkdocs-material&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install PyMdown Extensions&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install pymdown-extensions&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install mkdocs-git-revision-date&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install mkdocs-git-revision-date-plugin&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install mkdocs-macros&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install mkdocs-macros-plugin&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Build the site&lt;/p&gt; &lt;pre&gt;&lt;code&gt;mkdocs build&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The documentation will be built to the &lt;code&gt;site&lt;/code&gt; directory; preview any changes with MkDocs&#39; built-in dev-server before submitting a pull request&lt;/p&gt; &lt;pre&gt;&lt;code&gt;mkdocs serve&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.mkdocs.org/#installation&#34;&gt;Guide to installing mkdocs&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Adding a new core&lt;/h2&gt; &#xA;&lt;p&gt;These are the documents that should be added/updated when a new core is added to the libretro ecosystem.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add the core to docs/library/ (Follow the latest core template. docs/meta/core-template.md)&lt;/li&gt; &#xA; &lt;li&gt;Add the core to mkdocs.yml&lt;/li&gt; &#xA; &lt;li&gt;Add the core to docs/meta/core-list.md&lt;/li&gt; &#xA; &lt;li&gt;Add the core to docs/meta/see-also.md if it&#39;s related to another core in some way&lt;/li&gt; &#xA; &lt;li&gt;Add the core to docs/development/licenses.md&lt;/li&gt; &#xA; &lt;li&gt;Add the core to docs/guides/softpatching.md if it supports softpatching&lt;/li&gt; &#xA; &lt;li&gt;Add the core to docs/guides/retroachievements.md if it supports cheevos&lt;/li&gt; &#xA; &lt;li&gt;Add the core to docs/library/bios.md if it needs a BIOS&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>KSESEU/LLMPapers</title>
    <updated>2023-03-26T02:04:07Z</updated>
    <id>tag:github.com,2023-03-26:/KSESEU/LLMPapers</id>
    <link href="https://github.com/KSESEU/LLMPapers" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Papers &amp; Works for large languange models (ChatGPT, GPT-3, Codex etc.).&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Resources on ChatGPT and Large Language Models&lt;/h1&gt; &#xA;&lt;p&gt;Collection of papers and related works for Large Language Models (ChatGPT, GPT-3, Codex etc.).&lt;/p&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;p&gt;This repository is contributed by the following contributors.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Organizers&lt;/strong&gt;: &lt;a href=&#34;https://cse.seu.edu.cn/2019/0103/c23024a257135/page.htm&#34;&gt;Guilin Qi (漆桂林)&lt;/a&gt;, &lt;a href=&#34;https://cse.seu.edu.cn/2019/0103/c23024a257134/page.htm&#34;&gt;Xiaofang Qi (戚晓芳)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Paper Collectors&lt;/strong&gt;: Zafar Ali, &lt;a href=&#34;https://github.com/bisheng&#34;&gt;Sheng Bi (毕胜)&lt;/a&gt;, &lt;a href=&#34;https://github.com/Bahuia&#34;&gt;Yongrui Chen (陈永锐)&lt;/a&gt;, Zizhuo Chen (陈孜卓), &lt;a href=&#34;https://github.com/OBriennnnn&#34;&gt;Xinbang Dai (戴鑫邦)&lt;/a&gt;, Huan Gao (高桓), &lt;a href=&#34;https://github.com/HuuuNan&#34;&gt;Nan Hu (胡楠)&lt;/a&gt;, Shilong Hu (胡世龙), &lt;a href=&#34;https://github.com/JingqiKang&#34;&gt;Jingqi Kang (康婧淇)&lt;/a&gt;, &lt;a href=&#34;https://github.com/aoluming&#34;&gt;Jiaqi Li (李嘉琦)&lt;/a&gt;, &lt;a href=&#34;https://github.com/ZhishanQ&#34;&gt;Dehai Min (闵德海)&lt;/a&gt;, Yiming Tan (谭亦鸣), &lt;a href=&#34;http://wutong8023.site/&#34;&gt;Tongtong Wu (吴桐桐)&lt;/a&gt;, &lt;a href=&#34;https://github.com/SonglinZhai&#34;&gt;Songlin Zhai (翟松林)&lt;/a&gt;, &lt;a href=&#34;https://github.com/Zzyx1996&#34;&gt;Yuxin Zhang (张裕欣)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Maintainers&lt;/strong&gt;: &lt;a href=&#34;https://github.com/sid0527&#34;&gt;Runzhe Wang (王润哲)&lt;/a&gt;, &lt;a href=&#34;https://github.com/ZSY-SZ&#34;&gt;Shenyu Zhang (张沈昱)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The automation script of this repo is powered by &lt;a href=&#34;https://github.com/wutong8023/Auto-Bibfile.git&#34;&gt;Auto-Bibfile&lt;/a&gt;. If you&#39;d like to commit to this repo, please modify &lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/bibtex.bib&#34;&gt;bibtex.bib&lt;/a&gt; or &lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/related_works.json&#34;&gt;related_works.json&lt;/a&gt; and re-generate &lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/README.md&#34;&gt;README.md&lt;/a&gt; using &lt;code&gt;python scripts/run.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Papers&lt;/h2&gt; &#xA;&lt;h3&gt;Outline&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#evaluation&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Evaluation-23-blue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#survey&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Survey-20-blue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#in-context-learning&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/In--Context_Learning-25-blue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#instruction-tuning&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Instruction_Tuning-8-blue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#rlhf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/RLHF-17-blue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#pre-training-techniques&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Pre--Training_Techniques-19-blue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#mixtures-of-experts&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Mixtures_of_Experts-4-deepskyblue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#knowledge-enhanced&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Knowledge_Enhanced-19-blue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#knowledge-distillation&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Knowledge_Distillation-23-blue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#knowledge-generation&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Knowledge_Generation-9-blue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#knowledge-editing&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Knowledge_Editing-7-blue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#reasoning&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Reasoning-60-blue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#chain-of-thought&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Chain_of_Thought-27-deepskyblue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#multi-step-reasoning&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Multi--Step_Reasoning-3-deepskyblue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#arithmetic-reasoning&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Arithmetic_Reasoning-4-deepskyblue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#symbolic-reasoning&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Symbolic_Reasoning-5-deepskyblue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#federated-learning&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Federated_Learning-14-blue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#distributed-ai&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Distributed_AI-9-blue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#selective-annotation&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Selective_Annotation-2-blue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#code-generation&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code_Generation-41-blue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#code-representation&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code_Representation-5-deepskyblue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#code-fixing&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code_Fixing-8-deepskyblue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#code-review&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code_Review-5-deepskyblue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#software-engineering&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Software_Engineering-5-blue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#aigc&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/AIGC-77-blue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#controllable-text-generation&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Controllable_Text_Generation-9-deepskyblue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#continual-learning&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Continual_Learning-42-blue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#prompt-engineering&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Prompt_Engineering-30-blue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#natural-language-understanding&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Natural_Language_Understanding-8-blue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#multimodal&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Multimodal-21-blue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#multilingual&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Multilingual-1-blue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#reliability&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Reliability-5-blue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#robustness&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Robustness-2-blue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#dialogue-system&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Dialogue_System-15-blue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#recommender-system&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Recommender_System-7-blue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#event-extraction&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Event_Extraction-6-blue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#event-relation-extraction&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Event_Relation_Extraction-6-blue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#data-argumentation&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Data_Argumentation-4-blue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#data-annotation&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Data_Annotation-2-blue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#information-extraction&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Information_Extraction-3-blue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#domain-adaptive&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Domain_Adaptive-3-blue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#question-answering&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Question_Answering-5-blue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#application&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Application-3-blue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#meta-learning&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Meta_Learning-2-blue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#generalizability&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Generalizability-3-blue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#language-model-as-knowledge-base&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Language_Model_as_Knowledge_Base-6-blue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#retrieval-augmented-language-model&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Retrieval--Augmented_Language_Model-11-blue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#quality&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Quality-1-blue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#interpretability/explainability&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Interpretability/Explainability-3-blue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#data-generation&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Data_Generation-3-blue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/./README.md#others&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Others-6-blue&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Hyperlinks&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/README.md&#34;&gt;[Overview]&lt;/a&gt; -- &lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/README.md&#34;&gt;Homepage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;-- &lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/taxonomy/./&#34;&gt;Summary&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;-- &lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/taxonomy/author&#34;&gt;Author&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;-- &lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/taxonomy/techniques&#34;&gt;Techniques&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;-- &lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/taxonomy/time&#34;&gt;Published Time&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;-- &lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/taxonomy/venue&#34;&gt;Published Venue&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Evaluation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.04023&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.04023&#34;&gt;&lt;strong&gt;A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji et al.&lt;/em&gt; &lt;br&gt;&lt;code&gt;本文提出了一个使用公开数据集定量评估交互式LLM（如ChatGPT）的框架。我们使用涵盖8个不同的常见NLP应用任务的21个数据集对ChatGPT进行了广泛的技术评估。我们基于这些数据集和一个新设计的多模态数据集评估了ChatGPT的多任务、多语言和多模态方面。&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.06476&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2302.06476&#34;&gt;&lt;strong&gt;Is ChatGPT a General-Purpose Natural Language Processing Task Solver?&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Qin, Chengwei, Zhang, Aston, Zhang, Zhuosheng, Chen, Jiaao, Yasunaga, Michihiro and Yang, Diyi&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.06466&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.06466&#34;&gt;&lt;strong&gt;ChatGPT versus Traditional Question Answering for Knowledge Graphs: Current Status and Future Directions Towards Knowledge Graph Chatbots&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Reham Omar, Omij Mangukiya, Panos Kalnis and Essam Mansour&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.13867&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.13867&#34;&gt;&lt;strong&gt;Mathematical Capabilities of ChatGPT&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Christian Petersen, Alexis Chevalier and Julius Berner&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.08081&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.08081&#34;&gt;&lt;strong&gt;Exploring the Limits of ChatGPT for Query or Aspect-based Text Summarization&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Xianjun Yang, Yan Li, Xinlu Zhang, Haifeng Chen and Wei Cheng&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.12095&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.12095&#34;&gt;&lt;strong&gt;On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Jindong Wang, Xixu Hu, Wenxin Hou, Hao Chen, Runkai Zheng, Yidong Wang, Linyi Yang, Haojun Huang et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.04655&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.04655&#34;&gt;&lt;strong&gt;ChatGPT is not all you need. A State of the Art Review of large Generative AI models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Roberto Gozalo-Brizuela and Eduardo C. Garrido-Merch&#39;an&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.10198&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2302.10198&#34;&gt;&lt;strong&gt;Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Qihuang Zhong, Liang Ding, Juhua Liu, Bo Du and Dacheng Tao&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2303.07992&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2303.07992&#34;&gt;&lt;strong&gt;Evaluation of ChatGPT as a Question Answering System for Answering Complex Questions&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Yiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan Hu, Yongrui Chen and Guilin Qi&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2211.09110&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2211.09110&#34;&gt;&lt;strong&gt;Holistic Evaluation of Language Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2204.00498&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2204.00498&#34;&gt;&lt;strong&gt;Evaluating the Text-to-SQL Capabilities of Large Language Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Nitarshan Rajkumar, Raymond Li and Dzmitry Bahdanau&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2022.coling-1.491&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/COLING-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2022.coling-1.491&#34;&gt;&lt;strong&gt;Are Visual-Linguistic Models Commonsense Knowledge Bases?&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Hsiu-Yu Yang and Carina Silberer&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.10529&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.10529&#34;&gt;&lt;strong&gt;Is GPT-3 a Psychopath? Evaluating Large Language Models from a Psychological Perspective&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Xingxuan Li, Yutong Li, Linlin Liu, Lidong Bing and Shafiq R. Joty&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.132&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.132&#34;&gt;&lt;strong&gt;GeoMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Da Yin, Hritik Bansal, Masoud Monajatipoor, Liunian Harold Li and Kai-Wei Chang&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.653&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.653&#34;&gt;&lt;strong&gt;RobustLR: A Diagnostic Benchmark for Evaluating Logical Robustness of Deductive Reasoners&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Soumya Sanyal, Zeyi Liao and Xiang Ren&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2107.03374&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2107.03374&#34;&gt;&lt;strong&gt;Evaluating Large Language Models Trained on Code&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pond&#39;e de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.findings-acl.36&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.findings-acl.36&#34;&gt;&lt;strong&gt;GLGE: A New General Language Generation Evaluation Benchmark&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Dayiheng Liu, Yu Yan, Yeyun Gong, Weizhen Qi, Hang Zhang, Jian Jiao, Weizhu Chen, Jie Fu et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2104.05861&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2104.05861&#34;&gt;&lt;strong&gt;Evaluating Pre-Trained Models for User Feedback Analysis in Software Engineering: A Study on Classification of App-Reviews&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Mohammad Abdul Hadi and Fatemeh H. Fard&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.findings-acl.322&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL_Findings-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.findings-acl.322&#34;&gt;&lt;strong&gt;Do Language Models Perform Generalizable Commonsense Inference?&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/wangpf3/LM-for-CommonsenseInference&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code-skyblue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;br&gt; by &lt;em&gt;Peifeng Wang, Filip Ilievski, Muhao Chen and Xiang Ren&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.emnlp-main.598&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.emnlp-main.598&#34;&gt;&lt;strong&gt;RICA: Evaluating Robust Inference Capabilities Based on Commonsense Axioms&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Pei Zhou, Rahul Khanna, Seyeon Lee, Bill Yuchen Lin, Daniel Ho, Jay Pujara and Xiang Ren&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2006.14799&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2006.14799&#34;&gt;&lt;strong&gt;Evaluation of Text Generation: A Survey&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Asli Celikyilmaz, Elizabeth Clark and Jianfeng Gao&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2007.15780&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2007.15780&#34;&gt;&lt;strong&gt;Neural Language Generation: Formulation, Methods, and Evaluation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Cristina Garbacea and Qiaozhu Mei&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=SkeHuCVFDr&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ICLR-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://openreview.net/forum?id=SkeHuCVFDr&#34;&gt;&lt;strong&gt;BERTScore: Evaluating Text Generation with BERT&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger and Yoav Artzi&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Survey&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.00234&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.00234&#34;&gt;&lt;strong&gt;A Survey for In-context Learning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu et al.&lt;/em&gt; &lt;br&gt;&lt;code&gt;This paper surveys and summarizes the progress and challenges of ICL, including ICL&#39;s formal definition, correlation to related studies, advanced techniques (training strategies, related analysis) and potential directions.&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.09419&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.09419&#34;&gt;&lt;strong&gt;A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Ce Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu, Guangjing Wang, Kai Zhang, Cheng Ji et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.09051&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.09051&#34;&gt;&lt;strong&gt;Complex QA and language models hybrid architectures, Survey&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Xavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin and Elisabeth Murisasco&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.07842&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.07842&#34;&gt;&lt;strong&gt;Augmented Language Models: a Survey&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Gr&#39;egoire Mialon, Roberto Dess`\i, Maria Lomeli, Christoforos Nalmpantis, Ramakanth Pasunuru, Roberta Raileanu, Baptiste Rozi`ere, Timo Schick et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.09420&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.09420&#34;&gt;&lt;strong&gt;When Neural Model Meets NL2Code: A Survey&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Daoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu, Bingchao Wu, Bei Guan, Yongji Wang and Jian-Guang Lou&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.13428&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/TKDE-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.13428&#34;&gt;&lt;strong&gt;A Survey on Knowledge-Enhanced Pre-trained Language Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Chaoqi Zhen, Yanlei Shang, Xiangyu Liu, Yifei Li, Yong Chen and Dell Zhang&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1109/TPAMI.2021.3057446&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/T--PAMI-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1109/TPAMI.2021.3057446&#34;&gt;&lt;strong&gt;A Continual Learning Survey: Defying Forgetting in Classification Tasks&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory G. Slabaugh and Tinne Tuytelaars&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1016/j.jksuci.2020.04.001&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/JKSUCIS-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1016/j.jksuci.2020.04.001&#34;&gt;&lt;strong&gt;The survey: Text generation models in deep learning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Touseef Iqbal and Shaima Qureshi&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1007/s10115-022-01664-x&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/KIS-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1007/s10115-022-01664-x&#34;&gt;&lt;strong&gt;From distributed machine learning to federated learning: a survey&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Ji Liu, Jizhou Huang, Yang Zhou, Xuhong Li, Shilei Ji, Haoyi Xiong and Dejing Dou&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.24963/ijcai.2022/775&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/IJCAI-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.24963/ijcai.2022/775&#34;&gt;&lt;strong&gt;Deep Learning Meets Software Engineering: A Survey on Pre-Trained Models of Source Code&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Changan Niu, Chuanyi Li, Bin Luo and Vincent Ng&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1109/TKDE.2020.3028705&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/TKDE-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1109/TKDE.2020.3028705&#34;&gt;&lt;strong&gt;A Survey on Knowledge Graph-Based Recommender Systems&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Qingyu Guo, Fuzhen Zhuang, Chuan Qin, Hengshu Zhu, Xing Xie, Hui Xiong and Qing He&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.09252&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.09252&#34;&gt;&lt;strong&gt;Mind the Knowledge Gap: A Survey of Knowledge-enhanced Dialogue Systems&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Sagi Shaier, Lawrence Hunter and Katharina Kann&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.10403&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.10403&#34;&gt;&lt;strong&gt;Towards Reasoning in Large Language Models: A Survey&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Jie Huang and Kevin Chen-Chuan Chang&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.09597&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.09597&#34;&gt;&lt;strong&gt;Reasoning with Language Model Prompting: A Survey&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2202.01110&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2202.01110&#34;&gt;&lt;strong&gt;A Survey on Retrieval-Augmented Text Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Huayang Li, Yixuan Su, Deng Cai, Yan Wang and Lemao Liu&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/view/21496&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/AAAI-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/view/21496&#34;&gt;&lt;strong&gt;Commonsense Knowledge Reasoning and Generation with Pre-trained Language Models: A Survey&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Prajjwal Bhargava and Vincent Ng&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2101.09459&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2101.09459&#34;&gt;&lt;strong&gt;Advances and Challenges in Conversational Recommender Systems: A Survey&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Chongming Gao, Wenqiang Lei, Xiangnan He, Maarten de Rijke and Tat-Seng Chua&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2105.04387&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2105.04387&#34;&gt;&lt;strong&gt;Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Survey&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Jinjie Ni, Tom Young, Vlad Pandelea, Fuzhao Xue, Vinay Adiga and Erik Cambria&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.emnlp-main.81&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.emnlp-main.81&#34;&gt;&lt;strong&gt;Relational World Knowledge Representation in Contextual Language Models: A Review&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Tara Safavi and Danai Koutra&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2006.14799&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2006.14799&#34;&gt;&lt;strong&gt;Evaluation of Text Generation: A Survey&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Asli Celikyilmaz, Elizabeth Clark and Jianfeng Gao&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;In-Context Learning&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.00234&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.00234&#34;&gt;&lt;strong&gt;A Survey for In-context Learning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu et al.&lt;/em&gt; &lt;br&gt;&lt;code&gt;This paper surveys and summarizes the progress and challenges of ICL, including ICL&#39;s formal definition, correlation to related studies, advanced techniques (training strategies, related analysis) and potential directions.&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.04813&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.04813&#34;&gt;&lt;strong&gt;Explanation Selection Using Unlabeled Data for In-Context Learning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Xi Ye and Greg Durrett&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.04931&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.04931&#34;&gt;&lt;strong&gt;In-Context Learning with Many Demonstration Examples&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Mukai Li, Shansan Gong, Jiangtao Feng, Yiheng Xu, Jun Zhang, Zhiyong Wu and Lingpeng Kong&lt;/em&gt; &lt;br&gt;&lt;code&gt;This paper proposes a LM named EvaLM to scale up the sequence length (trained with 8k tokens per batch line). Experiments based on EvaLM prove that in-context learning can achieve higher performance with more demonstrations under many-shot instruction tuning (8k) and further extending the length of instructions (16k) can further improve the upper bound of scaling in-context learning.&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.11916&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.11916&#34;&gt;&lt;strong&gt;Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Xinyi Wang, Wanrong Zhu and William Yang Wang&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.13539&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.13539&#34;&gt;&lt;strong&gt;Finding Supporting Examples for In-Context Learning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Xiaonan Li and Xipeng Qiu&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2022.acl-long.53&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2022.acl-long.53&#34;&gt;&lt;strong&gt;Meta-learning via Language Model In-context Tuning&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://aclanthology.org/N19-1423/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/BERT-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2006.03654&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/DeBERTa-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GPT--2-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;br&gt; by &lt;em&gt;Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis and He He&lt;/em&gt; &lt;br&gt;&lt;code&gt;This paper proposes in-context tuning, which recasts task adaptation and prediction as a simple sequence prediction problem: to form the input sequence, concatenate the task instruction, labeled in-context examples, and the target input to predict; to meta train the model to learn from in-context examples, finetune a PLM to predict the target label given the input sequence on a collection of tasks (very similar to MetaICL). On LAMA and BinaryClfs, the proposed method outperforms MAML.&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2022.naacl-main.201&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NAACL-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2022.naacl-main.201&#34;&gt;&lt;strong&gt;MetaICL: Learning to Learn In Context&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/facebookresearch/MetaICL&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code-skyblue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GPT--2-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;br&gt; by &lt;em&gt;Sewon Min, Mike Lewis, Luke Zettlemoyer and Hannaneh Hajishirzi&lt;/em&gt; &lt;br&gt;&lt;code&gt;MetaICL proposes a supervised meta-training framework to enable LMs to more effectively learn a new task in context. In MetaICL, each meta-training example includes several training examples from one task that will be presented together as a single sequence to the LM, and the prediction of the final example is used to calculate the loss.&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2209.01975&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2209.01975&#34;&gt;&lt;strong&gt;Selective Annotation Makes Language Models Better Few-Shot Learners&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/HKUNLP/icl-selective-annotation&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code-skyblue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/D19-1410/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/SBERT-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/gptj&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GPT--J-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/gpt_neo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GPT--Neo-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GPT--3-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2107.03374&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Codex-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2205.01068&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/OPT-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;br&gt; by &lt;em&gt;Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf et al.&lt;/em&gt; &lt;br&gt;&lt;code&gt;This paper proposes a graph-based selective annotation method named vote-k to&lt;/code&gt;&lt;br&gt;&lt;code&gt;(1) select a pool of examples to annotate from unlabeled data,&lt;/code&gt;&lt;br&gt;&lt;code&gt;(2) retrieve prompts (contexts) from the annotated data pool for in-context learning.&lt;/code&gt;&lt;br&gt;&lt;code&gt;Specifically, the selection method first selects a small set of unlabeled examples iteratively and then labels them to serve as contexts for LLMs to predict the labels of the rest unlabeled data. The method selects the predictions with highest confidence (log probability of generation output) to fill up the selective annotation pool.&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2022.naacl-main.260&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NAACL-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2022.naacl-main.260&#34;&gt;&lt;strong&gt;Improving In-Context Few-Shot Learning via Self-Supervised Training&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://doi.org/10.18653/v1/2022.naacl-main.260&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/MoE-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;br&gt; by &lt;em&gt;Mingda Chen, Jingfei Du, Ramakanth Pasunuru, Todor Mihaylov, Srini Iyer, Veselin Stoyanov and Zornitsa Kozareva&lt;/em&gt; &lt;br&gt;&lt;code&gt;This paper proposes to use self-supervision (MLM, NSP, CL, etc.) between pre-training and downstream usage to teach the LM to perform in-context learning. Analysis reveals that:&lt;/code&gt;&lt;br&gt;&lt;code&gt;(1) benefits of self-supervised depends on the amount of training data,&lt;/code&gt;&lt;br&gt;&lt;code&gt;(2) semantic similarity between training and evaluation tasks matters,&lt;/code&gt;&lt;br&gt;&lt;code&gt;(3) adding training objectives without diversity does not help,&lt;/code&gt;&lt;br&gt;&lt;code&gt;(4) model performance improves when choosing similar templates for both self-supervised and downstream tasks,&lt;/code&gt;&lt;br&gt;&lt;code&gt;(5) self-supervised tasks and human-annotated datasets are complementary,&lt;/code&gt;&lt;br&gt;&lt;code&gt;(6) self-supervised-trained models are better at following task instructions.&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2205.10782&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2205.10782&#34;&gt;&lt;strong&gt;Instruction Induction: From Few Examples to Natural Language Task Descriptions&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Or Honovich, Uri Shaham, Samuel R. Bowman and Omer Levy&lt;/em&gt; &lt;br&gt;&lt;code&gt;(1) 探索了利用LLM在几个样本的情况下归纳出任务指令的能力；&lt;/code&gt;&lt;br&gt;&lt;code&gt;(2) 测量两个指标：1. 模型归纳指令与人类归纳的指令对比，2. 利用模型归纳的指令作为prompt进行预测的执行准确率；&lt;/code&gt;&lt;br&gt;&lt;code&gt;(3) 相比于GPT-3，InstructGPT效果更好，理所当然。&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2022.acl-long.556&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2022.acl-long.556&#34;&gt;&lt;strong&gt;Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GPT--2-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GPT--3-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;br&gt; by &lt;em&gt;Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel and Pontus Stenetorp&lt;/em&gt; &lt;br&gt;&lt;code&gt;(1) This work demonstrates that few-shot prompts suffer from order sensitivity, in that for the same prompt the order in which samples are provided can make a difference to model performance.&lt;/code&gt;&lt;br&gt;&lt;code&gt;(2) This work introduces a probing method which constructs an artificial development set by language models themselves to alleviate the order sensitivity problem.&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2022.naacl-main.191&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NAACL-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2022.naacl-main.191&#34;&gt;&lt;strong&gt;Learning To Retrieve Prompts for In-Context Learning&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GPT--3-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/gpt_neo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GPT--Neo-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2107.03374&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Codex-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/gptj&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GPT--J-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/D19-1410/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/SBERT-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/N19-1423/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/BERT-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;br&gt; by &lt;em&gt;Ohad Rubin, Jonathan Herzig and Jonathan Berant&lt;/em&gt; &lt;br&gt;&lt;code&gt;This paper proposes a method to retrieve good contexts for in-context learning. Specifically, the method&lt;/code&gt;&lt;br&gt;&lt;code&gt;(1) uses an unsupervised retriever (BM25/SBERT) to obtain a set of context candidates,&lt;/code&gt;&lt;br&gt;&lt;code&gt;(2) passes the candidates to a scoring model (GPT-Neo/GPT-J/GPT-3/Codex) and select the top/bottom k as positive/negative examples,&lt;/code&gt;&lt;br&gt;&lt;code&gt;(3) uses the examples to train a dense retriever (BERT-based).&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.622&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.622&#34;&gt;&lt;strong&gt;Active Example Selection for In-Context Learning&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/ChicagoHAI/active-example-selection&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code-skyblue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GPT--2-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GPT--3-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;br&gt; by &lt;em&gt;Yiming Zhang, Shi Feng and Chenhao Tan&lt;/em&gt; &lt;br&gt;&lt;code&gt;(1) This paper revisits the effect of example selection (re-ordering &amp;amp; calibration) for ICL, observing that a large variance across set of demonstration examples still exists.&lt;/code&gt;&lt;br&gt;&lt;code&gt;(2) This paper applies reinforcement learning (Q-Learning) to optimize example selection by formulating this task as sequential decision-making problem, which is appropriate for example selection from unlabeled datasets. &lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2206.08082&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2206.08082&#34;&gt;&lt;strong&gt;Self-Generated In-Context Learning: Leveraging Auto-regressive Language Models as a Demonstration Generator&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Hyuhng Joon Kim, Hyunsoo Cho, Junyeob Kim, Taeuk Kim, Kang Min Yoo and Sang-goo Lee&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1007/978-3-031-19759-8_15&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ISoLA-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1007/978-3-031-19759-8_15&#34;&gt;&lt;strong&gt;Measuring Convergence Inertia: Online Learning in Self-adaptive Systems with Context Shifts&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Elvin Alberts and Ilias Gerostathopoulos&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=RdJVFCHjUMI&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ICLR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://openreview.net/forum?id=RdJVFCHjUMI&#34;&gt;&lt;strong&gt;An Explanation of In-context Learning as Implicit Bayesian Inference&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Sang Michael Xie, Aditi Raghunathan, Percy Liang and Tengyu Ma&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.759&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.759&#34;&gt;&lt;strong&gt;Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi and Luke Zettlemoyer&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.08686&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.08686&#34;&gt;&lt;strong&gt;The Impact of Symbolic Representations on In-context Learning for Few-shot Reasoning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Hanlin Zhang, Yi-Fan Zhang, Li Erran Li and Eric P. Xing&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2022.deelio-1.10&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NAACL-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2022.deelio-1.10&#34;&gt;&lt;strong&gt;What Makes Good In-Context Examples for GPT-3?&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/jiachangliu/KATEGPT3&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code-skyblue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/1907.11692&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/RoBERTa-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://jmlr.org/papers/v21/20-074.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/T5-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/D19-1410/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/SBERT-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GPT--3-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;br&gt; by &lt;em&gt;Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin and Weizhu Chen&lt;/em&gt; &lt;br&gt;&lt;code&gt;(1) 探索了在in-context learning中什么样的demonstration example可以对GPT-3的效果取得帮助；&lt;/code&gt;&lt;br&gt;&lt;code&gt;(2) 利用roberta对样本进行编码，并计算demonstration与test example的向量距离（欧氏距离），最终发现与test example越相近的demonstration越能取得较好的效果。&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2022.findings-emnlp.329&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP_Findings-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2022.findings-emnlp.329&#34;&gt;&lt;strong&gt;Thinking about GPT-3 In-Context Learning for Biomedical IE? Think Again&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Bernal Jimenez Gutierrez, Nikolas McNeal, Clayton Washington, You Chen, Lang Li, Huan Sun and Yu Su&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.10559&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.10559&#34;&gt;&lt;strong&gt;Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui and Furu Wei&lt;/em&gt; &lt;br&gt;&lt;code&gt;(1) 与The Dual Form of Neural Networks Revisited结合一起看，可以进一步理解in-context learning，通过与NN线性层对偶形式的类比，可以将ICL流程描述为：1. 基于Transformer的预训练语言模型作为元优化器；2. 通过正向计算，根据示范例子产生元梯度；3. 通过关注，将元梯度应用于原始语言模型，建立一个ICL模型；&lt;/code&gt;&lt;br&gt;&lt;code&gt;(2)与Fine-tune类似，ICL也是在zero-shot learning参数的基础上，提供了一个更新量。&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://proceedings.mlr.press/v162/irie22a.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ICML-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://proceedings.mlr.press/v162/irie22a.html&#34;&gt;&lt;strong&gt;The Dual Form of Neural Networks Revisited: Connecting Test Time Predictions to Training Patterns via Spotlights of Attention&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Kazuki Irie, R&#39;obert Csord&#39;as and J&#34;urgen Schmidhuber&lt;/em&gt; &lt;br&gt;&lt;code&gt;(1) 很有意思的一篇，回顾神经网络（NN）线性层Y=WX（省略偏置b）的原始形式与对偶形式，两种形式完全等价；&lt;/code&gt;&lt;br&gt;&lt;code&gt;(2) 从对偶形式中可以发现，通过反向传播训练的NN线性层的输出主要是该层在训练期间的训练误差信号et的线性组合，其中权重是通过比较测试查询x和每个训练输入计算出来的；进一步可以得出，如果测试时输入的x和训练时的输入是正交的，那么梯度下降所得到的参数更新对于该样本x完全没有影响。&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.10375&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.10375&#34;&gt;&lt;strong&gt;Self-adaptive In-context Learning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye and Lingpeng Kong&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.10378&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.10378&#34;&gt;&lt;strong&gt;Careful Data Curation Stabilizes In-context Learning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Ting-Yun Chang and Robin Jia&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.09095&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.09095&#34;&gt;&lt;strong&gt;Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Hritik Bansal, Karthik Gopalakrishnan, Saket Dingliwal, Sravan Bodapati, Katrin Kirchhoff and Dan Roth&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Instruction Tuning&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.00093&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.00093&#34;&gt;&lt;strong&gt;Large Language Models Can Be Easily Distracted by Irrelevant Context&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H. Chi, Nathanael Sch&#34;arli and Denny Zhou&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.07459&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.07459&#34;&gt;&lt;strong&gt;The Capacity for Moral Self-Correction in Large Language Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas I. Liao, Kamile Lukosiute, Anna Chen, Anna Goldie, Azalia Mirhoseini et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=gEZrGCozdqR&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ICLR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://openreview.net/forum?id=gEZrGCozdqR&#34;&gt;&lt;strong&gt;Finetuned Language Models are Zero-Shot Learners&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2201.08239&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2201.08239&#34;&gt;&lt;strong&gt;LaMDA: Language Models for Dialog Applications&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.11416&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.11416&#34;&gt;&lt;strong&gt;Scaling Instruction-Finetuned Language Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.340&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.340&#34;&gt;&lt;strong&gt;Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.10560&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.10560&#34;&gt;&lt;strong&gt;Self-Instruct: Aligning Language Model with Self Generated Instructions&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi and Hannaneh Hajishirzi&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2203.09161&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2203.09161&#34;&gt;&lt;strong&gt;How Many Data Samples is an Additional Instruction Worth?&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Ravsehaj Singh Puri, Swaroop Mishra, Mihir Parmar and Chitta Baral&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;RLHF&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.07459&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.07459&#34;&gt;&lt;strong&gt;The Capacity for Moral Self-Correction in Large Language Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas I. Liao, Kamile Lukosiute, Anna Chen, Anna Goldie, Azalia Mirhoseini et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.12192&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.12192&#34;&gt;&lt;strong&gt;Aligning Text-to-Image Models using Human Feedback&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2204.05862&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2204.05862&#34;&gt;&lt;strong&gt;Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.01241&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.01241&#34;&gt;&lt;strong&gt;Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kiant&#39;e Brantley, Jack Hessel, Rafet Sifa, Christian Bauckhage, Hannaneh Hajishirzi and Yejin Choi&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2203.11147&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2203.11147&#34;&gt;&lt;strong&gt;Teaching language models to support answers with verified quotes&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, H. Francis Song, Martin Chadwick, Mia Glaese, Susannah Young et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2209.14375&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2209.14375&#34;&gt;&lt;strong&gt;Improving alignment of dialogue agents via targeted human judgements&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Amelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.10760&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2210.10760&#34;&gt;&lt;strong&gt;Scaling Laws for Reward Model Overoptimization&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Gao, Leo, Schulman, John and Hilton, Jacob&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2209.07858&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2209.07858&#34;&gt;&lt;strong&gt;Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2208.02294&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2208.02294&#34;&gt;&lt;strong&gt;Dynamic Planning in Open-Ended Dialogue using Reinforcement Learning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Deborah Cohen, Moonkyung Ryu, Yinlam Chow, Orgad Keller, Ido Greenberg, Avinatan Hassidim, Michael Fink, Yossi Matias et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2203.02155&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2203.02155&#34;&gt;&lt;strong&gt;Training language models to follow instructions with human feedback&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1177/02783649211041652&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/IJRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1177/02783649211041652&#34;&gt;&lt;strong&gt;Learning reward functions from diverse sources of human feedback: Optimally integrating demonstrations and preferences&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Erdem Biyik, Dylan P. Losey, Malayandi Palan, Nicholas C. Landolfi, Gleb Shevchuk and Dorsa Sadigh&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.09332&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2112.09332&#34;&gt;&lt;strong&gt;WebGPT: Browser-assisted question-answering with human feedback&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2109.10862&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2109.10862&#34;&gt;&lt;strong&gt;Recursively Summarizing Books with Human Feedback&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Jeff Wu, Long Ouyang, Daniel M. Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike and Paul F. Christiano&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NeurIPS-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html&#34;&gt;&lt;strong&gt;Learning to summarize with human feedback&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2020.emnlp-main.28&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2020.emnlp-main.28&#34;&gt;&lt;strong&gt;Dialogue Response Ranking Training with Large-Scale Human Feedback Data&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Xiang Gao, Yizhe Zhang, Michel Galley, Chris Brockett and Bill Dolan&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://arxiv.org/abs/1909.08593&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2019-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://arxiv.org/abs/1909.08593&#34;&gt;&lt;strong&gt;Fine-Tuning Language Models from Human Preferences&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul F. Christiano and Geoffrey Irving&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NeurIPS-2017-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html&#34;&gt;&lt;strong&gt;Deep Reinforcement Learning from Human Preferences&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg and Dario Amodei&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Pre-Training Techniques&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cdn.openai.com/papers/gpt-4.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/OpenAI-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://cdn.openai.com/papers/gpt-4.pdf&#34;&gt;&lt;strong&gt;GPT-4 Technical Report&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://cdn.openai.com/papers/gpt-4.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GPT--4-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;br&gt; by &lt;em&gt;OpenAI&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cdn.openai.com/papers/gpt-4-system-card.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/OpenAI-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://cdn.openai.com/papers/gpt-4-system-card.pdf&#34;&gt;&lt;strong&gt;GPT-4 System Card&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://cdn.openai.com/papers/gpt-4.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GPT--4-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;br&gt; by &lt;em&gt;OpenAI&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2205.01068&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2205.01068&#34;&gt;&lt;strong&gt;OPT: Open Pre-trained Transformer Language Models&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2205.01068&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/OPT-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;br&gt; by &lt;em&gt;Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T. Diab et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2209.10372&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2209.10372&#34;&gt;&lt;strong&gt;WeLM: A Well-Read Pre-trained Language Model for Chinese&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://welm.weixin.qq.com/docs/api/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code-skyblue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;br&gt; by &lt;em&gt;Hui Su, Xiao Zhou, Houjin Yu, Yuwen Chen, Zilin Zhu, Yang Yu and Jie Zhou&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NeurIPS-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html&#34;&gt;&lt;strong&gt;Language Models are Few-Shot Learners&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GPT--3-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;br&gt; by &lt;em&gt;Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=r1xMH1BtvB&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ICLR-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://openreview.net/forum?id=r1xMH1BtvB&#34;&gt;&lt;strong&gt;ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://openreview.net/forum?id=r1xMH1BtvB&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ELECTRA-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;br&gt; by &lt;em&gt;Kevin Clark, Minh-Thang Luong, Quoc V. Le and Christopher D. Manning&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2020.findings-emnlp.58&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP_Findings-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2020.findings-emnlp.58&#34;&gt;&lt;strong&gt;Revisiting Pre-Trained Models for Chinese Natural Language Processing&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang and Guoping Hu&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2006.03654&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2006.03654&#34;&gt;&lt;strong&gt;DeBERTa: Decoding-enhanced BERT with Disentangled Attention&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2006.03654&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/DeBERTa-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;br&gt; by &lt;em&gt;Pengcheng He, Xiaodong Liu, Jianfeng Gao and Weizhu Chen&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://jmlr.org/papers/v21/20-074.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/JMLR-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://jmlr.org/papers/v21/20-074.html&#34;&gt;&lt;strong&gt;Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;http://jmlr.org/papers/v21/20-074.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/T5-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;br&gt; by &lt;em&gt;Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1162/tacl_a_00349&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/TACL-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1162/tacl_a_00349&#34;&gt;&lt;strong&gt;A Primer in BERTology: What We Know About How BERT Works&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Anna Rogers, Olga Kovaleva and Anna Rumshisky&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/OpenAI-2019-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&#34;&gt;&lt;strong&gt;Language Models are Unsupervised Multitask Learners&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GPT--2-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;br&gt; by &lt;em&gt;Radford, Alec, Wu, Jeffrey, Child, Rewon, Luan, David, Amodei, Dario and Sutskever, Ilya&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/n19-1423&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NAACL-2019-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/n19-1423&#34;&gt;&lt;strong&gt;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://aclanthology.org/N19-1423/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/BERT-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;br&gt; by &lt;em&gt;Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://arxiv.org/abs/1907.11692&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2019-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://arxiv.org/abs/1907.11692&#34;&gt;&lt;strong&gt;RoBERTa: A Robustly Optimized BERT Pretraining Approach&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/1907.11692&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/RoBERTa-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;br&gt; by &lt;em&gt;Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/D19-1410&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2019-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/D19-1410&#34;&gt;&lt;strong&gt;Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://aclanthology.org/D19-1410/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/SBERT-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;br&gt; by &lt;em&gt;Nils Reimers and Iryna Gurevych&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/OpenAI-2018-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf&#34;&gt;&lt;strong&gt;Improving language understanding by generative pre-training&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GPT--1-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;br&gt; by &lt;em&gt;Radford, Alec, Narasimhan, Karthik, Salimans, Tim, Sutskever, Ilya and others&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Mixtures of Experts&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.804&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.804&#34;&gt;&lt;strong&gt;Efficient Large Scale Language Modeling with Mixtures of Experts&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/facebookresearch/fairseq/tree/main/examples/moe_lm&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code-skyblue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.804&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/MoE-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;br&gt; by &lt;em&gt;Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin, Jingfei Du et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2022.naacl-main.116&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NAACL-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2022.naacl-main.116&#34;&gt;&lt;strong&gt;MoEBERT: from BERT to Mixture-of-Experts via Importance-Guided Adaptation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Simiao Zuo, Qingru Zhang, Chen Liang, Pengcheng He, Tuo Zhao and Weizhu Chen&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.05055&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.05055&#34;&gt;&lt;strong&gt;Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Aran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Carlos Riquelme Ruiz, Basil Mustafa, Joshua Ainslie, Yi Tay, Mostafa Dehghani et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.03885&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.03885&#34;&gt;&lt;strong&gt;Meta-DMoE: Adapting to Domain Shift by Meta-Distillation from Mixture-of-Experts&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Tao Zhong, Zhixiang Chi, Li Gu, Yang Wang, Yuanhao Yu and Jin Tang&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Knowledge Enhanced&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.02093&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.02093&#34;&gt;&lt;strong&gt;Knowledge-enhanced Neural Machine Reasoning: A Review&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Tanmoy Chowdhury, Chen Ling, Xuchao Zhang, Xujiang Zhao, Guangji Bai, Jian Pei, Haifeng Chen and Liang Zhao&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.09338&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NeurIPS-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.09338&#34;&gt;&lt;strong&gt;Deep Bidirectional Language-Knowledge Graph Pretraining&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Michihiro Yasunaga, Antoine Bosselut, Hongyu Ren, Xikun Zhang, Christopher D. Manning, Percy Liang and Jure Leskovec&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.13428&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/TKDE-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.13428&#34;&gt;&lt;strong&gt;A Survey on Knowledge-Enhanced Pre-trained Language Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Chaoqi Zhen, Yanlei Shang, Xiangyu Liu, Yifei Li, Yong Chen and Dell Zhang&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.3778/j.issn.1673-9418.2108105&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/FCST-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.3778/j.issn.1673-9418.2108105&#34;&gt;&lt;strong&gt;Review of Knowledge-Enhanced Pre-trained Language Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Yi, HAN, Linbo, QIAO, Dongsheng, LI and Xiangke, LIAO&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.09252&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.09252&#34;&gt;&lt;strong&gt;Mind the Knowledge Gap: A Survey of Knowledge-enhanced Dialogue Systems&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Sagi Shaier, Lawrence Hunter and Katharina Kann&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2022.coling-1.85&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/COLING-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2022.coling-1.85&#34;&gt;&lt;strong&gt;A Domain Knowledge Enhanced Pre-Trained Language Model for Vertical Search: Case Study on Medicinal Products&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Kesong Liu, Jianhui Jiang and Feifei Lyu&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.207&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.207&#34;&gt;&lt;strong&gt;Knowledge Prompting in Pre-trained Language Model for Natural Language Understanding&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Jianing Wang, Wenkang Huang, Minghui Qiu, Qiuhui Shi, Hongbin Wang, Xiang Li and Ming Gao&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2022.findings-acl.150&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL_Findings-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2022.findings-acl.150&#34;&gt;&lt;strong&gt;Dict-BERT: Enhancing Language Model Pre-training with Dictionary&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Wenhao Yu, Chenguang Zhu, Yuwei Fang, Donghan Yu, Shuohang Wang, Yichong Xu, Michael Zeng and Meng Jiang&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=41e9o6cQPj&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ICLR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://openreview.net/forum?id=41e9o6cQPj&#34;&gt;&lt;strong&gt;GreaseLM: Graph REASoning Enhanced Language Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Xikun Zhang, Antoine Bosselut, Michihiro Yasunaga, Hongyu Ren, Percy Liang, Christopher D. Manning and Jure Leskovec&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1145/3404835.3462865&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/SIGIR-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1145/3404835.3462865&#34;&gt;&lt;strong&gt;Knowledge-based Review Generation by Coherence Enhanced Text Planning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Junyi Li, Wayne Xin Zhao, Zhicheng Wei, Nicholas Jing Yuan and Ji-Rong Wen&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.emnlp-main.173&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.emnlp-main.173&#34;&gt;&lt;strong&gt;A Three-Stage Learning Framework for Low-Resource Knowledge-Grounded Dialogue Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Shilei Liu, Xiaofeng Zhao, Bochao Li, Feiliang Ren, Longhui Zhang and Shujuan Yin&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.naacl-main.340&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NAACL-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.naacl-main.340&#34;&gt;&lt;strong&gt;Ask what&#39;s missing and what&#39;s useful: Improving Clarification Question Generation using Global Knowledge&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Bodhisattwa Prasad Majumder, Sudha Rao, Michel Galley and Julian J. McAuley&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2107.02137&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2107.02137&#34;&gt;&lt;strong&gt;ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Yu Sun, Shuohuan Wang, Shikun Feng, Siyu Ding, Chao Pang, Junyuan Shang, Jiaxiang Liu, Xuyi Chen et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.bionlp-1.20&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NAACL-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.bionlp-1.20&#34;&gt;&lt;strong&gt;Improving Biomedical Pretrained Language Models with Knowledge&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Zheng Yuan, Yijia Liu, Chuanqi Tan, Songfang Huang and Fei Huang&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2020.emnlp-main.697&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2020.emnlp-main.697&#34;&gt;&lt;strong&gt;KGPT: Knowledge-Grounded Pre-Training for Data-to-Text Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Wenhu Chen, Yu Su, Xifeng Yan and William Yang Wang&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1162/tacl_a_00302&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/TACL-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1162/tacl_a_00302&#34;&gt;&lt;strong&gt;A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Jian Guan, Fei Huang, Minlie Huang, Zhihao Zhao and Xiaoyan Zhu&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1145/3340531.3411893&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CIKM-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1145/3340531.3411893&#34;&gt;&lt;strong&gt;Knowledge-Enhanced Personalized Review Generation with Capsule Graph Neural Network&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Junyi Li, Siqing Li, Wayne Xin Zhao, Gaole He, Zhicheng Wei, Nicholas Jing Yuan and Ji-Rong Wen&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2020.emnlp-main.226&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2020.emnlp-main.226&#34;&gt;&lt;strong&gt;MEGATRON-CNTRL: Controllable Story Generation with External Knowledge Using Large-Scale Language Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Peng Xu, Mostofa Patwary, Mohammad Shoeybi, Raul Puri, Pascale Fung, Anima Anandkumar and Bryan Catanzaro&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper/2009/hash/1543843a4723ed2ab08e18053ae6dc5b-Abstract.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NeurIPS-2009-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://proceedings.neurips.cc/paper/2009/hash/1543843a4723ed2ab08e18053ae6dc5b-Abstract.html&#34;&gt;&lt;strong&gt;Zero-shot Learning with Semantic Output Codes&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Mark Palatucci, Dean Pomerleau, Geoffrey E. Hinton and Tom M. Mitchell&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Knowledge Distillation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1145/3551349.3556964&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ASE-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1145/3551349.3556964&#34;&gt;&lt;strong&gt;Compressing Pre-trained Models of Code into 3 MB&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Jieke Shi, Zhou Yang, Bowen Xu, Hong Jin Kang and David Lo&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.01351&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.01351&#34;&gt;&lt;strong&gt;Less is More: Task-aware Layer-wise Distillation for Language Model Compression&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Chen Liang, Simiao Zuo, Qingru Zhang, Pengcheng He, Weizhu Chen and Tuo Zhao&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.03885&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.03885&#34;&gt;&lt;strong&gt;Meta-DMoE: Adapting to Domain Shift by Meta-Distillation from Mixture-of-Experts&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Tao Zhong, Zhixiang Chi, Li Gu, Yang Wang, Yuanhao Yu and Jin Tang&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.628&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.628&#34;&gt;&lt;strong&gt;CN-AutoMIC: Distilling Chinese Commonsense Knowledge from Pretrained Language Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Chenhao Wang, Jiachun Li, Yubo Chen, Kang Liu and Jun Zhao&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2022.acl-long.116&#34;&gt;&amp;lt;img src=https://img.shields.io/badge/the_60th_Annual_Meeting_of_the_Association_for_Computational Linguistics_(Volume_1:&lt;em&gt;Long_Papers),&lt;/em&gt;{ACL}_2022,_Dublin,_Ireland, May_22--27,_2022-2022-blue alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34; /&amp;gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2022.acl-long.116&#34;&gt;&lt;strong&gt;Domain Knowledge Transferring for Pre-trained Language Model via Calibrated Activation Boundary Distillation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Dongha Choi, Hongseok Choi and Hyunju Lee&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1016/j.neucom.2021.04.102&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Neurocomputing-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1016/j.neucom.2021.04.102&#34;&gt;&lt;strong&gt;Preparing lessons: Improve knowledge distillation with better supervision&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/SforAiDl/KD_Lib&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code-skyblue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;br&gt; by &lt;em&gt;Tiancheng Wen, Shenqi Lai and Xueming Qian&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.findings-acl.40&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL_Findings-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.findings-acl.40&#34;&gt;&lt;strong&gt;Adapt-and-Distill: Developing Small, Fast and Effective Pretrained Language Models for Domains&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Yunzhi Yao, Shaohan Huang, Wenhui Wang, Li Dong and Furu Wei&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.acl-long.259&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.acl-long.259&#34;&gt;&lt;strong&gt;Taming Pre-trained Language Models with N-gram Representations for Low-Resource Domain Adaptation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Shizhe Diao, Ruijia Xu, Hongjin Su, Yilei Jiang, Yan Song and Tong Zhang&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2020.acl-main.705&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2020.acl-main.705&#34;&gt;&lt;strong&gt;Distilling Knowledge Learned in BERT for Text Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Yen-Chun Chen, Zhe Gan, Yu Cheng, Jingzhou Liu and Jingjing Liu&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/view/5963&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/AAAI-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/view/5963&#34;&gt;&lt;strong&gt;Improved Knowledge Distillation via Teacher Assistant&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/SforAiDl/KD_Lib&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code-skyblue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;br&gt; by &lt;em&gt;Seyed-Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine, Akihiro Matsukawa and Hassan Ghasemzadeh&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2020/papers/Yun_Regularizing_Class-Wise_Predictions_via_Self-Knowledge_Distillation_CVPR_2020_paper.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CVPR-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2020/papers/Yun_Regularizing_Class-Wise_Predictions_via_Self-Knowledge_Distillation_CVPR_2020_paper.pdf&#34;&gt;&lt;strong&gt;Regularizing Class-Wise Predictions via Self-Knowledge Distillation&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/SforAiDl/KD_Lib&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code-skyblue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;br&gt; by &lt;em&gt;Sukmin Yun, Jongjin Park, Kimin Lee and Jinwoo Shin&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://openaccess.thecvf.com/content_CVPR_2019/html/Park_Relational_Knowledge_Distillation_CVPR_2019_paper.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CVPR-2019-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://openaccess.thecvf.com/content_CVPR_2019/html/Park_Relational_Knowledge_Distillation_CVPR_2019_paper.html&#34;&gt;&lt;strong&gt;Relational Knowledge Distillation&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/SforAiDl/KD_Lib&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code-skyblue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;br&gt; by &lt;em&gt;Wonpyo Park, Dongju Kim, Yan Lu and Minsu Cho&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://arxiv.org/abs/1909.11723&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2019-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://arxiv.org/abs/1909.11723&#34;&gt;&lt;strong&gt;Revisit Knowledge Distillation: a Teacher-free Framework&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/SforAiDl/KD_Lib&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code-skyblue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;br&gt; by &lt;em&gt;Li Yuan, Francis E. H. Tay, Guilin Li, Tao Wang and Jiashi Feng&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1109/ICCV.2019.00143&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ICCV-2019-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1109/ICCV.2019.00143&#34;&gt;&lt;strong&gt;Knowledge Distillation via Route Constrained Optimization&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/SforAiDl/KD_Lib&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code-skyblue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;br&gt; by &lt;em&gt;Xiao Jin, Baoyun Peng, Yichao Wu, Yu Liu, Jiaheng Liu, Ding Liang, Junjie Yan and Xiaolin Hu&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://arxiv.org/abs/1910.05057&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2019-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://arxiv.org/abs/1910.05057&#34;&gt;&lt;strong&gt;Improving Generalization and Robustness with Noisy Collaboration in Knowledge Distillation&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/SforAiDl/KD_Lib&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code-skyblue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;br&gt; by &lt;em&gt;Elahe Arani, Fahad Sarfraz and Bahram Zonooz&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://arxiv.org/abs/1903.12136&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2019-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://arxiv.org/abs/1903.12136&#34;&gt;&lt;strong&gt;Distilling Task-Specific Knowledge from BERT into Simple Neural Networks&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/SforAiDl/KD_Lib&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code-skyblue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;br&gt; by &lt;em&gt;Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova and Jimmy Lin&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=rJl-b3RcF7&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ICLR-2019-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://openreview.net/forum?id=rJl-b3RcF7&#34;&gt;&lt;strong&gt;The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/SforAiDl/KD_Lib&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code-skyblue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;br&gt; by &lt;em&gt;Jonathan Frankle and Michael Carbin&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://proceedings.mlr.press/v80/furlanello18a.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ICML-2018-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://proceedings.mlr.press/v80/furlanello18a.html&#34;&gt;&lt;strong&gt;Born-Again Neural Networks&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/SforAiDl/KD_Lib&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code-skyblue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;br&gt; by &lt;em&gt;Tommaso Furlanello, Zachary Chase Lipton, Michael Tschannen, Laurent Itti and Anima Anandkumar&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=Sks9_ajex&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ICLR-2017-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://openreview.net/forum?id=Sks9_ajex&#34;&gt;&lt;strong&gt;Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/SforAiDl/KD_Lib&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code-skyblue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;br&gt; by &lt;em&gt;Sergey Zagoruyko and Nikos Komodakis&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=ry8u21rtl&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ICLR-2017-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://openreview.net/forum?id=ry8u21rtl&#34;&gt;&lt;strong&gt;Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/SforAiDl/KD_Lib&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code-skyblue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;br&gt; by &lt;em&gt;Antti Tarvainen and Harri Valpola&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://arxiv.org/abs/1706.00384&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2017-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://arxiv.org/abs/1706.00384&#34;&gt;&lt;strong&gt;Deep Mutual Learning&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/SforAiDl/KD_Lib&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code-skyblue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;br&gt; by &lt;em&gt;Ying Zhang, Tao Xiang, Timothy M. Hospedales and Huchuan Lu&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://arxiv.org/abs/1610.09650&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2016-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://arxiv.org/abs/1610.09650&#34;&gt;&lt;strong&gt;Deep Model Compression: Distilling Knowledge from Noisy Teachers&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/SforAiDl/KD_Lib&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code-skyblue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;br&gt; by &lt;em&gt;Bharat Bhusan Sau and Vineeth N. Balasubramanian&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://arxiv.org/abs/1503.02531&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2015-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://arxiv.org/abs/1503.02531&#34;&gt;&lt;strong&gt;Distilling the Knowledge in a Neural Network&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/SforAiDl/KD_Lib&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code-skyblue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;br&gt; by &lt;em&gt;Geoffrey E. Hinton, Oriol Vinyals and Jeffrey Dean&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Knowledge Generation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.12810&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EACL-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.12810&#34;&gt;&lt;strong&gt;Crawling the Internal Knowledge-Base of Language Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Roi Cohen, Mor Geva, Jonathan Berant and Amir Globerson&lt;/em&gt; &lt;br&gt;&lt;code&gt;本文提出一种从语言模型中提取结构化知识图谱的方法；使用专门设计的提示来控制提取过程中的精度和召回率；在GPT-3上进行了评估，显示了高精确度的结果。&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.11293&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.11293&#34;&gt;&lt;strong&gt;Understanding Finetuning for Factual Knowledge Extraction from Language Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Mehran Kazemi, Sid Mittal and Deepak Ramachandran&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.1&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.1&#34;&gt;&lt;strong&gt;Generative Knowledge Graph Construction: A Review&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Hongbin Ye, Ningyu Zhang, Hui Chen and Huajun Chen&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2022.findings-emnlp.438&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP_Findings-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2022.findings-emnlp.438&#34;&gt;&lt;strong&gt;Calibrating Factual Knowledge in Pretrained Language Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Qingxiu Dong, Damai Dai, Yifan Song, Jingjing Xu, Zhifang Sui and Lei Li&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=DhzIU48OcZh&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ICLR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://openreview.net/forum?id=DhzIU48OcZh&#34;&gt;&lt;strong&gt;P-Adapters: Robustly Extracting Factual Information from Language Models with Diverse Prompts&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Benjamin Newman, Prafulla Kumar Choubey and Nazneen Rajani&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2022.acl-long.225&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2022.acl-long.225&#34;&gt;&lt;strong&gt;Generated Knowledge Prompting for Commonsense Reasoning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Jiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Peter West, Ronan Le Bras, Yejin Choi and Hannaneh Hajishirzi&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.611&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.611&#34;&gt;&lt;strong&gt;Rainier: Reinforced Knowledge Introspector for Commonsense Question Answering&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Jiacheng Liu, Skyler Hallinan, Ximing Lu, Pengfei He, Sean Welleck, Hannaneh Hajishirzi and Yejin Choi&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2022.naacl-main.341&#34;&gt;&amp;lt;img src=https://img.shields.io/badge/the_2022_Conference_of_the_North_American_Chapter_of the_Association_for_Computational_Linguistics:_Human_Language_Technologies, {NAACL}_2022,_Seattle,_WA,_United_States,_July_10--15,_2022-2022-blue alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34; /&amp;gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2022.naacl-main.341&#34;&gt;&lt;strong&gt;Symbolic Knowledge Distillation: from General Language Models to Commonsense Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Peter West, Chandra Bhagavatula, Jack Hessel, Jena D. Hwang, Liwei Jiang, Ronan Le Bras, Ximing Lu, Sean Welleck et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.09246&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.09246&#34;&gt;&lt;strong&gt;I2D2: Inductive Knowledge Distillation with NeuroLogic and Self-Imitation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Chandra Bhagavatula, Jena D. Hwang, Doug Downey, Ronan Le Bras, Ximing Lu, Keisuke Sakaguchi, Swabha Swayamdipta, Peter West et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Knowledge Editing&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2303.00046&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2303.00046&#34;&gt;&lt;strong&gt;Robustness of edited neural networks&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Davis Brown, Charles Godfrey, Cody Nizinski, Jonathan Tu and Henry Kvinge&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.09785&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.09785&#34;&gt;&lt;strong&gt;Transformer-Patcher: One Mistake worth One Neuron&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Zeyu Huang, Yikang Shen, Xiaofeng Zhang, Jie Zhou, Wenge Rong and Zhang Xiong&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=0DcZxeWfOPt&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ICLR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://openreview.net/forum?id=0DcZxeWfOPt&#34;&gt;&lt;strong&gt;Fast Model Editing at Scale&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn and Christopher D. Manning&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://proceedings.mlr.press/v162/mitchell22a.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ICML-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://proceedings.mlr.press/v162/mitchell22a.html&#34;&gt;&lt;strong&gt;Memory-Based Model Editing at Scale&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher D. Manning and Chelsea Finn&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=-h6WAS6eE4&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NeurIPS-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://openreview.net/forum?id=-h6WAS6eE4&#34;&gt;&lt;strong&gt;Locating and editing factual associations in gpt&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Meng, Kevin, Bau, David, Andonian, Alex J and Belinkov, Yonatan&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2211.11031&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2211.11031&#34;&gt;&lt;strong&gt;Aging with GRACE: Lifelong Model Editing with Discrete Key-Value Adaptors&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Thomas Hartvigsen, Swami Sankaranarayanan, Hamid Palangi, Yoon Kim and Marzyeh Ghassemi&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.emnlp-main.522&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.emnlp-main.522&#34;&gt;&lt;strong&gt;Editing Factual Knowledge in Language Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Nicola De Cao, Wilker Aziz and Ivan Titov&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Reasoning&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.11596&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.11596&#34;&gt;&lt;strong&gt;ThoughtSource: A central hub for large language model reasoning data&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Simon Ott, Konstantin Hebenstreit, Valentin Li&#39;evin, Christoffer Egeberg Hother, Milad Moradi, Maximilian Mayrhauser, Robert Praas, Ole Winther et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.02093&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.02093&#34;&gt;&lt;strong&gt;Knowledge-enhanced Neural Machine Reasoning: A Review&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Tanmoy Chowdhury, Chen Ling, Xuchao Zhang, Xujiang Zhao, Guangji Bai, Jian Pei, Haifeng Chen and Liang Zhao&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.13808&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.13808&#34;&gt;&lt;strong&gt;Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Yunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei Huang and Yongbin Li&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.12726&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.12726&#34;&gt;&lt;strong&gt;Specializing Smaller Language Models towards Multi-Step Reasoning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal and Tushar Khot&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.05398&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2303.05398&#34;&gt;&lt;strong&gt;MathPrompter: Mathematical Reasoning using Large Language Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Imani, Shima, Du, Liang and Shrivastava, Harsh&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=8lNy3QCaxHX&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ICML-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://openreview.net/forum?id=8lNy3QCaxHX&#34;&gt;&lt;strong&gt;Improved logical reasoning of language models via differentiable symbolic programming&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Zhang, Hanlin, Li, Ziyang, Huang, Jiani, Naik, Mayur and Xing, Eric&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.392&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.392&#34;&gt;&lt;strong&gt;LILA: A Unified Benchmark for Mathematical Reasoning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.82&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.82&#34;&gt;&lt;strong&gt;Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras and Yejin Choi&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.08607&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.08607&#34;&gt;&lt;strong&gt;MURMUR: Modular Multi-Step Reasoning for Semi-Structured Data-to-Text Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Swarnadeep Saha, Xinyan Velocity Yu, Mohit Bansal, Ramakanth Pasunuru and Asli Celikyilmaz&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2211.12588&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2211.12588&#34;&gt;&lt;strong&gt;Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Wenhu Chen, Xueguang Ma, Xinyi Wang and William W. Cohen&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.08686&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.08686&#34;&gt;&lt;strong&gt;The Impact of Symbolic Representations on In-context Learning for Few-shot Reasoning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Hanlin Zhang, Yi-Fan Zhang, Li Erran Li and Eric P. Xing&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.10403&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.10403&#34;&gt;&lt;strong&gt;Towards Reasoning in Large Language Models: A Survey&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Jie Huang and Kevin Chen-Chuan Chang&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.218&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.218&#34;&gt;&lt;strong&gt;UniGeo: Unifying Geometry Logical Reasoning via Reformulating Mathematical Expression&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Jiaqi Chen, Tong Li, Jinghui Qin, Pan Lu, Liang Lin, Chongyu Chen and Xiaodan Liang&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2205.10625&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2205.10625&#34;&gt;&lt;strong&gt;Least-to-Most Prompting Enables Complex Reasoning in Large Language Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Denny Zhou, Nathanael Sch&#34;arli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet et al.&lt;/em&gt; &lt;br&gt;&lt;code&gt;(1) 两阶段的prompt，第一阶段问题分解（通过in-context learning实现，context中包含了其他问题的分解示例），对于每个问题，分解出回答该问题需要先回答什么子问题；&lt;/code&gt;&lt;br&gt;&lt;code&gt;(2) 在第二阶段中，从后往前依次解决子问题，同样通过in-context learing得到，每次LLM的回答会参与组成下一个问题的prompt。&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2207.00747&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2207.00747&#34;&gt;&lt;strong&gt;Rationale-Augmented Ensembles in Language Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi and Denny Zhou&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://par.nsf.gov/biblio/10380030&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NeurIPS-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://par.nsf.gov/biblio/10380030&#34;&gt;&lt;strong&gt;The unreliability of explanations in few-shot prompting for textual reasoning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Ye, Xi and Durrett, Greg&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1145/3534678.3539131&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/KDD-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1145/3534678.3539131&#34;&gt;&lt;strong&gt;JiuZhang: A Chinese Pre-trained Language Model for Mathematical Problem Understanding&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Wayne Xin Zhao, Kun Zhou, Zheng Gong, Beichen Zhang, Yuanhang Zhou, Jing Sha, Zhigang Chen, Shijin Wang et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.01293&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.01293&#34;&gt;&lt;strong&gt;ThinkSum: Probabilistic reasoning over sets using large language models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Batu Ozturkler, Nikolay Malkin, Zhen Wang and Nebojsa Jojic&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.653&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.653&#34;&gt;&lt;strong&gt;RobustLR: A Diagnostic Benchmark for Evaluating Logical Robustness of Deductive Reasoners&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Soumya Sanyal, Zeyi Liao and Xiang Ren&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2206.14858&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2206.14858&#34;&gt;&lt;strong&gt;Solving Quantitative Reasoning Problems with Language Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay V. Ramasesh, Ambrose Slone, Cem Anil et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2020.findings-emnlp.418&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP_Findings-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2020.findings-emnlp.418&#34;&gt;&lt;strong&gt;Thinking Like a Skeptic: Defeasible Inference in Natural Language&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Rachel Rudinger, Vered Shwartz, Jena D. Hwang, Chandra Bhagavatula, Maxwell Forbes, Ronan Le Bras, Noah A. Smith and Yejin Choi&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Chain of Thought&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.00923&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.00923&#34;&gt;&lt;strong&gt;Multimodal Chain-of-Thought Reasoning in Language Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis and Alex Smola&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.00303&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.00303&#34;&gt;&lt;strong&gt;Rethinking with Retrieval: Faithful Large Language Model Inference&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Hangfeng He, Hongming Zhang and Dan Roth&lt;/em&gt; &lt;br&gt;&lt;code&gt;本文通过用GPT-3在三个复杂的推理任务：常识推理，时间推理和表格推理上进行大量实验来评估RR的有效性。结果表明，RR可以产生更忠实的解释，并提高LLM的性能。&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.12246&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.12246&#34;&gt;&lt;strong&gt;Active Prompting with Chain-of-Thought for Large Language Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Shizhe Diao, Pengcheng Wang, Yong Lin and Tong Zhang&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.12822&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.12822&#34;&gt;&lt;strong&gt;Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Kashun Shum, Shizhe Diao and Tong Zhang&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2205.10782&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2205.10782&#34;&gt;&lt;strong&gt;Instruction Induction: From Few Examples to Natural Language Task Descriptions&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Or Honovich, Uri Shaham, Samuel R. Bowman and Omer Levy&lt;/em&gt; &lt;br&gt;&lt;code&gt;(1) 探索了利用LLM在几个样本的情况下归纳出任务指令的能力；&lt;/code&gt;&lt;br&gt;&lt;code&gt;(2) 测量两个指标：1. 模型归纳指令与人类归纳的指令对比，2. 利用模型归纳的指令作为prompt进行预测的执行准确率；&lt;/code&gt;&lt;br&gt;&lt;code&gt;(3) 相比于GPT-3，InstructGPT效果更好，理所当然。&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.174&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.174&#34;&gt;&lt;strong&gt;Iteratively Prompt Pre-trained Language Models for Chain of Thought&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Boshi Wang, Xiang Deng and Huan Sun&lt;/em&gt; &lt;br&gt;&lt;code&gt;(1) 提出了一种迭代式的prompt-tuning方法，他们认为soft prompt应该带有语境，即在自回归解码时不同时刻应该有不同的prompt向量；&lt;/code&gt;&lt;br&gt;&lt;code&gt;(2) 利用BERT为encoder-decoder架构的PLM生成prompt，在每个解码时刻BERT都会根据先前时刻的上下文生成一组新的prompt向量，提供给PLM生成新的上下文，迭代往复。&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.00720&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.00720&#34;&gt;&lt;strong&gt;Complexity-Based Prompting for Multi-Step Reasoning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark and Tushar Khot&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.03350&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.03350&#34;&gt;&lt;strong&gt;Measuring and Narrowing the Compositionality Gap in Language Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith and Mike Lewis&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.03493&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.03493&#34;&gt;&lt;strong&gt;Automatic Chain of Thought Prompting in Large Language Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Zhuosheng Zhang, Aston Zhang, Mu Li and Alex Smola&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2201.11903&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2201.11903&#34;&gt;&lt;strong&gt;Chain of Thought Prompting Elicits Reasoning in Large Language Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc Le and Denny Zhou&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2203.11171&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2203.11171&#34;&gt;&lt;strong&gt;Self-Consistency Improves Chain of Thought Reasoning in Language Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi and Denny Zhou&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2209.07686&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2209.07686&#34;&gt;&lt;strong&gt;Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Aman Madaan and Amir Yazdanbakhsh&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.10001&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.10001&#34;&gt;&lt;strong&gt;Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer and Huan Sun&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2204.02311&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2204.02311&#34;&gt;&lt;strong&gt;PaLM: Scaling Language Modeling with Pathways&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.13894&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.13894&#34;&gt;&lt;strong&gt;LAMBADA: Backward Chaining for Automated Reasoning in Natural Language&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Seyed Mehran Kazemi, Najoung Kim, Deepti Bhatia, Xin Xu and Deepak Ramachandran&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://research.google/pubs/pub51694/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NeurIPS-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://research.google/pubs/pub51694/&#34;&gt;&lt;strong&gt;Star: Self-taught reasoner bootstrapping reasoning with reasoning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Zelikman, Eric, Mu, Jesse, Goodman, Noah D and Wu, Yuhuai Tony&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.01240&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.01240&#34;&gt;&lt;strong&gt;Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Abulhair Saparov and He He&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2205.11916&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2205.11916&#34;&gt;&lt;strong&gt;Large Language Models are Zero-Shot Reasoners&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo and Yusuke Iwasawa&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2205.09712&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2205.09712&#34;&gt;&lt;strong&gt;Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Antonia Creswell, Murray Shanahan and Irina Higgins&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2206.07682&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2206.07682&#34;&gt;&lt;strong&gt;Emergent Abilities of Large Language Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1145/3534678.3539131&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/KDD-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1145/3534678.3539131&#34;&gt;&lt;strong&gt;JiuZhang: A Chinese Pre-trained Language Model for Mathematical Problem Understanding&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Wayne Xin Zhao, Kun Zhou, Zheng Gong, Beichen Zhang, Yuanhang Zhou, Jing Sha, Zhigang Chen, Shijin Wang et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.10071&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.10071&#34;&gt;&lt;strong&gt;Large Language Models Are Reasoning Teachers&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Namgyu Ho, Laura Schmid and Se-Young Yun&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.09561&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.09561&#34;&gt;&lt;strong&gt;Large Language Models are reasoners with Self-Verification&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Yixuan Weng, Minjun Zhu, Shizhu He, Kang Liu and Jun Zhao&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.09597&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.09597&#34;&gt;&lt;strong&gt;Reasoning with Language Model Prompting: A Survey&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2211.10435&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2211.10435&#34;&gt;&lt;strong&gt;PAL: Program-aided Language Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan and Graham Neubig&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.06710&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.06710&#34;&gt;&lt;strong&gt;Large Language Models are few(1)-shot Table Reasoners&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Wenhu Chen&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.11610&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.11610&#34;&gt;&lt;strong&gt;Large Language Models Can Self-Improve&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu and Jiawei Han&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Multi-Step Reasoning&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.12726&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.12726&#34;&gt;&lt;strong&gt;Specializing Smaller Language Models towards Multi-Step Reasoning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal and Tushar Khot&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.08607&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.08607&#34;&gt;&lt;strong&gt;MURMUR: Modular Multi-Step Reasoning for Semi-Structured Data-to-Text Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Swarnadeep Saha, Xinyan Velocity Yu, Mohit Bansal, Ramakanth Pasunuru and Asli Celikyilmaz&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2207.00747&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2207.00747&#34;&gt;&lt;strong&gt;Rationale-Augmented Ensembles in Language Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi and Denny Zhou&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Arithmetic Reasoning&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.392&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.392&#34;&gt;&lt;strong&gt;LILA: A Unified Benchmark for Mathematical Reasoning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2211.12588&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2211.12588&#34;&gt;&lt;strong&gt;Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Wenhu Chen, Xueguang Ma, Xinyi Wang and William W. Cohen&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1145/3534678.3539131&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/KDD-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1145/3534678.3539131&#34;&gt;&lt;strong&gt;JiuZhang: A Chinese Pre-trained Language Model for Mathematical Problem Understanding&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Wayne Xin Zhao, Kun Zhou, Zheng Gong, Beichen Zhang, Yuanhang Zhou, Jing Sha, Zhigang Chen, Shijin Wang et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2206.14858&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2206.14858&#34;&gt;&lt;strong&gt;Solving Quantitative Reasoning Problems with Language Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay V. Ramasesh, Ambrose Slone, Cem Anil et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Symbolic Reasoning&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=8lNy3QCaxHX&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ICML-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://openreview.net/forum?id=8lNy3QCaxHX&#34;&gt;&lt;strong&gt;Improved logical reasoning of language models via differentiable symbolic programming&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Zhang, Hanlin, Li, Ziyang, Huang, Jiani, Naik, Mayur and Xing, Eric&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.82&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.82&#34;&gt;&lt;strong&gt;Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras and Yejin Choi&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.08686&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.08686&#34;&gt;&lt;strong&gt;The Impact of Symbolic Representations on In-context Learning for Few-shot Reasoning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Hanlin Zhang, Yi-Fan Zhang, Li Erran Li and Eric P. Xing&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.218&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.218&#34;&gt;&lt;strong&gt;UniGeo: Unifying Geometry Logical Reasoning via Reformulating Mathematical Expression&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Jiaqi Chen, Tong Li, Jinghui Qin, Pan Lu, Liang Lin, Chongyu Chen and Xiaodan Liang&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2020.findings-emnlp.418&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP_Findings-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2020.findings-emnlp.418&#34;&gt;&lt;strong&gt;Thinking Like a Skeptic: Defeasible Inference in Natural Language&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Rachel Rudinger, Vered Shwartz, Jena D. Hwang, Chandra Bhagavatula, Maxwell Forbes, Ronan Le Bras, Noah A. Smith and Yejin Choi&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Federated Learning&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1016/j.ins.2021.12.102&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/JIS-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1016/j.ins.2021.12.102&#34;&gt;&lt;strong&gt;Fairness and accuracy in horizontal federated learning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Wei Huang, Tianrui Li, Dexian Wang, Shengdong Du, Junbo Zhang and Tianqiang Huang&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1109/TNSE.2022.3169117&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/TNSE-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1109/TNSE.2022.3169117&#34;&gt;&lt;strong&gt;Federated Learning Meets Multi-Objective Optimization&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Zeou Hu, Kiarash Shaloudegi, Guojun Zhang and Yaoliang Yu&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1007/s10115-022-01664-x&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/KIS-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1007/s10115-022-01664-x&#34;&gt;&lt;strong&gt;From distributed machine learning to federated learning: a survey&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Ji Liu, Jizhou Huang, Yang Zhou, Xuhong Li, Shilei Ji, Haoyi Xiong and Dejing Dou&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.24963/ijcai.2022/273&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/IJCAI-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.24963/ijcai.2022/273&#34;&gt;&lt;strong&gt;Meta-Learning Based Knowledge Extrapolation for Knowledge Graphs in the Federated Setting&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Mingyang Chen, Wen Zhang, Zhen Yao, Xiangnan Chen, Mengxiao Ding, Fei Huang and Huajun Chen&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1145/3511808.3557108&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CIKM-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1145/3511808.3557108&#34;&gt;&lt;strong&gt;Mitigating Biases in Student Performance Prediction via Attention-Based Personalized Federated Learning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Yun-Wei Chu, Seyyedali Hosseinalipour, Elizabeth Tenorio, Laura M. Cruz Castro, Kerrie A. Douglas, Andrew Lan and Christopher G. Brinton&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2022.naacl-main.101&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NAACL-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2022.naacl-main.101&#34;&gt;&lt;strong&gt;Pretrained Models for Multilingual Federated Learning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Orion Weller, Marc Marone, Vladimir Braverman, Dawn J. Lawrie and Benjamin Van Durme&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1109/CVPR52688.2022.00982&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CVPR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1109/CVPR52688.2022.00982&#34;&gt;&lt;strong&gt;Rethinking Architecture Design for Tackling Data Heterogeneity in Federated Learning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Liangqiong Qu, Yuyin Zhou, Paul Pu Liang, Yingda Xia, Feifei Wang, Ehsan Adeli, Li Fei-Fei and Daniel L. Rubin&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1145/3510033&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/TIST-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1145/3510033&#34;&gt;&lt;strong&gt;FedBERT: When Federated Learning Meets Pre-training&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Yuanyishu Tian, Yao Wan, Lingjuan Lyu, Dezhong Yao, Hai Jin and Lichao Sun&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.08090&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.08090&#34;&gt;&lt;strong&gt;Where to Begin? On the Impact of Pre-Training and Initialization in Federated Learning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;John Nguyen, Jianyu Wang, Kshitiz Malik, Maziar Sanjabi and Michael Rabbat&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://proceedings.mlr.press/v139/li21h.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ICML-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://proceedings.mlr.press/v139/li21h.html&#34;&gt;&lt;strong&gt;Ditto: Fair and Robust Federated Learning Through Personalization&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Tian Li, Shengyuan Hu, Ahmad Beirami and Virginia Smith&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2108.07313&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2108.07313&#34;&gt;&lt;strong&gt;Fine-tuning is Fine in Federated Learning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Gary Cheng, Karan N. Chadha and John C. Duchi&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.24963/ijcai.2021/223&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/IJCAI-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.24963/ijcai.2021/223&#34;&gt;&lt;strong&gt;Federated Learning with Fair Averaging&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Zheng Wang, Xiaoliang Fan, Jianzhong Qi, Chenglu Wen, Cheng Wang and Rongshan Yu&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1007/978-3-030-63076-8_14&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/FLPI-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1007/978-3-030-63076-8_14&#34;&gt;&lt;strong&gt;Collaborative Fairness in Federated Learning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Lingjuan Lyu, Xinyi Xu, Qian Wang and Han Yu&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1007/978-3-030-58607-2_5&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ECCV-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1007/978-3-030-58607-2_5&#34;&gt;&lt;strong&gt;Federated Visual Classification with Real-World Data Distribution&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Tzu-Ming Harry Hsu, Hang Qi and Matthew Brown&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Distributed AI&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48786/edbt.2022.48&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EDBT-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48786/edbt.2022.48&#34;&gt;&lt;strong&gt;Distributed Training of Knowledge Graph Embedding Models using Ray&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Nasrullah Sheikh, Xiao Qin, Yaniv Gur and Berthold Reinwald&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1109/JSTSP.2022.3162989&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/IEEE_-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1109/JSTSP.2022.3162989&#34;&gt;&lt;strong&gt;Distributed Learning With Sparsified Gradient Differences&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Yicheng Chen, Rick S. Blum, Martin Tak&#39;ac and Brian M. Sadler&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/9817156&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/AIIoT-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ieeexplore.ieee.org/document/9817156&#34;&gt;&lt;strong&gt;Graph Attention Neural Network Distributed Model Training&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Esmaeilzadeh, Armin, Zadeh Nojoo Kambar, Mina Esmail and Heidari, Maryam&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1007/978-3-031-06156-1_10&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Euro--Par-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1007/978-3-031-06156-1_10&#34;&gt;&lt;strong&gt;Elastic Deep Learning Using Knowledge Distillation with Heterogeneous Computing Resources&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Daxiang Dong, Ji Liu, Xi Wang, Weibao Gong, An Qin, Xingjian Li, Dianhai Yu, Patrick Valduriez et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1109/ICDCS51616.2021.00060&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ICDCS-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1109/ICDCS51616.2021.00060&#34;&gt;&lt;strong&gt;GRACE: A Compressed Communication Framework for Distributed Machine Learning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Hang Xu, Chen-Yu Ho, Ahmed M. Abdelmoniem, Aritra Dutta, El Houcine Bergou, Konstantinos Karatsenidis, Marco Canini and Panos Kalnis&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1109/ICPADS53394.2021.00109&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ICPADS-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1109/ICPADS53394.2021.00109&#34;&gt;&lt;strong&gt;Load Balancing Optimization for Transformer in Distributed Environment&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Delu Ma, Zhou Lei, Shengbo Chen and Peng Wang&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1109/IA351965.2020.00011&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/IA3-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1109/IA351965.2020.00011&#34;&gt;&lt;strong&gt;DistDGL: Distributed Graph Neural Network Training for Billion-Scale Graphs&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Da Zheng, Chao Ma, Minjie Wang, Jinjing Zhou, Qidong Su, Xiang Song, Quan Gan, Zheng Zhang et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.vldb.org/pvldb/vol13/p3005-li.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/VLDB-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://www.vldb.org/pvldb/vol13/p3005-li.pdf&#34;&gt;&lt;strong&gt;PyTorch Distributed: Experiences on Accelerating Data Parallel Training&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.usenix.org/conference/osdi18/presentation/nishihara&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/OSDI-2018-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.usenix.org/conference/osdi18/presentation/nishihara&#34;&gt;&lt;strong&gt;Ray: A Distributed Framework for Emerging AI Applications&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang, Melih Elibol, Zongheng Yang et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Selective Annotation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2209.01975&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2209.01975&#34;&gt;&lt;strong&gt;Selective Annotation Makes Language Models Better Few-Shot Learners&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/HKUNLP/icl-selective-annotation&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code-skyblue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/D19-1410/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/SBERT-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/gptj&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GPT--J-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/gpt_neo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GPT--Neo-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GPT--3-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2107.03374&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Codex-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2205.01068&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/OPT-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;br&gt; by &lt;em&gt;Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf et al.&lt;/em&gt; &lt;br&gt;&lt;code&gt;This paper proposes a graph-based selective annotation method named vote-k to&lt;/code&gt;&lt;br&gt;&lt;code&gt;(1) select a pool of examples to annotate from unlabeled data,&lt;/code&gt;&lt;br&gt;&lt;code&gt;(2) retrieve prompts (contexts) from the annotated data pool for in-context learning.&lt;/code&gt;&lt;br&gt;&lt;code&gt;Specifically, the selection method first selects a small set of unlabeled examples iteratively and then labels them to serve as contexts for LLMs to predict the labels of the rest unlabeled data. The method selects the predictions with highest confidence (log probability of generation output) to fill up the selective annotation pool.&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.vldb.org/pvldb/vol15/p1466-li.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/VLDB-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.vldb.org/pvldb/vol15/p1466-li.pdf&#34;&gt;&lt;strong&gt;Selective Data Acquisition in the Wild for Model Charging&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Chengliang Chai, Jiabin Liu, Nan Tang, Guoliang Li and Yuyu Luo&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Code Generation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.12868&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.12868&#34;&gt;&lt;strong&gt;On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on Codex&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Terry Yue Zhuo, Zhuang Li, Yujin Huang, Yuan-Fang Li, Weiqing Wang, Gholamreza Haffari and Fatemeh Shiri&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/pdf?id=VPCi3STZcaO&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/openreview-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://openreview.net/pdf?id=VPCi3STZcaO&#34;&gt;&lt;strong&gt;CodeT5Mix: A Pretrained Mixture of Encoder-decoder Transformers for Code Understanding and Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Wang, Yue, Le, Hung, Gotmare, Akhilesh Deepak, Li, Junnan and Hoi, Steven&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.05527&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2302.05527&#34;&gt;&lt;strong&gt;CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Zhou, Shuyan, Alon, Uri, Agarwal, Sumit and Neubig, Graham&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.12810&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.12810&#34;&gt;&lt;strong&gt;Code4Struct: Code Generation for Few-Shot Structured Prediction from Natural Language&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Xingyao Wang, Sha Li and Heng Ji&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.07128&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.07128&#34;&gt;&lt;strong&gt;Language Models of Code are Few-Shot Commonsense Learners&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang and Graham Neubig&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.09420&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.09420&#34;&gt;&lt;strong&gt;When Neural Model Meets NL2Code: A Survey&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Daoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu, Bingchao Wu, Bei Guan, Yongji Wang and Jian-Guang Lou&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2204.00498&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2204.00498&#34;&gt;&lt;strong&gt;Evaluating the Text-to-SQL Capabilities of Large Language Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Nitarshan Rajkumar, Raymond Li and Dzmitry Bahdanau&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1145/3533767.3534390&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ISSTA-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1145/3533767.3534390&#34;&gt;&lt;strong&gt;An extensive study on pre-trained models for program understanding and generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Zhengran Zeng, Hanzhuo Tan, Haotian Zhang, Jing Li, Yuqun Zhang and Lingming Zhang&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2207.01780&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2207.01780&#34;&gt;&lt;strong&gt;CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese and Steven C. H. Hoi&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1145/3551349.3556955&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ASE-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1145/3551349.3556955&#34;&gt;&lt;strong&gt;CoditT5: Pretraining for Source Code and Natural Language Editing&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Jiyang Zhang, Sheena Panthaplackel, Pengyu Nie, Junyi Jessy Li and Milos Gligoric&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1145/3551349.3556964&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ASE-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1145/3551349.3556964&#34;&gt;&lt;strong&gt;Compressing Pre-trained Models of Code into 3 MB&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Jieke Shi, Zhou Yang, Bowen Xu, Hong Jin Kang and David Lo&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1145/3540250.3549094&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/FSE-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1145/3540250.3549094&#34;&gt;&lt;strong&gt;Diet code is healthy: simplifying programs for pre-trained models of code&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Zhaowei Zhang, Hongyu Zhang, Beijun Shen and Xiaodong Gu&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1145/3540250.3549162&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/FSE-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1145/3540250.3549162&#34;&gt;&lt;strong&gt;NatGen: generative pre-training by &#34;naturalizing&#34; source code&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Saikat Chakraborty, Toufique Ahmed, Yangruibo Ding, Premkumar T. Devanbu and Baishakhi Ray&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1145/3510003.3510203&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ICSE-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1145/3510003.3510203&#34;&gt;&lt;strong&gt;Jigsaw: Large Language Models meet Program Synthesis&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Naman Jain, Skanda Vaidyanath, Arun Shankar Iyer, Nagarajan Natarajan, Suresh Parthasarathy, Sriram K. Rajamani and Rahul Sharma&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1145/3510003.3510146&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ICSE-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1145/3510003.3510146&#34;&gt;&lt;strong&gt;Natural Attack for Pre-trained Models of Code&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Zhou Yang, Jieke Shi, Junda He and David Lo&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1145/3501385.3543957&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ICER-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1145/3501385.3543957&#34;&gt;&lt;strong&gt;Automatic Generation of Programming Exercises and Code Explanations Using Large Language Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Sami Sarsa, Paul Denny, Arto Hellas and Juho Leinonen&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2107.03374&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2107.03374&#34;&gt;&lt;strong&gt;Evaluating Large Language Models Trained on Code&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pond&#39;e de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.emnlp-main.685&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.emnlp-main.685&#34;&gt;&lt;strong&gt;CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Yue Wang, Weishi Wang, Shafiq R. Joty and Steven C. H. Hoi&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/c16a5320fa475530d9583c34fd356ef5-Abstract-round1.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NeurIPS-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/c16a5320fa475530d9583c34fd356ef5-Abstract-round1.html&#34;&gt;&lt;strong&gt;CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin B. Clement, Dawn Drain et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.naacl-main.211&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NAACL-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.naacl-main.211&#34;&gt;&lt;strong&gt;Unified Pre-training for Program Understanding and Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray and Kai-Wei Chang&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1109/ICSE43902.2021.00040&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ICSE-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1109/ICSE43902.2021.00040&#34;&gt;&lt;strong&gt;Traceability Transformed: Generating more Accurate Links with Pre-Trained BERT Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Jinfeng Lin, Yalin Liu, Qingkai Zeng, Meng Jiang and Jane Cleland-Huang&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1145/3368089.3417058&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/FSE-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1145/3368089.3417058&#34;&gt;&lt;strong&gt;IntelliCode compose: code generation using transformer&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu and Neel Sundaresan&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1145/3324884.3416591&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ASE-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1145/3324884.3416591&#34;&gt;&lt;strong&gt;Multi-task Learning based Pre-trained Language Model for Code Completion&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Fang Liu, Ge Li, Yunfei Zhao and Zhi Jin&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Code Representation&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2022.findings-naacl.80&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NAACL-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2022.findings-naacl.80&#34;&gt;&lt;strong&gt;CODE-MVP: Learning to Represent Source Code from Multiple Views with Contrastive Pre-Training&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Xin Wang, Yasheng Wang, Yao Wan, Jiawei Wang, Pingyi Zhou, Li Li, Hao Wu and Jin Liu&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2022.acl-long.499&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2022.acl-long.499&#34;&gt;&lt;strong&gt;UniXcoder: Unified Cross-Modal Pre-training for Code Representation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou and Jian Yin&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1145/3551349.3556900&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ASE-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1145/3551349.3556900&#34;&gt;&lt;strong&gt;AST-Probe: Recovering abstract syntax trees from hidden representations of pre-trained language models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Jos&#39;e Antonio Hern&#39;andez L&#39;opez, Martin Weyssow, Jes&#39;us S&#39;anchez Cuadrado and Houari A. Sahraoui&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=jLoC4ez43PZ&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ICLR-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://openreview.net/forum?id=jLoC4ez43PZ&#34;&gt;&lt;strong&gt;GraphCodeBERT: Pre-training Code Representations with Data Flow&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2108.04556&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2108.04556&#34;&gt;&lt;strong&gt;CLSEBERT: Contrastive Learning for Syntax Enhanced Code Pre-Trained Model&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Xin Wang, Yasheng Wang, Pingyi Zhou, Fei Mi, Meng Xiao, Yadao Wang, Li Li, Xiao Liu et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Code Fixing&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1145/3533767.3534219&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ISSTA-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1145/3533767.3534219&#34;&gt;&lt;strong&gt;CIRCLE: continual repair across programming languages&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Wei Yuan, Quanjun Zhang, Tieke He, Chunrong Fang, Nguyen Quoc Viet Hung, Xiaodong Hao and Hongzhi Yin&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2022.findings-emnlp.57&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP_Findings-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2022.findings-emnlp.57&#34;&gt;&lt;strong&gt;Detect-Localize-Repair: A Unified Framework for Learning to Debug with CodeT5&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Nghi Bui, Yue Wang and Steven C. H. Hoi&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1007/978-3-031-19211-1_11&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/WASA-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1007/978-3-031-19211-1_11&#34;&gt;&lt;strong&gt;Multi-view Pre-trained Model for Code Vulnerability Identification&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Xuxiang Jiang, Yinhao Xiao, Jun Wang and Wei Zhang&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1145/3524459.3527350&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ICSE-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1145/3524459.3527350&#34;&gt;&lt;strong&gt;Towards JavaScript program repair with Generative Pre-trained Transformer (GPT-2)&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;M&#39;ark Lajk&#39;o, Viktor Csuvik and L&#39;aszl&#39;o Vid&#39;acs&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1145/3510003.3510042&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ICSE-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1145/3510003.3510042&#34;&gt;&lt;strong&gt;Fast Changeset-based Bug Localization with BERT&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Agnieszka Ciborowska and Kostadin Damevski&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1109/MSR52588.2021.00063&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/MSR-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1109/MSR52588.2021.00063&#34;&gt;&lt;strong&gt;Applying CodeBERT for Automated Program Repair of Java Simple Bugs&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Ehsan Mashhadi and Hadi Hemmati&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.mdpi.com/2076-3417/11/11/4755&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Applied_Sciences-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.mdpi.com/2076-3417/11/11/4755&#34;&gt;&lt;strong&gt;A model with iterative trials for correcting logic errors in source code&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Matsumoto, Taku, Watanobe, Yutaka and Nakamura, Keita&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2105.09352&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2105.09352&#34;&gt;&lt;strong&gt;DeepDebug: Fixing Python Bugs Using Stack Traces, Backtranslation, and Code Skeletons&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Dawn Drain, Colin B. Clement, Guillermo Serrato and Neel Sundaresan&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Code Review&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1145/3540250.3549099&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/FSE-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1145/3540250.3549099&#34;&gt;&lt;strong&gt;AUGER: automatically generating review comments with pre-training models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Lingwei Li, Li Yang, Huaxi Jiang, Jun Yan, Tiejian Luo, Zihan Hua, Geng Liang and Chun Zuo&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1145/3540250.3549081&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/FSE-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1145/3540250.3549081&#34;&gt;&lt;strong&gt;Automating code review activities by large-scale pre-training&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Zhiyu Li, Shuai Lu, Daya Guo, Nan Duan, Shailesh Jannu, Grant Jenks, Deep Majumder, Jared Green et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1145/3510003.3510062&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ICSE-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1145/3510003.3510062&#34;&gt;&lt;strong&gt;Bridging Pre-trained Models and Downstream Tasks for Source Code Understanding&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Deze Wang, Zhouyang Jia, Shanshan Li, Yue Yu, Yun Xiong, Wei Dong and Xiangke Liao&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1145/3510003.3510621&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ICSE-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1145/3510003.3510621&#34;&gt;&lt;strong&gt;Using Pre-Trained Models to Boost Code Review Automation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Rosalia Tufano, Simone Masiero, Antonio Mastropaolo, Luca Pascarella, Denys Poshyvanyk and Gabriele Bavota&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1145/3510003.3510050&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ICSE-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1145/3510003.3510050&#34;&gt;&lt;strong&gt;What Do They Capture? - A Structural Analysis of Pre-Trained Language Models for Source Code&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Yao Wan, Wei Zhao, Hongyu Zhang, Yulei Sui, Guandong Xu and Hai Jin&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Software Engineering&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.24963/ijcai.2022/775&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/IJCAI-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.24963/ijcai.2022/775&#34;&gt;&lt;strong&gt;Deep Learning Meets Software Engineering: A Survey on Pre-Trained Models of Source Code&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Changan Niu, Chuanyi Li, Bin Luo and Vincent Ng&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2211.10623&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2211.10623&#34;&gt;&lt;strong&gt;Do Pre-trained Language Models Indeed Understand Software Engineering Tasks?&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Yao Li, Tao Zhang, Xiapu Luo, Haipeng Cai, Sen Fang and Dawei Yuan&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2104.05861&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2104.05861&#34;&gt;&lt;strong&gt;Evaluating Pre-Trained Models for User Feedback Analysis in Software Engineering: A Study on Classification of App-Reviews&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Mohammad Abdul Hadi and Fatemeh H. Fard&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1109/ASE51524.2021.9678927&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ASE-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1109/ASE51524.2021.9678927&#34;&gt;&lt;strong&gt;What do pre-trained code models know about code?&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Anjan Karmakar and Romain Robbes&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/9240704/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ICSME-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/9240704/&#34;&gt;&lt;strong&gt;Sentiment analysis for software engineering: How far can pre-trained transformer models go?&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Zhang, Ting, Xu, Bowen, Thung, Ferdian, Haryono, Stefanus Agus, Lo, David and Jiang, Lingxiao&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;AIGC&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cdn.openai.com/papers/gpt-4.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/OpenAI-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://cdn.openai.com/papers/gpt-4.pdf&#34;&gt;&lt;strong&gt;GPT-4 Technical Report&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://cdn.openai.com/papers/gpt-4.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GPT--4-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;br&gt; by &lt;em&gt;OpenAI&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cdn.openai.com/papers/gpt-4-system-card.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/OpenAI-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://cdn.openai.com/papers/gpt-4-system-card.pdf&#34;&gt;&lt;strong&gt;GPT-4 System Card&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://cdn.openai.com/papers/gpt-4.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GPT--4-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;br&gt; by &lt;em&gt;OpenAI&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2209.12356&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NeurIPS-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2209.12356&#34;&gt;&lt;strong&gt;News Summarization and Evaluation in the Era of GPT-3&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Tanya Goyal, Junyi Jessy Li and Greg Durrett&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2022.acl-long.471&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2022.acl-long.471&#34;&gt;&lt;strong&gt;Fine-Grained Controllable Text Generation Using Non-Residual Prompting&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Fredrik Carlsson, Joey &#34;Ohman, Fangyu Liu, Severine Verlinden, Joakim Nivre and Magnus Sahlgren&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1016/j.jksuci.2020.04.001&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/JKSUCIS-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1016/j.jksuci.2020.04.001&#34;&gt;&lt;strong&gt;The survey: Text generation models in deep learning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Touseef Iqbal and Shaima Qureshi&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2206.04624&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2206.04624&#34;&gt;&lt;strong&gt;Factuality Enhanced Language Models for Open-Ended Text Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Mohammad Shoeybi and Bryan Catanzaro&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.emnlp-main.491&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.emnlp-main.491&#34;&gt;&lt;strong&gt;FewshotQA: A simple framework for few-shot learning of question answering tasks using pre-trained text-to-text models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Rakesh Chada and Pradeep Natarajan&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2103.10360&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2103.10360&#34;&gt;&lt;strong&gt;All NLP Tasks Are Generation Tasks: A General Pretraining Framework&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang and Jie Tang&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.emnlp-main.57&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.emnlp-main.57&#34;&gt;&lt;strong&gt;Smelting Gold and Silver for Improved Multilingual AMR-to-Text Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Leonardo F. R. Ribeiro, Jonas Pfeiffer, Yue Zhang and Iryna Gurevych&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.acl-short.40&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.acl-short.40&#34;&gt;&lt;strong&gt;PRAL: A Tailored Pre-Training Model for Task-Oriented Dialog Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Jing Gu, Qingyang Wu, Chongruo Wu, Weiyan Shi and Zhou Yu&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/view/17527&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/AAAI-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/view/17527&#34;&gt;&lt;strong&gt;DialogBERT: Discourse-Aware Response Generation via Learning to Recover and Rank Utterances&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Xiaodong Gu, Kang Min Yoo and Jung-Woo Ha&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.acl-long.501&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.acl-long.501&#34;&gt;&lt;strong&gt;DYPLOC: Dynamic Planning of Content Using Mixed Language Models for Text Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Xinyu Hua, Ashwin Sreevatsa and Lu Wang&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.findings-acl.265&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL_Findings-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.findings-acl.265&#34;&gt;&lt;strong&gt;Latent Reasoning for Low-Resource Question Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Xinting Huang, Jianzhong Qi, Yu Sun and Rui Zhang&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.findings-acl.223&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL_Findings-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.findings-acl.223&#34;&gt;&lt;strong&gt;JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Pei Ke, Haozhe Ji, Yu Ran, Xin Cui, Liwei Wang, Linfeng Song, Xiaoyan Zhu and Minlie Huang&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.acl-demo.4&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.acl-demo.4&#34;&gt;&lt;strong&gt;TextBox: A Unified, Modularized, and Extensible Framework for Text Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Junyi Li, Tianyi Tang, Gaole He, Jinhao Jiang, Xiaoxuan Hu, Puzhao Xie, Zhipeng Chen, Zhuohao Yu et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.findings-acl.136&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL_Findings-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.findings-acl.136&#34;&gt;&lt;strong&gt;Few-shot Knowledge Graph-to-Text Generation with Pretrained Language Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Junyi Li, Tianyi Tang, Wayne Xin Zhao, Zhicheng Wei, Nicholas Jing Yuan and Ji-Rong Wen&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1145/3404835.3462865&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/SIGIR-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1145/3404835.3462865&#34;&gt;&lt;strong&gt;Knowledge-based Review Generation by Coherence Enhanced Text Planning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Junyi Li, Wayne Xin Zhao, Zhicheng Wei, Nicholas Jing Yuan and Ji-Rong Wen&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.acl-long.353&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.acl-long.353&#34;&gt;&lt;strong&gt;Prefix-Tuning: Optimizing Continuous Prompts for Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Xiang Lisa Li and Percy Liang&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.findings-acl.36&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.findings-acl.36&#34;&gt;&lt;strong&gt;GLGE: A New General Language Generation Evaluation Benchmark&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Dayiheng Liu, Yu Yan, Yeyun Gong, Weizhen Qi, Hang Zhang, Jian Jiao, Weizhu Chen, Jie Fu et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.emnlp-main.173&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.emnlp-main.173&#34;&gt;&lt;strong&gt;A Three-Stage Learning Framework for Low-Resource Knowledge-Grounded Dialogue Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Shilei Liu, Xiaofeng Zhao, Bochao Li, Feiliang Ren, Longhui Zhang and Shujuan Yin&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.acl-long.308&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.acl-long.308&#34;&gt;&lt;strong&gt;VECO: Variable and Flexible Cross-lingual Pre-training for Language Understanding and Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Fuli Luo, Wei Wang, Jiahao Liu, Yijia Liu, Bin Bi, Songfang Huang, Fei Huang and Luo Si&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.naacl-main.340&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NAACL-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.naacl-main.340&#34;&gt;&lt;strong&gt;Ask what&#39;s missing and what&#39;s useful: Improving Clarification Question Generation using Global Knowledge&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Bodhisattwa Prasad Majumder, Sudha Rao, Michel Galley and Julian J. McAuley&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.findings-acl.248&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL_Findings-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.findings-acl.248&#34;&gt;&lt;strong&gt;ZmBART: An Unsupervised Cross-lingual Transfer Framework for Language Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Kaushal Kumar Maurya, Maunendra Sankar Desarkar, Yoshinobu Kano and Kumari Deepshikha&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.emnlp-main.351&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.emnlp-main.351&#34;&gt;&lt;strong&gt;Structural Adapters in Pretrained Language Models for AMR-to-Text Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Leonardo F. R. Ribeiro, Yue Zhang and Iryna Gurevych&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.acl-long.115&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.acl-long.115&#34;&gt;&lt;strong&gt;Towards Table-to-Text Generation with Numerical Reasoning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Lya Hulliyyatus Suadaa, Hidetaka Kamigaito, Kotaro Funakoshi, Manabu Okumura and Hiroya Takamura&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2107.02137&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2107.02137&#34;&gt;&lt;strong&gt;ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Yu Sun, Shuohuan Wang, Shikun Feng, Siyu Ding, Chao Pang, Junyuan Shang, Jiaxiang Liu, Xuyi Chen et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.naacl-main.341&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NAACL-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.naacl-main.341&#34;&gt;&lt;strong&gt;Progressive Generation of Long Text with Pretrained Language Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Bowen Tan, Zichao Yang, Maruan Al-Shedivat, Eric P. Xing and Zhiting Hu&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1007/978-3-030-72113-8_46&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ECIR-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1007/978-3-030-72113-8_46&#34;&gt;&lt;strong&gt;Consistency and Coherency Enhanced Story Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Wei Wang, Piji Li and Hai-Tao Zheng&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.findings-acl.200&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL_Findings-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.findings-acl.200&#34;&gt;&lt;strong&gt;Structure-Aware Pre-Training for Table-to-Text Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Xinyu Xing and Xiaojun Wan&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.acl-long.95&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.acl-long.95&#34;&gt;&lt;strong&gt;AugNLG: Few-shot Natural Language Generation using Self-trained Data Augmentation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Xinnuo Xu, Guoyin Wang, Young-Bum Kim and Sungjin Lee&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.acl-long.6&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.acl-long.6&#34;&gt;&lt;strong&gt;DeepRapper: Neural Rap Generation with Rhyme and Rhythm Modeling&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Lanqing Xue, Kaitao Song, Duocai Wu, Xu Tan, Nevin L. Zhang, Tao Qin, Wei-Qiang Zhang and Tie-Yan Liu&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.acl-demo.26&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.acl-demo.26&#34;&gt;&lt;strong&gt;FastSeq: Make Sequence Generation Faster&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Yu Yan, Fei Hu, Jiusheng Chen, Nikhil Bhendawade, Ting Ye, Yeyun Gong, Nan Duan, Desheng Cui et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.naacl-main.392&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NAACL-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.naacl-main.392&#34;&gt;&lt;strong&gt;A Simple and Efficient Multi-Task Learning Approach for Conditioned Dialogue Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Yan Zeng and Jian-Yun Nie&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1145/3404835.3463037&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/SIGIR-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1145/3404835.3463037&#34;&gt;&lt;strong&gt;DSGPT: Domain-Specific Generative Pre-Training of Transformers for Text Generation in E-commerce Title and Review Summarization&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Xueying Zhang, Yunjiang Jiang, Yue Shang, Zhaomeng Cheng, Chi Zhang, Xiaochuan Fan, Yun Xiao and Bo Long&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NeurIPS-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html&#34;&gt;&lt;strong&gt;Language Models are Few-Shot Learners&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GPT--3-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;br&gt; by &lt;em&gt;Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2020.acl-main.9&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2020.acl-main.9&#34;&gt;&lt;strong&gt;PLATO: Pre-trained Dialogue Generation Model with Discrete Latent Variable&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Siqi Bao, Huang He, Fan Wang, Hua Wu and Haifeng Wang&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2006.14799&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2006.14799&#34;&gt;&lt;strong&gt;Evaluation of Text Generation: A Survey&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Asli Celikyilmaz, Elizabeth Clark and Jianfeng Gao&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2020.emnlp-main.697&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2020.emnlp-main.697&#34;&gt;&lt;strong&gt;KGPT: Knowledge-Grounded Pre-Training for Data-to-Text Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Wenhu Chen, Yu Su, Xifeng Yan and William Yang Wang&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2020.acl-main.705&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2020.acl-main.705&#34;&gt;&lt;strong&gt;Distilling Knowledge Learned in BERT for Text Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Yen-Chun Chen, Zhe Gan, Yu Cheng, Jingzhou Liu and Jingjing Liu&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2020.findings-emnlp.190&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP_Findings-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2020.findings-emnlp.190&#34;&gt;&lt;strong&gt;Logic2Text: High-Fidelity Natural Language Generation from Logical Forms&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Zhiyu Chen, Wenhu Chen, Hanwen Zha, Xiyou Zhou, Yunkai Zhang, Sairam Sundaresan and William Yang Wang&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/view/6256&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/AAAI-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/view/6256&#34;&gt;&lt;strong&gt;Cross-Lingual Natural Language Generation via Pre-Training&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Zewen Chi, Li Dong, Furu Wei, Wenhui Wang, Xian-Ling Mao and Heyan Huang&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2007.15780&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2007.15780&#34;&gt;&lt;strong&gt;Neural Language Generation: Formulation, Methods, and Evaluation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Cristina Garbacea and Qiaozhu Mei&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2020.coling-main.179&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/COLING-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2020.coling-main.179&#34;&gt;&lt;strong&gt;TableGPT: Few-shot Table-to-Text Generation with Table Structure Reconstruction and Content Matching&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Heng Gong, Yawei Sun, Xiaocheng Feng, Bing Qin, Wei Bi, Xiaojiang Liu and Ting Liu&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1162/tacl_a_00302&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/TACL-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1162/tacl_a_00302&#34;&gt;&lt;strong&gt;A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Jian Guan, Fei Huang, Minlie Huang, Zhihao Zhao and Xiaoyan Zhu&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2020.coling-main.218&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/COLING-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2020.coling-main.218&#34;&gt;&lt;strong&gt;Have Your Text and Use It Too! End-to-End Neural Data-to-Text Generation with Semantic Fidelity&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Hamza Harkous, Isabel Groves and Amir Saffari&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2020.emnlp-main.55&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2020.emnlp-main.55&#34;&gt;&lt;strong&gt;Reformulating Unsupervised Style Transfer as Paraphrase Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Kalpesh Krishna, John Wieting and Mohit Iyyer&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2020.acl-main.703&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2020.acl-main.703&#34;&gt;&lt;strong&gt;BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov and Luke Zettlemoyer&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1145/3340531.3411893&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CIKM-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1145/3340531.3411893&#34;&gt;&lt;strong&gt;Knowledge-Enhanced Personalized Review Generation with Capsule Graph Neural Network&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Junyi Li, Siqing Li, Wayne Xin Zhao, Gaole He, Zhicheng Wei, Nicholas Jing Yuan and Ji-Rong Wen&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2020.acl-main.68&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2020.acl-main.68&#34;&gt;&lt;strong&gt;Rigid Formats Controlled Text Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Piji Li, Haisong Zhang, Xiaojiang Liu and Shuming Shi&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2002.06353&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2002.06353&#34;&gt;&lt;strong&gt;UniViLM: A Unified Video and Language Pre-Training Model for Multimodal Understanding and Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan Duan, Tianrui Li, Xilin Chen and Ming Zhou&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2020.acl-main.167&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2020.acl-main.167&#34;&gt;&lt;strong&gt;GPT-too: A Language-Model-First Approach for AMR-to-Text Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Manuel Mager, Ram&#39;on Fernandez Astudillo, Tahira Naseem, Md. Arafat Sultan, Young-Suk Lee, Radu Florian and Salim Roukos&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2020.findings-emnlp.17&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP_Findings-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2020.findings-emnlp.17&#34;&gt;&lt;strong&gt;Few-shot Natural Language Generation for Task-Oriented Dialog&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Baolin Peng, Chenguang Zhu, Chunyuan Li, Xiujun Li, Jinchao Li, Michael Zeng and Jianfeng Gao&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2020.emnlp-main.349&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2020.emnlp-main.349&#34;&gt;&lt;strong&gt;PlotMachines: Outline-Conditioned Generation with Dynamic Plot State Tracking&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Hannah Rashkin, Asli Celikyilmaz, Yejin Choi and Jianfeng Gao&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2007.08426&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2007.08426&#34;&gt;&lt;strong&gt;Investigating Pretrained Language Models for Graph-to-Text Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Leonardo F. R. Ribeiro, Martin Schmitt, Hinrich Sch&#34;utze and Iryna Gurevych&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1162/tacl_a_00313&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/TACL-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1162/tacl_a_00313&#34;&gt;&lt;strong&gt;Leveraging Pre-trained Checkpoints for Sequence Generation Tasks&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Sascha Rothe, Shashi Narayan and Aliaksei Severyn&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2020.emnlp-main.495&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2020.emnlp-main.495&#34;&gt;&lt;strong&gt;T3: Tree-Autoencoder Constrained Adversarial Text Generation for Targeted Attack&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Boxin Wang, Hengzhi Pei, Boyuan Pan, Qian Chen, Shuohang Wang and Bo Li&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2020.emnlp-main.226&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2020.emnlp-main.226&#34;&gt;&lt;strong&gt;MEGATRON-CNTRL: Controllable Story Generation with External Knowledge Using Large-Scale Language Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Peng Xu, Mostofa Patwary, Mohammad Shoeybi, Raul Puri, Pascale Fung, Anima Anandkumar and Bryan Catanzaro&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2020.findings-emnlp.140&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP_Findings-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2020.findings-emnlp.140&#34;&gt;&lt;strong&gt;StyleDGPT: Stylized Response Generation with Pre-trained Language Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Ze Yang, Wei Wu, Can Xu, Xinnian Liang, Jiaqi Bai, Liran Wang, Wei Wang and Zhoujun Li&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2010.11140&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.11140&#34;&gt;&lt;strong&gt;Generalized Conditioned Dialogue Generation Based on Pre-trained Language Model&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Yan Zeng and Jian-Yun Nie&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=SkeHuCVFDr&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ICLR-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://openreview.net/forum?id=SkeHuCVFDr&#34;&gt;&lt;strong&gt;BERTScore: Evaluating Text Generation with BERT&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger and Yoav Artzi&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2020.acl-demos.30&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2020.acl-demos.30&#34;&gt;&lt;strong&gt;DIALOGPT : Large-Scale Generative Pre-training for Conversational Response Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/OpenAI-2019-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&#34;&gt;&lt;strong&gt;Language Models are Unsupervised Multitask Learners&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GPT--2-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;br&gt; by &lt;em&gt;Radford, Alec, Wu, Jeffrey, Child, Rewon, Luan, David, Amodei, Dario and Sutskever, Ilya&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper/2019/hash/c20bb2d9a50d5ac1f713f8b34d9aac5a-Abstract.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NeurIPS-2019-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://proceedings.neurips.cc/paper/2019/hash/c20bb2d9a50d5ac1f713f8b34d9aac5a-Abstract.html&#34;&gt;&lt;strong&gt;Unified Language Model Pre-training for Natural Language Understanding and Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/p19-1608&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2019-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/p19-1608&#34;&gt;&lt;strong&gt;Large-Scale Transfer Learning for Natural Language Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Sergey Golovanov, Rauf Kurbanov, Sergey I. Nikolenko, Kyryl Truskovskyi, Alexander Tselousov and Thomas Wolf&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/D19-1615&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2019-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/D19-1615&#34;&gt;&lt;strong&gt;Improving Neural Story Generation by Targeted Common Sense Grounding&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Huanru Henry Mao, Bodhisattwa Prasad Majumder, Julian J. McAuley and Garrison W. Cottrell&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://proceedings.mlr.press/v97/song19d.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ICML-2019-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://proceedings.mlr.press/v97/song19d.html&#34;&gt;&lt;strong&gt;MASS: Masked Sequence to Sequence Pre-training for Language Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu and Tie-Yan Liu&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=Hyg0vbWC-&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ICLR-2018-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://openreview.net/forum?id=Hyg0vbWC-&#34;&gt;&lt;strong&gt;Generating Wikipedia by Summarizing Long Sequences&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser and Noam Shazeer&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/OpenAI-2018-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf&#34;&gt;&lt;strong&gt;Improving language understanding by generative pre-training&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GPT--1-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;br&gt; by &lt;em&gt;Radford, Alec, Narasimhan, Karthik, Salimans, Tim, Sutskever, Ilya and others&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Controllable Text Generation&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2205.13636&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2205.13636&#34;&gt;&lt;strong&gt;Quark: Controllable Text Generation with Reinforced Unlearning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Ximing Lu, Sean Welleck, Liwei Jiang, Jack Hessel, Lianhui Qin, Peter West, Prithviraj Ammanabrolu and Yejin Choi&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.acl-long.502&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.acl-long.502&#34;&gt;&lt;strong&gt;Controllable Open-ended Question Generation with A New Question Type Ontology&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Shuyang Cao and Lu Wang&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=jWkw45-9AbL&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ICLR-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://openreview.net/forum?id=jWkw45-9AbL&#34;&gt;&lt;strong&gt;A Distributional Approach to Controlled Text Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Muhammad Khalifa, Hady Elsahar and Marc Dymetman&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.findings-emnlp.334&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL_Findings-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.findings-emnlp.334&#34;&gt;&lt;strong&gt;A Plug-and-Play Method for Controlled Text Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Damian Pascual, Beni Egressy, Clara Meister, Ryan Cotterell and Roger Wattenhofer&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=3k20LAiHYL2&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ICLR-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://openreview.net/forum?id=3k20LAiHYL2&#34;&gt;&lt;strong&gt;Pre-training Text-to-Text Transformers for Concept-centric Common Sense&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Wangchunshu Zhou, Dong-Ho Lee, Ravi Kiran Selvam, Seyeon Lee and Xiang Ren&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.acl-long.9&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.acl-long.9&#34;&gt;&lt;strong&gt;Mention Flags (MF): Constraining Transformer-based Text Generators&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Yufei Wang, Ian D. Wood, Stephen Wan, Mark Dras and Mark Johnson&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=H1edEyBKDS&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ICLR-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://openreview.net/forum?id=H1edEyBKDS&#34;&gt;&lt;strong&gt;Plug and Play Language Models: A Simple Approach to Controlled Text Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski and Rosanne Liu&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2020.findings-emnlp.165&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP_Findings-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2020.findings-emnlp.165&#34;&gt;&lt;strong&gt;CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi and Xiang Ren&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://arxiv.org/abs/1909.05858&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2019-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://arxiv.org/abs/1909.05858&#34;&gt;&lt;strong&gt;CTRL: A Conditional Transformer Language Model for Controllable Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney, Caiming Xiong and Richard Socher&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Continual Learning&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2303.01081&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2303.01081&#34;&gt;&lt;strong&gt;Can BERT Refrain from Forgetting on Sequential Tasks? A Probing Study&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Mingxu Tao, Yansong Feng and Dongyan Zhao&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1109/CVPR52688.2022.00024&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CVPR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1109/CVPR52688.2022.00024&#34;&gt;&lt;strong&gt;Learning to Prompt for Continual Learning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1109/TPAMI.2021.3057446&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/T--PAMI-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1109/TPAMI.2021.3057446&#34;&gt;&lt;strong&gt;A Continual Learning Survey: Defying Forgetting in Classification Tasks&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory G. Slabaugh and Tinne Tuytelaars&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2022.findings-acl.220&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL_Findings-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2022.findings-acl.220&#34;&gt;&lt;strong&gt;ELLE: Efficient Lifelong Pre-training for Emerging Data&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Yujia Qin, Jiajie Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun and Jie Zhou&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2022.naacl-main.351&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NAACL-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2022.naacl-main.351&#34;&gt;&lt;strong&gt;Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Xisen Jin, Dejiao Zhang, Henghui Zhu, Wei Xiao, Shang-Wen Li, Xiaokai Wei, Andrew O. Arnold and Xiang Ren&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1613/jair.1.13673&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/JAIR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1613/jair.1.13673&#34;&gt;&lt;strong&gt;Towards Continual Reinforcement Learning: A Review and Perspectives&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Khimya Khetarpal, Matthew Riemer, Irina Rish and Doina Precup&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2022.acl-long.408&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2022.acl-long.408&#34;&gt;&lt;strong&gt;Continual Pre-training of Language Models for Math Problem Understanding with Syntax-Aware Memory Network&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Zheng Gong, Kun Zhou, Xin Zhao, Jing Sha, Shijin Wang and Ji-Rong Wen&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=HCRVf71PMF&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ICLR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://openreview.net/forum?id=HCRVf71PMF&#34;&gt;&lt;strong&gt;LFPT5: A Unified Framework for Lifelong Few-shot Language Learning Based on Prompt Tuning of T5&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Chengwei Qin and Shafiq Joty&lt;/em&gt; &lt;br&gt;&lt;code&gt;We define a challenging yet practical problem as Lifelong Few-shot Language Learning and propose a unified framework for it based on prompt tuning of T5.&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=vfsRB5MImo9&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ICLR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://openreview.net/forum?id=vfsRB5MImo9&#34;&gt;&lt;strong&gt;Towards Continual Knowledge Learning of Language Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Joel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun KIM, Stanley Jungkyu Choi and Minjoon Seo&lt;/em&gt; &lt;br&gt;&lt;code&gt;We propose a novel continual learning formulation named Continual Knowledge Learning which allows large language models to constantly obtain new and updated knowledge while mitigating forgetting of previous learned time-invariant knowledge.&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=figzpGMrdD&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ICLR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://openreview.net/forum?id=figzpGMrdD&#34;&gt;&lt;strong&gt;Pretrained Language Model in Continual Learning: A Comparative Study&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Tongtong Wu, Massimo Caccia, Zhuang Li, Yuan-Fang Li, Guilin Qi and Gholamreza Haffari&lt;/em&gt; &lt;br&gt;&lt;code&gt;To explore the layer-wise property of pretrained languge models in continual learning, we thoroughly compare the continual learning performance over the combination of 5 PLMs and 4 veins of CL methods on 3 benchmarks in 2 typical incremental settings.&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.418&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.418&#34;&gt;&lt;strong&gt;TemporalWiki: A Lifelong Benchmark for Training and Evaluating Ever-Evolving Language Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Joel Jang, Seonghyeon Ye, Changho Lee, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim and Minjoon Seo&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://proceedings.mlr.press/v162/liska22a.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ICML-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://proceedings.mlr.press/v162/liska22a.html&#34;&gt;&lt;strong&gt;StreamingQA: A Benchmark for Adaptation to New Knowledge over Time in Question Answering Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Adam Liska, Tom&#39;as Kocisk&#39;y, Elena Gribovskaya, Tayfun Terzi, Eren Sezener, Devang Agrawal, Cyprien de Masson d&#39;Autume, Tim Scholtes et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper/2021/hash/bcd0049c35799cdf57d06eaf2eb3cff6-Abstract.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NeurIPS-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://proceedings.neurips.cc/paper/2021/hash/bcd0049c35799cdf57d06eaf2eb3cff6-Abstract.html&#34;&gt;&lt;strong&gt;Achieving Forgetting Prevention and Knowledge Transfer in Continual Learning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Zixuan Ke, Bing Liu, Nianzu Ma, Hu Xu and Lei Shu&lt;/em&gt; &lt;br&gt;&lt;code&gt;NeurIPS 2021, The key component of CTR is the CL-plugin inserted in BERT. A CL-plugin is a capsule network with a new transfer routing mechanism to encourage knowledge transfer among tasks and also to isolate task-specific knowledge to avoid forgetting.&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2021.findings-emnlp.62&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP_Findings-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2021.findings-emnlp.62&#34;&gt;&lt;strong&gt;Learn Continually, Generalize Rapidly: Lifelong Knowledge Accumulation for Few-shot Learning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Jin, Xisen , Lin, Bill Yuchen , Rostami, Mohammad and Ren, Xiang&lt;/em&gt; &lt;br&gt;&lt;code&gt;We present a new learning setup, Continual Learning of Few-Shot Learners, to address challenges of both learning settings in a unified setup, with a hyper-network for task-specific adapter generation.&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.eacl-main.95&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EACL-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.eacl-main.95&#34;&gt;&lt;strong&gt;Analyzing the Forgetting Problem in Pretrain-Finetuning of Open-domain Dialogue Response Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Tianxing He, Jun Liu, Kyunghyun Cho, Myle Ott, Bing Liu, James R. Glass and Fuchun Peng&lt;/em&gt; &lt;br&gt;&lt;code&gt;Our major finding is that after standard finetuning, the model forgets some of the important language generation skills acquired during large-scale pretraining. We propose an intuitive finetuning strategy named “mix-review”: : For each finetuning epoch, we mix the target dialogue data with a random subset of the pretraining data, mix_ratio is 4, decay is 0.9.&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.findings-acl.121&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL_Findings-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.findings-acl.121&#34;&gt;&lt;strong&gt;K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Jianshu Ji, Guihong Cao, Daxin Jiang et al.&lt;/em&gt; &lt;br&gt;&lt;code&gt;We propose KADAPTER, a framework that retains the original parameters of the pre-trained model fixed&lt;/code&gt;&lt;br&gt;&lt;code&gt;and supports the development of versatile&lt;/code&gt;&lt;br&gt;&lt;code&gt;knowledge-infused model.&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2021.emnlp-main.176&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2021.emnlp-main.176&#34;&gt;&lt;strong&gt;Domain-Lifelong Learning for Dialogue State Tracking via Knowledge Preservation Networks&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Liu, Qingbin , Cao, Pengfei , Liu, Cao , Chen, Jiansong , Cai, Xunliang , Yang, Fan , He, Shizhu , Liu, Kang et al.&lt;/em&gt; &lt;br&gt;&lt;code&gt;This paper explores Domain-Lifelong Learning for Dialogue State Tracking, we propose Knowledge Preservation Network, which consists of multi-prototype enhanced retrospection and multi-strategy knowledge distillation, to solve the problems of expression diversity and combinatorial explosion in the DLL-DST task&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2021.emnlp-main.550&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2021.emnlp-main.550&#34;&gt;&lt;strong&gt;CLASSIC: Continual and Contrastive Learning of Aspect Sentiment Classification Tasks&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Ke, Zixuan , Liu, Bing , Xu, Hu and Shu, Lei&lt;/em&gt; &lt;br&gt;&lt;code&gt;The key novelty is a contrastive continual learning method that enables both knowledge transfer across tasks and knowledge distillation from old tasks to the new task, which eliminates the need for task ids in testing.&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2021.emnlp-main.233&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2021.emnlp-main.233&#34;&gt;&lt;strong&gt;Lifelong Explainer for Lifelong Learners&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Situ, Xuelin , Maruf, Sameen , Zukerman, Ingrid , Paris, Cecile and Haffari, Gholamreza&lt;/em&gt; &lt;br&gt;&lt;code&gt;We propose a novel Lifelong Explanation approach that continuously trains a student explainer under the supervision of a teacher – an arbitrary explanation algorithm – on different tasks undertaken in LL. We also leverage the Experience Replay mechanism to prevent catastrophic forgetting in the student explainer.&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2021.emnlp-main.737&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2021.emnlp-main.737&#34;&gt;&lt;strong&gt;A Unified Speaker Adaptation Approach for ASR&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Yingzhu Zhao, Chongjia Ni, Cheung-Chi Leung, Shafiq R. Joty, Eng Siong Chng and Bin Ma&lt;/em&gt; &lt;br&gt;&lt;code&gt;Prefix-based user identifier, Continual ASR / Architecture Search / Network Pruning.&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1145/3447548.3467162&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/SIGKDD-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1145/3447548.3467162&#34;&gt;&lt;strong&gt;Dynamic Language Models for Continuously Evolving Content&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Amba Hombaiah, Spurthi, Chen, Tao, Zhang, Mingyang, Bendersky, Michael and Najork, Marc&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2021.acl-long.378&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2021.acl-long.378&#34;&gt;&lt;strong&gt;Parameter-Efficient Transfer Learning with Diff Pruning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Guo, Demi , Rush, Alexander and Kim, Yoon&lt;/em&gt; &lt;br&gt;&lt;code&gt;The approach learns a task-specific “diff” vector that extends the original pretrained parameters. As the number of tasks increases, diff pruning remains parameter-efficient, as it requires storing only a small diff vector for each task.&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2021.acl-long.20&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2021.acl-long.20&#34;&gt;&lt;strong&gt;Refining Sample Embeddings with Relation Prototypes to Enhance Continual Relation Extraction&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Cui, Li , Yang, Deqing , Yu, Jiaxin , Hu, Chengwei , Cheng, Jiayang , Yi, Jingjie and Xiao, Yanghua&lt;/em&gt; &lt;br&gt;&lt;code&gt;To fully utilize memorized samples, in this paper, we employ relation prototype to extract useful information of each relation. &lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2021.acl-long.172&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2021.acl-long.172&#34;&gt;&lt;strong&gt;On the Effectiveness of Adapter-based Tuning for Pretrained Language Model Adaptation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;He, Ruidan , Liu, Linlin , Ye, Hai , Tan, Qingyu , Ding, Bosheng , Cheng, Liying , Low, Jiawei , Bing, Lidong et al.&lt;/em&gt; &lt;br&gt;&lt;code&gt;we first show that adapter-based tuning better mitigates forgetting issues than fine-tuning since it yields representations with less deviation from those generated by the initial PrLM. Effectiveness: it tendsto outperform fine-tuning on both low-resource and cross-lingual tasks; 2 it demonstrates higher stability under different learning rates compared to fine-tuning.&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2021.acl-long.229&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2021.acl-long.229&#34;&gt;&lt;strong&gt;Rational LAMOL: A Rationale-based Lifelong Learning Framework&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Kanwatchara, Kasidis , Horsuwan, Thanapapas , Lertvittayakumjorn, Piyawat , Kijsirikul, Boonserm and Vateekul, Peerapon&lt;/em&gt; &lt;br&gt;&lt;code&gt;Rational LAMOL enhances LAMOL, a recent LL model, by applying critical freezing guided by human rationales. When the human rationales are not available, we propose exploiting unsupervised generated rationales as substitutions.&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/2021.naacl-main.93&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NAACL--HLT-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2021.naacl-main.93&#34;&gt;&lt;strong&gt;Towards Continual Learning for Multilingual Machine Translation via Vocabulary Substitution&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Garcia, Xavier , Constant, Noah , Parikh, Ankur and Firat, Orhan&lt;/em&gt; &lt;br&gt;&lt;code&gt;Introducing the catastrophic forgetting problem in incremental multi-language translation, and utilizing a vocabulary substitution manner to alleviate the above problem.&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/2021.naacl-main.218&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NAACL--HLT-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2021.naacl-main.218&#34;&gt;&lt;strong&gt;Continual Learning for Text Classification with Information Disentanglement Based Regularization&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Huang, Yufan , Zhang, Yanzhe , Chen, Jiaao , Wang, Xuezhi and Yang, Diyi&lt;/em&gt; &lt;br&gt;&lt;code&gt;Proposing a regularization-based method for continual text classification, introducing the next sentence prediction and task id prediction as auxiliary tasks.&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/2021.naacl-main.106&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NAACL--HLT-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2021.naacl-main.106&#34;&gt;&lt;strong&gt;Incremental Few-shot Text Classification with Multi-round New Classes: Formulation, Dataset and System&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Xia, Congying , Yin, Wenpeng , Feng, Yihao and Yu, Philip&lt;/em&gt; &lt;br&gt;&lt;code&gt;Proposing a new setting and respective benchmark for few-shot incremental text classification, modeling continual text classification with text entailment.&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/2021.naacl-main.212&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NAACL--HLT-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2021.naacl-main.212&#34;&gt;&lt;strong&gt;Hyperparameter-free Continuous Learning for Domain Classification in Natural Language Understanding&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Hua, Ting , Shen, Yilin , Zhao, Changsheng , Hsu, Yen-Chang and Jin, Hongxia&lt;/em&gt; &lt;br&gt;&lt;code&gt;Inspired by EWC and proposing a hyperparameter-free (Fisher information-based) sampling method for memory replay.&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/2021.eacl-main.317&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EACL-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2021.eacl-main.317&#34;&gt;&lt;strong&gt;Lifelong Knowledge-Enriched Social Event Representation Learning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Vijayaraghavan, Prashanth and Roy, Deb&lt;/em&gt; &lt;br&gt;&lt;code&gt;Proposing a rehearsal-based method, i.e.,Domain-Representative Episodic Memory Replay (DR-EMR), for lifelong event representation with embedding alignment and external social commonsense knowledge.&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2108.04445&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2108.04445&#34;&gt;&lt;strong&gt;Lifelong Intent Detection via Multi-Strategy Rebalancing&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Qingbin Liu, Xiaoyan Yu, Shizhu He, Kang Liu and Jun Zhao&lt;/em&gt; &lt;br&gt;&lt;code&gt;We propose the lifelong intent detection task to handle continually emerging user intents. And, we propose multistrategy rebalancing to address multiple adverse effects caused by the data imbalance problem.&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2020.emnlp-main.634&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2020.emnlp-main.634&#34;&gt;&lt;strong&gt;Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Sanyuan Chen, Yutai Hou, Yiming Cui, Wanxiang Che, Ting Liu and Xiangzhan Yu&lt;/em&gt; &lt;br&gt;&lt;code&gt;We propose a recall and learn mechanism, which adopts the idea of multi-task learning and jointly learns pretraining tasks and downstream tasks. Specifically, we introduce a Pretraining Simulation mechanism to recall the knowledge from pretraining tasks without data, and an Objective Shifting mechanism to focus the learning on downstream tasks gradually.&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2020.findings-emnlp.41&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP_Findings-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2020.findings-emnlp.41&#34;&gt;&lt;strong&gt;Exploring Versatile Generative Language Model Via Parameter-Efficient Transfer Learning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Zhaojiang Lin, Andrea Madotto and Pascale Fung&lt;/em&gt; &lt;br&gt;&lt;code&gt;Proposing an adapter-based method for continual learning in text generation. One of the insights is a frozen PLM can be well-applied in continual learning.&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/2020.emnlp-main.394&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.emnlp-main.394&#34;&gt;&lt;strong&gt;An Empirical Investigation Towards Efficient Multi-Domain Language Model Pre-training&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Arumae, Kristjan , Sun, Qing and Bhatia, Parminder&lt;/em&gt; &lt;br&gt;&lt;code&gt;We find that elastic weight consolidation provides best overall scores yielding only a 0.33% drop in performance across seven generic tasks while remaining competitive in bio-medical tasks.&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/2020.emnlp-main.158&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.emnlp-main.158&#34;&gt;&lt;strong&gt;Visually Grounded Continual Learning of Compositional Phrases&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Jin, Xisen , Du, Junyi , Sadhu, Arka , Nevatia, Ram and Ren, Xiang&lt;/em&gt; &lt;br&gt;&lt;code&gt;A novel continual learning setting and a new benchmark for continual caption generation, evaluated with exiting rehearsal-based methods&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/2020.emnlp-main.52&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.emnlp-main.52&#34;&gt;&lt;strong&gt;Incremental Event Detection via Knowledge Consolidation Networks&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Cao, Pengfei , Chen, Yubo , Zhao, Jun and Wang, Taifeng&lt;/em&gt; &lt;br&gt;&lt;code&gt;Proposing a hybrid continual learning method for event detection, combining experience replay and Knowledge Distillation, focusing on (1) semantic ambiguity in NLP and (2) data imbalance between memory and current task.&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/2020.emnlp-main.565&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.emnlp-main.565&#34;&gt;&lt;strong&gt;A Multi-Task Incremental Learning Framework with Category Name Embedding for Aspect-Category Sentiment Analysis&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Dai, Zehui , Peng, Cheng , Chen, Huajie and Ding, Yadong&lt;/em&gt; &lt;br&gt;&lt;code&gt;Utilizing BERT for sentence and category encoding, preserving category encoding to prevent catastrophic forgetting.&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/2020.emnlp-main.39&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.emnlp-main.39&#34;&gt;&lt;strong&gt;Efficient Meta Lifelong-Learning with Limited Memory&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Wang, Zirui , Mehta, Sanket Vaibhav , Poczos, Barnabas and Carbonell, Jaime&lt;/em&gt; &lt;br&gt;&lt;code&gt;A meta learning-enhanced version of MbPA (NeurIPS19), sharing the continual setting as well. Figure 1 is interesting.&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/2020.emnlp-main.233&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.emnlp-main.233&#34;&gt;&lt;strong&gt;Lifelong Language Knowledge Distillation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Chuang, Yung-Sung , Su, Shang-Yu and Chen, Yun-Nung&lt;/em&gt; &lt;br&gt;&lt;code&gt;Proposing a Knowledge Distillation-enhanced Method LLL based on LAMOL (ICLR 2020) model for continual learning, evaluated on text generation and text classification.&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/2020.coling-main.318&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/COLING-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.coling-main.318&#34;&gt;&lt;strong&gt;Distill and Replay for Continual Language Learning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Sun, Jingyuan , Wang, Shaonan , Zhang, Jiajun and Zong, Chengqing&lt;/em&gt; &lt;br&gt;&lt;code&gt;Proposing a distill and replay method (DnR) which follows the setting of LAMOL. As a distillation-based method, DnR also shows the ability in incrementally compressing the model size while still outperforming most of the baselines.&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/view/6428&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/AAAI-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/view/6428&#34;&gt;&lt;strong&gt;ERNIE 2.0: A Continual Pre-Training Framework for Language Understanding&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Sun, Yu, Wang, Shuohuan, Li, Yukun, Feng, Shikun, Tian, Hao, Wu, Hua and Wang, Haifeng&lt;/em&gt; &lt;br&gt;&lt;code&gt;In order to extract the lexical, syntactic and semantic information from training corpora, we propose a continual pre-training framework named ERNIE 2.0 which incrementally builds pre-training tasks and then learn pre-trained models on these constructed tasks via continual multi-task learning.&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper/2019/hash/f8d2e80c1458ea2501f98a2cafadb397-Abstract.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NeurIPS-2019-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://proceedings.neurips.cc/paper/2019/hash/f8d2e80c1458ea2501f98a2cafadb397-Abstract.html&#34;&gt;&lt;strong&gt;Episodic Memory in Lifelong Language Learning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Cyprien de Masson d&#39;Autume, Sebastian Ruder, Lingpeng Kong and Dani Yogatama&lt;/em&gt; &lt;br&gt;&lt;code&gt;MbPA++. This paper proposes the use of memory (a fixed memory network) in life-long learning to prevent catastrophic forgetting by means of experience replay and local adaptation. &lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Prompt Engineering&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.12868&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.12868&#34;&gt;&lt;strong&gt;On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on Codex&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Terry Yue Zhuo, Zhuang Li, Yujin Huang, Yuan-Fang Li, Weiqing Wang, Gholamreza Haffari and Fatemeh Shiri&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.11382&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.11382&#34;&gt;&lt;strong&gt;A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1109/CVPR52688.2022.00024&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CVPR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1109/CVPR52688.2022.00024&#34;&gt;&lt;strong&gt;Learning to Prompt for Continual Learning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2022.naacl-main.167&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NAACL-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2022.naacl-main.167&#34;&gt;&lt;strong&gt;Do Prompt-Based Models Really Understand the Meaning of Their Prompts?&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Albert Webson and Ellie Pavlick&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2211.01910&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2211.01910&#34;&gt;&lt;strong&gt;Large Language Models Are Human-Level Prompt Engineers&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan and Jimmy Ba&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2022.acl-long.60&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2022.acl-long.60&#34;&gt;&lt;strong&gt;An Information-theoretic Approach to Prompt Engineering Without Ground Truth Labels&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Taylor Sorensen, Joshua Robinson, Christopher Michael Rytting, Alexander Glenn Shaw, Kyle Jeffrey Rogers, Alexia Pauline Delorey, Mahmoud Khalil, Nancy Fulda et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.04037&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.04037&#34;&gt;&lt;strong&gt;Demystifying Prompts in Language Models via Perplexity Estimation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Hila Gonen, Srini Iyer, Terra Blevins, Noah A. Smith and Luke Zettlemoyer&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2022.findings-acl.222&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL_Findings-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2022.findings-acl.222&#34;&gt;&lt;strong&gt;Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Robert L. Logan IV, Ivana Balazevic, Eric Wallace, Fabio Petroni, Sameer Singh and Sebastian Riedel&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2022.acl-long.174&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2022.acl-long.174&#34;&gt;&lt;strong&gt;Adversarial Soft Prompt Tuning for Cross-Domain Sentiment Analysis&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Hui Wu and Xiaodong Shi&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2022.acl-long.471&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2022.acl-long.471&#34;&gt;&lt;strong&gt;Fine-Grained Controllable Text Generation Using Non-Residual Prompting&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Fredrik Carlsson, Joey &#34;Ohman, Fangyu Liu, Severine Verlinden, Joakim Nivre and Magnus Sahlgren&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2022.acl-long.424&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2022.acl-long.424&#34;&gt;&lt;strong&gt;MSP: Multi-Stage Prompting for Making Pre-trained Language Models Better Translators&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Zhixing Tan, Xiangwen Zhang, Shuo Wang and Yang Liu&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2022.acl-long.365&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2022.acl-long.365&#34;&gt;&lt;strong&gt;Noisy Channel Language Model Prompting for Few-Shot Text Classification&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Sewon Min, Mike Lewis, Hannaneh Hajishirzi and Luke Zettlemoyer&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2022.acl-long.346&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2022.acl-long.346&#34;&gt;&lt;strong&gt;SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Tu Vu, Brian Lester, Noah Constant, Rami Al-Rfou&#39; and Daniel Cer&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2203.06904&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2203.06904&#34;&gt;&lt;strong&gt;Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://proceedings.mlr.press/v188/bansal22a.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/AutoML-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://proceedings.mlr.press/v188/bansal22a.html&#34;&gt;&lt;strong&gt;Meta-Adapters: Parameter Efficient Few-shot Fine-tuning through Meta-Learning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Trapit Bansal, Salaheddin Alzubi, Tong Wang, Jay-Yoon Lee and Andrew McCallum&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=oOte_397Q4P&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NeurIPS-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://openreview.net/forum?id=oOte_397Q4P&#34;&gt;&lt;strong&gt;Sparse Structure Search for Delta Tuning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Shengding Hu, Zhen Zhang, Ning Ding, Yadao Wang, Yasheng Wang, Zhiyuan Liu and Maosong Sun&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1145/3485447.3511921&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/WWW-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1145/3485447.3511921&#34;&gt;&lt;strong&gt;Ontology-enhanced Prompt-tuning for Few-shot Learning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Hongbin Ye, Ningyu Zhang, Shumin Deng, Xiang Chen, Hui Chen, Feiyu Xiong, Xi Chen and Huajun Chen&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.06950&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.06950&#34;&gt;&lt;strong&gt;Pre-trained Language Models can be Fully Zero-Shot Learners&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Xuandong Zhao, Siqi Ouyang, Zhiguo Yu, Ming Wu and Lei Li&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2205.10625&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2205.10625&#34;&gt;&lt;strong&gt;Least-to-Most Prompting Enables Complex Reasoning in Large Language Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Denny Zhou, Nathanael Sch&#34;arli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet et al.&lt;/em&gt; &lt;br&gt;&lt;code&gt;(1) 两阶段的prompt，第一阶段问题分解（通过in-context learning实现，context中包含了其他问题的分解示例），对于每个问题，分解出回答该问题需要先回答什么子问题；&lt;/code&gt;&lt;br&gt;&lt;code&gt;(2) 在第二阶段中，从后往前依次解决子问题，同样通过in-context learing得到，每次LLM的回答会参与组成下一个问题的prompt。&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://par.nsf.gov/biblio/10380030&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NeurIPS-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://par.nsf.gov/biblio/10380030&#34;&gt;&lt;strong&gt;The unreliability of explanations in few-shot prompting for textual reasoning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Ye, Xi and Durrett, Greg&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.02441&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.02441&#34;&gt;&lt;strong&gt;Ask Me Anything: A simple strategy for prompting language models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Simran Arora, Avanika Narayan, Mayee F. Chen, Laurel J. Orr, Neel Guha, Kush Bhatia, Ines Chami, Frederic Sala et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2022.acl-long.398&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2022.acl-long.398&#34;&gt;&lt;strong&gt;Can Prompt Probe Pretrained Language Models? Understanding the Invisible Risks from a Causal View&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Boxi Cao, Hongyu Lin, Xianpei Han, Fangchao Liu and Le Sun&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2022.findings-acl.50&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL_Findings-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2022.findings-acl.50&#34;&gt;&lt;strong&gt;Reframing Instructional Prompts to GPTk&#39;s Language&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Daniel Khashabi, Chitta Baral, Yejin Choi and Hannaneh Hajishirzi&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.10539&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.10539&#34;&gt;&lt;strong&gt;Toward Human Readable Prompt Tuning: Kubrick&#39;s The Shining is a good movie, and a good prompt too?&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Weijia Shi, Xiaochuang Han, Hila Gonen, Ari Holtzman, Yulia Tsvetkov and Luke Zettlemoyer&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2022.findings-emnlp.37&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP_Findings-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2022.findings-emnlp.37&#34;&gt;&lt;strong&gt;Towards Unified Prompt Tuning for Few-shot Text Classification&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Jianing Wang, Chengyu Wang, Fuli Luo, Chuanqi Tan, Minghui Qiu, Fei Yang, Qiuhui Shi, Songfang Huang et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.12587&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.12587&#34;&gt;&lt;strong&gt;Model ensemble instead of prompt fusion: a sample-specific knowledge transfer method for few-shot prompt tuning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Xiangyu Peng, Chen Xing, Prafulla Kumar Choubey, Chien-Sheng Wu and Caiming Xiong&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.emnlp-main.491&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.emnlp-main.491&#34;&gt;&lt;strong&gt;FewshotQA: A simple framework for few-shot learning of question answering tasks using pre-trained text-to-text models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Rakesh Chada and Pradeep Natarajan&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.emnlp-main.243&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.emnlp-main.243&#34;&gt;&lt;strong&gt;The Power of Scale for Parameter-Efficient Prompt Tuning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Brian Lester, Rami Al-Rfou and Noah Constant&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.acl-long.353&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.acl-long.353&#34;&gt;&lt;strong&gt;Prefix-Tuning: Optimizing Continuous Prompts for Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Xiang Lisa Li and Percy Liang&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1145/3411763.3451760&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CHI-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1145/3411763.3451760&#34;&gt;&lt;strong&gt;Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Laria Reynolds and Kyle McDonell&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Natural Language Understanding&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cdn.openai.com/papers/gpt-4.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/OpenAI-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://cdn.openai.com/papers/gpt-4.pdf&#34;&gt;&lt;strong&gt;GPT-4 Technical Report&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://cdn.openai.com/papers/gpt-4.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GPT--4-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;br&gt; by &lt;em&gt;OpenAI&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cdn.openai.com/papers/gpt-4-system-card.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/OpenAI-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://cdn.openai.com/papers/gpt-4-system-card.pdf&#34;&gt;&lt;strong&gt;GPT-4 System Card&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://cdn.openai.com/papers/gpt-4.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GPT--4-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;br&gt; by &lt;em&gt;OpenAI&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.207&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.207&#34;&gt;&lt;strong&gt;Knowledge Prompting in Pre-trained Language Model for Natural Language Understanding&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Jianing Wang, Wenkang Huang, Minghui Qiu, Qiuhui Shi, Hongbin Wang, Xiang Li and Ming Gao&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2022.findings-emnlp.468&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP_Findings-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2022.findings-emnlp.468&#34;&gt;&lt;strong&gt;VarMAE: Pre-training of Variational Masked Autoencoder for Domain-adaptive Language Understanding&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Dou Hu, Xiaolong Hou, Xiyang Du, Mengyuan Zhou, Lianxin Jiang, Yang Mo and Xiaofeng Shi&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2202.04538&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2202.04538&#34;&gt;&lt;strong&gt;Generating Training Data with Language Models: Towards Zero-Shot Language Understanding&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Yu Meng, Jiaxin Huang, Yu Zhang and Jiawei Han&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.acl-long.308&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.acl-long.308&#34;&gt;&lt;strong&gt;VECO: Variable and Flexible Cross-lingual Pre-training for Language Understanding and Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Fuli Luo, Wei Wang, Jiahao Liu, Yijia Liu, Bin Bi, Songfang Huang, Fei Huang and Luo Si&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper/2019/hash/c20bb2d9a50d5ac1f713f8b34d9aac5a-Abstract.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NeurIPS-2019-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://proceedings.neurips.cc/paper/2019/hash/c20bb2d9a50d5ac1f713f8b34d9aac5a-Abstract.html&#34;&gt;&lt;strong&gt;Unified Language Model Pre-training for Natural Language Understanding and Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/OpenAI-2018-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf&#34;&gt;&lt;strong&gt;Improving language understanding by generative pre-training&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GPT--1-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;br&gt; by &lt;em&gt;Radford, Alec, Narasimhan, Karthik, Salimans, Tim, Sutskever, Ilya and others&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Multimodal&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.05442&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.05442&#34;&gt;&lt;strong&gt;Scaling Vision Transformers to 22 Billion Parameters&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Steiner, Mathilde Caron et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.03378&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2303.03378&#34;&gt;&lt;strong&gt;PaLM-E: An Embodied Multimodal Language Model&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2303.03378&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PaLM--E-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;br&gt; by &lt;em&gt;Driess, Danny, Xia, Fei, Sajjadi, Mehdi SM, Lynch, Corey, Chowdhery, Aakanksha, Ichter, Brian, Wahid, Ayzaan, Tompson, Jonathan et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.07094&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.07094&#34;&gt;&lt;strong&gt;Learning Customized Visual Models with Retrieval-Augmented Knowledge&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Haotian Liu, Kilho Son, Jianwei Yang, Ce Liu, Jianfeng Gao, Yong Jae Lee and Chunyuan Li&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.04671&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2303.04671&#34;&gt;&lt;strong&gt;Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Wu, Chenfei, Yin, Shengming, Qi, Weizhen, Wang, Xiaodong, Tang, Zecheng and Duan, Nan&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.12192&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.12192&#34;&gt;&lt;strong&gt;Aligning Text-to-Image Models using Human Feedback&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1109/CVPR52688.2022.01593&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CVPR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1109/CVPR52688.2022.01593&#34;&gt;&lt;strong&gt;CLIP-Event: Connecting Text and Images with Event Structures&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Manling Li, Ruochen Xu, Shuohang Wang, Luowei Zhou, Xudong Lin, Chenguang Zhu, Michael Zeng, Heng Ji et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2022.coling-1.491&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/COLING-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2022.coling-1.491&#34;&gt;&lt;strong&gt;Are Visual-Linguistic Models Commonsense Knowledge Bases?&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Hsiu-Yu Yang and Carina Silberer&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2211.12561&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2211.12561&#34;&gt;&lt;strong&gt;Retrieval-Augmented Multimodal Language Modeling&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.08901&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.08901&#34;&gt;&lt;strong&gt;Contrastive Language-Image Pre-Training with Knowledge Graphs&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Xuran Pan, Tianzhu Ye, Dongchen Han, Shiji Song and Gao Huang&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2108.04556&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2108.04556&#34;&gt;&lt;strong&gt;CLSEBERT: Contrastive Learning for Syntax Enhanced Code Pre-Trained Model&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Xin Wang, Yasheng Wang, Pingyi Zhou, Fei Mi, Meng Xiao, Yadao Wang, Li Li, Xiao Liu et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2021/html/Lei_Less_Is_More_ClipBERT_for_Video-and-Language_Learning_via_Sparse_Sampling_CVPR_2021_paper.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CVPR-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2021/html/Lei_Less_Is_More_ClipBERT_for_Video-and-Language_Learning_via_Sparse_Sampling_CVPR_2021_paper.html&#34;&gt;&lt;strong&gt;Less Is More: ClipBERT for Video-and-Language Learning via Sparse Sampling&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L. Berg, Mohit Bansal and Jingjing Liu&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2102.10772&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2102.10772&#34;&gt;&lt;strong&gt;Transformer is All You Need: Multimodal Multitask Learning with a Unified Transformer&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Ronghang Hu and Amanpreet Singh&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1145/3474085.3475709&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/MM-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1145/3474085.3475709&#34;&gt;&lt;strong&gt;Pre-training Graph Transformer with Multimodal Side Information for Recommendation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Yong Liu, Susen Yang, Chenyi Lei, Guoxin Wang, Haihong Tang, Juyong Zhang, Aixin Sun and Chunyan Miao&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2002.06353&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2002.06353&#34;&gt;&lt;strong&gt;UniViLM: A Unified Video and Language Pre-Training Model for Multimodal Understanding and Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan Duan, Tianrui Li, Xilin Chen and Ming Zhou&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper/2020/hash/49562478de4c54fafd4ec46fdb297de5-Abstract.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NeurIPS-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://proceedings.neurips.cc/paper/2020/hash/49562478de4c54fafd4ec46fdb297de5-Abstract.html&#34;&gt;&lt;strong&gt;Large-Scale Adversarial Training for Vision-and-Language Representation Learning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng and Jingjing Liu&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2020.emnlp-main.162&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2020.emnlp-main.162&#34;&gt;&lt;strong&gt;Vokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervision&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Hao Tan and Mohit Bansal&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2020.acl-main.214&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2020.acl-main.214&#34;&gt;&lt;strong&gt;Integrating Multimodal Information in Large Pretrained Transformers&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Wasifur Rahman, Md. Kamrul Hasan, Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, Louis-Philippe Morency and Mohammed E. Hoque&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=SygXPaEYvH&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ICLR-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://openreview.net/forum?id=SygXPaEYvH&#34;&gt;&lt;strong&gt;VL-BERT: Pre-training of Generic Visual-Linguistic Representations&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei and Jifeng Dai&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://arxiv.org/abs/1908.03557&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2019-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://arxiv.org/abs/1908.03557&#34;&gt;&lt;strong&gt;VisualBERT: A Simple and Performant Baseline for Vision and Language&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh and Kai-Wei Chang&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper/2019/hash/c74d97b01eae257e44aa9d5bade97baf-Abstract.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NeurIPS-2019-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://proceedings.neurips.cc/paper/2019/hash/c74d97b01eae257e44aa9d5bade97baf-Abstract.html&#34;&gt;&lt;strong&gt;ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Jiasen Lu, Dhruv Batra, Devi Parikh and Stefan Lee&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1109/ICCV.2019.00756&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ICCV-2019-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1109/ICCV.2019.00756&#34;&gt;&lt;strong&gt;VideoBERT: A Joint Model for Video and Language Representation Learning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy and Cordelia Schmid&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Multilingual&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.132&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.132&#34;&gt;&lt;strong&gt;GeoMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Da Yin, Hritik Bansal, Masoud Monajatipoor, Liunian Harold Li and Kai-Wei Chang&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Reliability&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.12095&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.12095&#34;&gt;&lt;strong&gt;On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Jindong Wang, Xixu Hu, Wenxin Hou, Hao Chen, Runkai Zheng, Yidong Wang, Linyi Yang, Haojun Huang et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.09150&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.09150&#34;&gt;&lt;strong&gt;Prompting GPT-3 To Be Reliable&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan L. Boyd-Graber and Lijuan Wang&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2207.07411&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2207.07411&#34;&gt;&lt;strong&gt;Plex: Towards Reliability using Pretrained Large Model Extensions&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Dustin Tran, Jeremiah Z. Liu, Michael W. Dusenberry, Du Phan, Mark Collier, Jie Ren, Kehang Han, Zi Wang et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper/2021/hash/8420d359404024567b5aefda1231af24-Abstract.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NeurIPS-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://proceedings.neurips.cc/paper/2021/hash/8420d359404024567b5aefda1231af24-Abstract.html&#34;&gt;&lt;strong&gt;Revisiting the Calibration of Modern Neural Networks&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Matthias Minderer, Josip Djolonga, Rob Romijnders, Frances Hubis, Xiaohua Zhai, Neil Houlsby, Dustin Tran and Mario Lucic&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper/2021/hash/f8905bd3df64ace64a68e154ba72f24c-Abstract.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NeurIPS-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://proceedings.neurips.cc/paper/2021/hash/f8905bd3df64ace64a68e154ba72f24c-Abstract.html&#34;&gt;&lt;strong&gt;Soft Calibration Objectives for Neural Networks&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Archit Karandikar, Nicholas Cain, Dustin Tran, Balaji Lakshminarayanan, Jonathon Shlens, Michael C. Mozer and Becca Roelofs&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Robustness&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.07663&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.07663&#34;&gt;&lt;strong&gt;Pretrained Transformers Do not Always Improve Robustness&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Swaroop Mishra, Bhavdeep Singh Sachdeva and Chitta Baral&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2020.acl-main.244&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2020.acl-main.244&#34;&gt;&lt;strong&gt;Pretrained Transformers Improve Out-of-Distribution Robustness&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan and Dawn Song&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Dialogue System&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.02851&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.02851&#34;&gt;&lt;strong&gt;DiSTRICT: Dialogue State Tracking with Retriever Driven In-Context Tuning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Praveen Venkateswaran, Evelyn Duesterwald and Vatche Isahagian&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2022.coling-1.56&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/COLING-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2022.coling-1.56&#34;&gt;&lt;strong&gt;Does GPT-3 Generate Empathetic Dialogues? A Novel In-Context Example Selection Method and Automatic Evaluation Metric for Empathetic Dialogue Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Young-Jun Lee, Chae-Gyun Lim and Ho-Jin Choi&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/view/21416&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/AAAI-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/view/21416&#34;&gt;&lt;strong&gt;Fusing Task-Oriented and Open-Domain Dialogues in Conversational Agents&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Tom Young, Frank Xing, Vlad Pandelea, Jinjie Ni and Erik Cambria&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2206.11309&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2206.11309&#34;&gt;&lt;strong&gt;GODEL: Large-Scale Pre-Training for Goal-Directed Dialog&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Baolin Peng, Michel Galley, Pengcheng He, Chris Brockett, Lars Liden, Elnaz Nouri, Zhou Yu, Bill Dolan et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.09252&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.09252&#34;&gt;&lt;strong&gt;Mind the Knowledge Gap: A Survey of Knowledge-enhanced Dialogue Systems&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Sagi Shaier, Lawrence Hunter and Katharina Kann&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.emnlp-main.404&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.emnlp-main.404&#34;&gt;&lt;strong&gt;Dialogue State Tracking with a Language Model using Schema-Driven Prompting&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Chia-Hsuan Lee, Hao Cheng and Mari Ostendorf&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2110.08118&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2110.08118&#34;&gt;&lt;strong&gt;Few-Shot Bot: Prompt-Based Learning for Dialogue Systems&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Andrea Madotto, Zhaojiang Lin, Genta Indra Winata and Pascale Fung&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.naacl-main.239&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NAACL-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.naacl-main.239&#34;&gt;&lt;strong&gt;Action-Based Conversations Dataset: A Corpus for Building More In-Depth Task-Oriented Dialogue Systems&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Derek Chen, Howard Chen, Yi Yang, Alexander Lin and Zhou Yu&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.naacl-main.122&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NAACL-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.naacl-main.122&#34;&gt;&lt;strong&gt;Fine-grained Post-training for Improving Retrieval-based Dialogue Systems&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Janghoon Han, Taesuk Hong, Byoungjae Kim, Youngjoong Ko and Jungyun Seo&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2105.04387&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2105.04387&#34;&gt;&lt;strong&gt;Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Survey&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Jinjie Ni, Tom Young, Vlad Pandelea, Fuzhao Xue, Vinay Adiga and Erik Cambria&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1145/3442381.3449939&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/WWW-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1145/3442381.3449939&#34;&gt;&lt;strong&gt;Slot Self-Attentive Dialogue State Tracking&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Fanghua Ye, Jarana Manotumruksa, Qiang Zhang, Shenghui Li and Emine Yilmaz&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1162/tacl_a_00390&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/TACL-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1162/tacl_a_00390&#34;&gt;&lt;strong&gt;Pretraining the Noisy Channel Model for Task-Oriented Dialogue&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Qi Liu, Lei Yu, Laura Rimell and Phil Blunsom&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/view/17674&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/AAAI-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/view/17674&#34;&gt;&lt;strong&gt;UBAR: Towards Fully End-to-End Task-Oriented Dialog System with GPT-2&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Yunyi Yang, Yunhao Li and Xiaojun Quan&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2020.acl-main.54&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2020.acl-main.54&#34;&gt;&lt;strong&gt;End-to-End Neural Pipeline for Goal-Oriented Dialogue Systems using GPT-2&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;DongHoon Ham, Jeong-Gwan Lee, Youngsoo Jang and Kee-Eung Kim&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper/2020/hash/e946209592563be0f01c844ab2170f0c-Abstract.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NeurIPS-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://proceedings.neurips.cc/paper/2020/hash/e946209592563be0f01c844ab2170f0c-Abstract.html&#34;&gt;&lt;strong&gt;A Simple Language Model for Task-Oriented Dialogue&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Ehsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu, Semih Yavuz and Richard Socher&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Recommender System&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1109/TKDE.2020.3028705&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/TKDE-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1109/TKDE.2020.3028705&#34;&gt;&lt;strong&gt;A Survey on Knowledge Graph-Based Recommender Systems&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Qingyu Guo, Fuzhen Zhuang, Chuan Qin, Hengshu Zhu, Xing Xie, Hui Xiong and Qing He&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1145/3477495.3531937&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/SIGIR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1145/3477495.3531937&#34;&gt;&lt;strong&gt;Are Graph Augmentations Necessary?: Simple Graph Contrastive Learning for Recommendation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Junliang Yu, Hongzhi Yin, Xin Xia, Tong Chen, Lizhen Cui and Quoc Viet Hung Nguyen&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3572835&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/TOIS-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3572835&#34;&gt;&lt;strong&gt;Disentangled Representations Learning for Multi-Target Cross-Domain Recommendation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Guo, Xiaobo, Li, Shaoshuai, Guo, Naicheng, Cao, Jiangxia, Liu, Xiaolei, Ma, Qiongxu, Gan, Runsheng and Zhao, Yunan&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1145/3477495.3531714&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/SIGIR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1145/3477495.3531714&#34;&gt;&lt;strong&gt;Rethinking Reinforcement Learning for Recommendation: A Prompt Perspective&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Xin Xin, Tiago Pimentel, Alexandros Karatzoglou, Pengjie Ren, Konstantina Christakopoulou and Zhaochun Ren&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2101.09459&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2101.09459&#34;&gt;&lt;strong&gt;Advances and Challenges in Conversational Recommender Systems: A Survey&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Chongming Gao, Wenqiang Lei, Xiangnan He, Maarten de Rijke and Tat-Seng Chua&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1145/3474085.3475709&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/MM-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1145/3474085.3475709&#34;&gt;&lt;strong&gt;Pre-training Graph Transformer with Multimodal Side Information for Recommendation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Yong Liu, Susen Yang, Chenyi Lei, Guoxin Wang, Haihong Tang, Juyong Zhang, Aixin Sun and Chunyan Miao&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/view/5465&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/AAAI-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/view/5465&#34;&gt;&lt;strong&gt;Towards Hands-Free Visual Dialog Interactive Recommendation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Tong Yu, Yilin Shen and Hongxia Jin&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Event Extraction&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2022.starsem-1.11&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NAACL-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2022.starsem-1.11&#34;&gt;&lt;strong&gt;Word-Label Alignment for Event Detection: A New Perspective via Optimal Transport&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Amir Pouran Ben Veyseh and Thien Huu Nguyen&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.634&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.634&#34;&gt;&lt;strong&gt;Learning Cross-Task Dependencies for Joint Extraction of Entities, Events, Event Arguments, and Relations&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Minh Van Nguyen, Bonan Min, Franck Dernoncourt and Thien Nguyen&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1109/CVPR52688.2022.01593&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CVPR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1109/CVPR52688.2022.01593&#34;&gt;&lt;strong&gt;CLIP-Event: Connecting Text and Images with Event Structures&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Manling Li, Ruochen Xu, Shuohang Wang, Luowei Zhou, Xudong Lin, Chenguang Zhu, Michael Zeng, Heng Ji et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1007/978-3-030-86523-8_39&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ECML-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1007/978-3-030-86523-8_39&#34;&gt;&lt;strong&gt;Augmenting Open-Domain Event Detection with Synthetic Data from GPT-2&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Amir Pouran Ben Veyseh, Minh Van Nguyen, Bonan Min and Thien Huu Nguyen&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2020.emnlp-main.691&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2020.emnlp-main.691&#34;&gt;&lt;strong&gt;SeqMix: Augmenting Active Sequence Labeling via Sequence Mixup&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Rongzhi Zhang, Yue Yu and Chao Zhang&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/p19-1522&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2019-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/p19-1522&#34;&gt;&lt;strong&gt;Exploring Pre-trained Language Models for Event Extraction and Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Sen Yang, Dawei Feng, Linbo Qiao, Zhigang Kan and Dongsheng Li&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Event Relation Extraction&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.634&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.634&#34;&gt;&lt;strong&gt;Learning Cross-Task Dependencies for Joint Extraction of Entities, Events, Event Arguments, and Relations&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Minh Van Nguyen, Bonan Min, Franck Dernoncourt and Thien Nguyen&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/view/21354&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/AAAI-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/view/21354&#34;&gt;&lt;strong&gt;Selecting Optimal Context Sentences for Event-Event Relation Extraction&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Hieu Man, Nghia Trung Ngo, Linh Ngo Van and Thien Huu Nguyen&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2022.findings-emnlp.407&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP_Findings-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2022.findings-emnlp.407&#34;&gt;&lt;strong&gt;Multilingual SubEvent Relation Extraction: A Novel Dataset and Structure Induction Method&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Viet Dac Lai, Hieu Man, Linh Ngo Van, Franck Dernoncourt and Thien Nguyen&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2022.coling-1.200&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/COLING-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2022.coling-1.200&#34;&gt;&lt;strong&gt;Event Causality Identification via Derivative Prompt Joint Learning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Shirong Shen, Heng Zhou, Tongtong Wu and Guilin Qi&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.emnlp-main.107&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.emnlp-main.107&#34;&gt;&lt;strong&gt;Salience-Aware Event Chain Modeling for Narrative Understanding&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Xiyang Zhang, Muhao Chen and Jonathan May&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2020.emnlp-main.51&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2020.emnlp-main.51&#34;&gt;&lt;strong&gt;Joint Constrained Learning for Event-Event Relation Extraction&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Haoyu Wang, Muhao Chen, Hongming Zhang and Dan Roth&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Data Argumentation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.13007&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.13007&#34;&gt;&lt;strong&gt;ChatAug: Leveraging ChatGPT for Text Data Augmentation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Haixing Dai, Zhengliang Liu, Wenxiong Liao, Xiaoke Huang, Zihao Wu, Lin Zhao, Wei Liu, Ninghao Liu et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=g11CZSghXyY&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ICLR-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://openreview.net/forum?id=g11CZSghXyY&#34;&gt;&lt;strong&gt;Combining Ensembles and Data Augmentation Can Harm Your Calibration&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Yeming Wen, Ghassen Jerfel, Rafael Muller, Michael W. Dusenberry, Jasper Snoek, Balaji Lakshminarayanan and Dustin Tran&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.findings-emnlp.192&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP_Findings-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.findings-emnlp.192&#34;&gt;&lt;strong&gt;GPT3Mix: Leveraging Large-scale Language Models for Text Augmentation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Kang Min Yoo, Dongju Park, Jaewook Kang, Sang-Woo Lee and Woo-Myoung Park&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2020.emnlp-main.691&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2020.emnlp-main.691&#34;&gt;&lt;strong&gt;SeqMix: Augmenting Active Sequence Labeling via Sequence Mixup&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Rongzhi Zhang, Yue Yu and Chao Zhang&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Data Annotation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.10450&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.10450&#34;&gt;&lt;strong&gt;Is GPT-3 a Good Data Annotator?&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Bosheng Ding, Chengwei Qin, Linlin Liu, Lidong Bing, Shafiq R. Joty and Boyang Li&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.findings-emnlp.354&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP_Findings-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.findings-emnlp.354&#34;&gt;&lt;strong&gt;Want To Reduce Labeling Cost? GPT-3 Can Help&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Shuohang Wang, Yang Liu, Yichong Xu, Chenguang Zhu and Michael Zeng&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Information Extraction&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.10205&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.10205&#34;&gt;&lt;strong&gt;Zero-Shot Information Extraction via Chatting with ChatGPT&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Xiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang, Xin Zhang, Shen Huang, Pengjun Xie, Jinan Xu et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.130&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.130&#34;&gt;&lt;strong&gt;Large language models are few-shot clinical information extractors&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Monica Agrawal, Stefan Hegselmann, Hunter Lang, Yoon Kim and David A. Sontag&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2022.findings-emnlp.329&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP_Findings-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2022.findings-emnlp.329&#34;&gt;&lt;strong&gt;Thinking about GPT-3 In-Context Learning for Biomedical IE? Think Again&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Bernal Jimenez Gutierrez, Nikolas McNeal, Clayton Washington, You Chen, Lang Li, Huan Sun and Yu Su&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Domain Adaptive&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2022.coling-1.85&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/COLING-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2022.coling-1.85&#34;&gt;&lt;strong&gt;A Domain Knowledge Enhanced Pre-Trained Language Model for Vertical Search: Case Study on Medicinal Products&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Kesong Liu, Jianhui Jiang and Feifei Lyu&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2022.findings-emnlp.163&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP_Findings-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2022.findings-emnlp.163&#34;&gt;&lt;strong&gt;Snapshot-Guided Domain Adaptation for ELECTRA&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Daixuan Cheng, Shaohan Huang, Jianfeng Liu, Yuefeng Zhan, Hao Sun, Furu Wei, Denvy Deng and Qi Zhang&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2022.findings-emnlp.468&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP_Findings-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2022.findings-emnlp.468&#34;&gt;&lt;strong&gt;VarMAE: Pre-training of Variational Masked Autoencoder for Domain-adaptive Language Understanding&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Dou Hu, Xiaolong Hou, Xiyang Du, Mengyuan Zhou, Lianxin Jiang, Yang Mo and Xiaofeng Shi&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Question Answering&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.06466&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.06466&#34;&gt;&lt;strong&gt;ChatGPT versus Traditional Question Answering for Knowledge Graphs: Current Status and Future Directions Towards Knowledge Graph Chatbots&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Reham Omar, Omij Mangukiya, Panos Kalnis and Essam Mansour&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2303.07992&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2303.07992&#34;&gt;&lt;strong&gt;Evaluation of ChatGPT as a Question Answering System for Answering Complex Questions&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Yiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan Hu, Yongrui Chen and Guilin Qi&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1145/3534678.3539472&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/KDD-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1145/3534678.3539472&#34;&gt;&lt;strong&gt;Mask and Reason: Pre-Training Knowledge Graph Transformers for Complex Logical Queries&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Xiao Liu, Shiyu Zhao, Kai Su, Yukuo Cen, Jiezhong Qiu, Mengdi Zhang, Wei Wu, Yuxiao Dong et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2022.acl-long.201&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2022.acl-long.201&#34;&gt;&lt;strong&gt;Sequence-to-Sequence Knowledge Graph Completion and Question Answering&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Apoorv Saxena, Adrian Kochsiek and Rainer Gemulla&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2207.13332&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2207.13332&#34;&gt;&lt;strong&gt;RealTime QA: What&#39;s the Answer Right Now?&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Jungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi, Ronan Le Bras, Akari Asai, Xinyan Yu, Dragomir R. Radev, Noah A. Smith et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Application&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://journals.plos.org/digitalhealth/article?id=10.1371/journal.pdig.0000198&amp;amp;trk=public_post_comment-text&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PLOS_Digital_Health-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://journals.plos.org/digitalhealth/article?id=10.1371/journal.pdig.0000198&amp;amp;trk=public_post_comment-text&#34;&gt;&lt;strong&gt;Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Kung, Tiffany H, Cheatham, Morgan, Medenilla, Arielle, Sillos, Czarina, De Leon, Lorie, Elepa~no, Camille, Madriaga, Maria, Aggabao, Rimel et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://journals.plos.org/digitalhealth/article?id=10.1371/journal.pdig.0000205&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PLOS_Digital_Health-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://journals.plos.org/digitalhealth/article?id=10.1371/journal.pdig.0000205&#34;&gt;&lt;strong&gt;ChatGPT passing USMLE shines a spotlight on the flaws of medical education&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Mbakwe, Amarachi B, Lourentzou, Ismini, Celi, Leo Anthony, Mechanic, Oren J and Dagan, Alon&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1007/978-3-031-03789-4_9&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EvoMUSART-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1007/978-3-031-03789-4_9&#34;&gt;&lt;strong&gt;Towards the Generation of Musical Explanations with GPT-3&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Stephen James Krol, Maria Teresa Llano and Jon McCormack&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Meta Learning&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2022.acl-long.53&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2022.acl-long.53&#34;&gt;&lt;strong&gt;Meta-learning via Language Model In-context Tuning&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://aclanthology.org/N19-1423/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/BERT-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2006.03654&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/DeBERTa-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GPT--2-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;br&gt; by &lt;em&gt;Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis and He He&lt;/em&gt; &lt;br&gt;&lt;code&gt;This paper proposes in-context tuning, which recasts task adaptation and prediction as a simple sequence prediction problem: to form the input sequence, concatenate the task instruction, labeled in-context examples, and the target input to predict; to meta train the model to learn from in-context examples, finetune a PLM to predict the target label given the input sequence on a collection of tasks (very similar to MetaICL). On LAMA and BinaryClfs, the proposed method outperforms MAML.&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2022.naacl-main.201&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NAACL-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2022.naacl-main.201&#34;&gt;&lt;strong&gt;MetaICL: Learning to Learn In Context&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/facebookresearch/MetaICL&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code-skyblue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GPT--2-yellow&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;br&gt; by &lt;em&gt;Sewon Min, Mike Lewis, Luke Zettlemoyer and Hannaneh Hajishirzi&lt;/em&gt; &lt;br&gt;&lt;code&gt;MetaICL proposes a supervised meta-training framework to enable LMs to more effectively learn a new task in context. In MetaICL, each meta-training example includes several training examples from one task that will be presented together as a single sequence to the LM, and the prediction of the final example is used to calculate the loss.&lt;/code&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Generalizability&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.03154&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.03154&#34;&gt;&lt;strong&gt;Conversation Regression Testing: A Design Technique for Prototyping Generalizable Prompt Strategies for Pre-trained Language Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;J. D. Zamfirescu-Pereira, Bjoern Hartmann and Qian Yang&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2206.05658&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2206.05658&#34;&gt;&lt;strong&gt;Fine-tuning Pre-trained Language Models with Noise Stability Regularization&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Hang Hua, Xingjian Li, Dejing Dou, Cheng-Zhong Xu and Jiebo Luo&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.findings-acl.322&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL_Findings-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.findings-acl.322&#34;&gt;&lt;strong&gt;Do Language Models Perform Generalizable Commonsense Inference?&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/wangpf3/LM-for-CommonsenseInference&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code-skyblue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;br&gt; by &lt;em&gt;Peifeng Wang, Filip Ilievski, Muhao Chen and Xiang Ren&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Language Model as Knowledge Base&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.11293&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.11293&#34;&gt;&lt;strong&gt;Understanding Finetuning for Factual Knowledge Extraction from Language Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Mehran Kazemi, Sid Mittal and Deepak Ramachandran&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2022.findings-emnlp.147&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP_Findings-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2022.findings-emnlp.147&#34;&gt;&lt;strong&gt;Can Language Models Serve as Temporal Knowledge Bases?&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Ruilin Zhao, Feng Zhao, Guandong Xu, Sixiao Zhang and Hai Jin&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2022.acl-long.388&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACL-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2022.acl-long.388&#34;&gt;&lt;strong&gt;Finding Structural Knowledge in Multimodal-BERT&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Victor Milewski, Miryam de Lhoneux and Marie-Francine Moens&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.eacl-main.153&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EACL-2021-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.eacl-main.153&#34;&gt;&lt;strong&gt;Language Models as Knowledge Bases: On Entity Representations, Storage Capacity, and Paraphrased Queries&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Benjamin Heinzerling and Kentaro Inui&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2020.emnlp-main.346&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2020.emnlp-main.346&#34;&gt;&lt;strong&gt;AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace and Sameer Singh&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/D19-1250&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2019-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/D19-1250&#34;&gt;&lt;strong&gt;Language Models as Knowledge Bases?&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Fabio Petroni, Tim Rockt&#34;aschel, Sebastian Riedel, Patrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu and Alexander H. Miller&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Retrieval-Augmented Language Model&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.00083&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.00083&#34;&gt;&lt;strong&gt;In-Context Retrieval-Augmented Language Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown and Yoav Shoham&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.07094&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.07094&#34;&gt;&lt;strong&gt;Learning Customized Visual Models with Retrieval-Augmented Knowledge&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Haotian Liu, Kilho Son, Jianwei Yang, Ce Liu, Jianfeng Gao, Yong Jae Lee and Chunyuan Li&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.12652&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.12652&#34;&gt;&lt;strong&gt;REPLUG: Retrieval-Augmented Black-Box Language Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer and Wen-tau Yih&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.04858&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.04858&#34;&gt;&lt;strong&gt;Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Zhuolin Yang, Wei Ping, Zihan Liu, Vijay Korthikanti, Weili Nie, De-An Huang, Linxi Fan, Zhiding Yu et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2202.01110&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2202.01110&#34;&gt;&lt;strong&gt;A Survey on Retrieval-Augmented Text Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Huayang Li, Yixuan Su, Deng Cai, Yan Wang and Lemao Liu&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2211.12561&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2211.12561&#34;&gt;&lt;strong&gt;Retrieval-Augmented Multimodal Language Modeling&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2208.03299&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv_preprint_arXiv-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2208.03299&#34;&gt;&lt;strong&gt;Atlas: Few-shot learning with retrieval augmented language models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Izacard, Gautier, Lewis, Patrick, Lomeli, Maria, Hosseini, Lucas, Petroni, Fabio, Schick, Timo, Dwivedi-Yu, Jane, Joulin, Armand et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.382&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.382&#34;&gt;&lt;strong&gt;Training Language Models with Memory Augmentation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Zexuan Zhong, Tao Lei and Danqi Chen&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://proceedings.mlr.press/v162/borgeaud22a.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ICML-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://proceedings.mlr.press/v162/borgeaud22a.html&#34;&gt;&lt;strong&gt;Improving Language Models by Retrieving from Trillions of Tokens&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2002.08909&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2002.08909&#34;&gt;&lt;strong&gt;REALM: Retrieval-Augmented Language Model Pre-Training&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat and Ming-Wei Chang&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NeurIPS-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html&#34;&gt;&lt;strong&gt;Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K&#34;uttler, Mike Lewis et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Quality&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2211.00053&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2211.00053&#34;&gt;&lt;strong&gt;Generating Sequences by Learning to Self-Correct&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi and Yejin Choi&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Interpretability/Explainability&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.09095&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.09095&#34;&gt;&lt;strong&gt;Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Hritik Bansal, Karthik Gopalakrishnan, Saket Dingliwal, Sravan Bodapati, Katrin Kirchhoff and Dan Roth&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.137&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/EMNLP-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.137&#34;&gt;&lt;strong&gt;Are Hard Examples also Harder to Explain? A Study with Human and Model-Generated Explanations&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Swarnadeep Saha, Peter Hase, Nazneen Rajani and Mohit Bansal&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.findings-acl.366&#34;&gt;&amp;lt;img src=https://img.shields.io/badge/Findings_of_the_Association_for_Computational_Linguistics:_{ACL/IJCNLP} 2021,_Online_Event,_August_1--6,_2021-2021-blue alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34; /&amp;gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/2021.findings-acl.366&#34;&gt;&lt;strong&gt;Prompting Contrastive Explanations for Commonsense Reasoning Tasks&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Bhargavi Paranjape, Julian Michael, Marjan Ghazvininejad, Hannaneh Hajishirzi and Luke Zettlemoyer&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Data Generation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=h5OpjGd_lo6&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ICLR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://openreview.net/forum?id=h5OpjGd_lo6&#34;&gt;&lt;strong&gt;Self-Guided Noise-Free Data Generation for Efficient Zero-Shot Learning&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Gao, Jiahui, Pi, Renjie, Yong, LIN, Xu, Hang, Ye, Jiacheng, Wu, Zhiyong, ZHANG, WEIZHONG, Liang, Xiaodan et al.&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.801&#34;&gt;&amp;lt;img src=https://img.shields.io/badge/the_2022_Conference_on_Empirical_Methods_in_Natural Language_Processing,_{EMNLP}_2022,_Abu_Dhabi,_United_Arab_Emirates, December_7--11,_2022-2022-blue alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34; /&amp;gt;&lt;/a&gt; &lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.801&#34;&gt;&lt;strong&gt;ZeroGen: Efficient Zero-shot Learning via Dataset Generation&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Jiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao Feng, Zhiyong Wu, Tao Yu and Lingpeng Kong&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2202.04538&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2202.04538&#34;&gt;&lt;strong&gt;Generating Training Data with Language Models: Towards Zero-Shot Language Understanding&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Yu Meng, Jiaxin Huang, Yu Zhang and Jiawei Han&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Others&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.05578&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CoRR-2023-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.05578&#34;&gt;&lt;strong&gt;Toward General Design Principles for Generative AI Applications&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Justin D. Weisz, Michael J. Muller, Jessica He and Stephanie Houde&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.12990&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/NeurIPS-2022-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.12990&#34;&gt;&lt;strong&gt;Unsupervised Representation Learning from Pre-trained Diffusion Probabilistic Models&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Zijian Zhang, Zhou Zhao and Zhijie Lin&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/view/6446&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/AAAI-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/view/6446&#34;&gt;&lt;strong&gt;Parsing as Pretraining&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;David Vilares, Michalina Strzyz, Anders S\ogaard and Carlos G&#39;omez-Rodr&#39;\iguez&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/view/6757&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/AAAI-2020-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/view/6757&#34;&gt;&lt;strong&gt;Unsupervised Deep Learning via Affinity Diffusion&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Jiabo Huang, Qi Dong, Shaogang Gong and Xiatian Zhu&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/p19-1472&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-2019-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.18653/v1/p19-1472&#34;&gt;&lt;strong&gt;HellaSwag: Can a Machine Really Finish Your Sentence?&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://rowanzellers.com/hellaswag&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code-skyblue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt;&lt;br&gt; by &lt;em&gt;Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi and Yejin Choi&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1109/CVPR.2009.5206594&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CVPR-2009-blue&#34; alt=&#34;img&#34; style=&#34;zoom:100%; vertical-align: middle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1109/CVPR.2009.5206594&#34;&gt;&lt;strong&gt;Learning to detect unseen object classes by between-class attribute transfer&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; by &lt;em&gt;Christoph H. Lampert, Hannes Nickisch and Stefan Harmeling&lt;/em&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Related Works&lt;/h2&gt; &#xA;&lt;h3&gt;Git Repos&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/dalinvip/Awesome-ChatGPT&#34;&gt;&lt;strong&gt;Awesome-ChatGPT&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; &lt;code&gt;ChatGPT资料汇总学习，持续更新......&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/f/awesome-chatgpt-prompts&#34;&gt;&lt;strong&gt;Awesome ChatGPT Prompts&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; &lt;code&gt;In this repository, you will find a variety of prompts that can be used with ChatGPT.&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/BlinkDL/ChatRWKV&#34;&gt;&lt;strong&gt;ChatRWKV&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; &lt;code&gt;ChatRWKV is like ChatGPT but powered by my RWKV (100% RNN) language model, which is the only RNN (as of now) that can match transformers in quality and scaling, while being faster and saves VRAM. Training sponsored by Stability EleutherAI.&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/chenweiphd/ChatGPT-Hub&#34;&gt;&lt;strong&gt;ChatGPT-Hub&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; &lt;code&gt;ChatGPT资源汇总&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/lucidrains/PaLM-rlhf-pytorch&#34;&gt;&lt;strong&gt;PaLM-rlhf-pytorch&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; &lt;code&gt;Implementation of RLHF (Reinforcement Learning with Human Feedback) on top of the PaLM architecture.&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/BAAI-WuDao/Data&#34;&gt;&lt;strong&gt;BAAI-WuDao/Data&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; &lt;code&gt;“悟道”项目构建了高质量的数据集，用于支撑大模型的训练和测评工作，本仓库提供所有开源数据集的链接。&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/hpcaitech/ColossalAI&#34;&gt;&lt;strong&gt;Colossal-AI&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; &lt;code&gt;Colossal-AI provides a collection of parallel components for you. We aim to support you to write your distributed deep learning models just like how you write your model on your laptop. We provide user-friendly tools to kickstart distributed training and inference in a few lines.&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Articles&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://research.nccgroup.com/2022/12/05/exploring-prompt-injection-attacks/&#34;&gt;&lt;strong&gt;Exploring Prompt Injection Attacks&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt;by Jose Selvi&lt;br&gt; &lt;code&gt;Prompt Injection is a new vulnerability that is affecting some AI/ML models and, in particular, certain types of language models using prompt-based learning.&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/590655677?utm_source=wechat_session&amp;amp;utm_medium=social&amp;amp;utm_oi=714896487502315520&amp;amp;s_r=0&#34;&gt;&lt;strong&gt;ChatGPT发展历程、原理、技术架构详解和产业未来&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt;by 陈巍&lt;br&gt; &lt;code&gt;本文将介绍ChatGPT的特点、功能、技术架构、局限、产业应用、投资机会和未来。作者本人曾担任华为系自然语言处理（ NLP ）企业的首席科学家。&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Blogs&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1&#34;&gt;&lt;strong&gt;How does GPT Obtain its Ability?&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt;by Yao Fu&lt;br&gt; &lt;code&gt;Tracing emergent abilities of language models to their sources.&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.hpc-ai.tech/blog/colossal-ai-chatgpt&#34;&gt;&lt;strong&gt;Open source solution replicates ChatGPT training process&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; &lt;code&gt;Colossal-AI, as one of the hottest open-source solutions for large AI models, presents an open-source low-cost ChatGPT equivalent implementation process.&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Demos&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://live.openbmb.org/models/bee&#34;&gt;&lt;strong&gt;CPM-Bee&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; &lt;code&gt;CPM-Bee是一个开源的双语预训练语言模型，参数量为10B，拥有十余种原生能力和强大的通用语言能力，并支持结构化输入和输出。&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Reports&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/res/%E4%B9%85%E8%B0%A6%EF%BC%9AChatGPT%E7%BA%AA%E8%A6%81%E5%88%86%E4%BA%AB.pdf&#34;&gt;&lt;strong&gt;久谦：ChatGPT纪要分享&lt;/strong&gt;&lt;/a&gt;,&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/res/%E5%9B%BD%E6%B3%B0%E5%90%9B%E5%AE%89ChatGPT%E7%A0%94%E7%A9%B6%E6%A1%86%E6%9E%B6.pdf&#34;&gt;&lt;strong&gt;国泰君安ChatGPT研究框架&lt;/strong&gt;&lt;/a&gt;,&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/res/%E5%93%88%E5%B0%94%E6%BB%A8%E5%B7%A5%E4%B8%9A%E5%A4%A7%E5%AD%A6%EF%BC%9AChatGPT%E8%B0%83%E7%A0%94%E6%8A%A5%E5%91%8A.pdf&#34;&gt;&lt;strong&gt;哈尔滨工业大学：ChatGPT调研报告&lt;/strong&gt;&lt;/a&gt;,&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Lectures&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/res/Chain%20of%20Thought%20Prompting%20for%20Large%20Language%20Model%20Reasoning.pdf&#34;&gt;&lt;strong&gt;Chain of Thought Prompting for Large Language Model Reasoning&lt;/strong&gt;&lt;/a&gt;,&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Related Works&lt;/h2&gt; &#xA;&lt;h3&gt;Git Repos&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/dalinvip/Awesome-ChatGPT&#34;&gt;&lt;strong&gt;Awesome-ChatGPT&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; &lt;code&gt;ChatGPT资料汇总学习，持续更新......&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/f/awesome-chatgpt-prompts&#34;&gt;&lt;strong&gt;Awesome ChatGPT Prompts&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; &lt;code&gt;In this repository, you will find a variety of prompts that can be used with ChatGPT.&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/BlinkDL/ChatRWKV&#34;&gt;&lt;strong&gt;ChatRWKV&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; &lt;code&gt;ChatRWKV is like ChatGPT but powered by my RWKV (100% RNN) language model, which is the only RNN (as of now) that can match transformers in quality and scaling, while being faster and saves VRAM. Training sponsored by Stability EleutherAI.&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/chenweiphd/ChatGPT-Hub&#34;&gt;&lt;strong&gt;ChatGPT-Hub&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; &lt;code&gt;ChatGPT资源汇总&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/lucidrains/PaLM-rlhf-pytorch&#34;&gt;&lt;strong&gt;PaLM-rlhf-pytorch&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; &lt;code&gt;Implementation of RLHF (Reinforcement Learning with Human Feedback) on top of the PaLM architecture.&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/BAAI-WuDao/Data&#34;&gt;&lt;strong&gt;BAAI-WuDao/Data&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; &lt;code&gt;“悟道”项目构建了高质量的数据集，用于支撑大模型的训练和测评工作，本仓库提供所有开源数据集的链接。&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/hpcaitech/ColossalAI&#34;&gt;&lt;strong&gt;Colossal-AI&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; &lt;code&gt;Colossal-AI provides a collection of parallel components for you. We aim to support you to write your distributed deep learning models just like how you write your model on your laptop. We provide user-friendly tools to kickstart distributed training and inference in a few lines.&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Articles&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://research.nccgroup.com/2022/12/05/exploring-prompt-injection-attacks/&#34;&gt;&lt;strong&gt;Exploring Prompt Injection Attacks&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt;by Jose Selvi&lt;br&gt; &lt;code&gt;Prompt Injection is a new vulnerability that is affecting some AI/ML models and, in particular, certain types of language models using prompt-based learning.&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/590655677?utm_source=wechat_session&amp;amp;utm_medium=social&amp;amp;utm_oi=714896487502315520&amp;amp;s_r=0&#34;&gt;&lt;strong&gt;ChatGPT发展历程、原理、技术架构详解和产业未来&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt;by 陈巍&lt;br&gt; &lt;code&gt;本文将介绍ChatGPT的特点、功能、技术架构、局限、产业应用、投资机会和未来。作者本人曾担任华为系自然语言处理（ NLP ）企业的首席科学家。&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Blogs&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1&#34;&gt;&lt;strong&gt;How does GPT Obtain its Ability?&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt;by Yao Fu&lt;br&gt; &lt;code&gt;Tracing emergent abilities of language models to their sources.&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.hpc-ai.tech/blog/colossal-ai-chatgpt&#34;&gt;&lt;strong&gt;Open source solution replicates ChatGPT training process&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; &lt;code&gt;Colossal-AI, as one of the hottest open-source solutions for large AI models, presents an open-source low-cost ChatGPT equivalent implementation process.&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Demos&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://live.openbmb.org/models/bee&#34;&gt;&lt;strong&gt;CPM-Bee&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; &lt;code&gt;CPM-Bee是一个开源的双语预训练语言模型，参数量为10B，拥有十余种原生能力和强大的通用语言能力，并支持结构化输入和输出。&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Reports&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/res/%E4%B9%85%E8%B0%A6%EF%BC%9AChatGPT%E7%BA%AA%E8%A6%81%E5%88%86%E4%BA%AB.pdf&#34;&gt;&lt;strong&gt;久谦：ChatGPT纪要分享&lt;/strong&gt;&lt;/a&gt;,&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/res/%E5%9B%BD%E6%B3%B0%E5%90%9B%E5%AE%89ChatGPT%E7%A0%94%E7%A9%B6%E6%A1%86%E6%9E%B6.pdf&#34;&gt;&lt;strong&gt;国泰君安ChatGPT研究框架&lt;/strong&gt;&lt;/a&gt;,&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/res/%E5%93%88%E5%B0%94%E6%BB%A8%E5%B7%A5%E4%B8%9A%E5%A4%A7%E5%AD%A6%EF%BC%9AChatGPT%E8%B0%83%E7%A0%94%E6%8A%A5%E5%91%8A.pdf&#34;&gt;&lt;strong&gt;哈尔滨工业大学：ChatGPT调研报告&lt;/strong&gt;&lt;/a&gt;,&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Lectures&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KSESEU/LLMPapers/raw/main/res/Chain%20of%20Thought%20Prompting%20for%20Large%20Language%20Model%20Reasoning.pdf&#34;&gt;&lt;strong&gt;Chain of Thought Prompting for Large Language Model Reasoning&lt;/strong&gt;&lt;/a&gt;,&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/77821103?s=400&amp;amp;u=17b0ffcd148c697c9f604d8ed4241ffa8fb62257&amp;amp;v=4&#34; alt=&#34;img&#34; style=&#34;zoom:25%; vertical-align: middle&#34;&gt; Researcher Recruitment 科研人员招聘&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;Knowledge Science and Engineering Lab&lt;/em&gt; is recruiting researchers! You are welcome to apply for the following positions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Research Assistant&lt;/strong&gt;: Bachelor degree or above, proficient in Python/Java, familiar with machine learning espicially deep learning models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Postdoctoral Fellow&lt;/strong&gt;: Doctoral research in Artificial Intelligence, published at least 3 high-quality papers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Lecturer, Associate Professor and Professor&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you are interested in our research and meet the above requirements, feel free to contact Prof. &lt;a href=&#34;https://cse.seu.edu.cn/2019/0103/c23024a257134/page.htm&#34;&gt;Guilin Qi&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;知识科学与工程实验室&lt;/em&gt;正在招聘科研人员！欢迎申请以下岗位：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;科研助理&lt;/strong&gt;：本科学历以上，精通Python/Java，熟悉机器学习，特别是深度学习模型。&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;博士后&lt;/strong&gt;：博士研究人工智能相关方向，发表至少3篇高水平论文。&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;讲师、副教授、教授等教职&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;如果您对我们的研究工作感兴趣并满足以上要求，欢迎您与&lt;a href=&#34;https://cse.seu.edu.cn/2019/0103/c23024a257135/page.htm&#34;&gt;漆桂林&lt;/a&gt;教授联系。&lt;/p&gt;</summary>
  </entry>
</feed>