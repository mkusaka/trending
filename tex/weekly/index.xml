<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TeX Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-04T02:10:11Z</updated>
  <subtitle>Weekly Trending of TeX in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>anoma/whitepaper</title>
    <updated>2023-06-04T02:10:11Z</updated>
    <id>tag:github.com,2023-06-04:/anoma/whitepaper</id>
    <link href="https://github.com/anoma/whitepaper" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;This is a working draft of the the Anoma whitepaper.&lt;/p&gt; &#xA;&lt;p&gt;Run &lt;code&gt;make&lt;/code&gt; to recreate the PDF from the latest Markdown source.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>edoliberty/vector-search-class-notes</title>
    <updated>2023-06-04T02:10:11Z</updated>
    <id>tag:github.com,2023-06-04:/edoliberty/vector-search-class-notes</id>
    <link href="https://github.com/edoliberty/vector-search-class-notes" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Class notes for the course &#34;Long Term Memory in AI - Vector Search and Databases&#34; COS 495 @ Princeton Fall 2023&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Long Term Memory in AI - Vector Search and Databases&lt;/h1&gt; &#xA;&lt;p&gt;COS 495 is given at Princeton during the Fall semester 2023 by&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=QHS_pZAAAAAJ&amp;amp;hl=en&#34;&gt;Edo Liberty&lt;/a&gt;, the Founder and CEO of &lt;a href=&#34;https://www.pinecone.io&#34;&gt;Pinecone&lt;/a&gt;, the world&#39;s leading Vector Database.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=0eFZtREAAAAJ&amp;amp;hl=en%5D&#34;&gt;Matthijs Douze&lt;/a&gt; the architect and main developer of &lt;a href=&#34;https://github.com/facebookresearch/faiss&#34;&gt;FAISS&lt;/a&gt; the most popular and advanced open source library for vector search.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The course covers the core concepts, algorithms, and data structures used for modern vector search systems and platforms. An advanced undergraduate or graduate student with some hands-on experience in linear algebra, probability, algorithms, and data structures should be able to follow this course.&lt;/p&gt; &#xA;&lt;h2&gt;Abstract&lt;/h2&gt; &#xA;&lt;p&gt;Long Term Memory is a fundamental capability in the modern AI Stack. At their core, these systems are using Vector search. Vector search is also a fundamental tool for systems that manipulate large collections of media like search engines, knowledge bases, content moderation tools, recommendation systems, etc. As such, the discipline lays at the intersection of Artificial Intelligence and Database Management Systems. This course will cover the scientific foundations and practical implementation of vector search applications, algorithms, and systems. The course will be evaluated with project and in-class presentation.&lt;/p&gt; &#xA;&lt;h2&gt;Syllabus&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;The class contents below are tentative.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Introduction to Vector Search [Matthijs]&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Embeddings as an information bottleneck. Instead of learning end-to-end, use embeddings as an intermediate representation&lt;/li&gt; &#xA;   &lt;li&gt;Advantages: scalability, instant updates, and explainability&lt;/li&gt; &#xA;   &lt;li&gt;Typical volumes of data and scalability. Embeddings are the only way to manage / access large databases&lt;/li&gt; &#xA;   &lt;li&gt;The embedding contract: the embedding extractor and embedding indexer agree on the meaning of the distance. Separation of concerns.&lt;/li&gt; &#xA;   &lt;li&gt;The vector space model in information retrieval&lt;/li&gt; &#xA;   &lt;li&gt;Vector embeddings in machine learning&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Text embeddings [Matthijs]&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;2-layer word embeddings. Word2vec and fastText, obtained via a factorization of a co-occurrence matrix. Embedding arithmetic: king + woman - man = queen, (already based on similarity search)&lt;/li&gt; &#xA;   &lt;li&gt;Sentence embeddings: How to train, masked LM. Properties of sentence embeddings.&lt;/li&gt; &#xA;   &lt;li&gt;Large Language Models: reasoning as an emerging property of a LM. What happens when the training set = the whole web&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Image embeddings [Matthijs]&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Pixel structures of images. Early works on direct pixel indexing&lt;/li&gt; &#xA;   &lt;li&gt;Traditional CV models. Global descriptors (GIST). Local descriptors (SIFT and friends)Direct indexing of local descriptors for image matching, local descriptor pooling (Fisher, VLAD)&lt;/li&gt; &#xA;   &lt;li&gt;Convolutional Neural Nets. Off-the-shelf models. Trained specifically (contrastive learning, self-supervised learning)&lt;/li&gt; &#xA;   &lt;li&gt;Modern Computer Vision models&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Practical indexing [Matthijs]&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;How an index works: basic functionalities (search, add). Optional functionalities: snapshot, incremental add, remove&lt;/li&gt; &#xA;   &lt;li&gt;k-NN search vs. range search&lt;/li&gt; &#xA;   &lt;li&gt;maintaining top-k results&lt;/li&gt; &#xA;   &lt;li&gt;Criteria: Speed / accuracy / memory usage / updateability / index construction time&lt;/li&gt; &#xA;   &lt;li&gt;Early works on bag-of-visual-words inspiration, based on quantization&lt;/li&gt; &#xA;   &lt;li&gt;Voronoi diagram with search buckets&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Low Dimensional Vector Search [Edo]&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;k-d tree, space partitioning based algorithms, proof, structures, and asymptotic behavior&lt;/li&gt; &#xA;   &lt;li&gt;Probabilistic inequalities. Recap of basic inequalities: Markov, Chernoof, Hoeffding&lt;/li&gt; &#xA;   &lt;li&gt;Concentration Of Measure phenomena. Orthogonality of random vectors&lt;/li&gt; &#xA;   &lt;li&gt;Curse of dimensionality. The failure of space partitioning&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Dimensionality Reduction [Edo]&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Principal Components Analysis, optimal dimension reduction in the sum of squared distance measure&lt;/li&gt; &#xA;   &lt;li&gt;Fast PCA algorithms&lt;/li&gt; &#xA;   &lt;li&gt;Random Projections. Gaussian random i.i.d. dimension reduction&lt;/li&gt; &#xA;   &lt;li&gt;Fast Random Projections. Accelerating the above to near linear time&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Locality Sensitive Hashing [Edo]&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Definition of Approximate Nearest Neighbor Search (ANNS)&lt;/li&gt; &#xA;   &lt;li&gt;Criteria: Speed / accuracy / memory usage / updateability / index construction time&lt;/li&gt; &#xA;   &lt;li&gt;Definition of Locality Sensitive Hashing and examples&lt;/li&gt; &#xA;   &lt;li&gt;The LSH Algorithm&lt;/li&gt; &#xA;   &lt;li&gt;LSH Analysis, proof of correctness, and asymptotics&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Clustering [Edo]&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Semantic clustering: properties (purity, as aid for annotation)&lt;/li&gt; &#xA;   &lt;li&gt;Clustering from a similarity graph (spectral clustering, agglomerative clustering)&lt;/li&gt; &#xA;   &lt;li&gt;Vector clustering: mean squared error criterion. Tradeoff with number of clusters&lt;/li&gt; &#xA;   &lt;li&gt;Relationship between vector clustering and quantization (OOD extension)&lt;/li&gt; &#xA;   &lt;li&gt;The k-means clustering measure and Lloyd&#39;s algorithm&lt;/li&gt; &#xA;   &lt;li&gt;Lloyd&#39;s optimality conditions&lt;/li&gt; &#xA;   &lt;li&gt;Initialization strategies (kmeans++, progressive dimensions with PCA)&lt;/li&gt; &#xA;   &lt;li&gt;The inverted file model. Relationship with sparse matrices&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Quantization for lossy vector compression [Matthijs]&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Vector quantization is a topline (directly optimizes the objective)&lt;/li&gt; &#xA;   &lt;li&gt;Binary quantization and hamming comparison&lt;/li&gt; &#xA;   &lt;li&gt;Product quantization. Chunked vector quantization. Optimized vector quantization&lt;/li&gt; &#xA;   &lt;li&gt;Additive quantization. Extension of product quantization. Difficulty in training approximations (Residual quantization, CQ, TQ, LSQ, etc.)&lt;/li&gt; &#xA;   &lt;li&gt;Cost of coarse quantization vs. inverted list scanning&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Graph based indexes [Guest lecture]&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Early works: hierarchical k-means&lt;/li&gt; &#xA;   &lt;li&gt;Neighborhood graphs. How to construct them. Nearest Neighbor Descent&lt;/li&gt; &#xA;   &lt;li&gt;Greedy search in Neighborhood graphs. That does not work -- need long jumps&lt;/li&gt; &#xA;   &lt;li&gt;HNSW. A practical hierarchical graph-based index&lt;/li&gt; &#xA;   &lt;li&gt;NSG. Evolving a graph k-NN graph&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Computing Hardware and Vector Search [Guest lecture]&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Computing platform: local vs. service / CPU vs. GPU&lt;/li&gt; &#xA;   &lt;li&gt;efficient implementation of brute force search&lt;/li&gt; &#xA;   &lt;li&gt;distance computations for product quantization -- tradeoffs. SIMD implementation&lt;/li&gt; &#xA;   &lt;li&gt;Parallelization and distribution -- sharding vs. inverted list distribution&lt;/li&gt; &#xA;   &lt;li&gt;Using co-processors (GPUs)&lt;/li&gt; &#xA;   &lt;li&gt;Using a hierarchy of memory types (RAM + SSD or RAM + GPU RAM)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Advanced topics -- articles to be presented by students [all]&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Vectors in Generative AI&lt;/li&gt; &#xA;   &lt;li&gt;Neural quantization&lt;/li&gt; &#xA;   &lt;li&gt;Vector Databases&lt;/li&gt; &#xA;   &lt;li&gt;Beyond feature extraction and indexing: neural indexing&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In class project presentations&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Selected literature&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Product quantization (PQ) and inverted file: &lt;a href=&#34;https://hal.inria.fr/inria-00514462v2/document&#34;&gt;“Product quantization for nearest neighbor search”&lt;/a&gt;, Jégou &amp;amp; al., PAMI 2011.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Fast k-selection on wide SIMD architectures: &lt;a href=&#34;https://arxiv.org/abs/1702.08734&#34;&gt;“Billion-scale similarity search with GPUs”&lt;/a&gt;, Johnson &amp;amp; al, ArXiv 1702.08734, 2017&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;HNSW indexing method: &lt;a href=&#34;https://arxiv.org/abs/1603.09320&#34;&gt;&#34;Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs&#34;&lt;/a&gt;, Malkov &amp;amp; al., ArXiv 1603.09320, 2016&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In-register vector comparisons: &lt;a href=&#34;https://arxiv.org/abs/1812.09162&#34;&gt;&#34;Quicker ADC : Unlocking the Hidden Potential of Product Quantization with SIMD&#34;&lt;/a&gt;, André et al, PAMI&#39;19, also used in &lt;a href=&#34;https://arxiv.org/abs/1908.10396&#34;&gt;&#34;Accelerating Large-Scale Inference with Anisotropic Vector Quantization&#34;&lt;/a&gt;, Guo, Sun et al, ICML&#39;20.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Graph-based indexing method NSG: &lt;a href=&#34;https://arxiv.org/abs/1707.00143&#34;&gt;&#34;Fast Approximate Nearest Neighbor Search With The Navigating Spreading-out Graph&#34;&lt;/a&gt;, Cong Fu et al, VLDB 2019.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Local search quantization (an additive quantization method): &lt;a href=&#34;https://drive.google.com/file/d/1dDuv6fQozLQFS2AJoNNFGTH499QIp_vO/view&#34;&gt;&#34;Revisiting additive quantization&#34;&lt;/a&gt;, Julieta Martinez, et al. ECCV 2016 and &lt;a href=&#34;https://openaccess.thecvf.com/content_ECCV_2018/html/Julieta_Martinez_LSQ_lower_runtime_ECCV_2018_paper.html&#34;&gt;&#34;LSQ++: Lower running time and higher recall in multi-codebook quantization&#34;&lt;/a&gt;, Julieta Martinez, et al. ECCV 2018.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Learning-based quantizer: &lt;a href=&#34;https://openaccess.thecvf.com/content_ICCV_2019/html/Morozov_Unsupervised_Neural_Quantization_for_Compressed-Domain_Similarity_Search_ICCV_2019_paper.html&#34;&gt;&#34;Unsupervised Neural Quantization for Compressed-Domain Similarity Search&#34;&lt;/a&gt;, Morozov and Banenko, ICCV&#39;19&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Neural indexing: &lt;a href=&#34;https://arxiv.org/abs/2202.06991&#34;&gt;&#34;Transformer Memory as a Differentiable Search Index&#34;&lt;/a&gt;, Tan &amp;amp; al. ArXiV&#39;22&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The hybrid RAM-disk index: &lt;a href=&#34;https://proceedings.neurips.cc/paper/2019/hash/09853c7fb1d3f8ee67a61b6bf4a7f8e6-Abstract.html&#34;&gt;&#34;Diskann: Fast accurate billion-point nearest neighbor search on a single node&#34;&lt;/a&gt;, Subramanya &amp;amp; al. NeurIPS&#39;19&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The nearest neighbor descent method: &lt;a href=&#34;https://www.ambuehler.ethz.ch/CDstore/www2011/proceedings/p577.pdf&#34;&gt;Efficient k-nearest neighbor graph construction for generic similarity measures&lt;/a&gt; from Dong et al., WWW&#39;11&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Build&lt;/h2&gt; &#xA;&lt;p&gt;On unix-like systems with the bibtex and pdflatex available you should be able to do this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone git@github.com:edoliberty/vector-search-class-notes.git&#xA;cd vector-search-class-notes&#xA;./build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contribute&lt;/h2&gt; &#xA;&lt;p&gt;These class notes are intended to be used freely by academics anywhere, students and professors alike. Please feel free to contribute in the form of pull requests or opening issues.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>urgent-learner/mlentary</title>
    <updated>2023-06-04T02:10:11Z</updated>
    <id>tag:github.com,2023-06-04:/urgent-learner/mlentary</id>
    <link href="https://github.com/urgent-learner/mlentary" rel="alternate"></link>
    <summary type="html">&lt;p&gt;quasi-open-source introductory book about machine learning, emphasis on geometry and modern concepts&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;A quasi-open-source effort for a book on machine learning.&lt;/h1&gt; &#xA;&lt;p&gt;A quasi-open-source introductory book on machine learning, focusing on geometry and modern concepts!&lt;/p&gt; &#xA;&lt;p&gt;CONTENTS:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;repo organization&#xA;    latex and python requirements&#xA;    desired directory structure&#xA;    branch etiquette&#xA;vision for the book/notes&#xA;    story arc and learning goals&#xA;    what we want for writing&#xA;    what we want for code&#xA;    what we want for figures&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;repo organization&lt;/h2&gt; &#xA;&lt;h4&gt;latex and python requirements&lt;/h4&gt; &#xA;&lt;p&gt;Python 3.8, numpy ?.? etc FILLIN&lt;/p&gt; &#xA;&lt;p&gt;Run &lt;code&gt;make ml&lt;/code&gt; to compile &lt;code&gt;mlentary.tex&lt;/code&gt; and display the pdf. I use &lt;code&gt;pdflatex&lt;/code&gt; to compile latex source and &lt;code&gt;evince&lt;/code&gt; to view pdfs, but you can use whatever you like: (just edit your &lt;code&gt;Makefile&lt;/code&gt; and include your Makefile in your &lt;code&gt;.gitignore&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;h4&gt;desired directory structure&lt;/h4&gt; &#xA;&lt;p&gt;The following is NOT the current directory structure. It is instead what I WANT the repo to look like. I haven&#39;t had time to organize.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mlentary.tex&#xA;&#xA;tex-source/&#xA;    sam.sty&#xA;    ... [a bunch of `.tex`s, to be included in `mlentary.tex`]&#xA;&#xA;figures/&#xA;    ... [a bunch of `.png`s, most generated by little scripts in `code/`]&#xA;&#xA;code/&#xA;    plot_helpers.py&#xA;    extra/&#xA;        ... [a bunch of `.py`s for little examples not part of a big task]&#xA;    mnist/&#xA;        ... [a bunch of `.py`s for basic and advanced digit classification]&#xA;    words/&#xA;        ... [a bunch of `.py`s for basic and advanced text extrapolation]&#xA;    faces/&#xA;        ... [a bunch of `.py`s for basic and advanced face generation]&#xA;    polls/&#xA;        ... [a bunch of `.py`s for basic political poll prediction]&#xA;    robot/&#xA;        ... [a bunch of `.py`s for basic robotic arm control]&#xA;    atari/&#xA;        ... [a bunch of `.py`s for basic video game exploration]&#xA;&#xA;Makefile&#xA;&#xA;README.md&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;branch etiquette&lt;/h4&gt; &#xA;&lt;p&gt;EDIT: let&#39;s use the system of claiming or assigning parts of work (e.g. at least as fine-grained as which body.N.M file, if that is applicable) so that everone can see and check before working on it themselves. Let&#39;s adopt a format like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Issue-&#34;current_number&#34;: &#34;a brief description&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;New ideas can be discussed in this thread so that everyone can read.&lt;/p&gt; &#xA;&lt;p&gt;EDIT: instead of adding everyone as a collaborator immediately, I will instead accept pull requests and then only after making sure they look alright will I add you!&lt;/p&gt; &#xA;&lt;p&gt;We will primarily make commits to branches called &lt;code&gt;unit0&lt;/code&gt;, ..., &lt;code&gt;unit6&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;BRANCH...       ...CONTAINS A...&#xA;&#xA;main            (most recent quasi-coherent draft)&#xA;release         (older but refined and thoroughly checked draft)&#xA;unit0           (most recent compiling draft of: friendly invitation to ML)&#xA;unit1           (most recent compiling draft of: linear models)&#xA;unit2           (most recent compiling draft of: nonlinear models)&#xA;unit3           (most recent compiling draft of: deep learning)&#xA;unit4           (most recent compiling draft of: modeling uncertainty)&#xA;unit5           (most recent compiling draft of: reinforcement learning)&#xA;unit6           (most recent compiling draft of: appendices etc)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We will have a &lt;code&gt;main&lt;/code&gt; branch for our communal &#34;nearly latest draft so far&#34; and a more sacred, less frequently-committed-to called &lt;code&gt;release&lt;/code&gt; for a less recent containing a more thoroughly refined and checked draft. The branches &lt;code&gt;unit1&lt;/code&gt; through &lt;code&gt;unit5&lt;/code&gt; hold our 5 &#34;actual&#34; chunks of content, &lt;code&gt;unit0&lt;/code&gt; is to be a friendly invitation and orienting up-ramp to the subject, and &lt;code&gt;unit6&lt;/code&gt; is for miscellaneous items such as a short &#34;what&#39;s next&#34; section and maybe a sketch of some mathematical prerequisites.&lt;/p&gt; &#xA;&lt;p&gt;Easter Egg. 1729 is a magic number. Direct message sam (&lt;code&gt;bohrium&lt;/code&gt;) on discord that you&#39;ve seen this magic number. This tells me you&#39;ve looked at this!&lt;/p&gt; &#xA;&lt;p&gt;So the information flow goes something like&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    authors (y&#39;all and me)&#xA;          /       \       \&#xA;         |write    |check  \_check&#xA;         |and      |before   \really&#xA;         |program  |merging   \_carefully&#xA;         |          \           \before&#xA;         v           |           \_merging&#xA;                     |             |&#xA;        unit0  --+   v             v&#xA;                  \ &#xA;         ...       ------&amp;gt;  main  -----------&amp;gt;  release&#xA;                  /&#xA;        unit6  --+&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you find it helpful, feel free to make one or more &#34;personal&#34; branches for experimental stuff you want to try. If you do this, use the form &lt;code&gt;username-unit-branchdescription&lt;/code&gt; for the branch name. For example, if your github username is &lt;code&gt;galapagos&lt;/code&gt; and you&#39;re trying out some new thing with kernel visualization for unit 2, then an appropriate branch name could be &lt;code&gt;galapagos-unit2-kerviz&lt;/code&gt; Of course, you could make these branches in just your local repo, but if you also do so in the remote repo then others can take a look.&lt;/p&gt; &#xA;&lt;h2&gt;vision for the book/notes&lt;/h2&gt; &#xA;&lt;h4&gt;story arc and learning goals&lt;/h4&gt; &#xA;&lt;p&gt;The basic story arc is as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;unit0 --- what does it mean to learn from examples?&#xA;unit1 --- visualize high-D space to learn-from-examples via linear models&#xA;unit2 --- extend unit1&#39;s superpower to new tasks by hand-crafting nonlinearities&#xA;unit3 --- extend unit2&#39;s superpower to richer tasks by automating the crafting of nonlinearities&#xA;unit4 --- extend unit3&#39;s superpower to tasks with structured uncertainty/diversity&#xA;unit5 --- learn-from-rewards by framing learning-from-rewards problems as learning-from-examples problems&#xA;&#xA;unit6 --- extras (e.g. where to go next, helpful prereq refreshers)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;What are 3 concrete verb phrases I want the notes to &lt;em&gt;enable&lt;/em&gt; a reader to do?&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Attack arbitrary prediction-from-example tasks by designing, training, and tuning an appropriate linear model from scratch.&lt;/li&gt; &#xA; &lt;li&gt;Tailor nonlinearities, priors, and optimization to navigate the generalization-approximation tradeoff.&lt;/li&gt; &#xA; &lt;li&gt;Impress friends by completing a small project that actually helps or entertains in daily life.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;I avoided words like &#34;understand&#34; above because such words are too temptingly vague. But if I allow such words then I add these three verb phrases:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Diagnose, at a glance, glaring missteps in a friend&#39;s ML project.&lt;/li&gt; &#xA; &lt;li&gt;Frame common ML algorithms &lt;em&gt;probabilistically&lt;/em&gt; to reveal what domain assumptions the algorithm incorporates.&lt;/li&gt; &#xA; &lt;li&gt;Parse an old deep learning paper (say, 2017) well enough to explain to a friend the paper&#39;s main problem and contribution.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Easter Egg. (-1/12) is a magic number. Direct message sam (&lt;code&gt;bohrium&lt;/code&gt;) on discord that you&#39;ve seen this magic number. This tells me you&#39;ve looked at this!&lt;/p&gt; &#xA;&lt;p&gt;In a bit more detail, something like the following makes sense to me.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Unit 0: Prologue.  Learning from Examples.&#xA;  Lecture 1a: What is Machine Learning?&#xA;  Lecture 1b: Tiny Example: Classifying Handwritten Digits&#xA;  Lecture 1c: How well did we do?  %  Training vs Testing Error ; Generalization, Optimization, Approximation&#xA;  Lecture 1d: How can we do better?  Survey of rest of notes&#xA;&#xA;Unit 1: Optimize Linear Models to Learn from Examples.&#xA;  Lecture 1a: Hyperplanes, Likelihoods, Goodness of Fit&#xA;  Lecture 1b: Accelerate Learning using Gradient Descent&#xA;  Lecture 1c: Priors and Intrinsic Plausibility&#xA;  Lecture 1d: Model Selection&#xA;&#xA;  Coding Lecture 1: Coding the image classifier from lectures, from scratch.  Examining behavior on a concrete image.&#xA;&#xA;  Homework 1a: Hyperplanes and Dataclouds&#xA;&#xA;  Homework 1b: Learning.  Weights-vs-Correlations&#xA;&#xA;  Project 1a: Text Classification % not 3-Way.  Just 2-way.&#xA;&#xA;  Project 1b: Measuring Overfitting; Hyperparameter Search&#xA;&#xA;Unit 2: Engineering Features (and Friends) by Hand&#xA;  Lecture 2a: Feature Engineering&#xA;    Bias Trick&#xA;    Black Holes&#xA;    Specialty Features, e.g.\ Trig or Threshold&#xA;    Interpreting Weights (vs Correlations)&#xA;    Feature Selection and Generalization&#xA;&#xA;  Lecture 2b: Loss Functions, Regularization, Margins % Logistic, Hinge, Perceptron, Square; acc bound; L-inf, L2, L1; prob interp&#xA;    Humble Models, Acc Bound&#xA;    Regularization: Why and How&#xA;    Probabilistic Interpretation, Likelihood, Gradient Descent&#xA;    Margin Maximization and Supports&#xA;    Feature Selection and Optimization (Convexity)&#xA;&#xA;  Lecture 2c: Data-based Featurization % Kernels, PCA, Quantiles &amp;amp; Trees&#xA;    Quantiles (Binning), Stumps, Trees&#xA;    Linear Dimension Reduction: PCA&#xA;    Matrix Factorization Perspective, ICA&#xA;    Nonlinear Dimension Reduction: Similarity-to-Landmark Embeddings.  Kernels&#xA;    Designing Similarity Functions.  Consideration for Text, Images.&#xA;&#xA;  Lecture 2d: Reducing to Linear Learning % Softmax, Regression, Sequences, etc&#xA;    Specialized Readouts&#xA;    Soft Classes, Multiple Classes&#xA;    Regression&#xA;    Factoring Big Problems.  E.g.\ Sequences&#xA;    Shared Features for Multiple Outputs.  Examples of Overall Architectures&#xA;&#xA;  Coding Lecture 2: SVM with RBF Features from Scratch, for Image Classification&#xA;    Plan, Pictures, Meeting Data&#xA;    Forward Model&#xA;    Gradient Step&#xA;    Training and Model Selection&#xA;    Testing, Interpreting Weights&#xA;&#xA;  Homework 2a: Feature Geometry, Geometry of Margins and Regression&#xA;    Matching Features (and Losses/Architectures) and Domain Knowledge, Practice&#xA;    Recognizing Decision Boundaries&#xA;    Probabilistic Interpretation of Losses, Regularizers&#xA;    Support Vector Bound.  Perceptron Update.  Perceptron worse generalization.&#xA;    Implicit Regularization in Least Squares Regression by Gradient Descent&#xA;&#xA;  Homework 2b: Kernel Trick&#xA;    Data-Dependent Features.  Similarity, Inner Products, Generalization.&#xA;    Why We Want Positive Definite Kernel&#xA;    Kernelized Update&#xA;    Recognizing Decision Boundaries % ??&#xA;    Wide Random Features Kernel&#xA;&#xA;  Project 2a: Overfitting to Features on Kaggle-Style Challenge % Selection/Hyperparam/Unsupervised Featurization&#xA;    Adapt the SVM to Kaggle Task; Weights and Overfitting&#xA;    Success Measures.  Accuracy vs Loss.  ROC Curve, Ranking.  Class-imbalance.&#xA;    Feature Selection and Overfitting&#xA;    Quantilization and Overfitting&#xA;    DataSnooping and Overfitting % difficult but important lesson to design!&#xA;&#xA;  Project 2b: 10-Way Image Softmax Classification.  Calibration Prediction Challenge&#xA;    Softmax, Prob Calibration&#xA;    Basic Features.  Add your twist!&#xA;    Badly Set Hyperparams.  Add your insight!&#xA;    Assessing Confidence: Generalization Bounds, Margin-Based Cross-Validation&#xA;    Prediction Challenge&#xA;&#xA;Unit 3: Learning Features from Data&#xA;  Lecture 3a: Shallow Learning % Optimization tricks&#xA;    Architecture, Intuiting Decision Boundaries (Universality)&#xA;    Gradient Descent (Backprop)&#xA;    An Example Learning Trajectory: Vertical vs Horizontal Learning&#xA;    MLP as Butter: add to everything.  Regularization.  Examples.&#xA;    BIC, Johann-Lindenstrauss, Wide Random Features, Generalization.&#xA;&#xA;  Lecture 3b: Deep Learning and Ideas in Architecture % Differentiable-Blah Flexibility in architecture.  Mention RNNs&#xA;    ``To Build It, Use It&#39;&#39;&#xA;    Feature Hierarchy.  Learned ``subroutines&#39;&#39;&#xA;    Ideas in Optimization: Backprop, Weight Initialization, Batch Normalization, ADAM, etc&#xA;    Getting Creative.  Skip Connections etc&#xA;    Curvature-Noise Interactions and Generalization&#xA;&#xA;  Lecture 3c: Symmetry and Locality: Convolution&#xA;    CNN forward encodes basic image priors.  Pooling.&#xA;    Backprop formulas&#xA;    What can K Layers of a CNN Represent?&#xA;    How GPUs Work&#xA;    Pottery Image.  Symmetries and Representations.  Sports.  Melodies.&#xA;&#xA;  Lecture 3d: Symmetry and Locality: Attention&#xA;    Attention forward encodes basic sequence/table priors (sparse).&#xA;    Bells and Whistles: Multiple Heads, Positional Encoding&#xA;    What is a Transformer?&#xA;    What can K Transformer Layers Represent?&#xA;    Differentiable Computers.  Stacks.  Graphs.&#xA;&#xA;  Coding Lecture 3: A Deep Image Classifier from Scratch&#xA;    Plan of what we need to write, picture of architecture&#xA;    Forward Model (Including conv layers)&#xA;    Backward Model (Including conv layers)&#xA;    Quick Checks for Silly Mistakes&#xA;    Training and Interpretation of Weights&#xA;&#xA;  Homework 3a: Decision Boundaries, Backpropagation&#xA;    Backprop for Simple, Fanciful Architecture&#xA;    Vanilla Neural Network Architecture&#xA;    VNN Backprop Formulas and Intuition&#xA;    Recognizing Decision Boundaries for Wide vs Deep&#xA;    Initialization of Weights, Match Comments to Lines in Training Code&#xA;&#xA;  Homework 3b: Zoo of Architectural Ideas % Light Load.  Only few questions, all conceptual.&#xA;    Word Embeddings&#xA;    Siamese, Metric, LeCun Representation Learners&#xA;    RNNs, LSTMs&#xA;    Differentiable X (Renderer etc)&#xA;    Learned Losses / Ecosystems (Wass GAN, etc)&#xA;&#xA;  Project 3a: An Image Sharpener (or maybe next-frame-in-video)? % Perhaps Image Sharpener? % on both Digits and Face data?&#xA;    U-Net Architecture (Pix2Pix but no GAN?)&#xA;    Forward Model&#xA;    Backward Model&#xA;    Training&#xA;    A word on Laplacian Pyramid and Diffusion Models.  Adapt to Image Generator and see bad.&#xA;&#xA;  Project 3b: Text Generation Through Classification&#xA;    Transformer Architecture, Meet the Data&#xA;    Run Training Code&#xA;    Interpreting Learned Weights&#xA;    Playing with Model on Out-of-Distr Data&#xA;    Text Compression Challenge&#xA;&#xA;Unit 4: Modeling Structured Uncertainties&#xA;  Lecture 4a: Square Loss Muddies, Probabilistic Models.  Examples: 3State Traffic, HMM, GMM&#xA;    Structured Uncertainties in the World.  Rapid Holmesian&#xA;    Inferences.  Square Loss Muddies.  Recall prob interp of e.g. least&#xA;    squares regfression.  Unsup Learning.&#xA;    Latents, Marginal Likelihood for Latents, Weights as Latents.&#xA;    Example: 3State Traffic.  Challenge of Normalization.&#xA;    Example: HMM.  0s and SoftLogic and backflow&#xA;    Example: GMM.  Metapriors, hierarchy, Transfer&#xA;&#xA;  Lecture 4b: Inference via Sampling % MH, EM,&#xA;    Challenge.  MH Algorithm.&#xA;    Visualizing MH.  Proposals Matter&#xA;    EM: 3State Traffic example&#xA;    EM: HMM example&#xA;    EM: GMM example&#xA;&#xA;  Lecture 4c: Inference via Variation % MH, EM,&#xA;    EM overview, 3State Traffic example&#xA;    ELBO bound and pingpong KL geometry&#xA;    EM: HMM example (more detail in coding example)&#xA;    EM: GMM example (more detail in pset)&#xA;    Variational Parts as Neural Net&#xA;&#xA;  Lecture 4d: Variational Autoencoders (or Encoders) % so connects back to Unit 3&#xA;    Architecture % comparison to matrix factorization, etc&#xA;    VAEs and ELBO&#xA;    Interpreting Update Intuitively&#xA;    Output side Noise Model (e.g. square loss)&#xA;    Conditional VAEs&#xA;&#xA;  Coding Lecture 4: Expectation-Maximization for Sequences&#xA;    HMM architecture.  Remark on RNNa&#xA;    E-step: dynamic programming&#xA;    M-step with regularization&#xA;    End-to-end reading and mimicry of cookbook text&#xA;    Investigation of State Meanings&#xA;&#xA;  Homework 4a: Gaussian Mixtures for Clustering % k-means as limit&#xA;    forward model&#xA;    E step and M step&#xA;    k-means as limit&#xA;    small-var regularization from prior&#xA;    convergence near minimum&#xA;&#xA;  Homework 4b: Bayes Nets.  NNs as Amortized Inference&#xA;    Marginalization over Latents&#xA;    Weights as Latents in Linear Classification (bowtie vs pipe)&#xA;    BIC and structural penalty&#xA;    Causal modeling&#xA;    Very Basic Occlusion Inference Object Net&#xA;&#xA;  Project 4a: Predicting Political Polls&#xA;    Meeting the Polling Data&#xA;    Forward Model&#xA;    Inference through Sampling&#xA;    Wrestling with Training Difficulties (burnin, etc)&#xA;    Interpreting Latents&#xA;&#xA;  Project 4b: A Deep Image Generator (VAE)&#xA;    Meeting the Face Data.  How to assess success? % maybe met face data in previous unit?&#xA;    Forward Model including reparam trick&#xA;    Backward model and intuitive interpretation&#xA;    Training and Testing&#xA;    Interpreting Latents.  Sources of Prejudice and Capriciousness&#xA;&#xA;Unit 5: Learning while Acting (and from Rewards)&#xA;  Lecture 5a: Rewards and States.  Explore-Exploit Challenge.&#xA;    Learning from Rewards: overview and goal to reduce to supervision&#xA;    Conditional Bandits are kin to Supervised Learning&#xA;    Explore-Exploit Challenge&#xA;    Optimism in Face of Uncertainty&#xA;    Introduction of State Concept.  Total Utility, Episodes,  Online blurs Train and Test&#xA;&#xA;  Lecture 5b: Qualitative Solutions to MDP.  Dynamic Programming, Idea of Bootstrap&#xA;    MDP framework again.  State Correlations (Credit-Assignment) Challenge&#xA;    Dynamic Programming&#xA;    Bootstrap: Online dynamic programming (SARSA)&#xA;    Example Policy in Gridworld&#xA;    Function Approximation (or Partial Observability)&#xA;&#xA;  Lecture 5c: Reduce to Supervision using Q-Learning.&#xA;    Q-Learning vs Naive SARSA&#xA;    Q-Learning plus Exploration Policy&#xA;    Featurization of Huge State (Deep Q-Learning)&#xA;    LSTM for non-Markov State&#xA;    Deadly Triad, Q-Delusion&#xA;&#xA;  Lecture 5d: Non-technical Discussion of: Curiosity, Language, Instruction, World-Models&#xA;    Curiosity Bonuses and Self-Curriculum % mention Adversarial self-play?&#xA;    Curricula in RL and Elsewhere&#xA;    Symbols, Contextual Learning in GPT, Instructions guide Symbol Search in World-Models&#xA;    Evolution of Programming.  Self-Coding Computers&#xA;    Speculations on Future of Learning&#xA;&#xA;  Coding Lecture 5: Q-Learning for a Gridworld, from Scratch&#xA;    World Simulator&#xA;    Q-Table (with flexibility to be function approx)&#xA;    Q-Learning Update&#xA;    Training and Reading out Policy&#xA;    Visualizing Learning curves as add challenges&#xA;&#xA;  Homework 5a: Bandits, Explore-Exploit, Conditional Bandits&#xA;    Epsilon Greedy for Known Timescale&#xA;    UCB Bound vs Epsilon Greedy at Various Timescales&#xA;    Function Approximation in Conditional Bandits&#xA;    Movie Recommendation Update (Linear Embeddings)&#xA;    A Very Basic Adversarial BoardGame Player&#xA;&#xA;  Homework 5b: Q-Learning, on-vs-off policy, Horizons&#xA;    Practice Modeling Situations by MDPs.  Get Rich Enough State Space for Markov Property!&#xA;    Manually Solve Various MDPs&#xA;    Computations of Q-Learning Convergence&#xA;    As Human, Explore a Maze.  Short Essay on Exploration Strategy&#xA;    Baud Rates for Learning Signals Comparison&#xA;&#xA;  Project 5a: Robotic Arm&#xA;    Robot Physics Overview.  Collisions?&#xA;    Clasping Reward Function (Helper to Smooth)&#xA;    Set up Q-learning for parameterized task (what are inputs to Q network etc?) % mention policy gradient?&#xA;    Training&#xA;    Visualize and Describe Motions&#xA;&#xA;  Project 5a: Simple Atari-Style Games % (Fully Observed?  Partially Observed?)&#xA;    Game Family Overview&#xA;    Function Approximation&#xA;    Experience Replay&#xA;    Creative Part: Design Featurization and Exploration Policy!&#xA;    Action Challenge: Learn an Unknown Game in Same Family!&#xA;&#xA;Unit 6: Farewell and Prereq Helpers&#xA;  Bonus Lecture 6a: What We Learned&#xA;  Bonus Lecture 6b: What to Learn Next&#xA;&#xA;  Coding Lecture 6: Python, Numpy, Pytorch&#xA;    Multi-axis arrays.  Speedups.  Index kung-fu.&#xA;    Common numpy maps and zips, filters, reduces, and contractions&#xA;    Example: large-numbers dice statistics, plotting&#xA;    Example: (batched) image filter made by hand&#xA;    Intro to Pytorch&#xA;&#xA;  Homework 6a: Math&#xA;    Types, Functions and Dependencies, Notation&#xA;    Linear Algebra: High Dimensions, Hyperplanes, Linear Maps, Trace and Det; Quadratic Forms, Dot Products, SVD&#xA;    Probability: Expectations, Independence, Bayes, Concentration; Coinflips, Gaussians&#xA;    Optimization: Visualizing Derivative Rules, Sums of Terms (Constraints); Vectors vs Covectors, Overshooting, Convexity&#xA;    Examples: Least Squares; Gaussian Fitting M-Step&#xA;&#xA;  Homework 6b: Programming&#xA;    Matrix Multiply Speed Test&#xA;    Numpy Safari / Treasurehunt&#xA;    Softmax Speed Test&#xA;    Debugging Randomized Code&#xA;    Debugging Many-File Codebase&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;what we want for writing&lt;/h4&gt; &#xA;&lt;p&gt;FILLIN&lt;/p&gt; &#xA;&lt;h4&gt;what we want for code&lt;/h4&gt; &#xA;&lt;p&gt;FILLIN&lt;/p&gt; &#xA;&lt;h4&gt;what we want for figures&lt;/h4&gt; &#xA;&lt;p&gt;FILLIN&lt;/p&gt;</summary>
  </entry>
</feed>